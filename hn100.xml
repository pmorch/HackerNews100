<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 30 Oct 2025 23:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[I have released a 69.0MB version of Windows 7 x86 (138 pts)]]></title>
            <link>https://twitter.com/XenoPanther/status/1983477707968291075</link>
            <guid>45763076</guid>
            <pubDate>Thu, 30 Oct 2025 18:05:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/XenoPanther/status/1983477707968291075">https://twitter.com/XenoPanther/status/1983477707968291075</a>, See on <a href="https://news.ycombinator.com/item?id=45763076">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ScriptLoadFailure"><form action="" method="GET"><div><p><span>Something went wrong, but don’t fret — let’s give it another shot.</span></p><p><img alt="⚠️" draggable="false" src="https://abs-0.twimg.com/emoji/v2/svg/26a0.svg"><span> Some privacy related extensions may cause issues on x.com. Please disable them and try again.</span></p></div></form></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Some people can't see mental images (126 pts)]]></title>
            <link>https://www.newyorker.com/magazine/2025/11/03/some-people-cant-see-mental-images-the-consequences-are-profound</link>
            <guid>45762837</guid>
            <pubDate>Thu, 30 Oct 2025 17:45:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newyorker.com/magazine/2025/11/03/some-people-cant-see-mental-images-the-consequences-are-profound">https://www.newyorker.com/magazine/2025/11/03/some-people-cant-see-mental-images-the-consequences-are-profound</a>, See on <a href="https://news.ycombinator.com/item?id=45762837">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><figure data-testid="cne-audio-embed-figure"></figure><p>When Nick Watkins was a child, he pasted articles about space exploration into scrapbooks and drew annotated diagrams of rockets. He knew this because, years later, he still had the scrapbooks, and took them to be evidence that he had been a happy child, although he didn’t remember making them. When he was seven, in the summer of 1969, his father woke him up to watch the moon landing; it was the middle of the night where they lived, near Southampton, in England. He didn’t remember this, either, but he’d been told that it happened. That Christmas, he and his brother were given matching space helmets. He knew that on Christmas morning the helmets had been waiting in the kitchen and that, on discovering his, he felt joy, but this was not a memory, exactly. The knowledge seemed to him more personal than an ordinary fact, but he could not feel or picture what it had been like to be that boy in the kitchen.</p><p>When he was eight or nine, he read Arthur&nbsp;C. Clarke’s novel “2001: A Space Odyssey” over and over. At the beginning of the book, aliens implant images of tool-using into the minds of man-apes. Near the end, the main character, David Bowman, spools backward through memories of his life:</p><blockquote data-testid="blockquote-wrapper"><p>Not only vision, but all the sense impressions, and all the emotions he had felt at the time, were racing past, more and more swiftly. His life was unreeling like a tape recorder playing back at ever-increasing speed.&nbsp;.&nbsp;.&nbsp;. Faces he had once loved, and had thought lost beyond recall, smiled at him.</p></blockquote><p>To Nick, these events—the images in the minds of the man-apes, David Bowman’s reliving of his life—were thrilling and otherworldly, with no connection to reality, brought about through the intervention of aliens, in distant, fictional worlds.</p><p>He became a physicist. He was drawn to statistical physics and quantum mechanics, whose concepts were best described in equations. The abstraction of these ideas suited him.</p><p>One morning in 1997, when he was thirty-five, he was sitting at breakfast, paging through the newspaper. He started to read an article by a columnist he admired, Michael Bywater. Time was an illusion, Bywater wrote, because you could roll it backward and relive it: “You choose a memory, focus on it, let the rest of the mind go blank, and wait.” Bywater described particular memories of his own, not only the sight but the sound and feel of them—“the special <em>weight</em> of girls in autumn&nbsp;.&nbsp;.&nbsp;. when they lean against you as you walk along.” For some reason, these sentences revealed all at once to Nick what in the whole course of his life he had not realized: that it was possible to see pictures in your mind and use those pictures to reëxperience your past.</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><p>This was startling information. He knew, of course, that people talked about “picturing” or “visualizing,” but he had always taken this to be just a metaphorical way of saying “thinking.” Now it appeared that, in some incomprehensible sense, people meant these words <em>literally</em>. And then there was the notion of using those mental images to revisit a memory. It was an astonishing idea. Was it possible that this was a thing that people other than Bywater could do? Bywater had written about it quite casually, as though he took it for granted. Nick asked some people he knew, and all of them seemed to be able to do it.</p><p>He wondered whether there was something wrong with him—some kind of amnesia. He’d had no reason to worry about his memory before. He had a Ph.D. in physics; clearly his mind was functioning reasonably well. He knew the usual facts about his life—his parentage, the places he’d lived as a child, important things that had happened. It had never occurred to him that remembering could be more than that.</p><p>For many years, Nick would search for information about mental imagery, sporadically and alone. In the beginning, he did not yet know that his inability to visualize—this odd feature of his mind which appeared so insignificant that he hadn’t even noticed it for thirty-five years—would come to seem a central wellspring of his self. But then, in 2015, his condition was given a scientific name, aphantasia, and tens of thousands of people experienced the same shocked realization that he had. A flurry of research in the following decade would uncover associations between mental imagery and a bewildering variety of human traits and capacities: a propensity to hold grudges; autism; a vulnerability to trauma; emotional awareness; ways of making art and hearing music; memory of one’s life.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a60873&quot;}" href="https://www.newyorker.com/cartoon/a60873" rel="nofollow noopener" target="_blank"><picture><img alt="Robber on roller coaster sticking up fellowriders." loading="lazy" srcset="https://media.newyorker.com/cartoons/68f80218837f0cdac35b772e/master/w_120,c_limit/a60873.jpg 120w, https://media.newyorker.com/cartoons/68f80218837f0cdac35b772e/master/w_240,c_limit/a60873.jpg 240w, https://media.newyorker.com/cartoons/68f80218837f0cdac35b772e/master/w_320,c_limit/a60873.jpg 320w, https://media.newyorker.com/cartoons/68f80218837f0cdac35b772e/master/w_640,c_limit/a60873.jpg 640w, https://media.newyorker.com/cartoons/68f80218837f0cdac35b772e/master/w_960,c_limit/a60873.jpg 960w, https://media.newyorker.com/cartoons/68f80218837f0cdac35b772e/master/w_1280,c_limit/a60873.jpg 1280w, https://media.newyorker.com/cartoons/68f80218837f0cdac35b772e/master/w_1600,c_limit/a60873.jpg 1600w" sizes="100vw" src="https://media.newyorker.com/cartoons/68f80218837f0cdac35b772e/master/w_1600%2Cc_limit/a60873.jpg"></picture></a><p><span>Cartoon by Seth Fleishman</span></p></div></span></p></figure><p>But this was all in the future. In 1997, as much as he interrogated his acquaintances, Nick did not find anyone like him. He couldn’t be the only person who lacked this ability to visualize, he thought. Surely it was extremely unlikely that he was unique. But, until he encountered someone else, he had to admit that it was a working possibility.</p><p>He went online and started looking. Initially, he found only work from the nineteenth century. The first useful thing he came across was William James’s book “Principles of Psychology.” James referred to observations recorded in 1860 by Gustav Fechner, a German scientist and philosopher. Fechner had subjected his own “optical memory-pictures” to introspective scrutiny and deemed them weak and lacking:</p><blockquote data-testid="blockquote-wrapper"><div><p>With all my efforts, I cannot reproduce colours in the memory images of coloured objects.&nbsp;.&nbsp;.&nbsp;. I also never dream in colours, but all my experiences in dreams seem to me to proceed in a kind of twilight or night.</p><p>I can’t hold the image steadily for even a short time, but in order to observe it for a longer time, I have to recreate it again and again.</p><p>What was very unexpected to me&nbsp;.&nbsp;.&nbsp;. is that it is easier for me to produce memory images&nbsp;.&nbsp;.&nbsp;. with open eyes than with closed eyes.</p></div></blockquote></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><p>Fechner didn’t pursue the subject, however, and it lay dormant until 1880, when it was taken up by Francis Galton, a British scientist who later became notorious as the father of eugenics. Galton, supposing that he could depend on scientists to give accurate answers, wrote to several of them with a query:</p><blockquote data-testid="blockquote-wrapper"><p>Think of some definite object—suppose it is your breakfast-table as you sat down to it this morning—and consider carefully the picture that rises before your mind’s eye.&nbsp;.&nbsp;.&nbsp;. Is the image dim or fairly clear?&nbsp;.&nbsp;.&nbsp;. Are the colours of the china, of the toast, bread-crust, mustard, meat, parsley, or whatever may have been on the table, quite distinct and natural?</p></blockquote><p>The responses he received were not at all what he had expected.</p><blockquote data-testid="blockquote-wrapper"><p>To my astonishment, I found that the great majority of the men of science to whom I first applied protested that mental imagery was unknown to them, and they looked on me as fanciful and fantastic in supposing that the words “mental imagery” really expressed what I believed everybody to suppose them to mean.&nbsp;.&nbsp;.&nbsp;. They had a mental deficiency of which they were unaware, and naturally enough supposed that those who were normally endowed, were romancing.</p></blockquote><p>Finding this Galton study came as a relief to Nick. Now at least he knew that there had been other people lacking mental imagery who’d lived normal lives, so it wasn’t a disease, or a symptom of a brain tumor. Galton had subsequently observed that women and children appeared to have more vivid imagery than the scientists did. “Scientific men as a class,” he concluded, “have feeble powers of visual representation.” Nick found this intriguing. Perhaps his own lack of imagery had somehow enhanced his scientific ability. He knew that there was, among some mathematicians, a kind of snobbery about images—a notion that, even in geometry, drawings were distractions from a purely analytical proof. But he also knew that there were any number of legends in the history of science of visions leading to discoveries. Einstein had visualized himself travelling alongside a beam of light, and this had led to his conception of relativity. The best-known instance that Nick was aware of was the German chemist August Kekulé, to whom the structure of the benzene ring had appeared in a dream:</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><blockquote data-testid="blockquote-wrapper"><p>Long rows&nbsp;.&nbsp;.&nbsp;. all twining and twisting in snake-like motion.&nbsp;.&nbsp;.&nbsp;. One of the snakes had seized hold of its own tail, and the form whirled mockingly before my eyes. As if by a flash of lightning I awoke; and&nbsp;.&nbsp;.&nbsp;. spent the rest of the night in working out the consequences of the hypothesis.</p></blockquote><p>At one point, Nick came across a paper from 1909 that stressed the importance of distinguishing between voluntary imagery (the ability to call up mental pictures at will) and involuntary imagery. Sometimes people who couldn’t call up images on purpose did experience them involuntarily—usually during migraines, or high, hallucinatory fevers, or in dreams, or the hypnagogic state just before sleep. This caught his attention because he was almost certain that he saw images in dreams, although he couldn’t be sure, since nothing remained of the images after he woke. If he was right, and he did see images in sleep, then it was strange that he couldn’t summon them at other times. Was he repressing them?</p><p>When he searched for scientific studies on imagery in the mid-twentieth century, he found very little. It seemed that the study of imagery had largely disappeared from scientific research from the nineteen-twenties to the fifties, owing in part to the dominance of behaviorism in America, which condemned inquiry into internal psychological states as unscientific. J.&nbsp;B. Watson, behaviorism’s founder, repudiated the existence of mental imagery altogether:</p><blockquote data-testid="blockquote-wrapper"><p>What does a person mean when he closes his eyes or ears (figuratively speaking) and says, “I see the house where I was born, the trundle bed in my mother’s room where I used to sleep—I can even see my mother as she comes to tuck me in and I can even hear her voice as she softly says good-night”? Touching, of course, but sheer bunk. We are merely dramatizing. The behaviorist finds no proof of imagery at all in this.</p></blockquote><p>Later, researchers would debate whether Watson became a behaviorist because he had no internal imagery, or whether he actually had strong imagery but denied it because of “ideological blindness.”</p><p>In the nineteen-seventies, Nick discovered, a few psychologists, liberated from mid-century behaviorist orthodoxy, had begun to explore imagery again. A British psychologist named David Marks, for instance, developed the Vividness of Visual Imagery Questionnaire, which sought to measure a person’s ability to picture not only a stationary object but also movement (the characteristic gait of a familiar person), change (the shifting color of the sky at sunrise), and degree of detail (the window of a shop you frequently go to). But the psychologists in the nineteen-seventies were interested in people with typical imagery. When Nick searched for studies on people like himself, he found nothing.</p><p>Sometime in the early two-thousands, Jim Campbell, a Scottish surveyor in his mid-sixties, made an appointment with a neurologist at the University of Edinburgh named Adam Zeman. Jim had recently had a cardiac procedure, and afterward he’d noticed that he could no longer picture anything in his head. Before the surgery, he used to put himself to sleep by visualizing his children and grandchildren; now he couldn’t see anything at all.</p><p>Zeman had a general neurology practice—Parkinson’s, M.S., dementia—but he had also been interested in consciousness since he was a student. He speculated that one of the things that made humans different from other primates was their ability to mentally project themselves into the past or future, or into worlds that were purely imaginary. So he was fascinated to encounter, in Jim, a syndrome he had never heard of before, which appeared to be an excision of just this species-defining ability. And yet Jim was clearly very much a human—wry, reserved, down to earth. His neurological, psychiatric, and cognitive tests were all normal. If Jim had not described his condition, Zeman would not have known there was anything unusual about him.</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><p>Even questions designed to evoke imagery—Which is darker, grass or pine needles? Do squirrels have long or short tails?—Jim answered without hesitation. When Zeman asked him how he could answer without picturing these things, he said that he just knew. Zeman searched for recent scientific papers that could shed light on this strange condition but was unable to find anything useful. The case reminded him of blindsight—a rare phenomenon in which people who can’t see behave as though they can, picking up objects and avoiding obstacles. Their eyes and brains can take in visual information, but the information doesn’t rise to consciousness.</p><p>Zeman felt that Jim was not the sort of person who would make something like this up, but he wanted proof that his brain was functioning in an unusual manner. He recruited a control group of men of similar age and put them and Jim through cognitive tests in an MRI scanner. Here, he found the neurological correlate that he was looking for. Although Jim’s brain responded normally to tests of recognition (being shown images of famous faces), when he was asked to <em>generate</em> a mental image the scanner showed only faint brain activity, compared with the brain activity in the control group. Instead, there was activation in areas of the frontal lobe that were typically activated in situations of cognitive effort or dissonance. Jim was trying, but failing.</p><p>In 2010, Zeman, along with several colleagues, published these findings in the journal <em>Neuropsychologia</em>, terming the syndrome “blind imagination.” The science journalist Carl Zimmer noticed the study and wrote an article about it in <em>Discover</em> magazine. In the years that followed, a couple of dozen people contacted Zeman to tell him that they had the same condition, except they’d had it since birth. Zeman sent them questionnaires and tabulated their answers. At this point, he decided that lack of mental imagery was a valid syndrome that ought to have a name. After consulting with a classicist friend, he decided on “aphantasia,” <em>phantasia</em> being defined by Aristotle as the ability to conjure an image in the imagination. In 2015, Zeman co-wrote a paper in <em>Cortex</em> describing the condition as it appeared in twenty-one subjects: “Lives without imagery—Congenital aphantasia.”</p><p>An article about Zeman’s second paper appeared in the New York <em>Times</em>, and, after that, e-mails poured in. Around seventeen thousand people contacted him. Most were congenital aphantasics, and most not only lacked visual imagery; they could not mentally call up sounds, either, or touch, or the sensation of movement. Many had difficulty recognizing faces. Many said that they had a family member who was aphantasic, too. Most said that they saw images in dreams. Zeman recruited colleagues to work with him, and together they tried to reply to every correspondent.</p><p>Some people who wrote had once had imagery but lost it. About half of these had lost it as a consequence of physical injury—stroke, meningitis, head trauma, suffocation. The other half attributed their loss to a psychiatric cause—depersonalization syndrome, depression. A few told him that they thought they’d suppressed their capacity to visualize because traumatic memories had made imagery intolerable. Zeman learned that there had been a case in 1883, described by the French neurologist Jean-Martin Charcot, in which a man, Monsieur X, had lost his imagery; at the same time, the world suddenly appeared alien to him, and he became intensely anxious. “I observed a drastic change in my existence that obviously mirrored a remarkable change in my personality,” Monsieur X wrote to Charcot. “Before, I used to be emotional, enthusiastic with a prolific imagination; today I am calm, cold and I lost my imagination.” Another nineteenth-century French neurologist, Jules Cotard, described a patient whose loss of mental imagery was accompanied by what became known as Cotard’s delusion, or walking-corpse syndrome—the belief that he was dead.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a60541&quot;}" href="https://www.newyorker.com/cartoon/a60541" rel="nofollow noopener" target="_blank"><picture><img alt="Mountain climber looking at sage on mountain peak." loading="lazy" srcset="https://media.newyorker.com/cartoons/68f802186903ce4a69dcb009/master/w_120,c_limit/a60541.jpg 120w, https://media.newyorker.com/cartoons/68f802186903ce4a69dcb009/master/w_240,c_limit/a60541.jpg 240w, https://media.newyorker.com/cartoons/68f802186903ce4a69dcb009/master/w_320,c_limit/a60541.jpg 320w, https://media.newyorker.com/cartoons/68f802186903ce4a69dcb009/master/w_640,c_limit/a60541.jpg 640w, https://media.newyorker.com/cartoons/68f802186903ce4a69dcb009/master/w_960,c_limit/a60541.jpg 960w, https://media.newyorker.com/cartoons/68f802186903ce4a69dcb009/master/w_1280,c_limit/a60541.jpg 1280w, https://media.newyorker.com/cartoons/68f802186903ce4a69dcb009/master/w_1600,c_limit/a60541.jpg 1600w" sizes="100vw" src="https://media.newyorker.com/cartoons/68f802186903ce4a69dcb009/master/w_1600%2Cc_limit/a60541.jpg"></picture></a><p><span>“I hope you brought me food that isn’t trail mix or energy bars.”</span></p><p><span>Cartoon by Amy Hwang</span></p></div></span></p></figure><p>Zeman also received messages from people who appeared to have the opposite of aphantasia: they told him that their mental pictures were graphic and inescapable. There was evidently a spectrum of mental imagery, with aphantasia on one end and extraordinarily vivid imagery on the other and most people’s experience somewhere in between. Zeman figured that the vivid extreme needed a name as well; he dubbed it hyperphantasia. It seemed that two or three per cent of people were aphantasic and somewhat more were hyperphantasic.</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><p>Many of his correspondents, he learned, had discovered their condition very recently, after reading about it or hearing it described on the radio. Their whole lives, they had heard people talk about picturing, and imagining, and counting sheep, and visualizing beaches, and seeing in the mind’s eye, and assumed that all those idioms were only metaphors or colorful hyperbole. It was amazing how profoundly people could misunderstand one another, and assume that others didn’t mean what they were saying—how minds could wrest sense out of things that made no sense.</p><p>Some said that they had a tantalizing feeling that images were somewhere in their minds, only just out of reach, like a word on the tip of their tongue. This sounded right to Zeman—the images must be stored in some way, since aphantasics were able to recognize things. In fact, it seemed that most aphantasics weren’t hampered in their everyday functioning. They had good memories for facts and tasks. But many of them said that they remembered very little about their own lives.</p><p>Among the e-mails that Zeman received, there were, to his surprise, several from aphantasic professional artists. One of these was Sheri Paisley (at the time, Sheri Bakes), a painter in her forties who lived in Vancouver. When Sheri was young, she’d had imagery so vivid that she sometimes had difficulty distinguishing it from what was real. She painted intricate likenesses of people and animals; portraiture attracted her because she was interested in psychology. Then, when she was twenty-nine, she had a stroke, and lost her imagery altogether.</p><p>To her, the loss of imagery was a catastrophe. She felt as though her mind were a library that had burned down. She no longer saw herself as a person. Gradually, as she recovered from her stroke, she made her way back to painting, working very slowly. She switched from acrylic paints to oils because acrylics dried too fast. She found that her art had drastically changed. She no longer wanted to paint figuratively; she painted abstractions that looked like galaxies seen through a space telescope. She lost interest in psychology—she wanted to connect to the foundations of the universe.</p><p>Years later, she remembered that, one night at her parents’ house, when she was still in art school, she had stayed up very late painting. She suddenly felt a strong presence behind her, and, even as she kept working, she felt the presence ask her, What do you want? In her thoughts, she responded, I want to be a great painter, and I will do whatever I have to, except take drugs. Later, she thought, Well, that is what happened. My life is very hard, but my painting is so much better.</p><p>Sheri had been an artist before she lost her imagery, but there were others who had been aphantasic for as long as they could remember. Isabel Nolan, a well-known Irish artist, had recently discovered, in her forties, while reading about Zeman’s work in <em>New Scientist</em>, that other people could see pictures in their heads:</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><blockquote data-testid="blockquote-wrapper"><p>There was an element of like—fuck! is the only way I can put it. Horrified and cheated. I still feel a bit cheated.</p></blockquote><p>She wondered whether she had always been like this. When she was a child, her mother would occasionally go on business trips, and while she was away Isabel stayed with cousins who lived up the road. She remembered lying in bed one night at her cousins’ house, thinking, What if Mam dies? I can’t remember what she looks like. She was an anxious child, frightened of many things, but this particular thought stuck in her mind for years. Now she wondered how she could have been so upset at the thought that she couldn’t picture her mother unless she’d had a notion, some vestigial memory, that such a thing was possible.</p><p>Her fear of things vanishing had not gone away. In fact, it had expanded, from her mother to everything. She had lived in Dublin almost all her life, although it would probably have been better for her career if she’d moved to London. As it turned out, it hadn’t held her back—she would be representing Ireland in the Venice Biennale in 2026—but when she was younger she’d wondered if she was making a mistake. She thought that maybe she’d stayed because having the physical infrastructure of her past around helped her to remember it. For a long time, she had felt that everything around her was ephemeral, precarious, not to be relied on:</p><blockquote data-testid="blockquote-wrapper"><p>I was putting together a book in 2020, gathering about nine years of work and writing, and there was an awful lot of writing alluding to the fact of barely having a grasp on the world and how slippery it all is. I realized that my inability to recall things was really playing on my mind, and that my connection to my social world felt insubstantial. I wrote a lot about how touch was important to me, and how making work was setting down little anchors that reminded me that the world does exist.</p></blockquote><p>Surely this had something to do with not being able to picture anything when she wasn’t looking at it.</p><blockquote data-testid="blockquote-wrapper"><p>The world does disappear completely when you close your eyes.</p></blockquote><p>At a conference, she heard artists with vivid imagery say that they were often disappointed by their work because it could never match up to the glowing vision in their heads; she felt sorry for them. When she was working on something, she never knew how it would end up. Sometimes she started with an idea, like the cosmos; she liked to look at images of deep space and draw abstractions that resembled them. She thought a lot about subjective experience, but not her own experience in particular—more what it was like to be any human, wandering through the world. She didn’t feel that her work was an extension or expression of herself, so she didn’t mind criticism, or not being understood:</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><blockquote data-testid="blockquote-wrapper"><p>I don’t think I have a very strong sense of self, and it’s not something I’m super interested in.</p></blockquote><p>Was this because of her aphantasia? If her mind were filled with pictures, would her self feel fuller, more robust? When people learned that they were aphantasic, they tended to wonder whether this or that aspect of themselves was due to their lack of imagery; sometimes it had nothing to do with it, but in this case it did—several studies had found that people with vivid imagery tended to be more inward, absorbed in the drift of their own minds.</p><p>Someone had told Isabel about a British moral philosopher, Derek Parfit, who had no imagery. He had few memories and little connection to his past, although he felt strong emotions about people and ideas in the present. Parfit believed that a self was not a unique, distinct thing but a collection of shifting memories and thoughts which intersected with the memories and thoughts of others. Ultimately, he thought, selves were not important. What mattered was the moral imperatives that drove everyone, or ought to—preventing suffering, the future welfare of humanity, the search for truth.</p><p>Isabel, like Parfit, remembered very little about her life. She kept boxes of souvenirs—ticket stubs, programs—but unless she looked at these things, or a friend reminded her, she didn’t recall most of the places she’d visited or things she’d done. She imagined that this could be a problem in a relationship, if you didn’t remember what you’d done together and the other person got upset and accused you of not caring, though fortunately she’d never been with someone like that. When she went out with friends who were full of stories, she’d worry that she wasn’t entertaining enough; normally, she drew people out and got them talking so she didn’t have to:</p><blockquote data-testid="blockquote-wrapper"><p>I don’t really have a sense, with some friends—how well I know them. I’ve kind of forgotten. They’d be like, Oh, yeah, do you remember we went to the play that night? And sometimes I’d just pretend, I’d be like, Oh, yeah, yeah, yeah. I think when my relationships are continuous, it’s much easier. You remember more because you’re constantly reëxercising the memories of stuff that you did together—you know, the time we went to Cologne and got hammered and lost our cameras, blah, blah, blah.</p></blockquote><p>It would be nice to remember all the funny stories that people told, but in the end she didn’t mind too much. She could just sit there and bask in the pleasure of being with old friends. It was the feeling that was important; she didn’t need to know what had happened years ago. In some ways, this made things easier—she mostly didn’t remember arguments or bad feelings. She hoped that the significant moments in her life, good and bad, had left their imprint on her in some way, but it was impossible to know:</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a27994&quot;}" href="https://www.newyorker.com/cartoon/a27994" rel="nofollow noopener" target="_blank"><picture><img alt="Two men in suits standing in office." loading="lazy" srcset="https://media.newyorker.com/cartoons/68f7e872a23d709c643e6d95/master/w_120,c_limit/a27994.jpg 120w, https://media.newyorker.com/cartoons/68f7e872a23d709c643e6d95/master/w_240,c_limit/a27994.jpg 240w, https://media.newyorker.com/cartoons/68f7e872a23d709c643e6d95/master/w_320,c_limit/a27994.jpg 320w, https://media.newyorker.com/cartoons/68f7e872a23d709c643e6d95/master/w_640,c_limit/a27994.jpg 640w, https://media.newyorker.com/cartoons/68f7e872a23d709c643e6d95/master/w_960,c_limit/a27994.jpg 960w, https://media.newyorker.com/cartoons/68f7e872a23d709c643e6d95/master/w_1280,c_limit/a27994.jpg 1280w, https://media.newyorker.com/cartoons/68f7e872a23d709c643e6d95/master/w_1600,c_limit/a27994.jpg 1600w" sizes="100vw" src="https://media.newyorker.com/cartoons/68f7e872a23d709c643e6d95/master/w_1600%2Cc_limit/a27994.jpg"></picture></a><p><span>“I’m all for democracy as long as we retain veto power.”</span></p><p><span>Cartoon by Barbara Smaller</span></p></div></span></p></figure><blockquote data-testid="blockquote-wrapper"><p>I feel like my past is kind of imaginary. I know what happened, but it doesn’t feel like—I don’t know. It’s hard to know what having experiences means, because sometimes experiences that I have can leave one so quickly.&nbsp;.&nbsp;.&nbsp;. One can feel a little disconnected from your own past.</p></blockquote><p>Clare Dudeney was an artist who worked in southeast London, in a warren of old factory buildings by the Thames. Against one wall of her studio was a wooden loom, above which large spools of cotton thread in a rainbow of colors were slotted on pegs. She made works in many media, all cornucopias of color: pieces of fabric dyed robin’s-egg blue or pistachio or hazelnut or citrine and pasted into collages, some so long that you couldn’t take them in at once and hung near open doors so that they rippled. She made murals of ceramic tiles painted with irregular shapes, like countries on a map, in powder-puff pink and celery and yellow and wheat; rectangular blocks of rough wood that she called woodcut paintings, with teal, red, cornflower, and lime pigment staining or filling the crevices and gouges of the surface; long clay worms, basket-woven and glazed—forest, mustard, chestnut—like ceramic macramé. She draped herself in colors, too: thick scarves and nubby sweaters that she knitted herself; geometric-patterned skirts.</p><p>In talking to a friend of hers, an aphantasic painter who was one of Zeman’s research subjects, Clare had realized that she was the opposite—hyperphantasic. Her imagery was extraordinarily vivid. There was always so much going on inside her head, her mind skittering and careening about, that it was difficult to focus on what or who was actually in front of her. There were so many pictures and flashes of memory, and glimpses of things she thought were memory but wasn’t sure, and scenarios real and imaginary, and schemes and speculations and notions and plans, a relentless flood of images and ideas continuously coursing through her mind. It was hard to get to sleep.</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><p>At one point, in an effort to slow the flood, she tried meditation. She went on a ten-day silent retreat, but she disliked it so much—too many rules, getting up far too early—that she rebelled. While sitting in a room with no pictures or stimulation of any kind, supposedly meditating, she decided to watch the first Harry Potter movie in her head. She wasn’t able to recall all two hours of it, but watching what she remembered lasted for forty-five minutes. Then she did the same with the other seven films.</p><p>She tried not to expose herself to ugly or violent images because she knew they would stick in her mind for years. But even without a picture, if she even heard about violence her mind would produce one. Once, reading about someone undergoing surgery without anesthetic, she imagined it so graphically that she fainted. (In 2012, two Harvard psychologists published a study about visual imagery and moral judgment. They found that people with weak imagery tended to think more abstractly about moral questions and believe that good ends sometimes justified harmful means. But for people with strong imagery, the harmful means—injuries done to one person in order to save several others, say—formed such lurid pictures in their minds that they responded emotionally and rejected them.)</p><p>Even joyful images could turn on her. She’d had a cat that she loved; she was separated from her husband and living on her own, so she had spent more time with the cat than with any other creature. Then the cat died, and after his death she saw him everywhere—on the sofa, on the floor, on her bed, wherever he had been in life. She saw him so clearly that it was as though he were actually there in front of her. Her grief was made so much worse by this relentless haunting that she began to feel as if she would not be able to cope.</p><p>Her father was a physicist and for many years the deputy director of the British Antarctic Survey. When Clare was a child, he promised that one day he would take her to Antarctica, and finally, when she was in her thirties, in 2013, he did. There, on the boat, she found herself looking at a landscape so wholly unfamiliar that her brain struggled to make sense of it. At times, it barely appeared to her like a landscape at all, more like an abstract surface, without reference or meaning. The place was vast, and there were no people. Snow and ice formed strange patterns on the surface of the sea. As they travelled, the terrain kept changing, so her sense of alien newness persisted. It was as if, for the first time, she was seeing not through the cluttered, obscuring scrim of her visual memories but directly, at the world itself. Just looking at it was so demanding that it occupied her whole mind, so that she wasn’t thinking about anything else, she was just there. At the time, she was consulting on climate and sustainability issues, but after that trip she decided to become an artist.</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><p>Usually, her ideas for art works came not from anything external but from images in her head. For a while, she had made paintings based on her dreams. She kept a journal and a pen by the side of her bed so that she could describe what she’d dreamed the moment she woke. The more she wrote down her dreams, the more she remembered them; sometimes she would remember ten dreams in a single night. Eventually, the process began to fold in on itself—while she was still asleep, she’d begin to dream that she was taking notes on the dream, and planning how to draw what she saw.</p><p>When she thought about making a new piece, she often worked it out in her mind beforehand. Being hyperphantasic didn’t mean only that your imagery was bright and sharp; it meant that you could manipulate your images at will, zooming in and out, cutting and pasting, flipping and mirroring, creating pictures from scratch, assembling and disassembling complicated objects. Even when she was trying to evoke the colors of a landscape at a certain time of day, she did it not from life but from memory.</p><p>She didn’t know how common this was among artists, but there were some who she was fairly sure had worked from their imaginations rather than from life. J.&nbsp;M.&nbsp;W. Turner, for instance, made rough sketches outdoors, but the seas and skies and light of his paintings all came from his head. There was an English portraitist working in the late eighteenth century whose prodigious powers of visualization had been described in a case study. The study didn’t name the painter but said that he’d inherited most of the clients of Sir Joshua Reynolds after Reynolds’s death, and had proceeded to take full advantage of this by painting three hundred portraits in a single year. The study’s author, a British physician named A.&nbsp;L. Wigan, reported:</p><blockquote data-testid="blockquote-wrapper"><p>This would seem physically impossible, but the secret of his rapidity and of his astonishing success was this: He required but one sitting, and painted with miraculous facility. I myself saw him execute a kit-cat portrait of a gentleman well known to me, in little more than eight hours; it was minutely finished, and a most striking likeness. On asking him to explain it, he said, “When a sitter came, I looked at him attentively for half-an-hour, sketching from time to time on the canvass. I wanted no more—I put away my canvass, and took another sitter. When I wished to resume my first portrait, I took the man and set him in the chair, where I saw him as distinctly as if he had been before me in his own proper person—I may almost say more vividly. I looked from time to time at the imaginary figure, then worked with my pencil, then referred to the countenance, and so on, just as I should have done had the sitter been there—<em>when I looked at the chair I saw the man!</em>”</p></blockquote></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><p>This painter’s imagery was so lifelike, however, that he began to confuse his mind’s pictures with reality, and succumbed to a mental illness that lasted thirty years.</p><p>Hyperphantasia often seemed to function as an emotional amplifier in mental illness—heightening hypomania, worsening depression, causing intrusive traumatic imagery in P.T.S.D. to be more realistic and disturbing. Reshanne Reeder, a neuroscientist at the University of Liverpool, began interviewing hyperphantasics in 2021 and found that many of them had a fantasy world that they could enter at will. But they were also prone to what she called maladaptive daydreaming. They might become so absorbed while on a walk that they would wander, not noticing their surroundings, and get lost. It was difficult for them to control their imaginations: once they pictured something, it was hard to get rid of it. It was so easy for hyperphantasics to imagine scenes as lifelike as reality that they could later become unsure what had actually happened and what had not.</p><blockquote data-testid="blockquote-wrapper"><p>I can imagine my hand burning, to the point where it’s painful. I’ve always been curious—if they put me in an fMRI, would that show up? That’s one of the biggest problems in my life: when I feel something, is it real?</p></blockquote><p>One hyperphantasic told a researcher that he had more than once walked into a wall because he had pictured a doorway.</p><p>Because their imaginative lives were so compelling, hyperphantasics tended to be inwardly focussed. This could mean that they were detached from reality, living in the remembered past and the imaginary future rather than in the actual present. But it could also mean that they were hyperaware of their internal reality, tuned in to the cues of their bodies and the shifts in their emotions. Some researchers hypothesized that the heightened awareness of these bodily and emotional signals were one reason that people with vivid imagery usually had strong memories of their pasts—these signals somehow helped to “anchor memories to the self.”</p><p>Hyperphantasics’ memories could be exceptionally detailed.</p><blockquote data-testid="blockquote-wrapper"><p>Someone might mention something like Did you ever skateboard as a kid? And then I have to watch out for the avalanche of every skateboarding experience I ever had. It’s like being in virtual reality and having a three-hundred-and-sixty-degree video of a thousand skateboarding experiences at the same time.</p></blockquote><p>Memories might take on quasi-physical forms in their minds. They might picture sheaves of recollections, or files of information, sitting on shelves in a mental warehouse. They might envision lists of facts about a particular place pinned to that place on a vast and detailed mental map that they saw spread out before them, like a hologram.</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><p>Reeder had tested children’s imagery and believed that most children were hyperphantasic. They had not yet undergone the synaptic pruning that took place in adolescence, so there were incalculably more neuronal connections linking different parts of their brain, giving rise to fertile imagery. Then, as they grew older, the weaker connections were pruned away. Because the synapses that were pruned tended to be the ones that were used less, Reeder thought it was possible that the children who grew up to be hyperphantasic adults were those who kept on wanting to conjure up visual fantasy worlds, even as they grew older. Conversely, perhaps children who grew up to become typical imagers daydreamed less and less, becoming more interested in the real people and things around them. Maybe some children who loved to daydream were scolded, in school or at home, to pay attention, and maybe these children disciplined themselves to focus on the here and now and lost the ability to travel to the imaginary worlds they’d known when they were young.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a26838&quot;}" href="https://www.newyorker.com/cartoon/a26838" rel="nofollow noopener" target="_blank"><picture><img alt="Person looking at tiny free library named “Tiny Library of Babel.”" loading="lazy" srcset="https://media.newyorker.com/cartoons/68f7e87291b5efc596f6319c/master/w_120,c_limit/a26838.jpg 120w, https://media.newyorker.com/cartoons/68f7e87291b5efc596f6319c/master/w_240,c_limit/a26838.jpg 240w, https://media.newyorker.com/cartoons/68f7e87291b5efc596f6319c/master/w_320,c_limit/a26838.jpg 320w, https://media.newyorker.com/cartoons/68f7e87291b5efc596f6319c/master/w_640,c_limit/a26838.jpg 640w, https://media.newyorker.com/cartoons/68f7e87291b5efc596f6319c/master/w_960,c_limit/a26838.jpg 960w, https://media.newyorker.com/cartoons/68f7e87291b5efc596f6319c/master/w_1280,c_limit/a26838.jpg 1280w, https://media.newyorker.com/cartoons/68f7e87291b5efc596f6319c/master/w_1600,c_limit/a26838.jpg 1600w" sizes="100vw" src="https://media.newyorker.com/cartoons/68f7e87291b5efc596f6319c/master/w_1600%2Cc_limit/a26838.jpg"></picture></a><p><span>Cartoon by Adam Douglas Thompson</span></p></div></span></p></figure><p>Clare had not been discouraged from daydreaming as a child, and she had preferred it to the other common form of imaginative dissociation, reading. Daydreaming was more pleasurable for her because she had struggled to learn to read, and even once she knew how she’d found it slow going. When she received a diagnosis of dyslexia, as an adult, the tester told her that, rather than processing individual letters or sounds, she was memorizing pictures of whole words, which made it hard to recognize words in different fonts. Her visual sense was so overweening that reading was strenuous, because she was easily distracted by the squiggles and lines of the text.</p><p>Naturally, aphantasics usually had a very different experience of reading. Like most people, as they became absorbed, they stopped noticing the visual qualities of the words on the page, and, because their eyes were fully employed in reading, they also stopped noticing the visual world around them. But, because the words prompted no mental images, it was almost as if reading bypassed the visual world altogether and tunnelled directly into their minds.</p><p>Aphantasics might skip over descriptive passages in books—since description aroused no images in their minds, they found it dull—or, because of such passages, avoid fiction altogether. Some aphantasics found the movie versions of novels more compelling, since these supplied the pictures that they were unable to imagine. Of course, for people who did have imagery, seeing a book character in a movie was often unsettling—because they already had a sharp mental image of the character which didn’t look like the actor, or because their image was vague but just particular enough that the actor looked wrong, or because their image was barely there at all and the physical solidity of the actor conflicted with that amorphousness.</p><p>Presumably, novelists who invented characters also had a variety of responses to seeing them instantiated in solid form. Jane Austen wrote a letter to her sister in 1813 in which she described going to an exhibition of paintings in London and searching for portraits that looked like Elizabeth Bennet and Jane Bingley, two main characters from “Pride and Prejudice.” To her delight, she’d seen “a small portrait of Mrs Bingley, excessively like her&nbsp;.&nbsp;.&nbsp;. exactly herself, size, shaped face, features &amp; sweetness; there never was a greater likeness. She is dressed in a white gown, with green ornaments, which convinces me of what I had always supposed, that green was a favourite color with her.” Austen did not see Elizabeth at the exhibition but hoped, she told her sister, to find a painting of her somewhere in the future. “I dare say Mrs D.”—she wrote, Darcy being Elizabeth’s married name—“will be in Yellow.”</p><p>One of the twenty or so congenital aphantasics who contacted Adam Zeman after his original 2010 paper was a Canadian man in his twenties, Tom Ebeyer. Ebeyer volunteered to participate in Zeman’s studies, and, after Zeman published his 2015 <em>Cortex</em> paper on congenital aphantasia, Ebeyer was one of the participants quoted in the <em>Times</em> article about it. After that, hundreds of aphantasics reached out to him on Facebook and LinkedIn. They asked him questions he didn’t know the answers to: Does this mean I have a disability? Is there a cure?</p><p>Many of Ebeyer’s correspondents felt shocked and isolated, as he had; he decided that what was needed was a online forum where aphantasics could go for information and community. He set up a website, the Aphantasia Network. He didn’t want it to be a sad place where people commiserated with one another, however. There were good things about aphantasia, he believed, and he began to write uplifting posts pointing them out. In one, he argued that aphantasia was an advantage in abstract thinking. When prompted by the word “horse,” a person with imagery would likely picture a particular horse—one they’d seen in life, perhaps, or in a painting. An aphantasic, on the other hand, focussed on the concept of a horse—on the abstract essence of horseness. Ebeyer published posts about famous people who had realized that they were aphantasic: Glen Keane, one of the leading Disney animators on “The Little Mermaid” and “Beauty and the Beast”; John Green, the author of “The Fault in Our Stars,” whose books had sold more than fifty million copies; J.&nbsp;Craig Venter, the biologist who led the first team to sequence the human genome; Blake Ross, who co-created the Mozilla-Firefox web browser when he was nineteen.</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><p>Ebeyer also wanted the Aphantasia Network to be a place where aphantasics could find recent scientific research. For instance, estimating the strength of a person’s imagery had been thoroughly subjective until Joel Pearson, a cognitive neuroscientist at the University of New South Wales, in Australia, devised tests to measure it more precisely. In a paper from 2022, Pearson reported that when people with imagery visualized a bright object their pupils contracted, as though they were seeing a bright object in real life, but the pupils of aphantasics imagining a bright object stayed the same. Another study of his had shown that, although aphantasics had the same fear response (sweating) as typical imagers to a frightening image shown on a screen, when exposed to a frightening story they barely responded at all.</p><p>Ebeyer kept in touch with Zeman and published bulletins about his research. Zeman had found that aphantasics could solve many problems that would seem to require imagery, such as counting the number of windows in their home. This, Zeman hypothesized, was due to the difference between object imagery and spatial imagery. There were two streams of visual information in the brain that were, to a surprising degree, distinct from each other: one had to do with recognition of objects; the other, with guiding action through space. Aphantasics lacked object imagery, but they might have the kind of spatial imagery that would enable them to count windows. One aphantasic described his ability to do this as a kind of echolocation.</p><p>To Zeman, one of the most tantalizing promises of the study of mental imagery was the light it might shed on the neural correlates of consciousness. Connectivity in the brain seemed to be particularly important in both consciousness and aphantasia. fMRI studies had shown reduced connectivity in aphantasics, and Brian Levine, a neuropsychologist at the Baycrest Academy for Research and Education, in Toronto, had found that connectivity between the memory system and the visual-perceptual regions in the brain correlated to how well people remembered their lives. Many of the aphantasics who had written to Zeman identified themselves as autistic. Autism was thought to be a state of reduced long-range connectivity in the brain, so Zeman theorized that there could be a link. But autism had also been associated with thinking in pictures—Temple Grandin, for instance, the autistic writer and professor of animal science, described her autism that way—so clearly the link was not a simple one.</p><p>After creating the Aphantasia Network, Ebeyer received tens of thousands of messages from all over the world—Korea, Venezuela, Madagascar. He launched Aphantasia Network Japan, and made plans for a Spanish-language site. When the city of Rowlett, a suburb of Dallas, declared the world’s first Aphantasia Awareness Day, on February 21, 2023, his site published a celebratory post. Once hyperphantasia began to be written about, he started to hear from hyperphantasics as well. When he wrote a post about how some people could “hear” music in their heads, or relive touch or tastes, most responses were from aphantasics amazed to learn that such things were possible. But one person wrote to him describing a kind of auditory hyperphantasia:</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><blockquote data-testid="blockquote-wrapper"><p>I can—and do—listen to entire classical works in my head. The longest continuous one was the entire Verdi requiem, listened to internally on a long-haul flight. The imagery is very detailed. I can summon up a work and identify the instruments playing in an orchestral texture, or the registration being used in a particular organ piece. I can’t turn it off though. It’s in the background as I write (Schumann, third symphony, last movement). Sometimes a short passage will repeat endlessly, typically when I am stressed. And if I wake at night, it is usually with a short sequence of harmonies repeating themselves.</p></blockquote><p>This past January, Zeman and others published a short article in <em>Cortex</em> clarifying that the definition of aphantasia encompassed people with weak imagery. Ebeyer wrote a post in response, wondering whether this inclusive definition risked diluting the experiences of those with total aphantasia, such as himself. Might it threaten the cohesion of the aphantasia community? Aphantasia, at this point, wasn’t only a syndrome, after all—it was an identity.</p><p>In the course of his quest to learn about imagery, Nick Watkins, the physicist, came across an essay by Oliver Sacks. Sacks mentioned that he normally had almost no mental imagery but that, during a two-week period in his thirties when he’d been downing heroic quantities of amphetamines, he’d suddenly been able to retain images in his mind—though only images of things that he had just looked at. During that time, he also found it much more difficult to think in abstractions. When the drugs wore off, the images dissipated and his abstract thinking returned. This was an auspicious discovery, Nick thought, that you could somehow turn imagery on. He was certainly not going to take amphetamines himself—he was a pretty cautious person—especially if doing so might jeopardize his ability to think abstractly. But if amphetamines could work, maybe something else could, too.</p><p>He kept looking. He discovered that Aldous Huxley was aphantasic and that, in “The Doors of Perception,” he had written that he was expecting mescaline to change this, even if only for a few hours. (It didn’t.) Unsurprisingly, amid the recent research on psychedelics, this hope of arousing mental vision with drugs had been revived. In 2018, the <em>Journal of Psychedelic Studies</em> published a paper about an aphantasic man, S.E., who had taken ayahuasca and had an intensely emotional experience of visualizing, and then forgiving, his father, long dead, who had left him when he was very young. Afterward, S.E. was still able to see images, but only faintly. He and the paper’s authors concluded that his aphantasia had likely been psychological in origin, since it was resolved by his feeling that things between him and his father had been settled. Another paper, published in the same journal in 2025, described an autistic aphantasic woman in her mid-thirties who had eaten psilocybin truffles and experienced mental imagery for the first time. Her imagery persisted for many months, although it was not quite as vivid as during the trip itself.</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><p>Nick kept hoping that someone would find a way of stimulating imagery that didn’t involve drugs. On the other hand, as he learned more about people with imagery, he was less inclined to envy them. At first, he had thought that having imagery would be like having a VCR, being able to play home movies whenever you felt wistful. But, reading more about it, he had learned that memories and images could break in on you, unbidden and uncontrollable, and not necessarily happy ones. Even if the imagery wasn’t frightening, it would surely be a distraction. He had come to value the dark and quiet of his mind.</p><p>Nick knew that whenever Zeman talked about aphantasia he was at pains to emphasize that it was not a disorder, or even a bad thing. It was best described as an interesting variant in human experience, like synesthesia. Nick appreciated this about Zeman, and reckoned that it was probably the right thing to say, but he thought that, though aphantasia itself might be neutral, the memory loss that came with it was definitely a bad thing. Many others felt the same. At one point, Zeman had been contacted by an automotive engineer from Essex named Alan Kendle, who had realized that he was aphantasic while listening to a radio segment about the condition. This revelation affected him so strongly that he put together a book of interviews with aphantasics, identified just by their initials, to help others navigate the discovery. Some people he interviewed were unbothered—there was definitely a range of responses—but others saw it as a curse.</p><p>Many could remember very little about their lives, and even with the events they did remember they could not muster the feeling of what they’d been like. They knew that some things had made them happy and others had made them sad, but that knowledge was factual—it didn’t evoke any emotions in the present.</p><blockquote data-testid="blockquote-wrapper"><p>M.L.: It leaves me as an outsider. As a viewer of life, not particularly a participant. I don’t like holidays or sightseeing—what is the point? You go, you see things, you leave, and it is gone. Not a trace or a sensation remains.</p></blockquote><p>The advantage of a bad memory was that aphantasics seemed to suffer less from regret, or shame, or resentment.</p><blockquote data-testid="blockquote-wrapper"><div><p>L.: I can easily move on, forget, not hold grudges, no living in the past, and no dreaming of the future. This is it! I can live in the NOW.</p><p>S.C.: I work for the emergency services, and I’ve spoken with my workmates about what they think the hardest part of the job is. They all said it is definitely reliving traumatic things they have seen.&nbsp;.&nbsp;.&nbsp;. It is for this reason that I am glad I can’t visualize. When I go home, after having someone die in front of me, I go to bed, close my eyes, and see nothing but black for a minute. Then, I’m off in my dream world.</p></div></blockquote><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a23887&quot;}" href="https://www.newyorker.com/cartoon/a23887" rel="nofollow noopener" target="_blank"><picture><img alt="Traditional healer asks person if they have questions about their toad prescription." loading="lazy" srcset="https://media.newyorker.com/cartoons/68f7e87193ae2a49734b67f4/master/w_120,c_limit/a23887.jpg 120w, https://media.newyorker.com/cartoons/68f7e87193ae2a49734b67f4/master/w_240,c_limit/a23887.jpg 240w, https://media.newyorker.com/cartoons/68f7e87193ae2a49734b67f4/master/w_320,c_limit/a23887.jpg 320w, https://media.newyorker.com/cartoons/68f7e87193ae2a49734b67f4/master/w_640,c_limit/a23887.jpg 640w, https://media.newyorker.com/cartoons/68f7e87193ae2a49734b67f4/master/w_960,c_limit/a23887.jpg 960w, https://media.newyorker.com/cartoons/68f7e87193ae2a49734b67f4/master/w_1280,c_limit/a23887.jpg 1280w, https://media.newyorker.com/cartoons/68f7e87193ae2a49734b67f4/master/w_1600,c_limit/a23887.jpg 1600w" sizes="100vw" src="https://media.newyorker.com/cartoons/68f7e87193ae2a49734b67f4/master/w_1600%2Cc_limit/a23887.jpg"></picture></a><p><span>“Any questions about the medication?”</span></p><p><span>Cartoon by David Sipress</span></p></div></span></p></figure><p>But this supposed advantage was just the silver lining of something pretty dark. When aphantasics recovered from bereavement, or breakups, or trauma, more quickly than others, they worried that they were overly detached or emotionally deficient. When they didn’t see people regularly, even family, they tended not to think about them.</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><blockquote data-testid="blockquote-wrapper"><p>M.L.: I do not miss people when they are not there. My children and grandchildren are dear to me, in a muffled way. I am fiercely protective of them but am not bothered if they don’t visit or call.&nbsp;.&nbsp;.&nbsp;. I think that leaves them feeling as if I don’t love them at all. I do, but only when they are with me, when they go away they really cease to exist, except as a “story.”</p></blockquote><p>One of Kendle’s interviewees was Melinda Utal, a hypnotherapist and a freelance writer from California. She had trouble recognizing people, including people she knew pretty well, so she tended to avoid social situations where she might hurt someone’s feelings. When she first discovered that she was aphantasic, she called her father, who was in the early stages of Alzheimer’s disease and living in a nursing home in Oregon. He had been a musician in big bands—he had toured with Bob Hope and played with Les Brown and his Band of Renown. She asked him whether he could imagine a scene in his head, and he said, Of course. I can imagine going into a concert hall. I see the wood on the walls, I see the seats, I know I’m going to sit at the back, because that’s where you get the best sound. I can see the orchestra playing a symphony, I can hear all the different instruments, and I can stop it and go backward to wherever I want it to start up and hear it again. She explained to her father what aphantasia was, how she couldn’t see images in her mind, or hear music, either. On the phone, her father started to cry. He said, But, Melinda, that’s what makes us human.</p><p>Melinda had an extremely bad memory for her life, even for an aphantasic. She once had herself checked for dementia, but the doctor found nothing wrong. She had become aware when she was in second grade that she had a bad memory, after a friend pointed it out. In an effort to hold on to her memories, she started keeping a journal in elementary school, recording what she did almost every single day, and continued this practice for decades. When, in her sixties, she got divorced and moved into an apartment by herself, she thought it would be a good time to look through her journals and revisit her younger days. She opened one and began to sob because, to her horror, the words she had written meant nothing to her. The journals were useless. She read about things she had done and it was as though they had happened to someone else.</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><p>It was not just the distant past that she had lost—she was continuously aware of the present slipping away as soon as it happened. She had already forgotten what her two sons had been like when they were little, the feeling of holding them:</p><blockquote data-testid="blockquote-wrapper"><p>It’s like, this is who they are, they were never anybody else, and that’s like a knife in my chest.</p></blockquote><p>Now her greatest fear was that, if she hadn’t seen her sons in a while, she might forget them altogether:</p><blockquote data-testid="blockquote-wrapper"><p>I have had to accept that my life is like water flowing through my fingers. It’s just experiences moving through my hand that I can’t hold onto.</p></blockquote><p>Although Nick had made his peace with his lack of imagery, he still grieved his inability to revisit his past. At one point, he came across the work of a Canadian psychologist, Endel Tulving, who, in the early nineteen-seventies, proposed that memory was not a single thing but two distinct systems: semantic memory, which consisted of general knowledge about the world, and episodic memory—recollection of experiences from your own life. Episodic memory, the sense of reliving the past, was, Tulving believed, unique to humans, and among the most astonishing products of evolution. This, Nick realized, was what he didn’t have. Learning that he lacked a profound human ability—one that, he had to assume, regenerated and immeasurably deepened your connection to your past life and the people in it who were now gone, including yourself as a child—well, there was nothing good about it. He would have preferred not to know.</p><p>He wrote to Tulving, who told him about a study to be conducted by Brian Levine, the Baycrest neuropsychologist, who had been a colleague of his in Toronto. The study would investigate exceptionally poor autobiographical memory in healthy adults—people who did not have amnesia or dementia or brain injury or psychological trauma. Levine later named this syndrome “severely deficient autobiographical memory,” or <em>sdam</em>. Nick was accepted as a participant and travelled to Toronto. The study found that the participants’ experience of <em>sdam</em> could be objectively corroborated, using a variety of methods, by comparing them to a control group. fMRI, for instance, showed reduced activation in the midline regions of their brains, an area normally associated with mental time travel.</p><p>Nick was surprised to hear that another participant in the study had described an even starker experience of episodic memory loss than his. She felt so detached from her past that the facts she knew about it felt to her no more personal than facts about someone else. He definitely didn’t feel that way. The things he knew about his life felt more personal to him than facts he knew about physics, say, even though he couldn’t inhabit them in the way that other people could. He realized that Tulving’s binary schema, which categorized all memory as either episodic or semantic, was too simple. His own memories were somewhere in between. He remembered that on the day that his mother died, in 2003, his sister had phoned him to say that their mother was being admitted to the hospital; he had taken a train from Cambridge to London, and he had phoned an old friend to meet him in London because he was worried that, in his distress, he might go to the wrong station and miss the second train he needed to catch, but the friend helped him, and he got on the right train, and it was around Guy Fawkes Night, fireworks going off outside the train window, and then he got to the hospital and was there for a while, and then his mother died. He knew these things, and the idea of his mother dying aroused emotion in him, but he couldn’t feel what it had been like to be in the train, or the hospital, and he could not remember his mother’s face.</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><p>From an evolutionary point of view, he supposed, he had all the memory he needed: enough to know what and whom he had loved, and what he should try to avoid doing again. But to think about it that way was to miss what was most important—not the function of episodic memory but the experience of it. As he absorbed what it meant to lack episodic memory, he started wondering whether there were ways he could simulate it. He was attracted to the idea of video life-logging with wearable cameras—the footage would be a decent substitute for mental time travel. His childhood and early adulthood were lost to him, but if he started filming now he would be able to relive at least the last decade or two of his life.</p><p>On a trip to Pasadena, he went to the Apple Store and tried on a virtual-reality headset. This, he thought, must be what episodic memory is like. He knew it would probably be a long time before people accepted such technologies, but perhaps one day wearable cameras would be recognized as prosthetics for people with <em>SDAM</em>, no more remarkable than glasses. Then again, film would be very different from memory. Like memory, it would be partial, but, unlike memory, it would be accurate. This, he suspected, might not necessarily be a good thing. There was something to be said for a degree of blurriness and uncertainty in recalling the past; it was helpful in forgiving other people, and yourself.</p><p>At some point, Nick became interested in the ideas of a British philosopher, Galen Strawson, who claimed to have no sense of himself as a continuously evolving being—a creature whose self consisted of a coherent story about accumulating memories and distinctive traits. Strawson was, for that reason, uninterested in his past. He acknowledged that his life had shaped him, but he believed that whether or not he consciously remembered it didn’t matter to who he was now, any more than it mattered whether a musician playing a piece could call to mind a memory of each time he’d practiced: what mattered was how well he played. What was important, Strawson felt, was his life in the present. He liked to quote the third Earl of Shaftesbury, a British philosopher of the late seventeenth and early eighteenth centuries, who had felt the same way:</p><blockquote data-testid="blockquote-wrapper"><p>The metaphysicians&nbsp;.&nbsp;.&nbsp;. affirm that if memory be taken away, the self is lost. [But] what matter for memory? What have I to do with that part? If, <em>whilst I am</em>, I am but as I should be, what do I care more?</p></blockquote><p>Nick wasn’t sure he agreed with Strawson, and he certainly didn’t feel, as Strawson did, that his memory of his own life was unimportant, but he found the argument somewhat comforting. He still longed to relive important moments in his life, but it was easier to think about this experience as just one of many he hadn’t had, like paragliding, or visiting Peru, than as a void at the core of his self. Many people believed that their selves were made up largely of memories, and that the loss of those memories would be a self-ending catastrophe. But he knew now that there were also thousands of people like him, who had work and marriages and ideas and thwarted desires and good days and bad days and the rest of it. All they lacked was a past.&nbsp;♦</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The ear does not do a Fourier transform (307 pts)]]></title>
            <link>https://www.dissonances.blog/p/the-ear-does-not-do-a-fourier-transform</link>
            <guid>45762259</guid>
            <pubDate>Thu, 30 Oct 2025 17:01:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dissonances.blog/p/the-ear-does-not-do-a-fourier-transform">https://www.dissonances.blog/p/the-ear-does-not-do-a-fourier-transform</a>, See on <a href="https://news.ycombinator.com/item?id=45762259">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!KT9h!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0ee9e15-988a-44d6-a1c2-22b14fd0a53c_1836x325.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!KT9h!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0ee9e15-988a-44d6-a1c2-22b14fd0a53c_1836x325.png 424w, https://substackcdn.com/image/fetch/$s_!KT9h!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0ee9e15-988a-44d6-a1c2-22b14fd0a53c_1836x325.png 848w, https://substackcdn.com/image/fetch/$s_!KT9h!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0ee9e15-988a-44d6-a1c2-22b14fd0a53c_1836x325.png 1272w, https://substackcdn.com/image/fetch/$s_!KT9h!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0ee9e15-988a-44d6-a1c2-22b14fd0a53c_1836x325.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!KT9h!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0ee9e15-988a-44d6-a1c2-22b14fd0a53c_1836x325.png" width="1836" height="325" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d0ee9e15-988a-44d6-a1c2-22b14fd0a53c_1836x325.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:325,&quot;width&quot;:1836,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:133876,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!KT9h!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0ee9e15-988a-44d6-a1c2-22b14fd0a53c_1836x325.png 424w, https://substackcdn.com/image/fetch/$s_!KT9h!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0ee9e15-988a-44d6-a1c2-22b14fd0a53c_1836x325.png 848w, https://substackcdn.com/image/fetch/$s_!KT9h!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0ee9e15-988a-44d6-a1c2-22b14fd0a53c_1836x325.png 1272w, https://substackcdn.com/image/fetch/$s_!KT9h!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0ee9e15-988a-44d6-a1c2-22b14fd0a53c_1836x325.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Let’s talk about how the cochlea computes!</p><p><span>The tympanic membrane (eardrum) is vibrated by changes in air pressure (sound waves). Bones in the middle ear amplify and send these vibrations to the fluid-filled, snail-shaped cochlea. Vibrations travel through the fluid to the basilar membrane, which remarkably performs </span><em>frequency separation</em><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-148555184" href="https://www.dissonances.blog/p/the-ear-does-not-do-a-fourier-transform#footnote-1-148555184" target="_self" rel="">1</a></span><span>: the stiffer, lighter base resonates with high frequency components of the signal, and the more flexible, heavier apex resonates with lower frequencies. Between the two ends, the resonant frequencies decrease logarithmically in space</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-148555184" href="https://www.dissonances.blog/p/the-ear-does-not-do-a-fourier-transform#footnote-2-148555184" target="_self" rel="">2</a></span><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!5ZZC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe92e54-fc41-477d-891c-a6e968931613_894x772.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!5ZZC!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe92e54-fc41-477d-891c-a6e968931613_894x772.png 424w, https://substackcdn.com/image/fetch/$s_!5ZZC!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe92e54-fc41-477d-891c-a6e968931613_894x772.png 848w, https://substackcdn.com/image/fetch/$s_!5ZZC!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe92e54-fc41-477d-891c-a6e968931613_894x772.png 1272w, https://substackcdn.com/image/fetch/$s_!5ZZC!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe92e54-fc41-477d-891c-a6e968931613_894x772.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!5ZZC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe92e54-fc41-477d-891c-a6e968931613_894x772.png" width="366" height="316.0536912751678" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bbe92e54-fc41-477d-891c-a6e968931613_894x772.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:772,&quot;width&quot;:894,&quot;resizeWidth&quot;:366,&quot;bytes&quot;:127605,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!5ZZC!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe92e54-fc41-477d-891c-a6e968931613_894x772.png 424w, https://substackcdn.com/image/fetch/$s_!5ZZC!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe92e54-fc41-477d-891c-a6e968931613_894x772.png 848w, https://substackcdn.com/image/fetch/$s_!5ZZC!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe92e54-fc41-477d-891c-a6e968931613_894x772.png 1272w, https://substackcdn.com/image/fetch/$s_!5ZZC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe92e54-fc41-477d-891c-a6e968931613_894x772.png 1456w" sizes="100vw"></picture></div></a><figcaption><span>Resonant frequencies of the basilar membrane. Outer, larger numbers are frequencies (Hz). Inner, smaller numbers are distance along the unrolled basilar membrane (mm). From </span><a href="https://redwood.berkeley.edu/wp-content/uploads/2024/09/auditory-coding.pdf" rel="">lecture slides</a><span>.</span></figcaption></figure></div><p>The hair cells on different parts of the basilar membrane wiggle back and forth at the frequency corresponding to their position on the membrane. But how do wiggling hair cells translate to electrical signals? This mechanoelectrical transduction process feels like it could be from a Dr. Seuss world: springs connected to the ends of hair cells open and close ion channels at the frequency of the vibration, which then cause neurotransmitter release. Bruno calls them “trapdoors”. Here’s a visualization:</p><div id="youtube2-y_hQiIH_aAc" data-attrs="{&quot;videoId&quot;:&quot;y_hQiIH_aAc&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/y_hQiIH_aAc?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><span>It’s clear that the hardware of the ear is well-equipped for frequency analysis. Nerve fibers serve as </span><em>filters</em><span> to extract temporal and frequency information about a signal. Below are examples of filters (not necessarily of the ear) shown in the time domain. On the left are filters that are more localized in time, i.e. when a filter is applied to a signal, it is clear when in the signal the corresponding frequency occurred. On the right are filters that have less temporal specificity, but are more uniformly distributed across frequencies compared to the left one.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!6Ysl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78354a05-d8c7-4c3f-97bc-547625619cd5_1547x715.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!6Ysl!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78354a05-d8c7-4c3f-97bc-547625619cd5_1547x715.png 424w, https://substackcdn.com/image/fetch/$s_!6Ysl!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78354a05-d8c7-4c3f-97bc-547625619cd5_1547x715.png 848w, https://substackcdn.com/image/fetch/$s_!6Ysl!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78354a05-d8c7-4c3f-97bc-547625619cd5_1547x715.png 1272w, https://substackcdn.com/image/fetch/$s_!6Ysl!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78354a05-d8c7-4c3f-97bc-547625619cd5_1547x715.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!6Ysl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78354a05-d8c7-4c3f-97bc-547625619cd5_1547x715.png" width="632" height="292.10084033613447" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/78354a05-d8c7-4c3f-97bc-547625619cd5_1547x715.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:715,&quot;width&quot;:1547,&quot;resizeWidth&quot;:632,&quot;bytes&quot;:176287,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!6Ysl!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78354a05-d8c7-4c3f-97bc-547625619cd5_1547x715.png 424w, https://substackcdn.com/image/fetch/$s_!6Ysl!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78354a05-d8c7-4c3f-97bc-547625619cd5_1547x715.png 848w, https://substackcdn.com/image/fetch/$s_!6Ysl!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78354a05-d8c7-4c3f-97bc-547625619cd5_1547x715.png 1272w, https://substackcdn.com/image/fetch/$s_!6Ysl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78354a05-d8c7-4c3f-97bc-547625619cd5_1547x715.png 1456w" sizes="100vw"></picture></div></a><figcaption><span>Filters as a function of time. Left: mostly high temporal precision (short duration), but less uniform tiling of frequencies. Right: mostly low temporal precision (long duration), but more uniform tiling of frequencies. </span><a href="https://pubmed.ncbi.nlm.nih.gov/11896400/" rel="">Lewicki 2002</a><span>.</span></figcaption></figure></div><p>Wouldn’t it be convenient if the cochlea were doing a Fourier transform, which would fit cleanly into how we often analyze signals in engineering? But no 🙅🏻‍♀️! A Fourier transform has no explicit temporal precision, and resembles something closer to the waveforms on the right; this is not what the filters in the cochlea look like. </p><p><span>We can visualize different filtering schemes, or tiling of the time-frequency domain, in the following figure. In the leftmost box, where each rectangle represents a filter, a signal could be represented at a high temporal resolution (similar to left filters above), but without information about its constituent frequencies. On the other end of the spectrum, the Fourier transform performs precise frequency decomposition, but we cannot tell when in the signal that frequency occurred (similar to right filters)</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-148555184" href="https://www.dissonances.blog/p/the-ear-does-not-do-a-fourier-transform#footnote-3-148555184" target="_self" rel="">3</a></span><span>. What the cochlea is actually doing is somewhere between a wavelet and Gabor. At high frequencies, frequency resolution is sacrificed for temporal resolution, and vice versa at low frequencies.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ckic!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927e2b1f-7d82-4c82-90d2-aebb2702ab5b_1910x460.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ckic!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927e2b1f-7d82-4c82-90d2-aebb2702ab5b_1910x460.png 424w, https://substackcdn.com/image/fetch/$s_!ckic!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927e2b1f-7d82-4c82-90d2-aebb2702ab5b_1910x460.png 848w, https://substackcdn.com/image/fetch/$s_!ckic!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927e2b1f-7d82-4c82-90d2-aebb2702ab5b_1910x460.png 1272w, https://substackcdn.com/image/fetch/$s_!ckic!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927e2b1f-7d82-4c82-90d2-aebb2702ab5b_1910x460.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ckic!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927e2b1f-7d82-4c82-90d2-aebb2702ab5b_1910x460.png" width="1456" height="351" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/927e2b1f-7d82-4c82-90d2-aebb2702ab5b_1910x460.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:351,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:68414,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ckic!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927e2b1f-7d82-4c82-90d2-aebb2702ab5b_1910x460.png 424w, https://substackcdn.com/image/fetch/$s_!ckic!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927e2b1f-7d82-4c82-90d2-aebb2702ab5b_1910x460.png 848w, https://substackcdn.com/image/fetch/$s_!ckic!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927e2b1f-7d82-4c82-90d2-aebb2702ab5b_1910x460.png 1272w, https://substackcdn.com/image/fetch/$s_!ckic!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927e2b1f-7d82-4c82-90d2-aebb2702ab5b_1910x460.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>In each large box, each rectangle represents a filter. The human ear does not perform a Fourier transform, but rather employs filters that are somewhere between a wavelet and Gabor. From </span><a href="https://redwood.berkeley.edu/wp-content/uploads/2020/08/new-window-on-sound.pdf" rel="">Olshausen &amp; O’Connor 2002</a><span>.</span></figcaption></figure></div><p><span>Why would this type of frequency-temporal precision tradeoff be a good representation? One theory, explored in </span><a href="https://pubmed.ncbi.nlm.nih.gov/11896400/" rel="">Lewicki 2002</a><span>, is that these filters are a strategy to </span><em>reduce the redundancy</em><span> in the representation of natural sounds. Lewicki performed independent component analysis (ICA) to produce filters maximizing statistical independence, comparing environmental sounds, animal vocalizations, and human speech. The tradeoffs look different for each one, and you can kind of map them to somewhere in the above cartoon.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!kXE0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F529039e3-0143-4af9-9d85-d39ef36b662d_2320x1096.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!kXE0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F529039e3-0143-4af9-9d85-d39ef36b662d_2320x1096.png 424w, https://substackcdn.com/image/fetch/$s_!kXE0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F529039e3-0143-4af9-9d85-d39ef36b662d_2320x1096.png 848w, https://substackcdn.com/image/fetch/$s_!kXE0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F529039e3-0143-4af9-9d85-d39ef36b662d_2320x1096.png 1272w, https://substackcdn.com/image/fetch/$s_!kXE0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F529039e3-0143-4af9-9d85-d39ef36b662d_2320x1096.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!kXE0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F529039e3-0143-4af9-9d85-d39ef36b662d_2320x1096.png" width="1456" height="688" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/529039e3-0143-4af9-9d85-d39ef36b662d_2320x1096.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:688,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:653109,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!kXE0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F529039e3-0143-4af9-9d85-d39ef36b662d_2320x1096.png 424w, https://substackcdn.com/image/fetch/$s_!kXE0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F529039e3-0143-4af9-9d85-d39ef36b662d_2320x1096.png 848w, https://substackcdn.com/image/fetch/$s_!kXE0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F529039e3-0143-4af9-9d85-d39ef36b662d_2320x1096.png 1272w, https://substackcdn.com/image/fetch/$s_!kXE0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F529039e3-0143-4af9-9d85-d39ef36b662d_2320x1096.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>ICA on environmental sounds (rustling brush, rain, etc.) and human speech (various American English dialects) result in wavelets, while animal vocalizations (rainforest mammals) result in something closer to a Fourier transform. From </span><a href="https://pubmed.ncbi.nlm.nih.gov/11896400/" rel="">Lewicki 2002</a><span>.</span></figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!eStx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4533f3e-748f-4141-b2ef-2738ef54ca02_2414x830.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!eStx!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4533f3e-748f-4141-b2ef-2738ef54ca02_2414x830.png 424w, https://substackcdn.com/image/fetch/$s_!eStx!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4533f3e-748f-4141-b2ef-2738ef54ca02_2414x830.png 848w, https://substackcdn.com/image/fetch/$s_!eStx!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4533f3e-748f-4141-b2ef-2738ef54ca02_2414x830.png 1272w, https://substackcdn.com/image/fetch/$s_!eStx!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4533f3e-748f-4141-b2ef-2738ef54ca02_2414x830.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!eStx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4533f3e-748f-4141-b2ef-2738ef54ca02_2414x830.png" width="1456" height="501" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a4533f3e-748f-4141-b2ef-2738ef54ca02_2414x830.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:501,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:313516,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!eStx!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4533f3e-748f-4141-b2ef-2738ef54ca02_2414x830.png 424w, https://substackcdn.com/image/fetch/$s_!eStx!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4533f3e-748f-4141-b2ef-2738ef54ca02_2414x830.png 848w, https://substackcdn.com/image/fetch/$s_!eStx!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4533f3e-748f-4141-b2ef-2738ef54ca02_2414x830.png 1272w, https://substackcdn.com/image/fetch/$s_!eStx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4533f3e-748f-4141-b2ef-2738ef54ca02_2414x830.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Examples of filters shown above.</figcaption></figure></div><p>It appears that human speech occupies a distinct time-frequency space. Some speculate that speech evolved to fill a time-frequency space that wasn’t yet occupied by other existing sounds.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!K57D!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e9195f-082d-4f7e-a8c2-9ecab3fcda9d_780x705.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!K57D!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e9195f-082d-4f7e-a8c2-9ecab3fcda9d_780x705.png 424w, https://substackcdn.com/image/fetch/$s_!K57D!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e9195f-082d-4f7e-a8c2-9ecab3fcda9d_780x705.png 848w, https://substackcdn.com/image/fetch/$s_!K57D!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e9195f-082d-4f7e-a8c2-9ecab3fcda9d_780x705.png 1272w, https://substackcdn.com/image/fetch/$s_!K57D!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e9195f-082d-4f7e-a8c2-9ecab3fcda9d_780x705.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!K57D!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e9195f-082d-4f7e-a8c2-9ecab3fcda9d_780x705.png" width="376" height="339.84615384615387" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/96e9195f-082d-4f7e-a8c2-9ecab3fcda9d_780x705.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:705,&quot;width&quot;:780,&quot;resizeWidth&quot;:376,&quot;bytes&quot;:80533,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!K57D!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e9195f-082d-4f7e-a8c2-9ecab3fcda9d_780x705.png 424w, https://substackcdn.com/image/fetch/$s_!K57D!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e9195f-082d-4f7e-a8c2-9ecab3fcda9d_780x705.png 848w, https://substackcdn.com/image/fetch/$s_!K57D!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e9195f-082d-4f7e-a8c2-9ecab3fcda9d_780x705.png 1272w, https://substackcdn.com/image/fetch/$s_!K57D!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e9195f-082d-4f7e-a8c2-9ecab3fcda9d_780x705.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>+: animal vocalizations, x: environmental sounds, o: human speech. </span><a href="https://pubmed.ncbi.nlm.nih.gov/11896400/" rel="">Lewicki 2002</a><span>.</span></figcaption></figure></div><p>To drive the theory home, one that we have been hinting at since the outset: forming ecologically-relevant representations makes sense, as behavior is dependent on the environment. It appears that for audition, as well as other sensory modalities, we are doing this. This is a bit of a teaser for efficient coding, which we will get to soon.</p><p><span>We’ve talked about some incredible mechanisms that occur at the beginning of the sensory coding process, but it’s truly just the tiny tip of the ice burg. We also glossed over </span><em>how</em><span> these computations occur. The next lecture will zoom into the biophysics of computation in neurons.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Falling panel prices lead to global solar boom, except for the US (218 pts)]]></title>
            <link>https://arstechnica.com/science/2025/10/theres-a-global-boom-in-solar-except-in-the-united-states/</link>
            <guid>45761902</guid>
            <pubDate>Thu, 30 Oct 2025 16:32:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/science/2025/10/theres-a-global-boom-in-solar-except-in-the-united-states/">https://arstechnica.com/science/2025/10/theres-a-global-boom-in-solar-except-in-the-united-states/</a>, See on <a href="https://news.ycombinator.com/item?id=45761902">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            <article data-id="2124864">
  
  <header>
  <div>
    <div>
      

      <h2>
        Falling panel prices lead to global solar boom, except for the US
      </h2>

      <p>
        The economic case for solar power is stronger than ever.
      </p>

      
    </div>

    <div>
    
    <p>
      White clouds drift over a combined wind-solar installation in Shandong province, China. Beijing’s support for a rapid rollout of solar and wind power forms a stark contrast with the growing antipathy of the Trump administration towards renewables.

              <span>
          Credit:

          
          CFOTO/Future Publishing/Getty Images

                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>To the south of the Monte Cristo mountain range and west of Paymaster Canyon, a vast stretch of the Nevada desert has attracted modern-day prospectors chasing one of 21st-century America’s greatest investment booms.</p>
<p>Solar power developers want to cover an area larger than Washington, DC, with silicon panels and batteries, converting sunlight into electricity that will power air conditioners in sweltering Las Vegas along with millions of other homes and businesses.</p>
<p>But earlier this month, bureaucrats in charge of federal lands scrapped collective approval for the Esmeralda 7 projects, in what campaigners fear is part of an attack on renewable energy under President Donald Trump. “We will not approve wind or farmer destroying [sic] Solar,” he posted on his Truth Social platform in August. Developers will need to reapply individually, slowing progress.</p>
<p>Thousands of miles away on the other side of the Pacific Ocean, it is a different story. China has laid solar panels across an area the size of Chicago high up on the Tibetan Plateau, where the thin air helps more sunlight get through.</p>
<p>The Talatan Solar Park is part of China’s push to double its solar and wind generation capacity over the coming decade. “Green and low-carbon transition is the trend of our time,” President Xi Jinping told delegates at a UN summit in New York last month.</p>
<p>China’s vast production of solar panels and batteries has also pushed down the prices of renewables hardware for everyone else, meaning it has “become very difficult to make any other choice in some places,” according to Heymi Bahar, senior analyst at the International Energy Agency.</p>
<p>In 2010, the IEA estimated that there would be 410 gigawatts (GW) of solar panels installed around the world by 2035. There is already more than four times that capacity, with about half of it in China.</p>
<p>Many countries in Africa and the Middle East, even in petrostates such as Saudi Arabia, are rapidly developing solar power. “It’s a very cheap way to harness the sun,” says Kingsmill Bond, an energy strategist at think-tank Ember.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar1.jpg">
    <p><img width="640" height="326" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar1-640x326.jpg" alt="chart showing global renewables growth" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar1-640x326.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar1-1024x522.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar1-768x391.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar1-980x499.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar1.jpg 1380w" sizes="auto, (max-width: 640px) 100vw, 640px">
                  </p>
          <figcaption>
        <div>
    
    <p><span>
          Credit:

          
          FT

                  </span>
          </p>
  </div>
      </figcaption>
      </a></figure>
<p>Its analysis suggests that, helped by rapid growth in solar and wind energy, renewables generated more electricity than coal-fired power plants during the first half of this year.</p>
<p>Progress in energy and other areas has damped some of the pessimism around global warming. In 2015, the UN predicted temperatures would rise by 4° C compared to pre-industrial levels by 2100. It now projects a rise of 2.6° C, if climate policies are followed through.</p>
<p>But for delegates set to gather in Belém, Brazil, next month for the COP30 climate summit, any jubilation will be tempered by the knowledge that the renewables revolution is a long way from being fulfilled. Emissions from the energy sector rose for the fourth straight year in 2024 to a record high, while the slower growth in US renewables means an ambitious target to triple global capacity by 2030 will probably be missed.</p>
<p>“It’s not job done, [IEA analysis] does throw some genuine caution out there,” says Mike Hemsley, deputy director at the Energy Transitions Commission think-tank.</p>
<p>Renewable energy has lowered wholesale power costs, but that has not necessarily fed through into the prices that consumers pay, while users in many countries have not yet switched to electricity for things like transport and domestic heating in the numbers required to reduce fossil fuel usage.</p>
<p>Calculations by the Energy Institute, the sector’s global body, show that the supply of oil, gas, and coal for energy—electricity generation, heating, industrial usage, and transport—in 2024 rose by more than the supply of energy from low-carbon sources, which also includes nuclear and hydropower. That has led some to argue that renewables are merely helping to meet climbing energy demand, rather than replacing fossil fuels.</p>
<p>“The world remains in an energy addition mode, rather than a clear transition,” said Andy Brown, president of the institute, as it launched its report in August.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          

<h2>“Renewables is the place to be”</h2>
<p>At a solar farm operated by ReNew, one of India’s biggest green energy companies, hundreds of panels glint in the sharp desert sun of surrounding Rajasthan.</p>
<p>India, the world’s third largest carbon emitter, wants to develop 500 gigawatts of clean-energy capacity by 2030, and earlier this year reached 243 GW—meaning more than half of its current installed power capacity is now from renewables.</p>
<p>“Every group in India is now saying: ‘You know what, renewables is the place to be,” says Sumant Sinha, chair and chief executive of ReNew.</p>
<p>Saudi Arabia, blessed with both oil and sun, has developed around 4.34 GW of solar capacity as it tries to free up more oil for export, rather than burning it in its own power stations. It wants to build up to 130 GW by the end of the decade.</p>
<p>“It’s massive, what’s going on,” Marco Arcelli, chief executive of utility ACWA Power, which is part-owned by the kingdom’s sovereign wealth fund, told the FT earlier this year. The company is developing 30 GW of renewables in Saudi Arabia.</p>
<p>South Africa has authorized at least 6 GW of renewable energy capacity since President Cyril Ramaphosa removed the capacity limit on private electricity providers in 2022, breaking years of reluctance among the ruling African National Congress to challenge the dominance of state monopoly utility Eskom.</p>
<figure>
    <div>
            <p><a data-pswp-width="1400" data-pswp-height="933" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/10/energyafrica.jpg 1400w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/energyafrica-640x427.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/energyafrica-1024x682.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/energyafrica-768x512.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/energyafrica-980x653.jpg 980w" data-cropped="false" href="https://cdn.arstechnica.net/wp-content/uploads/2025/10/energyafrica.jpg" target="_blank">
              <img width="640" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/energyafrica-640x427.jpg" alt="factory workers" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/10/energyafrica-640x427.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/energyafrica-1024x682.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/energyafrica-768x512.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/energyafrica-980x653.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/energyafrica.jpg 1400w" sizes="auto, (max-width: 640px) 100vw, 640px">
            </a></p><div id="caption-2124870"><p>
              Workers at the Ener-G-Africa factory in Cape Town test LED lights on solar panels. South Africans are increasingly installing such panels because of the unreliability of normal power supplies.
                              </p><p>
                  Credit:
                                      Esa Alexander/Reuters
                                  </p>
                          </div>
          </div>
          <figcaption>
        <div>
    
    <p>
      Workers at the Ener-G-Africa factory in Cape Town test LED lights on solar panels. South Africans are increasingly installing such panels because of the unreliability of normal power supplies.

              <span>
          Credit:

          
          Esa Alexander/Reuters

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Middle-class households in the country have also rapidly installed solar panels on their roofs to cope with years of planned rolling blackouts due to power shortages. It is part of a worldwide trend for smaller installations as homes and businesses tire of waiting for governments or big utilities to fix power shortages.</p>
<p>Solar panel installations of less than 1MW accounted for about 42 percent of global installations last year, according to BloombergNEF, almost double the 22 percent recorded in 2015. Factories, mosques, and farms in Pakistan have covered their roofs in Chinese-made solar panels to try to avoid surging tariffs for state-provided power.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>“We’ve displaced tens of thousands of diesel generators,” says William Brent, chief marketing officer at Husk Power Systems, which has installed about 400 “mini-grids” of solar and batteries across Nigeria and India. These are helping pharmacies store medicines and shopkeepers keep drinks cool at around half the cost of power from the grid.</p>
<p>The construction of vast solar arrays in deserts and small installations on rooftops have largely been driven by the same underlying trend: falling costs. The huge surfeit of production capacity in China, which produced about eight out of 10 of the world’s solar modules in 2024, has pushed the cost of panels down by almost 90 percent over the past decade and dragged overall capital expenditure costs down 70 percent, according to analysts.</p>
<p>Yet even in places like India, fossil fuels still hold sway. Coal still generates more than 70 percent of the country’s power output and remains politically protected, employing hundreds of thousands directly and many more indirectly in some of India’s poorest regions. “India still has a massive way to go,” says Hemsley at the ETC.</p>
<p>PM Prasad, chair of state-owned Coal India, told the FT earlier this year that it was reopening more than 30 mines and launching up to five new sites, arguing that renewables were not yet capable of meeting fast-growing energy demand.</p>
<p>The painful process of acquiring large tracts of land for solar arrays in a country with millions of smallholder farmers has also led to delays across the renewables sector, many Indian developers grumble. More than 50 GW of renewable power projects are waiting to connect to an overstretched transmission network, estimates the Institute for Energy Economics and Financial Analysis, a think-tank, and cleantech consultancy JMK Research.</p>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar2.jpg">
    <p><img width="640" height="382" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar2-640x382.jpg" alt="Chart showing relative amount of small solar installations" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar2-640x382.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar2-1024x612.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar2-768x459.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar2-980x585.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar2.jpg 1406w" sizes="auto, (max-width: 640px) 100vw, 640px">
                  </p>
          <figcaption>
        <div>
    
    <p><span>
          Credit:

          
          FT

                  </span>
          </p>
  </div>
      </figcaption>
      </a></figure>
<p>Even as solar panels become more popular in Sub-Saharan Africa, millions of homes and businesses still rely on expensive and polluting diesel generators, and roughly 600 million people lack access to power.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>Many people also lack the means to pay commercial rates for electricity, even before factoring in the extra levies needed to finance the cost of new transmission lines, a key enabler of renewables projects around the world.</p>
<p>Electricity storage capabilities also need to dramatically improve if countries want to rely more heavily on intermittent wind and solar farms and phase out backup fossil-fuel capacity.</p>
<p>Large-scale batteries are being deployed rapidly—spurred again by China’s prolific manufacturing output. James Mittell, director at developer Actis Energy, says costs have fallen so much that it is already possible in many markets to build large-scale battery and solar systems, which can deliver power with similar consistency to gas-fired power plants, but at lower cost. “It’s a complete game-changer,” he says.</p>
<p>But progress is also mixed on the second phase of any “transition” to renewable power: persuading consumers and industries to switch to equipment that runs on electricity rather than combustion processes using fossil fuels.</p>
<p>The share of electricity in final energy demand has flatlined in the US and the EU over the past few years, with the growth of electric cars offset by the difficulty of getting people to switch away from gas or oil heating systems to low-carbon electric ones such as heat pumps.</p>
<p>“For electricity [generation] we have a success story,” says Bahar, at the IEA. “For other sectors, it’s way more complicated.”</p>

<h2>Massive growth in China</h2>
<p>China and some parts of Southeast Asia stand out in terms of the portion of energy supplied by electricity increasing—in China’s case, from about 12 percent in 2000 to about 30 percent in 2023—as millions of citizens start driving electric cars and factories switch away from fossil-fueled boilers.</p>
<p>Ember points to data showing that renewables met 84 percent of China’s new electricity demand last year as evidence that coal-powered generation in the country is nearing its peak. “We’re confident renewables can meet all China’s [power] demand growth,” adds Hemsley at the ETC.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>But even here, challenges loom. Major electricity market reforms introduced by Beijing in July mean renewable energy developers no longer get a fixed price akin to that received by coal-fired generators and are instead more exposed to market forces.</p>
<p>“They clearly don’t want to harm the build out of renewables, but they just want it to be done on a more commercial basis,” says Neil Beveridge, who leads Bernstein’s energy analysis in Hong Kong.</p>
<p>But the IEA warns it will lower returns and cut the growth of renewables. “That [impact of the reform] is the biggest uncertainty in our outlook,” adds Bahar at the IEA.</p>
<p>A far sharper slowdown is already underway in the US, where incentives introduced as part of former President Joe Biden’s Inflation Reduction Act in 2022 are rolled back by the second Trump administration. Tax credits have been cut and major projects blocked—spooking investors and leaving existing developers trying to stay afloat.</p>
<figure>
    <div>
            <p><a data-pswp-width="1400" data-pswp-height="933" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/10/solarworkers.jpg 1400w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solarworkers-640x427.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solarworkers-1024x682.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solarworkers-768x512.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solarworkers-980x653.jpg 980w" data-cropped="false" href="https://cdn.arstechnica.net/wp-content/uploads/2025/10/solarworkers.jpg" target="_blank">
              <img width="640" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/solarworkers-640x427.jpg" alt="workers carrying solar panels" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/10/solarworkers-640x427.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solarworkers-1024x682.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solarworkers-768x512.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solarworkers-980x653.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solarworkers.jpg 1400w" sizes="auto, (max-width: 640px) 100vw, 640px">
            </a></p><div id="caption-2124872"><p>
              Workers carry solar panels for a project in Lingwu, China. The country accounts for half the world’s installed solar capacity, but its fossil fuel usage also continues to grow.
                              </p><p>
                  Credit:
                                      Sara Hussein/AFP/Getty Images
                                  </p>
                          </div>
          </div>
          <figcaption>
        <div>
    
    <p>
      Workers carry solar panels for a project in Lingwu, China. The country accounts for half the world’s installed solar capacity, but its fossil fuel usage also continues to grow.

              <span>
          Credit:

          
          Sara Hussein/AFP/Getty Images

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>“It’s very difficult to make big capital decisions based on this,” says Reagan Farr, chief executive of Silicon Ranch, a solar developer. “We don’t have a bipartisan energy policy in the US, which is very bad for the industry and our economy.”</p>
<p>Ørsted, the world’s largest offshore wind company, has had to raise an extra $9 billion from investors after Trump’s hostility to the offshore wind sector prevented it from selling a stake in one of its major US projects.</p>
<p>His tariffs on products from China mean higher costs for solar projects. Analysts say more large-scale solar projects are likely to have their permits revoked or reviewed.</p>
<p>Developers are currently rushing to build, as they have until July 2026 to start construction to capture the tail-end of the tax credits. But some projects and companies are bound to fail. “We’re likely facing several more years of uphill battles for many large-scale projects,” says Abby Watson, president at Groundwire Group, a consultancy.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>The IEA has halved its forecast for renewables growth by 2030 in the US to around 250 GW as a result of Trump’s policies. Analysts at Carbon Brief estimate the country will emit 7 billion tonnes more CO₂ equivalent by 2030 under Trump’s policies than if the country had met its obligations under the 2015 Paris agreement, which he is withdrawing from.</p>
<p>The reduction in renewables growth comes as the country’s electricity demand is rising due to the growth of data centers, many of which are looking to gas-fired or nuclear power stations because they need constant, steady power.</p>
<p>Gas turbine makers are struggling to keep up with demand, while new nuclear power plants are often delayed.</p>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar3.jpg">
    <p><img width="640" height="507" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar3-640x507.jpg" alt="chart showing continued growth of fossil fuels" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar3-640x507.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar3-1024x811.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar3-768x608.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar3-980x776.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/solar3.jpg 1424w" sizes="auto, (max-width: 640px) 100vw, 640px">
                  </p>
          <figcaption>
        <div>
    
    <p><span>
          Credit:

          
          FT

                  </span>
          </p>
  </div>
      </figcaption>
      </a></figure>
<p>Retail electricity prices have already risen by 5 percent since July, according to the Energy Information Administration, and some experts caution they could rise further if supplies are constrained. “The writing is on the wall,” says Pol Lezcano, director of energy and renewables at the CBRE real estate group.</p>
<p>Supporters of renewable electricity argue that the US is missing out on a revolution in cleaner, cheaper technology sweeping the world, with some likening it to the aging cars on Cuba’s roads.</p>
<p>But the relationship between renewable generation and consumer energy bills is complicated. The free energy from the sun or the wind means that the wholesale price of renewable-generated power is lower, but developers still need to make a return on their investment, and grid operators may need to step in to ensure continuity of supply when the wind and the sun are low.</p>
<p>“Even as the cost of producing electricity from renewables falls, consumers may not see immediate or proportional reductions in their bills, raising questions over the impact of renewables on power affordability,” the IEA said in its latest report.</p>
<p>More broadly, the US’s focus on fossil fuels and pullback of support for clean energy further cedes influence over the future global energy system to China.</p>
<p>The US is trying to tie its trading partners into fossil fuels, pressing the EU to buy $750 billion of American oil, natural gas, and nuclear technologies during his presidency as part of a trade deal, scuppering an initiative to begin decarbonizing world shipping and pressuring others to reduce their reliance on Chinese technology.</p>
<p>But the collapsing cost of solar panels in particular has spoken for itself in many parts of the world. Experts caution that the US’s attacks on renewables could cause lasting damage to its competitiveness against China, even if an administration more favorable to renewables were to follow Trump’s.</p>
<p>“China has run far away in terms of competitiveness,” says Antonio Cammisecra, chief executive of ContourGlobal, an independent power producer.</p>
<p>“The US is capable of rebuilding, but it will take time.”</p>
<p><em>Additional reporting by Ahmed Al Omran and David Pilling. Data visualization by Jana Tauschinski.</em></p>
<p><em><a href="https://www.ft.com/">© 2025 The Financial Times Ltd</a>. <a href="https://www.ft.com/">All rights reserved.</a> Not to be redistributed, copied, or modified in any way.</em></p>


          
                  </div>

                  
          






  


  <p>
    <a href="https://arstechnica.com/science/2025/10/theres-a-global-boom-in-solar-except-in-the-united-states/#comments" title="76 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    76 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/tech-policy/2025/10/meta-says-porn-downloads-on-its-ips-were-for-personal-use-not-ai-training/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2171230457-768x432.jpg" alt="Listing image for first story in Most Read: Meta denies torrenting porn to train AI, says downloads were for “personal use”" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Qt Creator 18 Released (131 pts)]]></title>
            <link>https://www.qt.io/blog/qt-creator-18-released</link>
            <guid>45761789</guid>
            <pubDate>Thu, 30 Oct 2025 16:23:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.qt.io/blog/qt-creator-18-released">https://www.qt.io/blog/qt-creator-18-released</a>, See on <a href="https://news.ycombinator.com/item?id=45761789">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          

          

          <p>
            October 30, 2025 by <a href="https://www.qt.io/blog/author/eike-ziller">Eike Ziller</a>
            | <a href="#commento">Comments</a>
          </p>

          

          <p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><h5>We are happy to announce the release of Qt Creator 18!</h5>
<p>Qt Creator 18 adds experimental support for Development Containers and many more improvements.</p>
<!--more-->
<h4>Development Container Support</h4>
<p><img src="https://www.qt.io/hs-fs/hubfs/devcontainer2_s-1.webp?width=474&amp;height=379&amp;name=devcontainer2_s-1.webp" width="474" height="379" loading="lazy" alt="devcontainer2_s-1" srcset="https://www.qt.io/hs-fs/hubfs/devcontainer2_s-1.webp?width=237&amp;height=190&amp;name=devcontainer2_s-1.webp 237w, https://www.qt.io/hs-fs/hubfs/devcontainer2_s-1.webp?width=474&amp;height=379&amp;name=devcontainer2_s-1.webp 474w, https://www.qt.io/hs-fs/hubfs/devcontainer2_s-1.webp?width=711&amp;height=569&amp;name=devcontainer2_s-1.webp 711w, https://www.qt.io/hs-fs/hubfs/devcontainer2_s-1.webp?width=948&amp;height=758&amp;name=devcontainer2_s-1.webp 948w, https://www.qt.io/hs-fs/hubfs/devcontainer2_s-1.webp?width=1185&amp;height=948&amp;name=devcontainer2_s-1.webp 1185w, https://www.qt.io/hs-fs/hubfs/devcontainer2_s-1.webp?width=1422&amp;height=1137&amp;name=devcontainer2_s-1.webp 1422w" sizes="(max-width: 474px) 100vw, 474px"></p>
<p>Qt Creator 18 adds support for <a href="https://containers.dev/" rel="noopener">development containers</a> to automate setting up the development environment of a project. It detects a "devcontainer.json" file in your project directory and creates a Docker container for it. You can let Qt Creator auto-detect kits or specify custom kits and control other aspects like the command bridge (our service for communicating with remote devices) with Qt Creator specific customizations in the development container definition. Note that it is still experimental and does not support all aspects of development containers yet. <a href="https://doc-snapshots.qt.io/qtcreator-18.0/creator-how-to-load-extensions.html" rel="noopener">Enable the extension</a> to use this functionality. <a href="https://code.qt.io/cgit/qt-creator/qt-creator.git/about/src/plugins/devcontainer/README.md" rel="noopener">Find out more</a>.</p>
<h4>General UI</h4>
<p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text">We added an <em>Overview</em> tab on <em>Welcome</em> mode</span> that aggregates content from the other tabs. It suggests tutorials and examples based on your experience and needs, and highlights developer-targeted posts in the Qt blog.</p>
<p>The notifications received a facelift and are now part of the progress notification popups. Y<span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text">ou can opt-out of this with <em>Environment &gt; Interface &gt; Prefer banner style info bars over pop-ups</em>.</span></p>
<h4>Editing</h4>
<p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><img src="https://www.qt.io/hs-fs/hubfs/editor_tabs.png?width=400&amp;height=55&amp;name=editor_tabs.png" width="400" height="55" loading="lazy" alt="editor_tabs" srcset="https://www.qt.io/hs-fs/hubfs/editor_tabs.png?width=200&amp;height=28&amp;name=editor_tabs.png 200w, https://www.qt.io/hs-fs/hubfs/editor_tabs.png?width=400&amp;height=55&amp;name=editor_tabs.png 400w, https://www.qt.io/hs-fs/hubfs/editor_tabs.png?width=600&amp;height=83&amp;name=editor_tabs.png 600w, https://www.qt.io/hs-fs/hubfs/editor_tabs.png?width=800&amp;height=110&amp;name=editor_tabs.png 800w, https://www.qt.io/hs-fs/hubfs/editor_tabs.png?width=1000&amp;height=138&amp;name=editor_tabs.png 1000w, https://www.qt.io/hs-fs/hubfs/editor_tabs.png?width=1200&amp;height=165&amp;name=editor_tabs.png 1200w" sizes="(max-width: 400px) 100vw, 400px"></span></p>
<p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text">We added the option to use tabbed editors (<em>Environment &gt; Interface &gt; Use tabbed editors</em>).</span> But remember faster ways of navigating your code, such as <a href="https://doc.qt.io/qtcreator/creator-editor-locator.html" rel="noopener">Locator filters</a> for opening files or jumping to specific class or symbol, <span>Follow Symbol</span>, <span>Find References</span>, the <span>Open Documents</span> and <span>File System</span> views, the edit location history <span>Window &gt; Go Back/Forward</span> and the corresponding keyboard shortcuts, and <span>Window &gt; Previous/Next Open Document in History</span> and the corresponding keyboard shortcuts.</p>
<p>For the C++ support w<span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text">e updated Clangd/LLVM to the 21.1 release for our prebuilt binaries. Additionally,&nbsp;the built-in code model received a <a href="https://code.qt.io/cgit/qt-creator/qt-creator.git/tree/dist/changelog/changes-18.0.0.md?h=18.0#n85" rel="noopener">wide range of fixes</a> for newer C++ features. We added quick fixes for removing curly braces and for adding definitions for static data members.</span></p>
<p><span data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text">For QML you can now download and use the latest QML Language Server even if you are using older Qt versions for your projects (in the QML Language Server settings in <em>Preferences &gt; Language Client</em>).</span></p>
<p><span data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text">We also added support for GitHub Enterprise environments for GitHub Copilot.</span></p>
<h4><span data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text">Projects</span></h4>
<p><span data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text">We moved the ".user" files that contain the Qt Creator specific project settings into the ".qtcreator/" subdirectory of the project directory. Existing ".user" files from older projects are still updated for compatibility though.</span></p>
<p><span data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><img src="https://www.qt.io/hs-fs/hubfs/projectmodewithtabs.png?width=398&amp;height=74&amp;name=projectmodewithtabs.png" width="398" height="74" loading="lazy" alt="projectmodewithtabs" srcset="https://www.qt.io/hs-fs/hubfs/projectmodewithtabs.png?width=199&amp;height=37&amp;name=projectmodewithtabs.png 199w, https://www.qt.io/hs-fs/hubfs/projectmodewithtabs.png?width=398&amp;height=74&amp;name=projectmodewithtabs.png 398w, https://www.qt.io/hs-fs/hubfs/projectmodewithtabs.png?width=597&amp;height=111&amp;name=projectmodewithtabs.png 597w, https://www.qt.io/hs-fs/hubfs/projectmodewithtabs.png?width=796&amp;height=148&amp;name=projectmodewithtabs.png 796w, https://www.qt.io/hs-fs/hubfs/projectmodewithtabs.png?width=995&amp;height=185&amp;name=projectmodewithtabs.png 995w, https://www.qt.io/hs-fs/hubfs/projectmodewithtabs.png?width=1194&amp;height=222&amp;name=projectmodewithtabs.png 1194w" sizes="(max-width: 398px) 100vw, 398px"></span></p>
<p><span data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text">In Projects mode you can now choose to only show kits that are actually usable by the project, or only kits that the project is already configured for. We also split up the <em>Run</em> page into <em>Deploy Settings</em>&nbsp;and <em>Run Settings</em>, and together with the <em>Build Settings</em> moved them out of the kit selection to tabs in the content view. Normally the run configurations of the various build configurations are independent of each other. In Qt Creator 18 we have added the option to sync the run configurations within a single kit, or even between all kits that the project is configured for.</span></p>
<p><span data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text">For CMake projects we now also support <a href="https://cmake.org/cmake/help/v3.25/manual/cmake-presets.7.html#test-preset" rel="noopener">Test Presets</a> and added a Locator filter "ct" for running CTest based tests. We also fixed building CMake&nbsp;projects for all build configurations (<span>Build &gt; Build Project for All Configurations</span>).</span></p>
<h4><span data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text">Devices</span></h4>
<p>We added a configuration for various tools on remote Linux&nbsp;devices, like GDB server, CMake, clangd, rsync, qmake, and more, and the option to auto-detect them. This improves the configuration of remote devices as build devices. More is to come in future releases in this regard. You can now also decide if Qt Creator should try to automatically re-connect to devices at startup with a new <em>Auto-connect on startup</em>&nbsp;setting. We also fixed that it wasn't possibly to use rsync for deployment when building on a remote device as well as using a remote target device.</p>
<h4>Other Improvements</h4>
<p>Qt Creator 18 comes with many more improvements and fixes. For example the Git commit editor now provides many more actions on files, like staging, unstaging, and directly adding files to ".gitignore".</p>
<p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text">Please have a look at our <a href="https://code.qt.io/cgit/qt-creator/qt-creator.git/about/dist/changelog/changes-18.0.0.md?h=18.0" rel="noopener">change log</a> for more detailed information.</span></p>
<span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"></span>
<h3>Get Qt Creator 18</h3>
<p>The new version is available as an update in the Qt Online Installer (<a href="https://www.qt.io/download-dev?hsLang=en" rel="noopener">commercial</a>, <a href="https://www.qt.io/download-qt-installer-oss?hsLang=en" rel="noopener">opensource</a>). You also find commercially licensed offline installers on the <a href="https://login.qt.io/">Qt Account Portal</a>, and opensource packages on our <a href="https://www.qt.io/offline-installers?hsLang=en" rel="noopener">opensource download page</a>. <span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text">This<span data-teams="true">&nbsp;is a free upgrade for all users.</span></span></p>
<p>Please post issues in our <a href="https://bugreports.qt.io/projects/QTCREATORBUG">bug tracker</a>. You can also find us on IRC on #qt-creator on <a href="https://web.libera.chat/" rel="noopener">irc.libera.chat</a>, and on the <a href="http://lists.qt-project.org/mailman/listinfo/qt-creator">Qt Creator mailing list</a>.</p>
<p>You can read the Qt Creator Manual in Qt Creator in the <a href="https://doc.qt.io/qtcreator/creator-how-to-get-help.html" rel="noopener" target="_blank">Help mode</a> or access it online in the <a href="https://doc.qt.io/qtcreator/index.html" rel="noopener" target="_blank">Qt documentation portal.</a></p>
</span></p>

          

          
          <hr>

          <h6>Blog Topics:</h6>
          
          


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Affinity Studio now free (680 pts)]]></title>
            <link>https://www.affinity.studio/get-affinity</link>
            <guid>45761445</guid>
            <pubDate>Thu, 30 Oct 2025 15:54:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.affinity.studio/get-affinity">https://www.affinity.studio/get-affinity</a>, See on <a href="https://news.ycombinator.com/item?id=45761445">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Please update your browser</h2><p>It seems you are using an old or unsupported browser. To continue enjoying our product, please update to a recent version of one of the following browsers:</p><ul><li><a href="https://www.google.com/chrome/"><img src="https://static.canva.com/static/images/supported_browsers/chrome.2024.png" width="100" height="100"><p>Chrome</p></a></li><li><a href="https://www.firefox.com/"><img src="https://static.canva.com/static/images/supported_browsers/firefox.2024.png" width="100" height="100"><p>Firefox</p></a></li><li><a href="https://www.apple.com/safari/"><img src="https://static.canva.com/static/images/supported_browsers/safari.2024.png" width="100" height="100"><p>Safari<br>(macOS only)</p></a></li><li><a href="https://www.microsoft.com/windows/microsoft-edge/"><img src="https://static.canva.com/static/images/supported_browsers/edge.2024.png" width="100" height="100"><p>Edge</p></a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PlanetScale Offering $5 Databases (106 pts)]]></title>
            <link>https://planetscale.com/blog/5-dollar-planetscale</link>
            <guid>45761027</guid>
            <pubDate>Thu, 30 Oct 2025 15:20:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://planetscale.com/blog/5-dollar-planetscale">https://planetscale.com/blog/5-dollar-planetscale</a>, See on <a href="https://news.ycombinator.com/item?id=45761027">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>By <!-- -->Sam Lambert<!-- --> | <time datetime="2025-10-30T09:00:00.000Z">October 30, 2025</time></p><p>PlanetScale is synonymous with quality, performance, and reliability. Up until now, the entry level PlanetScale cluster configuration was 3 node, multi-AZ, and highly available. At $30 a month this is incredible value, however, not everyone wants or needs HA.</p><p>Every day we get requests for an entry level tier that is more accessible to builders on day 1. People want the quality of PlanetScale and our game changing features like <a href="https://planetscale.com/docs/postgres/monitoring/query-insights">Insights</a> without the cost overhead of 3 nodes.</p><p>Over the next couple of months we will be rolling out a single node, non-HA mode for PlanetScale Postgres and introducing a new node type: The <code>PS-5</code> which is priced at $5 a month. Single node is perfect for development, testing, and non-critical workloads. Customers will be able to vertically scale a single node to meet their needs without having to add replicas or sacrifice durability.</p><p>You can sign up <a href="https://planetscale.com/single-node">here</a> to be notified when single node releases.</p><p>Our starter pricing is now:</p><div><table><thead><tr><th>Node Class</th><th>Mode</th><th>Price</th></tr></thead><tbody><tr><td>PS-5 (arm and intel)</td><td>Single node</td><td>$5</td></tr><tr><td>PS-10 (arm)</td><td>Single node</td><td>$10</td></tr><tr><td>PS-10 (intel)</td><td>Single node</td><td>$13</td></tr><tr><td>PS-10 (arm)</td><td>HA (3 node)</td><td>$30</td></tr><tr><td>PS-10 (intel)</td><td>HA (3 node)</td><td>$39</td></tr></tbody></table></div><p>If you're bullish on your company's future, you know you'll need to scale eventually, and the database is usually the first bottleneck. We talk to startups daily who experienced unexpected fast growth and have to scramble through emergency migrations to PlanetScale to handle the load, a stressful process when you're in the spotlight. With more approachable pricing from day 1, you can now start small and grow to hyper scale without ever changing your database platform or dealing with a complex migration.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Free software scares normal people (407 pts)]]></title>
            <link>https://danieldelaney.net/normal/</link>
            <guid>45760878</guid>
            <pubDate>Thu, 30 Oct 2025 15:07:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danieldelaney.net/normal/">https://danieldelaney.net/normal/</a>, See on <a href="https://news.ycombinator.com/item?id=45760878">Hacker News</a></p>
<div id="readability-page-1" class="page"><a href="https://danieldelaney.net/" id="logo"><span>Dd</span></a>






<p>I’m the person my friends and family come to for computer-related help. (Maybe you, gentle reader, can relate.) This experience has taught me which computing tasks are frustrating for normal people.</p>

<p>Normal people often struggle with converting video. They will need to watch, upload, or otherwise do stuff with a video, but the format will be weird. (Weird, broadly defined, is anything that won’t play in QuickTime or upload to Facebook.)</p>

<p>I would love to recommend Handbrake to them, but the user interface is by and for power users. Opening it makes normal people feel unpleasant feelings.</p>



<p>This problem is rampant in free software. The FOSS world is full of powerful tools that only have a “power user” UI. As a result, people give up. Or worse: they ask people like you and I to do it for them.</p>

<p>I want to make the case to you that you can (and should) solve this kind of problem in a single evening.</p>

<p>Take the example of <a href="https://danieldelaney.net/magicbrake/">Magicbrake</a>, a simple front end I built. It hides the power and flexibility of Handbrake. It does only <em>the one thing</em> most people need Handbrake for: taking a weird video file and making it normal. (Normal, for our purposes, means a small MP4 that works just about anywhere.)</p>

<p>There is exactly one button.</p>



<p>This is a fast and uncomplicated thing to do. Unfortunately, the people who have the ability to solve problems like this are often disinclined to do it.</p>

<p>“Why would you make Handbrake less powerful on purpose?”</p>

<p>“What if someone wants a different format?”</p>

<p>“What about [feature/edge case]?”</p>

<p>The answer to all these questions is the same: a person who needs or wants that stuff can use Handbrake. If they don’t need everything Handbrake can do and find it bewildering, they can use this. Everyone wins.</p>

<p>It’s a bit like obscuring the less-used functions on a TV remote with tape. The functions still exist if you need them, but you’re not required to contend with them just to turn the TV on.</p>



<p>People benefit from stuff like this, and I challenge you to make more of it. Opportunities are everywhere. The world is full of media servers normal people can’t set up. Free audio editing software that requires hours of learning to be useful for simple tasks. Network monitoring tools that seem designed to ward off the uninitiated. Great stuff normal people don’t use. All because there’s only one UI, and it’s designed to do <em>everything.</em></p>

<p>80% of the people only need 20% of the features. Hide the rest from them and you’ll make them more productive and happy. That’s really all it takes.</p>




<!-- If you’d like to be notified via e-mail when a new article is published, <a href="/subscribe">subscribe</a>. -->

<!-- Privacy-friendly analytics by Plausible -->



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ventoy: Create Bootable USB Drive for ISO/WIM/IMG/VHD(x)/EFI Files (253 pts)]]></title>
            <link>https://github.com/ventoy/Ventoy</link>
            <guid>45760340</guid>
            <pubDate>Thu, 30 Oct 2025 14:23:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ventoy/Ventoy">https://github.com/ventoy/Ventoy</a>, See on <a href="https://news.ycombinator.com/item?id=45760340">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">
  <a href="https://www.ventoy.net/" rel="nofollow">Ventoy</a>
</h2><a id="user-content---ventoy" aria-label="Permalink: Ventoy" href="#--ventoy"></a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8e7245a50b60a6a40d959f36ab5d2ef6bdd4c96e1e761d1f40e0ba7a8afc6a4e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f76656e746f792f56656e746f792e7376673f7374796c653d666f722d7468652d6261646765"><img src="https://camo.githubusercontent.com/8e7245a50b60a6a40d959f36ab5d2ef6bdd4c96e1e761d1f40e0ba7a8afc6a4e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f76656e746f792f56656e746f792e7376673f7374796c653d666f722d7468652d6261646765" data-canonical-src="https://img.shields.io/github/release/ventoy/Ventoy.svg?style=for-the-badge"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b84f4d5a1e5f5ab8dd68b766cd4e96eca74ac0eeeb4f05d521b9607faa118c2d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f76656e746f792f56656e746f793f7374796c653d666f722d7468652d6261646765"><img src="https://camo.githubusercontent.com/b84f4d5a1e5f5ab8dd68b766cd4e96eca74ac0eeeb4f05d521b9607faa118c2d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f76656e746f792f56656e746f793f7374796c653d666f722d7468652d6261646765" data-canonical-src="https://img.shields.io/github/license/ventoy/Ventoy?style=for-the-badge"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4ff60dfbfca50e9d11fbf34acc05bdfe43543a7f505577220632ed41a6baba89/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f76656e746f792f56656e746f793f7374796c653d666f722d7468652d6261646765"><img src="https://camo.githubusercontent.com/4ff60dfbfca50e9d11fbf34acc05bdfe43543a7f505577220632ed41a6baba89/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f76656e746f792f56656e746f793f7374796c653d666f722d7468652d6261646765" data-canonical-src="https://img.shields.io/github/stars/ventoy/Ventoy?style=for-the-badge"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/0d8677afad815d962e1745279546bd4e46937609fbf4b3bfe9ea18c0367d96b8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f76656e746f792f56656e746f792f746f74616c2e7376673f7374796c653d666f722d7468652d6261646765"><img src="https://camo.githubusercontent.com/0d8677afad815d962e1745279546bd4e46937609fbf4b3bfe9ea18c0367d96b8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f76656e746f792f56656e746f792f746f74616c2e7376673f7374796c653d666f722d7468652d6261646765" data-canonical-src="https://img.shields.io/github/downloads/ventoy/Ventoy/total.svg?style=for-the-badge"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/eccdd1e3ac8e0cb6d94cad910ae73790840aa66e1f887f171bcb4bbb29e31b35/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f76656e746f792f56656e746f792f63692e796d6c3f6c6162656c3d616374696f6e73266c6f676f3d676974687562267374796c653d666f722d7468652d6261646765"><img src="https://camo.githubusercontent.com/eccdd1e3ac8e0cb6d94cad910ae73790840aa66e1f887f171bcb4bbb29e31b35/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f76656e746f792f56656e746f792f63692e796d6c3f6c6162656c3d616374696f6e73266c6f676f3d676974687562267374796c653d666f722d7468652d6261646765" data-canonical-src="https://img.shields.io/github/actions/workflow/status/ventoy/Ventoy/ci.yml?label=actions&amp;logo=github&amp;style=for-the-badge"></a>
</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">
Ventoy is an open source tool to create bootable USB drive for ISO/WIM/IMG/VHD(x)/EFI files. <br>
With ventoy, you don't need to format the disk over and over, just copy the image files to the USB drive and boot them.   
You can copy many image files at a time and ventoy will give you a boot menu to select them. <br> 
You can also browse ISO/WIM/IMG/VHD(x)/EFI files in local disk and boot them.<br>
x86 Legacy BIOS, IA32 UEFI, x86_64 UEFI, ARM64 UEFI and MIPS64EL UEFI are supported in the same way.<br>
Both MBR and GPT partition style are supported in the same way.<br>
Most type of OS supported(Windows/WinPE/Linux/Unix/ChromeOS/Vmware/Xen...) <br>
  1200+ ISO files are tested (<a href="https://www.ventoy.net/en/isolist.html" rel="nofollow">List</a>). 90%+ distros in <a href="https://distrowatch.com/" rel="nofollow">distrowatch.com</a> supported (<a href="https://www.ventoy.net/en/distrowatch.html" rel="nofollow">Details</a>). <p>
Official Website: <a href="https://www.ventoy.net/" rel="nofollow">https://www.ventoy.net</a></p></h4><a id="user-content-ventoy-is-an-open-source-tool-to-create-bootable-usb-drive-for-isowimimgvhdxefi-files-with-ventoy-you-dont-need-to-format-the-disk-over-and-over-just-copy-the-image-files-to-the-usb-drive-and-boot-them---you-can-copy-many-image-files-at-a-time-and-ventoy-will-give-you-a-boot-menu-to-select-them--you-can-also-browse-isowimimgvhdxefi-files-in-local-disk-and-boot-themx86-legacy-bios-ia32-uefi-x86_64-uefi-arm64-uefi-and-mips64el-uefi-are-supported-in-the-same-wayboth-mbr-and-gpt-partition-style-are-supported-in-the-same-waymost-type-of-os-supportedwindowswinpelinuxunixchromeosvmwarexen---1200-iso-files-are-tested-list-90-distros-in-distrowatchcom-supported-details-official-website-httpswwwventoynet" aria-label="Permalink: 
Ventoy is an open source tool to create bootable USB drive for ISO/WIM/IMG/VHD(x)/EFI files. 
With ventoy, you don't need to format the disk over and over, just copy the image files to the USB drive and boot them.   
You can copy many image files at a time and ventoy will give you a boot menu to select them.  
You can also browse ISO/WIM/IMG/VHD(x)/EFI files in local disk and boot them.
x86 Legacy BIOS, IA32 UEFI, x86_64 UEFI, ARM64 UEFI and MIPS64EL UEFI are supported in the same way.
Both MBR and GPT partition style are supported in the same way.
Most type of OS supported(Windows/WinPE/Linux/Unix/ChromeOS/Vmware/Xen...) 
  1200+ ISO files are tested (List). 90%+ distros in distrowatch.com supported (Details). Official Website: https://www.ventoy.net" href="#ventoy-is-an-open-source-tool-to-create-bootable-usb-drive-for-isowimimgvhdxefi-files-with-ventoy-you-dont-need-to-format-the-disk-over-and-over-just-copy-the-image-files-to-the-usb-drive-and-boot-them---you-can-copy-many-image-files-at-a-time-and-ventoy-will-give-you-a-boot-menu-to-select-them--you-can-also-browse-isowimimgvhdxefi-files-in-local-disk-and-boot-themx86-legacy-bios-ia32-uefi-x86_64-uefi-arm64-uefi-and-mips64el-uefi-are-supported-in-the-same-wayboth-mbr-and-gpt-partition-style-are-supported-in-the-same-waymost-type-of-os-supportedwindowswinpelinuxunixchromeosvmwarexen---1200-iso-files-are-tested-list-90-distros-in-distrowatchcom-supported-details-official-website-httpswwwventoynet"></a></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tested OS</h2><a id="user-content-tested-os" aria-label="Permalink: Tested OS" href="#tested-os"></a></p>
<p dir="auto"><strong>Windows</strong><br>
Windows 7, Windows 8, Windows 8.1, Windows 10, Windows 11, Windows Server 2012, Windows Server 2012 R2, Windows Server 2016, Windows Server 2019, Windows Server 2022, Windows Server 2025, WinPE</p>
<p dir="auto"><strong>Linux</strong><br>
Debian, Ubuntu, CentOS(6/7/8/9), RHEL(6/7/8/9), Deepin, Fedora, Rocky Linux, AlmaLinux, EuroLinux(6/7/8/9), openEuler, OpenAnolis, SLES, openSUSE, MX Linux, Manjaro, Linux Mint, Endless OS, Elementary OS, Solus, Linx, Zorin, antiX, PClinuxOS, Arch, ArcoLinux, ArchLabs, BlackArch, Obarun, Artix Linux, Puppy Linux, Tails, Slax, Kali, Mageia, Slackware, Q4OS, Archman, Gentoo, Pentoo, NixOS, Kylin, openKylin, Ubuntu Kylin, KylinSec, Lubuntu, Xubuntu, Kubuntu, Ubuntu MATE, Ubuntu Budgie, Ubuntu Studio, Bluestar, OpenMandriva, ExTiX, Netrunner, ALT Linux, Nitrux, Peppermint, KDE neon, Linux Lite, Parrot OS, Qubes, Pop OS, ROSA, Void Linux, Star Linux, EndeavourOS, MakuluLinux, Voyager, Feren, ArchBang, LXLE, Knoppix, Calculate Linux, Clear Linux, Pure OS, Oracle Linux, Trident, Septor, Porteus, Devuan, GoboLinux, 4MLinux, Simplicity Linux, Zeroshell, Android-x86, netboot.xyz, Slitaz, SuperGrub2Disk, Proxmox VE, Kaspersky Rescue, SystemRescueCD, MemTest86, MemTest86+, MiniTool Partition Wizard, Parted Magic, veket, Sabayon, Scientific, alpine, ClearOS, CloneZilla, Berry Linux, Trisquel, Ataraxia Linux, Minimal Linux Live, BackBox Linux, Emmabuntüs, ESET SysRescue Live,Nova Linux, AV Linux, RoboLinux, NuTyX, IPFire, SELKS, ZStack, Enso Linux, Security Onion, Network Security Toolkit, Absolute Linux, TinyCore, Springdale Linux, Frost Linux, Shark Linux, LinuxFX, Snail Linux, Astra Linux, Namib Linux, Resilient Linux, Virage Linux, Blackweb Security OS, R-DriveImage, O-O.DiskImage, Macrium, ToOpPy LINUX, GNU Guix, YunoHost, foxclone, siduction, Adelie Linux, Elive, Pardus, CDlinux, AcademiX, Austrumi, Zenwalk, Anarchy, DuZeru, BigLinux, OpenMediaVault, Ubuntu DP, Exe GNU/Linux, 3CX Phone System, KANOTIX, Grml, Karoshi, PrimTux, ArchStrike, CAELinux, Cucumber, Fatdog, ForLEx, Hanthana, Kwort, MiniNo, Redcore, Runtu, Asianux, Clu Linux Live, Uruk, OB2D, BlueOnyx, Finnix, HamoniKR, Parabola, LinHES, LinuxConsole, BEE free, Untangle, Pearl, Thinstation, TurnKey, tuxtrans, Neptune, HefftorLinux, GeckoLinux, Mabox Linux, Zentyal, Maui, Reborn OS, SereneLinux , SkyWave Linux, Kaisen Linux, Regata OS, TROM-Jaro, DRBL Linux, Chalet OS, Chapeau, Desa OS, BlankOn, OpenMamba, Frugalware, Kibojoe Linux, Revenge OS, Tsurugi Linux, Drauger OS, Hash Linux, gNewSense, Ikki Boot, SteamOS, Hyperbola, VyOS, EasyNAS, SuperGamer, Live Raizo, Swift Linux, RebeccaBlackOS, Daphile, CRUX, Univention, Ufficio Zero, Rescuezilla, Phoenix OS, Garuda Linux, Mll, NethServer, OSGeoLive, Easy OS, Volumio, FreedomBox, paldo, UBOS, Recalbox, batocera, Lakka, LibreELEC, Pardus Topluluk, Pinguy, KolibriOS, Elastix, Arya, Omoikane, Omarine, Endian Firewall, Hamara, Rocks Cluster, MorpheusArch, Redo, Slackel, SME Server, APODIO, Smoothwall, Dragora, Linspire, Secure-K OS, Peach OSI, Photon, Plamo, SuperX, Bicom, Ploplinux, HP SPP, LliureX, Freespire, DietPi, BOSS, Webconverger, Lunar, TENS, Source Mage, RancherOS, T2, Vine, Pisi, blackPanther, mAid, Acronis, Active.Boot, AOMEI, Boot.Repair, CAINE, DaRT, EasyUEFI, R-Drive, PrimeOS, Avira Rescue System, bitdefender, Checkra1n Linux, Lenovo Diagnostics, Clover, Bliss-OS, Lenovo BIOS Update, Arcabit Rescue Disk, MiyoLinux, TeLOS, Kerio Control, RED OS, OpenWrt, MocaccinoOS, EasyStartup, Pyabr, Refracta, Eset SysRescue, Linpack Xtreme, Archcraft, NHVBOOT, pearOS, SeaTools, Easy Recovery Essentional, iKuai, StorageCraft SCRE, ZFSBootMenu, TROMjaro, BunsenLabs, Todo en Uno, ChallengerOS, Nobara, Holo, CachyOS, Peux OS, Vanilla OS, ShredOS, paladin, Palen1x, dban, ReviOS, HelenOS, XeroLinux, Tiny 11, chimera linux, CuteFish, DragonOs, Rhino Linux, vanilladpup, crystal, IGELOS, MiniOS, gnoppix, PikaOS, UwUntu, Noble, PocketHandyBox, DiskGenius, ......</p>
<p dir="auto"><strong>Unix</strong><br>
DragonFly, FreeBSD, pfSense, OPNsense, GhostBSD, FreeNAS, TrueNAS, XigmaNAS, FuryBSD, HardenedBSD, MidnightBSD, ClonOS, EmergencyBootKit, helloSystem</p>
<p dir="auto"><strong>ChromeOS</strong><br>
FydeOS, CloudReady, ChromeOS Flex</p>
<p dir="auto"><strong>Other</strong><br>
VMware ESXi, Citrix XenServer, Xen XCP-ng</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tested Image Report</h2><a id="user-content-tested-image-report" aria-label="Permalink: Tested Image Report" href="#tested-image-report"></a></p>
<p dir="auto"><a href="https://github.com/ventoy/Ventoy/issues/1195" data-hovercard-type="issue" data-hovercard-url="/ventoy/Ventoy/issues/1195/hovercard">【How to report a successfully tested image file】</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Ventoy Browser</h2><a id="user-content-ventoy-browser" aria-label="Permalink: Ventoy Browser" href="#ventoy-browser"></a></p>
<p dir="auto">With Ventoy, you can also browse ISO/WIM/IMG/VHD(x)/EFI files in local disk and boot them. <a href="https://www.ventoy.net/en/doc_browser.html" rel="nofollow">Notes</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">VentoyPlugson</h2><a id="user-content-ventoyplugson" aria-label="Permalink: VentoyPlugson" href="#ventoyplugson"></a></p>
<p dir="auto">A GUI Ventoy plugin configurator. <a href="https://www.ventoy.net/en/plugin_plugson.html" rel="nofollow">VentoyPlugson</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>100% open source</li>
<li>Simple to use</li>
<li>Fast (limited only by the speed of copying iso file)</li>
<li>Can be installed in USB/Local Disk/SSD/NVMe/SD Card</li>
<li>Directly boot from ISO/WIM/IMG/VHD(x)/EFI files, no extraction needed</li>
<li>Support to browse and boot ISO/WIM/IMG/VHD(x)/EFI files in local disk</li>
<li>No need to be continuous in disk for ISO/WIM/IMG/VHD(x)/EFI files</li>
<li>MBR and GPT partition style supported (1.0.15+)</li>
<li>x86 Legacy BIOS, IA32 UEFI, x86_64 UEFI, ARM64 UEFI, MIPS64EL UEFI supported</li>
<li>IA32/x86_64 UEFI Secure Boot supported (1.0.07+)</li>
<li>Linux Persistence supported (1.0.11+)</li>
<li>Windows auto installation supported (1.0.09+)</li>
<li>Linux auto installation supported (1.0.09+)</li>
<li>Variables Expansion supported for Windows/Linux auto installation script</li>
<li>FAT32/exFAT/NTFS/UDF/XFS/Ext2(3)(4) supported for main partition</li>
<li>ISO files larger than 4GB supported</li>
<li>Menu alias, Menu tip message supported</li>
<li>Password protect supported</li>
<li>Native boot menu style for Legacy &amp; UEFI</li>
<li>Most types of OS supported, 1200+ iso files tested</li>
<li>Linux vDisk boot supported</li>
<li>Not only boot but also complete installation process</li>
<li>Menu dynamically switchable between List/TreeView mode</li>
<li>"Ventoy Compatible" concept</li>
<li>Plugin Framework and GUI plugin configurator</li>
<li>Injection files to runtime environment</li>
<li>Boot configuration file dynamically replacement</li>
<li>Highly customizable theme and menu</li>
<li>USB drive write-protected support</li>
<li>USB normal use unaffected</li>
<li>Data nondestructive during version upgrade</li>
<li>No need to update Ventoy when a new distro is released</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f3319dcbda5d19f870d113c7c6931702bcf6b7c165b9f3e178655869d7e9175d/68747470733a2f2f7777772e76656e746f792e6e65742f7374617469632f696d672f73637265656e2f73637265656e5f756566692e706e67"><img src="https://camo.githubusercontent.com/f3319dcbda5d19f870d113c7c6931702bcf6b7c165b9f3e178655869d7e9175d/68747470733a2f2f7777772e76656e746f792e6e65742f7374617469632f696d672f73637265656e2f73637265656e5f756566692e706e67" alt="avatar" data-canonical-src="https://www.ventoy.net/static/img/screen/screen_uefi.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation Instructions</h2><a id="user-content-installation-instructions" aria-label="Permalink: Installation Instructions" href="#installation-instructions"></a></p>
<p dir="auto">See <a href="https://www.ventoy.net/en/doc_start.html" rel="nofollow">https://www.ventoy.net/en/doc_start.html</a> for detailed instructions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Compile Instructions</h2><a id="user-content-compile-instructions" aria-label="Permalink: Compile Instructions" href="#compile-instructions"></a></p>
<p dir="auto">Please refer to <a href="https://github.com/ventoy/Ventoy/blob/master/DOC/BuildVentoyFromSource.txt">BuildVentoyFromSource.txt</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Document</h2><a id="user-content-document" aria-label="Permalink: Document" href="#document"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Title</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Install &amp; Update</strong></td>
<td><a href="https://www.ventoy.net/en/doc_start.html" rel="nofollow">https://www.ventoy.net/en/doc_start.html</a></td>
</tr>
<tr>
<td><strong>Browse/Boot Files In Local Disk</strong></td>
<td><a href="https://www.ventoy.net/en/doc_browser.html" rel="nofollow">https://www.ventoy.net/en/doc_browser.html</a></td>
</tr>
<tr>
<td><strong>Secure Boot</strong></td>
<td><a href="https://www.ventoy.net/en/doc_secure.html" rel="nofollow">https://www.ventoy.net/en/doc_secure.html</a></td>
</tr>
<tr>
<td><strong>Customize Theme</strong></td>
<td><a href="https://www.ventoy.net/en/plugin_theme.html" rel="nofollow">https://www.ventoy.net/en/plugin_theme.html</a></td>
</tr>
<tr>
<td><strong>Global Control</strong></td>
<td><a href="https://www.ventoy.net/en/plugin_control.html" rel="nofollow">https://www.ventoy.net/en/plugin_control.html</a></td>
</tr>
<tr>
<td><strong>Image List</strong></td>
<td><a href="https://www.ventoy.net/en/plugin_imagelist.html" rel="nofollow">https://www.ventoy.net/en/plugin_imagelist.html</a></td>
</tr>
<tr>
<td><strong>Auto Installation</strong></td>
<td><a href="https://www.ventoy.net/en/plugin_autoinstall.html" rel="nofollow">https://www.ventoy.net/en/plugin_autoinstall.html</a></td>
</tr>
<tr>
<td><strong>Injection Plugin</strong></td>
<td><a href="https://www.ventoy.net/en/plugin_injection.html" rel="nofollow">https://www.ventoy.net/en/plugin_injection.html</a></td>
</tr>
<tr>
<td><strong>Persistence Support</strong></td>
<td><a href="https://www.ventoy.net/en/plugin_persistence.html" rel="nofollow">https://www.ventoy.net/en/plugin_persistence.html</a></td>
</tr>
<tr>
<td><strong>Boot WIM file</strong></td>
<td><a href="https://www.ventoy.net/en/plugin_wimboot.html" rel="nofollow">https://www.ventoy.net/en/plugin_wimboot.html</a></td>
</tr>
<tr>
<td><strong>Windows VHD Boot</strong></td>
<td><a href="https://www.ventoy.net/en/plugin_vhdboot.html" rel="nofollow">https://www.ventoy.net/en/plugin_vhdboot.html</a></td>
</tr>
<tr>
<td><strong>Linux vDisk Boot</strong></td>
<td><a href="https://www.ventoy.net/en/plugin_vtoyboot.html" rel="nofollow">https://www.ventoy.net/en/plugin_vtoyboot.html</a></td>
</tr>
<tr>
<td><strong>DUD Plugin</strong></td>
<td><a href="https://www.ventoy.net/en/plugin_dud.html" rel="nofollow">https://www.ventoy.net/en/plugin_dud.html</a></td>
</tr>
<tr>
<td><strong>Password Plugin</strong></td>
<td><a href="https://www.ventoy.net/en/plugin_password.html" rel="nofollow">https://www.ventoy.net/en/plugin_password.html</a></td>
</tr>
<tr>
<td><strong>Conf Replace Plugin</strong></td>
<td><a href="https://www.ventoy.net/en/plugin_bootconf_replace.html" rel="nofollow">https://www.ventoy.net/en/plugin_bootconf_replace.html</a></td>
</tr>
<tr>
<td><strong>Menu Class</strong></td>
<td><a href="https://www.ventoy.net/en/plugin_menuclass.html" rel="nofollow">https://www.ventoy.net/en/plugin_menuclass.html</a></td>
</tr>
<tr>
<td><strong>Menu Alias</strong></td>
<td><a href="https://www.ventoy.net/en/plugin_menualias.html" rel="nofollow">https://www.ventoy.net/en/plugin_menualias.html</a></td>
</tr>
<tr>
<td><strong>Menu Extension</strong></td>
<td><a href="https://www.ventoy.net/en/plugin_grubmenu.html" rel="nofollow">https://www.ventoy.net/en/plugin_grubmenu.html</a></td>
</tr>
<tr>
<td><strong>Memdisk Mode</strong></td>
<td><a href="https://www.ventoy.net/en/doc_memdisk.html" rel="nofollow">https://www.ventoy.net/en/doc_memdisk.html</a></td>
</tr>
<tr>
<td><strong>TreeView Mode</strong></td>
<td><a href="https://www.ventoy.net/en/doc_treeview.html" rel="nofollow">https://www.ventoy.net/en/doc_treeview.html</a></td>
</tr>
<tr>
<td><strong>Disk Layout MBR</strong></td>
<td><a href="https://www.ventoy.net/en/doc_disk_layout.html" rel="nofollow">https://www.ventoy.net/en/doc_disk_layout.html</a></td>
</tr>
<tr>
<td><strong>Disk Layout GPT</strong></td>
<td><a href="https://www.ventoy.net/en/doc_disk_layout_gpt.html" rel="nofollow">https://www.ventoy.net/en/doc_disk_layout_gpt.html</a></td>
</tr>
<tr>
<td><strong>Search Configuration</strong></td>
<td><a href="https://www.ventoy.net/en/doc_search_path.html" rel="nofollow">https://www.ventoy.net/en/doc_search_path.html</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto">See <a href="https://www.ventoy.net/en/faq.html" rel="nofollow">https://www.ventoy.net/en/faq.html</a> for detail</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Forum</h2><a id="user-content-forum" aria-label="Permalink: Forum" href="#forum"></a></p>
<p dir="auto"><a href="https://forums.ventoy.net/" rel="nofollow">https://forums.ventoy.net</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Donation</h2><a id="user-content-donation" aria-label="Permalink: Donation" href="#donation"></a></p>
<p dir="auto">It would be much appreciated if you want to make a small donation to support my work!<br>
Alipay, WeChat Pay, PayPal and Bitcoin are available for donation. You can choose any of them.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Alipay</th>
<th>WeChat Pay</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6e4429826e252d4c9a4b30bacadcd0e315b6c9a4d67195181619ee8ef3a95b1a/68747470733a2f2f7777772e76656e746f792e6e65742f7374617469632f696d672f416c695061792e706e67"><img src="https://camo.githubusercontent.com/6e4429826e252d4c9a4b30bacadcd0e315b6c9a4d67195181619ee8ef3a95b1a/68747470733a2f2f7777772e76656e746f792e6e65742f7374617469632f696d672f416c695061792e706e67" width="250" height="250" data-canonical-src="https://www.ventoy.net/static/img/AliPay.png"></a></td>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/de3b98955727978d085bafc33a2268f907c9ed25827e8cc82807cc47e2d9c2cc/68747470733a2f2f7777772e76656e746f792e6e65742f7374617469632f696d672f5765436861745061792e706e67"><img src="https://camo.githubusercontent.com/de3b98955727978d085bafc33a2268f907c9ed25827e8cc82807cc47e2d9c2cc/68747470733a2f2f7777772e76656e746f792e6e65742f7374617469632f696d672f5765436861745061792e706e67" width="250" height="250" data-canonical-src="https://www.ventoy.net/static/img/WeChatPay.png"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>PayPal</strong><br>
You can transfer to my paypal account <code>admin@ventoy.net</code> or just click <a href="https://www.paypal.me/ventoy" rel="nofollow">https://www.paypal.me/ventoy</a></p>
<p dir="auto"><strong>Bitcoin</strong><br>
Bitcoin Address <code>19mZDWzZgzkHCi9YX9H3fYCUuCHq3W6wfT</code></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US declines to join more than 70 countries in signing UN cybercrime treaty (295 pts)]]></title>
            <link>https://therecord.media/us-declines-signing-cybercrime-treaty?</link>
            <guid>45760328</guid>
            <pubDate>Thu, 30 Oct 2025 14:22:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://therecord.media/us-declines-signing-cybercrime-treaty?">https://therecord.media/us-declines-signing-cybercrime-treaty?</a>, See on <a href="https://news.ycombinator.com/item?id=45760328">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span><p> More than 70 countries signed the landmark <a href="https://www.unodc.org/unodc/en/cybercrime/convention/home.html" target="_blank" rel="noopener noreferrer">U.N. Convention against Cybercrime</a> in Hanoi this weekend, a significant step in the yearslong effort to create a global mechanism to counteract digital crime. </p><p> The U.K. and European Union joined China, Russia, Brazil, Nigeria and dozens of other nations in signing the convention, which lays out new mechanisms for governments to coordinate, build capacity and track those who use technology to commit crimes.&nbsp; </p><p> In his speech at the event, U.N. Secretary-General António Guterres said cyberspace “has become fertile ground for criminals” and has allowed them to “defraud families, steal livelihoods, and drain billions of dollars from our economies.” </p><p> “The UN Cybercrime Convention is a powerful, legally binding instrument to strengthen our collective defences against cybercrime,” Guterres <a href="https://webtv.un.org/en/asset/k1m/k1mmr6e30l?fbclid=IwY2xjawNsbKBleHRuA2FlbQIxMABicmlkETFEaEJoeEdZVm5jOHFrbUZwAR6naDVwPBo0PIG40vUDzWufVp-JMZnPlhXObSoo82Td1voIWEEcVPyLc4VCqw_aem_BZcBFnBsiMKuxTexgtKUBA" target="_blank" rel="noopener noreferrer">said</a>.&nbsp; </p><p> “Illicit flows of money, concealed through cryptocurrencies and digital transactions, finance the trafficking of drugs, arms, and terror. And businesses, hospitals, and airports are brought to a standstill by ransomware attacks.” </p><p> He added that the convention would be critical for governments in the Global South that need assistance and funding for the training required to address cybercrime — which the U.N. estimates costs $10.5 trillion around the world annually.&nbsp; </p><p> While many countries did not sign the treaty, the most notable missing signature was that of the U.S. </p><p> Officials at the State Department <a href="https://therecord.media/cybercrime-treaty-signing-hanoi" target="_blank" rel="noopener noreferrer">told Recorded Future News on Friday</a> that Marc Knapper, the U.S. ambassador to Vietnam, and representatives from the U.S. Mission to Vietnam would be attending the signing.&nbsp; </p><p> The State Department confirmed on Monday that the U.S. did not sign the treaty.&nbsp; </p><p> “The United States continues to review the treaty,” a State Department spokesperson said in a brief statement.&nbsp; </p><p> The U.N. Convention against Cybercrime was adopted by the General Assembly <a href="https://therecord.media/un-general-assembly-approves-cybercrime-treaty-despite-industry-pushback" target="_blank" rel="noopener noreferrer">in December 2024</a> and will enter into force 90 days after being ratified by the 40th signatory. Signatories will have to ratify the convention according to their own procedures.&nbsp; </p><p> At the ceremony, UNODC Executive Director Ghada Waly argued that cybercrime is changing the face of organized crime and required global coordination to address. Waly said the convention would be a “vital tool” that will ensure “a safer digital world for all.” </p><p> U.N. officials said the convention would help governments address terrorism, human trafficking, money laundering and drug smuggling, all of which have been turbo-charged by the internet.&nbsp; </p><p> The U.N. noted that the convention is the first global framework “for the collection, sharing and use of electronic evidence for all serious offenses” — noting that until now there have been no broadly accepted international standards on electronic evidence.&nbsp; </p><p> It is also the first global treaty to criminalize crimes that depend on the internet and is the first international treaty “to recognize the non-consensual dissemination of intimate images as an offense.” </p><p> “It creates the first global 24/7 network where countries can quickly initiate cooperation,” the U.N. said. “It recognizes and promotes the need to build capacity in countries to pursue and cooperate on fast-moving cybercrimes.” </p><p> The convention has been heavily criticized by the tech industry, which has warned that it criminalizes cybersecurity research and exposes companies to legally thorny data requests. </p><p> Human rights groups warned on Friday that it effectively forces member states to create a broad electronic surveillance dragnet that would include crimes that have nothing to do with technology.&nbsp; </p><p> Many expressed concern that the convention will be abused by dictatorships and rogue governments who will deploy it against critics or protesters — even those outside of a regime’s jurisdiction.&nbsp; </p><p> It also creates legal regimes to monitor, store and allow cross-border sharing of information without specific data protections. Access Now’s Raman Jit Singh Chima said the convention effectively justifies “cyber authoritarianism at home and transnational repression across borders.”&nbsp; </p><p> Any countries ratifying the treaty, he added, risks “actively validating cyber authoritarianism and facilitating the global erosion of digital freedoms, choosing procedural consensus over substantive human rights protection.” </p><p> In his speech, Guterres referenced the backlash to the convention, telling member states that the treaty has to be a “promise that fundamental human rights such as privacy, dignity, and safety must be protected both offline and online.”&nbsp; </p><p> But at its core, according to Guterres, the convention solves one of the thorniest issues law enforcement agencies have faced over the last two decades. Countries have only recently begun to share digital evidence across borders but the convention would increase that practice.&nbsp; </p><p> “This has long been a major obstacle to justice — with perpetrators in one country, victims in another, and data stored in a third,” he said. “The Convention provides a clear pathway for investigators and prosecutors to finally overcome this barrier.” </p></span></p><div><div><p>Get more insights with the </p><p>Recorded Future</p><p>Intelligence Cloud.</p></div><p><a target="_blank" rel="noopener noreferrer" href="https://www.recordedfuture.com/platform?mtm_campaign=ad-unit-record">Learn more.</a></p></div><div><a href="https://www.recordedfuture.com/?utm_source=therecord&amp;utm_medium=ad" target="_blank" rel="noopener"><figure><img alt="Recorded Future" fetchpriority="high" width="1000" height="500" decoding="async" data-nimg="1" src="https://cms.therecord.media/uploads/format_webp/2025_0514_Record_Ads_970x250_1_d144dbf901.png"></figure></a></div><div><p><span>Tags</span></p><ul><li><a href="https://therecord.media/tag/united-nations">United Nations</a></li><li><a href="https://therecord.media/tag/un-cybercrime-treaty">UN Cybercrime Treaty</a></li><li><a href="https://therecord.media/tag/cybercrime">Cybercrime</a></li></ul></div><div><p><span>No previous article</span></p><p><span>No new articles</span></p></div><div><p><a href="https://therecord.media/author/jonathan-greig"><img alt="Jonathan Greig" fetchpriority="auto" loading="lazy" width="384" height="384" decoding="async" data-nimg="1" src="https://cms.therecord.media/uploads/format_webp/DSC_0283_1_a6f4e4e315.jpg"></a></p><div><figcaption><a href="https://therecord.media/author/jonathan-greig"><p>Jonathan Greig</p></a><p><span><p>is a Breaking News Reporter at Recorded Future News. Jonathan has worked across the globe as a journalist since 2014. Before moving back to New York City, he worked for news outlets in South Africa, Jordan and Cambodia. He previously covered cybersecurity at ZDNet and TechRepublic.</p></span></p></figcaption></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a heatmap diff viewer for code reviews (154 pts)]]></title>
            <link>https://0github.com</link>
            <guid>45760321</guid>
            <pubDate>Thu, 30 Oct 2025 14:21:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://0github.com">https://0github.com</a>, See on <a href="https://news.ycombinator.com/item?id=45760321">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Heatmap color-codes every diff line/token by how much<!-- --> <span>human attention</span> it probably needs. Unlike PR-review bots, we try to flag not just by “is it a bug?” but by “is it worth a second look?” (examples:<!-- --> <span>hard-coded secret</span>,<!-- --> <span>weird crypto mode</span>,<!-- --> <span>gnarly logic</span>).</p><p>To try it, replace github.com with<!-- --> <span>0github.com</span> in any GitHub pull request url. Under the hood, we clone the repo into a VM, spin up <span>gpt-5-codex</span> <!-- -->for every diff, and ask it to output a JSON data structure that we parse into a<!-- --> <span>colored heatmap</span>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The International Criminal Court wants to become independent of USA technology (131 pts)]]></title>
            <link>https://www.heise.de/en/news/International-Criminal-Court-Kicks-Out-Microsoft-10964189.html</link>
            <guid>45759891</guid>
            <pubDate>Thu, 30 Oct 2025 13:35:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.heise.de/en/news/International-Criminal-Court-Kicks-Out-Microsoft-10964189.html">https://www.heise.de/en/news/International-Criminal-Court-Kicks-Out-Microsoft-10964189.html</a>, See on <a href="https://news.ycombinator.com/item?id=45759891">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        

        <p>The International Criminal Court (ICC) wants to become independent of technology from the USA – for fear of reprisals from Donald Trump, Handelsblatt has learned. The institution in The Hague wants to replace the Microsoft software currently used on workstations with OpenDesk.</p>
<!-- RSPEAK_STOP -->




  


<!-- RSPEAK_START -->

<p>According to <a href="https://www.handelsblatt.com/technik/it-internet/software-strafgerichtshof-ersetzt-microsoft-durch-deutsche-loesung/100166382.html" rel="external noopener" target="_blank">Handelsblatt</a>, the decision is to be seen against the backdrop of sanctions by the current US administration under President Donald Trump against employees such as Chief Prosecutor Karim Khan. <a href="https://www.heise.de/news/Strafgerichtshof-Microsofts-E-Mail-Sperre-als-Weckruf-fuer-digitale-Souveraenitaet-10387368.html?from-en=1">Microsoft simply blocked his email access</a>. He therefore had to switch to the Swiss email service Proton. Since the ICC is highly dependent on service providers like Microsoft, its work is being paralyzed, it was stated in May.</p>
<p>Furthermore, the US government in Washington is examining further measures against the International Criminal Court, Handelsblatt further reports. This could also significantly restrict the institution's ability to work.</p>
<h3 id="nav_achieving__0">Achieving Digital Sovereignty</h3>
<p>The OpenDesk software is developed by the Center for Digital Sovereignty (Zendis), a federal company. Its task is to help resolve critical dependencies on individual technology providers.</p>
<!-- RSPEAK_STOP -->

  




<!-- RSPEAK_START -->

<p>At the International Criminal Court, it's “only” about 1800 workstations that are to be freed from US dependency. However, Handelsblatt sees this as an indication that geopolitics is increasingly revolving around technology. Businesses and politicians recognize the dependence on US digital corporations as a problem, especially regarding the fact that the USA uses technology as a means of pressure.</p>
<p>The ICC is not alone in these ambitions. For example, the <a href="https://www.heise.de/news/Souveraenitaet-Oeffentlicher-Gesundheitsdienst-setzt-verstaerkt-auf-Open-Source-10443399.html?from-en=1">Public Health Service wants to use OpenDesk</a>, and the <a href="https://www.heise.de/news/Rahmenvertrag-MS-365-Alternative-OpenDesk-soll-die-Bundeswehr-erobern-10342327.html?from-en=1">German Armed Forces has a framework agreement with Zendis</a> has concluded a framework agreement on “sovereign communication and collaboration solutions” such as OpenDesk.</p>
<!-- RSPEAK_STOP -->


  



  




<!-- RSPEAK_START -->



<!-- RSPEAK_STOP -->

<!-- RSPEAK_START -->
<p>

<!-- RSPEAK_STOP -->
<span>(<a href="mailto:dmk@heise.de" title="Dirk Knop">dmk</a>)</span>
<!-- RSPEAK_START -->
</p>
<div>
    <p>
      Don't miss any news – follow us on
      <a href="https://www.facebook.com/heiseonlineEnglish">Facebook</a>,
      <a href="https://www.linkedin.com/company/104691972">LinkedIn</a> or
      <a href="https://social.heise.de/@heiseonlineenglish">Mastodon</a>.
    </p>
    <p>
      <em>This article was originally published in
      
        <a href="https://www.heise.de/news/Internationaler-Strafgerichtshof-wirft-Microsoft-raus-10964080.html">German</a>.
      
      It was translated with technical assistance and editorially reviewed before publication.</em>
    </p>
  </div>



        

        
        <!-- RSPEAK_STOP -->
        

<a-gift has-access="">
    
</a-gift>


        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jujutsu at Google [video] (113 pts)]]></title>
            <link>https://www.youtube.com/watch?v=v9Ob5yPpC0A</link>
            <guid>45759572</guid>
            <pubDate>Thu, 30 Oct 2025 13:00:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=v9Ob5yPpC0A">https://www.youtube.com/watch?v=v9Ob5yPpC0A</a>, See on <a href="https://news.ycombinator.com/item?id=45759572">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Alphabet tops $100B quarterly revenue for first time, cloud grows 34% (128 pts)]]></title>
            <link>https://www.cnbc.com/2025/10/29/alphabet-google-q3-earnings.html</link>
            <guid>45758874</guid>
            <pubDate>Thu, 30 Oct 2025 11:33:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/10/29/alphabet-google-q3-earnings.html">https://www.cnbc.com/2025/10/29/alphabet-google-q3-earnings.html</a>, See on <a href="https://news.ycombinator.com/item?id=45758874">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="Placeholder-ArticleBody-Video-108218960" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000393928" aria-labelledby="Placeholder-ArticleBody-Video-108218960"><p><img src="https://image.cnbcfm.com/api/v1/image/108218961-17617690881761769086-42326895190-1080pnbcnews.jpg?v=1761769087&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Alphabet tops $100 billion quarterly revenue for first time"><span></span><span></span></p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/GOOG/">Alphabet</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> reported third-quarter <a href="https://s206.q4cdn.com/479360582/files/doc_financials/2025/q3/2025q3-alphabet-earnings-release.pdf" target="_blank">earnings</a> that beat analyst expectations. Shares rose 5% in after-hours trading.</p><p>Here's how the company did, compared with estimates from analysts polled by LSEG:</p><ul><li><strong>Revenue:</strong>&nbsp;$102.35 billion vs. $99.89 billion estimated</li><li><strong>Earnings per share:</strong>&nbsp;$3.10 adj. vs $2.33 estimated</li></ul><p>Wall Street was also watching several other numbers in the report:</p><ul><li><strong>YouTube advertising revenue</strong>:&nbsp;$10.26 billion vs. $10.01 billion, according to StreetAccount</li><li><strong>Google Cloud revenue:&nbsp;</strong> $15.15 billion vs. $14.74 billion, according to StreetAccount</li><li><strong>Traffic acquisition costs (TAC)</strong>:&nbsp;$14.87 billion vs. $14.82 billion, according to StreetAccount</li></ul><p>Alphabet reported solid momentum in its cloud business, thanks to strong demand for <a href="https://www.cnbc.com/ai-effect/">artificial intelligence</a>. The company also announced an increase in expected capital expenditures for the fiscal year 2025.</p><p>"With the growth across our business and demand from Cloud customers, we now expect 2025 capital expenditures to be in a range of $91 billion to $93 billion," the company said in its earnings <a href="https://s206.q4cdn.com/479360582/files/doc_financials/2025/q3/2025q3-alphabet-earnings-release.pdf" target="_blank">report</a> Wednesday. </p><p>"Looking out to 2026, we expect a significant increase in CapEx and will provide more detail on our fourth quarter earnings call," said finance chief Anat Ashkenazi on the earnings call with investors Wednesday.</p><p>Earlier this year, the company increased its capital expenditure expectation from $75 billion to $85 billion. Most of that goes toward technical infrastructure such as data centers.</p><p>The latest earnings show the company is seeing rising demand for its AI services, which largely sit in its cloud unit. It also shows the company is continuing to spend more to try and build out more infrastructure to accomodate the backlog of customer requests.</p><p>"We continue to drive strong growth in new businesses. Google Cloud accelerated, ending the quarter with $155 billion in backlog," CEO Sundar Pichai said in the earnings release.</p><p>The backlog comes from demand for enterprise AI infrastructure, including chips and demand for Gemini 2.5, said Ashkenazi.</p></div><div id="RegularArticle-RelatedContent-1"><h2>Read more CNBC tech news</h2><div><ul><li><a href="https://www.cnbc.com/2025/10/29/apples-iphone-air-sales-in-focus-at-q4-earnings.html">Apple's iPhone Air doesn't look like a best-seller. It might not matter</a></li><li><a href="https://www.cnbc.com/2025/10/29/amazon-opens-11-billion-ai-data-center-project-rainier-in-indiana.html">Amazon opens $11 billion AI data center in rural Indiana as rivals race to break ground</a></li><li><a href="https://www.cnbc.com/2025/10/29/nvidia-ceo-jensen-huang-south-korea-trip-what-to-expect.html">Nvidia CEO Jensen Huang starts a key trip to South Korea — here's what he might be up to</a></li><li><a href="https://www.cnbc.com/2025/10/29/sk-hynix-q3-profit-revenue-record-.html">SK Hynix, a critical Nvidia supplier, has already sold out chips for 2026 as AI demand booms</a></li></ul></div></div><div><p>The company reported cloud revenue of $15.15 billion, a 35% increase from the same period last year.</p><p>"We have signed more deals over one billion dollars through Q3 this year than we did in the previous two years combined," said Pichai, referring to the first nine months of the year. In August,&nbsp;Google won a $10 billion <a href="https://www.cnbc.com/2025/08/21/google-scores-six-year-meta-cloud-deal-worth-over-10-billion.html">cloud contract</a> from Meta spanning six years.&nbsp;</p><p>Alphabet, which reported 32% <a href="https://www.cnbc.com/2025/07/23/alphabet-google-q2-earnings.html">cloud revenue</a> growth last quarter, is keeping pace with its megacap competitors. <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-7"><a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> posted 40% revenue growth in its <a href="https://www.cnbc.com/2025/10/29/microsoft-msft-q1-2026-earnings-report.html">Azure cloud</a> business as it reported earnings on Wednesday.</p><p>Over 70% of existing Google Cloud customers use its AI products, Pichai said. That's a result of the company's strategy to <a href="https://www.cnbc.com/2025/09/09/google-cloud-chief-details-how-tech-company-is-monetizing-ai.html">upsell</a> existing customers.</p><p>Google's flagship AI app Gemini now has more than 650 million monthly active users, the company said in its Wednesday report. That's up from the 450 million active users Pichai said it had last quarter.&nbsp;</p><p>OpenAI CEO Sam Altman <a href="https://techcrunch.com/2025/10/06/sam-altman-says-chatgpt-has-hit-800m-weekly-active-users/" target="_blank">said</a> earlier this month that&nbsp;ChatGPT now has 800 million users per week.</p><p>Google's search business generated $56.56 billion in revenue — up&nbsp;15% from the prior year.</p><p>Alphabet's net income increased to $34.97 billion, or $2.87 per share, compared to $26.3 billion, or $2.12 per share, in the year-ago quarter. In September, Google was <a href="https://www.cnbc.com/2025/09/05/google-slapped-by-eu-with-3point45-billion-antitrust-fine.html">slapped</a> with a $3.45 billion antitrust fine from European Union regulators for anti-competitive practices in its lucrative advertising technology business.&nbsp;That fine impacted the reported net income.</p><p>YouTube advertising revenue came in at $10.26 billion, higher than Wall Street expected. Alphabet reported overall advertising revenue of $74.18 billion — up from $65.85 billion last year.</p><p>Other Bets, which includes the company's life sciences unit Verily and self-driving car unit Waymo, reported revenue of $344 million during the quarter. That's lower than the $388 million from the same quarter last year.&nbsp;</p><p>Alphabet reported a loss of $1.42 billion on other bets, compared to a loss of $1.12 billion the year before.</p><p>The Google parent's stock is up 45% so far this year.</p></div><div><div role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="256" height="256" viewBox="0 0 256 256" aria-labelledby="title desc" role="img" focusable="false" preserveAspectRatio="xMinYMin"><title>Stock Chart Icon</title><desc>Stock chart icon</desc><g transform="translate(1.4065934065934016 1.4065934065934016) scale(2.81 2.81)"><path d="M 87.994 0 H 69.342 c -1.787 0 -2.682 2.16 -1.418 3.424 l 5.795 5.795 l -33.82 33.82 L 28.056 31.196 l -3.174 -3.174 c -1.074 -1.074 -2.815 -1.074 -3.889 0 L 0.805 48.209 c -1.074 1.074 -1.074 2.815 0 3.889 l 3.174 3.174 c 1.074 1.074 2.815 1.074 3.889 0 l 15.069 -15.069 l 14.994 14.994 c 1.074 1.074 2.815 1.074 3.889 0 l 1.614 -1.614 c 0.083 -0.066 0.17 -0.125 0.247 -0.202 l 37.1 -37.1 l 5.795 5.795 C 87.84 23.34 90 22.445 90 20.658 V 2.006 C 90 0.898 89.102 0 87.994 0 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 65.626 37.8 v 49.45 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 23.518 L 65.626 37.8 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 47.115 56.312 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 42.03 L 47.115 56.312 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 39.876 60.503 c -1.937 0 -3.757 -0.754 -5.127 -2.124 l -6.146 -6.145 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 59.844 C 41.952 60.271 40.933 60.503 39.876 60.503 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 22.937 46.567 L 11.051 58.453 c -0.298 0.298 -0.621 0.562 -0.959 0.8 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 48.004 L 22.937 46.567 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path></g></svg><p><img src="https://static-redesign.cnbcfm.com/dist/a54b41835a8b60db28c2.svg" alt="hide content"></p></div><p>Alphabet one-day stock chart.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: In a single HTML file, an app to encourage my children to invest (181 pts)]]></title>
            <link>https://roberdam.com/en/dinversiones.html</link>
            <guid>45758421</guid>
            <pubDate>Thu, 30 Oct 2025 10:39:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://roberdam.com/en/dinversiones.html">https://roberdam.com/en/dinversiones.html</a>, See on <a href="https://news.ycombinator.com/item?id=45758421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->



<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" role="main">
    <div>

        <article>

            

            
            <figure>
            </figure>
            

            <div>
                    <p><em>“What comes with the milk, leaves with the soul”</em><br>
<small>— Russian proverb.</small></p>

<hr>

<p><strong>Access the app:</strong><br>
<a href="https://roberdam.com/public/alcancia/index.html?l=en">Click here to open and install D-i<del>n</del>vestments</a></p>

<!-- _includes/image.html -->
<div>
    
        <p><img src="https://roberdam.com/assets/images/iconodinversion.jpg" alt="This is how the icon will appear on your phone"></p><p>This is how the icon will appear on your phone</p>
    
</div>



<p>One thing that <strong>school doesn’t teach you</strong> (not even high school) is <strong>how to manage your personal finances</strong>.</p>

<p>As my eldest son’s birthday was approaching, we suggested that instead of asking for physical gifts, he ask for their equivalent in money. That way, he gathered a <strong>decent amount of capital for his first investment adventure.</strong></p>

<p>I explained to my kids that <strong>investing is like having a magic box</strong> that generates more money over time. To make it more visual and interactive, I decided to create a small app where they could <strong>see their investment grow day by day</strong>.</p>

<hr>

<h2 id="from-idea-to-app">From Idea to App</h2>

<p>My first idea was to build a <strong>physical piggy bank with a display</strong>, showing the accumulated amount. However, that mixed up the concept of <strong>saving</strong> with <strong>investing</strong>, and also required buying extra hardware.</p>

<p>So I looked for a quicker, cheaper way: <strong>revive an old smartphone</strong> and create a simple app using plain HTML.</p>

<p>The result was <strong>D-i<del>n</del>vestments</strong>, a mix between <em>Diversions</em> and <em>Investments</em>.</p>

<hr>

<h2 id="how-it-works">How It Works</h2>

<p>The app is essentially <strong>a single HTML file</strong> that installs on the phone as a <strong>PWA (Progressive Web App)</strong>.</p>

<p>The phone is <strong>attached to the fridge</strong> and works as a <strong>panel or dashboard</strong> where my kids can see <strong>their money growing each day.</strong></p>

<p>I act as their <strong>investment agent</strong>, assigning <strong>realistic interest rates</strong> — high enough to keep them motivated, but moderate enough to reflect how the real world works.</p>

<hr>

<h2 id="configuration-screen">Configuration Screen</h2>

<p>The app includes a screen where you can enter:</p>

<ul>
  <li>The kids’ names</li>
  <li>The invested amount</li>
  <li>The interest rate</li>
  <li>The start date</li>
</ul>



<!-- _includes/image.html -->
<div>
    
        <p><img src="https://roberdam.com/assets/images/pantacel1.jpg" alt="Configuration screen"></p><p>Configuration screen</p>
    
</div>



<p>With that data, the app automatically calculates and displays:</p>

<ul>
  <li><strong>Daily gain</strong></li>
  <li><strong>Weekly gain</strong></li>
  <li><strong>Monthly gain</strong></li>
  <li><strong>Total updated balance</strong></li>
</ul>

<!-- _includes/image.html -->
<div>
    
        <p><img src="https://roberdam.com/assets/images/pantacel0.jpg" alt="D-iNvestments dashboard screen"></p><p>Dashboard view installed on the fridge showing daily growth.</p>
    
</div>



<hr>

<h2 id="materials-used">Materials Used</h2>

<ul>
  <li>An <strong>old smartphone</strong></li>
  <li>A <strong>suction mount</strong> to attach it to the fridge</li>
</ul>


<!-- _includes/image.html -->
<div>
    
        <p><img src="https://roberdam.com/assets/images/celular1.jpg" alt="Affordable phone mount"></p><p>Affordable phone mount - Price on AliExpress: $0.90</p>
    
</div>



<ul>
  <li><strong>The D-iNvestments app</strong>, in HTML format</li>
</ul>

<!-- _includes/image.html -->
<div>
    
        <p><img src="https://roberdam.com/assets/images/heladera.jpg" alt="Phone installed on the fridge"></p><p>D-iNvestments showing daily capital growth.</p>
    
</div>



<hr>

<h2 id="installation">Installation</h2>

<p>The process is as simple as opening the link from a smartphone and tapping <strong>“Install”</strong> when prompted by the browser.<br>
From then on, it behaves like a native app.</p>

<blockquote>
  <p><strong>Access the app:</strong><br>
<a href="https://roberdam.com/public/alcancia/index.html?l=en">Click here to open and install D-i<del>n</del>vestments</a></p>
</blockquote>

<hr>

<h2 id="final-reflection">Final Reflection</h2>

<p>The goal wasn’t just to teach my kids the value of money, but to <strong>show them visually how investment and time work as allies.</strong></p>

<p>Each day, as they watch their small fund grow, <strong>they grasp the magic of compound interest</strong> — and that, more than any gift, is a lesson I hope will stay with them for life.</p>

<blockquote>
  <p>💬 <strong>Want to comment or improve the app? Contact me at:</strong><br>
<a href="https://x.com/roberdam">@roberdam</a></p>
</blockquote>

                </div>

            <!-- Email subscribe form at the bottom of the page -->
            

            

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            

        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->


<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->



<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Language models are injective and hence invertible (211 pts)]]></title>
            <link>https://arxiv.org/abs/2510.15511</link>
            <guid>45758093</guid>
            <pubDate>Thu, 30 Oct 2025 09:47:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2510.15511">https://arxiv.org/abs/2510.15511</a>, See on <a href="https://news.ycombinator.com/item?id=45758093">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2510.15511">View PDF</a>
    <a href="https://arxiv.org/html/2510.15511v3">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Transformer components such as non-linear activations and normalization are inherently non-injective, suggesting that different inputs could map to the same output and prevent exact recovery of the input from a model's representations. In this paper, we challenge this view. First, we prove mathematically that transformer language models mapping discrete input sequences to their corresponding sequence of continuous representations are injective and therefore lossless, a property established at initialization and preserved during training. Second, we confirm this result empirically through billions of collision tests on six state-of-the-art language models, and observe no collisions. Third, we operationalize injectivity: we introduce SipIt, the first algorithm that provably and efficiently reconstructs the exact input text from hidden activations, establishing linear-time guarantees and demonstrating exact invertibility in practice. Overall, our work establishes injectivity as a fundamental and exploitable property of language models, with direct implications for transparency, interpretability, and safe deployment.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Andrea Santilli [<a href="https://arxiv.org/show-email/76ad2a0c/2510.15511" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2510.15511v1" rel="nofollow">[v1]</a></strong>
        Fri, 17 Oct 2025 10:25:30 UTC (3,980 KB)<br>
            <strong><a href="https://arxiv.org/abs/2510.15511v2" rel="nofollow">[v2]</a></strong>
        Mon, 20 Oct 2025 07:29:02 UTC (3,980 KB)<br>
    <strong>[v3]</strong>
        Tue, 21 Oct 2025 14:44:49 UTC (3,980 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Carlo Rovelli’s radical perspective on reality (104 pts)]]></title>
            <link>https://www.quantamagazine.org/carlo-rovellis-radical-perspective-on-reality-20251029/</link>
            <guid>45756445</guid>
            <pubDate>Thu, 30 Oct 2025 04:29:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/carlo-rovellis-radical-perspective-on-reality-20251029/">https://www.quantamagazine.org/carlo-rovellis-radical-perspective-on-reality-20251029/</a>, See on <a href="https://news.ycombinator.com/item?id=45756445">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="postBody">
                <div>
        <p>
            The theoretical physicist and best-selling author finds inspiration in politics and philosophy for rethinking space and time.        </p>
        
    </div>
    <figure>
        <div>
                            <p><img width="2560" height="1440" src="https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Lede-V2-scaled.webp" alt="A man with frizzy gray hair and wire-frame glasses peers into the distance in the foreground. He is on a boat on the water with the shoreline visible behind." decoding="async" fetchpriority="high" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Lede-V2-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Lede-V2-1720x968.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Lede-V2-520x293.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Lede-V2-768x432.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Lede-V2-1536x864.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Lede-V2-2048x1152.webp 2048w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Lede-V2-98x55.webp 98w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>There is no objective reality, according to Rovelli — only perspectives. “This is very radical, because you can no longer say, ‘This is a list of things in the world, and this is how they are.’”</p>
            <p>Jan Jackle for <i data-stringify-type="italic">Quanta Magazine</i></p>
        </div>
</figcaption>
    </figure>
<div>
            <h2>Introduction</h2>
            <div data-role="selectable">
    <p>Sitting outside a Catholic church on the French Riviera, Carlo Rovelli jutted his head forward and backward, imitating a pigeon trotting by. Pigeons bob their heads, he told me, not only to stabilize their vision but also to <a href="https://journals.biologists.com/jeb/article/224/3/jeb236547/223407/Motion-parallax-via-head-movements-modulates-visuo">gauge distances</a> to objects — compensating for their limited binocular vision. “It’s all perspectival,” he said.</p>
<p>A theoretical physicist affiliated with Aix-Marseille University, Rovelli studies how we perceive reality from our limited vantage point. His research is wide-ranging, running the gamut from quantum information to black holes, and often delves into the history and philosophy of science. In the late 1980s, he helped develop a theory called loop quantum gravity that aims to describe the quantum underpinnings of space and time. A decade later, he proposed a new “relational” interpretation of quantum mechanics, which goes so far as to suggest that there is no objective reality whatsoever, only perspectives on reality —&nbsp;be they a physicist’s or a pigeon’s.</p>
<p>More recently, he’s gained recognition as a best-selling author of popular science books, including <em>Seven Brief Lessons on Physics</em>, which has sold more than 2 million copies worldwide —&nbsp;placing him in a limelight he’s still adjusting to. “I’m very bad at being somewhat famous,” he said. “I’m always getting myself in trouble.” (During my visit, he was fending off criticism from the president of the Italian Physical Society, who <a href="https://www.corriere.it/cronache/25_agosto_14/dibattito-fermi-bracco-rovelli-c0466991-a9e2-44f2-a055-ce4262dc4xlk.shtml">accused him</a> of defaming Enrico Fermi as a “bloodthirsty fascist/Nazi.”)</p>
<p>Rovelli’s own perspective on physics is heavily influenced by his rebellious, countercultural youth. A student protestor in an attempted political revolution in Bologna in 1977, Rovelli worked at a subversive left-wing radio station, drafted an illegal manifesto, and was later detained for refusing compulsory military service. Disillusioned by societal norms, “I had a sense that we were confused about how to think about reality around us,” he said. At 69, he remains politically engaged (and often enraged). “Part of me is still an old hippie.”</p>
<p>After the political unrest in Bologna petered out, Rovelli transferred his deep misgivings to the very fabric of reality. He used the same proclivity for challenging traditional ways of thinking to confront long-standing problems in the foundations of physics —&nbsp;not by rejecting established theories, but by embracing a new perspective on them. His approach centers around a radical openness to abandoning intuitions about how the world works.</p>
</div>
    </div>

<div data-role="selectable">
    <p>To confront his own biases,&nbsp;whether about physics or society,&nbsp;Rovelli turns to philosophy. He often publishes on <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/meta.12589">metaphysical topics</a> and <a href="https://arxiv.org/abs/1805.10602">advocates for more dialogue</a> between the disciplines. His newest book, published this month <a href="https://www.amazon.it/Sulleguaglianza-tutte-cose-Carlo-Rovelli/dp/8845940292">in Italian</a>, is a deep dive into the intersection of philosophy and physics, a mash-up he sees as the key to understanding what our existing theories are really telling us.<em>&nbsp;</em></p>
<p><em>Quanta</em> visited Rovelli at his home overlooking the cliffs of Cassis. Over a 12-hour conversation, held while we lounged on his patio, strolled around town, and cruised on his 100-year-old sailboat, we discussed religion, war, consciousness, media, love, pigeons&nbsp;and, of course, physics. The interview has been condensed and edited for clarity.</p>
<h3><strong>What is your central question, and how did it lead you to study quantum gravity? </strong></h3>
<p>My central question has always been: How does the world work? We have two main theories that work incredibly well for different domains: general relativity and quantum mechanics. When I learned about these theories in school, I was impressed by how radical they were. They both challenge very foundational conceptions that we have about the world around us — of space as an empty stage where objects exist, and of time as a steady linear flow. They resonated with this idea I had that if you really want to understand reality, you have to be ready to be radical.</p>
<p>All attempts to disprove quantum mechanics and general relativity have failed. But nevertheless, in this picture, there’s clearly a crack. There are phenomena out there —&nbsp;like objects falling into a black hole —&nbsp;that fall outside the domain of both theories. When you try to put the two theories together, they appear to result in all sorts of contradictions and paradoxes. To me, the interface of these two theories — the&nbsp;problem of quantum gravity — was really this deep, profound gap in our fundamental physical picture of the world.</p>
</div>
    <figure>
        <div>
                            <p><img width="2400" height="1302" src="https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Chalkboard2.webp" alt="A gray-haired man in a white shirt points at some writing on a blackboard that’s on an outside wall on a porch overlooking some mountains." decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Chalkboard2.webp 2400w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Chalkboard2-1720x933.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Chalkboard2-520x282.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Chalkboard2-768x417.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Chalkboard2-1536x833.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Chalkboard2-2048x1111.webp 2048w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Chalkboard2-98x53.webp 98w" sizes="(max-width: 2400px) 100vw, 2400px">                </p>
                        </div>
        <figcaption>
    <div>
            <p>Jan Jackle for&nbsp;<i data-stringify-type="italic">Quanta Magazine</i></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <h3><strong>Tell me about the approach you’ve taken to fill that gap: loop quantum gravity.</strong></h3>
<p>Loop quantum gravity is a very conservative approach with a very radical consequence. It’s an attempt to say: Let’s take seriously what we’ve learned from general relativity and quantum mechanics all the way through and see where they lead us. There are no extra fields, extra particles, modifications of the Einstein equations, or other hypotheses about nature. It’s just an effort to make coherent what we know so far.</p>
<p>Basically, loop quantum gravity implies that space is not infinitely divisible — it’s made of elementary chunks, which are linked together into loops. The theory is a very simple set of equations, but there’s no time variables and no space variables. Those concepts emerge from the way these quanta of gravity interact and transform. What we call space is the quantity of these loops, and what we call time is how the loops evolve continuously.<strong>&nbsp;</strong></p>
<h3><strong>How do we account for our common experience of time if it’s not fundamental?</strong></h3>
<p>Our experience of time flowing forward is a product of the second law of thermodynamics — the tendency for physical systems to increase in disorder, or what we call <a href="https://www.quantamagazine.org/what-is-entropy-a-measure-of-just-how-little-we-really-know-20241213/">entropy</a>. But this only appears fundamental from our perspective. We happen to be beings that are connected to certain macroscopic variables with respect to which entropy is globally moving in one direction.</p>
<p>My intuition is that the overall flow of time really could be like the rotation of the sky every day. It’s a majestic, immense phenomenon, but it’s actually an illusion. This is a totally perspectival understanding of the second law of thermodynamics. It’s real in the same sense that the rotating sky is real, but it’s real only with respect to us.<strong>&nbsp;</strong></p>
</div>
    <figure>
        <div>
                            <p><img width="1600" height="1516" src="https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-BigClock.webp" alt="A man is seen from behind, looking up at the face of a grandfather clock." decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-BigClock.webp 1600w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-BigClock-520x493.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-BigClock-768x728.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-BigClock-1536x1455.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-BigClock-98x93.webp 98w" sizes="(max-width: 1600px) 100vw, 1600px">                </p>
                        </div>
        <figcaption>
    <div>
            <p>Jan Jackle for&nbsp;<i data-stringify-type="italic">Quanta Magazine</i></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <h3><strong>One critique of loop quantum gravity is that it contradicts certain predictions of Einstein, namely that the speed of light is constant for all wavelengths. What do you make of this critique?</strong></h3>
<p>The theory has evolved a lot over the last 20 years, and the current version is not incompatible with Einstein’s predictions —&nbsp;the speed of light is indeed constant at all physical wavelengths. That said, there are some things about loop quantum gravity that still need resolving. We’re not sure how the different versions of the theory are equivalent to one another. We have a problem in which particle scattering seems to generate infinite amounts of low-energy radiation. And solving the equations is still a very complicated task that we’re working to simplify.</p>

<p>The main shortcoming is the lack of experiments supporting it. However, there’s hope on the horizon. There are some proposals to use loop quantum gravity to make sense of signatures in the cosmic microwave background radiation that’s left over from the Big Bang. And there’s another new idea I’m very excited about: If loop quantum gravity is right, there should exist tiny black holes weighing around 10 micrograms that are long-living and that interact only gravitationally. We’re thinking about ways to detect a background “wind” of these particles. And perhaps these tiny black holes are actually what we call dark matter, a mysterious widespread astronomical phenomenon that we have not yet understood.</p>
<p>Detection will be difficult, but it’s not out of the game. I’m hopeful there will be some experiment that will make the larger community see loop quantum gravity as the natural explanation. It’s far from clear that we cannot account for all of these phenomena using the existing theories that have worked so well for 100 years.</p>
<h3><strong>If we are to hold on to our existing theories, what picture do they paint about the nature of reality when taken together?</strong></h3>
<p>Rethinking space and time pushed me to view reality in a completely different way —&nbsp;not as a universe made of objects with defined properties, but as a network of interactions. This is the “relational” interpretation of quantum mechanics. In some sense, it’s a continuation of the trend in modern physics that we have seen with general relativity and quantum mechanics — a strong push toward perspectivalism.</p>
<p>We’re used to velocity being relative: The velocity of this table is different with respect to me, with respect to [that pigeon flying] outside, or with respect to the sun. Einstein showed us that time and length are also relative to different observers. Relational quantum mechanics takes this idea a step further. It argues that all properties of an object — its color, location, size, etc. — are in principle only definable in relation to another system. We need to give up the idea that there are material things which we’re describing from the outside. The best way of conceptualizing reality in light of modern science is in terms of the relative information that pieces of nature have about one another.</p>
<p>We can only say how the world looks from our limited, biased perspective. This is very radical, because you can no longer say, “This is a list of things in the world, and this is how they are.” We have to live with this lack of total description over reality.</p>
</div>
    <figure>
        <div>
                            <p><img width="2200" height="1229" src="https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-CloseUp.webp" alt="A close-up of Rovelli’s face." decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-CloseUp.webp 2200w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-CloseUp-1720x961.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-CloseUp-520x290.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-CloseUp-768x429.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-CloseUp-1536x858.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-CloseUp-2048x1144.webp 2048w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-CloseUp-98x55.webp 98w" sizes="(max-width: 2200px) 100vw, 2200px">                </p>
                        </div>
        <figcaption>
    <div>
            <p>Jan Jackle for&nbsp;<i data-stringify-type="italic">Quanta Magazine</i></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <h3><strong>There’s something unsettling about this argument. It seems to undermine the ultimate goal of physics to describe the “true” nature of reality, does it not? </strong></h3>
<p>It very much does, but if you look at the history of science, the ultimate goal has been changing constantly. It went from describing the rotation of heavenly bodies to tracking the forces that guide particles to following the evolution of fields in space-time. I think that the problem of science is to figure out the right conceptual scheme to best understand nature as we see it. The relational perspective is rooted in a deep awareness that our knowledge about the world is fundamentally limited and that everything we see is partial. We have a much stronger and more honest way of approaching reality without being attached to this misleading idea of there being an ultimate truth. We must not confuse the knowledge we have with the reality of the world.</p>
<p>If this leaves you with a sense of emptiness about reality, that’s fair. But it’s precisely by knowing that our knowledge is limited that we are able to learn. Between absolute certainty and ignorance there’s all this interesting space in which we live.</p>
<h3><strong>You’ve written about how your change in worldview has been guided by philosophers. How do you view the relationship between philosophy and physics?</strong></h3>
<p>The disciplines desperately need one another. A philosopher who doesn’t think about science is not willing to engage with the knowledge we have, and that’s just silly. And a scientist who refuses to look at philosophy is trapped in ways of thinking from which there may be an escape. Historically, the relationship between physicists and philosophers has been very strong. All scientific revolutions have been strongly influenced by philosophical ideas. Copernicus, Galileo and Newton were all philosophers themselves. Einstein very explicitly credited his insights to philosophers like Immanuel Kant, Ernst Mach and others. And Erwin Schrödinger was likely influenced by his reading of the Upanishads, the sacred Hindu texts, when he came up with wave mechanics.</p>
</div>
    <figure>
        <div>
                            <p><img width="2560" height="1438" src="https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Smiling-scaled.webp" alt="A man with glasses smiling" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Smiling-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Smiling-1720x966.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Smiling-520x292.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Smiling-768x431.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Smiling-1536x863.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Smiling-2048x1150.webp 2048w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-Smiling-98x55.webp 98w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
            <p>Jan Jackle for&nbsp;<i data-stringify-type="italic">Quanta Magazine</i></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>But lately, the relationship between physicists and philosophers has been at an all-time low. Stephen Hawking famously pronounced that “philosophy is dead,” and Richard Feynman said things like “Philosophers are as good for science as ornithologists are good for birds.” What they don’t realize is that, first, they are doing philosophy by commenting on what it means to do science; and second, their whole view of science is already under the influence of American pragmatism thinking and philosophers like Karl Popper and Thomas Kuhn. What the physics community took away from these philosophers was that science is about picking new ideas out of thin air, developing a theory, and testing whether it’s right or wrong. This gives the false impression that scientific progress comes only in paradigm-shifting insights that overturn previous thinking, and that all new hypotheses are equally probable until falsified. But science is so much more than that. It’s a continuous process of building on past knowledge to refine our perspective.</p>
<p>In my opinion, this closed-mindedness is precisely the problem with modern theoretical physics. We’re undergoing a colossal jump in knowledge that’s forcing us to rethink notions of reality, information, time and space. Our community has wasted a lot of time searching after speculative ideas. What we need instead is to digest the knowledge we already have. And to do that, we need philosophy. Philosophers help us not to find the right answers to given questions, but to find the right questions to better conceptualize reality.</p>
</div>
    <figure>
        <div>
                            <p><img width="1600" height="1117" src="https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-CloseUp-End.webp" alt="A man with glasses looking into the camera" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-CloseUp-End.webp 1600w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-CloseUp-End-520x363.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-CloseUp-End-768x536.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-CloseUp-End-1536x1072.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Carlo-Rovelli-cr.Jan-Jackle-CloseUp-End-98x68.webp 98w" sizes="(max-width: 1600px) 100vw, 1600px">                </p>
                        </div>
        <figcaption>
    <div>
            <p>Jan Jackle for&nbsp;<i data-stringify-type="italic">Quanta Magazine</i></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <h3><strong>In your book <em>Helgoland</em>, you talk about how the Buddhist philosopher Nagarjuna shaped your work. In what way did his texts open your mind?</strong></h3>
<p>The core idea of relational quantum mechanics is that when we talk about an object —&nbsp;be it an atom, a person or a galaxy — we are never just referring to the system alone. Rather, we are always referring to the interactions between this system and something else. We can only describe — and in fact understand —&nbsp;a thing as it relates to ourselves, or to our measuring devices.</p>
        
        
<p>Nagarjuna expresses a very similar idea: that no entity has a proper independent existence — things only exist depending on one another. By renouncing “primary” entities or any “ultimate absolute reality,” we can better make sense of the world in terms of how things manifest themselves to other things.</p>
<p>Relational quantum mechanics uses similar ideas to make sense of all quantum paradoxes in a precise mathematical way. The main idea is to give up questions about how things&nbsp;really&nbsp;are, in absolute&nbsp;terms. It’s just like how Galileo taught us that asking “Is this object really moving?” is meaningless, and Einstein taught us that asking “Are these two events&nbsp;really&nbsp;simultaneous?” is meaningless. The confusion about quantum mechanics, I believe, is generated by asking questions that have no meaning. The answer to the riddle is that there is no riddle.</p>
</div>
                
                
            </div><div>
        <div data-name="next-post__image-wrapper">
    <p><img width="1720" height="728" src="https://www.quantamagazine.org/wp-content/uploads/2025/10/Shark-Metabolic-Scaling-cr-Samantha-Mash-HP-1720x728.webp" alt="Multiple sharks swimming together, each one a different size" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/10/Shark-Metabolic-Scaling-cr-Samantha-Mash-HP-1720x728.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Shark-Metabolic-Scaling-cr-Samantha-Mash-HP-520x220.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Shark-Metabolic-Scaling-cr-Samantha-Mash-HP-768x325.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Shark-Metabolic-Scaling-cr-Samantha-Mash-HP-1536x650.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Shark-Metabolic-Scaling-cr-Samantha-Mash-HP-2048x867.webp 2048w, https://www.quantamagazine.org/wp-content/uploads/2025/10/Shark-Metabolic-Scaling-cr-Samantha-Mash-HP-98x41.webp 98w" sizes="(max-width: 1720px) 100vw, 1720px">    </p>
</div>
        
        <div>
                <h2>Next article</h2>
                <p>Shark Data Suggests Animals Scale Like Geometric Objects</p>
            </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[One year with Next.js App Router and why we're moving on (108 pts)]]></title>
            <link>https://paperclover.net/blog/webdev/one-year-next-app-router</link>
            <guid>45755911</guid>
            <pubDate>Thu, 30 Oct 2025 02:48:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://paperclover.net/blog/webdev/one-year-next-app-router">https://paperclover.net/blog/webdev/one-year-next-app-router</a>, See on <a href="https://news.ycombinator.com/item?id=45755911">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header><a href="https://paperclover.net/">back to the home page</a><p><em>A critique of React Server Components and Next.js 15.</em></p><div><p>Oct 21st, 2025</p><p>webdev</p><p>technical analysis</p><p>opinion</p></div><hr></header><p>As I've been using <a href="https://nextjs.org/">Next.js</a> professionally on my employer's web app, I find the
core design of their App Router and <a href="https://react.dev/reference/rsc/server-components">React Server Components</a> (RSC) to be
extremely frustrating. And it's not small bugs or that the API is confusing,
but large disagreements about the fundamental design decisions that Vercel and
the React team made when building it.</p><p>The more webdev events I go to, the more I see people who dislike Next.js, but
still get stuck using it. By the end of this article, I will share how me and
my colleagues escaped this hell, seamlessly migrating our entire frontend to
<a href="https://tanstack.com/start/latest">TanStack Start</a>.</p><div id="toc"><h2>Contents:</h2><ul>
<li><a href="#technical-review">A Technical Review, What are Server Components?</a></li>
<li><a href="#real-world-pitfalls">Real-world Pitfalls of the App Router</a>
<ul>
<li><a href="#optimistic-updates">Optimistic Updates are Impossible</a></li>
<li><a href="#redundant-fetches">Every Navigation is Another Fetch</a></li>
<li><a href="#layout-restrictions">Layouts are Artificially Restricted</a></li>
<li><a href="#rsc-payload">You Still Download All the Content Twice</a></li>
<li><a href="#turbopack">Turbopack Sucks</a></li>
</ul>
</li>
<li><a href="#ditching-nextjs">Seamlessly Ditching Next.js and Vercel at Work</a>
<ul>
<li><a href="#next-metadata"><code>next/metadata</code> is Great</a></li>
<li><a href="#vercel-og"><code>next/og</code> is Good Too</a></li>
</ul>
</li>
<li><a href="#experience-feels-like-the-usual">My Experience Feels Like the Usual</a></li>
<li><a href="#prefer-respectful-tools">Prefer Tools that Respect You</a></li>
</ul></div><h2 id="technical-review"><a href="#toc">§</a>A Technical Review, What are Server Components?</h2><p>The pitch of RSC is that components are put into two categories,
<b>"server"</b> components and <b>"client"</b>
components. Server components don't have <code>useState</code>, <code>useEffect</code>, but can be
<code>async function</code>s and refer to backend tools like directly calling into a
database. Client components are the existing
model, where there is code on the backend to generate HTML text and frontend
code to manage the DOM using <code>window.document.*</code>.</p><blockquote>
<p>The first disaster: naming!! React is now using the words
<b>"server"</b> and <b>"client"</b> to refer to
a very specific things, ignoring their existing definitions. This would be
fine, except <b>Client</b> components can run on the backend
too! In this article, I'll be using the terms <b>"backend"</b> and
<b>"frontend"</b> to describe the two execution environments that web apps
exist in: a Node.js process and a Web browser, respectively.</p>
</blockquote><p>This <b>Server</b>/<b>Client</b> component model
is interesting. Since built-ins like <code>&lt;Suspense /&gt;</code> get serialized across the
network, data fetching can be very trivially modeled with async <b>server components</b>, and the fallback UI works as if it were
client-side.</p><figure><figcaption>src/app/[username]/page.tsx</figcaption><pre><code><span>// For this article, server components will be highlighted in red
</span><span>export default async function </span><span>Page</span><span>({ params }) </span>{
    <span>// Page params are given as a resolved promise
    </span><span>const </span>{ <span>username </span>} <span>= await </span><span>params</span>;

    <span>// The components `UserInfo` and `UserPostList` will be run at the same
    // time. Once `UserInfo` is ready, the visitor will see the page with a
    // `PostListSkeleton` if the post list is not yet ready.
    </span><span>return &lt;</span><span>main</span><span>&gt;
        &lt;</span><span>UserInfo </span><span>username</span><span>=</span><span>{</span><span>username</span><span>} </span><span>/&gt;

        &lt;</span><span>Suspense </span><span>fallback</span><span>=</span><span>{&lt;</span><span>PostListSkeleton </span><span>/&gt;}</span><span>&gt;
            &lt;</span><span>UserPostList </span><span>username</span><span>=</span><span>{</span><span>username</span><span>} </span><span>/&gt;
        &lt;/</span><span>Suspense</span><span>&gt;
    &lt;/</span><span>main</span><span>&gt;
</span>}

<span>// Waterfalls are avoided by having multiple components, which
// are all evaluated at the same time.

</span><span>async function </span><span>UserInfo</span><span>({ username }) </span>{
    <span>const </span><span>user </span><span>= await </span><span>fetchUserInfo</span>(<span>username</span>);
    <span>return &lt;&gt;
        &lt;</span><span>h1</span><span>&gt;</span>{<span>user</span>.<span>displayName</span>}<span>&lt;/</span><span>h1</span><span>&gt;
        </span>{<span>user</span>.<span>bio </span><span>? &lt;</span><span>Markdown </span><span>content</span><span>=</span><span>{</span><span>user</span><span>.</span><span>bio</span><span>} </span><span>/&gt; : </span><span>""</span>}
    <span>&lt;/&gt;
</span>}

<span>async function </span><span>UserPostList</span><span>({ username }) </span>{
    <span>const </span><span>posts </span><span>= await </span><span>fetchUserPostList</span>(<span>username</span>);
    <span>return </span><span>/* post list ui omitted for brevity */</span>;
}
</code></pre></figure><p>If we ignore the 40kB gzipped bundle size of React itself, the above example
has zero JavaScript for the UI and data fetching — it just streams the
markup! For example, the imagined markdown parser within the <code>&lt;Markdown /&gt;</code>
component stays on the backend. When an interactive frontend is needed, <b>Client
components</b> can be created by putting them in a file starting with <code>"use client"</code>.</p><figure><figcaption>src/components/CopyButton.tsx</figcaption><pre><code><span>"use client"</span>; <span>// This comment marks the file for client-side bundling.

</span><span>export function </span><span>CopyButton</span><span>({ url }) </span>{
    <span>return &lt;&gt;
        &lt;</span><span>span</span><span>&gt;</span>{<span>url</span>}<span>&lt;/</span><span>span</span><span>&gt;
        &lt;</span><span>button </span><span>onClick</span><span>=</span><span>{</span><span>() </span><span>=&gt; </span><span>{
            </span><span>const </span><span>full </span><span>= new </span><span>URL</span><span>(</span><span>url</span><span>, </span><span>location</span><span>.href);
            navigator.</span><span>clipboard</span><span>.</span><span>writeText</span><span>(</span><span>full</span><span>.href);
            </span><span>// omitting error handling, success ui, styles
        </span><span>}}</span><span>&gt;</span>copy<span>&lt;/</span><span>button</span><span>&gt;
    &lt;/&gt;
</span>}
</code></pre></figure><figure><figcaption>src/app/q+a/Card.tsx</figcaption><pre><code><span>export function </span><span>Card</span><span>() </span>{
    <span>return &lt;</span><span>article</span><span>&gt;
        &lt;</span><span>header</span><span>&gt;
            </span>{<span>/* Make the browser import the copy button */</span>}
            <span>&lt;</span><span>CopyButton </span><span>url</span><span>=</span><span>"/q+a/2506010139" </span><span>/&gt;
        &lt;/</span><span>header</span><span>&gt;
        &lt;</span><span>p</span><span>&gt;
            </span>{<span>/* Process markdown on the backend */</span>}
            <span>&lt;</span><span>Markdown </span><span>content</span><span>=</span><span>".........." </span><span>/&gt;
        &lt;/</span><span>p</span><span>&gt;
    &lt;/</span><span>article</span><span>&gt;
</span>}
</code></pre></figure><h2 id="real-world-pitfalls"><a href="#toc">§</a>Real-world Pitfalls of the App Router</h2><p>After quitting <a href="https://bun.com/">Bun</a> as a runtime engineer (I implemented <a href="https://github.com/oven-sh/bun/blob/67f0c3e016aa479738469adac2b79a1862b88122/src/bake/bake.d.ts">Server Components
bundling</a> and <a href="https://github.com/oven-sh/bun/tree/e7790894d92b730758ecadf971cb935063508dfb/src/bake/bun-framework-react">a RSC template</a> there), I joined a small company working on the
front lines: a Next.js app with a Hono backend. The following notes are
simplifications from the real world problems I've encountered when trying to
maintain and develop new features. As a result of all of these, everyone's time
is wasted either working around design flaws, or explaining to each other why
what should be a non-issue is an immovable object.</p><h3 id="optimistic-updates"><a href="#toc">§</a>Optimistic Updates are Impossible</h3><p>The Next.js documentation for performing mutations <a href="https://nextjs.org/docs/app/getting-started/updating-data">does not mention optimistic
updates</a>; it appears this case was not thought about.
Components rendered by the <b>React Server</b>, by design, can
not be modified after mounting. Elements that could change need to be inside a
client component, but data fetching cannot happen on the client components,
even during SSR on the backend. This results in awkwardly small server
components that only do data fetching and then have a client component that
contains a mostly-static version of the page.</p><figure><figcaption>src/app/user/[username]/page.tsx</figcaption><pre><code><span>export default async function </span><span>Page</span><span>() </span>{
    <span>const </span><span>user </span><span>= await </span><span>fetchUserInfo</span>(<span>username</span>);
    <span>return &lt;</span><span>ProfileLayout</span><span>&gt;
        &lt;</span><span>UserProfile </span><span>user</span><span>=</span><span>{</span><span>user</span><span>} </span><span>/&gt;
    &lt;/</span><span>ProfileLayout</span><span>&gt;</span>;
}
</code></pre></figure><figure><figcaption>src/app/user/[username]/UserProfile.tsx</figcaption><pre><code><span>"use client"</span>; <span>// Must separate the client code into a second file!

</span><span>export function </span><span>UserProfile</span><span>({ user: initialUser }) </span>{
    <span>// There are many great state management libraries out there;
    // for simplicity, this example will use one state cell.
    </span><span>const </span>[<span>user</span>, <span>optimisticUpdateUser</span>] <span>= </span><span>useState</span>(<span>initialUser</span>);

    <span>async function </span><span>onEdit</span><span>(newUser) </span>{
        <span>optimisticUpdateUser</span>(<span>newUser</span>);
        <span>const </span><span>resp </span><span>= await </span><span>fetch</span>(<span>"..."</span>, {
            <span>method: </span><span>'POST'</span>,
            <span>body: </span><span>JSON</span>.<span>stringify</span>(<span>newUser</span>),
            <span>... </span><span>// (headers, credentials, tracing, and more)
        </span>})
        <span>if </span>(<span>!</span><span>resp</span>.<span>ok</span>) <span>/* always remember to test for errors! */
    </span>}

    <span>return &lt;</span><span>main</span><span>&gt;</span>{<span>/* user interface with editable fields... */</span>}<span>&lt;/</span><span>main</span><span>&gt;</span>:
}
</code></pre></figure><p>As more of the page needs interactivity, it gets messier trying to keep the
static parts truly server-side. On the work app, nearly every piece of UI
displays some dynamic data. A <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API"><code>WebSocket</code></a> synchronizes data live as it
updates (for example, a user card's online state along with their basic
profile). Since these component setups are harder to understand and maintain
for engineers, almost all of our pages are entirely <code>"use client"</code> with a
<code>page.tsx</code> that defines the data fetching.</p><p>A more concrete example of what this looks like in practice with the
data-fetching library we use at work, <a href="https://github.com/tanstack/query#readme">TanStack Query</a>.</p><figure><figcaption>src/queries/users.ts</figcaption><pre><code><span>// At work, there is a helper function `defineQuery` for type safety.
// Fetchers are trivial and can run on the backend or the frontend.
</span><span>export const </span><span>queryUserInfo </span><span>= </span><span>(username) </span><span>=&gt; </span>({
    <span>queryKey: </span>[<span>'user'</span>, <span>username</span>],
    <span>queryFn: </span><span>async </span><span>({ ... }) </span><span>=&gt; </span><span>/* fetch data */
</span>});
</code></pre></figure><figure><figcaption>src/app/user/[username]/page.tsx</figcaption><pre><code><span>export default async function </span><span>Page</span><span>({ params }) </span>{
    <span>const </span>{ <span>username </span>} <span>= await </span><span>params</span>; 

    <span>// There's no global state in the React Server. Since layouts 
    // are executed in parallel, the TanStack `QueryClient` has to
    // be reconstructed multiple times per route.
    </span><span>const </span><span>queryClient </span><span>= new </span><span>QueryClient</span>();
    <span>await </span><span>queryClient</span>.<span>ensureQueryData</span>(<span>queryUserInfo</span>(<span>username</span>));

    <span>// HydrationBoundary is a client component that passes JSON
    // data from the React server to the client component.
    </span><span>return &lt;</span><span>HydrationBoundary </span><span>state</span><span>=</span><span>{</span><span>dehydrate</span><span>(</span><span>queryClient</span><span>)}</span><span>&gt;
        &lt;</span><span>ClientPage </span><span>/&gt;
    &lt;/</span><span>HydrationBoundary</span><span>&gt;</span>;
}
</code></pre></figure><figure><figcaption>src/app/user/[username]/ClientPage.tsx</figcaption><pre><code><span>"use client"</span>;
<span>export function </span><span>ClientPage</span><span>() </span>{
    <span>const </span>{ <span>username </span>} <span>= </span><span>useParams</span>();
    <span>const </span>{ <span>data</span>: <span>user </span>} <span>= </span><span>useSuspenseQuery</span>(<span>queryUserInfo</span>(<span>username</span>));

    <span>// ... some hooks 

    </span><span>return &lt;</span><span>main</span><span>&gt;
        </span>{<span>/* ... an interactive web page */</span>}
    <span>&lt;/</span><span>main</span><span>&gt;</span>;
}
</code></pre></figure><p>This example has to be three separate files because of the rules of server
component bundling. (The client component needs <code>"use client"</code>, and server
component files often can't be imported on the client due to server-only
imports.). In the Pages router, this could've been a single file because of the
tree-shaking that <code>getStaticProps</code> and <code>getServerSideProps</code> has.</p><h3 id="redundant-fetches"><a href="#toc">§</a>Every Navigation is Another Fetch</h3><p>Since the App Router starts every page as a server component, with (ideally)
small areas of interactivity, a navigation to a new page <em>has</em> to fetch the
Next.js server, regardless of what data the client already has available! Even
with a a <code>loading.tsx</code> file, opening <code>/</code>, navigating to <code>/other</code>, and then
going back to <code>/</code> will show the loading state while it re-fetches the homepage.</p><p>The only case this works is for <strong>perfectly static content</strong>, where instant
navigations and prefetching work great. But <strong>web apps are not static</strong>, they
have lots of dynamic content. Being logged in affects the homepage, which is
infuriating because the client literally has everything needed to display the
page instantly. It's not like the cookies changed.</p><blockquote>
<p><strong>aside</strong>: In further testing on a blank project, I observe cases where the
Next frontend code would pre-fetch routes, but <strong>without any real contents</strong>.
On the hello world example, this was a 1.8kB RSC payload that pointed to 2
different JS chunks 4 separate times. This is just pure waste of our
bandwidth and egress, especially considering all of this information is
re-fetched when I actually click the link.</p>
<figure><pre><code><span>1</span>:<span>"$Sreact.fragment"
</span><span>2</span>:I[<span>39756</span>,[<span>"/_next/static/chunks/ff1a16fafef87110.js"</span>,<span>"/_next/static/chunks/7dd66bdf8a7e5707.js"</span>],<span>"default"</span>]
<span>3</span>:I[<span>37457</span>,[<span>"/_next/static/chunks/ff1a16fafef87110.js"</span>,<span>"/_next/static/chunks/7dd66bdf8a7e5707.js"</span>],<span>"default"</span>]
<span>4</span>:I[<span>97367</span>,[<span>"/_next/static/chunks/ff1a16fafef87110.js"</span>,<span>"/_next/static/chunks/7dd66bdf8a7e5707.js"</span>],<span>"ViewportBoundary"</span>]
<span>6</span>:I[<span>97367</span>,[<span>"/_next/static/chunks/ff1a16fafef87110.js"</span>,<span>"/_next/static/chunks/7dd66bdf8a7e5707.js"</span>],<span>"MetadataBoundary"</span>]
<span>7</span>:<span>"$Sreact.suspense"
</span><span>0</span>:{<span>"b"</span>:<span>"TdwnOXsfOJapNex_HjHGt"</span>,<span>"f"</span>:[[<span>"children"</span>,<span>"other"</span>,[<span>"other"</span>,{<span>"children"</span>:[<span>"__PAGE__"</span>,{}]}],[<span>"other"</span>,[<span>"$"</span>,<span>"$1"</span>,<span>"c"</span>,{<span>"children"</span>:[<span>null</span>,[<span>"$"</span>,<span>"$L2"</span>,<span>null</span>,{<span>"parallelRouterKey"</span>:<span>"children"</span>,<span>"error"</span>:<span>"$undefined"</span>,<span>"errorStyles"</span>:<span>"$undefined"</span>,<span>"errorScripts"</span>:<span>"$undefined"</span>,<span>"template"</span>:[<span>"$"</span>,<span>"$L3"</span>,<span>null</span>,{}],<span>"templateStyles"</span>:<span>"$undefined"</span>,<span>"templateScripts"</span>:<span>"$undefined"</span>,<span>"notFound"</span>:<span>"$undefined"</span>,<span>"forbidden"</span>:<span>"$undefined"</span>,<span>"unauthorized"</span>:<span>"$undefined"</span>}]]}],{<span>"children"</span>:<span>null</span>},[[<span>"$"</span>,<span>"div"</span>,<span>"l"</span>,{<span>"children"</span>:<span>"loading..."</span>}],[],[]],<span>false</span>],[<span>"$"</span>,<span>"$1"</span>,<span>"h"</span>,{<span>"children"</span>:[<span>null</span>,[<span>"$"</span>,<span>"$1"</span>,<span>"KCFxAJdIDH3BlYXAHsbcVv"</span>,{<span>"children"</span>:[[<span>"$"</span>,<span>"$L4"</span>,<span>null</span>,{<span>"children"</span>:<span>"$L5"</span>}],[<span>"$"</span>,<span>"meta"</span>,<span>null</span>,{<span>"name"</span>:<span>"next-size-adjust"</span>,<span>"content"</span>:<span>""</span>}]]}],[<span>"$"</span>,<span>"$L6"</span>,<span>"KCFxAJdIDH3BlYXAHsbcVm"</span>,{<span>"children"</span>:[<span>"$"</span>,<span>"div"</span>,<span>null</span>,{<span>"hidden"</span>:<span>true</span>,<span>"children"</span>:[<span>"$"</span>,<span>"$7"</span>,<span>null</span>,{<span>"fallback"</span>:<span>null</span>,<span>"children"</span>:<span>"$L8"</span>}]}]}]]}],<span>false</span>]],<span>"S"</span>:<span>false</span>}
<span>5</span>:[[<span>"$"</span>,<span>"meta"</span>,<span>"0"</span>,{<span>"charSet"</span>:<span>"utf-8"</span>}],[<span>"$"</span>,<span>"meta"</span>,<span>"1"</span>,{<span>"name"</span>:<span>"viewport"</span>,<span>"content"</span>:<span>"width=device-width, initial-scale=1"</span>}]]
<span>9</span>:I[<span>27201</span>,[<span>"/_next/static/chunks/ff1a16fafef87110.js"</span>,<span>"/_next/static/chunks/7dd66bdf8a7e5707.js"</span>],<span>"IconMark"</span>]
<span>8</span>:[[<span>"$"</span>,<span>"title"</span>,<span>"0"</span>,{<span>"children"</span>:<span>"Create Next App"</span>}],[<span>"$"</span>,<span>"meta"</span>,<span>"1"</span>,{<span>"name"</span>:<span>"description"</span>,<span>"content"</span>:<span>"Generated by create next app"</span>}],[<span>"$"</span>,<span>"link"</span>,<span>"2"</span>,{<span>"rel"</span>:<span>"icon"</span>,<span>"href"</span>:<span>"/favicon.ico?favicon.0b3bf435.ico"</span>,<span>"sizes"</span>:<span>"256x256"</span>,<span>"type"</span>:<span>"image/x-icon"</span>}],[<span>"$"</span>,<span>"$L9"</span>,<span>"3"</span>,{}]]
</code></pre></figure>
<p>In review, I found there is actually some content in here: the loading state.
Do you see it?</p>
<figure><pre><code>[<span>"$"</span>,<span>"div"</span>,<span>"l"</span>,{<span>"children"</span>:<span>"loading..."</span>}]
</code></pre></figure>
<p>It's still a lot of waste, since all of this data gets re-emitted in the
actual page RSC.</p>
</blockquote><p>The solution to this appears to be <a href="https://nextjs.org/docs/app/api-reference/config/next-config-js/staleTimes"><code>staleTime</code></a>, but it's marked
experimental and "not recommended for production".  The fact this is a
non-default afterthought configuration option is embarassing. Even if we used
it, you cannot make multiple pages that refer to the same underlying data share
any of it.</p><p>One form of loading state that cannot be represented with the App Router is
having a page such as a page like a git project's issue page, and clicking on a
user name to navigate to their profile page. With <code>loading.tsx</code>, the entire
page is a skeleton, but when modeling these queries with TanStack Query it is
possible to show the username and avatar instantly while the user's bio and
repositories are fetched in. Server components don't support this form of
navigation because the data is only available in rendered components, so it
must be re-fetched.</p><p>In our Next.js site, we have this line of code on our server component data
fetchers to make soft navigations faster by skipping the data fetch phase all
together.</p><figure><figcaption>src/util/tanstack-query-helpers.server.ts</figcaption><pre><code><span>export function </span><span>serverSidePrefetchQueries</span><span>(queries) </span>{
    <span>if </span>((<span>await </span><span>headers</span>()).<span>get</span>(<span>"next-url"</span>)) {
        <span>// This is a soft-navigation. SKIP the prefetching to make it faster.
        // The client might already have this data, and if not, they have the
        // loading state. Ideally, this server request wouldn't exist -- The
        // client side has nearly ALL the code since the app is written mostly
        // as client components. Kind of a design flaw of the App router TBH.
        </span><span>return</span>;
    }
    <span>// ... data prefetching-logic ...
</span>}
</code></pre></figure><p>In addition to this, <code>loading.tsx</code> should contain the <code>useQuery</code> calls so that
while the network request for the empty RSC happens, the data is being fetched
if it actually is needed. In fact, the <code>loading.tsx</code> state can just be the
actual client component, and you'll see the client page.</p><figure><figcaption>src/app/user/[username]/loading.tsx</figcaption><pre><code><span>"use client"</span>;
<span>export default function </span><span>PageLoadingSkeleton</span><span>() </span>{
    <span>return &lt;</span><span>ClientPage </span><span>/&gt;</span>;
}
</code></pre></figure><blockquote>
<p>At work, we just make our <code>loading.tsx</code> files contain the <code>useQuery</code>
calls and show a skeleton. This is because when Next.js loads the actual Server
Component, no matter what, the entire page re-mounts. No VDOM diffing here,
meaning all hooks (<code>useState</code>) will reset slightly after the request
completes. I tried to reproduce a simple case where I was <em>begging</em> Next.js to
just <em>update the existing DOM</em> and preserve state, but it just doesn't.
Thankfully, the time the blank RSC call takes is short enough.</p>
</blockquote><h3 id="layout-restrictions"><a href="#toc">§</a>Layouts are Artificially Restricted</h3><p>Layouts can perform data fetching, but they can't observe or alter the request
in any way. This is done so that Next.js can fetch and cache layouts whenever they
want. In every other framework, layouts are just regular components that have
no feature difference compared to page components.</p><p>Fetching layouts in isolation is a cute idea, but it ends up being silly
because it also means that any data fetching has to be re-done per layout. You
can't share a <code>QueryClient</code>; instead, you must rely on their <a href="https://nextjs.org/docs/app/api-reference/functions/fetch">monkey-patched
<code>fetch</code></a> to cache the same <code>GET</code> request like they promise.</p><p>When a coworker asks me about why Next.js rejects some code, I've given up on
explaining the technical intricacies and just say <em>"It's a Next.js Skill Issue,
I'm going to blow it up soon don't worry."</em> These rules are too hard for normal
developers to understand.</p><h3 id="rsc-payload"><a href="#toc">§</a>You Still Download All the Content Twice</h3><p>Unlike the <a href="https://www.patterns.dev/vanilla/islands-architecture/">"Islands Architecture"</a>, Server Components still have to
be hydrated on the frontend to support <code>Suspense</code> and preserving client
component state. When doing soft navigations, the "RSC Payload" (which is not
HTML at all) is retrieved by <code>fetch</code>. On a fresh reload, HTML is needed for the
<a href="https://web.dev/articles/fcp">first paint</a>, but the information about Client components and <code>Suspense</code> is
not contained within that HTML. React's solution is to <strong>send a second copy of
the entire page's markup</strong>. An example of what a Next.js production server
would send in a dynamic page render would be something like this:</p><figure><figcaption>GET /user/clover</figcaption><pre><code><span>&lt;!DOCTYPE html&gt;
</span><span>&lt;html&gt;
&lt;head&gt;
</span>    {link and meta tags}
<span>&lt;/head&gt;
&lt;body&gt;
</span>    {server side render}
    <span>&lt;script&gt;
        </span><span>// a bootstrap script that sets up global `__next_f` as
        // an array. once React loads, this `.push` function
        // gets overwritten to write new chunks directly to the
        // RSC decoder. this script has some dom helpers too
        </span>(<span>self</span>.<span>__next_f</span><span>=</span><span>self</span>.<span>__next_f</span><span>||</span>[]).<span>push</span>([<span>0</span>])
    <span>&lt;/script&gt;
    &lt;script&gt;
        </span><span>// the RSC payload for the application shell.
        </span><span>self</span>.<span>__next_f</span>.<span>push</span>([<span>1</span>,<span>"1:\"$Sreact.fragment\"\n2:I[658993,[\"/_next/st{...}"</span>])
    <span>&lt;/script&gt;

    </span><span>&lt;!--
        the closing &lt;/body&gt; is NOT written yet, since there is a
        suspense boundary not resolved. time passes, and only
        then is more data is written
    --&gt;
    </span><span>&lt;div class="user-post-list"&gt;
</span>        {server side render of a Suspense boundary}
    <span>&lt;/div&gt;
    &lt;script&gt;
        </span><span>// the RSC payload for the suspense boundary
        </span><span>self</span>.<span>__next_f</span>.<span>push</span>([<span>2</span>,<span>"14:[\"$\",\"div\",null,{\"children\":[[\"$\",\"h4\"{...}"</span>])
    <span>&lt;/script&gt;
    
    </span><span>&lt;!-- HTML and script tags repeat until the entire page is done --&gt;
</span><span>&lt;/body&gt;
&lt;/html&gt;
</span></code></pre></figure><p>This solution <strong>doubles the size of the initial HTML payload</strong>. Except it's
worse, because the RSC payload includes JSON quoted in JS string literals, which
format is much less efficient than HTML. While it seems to compress fine
with brotli and render fast in the browser, this is wasteful. With the
hydration pattern, at least the data locally could be re-used for interactivity
and other pages.</p><p>Even on pages that have little to no interactivity, you pay the cost. To use
the Next.js documentation as an example, loading <a href="https://nextjs.org/docs">its
homepage</a> loads an page that is around 750kB (250kB of
HTML and the 500kB of script tags), and content is in there twice.</p><p>You can verify that by pressing <kbd>Cmd</kbd> + <kbd>Opt</kbd> + <kbd>u</kbd>
on Mac or <kbd>Ctrl</kbd> + <kbd>u</kbd> on other platforms. And then
<kbd>Cmd</kbd> / <kbd>Ctrl</kbd> + <kbd>f</kbd> to locate any string of the
blog, such as "building full-stack web applications". It's there twice. And
<strong>there is no way around this</strong>, since it's a fundamental piece of React Server
Components.</p><p>This RSC format certainly has more waste. But I really don't feel like digging into
why the string <code>/_next/static/chunks/6192a3719cda7dcc.js</code> appears 27 separate
times. What the hell, guys? Is your bandwidth free???</p><h3 id="turbopack"><a href="#toc">§</a>Turbopack Sucks</h3><p>This section is not constructive.</p><ul>
<li>Turbopack isn't fast</li>
<li>Turbopack emits code that is hard to debug in a debugger (in development mode)</li>
<li>Turbopack throws bad error messages in many cases</li>
</ul><p>I wouldn't have given this point a section in the blog normally, but I want to
point out three actual examples directly from the project.</p><p>The first is a place where during some refactoring to satisfy the Server/Client
component models, I accidentally made a Client component <code>async</code>. This one was
quite anoying because it didn't say at all where the issue was, but only
contained the <b>server</b> stack trace.</p><p><img src="https://paperclover.net/file/2025/blog-everyone-hates-nextjs/asyncerror.png" alt="Next.js error"></p><p>Another case of a terrible error message:</p><p><img src="https://paperclover.net/file/2025/blog-everyone-hates-nextjs/nexterror.png" alt="Next.js error"></p><blockquote>
<p>After fixing the underlying issue in this second error (which I cannot recall),
the Dev server hung and had to be restarted to recover.</p>
</blockquote><p>The final one is the dozen times I place a debugger breakpoint and the
variable name <code>hello</code> gets turned into
<code>__TURBOPACK__imported__module__$5b$project$5d2f$client$2f$src$2f$utils$2f$filename$2e$ts__$5b$app$2d$client$5d$__$28$ecmascript$29$__["hello"]</code>
and other bullshit.</p><p>Okay. This all sucks. What can we do?</p><h2 id="ditching-nextjs"><a href="#toc">§</a>Seamlessly Ditching Next.js and Vercel at Work</h2><p>There are two types of web projects:</p><ul>
<li>A web site with mostly static content.</li>
<li>A web app with majorly dynamic and interactive components.</li>
</ul><p>And Next.js is the wrong tool for both of these jobs. If you're in the first
category with a static web site, go for <a href="https://astro.build/">Astro</a> or <a href="https://fresh.deno.dev/">Fresh</a>. For everyone who
needs the full power of React, this section is about how I replaced the vendor
locked Next with <a href="https://tanstack.com/start/latest">TanStack Start</a>, incrementally and seamlessly.</p><p>It started with this Vite config.</p><figure><figcaption>vite.config.ts</figcaption><pre><code><span>const </span><span>config </span><span>= </span><span>defineConfig</span>(<span>({ mode }) </span><span>=&gt; </span>{
    <span>const </span><span>env </span><span>= </span><span>loadEnv</span>(<span>mode</span>, <span>process</span>.<span>cwd</span>(), <span>"NEXT_PUBLIC_"</span>);
    <span>return </span>{
        <span>// Use the Next.js default port 3000
        </span><span>server: </span>{ <span>port: </span><span>3000 </span>},
        <span>// Use the Next.js default env prefix "NEXT_PUBLIC_"
        </span><span>define: </span><span>Object</span>.<span>fromEntries</span>(<span>Object</span>.<span>entries</span>(<span>env</span>).<span>map</span>(
            <span>([k, v]) </span><span>=&gt; </span>[<span>`process.env.${k}`</span>, <span>JSON</span>.<span>stringify</span>(<span>v</span>)])),
        <span>plugins: </span>[
            <span>viteTsConfigPaths</span>({ <span>projects: </span>[<span>"./tsconfig.json"</span>] }),
            <span>tailwindcss</span>(),
            <span>// For ease of understanding from coworkers, I started porting
            // the routes in `src/tanstack-routes`. When the migration was
            // done, it would go back to the default `src/routes`.
            </span><span>tanstackStart</span>({
                <span>router: </span>{ <span>routesDirectory: </span><span>"src/tanstack-routes" </span>},
            }),
            <span>viteReact</span>(),
        ],
        <span>resolve: </span>{
            <span>// The key to the incremental migration: redirect `next` elsewhere
            </span><span>alias: </span>{ <span>next: </span><span>path</span>.<span>resolve</span>(<span>"./src/tanstack-next/"</span>) },
            <span>conditions: </span>[<span>"tanstack"</span>],
            <span>extensions: </span>[
                <span>// Allow a file named like `utils/session.tanstack.ts` to
                // override `utils/session.ts` when imported.
                </span><span>".tanstack.tsx"</span>, <span>".tanstack.ts"</span>,
                <span>// Default import extensions
                </span><span>".mjs"</span>, <span>".js"</span>, <span>".mts"</span>, <span>".ts"</span>,
                <span>".jsx"</span>, <span>".tsx"</span>, <span>".json"</span>,
            ],
        },
    };
});
</code></pre></figure><p>Then, I looked for every usage of a Next.js API, and either removed it or made
a stub for TanStack. For example, <code>src/tanstack-next/link.tsx</code> implements
<code>next/link</code>:</p><figure><figcaption>src/tanstack-next/link.tsx</figcaption><pre><code><span>import </span>{ <span>Link </span>} <span>from </span><span>"@tanstack/react-router"</span>;
<span>import type </span>{ <span>LinkProps </span>} <span>from </span><span>"next/link"</span>;

<span>export default function </span><span>LinkAdapter</span><span>({ href, </span><span>...</span><span>rest }</span><span>: </span><span>LinkProps</span><span>) </span>{
  <span>return &lt;</span><span>Link </span><span>{</span><span>...</span><span>rest</span><span>} to</span><span>=</span><span>{</span><span>href </span><span>as </span><span>unknown </span><span>as </span><span>any</span><span>} </span><span>/&gt;</span>;
}
</code></pre></figure><blockquote>
<p>Some of these stubs can be extremely simple. Starting out, my implementation
of <code>useRouter</code> was just <code>return {}</code>, but later I had to add a couple methods
to the object. The code here doesn't have to be clean, because it is
temporary.</p>
</blockquote><p>Now, the new site can import nearly every client component by either stubbing
out the Next.js APIs it needs, or by using the <code>.tanstack.ts</code> extension to
re-implement logic on a file-by-file basis. And shortly after, I got the site's
homepage to work in TanStack Start, and we merged the branch.</p><p><img src="https://paperclover.net/file/2025/blog-everyone-hates-nextjs/pr.png" alt="My &quot;nextgate&quot; PR"></p><blockquote>
<p>This first PR only supported one of our pages, and was able to do it in a
thousand lines of added code, and 40 lines deleted. I had previous patches to
remove the few uses of <code>next/image</code> and <code>next/font</code>.</p>
</blockquote><p>What was left was porting every other route over. The one thing we lose in
migrating from Next.js to any other framework is the ability to <code>await</code>
data-fetching functions in the UI. In practice, moving every route into a
<code>loader</code> function made it much more clear what happened when a page was SSR'd.
For pages that had multiple fetches, these could be combined into a single,
special API call that would return all of the relevant data for that page.</p><p>To re-iterate in bold font: <strong>The
migration path from Server Components is to just simplify your code — RSC
inherently drives you down a chaotic road of things you do not need</strong>.
Nearly every complex part of our site got easier to understand for all
engineers. The exception to this was having everyone get used to the new file
system routing conventions. With enough examples, we all got the hang of it.</p><p>With the incremental migration in place, new code did not break the existing
deployment. TanStack slowly took over the codebase, and we eventually deleted
all of the Next.js stubs and gained all of the beautiful <a href="https://tanstack.com/router/v1/docs/framework/react/guide/type-safety">type-safety features</a>
that the TanStack Router provides. At the end, the site performed faster from
every angle: Development Mode, Production page load times, Soft navigations,
and at a lower price than our Next depoyment with Vercel.</p><p>We're not the only ones seeing the change. While I try and keep myself off of
social media, someone sent me <a href="https://twitter.com/BriansAngles/status/1978834116079436242#m">the results of Brian Anglin's work at
Superwall</a>, showing incredible CPU reductions on TanStack
Start. I also recall ChatGPT switching from Next.js to Remix (random online
chatter: [<a href="https://xcancel.com/ryanflorence/status/1831379475654947233">1</a>] [<a href="https://old.reddit.com/r/reactjs/comments/1f97zgr/chatgpt_migrates_from_nextjs_to_remix">2</a>] [<a href="https://old.reddit.com/r/nextjs/comments/1f92jdv/chatgptcom_switched_from_nextjs_to_remix">3</a>]) a year ago.</p><h3 id="next-metadata"><a href="#toc">§</a><code>next/metadata</code> is Great</h3><p>In my opinion, this is one of the only good APIs Next.js has, and was the one
place in our code where moving to TanStack made things harder to do. Instead of
worsening the code, I just ported their metadata API into a regular function,
so everyone can use it. Originally, I had a 1:1 port on NPM, but earlier this
year I simplified it's API into one short and understandable
file. As of this blog post, I have added a TanStack-compatible
<code>meta.toTags</code> API, which can be installed from <a href="https://jsr.io/@clo/lib">JSR</a>, <a href="https://npmjs.com/@paperclover/lib">NPM</a>,
or simply copied into your project.</p><blockquote>
<p><strong>notice</strong>: Due to time constraints with writing this article, the library
has not yet been updated. I'll probably get around to it by the end of this
week (Oct 24th). As a placeholder, I'm able to share the version that is used
at work to my website: <a href="https://paperclover.net/file/2025/blog-everyone-hates-nextjs/meta.tanstack.ts"><code>meta.tanstack.ts</code></a>.</p>
</blockquote><figure><pre><code><span>// once in your project
</span><span>import </span><span>* </span><span>as </span><span>meta </span><span>from </span><span>"@clo/lib/meta.ts"</span>;

<span>export const </span><span>defineHead </span><span>= </span><span>meta</span>.<span>toTags</span>.<span>bind</span>(<span>null</span>, {
    <span>// site-wide options
    </span><span>base: </span><span>new </span><span>URL</span>(<span>"https://paperclover.net"</span>),
    <span>titleTemplate: </span><span>(title) </span><span>=&gt; </span>[<span>title</span>, <span>"paper clover"</span>]
        .<span>filter</span>(<span>Boolean</span>).<span>join</span>(<span>' | '</span>),
    <span>// ...
</span>});

<span>// for each page...
</span><span>export const </span><span>Route </span><span>= </span><span>createFileRoute</span>(<span>"/blog"</span>)({
    <span>head: </span><span>() </span><span>=&gt;
        </span><span>defineHead</span>({
            <span>title: </span><span>"clover's blog"</span>, <span>// templated with `titleTemplate`
            </span><span>description: </span><span>"a catgirl meows about her technology viewpoints"</span>,
            <span>canonical: </span><span>"/blog"</span>, <span>// joined with `base`

            // When specified, configures Open Graph and Twitter embed,
            // using the page title and description as the default.
            // The defaults are good, but it supports more options.
            </span><span>embed: </span>{}, 

            <span>// Every exotic meta tag is done with a JSX fragment. This
            // doesn't render React, it just loops through the tags.
            // My goal was to cover the most common 99% of uses.
            </span><span>extra: </span><span>&lt;&gt;
                &lt;</span><span>meta </span><span>name</span><span>=</span><span>"site-verification" </span><span>content</span><span>=</span><span>"waffles" </span><span>/&gt;</span>,
            <span>&lt;/&gt;</span>,
        }),

    <span>component: </span><span>Page</span>,
});

<span>function </span><span>Page</span><span>() </span>{
    <span>...
</span>}
</code></pre></figure><p>My version wasn't concerned with covering the entire space of Next.js's metadata
object, but instead uses inline JSX to fill that gap.</p><h3 id="ditching-nextjs"><a href="#toc">§</a><code>next/og</code> is Good Too</h3><p>No strong opinions. I just want to remind everyone that the <code>@vercel/og</code> package exists.</p><h2 id="experience-feels-like-the-usual"><a href="#toc">§</a>My Experience Feels like the Usual</h2><p>At the Next.js Conf 2024, everyone there was raving about Server Components. I
forget exactly who I talked to, but the big people were all in on this. I,
having implemented the bundler end of RSC, saw a couple of the problems in the
format. With Next 15 "stabilizing" the App Router last year, many companies are
building their products on it, realizing these pitfalls first-hand.</p><p>I came into the Next.js game late, only starting in June with version 15.
But everyone I've talked to at events sympathize with my notes. All the people
I talked to on the subject at Bun's 1.3 Party agreed with me. Even some people
at Vercel told me they don't like how Next.js is to actually use.</p><p>I hope as TanStack Start stabilizes, it becomes the Next.js replacement everyone
wants.</p><p>A lot of in the JavaScript ecosystem is a mess. That mess is why web
development gets made fun of. There were a lot of times I thought that working
with the web was an unrecoverable mess, but the mess was actually just the
commonly-used libraries I surrounded myself with. When that is peeled back,
modern web development technologies are awesome.</p><p>I've been making this website from scratch without any framework since late
2024, by writing systems like my own <a href="https://git.paperclover.net/clo/sitegen/src/branch/master/lib/progress.ts">TUI progress widget</a>, <a href="https://git.paperclover.net/clo/sitegen/src/branch/master/src/file-viewer/cache.ts">static
file proxy</a>, incremental build system, and many more components.
Working on this code has produced some of my best coding sessions (by
happiness) in years. The viewers of <em><a href="https://paperclover.net/">paper clover</a></em> get a better quality
website; the mini-libraries I create get <a href="https://git.paperclover.net/clo/sitegen/src/branch/master/lib#readme">extracted for public use</a>,
everyone wins.</p><p>This level of from-scratch is too much for most people, especially at the
workplace. I say that at the minimum, we should only give our attention and
money to high quality tools that respect us. And Next.js and the company behind
it, Vercel, are not that.</p><p>If you use Next.js, and feel that the experience doesn't remind you of respect
too, consider whether you and your colleagues want to continue supporting their
<a href="https://youtu.be/SCIfWhAheVw">serverless empire</a>. The Vite ecosystem seems pretty decent to build on right
now, but I still have little experience in using their tools at scale in
production. The <a href="https://viteplus.dev/">Vite+ launch from Void0</a> seems interesting, but
only time will tell if these venture-funded tools will respect us (end-users
and developers) long term.</p><p>Next.js Conf 2025, as of writing, is <a href="https://nextjs.org/conf">tomorrow</a>. Instead of
purchasing a $800 ticket, I decided to put that money <a href="https://github.com/sponsors/tannerlinsley">toward the TanStack
team</a> for <a href="https://tanstack.com/ethos">respecting and improving the web development
ecosystem</a>.</p><h2>What the Future Holds</h2><p>Slowly, I've been replacing many pieces of software that disrespect me with
better alternatives. Some examples of this are GitHub, Visual Studio Code,
DaVinci Resolve, Discord, Google Drive/Workspace, along many more. I plan to
write more on this blog about the technical things I do (that progress library,
the purpose of my own site generator, learnings from my current job), including
some of my past projects at Bun (details on HMR, the crash reporter, and the
crazy system for bundling built-in modules). If it interests you, please
subscribe to the email list:</p><p><a href="mailto:subscribe@paperclover.net?subject=paper%20clover%20mailing%20list&amp;body=I%20would%20like%20to%20be%20subscribed%20to%20the%20following%20mailing%20lists%3A%0A%0A-%20Technical%20Blog%20Posts%20-%20YES%0A-%20Art%20(Original%20Music%2FVideo)%20-%20YES%0A%0A(feel%20free%20to%20write%20whatever%20else%20you%20want)">click here to send an email to <code>subscribe@paperclover.net</code>, requesting that you would like to be added to the mailing list.</a> (i manage this mailing list manually)</p><p><a href="#top">back to top</a> — <a href="https://paperclover.net/q+a">ask a question about this article</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hello-World iOS App in Assembly (170 pts)]]></title>
            <link>https://gist.github.com/nicolas17/966a03ce49f949dd17b0123415ef2e31</link>
            <guid>45755821</guid>
            <pubDate>Thu, 30 Oct 2025 02:37:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/nicolas17/966a03ce49f949dd17b0123415ef2e31">https://gist.github.com/nicolas17/966a03ce49f949dd17b0123415ef2e31</a>, See on <a href="https://news.ycombinator.com/item?id=45755821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="" data-tab-size="4" data-paste-markdown-skip="" data-tagsearch-path="yellow.asm">
        <tbody><tr>
          <td id="file-yellow-asm-L1" data-line-number="1"></td>
          <td id="file-yellow-asm-LC1">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L2" data-line-number="2"></td>
          <td id="file-yellow-asm-LC2"><span>.</span><span>global</span> <span>_main</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L3" data-line-number="3"></td>
          <td id="file-yellow-asm-LC3"><span>.</span><span>extern</span> <span>_putchar</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L4" data-line-number="4"></td>
          <td id="file-yellow-asm-LC4">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L5" data-line-number="5"></td>
          <td id="file-yellow-asm-LC5"><span>.</span><span>align</span><span> </span><span>4</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L6" data-line-number="6"></td>
          <td id="file-yellow-asm-LC6">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L7" data-line-number="7"></td>
          <td id="file-yellow-asm-LC7"><span>_main:</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L8" data-line-number="8"></td>
          <td id="file-yellow-asm-LC8"><span>    ; prolog; save fp,lr,x19</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L9" data-line-number="9"></td>
          <td id="file-yellow-asm-LC9"><span>    stp x29</span><span>,</span><span> x30</span><span>,</span><span> </span><span>[</span><span>sp</span><span>,</span><span> #</span><span>-</span><span>0x20</span><span>]</span><span>!</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L10" data-line-number="10"></td>
          <td id="file-yellow-asm-LC10"><span>    </span><span>str</span><span> x19</span><span>,</span><span> </span><span>[</span><span>sp</span><span>,</span><span> #</span><span>0x10</span><span>]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L11" data-line-number="11"></td>
          <td id="file-yellow-asm-LC11"><span>    </span><span>mov</span><span> x29</span><span>,</span><span> </span><span>sp</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L12" data-line-number="12"></td>
          <td id="file-yellow-asm-LC12"><span>    ; make space for 2 dword local vars</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L13" data-line-number="13"></td>
          <td id="file-yellow-asm-LC13"><span>    </span><span>sub</span><span> </span><span>sp</span><span>,</span><span> </span><span>sp</span><span>,</span><span> #</span><span>0x10</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L14" data-line-number="14"></td>
          <td id="file-yellow-asm-LC14"><span>    ; save argc/argv</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L15" data-line-number="15"></td>
          <td id="file-yellow-asm-LC15"><span>    stp x0</span><span>,</span><span> x1</span><span>,</span><span> </span><span>[</span><span>sp</span><span>]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L16" data-line-number="16"></td>
          <td id="file-yellow-asm-LC16">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L17" data-line-number="17"></td>
          <td id="file-yellow-asm-LC17"><span>    ; create autorelease pool and save into x19</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L18" data-line-number="18"></td>
          <td id="file-yellow-asm-LC18"><span>    </span><span>bl</span><span> _objc_autoreleasePoolPush</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L19" data-line-number="19"></td>
          <td id="file-yellow-asm-LC19"><span>    </span><span>mov</span><span> x19</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L20" data-line-number="20"></td>
          <td id="file-yellow-asm-LC20">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L21" data-line-number="21"></td>
          <td id="file-yellow-asm-LC21"><span>    ; initialize app delegate class</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L22" data-line-number="22"></td>
          <td id="file-yellow-asm-LC22"><span>    </span><span>bl</span><span> initAppDelegate</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L23" data-line-number="23"></td>
          <td id="file-yellow-asm-LC23">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L24" data-line-number="24"></td>
          <td id="file-yellow-asm-LC24"><span>    ; create CFString with delegate class name</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L25" data-line-number="25"></td>
          <td id="file-yellow-asm-LC25"><span>    </span><span>mov</span><span> x0</span><span>,</span><span> </span><span>0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L26" data-line-number="26"></td>
          <td id="file-yellow-asm-LC26"><span>    adrp x1</span><span>,</span><span>     str_AppDelegate@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L27" data-line-number="27"></td>
          <td id="file-yellow-asm-LC27"><span>    </span><span>add</span><span>  x1</span><span>,</span><span> x1</span><span>,</span><span> str_AppDelegate@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L28" data-line-number="28"></td>
          <td id="file-yellow-asm-LC28"><span>    </span><span>mov</span><span> x2</span><span>,</span><span> </span><span>0x0600</span><span> ; kCFStringEncodingASCII</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L29" data-line-number="29"></td>
          <td id="file-yellow-asm-LC29"><span>    </span><span>bl</span><span> _CFStringCreateWithCString</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L30" data-line-number="30"></td>
          <td id="file-yellow-asm-LC30">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L31" data-line-number="31"></td>
          <td id="file-yellow-asm-LC31"><span>    ; x0 = UIApplicationMain(argc, argv, nil, CFSTR("AppDelegate"));</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L32" data-line-number="32"></td>
          <td id="file-yellow-asm-LC32"><span>    </span><span>mov</span><span> x3</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L33" data-line-number="33"></td>
          <td id="file-yellow-asm-LC33"><span>    ldr x0</span><span>,</span><span> </span><span>[</span><span>sp</span><span>]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L34" data-line-number="34"></td>
          <td id="file-yellow-asm-LC34"><span>    ldr x1</span><span>,</span><span> </span><span>[</span><span>sp</span><span>,</span><span> #</span><span>0x8</span><span>]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L35" data-line-number="35"></td>
          <td id="file-yellow-asm-LC35"><span>    </span><span>mov</span><span> x2</span><span>,</span><span> #</span><span>0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L36" data-line-number="36"></td>
          <td id="file-yellow-asm-LC36"><span>    </span><span>bl</span><span> _UIApplicationMain</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L37" data-line-number="37"></td>
          <td id="file-yellow-asm-LC37"><span>    </span><span>mov</span><span> x7</span><span>,</span><span> x0</span><span> ; save retval</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L38" data-line-number="38"></td>
          <td id="file-yellow-asm-LC38">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L39" data-line-number="39"></td>
          <td id="file-yellow-asm-LC39"><span>    ; pop autorelease pool</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L40" data-line-number="40"></td>
          <td id="file-yellow-asm-LC40"><span>    </span><span>mov</span><span> x0</span><span>,</span><span> x19</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L41" data-line-number="41"></td>
          <td id="file-yellow-asm-LC41"><span>    </span><span>bl</span><span> _objc_autoreleasePoolPop</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L42" data-line-number="42"></td>
          <td id="file-yellow-asm-LC42">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L43" data-line-number="43"></td>
          <td id="file-yellow-asm-LC43"><span>    ; epilog</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L44" data-line-number="44"></td>
          <td id="file-yellow-asm-LC44"><span>    ; restore stack pointer</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L45" data-line-number="45"></td>
          <td id="file-yellow-asm-LC45"><span>    </span><span>add</span><span> </span><span>sp</span><span>,</span><span> </span><span>sp</span><span>,</span><span> </span><span>0x10</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L46" data-line-number="46"></td>
          <td id="file-yellow-asm-LC46"><span>    ; restore saved registers</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L47" data-line-number="47"></td>
          <td id="file-yellow-asm-LC47"><span>    ldr x19</span><span>,</span><span> </span><span>[</span><span>sp</span><span>,</span><span> #</span><span>0x10</span><span>]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L48" data-line-number="48"></td>
          <td id="file-yellow-asm-LC48"><span>    ldp x29</span><span>,</span><span> x30</span><span>,</span><span> </span><span>[</span><span>sp</span><span>],</span><span> #</span><span>0x20</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L49" data-line-number="49"></td>
          <td id="file-yellow-asm-LC49"><span>    ; get retval</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L50" data-line-number="50"></td>
          <td id="file-yellow-asm-LC50"><span>    </span><span>mov</span><span> x0</span><span>,</span><span> x7</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L51" data-line-number="51"></td>
          <td id="file-yellow-asm-LC51"><span>    </span><span>ret</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L52" data-line-number="52"></td>
          <td id="file-yellow-asm-LC52">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L53" data-line-number="53"></td>
          <td id="file-yellow-asm-LC53"><span>initAppDelegate:</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L54" data-line-number="54"></td>
          <td id="file-yellow-asm-LC54"><span>    ; prolog; save fp,lr,x20</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L55" data-line-number="55"></td>
          <td id="file-yellow-asm-LC55"><span>    stp x29</span><span>,</span><span> x30</span><span>,</span><span> </span><span>[</span><span>sp</span><span>,</span><span> #</span><span>-</span><span>0x20</span><span>]</span><span>!</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L56" data-line-number="56"></td>
          <td id="file-yellow-asm-LC56"><span>    </span><span>str</span><span> x20</span><span>,</span><span> </span><span>[</span><span>sp</span><span>,</span><span> #</span><span>0x10</span><span>]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L57" data-line-number="57"></td>
          <td id="file-yellow-asm-LC57"><span>    </span><span>mov</span><span> x29</span><span>,</span><span> </span><span>sp</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L58" data-line-number="58"></td>
          <td id="file-yellow-asm-LC58">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L59" data-line-number="59"></td>
          <td id="file-yellow-asm-LC59"><span>    ; Class c = objc_allocateClassPair(objc_getClass("NSObject"), "AppDelegate", 0);</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L60" data-line-number="60"></td>
          <td id="file-yellow-asm-LC60"><span>    adrp x0</span><span>,</span><span>     str_NSObject@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L61" data-line-number="61"></td>
          <td id="file-yellow-asm-LC61"><span>    </span><span>add</span><span>  x0</span><span>,</span><span> x0</span><span>,</span><span> str_NSObject@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L62" data-line-number="62"></td>
          <td id="file-yellow-asm-LC62"><span>    </span><span>bl</span><span> _objc_getClass</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L63" data-line-number="63"></td>
          <td id="file-yellow-asm-LC63"><span>    adrp x1</span><span>,</span><span>     str_AppDelegate@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L64" data-line-number="64"></td>
          <td id="file-yellow-asm-LC64"><span>    </span><span>add</span><span>  x1</span><span>,</span><span> x1</span><span>,</span><span> str_AppDelegate@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L65" data-line-number="65"></td>
          <td id="file-yellow-asm-LC65"><span>    </span><span>mov</span><span> x2</span><span>,</span><span> </span><span>0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L66" data-line-number="66"></td>
          <td id="file-yellow-asm-LC66"><span>    </span><span>bl</span><span> _objc_allocateClassPair</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L67" data-line-number="67"></td>
          <td id="file-yellow-asm-LC67">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L68" data-line-number="68"></td>
          <td id="file-yellow-asm-LC68"><span>    ; save the class since we'll clobber x0 several times</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L69" data-line-number="69"></td>
          <td id="file-yellow-asm-LC69"><span>    </span><span>mov</span><span> x20</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L70" data-line-number="70"></td>
          <td id="file-yellow-asm-LC70">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L71" data-line-number="71"></td>
          <td id="file-yellow-asm-LC71"><span>    ; class_addProtocol(c, objc_getProtocol("UIApplicationDelegate"));</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L72" data-line-number="72"></td>
          <td id="file-yellow-asm-LC72"><span>    adrp x0</span><span>,</span><span>     str_UIAppDelegate@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L73" data-line-number="73"></td>
          <td id="file-yellow-asm-LC73"><span>    </span><span>add</span><span>  x0</span><span>,</span><span> x0</span><span>,</span><span> str_UIAppDelegate@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L74" data-line-number="74"></td>
          <td id="file-yellow-asm-LC74"><span>    </span><span>bl</span><span> _objc_getProtocol</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L75" data-line-number="75"></td>
          <td id="file-yellow-asm-LC75"><span>    </span><span>mov</span><span> x1</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L76" data-line-number="76"></td>
          <td id="file-yellow-asm-LC76"><span>    </span><span>mov</span><span> x0</span><span>,</span><span> x20</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L77" data-line-number="77"></td>
          <td id="file-yellow-asm-LC77"><span>    </span><span>bl</span><span> _class_addProtocol</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L78" data-line-number="78"></td>
          <td id="file-yellow-asm-LC78">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L79" data-line-number="79"></td>
          <td id="file-yellow-asm-LC79"><span>    ; class_addMethod(c, S("application:didFinishLaunchingWithOptions:"), didFinishLaunching, "B@:@@");</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L80" data-line-number="80"></td>
          <td id="file-yellow-asm-LC80">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L81" data-line-number="81"></td>
          <td id="file-yellow-asm-LC81"><span>    adrp x0</span><span>,</span><span>     str_didFinishLaunchingSel@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L82" data-line-number="82"></td>
          <td id="file-yellow-asm-LC82"><span>    </span><span>add</span><span>  x0</span><span>,</span><span> x0</span><span>,</span><span> str_didFinishLaunchingSel@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L83" data-line-number="83"></td>
          <td id="file-yellow-asm-LC83"><span>    </span><span>bl</span><span> _sel_getUid</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L84" data-line-number="84"></td>
          <td id="file-yellow-asm-LC84"><span>    </span><span>mov</span><span> x1</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L85" data-line-number="85"></td>
          <td id="file-yellow-asm-LC85"><span>    </span><span>mov</span><span> x0</span><span>,</span><span> x20</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L86" data-line-number="86"></td>
          <td id="file-yellow-asm-LC86"><span>    adr x2</span><span>,</span><span> didFinishLaunching</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L87" data-line-number="87"></td>
          <td id="file-yellow-asm-LC87"><span>    adrp x3</span><span>,</span><span> str_typestr@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L88" data-line-number="88"></td>
          <td id="file-yellow-asm-LC88"><span>    </span><span>add</span><span>  x3</span><span>,</span><span> x3</span><span>,</span><span> str_typestr@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L89" data-line-number="89"></td>
          <td id="file-yellow-asm-LC89"><span>    </span><span>bl</span><span> _class_addMethod</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L90" data-line-number="90"></td>
          <td id="file-yellow-asm-LC90"><span>    </span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L91" data-line-number="91"></td>
          <td id="file-yellow-asm-LC91"><span>    ; objc_registerClassPair(c);</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L92" data-line-number="92"></td>
          <td id="file-yellow-asm-LC92"><span>    </span><span>mov</span><span> x0</span><span>,</span><span> x20</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L93" data-line-number="93"></td>
          <td id="file-yellow-asm-LC93"><span>    </span><span>bl</span><span> _objc_registerClassPair</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L94" data-line-number="94"></td>
          <td id="file-yellow-asm-LC94">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L95" data-line-number="95"></td>
          <td id="file-yellow-asm-LC95"><span>    ; epilog</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L96" data-line-number="96"></td>
          <td id="file-yellow-asm-LC96"><span>    ldr x20</span><span>,</span><span> </span><span>[</span><span>sp</span><span>,</span><span> #</span><span>0x10</span><span>]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L97" data-line-number="97"></td>
          <td id="file-yellow-asm-LC97"><span>    ldp x29</span><span>,</span><span> x30</span><span>,</span><span> </span><span>[</span><span>sp</span><span>],</span><span> #</span><span>0x20</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L98" data-line-number="98"></td>
          <td id="file-yellow-asm-LC98"><span>    </span><span>ret</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L99" data-line-number="99"></td>
          <td id="file-yellow-asm-LC99">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L100" data-line-number="100"></td>
          <td id="file-yellow-asm-LC100"><span>; parameters:</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L101" data-line-number="101"></td>
          <td id="file-yellow-asm-LC101"><span>; x0: self</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L102" data-line-number="102"></td>
          <td id="file-yellow-asm-LC102"><span>; x1: _sel</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L103" data-line-number="103"></td>
          <td id="file-yellow-asm-LC103"><span>; x2: application</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L104" data-line-number="104"></td>
          <td id="file-yellow-asm-LC104"><span>; x3: launchOptions</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L105" data-line-number="105"></td>
          <td id="file-yellow-asm-LC105"><span>didFinishLaunching:</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L106" data-line-number="106"></td>
          <td id="file-yellow-asm-LC106"><span>    ; prolog, save fp, lr, x19-x22</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L107" data-line-number="107"></td>
          <td id="file-yellow-asm-LC107"><span>    stp x29</span><span>,</span><span> x30</span><span>,</span><span> </span><span>[</span><span>sp</span><span>,</span><span> #</span><span>-</span><span>0x30</span><span>]</span><span>!</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L108" data-line-number="108"></td>
          <td id="file-yellow-asm-LC108"><span>    stp x19</span><span>,</span><span> x20</span><span>,</span><span> </span><span>[</span><span>sp</span><span>,</span><span> #</span><span>0x10</span><span>]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L109" data-line-number="109"></td>
          <td id="file-yellow-asm-LC109"><span>    stp x21</span><span>,</span><span> x22</span><span>,</span><span> </span><span>[</span><span>sp</span><span>,</span><span> #</span><span>0x20</span><span>]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L110" data-line-number="110"></td>
          <td id="file-yellow-asm-LC110"><span>    </span><span>mov</span><span> x29</span><span>,</span><span> </span><span>sp</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L111" data-line-number="111"></td>
          <td id="file-yellow-asm-LC111"><span>    </span><span>sub</span><span> </span><span>sp</span><span>,</span><span> </span><span>sp</span><span>,</span><span> </span><span>0x20</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L112" data-line-number="112"></td>
          <td id="file-yellow-asm-LC112">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L113" data-line-number="113"></td>
          <td id="file-yellow-asm-LC113"><span>    ; x19 = @selector(mainScreen)</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L114" data-line-number="114"></td>
          <td id="file-yellow-asm-LC114"><span>    adrp x0</span><span>,</span><span> str_mainScreen@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L115" data-line-number="115"></td>
          <td id="file-yellow-asm-LC115"><span>    </span><span>add</span><span>  x0</span><span>,</span><span> x0</span><span>,</span><span> str_mainScreen@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L116" data-line-number="116"></td>
          <td id="file-yellow-asm-LC116"><span>    </span><span>bl</span><span> _sel_getUid</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L117" data-line-number="117"></td>
          <td id="file-yellow-asm-LC117"><span>    </span><span>mov</span><span> x19</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L118" data-line-number="118"></td>
          <td id="file-yellow-asm-LC118">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L119" data-line-number="119"></td>
          <td id="file-yellow-asm-LC119"><span>    ; objc_getClass("UIScreen")</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L120" data-line-number="120"></td>
          <td id="file-yellow-asm-LC120"><span>    adrp x0</span><span>,</span><span> str_UIScreen@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L121" data-line-number="121"></td>
          <td id="file-yellow-asm-LC121"><span>    </span><span>add</span><span>  x0</span><span>,</span><span> x0</span><span>,</span><span> str_UIScreen@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L122" data-line-number="122"></td>
          <td id="file-yellow-asm-LC122"><span>    </span><span>bl</span><span> _objc_getClass</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L123" data-line-number="123"></td>
          <td id="file-yellow-asm-LC123">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L124" data-line-number="124"></td>
          <td id="file-yellow-asm-LC124"><span>    ; x20 = [UIScreen mainScreen]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L125" data-line-number="125"></td>
          <td id="file-yellow-asm-LC125"><span>    </span><span>mov</span><span> x1</span><span>,</span><span> x19</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L126" data-line-number="126"></td>
          <td id="file-yellow-asm-LC126"><span>    </span><span>bl</span><span> _objc_msgSend</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L127" data-line-number="127"></td>
          <td id="file-yellow-asm-LC127"><span>    </span><span>mov</span><span> x20</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L128" data-line-number="128"></td>
          <td id="file-yellow-asm-LC128"><span>    ; x19 is now free</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L129" data-line-number="129"></td>
          <td id="file-yellow-asm-LC129">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L130" data-line-number="130"></td>
          <td id="file-yellow-asm-LC130"><span>    ; x1 = @selector(bounds)</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L131" data-line-number="131"></td>
          <td id="file-yellow-asm-LC131"><span>    adrp x0</span><span>,</span><span> str_bounds@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L132" data-line-number="132"></td>
          <td id="file-yellow-asm-LC132"><span>    </span><span>add</span><span> x0</span><span>,</span><span> x0</span><span>,</span><span> str_bounds@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L133" data-line-number="133"></td>
          <td id="file-yellow-asm-LC133"><span>    </span><span>bl</span><span> _sel_getUid</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L134" data-line-number="134"></td>
          <td id="file-yellow-asm-LC134"><span>    </span><span>mov</span><span> x1</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L135" data-line-number="135"></td>
          <td id="file-yellow-asm-LC135">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L136" data-line-number="136"></td>
          <td id="file-yellow-asm-LC136"><span>    ; [x20 bounds]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L137" data-line-number="137"></td>
          <td id="file-yellow-asm-LC137"><span>    </span><span>mov</span><span> x0</span><span>,</span><span> x20</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L138" data-line-number="138"></td>
          <td id="file-yellow-asm-LC138"><span>    </span><span>bl</span><span> _objc_msgSend</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L139" data-line-number="139"></td>
          <td id="file-yellow-asm-LC139">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L140" data-line-number="140"></td>
          <td id="file-yellow-asm-LC140"><span>    stp d0</span><span>,</span><span> d1</span><span>,</span><span> </span><span>[</span><span>sp</span><span>]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L141" data-line-number="141"></td>
          <td id="file-yellow-asm-LC141"><span>    stp d2</span><span>,</span><span> d3</span><span>,</span><span> </span><span>[</span><span>sp</span><span>,</span><span> #</span><span>0x10</span><span>]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L142" data-line-number="142"></td>
          <td id="file-yellow-asm-LC142">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L143" data-line-number="143"></td>
          <td id="file-yellow-asm-LC143"><span>    ; x19 = @selector(initWithFrame:)</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L144" data-line-number="144"></td>
          <td id="file-yellow-asm-LC144"><span>    adrp x0</span><span>,</span><span> str_initWithFrame@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L145" data-line-number="145"></td>
          <td id="file-yellow-asm-LC145"><span>    </span><span>add</span><span> x0</span><span>,</span><span> x0</span><span>,</span><span> str_initWithFrame@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L146" data-line-number="146"></td>
          <td id="file-yellow-asm-LC146"><span>    </span><span>bl</span><span> _sel_getUid</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L147" data-line-number="147"></td>
          <td id="file-yellow-asm-LC147"><span>    </span><span>mov</span><span> x19</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L148" data-line-number="148"></td>
          <td id="file-yellow-asm-LC148">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L149" data-line-number="149"></td>
          <td id="file-yellow-asm-LC149"><span>    ; x0 = UIWindow</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L150" data-line-number="150"></td>
          <td id="file-yellow-asm-LC150"><span>    adrp x0</span><span>,</span><span> str_UIWindow@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L151" data-line-number="151"></td>
          <td id="file-yellow-asm-LC151"><span>    </span><span>add</span><span>  x0</span><span>,</span><span> x0</span><span>,</span><span> str_UIWindow@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L152" data-line-number="152"></td>
          <td id="file-yellow-asm-LC152"><span>    </span><span>bl</span><span> _objc_getClass</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L153" data-line-number="153"></td>
          <td id="file-yellow-asm-LC153">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L154" data-line-number="154"></td>
          <td id="file-yellow-asm-LC154"><span>    ; x0 = class_createInstance(x0)</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L155" data-line-number="155"></td>
          <td id="file-yellow-asm-LC155"><span>    </span><span>mov</span><span> x1</span><span>,</span><span> #</span><span>0x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L156" data-line-number="156"></td>
          <td id="file-yellow-asm-LC156"><span>    </span><span>bl</span><span> _class_createInstance</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L157" data-line-number="157"></td>
          <td id="file-yellow-asm-LC157"><span>    ; x0 now has the instance</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L158" data-line-number="158"></td>
          <td id="file-yellow-asm-LC158"><span>    </span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L159" data-line-number="159"></td>
          <td id="file-yellow-asm-LC159"><span>    ; x20 = [x0 initWithFrame:d]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L160" data-line-number="160"></td>
          <td id="file-yellow-asm-LC160"><span>    </span><span>mov</span><span> x1</span><span>,</span><span> x19</span><span> ;initWithFrame</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L161" data-line-number="161"></td>
          <td id="file-yellow-asm-LC161"><span>    ldp d0</span><span>,</span><span> d1</span><span>,</span><span> </span><span>[</span><span>sp</span><span>]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L162" data-line-number="162"></td>
          <td id="file-yellow-asm-LC162"><span>    ldp d2</span><span>,</span><span> d3</span><span>,</span><span> </span><span>[</span><span>sp</span><span>,</span><span> #</span><span>0x10</span><span>]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L163" data-line-number="163"></td>
          <td id="file-yellow-asm-LC163"><span>    </span><span>bl</span><span> _objc_msgSend</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L164" data-line-number="164"></td>
          <td id="file-yellow-asm-LC164"><span>    </span><span>mov</span><span> x20</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L165" data-line-number="165"></td>
          <td id="file-yellow-asm-LC165">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L166" data-line-number="166"></td>
          <td id="file-yellow-asm-LC166"><span>    ; x19 = @selector(init)</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L167" data-line-number="167"></td>
          <td id="file-yellow-asm-LC167"><span>    adrp x0</span><span>,</span><span> str_init@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L168" data-line-number="168"></td>
          <td id="file-yellow-asm-LC168"><span>    </span><span>add</span><span> x0</span><span>,</span><span> x0</span><span>,</span><span> str_init@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L169" data-line-number="169"></td>
          <td id="file-yellow-asm-LC169"><span>    </span><span>bl</span><span> _sel_getUid</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L170" data-line-number="170"></td>
          <td id="file-yellow-asm-LC170"><span>    </span><span>mov</span><span> x19</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L171" data-line-number="171"></td>
          <td id="file-yellow-asm-LC171">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L172" data-line-number="172"></td>
          <td id="file-yellow-asm-LC172"><span>    ; x0 = UIViewController</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L173" data-line-number="173"></td>
          <td id="file-yellow-asm-LC173"><span>    adrp x0</span><span>,</span><span> str_UIViewController@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L174" data-line-number="174"></td>
          <td id="file-yellow-asm-LC174"><span>    </span><span>add</span><span>  x0</span><span>,</span><span> x0</span><span>,</span><span> str_UIViewController@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L175" data-line-number="175"></td>
          <td id="file-yellow-asm-LC175"><span>    </span><span>bl</span><span> _objc_getClass</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L176" data-line-number="176"></td>
          <td id="file-yellow-asm-LC176">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L177" data-line-number="177"></td>
          <td id="file-yellow-asm-LC177"><span>    ; x0 = class_createInstance(UIViewController)</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L178" data-line-number="178"></td>
          <td id="file-yellow-asm-LC178"><span>    </span><span>mov</span><span> x1</span><span>,</span><span> #</span><span>0x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L179" data-line-number="179"></td>
          <td id="file-yellow-asm-LC179"><span>    </span><span>bl</span><span> _class_createInstance</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L180" data-line-number="180"></td>
          <td id="file-yellow-asm-LC180"><span>    ; x0 now has the instance</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L181" data-line-number="181"></td>
          <td id="file-yellow-asm-LC181">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L182" data-line-number="182"></td>
          <td id="file-yellow-asm-LC182"><span>    ; x21 = [x0 init]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L183" data-line-number="183"></td>
          <td id="file-yellow-asm-LC183"><span>    </span><span>mov</span><span> x1</span><span>,</span><span> x19</span><span> ;init</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L184" data-line-number="184"></td>
          <td id="file-yellow-asm-LC184"><span>    </span><span>bl</span><span> _objc_msgSend</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L185" data-line-number="185"></td>
          <td id="file-yellow-asm-LC185"><span>    </span><span>mov</span><span> x21</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L186" data-line-number="186"></td>
          <td id="file-yellow-asm-LC186">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L187" data-line-number="187"></td>
          <td id="file-yellow-asm-LC187"><span>    ; x19 = @selector(yellowColor)</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L188" data-line-number="188"></td>
          <td id="file-yellow-asm-LC188"><span>    adrp x0</span><span>,</span><span> str_yellowColor@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L189" data-line-number="189"></td>
          <td id="file-yellow-asm-LC189"><span>    </span><span>add</span><span>  x0</span><span>,</span><span> x0</span><span>,</span><span> str_yellowColor@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L190" data-line-number="190"></td>
          <td id="file-yellow-asm-LC190"><span>    </span><span>bl</span><span> _sel_getUid</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L191" data-line-number="191"></td>
          <td id="file-yellow-asm-LC191"><span>    </span><span>mov</span><span> x19</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L192" data-line-number="192"></td>
          <td id="file-yellow-asm-LC192">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L193" data-line-number="193"></td>
          <td id="file-yellow-asm-LC193"><span>    ; x22 = [UIColor yellowColor]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L194" data-line-number="194"></td>
          <td id="file-yellow-asm-LC194"><span>    adrp x0</span><span>,</span><span> str_UIColor@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L195" data-line-number="195"></td>
          <td id="file-yellow-asm-LC195"><span>    </span><span>add</span><span>  x0</span><span>,</span><span> x0</span><span>,</span><span> str_UIColor@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L196" data-line-number="196"></td>
          <td id="file-yellow-asm-LC196"><span>    </span><span>bl</span><span> _objc_getClass</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L197" data-line-number="197"></td>
          <td id="file-yellow-asm-LC197"><span>    </span><span>mov</span><span> x1</span><span>,</span><span> x19</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L198" data-line-number="198"></td>
          <td id="file-yellow-asm-LC198"><span>    </span><span>bl</span><span> _objc_msgSend</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L199" data-line-number="199"></td>
          <td id="file-yellow-asm-LC199"><span>    </span><span>mov</span><span> x22</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L200" data-line-number="200"></td>
          <td id="file-yellow-asm-LC200">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L201" data-line-number="201"></td>
          <td id="file-yellow-asm-LC201"><span>    ; x19 = @selector(setBackgroundColor:)</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L202" data-line-number="202"></td>
          <td id="file-yellow-asm-LC202"><span>    adrp x0</span><span>,</span><span> str_setBackgroundColor@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L203" data-line-number="203"></td>
          <td id="file-yellow-asm-LC203"><span>    </span><span>add</span><span>  x0</span><span>,</span><span> x0</span><span>,</span><span> str_setBackgroundColor@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L204" data-line-number="204"></td>
          <td id="file-yellow-asm-LC204"><span>    </span><span>bl</span><span> _sel_getUid</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L205" data-line-number="205"></td>
          <td id="file-yellow-asm-LC205"><span>    </span><span>mov</span><span> x19</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L206" data-line-number="206"></td>
          <td id="file-yellow-asm-LC206">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L207" data-line-number="207"></td>
          <td id="file-yellow-asm-LC207"><span>    ; x1 = @selector(view)</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L208" data-line-number="208"></td>
          <td id="file-yellow-asm-LC208"><span>    adrp x0</span><span>,</span><span> str_view@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L209" data-line-number="209"></td>
          <td id="file-yellow-asm-LC209"><span>    </span><span>add</span><span>  x0</span><span>,</span><span> x0</span><span>,</span><span> str_view@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L210" data-line-number="210"></td>
          <td id="file-yellow-asm-LC210"><span>    </span><span>bl</span><span> _sel_getUid</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L211" data-line-number="211"></td>
          <td id="file-yellow-asm-LC211"><span>    </span><span>mov</span><span> x1</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L212" data-line-number="212"></td>
          <td id="file-yellow-asm-LC212">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L213" data-line-number="213"></td>
          <td id="file-yellow-asm-LC213"><span>    ; x0 = [[controller view] setBackgroundColor: x22];</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L214" data-line-number="214"></td>
          <td id="file-yellow-asm-LC214"><span>    </span><span>mov</span><span> x0</span><span>,</span><span> x21</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L215" data-line-number="215"></td>
          <td id="file-yellow-asm-LC215"><span>    </span><span>bl</span><span> _objc_msgSend</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L216" data-line-number="216"></td>
          <td id="file-yellow-asm-LC216"><span>    </span><span>mov</span><span> x1</span><span>,</span><span> x19</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L217" data-line-number="217"></td>
          <td id="file-yellow-asm-LC217"><span>    </span><span>mov</span><span> x2</span><span>,</span><span> x22</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L218" data-line-number="218"></td>
          <td id="file-yellow-asm-LC218"><span>    </span><span>bl</span><span> _objc_msgSend</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L219" data-line-number="219"></td>
          <td id="file-yellow-asm-LC219">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L220" data-line-number="220"></td>
          <td id="file-yellow-asm-LC220"><span>    adrp x0</span><span>,</span><span> str_setRoot@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L221" data-line-number="221"></td>
          <td id="file-yellow-asm-LC221"><span>    </span><span>add</span><span>  x0</span><span>,</span><span> x0</span><span>,</span><span> str_setRoot@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L222" data-line-number="222"></td>
          <td id="file-yellow-asm-LC222"><span>    </span><span>bl</span><span> _sel_getUid</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L223" data-line-number="223"></td>
          <td id="file-yellow-asm-LC223"><span>    </span><span>mov</span><span> x1</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L224" data-line-number="224"></td>
          <td id="file-yellow-asm-LC224">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L225" data-line-number="225"></td>
          <td id="file-yellow-asm-LC225"><span>    ; [window setRootViewController:viewController]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L226" data-line-number="226"></td>
          <td id="file-yellow-asm-LC226"><span>    </span><span>mov</span><span> x0</span><span>,</span><span> x20</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L227" data-line-number="227"></td>
          <td id="file-yellow-asm-LC227"><span>    </span><span>mov</span><span> x2</span><span>,</span><span> x21</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L228" data-line-number="228"></td>
          <td id="file-yellow-asm-LC228"><span>    </span><span>bl</span><span> _objc_msgSend</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L229" data-line-number="229"></td>
          <td id="file-yellow-asm-LC229">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L230" data-line-number="230"></td>
          <td id="file-yellow-asm-LC230"><span>    ; [x20 makeKeyAndVisible]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L231" data-line-number="231"></td>
          <td id="file-yellow-asm-LC231"><span>    adrp x0</span><span>,</span><span> str_makeKeyAndVisible@PAGE</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L232" data-line-number="232"></td>
          <td id="file-yellow-asm-LC232"><span>    </span><span>add</span><span> x0</span><span>,</span><span> x0</span><span>,</span><span> str_makeKeyAndVisible@PAGEOFF</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L233" data-line-number="233"></td>
          <td id="file-yellow-asm-LC233"><span>    </span><span>bl</span><span> _sel_getUid</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L234" data-line-number="234"></td>
          <td id="file-yellow-asm-LC234">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L235" data-line-number="235"></td>
          <td id="file-yellow-asm-LC235"><span>    </span><span>mov</span><span> x1</span><span>,</span><span> x0</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L236" data-line-number="236"></td>
          <td id="file-yellow-asm-LC236"><span>    </span><span>mov</span><span> x0</span><span>,</span><span> x20</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L237" data-line-number="237"></td>
          <td id="file-yellow-asm-LC237"><span>    </span><span>bl</span><span> _objc_msgSend</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L238" data-line-number="238"></td>
          <td id="file-yellow-asm-LC238">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L239" data-line-number="239"></td>
          <td id="file-yellow-asm-LC239"><span>    ; return YES</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L240" data-line-number="240"></td>
          <td id="file-yellow-asm-LC240"><span>    </span><span>mov</span><span> x0</span><span>,</span><span> #</span><span>0x1</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L241" data-line-number="241"></td>
          <td id="file-yellow-asm-LC241">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L242" data-line-number="242"></td>
          <td id="file-yellow-asm-LC242"><span>    ; epilog</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L243" data-line-number="243"></td>
          <td id="file-yellow-asm-LC243"><span>    </span><span>add</span><span> </span><span>sp</span><span>,</span><span> </span><span>sp</span><span>,</span><span> </span><span>0x20</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L244" data-line-number="244"></td>
          <td id="file-yellow-asm-LC244"><span>    ldp    x19</span><span>,</span><span> x20</span><span>,</span><span> </span><span>[</span><span>sp</span><span>,</span><span> #</span><span>0x10</span><span>]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L245" data-line-number="245"></td>
          <td id="file-yellow-asm-LC245"><span>    ldp    x21</span><span>,</span><span> x22</span><span>,</span><span> </span><span>[</span><span>sp</span><span>,</span><span> #</span><span>0x20</span><span>]</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L246" data-line-number="246"></td>
          <td id="file-yellow-asm-LC246"><span>    ldp    x29</span><span>,</span><span> x30</span><span>,</span><span> </span><span>[</span><span>sp</span><span>],</span><span> #</span><span>0x30</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L247" data-line-number="247"></td>
          <td id="file-yellow-asm-LC247"><span>    </span><span>ret</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L248" data-line-number="248"></td>
          <td id="file-yellow-asm-LC248">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L249" data-line-number="249"></td>
          <td id="file-yellow-asm-LC249"><span>.data</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L250" data-line-number="250"></td>
          <td id="file-yellow-asm-LC250"><span>str_NSObject:               .asciz </span><span>"NSObject"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L251" data-line-number="251"></td>
          <td id="file-yellow-asm-LC251"><span>str_AppDelegate:            .asciz </span><span>"AppDelegate"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L252" data-line-number="252"></td>
          <td id="file-yellow-asm-LC252"><span>str_UIAppDelegate:          .asciz </span><span>"UIApplicationDelegate"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L253" data-line-number="253"></td>
          <td id="file-yellow-asm-LC253"><span>str_UIScreen:               .asciz </span><span>"UIScreen"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L254" data-line-number="254"></td>
          <td id="file-yellow-asm-LC254"><span>str_UIWindow:               .asciz </span><span>"UIWindow"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L255" data-line-number="255"></td>
          <td id="file-yellow-asm-LC255"><span>str_UIViewController:       .asciz </span><span>"UIViewController"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L256" data-line-number="256"></td>
          <td id="file-yellow-asm-LC256"><span>str_UIColor:                .asciz </span><span>"UIColor"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L257" data-line-number="257"></td>
          <td id="file-yellow-asm-LC257">
</td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L258" data-line-number="258"></td>
          <td id="file-yellow-asm-LC258"><span>str_typestr:                .asciz </span><span>"B@:@@"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L259" data-line-number="259"></td>
          <td id="file-yellow-asm-LC259"><span>str_didFinishLaunchingSel:  .asciz </span><span>"application:didFinishLaunchingWithOptions:"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L260" data-line-number="260"></td>
          <td id="file-yellow-asm-LC260"><span>str_mainScreen:             .asciz </span><span>"mainScreen"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L261" data-line-number="261"></td>
          <td id="file-yellow-asm-LC261"><span>str_bounds:                 .asciz </span><span>"bounds"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L262" data-line-number="262"></td>
          <td id="file-yellow-asm-LC262"><span>str_initWithFrame:          .asciz </span><span>"initWithFrame:"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L263" data-line-number="263"></td>
          <td id="file-yellow-asm-LC263"><span>str_makeKeyAndVisible:      .asciz </span><span>"makeKeyAndVisible"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L264" data-line-number="264"></td>
          <td id="file-yellow-asm-LC264"><span>str_init:                   .asciz </span><span>"init"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L265" data-line-number="265"></td>
          <td id="file-yellow-asm-LC265"><span>str_view:                   .asciz </span><span>"view"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L266" data-line-number="266"></td>
          <td id="file-yellow-asm-LC266"><span>str_setBackgroundColor:     .asciz </span><span>"setBackgroundColor:"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L267" data-line-number="267"></td>
          <td id="file-yellow-asm-LC267"><span>str_yellowColor:            .asciz </span><span>"yellowColor"</span></td>
        </tr>
        <tr>
          <td id="file-yellow-asm-L268" data-line-number="268"></td>
          <td id="file-yellow-asm-LC268"><span>str_setRoot:                .asciz </span><span>"setRootViewController:"</span></td>
        </tr>
  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IRCd service (2024) (105 pts)]]></title>
            <link>https://example.fi/blog/ircd.html</link>
            <guid>45755788</guid>
            <pubDate>Thu, 30 Oct 2025 02:31:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://example.fi/blog/ircd.html">https://example.fi/blog/ircd.html</a>, See on <a href="https://news.ycombinator.com/item?id=45755788">Hacker News</a></p>
<div id="readability-page-1" class="page">
<h2>IRCd service</h2>
<h2>10.6.2024</h2>
Ok. We have IRC at example.fi<p>

Internet Relay Chat (IRC) is a form of real-time text communication developed by Jarkko Oikarinen in 1988. Initially created to replace a local BBS system at the University of Oulu in Finland, IRC quickly gained global popularity, becoming a foundational technology for online chat communities and influencing the development of modern instant messaging and social media platforms. Its significance lies in its pioneering role in connecting people across the internet, fostering early online communities, and setting the stage for contemporary digital communication.
</p><p>
To commemorate this pivotal technology, example.fi provides a simple and limited IRC server. This server is uniquely written in AWK, a scripting language traditionally used for text processing, highlighting the adaptability and enduring legacy of IRC. This creative implementation serves as both an educational tool and a tribute to the foundational role of IRC in the evolution of online communication.
</p><p>
In the following picture you see Irssi in the background and Hexchat on top of it:</p><p>
<img src="https://example.fi/blog/example-irc.png"></p><p>

Note: if you plan to connect to example.fi, make sure you do not use any fancy features. In irssi, use -nocap option. In Windows, use for example hexchat.
As this is written in gawk, most IRC protocol features are not implemented. This includes, for example, channel and user listings, topics, the concept of "operator" etc.
Technical fun fact: Total code count is around 60 lines of awk and a few lines of bash.
</p><pre>$ telnet example.fi ircd
Trying 65.108.91.190...
Connected to example.fi.
Escape character is '^]'.
USER foo
NICK bar
:example.fi 001 bar :Welcome to Internet Relay Network bar!~foo@65.108.91.190
:example.fi 375 test :- example.fi Message of the day -
:example.fi 372 test :- Current time is @787.188.beats
:example.fi 376 test :End of MOTD command.
Connection closed by foreign host.
</pre>


<p>
Don't worry, we'll publish the code when it's "ready" :)</p><p>This site is HTML 2.0 compliant.</p>
<a href="http://validator.w3.org/check?uri=example.fi/blog/ircd.html"><img src="https://example.fi/blog/valid-html20.png" alt="Valid HTML 2.0"></a>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Crunchyroll is destroying its subtitles for no good reason (422 pts)]]></title>
            <link>https://daiz.moe/crunchyroll-is-destroying-its-subtitles-for-no-good-reason/</link>
            <guid>45754509</guid>
            <pubDate>Wed, 29 Oct 2025 23:31:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daiz.moe/crunchyroll-is-destroying-its-subtitles-for-no-good-reason/">https://daiz.moe/crunchyroll-is-destroying-its-subtitles-for-no-good-reason/</a>, See on <a href="https://news.ycombinator.com/item?id=45754509">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <article>   <p>Since the beginning of the Fall 2025 anime season, a major change has started taking place at the anime streaming service <a href="https://en.wikipedia.org/wiki/Crunchyroll" rel="noopener" target="_blank">Crunchyroll</a>: <strong>the presentation quality for translations of on-screen text has taken a total nosedive</strong> compared to what has been on offer for many years, all the way up until the previous Summer 2025 season. Now, more and more subtitles on Crunchyroll are looking like this:</p>

<p>Poor presentation quality like this isn’t entirely new to Crunchyroll, as a portion of the subtitles on the site have always been of third-party origin — that is, provided by the licensor — and Crunchyroll just puts them up with zero oversight. This in itself has caused <a href="https://www.animenewsnetwork.com/news/2025-07-01/crunchyroll-german-necronomico-and-the-cosmic-horror-show-subtitles-listed-chatgpt/.226206" rel="noopener" target="_blank">numerous</a> <a href="https://www.animenewsnetwork.com/news/2023-10-05/the-yuzuki-family-four-sons-episode-1-briefly-inaccessible-on-crunchyroll-after-subtitle-quality-/.203183" rel="noopener" target="_blank">issues</a> over the years, but the pressing issue here is that <strong>low quality presentation like this can now be found even in first-party subtitles created by Crunchyroll’s own subtitling staff.</strong> For comparison, here’s the kind of presentation quality that first-party subtitles were providing just earlier this year:</p>

<p>Given the technical capabilities on display in the above screenshots, it should be clear that <strong>first-party subtitles for Fall 2025 shows shouldn’t look as bad as they do.</strong> Yet for some reason, what we’re getting is this low quality presentation reminiscent of third-party subtitles, where translations for dialogue and on-screen text aren’t even separated to different sides of the screen – everything is just bunched up together at either the top or the bottom. Lots of on-screen text is even left straight up untranslated.</p>

<h2 id="and-thats-destroying-subtitles">And that’s “destroying subtitles”?</h2>
<p>It sure is when it’s anime we’re talking about! <strong>Anime as a medium has made prominent use of on-screen text basically since its inception.</strong> The amount of it varies from series to series, but almost every anime out there makes use of on-screen text at one point or another, with some featuring downright ridiculous amounts of <b>signs</b> (what on-screen text is called for short). With all this on-screen text, it is also very common for there to be text visible on the screen potentially in multiple positions, even when characters are speaking.</p>
<p>As such, <strong>if you are in the business of localizing anime for non-Japanese audiences, you need to be able to deal with on-screen text.</strong> At bare minimum, when subtitling anime, you should be able to do <b>overlaps</b> (multiple lines of text on the screen at the same time) and <b>positioning</b> (the ability to freely place subtitles anywhere on the screen). Anything less and you are likely to run into trouble the moment you get to something as simple as a next episode preview:</p>
<figure id="typesetting-introduction" data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/saki-s3-fansub-1.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/saki-s3-fansub-1.jpg" alt="A screenshot from a next episode preview. It features a static sign saying 'Next episode preview' on top left, a similar sign for the title of the next episode on the bottom right, and you have dialogue running through the whole preview while these signs are visible, meaning you need at least three overlaps and positioning to handle it gracefully." loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Multiple instances of on-screen text are running in parallel with dialogue. Screenshot from <em>Saki: The Nationals</em> (Winter&nbsp;2014, Underwater-FFF fansubs)</p></figcaption>  </figure>
<p>Overlaps and positioning are really just the bare necessities for dealing with on-screen text in anime though – ideally, you should also be able to use different fonts, colors, animate text in various ways, etc. Making use of all these possibilities is an art unto itself, and <strong>this art of on-screen text localization is commonly referred to as typesetting</strong>. Typesetting is important even when dubbing anime, as all that on-screen text is going to be there in the video all the same!</p>
<h2 id="so-why-would-crunchyroll-get-rid-of-typesetting">So why would Crunchyroll get rid of typesetting?</h2>
<p>That is a good question. It is no exaggeration to say that up to this point, Crunchyroll with its typesetting was the unambiguous market leader when it came to presentation quality for official anime subtitles… though for the most part, <strong>other services dealing in anime have never even bothered to try.</strong> Sentai Filmworks’ <a href="https://en.wikipedia.org/wiki/Hidive" rel="noopener" target="_blank">Hidive</a> is just about the only other anime service that even attempts to do typesetting, though they license so few shows per season that they are a tiny player compared to the Big Boys of anime streaming.</p>


<p>And it is very likely the existence of these Big Boys that has played a key part in Crunchyroll’s eradication of its typesetting. <a href="https://en.wikipedia.org/wiki/Netflix" rel="noopener" target="_blank">Netflix</a> and <a href="https://en.wikipedia.org/wiki/Amazon_Prime_Video" rel="noopener" target="_blank">Amazon Prime Video</a> probably need no introduction to anyone reading this – both are very popular general streaming services. <strong>Despite anime being only a minor part of their catalogs, a large chunk of today’s anime watching worldwide happens through said services</strong> thanks to their sheer user counts alone.</p>
<p>Crunchyroll clearly seems to know this, which is why it has been sublicensing its anime properties to both Amazon and Netflix for multiple years at this point. <strong>But with such sublicensing comes the matter of dealing with the subtitling standards of general streaming services.</strong> I’m not going to mince words: these standards are <em>awful,</em> at least as far as anime is concerned. Netflix for example insists that you stick to at most two lines of text on screen at once, which makes sense most of the time… if you’re talking about dialogue alone. Unfortunately, it becomes completely inadequate when dealing with anime’s plentiful on-screen text. Moreover, the standards of these services actively refuse to give you tools like positioning and overlaps, even though the <a href="https://en.wikipedia.org/wiki/Timed_Text_Markup_Language" rel="noopener" target="_blank">TTML subtitle format</a> they use supports said features!</p>

<p>With such typesetting-hostile standards to deal with, Crunchyroll had basically two choices for how to make sublicensing to Amazon and Netflix work with their existing subtitles that feature actual typesetting: Either 1) try to negotiate with the services for permission to make use of more TTML capabilities (that the subtitle renderers of said services should already support!) or 2) <strong>start mangling subtitles with typesetting into something compatible with the awful subtitling standards of the general streaming services.</strong> I am not aware if Crunchyroll ever attempted the former, but I can confirm that it eventually started doing the latter.</p>

<p>Editors among Crunchyroll’s subtitling staff were <strong>given an additional job</strong> to convert finished high quality subtitles with typesetting into limited low quality TTML subtitles without typesetting, compatible with Amazon &amp; Netflix subtitling standards. They got paid extra for the manual effort required by the process.</p>
<figure id="ttml-example" data-astro-cid-rkgwkvbt=""> <video poster="https://daiz.moe/content/crunchyroll/devilman-crybaby-netflix-1-poster.jpg" controls="" preload="metadata" data-astro-cid-rkgwkvbt=""> <source src="https://daiz.moe/content/crunchyroll/devilman-crybaby-netflix-1.mp4" type="video/mp4;codecs=avc1.640028,mp4a.40.2" data-astro-cid-rkgwkvbt=""> <span data-astro-cid-rkgwkvbt="">It appears that your browser does not support playing this video.
      However, you can always <a href="https://daiz.moe/content/crunchyroll/devilman-crybaby-netflix-1.mp4" data-astro-cid-rkgwkvbt="true" rel="noopener" target="_blank">download the video directly.</a></span> </video> <figcaption data-astro-cid-rkgwkvbt="">Overlapping on-screen text and dialogue makes for a miserable anime watching experience with limited TTML subtitles. Video clip from <em>Devilman Crybaby</em> (Winter&nbsp;2018, Netflix)</figcaption>  </figure>
<p>Unfortunately, after a couple years of this kind of manual conversion work, the Crunchyroll leadership seems to have decided that it isn’t enough, and that <strong>Crunchyroll must do away with high quality subtitles with typesetting entirely and only produce low quality TTML subtitles without typesetting from now on.</strong> But if they already had a working process for high quality subtitles at home and low quality TTML subtitles elsewhere, why would they just decide to give that up in order to produce exclusively low quality subtitles? It doesn’t seem to make very much sense, even as a cost-cutting measure. There should be so much value in being able to advertise <em>best viewed on Crunchyroll</em> to potential audiences for long-term growth, right?</p>
<p>To understand <em>how this is happening,</em> we need to look into some relevant history. <strong>Specifically, what happened after Sony bought Crunchyroll and merged it with Funimation,</strong> another US anime distributor that Sony had bought previously. But in order to also understand <em>why this is happening,</em> first we need to look at what both Crunchyroll and Funimation were like before this fateful merger happened, as well as how they approached anime subtitling over the years.</p>

<h2 id="a-short-history-of-crunchyroll-and-its-subtitling-standards">A short history of Crunchyroll and its subtitling standards</h2>
<p>Crunchyroll launched in 2006 as a pirate streaming site focused on East Asian media content, featuring <a href="https://en.wikipedia.org/wiki/Fansub" rel="noopener" target="_blank">fansubbed</a> anime, live action drama, music videos, and so on. There was nothing particularly remarkable about the site back then – as a rule of thumb, pirate streaming sites are always worse quality-wise than if you just directly downloaded the pirated releases they use as a base, and the sites mostly exist to make their admins illicit money through ads, begging for donations, and other shady crap. It is important to note though that <strong>legal anime streaming basically wasn’t a thing at this time.</strong></p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/crunchyroll-in-2007.png" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/crunchyroll-in-2007.png" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Crunchyroll in 2007. The “help out” message at the top is asking for donations.</p></figcaption>  </figure>
<p>Things started to change in 2008, <a href="https://www.animenewsnetwork.com/news/2008-03-21/gonzo-works-to-be-streamed-simultaneously-with-airing" rel="noopener" target="_blank">when the Japanese anime studio Gonzo started experimenting with legal internet distribution</a> for some of its titles. They struck deals with a couple companies for this, which is how Crunchyroll got its first few legitimate licenses. However, all the pirate material remained on the site while this was going on. Also in 2008: Crunchyroll managed to raise 4 million USD in venture capital funding while still operating as a pirate site, <a href="https://www.animenewsnetwork.com/news/2008-03-12/funimation-responds-to-crunchyroll-us$4m-funding" rel="noopener" target="_blank">which drew vocal criticism</a> from existing anime distributors at the time (for obvious reasons).</p>
<p>However, it was likely this exact venture capital funding that enabled Crunchyroll to negotiate a major deal with the Japanese broadcasting company TV Tokyo, which was announced at the start of 2009. <strong>This announcement brought with it the news that Crunchyroll was going full-time legitimate and getting rid of all its pirate content.</strong> With this move, Crunchyroll found itself in a position of having to start producing subtitles of its own (instead of just uploading fansubs) and somehow present said subtitles to its customers.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/aegisub.png" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/aegisub.png" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Aegisub is an advanced subtitling software built by fansubbers, for fansubbers.</p></figcaption>  </figure>
<p id="aegisub-ass-introduction">For the subtitle production part, Crunchyroll managed to strike a deal with a bunch of fansubbers to take on the job. This single decision was a fateful one, as it was the foundation for basically everything that came after – with former fansubbers on the job, the tools of the trade were set according to the standards of fansubbers: the subtitling software of choice was to be <a href="https://aegisub.org/" rel="noopener" target="_blank">Aegisub</a>, and <strong>the subtitle format of choice was to be Aegisub’s native format, Advanced SubStation Alpha, or ASS for short.</strong></p>

<p>ASS is an extremely powerful format in terms of formatting and styling capabilities, and with Aegisub, it is easy to produce ASS subtitles that make use of said capabilities. <strong>However, as a streaming site, Crunchyroll needed to be able to present these ASS subtitles in the browser somehow,</strong> and the only full-fledged ASS renderers that existed were only available in the traditional local media playback environments targeted by fansubbers, which meant that Crunchyroll couldn’t make use of said renderers on the web directly.</p>
<p>Now, there are two main ways to subtitle videos, with opposing pros and cons:</p>
<ul>
<li><b>Hardsubbing</b> – the subtitles are burned into the video itself. <strong>Simple to playback</strong> as you only need to be able to play video, <strong>but inflexible for updates and multiple languages</strong> as you have to recreate your video files over and over again with expensive processing called <b>encoding</b>.</li>
<li><b>Softsubbing</b> – the subtitles exist as their own separate media track that the video player renders on top of the video in realtime during playback, making softsubs <strong>complex to playback, but updates and multiple tracks are very cheap</strong> as you only need to deal with tiny subtitle files while the video files remain unchanged.</li>
</ul>
<p>As such, one way Crunchyroll could have solved the subtitle presentation problem would have been to simply hardsub its ASS subtitles, but despite the challenges it posed, Crunchyroll decided to go with softsubbing instead (which was also the fansub standard at the time). <strong>And so Crunchyroll set out to build its own ASS renderer in Flash,</strong> the primary technology used to play video on the web at the time. Here’s a screenshot of some of the first subtitles ever officially authored by the fully legitimate Crunchyroll, rendered in the current ASS renderer but adhering to the limits of the company’s very first Flash subtitle renderer:</p>
<figure id="cr-first-subs" data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/saki-cr-1.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/saki-cr-1.jpg" alt="A screenshot from a next episode preview. It features a static sign saying 'Next episode preview' on top left, a similar sign for the title of the next episode on the bottom right, and you have dialogue running through the whole preview while these signs are visible, meaning you need at least three overlaps and positioning to handle it gracefully." loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Screenshot from <em>Saki</em> (Spring&nbsp;2009, Crunchyroll)</p></figcaption>  </figure>
<p>As can be seen, even the very first version was already capable of handling both overlaps and positioning. Now, the positioning was limited to the eight edges and the center of the screen, making for just nine possible positions total, but even that was enough to handle the humble next episode preview at the very least. Beyond these, the first version also supported fading animations. It wasn’t much, <strong>but it did cover the bare minimum for dealing with on-screen text in anime.</strong></p>

<p>Over the years, Crunchyroll managed to slowly improve its Flash subtitle renderer to enable the use of more ASS features. Custom colors, multiple fonts, multiple styles, rotation, and full positioning were implemented (albeit in somewhat hacky and unwieldy fashion). This went on until 2018, when Crunchyroll was faced with a major issue: <strong>Flash was seeing rapid decline in use, and web streaming was shifting over to HTML5-based technology.</strong> However, with a custom ASS renderer built in Flash, Crunchyroll couldn’t easily make the change, as it would mean having to essentially rebuild the custom subtitle renderer they had from scratch in HTML5 (as much like in the Flash days, there still were no solutions native to the web available for rendering ASS subtitles).</p>
<p id="cr-libass">However, Crunchyroll managed to come up with a way to solve the problem of moving from Flash to HTML5 with the help of another new web technology called <a href="https://webassembly.org/" rel="noopener" target="_blank">WebAssembly</a>, which allowed developers to take code that wasn’t developed for the web and compile it for use on the web. With WebAssembly, Crunchyroll could take <a href="https://github.com/libass/libass" rel="noopener" target="_blank">libass</a>, one of the few fully-featured ASS renderers out there, and <a href="https://github.com/libass/JavascriptSubtitlesOctopus" rel="noopener" target="_blank">use it for their new HTML5 player</a>. Now, not only did all their old ASS subtitles render nicely in HTML5, but <strong>the possibilities for typesetting at Crunchyroll had taken a huge leap forward.</strong> And the subtitling staff at Crunchyroll was more than happy to make use of this newfound power.</p>
<figure data-astro-cid-rkgwkvbt=""> <video poster="https://daiz.moe/content/crunchyroll/bocchi-cr-1-poster.jpg" controls="" preload="metadata" data-astro-cid-rkgwkvbt=""> <source src="https://daiz.moe/content/crunchyroll/bocchi-cr-1.mp4" type="video/mp4;codecs=avc1.640028,mp4a.40.2" data-astro-cid-rkgwkvbt=""> <span data-astro-cid-rkgwkvbt="">It appears that your browser does not support playing this video.
      However, you can always <a href="https://daiz.moe/content/crunchyroll/bocchi-cr-1.mp4" data-astro-cid-rkgwkvbt="true" rel="noopener" target="_blank">download the video directly.</a></span> </video> <figcaption data-astro-cid-rkgwkvbt="">You couldn’t see typesetting like this on Crunchyroll back in the days of Flash. Video clip from <em>Bocchi the Rock!</em> (Fall&nbsp;2022, Crunchyroll)</figcaption>  </figure>
<p>That said, despite having a technically fully-featured ASS renderer to work with, <strong>there were still limitations.</strong> Code compiled with WebAssembly runs worse compared to its original native counterpart, which limits how heavy the typesetting can be (with the flexible features of ASS, it is very easy to produce typesetting that simply cannot be rendered in realtime even on powerful computers, resulting in notable lag during playback). A commercial service like Crunchyroll will also generally want to keep its content watchable even on lower-end devices, which further reduces how complex any typesetting can be.</p>
<p>And this is the limited but functional standard of typesetting that Crunchyroll users got to enjoy (with first-party subtitles) up until the fateful season of Fall 2025 that prompted the creation of this article.</p>
<p>Before we move to the conclusions for this section, though, it is worth noting that while Crunchyroll currently uses softsubbed ASS subtitles whenever it can, there are platforms and devices (like various TVs) where this kind of ASS rendering simply isn’t possible to do. <strong>Crunchyroll is available on some platforms like this, which means it has been making additional hardsubbed versions of everything on top of the usual softsubbed ones.</strong></p>
<hr>
<p>So, what can we learn from all this? At least one thing is abundantly clear: for most of its existence, <strong>the leadership at Crunchyroll had at least some respect and understanding for anime as a medium.</strong> They understood that it was important to be able to deal with on-screen text in their subtitles, and allocated enough resources to make typesetting possible. The company even managed to improve in this regard over time, albeit very slowly.</p>
<p>That said, anyone familiar with anime fansubs of the 2010s and 2020s probably can’t help but feel disappointed that even the highest effort typesetting from Crunchyroll could only ever be on the level of fansub releases from around 2010 at best. Why 2010 specifically? <strong>Because from 2011 onwards, fansubbers started widely incorporating advanced motion tracking into their typesetting.</strong> Observe an example of such fansub typesetting from over a decade ago, the likes of which has never been seen on Crunchyroll:</p>
<figure data-astro-cid-rkgwkvbt=""> <video poster="https://daiz.moe/content/crunchyroll/klk-underwater-1-poster.jpg" controls="" preload="metadata" data-astro-cid-rkgwkvbt=""> <source src="https://daiz.moe/content/crunchyroll/klk-underwater-1.mp4" type="video/mp4;codecs=avc1.640028,mp4a.40.2" data-astro-cid-rkgwkvbt=""> <span data-astro-cid-rkgwkvbt="">It appears that your browser does not support playing this video.
      However, you can always <a href="https://daiz.moe/content/crunchyroll/klk-underwater-1.mp4" data-astro-cid-rkgwkvbt="true" rel="noopener" target="_blank">download the video directly.</a></span> </video> <figcaption data-astro-cid-rkgwkvbt="">Video clip from <em>Kill la Kill</em> (Fall&nbsp;2013, Underwater fansubs)</figcaption>  </figure>
<p>Now, while fansubbers giving away their work for free might get away with saying <q>just get a better computer</q> to anyone whose devices can’t render softsubbed typesetting like this in realtime, an official service that lots of people pay for doesn’t really have the same luxury, which is the main reason why you don’t see stuff like this softsubbed on Crunchyroll. But this is not an insurmountable problem, so make no mistake: <strong>official anime services could absolutely offer typesetting with similar level of quality to the best of fansubs.</strong> The basic solution to the performance problem is very simple, even: you simply hardsub the typesetting. This would work from streaming to physical disc releases and only the sky would be the limit in terms of the typesetting quality you could offer, as realtime rendering would no longer be a concern!</p>
<p id="best-of-both-worlds">Now, as mentioned earlier, hardsubbing does make things more complicated and expensive on the backend as you need to encode and store multiple copies of video. Crunchyroll is already dealing with this, though! But if costs are an issue, the system is pretty easy to improve in theory: if you keep the dialogue softsubbed, only the parts of the video that actually feature typesetting would be hardsubbed, and with some clever engineering and an understanding of how modern media formats work, you would only have to keep multiple copies of the typeset parts. <strong>And since the average anime episode has on-screen text only for a small percentage of its total runtime, combining softsubbed dialogue and hardsubbed typesetting like this would make for a highly cost-effective setup.</strong></p>
<figure data-astro-cid-rkgwkvbt=""> <video poster="https://daiz.moe/content/crunchyroll/komi-novaworks-1-poster.jpg" controls="" preload="metadata" data-astro-cid-rkgwkvbt=""> <source src="https://daiz.moe/content/crunchyroll/komi-novaworks-1.mp4" type="video/mp4;codecs=avc1.640028,mp4a.40.2" data-astro-cid-rkgwkvbt=""> <span data-astro-cid-rkgwkvbt="">It appears that your browser does not support playing this video.
      However, you can always <a href="https://daiz.moe/content/crunchyroll/komi-novaworks-1.mp4" data-astro-cid-rkgwkvbt="true" rel="noopener" target="_blank">download the video directly.</a></span> </video> <figcaption data-astro-cid-rkgwkvbt="">Typesetting like this would be possible to do even for official anime services. Video clip from <em>Komi Can’t Communicate</em> (Fall&nbsp;2021, NovaWorks fansubs)</figcaption>  </figure>
<p>And since with a mixed system like this you would only have softsubs for the technically simpler dialogue, you could even convert these dialogue-only ASS subtitles to a simpler but more widely supported subtitle format for playback, <strong>which theoretically should do away with the need to keep fully hardsubbed copies around entirely, without any real loss in quality!</strong> I actually built a minimal version of a mixed system like this myself when I was doing some anime streaming work a few years back and can confidently say that <strong>this would be extremely doable for any official anime service… as long as they just cared enough.</strong></p>
<figure data-astro-cid-rkgwkvbt=""> <video poster="https://daiz.moe/content/crunchyroll/danganronpa-utw-1-poster.jpg" controls="" preload="metadata" data-astro-cid-rkgwkvbt=""> <source src="https://daiz.moe/content/crunchyroll/danganronpa-utw-1.mp4" type="video/mp4;codecs=avc1.640028,mp4a.40.2" data-astro-cid-rkgwkvbt=""> <span data-astro-cid-rkgwkvbt="">It appears that your browser does not support playing this video.
      However, you can always <a href="https://daiz.moe/content/crunchyroll/danganronpa-utw-1.mp4" data-astro-cid-rkgwkvbt="true" rel="noopener" target="_blank">download the video directly.</a></span> </video> <figcaption data-astro-cid-rkgwkvbt="">Keep this example of fansub typesetting in mind for later. Video clip from <em>Danganronpa: The Animation</em> (Summer&nbsp;2013, UTW fansubs)</figcaption>  </figure>
<p>Unfortunately, any interest Crunchyroll had for improving their subtitle rendering for typesetting seemed to run out after the 2018 transition to WebAssembly libass. <strong>Not that it actually ever seemed to be all that high to begin with, though,</strong> as evident by some of the low-hanging fruit that Crunchyroll never bothered to pick in this regard; the most obvious of which would be Crunchyroll’s dogged insistence to restrict typesetting font choices to <a href="https://en.wikipedia.org/wiki/Core_fonts_for_the_Web" rel="noopener" target="_blank">Core Fonts for the Web</a>. Free for commercial use fonts have been plentily available since the Flash days, and custom fonts have been well supported on the web for a similarly long time.</p>

<p>Anyway, it would have never been all that hard for Crunchyroll to support custom fonts for typesetting, especially after the 2018 move to HTML5. The underlying technology was there and font files are tiny in size compared to the video files being streamed – this would have been an extremely simple and effective improvement for all typesetting efforts. Yet Crunchyroll never reached for this improvement, which is why <span>Comic Sans</span> has kept appearing in Crunchyroll typesetting with depressing regularity.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/kanokari-cr-1.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/kanokari-cr-1.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>There it is again. <span>Taunting me.</span> Screenshot from <em>Rent-a-Girlfriend S3</em> (Summer&nbsp;2023, Crunchyroll)</p></figcaption>  </figure>
<p>It is also disappointing <strong>how regularly the anime staples of opening &amp; ending songs are still left untranslated on Crunchyroll,</strong> though this issue is admittedly <a href="https://www.animenewsnetwork.com/answerman/2017-09-01/.120752" rel="noopener" target="_blank">much harder to solve than you’d expect.</a> Still, it is possible to do so, especially with Sony’s resources behind the company today. That goes double when Sony is involved in anime production in any way, as then the songs being used should be well-known to all relevant parties well in advance of airing for timely rights-clearing. <strong>So if Crunchyroll/Sony is in any way involved with an anime’s production, it should basically always be possible for songs to be translated the moment the first episode is released.</strong></p>
<p>But that’s enough about Crunchyroll’s history. Now it’s time to look at the other company mentioned earlier and see how they’ve fared in comparison…</p>
<h2 id="a-short-history-of-funimation-and-its-subtitling-standards">A short history of Funimation and its subtitling standards</h2>
<p>In the early 90s, Japanese-American businessman Gen Fukunaga was approached by his uncle who was working as a producer for Toei. A proposal was made: if Fukunaga could start an anime company in US, Toei would license the rights to the <a href="https://en.wikipedia.org/wiki/Dragon_Ball" rel="noopener" target="_blank">Dragon Ball</a> franchise to it – a franchise that was already making mad cash in Japan. Sensing an opportunity, Fukunaga found investors, and thus in 1994 Funimation was born. A year later, Dragon Ball was on US TV, dubbed and edited to <a href="https://web.archive.org/web/20060518193827/http://www.animecauldron.com/dbzuncensored2/misc/footsteps.html" rel="noopener" target="_blank">“conform to American sensibilities and tastes”</a>.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/dbz-promo-art.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/dbz-promo-art.jpg" alt="Promotional art for the Dragon Ball Z anime, featuring some of its cast members." loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>It was especially <em>Dragon Ball Z</em> (1989-1996) that hit it big in the US.</p></figcaption>  </figure>
<p>In the early 2000s, fueled by Dragon Ball’s success, Funimation started expanding its business by getting home video distribution rights for <a href="https://en.wikipedia.org/wiki/4Kids_Entertainment" rel="noopener" target="_blank">4Kids Entertainment</a> licenses and non-Japanese kids’ cartoons, the latter eventually expanding into getting involved in production too. But beyond increased investment in kids’ cartoons, <strong>Funimation also started experimenting with more anime licenses of its own,</strong> the 2001 anime adaption for <a href="https://en.wikipedia.org/wiki/Fruits_Basket" rel="noopener" target="_blank">Fruits Basket</a> being one of its early standout releases.</p>
<p>Out of these various expansion attempts, <strong>“more anime” seemed to be the one to work out best,</strong> and towards the end of the 00s that became the main direction of Funimation’s business. This move was helped along by a bunch of licenses obtained from now-defunct US anime publishers Geneon USA and ADV. And in the spring of 2009, hot on the heels of Crunchyroll going legit, <a href="https://www.animenewsnetwork.com/news/2009-04-03/funimation-adds-toei-air-master-captain-harlock" rel="noopener" target="_blank">Funimation announced that they too were getting into the anime streaming business</a>. The resulting anime streams from Funimation were hardsubbed and looked like this:</p>
<div data-astro-cid-l2h4bdqm="">  <div id="view-inari-0" data-astro-cid-l2h4bdqm=""> <figure data-astro-cid-l2h4bdqm=""> <a href="https://daiz.moe/content/crunchyroll/inari-funi-1.jpg" target="_blank" rel="noopener" data-astro-cid-l2h4bdqm=""> <img src="https://daiz.moe/content/crunchyroll/inari-funi-1.jpg" alt="" loading="lazy" data-astro-cid-l2h4bdqm=""> </a> <figcaption data-astro-cid-l2h4bdqm="">Screenshot from <em>Inari, Konkon, Koi Iroha</em> (Winter 2014, Funimation)</figcaption> </figure> </div><div id="view-inari-1" data-astro-cid-l2h4bdqm=""> <figure data-astro-cid-l2h4bdqm=""> <a href="https://daiz.moe/content/crunchyroll/inari-funi-2.jpg" target="_blank" rel="noopener" data-astro-cid-l2h4bdqm=""> <img src="https://daiz.moe/content/crunchyroll/inari-funi-2.jpg" alt="" loading="lazy" data-astro-cid-l2h4bdqm=""> </a> <figcaption data-astro-cid-l2h4bdqm="">Screenshot from <em>Inari, Konkon, Koi Iroha</em> (Winter 2014, Funimation)</figcaption> </figure> </div><div id="view-inari-2" data-astro-cid-l2h4bdqm=""> <figure data-astro-cid-l2h4bdqm=""> <a href="https://daiz.moe/content/crunchyroll/inari-funi-3.jpg" target="_blank" rel="noopener" data-astro-cid-l2h4bdqm=""> <img src="https://daiz.moe/content/crunchyroll/inari-funi-3.jpg" alt="" loading="lazy" data-astro-cid-l2h4bdqm=""> </a> <figcaption data-astro-cid-l2h4bdqm="">Screenshot from <em>Inari, Konkon, Koi Iroha</em> (Winter 2014, Funimation)</figcaption> </figure> </div><div id="view-inari-3" data-astro-cid-l2h4bdqm=""> <figure data-astro-cid-l2h4bdqm=""> <a href="https://daiz.moe/content/crunchyroll/inari-funi-4.jpg" target="_blank" rel="noopener" data-astro-cid-l2h4bdqm=""> <img src="https://daiz.moe/content/crunchyroll/inari-funi-4.jpg" alt="" loading="lazy" data-astro-cid-l2h4bdqm=""> </a> <figcaption data-astro-cid-l2h4bdqm="">Screenshot from <em>Inari, Konkon, Koi Iroha</em> (Winter 2014, Funimation)</figcaption> </figure> </div>  </div>
<p>What you see here is exactly what you got: plain text at top center or bottom center, with dialogue on bottom, and translations for all on-screen text piled up top. <strong>So while overlaps were technically supported, full positioning did not seem to be possible,</strong> which made things quite awkward the moment there was more than one sign visible on the screen at the same time. <strong>This was also the standard you could expect from Funimation’s DVD and Blu-ray releases.</strong> And beyond the way too common dialogue three-liners (which are generally terrible for readability), sometimes you even saw <em>four-liners</em>:</p>
<figure id="maccaption-introduction" data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/d-frag-funi-1.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/d-frag-funi-1.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Screenshot from <em>D-Frag!</em> (Winter&nbsp;2014, Funimation)</p></figcaption>  </figure>
<p>The subtitling software that Funimation was using at the time was <a href="https://www.telestream.net/captioning/" rel="noopener" target="_blank">Telestream MacCaption</a>. In terms of usability and general authoring features, it was no match for Aegisub, although it was actually capable of doing <a href="https://www.youtube.com/watch?v=Vut8ucz_dYY" rel="noopener" target="_blank">some overlaps, positioning, and styling</a> – Funimation just never chose to make use of these capabilities for its anime subtitles.</p>

<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/maccaption.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/maccaption.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>TeleStream stopped supporting MacCaption in 2023.</p></figcaption>  </figure>
<p>This remained the Funimation subtitle standard all the way until 2016, when Funimation struck a deal with Crunchyroll. <strong>Going forward, subtitled releases for Funimation licenses would be found on Crunchyroll,</strong> while dubbed releases for said titles would be on Funimation’s new streaming platform, <b>FunimationNow.</b></p>

<p>However, the only thing that really changed is that instead of Funimation content being hardsubbed on their website, <strong>it was now softsubbed on Crunchyroll to the exact same standard:</strong> plain text on top center or bottom center, often with three or more lines of dialogue at once, even.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/sakura-quest-funiroll-1.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/sakura-quest-funiroll-1.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Sometimes you could see sign translations on bottom too. Screenshot from <em>Sakura Quest</em> (Spring&nbsp;2017, Funimation/Crunchyroll)</p></figcaption>  </figure>
<p>Nothing else of particular note happened during this time period when it comes to Funimation’s subtitles. However, it is worth mentioning that <strong>Funimation dubs did have simple hardsubbed typesetting sometimes;</strong> this only seemed happen at the whim of the dubbing side of Funimation though, as these hardsubbed signs were never present in the subbed versions, nor were they a consistent feature of Funimation dubs in general.</p>

<p><strong>In 2017, Sony purchased Funimation</strong> as part of its growing collection of international anime distributors (Sony had previously bought Madman Anime and AnimeLab in Australia and Wakanim in Europe). As a result of this buyout, towards the end of 2018 the license sharing deal between Funimation and Crunchyroll was dissolved and soon after <strong>Funimation started serving new subtitled streams on FunimationNow, which were softsubbed</strong> and looked like this:</p>

<p><strong>No longer were the subtitles even making use of overlaps.</strong> Where dialogue translation used to go on bottom and sign translation on top when both were present, now all text was stuck on the same side of the screen together, either on top or bottom, but never both at the same time anymore.</p>
<p>How this further reduction in subtitling capabilities came about cannot be said for sure, but there are several possible explanations. For one, another major thing that happened at the end of 2018: <a href="https://www.animenewsnetwork.com/news/2018-12-04/funimation-hulu-sign-first-look-streaming-deal-for-new-anime/.140359" rel="noopener" target="_blank">Funimation signed a big sublicensing deal</a> with the general streaming service <a href="https://en.wikipedia.org/wiki/Hulu" rel="noopener" target="_blank">Hulu</a>, which meant <strong>dealing with Hulu’s subtitling standards and authoring accordingly limited subtitles</strong> – because as could be expected, the subtitling standards of a general streaming service did not account for the needs of anime in any real way.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/mha-funi-hulu.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/mha-funi-hulu.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Only the middle column of the blackboard is translated here. Good luck figuring that out with subtitles like these. Screenshot from <em>My Hero Academia 4</em> (Fall&nbsp;2019, Funimation/Hulu)</p></figcaption>  </figure>
<p id="ooona-introduction">Another possible reason for these less-than-great changes in Funimation’s subtitling standards was that around this time the company <strong>started using the cloud-based subtitling toolkit OOONA Tools</strong> by the localization service provider <a href="https://www.ooona.net/" rel="noopener" target="_blank">OOONA</a>. OOONA Tools, by default, <strong>do not allow for the creation of subtitles with overlaps.</strong> While it can be done in OOONA today by tweaking the options or by using OOONA’s track features (which are quite similar to those of MacCaption, incidentally), it is possible that at the time these features were either not available or that it wasn’t possible to correctly export subtitles with overlaps to the <a href="https://www.w3.org/TR/webvtt1/" rel="noopener" target="_blank">WebVTT subtitle format</a> that was being used on FunimationNow.</p>

<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/ooona-create.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/ooona-create.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Screenshot of <em>OOONA Create</em>, the primary subtitling software in OOONA Tools.</p></figcaption>  </figure>
<p>Regarding that last possibility in particular, there is <a href="https://archive.is/nRpg1" rel="noopener" target="_blank">this OOONA FAQ entry</a> that mentions how <q>not all formats support […] overlapping subtitles</q> and that <q>Currently, it’s supported in IMSC1.1, ITT and Videotron Lambda CAP exports</q>. However, based on my own testing, OOONA Tools can properly export subtitles with overlaps in more formats today than just the ones mentioned here (including WebVTT), meaning that the FAQ entry is in fact outdated – <strong>but it was likely true at some point.</strong></p>
<p>In any case, this was the extremely limited standard of subtitling that Funimation customers had to live with until the service was shut down in 2024 as a result of the Funimation-Crunchyroll merger.</p>
<hr>
<p>Now, what can we conclude from all this? If nothing else, one thing seems abundantly clear: <strong>the Funimation leadership never truly cared about or respected anime as a medium.</strong> From the very beginning, it’s clear that Gen Fukunaga (a businessman in his 30s at the time) got into the business <strong>with the mindset of making money with kids’ cartoons,</strong> and this only became more evident with how Funimation tried to expand into more types of kids’ cartoons before eventually realizing that anime is where the money was at.</p>
<p>But even with this eventual focus on more anime, <strong>no resources seem to have ever been dedicated to make typesetting an actual thing at Funimation,</strong> despite how obviously beneficial it would have been for their key product of localized anime. And the way Funimation never even bothered to figure out how to make the most of MacCaption, the expensive enterprise subtitling software they kept using for over a decade… while I speculated about possible technical reasons for Funimation abandoning even overlaps when they started producing softsubs for FunimationNow, there was always one possible additional reason: <strong>they just didn’t care at all.</strong> They ran into a problem, no resources were dedicated to fix the problem, and the subtitles got permanently worse as a result.</p>
<figure data-astro-cid-rkgwkvbt=""> <video poster="https://daiz.moe/content/crunchyroll/danganronpa-funi-1-poster.jpg" controls="" preload="metadata" data-astro-cid-rkgwkvbt=""> <source src="https://daiz.moe/content/crunchyroll/danganronpa-funi-1.mp4" type="video/mp4;codecs=avc1.640028,mp4a.40.2" data-astro-cid-rkgwkvbt=""> <span data-astro-cid-rkgwkvbt="">It appears that your browser does not support playing this video.
      However, you can always <a href="https://daiz.moe/content/crunchyroll/danganronpa-funi-1.mp4" data-astro-cid-rkgwkvbt="true" rel="noopener" target="_blank">download the video directly.</a></span> </video> <figcaption data-astro-cid-rkgwkvbt="">Remember the fansubbed version of this from earlier? Here’s Funimation in comparison. Video clip from <em>Danganronpa: The Animation</em> (Summer&nbsp;2013, Funimation)</figcaption>  </figure>
<p>The whole move to OOONA was questionable in itself, as while OOONA was capable of exporting subtitles to both WebVTT for FunimationNow and TTML (or <a href="https://en.wikipedia.org/wiki/SubRip" rel="noopener" target="_blank">SRT</a>, a very limited subtitle format) for Hulu in 2018, <em>so was MacCaption.</em> Why start paying for a monthly subscription service when your existing paid-for enterprise software should be able to deal with your needs just fine? I suspect the primary motivation behind the move (which could have even originated from the new parent company Sony) might have been the fact that it was trendy for companies at the time to move everything they possibly could to The Cloud™, regardless of how much sense it actually made… but that’s enough about OOONA for now.</p>
<p>Ultimately, <strong>Funimation’s subtitling standards were extremely poor to begin with, and they only managed to make them worse over time.</strong> That is something that only utter indifference or outright disdain for anime as a medium could bring about, which seems to have been the exact attitude that Gen Fukunaga cultivated at the executive levels of Funimation – <strong>and his followers appear to have carried the torch even after his departure from the company.</strong> But more on that in the next section, when we finally get to the Funimation-Crunchyroll merger.</p>

<h2 id="the-funimation-crunchyroll-merger-and-its-consequences">The Funimation-Crunchyroll merger and its consequences</h2>
<p>Following Sony’s 2017 purchase of Funimation, in 2019 Sony bought out Gen Fukunaga from the company entirely, which led to him stepping down as the General Manager, with Colin Decker taking his place. Soon after, Sony formed the Funimation Global Group to consolidate all the international anime publishing services it had bought, with Decker in charge of the joint venture as the CEO. Then, in late 2020, <strong>Sony announced that they were going to buy Crunchyroll, placing it under the executive control of the Funimation Global Group.</strong> The acquisition was completed in August 2021, coming with a statement from Sony that their goal is to <a href="https://www.sonypictures.com/corp/press_releases/2021/0809/sonysfunimationglobalgroupcompletesacquisitionofcrunchyrollfromatt" rel="noopener" target="_blank">“create a unified anime subscription experience as soon as possible”</a>.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/funiroll.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/funiroll.jpg" alt="Crunchyroll and Funimation logos side by side." loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Soon, there would only be one.</p></figcaption>  </figure>
<p>Then, in March 2022, the news came that Funimation, Crunchyroll, Wakanim, and VRV (Crunchyroll’s more general streaming service) would all be merged together into a single streaming service that would exist under the name of Crunchyroll (as it had the strongest brand of the lot). <strong>Funimation Global Group LLC was renamed to Crunchyroll LLC, with Funimation executives remaining in charge.</strong> Soon after, Colin Decker stepped down as the CEO, with Rahul Purini (previously COO) taking his place. The merger was complete.</p>
<p>However, as is often the case with mergers &amp; acquisitions, layoffs were on the horizon. In 2023, <a href="https://www.animenewsnetwork.com/news/2023-02-21/crunchyroll-lays-off-approximately-85-employees-globally/.195161" rel="noopener" target="_blank">85 people were laid off globally</a> in the name of employee redundancy. More layoffs have happened since then, with the most recent one being from <a href="https://variety.com/2025/tv/news/crunchyroll-layoffs-restructuring-international-1236487273/" rel="noopener" target="_blank">just a couple months back in August 2025</a>.</p>

<p>Things weren’t much better for those left behind, as laid out in <a href="https://archive.ph/jxvFK" rel="noopener" target="_blank">this Bloomberg article</a> from 2024. Staff from Funimation was notably hostile towards those from Crunchyroll:</p>
<blockquote>
<p>Tension between the camps arose almost immediately. In a Zoom meeting announcing [Sony’s purchase of Crunchyroll], <strong>Funimation workers accused Crunchyroll of being pirates,</strong> alluding to the site’s history, according to two people who were present.</p>
</blockquote>
<p>While Crunchyroll workers were quickly frustrated with the new executives from Funimation:</p>
<blockquote>
<p>Current or former employees describe Crunchyroll’s new management–primarily from Funimation–as out-of-touch with employees and the anime fans the company once prioritized. <strong>Some executives write off anime as “kids’ cartoons,”</strong> they said, and resist hiring job candidates who describe themselves as fans.</p>
</blockquote>
<p>And while all these internal troubles were going on, Crunchyroll CEO Rahul Purini was excited to talk about <a href="https://archive.is/O1fjt" rel="noopener" target="_blank">how interested he is in AI-generated subtitles</a>.</p>

<h3 id="how-typesetting-gets-destroyed">How typesetting gets destroyed</h3>
<p>In 2025, the executives came up with an idea: Crunchyroll should move away from <a href="#aegisub-ass-introduction">Aegisub and ASS subtitles with typesetting</a> and start producing exclusively <a href="#ttml-example">limited TTML subtitles without typesetting</a> in <a href="#ooona-introduction">OOONA Tools</a>. <strong>The likely end goal of this is to get rid of Crunchyroll’s <a href="#cr-libass">unique ASS-based subtitle rendering</a> entirely in favor of something more “industry standard” like TTML-based subtitle rendering.</strong> This would mean no longer having to pay staff for manual ASS-to-TTML conversion, as well as being able to drop the relatively expensive fully hardsubbed encodes for limited playback environments where ASS rendering is not possible <em>(but some sort of TTML rendering usually is).</em></p>

<p><strong>However, a major change affecting all aspects of the company’s subtitling pipeline doesn’t happen overnight,</strong> especially considering Crunchyroll’s large back catalog of ASS subtitles with typesetting that couldn’t be automatically converted to limited TTML subtitles without typesetting. So while the subtitling staff was to be (begrudgingly) busy experimenting and onboarding with OOONA and doing manual ASS-to-TTML conversions for back catalog titles, <strong>technical work would also need to be done to prepare for this vision of a TTML-only future.</strong></p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/moneat-cr-1.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/moneat-cr-1.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>And what an exciting future of not being able to read signs that would be! Screenshot from <em>This Monster Wants to Eat Me</em> (Fall&nbsp;2025, Crunchyroll)</p></figcaption>  </figure>
<p>For this purpose, Crunchyroll seems to have decided that it would take its existing manual ASS-to-TTML conversions produced by the subtitling staff and treat them as the new master subtitle files. These TTML “masters” would then be—for the time being—converted back to ASS with <a href="https://www.closedcaptionconverter.com/" rel="noopener" target="_blank">Closed Caption Converter</a> for use with the current ASS-based subtitle rendering. And so, with the start of the Fall 2025 anime season, a plan like this was pushed to production; while regular ASS subtitles were still being produced by Crunchyroll’s subtitling staff, <strong>these ASS subtitles with typesetting were generally left unused, while only limited ASS-to-TTML-to-ASS conversions without typesetting were being presented to customers on most shows.</strong></p>

<p>Implementing this interim pipeline with Closed Caption Converter didn’t seem to go exactly as planned, though, <strong>as some Fall 2025 shows on Crunchyroll ended up having no subtitles at all on release,</strong> including the premieres of the latest seasons of hit shows <a href="https://en.wikipedia.org/wiki/My_Hero_Academia" rel="noopener" target="_blank">My Hero Academia</a> and <a href="https://en.wikipedia.org/wiki/Spy_%C3%97_Family" rel="noopener" target="_blank">Spy × Family</a>.</p>

<p id="cr-statement-2025-10-09">With the internet taking note of all this, on the 9th of October 2025 <a href="https://www.animenewsnetwork.com/news/2025-10-09/crunchyroll-cites-internal-system-problems-regarding-subtitles-for-fall-2025-anime/.229669" rel="noopener" target="_blank">Crunchyroll responded to a press inquiry</a> by Anime News Network with the following statement:</p>
<blockquote>
<p>Over the past few days, some users experienced delays in accessing the content they wanted and subtitle issues across certain series. These were caused by internal system problems – not by any change in how we create subtitles, use of new vendors or AI. Those internal issues have now been fully resolved.</p>
<p>Quality subtitles are a core part of what makes watching anime on Crunchyroll so special. They connect global fans to the heart of every story, and we take that responsibility seriously.</p>
<p>Thank you for your patience. We’re committed to continuing to deliver the authenticity, quality, and care that fans deserve.</p>
</blockquote>
<p id="cr-actions">Following this statement, some of the new Fall 2025 shows have had their ASS-to-TTML-to-ASS subtitles switched out to the previously unused regular ASS subtitles. Other shows haven’t. <strong>And some shows in the Crunchyroll back catalog have been updated with ASS-to-TTML-to-ASS subtitles,</strong> though the exact timing of these back catalog updates is unknown.</p>

<p>With all of this, <strong>the future of typesetting on Crunchyroll is unclear.</strong></p>
<hr>
<p>And that’s how we’ve found ourselves in the situation we face today. Remember what the <a href="#cr-first-subs">first Crunchyroll subtitles from 2009</a> looked like? Yeah, these new subtitles adhering to limited TTML standards <em>are even worse than the subtitles from 2009</em> in terms of how on-screen text can be handled! In other words: <strong>The presentation quality of Crunchyroll’s first-party subtitles has reached an all-time low in 2025.</strong></p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/chitose-cr-1.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/chitose-cr-1.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>Can’t even handle a next episode preview properly anymore. Screenshot from <em>Chitose Is in the Ramune Bottle</em> (Fall&nbsp;2025, Crunchyroll)</p></figcaption>  </figure>
<p>There is only one conclusion that can be drawn from that: <strong>the Funimation-turned-Crunchyroll executives still do not have any respect for anime as a medium.</strong> In addition, they seem to be treating Crunchyroll and its ways of doing things as the ways of <q>pirates</q> – which isn’t entirely incorrect, as Crunchyroll’s use of Aegisub and ASS <em>did</em> originate from the ways of pirate fansubbers. But fansubbers deeply care about anime as medium (they wouldn’t be illegally subtitling it for free as a hobby otherwise), which in turn means that <strong>the ways fansubbers have developed to subtitle anime are in fact extremely efficient for the job</strong> – much better than basically any “industry standards” for subtitling, even.</p>
<p>But that clearly doesn’t matter to the executives. <strong>The only thing that seems to be on their mind is how to best make money with kids’ cartoons</strong> that none of them personally watch, and what they seem to consider “best” is <em>getting rid of everything positively unique about Crunchyroll in favor of doing things the Funimation way,</em> even if that means ditching Aegisub and ASS in favor of OOONA Tools and TTML and getting rid of typesetting in the process. <span id="crunchyroll-maccaption">This</span> conclusion is further supported by the fact that <strong>Crunchyroll has kept Funimation’s old <a href="#maccaption-introduction">MacCaption-based workflow</a> around for its Blu-ray releases,</strong> with notable reduction in typesetting quality on Blu-ray as a result:</p>


<p>Then there’s the whole plan of moving to OOONA in general, which is even more questionable than it was back in the Funimation days. <strong>Crunchyroll has a lot more to lose in terms of subtitle quality than Funimation ever did,</strong> yet the executives seem to want to go back to their “old reliable” regardless. I can’t even see it saving them any money in the long run, considering that Aegisub is completely free software while OOONA will incur constant ongoing costs with its per-user subscription pricing. Rather than authoring limited TTML in OOONA directly, <strong>paying the subtitling staff to keep the manual ASS to TTML conversions going would likely be cheaper!</strong></p>

<p>Beyond that, there is also the thing about <strong>OOONA being an Israeli company.</strong> It is certainly a choice, not only in 2018 but most certainly in 2025, to heavily invest in the services of a company from a country that is <a href="https://en.wikipedia.org/wiki/Gaza_genocide" rel="noopener" target="_blank">actively committing genocide</a>. However, to quell some unsubstantiated internet discourse I have seen in relation to this, I do want to emphasize that <strong>OOONA being Israeli is not really directly relevant to the quality issues this article is about.</strong></p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/eztitles.jpg" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/eztitles.jpg" alt="" loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>EZTitles is another popular enterprise subtitling software. Notice how they mention AI directly in their navigation.</p></figcaption>  </figure>
<p>The reason for this lies in enterprise subtitling software (“industry standards”) being universally poor when it comes to producing high quality typesetting for anime, so it wouldn’t really matter which software suite a switch was being made to – <strong>no matter what, moving away from Aegisub would destroy typesetting as it currently exists on Crunchyroll.</strong> And while Crunchyroll’s CEO has expressed his interest in AI subtitles, at least currently there has been no signs of any kind of AI (Israeli or otherwise) being used to create first-party subtitles on Crunchyroll.</p>

<h3 id="why-crunchyroll-is-so-confident-it-will-get-away-with-this-or-how-capitalism-ruins-everything">Why Crunchyroll is so confident it will get away with this (or: how capitalism ruins everything)</h3>
<p>Finally, I want to talk about the possible reasons for Crunchyroll executives feeling so confident about getting away with making their own primary product so much worse. Ultimately, it comes down to the fact that <strong>international anime licensing operates primarily on an exclusive licensing model.</strong> This means that generally only one service will be able to offer a specific title in specific language(s) in specific region(s), unless the service voluntarily decides to sublicense it out to others. This in turn upends the assumption that the existence of multiple anime services would be beneficial to consumers, <strong>as the services don’t actually have to engage in competition on customer-beneficial factors like service quality almost at all</strong> – instead, they can just focus on hoarding as many exclusive licenses as possible.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/kun-gao-ama.png" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/kun-gao-ama.png" alt="Screenshot of three comments from a Reddit AMA. Check the link in the caption to read the contents in full." loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>I once asked former Crunchyroll CEO Kun Gao about “exclusivity or completeness” <a href="https://www.reddit.com/r/IAmA/comments/2b26ou/im_kun_gao_the_cofounder_and_ceo_of_crunchyroll/cj12zuo/?context=3" rel="noopener" target="_blank">in this Reddit AMA.</a> He dodged the question but basically said “exclusivity”.</p></figcaption>  </figure>
<p>This kind of “competition” twisted by exclusive licensing is more like a casino, where the customers might occasionally be thrown a bone, <strong>but at the end of the day, the house always wins.</strong> And the anime companies very much prefer to keep it that way, even if it means never being able to offer full coverage of new anime seasons – a limited amount of exclusives is much more important to them. Dreams of infinite growth are what drives the modern-day game of capitalism, and spending money to please customers rather than shareholders goes directly against said dreams. <strong>It’s all about spending as little money as possible to make as much money as possible.</strong></p>
<p>This is why the capitalists in charge of all the big companies these days are so excited about AI too: nothing gets them going more than the idea of not having for pay for those pesky human employees. <em>This is no doubt the actual reason why Crunchyroll CEO Rahul Purini is interested in AI subtitles.</em> It doesn’t matter that <em>anime localization costs are a drop in the bucket compared to the overall costs of anime production,</em> even if you were talking about super high quality work with fansub-level typesetting. <em>Any excuse to cut the wages of real human workers is one step closer to the next yacht purchase for the executive upper class.</em></p>
<p>…Whew, got a bit heated there. Anyway, the most likely reason why Crunchyroll executives believe they can get away with reducing the quality of their own service so much? <strong>Because Crunchyroll doesn’t have any meaningful competition thanks to the primarily-exclusive licensing model used by the international anime industry.</strong> Even if they make the service worse, what can you do about it? Cancel your subscription and not watch the new anime you’re excited about?</p>
<h2 id="what-you-can-do-about-it">What you can do about it</h2>
<p>If you are currently subscribed to Crunchyroll, <a href="https://www.crunchyroll.com/cancellation" rel="noopener" target="_blank">cancel your subscription.</a> <strong>When asked for a reason, mention the bad subtitle quality and lack of <a href="#typesetting-introduction">typesetting.</a></strong> <em>You could even link to this article.</em> Beyond that, and this applies to people who aren’t subscribed to Crunchyroll as well: <strong>spread the word!</strong> Share this article around, talk to people about how Crunchyroll is destroying its subtitles, make it so that Crunchyroll executives can’t ignore the issue. And the most important thing: <strong>Keep it up until Crunchyroll actually makes a clear public commitment to keep typesetting anime.</strong></p>

<p>Why ask for an explicit commitment? Because back in 2017 <a href="https://medium.com/@Daiz/crunchyrolls-reduced-video-quality-is-deliberate-cost-cutting-at-the-expense-of-paying-customers-c86c6899033b" rel="noopener" target="_blank">when Crunchyroll tried to drastically lower its video quality as a cost-cutting measure</a>, vocal user complaints and subscription cancellations forced them to backtrack on it, eventually leading the company to <a href="https://web.archive.org/web/20210622165834/http://www.crunchyroll.com/forumtopic-985103/our-statement-regarding-recent-video-quality-issues#expand" rel="noopener" target="_blank">make a statement</a> and not just <a href="https://medium.com/ellation-tech/improving-video-quality-for-crunchyroll-and-vrv-dd587261a364" rel="noopener" target="_blank">one</a> but <a href="https://medium.com/ellation-tech/stabilizing-improving-crunchyroll-service-c44c53789e86" rel="noopener" target="_blank">two</a> technical follow-up posts where it <em>explicitly promised</em> to do better, <strong>and in the end, video quality actually improved compared to what was previously available.</strong> Ideally, the same would happen with Crunchyroll’s typesetting here.</p>
<figure data-astro-cid-73yd5zmh=""> <a href="https://daiz.moe/content/crunchyroll/cr-video-quality-promise.png" target="_blank" rel="noopener" data-astro-cid-73yd5zmh=""><img src="https://daiz.moe/content/crunchyroll/cr-video-quality-promise.png" alt="Screenshot of the first technical post from 2017 linked above, showing it's title of Improving Video Quality for Crunchyroll and VRV." loading="lazy" data-astro-cid-73yd5zmh=""></a> <figcaption data-astro-cid-73yd5zmh=""><p>“Improving Subtitle Quality for Crunchyroll” is what we’d like to see here in 2025.</p></figcaption>  </figure>

<p>I also want to emphasize that the <a href="#cr-statement-2025-10-09">recent statement</a> Crunchyroll made about its Fall 2025 subtitles <strong>isn’t really worth anything.</strong> It’s worded in an intentionally obfuscated manner as to what actually has been <q>fixed</q> – is it the lack of typesetting or just the issues with subtitles not going up for new releases? Then it just outright lies about there being <q>no changes</q> with <a href="#how-typesetting-gets-destroyed">how subtitles are being handled</a>, before ending on empty platitudes about <q>quality subtitles</q> that mean nothing without concrete actions to back them up.</p>
<p>And so far, <strong><a href="#cr-actions">the actions of Crunchyroll</a> have made the future of typesetting on the service anything but clear.</strong> The lower quality subtitles in the back catalog are especially alarming, as the back catalog was exactly where Crunchyroll also started with its 2017 video quality reduction plans, all the while remaining careful with changes to simulcasts where people were paying closer attention – <strong>which is exactly what seems to be happening with subtitles on Crunchyroll right now.</strong></p>

<hr>
<p><b>To sum things up:</b> Without a clear public commitment to stick to higher subtitling standards that include typesetting, it is very likely that Crunchyroll executives will just delay their typesetting-killing plans and try again later. That’s why <em>you</em> need to <a href="#what-you-can-do-about-it">cancel your subscription</a>, encourage others to do so, and keep talking about this issue <em>until Crunchyroll explicitly promises to do better.</em></p>
<p><strong>Together, we can save Crunchyroll from itself!</strong></p>
<hr>
<h3 id="acknowledgements">Acknowledgements</h3>
<p>This article would have never been as thorough and detailed as it is without the assistance of the following people:</p>
<ul>
<li><strong>The multiple current and former Crunchyroll and Funimation workers</strong> who came forward to indepedently confirm the many previously unpublished details found in this article. <em>Huge thanks, all of you.</em></li>
<li><a href="https://bsky.app/profile/bigonanime.bsky.social" rel="noopener" target="_blank">BigOnAnime</a> – for his great help with researching the historical technical details of Funimation’s subtitling standards. <em>Thank you.</em></li>
<li><a href="https://bsky.app/profile/enonibobble.moe" rel="noopener" target="_blank">enonibobble</a> – for his help with various screenshots and technical analysis of Crunchyroll subtitles. <em>Thank you.</em></li>
<li><a href="https://bsky.app/profile/duxovni.com/post/3lydqz63sis2s" rel="noopener" target="_blank">Faye&nbsp;Duxovni</a> – for bringing Crunchyroll’s use of old Funimation workflows for Blu-rays to my attention and providing the screenshots of it that are used in the article. <em>Thank you.</em></li>
<li><a href="https://bsky.app/profile/rcombs.me" rel="noopener" target="_blank">Ridley</a>, <a href="https://bsky.app/profile/witchymary.fansubcar.tel" rel="noopener" target="_blank">witchymary</a>, <a href="https://bsky.app/profile/jhiday.bsky.social" rel="noopener" target="_blank">Jhiday</a> – for proofreading this article before release. <em>Thanks, all of you.</em></li>
<li><strong>People on social media</strong> who answered public questions I asked or otherwise helped with various small pieces of research. <em>Thanks, all of you.</em></li>
</ul>
<h3 id="external-coverage">External coverage</h3>
<p>I’m not the only one to have made note of Crunchyroll’s recent subtitle shenanigans, so here’s some additional reading/watching on the subject elsewhere:</p>
<ul>
<li><a href="https://animebythenumbers.substack.com/p/worse-crunchyroll-subtitles" rel="noopener" target="_blank">Why did Crunchyroll’s subtitles just get worse?</a> by <b>Miles Atherton</b> (former head of marketing for Crunchyroll), on the newsletter <b>Anime By The Numbers.</b> This includes some additional details (like numbers!) that I didn’t go over here (because this article was long enough as-is), so I can recommend giving it a read.</li>
<li><a href="https://www.youtube.com/watch?v=B-DX0Zolr6g" rel="noopener" target="_blank">The Absolute State of Crunchyroll</a> by YouTuber <b>Mother’s Basement.</b> This is a good watch just to see how bad the new Crunchyroll subtitles look like in action. Additionally, I didn’t really talk about how badly timing quality has been affected by the recent changes too, but this video has some good examples of that as well.</li>
<li><a href="https://www.animenewsnetwork.com/answerman/2025-10-27/.230367" rel="noopener" target="_blank">Are Subtitles Getting Smaller?</a> by <b>Jerome Mazandarani</b> on <b>Anime News Network</b>. This <b>Answerman</b> column is nominally about subtitles getting visually smaller, but most of it ends up being about the Crunchyroll subtitle situation. Jerome does keep incorrectly saying that general streaming services use the very bare-bones subtitle format SRT rather than TTML, though, and while these services do support SRT for ingestion (ie. content partners can deliver subtitles as SRT) and anime companies might even be making use of that, TTML is what the services actually use internally. SRT does not officially support any kind of positioning whatsoever, which means that even placing subtitles at the top of the screen would be impossible with it if the normal placement was on bottom.</li>
<li><a href="https://www.animenewsnetwork.com/this-week-in-anime/2025-10-14/.229891" rel="noopener" target="_blank">The Crunchyroll Sub Flub</a> by <b>Lucas DeRuyter</b> and <b>Coop Bicknell</b>, also on <b>Anime News Network</b>. Nothing particularly new in this one if you’re familiar with all the other coverage, but it’s nice to see this get discussed on the <b>This Week in Anime</b> column regardless. The more eyes on the subject, the better.</li>
</ul>
<hr>
<div id="about"><p><img src="https://daiz.moe/img/avatar-remilia-black-medium-3.jpg" alt="Daiz's avatar, featuring the female vampire Remilia Scarlet from Touhou Project" data-astro-cid-vjqwbl62=""></p><p>I’m <b>Daiz</b>, a digital distribution expert and high quality media enthusiast. I have over a decade of experience with Japanese-to-English media localization, including anime subtitling, and I also care deeply about consumer rights. <a href="https://bsky.app/profile/daiz.moe" rel="noopener" target="_blank">You can follow me on Bluesky</a>, or <a href="mailto:contact@daiz.moe">drop me a mail</a>.</p></div>
<hr>
<p>I’m working on getting Bluesky comments embedded at the end of the posts. For the time being though, <a href="https://bsky.app/profile/daiz.moe/post/3m4ejjogo5k2w" rel="noopener" target="_blank">you can read and join the discussion here!</a></p>  </article>  </div></div>]]></description>
        </item>
    </channel>
</rss>