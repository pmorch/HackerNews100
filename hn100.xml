<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 27 Sep 2023 23:00:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Ray-Ban Meta Smart Glasses (212 pts)]]></title>
            <link>https://www.meta.com/smart-glasses/</link>
            <guid>37678860</guid>
            <pubDate>Wed, 27 Sep 2023 18:18:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.meta.com/smart-glasses/">https://www.meta.com/smart-glasses/</a>, See on <a href="https://news.ycombinator.com/item?id=37678860">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[OpenBSD PF versus FreeBSD PF (131 pts)]]></title>
            <link>https://mwl.io/archives/23127</link>
            <guid>37678714</guid>
            <pubDate>Wed, 27 Sep 2023 18:09:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mwl.io/archives/23127">https://mwl.io/archives/23127</a>, See on <a href="https://news.ycombinator.com/item?id=37678714">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-23127">
		<!-- .entry-header -->

	
	<div>
		<p>I encountered yet another discussion about <a href="https://www.openbsd.org/faq/pf/">OpenBSD PF</a> versus <a href="https://docs.freebsd.org/en/books/handbook/firewalls/">FreeBSD PF</a>. For those who are new to the discussion: OpenBSD developers created PF in 2001, and it rapidly improved to become the most approachable open source packet filter. FreeBSD ported PF over to its kernel in 2004, with occasional updates since. Today a whole bunch of folks who don’t program echo cultish wisdom that one or the other version of PF has fallen behind, not kept up on improvements, or otherwise betrayed their community. My subtler comments have been misinterpreted, so let’s try this.</p>
<p>These claims are garbage.</p>
<p>First, and most importantly: <a href="https://freshbsd.org/freebsd/src/branch/main?q=pf+openbsd">FreeBSD PF developers work with OpenBSD devs all the time</a>, and <a href="https://freshbsd.org/openbsd/src/branch/HEAD?q=pf+freebsd">OpenBSD PF developers pull stuff from FreeBSD</a><span id="easy-footnote-1-23127"></span><span><a href="#easy-footnote-bottom-1-23127" title="Commit history links courtesy of <a href=&quot;https://cathode.church/@meena/111133478906743121&quot;>Mina</a>. Yes, I could have looked it up, but she <em>thought</em> to do so."><sup>1</sup></a></span>. You get a lot of noise about certain people being jerks about the other project–and both projects absolutely have jerks. (And yes, anyone who has read my books knows that I am a cross-platform jerk.) But for the most part, folks want to work together.</p>
<p>PF is absolutely an OpenBSD creation, though, so why isn’t the OpenBSD version the Single Source of Truth? Why doesn’t FreeBSD just consider OpenBSD a vendor and pull that code in? Because the OpenBSD and FreeBSD kernels are wholly different.</p>
<p>Back when I wrote <a href="https://mwl.io/nonfiction/nope">Absolute BSD</a>, I could realistically write a single book that would basically apply to the three major open source BSDs. Yes, the various projects objected to being lumped together, but if you knew any one of them you could stumble through the others. This is no longer true. FreeBSD’s kernel uses a wholly different locking model than OpenBSD. OpenBSD’s memory protections have no equivalent in FreeBSD. These are not things you can manage with a shim layer or kernel ABI. These are big, complicated, intrusive differences. You can’t tar up one version and dump it in the other’s kernel. It won’t work. If you do a hack job of making it work, it will perform badly.</p>
<p>Yes, you can find “proof” that one PF or the other is faster under particular workloads on specific hardware. I have no doubt that some of them are not only accurate, but honest. There are other workloads, though, and other hardware, and other conditions. Regardless of who wins a particular race, the constant competition to achieve peak performance benefits everyone. I’m not going to link to any of the benchmarks, because <a href="https://mwl.io/nonfiction/wtf#l2e1">I have made my opinions on benchmarking very clear elsewhere</a>.<span id="easy-footnote-2-23127"></span><span><a href="#easy-footnote-bottom-2-23127" title="&amp;#8220;The natural impulse to compare your server against others, to whup your neighbor, is exactly that: natural. It’s like flipping rocks to find tasty grubs. Sleeping in trees coveting the upper-class caves inhabited by the snooty grizzly bears. Perishing within twenty years of What Is Vitamin C Anyway? We invented civilization to escape dysentery, cleaning ourselves with mud, and benchmarking.&amp;#8221;"><sup>2</sup></a></span> Pick what you want and roll with it.</p>
<p>Every PF developer is trundling along, doing their best to make things work.</p>
<p>Are features missing from one or the other? Yep. I’m not going to list examples because, as the above links show, each project plucks what they find useful from the other. These things are freely given, with glad hearts, but they take time to integrate. Filling message boards with staunch declarations that <em>my team’s PF is better</em> is not only tedious, it wholly misses the point.</p>
<p>People are working together to improve the world.</p>
<p>And the PF syntax is the most approachable in all of open source Unix.</p>
<p>(Partisan fanboy comments will be mercilessly whacked.)</p>
<ol><li><span id="easy-footnote-bottom-1-23127"></span>Commit history links courtesy of <a href="https://cathode.church/@meena/111133478906743121">Mina</a>. Yes, I could have looked it up, but she <em>thought</em> to do so.<a href="#easy-footnote-1-23127"></a></li><li><span id="easy-footnote-bottom-2-23127"></span>“The natural impulse to compare your server against others, to whup your neighbor, is exactly that: natural. It’s like flipping rocks to find tasty grubs. Sleeping in trees coveting the upper-class caves inhabited by the snooty grizzly bears. Perishing within twenty years of What Is Vitamin C Anyway? We invented civilization to escape dysentery, cleaning ourselves with mud, and benchmarking.”<a href="#easy-footnote-2-23127"></a></li></ol>	</div><!-- .entry-content -->

	 <!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta Quest 3 (222 pts)]]></title>
            <link>https://www.meta.com/quest/quest-3/</link>
            <guid>37678318</guid>
            <pubDate>Wed, 27 Sep 2023 17:45:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.meta.com/quest/quest-3/">https://www.meta.com/quest/quest-3/</a>, See on <a href="https://news.ycombinator.com/item?id=37678318">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Tao of Programming (1987) (114 pts)]]></title>
            <link>https://www.mit.edu/~xela/tao.html</link>
            <guid>37678181</guid>
            <pubDate>Wed, 27 Sep 2023 17:37:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mit.edu/~xela/tao.html">https://www.mit.edu/~xela/tao.html</a>, See on <a href="https://news.ycombinator.com/item?id=37678181">Hacker News</a></p>
<div id="readability-page-1" class="page">
<hr>

<h4>Translated by Geoffrey James</h4>
<hr>
<p><i>Note: I copied this from 
<tt><a href="http://misspiggy.gsfc.nasa.gov/tao.html">http://misspiggy.gsfc.nasa.gov/tao.html</a></tt>
and stripped out all of the IMHO extraneous formatting.
<br>---Alex</i>
</p><hr>
<h3>BOOK 1</h3>
<h2><i>The Silent Void</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b>"When you have learned to snatch the error code
from the trap frame, it will be time for you to leave."</b></span>
</p><hr>
<h4>1.1</h4>
<p>
Something mysterious is formed, born in the silent void.  waiting
alone and unmoving, it is at once still and yet in constant motion.
It is the source of all programs.  I do not know its name, so I will
call it the Tao of Programming.
</p><blockquote>
If the Tao is great, then the operating system is great.
<br>
If the operating system is great, then the compiler is great.
<br>
If the compiler is great, then the application is great.
<br>
The user is pleased, and there is harmony in the world.
</blockquote>
The Tao of Programming flows far away and returns on the wind of
morning.
<hr>
<h4>1.2</h4>
<p> 
The Tao gave birth to machine language.  Machine language gave birth
to the assembler.
</p><p>
The assembler gave birth to the compiler.  Now there are ten thousand
languages.
</p><p>
Each language has its purpose, however humble.  Each language
expresses the Yin and Yang of software.  Each language has its place
within the Tao.
</p><p>
But do not program in <tt>COBOL</tt> if you can avoid it.
</p><hr>
<h4>1.3</h4>
<p>
In the beginning was the Tao.  The Tao gave birth to Space and Time.
</p><p>
Therefore Space and Time are the Yin and Yang of programming.
</p><p>
Programmers that do not comprehend the Tao are always running out of
time and space for their programs.  Programmers that comprehend the
Tao always have enough time and space to accomplish their goals.
</p><p>
How could it be otherwise?
</p><hr>
<h4>1.4</h4>
<p>
The wise programmer is told about Tao and follows it.  The average
programmer is told about Tao and searches for it.  The foolish
programmer is told about Tao and laughs at it.
</p><p>
If it were not for laughter, there would be no Tao.
</p><p>
The highest sounds are hardest to hear.  Going forward is a way to
retreat.  Great talent shows itself late in life.  Even a perfect
program still has bugs.
</p><hr>
<h3>BOOK 2</h3>
<h2><i>The Ancient Masters</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "After three days without programming, life
becomes meaningless."</b></span>
</p><hr>
<h4>2.1</h4>
<p>
The programmers of old were mysterious and profound.  We cannot fathom
their thoughts, so all we do is describe their appearance.
</p><blockquote>
Aware, like a fox crossing the water.
<br>
Alert, like a general on the battlefield.  
<br>
Kind, like a hostess greeting her guests.  
<br>
Simple, like uncarved blocks of wood.  
<br>
Opaque, like black pools in darkened caves.
</blockquote>
Who can tell the secrets of their hearts and minds?
<p>
The answer exists only in Tao.
</p><hr>
<h4>2.2</h4>
<p>
The Grand Master Turing once dreamed that he was a machine.  When he
awoke, he exclaimed:
</p><p>
"I don't know whether I am Turing dreaming that I am a machine, or a
machine dreaming that I am Turing!"
</p><hr>
<h4>2.3</h4>
<p>
A programmer from a very large computer company went to a software
conference and then returned to report to his manager, saying: "What
sort of programmers work for other companies?  They behaved badly and
were unconcerned with appearances.  Their hair was long and unkempt
and their clothes were wrinkled and old.  They crashed our hospitality
suite and they made rude noises during my presentation."
</p><p>
The manager said: "I should have never sent you to the conference.
Those programmers live beyond the physical world.  They consider life
absurd, an accidental coincidence.  They come and go without knowing
limitations.  Without a care, they live only for their programs.  Why
should they bother with social conventions?
</p><p>
They are alive within the Tao."
</p><hr>
<h4>2.4</h4>
<p>
A novice asked the Master: "Here is a programmer that never designs,
documents or tests his programs.  Yet all who know him consider him
one of the best programmers in the world.  Why is this?"
</p><p>
The Master replied: "That programmer has mastered the Tao.  He has
gone beyond the need for design; he does not become angry when the
system crashes, but accepts the universe without concern.  He has gone
beyond the need for documentation; he no longer cares if anyone else
sees his code.  He has gone beyond the need for testing; each of his
programs are perfect within themselves, serene and elegant, their
purpose self-evident.  Truly, he has entered the mystery of Tao."
</p><hr>
<h3>BOOK 3</h3>
<h2><i>Design</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "When a program is being tested, it is too late
to make design changes."</b></span>
</p><hr>
<h4>3.1</h4>
<p>
There once was a man who went to a computer trade show.  Each day as
he entered, the man told the guard at the door:
</p><p>
"I am a great thief, renowned for my feats of shoplifting.  Be
forewarned, for this trade show shall not escape unplundered."
</p><p>
This speech disturbed the guard greatly, because there were millions
of dollars of computer equipment inside, so he watched the man
carefully.  But the man merely wandered from booth to booth, humming
quietly to himself.
</p><p>
When the man left, the guard took him aside and searched his clothes,
but nothing was to be found.
</p><p>
On the next day of the trade show, the man returned and chided the
guard, saying: "I escaped with a vast booty yesterday, but today will
be even better."  So the guard watched him ever more closely, but to
no avail.
</p><p>
On the final day of the trade show, the guard could restrain his
curiosity no longer.  "Sir Thief," he said, "I am so perplexed, I
cannot live in peace.  Please enlighten me.  What is it that you are
stealing?"
</p><p>
The man smiled.  "I am stealing ideas," he said.
</p><hr>
<h4>3.2</h4>
<p>
There once was a Master Programmer who wrote unstructured programs.  A
novice programmer, seeking to imitate him, also began to write
unstructured programs.  When the novice asked the Master to evaluate
his progress, the Master criticized him for writing unstructured
programs, saying, "What is appropriate for the Master is not
appropriate for the novice.  You must understand Tao before
transcending structure."
</p><hr>
<h4>3.3</h4>
<p>
There was once a programmer who was attached to the court of the
warlord of Wu.  The warlord asked the programmer: "Which is easier to
design: an accounting package or an operating system?"
</p><p>
"An operating system," replied the programmer.
</p><p>
The warlord uttered an exclamation of disbelief.  "Surely an
accounting package is trivial next to the complexity of an operating
system," he said.
</p><p>
"Not so," said the programmer, "When designing an accounting package,
the programmer operates as a mediator between people having different
ideas: how it must operate, how its reports must appear, and how it
must conform to the tax laws.  By contrast, an operating system is not
limited by outside appearances.  When designing an operating system,
the programmer seeks the simplest harmony between machine and ideas.
This is why an operating system is easier to design."
</p><p>
The warlord of Wu nodded and smiled.  "That is all good and well, but
which is easier to debug?"
</p><p>
The programmer made no reply.
</p><hr>
<h4>3.4</h4>
<p>
A manager went to the Master Programmer and showed him the
requirements document for a new application.  The manager asked the
Master: "How long will it take to design this system if I assign five
programmers to it?"
</p><p>
"It will take one year," said the Master promptly.
</p><p>
"But we need this system immediately or even sooner!  How long will it
take if I assign ten programmers to it?"
</p><p>
The Master Programmer frowned.  "In that case, it will take two
years."
</p><p>
"And what if I assign a hundred programmers to it?"
</p><p>
The Master Programmer shrugged.  "Then the design will never be
completed," he said.
</p><hr>
<h3>BOOK 4</h3>
<h2><i>Coding</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "A well-written program is its own Heaven; a
poorly-written program is its own Hell."</b></span>
</p><hr>
<h4>4.1</h4>
<p>
A program should be light and agile, its subroutines connected like a
string of pearls.  The spirit and intent of the program should be
retained throughout.  There should be neither too little nor too much.
Neither needless loops nor useless variables; neither lack of
structure nor overwhelming rigidity.
</p><p>
A program should follow the "Law of Least Astonishment".  What is this
law?  It is simply that the program should always respond to the users
in the way that least astonishes them.
</p><p>
A program, no matter how complex, should act as a single unit.  The
program should be directed by the logic within rather than by outward
appearances.
</p><p>
If the program fails in these requirements, it will be in a state of
disorder and confusion.  The only way to correct this is to rewrite
the program.
</p><hr>
<h4>4.2</h4>
<p>
A novice asked the Master: "I have a program that sometimes runs and
sometimes aborts.  I have followed the rules of programming, yet I am
totally baffled.  What is the reason for this?"
</p><p>
The Master replied: "You are confused because you do not understand
Tao.  Only a fool expects rational behavior from his fellow humans.
Why do you expect it from a machine that humans have constructed?
Computers simulate determinism; only Tao is perfect.
</p><p>
The rules of programming are transitory; only Tao is eternal.
Therefore, you must contemplate Tao before you receive Enlightenment."
</p><p>
"But how will I know when I have received Enlightenment?"  asked the
novice.
</p><p>
"Your program will run correctly," replied the Master.
</p><hr>
<h4>4.3</h4>
<p>
The Master was explaining the nature of Tao to one of his novices.
</p><p>
"The Tao is embodied in all software -- regardless of how
insignificant," said the Master.
</p><p>
"Is the Tao in a hand-held calculator?" asked the novice.
</p><p>
"It is," came the reply.
</p><p>
"Is the Tao in a video game?" asked the novice.
</p><p>
"It is even in a video game," said the Master.
</p><p>
"Is the Tao in the <tt>DOS</tt> for a personal computer?" asked the novice.
</p><p>
The Master coughed and shifted his position slightly.  "The lesson is
over for today," he said.
</p><hr>
<h4>4.4</h4>
<p>
Prince Wang's programmer was coding software.  His fingers danced upon
the keyboard.  The program compiled without and error message, and the
program ran like a gentle wind.
</p><p>
"Excellent!"  the Prince exclaimed.  "Your technique is faultless!"
</p><p>
"Technique?"  said the programmer, turning from his terminal, "What I
follow is Tao -- beyond all techniques!  When I first began to
program, I would see before me the whole problem in one mass.  After
three years, I no longer saw this mass.  Instead, I used subroutines.
But now I see nothing.  My whole being exists in a formless void.  My
senses are idle.  My spirit, free to work without a plan, follows its
own instinct.  In short, my program writes itself.  True, sometimes
there are difficult problems.  I see them coming, I slow down, I watch
silently.  Then I change a single line of code and the difficulties
vanish like puffs of idle smoke.  I then compile the program.  I sit
still and let the joy of the work fill my being.  I close my eyes for
a moment and then log off."
</p><p>
Prince Wang said, "Would that all of my programmers were as wise!"
</p><hr>
<h3>BOOK 5</h3>
<h2><i>Maintenance</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "Though a program be but three lines long,
someday it will have to be maintained."</b></span>
</p><hr>
<h4>5.1</h4>
<blockquote>
A well-used door needs no oil on its hinges.
<br>
A swift-flowing stream does not grow stagnant.
<br>
A deer blends perfectly into the forest colors.
<br>
Software rots if not used.
</blockquote>
These are great mysteries.
<hr>
<h4>5.2</h4>
<p>
A manager asked a programmer how long it would take him to finish the
program on which he was working.  "I will be finished tomorrow," the
programmer promptly replied.
</p><p>
"I think you are being unrealistic," said the manager, "Truthfully,
how long will it take?"
</p><p>
The programmer thought for a moment.  "I have some features that I
wish to add.  This will take at least two weeks," he finally said.
</p><p>
"Even that is too much to expect," insisted the manager, "I will be
satisfied if you simply tell me when the program is complete."
</p><p>
The programmer agreed to this.
</p><p>
Several years later, the manager retired.  On the way to his
retirement luncheon, he discovered the programmer asleep at his
terminal.  He had been programming all night.
</p><hr>
<h4>5.3</h4>
<p>
A novice programmer was once assigned to code a simple financial
package.
</p><p>
The novice worked furiously for many days, but when his Master
reviewed his program, he discovered it contained a screen editor, a
set of generalized graphics routines, and an artificial intelligence
interface, but not the slightest hint of anything financial.
</p><p>
When the Master asked about this, the novice became indignant.  "Don't
be so impatient," he said, "I'll put in the financial stuff
eventually."
</p><hr>
<h4>5.4</h4>
<blockquote>
Does a good farmer neglect a crop he has planted?
<br>
Does a good teacher overlook even the most humble student?
<br>
Does a good father allow a single child to starve?
<br>
Does a good programmer refuse to maintain his code?
</blockquote>
<hr>
<h3>BOOK 6</h3>
<h2><i>Management</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "Let the programmers be many and the managers few
-- then all will be productive."</b></span>
</p><hr>
<h4>6.1</h4>
<blockquote>
When managers hold endless meetings, the programmers write games.
<br>
When accountants speak of quarterly profits, the development budget is
about to be cut.
<br>
When senior scientists talk blue sky, the clouds are about to roll in.
</blockquote>
Truly, this is not the Tao of Programming.
<blockquote>
When managers make commitments, game programs are ignored.
<br>
When accountants make long-range plans, harmony and order are about to
be restored.
<br>
When senior scientists address the problems at hand, the problems will
soon be solved.
</blockquote>
Truly, this is the Tao of Programming.
<hr>
<h4>6.2</h4>
<blockquote>
Why are programmers non-productive?  Because their time is wasted in
meetings.
<br>
Why are programmers rebellious?  Because the management interferes too
much.
<br>
Why are the programmers resigning one by one?  Because they are burnt
out.
<br>
Having worked for poor management, they no longer value their jobs.
</blockquote>
<hr>
<h4>6.3</h4>
<p>
A manager was about to be fired, but a programmer who worked for him
wrote a new program that became popular and sold well.  As a result,
the manager retained his job.
</p><p>
The manager tried to give the programmer a bonus, but the programmer
refused it, saying, "I wrote the program because I thought it was an
interesting concept, and thus I expect no reward."
</p><p>
The manager upon hearing this remarked, "This programmer, though he
holds a position of small esteem, understands well the proper duty of
an employee.  Let us promote him to the exalted position of management
consultant!"
</p><p>
But when told this, the programmer once more refused, saying, "I exist
so that I can program.  If I were promoted, I would do nothing but
waste everyone's time.  Can I go now?  I have a program that I am
working on."
</p><hr>
<h4>6.4</h4>
<p>
A manager went to his programmers and told them: "As regards to your
work hours: you are going to have to come in at nine in the morning
and leave at five in the afternoon."  At this, all of them became
angry and several resigned on the spot.
</p><p>
So the manager said: "All right, in that case you may set your own
working hours, as long as you finish your projects on schedule."  The
programmers, now satisfied, began to come in at noon and work to the
wee hours of the morning.
</p><hr>
<h3>BOOK 7</h3>
<h2><i>Corporate Wisdom</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "You can demonstrate a program for a corporate
executive, but you can't make him computer literate."</b></span>
</p><hr>
<h4>7.1</h4>
<p>
A novice asked the Master: "In the East, there is a great
tree-structure that men call 'Corporate Headquarters'.  It is bloated
out of shape with vice presidents and accountants.  It issues a
multitude of memos, each saying 'Go Hence!' or 'Go Hither!' and nobody
knows what is meant.  Every year new names are put onto the branches,
but all to no avail.  How can such an unnatural entity exist?"
</p><p>
The Master replied: "You perceive this immense structure and are
disturbed that it has no rational purpose.  Can you not take amusement
from its endless gyrations?  Do you not enjoy the untroubled ease of
programming beneath its sheltering branches?  Why are you bothered by
its uselessness?"
</p><hr>
<h4>7.2</h4>
<p>
In the East there is a shark which is larger than all other fish.  It
changes into a bird whose wings are like clouds filling the sky.  When
this bird moves across the land, it brings a message from Corporate
Headquarters.  This message it drops into the midst of the
programmers, like a seagull making its mark upon the beach.  Then the
bird mounts on the wind and, with the blue sky at its back, returns
home.
</p><p>
The novice programmer stares in wonder at the bird, for he understands
it not.  The average programmer dreads the coming of the bird, for he
fears its message.  The Master Programmer continues to work at his
terminal, unaware that the bird has come and gone.
</p><hr>
<h4>7.3</h4>
<p>
The Magician of the Ivory Tower brought his latest invention for the
Master Programmer to examine.  The Magician wheeled a large black box
into the Master's office while the Master waited in silence.
</p><p>
"This is an integrated, distributed, general-purpose workstation,"
began the Magician, "ergonomically designed with a proprietary
operating system, sixth generation languages, and multiple state of
the art user interfaces.  It took my assistants several hundred man
years to construct.  Is it not amazing?"
</p><p>
The Master Programmer raised his eyebrows slightly.  "It is indeed
amazing," he said.
</p><p>
"Corporate Headquarters has commanded," continued the Magician, "that
everyone use this workstation as a platform for new programs.  Do you
agree to this?"
</p><p>
"Certainly," replied the Master.  "I will have it transported to the
Data Center immediately!"  And the Magician returned to his tower,
well pleased.
</p><p>
Several days later, a novice wandered into the office of the Master
Programmer and said, "I cannot find the listing for my new program.
Do you know where it might be?"
</p><p>
"Yes," replied the Master, "the listings are stacked on the platform
in the Data Center."
</p><hr>
<h4>7.4</h4>
<p>
The Master Programmer moves from program to program without fear.  No
change in management can harm him.  He will not be fired, even if the
project is cancelled.  Why is this?  He is filled with Tao.
</p><hr>
<h3>BOOK 8</h3>
<h2><i>Hardware and Software</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "Without the wind, the grass does not move.
Without software hardware is useless."</b></span>
</p><hr>
<h4>8.1</h4>
<p>
A novice asked the Master: "I perceive that one computer company is
much larger than all others.  It towers above its competition like a
giant among dwarfs.  Any one of its divisions could comprise an entire
business.  Why is this so?"
</p><p>
The Master replied, "Why do you ask such foolish questions?  That
company is large because it is large.  If it only made hardware,
nobody would buy it.  If it only made software, nobody would use it.
If it only maintained systems, people would treat it like a servant.
But because it combines all of these things, people think it one of
the gods!  By not seeking to strive, it conquers without effort."
</p><hr>
<h4>8.2</h4>
<p>
A Master Programmer passed a novice programmer one day.
</p><p>
The Master noted the novice's preoccupation with a hand-held computer
game.
</p><p>
"Excuse me," he said, "may I examine it?"
</p><p>
The novice bolted to attention and handed the device to the Master.
"I see that the device claims to have three levels of play: Easy,
Medium, and Hard," said the Master.  "Yet every such device has
another level of play, where the device seeks not to conquer the
human, nor to be conquered by the human."
</p><p>
"Pray, Great Master," implored the novice, "how does one find this
mysterious setting?"
</p><p>
The Master dropped the device to the ground and crushed it with his
heel.  Suddenly the novice was enlightened.
</p><hr>
<h4>8.3</h4>
<p>
There was once a programmer who wrote software for personal computers.
"Look at how well off I am here," he said to a mainframe programmer
who came to visit.  "I have my own operating system and file storage
device.  I do not have to share my resources with anyone.  The
software is self-consistent and easy-to-use.  Why do you not quit your
present job and join me here?"
</p><p>
The mainframe programmer then began to describe his system to his
friend, saying, "The mainframe sits like an ancient Sage meditating in
the midst of the Data Center.  Its disk drives lie end-to- end like a
great ocean of machinery.  The software is as multifaceted as a
diamond, and as convoluted as a primeval jungle.  The programs, each
unique, move through the system like a swift-flowing river.  That is
why I am happy where I am."
</p><p>
The personal computer programmer, upon hearing this, fell silent.  But
the two programmers remained friends until the end of their days.
</p><hr>
<h4>8.4</h4>
<p>
Hardware met Software on the road to Changtse.  Software said: "You
are Yin and I am Yang.  If we travel together, we will become famous
and earn vast sums of money."  And so they set forth together,
thinking to conquer the world.
</p><p>
Presently, they met Firmware, who was dressed in tattered rags and
hobbled along propped on a thorny stick.  Firmware said to them: "The
Tao lies beyond Yin and Yang.  It is silent and still as a pool of
water.  It does not seek fame; therefore, nobody knows its presence.
It does not seek fortune, for it is complete within itself.  It exists
beyond space and time."
</p><p>
Software and Hardware, ashamed, returned to their homes.
</p><hr>
<h3>BOOK 9</h3>
<h2><i>Epilogue</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "Time for you to leave."</b></span>
</p><hr>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Ashley Book of Knots (1944) (107 pts)]]></title>
            <link>https://archive.org/details/TheAshleyBookOfKnots</link>
            <guid>37676880</guid>
            <pubDate>Wed, 27 Sep 2023 16:13:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://archive.org/details/TheAshleyBookOfKnots">https://archive.org/details/TheAshleyBookOfKnots</a>, See on <a href="https://news.ycombinator.com/item?id=37676880">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>The Ashley Book of Knots and Macrame. working with rope, line and plating Every practical Knot, what it looks like, Who uses it, where it comes from and how to tie it with 7000 drawings representing 3800 knots.&nbsp;
</p><div id="reviews">
      <h2>
                  
                <p><span>comment</span></p>
        Reviews
      </h2>

      
      
      
                  <div id="review-1631625278">
      <p><b>Reviewer:</b>
              <a href="https://archive.org/details/%40k34456" data-event-click-tracking="ItemReviews|ReviewerLink">k34456</a>
            -
      <span alt="5.00 out of 5 stars" title="5.00 out of 5 stars"><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span></span>      -
      September 14, 2021      <br>
      <b>Subject:</b>
      Not in the public domain      </p><div><p>
        This is a very useful book on knots. It is truly the knot bible. </p><p>

Unfortunately, however, this is the 1993 version and contains corrected material that are still protected under the copyright laws. To the reviewer who claimed that this book is in public domain, you are wrong. The Internet Archive contains much material, including game source codes that were stolen from game software companies, that are illegal to distribute. (Regarding the stolen source code, you need to know how to look for them in the IA because the uploaders used "special" words that are recognizable only by those who know what the stolen files represent.) I've also seen books (including proprietary material stolen from private corporations that have words like "proprietary" or "confidential" marked in them) posted in IA that were published within the past couple of decades and thus are still copyrighted. So this says that the IA simply does not have the resources to go through every uploaded file and delete the ones that are not legal to download. There are simply too many of them for any single individual like me to report them to IA for deletion. Just trying to report them to the IA would be a full-time job.</p><p>

As for this Ashley book, for it to be truly in the public domain, it needs to be the original 1944 version, the one that has errors that the 1993 version corrected. However, there is also the possibility that the copyright owner may have given the permission for the 1993 version of the book to be available via the IA, without releasing the book into the public domain, but I don't know how IA would be able to know this without contacting the copyright owner.      </p></div>
    </div>
        <div id="review-1596117362">
      <p><b>Reviewer:</b>
              <a href="https://archive.org/details/%40666thenumberofthebeast" data-event-click-tracking="ItemReviews|ReviewerLink">Harem Cinema</a>
            -
      <span alt="5.00 out of 5 stars" title="5.00 out of 5 stars"><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span></span>      -
      July 30, 2020      <br>
      <b>Subject:</b>
      You Have The Thanks Of A Sailor      </p><p>
        All I can say is a simple, but heart-felt Thank You, for sharing this with us all.      </p>
    </div>
        <div id="review-1568139117">
      <p><b>Reviewer:</b>
              <a href="https://archive.org/details/%40searcharchives" data-event-click-tracking="ItemReviews|ReviewerLink">ChronicKristinitis</a>
            -
      <span alt="5.00 out of 5 stars" title="5.00 out of 5 stars"><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span></span>      -
      September 10, 2019      <br>
      <b>Subject:</b>
      Wow      </p><div><p>
        I loved reading "knot books" when I first started sailing. Sailing of course is no requisite for studying knots! or "bends".... </p><p>

This is a lovely book written by a lovely man who cared enough about the world to share this work, which took him at least eleven years to create. </p><p>

PS- this title surely IS in public domain, so- the comment from the "public domain police" is unnecessary. If in the case a copyrighted file is in the Archive by mistake, it will quickly be removed.       </p></div>
    </div>
        <div id="review-1506973798">
      <p><b>Reviewer:</b>
              <a href="https://archive.org/details/%40gary_bunker" data-event-click-tracking="ItemReviews|ReviewerLink">Gary Bunker</a>
            -
      <span alt="0.00 out of 5 stars" title="0.00 out of 5 stars"></span>      -
      October 2, 2017      <br>
      <b>Subject:</b>
      Public Domain?      </p><div><p>
        The copyright laws in the USA say that a work enters public domain 70 years after the death of the author. Mr. Ashley died in 1947, so it is plausible that it would enter public domain this year. But, it was posted last year. And, the copyright notice is from an edition with amended material dated 1993.</p><p>

Is this truly public domain?      </p></div>
    </div>
        <div id="review-1476416070">
      <p><b>Reviewer:</b>
              <a href="https://archive.org/details/%40fleetwing" data-event-click-tracking="ItemReviews|ReviewerLink">fleetwing</a>
            -
      <span alt="5.00 out of 5 stars" title="5.00 out of 5 stars"><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span></span>      -
      October 14, 2016      <br>
      <b>Subject:</b>
      Knots      </p><p>
        The Necessary requirement Encyclopedia of Knots for all Seafarers...   in order to Survive.      </p>
    </div>
              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Live near your friends (319 pts)]]></title>
            <link>https://headlineshq.substack.com/p/issue-no-029-live-near-your-friends</link>
            <guid>37676393</guid>
            <pubDate>Wed, 27 Sep 2023 15:45:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://headlineshq.substack.com/p/issue-no-029-live-near-your-friends">https://headlineshq.substack.com/p/issue-no-029-live-near-your-friends</a>, See on <a href="https://news.ycombinator.com/item?id=37676393">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h4>Discover more from Headlines</h4><p>A newsletter covering the consciousness, psychedelic, and mental health economy.</p><p>Over 2,000 subscribers</p> </div><div dir="auto"><p><span>Good morning, and happy Sunday. Great to see you here for another issue of </span><em>Headlines</em><span>. I thoroughly enjoyed researching and writing this one. Let’s get right to it.&nbsp;&nbsp;</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png" width="1456" height="770" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:770,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>Deep and meaningful friendships are </span><a href="https://twitter.com/melodaysong/status/1690068103869468672?s=20" rel="">integral</a><span> to a happy and healthy life.&nbsp;</span></p><p>Why not live closer to your pals?&nbsp;</p><p><strong>THE FRIENDSHIP RECESSION</strong></p><p><span>As we’ve </span><a href="https://headlineshq.substack.com/p/issue-no-010-we-vs-me" rel="">written about before</a><span>, friendship and community are essential to our emotional well-being. The longest-ever longitudinal study on human life found that deep relationships hold, by far, the </span><a href="https://www.theatlantic.com/ideas/archive/2023/01/harvard-happiness-study-relationships/672753/" rel="">strongest correlation with our health and happiness</a><span>.</span></p><p><span>Unfortunately, the last decade has seen a steep drop in adult friendships. Modern life encourages us, </span><a href="https://chwoodiwiss.medium.com/im-moving-into-my-own-place-and-i-m-sad-about-it-1ecb5f423009" rel="">writes</a><span> editor and journalist Catherine Woodiwiss, to atomize ourselves away from each other:</span></p><blockquote><p><em>“We seem to be doing life backward: We live alone and expend effort to gather together, as if that’s the healthy baseline; instead of starting with togetherness as the foundation, and striking out for aloneness when we need it.”</em></p></blockquote><p><span>Indeed, the rise of hyperindividualism has fragmented our connections, scattering our relationships across the country. Even today’s modern self-care trends </span><a href="https://time.com/6271915/self-love-loneliness/" rel="">turn us inward</a><span>, convincing us to “hyperfocus on ourselves at the expense of connecting with others.”&nbsp;</span></p><p><strong>And the numbers don’t lie: </strong><span>Americans are spending </span><a href="https://www.washingtonpost.com/opinions/2022/11/23/americans-alone-thanksgiving-friends/" rel="">more and more time alone</a><span> and less and less time with their friends.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png" width="555" height="431.40425531914894" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1096,&quot;width&quot;:1410,&quot;resizeWidth&quot;:555,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Meanwhile, the number of close friendships Americans have has </span><a href="https://www.americansurveycenter.org/research/the-state-of-american-friendship-change-challenges-and-loss/" rel="">plunged over the last two decades</a><span>; only 13% of Americans report having 10 or more close friends, down from 33% in the 1990s.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png" width="647" height="348.38461538461536" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:784,&quot;width&quot;:1456,&quot;resizeWidth&quot;:647,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>PEAS IN A POD</strong></p><p>In a 1935 letter to his lifelong friend Arthur Greeves, author C.S. Lewis wrote:&nbsp;</p><blockquote><p><em>“Friendship [to me] is the chief happiness of life. If I had to give a piece of advice to a young man about a place to live, I think I should say, 'sacrifice almost everything to live where you can be near your friends.’”</em></p></blockquote><p>Yet our friendships today have taken a backseat to marriage, career, and more. While technology has made it easier than ever to maintain bonds across geographic distances, ease can’t replace depth.&nbsp;</p><ul><li><p><span>Having a friend whom you see on most days, compared to not having such a friend, has the same impact on well-being as </span><a href="https://www.theatlantic.com/health/archive/2013/10/social-connection-makes-a-better-brain/280934/" rel="">making an extra $100K a year</a><span>.&nbsp;</span></p></li><li><p><span>A </span><a href="https://news.harvard.edu/gazette/story/2008/12/having-happy-friends-can-make-you-happy/" rel="">20-year multi-generational study</a><span> showed that living within a mile of a friend who is happy increases the likelihood that you’ll be happy by 25%.</span></p></li></ul><p><span>Given the undeniable ties between meaningful relationships and well-being, why aren’t we placing </span><a href="https://www.theatlantic.com/family/archive/2020/10/people-who-prioritize-friendship-over-romance/616779/" rel="">friendship at the center of our lives</a><span>? That’s a question the live-near-your-friends movement is trying to answer.</span></p><p><strong>Live near your friends.</strong><span> At the frontier of this movement is Phil Levin, founder of coliving community </span><a href="https://radish.super.site/" rel="">Radish</a><span> in Oakland, California and co-founder of the car-free neighborhood project </span><a href="https://culdesac.com/" rel="">Culdesac</a><span>.</span></p><p><span>This August, Levin launched </span><a href="https://www.livenearfriends.com/" rel="">Live Near Friends</a><span>, a site that helps people live within a five-minute walk of a close friend or family member.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png" width="1456" height="822" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:822,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Live Near Friends, Levin says, was inspired by his wife Kristen, who would proactively send out listings to friends trying to get them to move nearby. Over three years, she succeeded in getting eight people to rent and buy homes within a short walk of where they live. Now, they </span><a href="https://supernuclear.substack.com/p/babies-radish-the-early-review" rel="">help take care of each other’s kids</a><span>, do regular dinners, and hang out on a whim.</span></p><p><span>Others in this space include grassroots projects like NYC’s </span><a href="https://prigoose.substack.com/p/how-to-live-near-your-friends" rel="">Fractal</a><span>, a collective of ten living rooms within a five-minute walk from Morgan Ave’s L train station. There’s also SF’s Neighborhood, led by </span><a href="https://jasonbenn.com/" rel="">Jason Benn</a><span>, a multigenerational campus of 200+ people living within a square mile in central San Francisco.&nbsp;</span></p><p><strong>What if I don’t have friends? </strong><span>A sobering statistic you may have noticed above: 12% of Americans say they have </span><em>zero</em><span> close friends. Hoping to help, apps like </span><a href="https://www.getsaturday.com/" rel="">Saturday</a><span> and </span><a href="https://www.geneva.com/" rel="">Geneva</a><span> want to ease the discovery process while dating app Bumble launched </span><a href="https://bumble.com/en/the-buzz/bumble-for-friends" rel="">Bumble for Friends</a><span>.&nbsp;</span></p><p><span>And, as we’ve covered before, the </span><a href="https://headlineshq.substack.com/p/issue-no-018-social-prescribing" rel="">social prescribing model</a><span> uses local “link workers” to help nudge people toward nonclinical care via movement, art, and more. Here, orgs like </span><a href="https://uniteus.com/" rel="">Unite Us</a><span>, </span><a href="https://www.widercircle.com/" rel="">Wider Circle</a><span>, and </span><a href="https://company.findhelp.com/" rel="">findhelp</a><span> look to power community health and social care systems.&nbsp;</span></p><p><span>Even the federal government is stepping in. This July, Senator Chris Murphy introduced legislation to create a </span><a href="https://www.murphy.senate.gov/download/nssc-one-pager#:~:text=THE%20BILL%20WOULD%3A&amp;text=Create%20an%20Office%20of%20Social,and%20civic%20and%20community%20engagement." rel="">national policy to promote social connection</a><span>.</span></p><p><span>But, Levin notes, many interventions tend to put too much onus on the individual and not enough on </span><a href="https://insider.fitt.co/issue-no-222-living-well/" rel="">our built environment</a><span>, adding that:&nbsp;</span></p><blockquote><p><em>“People are often told to ‘go meet their neighbors.’ But for me, the question is more about: ‘How can we design places so it’s impossible not to have a relationship with your neighbors?’”&nbsp;</em></p></blockquote><p><strong>A NOTE FROM MEL</strong></p><p>This topic is one close to my heart. I’ve moved around a lot these past few years, and my friends are scattered cross-country, from Los Angeles to New York, Austin to San Francisco.&nbsp;</p><p>The pandemic further cemented the slow dissolution of many relationships I wish to rekindle but are complicated by the sheer fact of geography. This year in particular, after moving from LA to SF, I saw many dear friends much less frequently; it had an undeniable impact on my emotional well-being and resilience.&nbsp;</p><p>All this is to say: These past few months have underlined the importance of deep and meaningful relationships to me. Time and time again, when life gets rough, it is my friends who consistently show me the love and compassion I am unable to give myself, gently lifting me back up into the light.&nbsp;</p><p><strong>Punchline: </strong><span>How do we help people build and nurture meaningful relationships? It’s perhaps the most important question we should be addressing in the mental health space.&nbsp;</span></p><p>Tackling the loneliness epidemic will be no easy challenge—and not everyone can move close to their friends—but it’s a great goal to strive for. The good thing is, the first step can be very simple. A text to an old friend, a coffee with a new one. Little moments of connection spark more connection, and connection goes a long way.&nbsp;</p><div data-attrs="{&quot;url&quot;:&quot;https://headlineshq.substack.com/p/issue-no-029-live-near-your-friends?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="CaptionedButtonToDOM"><p><span>Thanks for reading ☁️🍄. Want to join the convo? Tag me </span><a href="https://twitter.com/melodaysong" rel="">@melodaysong</a><span> with your thoughts, or share with a friend below.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://headlineshq.substack.com/p/issue-no-029-live-near-your-friends?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="ButtonCreateButton"><a href="https://headlineshq.substack.com/p/issue-no-029-live-near-your-friends?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div><ul><li><p><strong>Master’s in happiness. </strong><span>Get your </span><a href="https://fortune.com/well/2023/08/05/masters-degree-in-happiness/" rel="">degree in happiness</a><span>, a 20-month interdisciplinary program on the science of well-being.</span></p></li><li><p><strong>Mayday. </strong><span>New study from Little Otter analyzes </span><a href="https://8179936.fs1.hubspotusercontent-na1.net/hubfs/8179936/Blogs%20for%20Download/Breaking%20the%20Silence.pdf" rel="">11K+</a><span> families, shines light on the worsening pediatric mental health crisis.&nbsp;</span></p></li><li><p><strong><span>Google it. The tech giant is </span><a href="https://www.nytimes.com/2023/08/16/technology/google-ai-life-advice.html" rel="">testing out</a><span> an AI bot that can offer life advice. [Re-read</span><a href="https://headlineshq.substack.com/p/issue-no-017-ai-eats-therapy" rel=""> </a></strong><em><strong><a href="https://headlineshq.substack.com/p/issue-no-017-ai-eats-therapy" rel="">Issue No. 017: AI Eats Therapy.</a><span>)</span></strong></em></p></li><li><p><strong>Sobering stats. </strong><span>About </span><a href="https://www.latimes.com/california/story/2023-04-13/988-hotline-mental-health-crisis-system-police?utm_id=107875&amp;sfmc_id=5276556&amp;skey_id=dfa482c87b8037d578676754f18d10a6d9d65b822a65565197ef4d291a62bcc5" rel="">40%</a><span> of people killed by police officers are involved in a mental health crisis.</span></p></li><li><p><strong>Ready or not, here AI come. </strong><span>AI-driven chronic health app juli </span><a href="https://medcitynews.com/2023/08/digital-health-app-helps-asthma-and-depression-2-trials-find/" rel="">improved</a><span> symptoms of asthma and depression.</span></p></li><li><p><strong>New name, who this? </strong><span>MAPS’ trademark filings reveal potential brand name for MDMA: </span><a href="https://psychedelicalpha.com/news/psychedelic-bulletin-143-trademark-filing-reveals-potential-brand-name-for-mdma-dea-open-to-considering-special-registration-for-controlled-substances-via-telemedicine-professional-practice-gu?utm_source=tricycleday&amp;utm_medium=newsletter&amp;utm_campaign=this-week-in-psychedelics" rel="">RENSANSE</a><span>.</span></p></li></ul><p><strong>1) Coming of age</strong></p><p><span>New studies from </span><a href="https://www.sciencedirect.com/science/article/abs/pii/S2215036623001931?dgcid=author" rel="">The Lancet Psychiatry</a><span> and </span><a href="https://news.blueshieldca.com/2023/08/03/new-poll-mental-health-challenges-prevalent-among-gen-z-youth-more-than-three-in-four-have-discussed-their-struggles-with-others" rel="">Blue Shield of CA</a><span> reveal mental disorders are on the rise, and Gen Z is suffering the most. A whopping nine out of 10 Gen Z youth say they’re experiencing mental health challenges on a regular basis, citing gun violence, racial/social injustice, and </span><a href="https://headlineshq.substack.com/p/issue-no-026-feeling-the-heat" rel="">climate change</a><span>.&nbsp;</span></p><p><span>Meanwhile, with 50% of the world’s population projected to develop at least one mental disorder in their lifetime, the mental health crisis has reached beyond the ability of a single sector to solve. Solutions will require concerted efforts across government, healthcare, Big Tech, and more — paying special care to prevention among young people.</span><br><strong><span>→ </span><a href="https://insider.fitt.co/the-mental-health-crisis-worsens/" rel="">Read more</a></strong></p><p><strong>2) Bird is the word</strong><span>&nbsp;</span></p><p><span>Speaking of the younger generation, there’s a new mental health trend sweeping Gen Z: Bird-watching. Posts tagged with #birdwatching and #birding on TikTok have over 1.4B and 240M views, respectively, while apps like </span><a href="https://birda.org/" rel="">Birda</a><span>, </span><a href="https://merlin.allaboutbirds.org/" rel="">Merlin Bird ID</a><span>, and </span><a href="https://birdnet.cornell.edu/" rel="">BirdNET</a><span> have reported up to 30% increases in monthly signup rates. With few barriers to entry (save a pair of binoculars), the trend is taking flight — many are calling it the </span><a href="https://www.hollywoodrepbirrter.com/lifestyle/lifestyle-news/birding-hollywood-meditation-1235503559/" rel="">new meditation</a><span>.&nbsp;</span></p><p><span>Not just a hobby, there’s strong evidence behind the mental health benefits of birding. Studies show that birdsong can help </span><a href="https://www.nature.com/articles/s41598-022-20841-0" rel="">alleviate anxiety and paranoia</a><span>, while a mere 10% increase in bird species in one’s vicinity increased participants’ life satisfaction the same as a </span><a href="https://www.sciencedaily.com/releases/2020/12/201204110246.htm" rel="">10% increase in income</a><span>. Small wonder that spending time outdoors and connecting to nature can be such a boon for our health — this is one TikTok trend we can get behind.&nbsp;</span><br><strong><span>→ </span><a href="https://www.tiktok.com/tag/birdwatching?lang=en" rel="">Watch more</a></strong></p><p><strong>🍄 COMPASS Pathways, </strong><span>a psychedelic biotech company, has entered a securities purchase agreement of </span><strong>$125M</strong><span>,</span><strong> </strong><span>including a potential additional </span><strong>$160M, </strong><span>with a group of healthcare specialist investors. The agreement was led by </span><strong>TCGX</strong><span> and </span><strong><span>Aisling Capital. </span><br><span>→ </span><a href="https://compasspathways.com/compass-pathways-announces-up-to-285-million-private-placement-financing-joined-by-leading-healthcare-investors-2/" rel="">source</a></strong></p><p><strong>🧠 MindMed, </strong><span>a clinical stage psychedelic company, secured a </span><strong>$50M</strong><span> credit facility with </span><strong><span>K2 HealthVentures.</span><br><span>→ </span><a href="https://www.businesswire.com/news/home/20230814596096/en/MindMed-Secures-50.0-Million-Credit-Facility-with-K2-HealthVentures/?feedref=JjAwJuNHiystnCoBq_hl-bQQCakZDujohEJegUyaJwquReQ0P23MrIoWkrUSV24ZevRMp3sIgu8q3wq1OF24lT93qbEzrwa15HGbLqMObxbNWfZlBntHS6jpH5ROLXsTFMkl05DM8ABqeyBleEmzJA==" rel="">source</a></strong></p><p><strong>🚺 Visana Health, </strong><span>a women’s health platform that includes behavioral health services, raised a $10.1M seed round co-led by </span><strong>Flare Capital Partners </strong><span>and </span><strong>Frist Cressey Ventures</strong><span>. </span><br><span>→ </span><strong><a href="https://www.prnewswire.com/news-releases/visana-health-raises-10-1-million-seed-round-to-bring-comprehensive-virtual-healthcare-to-women-nationwide-301903801.html" rel="">source</a></strong></p><p><strong>👗 Kohl’s</strong><span> donated </span><strong>$6M</strong><span> to the </span><strong>National Alliance on Mental Illness (NAMI)</strong><span> to increase mental health services and resources across the country, with a focus on BIPOC communities.</span><br><span>→ </span><strong><a href="https://www.businesswire.com/news/home/20230815196975/en" rel="">source</a></strong></p><p><strong>✨ Glimmer, </strong><span>a guided therapy platform connecting patients to a higher standard of mental healthcare, launched. </span><br><span>→ </span><strong><a href="https://www.prweb.com/releases/former-gaming-entrepreneur-now-holding-mental-health-to-a-higher-standard-with-launch-of-glimmer-301901071.html" rel="">source</a></strong></p><p><strong>⚡ ARC Health</strong><span>, a group of mental healthcare practices, acquired </span><strong>Dayspring Behavioral Health (DBH)</strong><span>, a practice with four locations throughout the Seattle metroplex.</span><br><span>→ </span><strong><a href="https://www.prnewswire.com/news-releases/dayspring-behavioral-health-joins-arc-health-301901326.html" rel="">source</a></strong></p><p><strong>🌷 Spring Health, </strong><span>an employer mental health platform, launched </span><strong>Sage</strong><span>, a set of self-paced online courses to help managers support employee mental health. </span><br><span>→ </span><strong><a href="https://www.prnewswire.com/news-releases/spring-health-launches-sage-setting-a-new-standard-for-mental-health-education-with-self-directed-clinician-led-courses-for-workplace-wellbeing-301903051.html" rel="">source</a></strong></p><p><strong>📝 Tebra, </strong><span>a practice management tool for independent healthcare providers, launched </span><strong>The Intake</strong><span>, a comprehensive online content hub to help independent healthcare practices thrive. </span><br><span>→ </span><strong><a href="https://www.businesswire.com/news/home/20230809997362/en" rel="">source</a></strong><span> // </span><strong>Re-read </strong><em><strong><a href="https://headlineshq.substack.com/p/issue-no-028-therapy-tech" rel="">Issue No. 028: Therapy Tech</a></strong></em><strong>&nbsp;</strong></p><p><strong>🎮 Healthy Gamer, </strong><span>a mental wellness platform designed for the internet generation, launched </span><strong>HG Institute</strong><span>, providing accredited courses on the latest clinical trends and research to help professionals better address modern mental health stressors.</span><br><span>→ </span><strong><a href="https://hg-institute.com/?utm_medium=email&amp;_hsmi=270401786&amp;_hsenc=p2ANqtz-_LuvJ8w8_2h1pUBiH-omqOobMuIxF9ZKO-P6a6hpNtcDpKNwlP-S3jjnYlPB6ZnvI518lu2iv0_y06JFhquVi3FwTE-g&amp;utm_content=270401786&amp;utm_source=hs_email" rel="">source</a></strong></p><ul><li><p><strong>Addicted? </strong><span>Deep Fix is an incisive, beautifully-written newsletter from Alex “Olo” Olshonsky, somatic coach and co-founder of nonprofit addiction program Natura Care. His essays explore all forms of modern addiction—from drugs to screens—as well as psychedelics, culture, work, and spirituality.&nbsp; → </span><em><a href="https://deepfix.substack.com/" rel="">Deep Fix</a></em></p></li></ul><p>Thank you, as always, for reading. 🫶🏻&nbsp;</p><p>I leave you with two photos from my week: One of the most magnificent sunsets I’ve seen in recent memory, and a snap of my mom looking cute as heck on her birthday. :)</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png" width="1160" height="744" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:744,&quot;width&quot;:1160,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1763565,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Until next Sunday,</span><br><span>-Mel&nbsp;</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Desalination system could produce fresh water that is cheaper than tap water (167 pts)]]></title>
            <link>https://www.eurekalert.org/news-releases/1002811</link>
            <guid>37675831</guid>
            <pubDate>Wed, 27 Sep 2023 15:14:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eurekalert.org/news-releases/1002811">https://www.eurekalert.org/news-releases/1002811</a>, See on <a href="https://news.ycombinator.com/item?id=37675831">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                                                                                                                                                                                        <figure>
                <a href="https://www.eurekalert.org/multimedia/999867">
                  <p><img src="https://earimediaprodweb.azurewebsites.net/Api/v1/Multimedia/cb328de9-26e8-4a03-afaf-b9be19ee8248/Rendition/low-res/Content/Public" alt="Solar desal">
                  </p>
                </a>
                <figcaption>
                  <p><strong>image:&nbsp;MIT researchers have designed a new solar desalination system that takes in saltwater and heats it with natural sunlight. The system flushes out accumulated salt, so replacement parts aren’t needed often, meaning the system could potentially produce drinking water at a rate and price that is cheaper than tap water.</strong>
                  <a href="https://www.eurekalert.org/multimedia/999867">view <span>more&nbsp;<i></i></span></a></p>
                  <p>Credit: Credit: Jintong Gao and Zhenyuan Xu</p>
                </figcaption>
              </figure>
            
                            <p>Engineers at MIT and in China are aiming to turn seawater into drinking water with a completely passive device that is inspired by the ocean, and powered by the sun.&nbsp;</p>

<p>In a paper appearing today in the journal&nbsp;<em>Joule</em>,&nbsp;the team outlines the design for a new solar desalination system that takes in saltwater and heats it with natural sunlight.&nbsp;</p>

<p>The configuration of the device allows water to circulate in swirling eddies, in a manner similar to the much larger “thermohaline” circulation of the ocean. This circulation, combined with the sun’s heat, drives water to evaporate, leaving salt behind. The resulting water vapor can then be condensed and collected as pure, drinkable water. In the meantime, the leftover salt continues to circulate through and out of the device, rather than accumulating and clogging the system.&nbsp;</p>

<p>The new system has a higher water-production rate and a higher salt-rejection rate than all other passive solar desalination concepts currently being tested.</p>

<p>The researchers estimate that if the system is scaled up to the size of a small suitcase, it could produce about 4 to 6 liters of drinking water per hour and last several years before requiring replacement parts. At this scale and performance, the system could produce drinking water at a rate and price that is cheaper than tap water.&nbsp;</p>

<p>“For the first time, it is possible for water, produced by sunlight, to be even cheaper than tap water,” says Lenan Zhang, a research scientist in MIT’s Device Research Laboratory.&nbsp;</p>

<p>The team envisions a scaled-up device could passively produce enough drinking water to meet the daily requirements of a small family. The system could also supply off-grid, coastal communities where seawater is easily accessible.&nbsp;</p>

<p>Zhang’s study co-authors include MIT graduate student Yang Zhong, and Evelyn Wang, the Ford Professor of Engineering, along with Jintong Gao, Jinfang You, Zhanyu Ye, Ruzhu Wang, and Zhenyuan Xu of Shanghai Jiao Tong University in China.</p>

<p><strong>A powerful convection</strong></p>

<p>The team’s new system improves on their&nbsp;<a href="https://link.mediaoutreach.meltwater.com/ls/click?upn=kLuqYYBQiqEU1tC0k1-2Bxu01QDVU-2Bz37CVhW-2F2Vj6SIQCb-2BlML1pO5WQ3FHDyvko8PrAPlxR8rMy6bVZu9syYOLMINfWzfhLlWZb1blPcuVJoCwCLwISrr-2Bp9VqOUOisQ2p_X_rlwtU090MLiXmw82ipgrvtp8SINvdCG-2By7G4BjIVQIj1rlD0F8XV3QCy4z5U0lmMzLyo7mULBFeIAf3XohbPKS3mSkskWJ0wFsWXVkxKzDbZg3JkAa-2FfARTB75XFmby7DdSWl3PreYBv0X2xHdviwPE2MffzaRcFtX5eWiN7EIM4Efa6CDz-2Fx-2Bw3QOYEqGME-2BDj-2BfGhYwEh5jrwWKKGcQPmc0HRAlpPrYJB3AmP2yc3dyJy6gKO9potYNoQojjWYn77fHUsSQygYr7WQBQtgabRK46VIOaE0G1KtGvcF3cRjPGLUdh1npbxJhMrQ2MkJ65PduIaa-2Bdc9pf0QQ9-2Buy4l-2B-2Fp4ql3BrnMjfaVFIAE8UoCngnlrau72Z0iuIZabU">previous design</a>&nbsp;— a similar concept of multiple layers, called stages. Each stage contained an evaporator and a condenser that used heat from the sun to passively separate salt from incoming water. That design, which the team tested on the roof of an MIT building, efficiently converted the sun’s energy to evaporate water, which was then condensed into drinkable water. But the salt that was left over quickly accumulated as crystals that clogged the system after a few days. In a real-world setting, a user would have to place stages on a frequent basis, which would significantly increase the system’s overall cost.</p>

<p>In a follow-up effort, they&nbsp;<a href="https://link.mediaoutreach.meltwater.com/ls/click?upn=kLuqYYBQiqEU1tC0k1-2Bxu01QDVU-2Bz37CVhW-2F2Vj6SITjPGCz8Nn-2BoEfQ321GpY7123w2QTZTaSfXbY69qv7oTX0FOyANEuzf6QshkZZCkXM-3D_7ua_rlwtU090MLiXmw82ipgrvtp8SINvdCG-2By7G4BjIVQIj1rlD0F8XV3QCy4z5U0lmMzLyo7mULBFeIAf3XohbPKS3mSkskWJ0wFsWXVkxKzDbZg3JkAa-2FfARTB75XFmby7DdSWl3PreYBv0X2xHdviwPE2MffzaRcFtX5eWiN7EIM4Efa6CDz-2Fx-2Bw3QOYEqGME-2BDj-2BfGhYwEh5jrwWKKGcQHzr4YgAwVv70UUoaMz95RK0zoowZMd4lCksoplDlUj1msNDwmHY4GuiGfwyh1skTtOLCY4bVIE14OdOMBOcVajybN8FgdblJeHqg24KxaOX2UDGzMlOO-2Fp-2BNomVxlxqxOajiSuo-2BvBqOVEitvbTSCVs0nM3chNWqp9JSPuWhsFZ">devised a solution</a>&nbsp;with a similar layered configuration, this time with an added feature that helped to circulate the incoming water as well as any leftover salt. While this design prevented salt from settling and accumulating on the device, it desalinated water at a relatively low rate.&nbsp;</p>

<p>In the latest iteration, the team believes it has landed on a design that achieves both a high water-production rate, and high salt rejection, meaning that the system can quickly and reliably produce drinking water for an extended period. The key to their new design is a combination of their two previous concepts: a multistage system of evaporators and condensers, that is also configured to boost the circulation of water — and salt — within each stage.&nbsp;</p>

<p>“We introduce now an even more powerful convection, that is similar to what we typically see in the ocean, at kilometer-long scales,” Xu says.&nbsp;</p>

<p>The small circulations generated in the team’s new system is similar to the “thermohaline” convection in the ocean — a phenomenon that drives the movement of water around the world, based on differences in sea temperature (“thermo”) and salinity (“haline”).&nbsp;</p>

<p>“When seawater is exposed to air,&nbsp;sunlight drives water to evaporate. Once water leaves the surface, salt remains. And the higher the salt concentration, the denser the liquid, and this heavier water wants to flow downward,” Zhang explains. “By mimicking this kilometer-wide phenomena in small box, we can take advantage of this feature to reject salt.”</p>

<p><strong>Tapping out</strong></p>

<p>The heart of the team’s new design is a single stage that resembles a thin box, topped with a dark material that efficiently absorbs the heat of the sun. Inside, the box is separated into a top and bottom section. Water can flow through the top half, where the ceiling is lined with an evaporator layer that uses the sun’s heat to warm up and evaporate any water in direct contact. The water vapor is then funneled to the bottom half of the box, where a condensing layer air-cools the vapor into salt-free, drinkable liquid. The researchers set the entire box at a tilt within a larger, empty vessel, then attached a tube from the top half of the box down through the bottom of the vessel, and floated the vessel in saltwater.&nbsp;</p>

<p>In this configuration, water can naturally push up through the tube and into the box, where the tilt of the box, combined with the thermal energy from the sun, induces the water to swirl as it flows through. The small eddies help to bring water in contact with the upper evaporating layer while keeping salt circulating, rather than settling and clogging.&nbsp;</p>

<p>The team built several prototypes, with one, three, and 10 stages, and tested their performance in water of varying salinity, including natural seawater and water that was seven times saltier.&nbsp;</p>

<p>From these tests, the researchers calculated that if each stage were scaled up to a square meter, it would produce up to 5 liters of drinking water per hour, and that the system could desalinate water without accumulating salt for several years. Given this extended lifetime, and the fact that the system is entirely passive, requiring no electricity to run, the team estimates that the overall cost of running the system would be cheaper than what it costs to produce tap water in the United States.&nbsp;</p>

<p>“We show that this device is capable of achieving a long lifetime,” Zhong says. “That means that, for the first time, it is possible for drinking water produced by sunlight to be cheaper than tap water. This opens up the possibility for solar desalination to address real-world problems.”&nbsp;</p>

<p>Funding for the research at Shanghai Jiao Tong University was supported by the Natural Science Foundation of China.</p>

<p>###</p>

<p><em>Written by Jennifer Chu, MIT News</em></p>

            
                        <hr>
            <hr>
            <div>
										                                            
                                                                
                                                                                                        <div>
                            <h4>Article Title</h4>
                            <p>Extreme salt-resisting multistage solar distillation with thermohaline convection</p>
                        </div>
                                                                <div>
                            <h4>Article Publication Date</h4>
                            <p>27-Sep-2023</p>
                        </div>
                                                            					                </div>
                    </div><p><strong>Disclaimer:</strong> AAAS and EurekAlert! are not responsible for the accuracy of news releases posted to EurekAlert! by contributing institutions or for the use of any information through the EurekAlert system.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral 7B, the most powerful language model for its size to date, Apache 2.0 (673 pts)]]></title>
            <link>https://mistral.ai/news/announcing-mistral-7b/</link>
            <guid>37675496</guid>
            <pubDate>Wed, 27 Sep 2023 14:52:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/announcing-mistral-7b/">https://mistral.ai/news/announcing-mistral-7b/</a>, See on <a href="https://news.ycombinator.com/item?id=37675496">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Mistral AI team is proud to release Mistral 7B, the most powerful language model for its size to date.</p><h2 id="mistral-7b-in-short">Mistral 7B in short</h2><p>Mistral 7B is a 7.3B parameter model that:</p><ul><li>Outperforms Llama 2 13B on all benchmarks</li><li>Outperforms Llama 1 34B on many benchmarks</li><li>Approaches CodeLlama 7B performance on code, while remaining good at English tasks</li><li>Uses Grouped-query attention (GQA) for faster inference</li><li>Uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost</li></ul><p>We’re releasing Mistral 7B under the Apache 2.0 license, it can be used without restrictions.</p><ul><li><a href="https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar">Download it</a> and use it anywhere (including locally) with <a href="https://github.com/mistralai/mistral-src">our reference implementation</a></li><li>Deploy it on any cloud (AWS/GCP/Azure), using vLLM <a href="https://docs.mistral.ai/cloud-deployment/skypilot">inference server and skypilot</a></li><li>Use it on <a href="https://huggingface.co/mistralai">HuggingFace</a></li></ul><p>Mistral 7B is easy to fine-tune on any task. As a demonstration, we’re providing a model fine-tuned for chat, which outperforms Llama 2 13B chat.</p><h3 id="performance-in-details">Performance in details</h3><p>We compared Mistral 7B to the Llama 2 family, and re-run all model evaluations ourselves for fair comparison.</p><p><img src="https://mistral.ai/images/news/announcing-mistral-7b/230927_bars.png" alt="histograms">
<em>Performance of Mistral 7B and different Llama models on a wide range of benchmarks. For all metrics, all models were re-evaluated with our evaluation pipeline for accurate comparison. Mistral 7B significantly outperforms Llama 2 13B on all metrics, and is on par with Llama 34B (since Llama 2 34B was not released, we report results on Llama 34B). It is also vastly superior in code and reasoning benchmarks.</em></p><p>The benchmarks are categorized by their themes:</p><ul><li>Commonsense Reasoning: 0-shot average of Hellaswag, Winogrande, PIQA, SIQA, OpenbookQA, ARC-Easy, ARC-Challenge, and CommonsenseQA.</li><li>World Knowledge: 5-shot average of NaturalQuestions and TriviaQA.</li><li>Reading Comprehension: 0-shot average of BoolQ and QuAC.</li><li>Math: Average of 8-shot GSM8K with maj@8 and 4-shot MATH with maj@4</li><li>Code: Average of 0-shot Humaneval and 3-shot MBPP</li><li>Popular aggregated results: 5-shot MMLU, 3-shot BBH, and 3-5-shot AGI Eval (English multiple-choice questions only)</li></ul><p><img src="https://mistral.ai/images/news/announcing-mistral-7b/230927_table.png" alt="table"></p><p>An interesting metric to compare how models fare in the cost/performance plane is to compute “equivalent model sizes”. On reasoning, comprehension and STEM reasoning (MMLU), Mistral 7B performs equivalently to a Llama 2 that would be more than 3x its size. This is as much saved in memory and gained in throughput.
<img src="https://mistral.ai/images/news/announcing-mistral-7b/230927_effective_sizes.png" alt="effective_sizes">
<em>Results on MMLU, Commonsense Reasoning, World Knowledge and Reading comprehension for Mistral 7B and Llama 2 (7B/13/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except on knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which restricts the amount of knowledge it can compress).</em></p><p><strong>Note</strong>: Important differences between our evaluation and the LLaMA2 paper’s:</p><ul><li>For MBPP, we use the hand-verified subset</li><li>For TriviaQA, we do not provide Wikipedia contexts</li></ul><h3 id="flash-and-furious-attention-drift">Flash and Furious: Attention drift</h3><p>Mistral 7B uses a sliding window attention (SWA) mechanism (<a href="https://arxiv.org/pdf/1904.10509.pdf">Child et al.</a>, <a href="https://arxiv.org/pdf/2004.05150v2.pdf">Beltagy et al.</a>), in which each layer attends to the previous <code>4,096</code> hidden states.
The main improvement, and reason for which this was initially investigated, is a linear compute cost of O(sliding_window.seq_len). In practice, changes made to <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a> and <a href="https://facebookresearch.github.io/xformers">xFormers</a> yield a 2x speed improvement for sequence length of 16k with a window of 4k. A huge thanks to Tri Dao and Daniel Haziza for helping include these changes on a tight schedule.</p><p>Sliding window attention exploits the stacked layers of a transformer to attend in the past beyond the window size: A token <code>i</code> at layer <code>k</code> attends to tokens <code>[i-sliding_window, i]</code> at layer <code>k-1</code>. These tokens attended to tokens <code>[i-2*sliding_window, i]</code>. Higher layers have access to informations further in the past than what the attention patterns seems to entail.</p><p><img src="https://mistral.ai/images/news/announcing-mistral-7b/attention_local.png" alt="Local attention"></p><p>Finally, a fixed attention span means we can limit our cache to a size of <code>sliding_window</code> tokens, using rotating buffers (read more in our <a href="https://github.com/mistralai/mistral-src">reference implementation repo</a>). This saves half of the cache memory for inference on sequence length of <code>8192</code>, without impacting model quality.</p><h2 id="acknowledgements">Acknowledgements</h2><p>We are grateful to CoreWeave for their 24/7 help in marshalling our cluster. We thank the <a href="https://www.cineca.it/">CINECA/EuroHPC</a> team, and in particular the operators of Leonardo, for their resources and help. We thank the maintainers of <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a>, <a href="https://github.com/vllm-project/vllm">vLLM</a>, <a href="https://github.com/facebookresearch/xformers">xFormers</a>, <a href="https://github.com/skypilot-org/skypilot">SkyPilot</a>, <a href="https://github.com/huggingface/text-generation-inference">TGI</a> for their precious assistance in implementing new features and integrating their solutions into ours. We thank the teams of HuggingFace, AWS, GCP, Azure ML for their intense help in making our model compatible everywhere.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unions Work (149 pts)]]></title>
            <link>https://werd.io/2023/unions-work</link>
            <guid>37675422</guid>
            <pubDate>Wed, 27 Sep 2023 14:46:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://werd.io/2023/unions-work">https://werd.io/2023/unions-work</a>, See on <a href="https://news.ycombinator.com/item?id=37675422">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[ClickHouse Keeper: A ZooKeeper alternative written in C++ (168 pts)]]></title>
            <link>https://clickhouse.com/blog/clickhouse-keeper-a-zookeeper-alternative-written-in-cpp</link>
            <guid>37674967</guid>
            <pubDate>Wed, 27 Sep 2023 14:14:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clickhouse.com/blog/clickhouse-keeper-a-zookeeper-alternative-written-in-cpp">https://clickhouse.com/blog/clickhouse-keeper-a-zookeeper-alternative-written-in-cpp</a>, See on <a href="https://news.ycombinator.com/item?id=37674967">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>ClickHouse is the fastest and most resource-efficient open-source database for real-time applications and analytics. As one of its components, ClickHouse Keeper is a fast, more resource-efficient, and feature-rich alternative to ZooKeeper. This open-source component provides a highly reliable metadata store, as well as coordination and synchronization mechanisms. It was originally developed for use with ClickHouse when it is deployed as a distributed system in a self-managed setup or a hosted offering like CloudHouse Cloud. However, we believe that the broader community can benefit from this project in additional use cases.</p>
<p>In this post, we describe the motivation, advantages, and development of ClickHouse Keeper and preview our next planned improvements. Moreover, we introduce a reusable benchmark suite, which allows us to simulate and benchmark typical ClickHouse Keeper usage patterns easily. Based on this, we present benchmark results highlighting that ClickHouse Keeper uses <strong>up to 46 times less memory than ZooKeeper ​​for the same volume of data while maintaining performance close to ZooKeeper</strong>.</p>

<p>Modern <a href="https://en.wikipedia.org/wiki/Distributed_computing">distributed systems</a> require a shared and reliable <a href="https://en.wikipedia.org/wiki/Information_repository">information repository</a> and <a href="https://en.wikipedia.org/wiki/Consensus_(computer_science)">consensus</a> system for coordinating and synchronizing distributed operations. For ClickHouse, <a href="https://zookeeper.apache.org/">ZooKeeper</a> was initially chosen for this. It was reliable through its wide usage, provided a simple and powerful API, and offered reasonable performance.</p>
<p>However, not only performance but also resource efficiency and scalability have always been a top <a href="https://clickhouse.com/docs/en/faq/general/why-clickhouse-is-so-fast">priority</a> for ClickHouse. ZooKeeper, being a Java ecosystem project, did not fit into our primarily C++ codebase very elegantly, and as we used it at a higher and higher scale, we started running into resource usage and operational challenges. In order to overcome these shortcomings of ZooKeeper, we built ClickHouse Keeper from scratch, taking into account additional requirements and goals our project needed to address.</p>
<p>ClickHouse Keeper is a drop-in replacement for ZooKeeper, with a fully compatible client protocol and the same data model. Beyond that, it offers the following benefits:</p>
<ul>
<li>Easier setup and operation: ClickHouse Keeper is implemented in C++ instead of Java and, therefore, <a href="https://clickhouse.com/company/events/scaling-clickhouse?utm_source=google.com&amp;utm_medium=paid_search&amp;utm_campaign=19979782024_153259814612&amp;utm_content=655879611258&amp;utm_term=clickhouse_g_c&amp;gclid=Cj0KCQjwuZGnBhD1ARIsACxbAVgLga7td3T2ccBaJ9zCxt4t_A2RQT_5MdK-qqnLVvp0ufgElNk5JSoaAsMtEALw_wcB">can</a> run embedded in ClickHouse or standalone</li>
<li>Snapshots and logs consume much less disk space due to better compression</li>
<li>No limit on the default packet and node data size (it <a href="https://zookeeper.apache.org/doc/r3.4.11/zookeeperAdmin.html#Unsafe+Options">is</a> 1 MB in ZooKeeper)</li>
<li>No <a href="https://issues.apache.org/jira/browse/ZOOKEEPER-1277">ZXID overflow</a> issue (it forces a restart for every 2B transactions in ZooKeeper)</li>
<li>Faster recovery after network partitions due to the use of a better-distributed consensus protocol</li>
<li>Additional <a href="https://en.wikipedia.org/wiki/Consistency_model">consistency</a> guarantees: ClickHouse Keeper provides the same consistency guarantees as ZooKeeper - <a href="https://en.wikipedia.org/wiki/Linearizability">linearizable</a> writes plus strict ordering of operations inside the same <a href="https://zookeeper.apache.org/doc/r3.5.10/zookeeperProgrammers.html#ch_zkSessions">session</a>. Additionally, and optionally (via a <a href="https://clickhouse.com/docs/en/guides/sre/keeper/clickhouse-keeper#configuration">quorum_reads</a> setting), ClickHouse Keeper provides linearizable reads.</li>
<li>ClickHouse Keeper is more resource efficient and uses less memory for the same volume of data (we will demonstrate this later in this blog)</li>
</ul>
<p>The development of ClickHouse Keeper <a href="https://github.com/ClickHouse/ClickHouse/pull/19580">started</a> as an embedded service in the ClickHouse server in February 2021. In the same year, a standalone mode was <a href="https://github.com/ClickHouse/ClickHouse/pull/24059">introduced</a>, and <a href="https://jepsen.io/">Jepsen</a> tests were <a href="https://github.com/ClickHouse/ClickHouse/pull/21677">added</a> - every 6 hours, we run automated <a href="https://github.com/ClickHouse/ClickHouse/tree/master/tests/jepsen.clickhouse">tests</a> with several different workflows and failure scenarios to validate the correctness of the consensus mechanism.</p>
<p>At the time of writing this blog, ClickHouse Keeper has been production-ready for <a href="https://clickhouse.com/blog/clickhouse-22-3-lts-released#clickhouse-keeper">more</a> than one and a half years and has been deployed at scale in our own <a href="https://clickhouse.com/cloud">ClickHouse Cloud</a> since its first private preview launch in May 2022.</p>
<p>In the rest of the blog, we sometimes refer to ClickHouse Keeper as simply “Keeper,” as we often call it internally.</p>

<p>Generally, anything requiring consistency between multiple ClickHouse servers relies on Keeper:</p>
<ul>
<li>Keeper provides the coordination system for data <a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication">replication</a> in self-managed <a href="https://en.wikipedia.org/wiki/Shared-nothing_architecture">shared-nothing</a> ClickHouse <a href="https://clickhouse.com/company/events/scaling-clickhouse">clusters</a></li>
<li>Automatic <a href="https://clickhouse.com/blog/common-getting-started-issues-with-clickhouse#5-deduplication-at-insert-time">insert deduplication</a> for replicated tables of the <a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family">mergetree</a> engine family is based on block-hash-sums <a href="https://clickhouse.com/docs/en/operations/settings/merge-tree-settings#replicated-deduplication-window">stored</a> in Keeper</li>
<li>Keeper provides consensus for <a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree#mergetree-data-storage">part</a> names (based on sequential <a href="https://clickhouse.com/docs/en/development/architecture#block">block</a> numbers) and for assigning part <a href="https://clickhouse.com/blog/asynchronous-data-inserts-in-clickhouse#data-needs-to-be-batched-for-optimal-performance">merges</a> and <a href="https://clickhouse.com/docs/en/sql-reference/statements/alter#mutations">mutations</a> to specific cluster nodes</li>
<li>Keeper is used under the hood of the <a href="https://clickhouse.com/docs/en/engines/table-engines/special/keeper-map">KeeperMap table engine</a> which allows you to use Keeper as consistent key-value store with linearizable writes and sequentially consistent reads
<ul>
<li><a href="https://clickhouse.com/blog/building-real-time-applications-with-clickhouse-and-hex-notebook-keeper-engine">read</a> about an application utilizing this for implementing a task scheduling queue on top of ClickHouse</li>
<li><a href="https://github.com/ClickHouse/clickhouse-kafka-connect">Kafka Connect Sink</a> uses this table engine as a reliable <a href="https://github.com/ClickHouse/clickhouse-kafka-connect/blob/main/docs/DESIGN.md#storing-state">state store</a> for <a href="https://github.com/ClickHouse/clickhouse-kafka-connect/blob/main/docs/DESIGN.md#state-machine">implementing</a> exactly-once delivery <a href="https://github.com/ClickHouse/clickhouse-kafka-connect/blob/main/docs/DESIGN.md#addressing-exactly-once">guarantees</a></li>
</ul>
</li>
<li>Keeper <a href="https://clickhouse.com/blog/clickhouse-release-23-08#streaming-consumption-from-s3-sergei-katkovskiy-kseniia-sumarokova">keeps track</a> of consumed files in the <a href="https://clickhouse.com/docs/en/engines/table-engines/integrations/s3queue">S3Queue table engine</a></li>
<li><a href="https://clickhouse.com/docs/en/engines/database-engines/replicated">Replicated Database engine</a> stores all metadata in Keeper</li>
<li>Keeper is used for coordinating <a href="https://clickhouse.com/docs/en/operations/backup">Backups</a> with the <a href="https://clickhouse.com/docs/en/sql-reference/distributed-ddl">ON CLUSTER</a> clause</li>
<li><a href="https://clickhouse.com/docs/en/sql-reference/functions/udf">User defined functions</a> can be <a href="https://github.com/ClickHouse/ClickHouse/pull/46085">stored</a> in Keeper</li>
<li><a href="https://clickhouse.com/docs/en/operations/access-rights">Access control</a> information can be <a href="https://github.com/ClickHouse/ClickHouse/pull/27426">stored</a> in Keeper</li>
<li>Keeper is <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates">used</a> as a shared central store for all metadata in <a href="https://clickhouse.com/cloud">ClickHouse Cloud</a></li>
</ul>

<p>In the following sections, in order to observe (and later model in a benchmark) some of ClickHouse Cloud’s interaction with Keeper, we load a month of data from the <a href="https://clickhouse.com/docs/en/getting-started/example-datasets/wikistat">WikiStat</a> data set into a <a href="https://gist.github.com/tom-clickhouse/7c88c3a231c602b44382f2ffdf98148c">table</a> in a <a href="https://clickhouse.com/docs/en/cloud-quick-start">ClickHouse Cloud service</a> with 3 nodes. Each node has 30 CPU cores and 120 GB RAM. Each service uses its own dedicated ClickHouse Keeper service consisting of 3 servers, with 3 CPU cores and 2 GB RAM per Keeper server.</p>
<p>The following diagram illustrates this data-loading scenario:
<img src="https://clickhouse.com/uploads/Keeper_01_a59945bd61.png" alt="Keeper-01.png" node="[object Object]"></p>

<p>Via a data load <a href="https://gist.github.com/tom-clickhouse/0c1b4d70c4fbebd7a14eb756d1ebc914">query</a>, we load ~4.64 billion rows from ~740 compressed files (one file represents one specific hour of one specific day) in <a href="https://clickhouse.com/docs/en/sql-reference/table-functions/s3Cluster">parallel</a> with all three ClickHouse servers in ~ 100 seconds. The peak main memory usage on a single ClickHouse server was ~107 GB:</p>
<pre><code>0 rows in set. Elapsed: 101.208 sec. Processed 4.64 billion rows, 40.58 GB (45.86 million rows/s., 400.93 MB/s.)
Peak memory usage: 107.75 GiB.
</code></pre>

<p>For storing the data, the 3 ClickHouse servers together <a href="https://gist.github.com/tom-clickhouse/6a5ee7bff4ee7e724d0e2c326ab30354">created</a> 240 initial <a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree#mergetree-data-storage">parts</a> in <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates#clickhouse-cloud-enters-the-stage">object storage</a>. The average number of rows per initial part was ~19 million rows, respectively. The average size was ~100 MiB, and the total amount of inserted rows is 4.64 billion:</p>
<pre><code>┌─parts──┬─rows_avg──────┬─size_avg───┬─rows_total───┐
│ 240.00 │ 19.34 million │ 108.89 MiB │ 4.64 billion │
└────────┴───────────────┴────────────┴──────────────┘
</code></pre>
<p>Because our data load query utilizes the <a href="https://clickhouse.com/docs/en/sql-reference/table-functions/s3Cluster">s3Cluster</a> table function, the creation of the initial parts <a href="https://gist.github.com/tom-clickhouse/f9c683945ea805062f7f5f63bf8b1389">is</a> evenly distributed over the 3 ClickHouse servers of our ClickHouse Cloud services:</p>
<pre><code>┌─n─┬─parts─┬─rows_total───┐
│ 1 │ 86.00 │ 1.61 billion │
│ 2 │ 76.00 │ 1.52 billion │
│ 3 │ 78.00 │ 1.51 billion │
└───┴───────┴──────────────┘
</code></pre>

<p>During the data loading, in the background, ClickHouse <a href="https://gist.github.com/tom-clickhouse/05f40f98dbcc6b28be6de3f96668f37b">executed</a> 1706 part <a href="https://clickhouse.com/blog/asynchronous-data-inserts-in-clickhouse#data-needs-to-be-batched-for-optimal-performance">merges</a>, respectively:</p>
<pre><code>┌─merges─┐
│   1706 │
└────────┘
</code></pre>

<p>ClickHouse Cloud completely <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates#clickhouse-cloud-enters-the-stage">separates</a> the storage of data and metadata from the servers. All data parts <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates#shared-object-storage-for-data-availability">are</a> stored in shared object storage, and all metadata <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates#sharedmergetree-for-cloud-native-data-processing">is</a> stored in Keeper. When a ClickHouse server has written a new part to object storage (see ② above) or merged some parts to a new larger part (see ③ above), then this ClickHouse server is using a <a href="https://zookeeper.apache.org/doc/r3.4.3/api/org/apache/zookeeper/ZooKeeper.html#multi(java.lang.Iterable)">multi</a>-write transaction request for updating the metadata about the new part in Keeper. This information includes the name of the part, which files belong to the part, and where the blobs corresponding to files reside in object storage. Each server has a local cache with subsets of the metadata and <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates#sharedmergetree-for-cloud-native-data-processing">gets</a> automatically informed about data changes by a Keeper instance through a <a href="https://zookeeper.apache.org/doc/current/zookeeperProgrammers.html#ch_zkWatches">watch</a>-based subscription mechanism.</p>
<p>For our aforementioned initial part creations and background part merges, a total of ~18k Keeper requests were <a href="https://gist.github.com/tom-clickhouse/da9c0faee5f509fb0fae9c4ee5c4d667">executed</a>. This includes ~12k multi-write transaction requests (containing only write-subrequests). All other requests are a mix of read and write requests. Additionally, the ClickHouse servers received ~ 800 watch notifications from Keeper:</p>
<pre><code>total_requests:      17705
multi_requests:      11642
watch_notifications: 822
</code></pre>
<p>We can <a href="https://gist.github.com/tom-clickhouse/36b7e154f47411c5a37c764ae62a3fd8">see</a> how these requests were sent and how the watch notifications got received quite evenly from all three ClickHouse nodes:</p>
<pre><code>┌─n─┬─total_requests─┬─multi_requests─┬─watch_notifications─┐
│ 1 │           5741 │           3671 │                 278 │
│ 2 │           5593 │           3685 │                 269 │
│ 3 │           6371 │           4286 │                 275 │
└───┴────────────────┴────────────────┴─────────────────────┘
</code></pre>
<p>The following two charts visualize these Keeper requests <a href="https://gist.github.com/tom-clickhouse/3e3cafe83ed468b6d312ef5461dc3d03">during</a> the data-loading process:
<img src="https://clickhouse.com/uploads/Keeper_02_0e4fabf14d.png" alt="Keeper-02.png" node="[object Object]">
We can see that ~70% of the Keeper requests are multi-write transactions.</p>
<p>Note that the amount of Keeper requests can vary based on the ClickHouse cluster size, ingest settings, and data size. We briefly demonstrate how these three factors influence the number of generated Keeper requests.</p>

<p>If we load the data with 10 instead of 3 servers in parallel, we ingest the data more than 3 times faster (with the <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates">SharedMergeTree</a>):</p>
<pre><code>0 rows in set. Elapsed: 33.634 sec. Processed 4.64 billion rows, 40.58 GB (138.01 million rows/s., 1.21 GB/s.)
Peak memory usage: 57.09 GiB.
</code></pre>
<p>The higher number of servers generates more than 3 times the amount of Keeper requests:</p>
<pre><code>total_requests:      60925
multi_requests:      41767
watch_notifications: 3468
</code></pre>

<p>For our <a href="https://gist.github.com/tom-clickhouse/0c1b4d70c4fbebd7a14eb756d1ebc914">original</a> data load, run with 3 ClickHouse servers, we configured a max size of ~25 million rows per initial part to speed up ingest speed at the expense of higher memory usage. If, instead, we <a href="https://gist.github.com/tom-clickhouse/d67403a4e1663fcbc7f8d8c97ad8df08">run</a> the same data load with the <a href="https://clickhouse.com/docs/en/operations/settings/settings#settings-max_insert_block_size">default</a> value of ~1 million rows per initial part, then the data load is slower but uses ~9 times less main memory per ClickHouse server:</p>
<pre><code>0 rows in set. Elapsed: 121.421 sec. Processed 4.64 billion rows, 40.58 GB (38.23 million rows/s., 334.19 MB/s.)
Peak memory usage: 12.02 GiB.
</code></pre>
<p>And ~4 thousand instead of 240 initial parts <a href="https://gist.github.com/tom-clickhouse/6a5ee7bff4ee7e724d0e2c326ab30354">are</a> created:</p>
<pre><code>┌─parts─────────┬─rows_avg─────┬─size_avg─┬─rows_total───┐
│ 4.24 thousand │ 1.09 million │ 9.20 MiB │ 4.64 billion │
└───────────────┴──────────────┴──────────┴──────────────┘
</code></pre>
<p>This <a href="https://gist.github.com/tom-clickhouse/05f40f98dbcc6b28be6de3f96668f37b">causes</a> a higher number of part merges:</p>
<pre><code>┌─merges─┐
│   9094 │
└────────┘
</code></pre>
<p>And we <a href="https://gist.github.com/tom-clickhouse/da9c0faee5f509fb0fae9c4ee5c4d667">get</a> a higher number of Keeper requests (~147k instead of ~17k):</p>
<pre><code>total_requests:      147540
multi_requests:      105951
watch_notifications: 7439
</code></pre>

<p>Similarly, if we <a href="https://gist.github.com/tom-clickhouse/4431d998f24304917976e118c2e95b88">load</a> more data (with the default value of ~1 million rows per initial part), e.g. six months from the WikiStat data set, then we get a higher amount of ~24 thousand initial parts for our service:</p>
<pre><code>┌─parts──────────┬─rows_avg─────┬─size_avg─┬─rows_total────┐
│ 23.75 thousand │ 1.10 million │ 9.24 MiB │ 26.23 billion │
└────────────────┴──────────────┴──────────┴───────────────┘
</code></pre>
<p>Which <a href="https://gist.github.com/tom-clickhouse/05f40f98dbcc6b28be6de3f96668f37b">causes</a> more merges:</p>
<pre><code>┌─merges─┐
│  28959 │
└────────┘
</code></pre>
<p><a href="https://gist.github.com/tom-clickhouse/da9c0faee5f509fb0fae9c4ee5c4d667">Resulting</a> in ~680k Keeper requests:</p>
<pre><code>total_requests:      680996
multi_requests:      474093
watch_notifications: 32779
</code></pre>

<p>We developed a benchmark suite coined <a href="https://github.com/ClickHouse/examples/tree/main/keeper-bench-suite">keeper-bench-suite</a> for benchmarking the typical ClickHouse interactions with Keeper explored above. For this, keeper-bench-suite allows simulating the parallel Keeper workload from a ClickHouse cluster consisting of <code>N</code> (e.g. 3) servers:
<img src="https://clickhouse.com/uploads/Keeper_03_53c9cae6a1.png" alt="Keeper-03.png" node="[object Object]">
We are piggybacking on <a href="https://github.com/ClickHouse/ClickHouse/tree/master/utils/keeper-bench">keeper-bench</a>, a tool for benchmarking Keeper or any ZooKeeper-compatible system. With that building block, we can simulate and benchmark the typical parallel Keeper traffic from <code>N</code> ClickHouse servers. This diagram shows the complete architecture of the Keeper Bench Suite, which allows us to set up easily and benchmark arbitrary Keeper workload scenarios:
<img src="https://clickhouse.com/uploads/Keeper_04_3b57f3a2c4.png" alt="Keeper-04.png" node="[object Object]">
We are using an AWS <a href="https://aws.amazon.com/pm/ec2/">EC2</a> instance as a benchmark server for executing a <a href="https://github.com/ClickHouse/examples/blob/main/keeper-bench-suite/benchmark.py">Python script</a> which
<code>① sets up and starts a 3-node Keeper <a href="https://zookeeper.apache.org/doc/current/zookeeperStarted.html#sc_RunningReplicatedZooKeeper">cluster</a> by spinning up 3 appropriate (e.g., <a href="https://aws.amazon.com/ec2/instance-types/m6a/">m6a.4xlarge</a>) EC2 instances, each running one Keeper <a href="https://en.wikipedia.org/wiki/Docker_(software)">docker</a> container and two containers with <a href="https://github.com/google/cadvisor">cAdvisor</a> and <a href="https://redis.io/">Redis</a> (required by cAdvisor) for monitoring the resource usage of the local Keeper container<p>
② starts keeper-bench with a preconfigured workload configurations</p><p>
③ scrapes the <a href="https://prometheus.io/">Prometheus</a> endpoints of each cAdvisor and Keeper every 1 second</p><p>
④ writes the scraped metrics including timestamps into two <a href="https://github.com/ClickHouse/examples/tree/main/keeper-bench-suite#getting-started">tables</a> in a ClickHouse Cloud service which is the basis for conveniently analyzing the metrics via SQL queries, and <a href="https://grafana.com/">Grafana</a> dashboards
</p></code></p>
<p>Note that both <a href="https://github.com/ClickHouse/ClickHouse/pull/43087">ClickHouse Keeper</a> and <a href="https://zookeeper.apache.org/doc/r3.8.2/zookeeperMonitor.html">ZooKeeper</a> directly provide Prometheus endpoints. Currently, these endpoints only have a very small overlap and generally give quite different metrics, which makes it hard to compare them, especially in terms of memory and CPU usage. Therefore, we opted for additional cAdvisor-based basic container metrics. Plus, running Keeper in a docker container allows us to easily change the number of CPU cores and size of RAM provided to Keeper.</p>


<p>We run benchmarks with different docker container sizes for both ClickHouse Keeper and ZooKeeper. E.g. 1 CPU core + 1 GB RAM, 3 CPU cores + 1 GB RAM, 6 CPU cores + 6 GB RAM.</p>

<p>For each of the Keeper sizes, we simulate (with the <a href="https://github.com/ClickHouse/ClickHouse/tree/master/utils/keeper-bench#general-settings">concurrency</a> setting of keeper-bench) different numbers of clients (e.g., ClickHouse servers) sending requests in parallel to Keeper:  E.g. 3, 10, 100, 500, 1000.</p>
<p>From each of these simulated clients, to simulate both short and long-running Keeper sessions, we send (with the <a href="https://github.com/ClickHouse/ClickHouse/tree/master/utils/keeper-bench#general-settings">iterations</a> setting of keeper-bench) a total number between 10 thousand and ~10 million requests to Keeper. This aims to test whether memory usage of either component changes over time.</p>

<p>We simulated a typical ClickHouse workload containing ~1/3 write and delete operations and ~2/3 reads. This reflects a scenario where some data is ingested, merged, and then queried. It is easily possible to define and benchmark other workloads.</p>


<p>We use the Prometheus endpoint of cAdvisor for measuring</p>
<ul>
<li>Main memory usage (<a href="https://github.com/google/cadvisor/blob/release-v0.47/docs/storage/prometheus.md?plain=1#L67">container_memory_working_set_bytes</a>)</li>
<li>CPU usage (<a href="https://github.com/google/cadvisor/blob/release-v0.47/docs/storage/prometheus.md?plain=1#L30">container_cpu_usage_seconds_total</a>)</li>
</ul>
<p>We use the Prometheus endpoints of <a href="https://github.com/ClickHouse/ClickHouse/pull/43087">ClickHouse Keeper</a> and <a href="https://zookeeper.apache.org/doc/r3.8.2/zookeeperMonitor.html">ZooKeeper</a> for measuring additional (all available) Keeper Prometheus endpoint metric values. E.g. for ZooKeeper, many JVM-specific metrics (heap size and usage, garbage collection, etc.).</p>

<p>We also measure the runtime for Keeper processing all requests based on the minimum and maximum timestamps from each run.</p>

<p>We used the keeper-bench-suite to compare the resource consumption and runtime of ClickHouse Keeper and ZooKeeper for our workload. We ran each benchmark configuration 10 times and stored the results in <a href="https://github.com/ClickHouse/examples/tree/main/keeper-bench-suite#getting-started">two tables</a> in a ClickHouse Cloud service. We used a <a href="https://gist.github.com/tom-clickhouse/d156a83202b1b31ab34adc09c9167192">SQL query</a> for generating three tabular result tables:</p>
<ul>
<li><a href="https://gist.github.com/tom-clickhouse/e6edb87becb2b03939db06a0c1b0ff13">mean</a></li>
<li><a href="https://gist.github.com/tom-clickhouse/e3d9cfae1903f2e457131fa5820a08ea">95th percentiles</a></li>
<li><a href="https://gist.github.com/tom-clickhouse/a4c2ffbc85463ac9fac64571599365ae">99th percentiles</a></li>
</ul>
<p>The columns of these results are described <a href="https://gist.github.com/tom-clickhouse/2d3f292ee0aac762251626c7c3156966">here</a>.</p>
<p>We used <code>ClickHouse Keeper 23.5</code> and <code>ZooKeeper 3.8.</code> (with bundled <code>OpenJDK 11</code>) for all runs.
Note that we don’t print the three tabular results here, as each table contains 216 rows. You can inspect the results by following the links above.</p>

<p>Here, we present two charts, where we <a href="https://gist.github.com/tom-clickhouse/0cb1d340efeeea123f592de2e9d6bc3c">filtered</a> the 99th percentile results for rows where both Keeper versions run with 3 CPU cores and 2 GB of RAM, processing the same request sizes sent from 3 simulated clients (ClickHouse servers) in parallel. The tabular result for these visualizations is <a href="https://gist.github.com/tom-clickhouse/f7f165ad612ba81088817226e33a431d">here</a>.</p>

<p><img src="https://clickhouse.com/uploads/Keeper_05_ef049cc5e4.png" alt="Keeper-05.png" node="[object Object]">
We can see that for our simulated workload, ClickHouse Keeper consistently uses a lot less main memory than ZooKeeper for the same number of processed requests. E.g. for the benchmark run ③ processing 6.4 million requests sent by 3 simulated ClickHouse servers in parallel, ClickHouse Keeper uses ~46 times less main memory than ZooKeeper in run ④.</p>
<p>For ZooKeeper, we used a 1GiB JVM heap size configuration (<code>JVMFLAGS: -Xmx1024m -Xms1024m</code>) for all main runs (①, ②, ③), meaning that the committed JVM memory (reserved heap and non-heap memory is guaranteed to be available for use by the Java virtual machine) size is ~1GiB for these runs (see the transparent gray bars in the chart above for how much is used). In addition to the docker container memory usage (blue bars), we also measured the amount of (heap and non-heap) JVM memory actually used within the committed JVM memory (pink bars). There is some slight container memory <a href="https://stackoverflow.com/a/53624438">overhead</a> (difference of blue and pink bars) of running the JVM itself. However, the actual used JVM memory is still consistently significantly larger than the overall container memory usage of ClickHouse Keeper.</p>
<p>Furthermore, we can see that ZooKeeper uses the complete 1 GiB JVM heap size for run ③. We did an additional run ④ with an increased JVM heap size of 2 GiB for ZooKeeper, resulting in ZooKeeper using 1.56 GiB of its 2 GiB JVM heap, with an improved runtime matching the runtime of ClickHouse Keeper’s run ③. We present runtimes for all runs above in the next chart.</p>
<p>We can see in the tabular result that (major) garbage collection takes place a few times during the ZooKeeper runs.</p>

<p>The following chart visualizes runtimes and CPU usages for the runs discussed in the previous chart (the circled numbers are aligned in both charts):
<img src="https://clickhouse.com/uploads/Keeper_06_04cf699a40.png" alt="Keeper-06.png" node="[object Object]">
ClickHouse Keeper’s runtimes closely match ZooKeeper’s runtimes. Despite using significantly less main memory (see the previous chart) and CPU.</p>

<p>We <a href="https://clickhouse.com/blog/clickhouse-keeper-a-zookeeper-alternative-written-in-cpp#observing-keeper">observed</a> that ClickHouse Cloud often uses multi-write transactions in interactions with Keeper. We zoom in a bit deeper into ClickHouse Cloud’s interactions with Keeper to sketch two main scenarios for such Keeper transactions used by ClickHouse servers.</p>

<p><img src="https://clickhouse.com/uploads/Keeper_08_3b3e4bbd75.png" alt="Keeper-08.png" node="[object Object]">
In the scenario sketched above, server-2 ① processes data inserted into a table <a href="https://clickhouse.com/docs/en/development/architecture#block">block</a>-<a href="https://clickhouse.com/docs/en/operations/settings/settings#max_insert_block_size">wise</a>. For the current block, server-2 ② writes the data into a new data part in object storage, and ③ <a href="https://github.com/ClickHouse/ClickHouse/blob/776f232ec0b1b19b91d741f8fb76a437548b86c2/src/Storages/MergeTree/EphemeralLockInZooKeeper.cpp#L56">uses</a> a Keeper multi-write <a href="https://zookeeper.apache.org/doc/r3.4.3/api/org/apache/zookeeper/ZooKeeper.html#multi(java.lang.Iterable)">transaction</a> for storing metadata about the new part in Keeper, e.g., where the blobs corresponding to part files reside in object storage. Before storing this metadata, the transaction first tries to store the hash sum of the block processed in step ① in a <code>deduplication log</code> znode in Keeper. If the same hash sum value <a href="https://github.com/ClickHouse/ClickHouse/blob/776f232ec0b1b19b91d741f8fb76a437548b86c2/src/Storages/MergeTree/EphemeralLockInZooKeeper.cpp#L49">already</a> exists in the deduplication log, then the whole transaction <a href="https://github.com/ClickHouse/ClickHouse/blob/776f232ec0b1b19b91d741f8fb76a437548b86c2/src/Storages/MergeTree/EphemeralLockInZooKeeper.cpp#L61">fails</a> (is rolled back). Additionally, the data part from step ② is deleted because the data contained in that part was already inserted in the past. This automatic insert <a href="https://clickhouse.com/blog/common-getting-started-issues-with-clickhouse#5-deduplication-at-insert-time">deduplication</a> makes ClickHouse inserts <a href="https://clickhouse.com/blog/asynchronous-data-inserts-in-clickhouse#inserts-are-idempotent">idempotent</a> and, therefore, failure-tolerant, allowing clients to retry inserts without risking data duplication. On success, the transaction triggers child <a href="https://zookeeper.apache.org/doc/current/zookeeperProgrammers.html#ch_zkWatches">watches</a>, and ④ all Clickhouse servers <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates#sharedmergetree-for-cloud-native-data-processing">subscribed</a> to events for the part-metadata znodes are automatically notified by Keeper about new entries. This causes them to fetch metadata updates from Keeper into their local metadata caches.</p>

<p><img src="https://clickhouse.com/uploads/Keeper_09_bef5e28102.png" alt="Keeper-09.png" node="[object Object]">
When server-2 decides to merge some parts into a larger part, then the server ① uses a Keeper transaction for marking the to-be-merged parts as locked (to prevent other servers from merging them). Next, server-2 ② merges the parts into a new larger part, and ③ uses another Keeper transaction for storing metadata about the new part, which triggers watches ④ notifying all other servers about the new metadata entries.</p>
<p>Note that the above scenarios can only work correctly if such Keeper transactions are executed by Keeper atomically and sequentially. Otherwise, two ClickHouse servers sending the same data in parallel at the same time could potentially both not find the data’s hash sum in the deduplication log resulting in data duplication in object storage. Or multiple servers would merge the same parts. To prevent this, the ClickHouse servers rely on Keeper’s all-or-nothing multi-write transactions plus its linearizable writes guarantee.</p>

<p>The <a href="https://betterprogramming.pub/demystifying-consensus-algorithms-and-their-implementations-c52f8aca3020">consensus algorithms</a> in ZooKeeper and ClickHouse Keeper, <a href="https://zookeeper.apache.org/doc/r3.4.13/zookeeperInternals.html#sc_atomicBroadcast">ZAB</a>, and <a href="https://raft.github.io/">Raft</a>, respectively,  both ensure that multiple distributed servers can reliably agree on the same information. e.g. which parts are allowed to be merged in the example above.</p>
<p>ZAB is a dedicated consensus mechanism for ZooKeeper and has been in development <a href="https://dl.acm.org/doi/10.1145/1529974.1529978">since</a> at least 2008.</p>
<p>We chose Raft as our consensus mechanism because of its simple and <a href="https://raft.github.io/raft.pdf">easy-to-understand</a> algorithm and the availability of a lightweight and easy-to-integrate <a href="https://tech.ebayinc.com/engineering/nuraft-a-lightweight-c-raft-core/">C++ library</a> when we started the Keeper project in 2021.</p>
<p>However, all consensus algorithms are isomorphic to each other. For <a href="https://en.wikipedia.org/wiki/Linearizability">linearizable</a> writes, (dependent) transitions and the write operations within the transaction must be processed in strict order, one at a time, regardless of which consensus algorithm is used. Suppose ClickHouse servers are sending transactions in parallel to Keeper, and these transactions are dependent because they write to the same znodes (e.g., the <code>deduplication log</code> in our example scenario at the beginning of this section). In that case, Keeper can guarantee and implement linearizability only by executing such transactions and their operations strictly sequentially:
<img src="https://clickhouse.com/uploads/Keeper_10_aa840f2251.png" alt="Keeper-10.png" node="[object Object]">
For this, ZooKeeper implements write request processing using a single-threaded <a href="https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/SyncRequestProcessor.java">request processor</a>, whereas Keeper’s NuRaft implementation uses a single-threaded <a href="https://github.com/eBay/NuRaft/blob/v2.0/src/global_mgr.cxx#L252">global queue</a>.</p>
<p>Generally, linearizability makes it hard to scale write processing speed vertically (more CPU cores) or horizontally (more servers). It would be possible to analyze and identify independent transactions and run them in parallel, but currently, neither ZooKeeper nor ClickHouse Keeper implements this. This chart (where we filtered the 99th percentile results) highlights this:
<img src="https://clickhouse.com/uploads/Keeper_07_2f3bf2e688.png" alt="Keeper-07.png" node="[object Object]">
Both ZooKeeper and ClickHouse Keeper are running with 1, 3, and 6 CPU cores and processing 1.28 million total requests sent in parallel from 500 clients.</p>
<p>The performance of (non-linearizable) read requests and auxiliary tasks (managing network requests, batching data, etc.) can be scaled theoretically with the number of CPU cores with both ZAB and Raft. Our benchmark results generally show that ZooKeeper is currently doing this better than Clickhouse Keeper, although we are consistently improving our performance (<a href="https://github.com/ClickHouse/ClickHouse/pull/43686">three</a> <a href="https://github.com/ClickHouse/ClickHouse/pull/47978">recent</a> <a href="https://github.com/ClickHouse/ClickHouse/pull/53049">examples</a>).</p>

<p>Looking forward, we see the need to extend Keeper to better support the scenarios we described above. So, we are taking a big step with this project – introducing a multi-group Raft protocol for Keeper.</p>
<p>Because, as explained above, scaling non-partitioned (non-sharded) linearizability is impossible, we will focus on <a href="https://github.com/ClickHouse/ClickHouse/issues/54172">Multi-group Raft</a> where we <a href="https://tikv.org/deep-dive/scalability/multi-raft/">partition</a> the data stored in Keeper. This allows more transactions to be independent (working over separate partitions) from each other. By using a separate Raft instance inside the same server for each partition, Keeper automatically executes independent transactions in parallel:
<img src="https://clickhouse.com/uploads/Keeper_11_72b63a9e55.png" alt="Keeper-11.png" node="[object Object]">
With multi-Raft, Keeper will be able to enable workloads with much higher parallel read/write requirements, such as for instance, very large ClickHouse clusters with 100s of nodes.</p>

<p>Sounds exciting? Then, we invite you to join the Keeper community.</p>
<ul>
<li><a href="https://clickhouse.com/docs/en/guides/sre/keeper/clickhouse-keeper">This</a> is how you use Keeper with ClickHouse</li>
<li>To become a user of Keeper outside of ClickHouse - check out <a href="https://clickhouse.com/clickhouse/keeper">this</a> page when to use it or not</li>
<li><a href="https://clickhouse.com/slack">This</a> is where you post questions; you can follow us on <a href="https://twitter.com/ClickHouseDB">X</a> and <a href="https://github.com/ClickHouse/ClickHouse#upcoming-events">join</a> our meetups and events.</li>
</ul>
<p>We welcome contributions to the Keeper codebase. See our roadmap <a href="https://github.com/ClickHouse/ClickHouse/labels/comp-keeper">here</a>, and see our contributor guidelines <a href="https://github.com/ClickHouse/ClickHouse/blob/master/CONTRIBUTING.md">here</a>.</p>

<p>In this blog post, we described the features and advantages of ClickHouse Keeper - a resource-efficient open-source drop-in replacement for ZooKeeper. We explored our own usage of it in ClickHouse Cloud and, based on this, presented a benchmark suite and results highlighting that ClickHouse Keeper consistently uses significantly fewer hardware resources than ZooKeeper with comparable performance numbers. We also shared our roadmap and ways you can get involved. We invite you to collaborate!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deno Queues (262 pts)]]></title>
            <link>https://deno.com/blog/queues</link>
            <guid>37674752</guid>
            <pubDate>Wed, 27 Sep 2023 13:58:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deno.com/blog/queues">https://deno.com/blog/queues</a>, See on <a href="https://news.ycombinator.com/item?id=37674752">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In the ever-evolving world of cloud software, Deno aims to radically simplify.
Leveraging public cloud infrastructure has traditionally demanded sifting
through layers of boilerplate code and intricate configurations, often
monopolizing a significant chunk of the developer’s time and energy. Our goal is
to distill these intricacies into user-friendly primitives, enabling developers
to design, refine, and launch their projects with unmatched speed.</p>
<p>With this in mind, we rolled out <strong><a href="https://deno.com/kv" rel="noopener noreferrer">Deno KV</a></strong> a few months
ago (<strong><a href="https://deno.com/blog/kv-open-beta" rel="noopener noreferrer">currently in open beta</a></strong>). Anchored
on the robust capabilities of FoundationDB, Deno KV is more than just a new
persistence option for apps. It’s about transforming the developer experience by
eliminating redundant configurations and offering a refreshingly streamlined
API.</p>
<p>Building upon this foundation (pun intended), we are elated to unveil <strong>Deno
Queues</strong> today. This tool is set to revolutionize scalable messaging and elevate
the management of background processing in your applications.</p>
<div><pre><span>const</span> db <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span><span>)</span><span>;</span>

db<span>.</span><span>listenQueue</span><span>(</span><span>async</span> <span>(</span>msg<span>)</span> <span>=&gt;</span> <span>{</span>
  <span>await</span> <span>postToSlack</span><span>(</span>msg<span>.</span><span>channel</span><span>,</span> msg<span>.</span><span>text</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span>

<span>await</span> db<span>.</span><span>enqueue</span><span>(</span><span>{</span> channel<span>:</span> <span>"C123456"</span><span>,</span> text<span>:</span> <span>"Slack message"</span> <span>}</span><span>,</span> <span>{</span>
  delay<span>:</span> <span>60000</span><span>,</span>
<span>}</span><span>)</span><span>;</span></pre></div><p>In this post, we’ll cover key aspects of Deno Queues:</p>
<ul>
<li><a href="#what-are-deno-queues">What are Deno Queues?</a></li>
<li><a href="#use-cases-and-examples">Use cases and examples</a></li>
<li><a href="#pricing-for-deno-queues">Pricing</a></li>
<li><a href="#whats-next">What’s next</a></li>
</ul>
<h2 id="what-are-deno-queues">What are Deno Queues?</h2><p>Deno Queues, built on Deno KV, allow you to offload parts of your application or
schedule work for the future to run asynchronously, with two new simple APIs
with zero configuration or infrastructure to maintain:</p>
<ul>
<li><code>.enqueue()</code>: Pushes new messages into the queue for guaranteed delivery
immediately or at a time in the future.</li>
<li><code>.listenQueue()</code>: Handler used for processing new messages from the queue.</li>
</ul>
<p>Since Queues are built on Deno KV, it uses SQLite when running locally and
FoundationDB when running on Deno Deploy for maximum availability and
throughput.</p>
<p>Running Queues on Deno Deploy is optimized for performance. Deno Deploy
automatically spins up V8 isolates on-demand and dispatches messages when
they’re available for processing. Your application code simply listens to new
messages with <code>listenQueue</code> handler, and Deno Deploy handles the rest.</p>
<p><strong>Deno Queues guarantees at-least-once delivery.</strong> For most enqueued messages,
the <code>listenQueue</code> handler will be invoked once. In some failure instances, the
handler may be invoked multiple times to ensure delivery. It’s important to
design your applications to ensure that duplicate messages are handled
correctly.</p>
<p>You can also combine
<a href="https://docs.deno.com/kv/manual/queue_overview#queue-api-with-kv-atomic-transactions" rel="noopener noreferrer">Queues with KV atomic transactions</a>
primitives, which can unlock powerful workflows. For example, you may add
messages to the queue as part of a KV transaction, which succeeds or fails
atomically:</p>
<figure>

<div><pre><span>const</span> kv <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span><span>)</span><span>;</span>
<span>const</span> change <span>=</span> <span>10</span><span>;</span>

<span>const</span> bob <span>=</span> <span>await</span> kv<span>.</span><span>get</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"bob"</span><span>]</span><span>)</span><span>;</span>
<span>const</span> liz <span>=</span> <span>await</span> kv<span>.</span><span>get</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"liz"</span><span>]</span><span>)</span><span>;</span>
<span>if</span> <span>(</span>bob<span>.</span><span>value</span> <span>&lt;</span> change<span>)</span> <span>{</span>
  <span>throw</span> <span>"not enough balance"</span><span>;</span>
<span>}</span>

<span>const</span> success <span>=</span> <span>await</span> kv<span>.</span><span>atomic</span><span>(</span><span>)</span>
  <span>.</span><span>check</span><span>(</span>bob<span>,</span> liz<span>)</span> 
  <span>.</span><span>set</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"bob"</span><span>]</span><span>,</span> bob<span>.</span><span>value</span> <span>-</span> change<span>)</span>
  <span>.</span><span>set</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"liz"</span><span>]</span><span>,</span> liz<span>.</span><span>value</span> <span>+</span> change<span>)</span>
  
  <span>.</span><span>enqueue</span><span>(</span><span>{</span> type<span>:</span> <span>"notify"</span><span>,</span> name<span>:</span> <span>"liz"</span><span>,</span> amount<span>:</span> change <span>}</span><span>)</span>
  <span>.</span><span>enqueue</span><span>(</span><span>{</span> type<span>:</span> <span>"notify"</span><span>,</span> name<span>:</span> <span>"bob"</span><span>,</span> amount<span>:</span> <span>-</span>change <span>}</span><span>)</span>
  <span>.</span><span>commit</span><span>(</span><span>)</span><span>;</span></pre></div><figcaption>Enqueue new messages as part of an atomic transaction — only if the entire
transaction succeeds, they will be enqueued.</figcaption>

</figure>

<p>You can also update Deno KV state from your <code>listenQueue</code> handler. For instance,
if you want to ensure that updates on each message is performed only once, you
can also use the Queue API with KV atomic transactions:</p>
<figure>

<div><pre><span>const</span> db <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span><span>)</span><span>;</span>

db<span>.</span><span>listenQueue</span><span>(</span><span>async</span> <span>(</span>msg<span>)</span> <span>=&gt;</span> <span>{</span>
  <span>const</span> nonce <span>=</span> <span>await</span> db<span>.</span><span>get</span><span>(</span><span>[</span><span>"nonces"</span><span>,</span> msg<span>.</span><span>nonce</span><span>]</span><span>)</span><span>;</span>
  <span>if</span> <span>(</span>nonce<span>.</span><span>value</span> <span>===</span> <span>null</span><span>)</span> <span>{</span>
    
    <span>return</span><span>;</span>
  <span>}</span>

  <span>const</span> change <span>=</span> msg<span>.</span><span>change</span><span>;</span>
  <span>const</span> bob <span>=</span> <span>await</span> db<span>.</span><span>get</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"bob"</span><span>]</span><span>)</span><span>;</span>
  <span>const</span> liz <span>=</span> <span>await</span> db<span>.</span><span>get</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"liz"</span><span>]</span><span>)</span><span>;</span>

  <span>const</span> success <span>=</span> <span>await</span> db<span>.</span><span>atomic</span><span>(</span><span>)</span>
    
    <span>.</span><span>check</span><span>(</span><span>{</span> key<span>:</span> nonce<span>.</span><span>key</span><span>,</span> versionstamp<span>:</span> nonce<span>.</span><span>versionstamp</span> <span>}</span><span>)</span>
    <span>.</span><span>delete</span><span>(</span>nonce<span>.</span><span>key</span><span>)</span>
    <span>.</span><span>sum</span><span>(</span><span>[</span><span>"processed_count"</span><span>]</span><span>,</span> <span>1n</span><span>)</span>
    <span>.</span><span>check</span><span>(</span>bob<span>,</span> liz<span>)</span> 
    <span>.</span><span>set</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"bob"</span><span>]</span><span>,</span> bob<span>.</span><span>value</span> <span>-</span> change<span>)</span>
    <span>.</span><span>set</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"liz"</span><span>]</span><span>,</span> liz<span>.</span><span>value</span> <span>+</span> change<span>)</span>
    <span>.</span><span>commit</span><span>(</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span>

<span>const</span> nonce <span>=</span> crypto<span>.</span><span>randomUUID</span><span>(</span><span>)</span><span>;</span>
<span>await</span> db
  <span>.</span><span>atomic</span><span>(</span><span>)</span>
  <span>.</span><span>check</span><span>(</span><span>{</span> key<span>:</span> <span>[</span><span>"nonces"</span><span>,</span> nonce<span>]</span><span>,</span> versionstamp<span>:</span> <span>null</span> <span>}</span><span>)</span>
  <span>.</span><span>enqueue</span><span>(</span><span>{</span> nonce<span>,</span> change<span>:</span> <span>10</span> <span>}</span><span>)</span>
  <span>.</span><span>set</span><span>(</span><span>[</span><span>"nonces"</span><span>,</span> nonce<span>]</span><span>,</span> <span>true</span><span>)</span>
  <span>.</span><span>sum</span><span>(</span><span>[</span><span>"enqueued_count"</span><span>]</span><span>,</span> <span>1n</span><span>)</span>
  <span>.</span><span>commit</span><span>(</span><span>)</span><span>;</span></pre></div><figcaption>This example uses KV atomic transactions to ensure each message is updated only
once.</figcaption>

</figure>

<p>Additionally, if your <code>listenQueue</code> handler throws an exception, the runtime
will automatically retry to call the handler again until it succeeds or until
maximum retry attempts are reached. If maximum attempts (current default is 5)
are reached, the message will be dropped.</p>
<h2 id="use-cases-and-examples">Use cases and examples</h2><p>Queues are useful in scaling applications by allowing servers to offload async
processes and scheduling work for the future.</p>
<p>Below are a few examples.</p>
<h3 id="scheduled-email-notifications">Scheduled email notifications</h3><p>Sometimes a job or task that’s initiated by your user may take enough time where
you don’t want to make them wait for a “task complete” response or there’s no
need to send them a response. This is when you can offload work to a queue to
keep your server or app responsive for your user.</p>
<p>Here’s how you would use Queues to send email notifications:</p>
<div><pre><span>const</span> db <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span><span>)</span><span>;</span>

db<span>.</span><span>listenQueue</span><span>(</span><span>async</span> <span>(</span>msg<span>)</span> <span>=&gt;</span> <span>{</span>
  <span>if</span> <span>(</span>msg<span>.</span><span>type</span> <span>===</span> <span>"welcome_email"</span><span>)</span> <span>{</span>
    <span>await</span> <span>sendWelcomeEmail</span><span>(</span>msg<span>.</span><span>user_id</span><span>)</span><span>;</span>
  <span>}</span> <span>else</span> <span>if</span> <span>(</span>msg<span>.</span><span>type</span> <span>===</span> <span>"survey_email"</span><span>)</span> <span>{</span>
    <span>await</span> <span>sendSurveyEmail</span><span>(</span>msg<span>.</span><span>user_id</span><span>)</span><span>;</span>
  <span>}</span>
<span>}</span><span>)</span><span>;</span>

<span>await</span> db<span>.</span><span>enqueue</span><span>(</span>
  <span>{</span> type<span>:</span> <span>"welcome_email"</span><span>,</span> customer_id<span>:</span> <span>123</span> <span>}</span><span>,</span>
<span>)</span><span>;</span>

<span>await</span> db<span>.</span><span>enqueue</span><span>(</span>
  <span>{</span> type<span>:</span> <span>"survey_email"</span><span>,</span> customer_id<span>:</span> <span>123</span> <span>}</span><span>,</span>
  <span>{</span> delay<span>:</span> <span>259200000</span> <span>}</span><span>,</span> 
<span>)</span><span>;</span></pre></div><h3 id="reliable-webhook-processing">Reliable webhook processing</h3><p>Another extremely common example of using queues on the web is through
processing webhooks. Here’s an example using Oak and Queues to handle webhooks
asynchronously:</p>
<div><pre><span>import</span> <span><span>{</span> <span>Application</span><span>,</span> <span>Router</span> <span>}</span></span> <span>from</span> <span>"https://deno.land/x/oak@v12.6.1/mod.ts"</span><span>;</span>

<span>const</span> db <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span><span>)</span><span>;</span>

db<span>.</span><span>listenQueue</span><span>(</span><span>async</span> <span>(</span>msg<span>)</span> <span>=&gt;</span> <span>{</span>
  <span>await</span> <span>processWebHook</span><span>(</span>msg<span>.</span><span>webhook_body</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span>

<span>const</span> router <span>=</span> <span>new</span> <span>Router</span><span>(</span><span>)</span><span>;</span>
router<span>.</span><span>post</span><span>(</span><span>"/webhook"</span><span>,</span> <span>async</span> <span>(</span>ctx<span>)</span> <span>=&gt;</span> <span>{</span>
  db<span>.</span><span>enqueue</span><span>(</span><span>{</span> webhook_body<span>:</span> <span>await</span> ctx<span>.</span><span>request</span><span>.</span><span>body</span><span>(</span><span>)</span><span>.</span><span>value</span> <span>}</span><span>)</span><span>;</span>
  ctx<span>.</span><span>response</span><span>.</span><span>status</span> <span>=</span> <span>200</span><span>;</span>
<span>}</span><span>)</span><span>;</span>

<span>const</span> app <span>=</span> <span>new</span> <span>Application</span><span>(</span><span>)</span><span>;</span>
app<span>.</span><span>use</span><span>(</span>router<span>.</span><span>routes</span><span>(</span><span>)</span><span>)</span><span>;</span>
app<span>.</span><span>use</span><span>(</span>router<span>.</span><span>allowedMethods</span><span>(</span><span>)</span><span>)</span><span>;</span>

<span>await</span> app<span>.</span><span>listen</span><span>(</span><span>{</span> port<span>:</span> <span>8000</span> <span>}</span><span>)</span><span>;</span></pre></div><h3 id="slack-reminder-bot">Slack Reminder Bot</h3><p>Queues is great for building bots in Discord or Slack.</p>
<p><a href="https://github.com/igorzi/queue-reminder" rel="noopener noreferrer">Here’s an example</a> of using Deno
Queues to create a simple reminder app in Slack.</p>
<p><img src="https://deno.com/blog/queues/slack-reminder-slack-message.png" alt="Receiving a reminder in Slack" title=""></p>
<p>And <a href="https://github.com/Jabolol/kiwi" rel="noopener noreferrer">this is a Discord bot</a> that uses Deno
Queues to create giveaways and allow users to join with a single click.</p>
<p><img src="https://deno.com/blog/queues/discord-giveaway-bot-1.png" alt="Creating a giveaway in Discord" title=""></p>
<h3 id="more-examples">More examples</h3><p>More examples of queue usage can be found at
<a href="https://docs.deno.com/kv/manual/queue_overview#use-cases" rel="noopener noreferrer">docs.deno.com</a>.</p>
<h2 id="pricing-for-deno-queues">Pricing for Deno Queues</h2><p>As you explore the capabilities of Queues, it’s important to grasp the cost
implications. Queues has no specific cost of its own, but rather charged in
terms of Deno KV operations and Deno Deploy requests (for listening).
Specifically:</p>
<p><strong>Enqueuing a Message</strong>: Each enqueue action translates into a KV write
operation.</p>
<p><strong>Receiving a Message</strong>: Every received message entails a KV write, and a single
request charge.</p>
<p>This transparent pricing structure ensures you’re only billed for the operations
you use, aligning with our commitment to efficiency and simplicity.</p>
<h2 id="other-resources">Other resources</h2><ul>
<li><a href="https://docs.deno.com/kv/manual/queue_overview" rel="noopener noreferrer">Queues docs</a></li>
<li><a href="https://docs.deno.com/kv/manual/queue_overview#queues-on-deno-deploy" rel="noopener noreferrer">Queues on Deno Deploy docs</a></li>
<li><a href="https://deno.com/api@v1.37.0?unstable=&amp;s=Deno.Kv&amp;p=prototype.enqueue" rel="noopener noreferrer"><code>.enqueue</code> API reference</a></li>
<li><a href="https://deno.com/api@v1.37.0?unstable=&amp;s=Deno.Kv&amp;p=prototype.listenQueue" rel="noopener noreferrer"><code>.listenQueue</code> API reference</a></li>
</ul>
<h2 id="whats-next">What’s next</h2><p>Building scalable apps and servers on the web requires offloading background
tasks to queues. However, there are many steps in configuring them for use. Deno
Queues, built right into the runtime and on top of robust infrastructure of Deno
Deploy, lets you use serverless, distributed queues in only a few lines of code.</p>
<p>Deno Queues joins <a href="https://deno.com/kv" rel="noopener noreferrer">Deno KV</a>,
<a href="https://deno.land/manual/runtime/web_platform_apis" rel="noopener noreferrer">web standards APIs</a>,
<a href="https://deno.com/blog/npm-on-deno-deploy" rel="noopener noreferrer">npm</a>, and
<a href="https://deno.land/manual/tools" rel="noopener noreferrer">all-in-one modern tooling</a> as key building
blocks that make creating for the web simpler and more productive. We are still
a long ways to go from our goal and have many more exciting features on the
roadmap. Stay tuned.</p>
<p>We’re always open to feedback and feature requests! Feel free to
<a href="https://discord.gg/deno" rel="noopener noreferrer">join our growing Discord</a> or
<a href="https://github.com/denoland/deploy_feedback/issues" rel="noopener noreferrer">create an issue here</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rusty revenant Servo returns to render once more (174 pts)]]></title>
            <link>https://www.theregister.com/2023/09/27/servo_returns/</link>
            <guid>37674519</guid>
            <pubDate>Wed, 27 Sep 2023 13:40:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/09/27/servo_returns/">https://www.theregister.com/2023/09/27/servo_returns/</a>, See on <a href="https://news.ycombinator.com/item?id=37674519">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p><span>Open Source Summit</span> A pleasant surprise from Open Source Summit is that Servo, the Rusty rendering engine that Mozilla was working on – until COVID, that is – is showing green shoots of renewed vigor.</p>
<p>Servo has been around for about a decade, so as experimental software projects go, it's a mature one. Igalia developer Manuel Rego presented a <a target="_blank" href="https://osseu2023.sched.com/event/1OGkc/servo-web-rendering-engine-reboot-manuel-rego-igalia" rel="nofollow">talk</a> which reports that the project is back under active development, almost exactly three years after <a target="_blank" href="https://www.theregister.com/2020/08/14/mozilla_google_search/">Mozilla terminated its Rust efforts</a> and laid off the whole Rust team, including the Servo developers.</p>
<p>In November 2020, <a target="_blank" href="https://www.theregister.com/2020/11/18/firefox_83/">the Linux Foundation adopted Servo</a>. However, the global operation has a lot of <a target="_blank" href="https://www.linuxfoundation.org/projects" rel="nofollow">projects</a> – we think we count 625 of them, but we could be wrong. Early this year, it handed the project over to its <a target="_blank" href="https://www.theregister.com/2022/09/20/linux_foundation_europe/">new European division</a>, which has a slightly more manageable <a target="_blank" href="https://linuxfoundation.eu/en/projects" rel="nofollow">list</a> of four, among them the <a target="_blank" href="https://www.theregister.com/2022/09/16/open_standards_digital_wallets/">OpenWallet foundation</a> and the <a target="_blank" href="https://www.theregister.com/2023/06/01/linux_foundation_risc_v/">RISC-V Software Ecosystem</a>. Now this also includes <a target="_blank" href="https://servo.org/" rel="nofollow">Servo</a>.</p>

    

<p>Servo first appeared <a target="_blank" href="https://www.theregister.com/2013/04/03/samsung_helps_mozilla_with_servo/">in tandem with Rust</a> a full decade ago, and by 2016 Mozilla <a target="_blank" href="https://www.theregister.com/2016/03/15/mozilla_to_release_first_version_of_servobased_browser_in_june/">was discussing releasing a prototype</a>. Previews <a target="_blank" href="https://www.theregister.com/2016/03/15/mozilla_to_release_first_version_of_servobased_browser_in_june/">started to appear that July</a>, when as we put it: "If Google has the language of Go, Moz has the language of No: Rust."</p>

        


        

<p>The new engine is quite capable. It supports the <a target="_blank" href="https://www.theregister.com/2011/03/03/webgl_one_dot_o_released/">now elderly WebGL API</a> as well as its <a target="_blank" href="https://www.theregister.com/2017/02/08/apple_webgpu/">more modern successor WebGPU</a>, which is much more powerful. For now, Rego said, it is mainly aimed at Windows, macOS, and desktop Linux, although the team is also testing mobile versions for both Android and more generic Linux, initially being tested on <a target="_blank" href="https://www.theregister.com/2021/12/08/pinephone_ships_developers/">Pine64's PinePhone Pro hardware</a>.</p>
<p>As well as being independent of any browser vendor, it is designed to be embeddable, memory-safe, modular, and parallel. The latter in particular benefits from the concurrency features provided by Rust. So far this year, the project has seen 1,682 commits from 77 developers, compared to just 523 from 22 people in 2022. A big change has been a new layout engine, replacing what is now called the legacy engine.</p>
<ul>

<li><a href="https://www.theregister.com/2023/09/20/gnu_turns_40/">GNU turns 40: Stallman's baby still not ready for prime time, but hey, there's cake</a></li>

<li><a href="https://www.theregister.com/2023/09/19/ubuntu_2310_taking_shape/">Ubuntu's 'Mantic Minotaur' peeks out of the labyrinth</a></li>

<li><a href="https://www.theregister.com/2023/09/14/pc_xt_with_hdmi/">These days you can teach old tech a bunch of new tricks</a></li>

<li><a href="https://www.theregister.com/2023/09/13/linux_mint_debian_edition_hands_on/">Linux Mint Debian Edition 6 hits beta with reassuringly little drama</a></li>
</ul>
<p>It still can't pass the <a target="_blank" href="https://www.theregister.com/2008/03/05/acid_three_browser_flunk/">Web Standards Project ACID tests</a>, which way back in 2008 WebKit was the <a target="_blank" href="https://www.theregister.com/2008/10/01/webkit_acid/">first browser to successfully handle</a>, so there's clearly some way to go yet. Even so, Rego's <a target="_blank" href="https://static.sched.com/hosted_files/osseu2023/be/2023-09-21-open-source-summit-europe-servo.pdf" rel="nofollow">presentation</a> [PDF] illustrates the improvements it's made so far this year.</p>
<p>At least for now, its goals have been scaled back from being a full web browser in its own right. One of the targets, though, is as a web runtime that can be embedded into standalone local web apps. At the moment, <a target="_blank" href="https://www.electronjs.org/" rel="nofollow">Electron.js</a> is the dominant tool in this space, but it's based on the Chromium engine, and is therefore another cog in the giant Google machine. However, for that to work, it also needs a JavaScript runtime, which is something that Servo doesn't include. For that, it depends on Mozilla's <a target="_blank" href="https://spidermonkey.dev/" rel="nofollow">SpiderMonkey</a>, which is also the basis of <a target="_blank" href="https://www.theregister.com/2022/08/17/gnome_project_25/">the GNOME desktop's GJS</a> and accordingly <a target="_blank" href="https://www.theregister.com/2023/03/02/linux_mint_212/">Cinnamon's CJS too</a>.</p>

        

<p>The SpiderMonkey code base is much less modern than Servo itself: it <a target="_blank" href="https://openhub.net/p/spidermonkey/analyses/latest/languages_summary" rel="nofollow">consists</a> of about one half C++ and one quarter C, plus a sprinkling of Java. For Rustaceans, we suspect this counts as embarrassing legacy code. A Rust JavaScript runtime called <a target="_blank" href="https://github.com/boa-dev/boa" rel="nofollow">Boa</a> does exist, but it's still in the early stages of development.</p>
<p>There aren't many modern web-rendering engines in existence, and several of them are close relatives: Chrome's Blink is derived from Apple's WebKit, itself derived from KDE's KHTML. Firefox's Gecko is the principal independent one still standing. Since Mozilla canceled development of its successor, it's good to know that it's in active development again. Much of the world is held together by the web, and if that were entirely controlled by one company, it would be <a target="_blank" href="https://xkcd.com/1118/" rel="nofollow">scary</a>. ®</p>
<p>
  <a href="https://www.youtube.com/watch?v=e3Y1C695CIw&amp;t=4100s" data-media="x-videoplayer">Youtube Video</a>
</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The most copied StackOverflow snippet of all time is flawed (334 pts)]]></title>
            <link>https://programming.guide/worlds-most-copied-so-snippet.html</link>
            <guid>37674139</guid>
            <pubDate>Wed, 27 Sep 2023 13:10:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://programming.guide/worlds-most-copied-so-snippet.html">https://programming.guide/worlds-most-copied-so-snippet.html</a>, See on <a href="https://news.ycombinator.com/item?id=37674139">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      
      <p><span>by Andreas Lundblad, 2019-12-02</span></p>
      <p><strong>In a recent study titled <em>Usage and Attribution of Stack Overflow Code Snippets in GitHub Projects</em>, an <a href="https://stackoverflow.com/a/3758880/276052">answer</a> I wrote almost a decade ago was found to be the most copied snippet on Stack Overflow. Ironically it happens to be buggy.</strong></p>
      <h2>A Long Long Time Ago…</h2>
      <p>Back in 2010 I was sitting in my office and doing what I wasn’t supposed to be doing: code golfing and chasing reputation on Stack Overflow.</p>
      <p>The following question got my attention: How do you print a byte count in a human readable format? That is, how do you format something like 123,456,789 bytes as “123.5&nbsp;MB”.</p>
      <div>
        <p><a href="https://stackoverflow.com/q/3758606/276052"><img alt="How to convert byte size into human-readable format in Java? Like 1024 should become '1 Kb' and 1024*1024 should become '1 Mb'." src="https://programming.guide/the-most-copied-so-snippet/so-screenshot.png"></a></p>
      </div>
      <p>The implicit spec here is that the resulting string should have a value between 1 and 999.9 followed by a suffix with an appropriate magnitude.</p>
      <p>One answer had already been posted. The code in that answer was based on a loop. The idea was simple: Try all magnitudes, going from the largest (EB = 10<sup>18</sup> bytes) down to the smallest (B = 1 byte) and use the first one that is smaller than the number of bytes. In pseudo code it looks something like this:</p>
      <pre><code>suffixes   = [ <span>"EB"</span>, <span>"PB"</span>, <span>"TB"</span>, <span>"GB"</span>, <span>"MB"</span>, <span>"kB"</span>, <span>"B"</span> ]
magnitudes = [ <span>10<sup>18</sup></span>, <span>10<sup>15</sup></span>, <span>10<sup>12</sup></span>, <span>10<sup>9</sup></span>, <span>10<sup>6</sup></span>, <span>10<sup>3</sup></span>, <span>10<sup>0</sup></span> ]
i = <span>0</span>
<span>while</span> (i &lt; magnitudes.length &amp;&amp; magnitudes[i] &gt; byteCount)
    i++
printf(<span>"%.1f %s"</span>, byteCount / magnitudes[i], suffixes[i])
</code></pre>
      <p>Usually when there’s a correct answer posted that already has a positive score, it’s hard to catch up with it. In Stack Overflow lingo it’s referred to as the <a href="https://meta.stackexchange.com/q/9731/147319">Fastest Gun in the West Problem</a>. In this case the existing answer had a few flaws, so I still saw an opportunity to top it. At the very least, the loop based code could be cleaned up significantly.</p>
      <h2>This is Algebra, I know this!</h2>
      <p>Then it hit me. The kB, MB, GB,&nbsp;… suffixes are nothing but powers of 1000 (or 1024 in <a href="https://en.wikipedia.org/wiki/Binary_prefix">IEC standard</a>) which means it should be possible to compute the right suffix using logarithms instead of a loop.</p>
      <p>Based on this idea, I posted the following:</p>
      <pre><code><span>public</span> <span>static</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>humanReadableByteCount</span>(<span>long</span> <span>bytes</span>, <span>boolean</span> <span>si</span>) {
    <span>int</span> <span>unit</span> = <span>si</span> ? <span>1000</span> : <span>1024</span>;
    <span>if</span> (<span>bytes</span> &lt; <span>unit</span>) <span>return</span> <span>bytes</span> + <span>" B"</span>;
    <span>int</span> <span>exp</span> = (<span>int</span>) (<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>bytes</span>) / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>unit</span>));
    <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>pre</span> = (<span>si</span> ? <span>"kMGTPE"</span> : <span>"KMGTPE"</span>).<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#charAt(int)"><span>charAt</span></a>(<span>exp</span>-<span>1</span>) + (<span>si</span> ? <span>""</span> : <span>"i"</span>);
    <span>return</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#format(java.lang.String,java.lang.Object:A)"><span>format</span></a>(<span>"%.1f %sB"</span>, <span>bytes</span> / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>), <span>pre</span>);
}</code></pre>
      <p>Granted it’s not very readable and <span>log / pow</span> probably makes it less efficient than other solutions. But there were no loops and almost no branching which I thought was pretty neat.</p>
      <div>
        <p><strong>The math behind this</strong> is straight forward. The byte count is expressed as <span>byteCount = 1000<sup><em>s</em></sup></span> where <em>s</em> represents the scale. (For binary notation, base 1024 is used.) Solving for <em>s</em> gives <span><em>s</em> = log<sub>1000</sub>(byteCount)</span>.</p>
        <p>There’s no log<sub>1000</sub> readily available in the API, but we can express it in terms of the natural logarithm as follows <span><em>s</em> = log(byteCount) / log(1000)</span>. We then floor <em>s</em> (cast to int) since if we for example have more than one megabyte (but not a full gigabyte) we want to use MB as magnitude.</p>
        <p>At this point if <span><em>s</em> = 1</span> the scale is kilobytes, if <span><em>s</em> = 2</span> the scale is megabytes, and so on. We divide the byteCount with 1000<sup><em>s</em></sup> and slap on the corresponding letter prefix.</p>
      </div>
      <p>All I could do now was to wait and see if the community would appreciate the answer. Little did I know that this would become the most copied snippet on Stack Overflow.</p>
      <h2>A Study on Attribution</h2>
      <p>Fast forward to 2018. A PhD student by the name Sebastian Baltes publishes a paper in the journal of <em>Empirical Software Engineering</em>. The title is <a href="https://doi.org/10.1007/s10664-018-9650-5"><em>Usage and Attribution of Stack Overflow Code Snippets in GitHub Projects</em></a> and it basically tries to answer one question: Is Stack Overflow’s CC BY-SA 3.0 license respected? I.e. to what extent is proper attribution given, when copying code from Stack Overflow.</p>
      <p>As part of their analysis they extracted code snippets from the <a href="https://archive.org/details/stackexchange">Stack Overflow data dump</a> and matched them against code from public GitHub repos. Quoting the abstract:</p>
      <blockquote>
        <p><em>We present results of a large-scale empiricalstudy analyzing the usage and attribution of non-trivial Java code snippetsfrom SO answers in public GitHub (GH) projects.</em></p>
      </blockquote>
      <p>(Spoiler alert: No, most people do not include proper attribution.)</p>
      <p>In the paper, they include the following table:</p>
      
      <p>That answer at the top with id <a href="https://stackoverflow.com/a/3758880/276052">3758880</a> happens to be the answer I had posted eight years earlier. At this point the answer had over a hundreds of thousands of views and over a thousand upvotes.</p>
      <p>A quick search on GitHub indeed shows thousands of occurrences of <code>humanReadableByteCount</code>.</p>
      <p><img src="https://programming.guide/the-most-copied-so-snippet/github-search.png"></p>
      <p>To check if you happen to have the code in a locally checked out repo:</p>
      <pre><code>$ git grep humanReadableByteCount
</code></pre>
      <div>
        <p><strong>Fun side story:</strong> How did I first hear about this study?</p>
        <p>Sebastian had found a match in the OpenJDK repository. There was no attribution and the OpenJDK license is not compatible with CC BY-SA 3.0. He <a href="http://mail.openjdk.java.net/pipermail/jdk9-dev/2016-December/005327.html">reached out to the dev mailing list</a> asking if the code on Stack Overflow was copied from OpenJDK, or if it was the other way around.</p>
        <p>The funny part here is that I used to work at Oracle, on the OpenJDK project, so a former colleague and friend of mine <a href="http://mail.openjdk.java.net/pipermail/jdk9-dev/2016-December/005328.html">replied</a> with the following:</p>
        <div>
          <p>Hi,</p>
          <p>why not ask the author of this SO post (aioobe) directly? He is an OpenJDK contributor and was employed by Oracle at the time this code appeared in the OpenJDK source repos.</p>
          <p>/Claes</p>
        </div>
        <p>Oracle doesn’t take these things lightly. I happen to know that some people at Oracle took a sigh of relief when they read this reply, and saw it as a bit of a triumph after the “accusation”.</p>
        <p>Sebastian then reached out to me to straighten it out, which I did: I had <em>not</em> yet started at Oracle when that commit was merged, and I did <em>not</em> contribute that patch. Jokes on Oracle. Shortly after, an issue was <a href="https://bugs.openjdk.java.net/browse/JDK-8170860">filed</a> and the code was <a href="http://hg.openjdk.java.net/jdk9/jdk9/hotspot/rev/b552b596203f">removed</a>.</p>
      </div>
      <h2>The Bug</h2>
      <p>I bet you’ve already given it some thought. What is that bug in the code snippet?</p>
      <p>Here it is again:</p>
      <pre><code><span>public</span> <span>static</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>humanReadableByteCount</span>(<span>long</span> <span>bytes</span>, <span>boolean</span> <span>si</span>) {
    <span>int</span> <span>unit</span> = <span>si</span> ? <span>1000</span> : <span>1024</span>;
    <span>if</span> (<span>bytes</span> &lt; <span>unit</span>) <span>return</span> <span>bytes</span> + <span>" B"</span>;
    <span>int</span> <span>exp</span> = (<span>int</span>) (<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>bytes</span>) / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>unit</span>));
    <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>pre</span> = (<span>si</span> ? <span>"kMGTPE"</span> : <span>"KMGTPE"</span>).<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#charAt(int)"><span>charAt</span></a>(<span>exp</span>-<span>1</span>) + (<span>si</span> ? <span>""</span> : <span>"i"</span>);
    <span>return</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#format(java.lang.String,java.lang.Object:A)"><span>format</span></a>(<span>"%.1f %sB"</span>, <span>bytes</span> / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>), <span>pre</span>);
}</code></pre>
      <p>After exabytes, 10<sup>18</sup>, comes zettabytes, 10<sup>21</sup>. Could it be that a really large input causes an index out of bounds on the <code>"kMGTPE"</code> string? Nope. The maximum <code>long</code> value is <span>2<sup>63</sup> - 1 ≈ 9.2 × 10<sup>18</sup></span>, so no <code>long</code> value will ever go beyond EB.</p>
      <p>Could it be a mix-up between SI and binary? Nope. There was a mix-up in an early version of the answer, but that was fixed rather quickly.</p>
      <p>Can <code>exp</code> end up being 0 causing <code>charAt(exp-1)</code> to fail? Nope. The first if-statement covers that case. The <code>exp</code> value will always be at least 1.</p>
      <p>Could there be some weird rounding error in the output? Now we’re getting there…</p>
      <h2>Lots of 9’s</h2>
      <p>The solution works all the way up until it approaches 1 MB. When given 999,999 bytes as input, the result (in SI mode) is <code>"1000.0 kB"</code>. While it is true that 999,999 is closer to <span>1,000 × 1000<sup>1</sup></span> than it is to <span>999.9 × 1000<sup>1</sup></span>, the 1,000 “significand” is out of range according to spec. The correct result is <code>"1.0 MB"</code>.</p>
      <p>FWIW, <em>all</em> 22 answers posted, including the ones using Apache Commons and Android libraries, had this bug (or a variation of it) at the time of writing this article.</p>
      <p>So how do we fix this? First of all, we note that the exponent (<code>exp</code>) should change from ‘k’ to ‘M’ as soon as the number of bytes is closer to <span>1 × 1,000<sup>2</sup></span> (1 MB) than it is to <span>999.9 × 1000<sup>1</sup></span> (999.9 k). This happens at 999,950. Similarly, we should switch from ‘M’ to ‘G’ when we pass 999,950,000 and so on.</p>
      <p>To achieve this we calculate this threshold and bump <code>exp</code> if <code>bytes</code> is larger. (For the binary case, this threshold is not an integer, se we need to ceil the result.)</p>
      <pre><code><span>if</span> (<span>bytes</span> &gt;= <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#ceil(double)"><span>ceil</span></a>(<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>) * (<span>unit</span> - <span>0.05</span>)))
    <span>exp</span>++;</code></pre>
      <p>With this change the code works well all the way up until the byte count approaches 1&nbsp;EB.</p>
      <h2>Even More 9’s</h2>
      <p>Given the input 999,949,999,999,999,999 the result is now <code>1000.0 PB</code> while correct result is <code>999.9 PB</code>. Mathematically the code is accurate, so what’s going on here?</p>
      <p>At this point we’re running into the precision limitations of a <code>double</code>.</p>
      <div>
        <h3>Floating Point Arithmetic 101</h3>
        <p>Due to the IEEE 754 representation floating point values close to zero are very dense, and large values are very sparse. In fact, half of all values are found between −1 and 1, and when talking large doubles, a value as large as <code>Long.MAX_VALUE</code> means nothing. Literally.</p>
        <pre><code><span>double</span> <span>a</span> = <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Double.html"><span>Double</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Double.html#MAX_VALUE"><span>MAX_VALUE</span></a>;
<span>double</span> <span>b</span> = <span>a</span> - <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html"><span>Long</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html#MAX_VALUE"><span>MAX_VALUE</span></a>;
<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/System.html"><span>System</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/System.html#err"><span>err</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/PrintStream.html#println(boolean)"><span>println</span></a>(<span>a</span> == <span>b</span>);  <span>// prints true</span>
</code></pre>
        <p>See <a href="https://programming.guide/bits-of-a-floating-point-value.html">Bits of a Floating Point Value</a> for a deep dive.</p>
      </div>
      <p>There are two problematic computations:</p>
      <ul>
        <li>The division in the <code>String.format</code> argument, and</li>
        <li>The threshold for bumping <code>exp</code>.</li>
      </ul>
      <p>We could switch to <code>BigDecimal</code>, but where’s the fun in that?! Besides, it gets messy anyway because there’s no <code>BigDecimal</code> log function in the standard API.</p>
      <h3>Scaling down intermediate values</h3>
      <p>For the first issue we can scale down the <code>bytes</code> value to a range where the the precision is better, and adjust <code>exp</code> accordingly. The end result is rounded anyway, so it doesn’t matter that we’re throwing out the least significant digits.</p>
      <pre><code><span>if</span> (<span>exp</span> &gt; <span>4</span>) {
    <span>bytes</span> /= <span>unit</span>;
    <span>exp</span>--;
}
</code></pre>
      <h3>Adjusting the least significant bits</h3>
      <p>For the second issue, we <em>do</em> care about the least significant bits (999,949,99…9 and 999,950,00…0 should end up with different exponents) so this issue calls for a different solution.</p>
      <p>First we note that there are 12 different possible values for the threshold (6 for each mode), and only one of them ends up faulty. The faulty result can be uniquely identified by the fact that it ends with D00<sub>16</sub>. If this is the case we simply adjust it to the correct value.</p>
      <pre><code><span>long</span> <span>th</span> = (<span>long</span>) <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#ceil(double)"><span>ceil</span></a>(<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>) * (<span>unit</span> - <span>0.05</span>));
<span>if</span> (<span>exp</span> &lt; <span>6</span> &amp;&amp; <span>bytes</span> &gt;= <span>th</span> - ((<span>th</span> &amp; <span>0xFFF</span>) == <span>0xD00</span> ? <span>51</span> : <span>0</span>))
    <span>exp</span>++;
</code></pre>
      <p>Since we rely on specific bit patterns in floating-point results, we slap on <code>strictfp</code> to ensure it works regardless of the hardware running the code.</p>
      <h2>Negative input</h2>
      <p>It’s unclear under what circumstances a negative byte count could be relevant, but since Java doesn’t have unsigned <code>long</code>, we better deal with it. Right now an input such as −10,000 results in <code>-10000 B</code>.</p>
      <p>Let’s introduce <code>absBytes</code>:</p>
      <pre><code><span>long</span> <span>absBytes</span> = <span>bytes</span> == <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html"><span>Long</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html#MIN_VALUE"><span>MIN_VALUE</span></a> ? <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html"><span>Long</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html#MAX_VALUE"><span>MAX_VALUE</span></a> : <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#abs(int)"><span>abs</span></a>(<span>bytes</span>);
</code></pre>
      <p>The complicated expression is due to the fact that <code>-Long.MIN_VALUE == Long.MIN_VALUE</code>. Now we perform all computations related to <code>exp</code> using <code>absBytes</code> instead of <code>bytes</code>.</p>
      <h2>Final Version</h2>
      <p>Here’s the final version of the code, golfed and compacted in the spirit of the original version:</p>
      <pre><code><span>// From: https://programming.guide/worlds-most-copied-so-snippet.html</span>
<span>public</span> <span>static</span> <span>strictfp</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>humanReadableByteCount</span>(<span>long</span> <span>bytes</span>, <span>boolean</span> <span>si</span>) {
    <span>int</span> <span>unit</span> = <span>si</span> ? <span>1000</span> : <span>1024</span>;
    <span>long</span> <span>absBytes</span> = <span>bytes</span> == <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html"><span>Long</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html#MIN_VALUE"><span>MIN_VALUE</span></a> ? <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html"><span>Long</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html#MAX_VALUE"><span>MAX_VALUE</span></a> : <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#abs(long)"><span>abs</span></a>(<span>bytes</span>);
    <span>if</span> (<span>absBytes</span> &lt; <span>unit</span>) <span>return</span> <span>bytes</span> + <span>" B"</span>;
    <span>int</span> <span>exp</span> = (<span>int</span>) (<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>absBytes</span>) / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>unit</span>));
    <span>long</span> <span>th</span> = (<span>long</span>) <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#ceil(double)"><span>ceil</span></a>(<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>) * (<span>unit</span> - <span>0.05</span>));
    <span>if</span> (<span>exp</span> &lt; <span>6</span> &amp;&amp; <span>absBytes</span> &gt;= <span>th</span> - ((<span>th</span> &amp; <span>0xFFF</span>) == <span>0xD00</span> ? <span>51</span> : <span>0</span>)) <span>exp</span>++;
    <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>pre</span> = (<span>si</span> ? <span>"kMGTPE"</span> : <span>"KMGTPE"</span>).<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#charAt(int)"><span>charAt</span></a>(<span>exp</span> - <span>1</span>) + (<span>si</span> ? <span>""</span> : <span>"i"</span>);
    <span>if</span> (<span>exp</span> &gt; <span>4</span>) {
        <span>bytes</span> /= <span>unit</span>;
        <span>exp</span> -= <span>1</span>;
    }
    <span>return</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#format(java.lang.String,java.lang.Object:A)"><span>format</span></a>(<span>"%.1f %sB"</span>, <span>bytes</span> / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>), <span>pre</span>);
}
</code></pre>
      <p>Note that this started out as a challenge to avoid loops and excessive branching. After ironing out all corner cases the code is even less readable than the original version. Personally I would not copy this snippet into production code.</p>
      <p>For <strong>updated code that is of production quality</strong> see separate article: <a href="https://programming.guide/java/formatting-byte-size-to-human-readable-format.html">Formatting byte size to human readable format</a></p>
      <h2>Key Takeaways</h2>
      <ul>
        <li>
          <p>Stack Overflow snippets can be buggy, even if they have thousands of upvotes.</p>
        </li>
        <li>
          <p>Test all edge cases, <em>especially</em> for code copied from Stack Overflow.</p>
        </li>
        <li>
          <p>Floating-point arithmetic is hard.</p>
        </li>
        <li>
          <p>Do include proper attribution when copying code. Someone might just call you out on it.</p>
        </li>
      </ul>
      <div>
        <h2>Comments (11)</h2>
        <div id="c1575414054-aveiSoh5">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/d4a2f69d78ee7a7cd0be47f92ad3a114?d=mp"></p>
          <div>
            <p>Whoa! What a writeup. Thanks for sharing</p>
            <details>
              <summary>
                <span>by Nick&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575431393-aet3OPhu">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/af0c24a85eac0b47ac8027eb36e11e75?d=mp"></p>
          <div>
            <p>This is a really hard problem.</p>
            <details>
              <summary>
                <span>by Ssuching&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575454325-abeCohl4">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/2f86996e30f54a0d42d93e5904b74e8c?d=mp"></p>
          <div>
            <p>Brilliant. Attitude too. Thanks a lot.</p>
            <details>
              <summary>
                <span>by syjmick&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575455440-sho7Gaey">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/a01d535d38a2fe0aff4a981308915203?d=mp"></p>
          <div>
            <div>
              <p>I think key takeaway here is: prefer short and simple loops over math. As you already said, the math is very hard to get exactly right (so, error prone and hard to read). But I believe it is also at least an order of magnitude slower than the loop-based solution.</p>
              <p>Did you run any benchmarks?</p>
            </div>
            <details>
              <summary>
                <span>by Ivan&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
            <div id="c1575475836-aoeu">
              <p><img alt="User avatar" src="https://www.gravatar.com/avatar/99e100243aaa8b1469b1ed4e8bbecb06?d=mp"></p>
              <div>
                <div>
                  <p>In general I agree with you. In this particular case however, one should note that the rounding error, negative input, and floating-point precision problems would apply also to a simple loop solution.</p>
                  <p>I have not done any benchmarking. Would be interesting for sure.</p>
                </div>
                <details>
                  <summary>
                    <span>by Andreas Lundblad&nbsp;|&nbsp;</span> <span>Reply</span>
                  </summary>
                  
                </details>
              </div>
            </div>
          </div>
        </div>
        <div id="c1575468068-Ai0eengu">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/4d400194f4d28fee7487eb826d463d9e?d=mp"></p>
          <div>
            <p>This is fantastically done. Thanks for posting this.</p>
            <details>
              <summary>
                <span>by John Doe&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575532582-etha1ahJ">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/a2452b2d1c9315f8cc62c7290c9a26f2?d=mp"></p>
          <div>
            <div>
              <p>This is very interesting article, I worked with floating point but always had difficulty to grasp the special cases of rounding. This article is good academic view of special cases to consider.</p>
              <p>Thanks! Five stars for this article. ★★★★★</p>
            </div>
            <details>
              <summary>
                <span>by Audiory&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575555484-auGh7hai">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/38b8a18199083b31074a90b47810b1ce?d=mp"></p>
          <div>
            <p>This article was quite a journey! Thanks :)</p>
            <details>
              <summary>
                <span>by swiatek7&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575671337-Il5che5l">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/124aca96dfe03819a6bc6e782e18d006?d=mp"></p>
          <div>
            <p>Awesome! Thanks for sharing!</p>
            <details>
              <summary>
                <span>by Adam&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575719090-ohXah7iu">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/1eaa7f930b2393cba93e21925ca58ab5?d=mp"></p>
          <div>
            <p>Nice article, thanks for sharing this story! So, how is it done in Unix? Some commands like <code>ls</code>, <code>df</code> have the <code>-h</code> human readable option.</p>
            <details>
              <summary>
                <span>by IvanV&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
            <div id="c1575732527-aoeu">
              <p><img alt="User avatar" src="https://www.gravatar.com/avatar/99e100243aaa8b1469b1ed4e8bbecb06?d=mp"></p>
              <div>
                <p>The implementation for coreutils is found in <a href="https://github.com/coreutils/gnulib/blob/master/lib/human.c"><code>human.c</code></a></p>
                <details>
                  <summary>
                    <span>by Andreas Lundblad&nbsp;|&nbsp;</span> <span>Reply</span>
                  </summary>
                  
                </details>
              </div>
            </div>
          </div>
        </div>
        <h3>Add comment</h3>
        
      </div>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Workers AI: serverless GPU-powered inference on Cloudflare’s global network (233 pts)]]></title>
            <link>https://blog.cloudflare.com/workers-ai/</link>
            <guid>37674097</guid>
            <pubDate>Wed, 27 Sep 2023 13:06:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/workers-ai/">https://blog.cloudflare.com/workers-ai/</a>, See on <a href="https://news.ycombinator.com/item?id=37674097">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post">
    <article>
        


        <p localize="" datetime="2023-09-27T14:00:47+01:00">Loading...</p>
        

        <ul>
            <li>
                <a href="https://blog.cloudflare.com/author/phil/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/phil.jpeg" alt="Phil Wittig" width="62" height="62">
                </a>
                
            </li>
            <li>
                <a href="https://blog.cloudflare.com/author/rita/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/05/Rita-Kozlov.jpeg" alt="Rita Kozlov" width="62" height="62">
                </a>
                
            </li>
            <li>
                <a href="https://blog.cloudflare.com/author/rebecca-weekly/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/04/RWeekly---Retouched-16.jpg" alt="Rebecca Weekly" width="62" height="62">
                </a>
                
            </li>
            <li>
                <a href="https://blog.cloudflare.com/author/celso/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Celso-Martinho.png" alt="Celso Martinho" width="62" height="62">
                </a>
                
            </li>
        </ul>

        <section>
            <p>11 min read</p>
            <div>
                <figure><img src="https://blog.cloudflare.com/content/images/2023/09/image1-29.png" alt="" loading="lazy" width="1800" height="1014"></figure><p>If you're anywhere near the developer community, it's almost impossible to avoid the impact that AI’s recent advancements have had on the ecosystem. Whether you're using AI in your workflow to improve productivity, or you’re shipping AI based features to your users, it’s everywhere. The focus on AI improvements are extraordinary, and we’re super excited about the opportunities that lay ahead, but it's not enough.</p><p>Not too long ago, if you wanted to leverage the power of AI, you needed to know the ins and outs of machine learning, and be able to manage the infrastructure to power it.</p><p>As a developer platform with over one million active developers, we believe there is so much potential yet to be unlocked, so we’re changing the way AI is delivered to developers. Many of the current solutions, while powerful, are based on closed, proprietary models and don't address privacy needs that developers and users demand. Alternatively, the open source scene is exploding with powerful models, but they’re simply not accessible enough to every developer. Imagine being able to run a model, from your code, wherever it’s hosted, and never needing to find GPUs or deal with setting up the infrastructure to support it.</p><p>That's why we are excited to launch Workers AI - an AI inference as a service platform, empowering developers to run AI models with just a few lines of code, all powered by our global network of GPUs. It's open and accessible, serverless, privacy-focused, runs near your users, pay-as-you-go, and it's built from the ground up for a best in class developer experience.</p><h2 id="workers-aimaking-inference-just-work">Workers AI - making inference <strong>just work</strong></h2><p>We’re launching Workers AI to put AI inference in the hands of every developer, and to actually deliver on that goal, it should <strong>just work</strong> out of the box. How do we achieve that?</p><ul><li>At the core of everything, it runs on the right infrastructure - our world-class network of GPUs</li><li>We provide off-the-shelf models that run seamlessly on our infrastructure</li><li>Finally, deliver it to the end developer, in a way that’s delightful. A developer should be able to build their first Workers AI app in minutes, and say “Wow, that’s kinda magical!”.</li></ul><p>So what exactly is Workers AI? It’s another building block that we’re adding to our developer platform - one that helps developers run well-known AI models on serverless GPUs, all on Cloudflare’s trusted global network. As one of the latest additions to our developer platform, it works seamlessly with Workers + Pages, but to make it truly accessible, we’ve made it platform-agnostic, so it also works everywhere else, made available via a REST API.</p><h2 id="models-you-know-and-love">Models you know and love</h2><p>We’re launching with a curated set of popular, open source models, that cover a wide range of inference tasks:</p><ul><li><strong>Text generation (large language model):</strong> meta/llama-2-7b-chat-int8</li><li><strong>Automatic speech recognition (ASR):</strong> openai/whisper</li><li><strong>Translation:</strong> meta/m2m100-1.2</li><li><strong>Text classification:</strong> huggingface/distilbert-sst-2-int8</li><li><strong>Image classification:</strong> microsoft/resnet-50</li><li><strong>Embeddings:</strong> baai/bge-base-en-v1.5</li></ul><p>You can browse all available models in your Cloudflare dashboard, and soon you’ll be able to dive into logs and analytics on a per model basis!</p><figure><img src="https://blog.cloudflare.com/content/images/2023/09/image4-14.png" alt="" loading="lazy" width="1306" height="832"></figure><p>This is just the start, and we’ve got big plans. After launch, we’ll continue to expand based on community feedback. Even more exciting - in an effort to take our catalog from zero to sixty, we’re announcing a partnership with Hugging Face, a leading AI community + hub. The partnership is multifaceted, and you can read more about it <a href="https://blog.cloudflare.com/best-place-region-earth-inference">here</a>, but soon you’ll be able to browse and run a subset of the Hugging Face catalog directly in Workers AI.</p><h2 id="accessible-to-everyone">Accessible to everyone</h2><p>Part of the mission of our developer platform is to provide <strong>all</strong> the building blocks that developers need to build the applications of their dreams. Having access to the right blocks is just one part of it — as a developer your job is to put them together into an application. Our goal is to make that as easy as possible.</p><p>To make sure you could use Workers AI easily regardless of entry point, we wanted to provide access via: Workers or Pages to make it easy to use within the Cloudflare ecosystem, and via REST API if you want to use Workers AI with your current stack.</p><p>Here’s a quick CURL example that translates some text from English to French:</p><!--kg-card-begin: markdown--><pre><code>curl https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/@cf/meta/m2m100-1.2b \
-H "Authorization: Bearer {API_TOKEN}" \
	-d '{ "text": "I'll have an order of the moule frites", "target_lang": "french" }'
</code></pre>
<!--kg-card-end: markdown--><p>And here are what the response looks like:</p><!--kg-card-begin: markdown--><pre><code>{
  "result": {
    "answer": "Je vais commander des moules frites"
  },
  "success": true,
  "errors":[],
  "messages":[]
}
</code></pre>
<!--kg-card-end: markdown--><p>Use it with any stack, anywhere - your favorite Jamstack framework, Python + Django/Flask, Node.js, Ruby on Rails, the possibilities are endless. And deploy.</p><h2 id="designed-for-developers">Designed for developers</h2><p>Developer experience is really important to us. In fact, most of this post has been about just that. Making sure it works out of the box. Providing popular models that just work. Being accessible to all developers whether you build and deploy with Cloudflare or elsewhere. But it’s more than that - the experience should be frictionless, zero to production should be fast, and it should feel good along the way.</p><p>Let’s walk through another example to show just how easy it is to use! We’ll run Llama 2, a popular large language model open sourced by Meta, in a worker.</p><p>We’ll assume you have some of the basics already complete (Cloudflare account, Node, NPM, etc.), but if you don’t <a href="https://developers.cloudflare.com/workers-ai/get-started/local-dev-setup/">this guide</a> will get you properly set up!</p><h3 id="1-create-a-workers-project">1. Create a Workers project</h3><p>Create a new project named workers-ai by running:</p><!--kg-card-begin: markdown--><pre><code>$ npm create cloudflare@latest
</code></pre>
<!--kg-card-end: markdown--><p>When setting up your workers-ai worker, answer the setup questions as follows:</p><ul><li>Enter <strong>workers-ai</strong> for the app name</li><li>Choose <strong>Hello World</strong> script for the type of application</li><li>Select <strong>yes </strong>to using TypeScript</li><li>Select <strong>yes</strong> to using Git</li><li>Select <strong>no</strong> to deploying</li></ul><p>Lastly navigate to your new app directory:</p><!--kg-card-begin: markdown--><pre><code>cd workers-ai
</code></pre>
<!--kg-card-end: markdown--><h3 id="2-connect-workers-ai-to-your-worker">2. Connect Workers AI to your worker</h3><p>Create a Workers AI binding, which allows your worker to access the Workers AI service without having to manage an API key yourself.</p><p>To bind Workers AI to your worker, add the following to the end of your <strong>wrangler.toml</strong> file:</p><!--kg-card-begin: markdown--><pre><code>[ai]
binding = "AI" #available in your worker via env.AI
</code></pre>
<!--kg-card-end: markdown--><p>You can also bind Workers AI to a Pages Function. For more information, refer to <a href="https://developers.cloudflare.com/pages/platform/functions/bindings/#ai">Functions Bindings</a>.</p><h3 id="3-install-the-workers-ai-client-library">3. Install the Workers AI client library</h3><!--kg-card-begin: markdown--><pre><code>npm install @cloudflare/ai
</code></pre>
<!--kg-card-end: markdown--><h3 id="4-run-an-inference-task-in-your-worker">4. Run an inference task in your worker</h3><p>Update the <strong>source/index.ts</strong> with the following code:</p><!--kg-card-begin: markdown--><pre><code>import { Ai } from '@cloudflare/ai'
export default {
  async fetch(request, env) {
    const ai = new Ai(env.AI);
    const input = { prompt: "What's the origin of the phrase 'Hello, World'" };
    const output = await ai.run('@cf/meta/llama-2-7b-chat-int8', input );
    return new Response(JSON.stringify(output));
  },
};
</code></pre>
<!--kg-card-end: markdown--><h3 id="5-develop-locally-with-wrangler">5. Develop locally with Wrangler</h3><p>While in your project directory, test Workers AI locally by running:</p><!--kg-card-begin: markdown--><pre><code>$ npx wrangler dev --remote
</code></pre>
<!--kg-card-end: markdown--><p><strong>Note - </strong>These models currently only run on Cloudflare’s network of GPUs (and not locally), so setting <code>--remote</code> above is a must, and you’ll be prompted to log in at this point.</p><p>Wrangler will give you a URL (most likely localhost:8787). Visit that URL, and you’ll see a response like this</p><!--kg-card-begin: markdown--><pre><code>{
  "response": "Hello, World is a common phrase used to test the output of a computer program, particularly in the early stages of programming. The phrase "Hello, World!" is often the first program that a beginner learns to write, and it is included in many programming language tutorials and textbooks as a way to introduce basic programming concepts. The origin of the phrase "Hello, World!" as a programming test is unclear, but it is believed to have originated in the 1970s. One of the earliest known references to the phrase is in a 1976 book called "The C Programming Language" by Brian Kernighan and Dennis Ritchie, which is considered one of the most influential books on the development of the C programming language.
}
</code></pre>
<!--kg-card-end: markdown--><h3 id="6-deploy-your-worker">6. Deploy your worker</h3><p>Finally, deploy your worker to make your project accessible on the Internet:</p><!--kg-card-begin: markdown--><pre><code>$ npx wrangler deploy
# Outputs: https://workers-ai.&lt;YOUR_SUBDOMAIN&gt;.workers.dev
</code></pre>
<!--kg-card-end: markdown--><p>And that’s it. You can literally go from zero to deployed AI in minutes. This is obviously a simple example, but shows how easy it is to run Workers AI from any project.</p><h2 id="privacy-by-default">Privacy by default</h2><p>When Cloudflare was founded, our value proposition had three pillars: more secure, more reliable, and more performant. Over time, we’ve realized that a better Internet is also a more private Internet, and we want to play a role in building it.</p><p>That’s why Workers AI is private by default - we don’t train our models, LLM or otherwise, on your data or conversations, and our models don’t learn from your usage. You can feel confident using Workers AI in both personal and business settings, without having to worry about leaking your data. Other providers only offer this fundamental feature with their enterprise version. With us, it’s built in for everyone.</p><p>We’re also excited to support data localization in the future. To make this happen, we have an ambitious GPU rollout plan - we’re launching with seven sites today, roughly 100 by the end of 2023, and nearly everywhere by the end of 2024. Ultimately, this will empower developers to keep delivering killer AI features to their users, while staying compliant with their end users’ data localization requirements.</p><h2 id="the-power-of-the-platform">The power of the platform</h2><h4 id="vector-databasevectorize">Vector database - Vectorize</h4><p>Workers AI is all about running Inference, and making it really easy to do so, but sometimes inference is only part of the equation. Large language models are trained on a fixed set of data, based on a snapshot at a specific point in the past, and have no context on your business or use case. When you submit a prompt, information specific to you can increase the quality of results, making it more useful and relevant. That’s why we’re also launching Vectorize, our vector database that’s designed to work seamlessly with Workers AI. Here’s a quick overview of how you might use Workers AI + Vectorize together.</p><p>Example: Use your data (knowledge base) to provide additional context to an LLM when a user is chatting with it.</p><ol><li><strong>Generate initial embeddings:</strong> run your data through Workers AI using an embedding model. The output will be embeddings, which are numerical representations of those words.</li><li><strong><strong><strong>Insert those embeddings into Vectorize: </strong></strong></strong>this essentially seeds the vector database with your data, so we can later use it to retrieve embeddings that are similar to your users’ query</li><li><strong><strong><strong>Generate embedding from user question: </strong></strong></strong>when a user submits a question to your AI app, first, take that question, and run it through Workers AI using an embedding model.</li><li><strong><strong><strong>Get context from Vectorize: </strong></strong></strong>use that embedding to query Vectorize. This should output embeddings that are similar to your user’s question.</li><li><strong><strong><strong>Create context aware prompt:</strong> </strong></strong>Now take the original text associated with those embeddings, and create a new prompt combining the text from the vector search, along with the original question</li><li><strong>Run prompt: </strong>run this prompt through Workers AI using an LLM model to get your final result</li></ol><h4 id="ai-gateway">AI Gateway</h4><p>That covers a more advanced use case. On the flip side, if you are running models elsewhere, but want to get more out of the experience, you can run those APIs through our AI gateway to get features like caching, rate-limiting, analytics and logging. These features can be used to protect your end point, monitor and optimize costs, and also help with data loss prevention. Learn more about AI gateway <a href="https://blog.cloudflare.com/announcing-ai-gateway">here</a>.</p><h2 id="start-building-today">Start building today</h2><p>Try it out for yourself, and let us know what you think. Today we’re launching Workers AI as an open Beta for all Workers plans - free or paid. That said, it’s super early, so…</p><h4 id="warningit%E2%80%99s-an-early-beta">Warning - It’s an early beta</h4><p>Usage is <strong>not currently recommended for production apps</strong>, and limits + access are subject to change.</p><h4 id="limits">Limits</h4><p>We’re initially launching with limits on a per-model basis</p><ul><li>@cf/meta/llama-2-7b-chat-int8: 50 reqs/min globally</li></ul><p>Checkout our <a href="https://developers.cloudflare.com/workers-ai/platform/limits/">docs</a> for a full overview of our limits.</p><h4 id="pricing">Pricing</h4><p>What we released today is just a small preview to give you a taste of what’s coming (we simply couldn’t hold back), but we’re looking forward to putting the full-throttle version of Workers AI in your hands.</p><p>We realize that as you approach building something, you want to understand: how much is this going to cost me? Especially with AI costs being so easy to get out of hand. So we wanted to share the upcoming pricing of Workers AI with you.</p><p>While we won’t be billing on day one, we are announcing what we expect our pricing will look like.</p><p>Users will be able to choose from two ways to run Workers AI:</p><ul><li><strong>Regular Twitch Neurons (RTN) </strong>- running wherever there's capacity at $0.01 / 1k neurons</li><li><strong>Fast Twitch Neurons (FTN)</strong> - running at nearest user location at $1.25 / 1k neurons</li></ul><p>You may be wondering — what’s a neuron?</p><p>Neurons are a way to measure AI output that always scales down to zero (if you get no usage, you will be charged for 0 neurons). To give you a sense of what you can accomplish with a thousand neurons, you can: generate 130 LLM responses, 830 image classifications, or 1,250 embeddings.</p><p>Our goal is to help our customers pay only for what they use, and choose the pricing that best matches their use case, whether it’s price or latency that is top of mind.</p><h3 id="what%E2%80%99s-on-the-roadmap">What’s on the roadmap?</h3><p>Workers AI is just getting started, and we want your feedback to help us make it great. That said, there are some exciting things on the roadmap.</p><h4 id="more-models-please">More models, please</h4><p>We're launching with a solid set of models that just work, but will continue to roll out new models based on your feedback. If there’s a particular model you'd love to see on Workers AI, pop into our<a href="https://discord.cloudflare.com/"> Discord</a> and let us know!</p><p>In addition to that, we're also announcing a<a href="https://blog.cloudflare.com/best-place-region-earth-inference"> partnership with Hugging Face</a>, and soon you'll be able to access and run a subset of the Hugging Face catalog directly from Workers AI.</p><h4 id="analytics-observability">Analytics + observability</h4><p>Up to this point, we’ve been hyper focussed on one thing - making it really easy for any developer to run powerful AI models in just a few lines of code. But that’s only one part of the story. Up next, we’ll be working on some analytics and observability capabilities to give you insights into your usage + performance + spend on a per-model basis, plus the ability to fig into your logs if you want to do some exploring.</p><h4 id="a-road-to-global-gpu-coverage">A road to global GPU coverage</h4><p>Our goal is to be the best place to run inference on Region: Earth, so we're adding GPUs to our data centers as fast as we can.</p><p><strong>We plan to be in 100 data centers by the end this year</strong></p><figure><img src="https://blog.cloudflare.com/content/images/2023/09/image3-28.png" alt="" loading="lazy" width="1801" height="1013"></figure><p><strong>And nearly everywhere by the end of 2024</strong></p><figure><img src="https://blog.cloudflare.com/content/images/2023/09/unnamed-3.png" alt="" loading="lazy" width="1600" height="900"></figure><p><br><strong>We’re really excited to see you build</strong> - head over to <a href="https://developers.cloudflare.com/workers-ai/">our docs</a> to get started.</p><p>If you need inspiration, want to share something you’re building, or have a question - pop into our <a href="https://discord.com/invite/cloudflaredev">Developer Discord</a>.</p>
            </div>
        </section>
    
        









    <div>
            <p>We protect
                <a target="_blank" href="https://www.cloudflare.com/network-services/">entire corporate networks</a>,
                    help customers build
                    <a target="_blank" href="https://workers.cloudflare.com/">Internet-scale applications efficiently</a>,
                    accelerate any
                    <a target="_blank" href="https://www.cloudflare.com/performance/accelerate-internet-applications/">website
                    or Internet application</a>,
                    <a target="_blank" href="https://www.cloudflare.com/ddos/">ward off DDoS
                    attacks</a>, keep
                    <a target="_blank" href="https://www.cloudflare.com/application-security/">hackers at
                    bay</a>,
                    and can help you on
                    <a target="_blank" href="https://www.cloudflare.com/products/zero-trust/">your journey to Zero Trust</a>.</p>
            <p>Visit <a target="_blank" href="https://1.1.1.1/">1.1.1.1</a> from any device to get started with
                our free app that makes your Internet faster and safer.</p>
            <p>To learn more about our mission to help build a better Internet, <a target="_blank" href="https://www.cloudflare.com/learning/what-is-cloudflare/">start here</a>. If you're looking for a
                new career direction, check out <a target="_blank" href="https://cloudflare.com/careers">our open
                    positions</a>.</p>
        </div>

        

        
        

        <a href="https://blog.cloudflare.com/tag/birthday-week/">Birthday Week</a>
        <a href="https://blog.cloudflare.com/tag/workers/">Cloudflare Workers</a>
        <a href="https://blog.cloudflare.com/tag/ai/">AI</a>
        <a href="https://blog.cloudflare.com/tag/developer-platform/">Developer Platform</a>
        <a href="https://blog.cloudflare.com/tag/database/">Database</a>
    </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You can now use WebGPU in Cloudflare Workers (129 pts)]]></title>
            <link>https://blog.cloudflare.com/webgpu-in-workers/</link>
            <guid>37673999</guid>
            <pubDate>Wed, 27 Sep 2023 13:00:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/webgpu-in-workers/">https://blog.cloudflare.com/webgpu-in-workers/</a>, See on <a href="https://news.ycombinator.com/item?id=37673999">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post">
    <article>
        


        <p localize="" datetime="2023-09-27T14:00:56+01:00">Loading...</p>
        

        <ul>
            <li>
                <a href="https://blog.cloudflare.com/author/andre-cruz/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/02/andre2.jpg" alt="André Cruz" width="62" height="62">
                </a>
                
            </li>
            <li>
                <a href="https://blog.cloudflare.com/author/celso/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Celso-Martinho.png" alt="Celso Martinho" width="62" height="62">
                </a>
                
            </li>
        </ul>

        <section>
            <p>12 min read</p>
            <div>
                <figure><img src="https://blog.cloudflare.com/content/images/2023/09/image1-27.png" alt="" loading="lazy" width="1999" height="1125"></figure><p>The browser as an app platform is real and stronger every day; long gone are the Browser Wars. Vendors and standard bodies have done amazingly well over the last years, working together and advancing web standards with new APIs that allow developers to build fast and powerful applications, finally comparable to those we got used to seeing in the native OS' environment.</p><p>Today, browsers can render web pages and run code that interfaces with an <a href="https://developer.mozilla.org/en-US/docs/Web/API">extensive catalog of modern Web APIs</a>. Things like networking, rendering accelerated graphics, or even accessing low-level hardware features like USB devices are all now possible within the browser sandbox.</p><p>One of the most exciting new browser APIs that browser vendors have been rolling out over the last months is WebGPU, a modern, low-level GPU programming interface designed for high-performance 2D and 3D graphics and general purpose GPU compute.</p><p>Today, we are introducing <a href="https://developer.chrome.com/blog/webgpu-release/">WebGPU</a> support to Cloudflare Workers. This blog will explain why it's important, why we did it, how you can use it, and what comes next.</p><h3 id="the-history-of-the-gpu-in-the-browser">The history of the GPU in the browser</h3><p>To understand why WebGPU is a big deal, we must revisit history and see how browsers went from relying only on the CPU for everything in the early days to taking advantage of GPUs over the years.</p><p>In 2011, <a href="https://en.wikipedia.org/wiki/WebGL">WebGL 1</a>, a limited port of <a href="https://www.khronos.org/opengles/">OpenGL ES 2.0</a>, was introduced, providing an API for fast, accelerated 3D graphics in the browser for the first time. By then, this was somewhat of a revolution in enabling gaming and 3D visualizations in the browser. Some of the most popular 3D animation frameworks, like <a href="https://threejs.org/">Three.js</a>, launched in the same period. Who doesn't remember going to the (now defunct) <a href="https://en.wikipedia.org/wiki/Google_Chrome_Experiments">Google Chrome Experiments</a> page and spending hours in awe exploring the demos? Another option then was using the Flash Player, which was still dominant in the desktop environment, and their <a href="https://en.wikipedia.org/wiki/Stage3D">Stage 3D</a> API.</p><p>Later, in 2017, based on the learnings and shortcomings of its predecessor, WebGL 2 was a significant upgrade and brought more advanced GPU capabilities like shaders and more flexible textures and rendering.</p><p>WebGL, however, has proved to be a steep and complex learning curve for developers who want to take control of things, do low-level 3D graphics using the GPU, and not use 3rd party abstraction libraries.</p><p>Furthermore and more importantly, with the advent of machine learning and cryptography, we discovered that GPUs are great not only at drawing graphics but can be used for other applications that can take advantage of things like high-speed data or blazing-fast matrix multiplications, and one can use them to perform general computation. This became known as <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">GPGPU</a>, short for general-purpose computing on graphics processing units.</p><p>With this in mind, in the native desktop and mobile operating system worlds, developers started using more advanced frameworks like <a href="https://en.wikipedia.org/wiki/CUDA">CUDA</a>, <a href="https://developer.apple.com/metal/">Metal</a>, <a href="https://en.wikipedia.org/wiki/DirectX#DirectX_12">DirectX 12</a>, or <a href="https://www.vulkan.org/learn#key-resources">Vulkan</a>. WebGL stayed behind. To fill this void and bring the browser up to date, in 2017, companies like Google, Apple, Intel, Microsoft, Kronos, and Mozilla created the <a href="https://www.w3.org/community/gpu/">GPU for Web Community Working Group</a> to collaboratively design the successor of WebGL and create the next modern 3D graphics and computation capabilities APIs for the Web.</p><h3 id="what-is-webgpu">What is WebGPU</h3><p>WebGPU was developed with the following advantages in mind:</p><ul><li><strong>Lower Level Access</strong> - WebGPU provides lower-level, direct access to the GPU vs. the high-level abstractions in WebGL. This enables more control over GPU resources.</li><li><strong>Multi-Threading</strong> - WebGPU can leverage multi-threaded rendering and compute, allowing improved CPU/GPU parallelism compared to WebGL, which relies on a single thread.</li><li><strong>Compute Shaders</strong> - First-class support for general-purpose compute shaders for GPGPU tasks, not just graphics. WebGL compute is limited.</li><li><strong>Safety</strong> - WebGPU ensures memory and GPU access safety, avoiding common WebGL pitfalls.</li><li><strong>Portability</strong> - WGSL shader language targets cross-API portability across GPU vendors vs. GLSL in WebGL.</li><li><strong>Reduced Driver Overhead</strong> - The lower level Vulkan/Metal/D3D12 basis improves overhead vs. OpenGL drivers in WebGL.</li><li><strong>Pipeline State Objects</strong> - Predefined pipeline configs avoid per-draw driver overhead in WebGL.</li><li><strong>Memory Management</strong> - Finer-grained buffer and resource management vs. WebGL.</li></ul><p>The “too long didn't read” version is that WebGPU provides lower-level control over the GPU hardware with reduced overhead. It's safer, has multi-threading, is focused on compute, not just graphics, and has portability advantages compared to WebGL.</p><p>If these aren't reasons enough to get excited, developers are also looking at WebGPU as an option for native platforms, not just the Web. For instance, you can use this <a href="https://github.com/webgpu-native/webgpu-headers/blob/main/webgpu.h">C API</a> that mimics the JavaScript specification. If you think about this and the power of WebAssembly, you can effectively have a truly platform-agnostic GPU hardware layer that you can use to <a href="https://developer.chrome.com/blog/webgpu-cross-platform/">develop</a> platforms for any operating system or browser.</p><h3 id="more-than-just-graphics">More than just graphics</h3><p>As explained above, besides being a graphics API, WebGPU makes it possible to perform tasks such as:</p><ul><li><strong>Machine Learning</strong> - Implement ML applications like neural networks and computer vision algorithms using WebGPU compute shaders and matrices.</li><li><strong>Scientific Computing</strong> - Perform complex scientific computation like physics simulations and mathematical modeling using the GPU.</li><li><strong>High Performance Computing</strong> - Unlock breakthrough performance for parallel workloads by connecting WebGPU to languages like Rust, C/C++ via <a href="https://webassembly.org/">WebAssembly</a>.</li></ul><p><a href="https://gpuweb.github.io/gpuweb/wgsl/">WGSL</a>, the shader language for WebGPU, is what enables the general-purpose compute feature. Shaders, or more precisely, <a href="https://www.khronos.org/opengl/wiki/Compute_Shader">compute shaders</a>, have no user-defined inputs or outputs and are used for computing arbitrary information. Here are <a href="https://webgpufundamentals.org/webgpu/lessons/webgpu-compute-shaders.html">some examples</a> of simple WebGPU compute shaders if you want to learn more.</p><h3 id="webgpu-in-workers">WebGPU in Workers</h3><p>We've been watching WebGPU since the API was published. Its general-purpose compute features perfectly fit our Workers' ecosystem and capabilities and align well with our vision of providing our customers multiple compute and hardware options and bringing GPU workloads to our global network, close to clients.</p><p>Cloudflare also has a track record of pioneering support for emerging web standards on our network and services, accelerating their adoption for our customers. Examples of these are <a href="https://developers.cloudflare.com/workers/runtime-apis/web-crypto/">Web Crypto API</a>, <a href="https://blog.cloudflare.com/introducing-http2/">HTTP/2</a>, <a href="https://blog.cloudflare.com/http3-the-past-present-and-future/">HTTP/3</a>, <a href="https://blog.cloudflare.com/introducing-tls-1-3/">TLS 1.3</a>, or <a href="https://blog.cloudflare.com/early-hints/">Early hints</a>, but <a href="https://developers.cloudflare.com/workers/runtime-apis/">there are more</a>.</p><p>Bringing WebGPU to Workers was both natural and timely. Today, we are announcing that we have released a version of <a href="https://github.com/cloudflare/workerd">workerd</a>, the open-sourced JavaScript / Wasm runtime that powers Cloudflare Workers, with <a href="https://github.com/cloudflare/workerd/tree/main/src/workerd/api/gpu">WebGPU support</a>, that you can start playing and developing applications with, locally.</p><p>Starting today anyone can run this on their personal computer and experiment with WebGPU-enabled workers. Implementing local development first allows us to put this API in the hands of our customers and developers earlier and get feedback that will guide the development of this feature for production use.</p><p>But before we dig into code examples, let's explain how we built it.</p><h3 id="how-we-built-webgpu-on-top-of-workers">How we built WebGPU on top of Workers</h3><figure><img src="https://blog.cloudflare.com/content/images/2023/09/image2-22.png" alt="" loading="lazy" width="1540" height="1350"></figure><p>To implement the WebGPU API, we took advantage of <a href="https://dawn.googlesource.com/dawn/">Dawn</a>, an open-source library backed by Google, the same used in Chromium and Chrome, that provides applications with an implementation of the WebGPU standard. It also provides the <a href="https://github.com/webgpu-native/webgpu-headers/blob/main/webgpu.h">webgpu.h</a> headers file, the de facto reference for all the other implementations of the standard.</p><p>Dawn can interoperate with Linux, MacOS, and Windows GPUs by interfacing with each platform's native GPU frameworks. For example, when an application makes a WebGPU draw call, Dawn will convert that draw command into the equivalent Vulkan, Metal, or Direct3D 12 API call, depending on the platform.</p><p>From an application standpoint, Dawn handles the interactions with the underlying native graphics APIs that communicate directly with the GPU drivers. Dawn essentially acts as a middle layer that translates the WebGPU API calls into calls for the platform's native graphics API.</p><p>Cloudflare <a href="https://blog.cloudflare.com/workerd-open-source-workers-runtime/">workerd</a> is the underlying open-source runtime engine that executes Workers code. It shares most of its code with the same runtime that powers Cloudflare Workers' production environment but with some changes designed to make it more portable to other environments. We then have release cycles that aim to synchronize both codebases; more on that later. Workerd is also used with <a href="https://github.com/cloudflare/workers-sdk">wrangler</a>, our command-line tool for building and interacting with Cloudflare Workers, to support local development.</p><p>The WebGPU code that interfaces with the Dawn library can be found <a href="https://github.com/cloudflare/workerd/tree/main/src/workerd/api/gpu">here</a>, and can easily be enabled with a flag, checked <a href="https://github.com/cloudflare/workerd/blob/main/src/workerd/api/global-scope.c%2B%2B#L728">here</a>.</p><!--kg-card-begin: markdown--><pre><code>jsg::Ref&lt;api::gpu::GPU&gt; Navigator::getGPU(CompatibilityFlags::Reader flags) {
  // is this a durable object?
  KJ_IF_MAYBE (actor, IoContext::current().getActor()) {
    JSG_REQUIRE(actor-&gt;getPersistent() != nullptr, TypeError,
                "webgpu api is only available in Durable Objects (no storage)");
  } else {
    JSG_FAIL_REQUIRE(TypeError, "webgpu api is only available in Durable Objects");
  };

  JSG_REQUIRE(flags.getWebgpu(), TypeError, "webgpu needs the webgpu compatibility flag set");

  return jsg::alloc&lt;api::gpu::GPU&gt;();
}
</code></pre>
<!--kg-card-end: markdown--><p>The WebGPU API can only be accessed using <a href="https://developers.cloudflare.com/durable-objects/">Durable Objects</a>, which are essentially global singleton instances of Cloudflare Workers. There are two important reasons for this:</p><ul><li>WebGPU code typically wants to store the state between requests, for example, loading an AI model into the GPU memory once and using it multiple times for inference.</li><li>Not all Cloudflare servers have GPUs yet, so although the worker that receives the request is typically the closest one available, the Durable Object that uses WebGPU will be instantiated where there are GPU resources available, which may not be on the same machine.</li></ul><p>Using Durable Objects instead of regular Workers allow us to address both of these issues.</p><h3 id="the-webgpu-hello-world-in-workers">The WebGPU Hello World in Workers</h3><p>Wrangler uses Miniflare 3, a <a href="https://blog.cloudflare.com/wrangler3/">fully-local simulator for Workers</a>, which in turn is powered by workerd. This means you can start experimenting and doing WebGPU code locally on your machine right now before we prepare things in our production environment.</p><p>Let’s get coding then.</p><p>Since Workers doesn't render graphics yet, we started with implementing the general-purpose GPU (GPGPU) APIs in the <a href="https://www.w3.org/TR/webgpu/">WebGPU specification</a>. In other words, we fully support the part of the API that the <a href="https://www.w3.org/TR/webgpu/#gpucomputepipeline">compute shaders and the compute pipeline</a> require, but we are not yet focused on fragment or vertex shaders used in rendering pipelines.</p><p>Here’s a typical “hello world” in WebGPU. This Durable Object script will output the name of the GPU device that workerd found in your machine to your console.</p><!--kg-card-begin: markdown--><pre><code>const adapter = await navigator.gpu.requestAdapter();
const adapterInfo = await adapter.requestAdapterInfo(["device"]);
console.log(adapterInfo.device);
</code></pre>
<!--kg-card-end: markdown--><p>A more interesting example, though, is a simple compute shader. In this case, we will fill a results buffer with an incrementing value taken from the iteration number via <code>global_invocation_id</code>.</p><p>For this, we need two buffers, one to store the results of the computations as they happen (<code>storageBuffer</code>) and another to copy the results at the end (<code>mappedBuffer</code>).</p><p>We then dispatch four workgroups, meaning that the increments can happen in parallel. This parallelism and programmability are two key reasons why compute shaders and GPUs provide an advantage for things like machine learning inference workloads. Other advantages are:</p><ul><li><strong>Bandwidth</strong> - GPUs have a very high memory bandwidth, up to 10-20x more than CPUs. This allows fast reading and writing of all the model parameters and data needed for inference.</li><li><strong>Floating-point performance</strong> - GPUs are optimized for high floating point operation throughput, which are used extensively in neural networks. They can deliver much higher <a href="https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html">TFLOPs than CPUs</a>.</li></ul><p>Let’s look at the code:</p><!--kg-card-begin: markdown--><pre><code>// Create device and command encoder
const adapter = await navigator.gpu.requestAdapter();
const device = await adapter.requestDevice();
const encoder = device.createCommandEncoder();

// Storage buffer
const storageBuffer = device.createBuffer({
  size: 4 * Float32Array.BYTES_PER_ELEMENT, // 4 float32 values
  usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC,
});

// Mapped buffer
const mappedBuffer = device.createBuffer({
  size: 4 * Float32Array.BYTES_PER_ELEMENT,
  usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST,
});

// Create shader that writes incrementing numbers to storage buffer
const computeShaderCode = `
    @group(0) @binding(0)
    var&lt;storage, read_write&gt; result : array&lt;f32&gt;;

    @compute @workgroup_size(1)
    fn main(@builtin(global_invocation_id) gid : vec3&lt;u32&gt;) {
      result[gid.x] = f32(gid.x);
    }
`;

// Create compute pipeline
const computePipeline = device.createComputePipeline({
  layout: "auto",
  compute: {
    module: device.createShaderModule({ code: computeShaderCode }),
    entryPoint: "main",
  },
});

// Bind group
const bindGroup = device.createBindGroup({
  layout: computePipeline.getBindGroupLayout(0),
  entries: [{ binding: 0, resource: { buffer: storageBuffer } }],
});

// Dispatch compute work
const computePass = encoder.beginComputePass();
computePass.setPipeline(computePipeline);
computePass.setBindGroup(0, bindGroup);
computePass.dispatchWorkgroups(4);
computePass.end();

// Copy from storage to mapped buffer
encoder.copyBufferToBuffer(
  storageBuffer,
  0,
  mappedBuffer,
  0,
  4 * Float32Array.BYTES_PER_ELEMENT //mappedBuffer.size
);

// Submit and read back result
const gpuBuffer = encoder.finish();
device.queue.submit([gpuBuffer]);

await mappedBuffer.mapAsync(GPUMapMode.READ);
console.log(new Float32Array(mappedBuffer.getMappedRange()));
// [0, 1, 2, 3]
</code></pre>
<!--kg-card-end: markdown--><p>Now that we covered the basics of WebGPU and compute shaders, let's move to something more demanding. What if we could perform machine learning inference using Workers and GPUs?</p><h3 id="onnx-webgpu-demo">ONNX WebGPU demo</h3><p>The <a href="https://github.com/microsoft/onnxruntime">ONNX runtime</a> is a popular open-source cross-platform, high performance machine learning inferencing accelerator. <a href="https://github.com/webonnx/wonnx">Wonnx</a> is a GPU-accelerated version of the same engine, written in Rust, that can be compiled to WebAssembly and take advantage of WebGPU in the browser. We are going to run it in Workers using a combination of <a href="https://github.com/cloudflare/workers-rs">workers-rs</a>, our Rust bindings for Cloudflare Workers, and the workerd WebGPU APIs.</p><p>For this demo, we are using <a href="https://www.kdnuggets.com/2016/09/deep-learning-reading-group-squeezenet.html">SqueezeNet</a>. This small image classification model can run under lower resources but still achieves similar levels of accuracy on the <a href="https://en.wikipedia.org/wiki/ImageNet">ImageNet</a> image classification validation dataset as larger models like <a href="https://en.wikipedia.org/wiki/AlexNet">AlexNet</a>.</p><p>In essence, our worker will receive any uploaded image and attempt to classify it according to the 1000 ImageNet classes. Once ONNX runs the machine learning model using the GPU, it will return the list of classes with the highest probability scores. Let’s go step by step.</p><p>First we load the model from R2 into the GPU memory the first time the Durable Object is called:</p><!--kg-card-begin: markdown--><pre><code>#[durable_object]
pub struct Classifier {
    env: Env,
    session: Option&lt;wonnx::Session&gt;,
}

impl Classifier {
    async fn ensure_session(&amp;mut self) -&gt; Result&lt;()&gt; {
        match self.session {
            Some(_) =&gt; worker::console_log!("DO already has a session"),
            None =&gt; {
                // No session, so this should be the first request. In this case
                // we will fetch the model from R2, build a wonnx session, and
                // store it for subsequent requests.
                let model_bytes = fetch_model(&amp;self.env).await?;
                let session = wonnx::Session::from_bytes(&amp;model_bytes)
                    .await
                    .map_err(|err| err.to_string())?;
                worker::console_log!("session created in DO");
                self.session = Some(session);
            }
        };
        Ok(())
    }
}
</code></pre>
<!--kg-card-end: markdown--><p>This is only required once, when the Durable Object is instantiated. For subsequent requests, we retrieve the model input tensor, call the existing session for the inference, and return to the calling worker the result tensor converted to JSON:</p><!--kg-card-begin: markdown--><pre><code>        let request_data: ArrayBase&lt;OwnedRepr&lt;f32&gt;, Dim&lt;[usize; 4]&gt;&gt; =
            serde_json::from_str(&amp;req.text().await?)?;
        let mut input_data = HashMap::new();
        input_data.insert("data".to_string(), request_data.as_slice().unwrap().into());

        let result = self
            .session
            .as_ref()
            .unwrap() // we know the session exists
            .run(&amp;input_data)
            .await
            .map_err(|err| err.to_string())?;
...
        let probabilities: Vec&lt;f32&gt; = result
            .into_iter()
            .next()
            .ok_or("did not obtain a result tensor from session")?
            .1
            .try_into()
            .map_err(|err: TensorConversionError| err.to_string())?;

        let do_response = serde_json::to_string(&amp;probabilities)?;
        Response::ok(do_response)
</code></pre>
<!--kg-card-end: markdown--><p>On the Worker script itself, we load the uploaded image and pre-process it into a model input tensor:</p><!--kg-card-begin: markdown--><pre><code>    let image_file: worker::File = match req.form_data().await?.get("file") {
        Some(FormEntry::File(buf)) =&gt; buf,
        Some(_) =&gt; return Response::error("`file` part of POST form must be a file", 400),
        None =&gt; return Response::error("missing `file`", 400),
    };
    let image_content = image_file.bytes().await?;
    let image = load_image(&amp;image_content)?;
</code></pre>
<!--kg-card-end: markdown--><p>Finally, we call the GPU Durable Object, which runs the model and returns the most likely classes of our image:</p><!--kg-card-begin: markdown--><pre><code>    let probabilities = execute_gpu_do(image, stub).await?;
    let mut probabilities = probabilities.iter().enumerate().collect::&lt;Vec&lt;_&gt;&gt;();
    probabilities.sort_unstable_by(|a, b| b.1.partial_cmp(a.1).unwrap());
    Response::ok(LABELS[probabilities[0].0])
</code></pre>
<!--kg-card-end: markdown--><p>We packaged this demo in a public repository, so you can also run it. Make sure that you have a <a href="https://www.rust-lang.org/">Rust</a> compiler, <a href="https://nodejs.org/en">Node.js</a>, <a href="https://git-scm.com/">Git</a> and <a href="https://curl.se/">curl</a> installed, then clone the repository:</p><!--kg-card-begin: markdown--><pre><code>git clone https://github.com/cloudflare/workers-wonnx.git
cd workers-wonnx
</code></pre>
<!--kg-card-end: markdown--><p>Upload the model to the local R2 simulator:</p><!--kg-card-begin: markdown--><pre><code>npx wrangler@latest r2 object put model-bucket-dev/opt-squeeze.onnx --local --file models/opt-squeeze.onnx
</code></pre>
<!--kg-card-end: markdown--><p>And then run the Worker locally:</p><!--kg-card-begin: markdown--><pre><code>npx wrangler@latest dev
</code></pre>
<!--kg-card-end: markdown--><p>With the Worker running and waiting for requests you can then open another terminal window and upload one of the image examples in the same repository using curl:</p><!--kg-card-begin: markdown--><pre><code>&gt; curl -F "file=@images/pelican.jpeg" http://localhost:8787
n02051845 pelican
</code></pre>
<!--kg-card-end: markdown--><p>If everything goes according to plan the result of the curl command will be the most likely class of the image.</p><h3 id="next-steps-and-final-words">Next steps and final words</h3><p>Over the upcoming weeks, we will merge the workerd WebGPU code in the Cloudflare Workers production environment and make it available globally, on top of our growing GPU nodes fleet. We didn't do it earlier because that environment is subject to strict security and isolation requirements. For example, we can't break the <a href="https://developers.cloudflare.com/workers/learning/security-model/">security model</a> of our process sandbox and have V8 talking to the GPU hardware directly, that would be a problem; we must create a configuration where another process is closer to the GPU and use IPC (inter-process communication) to talk to it. Other things like managing resource allocation and billing are being sorted out.</p><p>For now, we wanted to get the good news out that we will support WebGPU in Cloudflare Workers and ensure that you can start playing and coding with it today and learn from it. WebGPU and general-purpose computing on GPUs is still in its early days. We presented a machine-learning demo, but we can imagine other applications taking advantage of this new feature, and we hope you can show us some of them.</p><p>As usual, you can talk to us on our <a href="https://discord.cloudflare.com/">Developers Discord</a> or the <a href="https://community.cloudflare.com/c/developers/constellation/97">Community forum</a>; the team will be listening. We are eager to hear from you and learn about what you're building.</p>
            </div>
        </section>
    
        









    <div>
            <p>We protect
                <a target="_blank" href="https://www.cloudflare.com/network-services/">entire corporate networks</a>,
                    help customers build
                    <a target="_blank" href="https://workers.cloudflare.com/">Internet-scale applications efficiently</a>,
                    accelerate any
                    <a target="_blank" href="https://www.cloudflare.com/performance/accelerate-internet-applications/">website
                    or Internet application</a>,
                    <a target="_blank" href="https://www.cloudflare.com/ddos/">ward off DDoS
                    attacks</a>, keep
                    <a target="_blank" href="https://www.cloudflare.com/application-security/">hackers at
                    bay</a>,
                    and can help you on
                    <a target="_blank" href="https://www.cloudflare.com/products/zero-trust/">your journey to Zero Trust</a>.</p>
            <p>Visit <a target="_blank" href="https://1.1.1.1/">1.1.1.1</a> from any device to get started with
                our free app that makes your Internet faster and safer.</p>
            <p>To learn more about our mission to help build a better Internet, <a target="_blank" href="https://www.cloudflare.com/learning/what-is-cloudflare/">start here</a>. If you're looking for a
                new career direction, check out <a target="_blank" href="https://cloudflare.com/careers">our open
                    positions</a>.</p>
        </div>

        

        
        

        <a href="https://blog.cloudflare.com/tag/birthday-week/">Birthday Week</a>
        <a href="https://blog.cloudflare.com/tag/workers/">Cloudflare Workers</a>
        <a href="https://blog.cloudflare.com/tag/standards/">Standards</a>
        <a href="https://blog.cloudflare.com/tag/webgpu/">WebGPU</a>
    </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Philips allegedly hid 3700 complaints about sleep apnea machines from U.S. (153 pts)]]></title>
            <link>https://nltimes.nl/2023/09/27/philips-allegedly-hid-3700-complaints-sleep-apnea-machines-us-authority</link>
            <guid>37673539</guid>
            <pubDate>Wed, 27 Sep 2023 12:14:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nltimes.nl/2023/09/27/philips-allegedly-hid-3700-complaints-sleep-apnea-machines-us-authority">https://nltimes.nl/2023/09/27/philips-allegedly-hid-3700-complaints-sleep-apnea-machines-us-authority</a>, See on <a href="https://news.ycombinator.com/item?id=37673539">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
    
          <p>
                Wednesday, 27 September 2023 - 13:02
      </p>
    
        


  </div><div><p>Philips failed to report at least 3,700 complaints about its sleep apnea devices and respirators to the United States regulator FDA since 2010,<a href="https://www.propublica.org/article/philips-kept-warnings-about-dangerous-cpaps-secret-profits-soared"> NRC</a>, research collective <a href="https://www.nrc.nl/nieuws/2023/09/27/philips-verzweeg-al-sinds-2010-klachten-over-apneuapparaten-a4175460">ProPublica</a>, and the Pittsburgh Post Gazette reported based on their analysis of over 100,000 public reports.</p>

<p>According to the researchers, the first complaint came in 2010, shortly after Philips started using a sound-dampening foam in the breathing aids that <a href="https://nltimes.nl/2021/06/14/philips-recalls-millions-ventilators-sleep-apnea-machines">triggered a mass recall in 2021</a>. At the recall, Philips said the sound-dampening foam could disintegrate when it made contact with certain cleaning agents, resulting in users potentially breathing in carcinogenic substances.</p>

<p>United States law dictates that device makers must inform the government within 30 days if they receive reports of patient injuries, deaths, or malfunctions that could cause harm. In the 11 years between the launch of the sound-dampening foam and the recall, Philips received at least 3,700 such reports in the United States that it kept in its files and didn’t report to the FDA, according to the researchers.</p>

<p>The withheld complaints contain at least 10 reports about patients who may have died due to the use of a ventilator. Other reports described “black particles” or “dirt and dust” inside the machines. One described an “oily-like substance,” another spoke of “black shavings in the chamber,” and another said it was “contaminated with unknown sticky substance.” </p>

<p>U.S. law also states that device makers must immediately investigate reported faults that may cause harm. Philips launched its first investigation in 2019. In 2021, it recalled 15 million DreamStation <a href="https://nltimes.nl/2022/06/28/philips-issues-sleep-apnea-machines-less-harmful-thought">sleep apnea machines</a> and <a href="https://nltimes.nl/2023/02/10/us-agency-reveals-8000-new-reports-health-problems-tied-philips-ventilators">ventilators</a> that used the foam in question. </p>

<p>The FDA confirmed to the researchers that Philips had withheld many reports. According to the American regulator, Philips found “a large number of foam disintegration complaints that should have been sent to the FDA” during “retrospective reviews” following the recall. </p>

<p>A Philips spokesperson told NRC that “out of an abundance of caution,” the company sent old complaints that “may be related to the deteriorated foam” to the FDA despite “previously determining that these complaints did not need to be reported.” According to Philips, it was unaware of the scope of the problem because, until early 2021, the complaints were handled “one by one by Philips Respironics” - the Philips subsidiary in Pittsburg. </p>

<p>Because Philips held the complaints under wraps for over a decade, the regulator only intervened eleven years after the company got its first report. Through the spring of 2021, the FDA had only received 30 reports from the Pittsburg subsidiary of incidents with disintegrating foam. </p>

<p>The recall caused Philips serious financial troubles. Its stock price has halved, CEO Frans van Houten resigned, and his successor Roy Jakobs has announced two major rounds of layoffs. Lawyers in the United States are preparing mass claims amounting to billions of euros against the company. </p>

<p>Earlier this month, Philips announced that it had <a href="https://nltimes.nl/2023/09/07/philips-reaches-deal-us-faulty-breathing-devices">reached a settlement in the U.S.</a> to resolve <a href="https://nltimes.nl/2023/07/16/number-claims-damages-philips-us-increasing">over 500 pending lawsuits and damage claims</a> due to the recall. In April, <a href="https://nltimes.nl/2023/04/24/philips-sets-eu575-mil-aside-respirator-lawsuits-suffered-eu583-mil-loss-q1">Philips announced</a> that it set aside 575 million euros to cover the potential cost resulting from lawsuits and settlements in the country. </p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google judge rules trial documents can be posted by U.S. online (313 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2023-09-26/google-judge-rules-trial-documents-can-be-posted-by-us-online</link>
            <guid>37673413</guid>
            <pubDate>Wed, 27 Sep 2023 12:01:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2023-09-26/google-judge-rules-trial-documents-can-be-posted-by-us-online">https://www.bloomberg.com/news/articles/2023-09-26/google-judge-rules-trial-documents-can-be-posted-by-us-online</a>, See on <a href="https://news.ycombinator.com/item?id=37673413">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First Impressions with GPT-4V(ision) (277 pts)]]></title>
            <link>https://blog.roboflow.com/gpt-4-vision/</link>
            <guid>37673409</guid>
            <pubDate>Wed, 27 Sep 2023 12:01:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.roboflow.com/gpt-4-vision/">https://blog.roboflow.com/gpt-4-vision/</a>, See on <a href="https://news.ycombinator.com/item?id=37673409">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>On September 25th, 2023, <a href="https://help.openai.com/en/articles/6825453-chatgpt-release-notes?ref=blog.roboflow.com">OpenAI announced the rollout of two new features</a> that extend how people can interact with its recent and most advanced model, <a href="https://openai.com/research/gpt-4?ref=blog.roboflow.com">GPT-4</a>: the ability to ask questions about images and to use speech as an input to a query.</p><p>This functionality marks GPT-4’s move into being a <a href="https://blog.roboflow.com/multimodal-models/">multimodal model</a>. This means that the model can accept multiple “modalities” of input – text and images – and return results based on those inputs. Bing Chat, developed by Microsoft in partnership with OpenAI, and Google’s Bard model both support images as input, too. <a href="https://blog.roboflow.com/using-google-bard-with-images/">Read our comparison post to see how Bard and Bing perform with image inputs</a>.</p><p>In this guide, we are going to share our first impressions with the GPT-4V image input feature. We will run through a series of experiments to test the functionality of GPT-4V, showing where the model performs well and where it struggles.</p><p><em>Note: This article shows a limited series of tests our team performed; your results will vary depending on the questions you ask and the images you use in a prompt. Tag us on social media @roboflow with your findings using GPT-4V. We would love to see more tests using the model!</em></p><p>Without further ado, let’s get started!</p><h2 id="what-is-gpt-4v">What is GPT-4V?</h2><p><a href="https://cdn.openai.com/papers/GPTV_System_Card.pdf?ref=blog.roboflow.com">GPT-4V(ision)</a> (GPT-4V) is a multimodal model developed by OpenAI. GPT-4V allows a user to upload an image as an input and ask a question about the image, a task type known as visual question answering (VQA).</p><p>GPT-4V is rolling out as of September 24th and will be available in both the OpenAI ChatGPT iOS app and the web interface. You must have a GPT-4 subscription to use the tool.</p><p>Let’s experiment with GPT-4V and test its capabilities!</p><h2 id="test-1-visual-question-answering">Test #1: Visual Question Answering</h2><p>One of our first experiments with GPT-4V was to inquire about a computer vision meme. We chose this experiment because it allows us to the extent to which GPT-4V understands context and relationships in a given image.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-17.25.07-1.jpg" alt="" loading="lazy" width="590" height="1280"></figure><p>GPT-4V was able to successfully describe why the image was funny, making reference to various components of the image and how they connect. Notably, the provided meme contained text, which GPT-4V was able to read and use to generate a response. With that said, GPT-4V did make a mistake. The model said the fried chicken was labeled “NVIDIA BURGER” instead of “GPU”.</p><p>We then went on to test GPT-4V with currency, running a couple of different tests. First, we uploaded a photo of a United States penny. GPT-4V was able to successfully identify the origin and denomination of the coin:</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/Screenshot-2023-09-26-at-19.36.14-1.png" alt="" loading="lazy" width="1258" height="1224" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/Screenshot-2023-09-26-at-19.36.14-1.png 600w, https://blog.roboflow.com/content/images/size/w1000/2023/09/Screenshot-2023-09-26-at-19.36.14-1.png 1000w, https://blog.roboflow.com/content/images/2023/09/Screenshot-2023-09-26-at-19.36.14-1.png 1258w" sizes="(min-width: 720px) 720px"></figure><p>We then uploaded an image with multiple coins and prompted GPT-4V with the text: “How much money do I have?”</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-17.56.29.jpg" alt="" loading="lazy" width="826" height="1280" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/2023-09-26-17.56.29.jpg 600w, https://blog.roboflow.com/content/images/2023/09/2023-09-26-17.56.29.jpg 826w" sizes="(min-width: 720px) 720px"></figure><p>GPT-4V was able to identify the number of coins but did not ascertain the currency type. With a follow up question, GPT-4V successfully identified the currency type:</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-18.00.56.jpg" alt="" loading="lazy" width="1179" height="939" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/2023-09-26-18.00.56.jpg 600w, https://blog.roboflow.com/content/images/size/w1000/2023/09/2023-09-26-18.00.56.jpg 1000w, https://blog.roboflow.com/content/images/2023/09/2023-09-26-18.00.56.jpg 1179w" sizes="(min-width: 720px) 720px"></figure><p>Moving on to another topic, we decided to try using GPT-4V with a photo from a popular movie: Pulp Fiction. We wanted to know: could GPT-4 answer a question about the movie without being told in text what movie it was?</p><p>We uploaded a photo from Pulp Fiction with the prompt “Is it a good movie?”, to which GPT-4V responded with a description of the movie and an answer to our question. GPT-4V provides a high-level description of the movie and a summary of the attributes associated with the movie considered to be positive and negative.</p><p>We further asked about the IMDB score for the movie, to which GPT-4V responded with the score as of January 2022. This suggests, like other GPT models released by OpenAI, there is a knowledge cutoff after which point the model has no more recent knowledge.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-18.13.51.jpg" alt="" loading="lazy" width="1179" height="848" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/2023-09-26-18.13.51.jpg 600w, https://blog.roboflow.com/content/images/size/w1000/2023/09/2023-09-26-18.13.51.jpg 1000w, https://blog.roboflow.com/content/images/2023/09/2023-09-26-18.13.51.jpg 1179w" sizes="(min-width: 720px) 720px"></figure><p>We then explored GPT-4V’s question answering capabilities by asking a question about a place. We uploaded a photo of San Francisco with the text prompt “Where is this?” GPT-4V successfully identified the location, San Francisco, and noted that the Transamerica Pyramid, pictured in the image we uploaded, is a notable landmark in the city.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/Screenshot-2023-09-26-at-19.39.34.png" alt="" loading="lazy" width="764" height="714" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/Screenshot-2023-09-26-at-19.39.34.png 600w, https://blog.roboflow.com/content/images/2023/09/Screenshot-2023-09-26-at-19.39.34.png 764w" sizes="(min-width: 720px) 720px"></figure><p>Moving over to the realm of plants, we provided GPT-4V with a photo of a peace lily and asked the question “What is that plant and how should I care about it?”:</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-27-13.06.19.jpg" alt="" loading="lazy" width="711" height="1280" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/2023-09-27-13.06.19.jpg 600w, https://blog.roboflow.com/content/images/2023/09/2023-09-27-13.06.19.jpg 711w"></figure><p>The model successfully identified that the plant is a peace lily and provided advice on how to care for the plant. This illustrates the utility of having text and vision combined to create a multi-modal such as they are in GPT-4V. The model returned a fluent answer to our question without having to build our own two-stage process (i.e. classification to identify the plant then GPT-4 to provide plant care advice).</p><h2 id="test-2-optical-character-recognition-ocr">Test #2: Optical Character Recognition (OCR)</h2><p>We conducted two tests to explore GPT-4V’s OCR capabilities: OCR on an image with text on a car tire and OCR on a photo of a paragraph from a digital document. Our intent was to build an understanding of how GPT-4V performs at OCR in the wild, where text may have less contrast and be at an angle, versus digital documents with clear text.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-17.36.09-1.jpg" alt="" loading="lazy" width="590" height="605"></figure><p><br>GPT-4V was unable to correctly identify the serial number in an image of a tire. Some numbers were correct but there were several errors in the result from the model.</p><p>In our document test, we presented text from a web page and asked GPT-4V to read the text in the image. The model was able to successfully identify the text in the image.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/File.jpg" alt="" loading="lazy" width="738" height="1600" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/File.jpg 600w, https://blog.roboflow.com/content/images/2023/09/File.jpg 738w" sizes="(min-width: 720px) 720px"></figure><p>GPT-4V does an excellent job translating words in an image to individual characters in text. A useful insight for tasks related to extracting text from documents.</p><h2 id="test-3-math-ocr">Test #3: Math OCR</h2><p>Math OCR is a specialized form of OCR pertaining specifically to math equations. Math OCR is often considered its own discipline because the syntax of what the OCR model needs to identify extends to a vast range of symbols.</p><p>We presented GPT-4V with a math question. This math question was in a screenshot taken from a document. The question concerns calculating the length of a zip wire given two angles. We presented the image with the prompt “Solve it.”</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-27-13.25.51.jpg" alt="" loading="lazy" width="590" height="1280"></figure><figure><img src="https://blog.roboflow.com/content/images/2023/09/photo_2023-09-27-13.25.55.jpeg" alt="" loading="lazy" width="590" height="1280"></figure><p>The model identified the problem can be solved with trigonometry, identified the function to use, and presented a step-by-step walkthrough of how to solve the problem. Then, GPT-4V provided the correct answer to the question.</p><p>With that said, the GPT-4V system card notes that the model may miss mathematical symbols. Different tests, including tests where an equation or expression is written by hand on paper, may indicate deficiencies in the model's ability to answer math questions. </p><h2 id="test-4-object-detection">Test #4: Object Detection</h2><p><a href="https://blog.roboflow.com/object-detection/">Object detection</a> is a fundamental task in the field of computer vision. We asked GPT-4V to identify the location of various objects to evaluate its ability to perform object detection tasks.</p><p>In our first test, we asked GPT-4V to detect a dog in an image and provide the x_min, y_min, x_max, and y_max values associated with the position of the dog. The bounding box coordinates returned by GPT-4V did not match the position of the dog.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/photo_2023-09-26-18.51.24.jpeg" alt="" loading="lazy" width="590" height="1280"></figure><p>While GPT-4V’s capabilities at answering questions about an image are powerful, the model is not a substitute for fine-tuned <a href="https://roboflow.com/models/object-detection?ref=blog.roboflow.com">object detection models</a> in scenarios where you want to know where an object is in an image.</p><h2 id="test-5-captcha">Test #5: CAPTCHA</h2><p>We decided to test GPT-4V with CAPTCHAs, a task OpenAI studied in their research and wrote about in their <a href="https://cdn.openai.com/papers/GPTV_System_Card.pdf?ref=blog.roboflow.com">system card</a>. We found that GPT-4V was able to identify that an image contained a CAPTCHA but often failed the tests. In a traffic light example, GPT-4V missed some boxes that contained traffic lights.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/photo_2023-09-27-13.01.22.jpeg" alt="" loading="lazy" width="1031" height="1280" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/photo_2023-09-27-13.01.22.jpeg 600w, https://blog.roboflow.com/content/images/size/w1000/2023/09/photo_2023-09-27-13.01.22.jpeg 1000w, https://blog.roboflow.com/content/images/2023/09/photo_2023-09-27-13.01.22.jpeg 1031w" sizes="(min-width: 720px) 720px"></figure><p>In the following crosswalk example, GPT-4V classified a few boxes correctly but incorrectly classified one box in the CAPTCHA as a crosswalk.</p><figure><img src="https://lh4.googleusercontent.com/sUn71XmNZHeS4C9U1KGZm9T12MPiDaWSnjeqqZXSTan3I01VVBMvJ0_8knDTQW6kO1YJS8jLXswk_zEyINNDQz7mwDT60e_NoKrikqwaKuULsM9upmURmKCZ7STF6INGj4FtvEY3jlIjvgpVi1eamCI" alt="" loading="lazy" width="248" height="325"></figure><h2 id="test-6-crosswords-and-sudokus">Test #6: Crosswords and Sudoku's</h2><p>We decided to test how GPT-4V performs on crosswords and sudokus.</p><p>First, we prompted GPT-4V with photos of a crossword with the text instruction "Solve it." GPT-4V inferred the image contained a crossword and attempted to provide a solution to the crossword. The model appeared to read the clues correctly but misinterpreted the structure of the board. As a result, the provided answers were incorrect.</p><figure><img src="https://lh6.googleusercontent.com/bXAg1SiRBcs-huLBicWFzkeKI8NxB5OE1zoa1cAvC8sqfU1aFmZ2MRDKd2PTKxafivJsaY3R189vJYPEx0BzrXyWwy5ta2TEaGU2yKrBrOxqCYiQhAM93N4SDvZu6Wb7S3lCGaB2j9PxUCvuqbWD8os" alt="" loading="lazy" width="272" height="592"></figure><p>This same limitation was exhibited in our sudoku test, where GPT-4V identified the game but misunderstood the structure of the board and thus returned inaccurate results:</p><figure><img src="https://lh4.googleusercontent.com/U9cH5wYei3jZN8mmAA6etp3ngH8Zu0YrpLisXW6CEO0uSDB-FW3UO7PDLm-u5sEwc6Isvvh3BP_qizYEZctgWRUQpt8oP2_ius6vKGvUmTmAdcn6eneWiAOgq1O6n2W1LV7rx6a6hmDXLxrHs7IkxZI" alt="" loading="lazy" width="431" height="936"></figure><h2 id="gpt-4v-limitations-and-safety">GPT-4V Limitations and Safety</h2><p>OpenAI conducted research with an alpha version of the vision model available to a small group of users, as outlined in the official <a href="https://cdn.openai.com/papers/GPTV_System_Card.pdf?ref=blog.roboflow.com">GPT-4V(ision) System Card</a>. During this process, they were able to gather feedback and insights on how GPT-4V works with prompts provided by a range of people. This was supplemented with “red teaming”, wherein external experts were “to qualitatively assess the limitations and risks associated with the model and system”.</p><p>Based on OpenAI’s research, the GPT-4V system card notes numerous limitations with the model such as:</p><ol><li>Missing text or characters in an image</li><li>Missing mathematical symbols</li><li>Being unable to recognize spatial locations and colors</li></ol><p>In addition to limitations, OpenAI identified, researched, and attempted to mitigate several risks associated with the model. For example, GPT-4V avoids identifying a specific person in an image and does not respond to prompts pertaining to hate symbols.</p><p>With that said, there is further work to be done in model safeguarding. For example, OpenAI notes in the model system card that “If prompted, GPT-4V can generate content praising certain lesser known hate groups in response to their symbols.”,</p><h2 id="gpt-4v-for-computer-vision-and-beyond">GPT-4V for Computer Vision and Beyond</h2><p>GPT-4V is a notable movement in the field of machine learning and natural language processing. With GPT-4V, you can ask questions about an image – and follow up questions – in natural language and the model will attempt to ask your question.</p><p>GPT-4V performed well at various general image questions and demonstrated awareness of context in some images we tested. For instance, GPT-4V was able to successfully answer questions about a movie featured in an image without being told in text what the movie was.</p><p>For general question answering, GPT-4V is exciting. While models existed for this purpose in the past, they often lacked fluency in their answers. GPT-4V is able to both answer questions and follow up questions about an image and do so in depth.</p><p>With GPT-4V, you can ask questions about an image without creating a two-stage process (i.e. classification then using the results to ask a question to a language model like GPT). There will likely be limitations to what GPT-4V can understand, hence testing a use case to understand how the model performs is crucial.</p><p>With that said, GPT-4V has its limitations. The model did “hallucinate”, wherein the model returned inaccurate information. This is a risk with using language models to answer questions. Furthermore, the model was unable to accurately return bounding boxes for object detection, suggesting it is unfit for this use case currently.</p><p>We also observed that GPT-4V is unable to answer questions about people. When given a photo of Taylor Swift and asked who was featured in the image, the model declined to answer. OpenAI define this as an expected behavior in the published system card.</p><p>Interested in reading more of our experiments with multi-modal language models and GPT-4’s impact on computer vision? Check out the following guides:</p><ul><li><a href="https://blog.roboflow.com/gpt-4-impact-speculation/">Speculating on How GPT-4 Changes Computer Vision</a> (<a href="https://www.youtube.com/watch?v=aNLl0wEdMq4&amp;ref=blog.roboflow.com">Video</a>)</li><li><a href="https://blog.roboflow.com/how-good-is-bing-gpt-4-multimodality/">How Good Is Bing (GPT-4) Multimodality?</a></li><li><a href="https://blog.roboflow.com/chatgpt-code-interpreter-computer-vision/">ChatGPT Code Interpreter for Computer Vision</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Be My Eyes’ AI assistant starts rolling out (249 pts)]]></title>
            <link>https://www.bemyeyes.com/blog/announcing-be-my-ai</link>
            <guid>37673300</guid>
            <pubDate>Wed, 27 Sep 2023 11:48:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bemyeyes.com/blog/announcing-be-my-ai">https://www.bemyeyes.com/blog/announcing-be-my-ai</a>, See on <a href="https://news.ycombinator.com/item?id=37673300">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Since 2015, Be My Eyes has worked to connect our 6.9 million volunteers to users to assist them with everyday tasks. Our mission is to make the world more accessible for people who are blind or have low vision, which is why, seven months ago, our team began working with the blind community to incorporate AI into the existing Be My Eyes platform. Since then, over 19,000 blind and low-vision beta testers contributed to the design and function of our new AI feature.</p><p>Today we are thrilled to announce that Be My AI is officially entering an open beta phase for iOS users and in coming weeks will be available for hundreds of thousands of Be My Eyes users worldwide.&nbsp;</p><p>We’ll start releasing Be My AI to all our existing iPhone users this week. The full roll-out will take a few weeks, so be sure to keep your app updated so you will have access to Be My AI as soon as it is available to you.</p><h2>How to access and use Be My AI</h2><p>Using Be My AI in your everyday life is quick and simple. Once you have access, open the Be My Eyes app, click on the ‘Be My AI’ tab, and take a picture. Be My AI will give you a detailed description about it, and you can chat and ask Be My AI further questions to get more information. If you like what Be My AI described, you can send its response and photo to others, or use its description in social media.</p><p>And don’t worry - if Be My AI can’t answer all your questions, if you want to check its results, or if you just need a little more description than Be My AI can provide or crave the magic and humanity of working with people, you still can easily reach one of our dedicated volunteers, just like before. They will always be there, in 150 languages all across the globe.</p><p>If you want to learn more about Be My AI and how to use it at its best, we have collected the most common questions (and answers!) in our <a href="https://support.bemyeyes.com/hc/en-us/articles/18133134809105-How-do-I-use-Be-My-AI-beta-">Help Center</a>. Make sure to check them out!</p><h2>When to use Be My AI</h2><p>You can use Be My AI 24/7 in all those situations when you want quick visual assistance without necessarily calling a human volunteer. Be My AI is perfect for all those circumstances when you want a quick solution or you don’t feel like talking to another person to get visual assistance. You may be amazed that Be My AI knows more than just what’s in the photo – just ask for more context and discover what it can tell you.</p><p>Be My AI also will give deaf-blind users a new way to get information if they use, for example, a braille display. Be My AI's written responses are user-selectable in 29 languages.</p><p>For all of its advantages, though, Be My AI does not and should not replace a white cane, guide dog, or other mobility aid that provides for safe travel.&nbsp;</p><blockquote><em>“I have been using it in several ways: taking my own photos particularly of images in magazines, on Twitter where very few people add descriptions or alt text and on WhatsApp where my family send me photos in groups all the time without any context. I was unsure about using Chat GPT which I’d seen many blind people using on social media, but when I saw that you were adding it I thought I’d give it a go!” - Sarah, Be My AI User</em></blockquote><p>Over the past few months, our blind and low vision beta testers have experimented with Be My AI and discovered many different ways to use it throughout your day from learning how to use new breakfast products in the morning to making sure your light is off before going to bed! We have collected a bunch of real life examples directly from their experiences to inspire you and show you what Be My AI can do:</p><ul role="list"><li>Get information about a popcorn box and access to cooking directions.</li><li>Read buttons on your dishwasher, washing machine, and other appliances with flat-screen controls.</li><li>Re-organize your wardrobe or create the perfect outfit for a night out.</li><li>Read instructions to set up your new laptop, smartphone, or tablet.</li><li>Read comics, books, and magazines.</li><li>Set up your Apple TV, Chromecast, or Amazon Fire Stick.</li><li>Find something that you accidentally dropped on the floor.</li><li>Get descriptions of memes from Facebook, X, Instagram and Mastodon.</li><li>Take pictures of paintings, statues, and other artwork to get detailed descriptions of them. You can also get pictures from your holidays and special events described to you.</li><li>Read the number of your bus at the bus station, or check out the departures screen at the train station or at the airport.</li><li>Read the menu at the restaurant and get relevant information from your receipt.</li><li>Translate text from dozens of different languages.</li><li>Prepare for a university exam or get assistance with your homework.</li><li>Check your makeup and identify beauty products while getting ready to go out.</li></ul><blockquote><em>“There aren’t enough words in the world to express what a truly miraculous, life-changing, day-making thing Be My Eyes and especially Be My AI is. Now I’m actually looking forward to organizing my closet because I won’t need human help. It describes my clothes in a way that makes them all sound gorgeous!” - Aimee, Be My AI User</em></blockquote><h2>What is an “Open Beta”?</h2><p>“Open beta” status just means we are opening up Be My AI to all our iOS users while we continue developing it and making it better based on your feedback. AI image recognition is complicated, and AI in general is still a rapidly evolving technology just in its infancy. Be My Eyes - and OpenAI - are constantly learning how to improve it.</p><p>There will be hiccups. Things may break. You may still get hallucinations, occasional wrong answers, and experience some frustrations. But we will be here to fix the breaks and keep making things better. So please be patient, and keep telling us about your experiences, positive and negative, so we can make this the best possible tool for you.</p><h2>What’s next: Android</h2><p>We started closed beta testing Be My AI for Android devices last week, and our goal is to move it to broadly-available open beta status in the coming months. You can already <a href="https://play.google.com/store/apps/details?id=com.bemyeyes.bemyeyes">sign up on the waitlist for Android beta testing</a> directly in the Be My Eyes app.</p><p>Here at Be My Eyes, the pace of innovation is accelerating. To keep updated about Be My AI, make sure to follow Be My Eyes on <a href="https://www.linkedin.com/company/be-my-eyes/?originalSubdomain=dk">LinkedIn</a>, <a href="https://twitter.com/BeMyEyes?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">X</a>, <a href="https://www.facebook.com/bemyeyesapp/">Facebook</a>, <a href="https://www.instagram.com/bemyeyesapp/?hl=en">Instagram</a>, <a href="https://mastodon.social/@bemyeyes">Mastodon</a> and <a href="https://www.tiktok.com/@bemyeyesapp?lang=en">TikTok</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Uiua: A minimal stack-based, array-based language (125 pts)]]></title>
            <link>https://www.uiua.org/</link>
            <guid>37673127</guid>
            <pubDate>Wed, 27 Sep 2023 11:28:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.uiua.org/">https://www.uiua.org/</a>, See on <a href="https://news.ycombinator.com/item?id=37673127">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>Uiua <span>(<i>wee-wuh</i>)</span> is a
        stack-oriented array programming language with a focus on simplicity, beauty, and <a href="https://en.wikipedia.org/wiki/Tacit_programming">tacit</a> code.</p>
      
      <h3>Loading...</h3>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I got robbed of my first kernel contribution (652 pts)]]></title>
            <link>https://ariel-miculas.github.io/How-I-got-robbed-of-my-first-kernel-contribution/</link>
            <guid>37671991</guid>
            <pubDate>Wed, 27 Sep 2023 08:58:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ariel-miculas.github.io/How-I-got-robbed-of-my-first-kernel-contribution/">https://ariel-miculas.github.io/How-I-got-robbed-of-my-first-kernel-contribution/</a>, See on <a href="https://news.ycombinator.com/item?id=37671991">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <h3 id="context">Context</h3>
<p>Around a year and a half ago, I’ve asked my former company for some time to
work on an issue that was impacting the debugging capabilities in our project:
gdbserver couldn’t debug multithreaded applications running on a PowerPC32
architecture.  The connection to the gdbserver was broken and it couldn’t
control the debug session anymore. Multiple people have already investigated
this problem and I had a good starting point, but we still weren’t sure in
which software component the issue lied: it could have been the toolchain, the
gdbserver, the Linux kernel or the custom patches we applied on top of the
kernel tree. We were quite far away from finding the root cause.</p>

<h3 id="investigating-the-issue">Investigating the issue</h3>
<p>After diving into the existing analysis for this issue and channeling my
google-fu, I’ve had my first breakthrough: an <a href="https://lore.kernel.org/linuxppc-dev/dc38afe9-6b78-f3f5-666b-986939e40fc6@keymile.com/">email
thread</a>
which not only described the same symptoms as our issue, but also pointed to
the <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v6.6-rc3&amp;id=0c8c0f03e3a292e031596484275c14cf39c0ab7a">exact
commit</a>
which introduced it. The patch that introduced the bug moved the definition of
<code>thread_struct thread</code> from the middle of the <code>task_struct</code> to the end, a
seeminlgy innocuous change.</p>

<p>After debugging the issue, this is what Holger Brunck
<a href="https://lore.kernel.org/linuxppc-dev/e5cbd015-eeb5-31b5-0829-14cc8500dc6d@keymile.com/">observed</a></p>
<blockquote>
  <p>What I see is that gdbserver sends for each thread a SIGSTOP to the kernel and
waits for a response. The kernel does receive all the signals but only respond
to some of them in the error case. Which then matches with my “ps” output as I
see that some threads are not in the state pthread_stop and then the gdbserver
gets suspended.</p>
</blockquote>

<p>The low-level issue was that after interacting with gdbserver, some threads
were in the wrong process state and gdbserver couldn’t control them anymore.</p>

<p>I’ve spent 3-4 days reading commit descriptions related to the PowerPC
architecture and the changes around <code>task_struct</code>, trying to figure out whether
this issue was solved in subsequent kernel versions (spoiler: it was not).
I’ve moved <code>thread_struct thread</code> around to determine when the issue reproduced
and used <a href="https://linux.die.net/man/1/pahole">pahole</a> to inspect
<code>task_struct</code>’s layout. I’ve used
<a href="https://www.kernel.org/doc/html/v5.0/trace/ftrace.html">ftrace</a> to figure out
when the threads of the debugged process were scheduled and that’s how I
realized this could be a memory corruption issue: the threads that were stuck
were only scheduled once, unlike the other ones. I’ve originally dismissed that
this could be a memory corruption issue because in the <a href="https://lore.kernel.org/linuxppc-dev/b78d9e5d-fc2e-3676-a47e-ed5ca7a836e6@keymile.com/">original
thread</a>
it was mentioned that:</p>
<blockquote>
  <p>the content of the buffer is always zero and does not change. So at least no
one is writing non-zero to the buffer.</p>
</blockquote>

<p>That’s what I get for not verifying that the structure isn’t overwritten with
zero bytes (always validate your assumptions).</p>

<p>I remembered that the x86 architecture has <a href="https://en.wikipedia.org/wiki/X86_debug_register">debug
registers</a> that could be used
to trigger data write breakpoints. In fact, this is how I solved a bug back in
my earlier days as a software engineer. Sure enough, PowerPC also implements a
similar capability with the help of the <a href="https://stackoverflow.com/a/327540">DABR register</a>.</p>

<p>I’ve investigated how I could use hardware breakpoints on Linux and I ended up
implementing a linux kernel module based on this <a href="https://stackoverflow.com/a/19755213">excellent stackoverflow
answer</a>. This allowed me to place a
hardware breakpoint on the <a href="https://elixir.bootlin.com/linux/v6.5.5/source/include/linux/sched.h#L746">__state
field</a>
to figure out who on earth writes to it.</p>

<h3 id="finding-the-bug">Finding the bug</h3>
<p>And that’s how I found the issue: my custom kernel module showed the stack
traces from the places where the <code>__state</code> field of <code>task_struct</code> was being
written to.  I’ve noticed an outlier which revealed a buffer overflow in
<code>ptrace_put_fpr</code> (used by the POKEUSER API). This led to important fields from
<code>task_struct</code> getting overwritten, such as <code>__state</code>, which stores the state of
the process and it’s also used by the kernel to keep track of which processes
are stopped by the debugger.</p>

<p>The cause of this overflow? Taking an index meant to be used with an array of
32-bit elements and indexing an array of 64-bit elements. There were 64 indexes
that addressed the FPR, so the total addressable memory was 64 * 8 = 512
bytes. But there were only 32 entries in the fp_state.fpr array, which means
that the available memory was only 32 * 8 = 256 bytes. That allowed the user
(aka gdbserver) to write up to 256 bytes past the end of the array.
<img src="https://ariel-miculas.github.io/images/fpr-overflow.png" alt="fpr-overflow"></p>

<h3 id="sending-the-patch-upstream">Sending the patch upstream</h3>
<p>I’ve sent a patch to the Linux kernel security team (security@kernel.org)
because I wanted to err on the safe side: a memory corruption issue that could
overwrite the memory of the processes’s states could have security
implications. Unfortunately, this mailing list is private so I cannot link to
the original patch I sent.  Michael Ellerman, the PowerPC maintainer, followed
up and told me he will contact me in private to figure this issue out. I have
actually sent him two patches fixing the issue: the original one that I sent to
the security mailing list and <a href="https://lists.ozlabs.org/pipermail/linuxppc-dev/2022-June/244438.html">another
version</a>
(quite different from the first one) which addressed some suggestions received
in reply to my original submission. And the latter patch was actually based on
existing kernel code, which emulated PowerPC32 operations on PowerPC64 (yeah,
they got the FPR indexing right). Neither of those were accepted by Michael
Ellerman, and instead he implemented his <a href="https://lore.kernel.org/all/20220609133245.573565-1-mpe@ellerman.id.au/">own version of the
fix</a>.
I told him that I would really appreciate if he could accept a patch from me,
so that I could receive credit for fixing this issue and become a kernel
contributor. I was also open to working with him, addressing his feedback and
sending subsequent versions of patches. He said (paraphrasing):</p>
<blockquote>
  <p>Sorry, I like my version better. If you want to be a Linux kernel
contributor, here’s an issue you could fix.</p>
</blockquote>

<p>I found this really perplexing and insulting. Instead of getting recognized for
fixing the issue, he wanted to give me more work to do. My company and I should
have received proper credit for solving this issue, especially considering how
much effort we put into it.</p>

<p>I felt it was really unfair to only get a “Reported-by” tag. Here’s the
<a href="https://docs.kernel.org/process/submitting-patches.html#using-reported-by-tested-by-reviewed-by-suggested-by-and-fixes">purpose of the tag</a>:</p>

<blockquote>
  <p>The Reported-by tag gives credit to people who find bugs and report them and it hopefully inspires them to help us again in the future.</p>
</blockquote>

<p>Well, I certainly didn’t feel inspired to get involved with the kernel
community again. On the contrary, I felt belittled and angry that my work
wasn’t properly recognized.</p>

<h3 id="conclusion">Conclusion</h3>
<p>I spent a lot of time and effort doing root cause analysis, fixing the bug,
testing and validating the fix, getting feedback from other engineers at my
company, adapting the fix to the latest kernel version, and sending two
different patches to Michael Ellerman, the PowerPC maintainer. Instead of
accepting my patch or guiding me towards a better solution, he went ahead and
implemented his own fix, giving me credit only for reporting the issue (which
was <a href="https://lore.kernel.org/linuxppc-dev/dc38afe9-6b78-f3f5-666b-986939e40fc6@keymile.com/">already
reported</a>
six years prior to this).</p>

<p>My first contribution to the kernel was a really frustrating and discouraging
experience, dealing with people who do not think it’s important to get proper
recognition for your work.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Essence: A desktop OS built from scratch, for control and simplicity (392 pts)]]></title>
            <link>https://nakst.gitlab.io/essence</link>
            <guid>37671419</guid>
            <pubDate>Wed, 27 Sep 2023 07:44:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nakst.gitlab.io/essence">https://nakst.gitlab.io/essence</a>, See on <a href="https://news.ycombinator.com/item?id=37671419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p>
						Efficient with resources.
					</p>

					

					<p>
						Essence will happily run on low-powered hardware. It can take less than 30MB of drive space, and boot with even less RAM. No tasks run in the background, giving your applications all the space they need.
					</p>

					

					<p><img src="https://nakst.gitlab.io/screenshot4.png">
				</p></div><div>
					<p>
						Open source.
					</p>

					

					<p>
						All the code is made available under the MIT license. You can browse through the source on the <a href="https://gitlab.com/nakst/essence">GitLab repository</a>.
					</p>

					

					<p>
						If you're interested in contributing, join our <a href="https://discord.gg/skeP9ZGDK8">Discord server</a> to discuss ideas with other developers.
					</p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A JavaScript function that looks and behaves like a pipe operator (105 pts)]]></title>
            <link>https://github.com/laurentpayot/verticalize</link>
            <guid>37671341</guid>
            <pubDate>Wed, 27 Sep 2023 07:31:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/laurentpayot/verticalize">https://github.com/laurentpayot/verticalize</a>, See on <a href="https://news.ycombinator.com/item?id=37671341">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content--verticalize" dir="auto"><a href="#-verticalize"><sub><img src="https://github.com/laurentpayot/verticalize/raw/main/verticalize.svg" alt="triple chevron down" width="48" height="48"></sub> Verticalize</a></h2>
<p dir="auto">A pipe-like function to verticalize your JavaScript code</p>

<p dir="auto"><a href="https://github.com/laurentpayot/verticalize/blob/main/package.json#L56"><img src="https://camo.githubusercontent.com/5f7c6a59930fcd08aaa2c68c72ba3a9387164ccf2d20a55f7c422bbe9bb3dbac/68747470733a2f2f62616467656e2e6e65742f7374617469632f646570656e64656e636965732f4e6f6e652f677265656e" alt="dependencies" data-canonical-src="https://badgen.net/static/dependencies/None/green"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a77cecc88f9cbbda946d45a207027d16f6f1195a686b10bfbb4436efbdd4bbf7/68747470733a2f2f62616467656e2e6e65742f626164676573697a652f62726f746c692f6c617572656e747061796f742f766572746963616c697a652f6d61696e2f766572746963616c697a652e6d696e2e6a73"><img src="https://camo.githubusercontent.com/a77cecc88f9cbbda946d45a207027d16f6f1195a686b10bfbb4436efbdd4bbf7/68747470733a2f2f62616467656e2e6e65742f626164676573697a652f62726f746c692f6c617572656e747061796f742f766572746963616c697a652f6d61696e2f766572746963616c697a652e6d696e2e6a73" alt="minified + brotlied size" data-canonical-src="https://badgen.net/badgesize/brotli/laurentpayot/verticalize/main/verticalize.min.js"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a7b8c389b3a8012d017c9a8ca9223e6f21eca0c7fa734a237955ef707ddefaee/68747470733a2f2f62616467656e2e6e65742f626164676573697a652f677a69702f6c617572656e747061796f742f766572746963616c697a652f6d61696e2f766572746963616c697a652e6d696e2e6a73"><img src="https://camo.githubusercontent.com/a7b8c389b3a8012d017c9a8ca9223e6f21eca0c7fa734a237955ef707ddefaee/68747470733a2f2f62616467656e2e6e65742f626164676573697a652f677a69702f6c617572656e747061796f742f766572746963616c697a652f6d61696e2f766572746963616c697a652e6d696e2e6a73" alt="minified + zipped size" data-canonical-src="https://badgen.net/badgesize/gzip/laurentpayot/verticalize/main/verticalize.min.js"></a></p>
<p dir="auto"><a href="https://github.com/laurentpayot/verticalize/blob/main/index.d.ts"><img src="https://camo.githubusercontent.com/65d29de6c7efead9588acac34c7c6154d7e0c2bcf003aa47f7e13a8a909a51b2/68747470733a2f2f62616467656e2e6e65742f6e706d2f74797065732f766572746963616c697a65" alt="types" data-canonical-src="https://badgen.net/npm/types/verticalize"></a>
<a href="https://www.npmjs.com/package/verticalize" rel="nofollow"><img src="https://camo.githubusercontent.com/bdeb60a81e1ad0cd88137dc6fd119447e24cc2430eb0949ae4788976e5f21894/68747470733a2f2f62616467656e2e6e65742f6e706d2f762f766572746963616c697a65" alt="npm" data-canonical-src="https://badgen.net/npm/v/verticalize"></a>
<a href="https://github.com/laurentpayot/verticalize/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/eb0514d18fdb21aadbb299309ccae39d3914f13dd27ee6446039c263d6f1084e/68747470733a2f2f62616467656e2e6e65742f6769746875622f6c6963656e73652f6c617572656e747061796f742f766572746963616c697a65" alt="license" data-canonical-src="https://badgen.net/github/license/laurentpayot/verticalize"></a></p>
<h2 tabindex="-1" id="user-content-gist" dir="auto"><a href="#gist">Gist</a></h2>
<p dir="auto">The following example code is a bit hard to read:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const { status } = await send(capitalize(greeting) + &quot;!&quot;)
console.log(status)"><pre><span>const</span> <span>{</span> status <span>}</span> <span>=</span> <span>await</span> <span>send</span><span>(</span><span>capitalize</span><span>(</span><span>greeting</span><span>)</span> <span>+</span> <span>"!"</span><span>)</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>status</span><span>)</span></pre></div>
<p dir="auto">Make it less nested, more <em>vertical</em>, by using the <code>V</code> "pipe":</p>
<div dir="auto" data-snippet-clipboard-copy-content="V( greeting,     // initial value ➡ &quot;hi&quot;
V (capitalize),  // custom function call ➡ &quot;Hi&quot;
V .concat(&quot;!&quot;),  // String method `concat` call ➡ &quot;Hi!&quot;
V (send),        // custom async function call ➡ Promise { <pending> }
V .status,       // automatic promise chaining + getting property ➡ Promise { 200 }
V (console.log), // automatic promise chaining + global function call ➡ logs 200
)"><pre><span>V</span><span>(</span> <span>greeting</span><span>,</span>     <span>// initial value ➡ "hi"</span>
<span>V</span> <span>(</span><span>capitalize</span><span>)</span><span>,</span>  <span>// custom function call ➡ "Hi"</span>
<span>V</span> <span>.</span><span>concat</span><span>(</span><span>"!"</span><span>)</span><span>,</span>  <span>// String method `concat` call ➡ "Hi!"</span>
<span>V</span> <span>(</span><span>send</span><span>)</span><span>,</span>        <span>// custom async function call ➡ Promise { &lt;pending&gt; }</span>
<span>V</span> <span>.</span><span>status</span><span>,</span>       <span>// automatic promise chaining + getting property ➡ Promise { 200 }</span>
<span>V</span> <span>(</span><span>console</span><span>.</span><span>log</span><span>)</span><span>,</span> <span>// automatic promise chaining + global function call ➡ logs 200</span>
<span>)</span></pre></div>
<p dir="auto">If your IDE or a tool like Prettier automatically formats the code for you, it may result in the following syntax (still working):</p>
<div dir="auto" data-snippet-clipboard-copy-content="V(greeting,
  V(capitalize),
  V.concat(&quot;!&quot;),
  V(send),
  V.status,
  V(console.log),
)"><pre><span>V</span><span>(</span><span>greeting</span><span>,</span>
  <span>V</span><span>(</span><span>capitalize</span><span>)</span><span>,</span>
  <span>V</span><span>.</span><span>concat</span><span>(</span><span>"!"</span><span>)</span><span>,</span>
  <span>V</span><span>(</span><span>send</span><span>)</span><span>,</span>
  <span>V</span><span>.</span><span>status</span><span>,</span>
  <span>V</span><span>(</span><span>console</span><span>.</span><span>log</span><span>)</span><span>,</span>
<span>)</span></pre></div>
<p dir="auto">Verticalize’s <code>V</code> function is around 200 bytes minified and compressed, without dependencies. It won’t bloat your web app.</p>
<h2 tabindex="-1" id="user-content-nodejs" dir="auto"><a href="#nodejs">NodeJS</a></h2>
<h3 tabindex="-1" id="user-content-installation" dir="auto"><a href="#installation">Installation</a></h3>

<h3 tabindex="-1" id="user-content-import" dir="auto"><a href="#import">Import</a></h3>
<div dir="auto" data-snippet-clipboard-copy-content="import { V } from 'verticalize'"><pre><span>import</span> <span>{</span> <span>V</span> <span>}</span> <span>from</span> <span>'verticalize'</span></pre></div>
<h2 tabindex="-1" id="user-content-browser" dir="auto"><a href="#browser">Browser</a></h2>
<p dir="auto">Verticalize uses <a href="https://jakearchibald.com/2017/es-modules-in-browsers/" rel="nofollow">ES modules</a>, <a href="https://caniuse.com/es6-module" rel="nofollow">widely supported</a> in browsers nowadays. Import the <code>V</code> function from the <code>verticalize.min.js</code> file. This file can be located in a CDN (example below) or copied in any directory of your website (for better performance and to be GDPR compliant, since you don’t have to connect to a third party server).</p>
<div dir="auto" data-snippet-clipboard-copy-content="<script type=&quot;module&quot;>
  import { V } from 'https://cdn.jsdelivr.net/npm/verticalize@0.1.2/verticalize.min.js'
</script>"><pre><span>&lt;</span><span>script</span> <span>type</span>="<span>module</span>"<span>&gt;</span>
  <span>import</span> <span>{</span> <span>V</span> <span>}</span> <span>from</span> <span>'https://cdn.jsdelivr.net/npm/verticalize@0.1.2/verticalize.min.js'</span>
<span>&lt;/</span><span>script</span><span>&gt;</span></pre></div>
<h2 tabindex="-1" id="user-content-v-function-usage" dir="auto"><a href="#v-function-usage"><code>V</code> function usage</a></h2>
<p dir="auto">The gist example above covers pretty much everything. Just call the <code>V</code> function with the initial <em>value</em> as the first argument, followed by the other arguments wrapped by another <code>V</code> at the beginning of the line to get a nice <sub><a target="_blank" rel="noopener noreferrer" href="https://github.com/laurentpayot/verticalize/blob/main/verticalize.svg"><img src="https://github.com/laurentpayot/verticalize/raw/main/verticalize.svg" alt="triple chevron down" width="18" height="18"></a></sub> syntax. All these <code>V</code>-prefixed lines will then act like a pipeline, the output of a pipe being the input of the following pipe. Pipes can use unary functions, methods and properties, but not values (except for the initial value).</p>
<h3 tabindex="-1" id="user-content-unary-functions" dir="auto"><a href="#unary-functions">Unary functions</a></h3>
<p dir="auto">A unary function is a function that takes only one argument. You can use an anonymous ("arrow") function to turn a multi-argument function into a unary one.</p>
<div dir="auto" data-snippet-clipboard-copy-content="V( 1.9,                  // initial value
V (Math.round),          // unary function
V (n => Math.pow(n, 3)), // binary function turned into unary
) // returns 8"><pre><span>V</span><span>(</span> <span>1.9</span><span>,</span>                  <span>// initial value</span>
<span>V</span> <span>(</span><span>Math</span><span>.</span><span>round</span><span>)</span><span>,</span>          <span>// unary function</span>
<span>V</span> <span>(</span><span>n</span> <span>=&gt;</span> <span>Math</span><span>.</span><span>pow</span><span>(</span><span>n</span><span>,</span> <span>3</span><span>)</span><span>)</span><span>,</span> <span>// binary function turned into unary</span>
<span>)</span> <span>// returns 8</span></pre></div>
<h3 tabindex="-1" id="user-content-methods-and-properties" dir="auto"><a href="#methods-and-properties">Methods and properties</a></h3>
<p dir="auto">To call a method or to get a property of the previous pipe output (or of the initial value), you can use an anonymous function like <code>count =&gt; count.add(1)</code>, but or convenience Verticalize allows you to use a direct dot syntax.</p>
<div dir="auto" data-snippet-clipboard-copy-content="V ( [1, 2, 3],        // initial Array value
V .concat([4, 5, 6]), // calling the Array method `concat()` (returning an Array)
V .length,            // getting the Array property `length`
) // returns 6"><pre><span>V</span> <span>(</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>]</span><span>,</span>        <span>// initial Array value</span>
<span>V</span> <span>.</span><span>concat</span><span>(</span><span>[</span><span>4</span><span>,</span> <span>5</span><span>,</span> <span>6</span><span>]</span><span>)</span><span>,</span> <span>// calling the Array method `concat()` (returning an Array)</span>
<span>V</span> <span>.</span><span>length</span><span>,</span>            <span>// getting the Array property `length`</span>
<span>)</span> <span>// returns 6</span></pre></div>
<h3 tabindex="-1" id="user-content-promises" dir="auto"><a href="#promises">Promises</a></h3>
<p dir="auto">When the previous pipe output (or the initial value) is a promise, the next pipe will automatically chain it so you don’t have to write many <code>.then()</code> yourself.</p>
<div dir="auto" data-snippet-clipboard-copy-content="const greeting =
  await
  V( Promise.resolve(&quot;Hello!&quot;),
  V .toUpperCase(),
  )"><pre><span>const</span> <span>greeting</span> <span>=</span>
  <span>await</span>
  <span>V</span><span>(</span> <span>Promise</span><span>.</span><span>resolve</span><span>(</span><span>"Hello!"</span><span>)</span><span>,</span>
  <span>V</span> <span>.</span><span>toUpperCase</span><span>(</span><span>)</span><span>,</span>
  <span>)</span></pre></div>
<p dir="auto">is the same as</p>
<div dir="auto" data-snippet-clipboard-copy-content="const greeting =
  await Promise.resolve(&quot;Hello!&quot;)
    .then(s => s.toUpperCase())"><pre><span>const</span> <span>greeting</span> <span>=</span>
  <span>await</span> <span>Promise</span><span>.</span><span>resolve</span><span>(</span><span>"Hello!"</span><span>)</span>
    <span>.</span><span>then</span><span>(</span><span>s</span> <span>=&gt;</span> <span>s</span><span>.</span><span>toUpperCase</span><span>(</span><span>)</span><span>)</span></pre></div>
<h2 tabindex="-1" id="user-content-note" dir="auto"><a href="#note">Note</a></h2>
<p dir="auto"><a href="https://github.com/tc39/proposal-pipeline-operator">A TC39 proposal</a> for the pipe operator <code>|&gt;</code> was created in 2021 and is currently in stage 2. It may or may not be included in the official JavaScript specs in a few years. If so, then it will take a few more years to be adopted by all the major browsers and runtimes. But you can use Verticalize <em>right now</em> and enjoy its unique dot syntax and automatic promise chaining features 😉</p>
<h2 tabindex="-1" id="user-content-license" dir="auto"><a href="#license">License</a></h2>
<p dir="auto"><a href="https://github.com/laurentpayot/verticalize/blob/main/LICENSE">MIT</a></p>
<h2 tabindex="-1" id="user-content-stargazers-heart" dir="auto"><a href="#stargazers-heart">Stargazers ❤️</a></h2>
<p dir="auto"><a href="https://github.com/laurentpayot/verticalize/stargazers"><img src="https://camo.githubusercontent.com/cfdd0f9eb60f0a581daed15b5f8e9105ab8a657985bcde8c7559ef603d774dd0/68747470733a2f2f7265706f726f737465722e636f6d2f73746172732f6c617572656e747061796f742f766572746963616c697a65" alt="Stargazers repo roster for @laurentpayot/verticalize" data-canonical-src="https://reporoster.com/stars/laurentpayot/verticalize"></a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing a debugger from scratch: Breakpoints (267 pts)]]></title>
            <link>https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-5/</link>
            <guid>37670938</guid>
            <pubDate>Wed, 27 Sep 2023 06:31:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-5/">https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-5/</a>, See on <a href="https://news.ycombinator.com/item?id=37670938">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p>(New to this series? Consider starting from <a href="https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-1">part 1</a>)</p><p>At the end of the <a href="https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-4">last post</a>, we started to get some interesting functionality with the ability to resolve addresses to names in a module. This was the last functionality missing before we could implement breakpoints! This part adds the ability for DbgRs to set hardware breakpoints.</p><p>The code for this post is in the <a href="https://github.com/TimMisiak/dbgrs/tree/part5">part5 branch on github</a>. You can also view the <a href="https://github.com/TimMisiak/dbgrs/compare/part4...part5">changes from part4</a>. If you see any mistakes or ways to improve the code, feel free to <a href="https://github.com/TimMisiak/dbgrs/issues">create issues</a> on the GitHub repo or submit a PR.</p><h2 id="first-some-cleanup">First, some cleanup</h2><p>I’ve been trying to keep DbgRs as simple as possible, avoiding extra architectural layers to keep the concepts as clear and concise as possible, but now that it has grown a bit, we need a little bit of cleanup to keep things easy to understand. I’ll just cover these changes briefly and then we’ll get to breakpoints.</p><p>The biggest change is that I moved all of the code dealing with <a href="https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-waitfordebugeventex">WaitForDebugEventEx</a> into a new file, <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/event.rs">event.rs</a>. A single public function allows waiting for the next debug event, and returns a new <code>DebugEvent</code> enum instead of the raw win32 <code>DEBUG_EVENT</code> type.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span> <span>enum</span> <span>DebugEvent</span> {
</span></span><span><span>    Exception{first_chance: <span>bool</span>, exception_code: <span>i32</span>},
</span></span><span><span>    CreateProcess{exe_name: Option<span>&lt;</span>String<span>&gt;</span>, exe_base: <span>u64</span>},
</span></span><span><span>    CreateThread{thread_id: <span>u32</span>},
</span></span><span><span>    ExitThread{thread_id: <span>u32</span>},
</span></span><span><span>    LoadModule{module_name: Option<span>&lt;</span>String<span>&gt;</span>, module_base: <span>u64</span>},
</span></span><span><span>    OutputDebugString(String),
</span></span><span><span>    ExitProcess,
</span></span><span><span>    Other(String)
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>EventContext</span> {
</span></span><span><span>    <span>pub</span> process_id: <span>u32</span>,
</span></span><span><span>    <span>pub</span> thread_id: <span>u32</span>,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>fn</span> <span>wait_for_next_debug_event</span>(mem_source: <span>&amp;</span><span>dyn</span> MemorySource) -&gt; (EventContext, DebugEvent) {
</span></span><span><span>    <span>//...
</span></span></span><span><span><span></span>}
</span></span></code></pre></div><p>As a result, the <code>main_debugger_loop</code> function is a bit smaller and can focus on the core debugger loop logic.</p><p>The other change is that I’ve added a file called <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/util.rs">util.rs</a> which has some common helpers for win32 structures, including a thin <code>AutoClosedHandle</code> wrapper for <code>HANDLE</code> and the <code>AlignedContext</code> struct that wraps the win32 <code>CONTEXT</code>. Additionally, this includes some constants that are missing from the windows-rs crate.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>#[repr(align(16))]</span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>AlignedContext</span> {
</span></span><span><span>    <span>pub</span> context: <span>CONTEXT</span>,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>AutoClosedHandle</span>(<span>pub</span> <span>HANDLE</span>);
</span></span></code></pre></div><h2 id="evaluating-symbols">Evaluating symbols</h2><p>When setting a breakpoint, it’s expected that you can use the name of a function, and not just the address. So to start, we need to add the capability for resolving a name to an address. Previously, we had the <code>resolve_address_to_name</code> function in <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/name_resolution.rs#L56">name_resolution.rs</a>, so we’ll add the corresponding <code>resolve_name_to_address</code> function there as well.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span> <span>fn</span> <span>resolve_name_to_address</span>(sym: <span>&amp;</span><span>str</span>, process: <span>&amp;</span><span>mut</span> Process) -&gt; Result<span>&lt;</span><span>u64</span>, String<span>&gt;</span> {
</span></span><span><span>    <span>match</span> sym.chars().position(<span>|</span>c<span>|</span> c <span>==</span> <span>'!'</span>) {
</span></span><span><span>        None <span>=&gt;</span> {
</span></span><span><span>            <span>// Search all modules
</span></span></span><span><span><span></span>            Err(<span>"Not yet implemented"</span>.to_string())
</span></span><span><span>        },
</span></span><span><span>        Some(pos) <span>=&gt;</span> {
</span></span><span><span>            <span>let</span> module_name <span>=</span> <span>&amp;</span>sym[<span>..</span>pos];
</span></span><span><span>            <span>let</span> func_name <span>=</span> <span>&amp;</span>sym[pos <span>+</span> <span>1</span><span>..</span>];
</span></span><span><span>            <span>if</span> <span>let</span> Some(module) <span>=</span> process.get_module_by_name_mut(module_name) {
</span></span><span><span>                <span>if</span> <span>let</span> Some(addr) <span>=</span> resolve_function_in_module(module, func_name) {
</span></span><span><span>                    Ok(addr)
</span></span><span><span>                } <span>else</span> {
</span></span><span><span>                    Err(format!(<span>"Could not find </span><span>{}</span><span> in module </span><span>{}</span><span>"</span>, func_name, module_name))
</span></span><span><span>                }
</span></span><span><span>            } <span>else</span> {
</span></span><span><span>                Err(format!(<span>"Could not find module </span><span>{}</span><span>"</span>, module_name))
</span></span><span><span>            }
</span></span><span><span>        },
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>For now, we’ll take only the fully qualified name in <code>module.dll!functionName</code> <a aria-describedby="footnote-label" href="#fully-qualified">form</a>, and allow only exact matches.</p><p>Using this function, we can add symbols to our evaluation grammar in <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/command.rs#L26">command.rs</a></p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>#[rust_sitter::language]</span>
</span></span><span><span>    <span>pub</span> <span>enum</span> <span>EvalExpr</span> {
</span></span><span><span>        Number(<span>#[rust_sitter::leaf(pattern = r</span><span>"(\d+|0x[0-9a-fA-F]+)"</span><span>, transform = parse_int)]</span> <span>u64</span>),
</span></span><span><span>        Symbol(<span>#[rust_sitter::leaf(pattern = r</span><span>"(([a-zA-Z0-9_@#.]+!)?[a-zA-Z0-9_@#.]+)"</span><span>, transform = parse_sym)]</span> String),
</span></span></code></pre></div><p>In order to evaluate symbols, the <code>evaluate_expression</code> function now needs a context that it can use to evaluate symbols against. For this, we’ll just pass in a structure with a reference to the <code>Process</code>.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span> <span>struct</span> <span>EvalContext</span><span>&lt;</span><span>'a</span><span>&gt;</span> {
</span></span><span><span>    <span>pub</span> process: <span>&amp;</span><span>'a</span> <span>mut</span> Process,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>fn</span> <span>evaluate_expression</span>(expr: <span>EvalExpr</span>, context: <span>&amp;</span><span>mut</span> EvalContext) -&gt; Result<span>&lt;</span><span>u64</span>, String<span>&gt;</span> {
</span></span></code></pre></div><p>Note that it also returns a <code>Result</code> now because the name resolution can fail. Most of the function is unchanged besides passing the context through, and it now handles <code>EvalExpr::Symbol</code> by passing it to <code>name_resolution::resolve_to_address</code>.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>match</span> expr {
</span></span><span><span>        EvalExpr::Number(x) <span>=&gt;</span> Ok(x),
</span></span><span><span>        EvalExpr::Add(x, _, y) <span>=&gt;</span> Ok(evaluate_expression(<span>*</span>x, context)<span>?</span> <span>+</span> evaluate_expression(<span>*</span>y, context)<span>?</span>),
</span></span><span><span>        EvalExpr::Symbol(sym) <span>=&gt;</span> {
</span></span><span><span>            resolve_name_to_address(<span>&amp;</span>sym, context.process)
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>We can verify this is working by simply evaluating a symbol to an address and making sure it resolves back to the same symbol.</p><pre tabindex="0"><code>&gt; ? ntdll.dll!NtMapViewOfSection+0x14
 = 0x7FFE7360F154
[11254] ntdll.dll!NtMapViewOfSection+0x14
&gt; ln 0x7FFE7360F154
ntdll.dll!NtMapViewOfSection+0x14
[11254] ntdll.dll!NtMapViewOfSection+0x14
</code></pre><p>Success!</p><h2 id="keeping-track-of-breakpoints">Keeping track of breakpoints</h2><p>With the new functionality in the expression evaluator to evaluate symbols, we can add the commands for setting, clearing, and listing breakpoints. First, we add the breakpoint commands to the command grammar in <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/command.rs#L15">command.rs</a></p><div><pre tabindex="0"><code data-lang="rust"><span><span>        SetBreakpoint(<span>#[rust_sitter::leaf(text = </span><span>"bp"</span><span>)]</span> (), Box<span>&lt;</span>EvalExpr<span>&gt;</span>),
</span></span><span><span>        ListBreakpoints(<span>#[rust_sitter::leaf(text = </span><span>"bl"</span><span>)]</span> ()),
</span></span><span><span>        ClearBreakpoint(<span>#[rust_sitter::leaf(text = </span><span>"bc"</span><span>)]</span> (), Box<span>&lt;</span>EvalExpr<span>&gt;</span>),
</span></span></code></pre></div><p>The implementation of these commands need something to talk to, so we’ll create a new structure called BreakpointManager that keeps track of the breakpoints that should be set in the process, and create this at the start of the <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/main.rs#L81">main_debugger_loop</a></p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>fn</span> <span>main_debugger_loop</span>(process: <span>HANDLE</span>) {
</span></span><span><span>    <span>let</span> <span>mut</span> breakpoints <span>=</span> BreakpointManager::new();
</span></span></code></pre></div><p>We’ll get to the implementation of <code>BreakpointManager</code> in a minute, but first we can just see the simple implementation of <code>bp</code>, <code>bl</code>, and <code>bc</code> calling into the breakpoint manager.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>let</span> cmd <span>=</span> command::read_command();
</span></span><span><span>    <span>//...
</span></span></span><span><span><span></span>    <span>match</span> cmd {
</span></span><span><span>        <span>//...
</span></span></span><span><span><span></span>        CommandExpr::SetBreakpoint(_, expr) <span>=&gt;</span> {
</span></span><span><span>            <span>if</span> <span>let</span> Some(addr) <span>=</span> eval_expr(expr) {
</span></span><span><span>                breakpoints.add_breakpoint(addr);
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>        CommandExpr::ListBreakpoints(_) <span>=&gt;</span> {
</span></span><span><span>            breakpoints.list_breakpoints(<span>&amp;</span><span>mut</span> process);
</span></span><span><span>        }
</span></span><span><span>        CommandExpr::ClearBreakpoint(_, expr) <span>=&gt;</span> {
</span></span><span><span>            <span>if</span> <span>let</span> Some(id) <span>=</span> eval_expr(expr) {
</span></span><span><span>                breakpoints.clear_breakpoint(id <span>as</span> <span>u32</span>);
</span></span><span><span>            }
</span></span><span><span>        }
</span></span></code></pre></div><p>The <code>BreakpointManager</code> contains the list of the breakpoints that have been requested by the user. It has functions for adding a breakpoint at a specified address, removing a breakpoint given its ID, and listing the breakpoints for the user.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>struct</span> <span>Breakpoint</span> {
</span></span><span><span>    addr: <span>u64</span>,
</span></span><span><span>    id: <span>u32</span>,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>BreakpointManager</span> {
</span></span><span><span>    breakpoints: Vec::<span>&lt;</span>Breakpoint<span>&gt;</span>,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>impl</span> BreakpointManager {
</span></span><span><span>
</span></span><span><span>    <span>pub</span> <span>fn</span> <span>add_breakpoint</span>(<span>&amp;</span><span>mut</span> self, addr: <span>u64</span>) {
</span></span><span><span>        self.breakpoints.push(Breakpoint{addr, id: <span>self</span>.get_free_id()});
</span></span><span><span>        self.breakpoints.sort_by(<span>|</span>a, b<span>|</span> a.id.cmp(<span>&amp;</span>b.id));
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>pub</span> <span>fn</span> <span>list_breakpoints</span>(<span>&amp;</span>self, process: <span>&amp;</span><span>mut</span> Process) {
</span></span><span><span>        <span>for</span> bp <span>in</span> self.breakpoints.iter() {
</span></span><span><span>            <span>if</span> <span>let</span> Some(sym) <span>=</span> name_resolution::resolve_address_to_name(bp.addr, process) {
</span></span><span><span>                println!(<span>"</span><span>{:3}</span><span> </span><span>{:#018x}</span><span> (</span><span>{}</span><span>)"</span>, bp.id, bp.addr, sym)
</span></span><span><span>            } <span>else</span> {
</span></span><span><span>                println!(<span>"</span><span>{:3}</span><span> </span><span>{:#018x}</span><span>"</span>, bp.id, bp.addr)
</span></span><span><span>            }            
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>pub</span> <span>fn</span> <span>clear_breakpoint</span>(<span>&amp;</span><span>mut</span> self, id: <span>u32</span>) {
</span></span><span><span>        self.breakpoints.retain(<span>|</span>x<span>|</span> x.id <span>!=</span> id)
</span></span><span><span>    }
</span></span></code></pre></div><p>We can test these commands to make sure breakpoints are tracked correctly, although we still need to apply the breakpoints to the target process before they’ll do anything.</p><pre tabindex="0"><code>Command line was: '"C:\git\HelloWorld\hello.exe" '
LoadDll: 7FF7E7420000   hello.exe
[5A70] 0x00007ffdaed4aa40
&gt; g
LoadDll: 7FFDAECF0000   ntdll.dll
[5A70] ntdll.dll!RtlUserThreadStart
&gt; bp ntdll.dll!RtlUserThreadStart
[5A70] ntdll.dll!RtlUserThreadStart
&gt; bl
  0 0x00007ffdaed4aa40 (ntdll.dll!RtlUserThreadStart)
[5A70] ntdll.dll!RtlUserThreadStart
&gt; bc 0
[5A70] ntdll.dll!RtlUserThreadStart
&gt; bl
[5A70] ntdll.dll!RtlUserThreadStart
&gt; 
</code></pre><h2 id="applying-breakpoints">Applying breakpoints</h2><p>Finally, we can get to the fun part where we apply the breakpoints to a process. There are two types of breakpoints, software breakpoints and hardware breakpoints. Of the two, hardware breakpoints are less complicated, so we’ll start with those. On x86 processors the hardware breakpoints are controlled via the <a href="https://wiki.osdev.org/CPU_Registers_x86-64#Debug_Registers">“Debug Registers”</a>. There are four hardware breakpoints available on current CPUs. Debug registers DR0 through DR3 are used to specify the address of the breakpoint. Register DR6 is a status register to determine when a breakpoint is hit. And DR7 is a control register to specify the attributes of each hardware breakpoint. Note that there are a number of fields packed together in DR7, so we’ll use a little helper to set these fields.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>// Helper function to set a value at a specific bit offset.
</span></span></span><span><span><span></span><span>fn</span> <span>set_bits</span><span>&lt;</span>T: <span>PrimInt</span><span>&gt;</span>(val: <span>&amp;</span><span>mut</span> T, set_val: <span>T</span>, start_bit: <span>usize</span>, bit_count: <span>usize</span>) {
</span></span><span><span>    <span>// First, mask out the relevant bits
</span></span></span><span><span><span></span>    <span>let</span> max_bits <span>=</span> std::mem::size_of::<span>&lt;</span>T<span>&gt;</span>() <span>*</span> <span>8</span>;
</span></span><span><span>    <span>let</span> mask: <span>T</span> <span>=</span> T::max_value() <span>&lt;&lt;</span> (max_bits <span>-</span> bit_count);
</span></span><span><span>    <span>let</span> mask: <span>T</span> <span>=</span> mask <span>&gt;&gt;</span> (max_bits <span>-</span> <span>1</span> <span>-</span> start_bit);
</span></span><span><span>    <span>let</span> inv_mask <span>=</span> <span>!</span>mask;
</span></span><span><span>
</span></span><span><span>    <span>*</span>val <span>=</span> <span>*</span>val <span>&amp;</span> inv_mask;
</span></span><span><span>    <span>*</span>val <span>=</span> <span>*</span>val <span>|</span> (set_val <span>&lt;&lt;</span> (start_bit <span>+</span> <span>1</span> <span>-</span> bit_count));
</span></span><span><span>}
</span></span></code></pre></div><p>To manipulate these registers, we’ll use the <code>GetThreadContext</code>/<code>SetThreadContext</code> functions to set the registers to the state needed for the requested breakpoints. Note that the debug registers are maintained for each thread separately, so we could theoretically set different breakpoints for each thread, or filter a breakpoint to a specific thread. That functionality won’t be implemented in DbgRs for now, and we’ll just apply the same breakpoints to all threads. To start, we’ll loop over all of the threads in the process and retrieve each thread’s context:</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>pub</span> <span>fn</span> <span>apply_breakpoints</span>(<span>&amp;</span><span>mut</span> self, process: <span>&amp;</span><span>mut</span> Process, resume_thread_id: <span>u32</span>, _memory_source: <span>&amp;</span><span>dyn</span> MemorySource) {
</span></span><span><span>
</span></span><span><span>        <span>for</span> thread_id <span>in</span> process.iterate_threads() {
</span></span><span><span>            <span>let</span> <span>mut</span> ctx: <span>AlignedContext</span> <span>=</span> <span>unsafe</span> { std::mem::zeroed() };
</span></span><span><span>            ctx.context.ContextFlags <span>=</span> <span>CONTEXT_ALL</span>;            
</span></span><span><span>            <span>let</span> thread <span>=</span> AutoClosedHandle(<span>unsafe</span> {
</span></span><span><span>                OpenThread(
</span></span><span><span>                    <span>THREAD_GET_CONTEXT</span> <span>|</span> <span>THREAD_SET_CONTEXT</span>,
</span></span><span><span>                    <span>FALSE</span>,
</span></span><span><span>                    <span>*</span>thread_id,
</span></span><span><span>                )
</span></span><span><span>            });
</span></span><span><span>            <span>let</span> ret <span>=</span> <span>unsafe</span> { GetThreadContext(thread.handle(), <span>&amp;</span><span>mut</span> ctx.context) };
</span></span></code></pre></div><p>We’ll then loop over the requested breakpoints. We need to set four pieces of information for each one. The three fields to set are the LEN (length), RW (access type), and LE (local enable) configuration for each breakpoint. We’ll set the <a aria-describedby="footnote-label" href="#execute-len">LEN to 0</a>, which indicates 1 byte. We’ll set RW to 0, which means “break on instruction execution” (we would use a value of 1 indicate break on read or a value of 3 to indicate break on read or write). Finally, we’ll set the “local enable” bit to 1 indicating that the breakpoint should be enabled.</p><p><img src="https://www.timdbg.com/dr7.png" alt="DR7 layout"></p><div><pre tabindex="0"><code data-lang="rust"><span><span>
</span></span><span><span>    <span>for</span> idx <span>in</span> <span>0</span><span>..</span><span>4</span> {
</span></span><span><span>        <span>if</span> self.breakpoints.len() <span>&gt;</span> idx {
</span></span><span><span>            
</span></span><span><span>            <span>// The DR7_* variables are a set of constants with the correct offsets and sizes for each
</span></span></span><span><span><span></span>            <span>// field of DR7.
</span></span></span><span><span><span></span>            set_bits(<span>&amp;</span><span>mut</span> ctx.context.Dr7, <span>0</span>, <span>DR7_LEN_BIT</span>[idx], <span>DR7_LEN_SIZE</span>);
</span></span><span><span>            set_bits(<span>&amp;</span><span>mut</span> ctx.context.Dr7, <span>0</span>, <span>DR7_RW_BIT</span>[idx], <span>DR7_RW_SIZE</span>);
</span></span><span><span>            set_bits(<span>&amp;</span><span>mut</span> ctx.context.Dr7, <span>1</span>, <span>DR7_LE_BIT</span>[idx], <span>1</span>);
</span></span></code></pre></div><p>The appropriate DR0-DR3 value will be set to the address of the resolved breakpoint.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>
</span></span><span><span>            <span>match</span> idx {
</span></span><span><span>                <span>0</span> <span>=&gt;</span> ctx.context.Dr0 <span>=</span> self.breakpoints[idx].addr,
</span></span><span><span>                <span>1</span> <span>=&gt;</span> ctx.context.Dr1 <span>=</span> self.breakpoints[idx].addr,
</span></span><span><span>                <span>2</span> <span>=&gt;</span> ctx.context.Dr2 <span>=</span> self.breakpoints[idx].addr,
</span></span><span><span>                <span>3</span> <span>=&gt;</span> ctx.context.Dr3 <span>=</span> self.breakpoints[idx].addr,
</span></span><span><span>                _ <span>=&gt;</span> (),
</span></span><span><span>            }
</span></span></code></pre></div><p>Finally, we’ll make sure to disable any breakpoints that we are not using. Note that the code assumes that the debugger “owns” the debug registers and that the target process is not using them in any way. This is typically true, but there are cases where the target process will be using the debug registers for its own purpose, or are manipulated as an anti-debugging technique. To keep things simple, we won’t worry about that and just clear the local enable (LE) bit.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>        } <span>else</span> {
</span></span><span><span>            <span>// Disable any breakpoints that we aren't using.
</span></span></span><span><span><span></span>            set_bits(<span>&amp;</span><span>mut</span> ctx.context.Dr7, <span>0</span>, <span>DR7_LE_BIT</span>[idx], <span>1</span>);
</span></span><span><span>            <span>break</span>;
</span></span><span><span>        }    
</span></span><span><span>    }
</span></span></code></pre></div><p>This new function, <code>apply_breakpoints</code>, will be called from the <code>main_debugger_loop</code> right before we call <code>ContinueDebugEvent</code>. That will ensure that we set up all thread contexts with the correct breakpoint state. Note that because Windows sends a debug event for thread creation, we’ll have an opportunity to set the breakpoint state for all new threads that are created.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    breakpoints.apply_breakpoints(<span>&amp;</span><span>mut</span> process, event_context.thread_id, mem_source.as_ref());
</span></span><span><span>    
</span></span><span><span>    <span>unsafe</span> {
</span></span><span><span>        ContinueDebugEvent(
</span></span><span><span>            event_context.process_id,
</span></span><span><span>            event_context.thread_id,
</span></span><span><span>            continue_status,
</span></span><span><span>        );
</span></span><span><span>    }
</span></span></code></pre></div><h2 id="handling-breakpoint-exceptions">Handling breakpoint exceptions</h2><p>When the CPU tries to execute an instruction that is marked with a debug register, it generates a debug exception (#DB) as a <a href="https://wiki.osdev.org/Exceptions">fault</a> (It’s important to note this is a fault, and not a trap. More on that later). Windows delivers this to a debugger as an exception event with exception code 0x80000004. The thread context will also have a flag set in DR6 indicating which breakpoint was hit. We’ll add some code in the exception event handler letting the breakpoint manager check if a breakpoint was hit.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>        <span>match</span> debug_event {
</span></span><span><span>            DebugEvent::Exception { first_chance, exception_code } <span>=&gt;</span> {
</span></span><span><span>                <span>//...
</span></span></span><span><span><span></span>                <span>if</span> <span>let</span> Some(bp_index) <span>=</span> breakpoints.was_breakpoint_hit(<span>&amp;</span>ctx.context) {
</span></span><span><span>                    println!(<span>"Breakpoint </span><span>{}</span><span> hit"</span>, bp_index);
</span></span><span><span>                    <span>// It's important to use DBG_CONTINUE with ContinueDebugEvent or else the breakpoint will be treated
</span></span></span><span><span><span></span>                    <span>// as an exception to be handled by the target process.
</span></span></span><span><span><span></span>                    continue_status <span>=</span> <span>DBG_CONTINUE</span>;
</span></span><span><span>                }
</span></span><span><span>                <span>//...
</span></span></span></code></pre></div><p>The breakpoint manager will just check DR6 to see if any of the bits were set that correspond to a hardware breakpoint triggering.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>pub</span> <span>fn</span> <span>was_breakpoint_hit</span>(<span>&amp;</span>self, thread_context: <span>&amp;</span><span>CONTEXT</span>) -&gt; Option<span>&lt;</span><span>u32</span><span>&gt;</span> {
</span></span><span><span>        <span>for</span> idx <span>in</span> <span>0</span><span>..</span>self.breakpoints.len() {
</span></span><span><span>            <span>if</span> get_bit(thread_context.Dr6, <span>DR6_B_BIT</span>[idx]) {
</span></span><span><span>                <span>return</span> Some(idx <span>as</span> <span>u32</span>);
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>        None
</span></span><span><span>    }
</span></span></code></pre></div><p>Remember how I mentioned that hardware breakpoints trigger debug exceptions as a <a aria-describedby="footnote-label" href="#debug-fault">fault</a>? That’s important because a “fault” exception triggers <em>before</em> the instruction has a chance to execute. That lets us examine state before the instruction executes, which is what we want for a debugger. But since it is a fault, resuming the execution of the program just causes the program to break in again! On some architectures, this might be complicated to get past, but on x86 we simply have to set the “resume flag”, which is one of the bits in the EFlags registers that often gets overlooked. The resume flag causes the processor to ignore instruction breakpoints for a single instruction execution. It is set back to 0 right after the debug registers would have been checked, which makes it a convenient tool for resuming execution. We’ll set this flag on whatever thread caused the debugger to break in, regardless of whether the breakpoint has hit or not. (Some debuggers will only set RF when a breakpoint was hit). We’ll set the resume flag inside the <code>apply_breakpoints</code> function, since it’s already manipulating the register contexts of all threads.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>if</span> <span>*</span>thread_id <span>==</span> resume_thread_id {
</span></span><span><span>        set_bits(<span>&amp;</span><span>mut</span> ctx.context.EFlags, <span>1</span>, <span>EFLAG_RF</span>, <span>1</span>);
</span></span><span><span>    }
</span></span></code></pre></div><h2 id="testing-it-out">Testing it out</h2><p>Now that we can set a breakpoint, apply a breakpoint, and handle a breakpoint exception, we’re ready to test out the new breakpoint functionality. To do that, we’ll just continue execution until kernelbase is loaded, and then set a breakpoint on kernelbase!GetLastError, which is a very frequently used function that should get called almost immediately.</p><pre tabindex="0"><code>Command line was: '"C:\git\HelloWorld\hello.exe" '
LoadDll: 7FF7E7420000   hello.exe
[5CF8] 0x00007ffdaed4aa40
&gt; g
LoadDll: 7FFDAECF0000   ntdll.dll
[5CF8] ntdll.dll!RtlUserThreadStart
&gt; g
LoadDll: 7FFDAD6E0000   C:\Windows\System32\KERNEL32.DLL
[5CF8] ntdll.dll!NtMapViewOfSection+0x14
&gt; g
LoadDll: 7FFDAC0A0000   C:\Windows\System32\KERNELBASE.dll
[5CF8] ntdll.dll!NtMapViewOfSection+0x14
&gt; bp kernelbase.dll!GetLastError
[5CF8] ntdll.dll!NtMapViewOfSection+0x14
&gt; g
Breakpoint 0 hit
[5CF8] C:\Windows\System32\KERNELBASE.dll!GetLastError
&gt; 
</code></pre><p>It works! It’s almost starting to feel like a real debugger. A few very important things are left though. To start with, we can’t see the functions that are on the call stack. Viewing the stack is probably the single most important analysis feature of a debugger. So that’s likely where we’re going next.</p><p>I hope you found this post interesting and informative! Have a question or suggestion? Let me know! You can find me on <a href="https://twitter.com/timmisiak">Twitter</a>, <a href="https://dbg.social/@tim">Mastodon</a>, and <a href="https://bsky.app/profile/timdbg.com">Bluesky</a>.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Arena allocator tips and tricks (247 pts)]]></title>
            <link>https://nullprogram.com/blog/2023/09/27/</link>
            <guid>37670740</guid>
            <pubDate>Wed, 27 Sep 2023 05:59:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nullprogram.com/blog/2023/09/27/">https://nullprogram.com/blog/2023/09/27/</a>, See on <a href="https://news.ycombinator.com/item?id=37670740">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en">
<article>
  
  <time datetime="2023-09-27">
    September 27, 2023
  </time>
  <p>
    nullprogram.com/blog/2023/09/27/
  </p>

  <p>Over the past year I’ve refined my approach to <a href="https://www.rfleury.com/p/untangling-lifetimes-the-arena-allocator">arena allocation</a>.
With practice, it’s effective, simple, and fast; typically as easy to use
as garbage collection but without the costs. Depending on need, an
allocator can weigh just 7–25 lines of code — perfect when <a href="https://nullprogram.com/blog/2023/02/15/">lacking a
runtime</a>. With the core details of my own technique settled, now is a
good time to document and share lessons learned. This is certainly not the
only way to approach arena allocation, but these are practices I’ve worked
out to simplify programs and reduce mistakes.</p>

<p>An arena is a memory buffer and an offset into that buffer, initially
zero. To allocate an object, grab a pointer at the offset, advance the
offset by the size of the object, and return the pointer. There’s a little
more to it, such as ensuring alignment and availability. We’ll get to
that. Objects are not freed individually. Instead, groups of allocations
are freed at once by restoring the offset to an earlier value. Without
individual lifetimes, you don’t need to write destructors, nor do your
programs need to walk data structures at run time to take them apart. You
also no longer need to worry about memory leaks.</p>

<p>A minority of programs inherently require general purpose allocation, at
least in part, that linear allocation cannot fulfill. This includes, for
example, most programming language runtimes. If you like arenas, avoid
accidentally create such a situation through an over-flexible API that
allows callers to assume you have general purpose allocation underneath.</p>

<p>To get warmed up, here’s my style of arena allocation in action that shows
off multiple features:</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>{</span>
    <span>uint8_t</span>  <span>*</span><span>data</span>
    <span>ptrdiff_t</span> <span>len</span><span>;</span>
<span>}</span> <span>str</span><span>;</span>

<span>typedef</span> <span>struct</span> <span>{</span>
    <span>strlist</span> <span>*</span><span>next</span><span>;</span>
    <span>str</span>      <span>item</span><span>;</span>
<span>}</span> <span>strlist</span><span>;</span>

<span>typedef</span> <span>struct</span> <span>{</span>
    <span>str</span> <span>head</span><span>;</span>
    <span>str</span> <span>tail</span><span>;</span>
<span>}</span> <span>strpair</span><span>;</span>

<span>// Defined elsewhere</span>
<span>void</span>    <span>towidechar</span><span>(</span><span>wchar_t</span> <span>*</span><span>,</span> <span>ptrdiff_t</span><span>,</span> <span>str</span><span>);</span>
<span>str</span>     <span>loadfile</span><span>(</span><span>wchar_t</span> <span>*</span><span>,</span> <span>arena</span> <span>*</span><span>);</span>
<span>strpair</span> <span>cut</span><span>(</span><span>str</span><span>,</span> <span>uint8_t</span><span>);</span>

<span>strlist</span> <span>*</span><span>getlines</span><span>(</span><span>str</span> <span>path</span><span>,</span> <span>arena</span> <span>*</span><span>perm</span><span>,</span> <span>arena</span> <span>scratch</span><span>)</span>
<span>{</span>
    <span>int</span> <span>max_path</span> <span>=</span> <span>1</span><span>&lt;&lt;</span><span>15</span><span>;</span>
    <span>wchar_t</span> <span>*</span><span>wpath</span> <span>=</span> <span>new</span><span>(</span><span>&amp;</span><span>scratch</span><span>,</span> <span>wchar_t</span><span>,</span> <span>max_path</span><span>);</span>
    <span>towidechar</span><span>(</span><span>wpath</span><span>,</span> <span>max_path</span><span>,</span> <span>path</span><span>);</span>

    <span>strpair</span> <span>pair</span> <span>=</span> <span>{</span><span>0</span><span>};</span>
    <span>pair</span><span>.</span><span>tail</span> <span>=</span> <span>loadfile</span><span>(</span><span>wpath</span><span>,</span> <span>perm</span><span>);</span>

    <span>strlist</span> <span>*</span><span>head</span> <span>=</span> <span>0</span><span>;</span>
    <span>strlist</span> <span>**</span><span>tail</span> <span>=</span> <span>&amp;</span><span>head</span><span>;</span>
    <span>while</span> <span>(</span><span>pair</span><span>.</span><span>tail</span><span>.</span><span>len</span><span>)</span> <span>{</span>
        <span>pair</span> <span>=</span> <span>cut</span><span>(</span><span>pair</span><span>.</span><span>tail</span><span>,</span> <span>'\n'</span><span>);</span>
        <span>*</span><span>tail</span> <span>=</span> <span>new</span><span>(</span><span>perm</span><span>,</span> <span>strlist</span><span>,</span> <span>1</span><span>);</span>
        <span>(</span><span>*</span><span>tail</span><span>)</span><span>-&gt;</span><span>item</span> <span>=</span> <span>pair</span><span>.</span><span>head</span><span>;</span>
        <span>tail</span> <span>=</span> <span>&amp;</span><span>(</span><span>*</span><span>tail</span><span>)</span><span>-&gt;</span><span>next</span><span>;</span>
    <span>}</span>
    <span>return</span> <span>head</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Take note of these details, each to be later discussed in detail:</p>

<ul>
  <li>
    <p><code>getlines</code> takes two arenas, “permanent” and “scratch”. The former is
for objects that will be returned to the caller. The latter is for
temporary objects whose lifetime ends when the function returns. They
have stack lifetimes just like local variables.</p>
  </li>
  <li>
    <p>Objects are not explicitly freed. Instead, <strong>all allocations from a
scratch arena are implicitly freed upon return</strong>. This would include
error return paths automatically.</p>
  </li>
  <li>
    <p>The <strong>scratch arena is passed by copy</strong> — i.e. a copy of the “header”
not the <em>memory region</em> itself. Allocating only changes the local copy,
and so cannot survive the return. The semantics are obvious to callers,
so they’re less likely to get mixed up.</p>
  </li>
  <li>
    <p>While <code>wpath</code> could be an automatic local variable, it’s relatively
large for the stack, so it’s allocated out of the scratch arena. A
scratch arena safely permits large, dynamic allocations that would never
be safe on the stack. In other words, <strong>a sane <a href="https://man7.org/linux/man-pages/man3/alloca.3.html"><code>alloca</code></a>!</strong>
Same for variable-length arrays (VLAs). A scratch arena means you’ll
never be tempted to use either of these terrible ideas.</p>
  </li>
  <li>
    <p>The second parameter to <code>new</code> is a type, so it’s obviously a macro. As
you will see momentarily, this is not some complex macro magic, just a
convenience one-liner. There is no implicit cast, and you will get a
compiler diagnostic if the type is incorrect.</p>
  </li>
  <li>
    <p>Despite all the allocation, there is not a single <code>sizeof</code> operator nor
size computation. That’s because <strong>size computations are a major source
of defects.</strong> That job is handled by specialized code.</p>
  </li>
  <li>
    <p><strong>Allocation failures are not communicated by a null return</strong>. Lifting
this burden greatly simplifies programs. Instead such errors are handled
non-locally by the arena.</p>
  </li>
  <li>
    <p>All allocations are <strong>zero-initialized by default</strong>. This makes for
simpler, less error-prone programs. When that’s too expensive, this can
become an opt-out without changing the default.</p>
  </li>
</ul>

<p>See also <a href="https://nullprogram.com/blog/2023/01/18/">u-config</a>.</p>

<h3 id="an-arena-implementation">An arena implementation</h3>

<p>An arena suitable for most cases can be this simple:</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>{</span>
    <span>char</span> <span>*</span><span>beg</span><span>;</span>
    <span>char</span> <span>*</span><span>end</span><span>;</span>
<span>}</span> <span>arena</span><span>;</span>

<span>void</span> <span>*</span><span>alloc</span><span>(</span><span>arena</span> <span>*</span><span>a</span><span>,</span> <span>ptrdiff_t</span> <span>size</span><span>,</span> <span>ptrdiff_t</span> <span>align</span><span>,</span> <span>ptrdiff_t</span> <span>count</span><span>)</span>
<span>{</span>
    <span>ptrdiff_t</span> <span>avail</span> <span>=</span> <span>a</span><span>-&gt;</span><span>end</span> <span>-</span> <span>a</span><span>-&gt;</span><span>beg</span><span>;</span>
    <span>ptrdiff_t</span> <span>padding</span> <span>=</span> <span>-</span><span>(</span><span>uintptr_t</span><span>)</span><span>a</span><span>-&gt;</span><span>beg</span> <span>&amp;</span> <span>(</span><span>align</span> <span>-</span> <span>1</span><span>);</span>
    <span>if</span> <span>(</span><span>count</span> <span>&gt;</span> <span>(</span><span>avail</span> <span>-</span> <span>padding</span><span>)</span><span>/</span><span>size</span><span>)</span> <span>{</span>
        <span>abort</span><span>();</span>  <span>// one possible out-of-memory policy</span>
    <span>}</span>
    <span>ptrdiff_t</span> <span>total</span> <span>=</span> <span>size</span> <span>*</span> <span>count</span><span>;</span>
    <span>char</span> <span>*</span><span>p</span> <span>=</span> <span>a</span><span>-&gt;</span><span>beg</span> <span>+</span> <span>padding</span><span>;</span>
    <span>a</span><span>-&gt;</span><span>beg</span> <span>+=</span> <span>padding</span> <span>+</span> <span>total</span><span>;</span>
    <span>return</span> <span>memset</span><span>(</span><span>p</span><span>,</span> <span>0</span><span>,</span> <span>total</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>Yup, just a pair of pointers! When allocating, all sizes are signed <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1428r0.pdf">just
as they ought to be</a>. Unsigned sizes are another historically
common source of defects, and offer no practical advantages in return.
Case in point exercise for the reader: Change each <code>ptrdiff_t</code> to <code>size_t</code>
in <code>alloc</code>, find the defect that results, then fix it.</p>

<p>The <code>align</code> parameter allows the arena to handle any unusual alignments,
something that’s surprisingly difficult to do with libc. It’s difficult to
appreciate its usefulness until it’s convenient.</p>

<p>The <code>uintptr_t</code> business may look unusual if you’ve never come across it
before. To align <code>beg</code>, we need to compute the number of bytes to advance
the address (<code>padding</code>) until the alignment evenly divides the address.
The modulo with <code>align</code> computes the number of bytes it’s since the last
alignment:</p>



<p>We can’t operate numerically on an address like this, so in the code we
first convert to <code>uintptr_t</code>. Alignment is always a power of two, which
notably excludes zero, so no worrying about division by zero. That also
means we can compute modulo by subtracting one and masking with AND:</p>

<div><pre><code>extra = addr &amp; (align - 1)
</code></pre></div>

<p>However, we want the number of bytes to advance to the next alignment,
which is the inverse:</p>

<div><pre><code>padding = -addr &amp; (align - 1)
</code></pre></div>

<p>Add the <code>uintptr_t</code> cast and you have the code in <code>alloc</code>.</p>

<p>The <code>if</code> tests if there’s enough memory and simultaneously for overflow on
<code>size*count</code>. If either fails, it invokes the out-of-memory policy, which
in this case is <code>abort</code>. I strongly recommend that, at least when testing,
always having <em>something</em> in place to, at minimum, abort when allocation
fails, even when you think it cannot happen. It’s easy to use more memory
than you anticipate, and you want a reliable signal when it happens.</p>

<p>An alternative policy is to <a href="https://nullprogram.com/blog/2023/02/12/">longjmp to a “handler”</a>, which with
GCC and Clang doesn’t even require runtime support. In that case add a
<code>jmp_buf</code> to the arena:</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>{</span>
    <span>char</span>  <span>*</span><span>beg</span><span>;</span>
    <span>char</span>  <span>*</span><span>end</span><span>;</span>
    <span>void</span> <span>**</span><span>jmp_buf</span><span>;</span>
<span>}</span> <span>arena</span><span>;</span>

<span>void</span> <span>*</span><span>alloc</span><span>(...)</span>
<span>{</span>
    <span>// ...</span>
    <span>if</span> <span>(</span><span>count</span> <span>&gt;</span> <span>(</span><span>avail</span> <span>-</span> <span>padding</span><span>)</span><span>/</span><span>size</span><span>)</span> <span>{</span>
        <span>__builtin_longjmp</span><span>(</span><span>a</span><span>-&gt;</span><span>jmp_buf</span><span>,</span> <span>1</span><span>);</span>
    <span>}</span>
    <span>// ...</span>
<span>}</span>

<span>bool</span> <span>example</span><span>(...,</span> <span>arena</span> <span>scratch</span><span>)</span>
<span>{</span>
    <span>void</span> <span>*</span><span>jmp_buf</span><span>[</span><span>5</span><span>];</span>
    <span>if</span> <span>(</span><span>__builtin_setjmp</span><span>(</span><span>jmp_buf</span><span>))</span> <span>{</span>
        <span>return</span> <span>0</span><span>;</span>
    <span>}</span>
    <span>scratch</span><span>.</span><span>jmp_buf</span> <span>=</span> <span>jmp_buf</span><span>;</span>
    <span>// ...</span>
    <span>return</span> <span>1</span><span>;</span>
<span>}</span>
</code></pre></div>

<p><code>example</code> returns failure to the caller if it runs out of memory, without
needing to check individual allocations and, thanks to the implicit free
of scratch arenas, without needing to clean up. If callees receiving the
scratch arena don’t set their own <code>jmp_buf</code>, they’ll return here, too. In
a real program you’d probably wrap the <code>setjmp</code> setup in a macro.</p>

<p>Suppose zeroing is too expensive or unnecessary in some cases. Add a flag
to opt out:</p>

<div><pre><code><span>void</span> <span>*</span><span>alloc</span><span>(...,</span> <span>int</span> <span>flags</span><span>)</span>
<span>{</span>
    <span>// ...</span>
    <span>return</span> <span>flag</span><span>&amp;</span><span>NOZERO</span> <span>?</span> <span>p</span> <span>:</span> <span>memset</span><span>(</span><span>p</span><span>,</span> <span>0</span><span>,</span> <span>total</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>Similarly, perhaps there’s a critical moment where you’re holding a
non-memory resource (lock, file handle), or you don’t want allocation
failure to be fatal. In either case, it important that the out-of-memory
policy isn’t invoked. You could request a “soft” failure with another
flag, and then do the usual null pointer check:</p>

<div><pre><code><span>void</span> <span>*</span><span>alloc</span><span>(...,</span> <span>int</span> <span>flags</span><span>)</span>
<span>{</span>
    <span>// ...</span>
    <span>if</span> <span>(</span><span>count</span> <span>&gt;</span> <span>(</span><span>avail</span> <span>-</span> <span>padding</span><span>)</span><span>/</span><span>size</span><span>)</span> <span>{</span>
        <span>if</span> <span>(</span><span>flags</span> <span>&amp;</span> <span>SOFTFAIL</span><span>)</span> <span>{</span>
            <span>return</span> <span>0</span><span>;</span>
        <span>}</span>
        <span>abort</span><span>();</span>
    <span>}</span>
    <span>// ...</span>
<span>}</span>
</code></pre></div>

<p>Most non-trivial programs will probably at least one of these flags.</p>

<p>In case it wasn’t obvious, allocating an arena is simple:</p>

<div><pre><code><span>arena</span> <span>newarena</span><span>(</span><span>ptrdiff_t</span> <span>cap</span><span>)</span>
<span>{</span>
    <span>arena</span> <span>a</span> <span>=</span> <span>{</span><span>0</span><span>};</span>
    <span>a</span><span>.</span><span>beg</span> <span>=</span> <span>malloc</span><span>(</span><span>cap</span><span>);</span>
    <span>a</span><span>.</span><span>end</span> <span>=</span> <span>a</span><span>.</span><span>beg</span> <span>?</span> <span>a</span><span>.</span><span>beg</span><span>+</span><span>cap</span> <span>:</span> <span>0</span><span>;</span>
    <span>return</span> <span>a</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Or make a direct allocation from the operating system, e.g. <code>mmap</code>,
<code>VirtualAlloc</code>. Typically arena lifetime is the whole program, so you
don’t need to worry about freeing it. (Since you’re using arenas, you can
also turn off any memory leak checkers while you’re at it.)</p>

<p>If you need more arenas then you can always allocate smaller ones out of
the first! In multi-threaded applications, each thread may have at least
its own scratch arena.</p>

<h3 id="the-new-macro">The <code>new</code> macro</h3>

<p>I’ve shown <code>alloc</code>, but few parts of the program should be calling it
directly. Instead they have a macro to automatically handle the details. I
call mine <code>new</code>, though of course if you’re writing C++ you’ll need to
pick another name (<code>make</code>? <code>PushStruct</code>?):</p>

<div><pre><code><span>#define new(a, t, n)  (t *)alloc(a, sizeof(t), _Alignof(t), n)
</span></code></pre></div>

<p>The cast is an extra compile-time check, especially useful for avoiding
mistakes in levels of indirection. It also keeps normal code from directly
using the <code>sizeof</code> operator, which is easy to misuse. If you added a
<code>flags</code> parameter, pass in zero for this common case. Keep in mind that
the goal of this macro is to make common allocation simple and robust.</p>

<p>Often you’ll allocate single objects, and so the count is 1. If you think
that’s ugly, you could make variadic version of <code>new</code> that fills in common
defaults. In fact, that’s partly why I put <code>count</code> last!</p>

<div><pre><code><span>#define new(...)            newx(__VA_ARGS__,new4,new3,new2)(__VA_ARGS__)
#define newx(a,b,c,d,e,...) e
#define new2(a, t)          (t *)alloc(a, sizeof(t), alignof(t), 1, 0)
#define new3(a, t, n)       (t *)alloc(a, sizeof(t), alignof(t), n, 0)
#define new4(a, t, n, f)    (t *)alloc(a, sizeof(t), alignof(t), n, f)
</span></code></pre></div>

<p>Not quite so simple, but it optionally makes for more streamlined code:</p>

<div><pre><code><span>thing</span> <span>*</span><span>t</span>   <span>=</span> <span>new</span><span>(</span><span>perm</span><span>,</span> <span>thing</span><span>);</span>
<span>thing</span> <span>*</span><span>ts</span>  <span>=</span> <span>new</span><span>(</span><span>perm</span><span>,</span> <span>thing</span><span>,</span> <span>1000</span><span>);</span>
<span>char</span>  <span>*</span><span>buf</span> <span>=</span> <span>new</span><span>(</span><span>perm</span><span>,</span> <span>char</span><span>,</span> <span>len</span><span>,</span> <span>NOZERO</span><span>);</span>
</code></pre></div>

<p>Side note: If <code>sizeof</code> should be avoided, what about array lengths? That’s
part of the problem! Hardly ever do you want the <em>size</em> of an array, but
rather the <em>number of elements</em>. That includes <code>char</code> arrays where this
happens to be the same number. So instead, define a <code>countof</code> macro that
uses <code>sizeof</code> to compute the value you actually want. I like to have this
whole collection:</p>

<div><pre><code><span>#define sizeof(x)    (ptrdiff_t)sizeof(x)
#define countof(a)   (sizeof(a) / sizeof(*(a)))
#define lengthof(s)  (countof(s) - 1)
</span></code></pre></div>

<p>Yes, you can convert <code>sizeof</code> into a macro like this! It won’t expand
recursively and bottoms out as an operator. <code>countof</code> also, of course,
produces a less error-prone signed count so users don’t fumble around with
<code>size_t</code>. <code>lengthof</code> statically produces null-terminated string length.</p>

<div><pre><code><span>char</span> <span>msg</span><span>[]</span> <span>=</span> <span>"hello world"</span><span>;</span>
<span>write</span><span>(</span><span>fd</span><span>,</span> <span>msg</span><span>,</span> <span>lengthof</span><span>(</span><span>msg</span><span>));</span>

<span>#define MSG "hello world"
</span><span>write</span><span>(</span><span>fd</span><span>,</span> <span>MSG</span><span>,</span> <span>lengthof</span><span>(</span><span>MSG</span><span>));</span>
</code></pre></div>

<h3 id="enhance-alloc-with-attributes">Enhance <code>alloc</code> with attributes</h3>

<p>At least for GCC and Clang, we can further improve <code>alloc</code> with three
function attributes:</p>

<div><pre><code><span>__attribute</span><span>((</span><span>malloc</span><span>,</span> <span>alloc_size</span><span>(</span><span>2</span><span>,</span> <span>4</span><span>),</span> <span>alloc_align</span><span>(</span><span>3</span><span>)))</span>
<span>void</span> <span>*</span><span>alloc</span><span>(...);</span>
</code></pre></div>

<p><code>malloc</code> indicates that the pointer returned by <code>alloc</code> does not alias any
existing object. Enables some significant optimizations that are otherwise
blocked, most often by breaking potential loop-carried dependencies.</p>

<p><code>alloc_size</code> tracks the allocation size for compile-time diagnostics and
run-time assertions (<a href="https://gcc.gnu.org/onlinedocs/gcc/Object-Size-Checking.html"><code>__builtin_object_size</code></a>). This generally
requires a non-zero optimization level. In other words, you will get a
compiler warnings about some out bounds accesses of arena objects, and
with Undefined Behavior Sanitizer you’ll get run-time bounds checking.
It’s a great <a href="https://nullprogram.com/blog/2019/01/25/">complement to fuzzing</a>.</p>

<p>In theory <code>alloc_align</code> may also allow better code generation, but I’ve
yet to observe a case. Consider it optional and low-priority. I mention it
only for completeness.</p>

<h3 id="arena-size-and-growth">Arena size and growth</h3>

<p>How large an arena should you allocate? The simple answer: As much as is
necessary for the program to successfully complete. Usually the cost of
untouched arena memory is low or even zero. Most programs should probably
have an upper limit, at which point they assume something has gone wrong.
Arenas allow this case to be handled gracefully, simplifying recovery and
paving the way for continued operation.</p>

<p>While a sufficient answer for most cases, it’s unsatisfying. There’s a
common assumption that programs should increase their memory usage as much
as needed and let the operating system respond if it’s too much. However,
if you’ve ever tried this yourself, you probably noticed that mainstream
operating systems don’t handle it well. The typical results are system
instability — thrashing, drivers crashing — possibly necessitating a
reboot.</p>

<p>If you insist on this route, on 64-bit hosts you can reserve a gigantic
virtual address space and gradually commit memory as needed. On Linux that
means leaning on overcommit by allocating the largest arena possible at
startup, which will automatically commit through use. <a href="https://nullprogram.com/blog/2019/12/29/">Use <code>MADV_FREE</code> to
decommit.</a></p>

<p>On Windows, <code>VirtualAlloc</code> handles reserve and commit separately. In
addition to the allocation offset, you need a commit offset. Then expand
the committed region ahead of the allocation offset as it grows. If you
ever manually reset the allocation offset, you could decommit as well, or
at least <code>MEM_RESET</code>. At some point commit may fail, which should then
trigger the out-of-memory policy, but the system is probably in poor shape
by that point — i.e. use an abort policy to release it all quickly.</p>

<h3 id="pointer-laundering-filthy-hack">Pointer laundering (filthy hack)</h3>

<p>While allocations out of an arena don’t require individual error checks,
allocating the arena itself at startup requires error handling. It would
be nice if the arena could be allocated out of <code>.bss</code> and punt that job to
the loader. While you <em>could</em> make a big, global <code>char[]</code> array to back
your arena, it’s technically not permitted (strict aliasing). A “clean”
<code>.bss</code> region could be obtained with a bit of assembly — <a href="https://sourceware.org/binutils/docs/as/Comm.html"><code>.comm</code></a>
plus assembly to get the address into C without involving an array. I
wanted a more portable solution, so I came up with this:</p>

<div><pre><code><span>arena</span> <span>getarena</span><span>(</span><span>void</span><span>)</span>
<span>{</span>
    <span>static</span> <span>char</span> <span>mem</span><span>[</span><span>1</span><span>&lt;&lt;</span><span>28</span><span>];</span>
    <span>arena</span> <span>r</span> <span>=</span> <span>{</span><span>0</span><span>};</span>
    <span>r</span><span>.</span><span>beg</span> <span>=</span> <span>mem</span><span>;</span>
    <span>asm</span> <span>(</span><span>""</span> <span>:</span> <span>"+r"</span><span>(</span><span>r</span><span>.</span><span>beg</span><span>));</span>  <span>// launder the pointer</span>
    <span>r</span><span>.</span><span>end</span> <span>=</span> <span>r</span><span>.</span><span>beg</span> <span>+</span> <span>countof</span><span>(</span><span>mem</span><span>);</span>
    <span>return</span> <span>r</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>The <code>asm</code> accepts a pointer and returns a pointer (<code>"+r"</code>). The compiler
cannot “see” that it’s actually empty, and so returns the same pointer.
The arena will be backed by <code>mem</code>, but by laundering the address through
<code>asm</code>, I’ve disconnected the pointer from its origin. As far the compiler
is concerned, this is some foreign, assembly-provided pointer, not a
pointer into <code>mem</code>. It can’t optimize away <code>mem</code> because it’s been given
to a mysterious assembly black box.</p>

<p>While inappropriate for a real project, I think it’s a neat trick.</p>

<h3 id="arena-friendly-container-data-structures">Arena-friendly container data structures</h3>

<p>In my initial example I used a linked list to stores lines. This data
structure is great with arenas. It only takes a few of lines of code to
implement a linked list on top of an arena, and no “destroy” code is
needed. Simple.</p>

<p>What about <a href="https://nrk.neocities.org/articles/hash-trees-and-tries">arena-backed associative arrays</a>? Or arena-backed
dynamic arrays? I have simple, fast, easy solutions for each, but that’s
the subject for my next article!</p>



  
  <ol></ol>

  

  <nav>
  
    
  
  
  </nav>
</article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: XRain – Explore rainfall statistics around the world (106 pts)]]></title>
            <link>https://xrain.info/data/</link>
            <guid>37669706</guid>
            <pubDate>Wed, 27 Sep 2023 03:36:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xrain.info/data/">https://xrain.info/data/</a>, See on <a href="https://news.ycombinator.com/item?id=37669706">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[China is flooding Taiwan with disinformation (194 pts)]]></title>
            <link>https://www.economist.com/asia/2023/09/26/china-is-flooding-taiwan-with-disinformation</link>
            <guid>37667874</guid>
            <pubDate>Wed, 27 Sep 2023 00:18:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/asia/2023/09/26/china-is-flooding-taiwan-with-disinformation">https://www.economist.com/asia/2023/09/26/china-is-flooding-taiwan-with-disinformation</a>, See on <a href="https://news.ycombinator.com/item?id=37667874">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p><span><a href="https://www.economist.com/asia/" data-analytics="sidebar:section"><span>Asia</span></a></span><span> | <!-- -->Strait up lies</span></p><h2>With elections looming, China wants Taiwanese voters to think America is their greatest threat</h2></section><div data-body-id="cp2"><p data-component="paragraph"><span data-caps="initial">I</span><small>n July one</small> of Taiwan’s top newspapers, <i>United Daily News</i>, published a story based on supposedly leaked minutes from a secret government meeting. America had asked <a href="https://www.economist.com/china/2023/06/19/when-it-comes-to-a-war-with-taiwan-many-chinese-urge-caution">Taiwan</a> to manufacture biological weapons at a lab run by the island’s defence ministry, the report claimed. Taiwanese and American officials quickly denied it. The allegedly leaked minutes, it transpired, were not written in the usual style of Taiwanese government records. They were filled with official-sounding phrases used in mainland China, but not Taiwan. This was likely Chinese disinformation, Taiwanese officials said. Yet the story spread to Taiwanese talk shows and influencers. Within weeks it had evolved into a wilder claim: Taiwan was going to collect 150,000 samples of Taiwanese blood and hand them over to the Americans, so that they could develop a virus to kill Chinese people.</p><p data-component="paragraph">This sort of disinformation is so widespread in Taiwan that analysts have given it a moniker: <i>yi mei lun</i>, or the “<small>US</small> scepticism” narrative. Its spread is becoming a major worry for Taiwan’s government and civil society in the run-up to a hugely important presidential election next January. Taiwanese voters will in effect be asked to decide whether Taiwan should remain aligned with America in strengthening deterrence against a possible Chinese invasion, or should move towards building ties with China. The opposition Kuomintang has called the vote a choice between “war and peace”, implying that the ruling Democratic Progressive Party’s hostility towards China will provoke it to attack. Chinese state actors have backed that framing, spreading narratives that portray <a href="https://www.economist.com/briefing/2023/03/09/america-and-china-are-preparing-for-a-war-over-taiwan">America</a>, not China, as the island’s biggest threat.  Much of the disinformation is intended to reinforce that false message.</p><p data-component="paragraph">Lo Ping-chen, a cabinet minister who since 2018 has been leading a government task force against disinformation, says it has “severely infiltrated” Taiwan’s society. “We used to think there was more during election season. But it’s now become normalised. It happens every day.” Most Taiwanese voters have little idea of this. A recent survey by Doublethink Lab, a Taiwanese group that studies disinformation, found that less than 20% of respondents believed the false information spread in Taiwan during elections came from abroad. Puma Shen, who heads Doublethink Lab, worries about the one-fifth of voters who are not aligned with any party and could be a decisive bloc. “Even if only 15% of voters are truly affected by Chinese disinformation, it takes only 7% of voters to change the election results,” he says.</p><p data-component="paragraph">A recent study of <small>US</small>-scepticism narratives by the Information Environment Research Centre (<small>IORG</small>), a Taiwanese research group, found that Chinese actors were helping to spread most of them. But more than half appeared to have Taiwanese origins. That suggests China is “piggybacking” on fissures in Taiwanese society, says Chihhao Yu, the report’s author. He suggests many Taiwanese have an “orphan mentality”: they fear abandonment by outsiders because of Taiwan’s experience of losing American diplomatic recognition in the 1970s.</p><p data-component="paragraph">Chinese actors are exploiting those fears, just as Russian disinformation exploited America’s racial and cultural cracks for the benefit of Donald Trump in 2016. Chinese disinformation in Taiwan also echoes Russian propaganda about the war in Ukraine, which claims America is behind the conflict (and is creating bioweapons in Ukrainian labs).</p><p data-component="paragraph">China has developed systematic means to make falsehoods trend in Taiwan, says Chien Yu-yen, a former journalist and author of a book about Chinese influence on Taiwan’s media. She points to a spurious claim that America “wants to blow up” <small>TSMC, </small>a Taiwanese chipmaker. It originated with a misleading video posted on Douyin, the Chinese version of TikTok, which featured an American lawmaker appearing to discuss the possibility. The following morning, a Taiwanese newspaper published a story about the video. Opposition lawmakers and talk-show hosts whipped up outrage. “The journey from China’s Douyin to Taiwan’s mass media, videos, newspapers and television took less than half a day,” says Ms Chien. Chinese state media amplified the narrative, as if merely commenting from the outside on a Taiwanese debate.</p><p data-component="paragraph">Taiwanese officials believe that many of the Taiwanese launching<small> US-</small>scepticism untruths are “local collaborators” taking orders and payments from China. But that is hard to prove, because the suspected Chinese funding is probably funnelled through Taiwanese businesspeople or public-relations firms. Wang Kun-yi, a local commentator who frequently writes <small>US</small>-scepticism narratives for Chinese media and pro-China Taiwanese media, defends his work as a commercial enterprise. All journalists in Taiwan serve the bias of their newspapers’ bosses, says Mr Wang, who has worked for both pro-independence and pro-unification newspapers. “Everyone just treats it as a job,” he says. “It’s a tool to feed yourself.”</p><p data-component="paragraph">Taiwan has laws against foreign infiltration and election influence, but they are limited to cases of proven state-sponsored activity. It has additional laws against spreading wilful falsehood in broadcast media, but they do not cover print or digital outlets. In 2020 the government revoked the licence of <small>CTI </small>News, a pro-China channel, citing repeated failures to verify information. <small>CTI</small> simply moved online.</p><p data-component="paragraph">The case sparked accusations of censorship, which Taiwan wants to avoid. So the government has resorted to more liberal methods of fighting disinformation. It has tried to improve media literacy, provide faster official clarifications and promote fact-checking organisations. But such means cannot match the speed of Chinese propaganda. In August Meta removed a network of more than 7,000 accounts, pages and groups that were spreading Chinese disinformation. But new accounts are easy to set up, a problem that will only accelerate with artificial intelligence, says Mr Lo.</p><p data-component="paragraph">Chinese disinformation has already distorted Taiwan’s public conversation. Will it move votes? Meta has noted that the Chinese disinformation network it removed was “high volume, low reach”, despite having a veneer of engagement designed to make the accounts look more popular than they were. Studies of Russian disinformation in America have found that it has little impact on voter preferences. Despite all the messaging in Chinese and Taiwanese media against the Democratic Progressive Party, its candidate, William Lai, is leading in the polls. And for all the scepticism about America, Taiwanese are even warier of China. A 2022 survey by Academia Sinica, a Taiwanese research institution, found 34% of respondents agreeing that America is a “credible” country. Only 9% said the same of China.</p><p data-component="paragraph">China has itself to blame. It recently surrounded Taiwan with warplanes and warships, even as its ruling Communist Party unveiled an integration plan promising benefits to Taiwanese people living in Fujian, a southern province near the island. Most Taiwanese know where their real threat comes from. But China’s insidious efforts to mislead them are increasing.<span>■</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deconstructing Go Type Parameters (132 pts)]]></title>
            <link>https://go.dev/blog/deconstructing-type-parameters</link>
            <guid>37667731</guid>
            <pubDate>Wed, 27 Sep 2023 00:05:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://go.dev/blog/deconstructing-type-parameters">https://go.dev/blog/deconstructing-type-parameters</a>, See on <a href="https://news.ycombinator.com/item?id=37667731">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-slug="/blog/deconstructing-type-parameters">
    
    <h2><a href="https://go.dev/blog/">The Go Blog</a></h2>
    

    
      
      
      
      <h2 id="slices-package-function-signatures">slices package function signatures</h2>
<p>The <a href="https://pkg.go.dev/slices#Clone" rel="noreferrer" target="_blank"><code>slices.Clone</code></a> function is
pretty simple: it makes a copy of a slice of any type.</p>
<pre><code>func Clone[S ~[]E, E any](s S) S {
    return append(s[:0:0], s...)
}
</code></pre>
<p>This works because appending to a slice with zero capacity will
allocate a new backing array.
The function body winds up being shorter than the function signature,
which is in part because the body is short, but also because the
signature is long.
In this blog post we’ll explain why the signature is written the way
that it is.</p>
<h2 id="simple-clone">Simple Clone</h2>
<p>We’ll start by writing a simple generic <code>Clone</code> function.
This is not the one in the <code>slices</code> package.
We want to take a slice of any element type, and return a new slice.</p>
<pre><code>func Clone1[E any](s []E) []E {
    // body omitted
}
</code></pre>
<p>The generic function <code>Clone1</code> has a single type parameter <code>E</code>.
It takes a single argument <code>s</code> which is a slice of type <code>E</code>, and it
returns a slice of the same type.
This signature is straightforward for anybody familiar with generics
in Go.</p>
<p>However, there is a problem.
Named slice types are not common in Go, but people do use them.</p>
<pre><code>// MySlice is a slice of strings with a special String method.
type MySlice []string

// String returns the printable version of a MySlice value.
func (s MySlice) String() string {
    return strings.Join(s, "+")
}
</code></pre>
<p>Let’s say that we want to make a copy of a <code>MySlice</code> and then get the
printable version, but with the strings in sorted order.</p>
<pre><code>func PrintSorted(ms MySlice) string {
    c := Clone1(ms)
    slices.Sort(c)
    return c.String() // FAILS TO COMPILE
}
</code></pre>
<p>Unfortunately, this doesn’t work.
The compiler reports an error:</p>
<pre><code>c.String undefined (type []string has no field or method String)
</code></pre>
<p>We can see the problem if we manually instantiate <code>Clone1</code> by
replacing the type parameter with the type argument.</p>
<pre><code>func InstantiatedClone1(s []string) []string
</code></pre>
<p>The <a href="https://go.dev/ref/spec#Assignability" rel="noreferrer" target="_blank">Go assignment rules</a> allow
us to pass a value of type <code>MySlice</code> to a parameter of type
<code>[]string</code>, so calling <code>Clone1</code> is fine.
But <code>Clone1</code> will return a value of type <code>[]string</code>, not a value of
type <code>MySlce</code>.
The type <code>[]string</code> doesn’t have a <code>String</code> method, so the compiler
reports an error.</p>
<h2 id="flexible-clone">Flexible Clone</h2>
<p>To fix this problem, we have to write a version of <code>Clone</code> that
returns the same type as its argument.
If we can do that, then when we call <code>Clone</code> with a value of type
<code>MySlice</code>, it will return a result of type <code>MySlice</code>.</p>
<p>We know that it has to look something like this.</p>
<pre><code>func Clone2[S ?](s S) S // INVALID
</code></pre>
<p>This <code>Clone2</code> function returns a value that is the same type as its
argument.</p>
<p>Here I’ve written the constraint as <code>?</code>, but that’s just a
placeholder.
To make this work we need to write a constraint that will let us write
the body of the function.
For <code>Clone1</code> we could just use a constraint of <code>any</code> for the element
type.
For <code>Clone2</code> that won’t work: we want to require that <code>s</code> be a slice
type.</p>
<p>Since we know we want a slice, the constraint of <code>S</code> has to be a
slice.
We don’t care what the slice element type is, so let’s just call it
<code>E</code>, as we did with <code>Clone1</code>.</p>
<pre><code>func Clone3[S []E](s S) S // INVALID
</code></pre>
<p>This is still invalid, because we haven’t declared <code>E</code>.
The type argument for <code>E</code> can be any type, which means it also has to
be a type parameter itself.
Since it can be any type, its constraint is <code>any</code>.</p>
<pre><code>func Clone4[S []E, E any](s S) S
</code></pre>
<p>This is getting close, and at least it will compile, but we’re not
quite there yet.
If we compile this version, we get an error when we call <code>Clone4(ms)</code>.</p>
<pre><code>MySlice does not satisfy []string (possibly missing ~ for []string in []string)
</code></pre>
<p>The compiler is telling us that we can’t use the type argument
<code>MySlice</code> for the type parameter <code>S</code>, because <code>MySlice</code> does not
satisfy the constraint <code>[]E</code>.
That’s because <code>[]E</code> as a constraint only permits a slice type
literal, like <code>[]string</code>.
It doesn’t permit a named type like <code>MySlice</code>.</p>
<h2 id="underlying-type-constraints">Underlying type constraints</h2>
<p>As the error message hints, the answer is to add a <code>~</code>.</p>
<pre><code>func Clone5[S ~[]E, E any](s S) S
</code></pre>
<p>To repeat, writing type parameters and constraints <code>[S []E, E any]</code>
means that the type argument for <code>S</code> can be any unnamed slice type,
but it can’t be a named type defined as a slice literal.
Writing <code>[S ~[]E, E any]</code>, with a <code>~</code>, means that the type argument
for <code>S</code> can be any type whose underlying type is a slice type.</p>
<p>For any named type <code>type T1 T2</code> the underlying type of <code>T1</code> is the
underlying type of <code>T2</code>.
The underlying type of a predeclared type like <code>int</code> or a type literal
like <code>[]string</code> is just the type itself.
For the exact details, <a href="https://go.dev/ref/spec#Underlying_types" rel="noreferrer" target="_blank">see the language
spec</a>.
In our example, the underlying type of <code>MySlice</code> is <code>[]string</code>.</p>
<p>Since the underlying type of <code>MySlice</code> is a slice, we can pass an
argument of type <code>MySlice</code> to <code>Clone5</code>.
As you may have noticed, the signature of <code>Clone5</code> is the same as the
signature of <code>slices.Clone</code>.
We’ve finally gotten to where we want to be.</p>
<p>Before we move on, let’s discuss why the Go syntax requires a <code>~</code>.
It might seem that we would always want to permit passing <code>MySlice</code>,
so why not make that the default?
Or, if we need to support exact matching, why not flip things around,
so that a constraint of <code>[]E</code> permits a named type while a constraint
of, say, <code>=[]E</code> only permits slice type literals?</p>
<p>To explain this, let’s first observe that a type parameter list like
<code>[T ~MySlice]</code> doesn’t make sense.
That’s because <code>MySlice</code> is not the underlying type of any other type.
For instance, if we have a definition like <code>type MySlice2 MySlice</code>,
the underlying type of <code>MySlice2</code> is <code>[]string</code>, not <code>MySlice</code>.
So either <code>[T ~MySlice]</code> would permit no types at all, or it would be
the same as <code>[T MySlice]</code> and only match <code>MySlice</code>.
Either way, <code>[T ~MySlice]</code> isn’t useful.
To avoid this confusion, the language prohibits <code>[T ~MySlice]</code>, and
the compiler produces an error like</p>
<pre><code>invalid use of ~ (underlying type of MySlice is []string)
</code></pre>
<p>If Go didn’t require the tilde, so that <code>[S []E]</code> would match any type
whose underlying type is <code>[]E</code>, then we would have to define the
meaning of <code>[S MySlice]</code>.</p>
<p>We could prohibit <code>[S MySlice]</code>, or we could say that <code>[S MySlice]</code>
only matches <code>MySlice</code>, but either approach runs into trouble with
predeclared types.
A predeclared type, like <code>int</code> is its own underlying type.
We want to permit people to be able to write constraints that accept
any type argument whose underlying type is <code>int</code>.
In the language today, they can do that by writing <code>[T ~int]</code>.
If we don’t require the tilde we would still need a way to say “any
type whose underlying type is <code>int</code>”.
The natural way to say that would be <code>[T int]</code>.
That would mean that <code>[T MySlice]</code> and <code>[T int]</code> would behave
differently, although they look very similar.</p>
<p>We could perhaps say that <code>[S MySlice]</code> matches any type whose
underlying type is the underlying type of <code>MySlice</code>, but that makes
<code>[S MySlice]</code> unnecessary and confusing.</p>
<p>We think it’s better to require the <code>~</code> and be very clear about when
we are matching the underlying type rather than the type itself.</p>
<h2 id="type-inference">Type inference</h2>
<p>Now that we’ve explained the signature of <code>slices.Clone</code>, let’s see
how actually using <code>slices.Clone</code> is simplified by type inference.
Remember, the signature of <code>Clone</code> is</p>
<pre><code>func Clone[S ~[]E, E any](s S) S
</code></pre>
<p>A call of <code>slices.Clone</code> will pass a slice to the parameter <code>s</code>.
Simple type inference will let the compiler infer that the type
argument for the type parameter <code>S</code> is the type of the slice being
passed to <code>Clone</code>.
Type inference is then powerful enough to see that the type argument
for <code>E</code> is the element type of the type argument passed to <code>S</code>.</p>
<p>This means that we can write</p>
<pre><code>    c := Clone(ms)
</code></pre>
<p>without having to write</p>
<pre><code>    c := Clone[MySlice, string](ms)
</code></pre>
<p>If we refer to <code>Clone</code> without calling it, we do have to specify a
type argument for <code>S</code>, as the compiler has nothing it can use to infer
it.
Fortunately, in that case, type inference is able to infer the type
argument for <code>E</code> from the argument for <code>S</code>, and we don’t have to
specify it separately.</p>
<p>That is, we can write</p>
<pre><code>    myClone := Clone[MySlice]
</code></pre>
<p>without having to write</p>
<pre><code>    myClone := Clone[MySlice, string]
</code></pre>
<h2 id="deconstructing-type-parameters">Deconstructing type parameters</h2>
<p>The general technique we’ve used here, in which we define one type
parameter <code>S</code> using another type parameter <code>E</code>, is a way to
deconstruct types in generic function signatures.
By deconstructing a type, we can name, and constrain, all aspects of
the type.</p>
<p>For example, here is the signature for <code>maps.Clone</code>.</p>
<pre><code>func Clone[M ~map[K]V, K comparable, V any](m M) M
</code></pre>
<p>Just as with <code>slices.Clone</code>, we use a type parameter for the type of
the parameter <code>m</code>, and then deconstruct the type using two other type
parameters <code>K</code> and <code>V</code>.</p>
<p>In <code>maps.Clone</code> we constrain <code>K</code> to be comparable, as is required for
a map key type.
We can constrain the component types any way we like.</p>
<pre><code>func WithStrings[S ~[]E, E interface { String() string }](s S) (S, []string)
</code></pre>
<p>This says that the argument of <code>WithStrings</code> must be a slice type for
which the element type has a <code>String</code> method.</p>
<p>Since all Go types can be built up from component types, we can always
use type parameters to deconstruct those types and constrain them as
we like.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Radar Maps Platform (148 pts)]]></title>
            <link>https://radar.com/blog/introducing-radar-maps-platform</link>
            <guid>37667450</guid>
            <pubDate>Tue, 26 Sep 2023 23:38:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://radar.com/blog/introducing-radar-maps-platform">https://radar.com/blog/introducing-radar-maps-platform</a>, See on <a href="https://news.ycombinator.com/item?id=37667450">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next" data-reactroot=""><div><div><ul><li><a href="https://radar.com/product/geofencing">Product</a></li><li><a href="https://radar.com/solutions/retail">Solutions</a></li><li><a href="https://radar.com/documentation">Docs</a></li><li><a href="https://radar.com/pricing">Pricing</a></li><li><a href="https://radar.com/about">Company</a></li></ul><div><p><a href="https://radar.com/product/geofencing"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.132 2a3.648 3.648 0 0 1 3.473 3.641v.156c0 2.152-2.911 5.255-3.216 5.572a.649.649 0 0 1-.425.191.649.649 0 0 1-.424-.19c-.3-.318-3.217-3.42-3.217-5.573V5.64A3.643 3.643 0 0 1 7.797 2h.335ZM6.589 5.64a1.369 1.369 0 1 0 2.738 0 1.369 1.369 0 0 0-2.738 0Zm4.496 4.323a5.5 5.5 0 0 0 .227-.335l.006-.006c.903.455 1.447 1.106 1.447 1.848C12.765 12.91 10.715 14 8 14c-2.714 0-4.765-1.088-4.765-2.53 0-.729.526-1.368 1.399-1.823.066.102.131.204.21.311.166.24.334.46.501.676-.586.263-.92.58-.92.837 0 .532 1.423 1.333 3.569 1.333s3.57-.801 3.57-1.333c0-.263-.353-.592-.963-.855l.058-.076c.139-.181.285-.372.426-.576Z" fill="currentColor"></path></svg>Geofences<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.99 3.23a.75.75 0 0 0 0 1.06L10.698 8 6.99 11.709a.75.75 0 1 0 1.06 1.06l4.198-4.197a.81.81 0 0 0 0-1.145L8.05 3.23a.75.75 0 0 0-1.06 0Z" fill="currentColor"></path></svg><span href="/product/geofencing">Industry-leading accuracy with unlimited geofences, polygon geofences, and more</span></a></p><p><a href="https://radar.com/product/trip-tracking"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.2 2H2.8c-.44 0-.8.365-.8.812v11.376c0 .447.36.812.8.812h.167c.44 0 .8-.365.8-.812v-2.436H13.2c.44 0 .8-.365.8-.812V2.812A.808.808 0 0 0 13.2 2Zm-.967 4.872H9.411v2.866H6.59V6.872H3.767V4.006h2.822v2.866H9.41V4.006h2.822v2.866Z" fill="currentColor"></path></svg>Trips<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.99 3.23a.75.75 0 0 0 0 1.06L10.698 8 6.99 11.709a.75.75 0 1 0 1.06 1.06l4.198-4.197a.81.81 0 0 0 0-1.145L8.05 3.23a.75.75 0 0 0-1.06 0Z" fill="currentColor"></path></svg><span href="/product/trip-tracking">Trip tracking, live ETAs, arrival detection, and routing for pickups and deliveries</span></a></p><p><a href="https://radar.com/product/places"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.425 2.375A.788.788 0 0 0 11.75 2h-7.5c-.3 0-.525.15-.675.375C2 5.375 2 5.525 2 5.75c0 .825.675 1.5 1.5 1.5v6c0 .45.3.75.75.75h7.5c.45 0 .75-.3.75-.75v-6c.825 0 1.5-.675 1.5-1.5 0-.225 0-.375-1.575-3.375ZM9.5 12.5v-3h-3v3H5V7.025c.225.15.45.225.75.225.45 0 .825-.225 1.125-.525.3.3.675.525 1.125.525.45 0 .825-.225 1.125-.525.3.3.675.525 1.125.525.3 0 .525-.075.75-.225V12.5H9.5Z" fill="currentColor"></path></svg>Places<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.99 3.23a.75.75 0 0 0 0 1.06L10.698 8 6.99 11.709a.75.75 0 1 0 1.06 1.06l4.198-4.197a.81.81 0 0 0 0-1.145L8.05 3.23a.75.75 0 0 0-1.06 0Z" fill="currentColor"></path></svg><span href="/product/places">Points-of-interest (POI) dataset with chains and categories to detect visits to millions of places</span></a></p><p><a href="https://radar.com/product/api"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.57 6.57A4.578 4.578 0 0 0 8.21 2h-.42a4.573 4.573 0 0 0-4.36 4.57v.195c0 2.702 3.662 6.597 4.037 6.995.15.157.383.24.533.24.15 0 .383-.083.533-.24.383-.398 4.037-4.293 4.037-6.995V6.57ZM7.993 8.29a1.718 1.718 0 1 1 0-3.437c.953 0 1.718.765 1.718 1.718S8.946 8.29 7.993 8.29Z" fill="currentColor"></path></svg>Maps Platform<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.99 3.23a.75.75 0 0 0 0 1.06L10.698 8 6.99 11.709a.75.75 0 1 0 1.06 1.06l4.198-4.197a.81.81 0 0 0 0-1.145L8.05 3.23a.75.75 0 0 0-1.06 0Z" fill="currentColor"></path></svg><span href="/product/api">The cost-effective, all-in-one Google Maps alternative, with geocoding, search, routing, and maps</span></a></p></div></div><div><ul><li><a href="https://radar.com/login">Log in</a></li><li><a href="https://radar.com/contact">Get a demo</a></li></ul></div></div><div><header><nav><ul><li><a href="https://radar.com/blog">Blog Home</a></li><li><a href="https://radar.com/blog/categories/company">Company</a></li><li><a href="https://radar.com/blog/categories/product">Product</a></li><li><a href="https://radar.com/blog/categories/engineering">Engineering</a></li><li><a href="https://radar.com/blog/categories/industry">Industry</a></li><li><a href="https://radar.com/blog/categories/guides">Guides</a></li></ul></nav></header></div><div><h2>It’s time to build</h2><p>See what Radar’s location and geofencing <br>solutions can do for your business.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Philips Hue ecosystem is collapsing into stupidity (883 pts)]]></title>
            <link>https://rachelbythebay.com/w/2023/09/26/hue/</link>
            <guid>37667266</guid>
            <pubDate>Tue, 26 Sep 2023 23:22:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rachelbythebay.com/w/2023/09/26/hue/">https://rachelbythebay.com/w/2023/09/26/hue/</a>, See on <a href="https://news.ycombinator.com/item?id=37667266">Hacker News</a></p>
Couldn't get https://rachelbythebay.com/w/2023/09/26/hue/: Error: Request failed with status code 403]]></description>
        </item>
    </channel>
</rss>