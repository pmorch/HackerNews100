<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 08 Jan 2025 02:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Laid off for the first time in my career, and twice in one year (124 pts)]]></title>
            <link>https://dillonshook.com/laid-off/</link>
            <guid>42627567</guid>
            <pubDate>Tue, 07 Jan 2025 21:14:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dillonshook.com/laid-off/">https://dillonshook.com/laid-off/</a>, See on <a href="https://news.ycombinator.com/item?id=42627567">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pagefind-body=""><p>So, I got laid off again… This time it was the week before Christmas, last time it was a week into paternity leave. Both great timing, I know. Last time I didn’t feel like writing about it since it was a swirl of emotions with a new baby in the house and the first time I had been laid off in my career, 13 years in at that point. But this time I’m ready to get it out in a post that’s cathartic for me to write and something you might relate to. If you don’t relate now, I hope it stays that way. Hopefully this post will give you a bit of perspective if you ever do unexpectedly lose your job.</p><h2 id="the-indigo-story">The Indigo Story<a href="#the-indigo-story"><span data-pagefind-ignore="">#</span></a></h2><p>For both my stories you’re not going to find any shit-talking here for a number of reasons. First, that’s not the kind of person I am. Second, I feel fairly treated given both companies circumstances. And third, I want to keep good relationships with the people both that were laid off at the same time and the people still at both companies. Keeping good connections with your coworkers and not burning bridges is one of the most important things I think you can do in your career.</p><p>So with that out of the way, let’s talk about the Indigo story a bit.</p><p>One week into paternity leave delirious with sleep deprivation and all the emotions of now caring and loving for such a tiny helpless little baby I remember getting a text from a coworker saying I should probably check my email. A bunch of thoughts and feels rushed in seemingly all at once. What’s going on? Am I getting laid off? I knew there was a chance since the company had multiple rounds of layoffs before but it wasn’t really happening to me on leave right? What was going to happen to our health insurance? Did I do something wrong? How many more people were getting laid off? It felt like ages till the HR call was scheduled and I found out some of the details.</p><p>A wave of relief took over after hearing I would get paid out for my paternity leave with severance on top so I could take a deep breath, relax, and enjoy being with my wife and daughter for her first few weeks on the planet we call Earth.</p><p>After a week or two when it felt like we were starting to form something resembling a routine it was time to crack back open the resume. This always feels daunting after being at a company for awhile. How do you succinctly summarize and highlight all you’ve done there? Luckily I had some notes from my own self review one year to help but I also made a mental note to myself to start writing down accomplishments as they happen to make this easier next time.</p><p>Once I was happy with the updates I got right into the most difficult job search process I’d experienced since trying to find my first software engineering job out of college. I was being selective and only applying for places I’d realistically want to work for and also screening out companies that didn’t seem to have a viable business model which narrowed the field by more than half.</p><p>Ultimately it’s a pipeline numbers game though so I kept applying, kept interviewing, and kept persisting until I found a match. My funnel ended up looking like this</p><p><img alt="Job Hunt" src="https://dillonshook.com/_astro/jobhunt.CgaTHyCX_oANgU.webp" decoding="async" height="597" loading="lazy" width="999"></p><p>It’s easy to get <a href="https://news.ycombinator.com/item?id=39615097&amp;ref=dillonshook.com" target="_blank" rel="noopener">disheartened by the process</a> when you’re in the middle of it and you get through so many interviews with a company you really want to work for only to be told a lame reason why you’re not getting an offer at the end. I really wish companies would start giving honest feedback even if it’s hard for the candidates to heard at first. It would be a much better way for candidates to improve themselves and we’re all adults here and can take the feedback.</p><h2 id="the-pryon-story">The Pryon Story<a href="#the-pryon-story"><span data-pagefind-ignore="">#</span></a></h2><p>The Pryon story is much shorter just like my employment was. I don’t feel like going into any of the details since they don’t feel that important now. It was an 85% surprise to me that it happened at all, and 100% surprise for how deep they cut without many signs the company was struggling. I can only speculate what the reasons were, but someone screwed up to hire dozens of people just to lay them off 6 months later. The shock and whirlwind of thoughts and feelings came and went much quicker this time around. Going through the process once before builds your resilience and you take it less personally.</p><h2 id="how-to-tell-a-layoff-is-coming">How to Tell a Layoff is Coming<a href="#how-to-tell-a-layoff-is-coming"><span data-pagefind-ignore="">#</span></a></h2><p>Based on my N=2 experience and the posts I’ve read here are some warning signs to look out for:</p><ul><li>The product org is struggling to set a vision</li><li>Many distractions and lack of focus</li><li>Low products usage</li><li>Perks being cut, cancelled onsite meetings, signs of financial troubles</li></ul><p>And last but certainly most telling:</p><ul><li>There’s a “company update” meeting unexpectedly scheduled four hours from now</li></ul><h2 id="what-to-do-if-youve-been-laid-off">What to Do if You’ve Been Laid Off<a href="#what-to-do-if-youve-been-laid-off"><span data-pagefind-ignore="">#</span></a></h2><p>First, take a deep breath, this is going to be stressful. But it’s also a new opportunity to find an even better job, meet new amazing people, and expand your skill set.</p><h3 id="update-and-improve-your-resume">Update and Improve your Resume<a href="#update-and-improve-your-resume"><span data-pagefind-ignore="">#</span></a></h3><p>Depending on how long you’ve been at your last job you might have a lot or a little to add. Regardless though, you should try and improve the design a little bit each time you update it. Your resume is highly personal and should reflect some of your personality while still looking as professional as possible, so I use any templates or resume builders myself. Just 100% artisanal HTML and CSS for me baby. I want it to stand out from the hundreds and hundreds of other resumes out there in the application pool.</p><p>Try some new <a href="https://fontjoy.com/#?ref=dillonshook.com" target="_blank" rel="noopener">font pairings</a>. Add a touch of iconography. Improve the spacing. Refine, refine, refine.</p><h3 id="file-for-unemployement-benefits">File for Unemployement Benefits<a href="#file-for-unemployement-benefits"><span data-pagefind-ignore="">#</span></a></h3><p>Don’t feel any stigma for filing for unemployment after you’ve been laid off. Your pay your taxes into the system and deserve to get money back when you’re unemployed. I didn’t file after the first time since I had a long period of severance and didn’t want to deal with the overhead but I’ve already filed this time.</p><h3 id="start-searching-for-jobs">Start Searching for Jobs<a href="#start-searching-for-jobs"><span data-pagefind-ignore="">#</span></a></h3><p>The best places I’ve found to look in the past are:</p><ul><li>Stack Overflow jobs (<a href="https://meta.stackoverflow.com/questions/415293/sunsetting-jobs-developer-story?ref=dillonshook.com" target="_blank" rel="noopener">RIP</a>)</li><li><a href="https://news.ycombinator.com/item?id=42575537&amp;ref=dillonshook.com" target="_blank" rel="noopener">Monthly Hacker News Who’s Hiring Thread</a></li><li>Networking and local meetups. This is much more hit and miss for finding a job quickly but it’s playing the long game to grow your network in your local community that is well worth the effort.</li><li>LinkedIn. I don’t apply to jobs directly since they’re flooded with applicants. Instead I find places I want to work at that have jobs posted on their company website then reach out to employees in the department and try to make a connection and get an introduction.</li><li>Recruiters. Don’t discount or blow them off. They have a vested interest in getting you hired and 3 of my jobs have been found through them.</li></ul><p>Some other site’s I’ve used with mixed results are <a href="https://hiring.cafe/?ref=dillonshook.com" target="_blank" rel="noopener">Hiring Cafe</a>, <del>Otta</del> <a href="https://employers.welcometothejungle.com/en/introducing-our-new-brand?ref=dillonshook.com" target="_blank" rel="noopener">Welcome To The Jungle</a> (weird rebrand name), and <a href="https://www.unlistedjobs.com/?ref=dillonshook.com" target="_blank" rel="noopener">Unlisted Jobs</a>.</p><p>As I mentioned before, when you’re looking for jobs and you want to avoid a layoff in the future it pays to understand the business model and how the company you’re applying to makes money. Ask where the company is in its profitability journey. How many customers do they have? Are they B2B (selling to other business) or B2C (selling directly to consumers)? How long is the sales or conversion process for each customer? How much money does each customer spend? How many rounds of funding has the company raised and for how much? Is the company visibly blowing a lot of money? One place I interviewed for flew its employees across the Atlantic for on sites once a quarter. You’re not going to get an exact answer to all these questions but you should be getting a sense in your head of how financially stable a company is while you’re interviewing.</p><h3 id="keep-track-of-the-jobs-you-applied-for">Keep Track of the Jobs you Applied For<a href="#keep-track-of-the-jobs-you-applied-for"><span data-pagefind-ignore="">#</span></a></h3><p>You’re likely going to have to do this for the unemployment benefits anyways but it also serves the purpose of making sure you don’t apply to a place twice (more likely than you think with the number of applications you’re going to have to send out), and tracking your progress through all the stages. You might be able to find patterns with what worked and what didn’t by tracking as well but for me it’s felt like there’s too many variables at play with each application to find much commonality.</p><h3 id="work-on-a-project">Work on a Project<a href="#work-on-a-project"><span data-pagefind-ignore="">#</span></a></h3><p>While you’re unemployed I’ve found it very helpful to have a side project to work on at the same time. It helps keep me sane through the process of looking through hundreds of job posts. Doing that for 8 hours a day will just make your eyes bleed and brain rot. It could be an idea you’ve had in the back of your head for awhile and never had the time to work on till now or it could be an open source project you’ve used and want to improve. Working on an actual project will keep your skills fresh, break up the days, and you’ll have something to show and talk about during the interview process. With any luck the project might even lead to finding job opportunities directly.</p><h3 id="keep-at-it">Keep At It<a href="#keep-at-it"><span data-pagefind-ignore="">#</span></a></h3><p>The software engineering job market today is very different than it has been prior to (and during Covid). If you haven’t searched for a job since then it will surprise you how much more of <a href="https://newsletter.pragmaticengineer.com/p/state-of-eng-market-2024?ref=dillonshook.com" target="_blank" rel="noopener">an employers market</a> it is now. Ultimately though, if you’ve got the skills you will find a job if you play the game long enough and stay positive through the process. You got this.</p><h2 id="the-takeaway">The Takeaway<a href="#the-takeaway"><span data-pagefind-ignore="">#</span></a></h2><p>Layoffs are increasingly becoming part of the normal tech industry experience. More than <a href="https://layoffs.fyi/?ref=dillonshook.com" target="_blank" rel="noopener">150,000 layoffs were reported</a> in 2024 and a whopping 264,000 in 2023. And those are just the reported ones. During the process the first time around I distinctly remember feeling some guilt/shame as if I hadn’t done enough to prove my worth and avoid the axe for the fourth (or more depending on how you count) time. But ultimately it’s not my fault or decision it happened, just like it’s very likely it’s not your fault if it happens to you. I’m not the one deciding to spend over 400 million dollars on spaghetti to see what sticks to the wall. Not yet at least :)</p><p>An important part of the process is also learning to <a href="https://cruciallearning.com/blog/retaking-your-pen-learning-to-author-your-worth/?ref=dillonshook.com" target="_blank" rel="noopener">retake your pen</a>. This is a concept from the book Crucial Conversations I read recently that’s all about learning to define your own self worth and your story in your own words. Don’t let other people tell your story for you or define success for you. In our society where we place such emphasis on your profession, a job loss can mean a shaken self identity. It’s up to you to tell your (truthful of course) side of the story and define yourself separately from any single employer.</p><p>I think it’ll still be a while longer before we can start to say the dust has settled in the tech industry from the end of ZIRP and the disruption AI tools are bringing. So keep your chin up and keep learning!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Magic/tragic email links: don't make them the only option (302 pts)]]></title>
            <link>https://recyclebin.zip/posts/annoyinglinks/</link>
            <guid>42627453</guid>
            <pubDate>Tue, 07 Jan 2025 21:06:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://recyclebin.zip/posts/annoyinglinks/">https://recyclebin.zip/posts/annoyinglinks/</a>, See on <a href="https://news.ycombinator.com/item?id=42627453">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><p><time>2025-01-07</time></p><div><p>The term “Magic Links” once meant a <a href="https://en.wikipedia.org/wiki/Magic_Link">futuristic PDA</a>. Nowdays, companies like <a href="https://auth0.com/docs/authenticate/passwordless/authentication-methods/email-magic-link">Auth0</a> use it to refer to the slightly-magical feat of including a login link in an email.</p><p>Last week, the great website you should subscribe to if you haven’t already (it’s great, when you’re not logged out), <a href="https://www.404media.co/">404 Media</a>, posted <a href="https://www.404media.co/we-dont-want-your-password-3/">“We Don’t Want Your Password”</a> in defense of so-called magic links.</p><p>Of course, as stated in the article, such email links are harder to phish than passwords, can’t lead to a breach of passwords, and protect the site itself against users who might reuse passwords previously compromised.</p><p>The article even covers some of my annoyances with this system, but throws out this sentence:</p><blockquote><p><a href="https://www.404media.co/we-dont-want-your-password-3/">We find this to be a much easier login process and wish it was more common across the web where appropriate.</a></p></blockquote><p>Easier than what? Easier than a long password, without a password manager? Easier than a passkey? Easier than an OTP sent to the same email address?</p><p>This sentence reads to me as one written by someone mostly working and <em>living</em> from a single laptop and mobile device. The second part of the sentence, calling for more sites to do this is why I am writing this.</p><p>For any scenario with a minimal amount of complexity, like users with multiple computers, and you’re looking at a scenario where the site’s unwillingness to deal with other login methods shoves friction on the end-user.</p><h3 id="what-makes-them-tragic">What makes them tragic:<a href="#what-makes-them-tragic" arialabel="Anchor">#</a></h3><ol><li>Multiple devices. Who doesn’t use at least a few computers weekly? I don’t have my email on my gaming PC, nor do I have it on my work laptops.</li><li>Slower. From 2 seconds slower to minutes slower, depending on SMTP delays as well as how awkward it is to get the link to the right browser.</li><li>Anti-mobile. As mentioned by 404 in their own article, this breaks the ability to use in-app browsers, which is quite annoying especially for RSS reader type apps. It makes interacting with any local link in the RSS feed extremely annoying.</li><li>Indirect security downsides. Pushing people to access personal email on work devices (or vice-versa) isn’t exactly a win for security.</li></ol><p>Another annoying <em>passwordless</em> system is to email or SMS an OTP the end user can type in.</p><p>While this sucks, it at least allows you to easily log in in situations where you don’t have a clear and easy copy/paste path from the email client to the browser you want to log in to.</p><p><a href="https://stratechery.com/">Stratechery</a>, powered by <a href="https://passport.online/">Passport</a>, uses this type of scheme (click link OR type in OTP), which is still shifting annoyances onto end-users to free developers from implementing passkeys, but at least has a bit more of an appreciation for end-users.</p><p>If you insist on using magic/tragic links by default, at least consider offering a robust alternative, such as <a href="https://fidoalliance.org/passkeys/">passkeys</a>, especially if your audience is technical and privacy-focused.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistakes engineers make in large established codebases (233 pts)]]></title>
            <link>https://www.seangoedecke.com/large-established-codebases/</link>
            <guid>42627227</guid>
            <pubDate>Tue, 07 Jan 2025 20:44:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seangoedecke.com/large-established-codebases/">https://www.seangoedecke.com/large-established-codebases/</a>, See on <a href="https://news.ycombinator.com/item?id=42627227">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p>Working in large established codebases is one of the hardest things to learn as a software engineer. You can’t practice it beforehand (no, open source does not give you the same experience). Personal projects can never teach you how to do it, because they’re necessarily small and from-scratch. For the record, when I say “large established codebases”, I mean:</p>
<ul>
<li>Single-digit million lines of code (~5M, let’s say)</li>
<li>Somewhere between 100 and 1000 engineers working on the same codebase</li>
<li>The first working version of the codebase is at least ten years old</li>
</ul>
<p>I’ve now spent a decade working in these codebases. Here’s what I wish I’d known at the start.</p>
<h3>The cardinal mistake is inconsistency</h3>
<p>There’s one mistake I see more often than anything else, and it’s absolutely deadly: ignoring the rest of the codebase and just implementing your feature in the most sensible way. In other words, limiting your touch points with the existing codebase in order to keep your nice clean code uncontaminated by legacy junk. For engineers that have mainly worked on small codebases, this is very hard to resist. But you must resist it! In fact, you must sink as deeply into the legacy codebase as possible, <strong>in order to maintain consistency.</strong></p>
<p>Why is consistency so important in large codebases? Because it protects you from nasty surprises, it slows down the codebase’s progression into a mess, and it allows you to take advantage of future improvements.</p>
<p>Suppose you’re building an API endpoint for a particular type of user. You could put some “return 403 if current user isn’t of that type” logic in your endpoint. But you should first go and look to see what other API endpoints in the codebase do for auth. If they use some specific set of helpers, you should also use that helper (even if it’s ugly, hard to integrate with, or seems like overkill for your use case). <strong>You must resist the urge to make your little corner of the codebase nicer than the rest of it.</strong></p>
<p>The main reason to do this is because large codebases have a lot of landmines in them. For instance, you might not know that the codebase has a concept of “bots”, which are like users but not quite, and require special treatment for auth. You might not know that the internal support tooling in the codebase allows an engineer to sometimes authenticate on behalf of a user, which requires special treatment for auth. There’s definitely another hundred things you might not know. Existing functionality represents a safe path through the minefield. If you do your auth like other API endpoints that have stuck around for a long time, you can follow that path without having to know all the surprising things that the codebase does.</p>
<p>On top of that, lack of consistency is the primary long-term killer of large codebases, because it makes it impossible to make any general improvements. Sticking with our auth example, if you want to ever introduce a new type of user, a consistent codebase lets you update the existing set of auth helpers to accommodate that. In an inconsistent codebase, where some API endpoints are doing things differently, you’ll have to go and update and test every one of those implementations. In practice, that means that the general change does not happen, or that the hardest 5% of endpoints to update are just left out of scope - which in turn decreases consistency further, because now you have a user type that works for most-but-not-all API endpoints.</p>
<p>So when you sit down to implement anything in a large codebase, you should always first go and look around for prior art, and follow that if at all possible.</p>
<h3>Is anything else important?</h3>
<p>Consistency is the most important thing. Let me quickly run through some other concerns as well, though:</p>
<p>You need to develop a good sense of how the service is used in practice (i.e. by users). Which endpoints are hit the most often? Which endpoints are the most crucial (i.e. are used by paying customers and cannot gracefully degrade)? What latency guarantees must the service obey, and what code gets run in the hot paths? One common large-codebase mistake is to make a “tiny tweak” that is unexpectedly in the hot path for a crucial flow, and thus causes a big problem.</p>
<p>You can’t rely on your ability to test the code in development like you can in a small project. Any large project accumulates state over time (for instance, how many kinds of user do you think GMail supports?) At a certain point, you can’t test every combination of states, even with automation. Instead, you have to test the crucial paths, code defensively, and rely on slow rollouts and monitoring to catch problems.</p>
<p>Be very, very reluctant to introduce new dependencies. In large codebases, code often lives forever. Dependencies introduce an ongoing cost in security vulnerabilities and package updates that will almost certainly outlive your tenure at the company. If you have to, make sure you pick dependencies that are widely-used and reliable, or that are easy to fork if needed. </p>
<p>For related reasons, if you ever get the chance to remove code, take it with both hands. This is some of the riskiest work in large codebases, so don’t half-ass it: first instrument the code to identify callers in production and drive them down to zero, so you can be absolutely certain it’s safe to remove. But it’s still worth doing. There are few things in a large codebase more worthwhile than safely removing code.</p>
<p>Work in small PRs and front-load the changes that affect other teams’ code. This one is important in small projects too, but it’s critical in large ones. That’s because you’ll often be relying on the domain experts in other teams to anticipate things you’ve missed (since large projects are just too complex to anticipate it all yourself). If you keep your changes to risky areas small and easy-to-read, those domain experts have a much better chance of noticing problems and saving you from an incident.</p>
<h3>Why bother?</h3>
<p>Finally, I want to take a second to defend these codebases in general. One common take I’ve heard goes like this:</p>
<blockquote>
<p>Why would you ever decide to work in the legacy mess? Spending time knee-deep in spaghetti code might be hard, but it isn’t good engineering. When faced with a large established codebase, our job should be to shrink it by splitting out small elegant services instead of wading in and making the mess larger.</p>
</blockquote>
<p>I think this is totally wrong-headed. The main reason is that, as a general rule, <strong>large established codebases produce 90% of the value</strong>. In any big tech company, the majority of the revenue-producing activity (i.e. the work that actually pays your engineering salary) comes from a large established codebase. If you work at a big tech company and don’t think this is true, maybe you’re right, but I’ll only take that opinion seriously if you’re deeply familiar with the large established codebase you think isn’t providing value. I’ve seen multiple cases where a small elegant service powers some core feature of a high-revenue product, but all the actual productizing code (settings, user management, billing, enterprise reporting, etc) still lives in the large established codebase.</p>
<p>So you should know how to work in the “legacy mess” because that’s what your company actually does. Good engineering or not, <em>it’s your job</em>. </p>
<p>The other reason is that <strong>you cannot split up a large established codebase without first understanding it</strong>. I have seen large codebases successfully split up, but I have never seen that done by a team that wasn’t already fluent at shipping features inside the large codebase. You simply cannot redesign any non-trivial project (i.e. a project that makes real money) from first-principles. There are too many accidental details that support tens of millions of dollars of revenue.</p>
<h3>Summary</h3>
<ul>
<li>Large codebases are worth working in because they usually pay your salary</li>
<li>By far the most important thing is consistency</li>
<li>Never start a feature without first researching prior art in the codebase</li>
<li>If you don’t follow existing patterns, you better have a very good reason for it</li>
<li>Understand the production footprint of the codebase</li>
<li>Don’t expect to be able to test every case - instead, rely on monitoring</li>
<li>Remove code any chance you get, but be very careful about it</li>
<li>Make it as easy as possible for domain experts to catch your mistakes</li>
</ul></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft disguises Bing as Google to fool inattentive searchers (239 pts)]]></title>
            <link>https://www.pcworld.com/article/2568916/microsoft-disguises-bing-as-google-to-fool-inattentive-searchers.html</link>
            <guid>42626431</guid>
            <pubDate>Tue, 07 Jan 2025 19:37:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pcworld.com/article/2568916/microsoft-disguises-bing-as-google-to-fool-inattentive-searchers.html">https://www.pcworld.com/article/2568916/microsoft-disguises-bing-as-google-to-fool-inattentive-searchers.html</a>, See on <a href="https://news.ycombinator.com/item?id=42626431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="link_wrapped_content">




<p>Microsoft would really, really, <em>really</em> like you to use Bing, its self-branded search engine that competes against Google’s monopoly. Not only is it <a href="https://www.pcworld.com/article/2509630/how-the-windows-search-uses-google-instead-of-bing.html">hardwired into much of Windows</a> and other Microsoft products like the Edge browser, it also employs a lot of sneaky visual tricks to steer you away from Google itself. But the company’s latest trick is more, well, tricky — in fact, it’s just straight-up Google camouflage.</p>



<p>This morning, users are discovering that if they search for “Google” in the primary Bing interface, they’re shown a special Bing search page. Before you scroll down to the actual search results, you’re presented with an all-white page with a centered, unbranded search bar and a multicolored doodle above it that’s heavy on yellow, red, blue, and green.</p>



<p>It is, in a very practical and (in this writer’s opinion) deliberate sense, a clone of the familiar Google search page. If you aren’t paying attention, you might put in a search or two in this disguised bar without ever realizing that you haven’t actually left Bing itself. You’ll have to scroll up or down to notice something’s off, if you haven’t immediately seen that it’s basically a knock-off Google doodle.</p>



<p>The change was <a href="https://go.skimresources.com/?id=111346X1569483&amp;xs=1&amp;url=https://www.windowslatest.com/2025/01/06/microsoft-bing-is-trying-to-spoof-google-ui-when-people-search-google-com/&amp;xcust=2-1-2568916-1-0-0-0-0&amp;sref=https://www.pcworld.com/article/2568916/microsoft-disguises-bing-as-google-to-fool-inattentive-searchers.html" rel="nofollow" data-subtag="2-1-2568916-1-0-0-0-0" data-domain-name="windowslatest" target="_blank">noted by  Windows Latest</a> (via <a href="https://go.skimresources.com/?id=111346X1569483&amp;xs=1&amp;url=https://9to5google.com/2025/01/06/bing-trick-users-google/&amp;xcust=2-1-2568916-1-0-0-0-0&amp;sref=https://www.pcworld.com/article/2568916/microsoft-disguises-bing-as-google-to-fool-inattentive-searchers.html" rel="nofollow" data-subtag="2-1-2568916-1-0-0-0-0" data-domain-name="9to5google" target="_blank">9to5Google</a>), which also points out that this page appears whether you’re going to Bing.com directly or searching from the Edge combined search/URL bar. The only place it <em>doesn’t</em> appear is if you’re searching from an Edge tab in which you’ve already signed into a Microsoft account. A little testing on my part verifies these parameters. (I’m seeing the “I can’t believe it’s not Google” search page in Vivaldi, even though I’m signed into my work Outlook account.)</p>

		
			
			


<p>This dedicated “I can’t believe it’s not Google” search page does not seem to appear for any searches less specific than “google,” “google search,” etc. Even something only slightly less immediate, like “google mail,” gives you the regular Bing interface and results.</p>



<p>The intent seems pretty obvious here, and it’s a very desperate look on the part of Microsoft. Not that I blame anyone for feeling a little desperate in the face of Google’s search dominance. I didn’t call it the Google monopoly idly, because that’s <a href="https://www.pcworld.com/article/2418714/googles-search-business-ruled-an-illegal-monopoly.html">the official determination of a United States federal court</a>. Google controlled an astonishing <a href="https://go.skimresources.com/?id=111346X1569483&amp;xs=1&amp;url=https://www.statista.com/statistics/1358006/worldwide-mobile-market-share-of-search-engines/&amp;xcust=2-1-2568916-1-0-0-0-0&amp;sref=https://www.pcworld.com/article/2568916/microsoft-disguises-bing-as-google-to-fool-inattentive-searchers.html" rel="nofollow" data-subtag="2-1-2568916-1-0-0-0-0" data-domain-name="statista" target="_blank">95 percent of the mobile search marketplace</a> as of 2024 according to Statista, a number that has barely moved since 2015, with Bing claiming less than one percent despite 15 years of Microsoft backing. The results are slightly less bleak if you look at the desktop search market, where Google has “only” 82 percent to Bing’s growing 10.51 percent share <a href="https://go.skimresources.com/?id=111346X1569483&amp;xs=1&amp;url=https://www.statista.com/statistics/216573/worldwide-market-share-of-search-engines/&amp;xcust=2-1-2568916-1-0-0-0-0&amp;sref=https://www.pcworld.com/article/2568916/microsoft-disguises-bing-as-google-to-fool-inattentive-searchers.html" rel="nofollow" data-subtag="2-1-2568916-1-0-0-0-0" data-domain-name="statista" target="_blank">as of early 2024</a>.</p>



<p>So, yeah, I don’t envy the Bing team’s task of slaying giants. But that being said, if a user wants Google and they search Bing to get it, deliberately obfuscating the search seems like a failure of a search engine’s sole task. Microsoft is putting its business goals above its users, a strategy that never endears users to the product.</p>



<p>Considering how poorly Microsoft’s push to <a href="https://www.pcworld.com/article/2548424/microsofts-wins-fails-and-wtf-moments-of-2024.html">move people off of Windows 10 is going</a>, I wonder if they might benefit from a bit of serious introspection on this approach. </p>

</div><div data-ga="article-footer-author">
								<p><img src="https://www.pcworld.com/wp-content/uploads/2021/10/author_photo_Michael-Crider_1635298804-1.jpg?quality=50&amp;strip=all&amp;w=150&amp;h=150&amp;crop=1" height="125" width="125">
				</p>
								<p>Michael is a 10-year veteran of technology journalism, covering everything from Apple to ZTE. On PCWorld he's the resident keyboard nut, always using a new one for a review and building a new mechanical board or expanding his desktop "battlestation" in his off hours. Michael's previous bylines include Android Police, Digital Trends, Wired, Lifehacker, and How-To Geek, and he's covered events like CES and Mobile World Congress live. Michael lives in Pennsylvania where he's always looking forward to his next kayaking trip.</p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Physicists Magnetize a Material with Light (113 pts)]]></title>
            <link>https://news.mit.edu/2024/physicists-magnetize-material-using-light-1218</link>
            <guid>42625219</guid>
            <pubDate>Tue, 07 Jan 2025 17:54:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.mit.edu/2024/physicists-magnetize-material-using-light-1218">https://news.mit.edu/2024/physicists-magnetize-material-using-light-1218</a>, See on <a href="https://news.ycombinator.com/item?id=42625219">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
          

            <p>MIT physicists have created a new and long-lasting magnetic state in a material, using only light.</p><p>In a study <a href="https://www.nature.com/articles/s41586-024-08226-x" target="_blank">appearing today in <em>Nature</em></a>, the researchers report using a terahertz laser — a light source that oscillates more than a trillion times per second — to directly stimulate atoms in an antiferromagnetic material. The laser’s oscillations are tuned to the natural vibrations among the material’s atoms, in a way that shifts the balance of atomic spins toward a new magnetic state.</p><p>The results provide a new way to control and switch antiferromagnetic materials, which are of interest for their potential to advance information processing and memory chip technology.</p><p>In common magnets, known as ferromagnets, the spins of atoms point in the same direction, in a way that the whole can be easily influenced and pulled in the direction of any external magnetic field. In contrast, antiferromagnets are composed of atoms with alternating spins, each pointing in the opposite direction from its neighbor. This up, down, up, down order essentially cancels the spins out, giving antiferromagnets a net zero magnetization that is impervious to any magnetic pull.</p><p>If a memory chip could be made from antiferromagnetic material, data could be “written” into microscopic regions of the material, called domains. A certain configuration of spin orientations (for example, up-down) in a given domain would represent the classical bit “0,” and a different configuration (down-up) would mean “1.” Data written on such a chip would be robust against outside magnetic influence.</p><p>For this and other reasons, scientists believe antiferromagnetic materials could be a more robust alternative to existing magnetic-based storage technologies. A major hurdle, however, has been in how to control antiferromagnets in a way that reliably switches the material from one magnetic state to another.</p><p>“Antiferromagnetic materials are robust and not influenced by unwanted stray magnetic fields,”&nbsp;says Nuh Gedik, the Donner Professor of Physics at MIT. “However, this robustness is a double-edged sword; their insensitivity to weak magnetic fields makes these materials difficult to control.”</p><p>Using carefully tuned terahertz light, the MIT team was able to controllably&nbsp;switch an antiferromagnet to a new magnetic state. Antiferromagnets could be incorporated into future memory chips that store and process more data while using less energy and taking up a fraction of the space of existing devices, owing to the stability of magnetic domains.</p><p>“Generally, such antiferromagnetic materials are not easy to control,” Gedik says. “Now we have some knobs to be able to tune and tweak them.”</p><p>Gedik is the senior author of the new study, which also includes MIT co-authors Batyr Ilyas, Tianchuang Luo, Alexander von Hoegen, Zhuquan Zhang, and Keith Nelson, along with collaborators at the Max Planck Institute for the Structure and Dynamics of Matter in Germany, University of the Basque Country in Spain, Seoul National University, and the Flatiron Institute in New York.</p><p><strong>Off balance</strong></p><p>Gedik’s group at MIT develops techniques to manipulate quantum materials in which interactions among atoms can give rise to exotic phenomena.</p><p>“In general, we excite materials with light to learn more about what holds them together fundamentally,” Gedik says. “For instance, why is this material an antiferromagnet, and is there a way to perturb microscopic interactions such that it turns into a ferromagnet?”</p><p>In their new study, the team worked with FePS<sub>3</sub> — a material that transitions to an antiferromagnetic phase at a critical temperature of around 118 kelvins (-247 degrees Fahrenheit).</p><p>The team suspected they might control the material’s transition by tuning into its atomic vibrations.</p><p>“In any solid, you can picture it as different atoms that are periodically arranged, and between atoms are tiny springs,” von Hoegen explains. “If you were to pull one atom, it would vibrate at a characteristic frequency which typically occurs in the terahertz range.”</p><p>The way in which atoms vibrate also relates to how their spins interact with each other. The team reasoned that if they could stimulate the atoms with a terahertz source that oscillates at the same frequency as the atoms’ collective vibrations, called phonons, the effect could also nudge the atoms’ spins out of their perfectly balanced, magnetically alternating alignment. Once knocked out of balance, atoms should have larger spins in one direction than the other, creating a preferred orientation that would shift the inherently nonmagnetized material into a new magnetic state with finite magnetization.</p><p>“The idea is that you can kill two birds with one stone: You excite the atoms’ terahertz vibrations, which also couples to the spins,” Gedik says.</p><p><strong>Shake and write</strong></p><p>To test this idea, the team worked with a sample of FePS<sub>3</sub> that was synthesized by colleages at Seoul National University. They placed the sample in a vacuum chamber and cooled it down to temperatures at and below 118 K. They then generated a terahertz pulse by aiming a beam of near-infrared light through an organic crystal, which transformed the light into the terahertz frequencies. They then directed this terahertz light toward the sample.</p><p>“This terahertz pulse is what we use to create a change in the sample,” Luo says. “It’s like ‘writing’ a new state into the sample.”</p><p>To confirm that the pulse triggered a change in the material’s magnetism, the team also aimed two near-infrared lasers at the sample, each with an opposite circular polarization. If the terahertz pulse had no effect, the researchers should see no difference in the intensity of the transmitted infrared lasers.</p><p>“Just seeing a difference tells us the material is no longer the original antiferromagnet, and that we are inducing a new magnetic state, by essentially using terahertz light to shake the atoms,” Ilyas says.</p><p>Over repeated experiments, the team observed that a terahertz pulse successfully switched the previously antiferromagnetic material to a new magnetic state — a transition that persisted for a surprisingly long time, over several milliseconds, even after the laser was turned off.</p><p>“People have seen these light-induced phase transitions before in other systems, but typically they live for very short times on the order of a picosecond, which is a trillionth of a second,” Gedik says.</p><p>In just a few milliseconds, scientists now might have a decent window of time during which they could probe the properties of the temporary new state before it settles back into its inherent antiferromagnetism. Then, they might be able to identify new knobs to tweak antiferromagnets and optimize their use in next-generation memory storage technologies.</p><p>This research was supported, in part, by the U.S. Department of Energy, Materials Science and Engineering Division, Office of Basic Energy Sciences, and the Gordon and Betty Moore Foundation.&nbsp;</p>        

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Type 2 Diabetes and cardiovascular disease attributable to sugar beverages (214 pts)]]></title>
            <link>https://www.nature.com/articles/s41591-024-03345-4</link>
            <guid>42625193</guid>
            <pubDate>Tue, 07 Jan 2025 17:51:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/s41591-024-03345-4">https://www.nature.com/articles/s41591-024-03345-4</a>, See on <a href="https://news.ycombinator.com/item?id=42625193">Hacker News</a></p>
Couldn't get https://www.nature.com/articles/s41591-024-03345-4: Error: Request failed with status code 406]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Tramway SDK – The Unholy Union Between Half-Life and Morrowind Engines (438 pts)]]></title>
            <link>https://racenis.github.io/tram-sdk/why.html</link>
            <guid>42624116</guid>
            <pubDate>Tue, 07 Jan 2025 16:22:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://racenis.github.io/tram-sdk/why.html">https://racenis.github.io/tram-sdk/why.html</a>, See on <a href="https://news.ycombinator.com/item?id=42624116">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<!-- PAGE_TITLE: Why Tramway SDK | Tramway Drifting and Dungeon Exploration Simulator Software Development Kit -->



<hr>

<p>
	<b>Tramway SDK</b> (heavy metal spelling –
	T̈ra̦m̊ẅa̦ÿ SD̈K) is a graphics
	package/framework/game engine that I have been working on for the past
	3 years.
</p>

<p>
	In this article I attempt to turn you against mainstream engines and I will
	explain, in detail, why Tramway SDK is not as awful as them.
</p>

<h2>
	Unreasonable system requirements due to Turbobloat
</h2>

<hr>

<p>
	Unity needs very powerful hardware and consumes enough power to burn down a
	rainforest. Godot is slightly better, but you still need relatively capable
	hardware.
</p>

<p>
	But what if all that you really want to make is just a lowpoly horror
	roguelite deckbuilder simulator? 15 year old hardware is more than capable
	of running a game like that, but not if you use a mainstream engine, due to
	Turbobloat.
</p>

<p>
	Tramway SDK can run on virtually any hardware from the last 15 years, since
	it is not Turbobloated. It doesn't even need a graphics card, since it can
	be switched to use software rasterization, making it perfect for displaying
	graphics on	toasters and fridges.
</p>

<p>
	Some might say "just get a better computer". This is why getting a better
	computer is bad:
</p>

<ol>
	<li>
		<h4>
			Affordance
		</h4>
		
		<p>
			A lot of people, especially from 3rd world countries are very poor
			and can't afford to buy hardware to run Turbobloat.
		</p>
	</li>
	
	<li>
		<h4>
			e-Waste
		</h4>
		
		<p>
			Producing computer chips is very bad on the environment. If modern
			software wasn't Turbobloated you would buy new hardware only when
			the previous hardware broke and wasn't repairable.
		</p>
	</li>
	
	<li>
		<h4>
			Not putting up with Turbobloat
		</h4>
		
		<p>
			Why spend money on another computer if you already have one that
			works perfectly fine? Just because of someone else's turbobloat? 
			You could buy 1000 cans of Dr. Pepper instead.
		</p>
	</li>
</ol>

<h2>
	Nodes are bad
</h2>

<hr>

<p>
	A thing should be a thing. It should not be a bunch of things pretending to
	be a single thing. With nodes you have to pretend that a collection of
	things is a single thing.
</p>

<p>
	Also when creating things with nodes, you have to go back and forth between
	node GUI and code. 
</p>

<p>
	In Tramway SDK you just subclass the Entity class and write the code. After
	that you make a level using the level editor. No going back and forth. No 
	nodes, just entities.
</p>

<h2>
	Monolithism
</h2>

<hr>

<p>
	All of the mainstream engines have a monolithic game editor. It doesn't
	matter how many features you use from it, you still have to wait 10 minutes
	for all of them to load in.
</p>

<p>
	Tramway SDK has editors, but all of them are optional. If you just want to
	use it as a framework, you can use only the C++ runtime. If you want to
	build levels, you can use only the level editor and no other GUI tool.
</p>

<p>
	Data files are stored as whitespace-seperated-values, so you could even edit
	all of the data files by hand, without using a single editor.
</p>

<h2>
	Bad graphics
</h2>

<hr>

<p>
	Most Unity games look like very bad, even with fancy shaders, normal mapping
	and other techniques.
</p>

<p>
	Look at what Tramway SDK can do with just lightmapping and Gouraud shading:
</p>

<center>
	<a target="_blank" href="https://racenis.github.io/tram-sdk/images/article/why/photorealism.png">
		<img src="https://racenis.github.io/tram-sdk/images/article/why/photorealism.png">
	</a><br>
	<i>
		This could be rendered on a Direct3D 7 level graphics card with a
		fixed-function pipeline.
	</i>
</center>

<h2>
	Quake level editors
</h2>

<hr>

<p>
	Brush based level geometry is very good and you can prototype levels with it
	very quickly. Tramway SDK has a <code>.map</code> file converter, which
	converts brushes into triangle meshes.
</p>

<center>
	<a target="_blank" href="https://racenis.github.io/tram-sdk/images/article/why/trenchbroom.png">
		<img src="https://racenis.github.io/tram-sdk/images/article/why/trenchbroom.png">
	</a><br>
	<i>Level being created in the Trenchbroom map editor.</i>
</center>

<br>

<center>
	<a target="_blank" href="https://racenis.github.io/tram-sdk/images/article/why/leveleditor.png">
		<img src="https://racenis.github.io/tram-sdk/images/article/why/leveleditor.png">
	</a><br>
	<i>Level being set up in the Tramway SDK level editor.</i>
</center>

<br>

<center>
	<a target="_blank" href="https://racenis.github.io/tram-sdk/images/article/why/inengine.png">
		<img src="https://racenis.github.io/tram-sdk/images/article/why/inengine.png">
	</a><br>
	<i>Level with lightmaps being rendered in-engine.</i>
</center>

<p>
	Wow! Look at this very fancy level. Not only can we create brush-based
	levels, we even have an input/output system like in Source to set up 
	interactions between entities in levels.
</p>

<center>
	<a target="_blank" href="https://racenis.github.io/tram-sdk/images/article/why/signals.png">
		<img src="https://racenis.github.io/tram-sdk/images/article/why/signals.png">
	</a><br>
	<i>Very fancy interaction editor.</i>
</center>

<p>
	In the future I will be implementing all of the Source logic entities, so
	that you can do visual scripting right in the level editor.
</p>

<h2>
	Framework for RPGs
</h2>

<hr>

<p>
	I think that it is very interesting to see the different kinds of games that
	can be created in a tool like RPG Maker. There's also tons of mods, even
	total conversions based on Morrowind. You can do a lot of stuff by changing
	the data, even in the RPG mechanics have already preprogrammed.
</p>

<center>
	<a target="_blank" href="https://racenis.github.io/tram-sdk/images/article/why/kitchensink.png">
		<img src="https://racenis.github.io/tram-sdk/images/article/why/kitchensink.png">
	</a><br>
	<i>Editor for the RPG framework.</i>
</center>

<p>
	Since the engine was designed to support level streaming from the very
	beginning, it should make the creation of open-world RPG-ish games very
	quick and very easy.
</p>

<center>
	<a target="_blank" href="https://racenis.github.io/tram-sdk/images/article/why/rpg.png">
		<img src="https://racenis.github.io/tram-sdk/images/article/why/rpg.png">
	</a><br>
	<i>
		Everyone always says that you "shouldn't create an open-world RPG", but
		that's just because they have never tried using the Trawmay SDK.
	</i>
</center>

<h2>
	TL; DR
</h2>

<hr>

<p>
	Tramway SDK is a game engine based on Quake/Source style entities, supports
	open-world streaming, comes with optional extensions, like the RPG framework
	that is sort like RPG Maker, but for 3D.
</p>

<p>
	This project is still in very early development. APIs are unstable, stuff
	breaks or just doesn't work, a lot of things still haven't been implemented,
	but it's getting better very quickly.
</p>

<center>
	<a href="https://github.com/racenis/tram-sdk">
		Click here for the Github repo!
	</a>
</center>




				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: HipScript – Run CUDA in the Browser with WebAssembly and WebGPU (166 pts)]]></title>
            <link>https://hipscript.lights0123.com/</link>
            <guid>42623593</guid>
            <pubDate>Tue, 07 Jan 2025 15:44:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hipscript.lights0123.com/">https://hipscript.lights0123.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42623593">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>HipScript</h2> <p>Online compiler for HIP and NVIDIA® CUDA® code to WebGPU</p> <p>By Ben Schattinger • <a href="https://lights0123.com/blog/2025/01/07/hip-script/" rel="noopener" target="_blank">Learn More</a></p> <p><small>Load sample code:</small> </p><!--[!--><details open=""><summary>Select GPU</summary>  <p>GPU Information</p> <ul><!--[--><!--]--></ul></details> <!--]--> <!--[!--><!--]--> <!--[!--><!--]--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Federal Reserve Bank of NY "Doomsday Book" 2022 via FOIA [pdf] (270 pts)]]></title>
            <link>https://www.crisesnotes.com/content/files/2023/12/NYFRB-2006.--Doomsday-Book--Searchable.pdf</link>
            <guid>42623144</guid>
            <pubDate>Tue, 07 Jan 2025 15:09:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.crisesnotes.com/content/files/2023/12/NYFRB-2006.--Doomsday-Book--Searchable.pdf">https://www.crisesnotes.com/content/files/2023/12/NYFRB-2006.--Doomsday-Book--Searchable.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=42623144">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Building Ultra Long Range Toslink (169 pts)]]></title>
            <link>https://blog.benjojo.co.uk/post/sfp-experiment-ultra-long-range-toslink</link>
            <guid>42621766</guid>
            <pubDate>Tue, 07 Jan 2025 12:39:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.benjojo.co.uk/post/sfp-experiment-ultra-long-range-toslink">https://blog.benjojo.co.uk/post/sfp-experiment-ultra-long-range-toslink</a>, See on <a href="https://news.ycombinator.com/item?id=42621766">Hacker News</a></p>
Couldn't get https://blog.benjojo.co.uk/post/sfp-experiment-ultra-long-range-toslink: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Ending our third party fact-checking program and moving to Community Notes model (535 pts)]]></title>
            <link>https://about.fb.com/news/2025/01/meta-more-speech-fewer-mistakes/</link>
            <guid>42621627</guid>
            <pubDate>Tue, 07 Jan 2025 12:15:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://about.fb.com/news/2025/01/meta-more-speech-fewer-mistakes/">https://about.fb.com/news/2025/01/meta-more-speech-fewer-mistakes/</a>, See on <a href="https://news.ycombinator.com/item?id=42621627">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p><span>Meta’s platforms are built to be places where people can express themselves freely. That can be messy. On platforms where billions of people can have a voice, all the good, bad and ugly is on display. But that’s free expression.</span></p>
<p><span>In his 2019 speech at Georgetown University, Mark Zuckerberg argued that free expression has been the driving force behind progress in American society and around the world and that inhibiting speech, however well-intentioned the reasons for doing so, often reinforces existing institutions and power structures instead of empowering people. He said: “Some people believe giving more people a voice is driving division rather than bringing us together. More people across the spectrum believe that achieving the political outcomes they think matter is more important than every person having a voice. I think that’s dangerous.”</span></p>
<p><span>In recent years we’ve developed increasingly complex systems to manage content across our platforms, partly in response to societal and political pressure to moderate content. This approach has gone too far. As well-intentioned as many of these efforts have been, they have expanded over time to the point where we are making too many mistakes, frustrating our users and too often getting in the way of the free expression we set out to enable. Too much harmless content gets censored, too many people find themselves wrongly locked up in “Facebook jail,” and we are often too slow to respond when they do.&nbsp;</span></p>
<p><span>We want to fix that and return to that fundamental commitment to free expression. Today, we’re making some changes to stay true to that ideal.</span></p>
<h2>Ending Third Party Fact Checking Program, Moving to Community Notes</h2>
<p><span>When we launched our independent fact checking program in 2016, we were very clear that we didn’t want to be the arbiters of truth. We made what we thought was the best and most reasonable choice at the time, which was to hand that responsibility over to independent fact checking organizations. The intention of the program was to have these independent experts give people more information about the things they see online, particularly viral hoaxes, so they were able to judge for themselves what they saw and read.</span></p>
<p><span>That’s not the way things played out, especially in the United States. Experts, like everyone else, have their own biases and perspectives. This showed up in the choices some made about what to fact check and how. Over time we ended up with too much content being fact checked that people would understand to be legitimate political speech and debate. Our system then attached real consequences in the form of intrusive labels and reduced distribution. A program intended to inform too often became a tool to censor.&nbsp;&nbsp;&nbsp;</span></p>
<p><span>We are now changing this approach. We will end the current third party fact checking program in the United States and instead begin moving to a Community Notes program. We’ve seen this approach work on X – where they empower their community to decide when posts are potentially misleading and need more context, and people across a diverse range of perspectives decide what sort of context is helpful for other users to see. We think this could be a better way of achieving our original intention of providing people with information about what they’re seeing – and one that’s less prone to bias.</span></p>
<ul>
<li><span>Once the program is up and running, Meta won’t write Community Notes or decide which ones show up. They are written and rated by contributing users.&nbsp;</span></li>
<li><span>Just like they do on X, Community Notes will require agreement between people with a range of perspectives to help prevent biased ratings.</span></li>
<li><span>We intend to be transparent about how different viewpoints inform the Notes displayed in our apps, and are working on the right way to share this information.</span></li>
<li><span>People can sign up today (</span><a href="https://www.facebook.com/help/contact/1914298425761977"><span>Facebook</span></a><span>, </span><a href="https://help.instagram.com/contact/1223551615403090"><span>Instagram</span></a><span>, </span><a href="https://help.instagram.com/contact/1638078013752611"><span>Threads</span></a><span>) for the opportunity to be among the first contributors to this program as it becomes available.&nbsp;</span></li>
</ul>
<p><span>We plan to phase in Community Notes in the US first over the next couple of months, and will continue to improve it over the course of the year. As we make the transition, we will get rid of our fact-checking control, stop demoting fact checked content and, instead of overlaying full screen interstitial warnings you have to click through before you can even see the post, we will use a much less obtrusive label indicating that there is additional information for those who want to see it.&nbsp;&nbsp;</span></p>
<h2>Allowing More Speech</h2>
<p><span>Over time, we have developed complex systems to manage content on our platforms, which are increasingly complicated for us to enforce. As a result, we have been over-enforcing our rules, limiting legitimate political debate and censoring too much trivial content and subjecting too many people to frustrating enforcement actions.</span></p>
<p><span>For example, in December 2024, we removed millions of pieces of content every day. While these actions account for less than 1% of content produced every day, we think one to two out of every 10 of these actions may have been mistakes (i.e., the content may not have actually violated our policies). This does not account for actions we take to tackle large-scale adversarial spam attacks. We plan to expand our transparency reporting to share numbers on our mistakes on a regular basis so that people can track our progress. As part of that we’ll also include more details on the mistakes we make when enforcing our spam policies.</span></p>
<p><span>We want to undo the mission creep that has made our rules too restrictive and too prone to over-enforcement. We’re getting rid of a number of restrictions on topics like immigration, gender identity and gender that are the subject of frequent political discourse and debate. It’s not right that things can be said on TV or the floor of Congress, but not on our platforms. These policy changes may take a few weeks to be fully implemented.&nbsp;</span></p>
<p><span>We’re also going to change how we enforce our policies to reduce the kind of mistakes that account for the vast majority of the censorship on our platforms. Up until now, we have been using automated systems to scan for all policy violations, but this has resulted in too many mistakes and too much content being censored that shouldn’t have been. So, we’re going to continue to focus these systems on tackling illegal and high-severity violations, like terrorism, child sexual exploitation, drugs, fraud and scams. For less severe policy violations, we’re going to rely on someone reporting an issue before we take any action. We also demote too much content that our systems predict might violate our standards. We are in the process of getting rid of most of these demotions and requiring greater confidence that the content violates for the rest. And we’re going to tune our systems to require a much higher degree of confidence before a piece of content is taken down. As part of these changes, we will be moving the trust and safety teams that write our content policies and review content out of California to Texas and other US locations.</span></p>
<p><span>People are often given the chance to appeal our enforcement decisions and ask us to take another look, but the process can be frustratingly slow and doesn’t always get to the right outcome. We’ve added extra staff to this work and in more cases, we are also now requiring multiple reviewers to reach a determination in order to take something down. We are working on ways to make recovering accounts more straightforward and testing facial recognition technology, and we’ve started using AI large language models (LLMs) to provide a second opinion on some content before we take enforcement actions.</span></p>
<h2>A Personalized Approach to Political Content</h2>
<p><span>Since 2021, we’ve made changes to reduce the amount of civic content people see – posts about elections, politics or social issues – based on the feedback our users gave us that they wanted to see less of this content. But this was a pretty blunt approach. We are going to start phasing this back into Facebook, Instagram and Threads with a more personalized approach so that people who want to see more political content in their feeds can.</span></p>
<p><span>We’re continually testing how we deliver personalized experiences and have recently conducted testing around civic content. As a result, we’re going to start treating civic content from people and Pages you follow on Facebook more like any other content in your feed, and we will start ranking and showing you that content based on explicit signals (for example, liking a piece of content) and implicit signals (like viewing posts) that help us predict what’s meaningful to people. We are also going to recommend more political content based on these personalized signals and are expanding the options people have to control how much of this content they see.</span></p>
<p><span>These changes are an attempt to return to the commitment to free expression that Mark Zuckerberg set out in his Georgetown speech. That means being vigilant about the impact our policies and systems are having on people’s ability to make their voices heard, and having the humility to change our approach when we know we’re getting things wrong.</span></p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getty Images and Shutterstock to Merge (161 pts)]]></title>
            <link>https://newsroom.gettyimages.com/en/getty-images/getty-images-and-shutterstock-to-merge-creating-a-premier-visual-content-company</link>
            <guid>42621544</guid>
            <pubDate>Tue, 07 Jan 2025 12:00:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsroom.gettyimages.com/en/getty-images/getty-images-and-shutterstock-to-merge-creating-a-premier-visual-content-company">https://newsroom.gettyimages.com/en/getty-images/getty-images-and-shutterstock-to-merge-creating-a-premier-visual-content-company</a>, See on <a href="https://news.ycombinator.com/item?id=42621544">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <ul><li><em>Merged company will be well‑positioned to meet the evolving needs of creative, media, and advertising industries through combined investment in content creation, event coverage, and product and technology innovation &nbsp; &nbsp;&nbsp;</em></li><li><em>Expected annual cost synergies between $150 million and $200 million by year three</em></li><li><em>Expected to be accretive to earnings and cash flow beginning in year two</em></li><li><em>Companies will hold conference call to discuss the transaction with investment community today at 8.30a.m. EST</em></li></ul><p><strong>NEW YORK, January 7, 2025</strong> – Getty&nbsp;Images Holdings, Inc. (NYSE: GETY) and Shutterstock (NYSE: SSTK) today announced that they entered into a definitive merger agreement to combine in a merger of equals transaction, creating a premier visual content company. The combined company, which would have an enterprise value of approximately $3.7 billion[i], will be named Getty&nbsp;Images Holdings, Inc and will continue to trade on the New York Stock Exchange under the ticker symbol “GETY”.&nbsp;</p><p>As a combined company, Getty&nbsp;Images and Shutterstock will offer a content library with greater depth and breadth for the benefit of customers, expanded opportunities for its contributor community and a reinforced commitment to the adoption of inclusive and representative content. Furthermore, the stronger financial profile of the combined company is expected to create increased capacity for product investment and innovation for customers in a fast‑evolving and highly competitive environment. &nbsp;&nbsp;</p><p>&nbsp;“Today’s announcement is exciting and transformational for our companies, unlocking multiple opportunities to strengthen our financial foundation and invest in the future—including enhancing our content offerings, expanding event coverage, and delivering new technologies to better serve our customers,” said Craig Peters, CEO, Getty&nbsp;Images. “With the rapid rise in demand for compelling visual content across industries, there has never been a better time for our two businesses to come together. By combining our complementary strengths, we can better address customer opportunities while delivering exceptional value to our partners, contributors, and stockholders.”</p><p>“We are excited by the opportunities we see to expand our creative content library and enhance our product offering to meet diverse customer needs,” said Paul Hennessy, CEO, Shutterstock. “We expect the merger to produce value for the customers and stockholders of both companies by capitalizing on attractive growth opportunities to drive combined revenues, accelerating product innovation, realizing significant cost synergies and improving cash flow. We look forward to working closely with the Getty&nbsp;Images management team to complete the transaction and drive the next chapter of growth.”</p><p><strong>Strategic and Financial Benefits</strong></p><ul><li><em>Cutting‑edge innovation:</em> Facilitates greater investment in innovative content creation, expanded event coverage, and customer‑facing technologies and capabilities such as search, 3D imagery and generative AI. &nbsp; &nbsp;</li><li><em>Complementary portfolios:&nbsp;</em>Creates a broader set of visual content products across still imagery, video, music, 3D and other asset types.</li><li><em>Expanded opportunities for content creators:&nbsp;</em>Provides contributors substantially greater opportunities to reach customers around the world.</li><li><em>Strengthened balance sheet and greater cash flow generation:</em> By deleveraging the combined balance sheet through the transaction and driving more robust cash flow, the combined company will be well positioned to accelerate debt repayment, reduce borrowing costs, and capitalize on new opportunities to create value for customers and stockholders.</li><li><em>Significant synergies:&nbsp;</em>Drives expected run rate synergies across SG&amp;A and CAPEX between $150 million and $200 million achieved within the first three years post‑close, with approximately two‑thirds expected to be delivered within the first twelve to twenty‑four months.</li><li><em>Compelling Financial Profile:</em>On a pro forma 2024 basis the combined company would have an attractive financial profile:<ul><li>Revenue of between $1,979 million and $1,993 million, including 46% of subscription revenue</li><li>Pre‑synergy EBITDA of between $569 million and $574 million</li><li>Pre‑synergy Adjusted EBITDA less capital expenditures of between $461 million and $466 million</li><li>Pre‑synergy net leverage of 3.0x pro forma 2024 pre‑synergy EBITDA</li></ul></li></ul><p><strong>Leadership and Governance</strong><br>At close, Getty&nbsp;Images’s CEO, Craig Peters, will serve as CEO of the combined company. The combined company will have an eleven‑member Board of Directors, comprised of Getty&nbsp;Images CEO Craig Peters, six directors designated by Getty&nbsp;Images and four directors designated by Shutterstock, including Paul Hennessy, Shutterstock CEO. The Chairman of the Board of Directors of the combined company will be Mark Getty, currently Chairman of Getty&nbsp;Images.</p><p><strong>Transaction Details</strong><br>Under the terms of the agreement, which was unanimously approved by the Boards of Directors of both companies, Shutterstock stockholders at close can elect to receive one of the following:</p><ul><li>$28.84870 per share in cash for each share of Shutterstock common stock they own;</li><li>13.67237 shares of Getty&nbsp;Images common stock for each share of Shutterstock common stock they own; or</li><li>a mixed consideration of 9.17 shares of Getty&nbsp;Images common stock plus $9.50 in cash for each share of Shutterstock common stock they own.</li></ul><p>Shutterstock shareholder elections at close are subject to proration to ensure that the aggregate consideration payable by Getty&nbsp;Images consist of $9.50 in cash per Shutterstock share as of immediately before close and 9.17 shares of Getty&nbsp;Images stock per Shutterstock share as immediately before close.</p><p>Based on the common shares outstanding as of the signing date, the aggregate consideration payable by Getty&nbsp;Images would consist of $331 million in cash and 319.4 million shares of Getty&nbsp;Images stock. These figures do not include the impact of unvested Shutterstock equityholders as of the signing date and do not assume any vesting of currently‑unvested Shutterstock equity holdings between signing and close.</p><p>Shutterstock equityholders with unvested RSU and PSU grants at close will only be eligible to receive the mixed consideration noted above upon vesting with respect to such grants. Shutterstock option holders will have their options and strike prices adjusted by a ratio equal to the sum of (i) 9.17 and (ii) $9.50 divided by the 10‑day average closing stock price of Getty&nbsp;Images common stock for the period ending two (2) business days prior to the closing as quoted on NYSE. Equity treatment will take into account any employment contracts in place at the close of the transaction. Aggregate cash and share amounts are estimates and are subject to change between signing and close.</p><p>At close, Getty&nbsp;Images stockholders will own approximately 54.7% and Shutterstock stockholders will own approximately 45.3% of the combined company on a fully diluted basis. Shutterstock will, at the discretion of its Board of Directors, continue to declare and pay quarterly cash dividends, in accordance with its dividend policy, pending the close of the transaction.</p><p><strong>Timing and Closing</strong><br>The transaction is subject to the satisfaction of customary closing conditions, including receipt of required regulatory approvals, the approval of Getty&nbsp;Images and Shutterstock stockholders and the extension or refinancing of Getty&nbsp;Images’ existing debt obligations. &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p><p><strong>Advisors</strong><br>Berenson &amp; Company, LLC is acting as lead financial advisor and J.P. Morgan Securities LLC is acting as a financial advisor to Getty&nbsp;Images and Skadden, Arps, Slate, Meagher &amp; Flom LLP is serving as legal advisor. Allen &amp; Company LLC is acting as exclusive financial advisor to Shutterstock and White &amp; Case LLP is serving as legal advisor.</p><p><strong>Conference Call</strong><br>Getty&nbsp;Images and Shutterstock will hold a conference call to discuss the transaction today, January 7, 2025, at 8:30 a.m. Eastern Time. The live webcast will be accessible through the Investor Relations section of the each company’s website at <a href="https://investors.gettyimages.com/" target="_blank">https://investors.gettyimages.com/</a> and&nbsp;<a href="https://investor.shutterstock.com/" target="_blank">https://investor.shutterstock.com</a>.</p><p>To access the call through a conference line, dial 1‑800‑445‑7795 (in the U.S.) or 1‑785‑424‑1699 (international callers). The conference ID for the call is GETTY. A replay of the conference call will be posted shortly after the call and will be available for fourteen days following the call. To access the replay, dial 1‑844‑512‑2921 (in the U.S.) or 1‑412‑317‑6671 (international callers). The access code for the replay is 11156500.</p><p><strong>Investor Contact Getty&nbsp;Images:</strong><br>Steven Kanner<br><a href="mailto:Investorrelations@gettyimages.com">Investorrelations@gettyimages.com</a></p><p>&nbsp;<strong>Media Contact Getty&nbsp;Images:</strong><br>Anne Flanagan<br><a href="mailto:Anne.flanagan@gettyimages.com">Anne.flanagan@gettyimages.com</a></p><p>&nbsp;<strong>Investor Contact Shutterstock:</strong><br>Chris Suh<br><a href="mailto:csuh@shutterstock.com">csuh@shutterstock.com</a></p><p>&nbsp;<strong>Media Contact Shutterstock:</strong><br>Lori Rodney<br><a href="mailto:lrodney@shutterstock.com">lrodney@shutterstock.com</a></p><p><strong>About Getty&nbsp;Images</strong><br>Getty&nbsp;Images (NYSE: GETY) is a preeminent global visual content creator and marketplace that offers a full range of content solutions to meet the needs of any customer around the globe, no matter their size. Through its <a href="http://www.gettyimages.com/" target="_blank">Getty&nbsp;Images</a>, <a href="https://www.istockphoto.com/" target="_blank">iStock</a> and <a href="https://unsplash.com/" target="_blank">Unsplash</a> brands, websites and APIs, Getty&nbsp;Images serves customers in almost every country in the world and is the first‑place people turn to discover, purchase and share powerful visual content from the world’s best photographers and videographers. Getty&nbsp;Images works with over 576,000 content creators and more than 340 content partners to deliver this powerful and comprehensive content. Each year Getty&nbsp;Images covers more than 160,000 <a href="http://www.gettyimages.com/editorialimages/news" target="_blank">news</a>, <a href="http://www.gettyimages.com/editorialimages/sport" target="_blank">sport</a> and <a href="http://www.gettyimages.com/editorialimages/entertainment" target="_blank">entertainment</a> events providing depth and breadth of coverage that is unmatched. Getty&nbsp;Images maintains one of the largest and best privately‑owned&nbsp;<a href="http://www.gettyimages.com/editorialimages/archival" target="_blank">photographic archives</a> in the world with millions of images dating back to the beginning of photography.</p><p>Through its <a href="https://www.gettyimages.com/creative" target="_blank">best‑in‑class creative library</a> and <a href="https://www.gettyimages.com/enterprise/custom-content" target="_blank">Custom Content</a> solutions, Getty&nbsp;Images helps customers elevate their creativity and entire end‑to‑end creative process to find the right visual for any need. With the adoption and distribution of generative AI technologies and tools trained on permissioned content that include indemnification and perpetual, worldwide usage rights, <a href="https://www.gettyimages.com/ai" target="_blank">Getty&nbsp;Images&nbsp;</a>and&nbsp;<a href="https://www.istockphoto.com/ai" target="_blank">iStock</a> customers can use text to image generation to ideate and create commercially safe compelling visuals, further expanding Getty&nbsp;Images capabilities to deliver exactly what customers are looking for.</p><p>For company news and announcements, visit our&nbsp;<a href="https://newsroom.gettyimages.com/" target="_blank">Newsroom</a>.</p><p><strong>About Shutterstock, Inc.</strong><br>Shutterstock, Inc. (NYSE:&nbsp;<a href="https://markets.financialcontent.com/prnews/quote?Symbol=SSTK" target="_blank">SSTK</a>) is a premier partner for transformative brands, digital media and marketing companies, empowering the world to create with confidence. Fueled by millions of creators around the world and a fearless approach to product innovation, Shutterstock is the leading global platform for licensing from the most extensive and diverse collection of high‑quality 3D models, videos, music, photographs, vectors and illustrations. From the world's largest content marketplace, to breaking news and A‑list entertainment editorial access, to all‑in‑one content editing platform and studio production service—all using the latest in innovative technology—Shutterstock offers the most comprehensive selection of resources to bring storytelling to life.</p><p>Learn more at <a href="https://www.shutterstock.com/" target="_blank">www.shutterstock.com</a> and follow us on <a href="https://www.linkedin.com/company/shutterstock/" target="_blank">LinkedIn</a>, <a href="https://www.instagram.com/shutterstock/" target="_blank">Instagram</a>, <a href="https://x.com/shutterst" target="_blank">Twitter</a>, <a href="https://www.facebook.com/Shutterstock/" target="_blank">Facebook</a> and&nbsp;<a href="https://www.youtube.com/channel/UCmhVYUCCOm0D5bT12MOr5_w" target="_blank">YouTube</a>.</p><p><strong>Additional Information about the Acquisition and Where to Find It</strong><br>In connection with the proposed transaction, Getty&nbsp;Images intends to file with the Securities and Exchange Commission (the “SEC”) a registration statement on Form S‑4 that will include an information statement of Getty&nbsp;Images and a proxy statement of Shutterstock and that also will constitute a prospectus with respect to shares of Getty&nbsp;Images’ common stock to be issued in the transaction (the “joint proxy and information statement/prospectus”). Each of Getty&nbsp;Images and Shutterstock may also file with or furnish to the SEC other relevant documents regarding the proposed transaction. This press release is not a substitute for the joint proxy and information statement/prospectus or any other document that Getty&nbsp;Images or Shutterstock may file with or furnish to the SEC. The definitive joint proxy and information statement/prospectus (if and when available) will be mailed to stockholders of Getty&nbsp;Images and Shutterstock. BEFORE MAKING ANY VOTING OR INVESTMENT DECISION, INVESTORS AND SECURITY HOLDERS ARE URGED TO READ THE JOINT PROXY AND INFORMATION STATEMENT/PROSPECTUS (WHEN AVAILABLE) AND ALL OTHER RELEVANT DOCUMENTS THAT ARE OR WILL BE FILED WITH OR FURNISHED TO THE SEC, AS WELL AS ANY AMENDMENTS OR SUPPLEMENTS TO THESE DOCUMENTS, CAREFULLY AND IN THEIR ENTIRETY BECAUSE THEY CONTAIN OR WILL CONTAIN IMPORTANT INFORMATION ABOUT THE PROPOSED TRANSACTION AND RELATED MATTERS. Investors and security holders will be able to obtain free copies of the joint proxy and information statement/prospectus (if and when available) and other documents containing important information about Getty&nbsp;Images, Shutterstock and the proposed transaction, once such documents are filed with or furnished to the SEC through the website maintained by the SEC at www.sec.gov. Copies of the documents filed with or furnished to the SEC by Getty&nbsp;Images will be available free of charge on Getty&nbsp;Images’ website at investors.gettyimages.com or by contacting Getty&nbsp;Images’ Investor Relations department by email at <a href="mailto:investorrelations@gettyimages.com">investorrelations@gettyimages.com</a>. Copies of the documents filed with or furnished to the SEC by Shutterstock will be available free of charge on Shutterstock’s website at investor. shutterstock.com or by contacting Shutterstock’s Investor Relations department by email at&nbsp;<a href="mailto:IR@Shutterstock.com">IR@Shutterstock.com</a>.</p><p><strong>Participants in the Solicitation</strong><br>This communication is not a solicitation of proxies in connection with the proposed transaction. Getty&nbsp;Images, Shutterstock and certain of their respective directors and executive officers and other members of their respective management and employees may be deemed to be participants in the solicitation of proxies in respect of the proposed transaction. Information about the directors and executive officers of Getty&nbsp;Images, including a description of their direct or indirect interests, by security holdings or otherwise, is set forth in Getty&nbsp;Images’ proxy statement for its 2024 annual meeting of stockholders, which was filed with the SEC on April 24, 2024. Information about the directors and executive officers of Shutterstock, including a description of their direct or indirect interests, by security holdings or otherwise, is set forth in Shutterstock’s proxy statement for its 2024 annual meeting of stockholders, which was filed with the SEC on April 26, 2024. Other information regarding the participants in the proxy solicitations and a description of their direct and indirect interests, by security holdings or otherwise, will be contained in the joint proxy and information statement/prospectus and other relevant materials to be filed with or furnished to the SEC regarding the proposed transaction. You may obtain free copies of these documents using the sources indicated above.</p><p><strong>No Offer or Solicitation</strong><br>This communication is not intended to and shall not constitute an offer to buy or sell or the solicitation of an offer to buy or sell any securities, or a solicitation of any vote or approval, nor shall there be any sale of securities in any jurisdiction in which such offer, solicitation or sale would be unlawful prior to registration or qualification under the securities laws of any such jurisdiction. No offer of securities shall be made, except by means of a prospectus meeting the requirements of Section 10 of the Securities Act of 1933, as amended.</p><p><strong>Forward Looking Statements</strong><br>The statements in this press release, and any related oral statements, include forward‑looking statements concerning Getty&nbsp;Images, Shutterstock, the proposed transaction described herein and other matters. All statements, other than historical facts, are forward‑looking statements. Forward‑looking statements may discuss goals, intentions and expectations as to future plans, trends, events, results of operations or financial condition, financings or otherwise, based on current beliefs and involve numerous risks and uncertainties that could cause actual results to differ materially from expectations. Forward‑looking statements speak only as of the date they are made or as of the dates indicated in the statements and should not be relied upon as predictions of future events, as there can be no assurance that the events or circumstances reflected in these statements will be achieved or will occur or the timing thereof. Forward‑looking statements can often, but not always, be identified by the use of forward‑looking terminology including “believes,” “expects,” “may,” “will,” “should,” “could,” “might,” “seeks,” “intends,” “plans,” “pro forma,” “estimates,” “anticipates,” “designed,” or the negative of these words and phrases, other variations of these words and phrases or comparable terminology, but not all forward‑looking statements include such identifying words. Forward‑looking statements are based upon current plans, estimates and expectations that are subject to risks, uncertainties and assumptions. Should one or more of these risks or uncertainties materialize, or should underlying assumptions prove incorrect, actual results may vary. The forward‑looking statements in this press release relate to, among other things, obtaining applicable regulatory and stockholder approvals on a timely basis or otherwise, satisfying other closing conditions to the proposed transaction, on a timely basis or otherwise, the expected tax treatment of the transaction, the expected timing of the transaction, and the integration of the businesses and the expected benefits, cost savings, accretion, synergies and growth to result therefrom. Important factors that could cause actual results to differ materially from such forward‑looking statements include, among other things: failure to obtain applicable regulatory or stockholder approvals in a timely manner or otherwise; interloper risk; failure to satisfy other closing conditions to the transaction or to complete the transaction on anticipated terms and timing (or at all); negative effects of the announcement of the transaction on the ability of Shutterstock or Getty&nbsp;Images to retain and hire key personnel and maintain relationships with customers, suppliers and others who Shutterstock or Getty&nbsp;Images does business, or on Shutterstock or Getty&nbsp;Images’ operating results and business generally; risks that the businesses will not be integrated successfully or that the combined company will not realize expected benefits, cost savings, accretion, synergies and/or growth, as expected (or at all), or that such benefits may take longer to realize or may be more costly to achieve than expected; the risk that disruptions from the transaction will harm business plans and operations; risks relating to unanticipated costs of integration; significant transaction and/or integration costs, or difficulties in connection with the transaction and/or unknown or inestimable liabilities; restrictions during the pendency of the transaction that may impact the ability to pursue certain business opportunities or strategic transactions; potential litigation associated with the transaction; the potential impact of the announcement or consummation of the transaction on Getty&nbsp;Images’, Shutterstock’s or the combined company’s relationships with suppliers, customers, employers and regulators; demand for the combined company’s products; potential changes in the Getty&nbsp;Images stock price that could negatively impact the value of the consideration offered to the Shutterstock stockholders; the occurrence of any event that could give rise to the termination of the proposed transaction; and Getty&nbsp;Images’ ability to complete any refinancing of its debt or new debt financing on a timely basis, on favorable terms or at all. A more fulsome discussion of the risks related to the proposed transaction will be included in the joint proxy and information statement/prospectus. For a discussion of factors that could cause actual results to differ materially from those contemplated by forward‑looking statements, see the section captioned “Risk Factors” in each of Getty&nbsp;Images’ and Shutterstock’s Annual Report on Form 10‑K for the fiscal year ended December 31, 2023, subsequent Quarterly Reports on Form 10‑Q and other filings with the SEC. Should one or more of these risks or uncertainties materialize, or should underlying assumptions prove incorrect, actual results may vary materially from those indicated or anticipated by such forward looking statements. While the list of factors presented here is, and the list of factors presented in the joint proxy and information statement/prospectus will be, considered representative, no such list should be considered to be a complete statement of all potential risks and uncertainties. Unlisted factors may present significant additional obstacles to the realization of forward looking statements. Neither Getty&nbsp;Images nor Shutterstock assumes, and each hereby disclaims, any obligation to update forward‑looking statements, except as may be required by law.</p><p>&nbsp;[i] Pro‑Forma Combined Enterprise Value is based on closing share prices as of January 6, 2025.&nbsp;&nbsp;&nbsp; </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia announces $3k personal AI supercomputer called Digits (123 pts)]]></title>
            <link>https://www.theverge.com/2025/1/6/24337530/nvidia-ces-digits-super-computer-ai</link>
            <guid>42621332</guid>
            <pubDate>Tue, 07 Jan 2025 11:11:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2025/1/6/24337530/nvidia-ces-digits-super-computer-ai">https://www.theverge.com/2025/1/6/24337530/nvidia-ces-digits-super-computer-ai</a>, See on <a href="https://news.ycombinator.com/item?id=42621332">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>If you were looking for your own personal AI supercomputer, Nvidia has you covered.</p><p>The chipmaker announced <a href="https://www.theverge.com/2025/1/4/24307731/ces-2025-tvs-gaming-smart-home-wearables-news">at CES</a> it’s launching a personal AI supercomputer called Project Digits in May. The heart of Project Digits is the new GB10 Grace Blackwell Superchip, which packs enough processing power to run sophisticated AI models while being compact enough to fit on a desk and run from a standard power outlet (this kind of processing power used to require much larger, more power-hungry systems). This desktop-sized system can handle AI models with up to 200 billion parameters, and has a starting price of $3,000. The product itself looks <a href="https://www.theverge.com/24289730/apple-mac-mini-m4-review">a lot like a Mac Mini</a>.</p><p>“AI will be mainstream in every application for every industry. With Project Digits, the Grace Blackwell Superchip comes to millions of developers,” Nvidia CEO Jensen Huang said in a press release. “Placing an AI supercomputer on the desks of every data scientist, AI researcher and student empowers them to engage and shape the age of AI.”</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Project Digits looks like a mini PC." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1319x741/376x211/filters:focal(660x371:661x372):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820689/chrome_HV8SYeFAav.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1319x741/384x216/filters:focal(660x371:661x372):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820689/chrome_HV8SYeFAav.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1319x741/415x233/filters:focal(660x371:661x372):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820689/chrome_HV8SYeFAav.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1319x741/480x270/filters:focal(660x371:661x372):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820689/chrome_HV8SYeFAav.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1319x741/540x303/filters:focal(660x371:661x372):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820689/chrome_HV8SYeFAav.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1319x741/640x360/filters:focal(660x371:661x372):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820689/chrome_HV8SYeFAav.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1319x741/750x421/filters:focal(660x371:661x372):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820689/chrome_HV8SYeFAav.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1319x741/828x465/filters:focal(660x371:661x372):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820689/chrome_HV8SYeFAav.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1319x741/1080x607/filters:focal(660x371:661x372):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820689/chrome_HV8SYeFAav.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1319x741/1200x674/filters:focal(660x371:661x372):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820689/chrome_HV8SYeFAav.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1319x741/1440x809/filters:focal(660x371:661x372):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820689/chrome_HV8SYeFAav.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1319x741/1920x1079/filters:focal(660x371:661x372):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820689/chrome_HV8SYeFAav.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1319x741/2048x1151/filters:focal(660x371:661x372):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820689/chrome_HV8SYeFAav.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1319x741/2400x1348/filters:focal(660x371:661x372):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820689/chrome_HV8SYeFAav.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1319x741/2400x1348/filters:focal(660x371:661x372):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820689/chrome_HV8SYeFAav.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>Project Digits looks like a mini PC.</em></figcaption> <p><cite>Image: Nvidia</cite></p></div></div><p>Each Project Digits system comes equipped with 128GB of unified, coherent memory (by comparison, a good laptop might have 16GB or 32GB of RAM) and up to 4TB of NVMe storage. For even more demanding applications, two Project Digits systems can be linked together to handle models with up to 405 billion parameters (Meta’s best model, Llama 3.1,  <a href="https://www.theverge.com/2024/7/23/24204055/meta-ai-llama-3-1-open-source-assistant-openai-chatgpt">has 405 billion parameters</a>).</p><p>The GB10 chip delivers up to 1 petaflop of AI performance (which means it can perform 1 quadrillion AI calculations per second) at FP4 precision (which helps make the calculations faster by making approximations), and the system features Nvidia’s latest-generation CUDA cores and fifth-generation Tensor Cores, connected via NVLink-C2C to a Grace CPU containing 20 power-efficient Arm-based cores. MediaTek, known for their Arm-based chip designs, collaborated on the GB10’s development to optimize its power efficiency and performance.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="The Digits supercomputer specs." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1322x733/376x208/filters:focal(661x367:662x368):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820687/chrome_NP3LJVKJfM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1322x733/384x213/filters:focal(661x367:662x368):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820687/chrome_NP3LJVKJfM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1322x733/415x230/filters:focal(661x367:662x368):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820687/chrome_NP3LJVKJfM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1322x733/480x266/filters:focal(661x367:662x368):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820687/chrome_NP3LJVKJfM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1322x733/540x299/filters:focal(661x367:662x368):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820687/chrome_NP3LJVKJfM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1322x733/640x355/filters:focal(661x367:662x368):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820687/chrome_NP3LJVKJfM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1322x733/750x416/filters:focal(661x367:662x368):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820687/chrome_NP3LJVKJfM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1322x733/828x459/filters:focal(661x367:662x368):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820687/chrome_NP3LJVKJfM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1322x733/1080x599/filters:focal(661x367:662x368):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820687/chrome_NP3LJVKJfM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1322x733/1200x665/filters:focal(661x367:662x368):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820687/chrome_NP3LJVKJfM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1322x733/1440x798/filters:focal(661x367:662x368):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820687/chrome_NP3LJVKJfM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1322x733/1920x1065/filters:focal(661x367:662x368):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820687/chrome_NP3LJVKJfM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1322x733/2048x1136/filters:focal(661x367:662x368):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820687/chrome_NP3LJVKJfM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1322x733/2400x1331/filters:focal(661x367:662x368):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820687/chrome_NP3LJVKJfM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1322x733/2400x1331/filters:focal(661x367:662x368):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820687/chrome_NP3LJVKJfM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>The Digits supercomputer specs.</em></figcaption> <p><cite>Image: Nvidia</cite></p></div></div><p>Users will also get access to Nvidia’s AI software library, including development kits, orchestration tools, and pre-trained models available through the Nvidia NGC catalog. The system runs on Linux-based Nvidia DGX OS and supports popular frameworks like PyTorch, Python, and Jupyter notebooks. Developers can fine-tune models using the Nvidia NeMo framework and accelerate data science workflows with Nvidia RAPIDS libraries.</p><p>Users can develop and test their AI models locally on Project Digits, then deploy them to cloud services or data center infrastructure using the same Grace Blackwell architecture and Nvidia AI Enterprise software platform.</p><p>Nvidia offers a range of similar devices in the same <a href="https://www.wsj.com/tech/ai/nvidia-introduces-device-aimed-at-small-companies-hobbyists-3972ae41">accessibility</a> style — in December, it announced a $249 version of its Jetson computer for AI applications, targeting hobbyists and startups, called the Jetson Orin Nano Super (it handles models <a href="https://gigazine.net/gsc_news/en/20241218-nvidia-jetson-orin-nano-super/?utm_source=chatgpt.com">up to 8 billion parameters</a>).</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First time a Blender-made production has won the Golden Globe (284 pts)]]></title>
            <link>https://variety.com/2025/film/columns/flow-golden-globe-win-independent-animation-1236266805/</link>
            <guid>42620656</guid>
            <pubDate>Tue, 07 Jan 2025 09:00:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://variety.com/2025/film/columns/flow-golden-globe-win-independent-animation-1236266805/">https://variety.com/2025/film/columns/flow-golden-globe-win-independent-animation-1236266805/</a>, See on <a href="https://news.ycombinator.com/item?id=42620656">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
	In a major upset, Latvian director <a href="https://variety.com/t/gints-zilbalodis/" id="auto-tag_gints-zilbalodis" data-tag="gints-zilbalodis">Gints Zilbalodis</a>‘ “<a href="https://variety.com/t/flow/" id="auto-tag_flow" data-tag="flow">Flow</a>” won the Golden Globe for best animated feature on Sunday evening. A co-production between Latvia, Belgium, and France with a modest $3.7 million budget, “Flow” made waves simply by earning a nomination in a category traditionally dominated by big-budget studio films.</p>



<p>
	While it’s handled by Sideshow and Janus Films whose track record includes the Oscar-winning “Drive My Car,” “Flow” faced an uphill battle in the Golden Globe race, competing against two Disney/Pixar blockbusters (“Inside Out 2” and “Moana 2”), a DreamWorks feature by animation legend Chris Sanders (“The Wild Robot”), a big-budget Netflix production from Oscar-winning Aardman Animations (“Wallace &amp; Gromit: Vengeance Most Fowl”) and Oscar-winning director Adam Elliot’s “Memoirs of a Snail.”

	</p>






<p>
	In the film, a biblical flood has submerged everything in its path, including Cat’s home. There are no humans to be found anywhere, although their material legacy remains. Luckily for “the “Flow’s” feline protagonist, it finds refuge on a boat full of other presumably homeless animals. Together, the group sets sail on the flood waters.</p>



<p>
	“Flow’s” victory reflects how inclusive the Golden Globes have become towards independent and international titles in recent years, especially when compared to other major Hollywood awards ceremonies.</p>



<p>
	Last year, Makoto Shinkai’s “Suzume” earned a surprise nomination, and the year before, Masaaki Yuasa’s “Inu-Oh” made an unexpected but welcome appearance alongside the charming “Marcel the Shell with Shoes On” by Dean Fleischer-Camp. This year, Elliot’s Australian stop-motion film “Memoirs of a Snail” also earned a nomination.</p>



<p>
	“Flow” didn’t come from nowhere, either. Represented internationally by animation experts at Paris-based Charades (“Mirai,” “I Lost My Body,” “Chicken for Linda!”), the movie had an impeccable festival run that kicked off at Cannes, where it world premiered in Un Certain Regard and went on to win awards at Annecy, Ottawa, Guadalajara, and Melbourne film fests. The film has also already scored wins at the European Film Awards, Los Angeles Film Critics Association Awards and New York Film Critics Circle Awards. On Jan. 12, it will compete for a Critics Choice Award.

	</p>




<p>
	“Flow’s” Golden Globe award is also a win for the democratization of the animation process. The film was created using Blender, a free, open-source software widely used by independent and amateur animators. This marks the first time a Blender-made production has won the Golden Globe for animated feature, proving that major success in the medium is, at least theoretically, open to all creators, not just those working in the studio system.</p>



<p>
	During his acceptance speech, Zilbalodis acknowledged the significance of his little film winning the award, not only for those involved but for the entire Latvian film industry. “This film is made by a very small young but very passionate team in a place where there isn’t a big film industry. So this is the first time that a film from Latvia has been here, so this is huge for us,” he explained.</p>



<p>
	The 31-year-old director went on to briefly discuss how the production of this film differed from his previous works and how both its on-screen and behind-the-scenes messages are especially poignant in 2025.</p>



<p>
	“This is a very personal story to me because I used to work alone. I made all my films by myself, but this time, I worked with a team, and just like the cat in ‘Flow,’ I had to learn to trust others and learn how to collaborate and overcome our differences. I think it’s very important to remember this nowadays,” he added.</p>



<p>
	Looking ahead, “Flow’s” unexpected Golden Globe win has opened up the race for the animated feature Oscar. With no clear frontrunner, pundits have previously favored “The Wild Robot” and “Wallace &amp; Gromit ” — the latter being <em>Variety</em>‘s <a href="https://variety.com/lists/2025-oscars-predictions/best-picture-6/">pick </a>as of Jan. 3 — over the more commercial or independent international contenders. A Golden Globe win for “Flow” may change some minds in that regard. </p>



<p>
	If “Flow” or any non-Disney film were to win the Oscar, it would signal the longest drought in the category for Disney. Since the creation of the animated feature Oscar in 2001, Disney/Pixar has never gone more than two years without winning. Its last win came in 2021 with “Encanto,” but the past two years saw victories for “Guillermo del Toro’s Pinocchio” (2022) and Hayao Miyazaki’s “The Boy and the Heron” (2023).</p>
















</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stay Gold, America (172 pts)]]></title>
            <link>https://blog.codinghorror.com/</link>
            <guid>42620278</guid>
            <pubDate>Tue, 07 Jan 2025 07:45:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.codinghorror.com/">https://blog.codinghorror.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42620278">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <header id="gh-navigation">
    <div>

        <p><a href="https://blog.codinghorror.com/">
                    <img src="https://blog.codinghorror.com/content/images/2025/01/codinghorror-landscape.png" alt="Coding Horror">
            </a>
                        
        </p>

        

        

    </div>
</header>

    

    


<div>

            <h2>
                Archive
            </h2>

        <main>
            <div>

                                <article>

    <header>

        <h2>
            <a href="https://blog.codinghorror.com/stay-gold-america/">Stay Gold, America</a>
        </h2>
            <p>We are at an unprecedented point in American history, and I'm concerned we may lose sight of the American Dream.</p>

            <div>
                <p><a href="https://blog.codinghorror.com/author/jeff-atwood/">
                                <img src="https://blog.codinghorror.com/assets/images/codinghorror.png" alt="Jeff Atwood">
                            </a>
                </p>
                <div>
                    
                    <p><time datetime="2025-01-07">07 Jan 2025</time>
                            <span><span>—</span> 10 min read</span>
                    </p>
                </div>
            </div>

        
    </header>

    <section>
        <p>We are at an unprecedented point in American history, and I'm concerned we may lose sight of the <a href="https://en.wikipedia.org/wiki/American_Dream?ref=blog.codinghorror.com" rel="noreferrer">American Dream</a>:</p><ul><li>The costs of housing, healthcare, and education have soared <a href="https://uvaro.com/blog/college-costs-vs-inflation-1982-present?ref=blog.codinghorror.com" rel="noreferrer">far beyond the pace of inflation and wage growth</a>.</li><li>We are a democracy, but 144 million Americans – 42% of the adults who live here – do not vote and have no say in what happens.</li><li><a href="https://youtu.be/QPKKQnijnsM?ref=blog.codinghorror.com" rel="noreferrer">Wealth concentration</a> has reached <a href="https://americancompass.org/economic-inequality-guide/?ref=blog.codinghorror.com" rel="noreferrer">historic levels</a>. The top 1% of households control 32% of all wealth, while the bottom 50% only have 2.6%.</li></ul><p>We must act now to keep the dream alive. Our family made <strong>eight $1 million donations</strong> to nonprofit groups working to support those most currently in need:</p><ul><li><a href="https://teamrubiconusa.org/?ref=blog.codinghorror.com" rel="noreferrer">Team Rubicon</a> – Mobilizing veterans to continue their service, leveraging their skills and experience to help Americans prepare, respond, and recover from natural disasters.</li><li><a href="https://childrenshungerfund.org/?ref=blog.codinghorror.com" rel="noreferrer">Children's Hunger Fund</a> – Provides resources to local churches in the United States and around the world to meet the needs of impoverished community members.</li><li><a href="https://pen.org/?ref=blog.codinghorror.com">PEN America</a> – Defends writers against censorship and abuse, supports writers in need of emergency assistance, and amplifies the writing of incarcerated prisoners. (One of my personal favorites; I've seen the power of writing transform our world <a href="https://www.weforum.org/stories/2016/03/10-novels-that-changed-the-world/?ref=blog.codinghorror.com" rel="noreferrer">many times</a>.)</li><li><a href="https://www.thetrevorproject.org/?ref=blog.codinghorror.com" rel="noreferrer">The Trevor Project</a> – Working to change hearts, minds, and laws to support the lives of young adults seeking acceptance as fellow Americans.</li><li><a href="https://www.naacpldf.org/?ref=blog.codinghorror.com" rel="noreferrer">NAACP Legal Defense and Educational Fund</a> – Legal organization with a historic record of advancing racial justice and reducing inequality.</li><li><a href="https://www.firstgenerationinvestors.com/?ref=blog.codinghorror.com" rel="noreferrer">First Generation Investors</a> –<strong> </strong>Introduces high school students in low-income areas to the fundamentals of investing, providing them real money to invest, encouraging long-term wealth accumulation and financial literacy among underserved youth.</li><li><a href="https://www.globalrefuge.org/?ref=blog.codinghorror.com" rel="noreferrer">Global Refuge</a> – Supporting migrants and refugees from around the globe, in partnership with community-based legal and social service providers nationwide, helping rebuild lives in America.</li><li><a href="https://www.plannedparenthood.org/?ref=blog.codinghorror.com" rel="noreferrer">Planned Parenthood</a> – Provides essential healthcare services and resources that help individuals and families lead healthier lives.</li></ul><p>I encourage every American to<strong> contribute soon, however you can, to organizations you feel are </strong><a href="https://www.charitynavigator.org/?ref=blog.codinghorror.com" rel="noreferrer"><strong>effectively helping</strong></a><strong> </strong>those most currently in need here in America.</p><p>We must also<em> </em>work toward deeper changes that will take decades to achieve. Over the next five years,<strong> my family pledges half our wealth </strong>towards long term efforts ensuring that all Americans continue to have access to the American Dream. </p><figure><img src="https://blog.codinghorror.com/content/images/2025/01/share-vertical-1.png" alt="" loading="lazy" width="328" height="500"></figure><div><p>I never thought my family would be able to do this. My parents are of hardscrabble rural West Virginia and rural North Carolina origins. They barely managed to claw their way to the bottom of the middle class by the time they ended up in Virginia. Unfortunately, due to the demons passed on to them by their parents, my father was an alcoholic and my mother participated in the drinking. She ended up divorcing my father when I was 16 years old. It was only after the divorce that my parents were able to heal themselves, heal their only child, and stop the drinking, which was so destructive to our family. If the divorce hadn't forced the issue, alcohol would have inevitably destroyed us all. </p><p>My parents may not have done everything right, but they both unconditionally loved me. <strong>They taught me how to fully, deeply receive love, and the profound joy of reflecting that love upon everyone around you. </strong></p></div><p>I went on to attend public school in Chesterfield County, Virginia. In 1992 I graduated from the University of Virginia, founded by <a href="https://en.wikipedia.org/wiki/Thomas_Jefferson?ref=blog.codinghorror.com">Thomas Jefferson</a>.</p><p>During college, I worked at Safeway as a part-time cashier, earning the <a href="https://www.dol.gov/agencies/whd/minimum-wage/history?ref=blog.codinghorror.com" rel="noreferrer">federal minimum wage</a>, scraping together whatever money I could through government Pell grants, scholarships, and other part-time work to pay my college tuition. Even with <a href="https://www.salliemae.com/blog/in-state-vs-out-of-state-tuition/?ref=blog.codinghorror.com" rel="noreferrer">lower in-state tuition</a>, it was rocky. Sometimes I could barely manage tuition payments. And that was in 1992, when tuition was only $3,000 per year. It is now $23,000 per year. <a href="https://educationdata.org/average-cost-of-college-by-year?ref=blog.codinghorror.com" rel="noreferrer">College tuition</a> at a state school increased by 8 times over the last 30 years. These <a href="https://uvaro.com/blog/college-costs-vs-inflation-1982-present?ref=blog.codinghorror.com" rel="noreferrer">huge cost increases</a> for healthcare, education, and housing are not compatible with the American Dream.</p><figure><a href="https://uvaro.com/blog/college-costs-vs-inflation-1982-present?ref=blog.codinghorror.com"><img src="https://blog.codinghorror.com/content/images/2025/01/image-6.png" alt="" loading="lazy" width="675" height="700" srcset="https://blog.codinghorror.com/content/images/size/w600/2025/01/image-6.png 600w, https://blog.codinghorror.com/content/images/2025/01/image-6.png 675w"></a></figure><div><p>Programmers all over the world helped make an American Dream happen <a href="https://web.archive.org/web/20080703183923/http://stackoverflow.com/" rel="noreferrer">in 2008</a> when we built <a href="https://stackoverflow.com/?ref=blog.codinghorror.com" rel="noreferrer">Stack Overflow</a>, a Q&amp;A website for programmers creating a shared <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.en?ref=blog.codinghorror.com" rel="noreferrer">Creative Commons</a> knowledge base for the world. We did it democratically, because that's the <a href="https://en.wikipedia.org/wiki/American_way?ref=blog.codinghorror.com" rel="noreferrer">American way</a>. We voted to rank questions and answers, and held elections for community moderators using <a href="https://en.m.wikipedia.org/wiki/Ranked-choice_voting_in_the_United_States?ref=blog.codinghorror.com" rel="noreferrer">ranked choice voting</a>. We built a <strong>digital democracy</strong> – of the programmers, by the programmers, for the programmers. <a href="https://www.forbes.com/sites/vijaygurbaxani/2021/06/08/the-18-billion-acquisition-of-stack-overflow-aims-to-turbocharge-the-worlds-software-knowhow/?ref=blog.codinghorror.com" rel="noreferrer">It worked</a>. </p><p>With the guidance of my co-founder <a href="https://en.wikipedia.org/wiki/Joel_Spolsky?ref=blog.codinghorror.com" rel="noreferrer">Joel Spolsky</a>, I came to understand that the digital democracy of Stack Overflow was not enough. We must be brave enough to actively, openly share love with each other. That became the foundation for <a href="https://discourse.org/?ref=blog.codinghorror.com">Discourse</a>, a free, open source tool for constructive, empathetic community discussions that are also Creative Commons. We can disagree in those discussions because Discourse empowers communities to <a href="https://frameshiftconsulting.com/2017/09/10/the-intolerable-speech-rule-the-paradox-of-tolerance-for-tech-companies?ref=blog.codinghorror.com" rel="noreferrer">set boundaries the community agrees on</a>, providing tools to democratically govern and strongly moderate by enforcing these boundaries. <strong>Digital democracy <em>and empathy</em>,</strong> for everyone.</p></div><p>In order for digital democracy to work, we need to see each other through our screens.</p><figure><a href="https://xkcd.com/438/?ref=blog.codinghorror.com"><img src="https://blog.codinghorror.com/content/images/2025/01/xkcd438_condensed_b.png" alt="" loading="lazy" width="382" height="253"></a></figure><p><a href="https://blog.codinghorror.com/they-have-to-be-monsters/"><u>We often behave online in ways we never would in the real world</u></a> because we cannot see the person on the other side of the screen. But as our world becomes more digital, we must extend our kindness through that screen.</p><p>I've always felt Stack Overflow and Discourse are <a href="https://www.bcorporation.net/en-us/?ref=blog.codinghorror.com" rel="noreferrer">projects for the public good</a> that happen to be <a href="https://en.wikipedia.org/wiki/Company?ref=blog.codinghorror.com" rel="noreferrer">corporations</a>. I probably couldn't have accomplished this in any other country, and I was rewarded handsomely for a combination of hard work and good luck. That's what the <a href="https://www.britannica.com/topic/American-Dream?ref=blog.codinghorror.com" rel="noreferrer">American Dream</a> promises us. </p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/mXBMqbWcqzg?start=165&amp;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="Field of Dreams (James Earl Jones Scene) | People Will Come in 4K HDR"></iframe></figure><p><a href="https://blog.codinghorror.com/the-field-of-dreams-strategy/"><u>We built it, and<em> people came</em></u></a>. I earned millions of dollars. I thought that was the final part of the American Dream. But it wasn't.</p><p>I recently attended a theater performance of <a href="https://en.wikipedia.org/wiki/The_Outsiders_(novel)?ref=blog.codinghorror.com">The Outsiders</a> at my son's public high school. All I really knew was the famous <a href="https://www.youtube.com/watch?v=f46JMzVzSB4&amp;ref=blog.codinghorror.com" rel="noreferrer">"stay gold" line</a> from the 1983 movie adaptation. But as I sat there in the audience among my neighbors, watching the complete story acted out in front of me by these teenagers, I slowly realized what staying gold actually meant: <a href="http://blog.asjournal.org/the-american-dream-reconsidered-the-outsiders-1967/?ref=blog.codinghorror.com" rel="noreferrer"><em>sharing</em> the American Dream</a>. </p><p>In the printed program, the director wrote:</p><blockquote>This play is a reminder that strength lies not just in overcoming hardships but in staying true to ourselves and lifting up those around us. <p>We hope you feel the raw emotions, sense the camaraderie, and <strong>connect with the enduring themes of resilience, empathy, and unity</strong>. Whether you've read this story recently, long ago, or not at all, I hope you are able to find inspiration in the strength and passion of youth. Thank you for being part of this journey with us.</p><p><a href="https://poets.org/poem/nothing-gold-can-stay?ref=blog.codinghorror.com">Stay gold</a>.</p></blockquote><p>I <a href="https://blog.codinghorror.com/im-loyal-to-nothing-except-the-dream/" rel="noreferrer">believe deeply</a> in sharing The American Dream. It is the foundation of our country, the second paragraph in our <a href="https://www.archives.gov/founding-docs/declaration-transcript?ref=blog.codinghorror.com" rel="noreferrer">Declaration of Independence</a>, written by the founder of the public university I attended:</p><blockquote>We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.</blockquote><p>But the American Dream is not always available to every American. Its meaning can be distorted. Jimi Hendrix <a href="https://www.newyorker.com/culture/cultural-comment/rewinding-jimi-hendrixs-national-anthem?ref=blog.codinghorror.com" rel="noreferrer">captured this distortion</a> so eloquently in his rendition of our national anthem. </p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/sjzZh6-h9fM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="Jimi Hendrix - The Star Spangled Banner [ National Anthem ] ( Live at Woodstock 1969 )"></iframe></figure><p>We are still trying to live up to those ideals today. In November 2024, <a href="https://en.wikipedia.org/wiki/2024_United_States_presidential_election?ref=blog.codinghorror.com" rel="noreferrer">enough of us voted</a> for people who interpret the dream in a way that I don't understand.</p><figure><a href="https://election.lab.ufl.edu/2024-general-election-turnout/?ref=blog.codinghorror.com"><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfe_wqbzOe_elPxK-9TqRLDAxtI2Qrd8GHAtxqtJ8DvQ87d69tFasQnClpWuRYvDP-Iofd-PDkfpQU5RayMlyDo2rlVZWzfacm-kr1aM8fZprnh_1XXkpbr63MbGUpiNNaibPmxVQ?key=gIYEBDEiC9EgZC24CSOXGvAl" alt="" loading="lazy" width="661" height="573"></a></figure><p>34% of adults in America <a href="https://apnews.com/projects/election-2024-our-very-complicated-democracy/election-2024-why-americans-dont-vote-episode-6.html?ref=blog.codinghorror.com" rel="noreferrer">did not exercise their right to vote</a>. Why? Is it <a href="https://en.wikipedia.org/wiki/Voter_suppression_in_the_United_States?ref=blog.codinghorror.com" rel="noreferrer">voter suppression</a>, <a href="https://en.wikipedia.org/wiki/Gerrymandering?ref=blog.codinghorror.com" rel="noreferrer">gerrymandering</a> causing indifference, or people who felt their vote didn't matter? The 7.6% that are ineligible to vote are mostly adults living in America who have not managed to attain citizenship, or people <a href="https://www.ncsl.org/elections-and-campaigns/felon-voting-rights?ref=blog.codinghorror.com" rel="noreferrer">convicted of a felony</a>. Whatever the reasons, 42% of adults living in America had no say in the 2024 election. The vote <a href="https://election.lab.ufl.edu/2024-general-election-turnout/?ref=blog.codinghorror.com" rel="noreferrer">failed to represent everyone</a>.</p><div><p>I think many of the Americans who did vote are telling us they no longer believe our government is effectively <em>keeping America fair for everyone</em>. Our status as the <a href="https://thefulcrum.us/ethics-leadership/democracy-index?ref=blog.codinghorror.com" rel="noreferrer">world's leading democracy</a> is in question. We should make it easier for more eligible Americans to vote, such as <strong>making election day a </strong><a href="https://www.actforamerica.org/act-now/Make-Election-Day-a-National-Holiday?ref=blog.codinghorror.com" rel="noreferrer"><strong>national holiday</strong></a><strong>, universal </strong><a href="https://tracker.votingrightslab.org/issues/universal-mailed-ballots?ref=blog.codinghorror.com#issues_map" rel="noreferrer"><strong>mail in voting</strong></a><strong>, and adopting </strong><a href="https://fairvote.org/our-reforms/ranked-choice-voting/?ref=blog.codinghorror.com" rel="noreferrer"><strong>ranked choice voting</strong></a> so all votes carry more weight. We should also strengthen institutions keeping democracy fair for everyone, such as state and local election boards, as well as the Federal Election Commission.</p><p>It was only after I attained the dream that I was able to <a href="https://www.youtube.com/watch?v=QPKKQnijnsM&amp;ref=blog.codinghorror.com" rel="noreferrer">fully see how many Americans have so very little</a>. This much wealth starts to unintentionally distance my family from other Americans. I no longer bother to look at how much items cost, because <em>I don't have to</em>. We don't have to think about all these things that are challenging or unreachable for so many others. The more wealth you attain, the more unmistakably clear it becomes how unequal life is for so many of us.</p><p>Even with the wealth I have, I can't imagine what it would feel like <a href="https://www.theatlantic.com/politics/archive/2024/11/democrats-harris-billionaire-mistake/680779/?ref=blog.codinghorror.com" rel="noreferrer">to be a billionaire</a><em>.</em> <strong>It is, for lack of a better word, </strong><a href="https://www.pewresearch.org/social-trends/2020/01/09/trends-in-income-and-wealth-inequality/?ref=blog.codinghorror.com" rel="noreferrer"><strong>unamerican</strong></a><strong>. </strong></p></div><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/QPKKQnijnsM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="Wealth Inequality in America"></iframe></figure><div><p>In 2012, the top 1% of Americans held 24% of our country's wealth. By 2021, the top 1% of Americans held 30%.  So many have so little, while a tiny few have massive, wildly disproportionate wealth, which keeps growing. Now the global top 1% hold nearly twice as much wealth <a href="https://www.oxfam.org/en/press-releases/richest-1-bag-nearly-twice-much-wealth-rest-world-put-together-over-past-two-years?ref=blog.codinghorror.com" rel="noreferrer">as the rest of the world combined</a>.</p><p>I grew up poor in America, inspired by the promise of the American Dream that I could better myself and my family by building things <a href="https://credo.library.umass.edu/view/full/mums312-b203-i031?ref=blog.codinghorror.com" rel="noreferrer">that <em>mattered</em></a>:</p></div><blockquote>Work is service, not gain. The object of work is life, not income. The reward of production is plenty, not private fortune. We should measure the prosperity of a nation not by the number of millionaires, but by<strong> the absence of poverty, the prevalence of health, the efficiency of the public schools, </strong>and the number of people who can and do read worthwhile books<strong>.</strong></blockquote><p>Our <a href="https://en.wikipedia.org/wiki/Capitalism?ref=blog.codinghorror.com#Types" rel="noreferrer">version of capitalism</a> delivered so much wealth to my family for my hard work in co-founding two successful companies. My partner and I gladly paid our <a href="https://en.wikipedia.org/wiki/Taxation_in_the_United_States?ref=blog.codinghorror.com" rel="noreferrer">full taxes</a>, and we always planned to give most of our remaining wealth to charities when we pass, following the <a href="https://givingpledge.org/pledger?pledgerId=177&amp;ref=blog.codinghorror.com" rel="noreferrer">Warren Buffet Philanthropic Pledge</a>:</p><blockquote>More than 99% of my wealth will go to philanthropy during my lifetime or at death.</blockquote><p>I admire Buffett, but even having only a tiny fraction of his $325 billion fortune, to me this pledge was incomplete. When would this wealth be transferred? </p><p>Last year he <a href="https://amp.cnn.com/cnn/2024/06/28/investing/warren-buffett-gates-donation?ref=blog.codinghorror.com" rel="noreferrer">amended the pledge</a>, giving all his wealth at death to a charitable trust run by his children, aged 71, 69, and 66, who <a href="https://www.axios.com/2024/07/01/warren-buffett-pledge-100-billion?ref=blog.codinghorror.com" rel="noreferrer">do not make for natural charitable bedfellows</a>. I am only holding back enough wealth for my children so they can afford college educations and buy a home. I am compelled to, because being a parent is <a href="https://blog.codinghorror.com/on-parenthood/" rel="noreferrer">the toughest job I've ever had</a>, and I am concerned about their future.</p><p>November 5th <a href="https://en.wikipedia.org/wiki/2024_United_States_presidential_election?ref=blog.codinghorror.com" rel="noreferrer">raised the stakes</a>. It is now time to allocate <strong>half the wealth</strong> I was so fortunate to be dealt within the next five years<strong>,</strong> not just for my own family, but for all my fellow Americans. </p><p>Our government seems to be slower and slower at delivering change due to the <a href="https://carnegieendowment.org/research/2023/09/polarization-democracy-and-political-violence-in-the-united-states-what-the-research-says?lang=en&amp;ref=blog.codinghorror.com" rel="noreferrer">increased polarization of our two party system</a>. The last meaningful constitutional amendment we've managed to pass in the last 60 years was the 26th amendment in 1971, lowering the voting age to 18 and giving more people a voice in our democracy. </p><p>Political polarization is at <a href="https://en.wikipedia.org/wiki/Political_polarization_in_the_United_States?ref=blog.codinghorror.com#Gilded_Age" rel="noreferrer">historically high levels</a> and rising. In a two party system, this <a href="https://blogs.cfainstitute.org/investor/2018/03/13/red-states-blue-states-two-economies-one-nation/?ref=blog.codinghorror.com" rel="noreferrer">level of polarization</a> is counterproductive and even dangerous.<strong> Do we all still believe in the same American Dream? </strong></p><figure><a href="https://www.facinghistory.org/resource-library/political-polarization-united-states?ref=blog.codinghorror.com"><img src="https://blog.codinghorror.com/content/images/2025/01/image-9.png" alt="" loading="lazy" width="739" height="500" srcset="https://blog.codinghorror.com/content/images/size/w600/2025/01/image-9.png 600w, https://blog.codinghorror.com/content/images/2025/01/image-9.png 739w" sizes="(min-width: 720px) 720px"></a></figure><p>I've always loved <a href="https://blog.codinghorror.com/im-loyal-to-nothing-except-the-dream/" rel="noreferrer">the ideals</a> behind the American Dream, though we continually struggle to live up to them. They are worth fighting for, even if it means making <a href="https://blogs.loc.gov/loc/2020/07/remembering-john-lewis-the-power-of-good-trouble/?ref=blog.codinghorror.com" rel="noreferrer">"good trouble"</a>. We must come together and believe in our shared American Dream so deeply that we can improve our democracy... but <a href="https://www.rogerebert.com/reviews/great-movie-the-night-of-the-hunter-1955?ref=blog.codinghorror.com" rel="noreferrer">which dream?</a></p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/QC6gI4gXYPw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="Do the Right Thing | Radio Raheem's Story of LOVE and HATE"></iframe></figure><p>The American Dream contains the <a href="https://www.goodreads.com/quotes/376797-returning-hate-for-hate-multiplies-hate-adding-deeper-darkness-to?ref=blog.codinghorror.com" rel="noreferrer">path of hate</a>, and the <a href="https://kinginstitute.stanford.edu/king-papers/documents/loving-your-enemies-sermon-delivered-dexter-avenue-baptist-church?ref=blog.codinghorror.com" rel="noreferrer">path of love</a>. Throughout our history, one hand is always fighting the other. Which path are we choosing?</p><blockquote>Our family pledges <strong>half our wealth</strong> toward an <strong>American Dream founded on love</strong>.</blockquote>
<!--kg-card-begin: html-->
<!-- CSS for BlockQuote -->

<!--kg-card-end: html-->
<p>Here are some starting points for longer term efforts:</p><ul><li>We can support organizations making it easier for Americans to vote for a new Congress in two years and a new president in four years. My concern is damage to our democratic institutions may happen so quickly that our votes could matter even less within the coming years.</li><li>We could fund nonprofits that have a <a href="https://www.charitynavigator.org/about-us/our-methodology/ratings/?ref=blog.codinghorror.com" rel="noreferrer">proven track record</a> of protecting democratic institutions.</li><li>We could found a new organization loosely based on the original <a href="https://en.wikipedia.org/wiki/RAND_Corporation?ref=blog.codinghorror.com" rel="noreferrer">RAND Corporation</a>, but modernized like <a href="https://www.leverforchange.org/?ref=blog.codinghorror.com" rel="noreferrer">Lever for Change</a>. We can empower the best and brightest to determine a realistic, achievable path toward preserving the American Dream for <em>everyone,</em> working within the current system or outside it.</li><li>All states are shades of purple, not fully red or blue. We have <a href="https://today.yougov.com/politics/articles/50343-national-policy-proposals-with-bipartisan-support?ref=blog.codinghorror.com" rel="noreferrer">more in common on specific policies</a> than we realize. It would be very difficult to draw borders if we split. I know what divorce feels like, and we don't want this. Let's come together <a href="https://carnegieendowment.org/research/2023/09/polarization-democracy-and-political-violence-in-the-united-states-what-the-research-says?lang=en&amp;ref=blog.codinghorror.com">through our shared American Dream</a>. </li><li>We can start with change in our local communities. Vote in your own city, county, and state elections. Support local independent journalism and media. Find a local organization doing work you admire, ask what they need, and help them meet those needs. Listen to the stories of fellow volunteers, listen to the stories of the people you’re serving – that is the heart of Democracy.</li></ul><p>We've already completed the eight $1 million donations <a href="#" rel="noreferrer">listed above</a> to help those most immediately in need. Within the next five years, half of our family wealth will support longer term efforts. There is no single solution, so let's work together. I will gladly advise and <strong>empower others </strong>working towards the same goal.</p><figure><img src="https://blog.codinghorror.com/content/images/2025/01/share-landscape.png" alt="" loading="lazy" width="1235" height="690" srcset="https://blog.codinghorror.com/content/images/size/w600/2025/01/share-landscape.png 600w, https://blog.codinghorror.com/content/images/size/w1000/2025/01/share-landscape.png 1000w, https://blog.codinghorror.com/content/images/2025/01/share-landscape.png 1235w" sizes="(min-width: 720px) 720px"></figure><p>Please join us in <strong>Sharing the American Dream</strong>:</p><ol><li>Support organizations you feel are <a href="https://www.charitynavigator.org/?ref=blog.codinghorror.com" rel="noreferrer">effectively helping</a> those most in need across America right now.</li><li>Within the next five years, also contribute <strong>public dedications of time or funds towards longer term efforts </strong>to keep the American Dream fair and attainable for all our children.</li></ol><p><a href="https://poets.org/poem/nothing-gold-can-stay?ref=blog.codinghorror.com" rel="noreferrer">Stay gold</a>, America.</p>
<!--kg-card-begin: html-->
<small>(I could not have done this without the support of my partner Betsy Burton and the rest of my family. I'd also like to thank <a href="https://en.wikipedia.org/wiki/Steve_McConnell?ref=blog.codinghorror.com">Steve McConnell</a>, whose writing inspired me to start this blog in 2004. So many people from all walks of life generously shared their feedback to improve this post. <a href="https://blog.codinghorror.com/on-the-meaning-of-coding-horror/">We wrote it together</a>. Thank you all.)</small>
<!--kg-card-end: html-->

        
    </section>
</article>
                                <article>

    <header>

        <h2>
            <a href="https://blog.codinghorror.com/the-great-filter-comes-for-us-all/">The Great Filter Comes For Us All</a>
        </h2>

            <div>
                <p><a href="https://blog.codinghorror.com/author/jeff-atwood/">
                                <img src="https://blog.codinghorror.com/assets/images/codinghorror.png" alt="Jeff Atwood">
                            </a>
                </p>
                
            </div>

        
    </header>

    <section>
        <p>With a <a href="https://imagine.gsfc.nasa.gov/science/featured_science/tenyear/age.html?ref=blog.codinghorror.com" rel="noreferrer">13 billion year head start</a> on evolution, why haven't any other forms of life in the universe contacted us by now?</p><figure><a href="https://en.wikipedia.org/wiki/Arrival_(film)?ref=blog.codinghorror.com"><img src="https://blog.codinghorror.com/content/images/2024/11/image-28.png" alt="" loading="lazy" width="1024" height="692" srcset="https://blog.codinghorror.com/content/images/size/w600/2024/11/image-28.png 600w, https://blog.codinghorror.com/content/images/size/w1000/2024/11/image-28.png 1000w, https://blog.codinghorror.com/content/images/2024/11/image-28.png 1024w" sizes="(min-width: 720px) 720px"></a><figcaption><span>teaching the aliens how to exit Vim</span></figcaption></figure><p>(<a href="https://en.wikipedia.org/wiki/Arrival_(film)?ref=blog.codinghorror.com" rel="noreferrer">Arrival</a> is a <a href="https://www.youtube.com/watch?v=tFMo3UJ4B4g&amp;ref=blog.codinghorror.com" rel="noreferrer">fantastic movie</a>. Watch it, but don't stop there - read the <a href="https://en.wikipedia.org/wiki/Story_of_Your_Life?ref=blog.codinghorror.com" rel="noreferrer">Story of Your Life</a> novella it was based on for so much additional nuance.)</p><p>This is called the <a href="https://en.wikipedia.org/wiki/Fermi_paradox?ref=blog.codinghorror.com" rel="noreferrer">Fermi paradox</a>: </p>
<!--kg-card-begin: html-->
<blockquote>The Fermi Paradox is a contradiction between high estimates of the probability of the existence of extraterrestrial civilizations, such as in <a href="https://en.wikipedia.org/wiki/Drake_equation?ref=codinghorror.obox.agency" rel="noopener noreferrer" target="_blank">the Drake equation</a>, and lack of any evidence for such civilizations.<ul>
<li>There are billions of stars in the galaxy that are similar to the Sun including many billions of years older than Earth.</li>
<li>With high probability, some of these stars will have Earth-like planets, and if the Earth is typical, some might develop intelligent life.</li><li>Some of these civilizations might develop interstellar travel, a step the Earth is investigating now.</li><li>Even at the slow pace of currently envisioned interstellar travel, the Milky Way galaxy could be completely traversed in about a million years.<p>According to this line of thinking, <strong>the Earth should have already been visited by extraterrestrial aliens</strong>. In an informal conversation, Fermi noted no convincing evidence of this, nor any signs of alien intelligence anywhere in the observable universe, leading him to ask, “Where is everybody?”</p></li></ul>
</blockquote>
<!--kg-card-end: html-->
<blockquote>To me, this is a compelling argument, in the same way that <a href="https://en.wikipedia.org/wiki/Time_travel?ref=blog.codinghorror.com#Absence_of_time_travelers_from_the_future" rel="noreferrer">the lack of evidence of any time travellers</a> is:</blockquote><blockquote>Many have argued that the absence of time travelers from the future demonstrates that such technology will never be developed, suggesting that it is impossible. This is analogous to the Fermi paradox related to the absence of evidence of extraterrestrial life. As the absence of extraterrestrial visitors does not categorically prove they do not exist, so the absence of time travelers fails to prove time travel is physically impossible; it might be that time travel is physically possible but is never developed or is cautiously used. Carl Sagan once suggested the possibility that time travelers could be here but are disguising their existence or are not recognized as time travelers.</blockquote><p>It seems, to me at least, clear evidence that time travel is not possible, given the enormous amount of time behind us. Something, somewhere, would certainly have invented it by now... right?</p><p>So if not, what happened? <a href="https://en.wikipedia.org/wiki/Great_Filter?ref=blog.codinghorror.com" rel="noopener nofollow ugc">The Great Filter</a>&nbsp;maybe?</p><blockquote>The Great Filter theory says that at some point from pre-life to Type III intelligence, there’s a wall that all or nearly all attempts at life hit. There’s some stage in that long evolutionary process that is extremely unlikely or impossible for life to get beyond. That stage is The Great Filter.</blockquote><p>I liked&nbsp;<a href="http://waitbutwhy.com/2014/05/fermi-paradox.html?ref=blog.codinghorror.com" rel="noopener nofollow ugc">Wait But Why’s take on this</a>&nbsp;a lot, which covers three main filter possibilities:</p><ol><li><strong>Life is extraordinarily rare, almost impossible</strong></li></ol><figure><img src="https://blog.codinghorror.com/content/images/2024/12/image-1.png" alt="" loading="lazy" width="1024" height="538" srcset="https://blog.codinghorror.com/content/images/size/w600/2024/12/image-1.png 600w, https://blog.codinghorror.com/content/images/size/w1000/2024/12/image-1.png 1000w, https://blog.codinghorror.com/content/images/2024/12/image-1.png 1024w" sizes="(min-width: 720px) 720px"></figure><ol start="2"><li><strong>We are not a rare form of life, but near the first to evolve</strong></li></ol><figure><img src="https://blog.codinghorror.com/content/images/2024/12/image-2.png" alt="" loading="lazy" width="1024" height="438" srcset="https://blog.codinghorror.com/content/images/size/w600/2024/12/image-2.png 600w, https://blog.codinghorror.com/content/images/size/w1000/2024/12/image-2.png 1000w, https://blog.codinghorror.com/content/images/2024/12/image-2.png 1024w" sizes="(min-width: 720px) 720px"></figure><ol start="3"><li><strong>Almost no life makes it to this point</strong></li></ol><figure><img src="https://blog.codinghorror.com/content/images/2024/12/image-3.png" alt="" loading="lazy" width="1024" height="528" srcset="https://blog.codinghorror.com/content/images/size/w600/2024/12/image-3.png 600w, https://blog.codinghorror.com/content/images/size/w1000/2024/12/image-3.png 1000w, https://blog.codinghorror.com/content/images/2024/12/image-3.png 1024w" sizes="(min-width: 720px) 720px"></figure><p>Those are three Great Filter possibilities, but the question remains: why are we so alone in the observable universe? I grant you that&nbsp;<a href="https://en.wikipedia.org/wiki/Observable_universe?ref=blog.codinghorror.com" rel="noopener nofollow ugc">what we&nbsp;<em>can</em>&nbsp;observe is appallingly tiny</a>&nbsp;given the unimaginable scale of the universe, so “what we can observe” may not be enough by many orders of magnitude.</p><p>I encourage you to <a href="http://waitbutwhy.com/2014/05/fermi-paradox.html?ref=blog.codinghorror.com" rel="noreferrer">read the entire article</a>, it's full of great ideas explained well, including many other Great Filter possibilites. Mostly I wanted to share <strong>my personal theory of why we haven't encountered alien life by now</strong>. Like computers themselves, things don't get larger. They get <em>smaller</em>. And <em>faster</em>. And so does intelligent life. </p><blockquote>Why build planet-size anything when the real action is in the small things? Small spaces, small units of time, <strong>everything gets smaller</strong>.</blockquote><p>Large is inefficient and unnecessary. Look at the history of computers: from giant to tiny and tinier. From slow to fast and faster. Personally, I have a feeling really advanced life eventually does away with all physical stuff that slows you down as soon as they can, and enters <a href="https://blog.codinghorror.com/the-infinite-space-between-words/" rel="noreferrer">the infinite spaces between</a>:</p><blockquote>This is, of course, a variant on the Fermi paradox: We don’t see clues to widespread, large-scale engineering, and consequently we must conclude that we’re alone. But the possibly flawed assumption here is when we say that highly visible construction projects are an inevitable outcome of intelligence.&nbsp;<strong>It could be that it’s the engineering of the small, rather than the large, that is inevitable.</strong>&nbsp;This follows from the laws of inertia (smaller machines are faster, and require less energy to function) as well as the speed of light (small computers have faster internal communication). It may be – and this is, of course, speculation – that advanced societies are building small technology and have little incentive or need to rearrange the stars in their neighborhoods, for instance. They may prefer to build nanobots instead.<p>— <a href="https://web.archive.org/web/20040604012520/http://www.usnews.com/usnews/tech/nextnews/archive/next031104.htm" rel="noreferrer">Seth Shostak</a></p></blockquote><p>Seth delivers an excellent TED talk on this topic as well:</p><figure></figure><p>If we can <a href="https://en.wikipedia.org/wiki/Observable_universe?ref=blog.codinghorror.com" rel="noreferrer">barely see far in the universe as is</a>, there's no way we could possibly see into the infinite space and time between.</p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/dJTU48_yghs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="Her Official Trailer #1 (2013) - Joaquin Phoenix, Scarlett Johansson Movie HD"></iframe></figure><p>That is of course just my opinion, but <a href="https://www.justwatch.com/us/movie/her?ref=blog.codinghorror.com" rel="noreferrer">we'll see</a>.. eventually.</p>
        
    </section>
</article>
                                <article>

    <header>

        <h2>
            <a href="https://blog.codinghorror.com/i-fight-for-the-users/">I Fight For The Users</a>
        </h2>

            <div>
                <p><a href="https://blog.codinghorror.com/author/jeff-atwood/">
                                <img src="https://blog.codinghorror.com/assets/images/codinghorror.png" alt="Jeff Atwood">
                            </a>
                </p>
                
            </div>

        
    </header>

    <section>
        <p>If you haven't been able to keep up with my blistering pace of one blog post per year, I don't blame you. There's a lot going on right now. It's a busy time. But let's pause and take a moment to celebrate that Elon Musk destroyed Twitter. I <a href="https://www.wired.com/story/god-did-us-a-favor-by-destroying-twitter/?ref=blog.codinghorror.com" rel="noreferrer">can't possibly say it better than Paul Ford</a> so I'll just refer you there:</p><blockquote>Every five or six minutes, someone in the social sciences publishes a PDF with a title like “Humans 95 Percent Happier in Small Towns, Waving at Neighbors and Eating Sandwiches.” When we gather in groups of more than, say, eight, it’s a disaster. Yet there is something fundamental in our nature that desperately wants to get everyone together in one big room, to “solve it.” Our smarter, richer betters (in Babel times, the king’s name was Nimrod) often preach the idea of a town square, a marketplace of ideas, a centralized hub of discourse and entertainment—and we listen. But when I go back and read Genesis, I hear God saying: “My children, I designed your brains to scale to 150 stable relationships. Anything beyond that is overclocking. You should all try Mastodon.”</blockquote><p>It's been clear for quite some time that the early social media strategery of "jam a million people in a colosseum and let them fight it out with free speech" isn't panning out, but never has it been more clear than now, under the Elon Musk regime, that <a href="https://www.rollingstone.com/culture/culture-features/elon-musk-isnt-funny-bad-jokes-twitter-1234712950/?ref=blog.codinghorror.com" rel="noreferrer">being beholden to the whims of a billionaire going through a midlife crisis</a> isn't exactly healthy for society. Or you. Or me. Or anyone, really.</p><p>I tried to be fair; I gave the post-Elon Twitter era a week, thinking "how bad could it possibly be?" and good lord, <em>it was so much worse than I could have possibly ever imagined</em>. It's like Elon read the Dilbert pointy-haired-manager book on management and bonked his head on every rung of the ladder going down, generating an ever-growing <a href="https://www.vanityfair.com/news/2022/04/elon-musk-twitter-terrible-things-hes-said-and-done?ref=blog.codinghorror.com" rel="noreferrer">laundry list of terrible things no manager should ever do</a>. And he <em>kept going!</em></p><p>It's undeniably sad. I really liked Twitter, warts and all, <a href="https://blog.codinghorror.com/twitter-service-vs-platform/" rel="noreferrer">from 2007 onward</a>. In fact, it was the only "social network" I liked at all. Even when it became clear in the Trump era that Twitter was unhealthy for human minds, I soldiered on, gleaning what I could. I'm not alone in that; Clay Shirky's moribund signoff at the end of 2022 reflected how I felt:</p><figure><img src="https://blog.codinghorror.com/content/images/2023/11/image.png" alt="" loading="lazy" width="1348" height="1495" srcset="https://blog.codinghorror.com/content/images/size/w600/2023/11/image.png 600w, https://blog.codinghorror.com/content/images/size/w1000/2023/11/image.png 1000w, https://blog.codinghorror.com/content/images/2023/11/image.png 1348w" sizes="(min-width: 720px) 720px"></figure><p>Indeed, Twitter was murdered at the whims of a billionaire high on Ketamine while it was (mostly) healthy, <a href="https://www.gbnews.com/news/world/elon-musk-x-twitter-purchase-daughter-jenna-musk-woke-virus?ref=blog.codinghorror.com" rel="noreferrer">because of the "trans woke virus"</a>.  </p><p>I urge you, all of you, <strong>to disavow Twitter and never look at it again</strong>. No one who cares about their mental health should be on Twitter at this point, or linking to Twitter and feeding it the attention it thrives on. We should entomb Twitter deep in concrete with this public warning on its capstone:</p><figure><img src="https://media.infosec.exchange/infosec.exchange/media_attachments/files/110/240/386/903/991/473/original/d3ff3462bf229383.png" alt="This place is not a place of honor...no highly esteemed deed is commemorated here ...nothing valued is here." loading="lazy"></figure><p>In the end, I begrudgingly realized, as did Paul Ford, that Elon unwittingly did us a favor by killing Twitter. He demonstrated <strong>the very real dangers of any platform run by a king, a dictator, a tyrant, a despot, an autocrat</strong>. You can have all your content rug-pulled out from under you at any time, or watch in horror as your favorite bar... slowly transforms into a nazi bar. </p><figure><img src="https://blog.codinghorror.com/content/images/2023/11/image-1.png" alt="" loading="lazy" width="1363" height="1920" srcset="https://blog.codinghorror.com/content/images/size/w600/2023/11/image-1.png 600w, https://blog.codinghorror.com/content/images/size/w1000/2023/11/image-1.png 1000w, https://blog.codinghorror.com/content/images/2023/11/image-1.png 1363w" sizes="(min-width: 720px) 720px"></figure><p>I've been saying for a long time that decentralization is the way to go. We can and should have sane centralized services, of course, but <strong>it's imperative that we <em>also</em> build decentralized services which empower users and give them control</strong>, rather than treating them <a href="https://blog.codinghorror.com/are-you-a-digital-sharecropper/" rel="noreferrer">like digital sharecroppers</a>. That's what our <a href="https://discourse.org/?ref=blog.codinghorror.com" rel="noreferrer">Discourse project</a> is all about. I propose collective ownership of the content and the communities we build online. Yeah, it's more work, it's not "free" (sorry not sorry), but I have some uncomfortable news for you: those so-called "free" services <em>aren't really free</em>.</p><figure><img src="https://blog.codinghorror.com/content/images/uploads/2014/02/6a0120a85dcdae970b01a3fcc55683970b-800wi.png" alt="Geek-and-poke-pigs-free" loading="lazy"></figure><p>Which, again, is not to say that "free" services don't have a place in the world, they do, but please don't harbor any illusions about what you are sacrificing in the name of "free". Grow up.</p><p>I take a rather <a href="https://en.wikipedia.org/wiki/Tron?ref=blog.codinghorror.com" rel="noreferrer">Tron-like</a> view of the world when it comes to this stuff; in the software industry, our goal should be to <em>empower</em> users (with strong moderation tools), not exploit them. </p><figure><img src="https://blog.codinghorror.com/content/images/2023/11/image-3.png" alt="" loading="lazy" width="504" height="640"></figure><p>So I encourage you to explore alternatives to Twitter, ideally open source, federated alternatives. Is it messy? <em>Hell yes it's messy</em>. But so is democracy; it's worth the work, because it's the only survivable long term path forward. Anything worth doing is never <em>easy</em>.</p><p>I'm currently on <a href="https://en.wikipedia.org/wiki/Mastodon_(social_network)?ref=blog.codinghorror.com" rel="noreferrer">Mastodon</a>, an open source, federated Twitter alternative at <a href="https://infosec.exchange/@codinghorror?ref=blog.codinghorror.com">https://infosec.exchange/@codinghorror</a> – I urge you to join me on the Mastodon server of your choice, or quite literally <em>any other platform</em> besides Twitter. Really, whatever works for you. Pick what you like. Help make it better for everyone.</p><p>To inspire that leap of faith, I am currently auctioning off, with all funds to benefit <a href="https://en.wikipedia.org/wiki/The_Trevor_Project?ref=blog.codinghorror.com" rel="noreferrer">the Trevor Project</a> which offers assistance to LGBTQ youth, these 10 museum quality brass plaques of what I consider to be the best tweet of all time, hands down:</p><figure><img src="https://blog.codinghorror.com/content/images/2023/11/IMG_3328.jpg" alt="" loading="lazy" width="2000" height="1134" srcset="https://blog.codinghorror.com/content/images/size/w600/2023/11/IMG_3328.jpg 600w, https://blog.codinghorror.com/content/images/size/w1000/2023/11/IMG_3328.jpg 1000w, https://blog.codinghorror.com/content/images/size/w1600/2023/11/IMG_3328.jpg 1600w, https://blog.codinghorror.com/content/images/size/w2400/2023/11/IMG_3328.jpg 2400w" sizes="(min-width: 720px) 720px"></figure><p>(Blissfully, <a href="https://mastodon.social/@Horse_ebooks?ref=blog.codinghorror.com" rel="noreferrer">@horse_ebooks is also on Mastodon</a>. As they should be. As should you. Because everything happens so much.)</p><p>If you'd like to bid on the 10 brass plaques, follow these links to eBay, and please remember, it's for a great cause, and will piss Elon off, which makes it even sweeter:</p><p>(apologies, I had to cancel the old auctions because I forgot to allow international shipping – I've also made shipping free, worldwide.)</p><ol><li><a href="https://www.ebay.com/itm/225903779136?ref=blog.codinghorror.com">https://www.ebay.com/itm/225903779136</a></li><li><a href="https://www.ebay.com/itm/225903780761?ref=blog.codinghorror.com">https://www.ebay.com/itm/225903780761</a></li><li><a href="https://www.ebay.com/itm/225903784597?ref=blog.codinghorror.com">https://www.ebay.com/itm/225903784597</a></li><li><a href="https://www.ebay.com/itm/225903785269?ref=blog.codinghorror.com">https://www.ebay.com/itm/225903785269</a></li><li><a href="https://www.ebay.com/itm/225903785648?ref=blog.codinghorror.com">https://www.ebay.com/itm/225903785648</a></li><li><a href="https://www.ebay.com/itm/225903786591?ref=blog.codinghorror.com">https://www.ebay.com/itm/225903786591</a></li><li><a href="https://www.ebay.com/itm/225903787053?ref=blog.codinghorror.com">https://www.ebay.com/itm/225903787053</a></li><li><a href="https://www.ebay.com/itm/225903788754?ref=blog.codinghorror.com">https://www.ebay.com/itm/225903788754</a></li><li><a href="https://www.ebay.com/itm/225903789412?ref=blog.codinghorror.com">https://www.ebay.com/itm/225903789412</a></li><li><a href="https://www.ebay.com/itm/225903789881?ref=blog.codinghorror.com">https://www.ebay.com/itm/225903789881</a></li></ol><p>I will sign the back of every plaque, because each one comes with my personal guarantee that it will easily outlive what's left of Twitter.</p>
        
    </section>
</article>

                            <p><a href="https://blog.codinghorror.com/page/2">More Posts</a>
                    


            </p></div>

                <p>
                    <a href="https://blog.codinghorror.com/page/2">See all <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" fill="currentColor" viewBox="0 0 256 256"><path d="M224.49,136.49l-72,72a12,12,0,0,1-17-17L187,140H40a12,12,0,0,1,0-24H187L135.51,64.48a12,12,0,0,1,17-17l72,72A12,12,0,0,1,224.49,136.49Z"></path></svg></a>
                </p>
        </main>


    </div>




    

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Playing Chess with 84,688 Regular Expressions (492 pts)]]></title>
            <link>https://nicholas.carlini.com/writing/2025/regex-chess.html</link>
            <guid>42619652</guid>
            <pubDate>Tue, 07 Jan 2025 05:46:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nicholas.carlini.com/writing/2025/regex-chess.html">https://nicholas.carlini.com/writing/2025/regex-chess.html</a>, See on <a href="https://news.ycombinator.com/item?id=42619652">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p><i>Can chess, with regular expressions?
        Yes. Can chess, with regular expressions.</i>
      </p>
      
      
      

      <p>
        Over the holidays I decided it's been too long since I did something
        with entirely no purpose. So without further ado, I present to you ...
        <b>Regex Chess</b>: sequence of 84,688 regular expressions that,
        when executed in order, will play a (valid; not <i>entirely</i> terrible) move given a chess board as input.
        Here, I'll show you.
      </p>

<div>
            
            <p>Current executing regular expression will show here...</p>
        </div>

    

    
      <p>
        Specifically, this is the entirety of the program that is playing a move against you (no really, <a href="https://github.com/carlini/regex-chess/blob/main/main.py">I'm not kidding</a>, it really is this short):
      </p>

    
      <div><p><span>let </span><span>regex_list</span><span> = [/* a very </span><span>long</span><span> </span><span>list</span><span> of regular expressions */]</span></p>
<p><span>let </span><span>board</span><span> = </span><span>"rnbqkbnr / pppppppp / 8 / 8 / 8 / 8 / PPPPPPPP / RNBQKBNR w KQkq - 0 1"</span><span>;</span></p>

<p><span>for</span><span> (regex of regex_list) {</span></p>
<p><span><span>&nbsp; &nbsp; </span>board = re.replace(regex.pattern, regex.target)</span></p>
<p><span>}</span></p>
<p><span>display(board)</span></p>
</div>

      <p>
        By the end of this post you'll (hopefully)
        understand why this sequence of regular
        
        <span id="footnote1">
          <a href="#footnote1">
            <label for="foot">[a]</label>
          </a>
          <span>
            Now some snobby people when they see this are going to say something like
        "yoU SaiD yoU'Re GoIng to uSe rEGUlar EXPresSIONs buT thESE ArE nOT <a href="https://en.wikipedia.org/wiki/Regular_language"><i>ReGULaR </i></a>!!" I do not care.
          </span>
        </span>

        
        expressions is possible,
        and also what the specific regular expressions do.
      </p>

      <p>
        (If you're someone who subscribed to this blog in the last six months or so,
        and have gotten accustom to me writing about "serious" and "important" things,
        please treat this as your fair warning that this is MY WEBSITE and I MAKE THE RULES HERE
        and so today you're learning about RegexChess whether you like it or not.)
      </p>

      <p>
        As always, code for this project is available on <a href="https://github.com/carlini/regex-chess">GitHub</a>.
      </p>
        
        
      <br>
      <h2>Getting Started: A Regular Expression CPU</h2>

      <p>
        So how are we going to get regular expressions to play chess?
        Well, by making a regular expression computer, of course!
        More specifically, we're going to design a Branch-Free,
        Conditional-Execution,
        Single-Instruction Multiple-Data instruction set.
        And then make a sequence of regular expressions that interpret these instructions.
        (Kind of like a GPU instruction set. And a bit of ARM.
        But a lot slower.)
        And then from there we can program our new computer to play chess.
        So let's get started.
      </p>
        

      <p>
        
        (Some people may say I have an unhealthy obsession with building
        weird computers, c.f. <a href="https://nicholas.carlini.com/writing/2021/unlimited-register-machine-game-of-life.html">my game of life computer</a>
        or my <a href="https://github.com/carlini/printf-tac-toe">printf computer</a>.
        Those people are wrong,
        and just have an unhealthy obsession with the banal and ordinary.)
       </p>
        

      <br>
  <h2>Computer Design</h2>
  
  <p>
    Let me get started by explaining how I'm going to organize the data
    that the computer is going to operate over.
    Because we're working with regular expressions,
    the current <code>state</code> of the computer is going to be represented as
    a single string containing both the program "stack", and all variables in the following format:
  </p>

  <p>%%
#stack:
top item on stack
second item on stack
....
#variable1: value 1
#variable2: value 2
...
#variablek: value k</p>

  <p>
    Each <code>instruction</code> will either manipulate some variables on the stack,
    or will read or write to a given variable. Let's look at some basic instructions.
  </p>

  <h3>Basic Stack Operations</h3>

<h4>The <code>Push</code> Instruction</h4>
<p>
  Here's the implementation of the <code>push</code> command that adds a value to the top of the stack:
</p>

<div><p><span>def</span><span> </span><span>push</span><span>(const):</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>return</span><span> [(r</span><span>"(%%\n#stack:\n)"</span><span>,</span></p>
<p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>r</span><span>"\g&lt;1&gt;"</span><span>+const+r</span><span>"\n"</span><span>)]</span></p>
</div>

<p>
  You should read the return type of these functions as a list of tuples.
  Each tuple represents a regex transformation to apply,
  where the left element is the pattern to match, and the right element is the replacement.
</p>

<p>
  As a brief regular expression refresher.
  Each tuple in this list has two parts: the regular expression, and the replacement.
  A regular expression will <i>match</i> a string if it can find a substring of whatever it's being applied against (the <i>state</i>, in our case).
  Most characters in a regular expression match themselves, but parentheses create
  a "match group" that can be referenced later.
</p>

<p>
  The second argument is the replacement string.
  Again, most characters mean "Replace with this character",
  but special sequences like \g&lt;1&gt; are back-references that refer to previously captured groups.
  In this case, \g&lt;1&gt; references the first captured group (anything matched within the first set of parentheses)---which in this case is the "%%\n#stack:\n" header.
</p>

<p>
  So, as a result of this operation executing on the stack, we find the occurrence of
  "%%\n#stack:\n" in the state,
  and insert the constant value below that (to the top of the stack).
</p>

<p>
  Let's see this in practice. Say we start with an empty stack:
</p>

<p><span data-group="Group 1 (Matched by regex)">%%
#stack:</span></p>

<p>
  If we execute <code>push("hello")</code>, the regular expression will:
</p>

<ul>
  <li>Match the pattern <code>%%\n#stack:\n</code> at the start of our state</li>
  <li>Capture this header into group 1 (the parentheses in the pattern create this capture group)</li>
  <li>Replace it with the captured group (<code>\g&lt;1&gt;</code>) followed by our constant "hello" and a newline</li>
</ul>

<p>
  This gives us:
</p>

<p><span data-group="Group 1 (Original header)">%%
#stack:</span>
hello</p>

<p>
  If we then execute <code>push("world")</code>, the same process repeats and we get:
</p>

<p><span data-group="Group 1 (Original header)">%%
#stack:</span>
world
hello</p>

<p>
  The regular expression always matches at the top of the stack area, so new items get pushed on top while preserving the existing stack contents below them.
</p>  

<h4>The <code>Pop</code> Instruction</h4>
<p>
  The <code>pop</code> instruction removes the top element from the stack:
</p>

<div><p><span>def</span><span> </span><span>pop</span><span>():</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>return</span><span> [(r</span><span>"(%%\n#stack:\n)([^\n]*)\n"</span><span>,</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>r</span><span>"\1"</span><span>)]</span></p>
</div>

<p>
  Here we start to see a few of the special operators that make regular expressions powerful.
  The [^\n] block means "match any character that isn't a newline" and the * means "match zero or more of these."
  So taken together we're looking for a line that starts with "%%\n#stack:\n",
  and then on the next line, zero or more characters that aren't a newline (so, the entire line).
  The replacement is just the first line, which has the effect of removing the second line,
  popping the top of the stack.
</p>

<p>
  Let's see how this works in practice. Say we start with this stack:
</p>

<p><span data-group="Group 1 (Stack header)">%%
#stack:</span>
<span data-group="Group 2 (Top element to pop)">world</span>
hello</p>

<p>
  When we execute <code>pop()</code>, the regular expression will:
</p>

<ul>
  <li>Match the pattern beginning with <code>%%\n#stack:\n</code> (captured in group 1)</li>
  <li>Match any characters until the next newline (captured in group 2 - the "world")</li>
  <li>Replace everything matched with just group 1 (the header), effectively removing the top element</li>
</ul>

<p>
  This gives us:
</p>

<p><span data-group="Group 1 (Stack header)">%%
#stack:</span>
hello</p>


<p>
  Each pop operation removes exactly one element from the top of the stack, preserving any remaining elements below it.
</p>
  <h3>Variable &lt;-&gt; Stack Instructions</h3>

  <h4>Variable <code>Lookup</code></h4>
<p>
  To load the contents of a variable onto the top of the stack:
</p>

<div><p><span>def</span><span> </span><span>lookup</span><span>(variable):</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span># Find the variable's value and push it onto the stack </span><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span></span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>return</span><span> [(r</span><span>"(%%\n#stack:)([^%]*\n#"</span><span>+variable+</span><span>": )([^</span><span>\n</span><span>]*)</span><span>\n</span><span>"</span><span>,</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>r</span><span>"\1\n\3\2\3\n"</span><span>)]</span></p>
</div>

<p>
  This regular expression is a bit more complex than our previous ones. Let's break down what it does:
</p>

<ul>
  <li>The <code>[^%]*</code> matches basically any character (% only appears at the start of the program)
    and so lets us find any variable anywhere in the program.
  </li><li>The <code>[^\n]*</code> matches the variable's value by capturing everything until the end of the line</li>
  <li>The replacement creates a copy of the value and places it on top of the stack</li>
</ul>

<p>
  Let's see how this works in practice. Say we start with this state:
</p>

<p><span data-group="Group 1 (Stack header)">%%
#stack:</span>
<span data-group="Group 2 (Everything up to our variable)">#foo: hello
#bar: </span><span data-group="Group 3 (The value we want)">world</span>
#baz: 42</p>

<p>
  If we execute <code>lookup("bar")</code>, the regular expression will:
</p>

<ul>
  <li>Match the stack header in group 1</li>
  <li>Match everything up to and including "#bar: " in group 2</li>
  <li>Match "world" in group 3</li>
  <li>Use these groups to reconstruct the state with the value copied to the top of the stack</li>
</ul>

<p>
  And so running the replacement will result in the following state:
</p>

<p><span data-group="Group 1 (Stack header)">%%
#stack:</span>
world
<span data-group="Group 2 (Original variable definition)">#foo: hello
#bar: </span><span data-group="Group 3 (Original value)">world</span>
#baz: 42</p>

<p>
  The lookup operation preserved the original variable and its value while also placing a copy of the value on top of the stack. This allows us to read variable values without modifying them.
</p>

  <h4>Variable <code>Assignment</code></h4>
  <p>
    Assigning to a variable presents an interesting challenge: we don't know if the variable
    already exists. We need to handle both cases: updating an existing variable or creating
    a new one.
  </p>


      <p>
        Let me show you the implementation and then I'll walk you through it case by case.
      </p>

      <div><p><span>def</span><span> </span><span>assign_pop</span><span>(varname):</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>return</span><span> [</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span>(r</span><span>"(%%)\n#stack:\n([^\n]*)\n"</span><span> +</span></p>
<p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>"([^%]*#"</span><span> + varname + r</span><span>": )[^\n]*"</span><span>,</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span>r</span><span>"\1`\n#stack:\n\3\2"</span><span>),</span></p>

<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span>(r</span><span>"(%%)([^`]\n?#stack:\n)([^\n%]*)\n([^%]*)"</span><span>,</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span>r</span><span>"\1`\2\4#"</span><span> + varname + r</span><span>": \3\n"</span><span>),</span></p>

<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span>(r</span><span>"%%`"</span><span>,</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span>r</span><span>"%%"</span><span>)</span></p>
<p><span><span>&nbsp; &nbsp; </span>]</span></p>
</div>

      <p>
        To begin let's assume the variable already exists. That is, the stack starts
        off looking like this, and assume we're calling assign_pop("bar"):
      </p>

      <p>%%
#stack:
world
#foo: hello
#bar: something
#othervar: othervalue</p>

      <p>
        When we run this regular expression list, we create the following capture groups:
      </p>
        

      <p><span data-group="Group 1 (%%)">%%</span>
#stack:
<span data-group="Group 2 (world)">world</span>
#foo: hello
<span data-group="Group 3 (#bar: )">#bar: </span>something
#othervar: othervalue</p>
  

  

  <p>
    After the replacement operation, we get this output:
  </p>

  <p><span data-group="Header">%%`</span>
#stack:
<span data-group="Stack declaration">#foo: hello
#bar: </span><span data-group="Variables">world</span>
#othervar: othervalue
  </p>

  <p>
    Now we proceed on to the next instruction, and <i>we don't match it</i> because the second regex fails if there's a back-tick after the program start %%. So nothing happens. And then finally, the third regex cleans things up for us.
  </p>
  
  
<p>
  <b>Handling Non-Existent Variables:</b>
    Let's consider what happens if the variable doesn't already exist. Again, assume we're calling <code>assign_pop("bar")</code>:
  </p>

  <p>
%%
#stack:
world
#foo: hello
#othervar: othervalue
  </p>

  <p>
    The first regex tries to match but fails because there is no "#bar" anywhere. So it doesn't do anything. But now the second regex tries to match and succeeds. It creates the following capture groups:
  </p>

  <p><span data-group="Group 1">%%</span>
<span data-group="Group 2">#stack:
world</span>
<span data-group="Group 3">#foo: hello
#othervar: othervalue</span>
  </p>

  <p>
    From here, we perform the rewrite and get the following output:
  </p>

  <p>
%%
#stack:
#foo: hello
#bar: world
#othervar: othervalue
  </p>

<p>
  And then the third regex is applied and does nothing.
</p>

<p>
  There are lots of instructions that use this trick to make sure we don't
  apply things in the order we don't want. For example, as an exercise for
  yourself, try to understand how the "is equal" instruction works:
</p>

<div><p><span>def</span><span> </span><span>eq</span><span>():</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>return</span><span> [</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span>(r</span><span>"(%%\n#stack:\n)([^\n]*)\n\2\n"</span><span>,</span></p>
<p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; </span>r</span><span>"\1`True\n"</span><span>),</span></p>

<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span>(r</span><span>"(%%\n#stack:\n)([^`][^\n]*)\n([^\n]*)\n"</span><span>,</span></p>
<p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; </span>r</span><span>"\1False\n"</span><span>),</span></p>

</div>

<br>
<h2>(Branch-Free) Conditionals</h2>

<p>
  Programming languages, in order to be interesting, usually need to have some kind of
  control flow. It's very hard to write some program without ever having an if statement.
  So let's now show how we're going to do this.
  (And I hope you did your homework, because we're going to use the
  same conditional execution trick again!)
  Here's the implementation of a conditional instruction:
</p>

<div><p><span>def</span><span> </span><span>cond</span><span>(tag):</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>return</span><span> [(r</span><span>"%(%\n#stack:\nTrue)"</span><span>,</span></p>
<p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>r</span><span>"%\1`"</span><span>),</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>(r</span><span>"%(\n#stack:\nFalse)"</span><span>,</span></p>
<p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>tag+r</span><span>"\1`"</span><span>),</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>(r</span><span>"\n(True|False)`\n"</span><span>,</span></p>
<p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>"</span><span>\n</span><span>"</span><span>)]</span></p>
</div>

<p>
  Let's walk through how this is going to work, starting with the case where the
  top of the stack is False.
</p>

<p>%%
#stack:
False
#variable: value</p>

<p>
  Initially, the first regex will fail to match, because the top element on the stack
  isn't True. So we go to the next regex, and see if it applies. This one does match,
  and makes the corresponding match groups.
</p>

<p><span data-group="Group 1 (Initial state)">%%
#stack:
False</span>
#variable: value</p>


<p>
  After we apply the replacement, we get the following stack.
</p>

<p>%tag
#stack:
False`
#variable: value</p>

<p>
  (And finally, using the same cleanup trick, we'll remove the used marker.)
</p>

<p>
  Now see what happened here? The program <i>no longer begins with <code>%%</code></i>.
  This means that EVERY instruction will fail to match, because they always make sure
  that the program begins with %%. So nothing else will happen.... until we <i>reactivate</i>
  it later with the following simple instruction:
</p>



<div><p><span>def</span><span> </span><span>reactivate</span><span>(tag):</span></p>
<p><span><span>&nbsp;&nbsp;&nbsp;&nbsp;</span></span><span>return</span><span> [(r</span><span>"%"</span><span>+tag+r</span><span>"\n([^%]*)"</span><span>,</span></p>
<p><span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>r</span><span>"%%\n\1"</span><span>)]</span></p></div>

<p>
  Let's now return to the True case for the conditional. This is the easy case: we
  basically don't do anything at all. We replace the stack with True` on the second
  regex, and then delete this line on the third. Easy.
</p>

<p>
        Notice that this means our code is actually branch-free,
        because every instruction is a conditional instruction. (Kind of like ARM's predicated execution, where most instructions can be conditionally executed based on status flags rather than using explicit branch instructions.)</p>
        
        
      <br>
      <h3>Loops (are impossible)</h3>

      <p>
        Because our program just consists of a sequence of regular expressions,
        you can't loop at all! That, technically, means we can't actually perform
        <a href="https://en.wikipedia.org/wiki/Turing_completeness">Turing Complete</a>
        But we can do any bounded computation by just <i>unrolling</i> any loops we may have.
        And fortunately computing the next move in a chess position is a bounded computation, so we can do just that. 
      </p>
        
        
      <h2>Single-Instruction Multiple-Data</h2>

<p>
  And now for my absolute favorite part of the language we've developed.
  By the magic of regular expressions (and the fact that they perform
  substitution globally over the entire string), we can run multiple <i>threads</i>
  simultaneously!
</p>

<p>
  That is, if we just write our state string as:
</p>

<p><span data-group="Group 1 (First thread header)">%%
#stack:</span>
<span data-group="Group 2 (First operand)">int0000101010</span>
<span data-group="Group 3 (Second operand)">int0001011100</span>
<span data-group="Group 1 (Second thread header)">%%
#stack:</span>
<span data-group="Group 2 (First operand)">int0000001101</span>
<span data-group="Group 3 (Second operand)">int0110000000</span></p>

<p>
  When we call <code>binary_add()</code>, both additions happen simultaneously! After execution:
</p>

<p>%%
#stack:
int0010001110
%%
#stack:
int0110001101</p>

<p>
  The reason this happens is because the regular expression matches work globally. When we match the "begin thread"
  operator (<code>%%</code>) twice, we get to perform operations on both threads simultaneously.
</p>

<p>
  So how do we actually make use of this feature? Let's look at some instructions that help us create and manage threads.
</p>

<br>
<h3>The Fork Instructions</h3>

<p>
  Here's a simple fork instruction that splits every currently running thread into two, 
  with the second one starting off inactive with a given tag:
</p>

<div><p><span>def</span><span> </span><span>fork_inactive</span><span>(tag):</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>return</span><span> [(r</span><span>"%%\n([^%]*)"</span><span>,</span></p>
<p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>r</span><span>"%%\n\1"</span><span> + </span><span>"%"</span><span>+tag+r</span><span>"\n\1"</span><span>)</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>]</span></p>
</div>

<p>
  We can also, for example, <code>fork()</code> on a boolean, giving one thread
  the True case and another the False case. (This is something like McCarthy's
  Amb operator <a href="https://linkinghub.elsevier.com/retrieve/pii/S0049237X08720184">reference</a>)
</p>

<div><p><span>def</span><span> </span><span>fork_bool</span><span>(variable):</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>return</span><span> [(r</span><span>"%%\n([^%]*)"</span><span>,</span></p>
<p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>r</span><span>"%%\n\1#"</span><span>+variable+r</span><span>": True\n%%\n\1#"</span><span>+variable+r</span><span>": False\n"</span><span>)</span></p></div>

<p>
  Let's see what happens when we apply multiple forks. Starting with a simple state:
</p>

<p><span data-group="Group 1 (Thread content)">%%
#stack:
somevalue
#x: 10</span></p>

<p>
  After calling <code>fork_bool("condition")</code>, we get:
</p>

<p>%%
#stack:
somevalue
#x: 10
#condition: True
%%
#stack:
somevalue
#x: 10
#condition: False</p>

<p>
  If we then call <code>fork_bool("c2")</code>, each existing thread splits into two:
</p>

<p>%%
#stack:
somevalue
#x: 10
#condition: True
#c2: True
%%
#stack:
somevalue
#x: 10
#condition: True
#c2: False
%%
#stack:
somevalue
#x: 10
#condition: False
#c2: True
%%
#stack:
somevalue
#x: 10
#condition: False
#c2: False</p>

<p>
  Now we have four simultaneous execution paths, exploring every possible combination of our boolean conditions at the same time.
  This is exceptionally useful for chess, when we might frequently want to consider multiple
  possible board states at the same time, and (for example) score them to see which is best.
  Instead of having to loop over every possible board state, we can just pretend we were doing it
  once but have them all happen at the same time.
</p>

        
        
<br>
<h2>Compiling to our little language</h2>

<p>
  Now that we have our CPU emulator, we can build a compiler to target our new assembly
  language.
</p>

<p>
  <i>"But wait I'm not reading this post for a lesson in compilers!"</i> you say??
  Fair point.
  Also I didn't go into this project trying to build a compiler,
  so instead what I have is more of a macro-assembler.
  It turns python-ish programs like this:
</p>

<div><p><span>def</span><span> </span><span>fib</span><span>():</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>a</span><span> = 1</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>b</span><span> = 2</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>for</span><span> _ </span><span>in</span><span> </span><span>range</span><span>(10):</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>next</span><span> = a + b</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>a</span><span> = b</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>b</span><span> = </span><span>next</span></p>
</div>

<p>
  Into a sequence of instructions like this:
</p>

<p>push(1)
assign_pop('a')
push(2) 
assign_pop('b')
lookup('a')
lookup('b')
binary_add()
assign_pop('next')
lookup('b')
assign_pop('a')
lookup('next')
assign_pop('b')
[... repeated 8 more times ...]
</p>

<br>
<h3>Compiling Through Symbolic Execution</h3>

<p>
  Rather than writing a traditional compiler with parsing and code generation
  phases, I took an unusual approach: symbolic execution. The key insight is
  that we can "execute" the Python code in a special way that records what
  operations would happen rather than actually performing them.
</p>

<p>
  Here's how it works: the <code>variables</code> argument isn't actually a
  dictionary---it's a special object that records every operation performed on
  it. It creates what we call a "trace" of the execution. When you write:
</p>

<p>a = b + 1</p>

<p>
  The tracer object records four operations:
</p>

<ol>
  <li>A <code>lookup</code> of variable 'b'</li>
  <li>A <code>push</code> of the constant 1</li>
  <li>A <code>binary_add</code> operation</li>
  <li>An <code>assign_pop</code> to variable 'a'</li>
</ol>

<br>
<h3>Handling Control Flow</h3>

<p>
  The trickiest part of this approach is handling branching control flow---<code>if</code> statements, specifically.
  (What about loops? We don't have those, so I never use them. Loops can only have constants.)
  We need to make sure we capture all possible execution paths. Here's how we do it:
</p>

<p>
  When we hit a conditional, we create two separate paths in our trace---one for when the condition is true, and one for when it's false. Each path records its own sequence of operations. Later, we merge these paths back together.
</p>

<p>
  For example, this Python code:
</p>

<div><p><span>if</span><span> x &gt; 0:</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>y</span><span> = 1</span></p>
<p><span>else</span><span>:</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>y</span><span> = 2</span></p>
</div>

<p>
  Generates something like this trace structure:
</p>

<p>lookup('x')      # Get x's value
push(0)          # Push 0
greater_than()   # Compare
cond('tag1')     # Branch on result
  # True path:
  push(1)
  assign_pop('y')
pause('tag2')
reactivate('tag1')
  # False path:
  push(2)
  assign_pop('y')
reactivate('tag2')</p>

<p>
  The magic of the compiler lies in how it handles control flow through branching.
  Let me explain this in a little bit more detail.
</p>

<p>
  When our compiler first starts processing code, it maintains a single linear
  path of instructions in the CallTree. Each instruction gets appended one after
  another as we go. This list of instructions looks entirely linear.
  When we   reach a conditional statement, though, things get interesting.
</p>

<p>
  Consider what happens when we hit a conditional statement like the <code>x &gt; 0</code>
  conditional above.
  When this happens, I detect it, and create a branch in the call tree representing the two
  paths simultaneously.
  Then, I just pretend this conditional was true, and fill out the true case of the tree.
  When we reach the end of the program we've done <i>some</i> of our compilation, but not all of it.
</p>


<p>
  Now comes the clever part.
  When compiling, we don't just trace the program once.
  We trace many times.
  And each time we trace, we arrange for the conditionals to go through whichever branch has been
  taken least often.
  In this way, the second time around, we record the instructions for the false branch.
</p>

      <p>
        Finally, once the conditional is over, we detect this and merge the two branches back together.
  This branching and merging mechanism is more than just a clever trick---it's essential to how our regex-based CPU actually works. When we convert this tree into instructions, each branch gets translated into a conditional regex operation (using our cond instruction) that can selectively activate and deactivate different parts of our program state. The merge points become reactivate instructions that ensure we continue execution on the correct path.
</p>

<br>
      <h2>Writing a Chess Engine</h2>

      <p>
        Okay, so we're finally to the part of this post where we can actually start writing
        a chess engine. (And at the part where some of the emulator design decisions will
        make sense.)
      </p>

      <p>
        For the most part, this is entirely straightforward and mirrors how you would write a chess
        engine in any other programming language.
        But this branching thing with SIMD is what gives us the power to make things go fast.
      </p>

      <p>
        Let's consider the following (simplified) program that calculates
        all the valid pawn moves.
      </p>


<div><p><span>def</span><span> </span><span>pawn_moves</span><span>(initial_board):</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span># Step 1: Find all pawns and create a list of their positions<span>&nbsp;</span></span><span></span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>pawnpos_list</span><span> = find_all_pawns(initial_board)</span></p>

<p><span><span>&nbsp; &nbsp; </span></span><span># Step 2: Create parallel states for each pawn (up to 8) </span><span></span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>MAX_PAWNS</span><span> = 8</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>for</span><span> iteration </span><span>in</span><span> </span><span>range</span><span>(MAX_PAWNS):</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>if</span><span> </span><span>not</span><span> pawn_list.is_empty():</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>pawn_pos</span><span> = pawnpos_lst.remove_first()</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>fork_inactive(</span><span>"waiting"</span><span>)</span></p>

<p><span><span>&nbsp; &nbsp; </span></span><span># Step 3: Switch to processing the parallel states </span><span></span></p>
<p><span><span>&nbsp; &nbsp; </span>pause(</span><span>"main"</span><span>)</span></p>
<p><span><span>&nbsp; &nbsp; </span>reactivate(</span><span>"inactive"</span><span>)</span></p>

<p><span><span>&nbsp; &nbsp; </span></span><span># Step 4: Generate moves for all pawns simultaneously<span>&nbsp;</span></span><span> </span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>candidate_moves</span><span> = []</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>if</span><span> initial_board[pawn_pos + (0, 1)].is_empty():</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span>candidate_moves.append(pawn_pos + (0, 1))</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>if</span><span> pawn_pos.y == 2:</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>if</span><span> initial_board[pawn_pos + (0, 2)].is_empty():</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>candidate_moves.append(pawn_pos + (0, 2))</span></p>

<p><span><span>&nbsp; &nbsp; </span></span><span>if</span><span> initial_board[pawn_pos + (1, 1)].has_opponent():</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span>candidate_moves.append(pawn_pos + (1, 1))</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>if</span><span> initial_board[pawn_pos + (-1, 1)].has_opponent():</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span>candidate_moves.append(pawn_pos + (-1, 1))</span></p>

<p><span><span>&nbsp; &nbsp; </span></span><span># Step 5: Switch back and merge results<span>&nbsp;</span></span><span> </span></p>
<p><span><span>&nbsp; &nbsp; </span>pause(</span><span>"inactive"</span><span>)</span></p>
<p><span><span>&nbsp; &nbsp; </span>reactivate(</span><span>"main"</span><span>)</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>candidate_moves</span><span> = merge_variables_from_threads(</span><span>"inactive"</span><span>)</span></p>
</div>

<h3>Step 1: Finding the Pawns</h3>
<p>
  The <code>find_all_pawns()</code> function scans through our board representation,
  looking for white pawns (represented as 'P' in the FEN string).
  It returns a list of the position of each of these pawns.
  As an example, if we run our program on the following
  position with three pawns on d2, e2, and f2, this creates a semicolon-separated list in the pawnpos_lst variable as follows
</p>

<p>%%
#stack:
#board: 4k3/8/8/8/8/8/3PPP2/4K3
#pawnpos_lst: d2;e2;f2;</p>

<h3>Step 2: State Creation</h3>
<p>
  Now comes the clever part. The <code>fork_inactive</code> instruction, as described above, duplicates our entire program state. Each time it runs, it creates an exact copy of the currently running thread, but marks the new copy with %waiting instead of %%. (Recall this means instructions won't apply to this thread.) At the same time, it takes one position from our pawnpos_lst and assigns it to a new variable pawnpos in the copied state.
</p>

<p>
  When our loop runs three times, each <code>fork_inactive</code> operation splits off a new parallel universe where we'll process a different pawn. The regex operation that does this copying preserves all existing variables but adds the new pawnpos variable with the specific position for that copy to process.
</p>

<p>%%
#stack:
#board: 4k3/8/8/8/8/8/3PPP2/4K3
#pawnpos_lst: 

%waiting
#stack:
#board: 4k3/8/8/8/8/8/3PPP2/4K3
#pawnpos: d2

%waiting
#stack:
#board: 4k3/8/8/8/8/3PPP2/4K3
#pawnpos: e2

%waiting
#stack:
#board: 4k3/8/8/8/8/3PPP2/4K3
#pawnpos: f2</p>

<h3>Step 3: Activation Switch</h3>
<p>
  Recall that the pause and reactivate operations work by manipulating the %% markers that indicate which states are active. The pause("main") operation changes our original %% to %main, effectively deactivating it. Then reactivate("inactive") finds all states marked with %waiting and changes them to %%, making them the new active states.
</p>

<h3>Step 4: Parallel Move Generation</h3>
<p>
  Here's where the SIMD magic happens. Each check for a possible move---forward one square, forward two squares, or diagonal captures---executes across all active states simultaneously. When we check if the square ahead is empty, we're actually checking d3, e3, and f3 all in one operation. For each valid move, we add this to the candidate_moves list. 
</p>

<p>
  (I've significantly simplified the program for visual purposes here. In
  reality I don't work directly over the FEN strings but expand them to 64
  independent variables for each of the 64 squares and read and write to these
  squares directly. This makes processing the board state much easier. But for
  the purpose of this blog post it's easier to show visually as if it worked on FEN strings alone.)
</p>

<p>%main
#stack:
#board: 4k3/8/8/8/8/8/3PPP2/4K3
#pawnpos_lst: 

%%
#stack:
#board: 4k3/8/8/8/8/8/3PPP2/4K3
#pawnpos: d2
#candidate_moves: d3;d4

%%
#stack:
#board: 4k3/8/8/8/8/8/3PPP2/4K3
#pawnpos: e2
#candidate_moves: e3;e4

%%
#stack:
#board: 4k3/8/8/8/8/8/3PPP2/4K3
#pawnpos: f2
#candidate_moves: f3;f4</p>

<h3>Step 5: Merging Results</h3>
<p>
  The final merge operation combines all the candidate_moves lists from our parallel states. It first switches activation back to the main state using pause and reactivate. Then merge_variables_from_threads (again, a pseudo-op I've made up for visual clarity, in practice this requires like 10 different instructions on my real machine) match all the candidate_moves lists across our inactive states and concatenates them together.
</p>

<p>
  What this means is that while the code we wrote code <i>looks</i> like it processes one pawn at a time, our regex engine's ability to process multiple states means we're actually handling all pawns simultaneously. Every operation, from checking empty squares to building move lists, happens in parallel across all our active states.
</p>

      <p>
        And this is how every piece operation works.
        Because this post is already getting quite long, I won't walk through each piece one by one, but if you're interested definitely go look at the <a href="https://github.com/carlini/regex-chess/blob/main/chess_engine.py">chess-engine.py</a> for the details.
      </p>

      <br>
<h2>Playing a Turn</h2>

<p>
  Now let's walk through the overall game loop that handles everything.
</p>


<div><p><span>def</span><span> </span><span>play_turn</span><span>():</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span># Step 1: Read the human entered move from the input </span><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span></span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>board_before_move</span><span>, </span><span>src</span><span>, </span><span>dst</span><span> = from_pretty_utf8_to_fen()</span></p>

<p><span><span>&nbsp; &nbsp; </span></span><span># Step 2: Check if their move is valid </span><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span></span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>after_move</span><span> = board_before_move.make_move(src, dst)</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>next_boards</span><span> = compute_legal_boards(board_before_move)</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>next_board</span><span> = fork_on_list(next_boards)</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>if</span><span> after_move != next_board:</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span>destroy_active_thread()</span></p>

<p><span><span>&nbsp; &nbsp; </span></span><span># Step 3: Generate the computer's reply<span>&nbsp;</span></span><span> <span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span></span></p>
  <p><span><span>&nbsp; &nbsp; </span></span><span>candidate_boards</span><span> = compute_and_score_legal_boards(after_move)</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>candidate_board</span><span> = fork_on_list(candidate_boards)</span></p>

<p><span><span>&nbsp; &nbsp; </span>keep_best_scoring_board(score)</span></p>
<p><span><span>&nbsp; &nbsp; </span>from_fen_to_pretty_utf8(candidate_board)</span></p>

</div>

<p>
  Say we're at the start of a game, and the human enters "e2e4" (the king's pawn opening).
  Here's how our code processes this:
</p>

<h3>Step 1: Reading the Move</h3>

<p>
  Initially, the function <code>from_pretty_utf8_to_fen()</code> converts our pretty-printed
  board with Unicode chess pieces back into FEN notation.
  It also extracts the source and destination squares from the input:
</p>

<p>%%
#stack:
#board: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR
#src: e2
#dst: e4</p>

<h3>Step 2: Move Validation</h3>

<p>
  Now we need to check if this is a valid move.
  Rather than writing entirely new code that explicitly checks if a move is legal,
  we use our parallel processing power again.
  The process works in three stages:
</p>

<p>
  First, <code>make_move</code> applies the human's move to create a new board state:
</p>

<p>%%
#stack:
#board: rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR
#after_move: rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR</p>

<p>
  Then <code>compute_legal_boards</code> generates <i>all</i> possible legal moves from the starting position, creating a list like:
</p>

<p>%%
#stack:
#next_boards: rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR;
              rnbqkbnr/pppppppp/8/8/8/3P4/PPP1PPPP/RNBQKBNR;
              rnbqkbnr/pppppppp/8/8/8/4P3/PPPP1PPP/RNBQKBNR;
              ...</p>

<p>
  Finally, fork_on_list creates parallel states for each legal board position:
</p>

<p>%%
#stack:
#board: rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR

%%
#stack:
#board: rnbqkbnr/pppppppp/8/8/8/3P4/PPP1PPPP/RNBQKBNR

%%
#stack:
#board: rnbqkbnr/pppppppp/8/8/8/4P3/PPPP1PPP/RNBQKBNR</p>

<p>
  The <code>destroy_active_thread()</code> call removes any thread where the board doesn't match after_move.
  In our case, only the e2-e4 position survives, confirming it's a legal move.
  (If no legal move is made, I have a special regex that will replace the entire output with just the hard-coded text "Illegal Move.")
</p>

<h3>Step 3: Computer's Reply</h3>

<p>
  Now we repeat a similar process to find the best reply.
  First, <code>compute_and_score_legal_boards</code> generates all possible black responses.
  This function does a little bit of magic,
  and when it returns the next possible boards, it returns with each board
  <i>the score of that board after whites next best move</i>
  I'll explain how this works below. But suffice to know that what this function returns for
  now the possible positions as <code>compute_legal_boards</code> does, but also the score of the position.
        (This is where the MiniMax happens.
        The score here is how good each position is from the player's perspective, after they have
        made their best reply move.)
      </p>

<o>
  So running this function gives us something like this as output


<p>%%
#stack:
#board: rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR
#score: 100

%%
#stack:
#board: rnbqkbnr/ppppp1pp/8/5p2/4P3/8/PPPP1PPP/RNBQKBNR
#score: 102

%%
#stack:
#board: rnbqkb1r/pppppppp/5n2/8/4P3/8/PPPP1PPP/RNBQKBNR
  #score: 98</p>


<p>
  <code>keep_best_scoring_board(score)</code> collapses these parallel states,
  but this time keeping only the position with the highest score according to black (in this example, the f7-f5 response).
  This is the second half of the minimax search---the one where we look at all of the options where our
  opponent had a pick the first time around, and now we pick among these the best for us.
  Finally, from_fen_to_pretty_utf8 converts this position back to the pretty Unicode display format.
  As a result, I never have to build any explicit search algorithms or lists to store lists of positions,
  that's all handled by the parallel processing.
</p>

<br>
<h2>MiniMax Search (for free)</h2>
  
      <p>
        The final missing piece of this process is how to implement the <code>compute_and_score_legal_boards</code>
      function in a way that gets us the depth-2 minimax search.
      And here I cheat (a lot).
      </p>

<p>
  It is relatively easy to generate <i>pseudo-</i>legal moves---that is, moves that are basically legal, but might leave the king in check.
  In chess this isn't allowed (you can't end your turn in check) and so the second thing that has to happen
  is I have to delete all of the moves that would leave the king in check.
</p>

<p>
  The way to do this is to again generate all possible opponent replies and see if any of them would possible "capture" the king. If they would, then the position is illegal because that's the definition of check.
  But what this means is we're already doing a depth-2 search.
  We have our initial board state.
  Depth 1, we generate all possible moves the computer wants to make.
  Then depth 2, we generate all possible moves the opponent could make to ensure we're not in check.
</p>

<p>
  All I have to do now then is score each of these opponent-generated moves and return the score of the candidate
  position as the best scoring among all of these (from the perspective of the opponent).
  And then I can just take the best scoring of these (from the persepctive of the computer) as its best move
</p>

<p>
  Now there's one slight problem here, which is why real chess engines don't do this.
  And that is this isn't technically a full depth-2 minimax search because it would allow the opponent
  moves generated in reply to be illegal because they could put their king in check.
  So doing this fully correctly would require a depth-3 search, which would significantly increase
  the cost of the search.
  So I just don't do that.
</p>

<p>
  Importantly, the computer will never generate an illegal reply.
  This just means that, in some cases, the reply it generates won't be quite as strong as if it had
  done a full depth-2 minimax search because it might assume the opponent could make an illegal move.
</p>


<br>
<h2>Everything Else</h2>
      <p>
        There's a lot more that goes into the program than this. If you're curious I'd encourage you to read through the source
        code on <a href="https://github.com/carlini/regex-chess">GitHub</a>.
        Among the things I think are particularly interesting that I didn't talk about here:
      </p>

      <ul>
        <li>How to parallelize move generation for sliding pieces like bishops, rooks, and queens all at exactly the same time</li>
        <li>How castling is implemented with a specialized "is this square under attack" procedures</li>
        <li>Conversion to and from the FEN chess board to one has a single variable per square</li>
        <li>Detection of castling rights by tracking the king and rook positions</li>
        <li>The implementation of en passant capture detection and tracking</li>
        <li>An extensive 2000 lines of tests, validating the correctness of the engine over several thousand games</li>
      </ul>
        
        
      <br>
      <h2>Some Performance Tricks</h2>

      <p>
        Before I leave you, I thought I'd describe a few fun performance optimizations I made.
        My initial code would take roughly 30 minutes to generate a single human reply---perfect for chess by mail, less perfect for chess by browser.
        The final code, on my machine, takes somewhere between one and ten seconds depending on the position.
        So I managed about a factor of 100 speedup by making sure I got all the details right.
      </p>

      <p>
        (I've already shown you my favorite performance trick: parallel processing.
        Now I'll tell you a few more.)
      </p>

      <br>
      <h3>Delete Intermediate Variables</h3>

      <p>
        One of the biggest performance killers in the initial implementation was keeping
        too many intermediate variables around. Every variable lookup requires scanning
        through the entire state string to find the variable's value - an O(n) operation.
        Additionally, when we fork execution states, we need to copy all these variables,
        which bloats our memory usage significantly.
      </p>

      <p>
        By aggressively deleting variables as soon as they're no longer needed,
        and reusing variable names where possible, I was able to dramatically reduce
        both the computation time and memory usage. Evaluating a single move now
        takes about 300MB of internal state, down from 10GB in the unoptimized version.
      </p>
      
      <br>
      <h3>Fast Regex Matching</h3>
      <p>
        Recall earlier that I defined the conditional operator like this
      </p>

      <div><p><span>def</span><span> </span><span>cond</span><span>(tag):</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>return</span><span> [(r</span><span>"%(%\n#stack:\nTrue)"</span><span>,</span></p>
<p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>r</span><span>"%\1`"</span><span>),</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>(r</span><span>"%(\n#stack:\nFalse)"</span><span>,</span></p>
<p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>tag+r</span><span>"\1`"</span><span>),</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>(r</span><span>"\n(True|False)`\n"</span><span>,</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>"</span><span>\n</span><span>"</span><span>)]</span></p>
</div>

      <p>
        Why do you think the third regex is written this way? Wouldn't it just be shorter
        if it was like this instead:
      </p>
        
      <div><p><span>def</span><span> </span><span>cond</span><span>(tag):</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>return</span><span> [(r</span><span>"%(%\n#stack:\nTrue)"</span><span>,</span></p>
<p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>r</span><span>"%\1`"</span><span>),</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>(r</span><span>"%(\n#stack:\nFalse)"</span><span>,</span></p>
<p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>tag+r</span><span>"\1`"</span><span>),</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>(r</span><span>"(True|False)`\n"</span><span>,</span></p>
<p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>""</span><span>)]</span></p></div>

      <p>
        Notice the missing newline <code>\n</code>.
      </p>

      <p>
        As it turns out, the first version actually improves the efficiency of this
        instruction by a factor of two! By changing just one character!
        The reason for this is due to the details about how regular expressions work
        and how they're matched.
      </p>

      <p>
        There are going to be lots of True/False strings all throughout the code.
        But most of them will be part of variables, and not the top element on the stack.
        By requiring the preceding newline and "#stack:\n" prefix in our pattern,
        we ensure the regex engine can quickly skip over all the True/False strings
        that appear in variable names or values. This dramatically reduces the number
        of attempted matches the regex engine needs to make.
      </p>
        
      <br>
      <h3>Writing Special-Purpose Instructions</h3>

      <p>
        Sometimes it's worth writing specialized instructions rather than composing existing ones.
        For example, one of the slowest parts of the initial implementation was in a loop that scanned
        each of the squares to find all pieces of a given type. This required a bunch of conditionals,
        and within each conditional, a bunch of stack operations.
        But you can just smash the entire loop body into a single regex instruction:
      </p>

      <div><p><span>def</span><span> </span><span>do_piece_assign</span><span>(piece_chr, piece, x, y, pos):</span></p>
<p><span><span>&nbsp; &nbsp; </span></span><span>return</span><span> [(f</span><span>"%%([^%]*#</span><span>{pos}</span><span>: </span><span>{piece_chr}</span><span>[^%]*)"</span><span> +</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>f</span><span>"#</span><span>{piece}</span><span>x_lst: ([^</span><span>\n</span><span>]*)</span><span>\n</span><span>"</span><span> +</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>f</span><span>"#</span><span>{piece}</span><span>y_lst: ([^</span><span>\n</span><span>]*)</span><span>\n</span><span>"</span><span> +</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>f</span><span>"#</span><span>{piece}</span><span>pos_lst: ([^</span><span>\n</span><span>]*)</span><span>\n</span><span>"</span><span>,</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>fr</span><span>"%%\1#</span><span>{piece}</span><span>x_lst: </span><span>{x}</span><span>;\2\n"</span><span> +</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>fr</span><span>"#</span><span>{piece}</span><span>y_lst: </span><span>{y}</span><span>;\3\n"</span><span> +</span></p>
<p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>fr</span><span>"#</span><span>{piece}</span><span>pos_lst: </span><span>{pos}</span><span>;\4\n"</span><span>)]</span></p></div>

      <p>
        The final major optimization comes from maximizing how much we can do in parallel.
        Remember that our regex engine can process multiple states simultaneously---this means
        that instead of generating moves one at a time, we can generate them all at once.
      </p>

      <p>
        For example, when generating rook moves, instead of checking each direction
        sequentially, we can fork eight parallel states---one for each possible direction.
        (Notice that "up" and "down" are different directions.)
        Each state then moves its rook as far as it can go in that direction. This turns
        what would be a loop into a single parallel operation.
      </p>

      <p>
        Similarly, when evaluating positions, instead of scoring one position at a time,
        we create parallel states for each candidate position and evaluate them all simultaneously.
        This parallel evaluation is particularly effective because most of our operations
        (like counting piece values) work identically across all states.
      </p>

      <br>
      <h2>Conclusion</h2>

      <p>
        What do you want out of a conclusion to a blog post like this?
        I don't really have much to conclude.
        I guess I'll just say that
        I think more people should do entirely pointless things like this.
        It's a lot of fun,
        no one cares how long it takes you to finish,
        no one cares if it works or not,
        and incidentally, it teaches you more than you wanted to know about
        dozens of areas of computer science outside your field.
      </p>

      <p>
        I hope to have a few more entirely pointless things like this coming later this year.
        If you liked this sort of thing, you might find it fun to read this other article I wrote
        about <a href="https://github.com/carlini/printf-tac-toe">how to play tic-tac-toe</a>
        using C's <code>printf</code> statement,
        or how I wrote <a href="https://nicholas.carlini.com/writing/2019/javascript-doom-clone-13k.html">a Doom clone in 13kB of JavaScript</a>.
      </p>

    </o></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia's Project Digits is a 'personal AI supercomputer' (578 pts)]]></title>
            <link>https://techcrunch.com/2025/01/06/nvidias-project-digits-is-a-personal-ai-computer/</link>
            <guid>42619139</guid>
            <pubDate>Tue, 07 Jan 2025 04:14:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/01/06/nvidias-project-digits-is-a-personal-ai-computer/">https://techcrunch.com/2025/01/06/nvidias-project-digits-is-a-personal-ai-computer/</a>, See on <a href="https://news.ycombinator.com/item?id=42619139">Hacker News</a></p>
Couldn't get https://techcrunch.com/2025/01/06/nvidias-project-digits-is-a-personal-ai-computer/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia announces next-gen RTX 5090 and RTX 5080 GPUs (446 pts)]]></title>
            <link>https://www.theverge.com/2025/1/6/24337396/nvidia-rtx-5080-5090-5070-ti-5070-price-release-date</link>
            <guid>42618761</guid>
            <pubDate>Tue, 07 Jan 2025 03:12:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2025/1/6/24337396/nvidia-rtx-5080-5090-5070-ti-5070-price-release-date">https://www.theverge.com/2025/1/6/24337396/nvidia-rtx-5080-5090-5070-ti-5070-price-release-date</a>, See on <a href="https://news.ycombinator.com/item?id=42618761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Nvidia is officially <a href="https://go.skimresources.com/?id=1025X1701640&amp;xs=1&amp;url=https%3A%2F%2Fwww.nvidia.com%2Fen-gb%2Fgeforce%2Fnews%2Frtx-50-series-graphics-cards-gpu-laptop-announcements%2F">announcing its RTX 50-series</a> GPUs today. After months of leaks and rumors, the next-generation RTX Blackwell GPUs are now official, and there are four of them on the way. </p><p>Nvidia CEO Jensen Huang revealed the RTX 50-series GPUs during a CES keynote this evening, announcing a $1,999 RTX 5090, a $999 RTX 5080, a $749 RTX 5070 Ti, and a $549 RTX 5070. Nvidia’s new RTX 5090 and RTX 5080 GPUs will both be available on January 30th, with the RTX 5070 Ti and RTX 5070 to follow in February.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Nvidia’s RTX 5090 GPU is surprisingly small." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:3840x2160/376x212/filters:focal(1920x1080:1921x1081):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820626/geforce_rtx_5090_key_visual.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3840x2160/384x216/filters:focal(1920x1080:1921x1081):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820626/geforce_rtx_5090_key_visual.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3840x2160/415x233/filters:focal(1920x1080:1921x1081):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820626/geforce_rtx_5090_key_visual.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3840x2160/480x270/filters:focal(1920x1080:1921x1081):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820626/geforce_rtx_5090_key_visual.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3840x2160/540x304/filters:focal(1920x1080:1921x1081):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820626/geforce_rtx_5090_key_visual.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3840x2160/640x360/filters:focal(1920x1080:1921x1081):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820626/geforce_rtx_5090_key_visual.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3840x2160/750x422/filters:focal(1920x1080:1921x1081):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820626/geforce_rtx_5090_key_visual.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3840x2160/828x466/filters:focal(1920x1080:1921x1081):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820626/geforce_rtx_5090_key_visual.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3840x2160/1080x608/filters:focal(1920x1080:1921x1081):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820626/geforce_rtx_5090_key_visual.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3840x2160/1200x675/filters:focal(1920x1080:1921x1081):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820626/geforce_rtx_5090_key_visual.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3840x2160/1440x810/filters:focal(1920x1080:1921x1081):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820626/geforce_rtx_5090_key_visual.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3840x2160/1920x1080/filters:focal(1920x1080:1921x1081):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820626/geforce_rtx_5090_key_visual.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3840x2160/2048x1152/filters:focal(1920x1080:1921x1081):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820626/geforce_rtx_5090_key_visual.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3840x2160/2400x1350/filters:focal(1920x1080:1921x1081):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820626/geforce_rtx_5090_key_visual.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:3840x2160/2400x1350/filters:focal(1920x1080:1921x1081):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820626/geforce_rtx_5090_key_visual.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>Nvidia’s RTX 5090 GPU is surprisingly small.</em></figcaption> <p><cite>Image: Nvidia</cite></p></div></div><p>The RTX 50-series GPUs include a new design for the Founders Edition, with just two double flow-through fans, a 3D vapor chamber, and GDDR7 memory. All of the RTX 50-series cards are PCIe Gen 5 and include DisplayPort 2.1b connectors to drive displays up to 8K and 165Hz. </p><p>Surprisingly, the <a href="https://go.skimresources.com/?id=1025X1701640&amp;xs=1&amp;url=https%3A%2F%2Fwww.nvidia.com%2Fen-gb%2Fgeforce%2Fgraphics-cards%2F50-series%2Frtx-5090%2F">RTX 5090 Founders Edition</a> will be a two-slot GPU and will be capable of fitting inside small form factor PCs — a big departure from the size of the RTX 4090. The RTX 5090 has 32GB of GDDR7, a memory bandwidth of 1,792GB/sec, and a massive 21,760 CUDA cores. </p><p>This all adds up to a GPU that Nvidia says will be two times faster than the RTX 4090, thanks to DLSS 4 and the Blackwell architecture. But it will come at a cost of power consumption, as Nvidia says the RTX 5090 will have a total graphics power of 575 watts and a recommended PSU requirement of 1000 watts. That’s 125 watts more than the RTX 4090, but hopefully the RTX 5090 will be a lot more power efficient so that you’ll rarely be using the full 575 watts.</p><p>Nvidia demonstrated <em>Cyberpunk 2077</em> running on an RTX 5090 with DLSS 4 at 238fps compared to 106fps on an RTX 4090 with DLSS 3.5. Both GPUs are running the game with full ray tracing enabled.</p><p><a href="https://go.skimresources.com/?id=1025X1701640&amp;xs=1&amp;url=https%3A%2F%2Fwww.nvidia.com%2Fen-gb%2Fgeforce%2Fgraphics-cards%2F50-series%2Frtx-5080%2F">The RTX 5080</a> is designed to be twice as fast as the RTX 4080 and will include 16GB of GDDR7 memory, a memory bandwidth of 960GB/sec, and 10,752 CUDA cores. The RTX 5080 will have a total graphics power of 360 watts and Nvidia is recommending a 850-watt power supply. Nvidia is promising big performance gains with the RTX 5080 over the previous RTX 4080 model as a result of these specs.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="RTX 5090 performance." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/376x247/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820676/nvidia_geforce_rtx_5090_performance_chart.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/384x252/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820676/nvidia_geforce_rtx_5090_performance_chart.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/415x273/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820676/nvidia_geforce_rtx_5090_performance_chart.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/480x315/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820676/nvidia_geforce_rtx_5090_performance_chart.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/540x355/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820676/nvidia_geforce_rtx_5090_performance_chart.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/640x420/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820676/nvidia_geforce_rtx_5090_performance_chart.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/750x493/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820676/nvidia_geforce_rtx_5090_performance_chart.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/828x544/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820676/nvidia_geforce_rtx_5090_performance_chart.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/1080x709/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820676/nvidia_geforce_rtx_5090_performance_chart.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/1200x788/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820676/nvidia_geforce_rtx_5090_performance_chart.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/1440x946/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820676/nvidia_geforce_rtx_5090_performance_chart.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/1920x1261/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820676/nvidia_geforce_rtx_5090_performance_chart.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/2048x1345/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820676/nvidia_geforce_rtx_5090_performance_chart.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/2400x1576/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820676/nvidia_geforce_rtx_5090_performance_chart.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/2400x1576/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820676/nvidia_geforce_rtx_5090_performance_chart.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>RTX 5090 performance.</em></figcaption> <p><cite>Image: Nvidia</cite></p></div></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Nvidia’s RTX 5080 performance promises." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/376x247/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820646/nvidia_geforce_rtx_5080_performance_chart.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/384x252/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820646/nvidia_geforce_rtx_5080_performance_chart.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/415x273/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820646/nvidia_geforce_rtx_5080_performance_chart.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/480x315/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820646/nvidia_geforce_rtx_5080_performance_chart.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/540x355/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820646/nvidia_geforce_rtx_5080_performance_chart.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/640x420/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820646/nvidia_geforce_rtx_5080_performance_chart.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/750x493/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820646/nvidia_geforce_rtx_5080_performance_chart.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/828x544/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820646/nvidia_geforce_rtx_5080_performance_chart.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/1080x709/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820646/nvidia_geforce_rtx_5080_performance_chart.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/1200x788/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820646/nvidia_geforce_rtx_5080_performance_chart.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/1440x946/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820646/nvidia_geforce_rtx_5080_performance_chart.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/1920x1261/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820646/nvidia_geforce_rtx_5080_performance_chart.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/2048x1345/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820646/nvidia_geforce_rtx_5080_performance_chart.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/2400x1576/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820646/nvidia_geforce_rtx_5080_performance_chart.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2687x1765/2400x1576/filters:focal(1344x883:1345x884):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820646/nvidia_geforce_rtx_5080_performance_chart.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>Nvidia’s RTX 5080 performance promises.</em></figcaption> <p><cite>Image: Nvidia</cite></p></div></div><p>Nvidia is also launching an <a href="https://go.skimresources.com/?id=1025X1701640&amp;xs=1&amp;url=https%3A%2F%2Fwww.nvidia.com%2Fen-gb%2Fgeforce%2Fgraphics-cards%2F50-series%2Frtx-5070-family%2F">RTX 5070 Ti</a> and <a href="https://go.skimresources.com/?id=1025X1701640&amp;xs=1&amp;url=https%3A%2F%2Fwww.nvidia.com%2Fen-gb%2Fgeforce%2Fgraphics-cards%2F50-series%2Frtx-5070-family%2F">RTX 5070</a>. The RTX 5070 Ti includes 16GB of GDDR7 memory, a memory bandwidth of 896GB/s, and 8,960 CUDA cores. The RTX 5070 has 12GB of GDDR7, a memory bandwidth of 672 GB/sec, and 6,144 CUDA cores. The RTX 5070 Ti will have a total graphics power of 300 watts and require a 750-watt PSU, while the RTX 5070 has a total graphics power of 250 watts and only needs a 650-watt PSU.</p><p>Nvidia says the RTX 5070 Ti will be 2x faster than the RTX 4070 Ti, and the RTX 5070 should be twice as fast as the RTX 4070. Huang even claimed on stage at CES that the RTX 5070 will deliver “RTX 4090 performance at $549,” but this will undoubtedly be because of DLSS 4 improvements and not pure rasterization performance.</p><p>Nvidia is also bringing its RTX 50-series to laptops, with the RTX 5090 laptop GPU debuting with 24GB of GDDR7 memory. The RTX 5080 laptop GPU will ship with 16GB of GDDR7 memory, the RTX 5070 Ti with 12GB of GDDR7 memory, and the RTX 5070 with just 8GB of GDDR7 memory. RTX 50-series laptops will be available starting in March from a variety of PC makers.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="RTX 50-series laptops are coming in March." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2400x1260/376x197/filters:focal(1200x630:1201x631):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820645/geforce_rtx_50_series_laptops_key_visual.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2400x1260/384x202/filters:focal(1200x630:1201x631):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820645/geforce_rtx_50_series_laptops_key_visual.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2400x1260/415x218/filters:focal(1200x630:1201x631):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820645/geforce_rtx_50_series_laptops_key_visual.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2400x1260/480x252/filters:focal(1200x630:1201x631):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820645/geforce_rtx_50_series_laptops_key_visual.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2400x1260/540x284/filters:focal(1200x630:1201x631):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820645/geforce_rtx_50_series_laptops_key_visual.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2400x1260/640x336/filters:focal(1200x630:1201x631):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820645/geforce_rtx_50_series_laptops_key_visual.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2400x1260/750x394/filters:focal(1200x630:1201x631):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820645/geforce_rtx_50_series_laptops_key_visual.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2400x1260/828x435/filters:focal(1200x630:1201x631):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820645/geforce_rtx_50_series_laptops_key_visual.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2400x1260/1080x567/filters:focal(1200x630:1201x631):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820645/geforce_rtx_50_series_laptops_key_visual.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2400x1260/1200x630/filters:focal(1200x630:1201x631):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820645/geforce_rtx_50_series_laptops_key_visual.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2400x1260/1440x756/filters:focal(1200x630:1201x631):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820645/geforce_rtx_50_series_laptops_key_visual.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2400x1260/1920x1008/filters:focal(1200x630:1201x631):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820645/geforce_rtx_50_series_laptops_key_visual.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2400x1260/2048x1075/filters:focal(1200x630:1201x631):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820645/geforce_rtx_50_series_laptops_key_visual.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2400x1260/2400x1260/filters:focal(1200x630:1201x631):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820645/geforce_rtx_50_series_laptops_key_visual.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2400x1260/2400x1260/filters:focal(1200x630:1201x631):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25820645/geforce_rtx_50_series_laptops_key_visual.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>RTX 50-series laptops are coming in March.</em></figcaption> <p><cite>Image: Nvidia</cite></p></div></div><p>Huang demonstrated Nvidia’s RTX Blackwell GPUs with a real-time rendering demo at the beginning of the company’s CES keynote today. The demo included new RTX Neural Materials, RTX Neural Faces, text to animation, and even DLSS 4. “The new generation of DLSS can generate beyond frames, it can predict the future,” says Huang. “We used GeForce to enable AI, and now AI is revolutionizing GeForce.”</p><p>Nvidia’s new RTX Neural Shaders can be used to compress textures in games, while RTX Neural Faces aim to improve face quality using generative AI. The next generation of DLSS includes Multi Frame Generation, which generates up to three additional frames per traditional frame and can multiply frame rates by up to 8x over traditional rendering, according to Nvidia.</p><p>DLSS 4 also includes a real-time application of transformers to improve image quality, reduce ghosting, and add higher detail in motion. The DLSS 4 upgrade will even work on existing RTX GPUs, as features have been upgraded to the new transformer AI models. You can <a href="https://www.theverge.com/2025/1/6/24337402/nvidia-dlss-4-upscaling-announcement-ces-2025">read more about DLSS 4 right here</a>.</p><p>Nvidia’s RTX 50-series announcement comes more than two years after the <a href="https://www.theverge.com/2022/9/20/23362653/nvidia-rtx-4080-4090-price-release-date">RTX 4090 and RTX 4080 were announced</a>, based on Nvidia’s Ada Lovelace architecture. Nvidia’s RTX 40-series of GPUs focused on improving ray tracing with Deep Learning Super Sampling (DLSS)  version 3, and the RTX 4090 delivered some <a href="https://www.theverge.com/23398201/nvidia-rtx-4090-review-test-benchmark">truly impressive performance gains</a> over the previous RTX 3090 GPU.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Roman Empire's use of lead lowered IQ levels across Europe, study finds (199 pts)]]></title>
            <link>https://www.theguardian.com/science/2025/jan/06/roman-empires-use-of-lead-lowered-iq-levels-across-europe-study-finds</link>
            <guid>42618625</guid>
            <pubDate>Tue, 07 Jan 2025 02:48:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/science/2025/jan/06/roman-empires-use-of-lead-lowered-iq-levels-across-europe-study-finds">https://www.theguardian.com/science/2025/jan/06/roman-empires-use-of-lead-lowered-iq-levels-across-europe-study-finds</a>, See on <a href="https://news.ycombinator.com/item?id=42618625">Hacker News</a></p>
Couldn't get https://www.theguardian.com/science/2025/jan/06/roman-empires-use-of-lead-lowered-iq-levels-across-europe-study-finds: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>