<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 03 Dec 2023 19:00:09 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Lobsters (132 pts)]]></title>
            <link>https://github.com/lobsters/lobsters</link>
            <guid>38508282</guid>
            <pubDate>Sun, 03 Dec 2023 16:34:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/lobsters/lobsters">https://github.com/lobsters/lobsters</a>, See on <a href="https://news.ycombinator.com/item?id=38508282">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h3 tabindex="-1" dir="auto">Lobsters Rails Project <a target="_blank" rel="noopener noreferrer" href="https://github.com/lobsters/lobsters/actions/workflows/check.yml/badge.svg"><img src="https://github.com/lobsters/lobsters/actions/workflows/check.yml/badge.svg" alt="build status"></a></h3>
<p dir="auto">This is the
<a href="https://web.archive.org/web/20230213161624/https://old.reddit.com/r/rails/comments/6jz7tq/source_code_lobsters_a_hacker_news_clone_built/" rel="nofollow">quite sad</a>
source code to the
<a href="https://twitter.com/webshitweekly/status/1399935275057389571" rel="nofollow">ghost town</a> at
<a href="https://lobste.rs/" rel="nofollow">https://lobste.rs</a>.
It is a Rails codebase and uses a SQL (MariaDB in production) backend for the database.</p>
<p dir="auto">You are free to use this code to start your own <a href="https://github.com/lobsters/lobsters/wiki">sister site</a>
because the code is available under a <a href="https://github.com/lobsters/lobsters/blob/master/LICENSE">permissive license</a> (3-clause BSD).
We welcome bug reports and code contributions that help use improve <a href="https://lobste.rs/" rel="nofollow">lobste.rs</a>.
As a volunteer project we're reluctant to take on work that's not useful to our site, so please understand if we don't want to adopt your custom feature.</p>
<h4 tabindex="-1" dir="auto">Contributing bugfixes and new features</h4>
<p dir="auto">We'd love to have your help.
Please see the <a href="https://github.com/lobsters/lobsters/blob/master/CONTRIBUTING.md">CONTRIBUTING</a> file for details.
If you have questions, there is usually someone in <a href="https://lobste.rs/chat" rel="nofollow">our chat room</a> who's familiar with the code.</p>
<h4 tabindex="-1" dir="auto">Initial setup</h4>
<p dir="auto">Use the steps below for a local install or
<a href="https://github.com/lobsters/lobsters-ansible">lobsters-ansible</a> for our production deployment config.
There's an external project <a href="https://github.com/utensils/docker-lobsters">docker-lobsters</a> if you want to use Docker.</p>
<ul dir="auto">
<li>
<p dir="auto">Install the Ruby version specified in <a href="https://github.com/lobsters/lobsters/blob/master/.ruby-version">.ruby-version</a></p>
</li>
<li>
<p dir="auto">Checkout the lobsters git tree from Github</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ git clone git@github.com:lobsters/lobsters.git
$ cd lobsters
lobsters$"><pre>$ git clone git@github.com:lobsters/lobsters.git
$ <span>cd</span> lobsters
lobsters$</pre></div>
</li>
<li>
<p dir="auto">Install Nodejs, needed (or other execjs) for uglifier</p>
<div dir="auto" data-snippet-clipboard-copy-content="Fedora: sudo yum install nodejs
Ubuntu: sudo apt-get install nodejs
OSX: brew install nodejs"><pre>Fedora: sudo yum install nodejs
Ubuntu: sudo apt-get install nodejs
OSX: brew install nodejs</pre></div>
</li>
<li>
<p dir="auto">Run Bundler to install/bundle gems needed by the project:</p>

<ul dir="auto">
<li>If when installing the <code>mysql2</code> gem on macOS, you see
<code>ld: library not found for -l-lpthread</code> in the output, see
<a href="https://stackoverflow.com/a/44790834/204052" rel="nofollow">this solution</a> for a fix.
You might also see <code>ld: library not found for -lssl</code> if you're using
macOS 10.4+ and Homebrew <code>openssl</code>, in which case see
<a href="https://stackoverflow.com/a/39628463/1042144" rel="nofollow">this solution</a>.</li>
</ul>
</li>
<li>
<p dir="auto">Create a MySQL (other DBs supported by ActiveRecord may work, only MySQL and
MariaDB have been tested) database, username, and password and put them in a
<code>config/database.yml</code> file.  You will also want a separate database for
running tests:</p>
<div dir="auto" data-snippet-clipboard-copy-content="development:
  adapter: mysql2
  encoding: utf8mb4
  reconnect: false
  database: lobsters_dev
  socket: /tmp/mysql.sock
  username: *dev_username*
  password: *dev_password*
  
test:
  adapter: mysql2
  encoding: utf8mb4
  reconnect: false
  database: lobsters_test
  socket: /tmp/mysql.sock
  username: *test_username*
  password: *test_password*"><pre><span>development</span>:
  <span>adapter</span>: <span>mysql2</span>
  <span>encoding</span>: <span>utf8mb4</span>
  <span>reconnect</span>: <span>false</span>
  <span>database</span>: <span>lobsters_dev</span>
  <span>socket</span>: <span>/tmp/mysql.sock</span>
  <span>username</span>: <span>*dev_username*</span>
  <span>password</span>: <span>*dev_password*</span>
  
<span>test</span>:
  <span>adapter</span>: <span>mysql2</span>
  <span>encoding</span>: <span>utf8mb4</span>
  <span>reconnect</span>: <span>false</span>
  <span>database</span>: <span>lobsters_test</span>
  <span>socket</span>: <span>/tmp/mysql.sock</span>
  <span>username</span>: <span>*test_username*</span>
  <span>password</span>: <span>*test_password*</span></pre></div>
</li>
<li>
<p dir="auto">Load the schema into the new database:</p>
<div dir="auto" data-snippet-clipboard-copy-content="lobsters$ rails db:schema:load"><pre>lobsters$ rails db:schema:load</pre></div>
</li>
<li>
<p dir="auto">On your production server, copy <code>config/initializers/production.rb.sample</code>
to <code>config/initalizers/production.rb</code> and customize it with your site's
<code>domain</code> and <code>name</code>. (You don't need this on your dev machine).</p>
</li>
<li>
<p dir="auto">Seed the database to create an initial administrator user, the <code>inactive-user</code>, and at least one tag:</p>

</li>
<li>
<p dir="auto">On your personal computer, you can add some sample data and run the Rails server in development mode.
You should be able to login to <code>http://localhost:3000</code> with your new <code>test</code> user:</p>
<div dir="auto" data-snippet-clipboard-copy-content="lobsters$ rails fake_data
lobsters$ rails server"><pre>lobsters$ rails fake_data
lobsters$ rails server</pre></div>
</li>
<li>
<p dir="auto">Deploying the site in production requires setting up a web server and running the app in production mode.
There are more tools and options available than we can describe; find a guide or an expert.
The lobsters-ansible repo has our config files to crib from. Some app-specific notes:</p>
</li>
<li>
<p dir="auto">Set up crontab or another scheduler to run regular jobs:</p>
<div data-snippet-clipboard-copy-content="*/5 * * * *  cd /path/to/lobsters &amp;&amp; env RAILS_ENV=production sh -c 'bundle exec ruby script/mail_new_activity; bundle exec ruby script/post_to_twitter; bundle exec ruby script/traffic_range'"><pre><code>*/5 * * * *  cd /path/to/lobsters &amp;&amp; env RAILS_ENV=production sh -c 'bundle exec ruby script/mail_new_activity; bundle exec ruby script/post_to_twitter; bundle exec ruby script/traffic_range'
</code></pre></div>
</li>
<li>
<p dir="auto">See <code>config/initializers/production.rb.sample</code> for GitHub/Twitter integration help.</p>
</li>
<li>
<p dir="auto">You probably want to use <a href="https://lobste.rs/s/dbm2d4" rel="nofollow">git-imerge</a> to pull in
changes from Lobsters to your site.</p>
</li>
</ul>
<h4 tabindex="-1" dir="auto">Administration</h4>
<p dir="auto">Basic moderation happens on-site, but most other administrative tasks require use of the rails console in production.
Administrators can create and edit tags at <code>/tags</code>.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLM Visualization (456 pts)]]></title>
            <link>https://bbycroft.net/llm</link>
            <guid>38507672</guid>
            <pubDate>Sun, 03 Dec 2023 15:22:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bbycroft.net/llm">https://bbycroft.net/llm</a>, See on <a href="https://news.ycombinator.com/item?id=38507672">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>LLM Visualization</p><div><p><a href="https://bbycroft.net/">Home</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The worst thing about Jenkins is that it works (118 pts)]]></title>
            <link>http://twitchard.github.io/posts/2019-06-21-life-is-too-short-for-jenkins.html</link>
            <guid>38507381</guid>
            <pubDate>Sun, 03 Dec 2023 14:40:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://twitchard.github.io/posts/2019-06-21-life-is-too-short-for-jenkins.html">http://twitchard.github.io/posts/2019-06-21-life-is-too-short-for-jenkins.html</a>, See on <a href="https://news.ycombinator.com/item?id=38507381">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
            <article>
    
    <section>
        <p><img src="http://twitchard.github.io/images/dropCapA.jpg" alt="A">bout nine months ago, I requested a transfer to the team working on the company’s CI tooling. In my judgement, CI was a major productivity blocker for the whole organization, and I hoped I would be able to help improve it and make a broad, positive impact.</p>
<p>At that time, CI was in Jenkins 1, which had three major problems:</p>
<ol type="1">
<li><p>Everybody’s CI pipeline was described in text boxes in the Jenkins UI, which meant they were not version controlled, discoverable, and editing/testing new configurations was a difficult experience.</p></li>
<li><p>The web interface was dated and unpleasant to use.</p></li>
<li><p>Developers had little control over the environment in which their jobs ran, because the VMs operating as Jenkins nodes were centrally managed.</p></li>
</ol>
<p>My team considered two options.</p>
<h3 id="option-1-switch-tools">Option 1: Switch tools</h3>
<p>The head of SRE championed Gitlab CI. I resisted this idea because I, the relatively inexperienced manager of a nascent team, was daunted by the prospect of trying to supplant Jenkins, Github, and JIRA all at once.</p>
<p>On a previous team I had used Concourse CI to some extent, but I wasn’t really blown away by the experience. Travis and Circle were mentioned. I was a fool. I should have committed to seriously researching some of the contenders and making a more informed decision, but I lacked the willpower and the discernment.</p>
<h3 id="option-2-upgrade-to-jenkins-2">Option 2: Upgrade to Jenkins 2</h3>
<p>On the face of it, Jenkins 2 seemed to meet all our needs. It:</p>
<ol type="1">
<li><p>Supports defining your CI job as a “declarative pipeline” that can live as a Jenkinsfile in the root of your repository. Hooray configuration as code!</p></li>
<li><p>Boasts a UX facelift called “Blue Ocean” that looks more modern.</p></li>
<li><p>Permits pipelines to request to be run on a docker “agent”, which lets application developers control the environment on which their job is run by specifying a Docker image or Dockerfile.</p></li>
</ol>
<h2 id="a-taxonomy-of-mistakes">A Taxonomy of Mistakes</h2>
<p>The worst mistakes come in two distinct flavors: catastrophic and insidious.</p>
<p>A catastrophic mistake is like triggering an outage, or deleting production data. The moment you realize what you’ve done is the worst single moment in your career. Your heart pounds in your chest. Is this a nightmare? Maybe in a second, you will wake up? No, it’s real. Hopefully, you’ve got a healthy culture at work, and you desperately describe the situation to your teammates, who rally to your side. Somebody with a cool head thinks of some way to make the best of things, and somehow – maybe that night, maybe the next day – you make it through. Things go back to normal. You write a postmortem, count your losses, and go back to work – a little less innocent, and a little wiser.</p>
<p>An insidious mistake, by contrast, does not reveal itself in a moment. It makes you suffer a little bit here, and a little bit there, until one day you wake up and you realize that there is a gaping hole where your humanity used to be. You are a miserable husk of a man, with cruelty on your lips and bile in your heart. You still greet your colleagues with that jolly smile of yours – but the sweetness in your smile is the saccharine of cynicism, not the honeyed optimism as it was in the days before, when life was cheerful and your burden was light. The light in your eyes used to be the hope for a better tomorrow. Now it is the glint of madness.</p>
<h2 id="whats-wrong-with-jenkins">What’s wrong with Jenkins</h2>
<p>Choosing Jenkins was the insidious kind of mistake. Warning – I’m going to rant for many, many paragraphs. My advice is to skim.</p>
<p>The worst thing about Jenkins is that it works. It can meet your needs. With a liiittle more effort, or by adopting sliiiightly lower standards, or with a liiiiitle more tolerance for pain, you can always get Jenkins to do aaaaalmost what you want it to. But let’s talk specifics. Jenkins features:</p>
<h3 id="high-indirection-between-you-and-the-execution-of-your-code.">High indirection between you and the execution of your code.</h3>
<p>For me, the bulk of the actual work of a CI pipeline takes the form of shell commands. are typically executed inside shell commands. In Jenkins pipeline, there is a ‘sh’ “step” that executes the shell. For example</p>
<div id="cb1"><pre><code><span id="cb1-1">  sh <span>'npm install'</span></span>
<span id="cb1-2">  sh <span>'make'</span></span></code></pre></div>
<p>So instead of writing Bash directly, you’re writing Bash inside Groovy. But:</p>
<ul>
<li>Your editor won’t syntax highlight the Bash inside Groovy.</li>
<li>You can’t run “shellcheck” (or any sort of Linter) on the Bash inside the groovy.</li>
<li>You can’t very easily execute your shell commands to test them.</li>
</ul>
<p>There are two ways to try and address this:</p>
<ol type="1">
<li>Write your shell in a separate Bash file that you execute from Groovy, avoid putting it inline in your pipeline.</li>
<li>Try to avoid writing shell at all – instead, implement everything as Groovy methods.</li>
</ol>
<p>I think #1 is actually the better approach. We started out there. The trouble was, we started wanting to abstract our pipeline steps and turn them into “shared libraries” and so we gravitated toward #2, so that we could share steps easily across pipelines.</p>
<p>The trouble is: Groovy is a much, much worse language for executing commands than Bash. Bash is interpreted, has a REPL that is great for experimentation, does require a ton of imports, and has lightweight syntax. Groovy has none of these things. The way that developers test their Groovy steps is by triggering a job on the remote Jenkins server to run them. The feedback loop is 2 orders of magnitude slower than it is for just executing Bash locally.</p>
<p>Are there ways to execute the Groovy steps locally? The way you’re supposed to do it is with <a href="https://github.com/jenkinsci/JenkinsPipelineUnit">JenkinsPipelineUnit</a> which is a very good idea – it lets you write unit tests against your Jenkins Pipeline, and gives you an interface for mocking various Jenkins things. But there are two problems:</p>
<ol type="1">
<li>As noted in the README, Groovy doesn’t run the same way on Jenkins as it does in your unit test, because the groovy DSL is “serialized” by Jenkins before running.</li>
<li>“Declarative” pipelines <a href="https://github.com/jenkinsci/JenkinsPipelineUnit/issues/10">are not supported</a> – a huge problem for us, since that’s how we’ve implemented all our stuff, since it seemed to be the newest and most modern thing to be doing.</li>
</ol>
<p>So basically, that’s a huge bust. Especially since we were not a Java shop. My team was barely able to kind of piece this together because it’s our job to work on the CI system, but there is absolutely no way that any of the PHP/Javascript/Golang/Python application developers who need to write pipelines will be able to download Gradle, figure out they need to run gradle init, install the pipeline unit testing library, figure out the proper way to initialize the “PipelineTestHelper”.</p>
<p>So we’re basically resigned to the workflow of running shell commands defined in methods used by a DSL embedded in groovy transmitted to the CI master node, serialized and passed to a CI worker node and executed there.</p>
<p>There’s a “replay script” feature that lets you edit your pipeline right in the web interface, which helps cut down on the feedback time a little bit if you don’t care about version controlling your changes or being able to use your own editor/tools. I personally am not willing to make that sacrifice.</p>
<p>TL;DR, the feedback loop sucks. You’ll never be able to effectively test any of the code running in your pipeline. Your best bet is to build it all entirely in Bash, build your own mechanism for testing it and sharing functionality. The ability to write Groovy shared libraries is a trap and leads only to misery.</p>
<h3 id="low-level-of-discoverability">Low level of discoverability</h3>
<p>A lot of functionality that Jenkins has in the web UI – especially the functionality that comes through plugins – is also possible to define in pipelines, but the means for doing this is not well-documented. For example, there’s this <a href="https://github.com/jenkinsci/throttle-concurrent-builds-plugin">plugin</a> that permits you to “throttle” a job so that multiple jobs don’t fire at once. that you can see inside the UI. After probably half a day of Googling, trial and error, and thanks to a stroke of luck, I figured out that I could accomplish what I wanted by putting the following in my Jenkinsfile:</p>
<div id="cb2"><pre><code><span id="cb2-1"><span>properties</span><span>([[</span></span>
<span id="cb2-2">        $class<span>:</span> <span>'ThrottleJobProperty'</span><span>,</span></span>
<span id="cb2-3">        maxConcurrentTotal<span>:</span> <span>5</span><span>,</span></span>
<span id="cb2-4">        throttleEnabled<span>:</span> <span>true</span><span>,</span></span>
<span id="cb2-5">        throttleOption<span>:</span> <span>'project'</span></span>
<span id="cb2-6"><span>]])</span></span></code></pre></div>
<p>Maybe if I were a Java/Groovy expert I could have read the source code for the plugin and determined this was possible. But I shouldn’t have to be. And the application developers trying to implement their own pipelines for there code <em>definitely</em> shouldn’t have to be.</p>
<p>There are two tricks I’ve developed to help the discovery of these magic incantations. Trick 1 is the “Snippet Generator”, which is basically a drop down box in the Jenkins UI with a pretty comprehensive list of options to explore and can help you find what you need maybe 15% of the time. Even if you can’t produce something usable, the snippet generator can give you an idea what to Google for.</p>
<p>Trick 2 I’ve had much more success with. Use the snippet generator or Google things just enough to find a function name or keyword relevant to whatever you’re trying to do. Then, go to <code>github.com/search</code> and put <code>filename:Jenkinsfile &lt;keyword&gt;</code>. You’ll probably find something you can copy and paste. It’s worked 90% of the time, for me.</p>
<p>Really, this experience sucks. I don’t really do any sort of other engineering like that, because sane systems have better documentation, more obvious abstractions, and better interactivity. I feel like a script kiddie, blindly typing incantations in to make magic happen through trial and error. Hugely demoralizing.</p>
<h3 id="blue-ocean-is-incomplete-and-unintuitive">Blue Ocean is Incomplete and Unintuitive</h3>
<p>Blue Ocean looks more modern than the classic Jenkins UI – I’ll give it that. Unfortunately, it’s missing functionality, so you’ll have to use and become familiar with the classic Jenkins UI anyway. It’s also just not a pleasant UI to use! I’m not much of a design person or a front-end developer, so I can’t articulate precisely what it is that makes the interface unpleasant, but it always seems to take several clicks in places I don’t expect in order to do what I’m trying to do – usually, I just want to run the build, or see the output of the build.</p>
<h3 id="docker">Docker</h3>
<p>It is possible to have your builds run inside docker containers. Jenkins 2 does let the job author specify a docker image, or dockerfile – even kubernetes configurations for autoscaling! So, in principle, the problem of letting job authors own their job’s environment is solved.</p>
<p>The only problem is that this problem is solved by incorporating the idea of a “Jenkins worker” INTO the idea of a Docker container. These two ideas don’t always play well together. For example, one thing I kind of expected/hoped for was that, defining a Jenkinsfile to use a Dockerfile, and then giving it a build step like</p>

<p>would be approximately the same thing as</p>
<div id="cb4"><pre><code><span id="cb4-1"><span>docker</span> build . <span>-t</span> foo</span>
<span id="cb4-2"><span>docker</span> run foo make test</span></code></pre></div>
<p>But it different in one very significant way. With <code>docker run</code>, your cwd is whatever the Dockerfile defined. In a Jenkins job, the cwd is the Jenkins workspace – which is bind mounted in from the host node. Basically, Jenkins tries to <em>turn your docker container</em> into a regular old Jenkins worker. This makes a degree of sense, but has a number of inconveniences.</p>
<ol type="1">
<li>You probably can’t be root inside your docker container. If your build produces any sort of persistent artifact in the workspace, that artifact will be owned by root and will end up on the filesystem of the host. Jenkins on the host doesn’t run as root, so it doesn’t have permissions to wipe the workspace when it needs to, and you’ll get janky permissions errors.</li>
</ol>
<p>So what we ended up doing is creating a user inside the dockerfile with the same UID as the user that Jenkins runs as. Passed through via a build arg. This is not something I’d really mind doing once – but you have to do this trick for <em>every single job</em> that is defined. So it’s not just something we could solve for everybody on the CI team. Every application developer who wanted to define their own job ran up against this problem. And it’s a confusing problem – it took me days to really make sense of what Jenkins was trying to do. We documented it internally about as well as we could, but still we ended up guiding probably at least a dozen application developers through this particular confusion.</p>
<ol start="2" type="1">
<li>You’re probably going to have to define a docker image <em>just</em> for the build.</li>
</ol>
<p>One of the mostly-false promises of Docker, as it was sold to me by the true believers who introduced me to it, was that, if you do it right, you can run the same docker image, and therefore have basically the same environment in production, in CI, and on your local development machine. I’ve never actually see this happen, but I can tell you right now – you’re going to have to define a special docker image just for Jenkins, because of how strangely it interacts with the world of containers.</p>
<p>Lest this turn into a rant against Docker – a tool I am also seriously disappointed with, I’ll end here. Long story short, we used Jenkins 2. It kind of solved our problems. So now our problems are kind of solved, which is the worst kind of solved.</p>
<h3 id="postlude">Postlude</h3>
<p>It’s a month after I started writing this post. Now I work at a different, bigger company. I no longer work on CI. What’s more, one of the principles I had never even thought to question at my old company – “everybody should be writing and maintaining their own CI jobs” – is just not at play here. There’s a team that seems almost completely to own CI and all CI jobs. I’m in week 4, and I know Jenkins is there, somewhere, lurking behind the scenes. But I have never interacted with it, and it seems like there are a lot of smart people working so that I never, ever need to. What a strange new world this is.</p>
    <hr>

    
    </section>
</article>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI Committed to Buying $51M of AI Chips from a Startup Backed by Sam Altman (220 pts)]]></title>
            <link>https://www.wired.com/story/openai-buy-ai-chips-startup-sam-altman/</link>
            <guid>38506660</guid>
            <pubDate>Sun, 03 Dec 2023 12:42:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/openai-buy-ai-chips-startup-sam-altman/">https://www.wired.com/story/openai-buy-ai-chips-startup-sam-altman/</a>, See on <a href="https://news.ycombinator.com/item?id=38506660">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Sam Altman was reinstated soon after being <a href="https://www.wired.com/story/sam-altman-officially-returns-to-openai-board-seat-microsoft/">fired as OpenAI CEO</a> last month, but still stood to gain had the company continued to develop <a href="https://www.wired.com/tag/chatgpt/">ChatGPT</a> without him. During Altman’s tenure as CEO, OpenAI signed a letter of intent to spend $51 million on AI chips from a startup called Rain AI into which he has also invested personally.</p><p>Rain is based less than a mile from OpenAI’s headquarters in San Francisco and is working on a chip it calls a <a href="https://www.technologyreview.com/2013/12/16/174934/thinking-in-silicon/">neuromorphic</a> processing unit, or NPU, designed to <a data-offer-url="https://johnkoetsier.com/artificial-brain-neuromorphic-chip/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://johnkoetsier.com/artificial-brain-neuromorphic-chip/&quot;}" href="https://johnkoetsier.com/artificial-brain-neuromorphic-chip/" rel="nofollow noopener" target="_blank">replicate features of the human brain</a>. OpenAI in 2019 signed a nonbinding agreement to spend $51 million on the chips when they became available, according to a copy of the deal and Rain disclosures to investors this year seen by WIRED. Rain told investors Altman had personally invested more than $1 million into the company. The letter of intent has not been previously reported.</p><p>The investor documents said that Rain could get its first hardware to customers as early as October next year. OpenAI and Rain declined to comment.</p><div><p>OpenAI’s letter of intent with Rain shows how Altman’s web of personal investments can entangle with his duties as OpenAI CEO. His prior position leading startup incubator Y Combinator helped Altman become one of Silicon Valley’s most prominent dealmakers, investing in dozens of startups and acting as a broker between entrepreneurs and the world’s biggest companies. But the distraction and intermingling of his myriad pursuits played some role in his recent <a href="https://www.wired.com/story/openai-ceo-sam-altman-is-out-after-losing-confidence-of-board/">firing</a> by OpenAI’s board for uncandid communications, according to people involved in the situation but not authorized to discuss it.</p><p>The Rain deal also underscores OpenAI’s willingness to spend large sums to secure supplies of chips needed to underpin pioneering AI projects. Altman has complained publicly of a <a href="https://www.ft.com/content/dd9ba2f6-f509-42f0-8e97-4271c7b84ded">“brutal crunch”</a> for AI chips and their <a data-offer-url="https://twitter.com/sama/status/1599669571795185665" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://twitter.com/sama/status/1599669571795185665&quot;}" href="https://twitter.com/sama/status/1599669571795185665" rel="nofollow noopener" target="_blank">“eye-watering”</a> costs. OpenAI taps the powerful cloud of Microsoft, <a href="https://www.wired.com/story/microsoft-emerges-as-the-winner-in-openai-chaos/">its primary investor</a>, but has regularly shut off access to features of ChatGPT due to hardware constraints. According to a blog post about a closed door meeting he held with developers, Altman <a href="https://finance.yahoo.com/news/blog-post-detailed-sam-altman-142219663.html">has said</a> the pace of AI progress may be dependent on new chip designs and supply chains.</p></div><p>Rain touted its progress to potential investors earlier this year, projecting that as soon as this month it could “tape out” a test chip, a standard milestone in chip development referring to a design ready for fabrication. But the startup also has recently reshuffled its leadership and investors after reportedly an interagency US government body that polices investments for national security risks mandated Saudi Arabia-affiliated fund Prosperity7 Ventures to sell its stake in the company. The fund led a $25 million fundraise <a data-offer-url="https://www.aramco.com/en/news-media/news/2022/aramco-announces-prosperity7-ventures" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.aramco.com/en/news-media/news/2022/aramco-announces-prosperity7-ventures&quot;}" href="https://www.aramco.com/en/news-media/news/2022/aramco-announces-prosperity7-ventures" rel="nofollow noopener" target="_blank">announced</a> by Rain in early 2022.</p><p>The forced removal of the fund, first <a href="https://www.bloomberg.com/news/articles/2023-11-30/us-compels-saudi-fund-to-exit-ai-chip-startup-backed-by-altman">reported by Bloomberg</a> Thursday and described in the documents seen by WIRED, could add to Rain’s challenges of bringing a novel chip technology to market, potentially delaying the day OpenAI can make good on its $51 million advance order. Silicon Valley-based <a data-offer-url="https://www.grepvc.com/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.grepvc.com/&quot;}" href="https://www.grepvc.com/" rel="nofollow noopener" target="_blank">Grep VC</a> acquired the shares; it and the Saudi fund did not respond to requests for comment.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>US concern about Prosperity7’s deal with Rain also raises questions about another effort by Altman to increase the world’s supply of AI chips. He’s talked to investors in the Middle East in recent months about raising money to start a new chip company to help OpenAI and others diversify beyond their current reliance on Nvidia GPUs and specialized chips from Google, Amazon, and a few smaller suppliers, according to two people seeking anonymity to discuss private talks.</p><p>Brain Trust</p><p>Rain, founded in 2017, has claimed that its brain-inspired NPUs will yield potentially 100 times more computing power and, for training, <a href="https://www.linkedin.com/posts/gordonhirschwilson_activity-difference-training-of-deep-neural-activity-7000595097717260288-pF04/">10,000 times</a> greater energy efficiency than GPUs, the graphics chips that are the workhorses for AI developers such as OpenAI and primarily sourced from Nvidia.</p><p>Altman led one of Rain’s seed financings in 2018, the company has said, the year before OpenAI committed to spend $51 million on its chips. Rain now has about 40 employees, including experts in both development of AI algorithms and traditional chip design, according the disclosures.</p><p>The startup appears to have quietly changed its CEO this year and now lists founding CEO Gordon Wilson as executive advisor on its website, with former white-shoe law firm attorney William Passo gaining a promotion to CEO from COO.</p><p>Wilson confirmed his exit in <a href="https://www.linkedin.com/feed/update/urn:li:activity:7135990941244411904/">a LinkedIn post</a> Thursday, but did not provide a reason. “Rain is poised to build a product that will define new AI chip markets and massively disrupt existing ones,” he wrote. “Moving forward I will continue to help Rain in every way I can.” Over 400 LinkedIn users including some whose profiles say they are Rain employees commented on Wilson's post or reacted to it with heart or thumbs up emojis—Passo wasn't among them. Wilson declined to comment for this story.</p><p>The company will search for an industry veteran to permanently replace Wilson, according to an October note to investors seen by WIRED.</p><p>Rain’s initial chips <a data-offer-url="https://rain.ai/approach" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://rain.ai/approach&quot;}" href="https://rain.ai/approach" rel="nofollow noopener" target="_blank">are based</a> on the <a href="https://www.wired.com/story/using-open-source-designs-to-create-more-specialized-chips/">RISC-V open-source architecture</a> endorsed by Google, Qualcomm, and other tech companies and aimed at what the tech industry calls edge devices, located far from data centers, such as phones, drones, cars, and robots. Rain aims to provide a chip capable of both training machine algorithms and running them once they’re ready for deployment. Most edge chip designs today, like <a href="https://www.wired.com/story/how-apple-makes-ai-chip-powering-iphones-fancy-tricks/">those found in smartphones</a>, focus on the latter, known as inference. How OpenAI would use Rain chips could not be learned.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Rain at one point has claimed to investors that it has held advanced talks to sell systems to Google, Oracle, Meta, Microsoft, and Amazon. Microsoft declined to comment, and the other companies did not respond to requests for comment.</p><p>Security Fears</p><p>The funding round led by Prosperity7 announced last year brought Rain’s total funding to $33 million as of April 2022. That was enough to operate through early 2025 and valued the company at $90 million excluding the new cash raised, according to the disclosures to investors. The documents cited Altman’s personal investment and Rain’s letter of intent with OpenAI as reasons to back the company.</p><p>In a Rain <a data-offer-url="https://www.einnews.com/pr_news/562154507/rain-neuromorphics-raises-25m-series-a-to-transform-ai-hardware-landscape" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.einnews.com/pr_news/562154507/rain-neuromorphics-raises-25m-series-a-to-transform-ai-hardware-landscape&quot;}" href="https://www.einnews.com/pr_news/562154507/rain-neuromorphics-raises-25m-series-a-to-transform-ai-hardware-landscape" rel="nofollow noopener" target="_blank">press release</a> for the fundraise last year, Altman applauded the startup for <a data-offer-url="https://www.eetimes.com/rain-neuromorphics-tapes-out-demo-chip-for-analog-ai/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.eetimes.com/rain-neuromorphics-tapes-out-demo-chip-for-analog-ai/&quot;}" href="https://www.eetimes.com/rain-neuromorphics-tapes-out-demo-chip-for-analog-ai/" rel="nofollow noopener" target="_blank">taping out a prototype in 2021</a> and said it “could vastly reduce the costs of creating powerful AI models and will hopefully one day help to enable true artificial general intelligence.”</p><p>Prosperity7’s investment in Rain drew the interest of the interagency Committee on Foreign Investment in the United States, which has the power to scuttle deals deemed to threaten national security.</p><p>CFIUS, as the committee is known, has long been concerned about China gaining access to advanced US semiconductors, and has grown increasingly <a href="https://www.ft.com/content/2a636cee-b0d2-45c2-a815-11ca32371763">worried about China using intermediaries in the Middle East</a> to quietly learn more about critical technology, says Nevena Simidjiyska, a partner at the law firm Fox Rothschild who helps clients with CFIUS reviews. “The government doesn’t care about the money,” she says. “It cares about access and control and the power of the foreign party.”</p><p>Rain received a small seed investment from the venture unit of Chinese search engine Baidu apparently without problems but the larger Saudi investment attracted significant concerns. Prosperity7, a unit of Aramco Ventures, which is part of state-owned Saudi Aramco, possibly could have let the oil giant and other large companies in the Middle East to become customers but also put Rain into close contact with the Saudi government.</p><p>Megan Apper, a spokesperson for CFIUS, says the panel is “committed to taking all necessary actions within its authority to safeguard U.S. national security” but that “consistent with law and practice, CFIUS does not publicly comment on transactions that it may or may not be reviewing.”</p><p>Data disclosed by CFIUS shows it <a data-offer-url="https://home.treasury.gov/system/files/206/CFIUS%20-%20Annual%20Report%20to%20Congress%20CY%202022_0.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://home.treasury.gov/system/files/206/CFIUS%20-%20Annual%20Report%20to%20Congress%20CY%202022_0.pdf&quot;}" href="https://home.treasury.gov/system/files/206/CFIUS%20-%20Annual%20Report%20to%20Congress%20CY%202022_0.pdf" rel="nofollow noopener" target="_blank">reviews hundreds of deals</a> annually and in the few cases where it has concerns typically works out safeguards, such as barring a foreign investor from taking a board seat. It couldn’t be learned why the committee required full divestment from Rain.</p><p>Three attorneys who regularly work on sensitive deals say they could not recall any previous Saudi Arabian deals fully blocked by CFIUS. “Divestment itself has been quite rare over the past 20 years and has largely been a remedy reserved for Chinese investors,” says Luciano Racco, cochair of the international trade and national security practice at law firm Foley Hoag.</p><p>OpenAI likely needs to find partners with deep-pocketed backers if it is to gain some control over its hardware needs. Competitors Amazon and Google have spent years developing their <a href="https://www.wired.com/story/new-amazon-chips-cloud-computing/">own</a> <a href="https://www.wired.com/story/fit-billions-transistors-chip-let-ai-do/">custom chips</a> for AI projects and can fund them with revenue from their lucrative core businesses. Altman <a href="https://www.businessinsider.com/sam-altman-says-cant-rule-out-openai-making-own-chips-2023-10">has refused to rule out</a> OpenAI making its own chips, but that too would require significant funding.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[1960s chatbot ELIZA beat OpenAI's GPT-3.5 in a recent Turing test study (103 pts)]]></title>
            <link>https://arstechnica.com/information-technology/2023/12/real-humans-appeared-human-63-of-the-time-in-recent-turing-test-ai-study/</link>
            <guid>38506175</guid>
            <pubDate>Sun, 03 Dec 2023 10:56:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/information-technology/2023/12/real-humans-appeared-human-63-of-the-time-in-recent-turing-test-ai-study/">https://arstechnica.com/information-technology/2023/12/real-humans-appeared-human-63-of-the-time-in-recent-turing-test-ai-study/</a>, See on <a href="https://news.ycombinator.com/item?id=38506175">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      transform and roll out    —
</h4>
            
            <h2 itemprop="description">AI chatbot deception paper suggests that some bots (and people) aren't very persuasive.</h2>
                    </header>
        <section>
            <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/turing_test_hero-800x450.jpg" alt="An illustration of a man and a robot sitting in boxes, talking.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/11/turing_test_hero.jpg" data-height="675" data-width="1200">Enlarge</a> <span>/</span> An artist's impression of a human and a robot talking.</p><p>Getty Images | Benj Edwards</p></figcaption>  </figure>

  




<!-- cache hit 274:single/related:75c96972efcfd5c809dcc7c1fd0a61ed --><!-- empty -->
<p>In a preprint <a href="https://arxiv.org/abs/2310.20216">research paper</a> titled "Does GPT-4 Pass the Turing Test?", two researchers from UC San Diego pitted OpenAI's <a href="https://arstechnica.com/information-technology/2023/03/openai-announces-gpt-4-its-next-generation-ai-language-model/">GPT-4</a> AI language model against human participants, GPT-3.5, and <a href="https://en.wikipedia.org/wiki/ELIZA">ELIZA</a> to see which could trick participants into thinking it was human with the greatest success. But along the way, the study, which has not been peer-reviewed, found that human participants correctly identified other humans in only 63 percent of the interactions—and that a 1960s computer program surpassed the AI model that powers the free version of ChatGPT.</p>

<p>Even with limitations and caveats, which we'll cover below, the paper presents a thought-provoking comparison between AI model approaches and raises further questions about using the <a href="https://en.wikipedia.org/wiki/Turing_test">Turing test</a> to evaluate AI model performance.</p>
<p>British mathematician and computer scientist Alan Turing first conceived the Turing test as "The Imitation Game" <a href="http://phil415.pbworks.com/f/TuringComputing.pdf">in 1950</a>. Since then, it has become a famous but controversial benchmark for determining a machine's ability to imitate human conversation. In modern versions of the test, a human judge typically talks to either another human or a chatbot without knowing which is which. If the judge cannot reliably tell the chatbot from the human a certain percentage of the time, the chatbot is said to have passed the test. The threshold for passing the test is subjective, so there has never been a broad consensus on what would constitute a passing success rate.</p>
<p>In the recent study, <a href="https://arxiv.org/abs/2310.20216">listed on arXiv</a> at the end of October, UC San Diego researchers Cameron Jones (a PhD student in Cognitive Science) and Benjamin Bergen (a professor in the university's Department of Cognitive Science) set up a website called <a href="https://turingtest.live/">turingtest.live</a>, where they hosted a two-player implementation of the Turing test over the Internet with the goal of seeing how well GPT-4, when prompted different ways, could convince people it was human.</p>                                            
                                                        
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/11/pass_rate_graph.jpg" data-height="871" data-width="1000" alt="A bar graph of success rates in the Turing test performed by Jones and Bergen, with humans on top and a GPT-4 model in the #2 slot. Ancient rules-based ELIZA outperformed GPT-3.5."><img alt="A bar graph of success rates in the Turing test performed by Jones and Bergen, with humans on top and a GPT-4 model in the #2 slot. Ancient rules-based ELIZA outperformed GPT-3.5." src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/pass_rate_graph-640x557.jpg" width="640" height="557" srcset="https://cdn.arstechnica.net/wp-content/uploads/2023/11/pass_rate_graph.jpg 2x"></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/11/pass_rate_graph.jpg" data-height="871" data-width="1000">Enlarge</a> <span>/</span> A bar graph of success rates in the Turing test performed by Jones and Bergen, with humans on top and a GPT-4 model in the #2 slot. Ancient rules-based ELIZA outperformed GPT-3.5.</p></figcaption></figure>
<p>Through the site, human interrogators interacted with various "AI witnesses" representing either other humans or AI models that included the aforementioned GPT-4, GPT-3.5, and <a href="https://en.wikipedia.org/wiki/ELIZA">ELIZA</a>, a rules-based conversational program from the 1960s. "The two participants in human matches were randomly assigned to the interrogator and witness roles," write the researchers. "Witnesses were instructed to convince the interrogator that they were human. Players matched with AI models were always interrogators."</p>
<p>The experiment involved 652 participants who completed a total of 1,810 sessions, of which 1,405 games were analyzed after excluding certain scenarios like repeated AI games (leading to the expectation of AI model interactions when other humans weren't online) or personal acquaintance between participants and witnesses, who were sometimes sitting in the same room.</p>
<p>Surprisingly, ELIZA, developed in the mid-1960s by computer scientist Joseph Weizenbaum at MIT, scored relatively well during the study, achieving a success rate of 27 percent. GPT-3.5, depending on the prompt, scored a 14 percent success rate, below ELIZA. GPT-4 achieved a success rate of 41 percent, second only to actual humans.</p>
<p>GPT-3.5, the base model behind the free version of ChatGPT, has been conditioned by OpenAI specifically not to present itself as a human, which <a href="https://x.com/emollick/status/1728899736609149287?s=20">may partially account</a> for its poor performance. In a post on X, Princeton computer science professor Arvind Narayanan <a href="https://x.com/random_walker/status/1729115756439511119?s=20">wrote</a>, "Important context about the 'ChatGPT doesn't pass the Turing test' paper. As always, testing behavior doesn't tell us about capability." In a <a href="https://x.com/random_walker/status/1729145124561797199?s=20">reply</a>, he continued, "ChatGPT is fine-tuned to have a formal tone, not express opinions, etc, which makes it less humanlike. The authors tried to change this with the prompt, but it has limits. The best way to pretend to be a human chatting is to fine-tune on human chat logs."</p>                                            
                                                        
<p>Further, the authors speculate about the reasons for ELIZA's relative success in the study:</p>
<blockquote><p>"First, ELIZA’s responses tend to be conservative. While this generally leads to the impression of an uncooperative interlocutor, it prevents the system from providing explicit cues such as incorrect information or obscure knowledge. Second, ELIZA does not exhibit the kind of cues that interrogators have come to associate with assistant LLMs, such as being helpful, friendly, and verbose. Finally, some interrogators reported thinking that ELIZA was “too bad” to be a current AI model, and therefore was more likely to be a human intentionally being uncooperative."</p></blockquote>
<p>During the sessions, the most common strategies used by interrogators included small talk and questioning about knowledge and current events. More successful strategies involved speaking in a non-English language, inquiring about time or current events, and directly accusing the witness of being an AI model.</p>
<p>The participants made their judgments based on the responses they received. Interestingly, the study found that participants based their decisions primarily on linguistic style and socio-emotional traits, rather than the perception of intelligence alone. Participants noted when responses were too formal or informal, or when responses lacked individuality or seemed generic. The study also showed that participants' education and familiarity with large language models (LLMs) did not significantly predict their success in detecting AI.</p>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/12/turing_game_instructions2.jpg" data-height="814" data-width="1072" alt="Instructions for the Turing test AI evaluation game from Jones and Bergen, 2023."><img alt="Instructions for the Turing test AI evaluation game from Jones and Bergen, 2023." src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/turing_game_instructions2-640x486.jpg" width="640" height="486" srcset="https://cdn.arstechnica.net/wp-content/uploads/2023/12/turing_game_instructions2.jpg 2x"></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/12/turing_game_instructions2.jpg" data-height="814" data-width="1072">Enlarge</a> <span>/</span> Instructions for the Turing test AI evaluation game from Jones and Bergen, 2023.</p><p>Jones and Bergen, 2023</p></figcaption></figure>
<p>The study's authors acknowledge the study's limitations, including potential sample bias by recruiting from social media and the lack of incentives for participants, which may have led to some people not fulfilling the desired role. They also say their results (especially the performance of ELIZA) may support common criticisms of the Turing test as an inaccurate way to measure machine intelligence. "Nevertheless," they write, "we argue that the test has ongoing relevance as a framework to measure fluent social interaction and deception, and for understanding human strategies to adapt to these devices."</p>

                                                </div>

            
            
                            <nav>Page: <span>1 <a href="https://arstechnica.com/information-technology/2023/12/real-humans-appeared-human-63-of-the-time-in-recent-turing-test-ai-study/2/">2</a> <a href="https://arstechnica.com/information-technology/2023/12/real-humans-appeared-human-63-of-the-time-in-recent-turing-test-ai-study/2/"><span>Next <span>→</span></span></a></span></nav>
            
        </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Simulate 3D Plants in the Browser (169 pts)]]></title>
            <link>https://plant.max-richter.dev</link>
            <guid>38506166</guid>
            <pubDate>Sun, 03 Dec 2023 10:55:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://plant.max-richter.dev">https://plant.max-richter.dev</a>, See on <a href="https://news.ycombinator.com/item?id=38506166">Hacker News</a></p>
<div id="readability-page-1" class="page">
		<div>       <header>  <div><div><form><h3>Login</h3>   <p><span>Username/Email</span>    </p>  <p><span>Password</span>    </p>  <div> <p>  <label for="c1z8t_QemVun-0nn8diiW"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><title>cross</title><line vector-effect="non-scaling-stroke" x1="0" y1="100" x2="100" y2="0"></line><line vector-effect="non-scaling-stroke" x1="0" y1="0" x2="100" y2="100"></line></svg></label></p><p>register</p> </div> </form></div> <div><div role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M13.172 12l-4.95-4.95 1.414-1.414L16 12l-6.364 6.364-1.414-1.414z"></path></svg> <p>Tutorials</p></div>  </div> </div> </header> <main> <div>  <canvas></canvas>   </div> </main> 
			
			
		</div>
	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text (163 pts)]]></title>
            <link>https://arxiv.org/abs/2311.18805</link>
            <guid>38506140</guid>
            <pubDate>Sun, 03 Dec 2023 10:48:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2311.18805">https://arxiv.org/abs/2311.18805</a>, See on <a href="https://news.ycombinator.com/item?id=38506140">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2311.18805.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>While Large Language Models (LLMs) have achieved remarkable performance in many tasks, much about their inner workings remains unclear. In this study, we present novel experimental insights into the resilience of LLMs, particularly GPT-4, when subjected to extensive character-level permutations. To investigate this, we first propose the Scrambled Bench, a suite designed to measure the capacity of LLMs to handle scrambled input, in terms of both recovering scrambled sentences and answering questions given scrambled context. The experimental results indicate that most powerful LLMs demonstrate the capability akin to typoglycemia, a phenomenon where humans can understand the meaning of words even when the letters within those words are scrambled, as long as the first and last letters remain in place. More surprisingly, we found that only GPT-4 nearly flawlessly processes inputs with unnatural errors, even under the extreme condition, a task that poses significant challenges for other LLMs and often even for humans. Specifically, GPT-4 can almost perfectly reconstruct the original sentences from scrambled ones, decreasing the edit distance by 95%, even when all letters within each word are entirely scrambled. It is counter-intuitive that LLMs can exhibit such resilience despite severe disruption to input tokenization caused by scrambled text.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Qi Cao [<a href="https://arxiv.org/show-email/8edf2a0c/2311.18805">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 30 Nov 2023 18:51:38 UTC (246 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Simple Mobile Tools suite to be acquired by Israeli adware company (144 pts)]]></title>
            <link>https://github.com/SimpleMobileTools/General-Discussion/issues/241</link>
            <guid>38505229</guid>
            <pubDate>Sun, 03 Dec 2023 06:12:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/SimpleMobileTools/General-Discussion/issues/241">https://github.com/SimpleMobileTools/General-Discussion/issues/241</a>, See on <a href="https://news.ycombinator.com/item?id=38505229">Hacker News</a></p>
<div id="readability-page-1" class="page"><div disabled="" sortable="">
          <blockquote>
<p dir="auto">not really, thats not how it works</p>
</blockquote>
<p dir="auto">First of all, lemme prepend this by saying that I Am Not A Lawyer, meaning I may miss a detail and be wrong.</p>
<p dir="auto">I have looked around and I was not able to find a CLA or similar, meaning that any contributions to this project has not resulted in any of the contributors waving their rights to the legal owner of the project. Though, if there was indeed a CLA to sign before contributing and it stated that you waved your rights, then the following paragraphs won't be true.</p>
<p dir="auto">Once the sale is complete, if zipoapps decides to go closed source <em>and</em> keep the external contributions, any contributors will have the ability to claim license infringement and ask zipoapps to comply under the terms of the license: either by removing every of your contributions (depending on how big of a contributor you are this might be a big blow to them), or by forcing them to keep the project open-source as defined by the GPLv3 license.</p>
<p dir="auto">If zipoapps do make those apps closed source, and you're a contributor for whom it is uncomfortable, I'd recommend to consult a lawyer first to inform yourself of what you can do and the proper steps to take.<br>
Though, there's also the question of is it worth it to go after them? Maybe energy is better spent working on the fork, given who this project panders to, I don't think zipoapps will inherit much of the userbase if they go down the bad road.</p>
<p dir="auto">While I am mostly an external observer in this story, I wish a good continuation to the original author of this project, I can see how bailing out can become a desirable option when things get bad. I do find it weird it has been chosen to sell the project instead of giving it to another maintainer who could have continued the project's legacy. I guess there's some personal things that pushed to the sale, so I wouldn't hold it too much against the author. I also wish a good continuation to the fork as well.</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Personal History of APL (1982) (102 pts)]]></title>
            <link>https://ed-thelen.org/comp-hist/APL-hist.html</link>
            <guid>38505138</guid>
            <pubDate>Sun, 03 Dec 2023 05:46:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ed-thelen.org/comp-hist/APL-hist.html">https://ed-thelen.org/comp-hist/APL-hist.html</a>, See on <a href="https://news.ycombinator.com/item?id=38505138">Hacker News</a></p>
<div id="readability-page-1" class="page"><div name="PersonalHistory">
<p>
<br>October 1982
<br>TR-03.214
</p><center>
<h2>A PERSONAL HISTORY OF APL</h2>
<br>
by
<br>
Michael S. Montalbano


<p>

International Business Machines Corporation
<br>General Products Division
<br>Santa Teresa Laboratory
<br>San Jose, California
</p></center>
<hr>
<p>
<b>
<font=+1>A PERSONAL HISTORY OF APL
</font=+1></b>
</p><p>
<b>Introduction</b>

</p><p>
I have several reasons for calling this talk a personal history.
</p><p>
For one, I want to make it clear that the opinions I express are my own:
they are not the opinions of my employer or of any other organization,
group or person. If you agree with them, I am happy to have your
concurrence: if you disagree, I'd be happy to defend them. In any event,
the praise, blame or indifference my views may inspire in you should be
directed to me and to no one else.
</p><p>
What I plan to discuss are things I have done, seen or experienced at
first hand. Thus, this talk is merely an opinionated collection of
anecdotes. I want to emphasize this from the outset: it is my second
reason for calling this a personal history.
But my most important reason is that I feel strongly that we need a good,
thoughtful, accurate history both of APL and of computing itself. While
I'd be flattered to have this account included as part of that history, I
don't want anyone to mistake it t as an attempt at the real thing.
</p><p>
<b>
The Importance of History
</b>
</p><p>
We neglect history at our peril. The truly incredible growth of digital
computer technology has transformed our world almost overnight. This
transformation is not only continuing, it is accelerating. It gives every
promise of continuing to change our institutions and the circumstances of
our daily lives at a faster and faster rate. If we are ever to understand
where we're going, it's important that we take a long, careful look at
where we've been, where we are, and how we got from there to here. If we
don't do this, we won't be able to control events; they will control us.
As far as I'm concerned, that's what's happening right now; a runaway
technology has us at its mercy because we have not developed techniques to
understand and control it.
</p><p>
H. G. Wells described human history as a race between education and
catastrophe. This observation is more pertinent today than it ever has
been in the past. But, in the specific case of the proliferation of 
stored-program digital computers, the education-catastrophe race, in my opinion,
takes a particular form: it is a race between technology and methodology,
between gadgets and ideas.
</p><p>
We are developing gadgets at an explosive, accelerating, self-fueling rate
we shall be swamped by these gadgets if we don't hasten to develop and
apply the ideas we need to control them.
</p><p>

To me, this need defines the importance and the mission of APL. APL
provides the best set of gadget-understanding and gadget-controlling ideas
currently available. Looking to the future, it provides the best base for
the methodology we must develop if we are ever to bring our gadget-based
technology under control.
</p><p>
This opinion is based on experience. I was working with computers when
they were merely gleams in the eyes of the early designers: I was working
with APL when it was nothing but a collection of incomprehensible
characters scattered through publications with catchy titles like: The
Description of Finite Sequential Processes. In other words, I've been
working with both computers and APL since their very early days. And, in
looking back on this experience of just over 34 years, (which is my own
personal experience of computing), I find that it can best be summarized
in n terms of two key ideas:
</p><ul>
<li>
The stored-program idea: the idea that a procedure or algorithm can be
stored as a collection of switch settings in exactly the same way as
the data on which the procedure is to work and, as a consequence, that
executing such a stored procedure consists of starting it and letting
it flip switches until it is done. This was apparently first stated
explicitly in n a paper drafted by John yon Neumann in n 1945. <sup><a href="#SUP">1</a></sup>
</li><li>
The efficient-notation idea: the idea that these vast collections of
switches, changing their settings in thousandths, millionths,
billionths, trillionths,... of a second, would be pretty hard to
manage if we didn't develop a good way to describe them and think
about them. I don't know whether this idea was ever presented anywhere
in just these terms. Instead, it seemed to be implicit in the
activities and writings of many people. But, in my opinion, it
received its most effective and fruitful expression in the writings of
Kenneth Iverson and others who followed his lead. The most important
publication expounding this idea is the book from whose title the
letters APL derive their significance.<sup><a href="#SUP">2</a></sup>
</li></ul>
<p>
The stored-program idea provided, and continues to provide, the basis for
our current runaway technology. The efficient-notation idea, if we take it
seriously and do a lot of thinking and hard work, will help us curb the
runaway and direct it t into fruitful and productive channels.
</p><p>

You may find this brief summary of computing history controversial. I hope
so. We need vigorous, informed, philosophical controversy. And, as a
contribution to this controversy, let me state some of my other biases in n
as controversial a manner as I can.
</p><p>
<b>Biases of an APL Bigot</b>
</p><p>
There exists a growing class of people, of which I am happy to consider
myself a member, who are called "APL bigots" by friends and foes alike. The
friends use the term affectionately; the others do not.
</p><p>
The bigotry consists in believing that APL is the way computing should be
done. I think it's fair to say that no one can properly be called an APL
bigot who doesn't believe this.
</p><p>
In one respect, my bigotry may not be as great as that of the general run.
I believe that APL and assembler language are the way computing should be
done. To this extent, I am held suspect by true APL evangelicals.
</p><p>
In another respect, my bigotry is so much greater than theirs that, now
that I am making it t known, I fear I may be excommunicated   as a 
schismatic.

I believe that, important as APL is in computing, it is even more important 
as
an instrument for rationalizing the management process.
</p><p>
Modern management is in trouble. Don't take my word for it. Read the daily
papers, the weekly newsmagazines and the flood of books and articles that
describe management's difficulties and offer cures. Some of these, of
course, assert that Japanese management is an exception to the general
rule; the cure these people offer to nonJapanese management is to learn
from the Japanese how to do it  right.
</p><p>
I don't believe this. I believe Japanese management is in as much trouble
as any other. Its current successes are no indication that its management
isn't afflicted by the same difficulty as the management of organizations
anywhere else in the world. Expressed simply, this difficulty is:
</p><dd>Nobody knows what's going on. 

<p>
Your reaction to this assertion may be emotional. You may believe it
passionately or reject it passionately. If you have either of these 
reactions,
you may not have understood what I said, so I'll repeat it:
</p><blockquote>
The problems of modern management are primarily attributable to one
cause:
<p>
Nobody knows what's going on.
</p></blockquote>
<p>
So there you have it: the basic message that motivates this talk:
</p><ul>
<li>Management's primary difficulty is that it has no good way to describe its
processes and thus develop objective means to correct or improve them.

</li><li>Other fields have faced this problem in the past. The ones that have solved
it most effectively are those that have developed a notation appropriate to
their subject matter. The most conspicuously successful examples of
notations that have virtually created the fields they describe are, of
'course, the specialized symbologies of the so-called "hard sciences".

</li><li>The workings of management can be expressed as extensive, intricate digital
procedures. These procedures cannot be designed, analyzed or described
effectively without a notation specifically designed for the purpose.
</li><li>APL is such a notation.
</li></ul>

There are, of course, many other important difficulties that management 
faces:
obsolescent plant, intensified competition, environmental concerns, employee
morale, strained labor relations, residues of past mismanagement (reflected 
in
such things as excessive debt, inadequate capital, incompetents in key
positions,...) and so on. But these difficulties are made unmanageable by 
our
inability to describe-them in a way that promotes insight and facilitates
communication.

<p>
Clearly, in expressing this opinion, I am swimming against the tide. The
widespread current belief is that we are experiencing an "information
explosion". How can I accuse management of being inadequately informed when
virtually everyone else says that they are overinformed? How can I say that
management doesn't have enough information when everyone else says they 
have so
much information that they are swamped by it?
</p><p>
Let me be blunt: I think the "information explosion" is a myth. I do not 
deny--
how can anyone deny?--that we are generating vast quantities of paper, 
tapes,
disks, etc., with optical or magnetic symbols recorded on their surfaces. 
What
I am denying is that these recorded symbols, in themselves, are information.
They are not. They become information only if, as a minimum:
</p></dd><dd>
They are accurately calculated.
</dd><dd>
They present a true picture of reality.
</dd><dd>
They are understood by the person to whom they are presented.
<p>

Of these three conditions, only the first stands a better than even chance 
of
being satisfied. For the purposes of effective management, all too much
recorded data does not present a true picture of reality and is not 
understood
by the person it is supposed to inform.
</p><p>
What does this all have to do with APL? Let me answer by recounting the 
pre-APL
experiences (at the U. S. National Bureau of Standards, the U. S. Naval
Research Laboratory, and the Kaiser Steel Corporation) that convinced me 
that
the computing field needed, more than anything else, an efficient notation 
for
describing digital procedures. Then let me follow up with the post-APL
experiences (at IBM and Stanford) that convinced me that APL was the 
notation
we needed.

</p><p>












<b>The	U. S. National Bureau of Standards, 1948-1952</b>
</p><p>
I started to work in May, 1948 as a mathematician in the Applied Mathematics
Laboratories of the National Bureau of Standards in Washington, D. C. I was
given a job description which, like every other one I have ever had, bore no
relation whatsoever to what I actually did. My first assignment was to 
program
a division subroutine for the UNIVAC. The UNIVAC that was then being 
designed
by the Eckert-Mauchly Corporation could add, subtract and multiply but it 
could
not divide. (Division was added later. The list of computer instructions or
"order codes", to use the terminology of the times, was updated and given an
identifying C-number as the UNIVAC's design progressed. If my memory's not
playing me tricks, the- order-code 1 list list that was current when I 
started
work was C-7. )
</p><p>

I was given a description of the UNIVAC. It described acoustic delay lines,
excess-three notation and end-around carry. I had the feeling that I was
entering a dark, eerie world in which words would be used as charms and
incantations rather than to communicate definite meanings. I was right.
</p><p>
I was surprised that no one took the trouble to show me the UNIVAC for 
which I
was to program division. After a few days I learned why. The machine 
described
so confidently and completely in the literature that I had been given had 
not
yet been built. (The first UNIVAC was not delivered until three years later.
But at least it was delivered. Our group spent a lot of time programming 
what
were then called "feasibility tests" for a lot of machines that never got 
off
the drawing boards.)
</p><p>
Programming for nonexistent machines started to pall after a few months. Our
group had punched-card equipment (including the 602 calculating punch) with
which we calculated mathematical tables. I switched to this activity at just
about the time the Office of the Air Comptroller asked the Bureau of 
Standards
for assistance in using electromechanical (later electronic) equipment to
calculate Air Force budgets. With the procedures then in use, it was taking
eighteen months to prepare a-yearly budget. There was general agreement that
this was not satisfactory.
</p><p>
I was assigned the task of getting budget computation mechanized under the
direction of George Dantzig, who was in charge of the Air Force budget 
project.
His job was to devise the calculations we were required to perform, that 
is, he
told me what was needed. I wired the plugboards and, later, wrote the 
programs
that gave him what he specified.
</p><p>
The original calculations were called "triangular model" calculations (I
understand they were given the acronym TRIM after I left the project.) The
later calculations were solutions of linear-programming problems, applying 
the
simplex technique that George Dantzig originated. The name of the Air Force
project, SCOOP, (Scientific Computation Of Optimum Programs), like TRIM,
suggests that wherever a computer goes an acronym's sure to follow.
</p><p>
I programmed the triangular-model calculations for the 602, the 602A, the 
604
(the electronic calculating punch) and was about to program them for the CPC
(the Card-Programmed Calculator) when the SEAC became available and I 
switched
back to programming instead of plugboard wiring.
</p><p>
The SEAC (National Bureau of Standards Eastern Automatic Computer) was the
first stored-program digital computer to operate successfully in the United
States. (The first stored-program computer to run successfully anywhere in 
the
world was the EDSAC, designed and built by the University Mathematical
Laboratory at Cambridge, England.) I introduce this information here 
because,
computing history being what it is, it does not seem to be available 
anywhere
else.
</p><p>
Of the many memories and ideas derived from my four years at the Bureau of
Standards, three are relevant to our present purpose:
</p><ol>
<li>I learned to be cautious about how I used the solutions of large systems
of equations with uncertain coefficients. I believe the triangular model
is an extremely effective, and neglected, tool for a large class of
planning problems. But, if the coefficients used in its equations have a
large measure of variability or uncertainty, you must use its answers with
caution.


</li><li>	I learned how desperately we needed a good notation for describing
algorithms. I sat through many presentations and discussions of solution
techniques for linear programming problems. These presentations were
largely chalk dust and arm-waving. The essential ideas, which are extremely
simple once they are understood, were obscured rather than illuminated by
the terminology and notation used to describe them-.
</li><li>	1 learned that gadget development was outstripping idea development and
would continue to do so unless we did something about it. I suggested that
we do something about it. My management was receptive to the idea (or told
me it was) but said there was no money in the budget for idea development.
That, of course, was the problem. It has been the problem ever since. This
failure on the part of my management to fight for research in ideas was one
of the reasons I left the Bureau.
</li></ol>
<p>
<b>U. S. Naval Research Laboratory, 1952-1954</b>
</p><p>
At the Naval Research Laboratory, I became part of an interdisciplinary team
applying the latest operations-research techniques to the development of 
man-machine systems.]
</p><p>
Brave words.
</p><p>
The "disciplines" included physics, mathematics, philosophy, sociology, 
various
brands of psychology, naval science (represented by officers in the United
States Navy), and I don't know what all.
</p><p>
Our tasks were the obvious ones: develop ways to make naval operations more
efficient by incorporating new gadgets into systems that used them to best
advantage. A typical assignment might be either general, for example design 
of
a combat information center for a ship, a task force, or a shore-based 
control
center or specific, for example, taking a new weapon, like a homing torpedo 
to
be fired from the deck of a ship into the water near a distant submarine, 
and
deciding how best to incorporate it into a system including a ship, sensing
devices, communicating devices, and other weapons.
</p><p>

I learned many things during my two years at the Naval Research Laboratory
(among them that clinical and experimental psychologists tend to despise 
each
other) but for our present purposes the most important things I learned 
were:
</p><ul>
<li>	It's hard to plan effectively for a future you can't predict.
</li><li> No numbers are frequently better than some numbers.
</li><li> Nobody knows what's going on.<sup><a href="#SUP">3</a></sup>
</li></ul>
<p>
Predicting the future is, of course, particularly difficult for the military
since a future requiring military action is apt to be precipitated by some
catastrophic event. I attempted to develop techniques for dealing with this 
by
first describing the three states in which the Navy might have to operate:
</p><ul>
<li>	The peacetime state.




<!-- start page 06   --->




</li><li>The transition state from peace to war immediately after hostilities have
commenced.
</li><li>The wartime state.

</li></ul>
and then describing the transformations required to convert from one state 
to the other in n the most efficient way possible.
<p>
I didn't get very far with this, but, if I had the responsibility for con
contingency planning for any organization, civil or military, I would 
return to the three-state model I started to develop for the Navy and build on it.
</p><p>
And, again, I would need an efficient notation.
</p><p>
It was during the time I worked at the Naval Research Laboratory that I 
first became aware of the ease with which people can deceive themselves with
meaningless figures. This is not a criticism of the Navy. It is a criticism 
of virtually all modern management.
</p><p>
You are much better off with no numbers than meaningless ones.. The minute 
you believe numbers uncritically, that is, without understanding how they're
calculated and how well they measure whatever they're supposed to measure, 
you will generate a breed of employee who will produce numbers and not results.
Your data-processing system will then serve not to describe reality but to 
lie about it.

</p><p>
<b>Kaiser Steel Corporation, 1954-1961</b>
</p><p>
I started with Kaiser Steel as a mathematician, a new kind of employee
requiring a new job description. I started in the Fontana Procedures 
Department of the Controller's Division. The Industrial Engineering Department (which 
was in the Operations Division and thus always in a kind of uneasy rivalry with 
the Procedures Department) had the responsibility for administering job
descriptions. I learned later that the one we filed threw them into a tizzy;
they felt it was another sinister move on the part of the Controller's 
Division to take over the work they were supposed to do.
</p><p>
I left Kaiser Steel, not quite seven years later, as Director of Research 
and Computer Planning.
</p><p>
In the interim, I learned about iron and steelmaking both at Kaiser and,
through industry association meetings and literature, at other American and
Canadian steel plants. I learned about other industries by participation in 
the activities of cross-industry associations. By the time I left Kaiser Steel, 
I had had several years in which to observe management in action. The comments
I've been making, which you may have regarded as flippancies, are honest
descriptions of what I observed.
</p><p>
Consider some examples. They are from Kaiser Steel, because that is where I
worked, but I can testify that they are representative of all management.
</p><p>
<b>Precision Rounds</b>. At the time I was there, Kaiser Steel manufactured an 
alloy steel product called "precision" rounds because the diameters had to be
controlled to very close tolerances. There were two schools of thought about
the place of precision rounds in our product line. One school held that it 
was the most profitable product we made. The other said we were losing our 
shirts on it.
</p><p>
How could this be? Couldn't our cost accountants tell us whether we were 
making money or losing money?
</p><p>

The answer is: no, they couldn't. Two different groups, working off the same
set of figures, reached diametrically opposite conclusions. This was the 
first of many experiences (including attendance at a Stanford gathering of the 
most prestigious accounting firms in the world) that led me to the conclusion 
that most cost accounting is applied metaphysics of an extremely ethereal kind.
Counting angels on the head of a pin is a useful exercise in data-gathering
compared to much cost accounting.






<!-- start page 07   --->





</p><p>
The problem was, as it  always is, the basic data with with which we had 
to work. Rounds of any kind had to go through a finishing operation. Scheduling
finishing operations and describing what took place was one of the most
difficult tasks in the mill. Production figures out of the finishing 
operation were always suspect. The difference between the two schools derived 
primarily from the different ways they interpreted those figures. At least, that's the
best explanation I was ever given.
</p><p>
<b>Tin Mill Flippers</b>. We installed a management incentive plan at Kaiser Steel
while I was there. The incentive plan for the tin mill was delayed for a 
while until we straightened out a rather embarrassing problem that cast doubts on 
the accuracy of our tin mill production recording. We were reporting more tin 
plate coming out of our shears than we put into them.
</p><p>
Management was understandably hesitant about paying performance incentives 
on figures that were so obviously, stupidly wrong.
</p><p>
The difficulty arose from the way we processed rejects. As the sheared plate
went by an inspection station, it was examined for pinholes, surface 
blemishes and other defects. When a defect was detected, the inspector pressed a 
button that switched the faulty plate to a reject pile. Unfortunately, a few plates
before and after the bad one were also diverted to the reject pile. These 
were usually prime plates that we could not afford to sell at secondary prices.
</p><p>

The prime plates in the reject piles were separated from the true rejects 
by a group of women called "tin mill flippers". They worked at large tables,
examining the plated surfaces carefully and separating the plates into prime
and secondary piles. The difficulty arose here. Because of the way reporting
was done, some of the plates were counted more than once.
</p><p>
We caught this bad data because it was so obvious. A shear can't produce tin
plate; it can only cut it. To shear more tin than you were given was clearly
impossible.
</p><p>
But, of course, this kind of mistake indicated just the tip of the iceberg. 
The errors that weren't so obvious weren't caught and corrected. And they 
existed, let me assure you of that. Even worse, they were almost certainly 
manipulated by people who were better at doctoring figures than at making steel.
</p><p>
<b>Slab Inventories</b>. Steel ingots are broken down into blooms if they are to 
be processed into products like H-beams, I-Beams and the like and into slabs if
they are to be processed into hot or cold-reduced product like sheet and 
strip.
Our biggest semi-finished inventory at Kaiser Steel was in slabs; we had
between 80,000 and 100,000 tons scattered in piles over wide areas of the 
mill grounds.
</p><p>
Slabs are big, heavy chunks of steel. You'd think it would be hard to lose 
one. It's not. It happens every day. Although I've been out of the steel industry
for more than twenty years, years in which we've landed men on the moon, I'm
willing to bet that right now, somewhere in the world, a rolling mill is 
idle because it's waiting for a slab that's sitting on the ground not too far 
away.
</p><p>
My reason for discussing these examples is to illustrate the complexity of 
the activities we are trying to manage. This particular example, as it happens,
also shows how quick management is to seek a technological rather than a
methodological solution.
</p><p>
There are many reasons why, despite the existence of a huge slab inventory, 
a rolling mill has to wait until the slabs it needs are found. The problem as 
a whole is a complex one and does not lend itself to quick-fix solutions. But 
a technological quick-fix for part of the problem was proposed and, after I 
had left the company, bought and installed. It was a costly failure.
</p><p>
At the risk of teaching you more about steelmaking than you ever wanted to
know, let me describe this attempt to solve a methodological problem by
technological gadgeteering.
</p><p>





<!-- start page 08   --->





The steel produced by one open-hearth melt is called a heat. A major part 
of a slab's identification is its heat number. Since the process of producing 
slabs from ingots usually grinds impurities into some of their surfaces, 
the slabs making up a heat had to be distributed among areas called
scarfing bays where men with oxyacetylene torches would burn out the
impurities. The slabs then had to be reassembled into a heat; the heat had 
to be deposited somewhere in the slab yard; and the heat number and slab-yard
location had to be reported to the production schedulers.
</p><p>
The existence of surface impurities that required scarfing thus caused much 
of the delay and confusion that attended the progression of the heat from the 
slab mill to the reduction mill.
The technological solution proposed for this problem was called a "hot
scarfer". It burnt off all the surface of a slab immediately after it was
reduced to its final dimensions. This was supposed to eliminate the need for
splitting a heat up into scarfing bays.
</p><p>

Thus, at this point, management had a choice:

</p><ol>
<li>	Methodological. Put in the time, effort and money it would take to develop
a satisfactory, realistic scheduling and inventory control system for
slabs. This would necessarily require investigation of all the many
sources of difficulty, not just the kind of difficulty that the hot
scarfer was supposed to eliminate.
</li><li>	Technological. Buy a hot scarfer and hope the slab scheduling and
inventory problem would go away.
</li></ol>
<p>

No contest. The lure of the tangible, glamorous gadget always wins out over 
the intangible, colorless idea. They got a hot scarfer. I say "they" because 
this was done after I left; I am happy to say that I had no part in the decision.
To the best of my knowledge, the hot scarfer never worked satisfactorily. 
It is one item of evidence justifying my belief that technological quick-fixes 
seldom achieve their objectives. (This, incidentally, is even more true for data
processing than it is for steelmaking.) In this particular instance, the 
gadget was expensive to purchase and to operate, it burnt off good steel as well as
surface impurities, and did not do what it was purchased to do: reduce 
wait-for-steel time in the mills that used slabs as inputs.
</p><p>

What does all this steelmaking jargon have to do with APL?
</p><p>

In all of my computer experience, I have found that the chief obstacle to
getting anything done is the absence of any clear, concise, precise, 
formally manageable way to describe and analyze what we're actually doing and to
describe and design a transformation to what we should be doing.

</p><p>

I gave several talks on this topic at professional meetings of various 
kinds. (I later used the written form of these talks, and other papers I wrote 
during my employment at Kaiser Steel, as class notes in the Business Information
Systems classes I taught at the Stanford Graduate School of Business.) One 
of the earliest of the talks, "Formalizing Business Problems", was given at the
first Electronic Business Systems Conference held in Los Angeles in 1955. 
This aroused the interest of Murray Lesser, at what was then the newly 
established IBM facility scattered throughout several locations in downtown San Jose 
(the one that developed into the General Products Division in which I now work). 
We met to discuss what we could do to develop some of the ideas we had in 
common.
</p><p>

The result of our discussions was a joint venture, called the Business 
Language Research Project, in which employees of IBM, Kaiser Steel and Touche, Niven,
Bailey and Smart participated. My contribution to the project was something 
I called "field-and-branch identification" which I later developed into the
approach to systematic systems analysis that I describe in my book on 
decision tables.<sup><a href="#SUP">4</a></sup> I will discuss this more fully later.
</p><p>

Lest I leave you with the impression that nothing effective can be done 
about the data-processing problems of industry, let me assure you that this is not
the case. Among the accomplishments of which I'm proudest are some of 
systems I installed at Kaiser Steel. At least one of them, a tin-mill in-process
inventory and production recording system impressed





<!-- start page 09   --->





the phone company so much that, since we were using some of their equipment 
to record production and to communicate between processing points, they ran an 
ad in the Wall Street Journal featuring a picture of our tin mill and 
describing our system as an example of what could be achieved if you called in their
Production Recording Consultant. We never had the benefit of a Production
Recording Consultant's services because the phone company never told us they
had one. Maybe he was made available to some of the people who read the ad.
</p><p>

So things can get done. But getting them done is slower, more difficult and
more costly than it has to be. That is why we have the application 
programming backlogs that we do. We need a better, more systematic way of dealing with
complex digital procedures.
</p><p>

We need systematic systems analysis.
</p><p>

Now do you see the connection with APL?
</p><p>


<b>IBM, 1961-</b>

</p><p>

I started to work for IBM in the Advanced Systems Development Division, San
Jose. I have since worked in the Palo Alto Scientific Center, the Palo Alto
Systems Center and the Santa Teresa Laboratory, where I am now in APL
Development.
</p><p>

At the time I started my IBM employment, the ASDD library used to circulate 
a daily list of its acquisitions to all employees. I am a kind of pack rat 
when it comes to written material and I acquired all kinds of library offerings
that, I'm sorry to report, I never took time to read.
Among the publications that I thus acquired, scanned, and filed for future
reference were several reports containing a strange, exotic notation. I was
skeptical about the value of these reports since, by that time, I not only 
had my own ideas as to what was needed but I had also seen many attempts by many
people to develop notations, charting techniques, and other descriptive
schemes. I didn't think much of any of them. By and large, history seems to
have agreed with me; most of them are happily forgotten.
</p><p>

However, when I learned that the author of several of the papers full of 
Greek letters, slashes, little circles, curlicues, and other cabalistic symbols 
was coming out to San Jose to talk about his ideas, I decided that I'd read one 
or two of his papers as a preparation for his talk.
</p><p>

My life hasn't been the same since.
</p><p>

My first acquaintance with the notation that has since become APL (for
several years it was either "Iverson Language", "Iverson Notation", or
just plain "Iverson") started with an IBM Research Report by Kenneth Iver-
son called, <b>The Description of Finite Sequential Processes.</b>

</p><p>

I don't have the paper handy at the moment so what I'm about to tell you is 
all memory; it may be mistaken in details but not in essence. I seem to
remember that the first page was mostly given over to heading material,
possibly an abstract, so that there were only two short columns of reading
matter on it. And, again memory, it took me several hours to understand what
those two short columns were all about.
</p><p>

The author's approach was so different from anything I'd ever encountered 
that I had a difficult time adjusting to his frame of reference. At the end of 
the first page, a fair assessment of my state of mind would be that I had
glimmerings but no hope.
</p><p>

The second page took about as much reading time as the first but, since it 
had twice as much matter, I was clearly improving. The glimmerings were now 
fitful gleams. One thing had definitely chanced, however. I had no doubts about the
value of what I was reading. I was now virtually certain that the author had
something to say and that I'd better find out what it was.







<!-- start page 10   --->







The third page had an illustration that, in a few short lines, described 
George Dantzig's simplex algorithm simply and precisely.
</p><p>

That was the overwhelming, crucial experience.
</p><p>

In the previous thirteen years, I had participated in so many murky 
discussions of what was here presented with crystal clarity that I knew that what I was
reading was of enormous significance to the future of computing.
</p><p>

So, when Dr. Kenneth Iverson came out to talk to us at San Jose, I was not 
only a convert, I had a fair idea of what he had to say. In the upshot, this 
meant that I was the only one who could ask him questions. Ken had some good, 
sharp people from Research and Advanced Systems Development in his audience but 
I'm pretty sure I was the only one who had been lucky enough to read what he 
had to say beforehand so that I had a fighting chance to follow him when he did 
what he usually does: hit you with one idea after another so fast that your mind
goes numb. 
</p><p>
I had been alerted to the fact that Ken might know what he was talking 
about by a fellow employee named Don Fisher who was working in the same group that I
was. After Ken's talk, he came to visit Don and I got a chance to meet him.
It's hard to believe that that first meeting took place more than twenty 
years ago. But it's true. I've been an APL bigot for a long, long time--since 
before there was an APL, in fact.
</p><p>

Stanford, 1962-1967?
</p><p>

Shortly after I went to work for IBM, Dan Teichroew, a friend of mine from 
the Bureau of Standards days, asked if I would be interested in spending part 
of my time at Stanford, participating in a study of "Quantititative Management
Techniques" being conducted at the Graduate School of Business. Dan was a
Professor of Management at the GSB and he wanted my permission to approach 
IBM about the idea. Naturally I was delighted by his proposal and even more
delighted when my management gave its approval.
</p><p>

The next several years were among the most satisfying and productive I've 
ever spent. And, in my opinion, not only did I benefit from them but so did
everybody else who was involved: Stanford, IBM, the students who 
participated in the research program and attended my Business Information Systems classes
and, more to the point of this talk, APL itself, whose first implementation 
was a FORTRAN-based batch interpreter that was developed on the IBM 7090 in 
Pine Hall at Stanford.
</p><p>

In the <b>Formalizing Business Problems </b>talk that I had given in 1955, I 
asserted that the problems we were facing required a partnership among computer 
users, computer manufacturers and academic institutions if we were ever to develop 
the body of knowledge we needed to manage computers properly. For a while there 
at Stanford we had two-thirds of what I'd recommended and, as you will see, I
concentrated a good deal of effort on seeing that the third was represented 
as well.
</p><p>

When I started at Stanford as an Industrial Research Fellow, the present 
School of Business building did not exist. I occupied an office in Polya Hall, near
the temporary buildings used to house Business School faculty and staff. 
IBM, at that time, shared the 7090 with Stanford and thus had the use of a few
offices in Polya Hall, the building which housed the university's Computer
Science faculty and staff. I was assigned one of those offices.
</p><p>

This was ideal. I not only participated in the activities of the Graduate
Business School: I was also part of the Computer Science complex at the
university. Both of these associations had APL implications and I'd like to
tell you about them.
</p><p>

My activities at the School of Business can be described in n four parts
</p><ol>
<li>		Work with graduate students on the "Quantitative Techniques" project.

</li><li>	Guest lecturing in Business Information Systems courses taught by Dan
Teichroew and John Lubin.







<!-- start page 11   --->







</li><li>	Development of my own Business Information Systems course after Teichroew
and Lubin left the university.
</li><li>	Teaching Operations Research courses as a Lecturer in Operations and
Systems Analysis.
</li></ol>
<p> 
APL and Quantitative Techniques. My purpose in describing my preAPL history 
to you was to let you know how my ideas about what management needed were 
formed. If you were paying attention, it should come as no surprise to you that, as
soon as I was given an opportunity to do something about it, I started to
investigate the implications for management of an efficient notation for
describing, analyzing and designing digital procedures.
</p><p> 
I investigated two techniques: decision tables and APL. The latter is the 
one relevant to this talk. I've long regretted that I never wrote up what I did;
let me remedy the deficiency now.
</p><p> 
<b>A Programming_Language </b> was published just in time for me to use in my
researches at Stanford. It was a godsend. I used it to try to answer the
question:
</p><blockquote>
<b>Is it possible to write programming specifications in such a way that
ambiguities, misunderstandings and outright mistakes in programming are
minimized?</b>

</blockquote>

The	answer I got surprised even me.
<p> 
Here's what I did. For a set of problems, of increasing difficulty, I would
write a solution procedure, using the notation of <b>A Programming Language </b>
I would then ask the graduate student assigned to me to:
</p><ul>
<li>
program the procedure in any programming language he chose,
</li><li>
execute the procedure for a representative set of data values
</li><li>

give me his program and answers so that I could compare them with the
specifications I had written and the answers I had previously calculated.
</li></ul>
The	result, which I still find surprising and impressive, was:
</dd><dd>In every case, what was programmed was exactly what I specified.
<p>
I'll comment in a moment on how rare this is under any circumstances. But 
these particular circumstances were so extreme that they merit some discussion.
</p><p>
<b>A Programming Language </b>was and is crammed full of ideas. I studied it
assiduously, and enjoyed it, but it was not easy reading, at least not for
someone of my mental capacity. I was lucky that my Stanford assignment 
provided me some time to spend on it so that I could both use it and explain it when 
my students asked questions.
</p><p>
But think of them. The life of a graduate student is a busy one. They had 
less time than I had to study strange notations. How could they make sense of the
chicken scratches I said were their programming specifications?
</p><p>
Somehow, they did.
</p><p>
I gave them completely abstract procedure descriptions, even avoiding 
standard words where they might have provided clues to the nature of the procedure, 
and they programmed exactly what I specified even though they had no prior
experience with the notation and never became expert at using it.
</p><p>
Thinking back on it, I understand why an intimate familiarity with this 
strange notation would have been desirable but was not essential. The only part of 
the notation they needed to understand was the part I used. I told them to look 
up the meaning of the symbols in Ken's book but I also told them that I'd 
explain and illustrate the meanings myself if they wanted help. But I wouldn't do
anything but explain the symbols. They had to translate the symbolic
description into a program.








<!-- start page 12   --->







</p><p>
They did, with no arguments and no discussions about what the procedure was
supposed to do. The procedure was described completely abstractly. They had 
no idea what external significance it had. They were not led astray by ideas of
their own about it. They figured out what I wanted done and then did it.

Contrast this with the usual way in which a business procedure gets 
programmed. Somebody, usually called a system�s analyst, casts about, comes up with some
general ideas, puts them down as programming specifications and presents 
them to somebody else, usually called a programmer, who
</p><ul>
<li> asks the analyst lots of questions, 
</li><li> programs what he thinks the analyst wants, 

</li><li> is told that what he has done is s completely wrong, 
</li><li>quarrels with the analyst about what he said and what it implied, 
</li><li>forces a reworking of the specifications,
</li><li>tries another program,
</li><li>is wrong again, forcing another rework of the specifications,
</li><li>and so on until finally, after a long period of false starts and reruns,
</li><li>some program is accepted as an implementation of some set of
	specifications, both programmer and analyst now being so sick of the
	entire procedure that they no longer care whether what is finally
	programmed is what was originally wanted.
</li></ul>
Is what I'm describing familiar to you? Does it, perchance, occur in your
organization?

<p>
To me, specifications cannot properly be called specifications until they 
are as abstract as blueprints or mathematical formulas. Specifications using the
cords of everyday speech are always subject to misinterpretation. Most of 
the costs, delays and other inefficiencies that attend the development of data-
processing procedures are due to these misinterpretations.
The abstract APL program specifications I tested at Stanford strenghtened my
belief in the opinion I've just expressed.
</p><p>

<b>Content</b>. What did I ask the students to program? Let me select four 
examples.
</p><p>

<b>Rings-O-Seven</b>. The first example was from A Programming Language page 63,
Exercise 1.5. It was a solution of a "rings-o-seven" puzzle, in which rings 
on a bar were to be removed according to certain rules. The bar was 
represented by a logical vector in which the presence of a ring was indicated by a 1 and 
the absence by a 0.
</p><p>

This was just a warmup experiment, but it had an informative result. My
programming specifications, naturally, described the solution procedure I 
had devised. It was wrong. But it was programned exactly the way I specified it.
</p><p>

Think of it. No arguments about who misunderstood whom. The systems analyst
was wrong; the programmer was right.
</p><p>

Wouldn't you like to be able to make that determination without bickering,
recriminations or tears?
</p><p>

(After my blushes subsided, I wrote a correct procedure. It was programmed
correctly and gave the right answers. I tell you this because I'm vain and
don't want you to remember me for the only mistake I ever made in my whole
life.)
</p><p>

<b>Internal Rates of Return on Investment</b>. Financial theory at that time was
troubled by the fact that the then current procedures for calculating








<!-- start page 13   --->






rates of return on an investment gave ambiguous answers in some cases. The
difficulty arose when the cash flows that characterized the investment were 
a mixture of positive and negative amounts. This led to an equation with 
multiple roots, so that, in many cases, two equally ridiculous rates of return were
calculated.

</p><p>

Dan Teichroew felt that the difficulty lay in the assumption that the same
interest rate should be applied to the negative cash flows as to the 
positive. What he suggested was that there would be a unique, meaningful solution if 
we assumed that the rate to be applied to the negative flows, which were, after
all, borrowings, should be a putative "cost of money" which would not, in
general, be the same as the return on the investment.
</p><p>

Given this assumption, I provided a proof that an "internal rate of return" 
on the investment would be uniquely determined when a "cyst of money" was 
assumed.
</p><p>
I then specified, in "Iverson language", a procedure that determined a rate 
of
return for a fixed cost of money and a specified series of cash flows. The
actual calculation was programmed by J. P. Seagle, a graduate student. I no
longer remember what programming language Pete used, possibly a home-grown
(Stanford) assembler for the 1401. The results were reported in a couple of
papers by authors the very ponderosity of whose names (Teichroew, Robichek,
Montalbano) testified to the validity, excellence and importance of the
research they described.
</p><p>
Critical-path calculations. Critical-path and PERT calculations were all the
rage at that time, with many papers being devoted to efficient schemes for
"topological sorting" and for detecting inconsistencies in precedence
relationships.
</p><p>
I became interested in the problem and decided that the need for topological
sorting could be eliminated and that consistency checking did not require a
separate, special program. I used APL (Iverson language) to describe a 
solution procedure in which topological sorting was not required and 
consistency-checking was a fallout from the basic critical-path calculation.
</p><p>
This time, in addition to having Pete Seagle program the calculation, I
programmed it myself, in MAP, the IBM 7090 assembly language, and FORTRAN 
(for input-output subroutines). My program exploited almost every bit in the 
36-bit 7090 word. This permitted me to store enormous networks internally, so that 
I was able to achieve calculation speeds far in excess of any other method 
then available.
</p><p>
I was not then, and have not since become, an expert assembly-language
programmer. I had risen to too august an eminence at Kaiser Steel to do much
programming, though I snuck some in now and then, when no one was looking. 
My critical-path algorithm was the first programming I had ever done for the 
7090 which, like MAP and FORTRAN, was completely new to me.
</p><p>

With the precise specifications of the APL procedure as my guide, 
programming assembly language for a machine with which I had little experience went very
quickly and with no errors other than mistypings. The program I developed 
was a useful one that I later used in classes in the Business School and in the
International Center for the Advancement of Management Education at 
Stanford. I described it in a paper called High-Sped Calculation of the Critical Paths
of Large Networks that appeared both as a Palo Alto Scientific Center report
and as an IBM Systems Journal article. The algorithm presented in the paper
used the old notation (the one in the book) since the new (the one for the
typewriter) had not yet been designed.
</p><p>
So APL turned out to be a particularly useful way to specify a program for 
an
inexperienced programmer--me.
</p><p>
<b>Linear Programming</b>. The last program specification I want to discuss was 
done by a student, Don Foster, who had even less time than the general run of
student I'd been working with. I believe it was his last term at the School 
of Business. This is always a hectic time but in Don's case he had the added
distraction of planning for a European trip. Even without my assignment he 
was leading a harried life.



</p><p>
I gave him a version of the simplex algorithm (basically the one that had
originally sold me on APL) from which I'd removed all clues like, 
"unbounded", "infeasible", etc.
</p><p>
The solution was programmed in jig time, since Don was champing at the bit
anyway. The programming language was FORTRAN. The program ran the first time
it was tried. It produced the right answers.
</p><p>
What was interesting was Don's reaction when I told him what he'd 
programmed. Like all good business school students of that era, he'd received 
instruction in linear programming. But he hadn't been told how easy it was. The 
arm-waving and chalk dust had concealed the basic Simplicity from him as they had from 
me.
</p><p>
<b>Business Information Systems</b>

</p><p>
As a guest lecturer in Business School courses, I advanced the argument that
I've been advancing throughout this talk, that we need an efficient notation
for describing procedures. I illustrated some of what could be done with APL
and decision tables.
</p><p>
In my Business Information Systems course, I did the same, but I also
encouraged activities that would permit students to find out for themselves
whether or not I had valid reasons for what I was recommending. One of the
course requirements was completion of an approved project. As an example of 
the kind of project I had in mind, I would suggest that they go to a local 
company, get a copy of an important report used by several departments, and visit 
each of the departments, asking whoever got the report what he thought the report
told him, how much he knew about how the figures in the report were 
determined, what actions he based on the report, and how he decided on his actions.
</p><p>
Few of the students had had detailed business experience at that point in 
their careers. Business to them was defined by the other business school courses
they'd taken: finance, marketing, micro- and macro-economics, accounting,
theory of the firm, organizational behavior, and so on. These were good 
courses but none of them were concerned with or had the time to devote to 
determining what was happening at the working or first-line management level of an
organization.
</p><p>
The intent of my course was to forewarn my students that business was, in
practice, a good deal more disorganized than they were likely to realize 
from most academic discussions.
</p><p>
I had some skeptics in my classes, people who felt I was overstating my 
case. None of the skeptics who attempted the kind of project I suggested remained
skeptics. Some, shattered by their experiences, felt even more strongly 
than I that no one in management knew what was going on.
</p><p>
Several students caught the APL bug. They went out on missionary activities 
of various kinds after graduation. The effects of some of these are still being
felt--in, for example, organizations like IBM and American Airlines, to name
the two I know most about.
</p><p>
<b>Stanford's Computer Science Department.</b>
</p><p>
Although I was housed with members of the Computer Science Department, I had
no official connection with it. All of my interaction with faculty, staff or
students was informal.

</p><p>
From the standpoint of APL as it now is, however, this interaction was the
important one. This was not because of anything I did. It was primarily 
because I was a reminder of the existence of "Iverson language" and a kind of 
catalyst who served to bring together the right people at the right time.
</p><p>
The Computing Science Department of those days was an ALGOL stronghold. It 
had a Burroughs B5000, later upgraded to a B5500, an IBM 7090, and a









<!-- start page 15   --->







PDP-1, probably the first computer I ever saw with a cathode-ray tube 
terminal. I don't know whether "Star Wars" (the game) was developed at Stanford. I do
know that a lot of "Star Wars" was played there.
</p><p>
Stanford had developed its own version of ALGOL, called SUBALGOL, for the
Burroughs computer that had preceded the 85000. I believe the number was 
B220, but my memory might be playing me tricks. At Kaiser Steel, our first 
computer had been a B205, predecessor of the B220, if that's what it was.
</p><p>
The significance of this information from the APL standpoint is that two,
possibly three, of the people who played key roles in developing the very 
first APL system had been instrumental in producing SUBALGOL for Stanford: Larry
Breed, Roger Moore, and (the one I'm not sure about) Phil Abrams.

</p><p>
I met Larry as a result of a talk I'd given as one of a series on 
"Programming Languages" conducted by the Computer Science Department. He expressed an
interest in what I'd had to say about what I was doing in the School of
Business with the notation described in <u>A Programming Language</u>. He and Phil
Abrams took action on this interest in a very real, very Productive way when
the IBM Systems Journal article, <u?a formal="" description="" of="" system="" 360<="" u="">
appeared. What they did, and its aftermath, is described in Appendix A, an annotated
verse history of APL's early days.
</u?a></p><p>
Larry and Phil not only developed the batch APL interpreter I mention in the
verse, they did so many other things that I wish they and others involved in
APL's origins would get them down on paper. For example, one of them should
tell the story of Elsie (for Low Cost), an APL mini before there were minis.
</p><p>
But, in essence, all I did was happen to be around, saying the right things 
to the right people. Things took off when the right people got together.
</p><p>
Incidentally, one of the people involved in the Programming Languages 
seminar to which I referred above was Niklaus Wirth. Unfortunately, Klaus didn't get
the proper message from my talk. He went his own way and developed PASCAL.
</p><p>
<b>APL at IBM</b>
</p><p>
The history of APL at IBM has been a curious one. In the early days, those 
of us who believed in APL were regarded as being a little (perhaps more than a
little) strange. Since much of the strangeness was concentrated in IBM
Research, this was tolerated. Practical people (the kind of people who make
sales and meet payrolls) expect research people to be strange and are 
usually disappointed when they're not. So the strange people in Research were 
written off as overhead and left to amuse themselves with their incomprehensible,
impractical symbols.

</p><p>
What that particular Research group did, of course, was produce the most 
solid, dependable, useful time-sharing system anyone had ever seen.
</p><p>
I wish I could tell you what it felt like in those early days to have the 
use of a system that was up twenty-four hours a day, seven days a week. No one 
had ever known such a luxury. People who didn't bother to investigate never
believed us when we told them about it.
</p><p>
But some people did investigate what the researchers had developed and 
started to use it to do IBM's key bread-and-butter applications. This way of doing
business was so productive that it spread like wildfire. By the time the
practical people found out what had happened; APL was so important a part of
how IBM ran its business that it could not possibly be uprooted. The 
wild-eyed researchers had produced a moneymaker. No talks about product "strategies" 
and the evils of language proliferation prevailed against the simple fact that:
</p><ul>
<li>if  you worked for IBM and
</li><li>had access to an APL time-sharing service and





<!-- start page 16   --->



</li><li>had something you wanted to get done on a computer quickly and
economically
</li></ul>

then the best way to get it done was to use APL.
<p>
I wish someone who knows the details of how that came about would write 
about it. I can't do it. I was three thousand miles away when it took place.
APL (called VS APL for reasons beyond the ken of mortal man) is, of course, 
now an IBM program product. I don't know how much more practical than that you 
can get.
</p><p>
<b>Summary: Systematic Systems Analysis</b>
</p><p>
I could go on but I see you're falling asleep. Let me end by rephrasing 
what is either explicit or implicit in what I've already said.
</p><p>
In the preface to my book on decision tables, I say
</p><blockquote>
<b>
if you wish to use digital computers effectively, the first thing you
should do is digitize your procedure descriptions.
</b>
</blockquote>
<p>
As usual, this was something I realized I was doing after I'd finished 
writing and took time to think about what I'd written. The key idea of the book (the
potential of which, incidentally, no one has as yet successfully exploited) 
is that procedures can be digitized in the same sense that bubble-chamber and
spark-chamber pictures are digitized for analysis by a digital computer.

</p><p>
A decision table is a digitized procedure description; it describes a
correspondence between vectors of decision values and vectors of action 
values.
</p><p>
The particular form of the digitizing is not important. Decision tables may 
or may not be the most effective way to get the digitizing done. The important
thing is that it be done and done in a way that permits checking for
consistency, redundancy, completeness.
</p><p>
But how is such a digitized procedure to be developed, maintained, managed,
modeled, interpreted, translated, improved, extended,...?
</p><p>
To me, the answer is clearly APL. If we are ever to do "systems analysis"
systematically, we must
</p><ol>
<li>	Digitize our procedure descriptions
</li><li>	Manage our digitized descriptions with APL
</li></ol>
<b>We must divert our research to developing ideas rather than gadgets. Good 
ideas are at hand. Let's develop them.</b>

<p><a name="Epilogue">
<b>Epilogue - Twenty Years After</b>
</a></p><p><a name="Epilogue">

The latest IBM version of APL is an Installed User Program called APL2. For
those of us who had to bootleg our APL efforts within IBM for a long time, 
the announcement of APL2 is gratifying because it indicates the kind of 
management backing and recognition that we missed when We felt that APL was regarded 
as a limited tool for a small, specialized audience.
</a></p><p><a name="Epilogue">
Management put its support on record in another significant way. IBM 
recently instituted awards for outstanding technical achievements. The first of 
these to anyone at the Santa Teresa Laboratory was just awarded to Jim Brown, 
manager of the group that developed APL2.
</a></p><p><a name="Epilogue">
I haven't had a chance to use APL2 very much yet. I've been too busy 
writing a workspace manual for VS APL. So I  can't pretend to an extensive







<!-- start page 17   --->






knowledge of its details. But, recently, I used APL2 to do something that I
haven't been able to do for 20 years.

Page 19 of <u>A Programming Language</u> describes a bank ledger that has three
columns: the first column contains customer names, the second account 
numbers, the third balances. Unassigned account numbers had the entry "none" in the
corresponding row of the name column.
</a></p><p><a name="Epilogue">
Until APL2, no system available to me within IBM would allow me to form an
array of that kind. Nor could I write, in any straightforward fashion, the 
four programs, producing reports from that ledger, that appear as one-liners in 
the book with which, twenty years ago, I started my investigation of 
"Quantitative Techniques in Management" at the Stanford Graduate School of Business.
</a></p><p><a name="Epilogue">
With APL2, I was able to do precisely that. As a way of rounding out twenty
years of APL history, I thought I'd show you what I did. It's contained in
Appendix B.





<!-- start page 19   --->





</a><a name="A">
</a></p><hr><a name="A">
<h3>APPENDIX A. APL BLOSSOM TIME -A HISTORY IN VERSE</h3>
<p>

My contribution to APL 81 was the verse that I discuss below. The most I'd
expected, when I wrote it, was that Jim Brown might play it at some informal
gathering. I couldn't anticipate what actually happened. A group including 
Jim Brown, Larry Breed, John Bunda, Diana Dloughy, A1 O'Hara and Rob Skinner
rehearsed their guitars and voices until they were of a truly harmonious
sweetness and sang "APL Blossom Time" at the APL 81 banquet as part of the
evening's entertainment. I. P. Sharp's Peter Wooster prepared overhead
transparencies that made it possible for the audience to sing along. And I'm
sure other people whose names I was never told contributed to what was, for 
me, an extremely heartwarming experience: the sound of people singing, laughing 
and giving every evidence of enjoying the words I'd written.
</p><p>
Despite its frivolity, <u>APL Blossom Time</u> is authentic history. I thought it
might be useful to get the details on record by annotating each section of 
the verse.

</p><center>
<b>APL BLOSSOM TIME</b>
</center>

A nostalgic reminiscence of the early days of APL, remembered to the tune of
The Battle of New Orleans.
<div>
<pre>Back in the old days, in 1962, 
A feller named Ken Iverson decided what to do. 
He gathered all the papers he'd been writing fer a spell 
And he put them in a little book and called it APL.

Well... 
    He got him a jot and he got him a ravel 
    And he revved his compression up as high as she could go 
    And he did some reduction and he did some expansion 
    And he sheltered all his numbers with a ceiling and a flo'.
</pre>
</div>
<p>
If you've read the earlier part of this book, this verse doesn't need
annotating. If you haven't, go back and read it.
</p><p>

<div>
<pre>
Now Sussenguth and Falkoff, they thought it would be fine 
To use the new notation to describe the product line. 
They got with Dr. Iverson and went behind the scenes 
And wrote a clear description of a batch of new machines.
Well...
	They wrote down dots and they wrote down squiggles
	And they wrote down symbols that they didn't even know
	And they wrote down questions when they didn't know the answer
	And they made the Systems Journal in nineteen sixty-fo'
</pre>
</div>
</p><p>

Though the scan required that I place Sussenguth's name first, this is 
perhaps misleading. Ed Sussenguth has done a lot of good work for IBM but, except 
for his participation in this paper, I don't know of any other in n which he 
used APL.
</p><p>
Adin Falkoff, on the other hand, was one of those crazy-symbol 
Iverson-language authors whose papers I started requesting from the ASDD Library when I first
joined IBM. I remember one paper in particular. It struck me because it 
seemed to be written by someone who hated jargon as much as I did. One of the data-
processing fads current at that time was





<!-- start page 20   --->




the "associative memory". Adin, like the rest of us, had to use the term
because everybody else was using it. But he rather wistfully (can you 
imagine Adin wistful?) pointed out that a more descriptive term would be "content-
addressable memory".
</p><p>
And, of course, as you all know (or should know, if you don't), Adin 
Falkoff, in both technical and administrative capacities, has been in the forefront 
of APL developments ever since the days of those early, incomprehensible 
reports out of IBM Research.
</p><p>
The paper referred to in the verse is A Formal E. escription of System/360,
by Falkoff, A. D., Iverson, K. E., Sussenguth,	H. IBM Systems Journal,
Vol. 4, No. 4, October, 1964.
</p><p>
About "questions where they didn't know the answers": the paper was indeed, 
to the best of my recollection, the first to use the question mark as an APL
function.
</p><p>
I gave my copy of that issue of the Systems Journal to Larry Breed. (I had
already ordered several more. John Lawrence, editor of the Systems Journal 
at the time, had the APL functions in the article printed separately for more
effective study. I ordered several copies of those, as well.) Larry and Phil
Abrams conducted a seminar on the System/360 paper that extended over 
several weeks. They also produced a list of "cliches", to assist in understanding
regularly recurring patterns, and a list of errata, to remind the authors 
(or the typesetters) that they didn't know it all.
</p><p>
The sessions conducted by Larry and Phil were well attended. When Ken 
Iverson came out to give a talk at Stanford, he drew the biggest crowd the Computer
Science auditorium had seen up to that time. I told my Business Information
Systems class to attend since they would hear something better than 
anything I had to say; this also gave me a chance to attend myself.
</p><p>
<div>
<pre>
Now writing dots and squiggles is a mighty pleasant task 
But it doesn't answer questions that a lot of people ask. 
Ken needed an interpreter for folks who couldn't read 
So he hiked to Californ-i-a to talk to Larry Breed.

   Oh, he got Larry Breed and he got Phil Abrams 
   And they started coding Fortran just as fast as they could go 
   And they punched up cards and ran them through the reader 
   In Stanford, Palo Alto, on the seventy ninety oh.

</pre></div>
</p><p>
Ken Iverson and Larry Breed first met in my office at Polya Hall. Since this
may be my only claim to fame, I'm glad to put this historical fact on the
record.
</p><p>
Larry was about to graduate. Ken had a job to offer him. We now have APL.
</p><p>
I remember a phone call of Ken's, shortly after Larry had joined him and 
Adin at IBM Research in Yorktown, in which he said something like: "This young 
man thinks he can write a translator in a couple of months." He sounded as if he
were wondering whether he'd made a bad bargain. I assured him that if Larry
said he could do something in a couple of months he would probably do it in 
a couple of weeks. He and Roger Moore were legends in their own time during
Stanford's SUBALGOL days.

</p><p>
Stanford has left a mark on APL second only to that of left-handed 
Canadians. For a while, there was a theory that all of APL was being dominated by 
left-handed Canadians. I have been told that when Mike Jenkins, at lunch in the
Yorktown cafeteria, was observed to be left-handed, someone facetiously 
asked him if he happened to be Canadian. He happened.




<!-- start page 21   --->



</p></a><p><a name="A">
I tried to start a similar factoid<sup></sup></a><a href="#SUP">5</a> about right-handed 
Brooklynites,
hoping to get included along with Falkoff and McDonnell. I forget what 
happened to that. I think one of them is lefthanded. I know I'm not.

</p><p>
<div>
<pre>
Well a Fortran batch interpreter's a mighty awesome thing 
But while it hums a pretty tune it doesn't really sing. 
The thing that we all had to have to make our lives sublime 
Was an interactive program that would let us share the time.

Oh, they got Roger Moore and they got Dick Lathwell, 
And they got Gene McDonnell with his carets and his sticks, 
And you should've heard the uproar in the Hudson River valley 
When they saved the first CLEANSPACE in 1966.
</pre></div>
</p><p>
 

APL bigots seem to be characterized by literacy and a feeling for history. 
The first time-sharing APL system was implemented (as IVSYS) on an IBM 7090 at
Mohansic. In those days, there was no )CLEAR command. To get a CLEAR 
workspace, you had to load one. The one that came with the system was called 
CLEANSPACE. Although it was no longer needed when )CLEAR was introduced, CLEANSPACE, 
along with the time and date it was originally stored, has been preserved in 
Library 1 of a continuous sequence of systems ever since: the Yorktown Model 50, the
Philadelphia Scientific Center Model 75, the Palo Alto model 158, the Santa
Teresa model 168 and, as I discovered for the first time just a few hours
before I wrote this, the Santa Teresa Model 3033. At one point, after a
disaster had caused the loss of CLEANSPACE, it was carefully restored with 
the correct date and time. The objective, of course, is to preserve a record of 
the moment when APL first became a time-sharing computer language.
</p><p>
Preserving CLEANSPACE in APL2 presented a problem, since workspace names are
limited to eight characters in CMS, the first "environment" in which APL2 
has been offered. However, as you can see from the following exhibit, which is a
copy of what appeared on the screen in response to a )LOAD 1 CLEANSPACE 
command that I executed on our APL2 system (which operates under CMS) the problem 
has been solved, or, better, circumvented.
</p><p>
<p>
<img src="https://ed-thelen.org/comp-hist/APL-hist-pg21.gif">
</p>

</p><p>
Note that not only is the time given, the time zone of the area in which the
storing was done is also given, indicating that the original workspace was
stored when United States Eastern Standard Time was in effect. Note also 
that the original workspace size was 32K and that the time zone in which 
CLEANSPACE was loaded to produce this example was Santa Teresa Daylight Savings Time.
</p><p>
What hard workers AP L. bigots are' I've checked my handy perpetual calendar
and, as far as I can tell, November 27, 1966 was the Sunday of what




<!-- start page 22   --->




must have been a four-day Thanksgiving weekend. What were those loonies 
doing working such crazy hours during the holiday season?
</p><p>
The !'carets and sticks" reference is to a paper by Gene McDonnell on the
logical and relational functions--the ones whose symbols can be constructed 
out of "carets and sticks".
</p><p>
<div>
<pre>Well, when A1 Rose saw this he took a little ride 
In a big station wagon with a type ball by his side. 
He did a lot of teaching and he had a lot of fun 
With an old, bent, beat-up 2741.

Oh, it typed out stars and it typed out circles 
And it twisted and it wiggled just like a living thing. 
Al fed it a tape when he couldn't get a phone line 
And it purred like a tiger with its trainer in the ring.

</pre>
</div>
</p><p>
A1 Rose was, and I assume still is, one of the most spectacular APL
demonstrators there ever has been. The verse refers to a vacation he took in
which he was accompanied not only by his family but by what was laughingly
called a portable 2741. This was a 2741 that came in two parts which, when
ready to be "ported", looked like two big, black pieces of luggage. 
Wherever Al went, he'd find some likely APL prospects, park the station wagon near an
electrical outlet and a phone, lower the tailgate and start hammering on the
keys.
</p><p>
In those days, getting connected to a working APL system was a chancy 
thing. As a safeguard, A1 recorded, on tape, what went across the acoustic coupler 
during a sample session. When he had problems getting to a real APL system, he`d 
play the tape into the acoustic coupler and produce a simulated computer session
that was an exact copy of the real thing.
</p><p>
I remember that double-black-box 2741 very well myself. I, too, did quite a 
bit of APL demonstrating in those days. At the University of California at 
Davis, the demonstration was given on the second floor and there was no elevator. I
had to haul those two big boxes up a long flight of stairs. I'm glad I 
didn't find out until later how heavy they were. When I sent them Air Express to an
IBM System Engineer in Seattle, I learned for the first time that they 
weighed 120 pounds. Well, I'm not too bright but I'm pretty strong.
</p><p>
<div>
<pre>
Now, there's much more to the story, but I just don't have the time 
(And I doubt you have the patience) for an even longer rhyme. 
So I'm ending this first chapter of the tale I hope to tell 
Of how Iverson's notation blossomed into APL.
So..
    Keep writing nands when you're not writing neithers, 
And point with an arrow to the place you want to be, 
But don't forget to bless those early APL sources 
Who preserved the little seedling that became an APL tree.

Dedicated to the pioneers of APL with respect and
affection by
J. C. L. Guest
</pre>
</div>

</p><p>



J. C. L. Guest is the pseudonym I used for sore light verse I submitted to
Datamation several years back. There were four pieces in all. <u>The Far-flung
Data Base</u>, <u>SYSABEND Dump</u>, <u>Virtual Memory</u> and <u>Decision Making</u>.
</p><p>
If you were offended by the unkind things I said about modern management in
this talk, don't read Decision Making. You won't like it.




<!-- start page 23   --->

<a name="B">


</a></p><hr><a name="B">
<h2>APPENDIX B - TWENTY YEARS AFTER </h2>
<p>
The following four figures show how I applied APL2 to Program 1.9 (Example 
1.1) of <b>A Programming Language</b>, page 19.
</p><p>
The first figure shows the sample bank ledger I used and the calculations I
performed to illustrate the ledger's shape and various facts about its
composition. Note that the name entry for an unassigned account number is a
single blank rather than the "none" used in the original example.
</p><p>
The second figure shows two versions of the four reports (P, Q, R, S) 
required in the example. In the first, the output is unformatted. In the second,
"picture format" is used to format the numeric part of the report.
</p><p>

The	four required reports are:
</p><ol>
<li> 	P - name, account number and balance for each account with a balance less
than two dollars. (Although the original example did not require this, the
illustrated calculations do not include unassigned account numbers in the
report.)

</li><li> 	Q - name, account number and balance for each account with a negative
balance exceeding one hundred dollars.

</li><li> 	R - name and account number of each account with a balance exceeding one
thousand dollars.
</li><li> 	S - all unassigned account numbers
</li></ol>
<p>
The third and fourth figures show the programs for the unformatted and
formatted reports respectively. They could have been written as the four 
one-liners of the original example, except that report P, the one producing a 
list of accounts with balances of less than two dollars would have included
unassigned account numbers. To avoid this, the unassigned account number
report, S, was produced first and an array T, consisting of all assigned
accounts, was used to create the subsequent reports.

</p><p>
The other lines in the report merely introduce spaces to separate the
successive reports.


<!-- start page 24   --->



</p><p>
<p>
<img src="https://ed-thelen.org/comp-hist/APL-hist-pg24.gif">
</p>
</p><p>
<p>
<img src="https://ed-thelen.org/comp-hist/APL-hist-pg25.gif">
</p>
</p><p>
<p>
<img src="https://ed-thelen.org/comp-hist/APL-hist-pg26.gif">

</p>
</p><p>
<p>
<img src="https://ed-thelen.org/comp-hist/APL-hist-pg27.gif">
</p>
</p></a><p><a name="B">

------------------------------------
</a><a name="SUP">
</a></p><ol><a name="SUP">

<li><b>First draft of a report on the EDVAC</b>, J. von Neumann, June 1945,
(report), Moore School of Electrical Engineering, Univerity of
Pennsylvania, Philadelphia, Pa.

</li><li>A Programming Language, Kenneth Iverson, John Wiley, 1962

</li><li>In making this comment here, and not earlier when I was describing the
work I did for the Air Force, I do not mean to make an invidious
comparison of the services. The work I did for the Air Force didn't
require me to know whether either the data we used or the answers we
calculated corresponded to reality; all I had to do was to supply
plugboards or programs. At the Naval Research Laboratory, on the other
hand, it was my responsibility to get good answers from good data. That's
when I found out there wasn't any good data.
<p>
To even things out, let me observe that the Air Force doesn't know what's
going on either. As for the Armor, well, I served in the Army. I can tell
you about the Army.

</p></li><li> Decision Tables, Michael Montalbano, Science Research Associates, 1974
</li></a></ol><a name="SUP">
</a></dd></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A new approach to domain ranking (180 pts)]]></title>
            <link>https://www.marginalia.nu/log/73-new-approach-to-ranking/</link>
            <guid>38504565</guid>
            <pubDate>Sun, 03 Dec 2023 03:34:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.marginalia.nu/log/73-new-approach-to-ranking/">https://www.marginalia.nu/log/73-new-approach-to-ranking/</a>, See on <a href="https://news.ycombinator.com/item?id=38504565">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>This is a very brief post announcing a fascinating discovery.</p><p>It appears to be possible to use the cosine similarity approach powering explore2.marginalia.nu as a substitute for the link graph in an eigenvector-based ranking algorithm (i.e. PageRank).</p><p>The original PageRank algorithm can be conceptualized as a simulation of where a random visitor would end up if they randomly clicked links on websites. With this model in mind, the modification replaces the link-clicking with using explore2 for navigation.</p><p>The performance of PageRank has been deteriorating for decades and it’s to a point where it barely is applicable for domain ranking anymore in part due to changes in how websites link to each other, but also a battery of well documented techniques for manipulating the algorithm in order to gain an unfair advantage. You may get decent results at the very top especially with personalized pagerank, but you don’t have to scroll particularly far down in the ranking to find spam earning a conspicuously high ranking using a vanilla pagerank approach.</p><p>This new approach seems remarkably resistant to existing pagerank manipulation techniques. Given a preference-vector, it stays “on topic” remarkably well.</p><ul><li><a href="https://www.marginalia.nu/domains/">Explore Sample Data</a></li></ul><h2 id="see-also">See Also</h2><ul><li><a href="https://www.marginalia.nu/log/69-creepy-website-similarity.gmi">/log/69-creepy-website-similarity.gmi</a></li><li><a href="https://www.marginalia.nu/log/20-dot-com-link-farms.gmi">/log/20-dot-com-link-farms.gmi</a></li><li><a href="https://www.marginalia.nu/log/04-link-farms.gmi">/log/04-link-farms.gmi</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Clang now makes binaries an original Pi B+ can't run (301 pts)]]></title>
            <link>https://rachelbythebay.com/w/2023/11/30/armv6/</link>
            <guid>38504134</guid>
            <pubDate>Sun, 03 Dec 2023 02:07:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rachelbythebay.com/w/2023/11/30/armv6/">https://rachelbythebay.com/w/2023/11/30/armv6/</a>, See on <a href="https://news.ycombinator.com/item?id=38504134">Hacker News</a></p>
Couldn't get https://rachelbythebay.com/w/2023/11/30/armv6/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[UniFi Express (199 pts)]]></title>
            <link>https://ui.com/cloud-gateways/express</link>
            <guid>38504027</guid>
            <pubDate>Sun, 03 Dec 2023 01:44:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ui.com/cloud-gateways/express">https://ui.com/cloud-gateways/express</a>, See on <a href="https://news.ycombinator.com/item?id=38504027">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Not a real engineer (2019) (275 pts)]]></title>
            <link>https://twitchard.github.io/posts/2019-05-29-not-a-real-engineer.html</link>
            <guid>38503486</guid>
            <pubDate>Sun, 03 Dec 2023 00:05:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitchard.github.io/posts/2019-05-29-not-a-real-engineer.html">https://twitchard.github.io/posts/2019-05-29-not-a-real-engineer.html</a>, See on <a href="https://news.ycombinator.com/item?id=38503486">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
            <article>
    
    <section>
        <p>You are not a real engineer.</p>
<p>You are a creature with a terrible countenance</p>
<p>the stature of a man</p>
<p>and the head of a lion</p>
<p>with sixteen wings about you</p>
<p>white as fresh snow</p>
<p>hundreds of eyes</p>
<p>that flicker like torches</p>
<p>and dart in all directions</p>
<p>the belt at your waist is a serpent</p>
<p>your breath is the gathering of stormclouds</p>
<p>your voice is the roar of the wind</p>
<p>the grass withers before your footsteps</p>
<p>and your writhing, ponderous tongue</p>
<p>black as the abyss</p>
<p>brings apocalypse upon everything it touches</p>

<p>We regret to inform you</p>
<p>we will not be offering you the role at this time –</p>
<p>we’re looking for somebody more technical</p>
    <hr>

    
    </section>
</article>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Factory construction boom in the US (101 pts)]]></title>
            <link>https://wolfstreet.com/2023/12/02/the-eyepopping-factory-construction-boom-in-the-us/</link>
            <guid>38503462</guid>
            <pubDate>Sat, 02 Dec 2023 23:59:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wolfstreet.com/2023/12/02/the-eyepopping-factory-construction-boom-in-the-us/">https://wolfstreet.com/2023/12/02/the-eyepopping-factory-construction-boom-in-the-us/</a>, See on <a href="https://news.ycombinator.com/item?id=38503462">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		    <h3><strong>The supply-chain catastrophe in 2020-2021, the edgy US-China relationship, and the scary dependence on China triggered a big corporate rethink.</strong></h3>
<h4>By&nbsp;<a href="https://wolfstreet.com/author/wolf-richter/" target="_blank" rel="noopener noreferrer">Wolf Richter</a>&nbsp;for&nbsp;<a href="https://wolfstreet.com/" target="_blank" rel="noopener noreferrer">WOLF STREET</a>.</h4>
<p>In October, $18.5 billion were plowed into construction of manufacturing plants in the US ($246 billion annualized), up by 73% from a year ago, by 136% from two years ago, and by 166% from October 2019. The relentless pace of the month-to-month increases is what’s amazing – from $12.5 billion spent in January to $18.5 billion in October.</p>
<p>The construction boom started in early to mid-2021, and since then, spending has tripled. About a year later, in July 2022, Congress passed a package of subsidies for select manufacturing industries to build factories, such as semiconductor makers (they’ll get $52 billion) and EV battery makers.</p>
<p>But the wheels of government turn slowly, approvals take their time, disbursements of funds for large projects takes time, so the flow of these government funds would just be the first trickle in 2023. The bulk is still coming.</p>
<p><img fetchpriority="high" decoding="async" src="https://wolfstreet.com/wp-content/uploads/2023/12/US-construction-spending-2023-12-02-manufacturing.png" alt="" width="542" height="437" srcset="https://wolfstreet.com/wp-content/uploads/2023/12/US-construction-spending-2023-12-02-manufacturing.png 542w, https://wolfstreet.com/wp-content/uploads/2023/12/US-construction-spending-2023-12-02-manufacturing-260x210.png 260w, https://wolfstreet.com/wp-content/uploads/2023/12/US-construction-spending-2023-12-02-manufacturing-160x129.png 160w" sizes="(max-width: 542px) 100vw, 542px"></p>
<p>For the calendar year 2023, spending on factory construction will likely get close to $200 billion. For the first 10 months, the ever-larger monthly amounts already reached $159 billion. If November and December are only flat with October, spending will reach $196 billion by year-end.</p>
<p><img decoding="async" src="https://wolfstreet.com/wp-content/uploads/2023/12/US-construction-spending-2023-12-02-manufacturing-annual.png" alt="" width="542" height="425" srcset="https://wolfstreet.com/wp-content/uploads/2023/12/US-construction-spending-2023-12-02-manufacturing-annual.png 542w, https://wolfstreet.com/wp-content/uploads/2023/12/US-construction-spending-2023-12-02-manufacturing-annual-260x204.png 260w, https://wolfstreet.com/wp-content/uploads/2023/12/US-construction-spending-2023-12-02-manufacturing-annual-160x125.png 160w" sizes="(max-width: 542px) 100vw, 542px"></p>
<p>The stagnation of factory construction through 2021 is a testimony to corporate chieftains in search of cheap labor.</p>
<p>But the supply-chain and transportation catastrophe they ran into during the pandemic, the edgy relationship between the US and China, and the scary dependence of US companies on Chinese suppliers have triggered a big corporate rethink.</p>
<p><strong>It’s not like the US “isn’t making anything anymore,” but… </strong>The US is the second largest manufacturing country by output, behind China and has a greater share of global production than the next three countries combined, Germany, Japan, and India.</p>
<p>But the US, as the largest economy in the world, has fallen far behind China in manufacturing and many sectors have become brutally dependent on China – and the shortages and supply-chain chaos of 2020-2021 were a wakeup call.</p>
<p>Manufacturing’s share of GDP had been on a long downward trend in the US. The data by the Bureau of Economic Analysis only goes back to 2006. Back then, manufacturing accounted for over 13% of GDP. By early 2020, manufacturing was down to 10.5% of GDP. Since then, the share has been wobbling higher.</p>
<p>This year’s construction spending boom will add to manufacturing’s share of GDP in the future when the plants are up and running and producing at scale, which takes time. In other words, this is a slow process, and it has a long way to go:</p>
<p><img decoding="async" src="https://wolfstreet.com/wp-content/uploads/2023/12/US-manufacturing-percent-GDP-2023-12-02.png" alt="" width="548" height="434" srcset="https://wolfstreet.com/wp-content/uploads/2023/12/US-manufacturing-percent-GDP-2023-12-02.png 548w, https://wolfstreet.com/wp-content/uploads/2023/12/US-manufacturing-percent-GDP-2023-12-02-260x206.png 260w, https://wolfstreet.com/wp-content/uploads/2023/12/US-manufacturing-percent-GDP-2023-12-02-160x127.png 160w" sizes="(max-width: 548px) 100vw, 548px"></p>

<p><strong>Manufacturing has a large-scale impact on the economy</strong>, for decades to come, with its secondary and tertiary effects. Construction spending is just the one-time activity to get the building and infrastructure in place. What comes afterwards – actual production with its secondary and tertiary effects on the local economy – is far more important.</p>
<p>Companies invest in factories in the US to make high-value technologically advanced products such as semiconductors and motor vehicles. Makers of computer, electronic, and electrical equipment are big drivers behind the surge of factory construction, according to an&nbsp;<a href="https://home.treasury.gov/news/featured-stories/unpacking-the-boom-in-us-construction-of-manufacturing-facilities">analysis</a>&nbsp; by the Treasury Department.</p>
<p><strong>Industrial robots reduce China’s cost advantages</strong>. Automation and industrial robots – key in mass production – cost roughly the same in the US as in China. US labor costs are far higher, but other costs are reduced by bringing manufacturing onshore: Transportation costs are lower, lead times are shorter, there is less geopolitical uncertainty, less risk of losing or having to surrender the IP via technology transfer, etc.</p>
<p><strong>Construction cost inflation is not the driver</strong> – it has ended this year. The Producer Price Index for construction of nonresidential buildings hit a peak in January. Since then, prices declined by 1.4%. So construction cost inflation was not a factor in the 2023 surge of construction spending, though it was in 2022. This makes the boom in factory construction spending in 2023 that much more amazing.</p>
<p><img loading="lazy" decoding="async" src="https://wolfstreet.com/wp-content/uploads/2023/12/US-construction-PPI-nonresidential-2023-12-02.png" alt="" width="546" height="417" srcset="https://wolfstreet.com/wp-content/uploads/2023/12/US-construction-PPI-nonresidential-2023-12-02.png 546w, https://wolfstreet.com/wp-content/uploads/2023/12/US-construction-PPI-nonresidential-2023-12-02-260x199.png 260w, https://wolfstreet.com/wp-content/uploads/2023/12/US-construction-PPI-nonresidential-2023-12-02-160x122.png 160w" sizes="(max-width: 546px) 100vw, 546px"></p>
<p><i><strong>Enjoy reading WOLF STREET and want to support it? You can donate. I appreciate it immensely. Click on the beer and iced-tea mug to find out how:</strong></i></p>
<p>
<a href="https://wolfstreet.com/how-to-donate-to-wolf-street/" target="_blank" rel="noopener"><img loading="lazy" decoding="async" src="https://wolfstreet.com/wp-content/uploads/2019/10/BeerMug2.jpg" alt="" width="100" height="115"></a>
</p>
<p><i><strong>Would you like to be notified via email when WOLF STREET publishes a new article? <a href="https://wolfstreet.com/sign-up-here-for-wolf-street-email-updates/" target="_blank" rel="noopener noreferrer">Sign up here</a>.</strong></i></p>
<p><img loading="lazy" decoding="async" src="https://wolfstreet.com/wp-content/uploads/2020/08/placeholder2.png" alt="" width="26" height="65"></p>

	    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Can't sign in with FIDO2 key on office.com (157 pts)]]></title>
            <link>https://bugzilla.mozilla.org/show_bug.cgi?id=1824831</link>
            <guid>38502340</guid>
            <pubDate>Sat, 02 Dec 2023 21:45:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1824831">https://bugzilla.mozilla.org/show_bug.cgi?id=1824831</a>, See on <a href="https://news.ycombinator.com/item?id=38502340">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">

 


<main id="bugzilla-body" tabindex="-1">



<div id="main-inner">










<div id="summary-container">


  
    <p><span id="field-value-status_summary">
      <span data-status="open">Open</span>
      <span id="field-value-bug_id">
        <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1824831">Bug 1824831</a>
      </span>
      <span>
        <span>Opened <span title="2023-03-27 12:53 PDT" data-time="1679946799">8 months ago</span></span>
          <span>Updated <span title="2023-12-02 13:43 PST" data-time="1701553423">1 hour ago</span></span>
      </span>
        </span>
    </p>

  
</div>








































<meta name="firefox-versions" content="{&quot;FIREFOX_AURORA&quot;:&quot;&quot;,&quot;FIREFOX_DEVEDITION&quot;:&quot;121.0b6&quot;,&quot;FIREFOX_ESR&quot;:&quot;115.5.0esr&quot;,&quot;FIREFOX_ESR_NEXT&quot;:&quot;&quot;,&quot;FIREFOX_NIGHTLY&quot;:&quot;122.0a1&quot;,&quot;LAST_MERGE_DATE&quot;:&quot;2023-11-20&quot;,&quot;LAST_RELEASE_DATE&quot;:&quot;2023-11-21&quot;,&quot;LAST_SOFTFREEZE_DATE&quot;:&quot;2023-11-16&quot;,&quot;LAST_STRINGFREEZE_DATE&quot;:&quot;2023-11-17&quot;,&quot;LATEST_FIREFOX_DEVEL_VERSION&quot;:&quot;121.0b6&quot;,&quot;LATEST_FIREFOX_OLDER_VERSION&quot;:&quot;3.6.28&quot;,&quot;LATEST_FIREFOX_RELEASED_DEVEL_VERSION&quot;:&quot;121.0b6&quot;,&quot;LATEST_FIREFOX_VERSION&quot;:&quot;120.0.1&quot;,&quot;NEXT_MERGE_DATE&quot;:&quot;2023-12-18&quot;,&quot;NEXT_RELEASE_DATE&quot;:&quot;2023-12-19&quot;,&quot;NEXT_SOFTFREEZE_DATE&quot;:&quot;2023-12-14&quot;,&quot;NEXT_STRINGFREEZE_DATE&quot;:&quot;2023-12-15&quot;}">



<div id="c0">

  <div id="ct-0" data-comment-id="16345428" data-ismarkdown="true"><p>+++ This bug was initially created as a clone of <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1820016" title="RESOLVED FIXED - Cannot assert discoverable credential on login.live.com">Bug #1820016</a> +++</p>
<p>Since #1820016 is fixed, I am able to log in on live.com using my FIDO2 security key.</p>
<p>However, on office.com I still am unable to do so (using a different business/paid account which works when using Chrome).</p>
</div><div><p>Flags: needinfo?(tim.cappalli)</p></div></div><div id="c1"><p>This is not a Firefox issue. A fix for AAD accounts on the Microsoft side is being worked on.</p><div><p>Flags: <span>needinfo?(tim.cappalli)</span></p></div></div><div id="c2"><p>Any chance to follow the progress as an outsider?</p></div><div id="c5"><div id="ct-5" data-comment-id="16370318" data-ismarkdown="true"><p>The severity field is not set for this bug.<br>
:jschanck, could you have a look please?</p>
<p>For more information, please visit <a href="https://wiki.mozilla.org/Release_Management/autonag#workflow.2Fno_severity.py" rel="nofollow">auto_nag documentation</a>.</p>
</div><div><p>Flags: needinfo?(jschanck)</p></div></div><div id="c6" data-comment-id="16443788" data-ismarkdown="true"><p>With Firefox 114.0 on Linux, I am able to log into by business account using a YubiKey on office.com. However, only when in private browsing mode of my day-to-day-profile. Using regular mode or a totally fresh profile (private or regular browsing mode), it does not work either ("We had a problem authenticating you. Please try again.").</p>
<p>Any chance to get an update here? It feels like this is really close to be usable.</p>
</div><div id="c7"><p>My understanding is that office.com is a weird exception because it can use the live.com personal account login flow for business accounts too. office.com works for me too, but I wouldn't say that's an indication of usability. I believe the AAD login flow fix by MSFT is still needed.</p></div><div id="a6993396_689878"><p>Severity: -- → S3</p><p>Flags: <span>needinfo?(jschanck)</span></p><p>Priority: -- → P5</p></div><div id="c9"><p>Is this still an issue, Rahul?</p><div><p>Flags: needinfo?(sergeantsagara)</p></div></div><div id="c10">

  <div id="ct-10" data-comment-id="16691323" data-ismarkdown="true"><p>Hi John,</p>
<p>Unfortunately, it is. Sharing a screenshot to demonstrate. Our company is working internally to push Microsoft to resolve this (it's not an issue with Firefox's implementation. This can be demonstrated by spoofing the useragent as a Chromium-based browser and attempting the same login flow or just using webauthn.io for validation testing). We unfortunately are not having much luck on our end with our support requests. If possible though, I would like to leave this issue open here for both Firefox users and Microsoft's reference.</p>
<p>Thanks,</p>
<p>Rahul Rameshbabu</p>
</div><div><p>Flags: <span>needinfo?(sergeantsagara)</span></p></div></div><div id="c11"><p>I can confirm, still an issue. M$ at its best. :/</p></div>



</div> 
</main> 
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Retinal cells that help stabilize our world view (111 pts)]]></title>
            <link>https://optometry.berkeley.edu/berkeley-scientists-discover-retinal-cells-that-help-stabilize-our-world-view/</link>
            <guid>38501878</guid>
            <pubDate>Sat, 02 Dec 2023 20:44:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://optometry.berkeley.edu/berkeley-scientists-discover-retinal-cells-that-help-stabilize-our-world-view/">https://optometry.berkeley.edu/berkeley-scientists-discover-retinal-cells-that-help-stabilize-our-world-view/</a>, See on <a href="https://news.ycombinator.com/item?id=38501878">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
  <div>
      <figure>
        <img width="1010" height="540" src="https://opt-cdn.berkeley.edu/app/uploads/2023/11/Human_RGCs_press-copy_Puthussery.png" alt="Human Retina" decoding="async" srcset="https://opt-cdn.berkeley.edu/app/uploads/2023/11/Human_RGCs_press-copy_Puthussery.png 1010w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Human_RGCs_press-copy_Puthussery-300x160.png 300w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Human_RGCs_press-copy_Puthussery-768x411.png 768w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Human_RGCs_press-copy_Puthussery-290x155.png 290w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Human_RGCs_press-copy_Puthussery-400x214.png 400w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Human_RGCs_press-copy_Puthussery-684x366.png 684w" sizes="(max-width: 1010px) 100vw, 1010px">                </figure>
    </div>
 
  
<p>The discovery will enable researchers to better understand eye movement disorders that cause significant visual impairment.</p>

<p><em>Article by <a href="https://vision.berkeley.edu/people/emily-ward/">Emily L Ward</a></em></p>

<p><span>H</span>erbert Wertheim School of Optometry &amp; Vision Science researchers have discovered rare neurons in the eye that are crucial for  our visual system to maintain a sharp, steady image of the world. These findings will impact our understanding of the human retina and likely provide insights into the pathology of eye movement disorders.</p>
<p>The study, recently published in <em>Nature</em>, was led by Teresa Puthussery, OD, PhD, an assistant professor at the Herbert Wertheim School of Optometry &amp; Vision Science and the <a href="https://neuroscience.berkeley.edu/">Helen Wills Neuroscience Institute</a>. First author, Anna Yao Mei Wang, PhD, is a postdoctoral scholar in <a href="https://www.retinalab.berkeley.edu/">The Puthussery Lab</a>. </p>

<h2>Gaze Stabilization in a Moving World</h2>
<p>The neurons identified are involved in a fundamental feature of everyday vision. As one walks down a busy street or looks out the window of a train, the gaze stabilization system operates below our conscious awareness causing the eyes to reflexively follow the direction in which the visual scene is moving. This visual mechanism works in concert with the vestibular system to maintain a sharp image of a moving world. Clinical conditions that interfere with gaze stabilization can therefore lead to significant visual impairment.</p>
<p>The new findings demonstrate for the first time that retinal neurons underlying gaze stabilization in other mammals are also present in primates, including humans. Neurons that send visual signals from the eye to the brain are called retinal ganglion cells. In humans, there are around 20 different retinal ganglion cell types, each of which responds to specific features of the visual scene, such as form, color, and motion (1–3).</p>
<p><img decoding="async" fetchpriority="high" src="https://opt-cdn.berkeley.edu/app/uploads/2023/11/Teresa-Puthuserry_Square-300x300.jpg" alt="Teresa Puthussery" width="400" height="400" srcset="https://opt-cdn.berkeley.edu/app/uploads/2023/11/Teresa-Puthuserry_Square-300x300.jpg 300w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Teresa-Puthuserry_Square-1024x1024.jpg 1024w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Teresa-Puthuserry_Square-150x150.jpg 150w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Teresa-Puthuserry_Square-768x768.jpg 768w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Teresa-Puthuserry_Square-1536x1536.jpg 1536w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Teresa-Puthuserry_Square-2048x2048.jpg 2048w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Teresa-Puthuserry_Square-1300x1300.jpg 1300w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Teresa-Puthuserry_Square-290x290.jpg 290w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Teresa-Puthuserry_Square-400x400.jpg 400w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Teresa-Puthuserry_Square-684x684.jpg 684w" sizes="(max-width: 400px) 100vw, 400px"></p>
<p>The researchers discovered a highly-specialized retinal ganglion cell type known as direction-selective ganglion cells (DSGCs). They respond to motion in the visual field by increasing their activity when movement occurs in their “preferred” direction, while showing little activity to motion in the opposite direction. Collectively, responses from these neurons tell the gaze stabilization system which way the visual scene is moving.</p>
<p>“This cell type in particular—the direction-selective ganglion cell—had not been discovered previously in primate despite concerted effort, leading the field to conclude it must not be there,” said Marla Feller, PhD, a distinguished professor at UC Berkeley and an elected member of the National Academy of Sciences. Dr. Feller is an expert in retinal circuit development and function. </p>

<h2>Finding the Needle in the Haystack</h2>
<p>DSGCs were discovered in the rabbit retina in 1964 by another Berkeley Optometry faculty member, <a href="https://optometry.berkeley.edu/alumni/hall-of-fame/horace-b-barlow/">Horace Barlow</a>, and his colleagues (4). However, in the decades since, the lack of evidence for DSGCs in higher species led scientists to speculate that primate direction selectivity was computed in the brain. But when new evidence emerged suggesting that some human gaze stabilization disorders could be linked to abnormal activity of DSGCs (5), Puthussery’s lab renewed their efforts to find them. “That was a tipping point. We thought DSGCs had to be there, but that they made up a very low percentage of retinal ganglion cells. Our challenge was to work out how to find the needle in the haystack,” said Dr. Puthussery.</p>
<p>The researchers used a multi-pronged approach to overcome this problem. First, they leveraged data from state-of-the-art genetic tools (2) to track down retinal neurons with molecular features resembling DSGCs in other animals. The researchers then labeled these neurons with fluorescent markers to show that they had the expected anatomical features. Finally, the team built a customized imaging system to track the activity of hundreds of retinal ganglion cells and show that the fluorescently tagged cells responded selectively to images moving in specific directions. This combination of molecular, anatomical, and functional evidence provided unequivocal identification of the long sought-after DSGCs.</p>
<p>“The Puthussery Lab was successful where others failed because of their novel approach,” said Dr. Marla Feller. She continued, “I also cannot overstate the high quality of the data, which is critical for such a breakthrough finding.”</p>

<h2>New Insights into Common Visual Disorders</h2>
<p><img decoding="async" src="https://opt-cdn.berkeley.edu/app/uploads/2023/11/Anna-Wang-300x300.jpg" alt="Anna Wang" width="400" height="400" srcset="https://opt-cdn.berkeley.edu/app/uploads/2023/11/Anna-Wang-300x300.jpg 300w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Anna-Wang-1024x1024.jpg 1024w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Anna-Wang-150x150.jpg 150w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Anna-Wang-768x768.jpg 768w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Anna-Wang-1536x1536.jpg 1536w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Anna-Wang-2048x2048.jpg 2048w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Anna-Wang-1300x1300.jpg 1300w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Anna-Wang-290x290.jpg 290w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Anna-Wang-400x400.jpg 400w, https://opt-cdn.berkeley.edu/app/uploads/2023/11/Anna-Wang-684x684.jpg 684w" sizes="(max-width: 400px) 100vw, 400px"></p>
<p>These findings will enable researchers to better understand how retinal mechanisms contribute to gaze stabilization in the normal visual system and in disorders that cause unstable gaze. For example, nystagmus is a repetitive, uncontrolled movement of the eyes that can lead to unsteady and blurry vision. Nystagmus can occur in isolation or can accompany other eye problems such as albinism and certain inherited retinal diseases. While many forms of nystagmus are caused by problems in the brain or inner ear, the results of this study suggest that some forms of nystagmus could originate from abnormal activity of DSGCs in the retina (5).</p>

<h2>Looking Forward: Testing for Blinding Diseases</h2>
<p>Overall, these results provide a vivid demonstration that a rare retinal ganglion cell type may nonetheless have a profound impact on our overall visual experience. The approach used in this study can now be applied to determine the roles of other human ganglion cell types whose functions are unknown. This will be an important step toward designing more sensitive tests for the detection of blinding diseases that cause ganglion cell degeneration such as glaucoma, which afflicts 80 million people worldwide and is the leading cause of irreversible blindness (7,8). For example, if direction-selective ganglion cells are damaged early in glaucoma, changes in eye movements might serve as an objective biomarker for early damage.</p>
<p>Remarkably, half of all individuals with glaucoma are unaware that they have it (7,8). Ultimately, detecting early changes in ganglion cell activity is vital to diagnosing disease and preventing vision loss in our aging population.</p>

<h2>About the Study</h2>
<p>The research was supported by the National Eye Institute (EY024265), Glaucoma Research Foundation (Shaffer Grant) and the Hellman Fellows Fund.</p>
<p>Wang, A.Y.M., Kulkarni, M.M., McLaughlin, A.J., Gayet J, Smith BE, Hauptschein M, McHugh CF, Yao YY, Puthussery T. An ON-type direction-selective ganglion cell in primate retina. Nature (2023).</p>
<p> <a href="https://doi.org/10.1038/s41586-023-06659-4">Read in Nature </a></p>
<h2>Related Information</h2>
<p> <a href="https://www.retinalab.berkeley.edu/">The Puthussery Lab </a></p>
<p><a href="https://vision.berkeley.edu/people/emily-ward/">Emily L Ward</a>, who wrote this web article, is a PhD student at UC Berkeley’s Herbert Wertheim School of Optometry &amp; Vision Science.</p>

  <div>
          <h2>About the Photos</h2>
        <p><strong>Top:</strong> A human retina labeled with a marker for all retinal ganglion cells in magenta. The sparse subset of retinal ganglion cells involved in gaze stabilization are labeled with a selective marker in green. <strong>Center:</strong> Teresa Puthussery, OD, PhD, heads the research group, which studies how retinal neurons process visual information before sending signals to the brain. <strong>Bottom:</strong> Anna Yao Mei Wang, PhD, is a postdoctoral scholar working in the Puthussery lab, and is first author of the study, which was published in <em>Nature</em>. Center and bottom photos by Elena Zhukova    </p>
  </div>
  

      <div id="accordion656c5fb7649ce" aria-labelledby="headingaccordion656c5fb7649e3"><p>
              1. Yan W, Peng YR, van Zyl T, et al. Cell Atlas of The Human Fovea and Peripheral Retina. Scientific Reports. 2020;10(1):9802. </p>
<p>2. Peng YR, Shekhar K, Yan W, et al. Molecular Classification and Comparative Taxonomics of Foveal and Peripheral Cells in Primate Retina. Cell. 2019;176(5):1222-1237.e22.</p>
<p>3. Wensel TG. Chapter 51 - Molecular Biology of Vision. Editor(s): Brady ST, Siegel GJ, Albers RW, Price DL. Basic Neurochemistry (Eighth Edition). Academic Press. 2012:889–903.</p>
<p>4. Barlow, HB, Hill, RM, Levick, WR. Retinal ganglion cells responding selectively to direction and speed of image motion in the rabbit. The Journal of Physiology. 1964;173.</p>
<p>5. Kamermans M, Winkelman BHJ, Hölzel MB, Howlett MHC, Kamermans W, Simonsz HJ, de Zeeuw CI. A retinal origin of nystagmus—a perspective. Frontiers in Ophthalmology. 2023;3.</p>
<p>6. Yonehara K, Fiscella M, Drinnenberg A, Esposti F, Trenholm S, Krol J, et al. Congenital Nystagmus Gene FRMD7 Is Necessary for Establishing a Neuronal Circuit Asymmetry for Direction Selectivity. Neuron. 2016;89:177–193.</p>
<p>7. Heijl A, Bengtsson B, Oskarsdottir SE. Prevalence and severity of undetected manifest glaucoma: results from the early manifest glaucoma trial screening. Ophthalmology. 2013;120(8):1541–1545. </p>
<p>8. “Glaucoma Worldwide: A Growing Concern.” Glaucoma.Org. https://glaucoma.org/glaucoma-worldwide-a-growing-concern/. Accessed October 25, 2023. Reviewed March 23, 2022.            </p></div>
  
  

<h2>Contacts</h2>
<p>Eric Craypo, Chief Communications Officer<br>
<a href="mailto:ecraypo@berkeley.edu"> ecraypo@berkeley.edu</a>
</p><p>Teresa Puthussery, Assistant Professor<br>
<a href="mailto:tputhussery@berkeley.edu"> tputhussery@berkeley.edu </a>
      </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Meta patches Linux at hyperscale (127 pts)]]></title>
            <link>https://thenewstack.io/how-meta-patches-linux-at-hyperscale/</link>
            <guid>38501779</guid>
            <pubDate>Sat, 02 Dec 2023 20:32:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thenewstack.io/how-meta-patches-linux-at-hyperscale/">https://thenewstack.io/how-meta-patches-linux-at-hyperscale/</a>, See on <a href="https://news.ycombinator.com/item?id=38501779">Hacker News</a></p>
Couldn't get https://thenewstack.io/how-meta-patches-linux-at-hyperscale/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Multifaceted: The linguistic echo chambers of LLMs (116 pts)]]></title>
            <link>https://blog.j11y.io/2023-11-22_multifaceted/</link>
            <guid>38501589</guid>
            <pubDate>Sat, 02 Dec 2023 20:10:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.j11y.io/2023-11-22_multifaceted/">https://blog.j11y.io/2023-11-22_multifaceted/</a>, See on <a href="https://news.ycombinator.com/item?id=38501589">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>This is a fun one.&nbsp;</p>
<p>I’ve spent more time than I’d care to admit staring at LLM output. And there’s something that I’ve noticed: LLM-generated prose has a kind of… vibe. It’s difficult to describe, but in this initial era of LLMs, it tends to be fairly obvious when you’re reading an AI-generated piece of prose.</p>
<p>One giveaway I've noticed is this particular turn of phrase:</p>
<blockquote>
<p>“Culture is a <u>complex and multifaceted</u> ...” <br>
“Intelligence is <u>complex and multifaceted</u> ...” <br>
“Technology is a <u>complex and multifaceted</u> ...”</p>
</blockquote>
<p>In the <a href="https://www.britannica.com/topic/meme">true Dawkinsian sense</a>, the phrase <em>'complex and multifaceted'</em> has become a <strong>meme</strong>. I've seen it again and again in outputs from GPT, but to double-check, I did a bunch of GPT-3.5 generations (<a href="https://gist.githubusercontent.com/padolsey/89469513af62b70301a2540bbf5fef7b/raw/9f479453116fcd0336377050eaa9c5b6e040602e/gen_gpt_complex_and_test.py">code here</a>). Here's what I found when generating completions for a prompt of <code>'complex and ...'</code>:</p>
<p><img src="https://blog.j11y.io/post_imgs/multifaceted/gpt_gens.png" alt="x"></p>
<p>There is a bizarre prevalence of the term <em>'multifaceted'</em> specifically. Why?</p>
<p>I wanted to understand whether this phrase and the specific word <em>'multifaceted'</em> was newly popular or had existed for a while. As a first port of call, I had a look at Google Trends. And I observed a very shocking increase within the last year:</p>
<p><img src="https://blog.j11y.io/post_imgs/multifaceted/goog.png" alt="google trends graph showing sharp climb in the last year for the word 'multifaceted'"></p>
<p>At this point I wanted to get an indication of whether this was an online-only trend. It's hard to establish this but I thought I'd try Google Books' N-gram viewer. Maybe it would show me. And, as suspected, we see <a href="https://books.google.com/ngrams/graph?content=multifaceted&amp;year_start=2004&amp;year_end=2019&amp;corpus=en-2019&amp;smoothing=3">no notable inflection</a>, although one can see there's a gentle increase over time.</p>
<p><em>Tangent</em>: For what it's worth, I find it a bit of a weird phrase. It's a tautology, as <em>'complex'</em> and <em>'multifaceted'</em> are almost synonomous. It reminds me of legal doublets like <em>'null and void'</em> and <em>'cease and desist'</em>. It's a rather nice and affirmatory way of saying something. I guess it sounds clever and informed, which is, after all, the vibe LLMs are going for.</p>
<p>Anyway, I wanted to go a bit further in order to ensure this was actually a newly prevalent phrase online. Google Trends isn't very convincing by itself. So I went digging for other places where linguistic trends over time might be queryable. I discovered that <a href="https://web.archive.org/">web archive</a> helpfully retains various PDFs over the years, ranging from whitepapers to general reference material from accross the web. It allows you to search for specific keywords as well.</p>
<p>I carried out a bunch of searches from 2006 to 2022 As well as the word <em>'multifaceted'</em>. Oh and I was also interested in another viral word I'd spotted: <em>'intricate'</em>. To ensure some level of scientific prudence, I compared these words with other terms as experimental controls.</p>
<p><img src="https://blog.j11y.io/post_imgs/multifaceted/webarchive_keyphrases.png" alt="words like 'multifaceted' and 'intricate' increased drastically inline with LLM popularity, unlike control terms like 'efficacious' and 'symbiotic' which have remained stable"></p>
<p>As we see, from 2021 onwards, just around the time when GPT and other LLMs started to take the world by storm, the prevalence of our word <em>'multifaceted'</em> increased significantly, from being in only 0.05% of PDFs to 0.23%.</p>
<hr>
<p>Now, to zoom out a bit. I discovered the entire phrase, <em>'a complex and multifaceted'</em>, exists in around <a href="https://www.google.com/search?q=%22a+complex+and+multifaceted%22">800,000 places</a> online.</p>
<p>If narrowed down, we see it composed of some particular domains ahead of others:</p>
<pre><code>Quora.com:      48,000
LinkedIn.com:   30,700
Facebook.com:   9,500
Instagram.com:  7,330
Medium.com:     6,250
Reddit.com:     1,370
CourseHero.com: 7,340
jstor.org:      1,320
wikipedia.org:  400
twitter.com:    798
classace.io:    842 (*notably an essay bank*)
chegg.com:      930 (*notably an essay bank*)
</code></pre>
<p>Quora has 5.7% of all occurances online! If it isn't the birthplace of this meme, it is definitely its breeding ground.</p>
<p>N.B. FWIW we can see what proportion Quora <em>~should</em> be taking up, all things being equal. An arbitrary word like "systemic" appears <a href="https://www.google.co.uk/search?q=%22systemic%22">445 million</a> online, yet only <a href="https://www.google.co.uk/search?q=%22systemic%22+site%3Aquora.com">272,000</a> times on Quora. That's 0.06% of all occurrances. So Quora's 5.7% share of our meme-phrase is completely disproportionate. Are we even surprised? Quora does have a reputation for its spam-bots. They are, at this point, mere regurgitation machines:</p>
<p><img src="https://blog.j11y.io/post_imgs/multifaceted/quora_shots.png" alt="Tonnes of the same sentence structure repeated like 'philosophy is a complex and multifaceted concept that encompasses.....'"></p>
<p>I also couldn't ignore the fact that Quora has lately been embedding a ChatGPT widget on almost every page, and this widget's content is pre-generated, static and available for crawling. It is thus liable to being used as additional training material for this and other LLMs.</p>
<p><img src="https://blog.j11y.io/post_imgs/multifaceted/quora_chatgpt.png" alt="Screenshot of ChatGPT widget embedded in a quora page"></p>
<p>ChatGPT specifically seems to absolutely adore the phrase, using it at every opportunity to explain higher level concepts. The most prevalent pattern seems to be <em>'[noun] is a complex and multifaceted [concept|theory|process]'</em>. Some common ones and their relative quantities across Quora:</p>
<ul>
<li>"a complex and multifaceted concept" - <code>4590</code></li>
<li>"a complex and multifaceted issue" - <code>4420</code></li>
<li>"a complex and multifaceted process" - <code>3550</code></li>
<li>"a complex and multifaceted phenomenon" - <code>2230</code></li>
<li>"a complex and multifaceted emotion" - <code>1650</code></li>
<li>"a complex and multifaceted trait" - <code>1560</code></li>
</ul>
<p><em>(these values vary across locales)</em></p>
<p>If we pick one of these and do a general search across the web, once again we observe incredibly sharp increases across time. The phrase <em>'a complex and multifaceted phenomenon'</em> has 74,900 occurances across the web according to Google. However, only 73 prior to 2010. That's a 1000x increase in only 13 years.</p>
<p>You get the idea. ChatGPT has taken this meme and and rolled with it. This silly LLM has assumed the phrase a core part of our language when it was only ever a narrowly used and awkward turn of phrase.</p>
<hr>
<p><strong>What's the conclusion to this absurd rabbit hole? Have we learned anything?</strong></p>
<p>We know that initial versions of GPT were trained quite significantly on Reddit, and it's probably also the case that a small selection of other websites have been used since then to build and bolster additional models. </p>
<p>Focusing the training on any particular website will lead to strong biases. For example, fixating too much on academic material or websites like Quora where bots formulaically re-use certain phrases (this occurred even in the era before LLMs).</p>
<p>Furthermore, since these models have taken off in popularity, and people have then been publishing their outputs back onto the internet. As this occurs, it's likely produced a feedback loop. LLMs are unknowingly training on their own regurgitated outputs. It's unavoidable.</p>
<p>So, by those very tiny initial training decisions, just a handful of engineers have begun a unstoppable chain of incestuous linguistic evolution. It is fascinating how powerful these models are becoming in shifting the nature of language itself.</p>
<hr>
<p>Thank you for reading! I hope it you found it interesting. If you want, you can read <a href="https://blog.j11y.io/">more of my posts here</a> or <a href="https://j11y.io/">find out more about me here</a>.</p>
</div></div>]]></description>
        </item>
    </channel>
</rss>