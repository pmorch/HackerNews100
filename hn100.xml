<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 09 Dec 2025 17:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Should "I asked $AI, and it said" replies be forbidden in HN guidelines? (164 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=46206457</link>
            <guid>46206457</guid>
            <pubDate>Tue, 09 Dec 2025 16:02:37 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=46206457">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><tbody><tr id="46206694"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206694" href="https://news.ycombinator.com/vote?id=46206694&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>While we will never be able to get folks to stop using AI to “help” them shape their replies, it’s super annoying to have folks think that by using AI that they’re doing others a favor.  If I wanted to know what an AI thinks I’ll ask it. I’m here because I want to know what other people think.</p><p>At this point, I make value judgments when folks use AI for their writing, and will continue to do so.</p></div></td></tr></tbody></table></td></tr><tr id="46206849"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46206849" href="https://news.ycombinator.com/vote?id=46206849&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>I strongly agree with this sentiment and I feel the same way.</p><p>The one exception for me though is when non-native English speakers want to participate in an English language discussion. LLMs produce by far the most natural sounding translations nowadays, but they imbue that "AI style" onto their output. I'm not sure what the solution here is because it's great for non-native speakers to be able to participate, but I find myself discarding any POV that was obviously expressed with AI.</p></div></td></tr></tbody></table></td></tr><tr id="46206795"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206795" href="https://news.ycombinator.com/vote?id=46206795&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>As a community I think we should encourage "disclaimers" aka "I asked &lt;AIVENDOR&gt;, and it said...." The information may still be valuable.</p><p>We can't stop AI comments, but we can encourage good behavior/disclosure.  I also think brevity should still be rewarded, AI or not.</p></div></td></tr></tbody></table></td></tr><tr id="46206777"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206777" href="https://news.ycombinator.com/vote?id=46206777&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>Does it need a rule?  These comments already get heavily down-voted.  People who can't take a hint aren't going to read the rules.</p></div></td></tr></tbody></table></td></tr><tr id="46206884"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206884" href="https://news.ycombinator.com/vote?id=46206884&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>There's hardly a standard for a 'quality' contribution to discussion. Many styles, many opinions, many ways to react and support one's statements.</p><p>If anything, it had been quite customary to supply references for some important facts. Thus letting readers to explore further and interpret the facts.</p><p>With AI in the mix the references become even more important, in the view of hallucinations and fact poisoning.</p><p>Otherwise, it's a forum.</p></div></td></tr></tbody></table></td></tr><tr id="46206706"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206706" href="https://news.ycombinator.com/vote?id=46206706&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>I think they should be banned, if there isnt a contribution besides what the llm answered. It's akin to 'I googled this', which is uninteresting.</p></div></td></tr></tbody></table></td></tr><tr id="46206818"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46206818" href="https://news.ycombinator.com/vote?id=46206818&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>I do find it useful in discussions of LLMs themselves. (Gemini did this; Claude did it too but it used to get tripped up like that).</p><p>I do wish people wouldn’t do it when it doesn’t add to the conversation but I would advocate for collective embarrassment over a ham-fisted regex.</p></div></td></tr></tbody></table></td></tr><tr id="46206875"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206875" href="https://news.ycombinator.com/vote?id=46206875&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>For better or worse, that ship has sailed. LLM's are now as omnipresent as websearch.</p><p>Some people will know how to use it in good taste, others will try to abuse it in bad taste.</p><p>It might not be universally agreed which is which in every case.</p></div></td></tr></tbody></table></td></tr><tr id="46206852"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206852" href="https://news.ycombinator.com/vote?id=46206852&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>There be a thing called Thee Undocumented Rules of HN, aka etiquette, in which states - and I quote: "Thou shall not post AI generated replies"</p><p>I can't locate them, but I'm sure they exist...</p></div></td></tr></tbody></table></td></tr><tr id="46206776"><td></td></tr><tr id="46206731"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206731" href="https://news.ycombinator.com/vote?id=46206731&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>What do you think about other low quality sources? For instance, "I checked on infowars.com, and this is what came up"? Should they be banned as well?</p></div></td></tr></tbody></table></td></tr><tr id="46206773"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46206773" href="https://news.ycombinator.com/vote?id=46206773&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>It depends on if you're saying "Infowars has the answer, check out this article" vs "I know this isn't a reputable source, however it's a popular source and there's an interesting debate to be had about Infowars' perspective, even if we can agree it's incorrect."</p></div></td></tr></tbody></table></td></tr><tr id="46206763"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206763" href="https://news.ycombinator.com/vote?id=46206763&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>To me, the valuable comments are the ones that share the writer's expertise and experiences (as opposed to opinions and hypothesizing) or the ones that ask interesting questions. LLMs have no experience and no real expertise, and nobody seems to be posting "I asked an LLM for questions and it said...". Thus, LLM-written comments (whether of the form "I asked ChatGPT..." or not) have no value to me.</p><p>I'm not sure a full ban is possible, but LLM-written comments should at least be strongly discouraged.</p></div></td></tr></tbody></table></td></tr><tr id="46206825"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206825" href="https://news.ycombinator.com/vote?id=46206825&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>I think disclosing the use the AI is better than hiding it. The alternative is people using it but not telling for fear of a ban.</p></div></td></tr></tbody></table></td></tr><tr id="46206685"><td></td></tr><tr id="46206853"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46206853" href="https://news.ycombinator.com/vote?id=46206853&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>I guess... That is the point in my opinion.</p><p>If you just say, "here is what llm said" if that turns out to be nonsense you can say something like, "I was just passing along the llm response, not my own opinion"</p><p>But if you take the llm response and present it as your own, at least there is slightly more ownership over the opinion.</p><p>This is kind of splitting hairs but hopefully it makes people actually read the response themselves before posting it.</p></div></td></tr></tbody></table></td></tr><tr id="46206767"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46206767" href="https://news.ycombinator.com/vote?id=46206767&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>Agreed - in fact these folks are going out of their way to be transparent about it. It's much easier to just take credit for a "smart" answer</p></div></td></tr></tbody></table></td></tr><tr id="46206878"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206878" href="https://news.ycombinator.com/vote?id=46206878&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>I think comments like this should link their generation rather than C+P it. Not sure if this should be a rule or we can just let downvoting do the work - I worry that a rule would be overapplied and I think there are contexts that are okay.</p></div></td></tr></tbody></table></td></tr><tr id="46206829"><td></td></tr><tr id="46206807"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206807" href="https://news.ycombinator.com/vote?id=46206807&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>Yes. Unless something useful is actually added by the commenter or the post is about, "I asked llm x and it said y (that was unexpected)".</p><p>I have a coworker who does this somewhat often and... I always just feel like saying well that is great but what do you think? What is your opinion?</p><p>At the very least the copy paster should read what the llm says, interpret it, fact check it, then write their own response.</p></div></td></tr></tbody></table></td></tr><tr id="46206845"><td></td></tr><tr id="46206848"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46206848" href="https://news.ycombinator.com/vote?id=46206848&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>I have a client who does this — pastes it into text messages! as if it will help me solve the problem they are asking me to solve — and I'm like "that's great I won't be reading it". You have to push back.</p></div></td></tr></tbody></table></td></tr><tr id="46206828"><td></td></tr><tr id="46206756"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206756" href="https://news.ycombinator.com/vote?id=46206756&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>Banning the disclosure of it is still an improvement. It forces the poster to take responsibility for what they have written, as now it is in their name.</p></div></td></tr></tbody></table></td></tr><tr id="46206872"><td></td></tr><tr id="46206854"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206854" href="https://news.ycombinator.com/vote?id=46206854&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>I don't think people should post the unfiltered output of an LLM as if it has value. If a question in a comment has a single correct answer that is so easily discoverable, I might downvote the comment instead.</p><p>I'm not sure making a rule would be helpful though, as I think people would ignore it and just not label the source of their comment. I'd like to be wrong about that.</p></div></td></tr></tbody></table></td></tr><tr id="46206677"><td></td></tr><tr id="46206787"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206787" href="https://news.ycombinator.com/vote?id=46206787&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>I endorse this. Please do take whatever measures are possible to discourage it, <i>even if it won't stop people</i>. It at least sends a message: this is not wanted, this is not helpful, this is not constructive.</p></div></td></tr></tbody></table></td></tr><tr id="46206834"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206834" href="https://news.ycombinator.com/vote?id=46206834&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>What is annoying about them is that they tend to be long with a low signal/noise ratio.    I'd be fine with a comment saying.   "I think the ChatGPT answer is informative: [link]".   It'd still likely get downvoted to the bottom of the discussion, where it likely belongs.</p></div></td></tr></tbody></table></td></tr><tr id="46206844"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206844" href="https://news.ycombinator.com/vote?id=46206844&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>If it’s part of an otherwise coherent post making a larger point I have no issue with it.</p><p>If it’s a low effort copy pasta post I think downvotes are sufficient unless it starts to obliterate the signal vs noise ratio on the site.</p></div></td></tr></tbody></table></td></tr><tr id="46206838"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206838" href="https://news.ycombinator.com/vote?id=46206838&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>I don’t see how it is much different than using Wikipedia.  They are usually about the same answer and at least in Gemini it is usually a correct answer now.</p></div></td></tr></tbody></table></td></tr><tr id="46206830"><td></td></tr><tr id="46206789"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206789" href="https://news.ycombinator.com/vote?id=46206789&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>Maybe I remember the Grok ones more clearly but it felt like “I asked Grok” was more prevalent than the others.</p><p>I feel like the HN guidelines could take inspiration from how Oxide uses LLMs. (<a href="https://rfd.shared.oxide.computer/rfd/0576" rel="nofollow">https://rfd.shared.oxide.computer/rfd/0576</a>). Specifically the part where using LLMs to write comments violates the implicit social contract that the writer should put more care and effort and time into it than the reader. The reader reads it because they assume this is something a person has put more time into than they need to. LLMs break that social contract.</p><p>Of course, if it’s banned maybe people just stop admitting it.</p></div></td></tr></tbody></table></td></tr><tr id="46206783"><td></td></tr><tr id="46206702"><td></td></tr><tr id="46206837"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46206837" href="https://news.ycombinator.com/vote?id=46206837&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>I like when someone has some problems and questions, goes after them, and shares the results. I just don't like the "I asked grok" part.</p><p>So I support banning it, and people being more sneaky about it.</p></div></td></tr></tbody></table></td></tr><tr id="46206785"><td></td></tr><tr id="46206813"><td></td></tr><tr id="46206671"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206671" href="https://news.ycombinator.com/vote?id=46206671&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>I find such replies to be worthless wastes of space on par with "let me google that for you" replies. If I want to know what genAI has to say about something, I can just ask it myself. I'm more interested in what the commenter has to say.</p><p>But I don't know that we need any sort of official ban against them. This community is pretty good about downvoting unhelpful comments, and there is a whole spectrum of unhelpful comments that have nothing to do with genAI. It seems impractical to overtly list them all.</p></div></td></tr></tbody></table></td></tr><tr id="46206696"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46206696" href="https://news.ycombinator.com/vote?id=46206696&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>There is friction to asking AI yourself. And a comment typically means that "I found the AI answer insightful enough to share".</p></div></td></tr></tbody></table></td></tr><tr id="46206804"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_46206804" href="https://news.ycombinator.com/vote?id=46206804&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>Unfortunately it's easier to train an AI to be convincing than to be correct, so it can look insightful before it's true.</p><p>Like horoscopes, only they're not actually that bad so roll a D20 and on a set of numbers known only to the DM (and varying with domain and task length) you get a textbook answer and on the rest you get convincing nonsense.</p></div></td></tr></tbody></table></td></tr><tr id="46206774"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_46206774" href="https://news.ycombinator.com/vote?id=46206774&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>The problem is that the AI answer could just be wrong, and there’s another step required to validate what it spit out. Sharing the conversation without fact checking it just adds noise.</p></div></td></tr></tbody></table></td></tr><tr id="46206836"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206836" href="https://news.ycombinator.com/vote?id=46206836&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>Honestly I judge people pretty harshly. I ask people a question in honest good faith. If they’re trying to help me out and genuinely care and use AI fine.</p><p>But most of the time it’s like they were bothered that I asked and copy paste what an AI said.</p><p>Pretty easy. Just add their name to my “GFY” list and move on in my life.</p></div></td></tr></tbody></table></td></tr><tr id="46206711"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206711" href="https://news.ycombinator.com/vote?id=46206711&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>You can add the guideline, but then people would skip the "I asked" part and post the answer straight away. Apart from the obvious LLMesque structure of most of those bot answers, how could you tell if one has crafted the answer so much that it looks like a genuine human answer?</p><p>Obligatory xkcd <a href="https://xkcd.com/810/" rel="nofollow">https://xkcd.com/810/</a></p></div></td></tr></tbody></table></td></tr><tr id="46206733"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206733" href="https://news.ycombinator.com/vote?id=46206733&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>Depends on the context.</p><p>I find myself downvoting (flagging) them when I see them as submissions, and I can't think of any examples where they were good submission content; but for comments? There's enough discussion where the AI is the subject itself and therefore it's genuinely relevant what the AI says.</p><p>Then there's stuff like this, which I'd not seen myself before seeing your question, but I'd say asking people here if an AI-generated TLDR of 74 (75?) page PDF is correct, is a perfectly valid and sensible use: <a href="https://news.ycombinator.com/item?id=46164360">https://news.ycombinator.com/item?id=46164360</a></p></div></td></tr></tbody></table></td></tr><tr id="46206809"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206809" href="https://news.ycombinator.com/vote?id=46206809&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>This is what DeepSeek said:</p><p>&gt; 1. Existing guidelines already handle low-value content. If an AI reply is shallow or off-topic, it gets downvoted or flagged.
&gt; 
&gt; 2. Transparency is good. Explicitly citing an AI is better than users passing off its output as their own, which a ban might encourage.
&gt; 
&gt; 3. The community can self-regulate. We don't need a new rule for every type of low-effort content.
&gt; 
&gt; The issue is low effort, not the tool used. Let downvotes handle it.</p></div></td></tr></tbody></table></td></tr><tr id="46206784"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46206784" href="https://news.ycombinator.com/vote?id=46206784&amp;how=up&amp;goto=item%3Fid%3D46206457"></a></center></td><td><br>
<div><p>I don't think they should be banned, I think they should be encouraged: I'm always appreciative when people who can't think for themselves openly identify themselves so that it costs me less effort to spot them.</p></div></td></tr></tbody></table></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New Pebble Device (189 pts)]]></title>
            <link>https://repebble.com/blog/meet-pebble-index-01-external-memory-for-your-brain</link>
            <guid>46205661</guid>
            <pubDate>Tue, 09 Dec 2025 15:03:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://repebble.com/blog/meet-pebble-index-01-external-memory-for-your-brain">https://repebble.com/blog/meet-pebble-index-01-external-memory-for-your-brain</a>, See on <a href="https://news.ycombinator.com/item?id=46205661">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span><img src="https://repebble.com/assets/meet-pebble-index-01-external-memory-for-your-brain-0-blackhero.jpg" alt="" loading="lazy"></span></p>
<p><strong>Catch your best ideas before they slip through your fingers</strong></p>
<p>Do you ever have flashes of insight or an idea worth remembering? This happens to me 5-10 times every day. If I don’t write down the thought immediately, it slips out of my mind. Worst of all, I <em>remember</em> that I’ve forgotten something and spend the next 10 minutes trying to remember what it is. So I invented external memory for my brain.</p>
<p><strong>Introducing</strong> <strong>Pebble Index 01</strong> - a small ring with a button and microphone. Hold the button, whisper your thought, and it’s sent to your phone. It’s added to your notes, set as a reminder, or saved for later review.</p>
<p>Index 01 is designed to become muscle memory, since it’s always with you. It’s private by design (no recording until you press the button) and requires no internet connection or paid subscription. It’s as small as a wedding band and comes in 3 colours. It’s made from durable stainless steel and is water-resistant. Like all Pebble products, it’s extremely customizable and built with open source software.</p>
<p>Here’s the best part: the battery lasts for years. You never need to charge it.</p>
<p><a href="https://repebble.com/index#buy" target="_blank" rel="noopener noreferrer">Pre-order today for $75</a>. After worldwide shipping begins in March 2026, the price will go up to $99.</p>
<p><span><p><iframe src="https://www.youtube.com/embed/ArxhS4SQaP0?vq=hd1080" width="1920" height="1080" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p></span></p>
<h3 id="design"><a href="#design"><span>#</span>Design</a></h3>
<p>Now that I’ve worn my Index 01 for several months, I can safely say that it has changed my life - just like with Pebble, I couldn’t go back to a world without this. There are so many situations each day where my hands are full (while biking or driving, washing dishes, wrangling my kids, etc) and I need to remember something. A random sampling of my recent recordings:</p>
<ul>
<li><em>Set a timer for 3pm to go pick up the kids</em></li>
<li><em>Remind me to phone the pharmacy at 11am</em></li>
<li><em>Peter is coming by tomorrow at 11:30am, add that to my calendar</em></li>
<li><em>Jerry recommends reading Breakneck</em></li>
<li><em>Mark wants a Black/Red PT2</em></li>
</ul>
<p>Before, I would take my phone out of my pocket to jot these down, but I couldn’t always do that (eg, while bicycling). I also wanted to start using my phone less, especially in front of my kids.</p>
<p><span><img src="https://repebble.com/assets/meet-pebble-index-01-external-memory-for-your-brain-1-2c146dc6-62c0-4079-8f46-56413e3f5ae9.png" alt="" loading="lazy"></span></p>
<p>Initially, we experimented by building this as an app on Pebble, since it has a mic and I’m always wearing one. But, I realized quickly that this was suboptimal - it required me to use my other hand to press the button to start recording (lift-to-wake gestures and wake-words are too unreliable). This was tough to use while bicycling or carrying stuff.</p>
<p>Then a genius electrical engineer friend of mine came up with an idea to fit everything into a tiny ring. It is the perfect form factor! Honestly, I’m still amazed that it all fits.</p>
<p>The design needed to satisfy several critical conditions:</p>
<ol>
<li>Must work reliably 100% of the time. If it didn’t work or failed to record a thought, I knew I would take it off and revert back to my old habit of just forgetting things.</li>
<li>It had to have a physical press-button, with a satisfying click-feel. I want to know for sure if the button is pressed and my thought is captured.</li>
<li>Long battery life - every time you take something off to charge, there’s a chance you’ll forget to put it back on.</li>
<li>Must be privacy-preserving. These are your inner thoughts. All recordings must be processed and stored on your phone. Only record when the button is pressed.</li>
<li>It had to be as small as a wedding band. Since it’s worn on the index finger, if it were too large or bulky, it would hit your phone while you held it in your hand.</li>
<li>Water resistance - must be able to wash hands, shower, and get wet.</li>
</ol>
<p>We’ve been working on this for a while, testing new versions and making tweaks. We’re really excited to get this out into the world.</p>
<p>Here are a few of my favourite things about Index 01:</p>
<ul>
<li>It does one thing really well - it helps me remember things.</li>
<li>It’s discreet. It's not distracting.  It doesn't take you out of the moment.</li>
<li>There’s no AI friend persona and it’s not always recording.</li>
<li>It’s inexpensive. We hope you try it and see if you like it as well!</li>
</ul>
<p><span><img src="https://repebble.com/assets/meet-pebble-index-01-external-memory-for-your-brain-2-grocery-bag-silver-small.jpeg" alt="" loading="lazy"></span></p>
<p><span><img src="https://repebble.com/assets/meet-pebble-index-01-external-memory-for-your-brain-3-995889e9-cbc4-4225-985a-17ec4aa5a9ee.png" alt="" loading="lazy"></span></p>
<h3 id="key-details"><a href="#key-details"><span>#</span><strong>Key Details</strong></a></h3>
<ul>
<li><strong>Available in 3 colours and 8 sizes</strong>
<ul>
<li>Colours: polished silver, polished gold, and matte black</li>
<li>US ring sizes: 6, 7, 8, 9, 10, 11, 12, 13</li>
<li>You can pre-order now and pick your size/colour later before your ring ships.</li>
</ul>
</li>
<li><strong>Cost and availability:</strong> Pre-order price is $75, rises to $99 later. Ships worldwide, beginning in March.</li>
<li><strong>Works with iPhone and Android</strong>: We overcame <a href="https://ericmigi.com/blog/apple-restricts-pebble-from-being-awesome-with-iphones" target="_blank" rel="noopener noreferrer">Apple’s best efforts</a> to make life terrible for 3rd party accessory makers and have Index 01 working well on iOS and Android.</li>
<li><strong>Extremely private and secure</strong>: Your thoughts are processed by open source speech-to-text (STT) and AI models locally on your phone. You can read the code and see exactly how it works - our <a href="https://github.com/coredevices/mobileapp" target="_blank" rel="noopener noreferrer">Pebble mobile app</a> is open source. Higher-quality STT is available through an optional cloud service.</li>
<li><strong>No charging:</strong> The battery lasts for up to years of average use. After the end of its life, send your ring back to us for recycling.</li>
<li><strong>On-ring storage:</strong> Recording works even if your phone is out of range. Up to 5 minutes of audio can be stored on-ring, then synced later.</li>
<li><strong>No speaker or vibrating motor:</strong> This is an input device only. There is an RGB LED, but it’s rarely used (to save battery life and to reduce distraction).</li>
<li><strong>Works great with Pebble</strong> or other smartwatches: After recording, the thought will appear on your watch, and you can check that it’s correct. You can ask questions like ‘What’s the weather today?’ and see the answer on your watch.</li>
<li><strong>Raw audio playback</strong>: Very helpful if STT doesn’t work perfectly due to wind or loud background noises.</li>
<li><strong>Actions: W</strong>hile the primary task is remembering things for you, you can also ask it to do things like ’Send a Beeper message to my wife - running late’ or answer simple questions that could be answered by searching the web. You can configure button clicks to control your music - I love using this to play/pause or skip tracks. You can also configure where to save your notes and reminders (I have it set to add to Notion).</li>
<li><strong>Customizable and hackable:</strong> Configure single/double button clicks to control whatever you want (take a photo, turn on lights, Tasker, etc). Add your own voice actions via MCP. Or route the audio recordings directly to your own app or server!</li>
<li><strong>99+ languages:</strong> Speech to text and local LLM support over 99 languages! Naturally, the quality of each may vary.</li>
</ul>
<h3 id="future-plans"><a href="#future-plans"><span>#</span>Future Plans</a></h3>
<p>Let me be very clear - Index 01 is designed at its core to be a device that helps you remember things. We want it to be 100% reliable at its primary task. But we’re leaving the side door open for folks to customize, build new interactions and actions.</p>
<p>Here’s how I’m thinking about it - a single click-hold + voice input will be routed to the primary memory processing path. Double-click-hold + voice input would be routed to a more general purpose voice agent (think ChatGPT with web search). Responses from the agent would be presented on Pebble (eg ‘What’s the weather tomorrow?’, ‘When’s the next northbound Caltrain?’) or other smartwatches (as a notification). Maybe this could even be an input for something like ChatGPT Voice Mode, enabling you to hear the AI response from your earbuds.</p>
<p>The built in actions, set reminder, create note, alarms, etc, are actually MCPs - basically mini apps that AI agents know how to operate. They run locally in WASM within the Pebble mobile app (no cloud MCP server required). Basically any MCP server can be used with the system, so intrepid folks may have fun adding various actions like Beeper, Google Calendar, weather, etc that already offer MCPs.</p>
<p>Not everything will be available at launch, but this is the direction we are working towards. There will be 3 ways to customize your Index 01:</p>
<ol>
<li>Trigger actions via button clicks - configure a single or double click to do things like take a photo, control your Home Assistant smart home, Tasker function, unlock your car. This will work better on Android since iOS Shortcuts doesn’t have an open API.</li>
<li>Trigger actions via voice input - write an MCP to do….basically anything? This is pretty open ended.</li>
<li>Route your voice recordings and/or transcriptions to your own webhook - or skip our AI processing entirely and send every recording to your own app or webapp.</li>
</ol>
<p>### FAQ</p>
<p><strong>How does it work?</strong></p>
<p>People usually wear it on the index finger. Inside the ring is a button, a microphone, a Bluetooth chip, memory, and a battery that lasts for years. Click the button with your thumb, talk into the mic, and it records to internal memory. When your phone is in range, the recording is streamed to the Pebble app. It’s converted to text on-device, then processed by an on-device large language model (LLM) which selects an action to take (create note, add to reminders, etc).</p>
<p><strong>When do I pick my size?</strong></p>
<p>You’ll be able to pick your ring size and color after placing a pre-order. If you have a 3D printer, you can <a href="https://github.com/coredevices/hardware/tree/main/ring/pebble-index-01/3d-cad" target="_blank" rel="noopener noreferrer">print our CAD designs</a> to try on. We’re also planning a sizing kit. You can view the <a href="https://repebble.com/index01-carousel/index01-dimensions.png" target="_blank" rel="noopener noreferrer">measurements of the inner diameter</a> of each ring size.</p>
<p><span><img src="https://repebble.com/assets/meet-pebble-index-01-external-memory-for-your-brain-4-dd455665-61da-4315-b377-e37e90cc57b3.png" alt="" loading="lazy"></span></p>
<p><strong>How long does the battery last?</strong></p>
<p>Roughly 12 to 15 hours of recording. On average, I use it 10-20 times per day to record 3-6 second thoughts. That’s up to 2 years of usage.</p>
<p><strong>Is it secure and private?</strong></p>
<p>Yes, extremely. The connection between ring and phone is encrypted. Recordings are processed locally on your phone in the open-source Pebble app. The app works offline (no internet connection) and does not require a cloud service. An optional cloud storage system for backing up recordings is available. Our plan is for this to be optionally encrypted, but we haven’t built it yet.</p>
<p><strong>Is a paid subscription required?</strong></p>
<p>No.</p>
<p><strong>What kind of battery is inside?</strong></p>
<p>Index 01 uses silver-oxide batteries.</p>
<p><strong>Why can’t it be recharged?</strong></p>
<p>We considered this but decided not to for several reasons:</p>
<ol>
<li>You’d probably lose the charger before the battery runs out!</li>
<li>Adding charge circuitry and including a charger would make the product larger and more expensive.</li>
<li>You send it back to us to recycle.</li>
</ol>
<p><strong>Wait, it’s single use?</strong></p>
<p>Yes. We know this sounds a bit odd, but in this particular circumstance we believe it’s the best solution to the given set of constraints. Other smart rings like Oura cost $250+ and need to be charged every few days. We didn’t want to build a device like that.  Before the battery runs out, the Pebble app notifies and asks if you’d like to order another ring.</p>
<p><strong>Is it always listening?</strong></p>
<p>No.  It only records while the button is pressed. It’s not designed to record your whole life, or meetings.</p>
<p><strong>What if the speech-to-text processing misses a word or something?</strong></p>
<p>You can always listen to the each recording in the app.</p>
<p><strong>Why no touchpad?</strong></p>
<p>We experimented with a touchpad, but found it too easy to accidentally swipe and press. Also, nothing beats the feedback of a real gosh darn pressable button.</p>
<p><strong>Is there a speaker or vibrating motor?</strong></p>
<p>No. The button has a great click-feel to indicate when you are pressing.</p>
<p><strong>Does it do health tracking like Oura?</strong></p>
<p>Nope</p>
<p><strong>How durable and water-resistant is it?</strong></p>
<p>It’s primarily made from stainless steel 316, with a liquid silicone rubber (LSR) button. It’s water-resistant to 1 meter. You can wash your hands, do dishes, and shower with it on, but we don’t recommend swimming with it.</p>
<p><strong>Does it work with iPhone and Android?</strong></p>
<p>Yes</p>
<p><strong>I love customizing and hacking on my devices. What could I do with Index 01?</strong></p>
<p>Lots of stuff! Control things with the buttons. Route raw audio or transcribed text directly to your own app via webhook. Use <a href="https://modelcontextprotocol.io/about" target="_blank" rel="noopener noreferrer">MCPs</a> (also run locally on-device! No cloud server required) to add more actions.</p>
<p><strong>Is this an AI friend thingy or always-recording device?</strong></p>
<p>No.</p>
<p><strong>How far along is development?</strong></p>
<p>We’ve been working on this in the background to watch development. It helps that our Pebble Time 2 partner factory is also building Index 01! We’re currently in the DVT stage, testing pre-production samples. We’ll start a wider alpha test in January with a lot more people. Here’s some shots from the pre-production assembly line:</p>
<p><video controls="" preload="metadata"><source src="https://repebble.com/assets/meet-pebble-index-01-external-memory-for-your-brain-6-ring_glue_machine.mp4" type="video/mp4">Your browser does not support the video tag.</video></p>
<p><span><img src="https://repebble.com/assets/meet-pebble-index-01-external-memory-for-your-brain-5-cf74d4f3b974212601f09fcf5109a989.jpg" alt="" loading="lazy"></span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Gemini Pro 3 hallucinates the HN front page 10 years from now (290 pts)]]></title>
            <link>https://dosaygo-studio.github.io/hn-front-page-2035/news</link>
            <guid>46205632</guid>
            <pubDate>Tue, 09 Dec 2025 15:00:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dosaygo-studio.github.io/hn-front-page-2035/news">https://dosaygo-studio.github.io/hn-front-page-2035/news</a>, See on <a href="https://news.ycombinator.com/item?id=46205632">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <td>
            <table>
                <!-- Item 1 -->
                <tbody><tr id="90100001">
                    <td><span>1.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90100001&amp;how=up&amp;auth=d7f2a1b9"></a></center></td>
                    <td><a href="https://www.spacex.com/updates/2035/starship-hls-9-tranquility-telemetry/">First successful telemetry from Starship HLS-9 on the Sea of Tranquility</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=spacex.com"><span>spacex.com</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>894 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=muskwatch">muskwatch</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90100001">4 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90100001&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90100001">312 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 2 -->
                <tr id="90100045">
                    <td><span>2.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90100045&amp;how=up&amp;auth=e1c8d4f2"></a></center></td>
                    <td><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/log/?h=v7.4-rust">A 100% Rust kernel is now upstream in Linux 7.4</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=kernel.org"><span>kernel.org</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>402 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=rust_evangelist">rust_evangelist</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90100045">6 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90100045&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90100045">156 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 3 -->
                <tr id="90099890">
                    <td><span>3.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90099890&amp;how=up&amp;auth=a2b3c4d5"></a></center></td>
                    <td><a href="https://nostalgic-coder.io/blog/2035/why-i-still-write-raw-code">Why I still write raw code instead of prompting the compiler</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=nostalgic-coder.io"><span>nostalgic-coder.io</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>128 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=oldtimer99">oldtimer99</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099890">3 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90099890&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099890">89 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 4 -->
                <tr id="90100123">
                    <td><span>4.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90100123&amp;how=up&amp;auth=f9e8d7c6"></a></center></td>
                    <td><a href="https://arxiv.org/abs/3512.08842">Running LLaMA-12 7B on a contact lens with WASM</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=arxiv.org"><span>arxiv.org</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>67 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=edge_compute">edge_compute</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90100123">2 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90100123&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90100123">14 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 5 -->
                <tr id="90099999">
                    <td><span>5.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90099999&amp;how=up&amp;auth=b5a6c7d8"></a></center></td>
                    <td><a href="https://algodrill.io/drills/patterns/dynamic-programming-4d">Show HN: AlgoDrill – Interactive drills to stop forgetting LeetCode patterns</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=algodrill.io"><span>algodrill.io</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>243 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=persistence_is_key">persistence_is_key</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099999">5 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90099999&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099999">98 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 6 -->
                <tr id="90098000">
                    <td><span>6.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90098000&amp;how=up&amp;auth=k1l2m3n4"></a></center></td>
                    <td><a href="https://www.nature.com/articles/s41586-035-01928-z-fusion-breakthrough">ITER achieves net positive energy for 20 consecutive minutes</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=nature.com"><span>nature.com</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>1205 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=physics_lover">physics_lover</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098000">12 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90098000&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098000">402 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 7 -->
                <tr id="90100005">
                    <td><span>7.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90100005&amp;how=up&amp;auth=q5w6e7r8"></a></center></td>
                    <td><a href="https://www.ifixit.com/News/98212/framework-2024-retrospective-repair">Restoring a 2024 Framework Laptop: A retrospective</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=ifixit.com"><span>ifixit.com</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>56 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=retro_fix">retro_fix</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90100005">4 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90100005&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90100005">22 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 8 -->
                <tr id="90099555">
                    <td><span>8.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90099555&amp;how=up&amp;auth=t9y8u7i6"></a></center></td>
                    <td><a href="https://killedbygoogle.com/cemetery/gemini-cloud">Google kills Gemini Cloud Services</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=killedbygoogle.com"><span>killedbygoogle.com</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>530 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=dang_fan">dang_fan</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099555">15 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90099555&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099555">330 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 9 -->
                <tr id="90099800">
                    <td><span>9.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90099800&amp;how=up&amp;auth=o1p2a3s4"></a></center></td>
                    <td><a href="https://graphics-shader.net/examples/webgpu2/5d-tesseract">Visualizing the 5th dimension with WebGPU 2.0</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=graphics-shader.net"><span>graphics-shader.net</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>88 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=webgl_wizard">webgl_wizard</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099800">7 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90099800&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099800">12 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 10 -->
                <tr id="90099010">
                    <td><span>10.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90099010&amp;how=up&amp;auth=d5f6g7h8"></a></center></td>
                    <td><a href="https://trynia.ai/launch-hn-context-agents">Launch HN: Nia (YC W36) – Give context to autonomous coding agents</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=trynia.ai"><span>trynia.ai</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>112 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=founder_jane">founder_jane</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099010">10 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90099010&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099010">45 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 11 -->
                <tr id="90099222">
                    <td><span>11.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90099222&amp;how=up&amp;auth=j9k0l1z2"></a></center></td>
                    <td><a href="https://www.debian.org/News/2035/20351208">Debian 18 "Trixie" released</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=debian.org"><span>debian.org</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>312 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=apt_get">apt_get</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099222">14 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90099222&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099222">78 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 12 -->
                <tr id="90100066">
                    <td><span>12.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90100066&amp;how=up&amp;auth=x3c4v5b6"></a></center></td>
                    <td><a href="https://github.com/sudo-project/sudo/discussions/rewrite-zig">Is it time to rewrite sudo in Zig?</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=github.com"><span>github.com</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>45 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=ziggy42">ziggy42</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90100066">3 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90100066&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90100066">60 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 13 -->
                <tr id="90098555">
                    <td><span>13.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90098555&amp;how=up&amp;auth=n7m8q9w0"></a></center></td>
                    <td><a href="https://ec.europa.eu/commission/presscorner/detail/en/ip_35_1234">EU passes "Right to Human Verification" Act</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=europa.eu"><span>europa.eu</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>670 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=policy_wonk">policy_wonk</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098555">1 day ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90098555&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098555">290 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 14 -->
                <tr id="90099777">
                    <td><span>14.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90099777&amp;how=up&amp;auth=r1t2y3u4"></a></center></td>
                    <td><a href="https://brain-hacks.org/2035/12/08/reversing-neuralink-v4-bluetooth">Reverse Engineering the Neuralink V4 Bluetooth Protocol</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=brain-hacks.org"><span>brain-hacks.org</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>220 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=cyborg_sec">cyborg_sec</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099777">8 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90099777&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099777">55 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 15 -->
                <tr id="90099812">
                    <td><span>15.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90099812&amp;how=up&amp;auth=i5o6p7a8"></a></center></td>
                    <td><a href="https://news.mit.edu/2035/intro-to-photonic-circuits">Post-Silicon Computing: An Intro to Photonic Circuits</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=mit.edu"><span>mit.edu</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>99 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=lightspeed">lightspeed</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099812">6 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90099812&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099812">18 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 16 -->
                <tr id="90098123">
                    <td><span>16.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90098123&amp;how=up&amp;auth=s9d0f1g2"></a></center></td>
                    <td><a href="https://www.fda.gov/news/2035/crispr-lactose-approval">FDA approves over-the-counter CRISPR for lactose intolerance</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=fda.gov"><span>fda.gov</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>415 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=bio_hacker">bio_hacker</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098123">16 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90098123&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098123">211 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 17 -->
                <tr id="90098234">
                    <td><span>17.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90098234&amp;how=up&amp;auth=h3j4k5l6"></a></center></td>
                    <td><a href="https://sqlite.org/releaselog/4_0_0.html">SQLite 4.0 Release Notes</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=sqlite.org"><span>sqlite.org</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>800 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=drh">drh</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098234">20 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90098234&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098234">140 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 18 -->
                <tr id="90099456">
                    <td><span>18.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90099456&amp;how=up&amp;auth=z7x8c9v0"></a></center></td>
                    <td><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099456">Ask HN: How do you prevent ad-injection in AR glasses?</a></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>320 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=glasshole2">glasshole2</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099456">11 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90099456&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099456">102 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 19 -->
                <tr id="90099678">
                    <td><span>19.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90099678&amp;how=up&amp;auth=b1n2m3q4"></a></center></td>
                    <td><a href="https://jepsen.io/analyses/nats-4.2">Jepsen: NATS 4.2 (Still losing messages?)</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=jepsen.io"><span>jepsen.io</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>88 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=aphyr_bot">aphyr_bot</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099678">9 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90099678&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099678">33 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 20 -->
                <tr id="90100099">
                    <td><span>20.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90100099&amp;how=up&amp;auth=w5e6r7t8"></a></center></td>
                    <td><a href="https://www.youtube.com/watch?v=riscv_gta6_demo">Playing GTA VI on a RISC-V Cluster</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=youtube.com"><span>youtube.com</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>45 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=tlyleung">tlyleung</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90100099">2 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90100099&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90100099">16 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 21 -->
                <tr id="90099888">
                    <td><span>21.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90099888&amp;how=up&amp;auth=y9u0i1o2"></a></center></td>
                    <td><a href="https://www.haskell.org/blog/2035/why-fp-future-again">Why functional programming is the future (again)</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=haskell.org"><span>haskell.org</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>102 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=monad_lover">monad_lover</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099888">7 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90099888&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099888">65 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 22 -->
                <tr id="90097777">
                    <td><span>22.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90097777&amp;how=up&amp;auth=p3a4s5d6"></a></center></td>
                    <td><a href="https://officewatch.com/2035/microsoft-365-price-hike-40">Microsoft Office 365 prices increase to $40/user/month</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=officewatch.com"><span>officewatch.com</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>900 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=taubek">taubek</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90097777">1 day ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90097777&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90097777">600 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 23 -->
                <tr id="90098888">
                    <td><span>23.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90098888&amp;how=up&amp;auth=f7g8h9j0"></a></center></td>
                    <td><a href="https://bellard.org/jslinux/win10-emu.html">Emulating Windows 10 in the browser</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=bellard.org"><span>bellard.org</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>341 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=qemu_fan">qemu_fan</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098888">19 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90098888&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098888">50 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 24 -->
                <tr id="90098765">
                    <td><span>24.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90098765&amp;how=up&amp;auth=k1l2z3x4"></a></center></td>
                    <td><a href="https://tailscale.com/blog/starlink-dish-mesh">Let's put Tailscale on a SpaceX Starlink Dish</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=tailscale.com"><span>tailscale.com</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>250 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=net_hacker">net_hacker</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098765">20 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90098765&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098765">45 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 25 -->
                <tr id="90098654">
                    <td><span>25.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90098654&amp;how=up&amp;auth=c5v6b7n8"></a></center></td>
                    <td><a href="https://www.aarp.org/technology/deep-fakes-seniors-manual.html">Manual: Deep Fakes detection for Seniors</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=aarp.org"><span>aarp.org</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>122 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=concerned_grandson">concerned_grandson</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098654">21 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90098654&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098654">77 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 26 -->
                <tr id="90097654">
                    <td><span>26.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90097654&amp;how=up&amp;auth=m9q0w1e2"></a></center></td>
                    <td><a href="https://www.bloomberg.com/news/articles/2035-12-08/ibm-rumored-to-acquire-openai">IBM to acquire OpenAI (Rumor)</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=bloomberg.com"><span>bloomberg.com</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>120 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=stock_watcher">stock_watcher</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90097654">1 day ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90097654&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90097654">338 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 27 -->
                <tr id="90098999">
                    <td><span>27.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90098999&amp;how=up&amp;auth=r3t4y5u6"></a></center></td>
                    <td><a href="https://htmx.org/essays/unexpected-return-ssr-2035/">The unexpected return of server-side rendering</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=htmx.org"><span>htmx.org</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>147 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=bikenaga">bikenaga</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098999">19 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90098999&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098999">48 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 28 -->
                <tr id="90098444">
                    <td><span>28.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90098444&amp;how=up&amp;auth=i7o8p9a0"></a></center></td>
                    <td><a href="https://privacy-first.com/guides/bedroom-faraday-cage">How to build a Faraday Cage for your bedroom</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=privacy-first.com"><span>privacy-first.com</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>267 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=tinfoil_hat">tinfoil_hat</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098444">22 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90098444&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098444">49 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 29 -->
                <tr id="90099333">
                    <td><span>29.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90099333&amp;how=up&amp;auth=s1d2f3g4"></a></center></td>
                    <td><a href="https://garymarcus.com/blog/2035/ai-stalling-mirage">AI progress is stalling. Human equivalence was a mirage</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=garymarcus.com"><span>garymarcus.com</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>485 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=skeptic_ai">skeptic_ai</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099333">14 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90099333&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90099333">416 comments</a>
                    </td>
                </tr>
                <tr></tr>

                <!-- Item 30 -->
                <tr id="90098800">
                    <td><span>30.</span></td>
                    <td><center><a href="https://dosaygo-studio.github.io/hn-front-page-2035/vote?id=90098800&amp;how=up&amp;auth=h5j6k7l8"></a></center></td>
                    <td><a href="https://github.com/pure-coder/no-ai-editor">Show HN: A text editor that doesn't use AI</a> <span> (<a href="https://dosaygo-studio.github.io/hn-front-page-2035/from?site=github.com"><span>github.com</span></a>)</span></td>
                </tr>
                <tr>
                    <td colspan="2"></td>
                    <td>
                        <span>270 points</span> by <a href="https://dosaygo-studio.github.io/hn-front-page-2035/user?id=pure_coder">pure_coder</a> <span><a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098800">22 hours ago</a></span> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/hide?id=90098800&amp;goto=news">hide</a> | <a href="https://dosaygo-studio.github.io/hn-front-page-2035/item?id=90098800">105 comments</a>
                    </td>
                </tr>
                <tr></tr>
                <tr></tr>
                <tr>
                    <td colspan="2"></td>
                    <td><a href="https://dosaygo-studio.github.io/hn-front-page-2035/newest" rel="next">More</a></td>
                </tr>
            </tbody></table>
        </td>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral Releases Devstral 2 (72.2% SWE-Bench Verified) and Vibe CLI (184 pts)]]></title>
            <link>https://mistral.ai/news/devstral-2-vibe-cli</link>
            <guid>46205437</guid>
            <pubDate>Tue, 09 Dec 2025 14:45:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/devstral-2-vibe-cli">https://mistral.ai/news/devstral-2-vibe-cli</a>, See on <a href="https://news.ycombinator.com/item?id=46205437">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p dir="ltr">Today, we're releasing Devstral 2—our next-generation coding model family available in two sizes: Devstral 2 (123B) and Devstral Small 2 (24B). Devstral 2 ships under a modified MIT license, while Devstral Small 2 uses Apache 2.0. Both are open-source and permissively licensed to accelerate distributed intelligence.</p>
<p dir="ltr">Devstral 2 is currently free to use via <a href="https://console.mistral.ai/">our API</a>.</p>
<p dir="ltr">We are also introducing Mistral Vibe, a native CLI built for Devstral that enables end-to-end code automation.</p>
<h2 dir="ltr">Highlights.</h2>
<ol>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Devstral 2: SOTA open model for code agents with a fraction of the parameters of its competitors and achieving 72.2% on SWE-bench Verified.</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Up to 7x more cost-efficient than Claude Sonnet at real-world tasks.</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Mistral Vibe CLI: Native, open-source agent in your terminal solving software engineering tasks autonomously.</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Devstral Small 2: 24B parameter model available via API or deployable locally on consumer hardware.</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Compatible with on-prem deployment and custom fine-tuning.</p>
</li>
</ol>
<h2 dir="ltr">Devstral: the next generation of SOTA coding.</h2>
<p dir="ltr">Devstral 2 is a 123B-parameter dense transformer supporting a 256K context window. It reaches 72.2% on SWE-bench Verified—establishing it as one of the best open-weight models while remaining highly cost efficient. Released under a modified MIT license, Devstral sets the open state-of-the-art for code agents.</p>
<p dir="ltr">Devstral Small 2 scores 68.0% on SWE-bench Verified, and places firmly among models up to five times its size while being capable of running locally on consumer hardware.</p>
<p><img src="https://cms.mistral.ai/assets/d295e716-acbe-4d05-8764-861ca2f2a2eb.png?width=1686&amp;height=1093" alt="Devstral   Swe Bench Verified  Open Weights Vs Proprietary Models (dark) (1)"></p>
<p><img src="https://cms.mistral.ai/assets/9c36eef1-2b4c-4fb8-8ef0-d531116ec53a.png?width=1686&amp;height=1093" alt="Devstral   Swe Bench Verified  Open Weights Vs Proprietary Models (light) (1)"></p>
<p dir="ltr">Devstral 2 (123B) and Devstral Small 2 (24B) are 5x and 28x smaller than DeepSeek V3.2, and 8x and 41x smaller than Kimi K2—proving that compact models can match or exceed the performance of much larger competitors. Their reduced size makes deployment practical on limited hardware, lowering barriers for developers, small businesses, and hobbyists.hardware.</p>
<p><img src="https://cms.mistral.ai/assets/3c7a5ea7-d83f-4dc4-9129-965c321bb379.png?width=1686&amp;height=969" alt="Devstral   Swe Bench Verified Regular Performance X Modelsize (dark)"></p>
<p><img src="https://cms.mistral.ai/assets/49e0d71c-436c-4334-9fff-fa68c9f60380.png?width=1686&amp;height=969" alt="Devstral   Swe Bench Verified Regular Performance X Modelsize (light)"></p>
<h3 dir="ltr">Built for production-grade workflows.</h3>
<p dir="ltr">Devstral 2 supports exploring codebases and orchestrating changes across multiple files while maintaining architecture-level context. It tracks framework dependencies, detects failures, and retries with corrections—solving challenges like bug fixing and modernizing legacy systems.</p>
<p dir="ltr">The model can be fine-tuned to prioritize specific languages or optimize for large enterprise codebases.</p>
<p dir="ltr">We evaluated Devstral 2 against DeepSeek V3.2 and Claude Sonnet 4.5 using human evaluations conducted by an independent annotation provider, with tasks scaffolded through Cline. Devstral 2 shows a clear advantage over DeepSeek V3.2, with a 42.8% win rate versus 28.6% loss rate. However, Claude Sonnet 4.5 remains significantly preferred, indicating a gap with closed-source models persists.</p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/542495d8-31d9-4053-a426-df9dccc58ef1.png?width=1371&amp;height=670" alt="Devstral   Model Performance Comparison (dark) (1)"> <img src="https://cms.mistral.ai/assets/48b2b0fc-f8d8-44da-a3a2-4961aad2f10e.png?width=1371&amp;height=670" alt="Devstral   Model Performance Comparison (light) (1)"></p>
<p dir="ltr">“Devstral 2 is at the frontier of open-source coding models. In Cline, it delivers a tool-calling success rate on par with the best closed models; it's a remarkably smooth driver. This is a massive contribution to the open-source ecosystem.” — Cline.</p>
<p dir="ltr">“Devstral 2 was one of our most successful stealth launches yet, surpassing 17B tokens in the first 24 hours. Mistral AI is moving at Kilo Speed with a cost-efficient model that truly works at scale.” — Kilo Code.</p>
<p dir="ltr">Devstral Small 2, a 24B-parameter model with the same 256K context window and released under Apache 2.0, brings these capabilities to a compact, locally deployable form. Its size enables fast inference, tight feedback loops, and easy customization—with fully private, on-device runtime. It also supports image inputs, and can power multimodal agents.&nbsp;</p>

<p dir="ltr">Mistral Vibe CLI is an open-source command-line coding assistant powered by Devstral. It explores, modifies, and executes changes across your codebase using natural language—in your terminal or integrated into your preferred IDE via the Agent Communication Protocol. It is released under the Apache 2.0 license.</p>
<p dir="ltr">Vibe CLI provides an interactive chat interface with tools for file manipulation, code searching, version control, and command execution. Key features:</p>
<ul>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Project-aware context: Automatically scans your file structure and Git status to provide relevant context</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Smart references: Reference files with @ autocomplete, execute shell commands with !, and use slash commands for configuration changes</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Multi-file orchestration: Understands your entire codebase—not just the file you're editing—enabling architecture-level reasoning that can halve your PR cycle time</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Persistent history, autocompletion, and customizable themes.</p>
</li>
</ul>
<p dir="ltr">You can run Vibe CLI programmatically for scripting, toggle auto-approval for tool execution, configure local models and providers through a simple config.toml, and control tool permissions to match your workflow.</p>
<h2 dir="ltr">Get started.</h2>
<p dir="ltr">Devstral 2 is currently offered free via <a href="http://console.mistral.ai/">our API</a>. After the free period, the API pricing will be $0.40/$2.00 per million tokens (input/output) for Devstral 2 and $0.10/$0.30 for Devstral Small 2.</p>
<p dir="ltr">We’ve partnered with leading, open agent tools <a href="https://kilo.ai/">Kilo Code</a> and <a href="https://cline.bot/">Cline</a> to bring Devstral 2 to where you already build.</p>
<p dir="ltr">Mistral Vibe CLI is available as an extension in <a href="https://zed.dev/extensions">Zed</a>, so you can use it directly inside your IDE.</p>
<h3 dir="ltr">Recommended deployment for Devstral.</h3>
<p dir="ltr">Devstral 2 is optimized for data center GPUs and requires a minimum of 4 H100-class GPUs for deployment. You can try it today on <a href="http://build.nvidia.com/">build.nvidia.com</a>. Devstral Small 2 is built for single-GPU operation and runs across a broad range of NVIDIA systems, including DGX Spark and GeForce RTX. NVIDIA NIM support will be available soon.</p>
<p dir="ltr">Devstral Small runs on consumer-grade GPUs as well as CPU-only configurations with no dedicated GPU required.</p>
<p dir="ltr">For optimal performance, we recommend a temperature of 0.2 and following the best practices defined for <a href="https://github.com/mistralai/mistral-vibe/blob/main/vibe/core/system_prompt.py">Mistral Vibe CLI</a>.</p>
<h2 dir="ltr">Contact us.</h2>
<p dir="ltr">We’re excited to see what you will build with Devstral 2, Devstral Small 2, and Vibe CLI!</p>
<p dir="ltr">Share your projects, questions, or discoveries with us on <a href="https://x.com/mistralai">X/Twitter</a>, <a href="https://discord.com/invite/mistralai">Discord</a>, or <a href="https://github.com/mistralai">GitHub</a>.</p>
<h2 dir="ltr">We’re hiring!</h2>
<p dir="ltr">If you’re interested in shaping open-source research and building world-class interfaces that bring truly open, frontier AI to users, we welcome you to <a href="https://mistral.ai/careers">apply to join our team</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Richard Stallman on ChatGPT (106 pts)]]></title>
            <link>https://www.stallman.org/chatgpt.html</link>
            <guid>46203591</guid>
            <pubDate>Tue, 09 Dec 2025 11:12:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.stallman.org/chatgpt.html">https://www.stallman.org/chatgpt.html</a>, See on <a href="https://news.ycombinator.com/item?id=46203591">Hacker News</a></p>
<div id="readability-page-1" class="page">
<h2>Richard Stallman's personal site.</h2>
<h2><a href="https://www.stallman.org/">https://stallman.org</a></h2>
<p>
For current political commentary, see
the <a href="https://www.stallman.org/archives/polnotes.html">daily
political notes</a>.
</p>
<p>
<a href="https://www.stallman.org/biographies.html#serious">RMS's Bio</a> |
<a href="http://gnu.org/">The GNU Project</a>
</p>

<hr>





ChatGPT is not "intelligence", so please don't call it "AI".
<p>
I define "intelligence" as being capable of knowing or understanding,
at least within some domain.  ChatGPT cannot know or understand
anything, so it is not intelligence.  It does not know what its
output means.  It has no idea that words can mean anything.
</p><p>
I call it a <a href="https://gnu.org/philosophy/words-to-avoid.html#ArtificialIntelligence">"bullshit generator"</a>
because it <a href="https://link.springer.com/article/10.1007/s10676-024-09775-5">generates output "with indifference to the truth"</a>.
</p><p>
The same applies to many other "generative systems", for the same
reasons
</p><p>
The widespread public error of attributing intelligence to those
systems leads millions of people to a misplaced trust for them.
Please join me in spreading the word that people should not trust
systems that mindlessly play with words to be correct in what those
words mean.
</p><p>
Another reason to reject ChatGPT in particular is that users cannot
get a copy of it.  It is unreleased software -- users cannot get even
an executable to run, let alone the source code.  The only way to use
it is by talking to a server which keeps users at arm's length.
</p><p>
Doing your own computing via software running on someone else's server
inherently <a href="https://gnu.org/philosophy/who-does-that-server-really-serve.html">trashes your computing freedom</a>.

</p><hr>
Return to <a href="https://www.stallman.org/home.html">Richard Stallman's home page</a>.
<p>
Please send comments on these web pages to
<a href="mailto:rms@gnu.org"><em>rms@gnu.org</em></a>.
</p><p>
Copyright (C) 2024 Richard Stallman
</p><p>
Verbatim copying and distribution of this entire page are permitted
in any medium, provided this notice is preserved.
</p><hr>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: AlgoDrill – Interactive drills to stop forgetting LeetCode patterns (109 pts)]]></title>
            <link>https://algodrill.io</link>
            <guid>46203581</guid>
            <pubDate>Tue, 09 Dec 2025 11:09:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://algodrill.io">https://algodrill.io</a>, See on <a href="https://news.ycombinator.com/item?id=46203581">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Joy of Playing Grandia, on Sega Saturn (143 pts)]]></title>
            <link>https://www.segasaturnshiro.com/2025/11/27/the-joy-of-playing-grandia-on-sega-saturn/</link>
            <guid>46203138</guid>
            <pubDate>Tue, 09 Dec 2025 09:48:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.segasaturnshiro.com/2025/11/27/the-joy-of-playing-grandia-on-sega-saturn/">https://www.segasaturnshiro.com/2025/11/27/the-joy-of-playing-grandia-on-sega-saturn/</a>, See on <a href="https://news.ycombinator.com/item?id=46203138">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<h2>The Renaissance Period</h2>



<p>We are living through a Saturn renaissance. Buckets of titles previously locked away in Japan are seeing new audiences, thanks to the herculean efforts of small but dedicated teams of enthusiast translators, removing the veil of Japanese illiteracy from before our tired eyes. Interestingly, the majority of efforts are being directed at the games with the biggest scripts, and no other genre was as impacted by the language barrier as the text-heavy, story-driven RPG. Over a dozen quality titles are now playable in English. The Saturn is, once again, ascendant…</p>



<h2>Ain’t life Grand?</h2>



<p>Enter <em>Grandia</em>.</p>



<p>What hasn’t been said about <em>Grandia</em>? In the run-up to its late 1997 release, the game enjoyed significant worldwide coverage in the gaming press, not least because some positioned it as the anti-FF7 title. Hot on the heels of the remaster of <em>Lunar: Silver Star Story</em> and hailing from respected software house Game Arts, featuring state of the art fully 3D environments, a score by notable composer Noriyuki Iwadare, sound effects produced by Skywalker Sound… <em>Grandia</em> was indeed shaping up to be one of the premier JRPG experiences of the 5th generation. There was serious talk of bringing the game out West — Working Designs was touted as the favoured house to do the honors, owing to their strong partnership with Game Arts, but the game’s massive script would have meant a late 1998 release by even the speediest conversion standards of the time. By then, the Western Saturn retail market had collapsed, and despite a shrinking but fervently dedicated base of Saturn fans holding on to hope of seeing the title cross the ocean, the game wound up locked away in Japan, forever.</p>


<div>
<figure><img decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002354.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002354.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002354.bmp 300w" sizes="(max-width: 704px) 100vw, 704px"><figcaption><em>Sue’s always looking out for Justin.</em></figcaption></figure>
</div>


<h2>NEVER say Forever</h2>



<p>Game Arts subsequently ported <em>Grandia</em> to the PlayStation, dropping it in Japan in the summer of 1999. Sony speedily localized the game for Western release later that same year… but we aren’t going to focus too much on the PlayStation version here because, at the time of writing, PlayStation discs don’t boot on your SEGA Saturn. It’s the Saturn game that we are concerned with. For us Saturn stalwarts, we had to wait to the mid-2020s for <strong>an intrepid team led by TrekkiesUnite113 to <a href="https://segaxtreme.net/resources/grandia-english-patch.67/" data-type="link" data-id="https://segaxtreme.net/resources/grandia-english-patch.67/">transplant the PlayStation’s English script into the Saturn code</a></strong>. By then, the game was decades old, not to mention re-released and ‘re-mastered’ on modern platforms. So, why translate <em>Grandia</em> for the Saturn, when multiple other English options exist?</p>



<p><strong>Because <em>Grandia</em> is Best on Saturn.</strong></p>



<h2>How do you do</h2>



<p>Set in an age of discovery at the dawn of the industrial revolution, <em>Grandia</em> initially tells the tale of young Justin — a 14-year-old fearless adolescent who can’t help but dream of adventure. When he isn’t playing at “hero” with his town friends, he’s dreaming of great expeditions to find the lost civilization of Angelou. He is joined by his friend Sue — an 8-year-old girl whose maturity belies her age, and who tries desperately to keep young Justin in line. Justin’s mom Lily runs the local Seagull Restaurant and does her best to raise Justin into a respectable young man… though in her youth, she was a scrappy pirate herself. In her heart, she knows her audacious spark has passed on to her son, and that <strong>Justin will one day take up the adventurer’s mantle and take off on a grand adventure of his own</strong>, so she does her best to prepare him for when the time comes.</p>



<p><strong>She gives Justin a Spirit Stone</strong> — a remnant of the Angelou civilization and a memento of his long-lost father — and in doing so, helps kick off a fantastic voyage that sees young Justin explore, learn, overcome all manner of obstacles, and ultimately, grow and become the hero that he always imagined himself to be.</p>


<div>
<figure><img decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002388.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002388.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002388.bmp 300w" sizes="(max-width: 704px) 100vw, 704px"><figcaption><em>The party’s travels take them to the most interesting locations.</em></figcaption></figure>
</div>


<p>During his quest, Justin encounters fascinating characters, both friend and foe. From quiet folk in sleepy villages to rambunctious youngsters eager for their own slice of adventure; from military platoons led by the most beautiful — but hopelessly shallow — lady sergeants to cunning merchants, towering warriors, alluring mermaids and ferocious dragons… Justin encounters them all, and for good or ill, manages to change the course of their lives in ways both subtle and grand.</p>



<figure>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="126" height="126" data-id="53357" src="https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/Justin.png?resize=126%2C126&amp;ssl=1" alt=""></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="126" height="126" data-id="53358" src="https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/Sue.png?resize=126%2C126&amp;ssl=1" alt=""></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="126" height="126" data-id="53359" src="https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/Feena.png?resize=126%2C126&amp;ssl=1" alt=""></figure>
</figure>



<p>Justin, Sue, and Feena are the first three playable characters in <em>Grandia</em>. Young Sue tries to keep Justin in line, while Feena searches for the true meaning of being an adventurer – with Justin slowly moving from admiring her to showing her the way.</p>



<p>The game is clever in pulling the player in for a ride that for a very long while feels very lighthearted and innocent. Even as Justin’s adventure begins in earnest and the player is exposed to antagonists, mysteries, undercurrents and intrigues, Justin can’t help but distill it back to the <strong>very pure essence of boyhood adventure.</strong> Mysterious tower causing problems for a nearby village for years? No problem, Justin will fix it! A dragon from a nearby volcano terrorizing the locals? Justin’s got this. A ghost ship sailing in to harass a passenger steamer? Justin is the answer, in the same way that, as youngsters, we all knew – we knew! – that <strong>WE were the heroes</strong>, and that WE would save the day, armed only with our courage and our grand imaginations. It was our duty, after all. We had it in us to go forth boldly, and change the world (and naturally, all before being called home for dinner).</p>



<p>This point is driven home by Justin’s insatiable desire to uncover the mystery of his Spirit Stone, and the ancient Angelou civilization. After an unfortunate but entirely predictable mishap in the local museum, followed by a mysterious revelation in the nearby Sult Ruins, Justin’s curiosity is ignited, and his drive for real adventure becomes indomitable. Meanwhile, forces are at work that care not for Justin’s explorations, and inevitably, the lad finds himself pitted against the Garlyle Forces and some of its top commanders. Their aims are complex and their operations span the world, and this scope creates a wonderful juxtaposition with Justin’s innocent demeanor and singular focus.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002397.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002397.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002397.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>The amount of architecture being displayed here is stunning, though Grandia makes the Saturn work for it.</em></figcaption></figure>
</div>


<h2>On Screen!</h2>



<p>The Fifth Generation of consoles marked the rise of 3D graphics, but some genres made the leap easier than others. This shift was a struggle for RPGs, with many excellent titles continuing to employ 2D visuals, albeit in richer color and more sophisticated detail than seen in previous generations. Early attempts at the 3D RPG (<em>Virtual Hydlide</em>) highlighted how difficult it was to run this style of game on the hardware of the time without wrecking the framerate or keeping textures from looking like a checkerboard mess. Dungeon crawlers (<em>Shining the Holy Ark</em>) were among the first titles to get the 3D right, though the player’s scope of movement was very restricted. Finally, some fantasized that “3D” meant pre-rendered backgrounds and copious FMV clips, with the only real 3D being battle scenes. Ahem!</p>



<p><em>Grandia</em> took the traditional overhead RPG view and transformed the landscapes into <strong>fully realized 3D polygonal playfields</strong> that can be rotated and zoomed at will. Character and enemy sprites are then overlain on the 3D world to make the scenes come to life. The addition of the third dimension affords the use of depth in the environments: hills, cliffs, and valleys; minecar rails that ran higher or lower relative to other tracks, and so on. In this way, the player initially feels right at home with a view that looks comfortably familiar, but must quickly learn to constantly rotate the viewpoint to catch enemies in hiding, spy treasures only visible from certain angles, judge heights, and evaluate other geometric details to plot their best course forward.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002414.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002414.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002414.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>Aside from technical achievements, the art direction is fantastic.</em></figcaption></figure>
</div>


<p><em>Grandia</em> wastes no time in getting gamers used to this new visual paradigm. One of the game’s first quests sees local frenemy Gantz challenge Justin and Sue to locate the three Legendary Treasures: the fabled helmet (Iron Pot), the storied shield (Pot Lid), and of course, the legendary (Wooden) Sword. The player must traverse all of Parm, climbing down river walkways, checking in enclosed spaces, and chasing down Gantz’s little brother to prove they are up to Gantz’ task — and in the process, get used to the game’s then-new control scheme.</p>



<p>The 3D is very well put together, both technical and artistically. The level of detail is truly phenomenal, from the tiniest objects and details, especially in the ‘in-town’ game sections. Justin is able to interact with some of the innocuous scenery — for example he can knock brooms over, disturb piles of plates, or bump into bells and chimes — just as any real, overly excited 14-year-old might clumsily do as they ran along. Animations, from little weathervanes rotating to laundry fluttering on a clothesline, to puffs of smoke coming up from fires or chimneys, all accentuate the feeling that these are real, living, bustling places. The level of detail, and all of it in 3D, is really special.</p>



<p>The coders at Game Arts made excellent use of the Saturn’s unique hardware when realizing <em>Grandia</em>’s locales. Where appropriate, textured infinite planes are used to draw floors, and they not only look good but also dramatically cut down on the usage of polygons in drawing the scene, leaving that much more in the processing budget to spend on other visual details. In later sections, those infinite planes take on a distortion effect to create some very cool-looking water flows — look for them initially in Parm’s pier, and later in places like the snowy Laine Village or the mysterious Castle of Dreams. The water shimmers as the player rotates their view to create a truly stunning effect.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002409-4.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002409-4.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002409-4.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>Slimes are never that tough to dispatch in any RPG.</em></figcaption></figure>
</div>


<p>The game’s characters and enemies are all represented by sprites that animate quite well and take viewpoints into account as the player rotates the camera. In yet more attention to detail, the sprites darken and then lighten again as the player moves in and out of shadowed areas — an impressive little detail that accentuates the visuals even further.</p>



<figure>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="126" height="126" data-id="53362" src="https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/Baal.png?resize=126%2C126&amp;ssl=1" alt=""></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="125" height="126" data-id="53361" src="https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/Mullen.png?resize=125%2C126&amp;ssl=1" alt=""></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="124" height="126" data-id="53360" src="https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/Leen.png?resize=124%2C126&amp;ssl=1" alt=""></figure>
</figure>



<p>The trio of General Baal, Colonel Mullen, and Leen is introduced in the game’s opening scene, and all three are more than they appear.</p>



<p>The care that Game Arts took in crafting the visuals is commendable and <em>Grandia</em> comes off as one of the very best-looking 3D RPGs for the system, but Game Arts was perhaps a mite too ambitious. There are sections of the game where the framerate really chugs. Now, it must be acknowledged that low framerates were a hallmark of many 3D games in the 32-bit era, so some of this is to be expected, but the more detail Grandia is trying to show you, the more you will feel the Saturn huffing and puffing to get the job done. The game’s 3D framerate is not high at the best of times but it is passable, so it’s somewhat of a relief that the areas where it truly takes a dive aren’t too common.</p>



<h2>Pump up the Jam!</h2>



<p>Game Arts’ attention to detail extends to the sound department. For <em>Grandia</em>, Game Arts commissioned Skywalker Sound to handle the game’s sound effects. The result is <strong>positional sound</strong> — effects like running water, crackling fire, etc. will fade in and out as Justin and co. move closer in or further away from the source. Often, if the effect is important, it will also somewhat dampen the volume of the BGM as it plays out. Additionally, the effects will pan left or right depending on the source, and especially as the player rotates the camera. These effects may be subtle, but they are very well implemented and add to the game’s overall polish.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002421.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002421.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002421.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>The game is very colorful.</em></figcaption></figure>
</div>


<p>The game’s <strong>soundtrack was composed by Noriyuki Iwadare</strong> and is both varied and excellent. Iwadare’s use of instruments appropriate to the on-screen action is uncanny — for example, running around Parm we are treated to an industrial sounding theme, perfect for the town’s motif. The varied use of strings, drums and winds is frankly excellent and lends to the atmosphere, imitating the clang of metal and steel which so permeates the city. Equally impressive is that the music somehow manages to be exciting or somber or poignant without ever sounding overly (excuse the wordplay) grandiose. This keeps the soundtrack in line with the game’s more lighthearted narrative. Of course, where appropriate, the soundtrack does take on that epic quality. The desperate tones that play when the Garlyle forces appear contrast so well with the carefree, upbeat “Off Runs Sue” tune. Mullen’s theme is at once wistful and ambitious, and even the theme from the Sult Ruins dungeon is perfectly mood-setting. Multiple <em>Grandia </em>soundtracks have been released since the game’s debut and the soundtrack is universally praised.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002437.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002437.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002437.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>Leen is one of Col. Mullen’s acolytes.</em></figcaption></figure>
</div>


<h2>How it Plays Out</h2>



<p><em>Grandia</em>’s gameplay, like so many RPGs before it, is split into two major gameplay slices: exposition-laden town sections and combat-focused dungeons.</p>



<p>Players will spend a fair bit of time in the ‘in-town’ sections of the game. Here, you will wander around, take in the scenery, interact with the NPCs of the area, and almost always, find a quest that must be completed. A quick word about the NPCs — there are quite a number of them in each town, and everyone has something interesting to say… and almost always, each NCP has at least two separate conversation sequences to offer, making for a truly large amount of story to soak in. And <strong>it’s all entirely optional!</strong> It’s completely possible to make one’s way through <em>Grandia</em> with only minimal NCP interaction, but the option to enhance the adventure with these extensive NPC interactions is always there, as each character will present a unique view or focused response.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002441.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002441.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002441.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>An unlikely pairing.</em></figcaption></figure>
</div>


<p>Predictably, the towns are filled with shops, though <em>Grandia</em> keeps things rather simple — there is but one general store which carries weapons, armor, accessories, and even magic all under the same roof. Buy, sell or trade up to the latest gear which gradually increases in the stat boosts it confers to your characters. Additionally, each town typically has one or more important locales, such as mayors’ offices or the chambers of village chiefs.</p>



<p>There is typically an inn or other house where the party can take rest, and at certain points in the game, resting triggers a shared meal scene that sees Justin break bread with his party mates. These meal scenes offer up critical dialogue, which the gamer can extend or keep short at their whim. When the critical conversation has been had, a bedtime icon will appear over Justin’s character sprite, and if the player is quite finished listening to the party chatter, they can select it to end the meal and get some rest. These mealtime conversations serve not only to flesh out what the party must tackle next, but also to offer a glimpse into the inner thoughts of the individual characters as they share their opinions, hopes and fears. Like so much in the game, <em>Grandia</em> implements this character exposition in a way that allows the player to decide how much of it to take in.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002480.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002480.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002480.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>Great use of color.</em></figcaption></figure>
</div>


<p><strong>The visuals in the town sections really stand out.</strong> The Saturn manages to shift not only impressive amounts of polygons for the various structures, but also vivid and complex textures. This technical prowess is coupled with lush and imaginative art direction, resulting in each locale feeling complete and distinct. The dense, humid and green surrounds of Luc Village, nestled deep within the Misty Forest and inhabited by humanoid creatures contrasts sharply with the seaside port town of Dight with its cerulean waves gently rolling in onto its sandy shores. Milda’s hometown village of Laine is covered in snow, and the ancient Zil Padon is an architectural wonder with a central fountain in the middle of the Savanna desert. Game Arts very clearly discarded their standard world building cookie cutters, and their efforts shine through.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002486.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002486.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002486.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>The world map. The feather icon indicates where you will travel next.</em></figcaption></figure>
</div>


<p>Once a locale has been explored, it will appear as a selectable destination on a gorgeous, hand-drawn high-resolution world map. Exiting an area often brings our party to this world map, and the next destination can be selected.</p>



<p>If the towns serve to heal the party, upgrade equipment, and advance the story, then the dungeons of the game offer treasure hunting, exploration, and of course, combat! Dungeons in <em>Grandia</em> range from literal underground labyrinths to above-ground forest mazes, to even large open plains that Justin et al. must traverse. Some of the more noteworthy settings include scaling a giant wall that keeps the world divided into two separate societies, negotiating the bowels of a ghost ship which appears out of nowhere to molest a transcontinental steamer, and even conquering the inside of an unstable volcano that’s inhabited by an ancient dragon.</p>



<p>Here, the player really must use their L and R buttons to shift the 3D landscape around, to find all the nooks of treasure or paths forward. Some areas feature set pieces that Justin and party can activate — for example, knocking over a loose pillar to bridge a gap. These are usually indicated by an exclamation point icon when the party nears the set piece.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002499.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002499.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002499.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>Some of the spells are quite spectacular.</em></figcaption></figure>
</div>


<p>All the while, treasure both great and small litters the landscape… but so do enemies! Enemies are visible in the dungeons and so can be avoided to an extent, but if Justin and party come in contact with an enemy, combat ensues.</p>



<h3><em>Grandia</em> Grinder Alert!<br>Grind for Experience Points Using Environmental Damage!</h3>



<p>Are YOU a <em>Grandia </em>grinder?? Some sections of the game will deal damage to Justin and party outside of combat. First noticed in the Dom Ruins, rock faces painted into some of the dungeon walls will cause mild HP damage by springing out and poking the party when the party doesn’t want to be poked! The player can then use heal magic and spam this process to quickly increase Water magic levels. Although definitely a grind, it’s much faster than earning those experience points via combat. A few other areas in the game present similar opportunities — such as the basement in the Castle of Dreams.</p>



<h2>A New Kind of Kombat</h2>



<p><em>Grandia</em> introduced an all-new combat system to the RPG genre, though it could be said to be a variant of other similar RPG battle systems. Essentially, all battle participants have icons that continuously move along a <strong>universal IP Gauge</strong>, until they reach the Command point. Here, the player will enter from a selection of commands which includes attacking, using an item or a spell, guarding, or even retreating. They then wait to reach the very end of the gauge to execute their selected action, and the more experienced the character, the faster that point is reached. <strong>A ton of strategy is introduced here</strong> as during this waiting period between selecting an action and executing it, they are vulnerable to both Cancels and Counterattacks from their opponents. Unlike many contemporary RPGs where the instinct is to simply unleash physical and magical attacks in a turn-based order, <strong>the player can take advantage of these waiting periods</strong> to cancel out incoming enemy attacks and push them back on their IP gauge. The system will take some getting used to, but can be used to devastating effect, especially in the more drawn-out boss battles. It is entirely possible to strategically get in a half-dozen actions by each character and prevent a boss from retaliating during the entire sequence, by carefully timing attacks. This makes combat a lot more involved and exciting.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002528.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002528.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002528.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>Cancel culture? Counterculture? Grandia’s got it all.</em></figcaption></figure>
</div>


<p>There are also advantages to catching an enemy unawares — player characters start much further ahead on their IP Gauge, with the reverse being true if Justin’s party is ambushed.</p>



<p>Players have a range of actions they can take when their IP Gauge is full, from the standard fare of using items, defending, running away, or even inspecting an enemy (is that slug-monster male or female, for example*).</p>



<figure>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="126" height="126" data-id="53363" src="https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/2.png?resize=126%2C126&amp;ssl=1" alt=""></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="126" height="126" data-id="53365" src="https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/3.png?resize=126%2C126&amp;ssl=1" alt=""></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="124" height="126" data-id="53364" src="https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/1.png?resize=124%2C126&amp;ssl=1" alt=""></figure>
</figure>



<p>Nana, Saki, and Mio are Mullen’s three she-sergeants. Serving as comedic relief, they are nevertheless quite capable opponents in battle.</p>



<h2>By Your Powers Combined… I Am Captain Planet!</h2>



<p>Earth. Fire. Wind. Water. These are the elemental forces that move the world, and most characters can master them! Learning magic in <em>Grandia</em> first requires that the party finds a <strong>Mana Egg</strong>. These rare items can then be exchanged in a shop for magic for a single character of your choice. That party member then learns the basics of your chosen magic element.</p>



<p>Inside of the four elements, magic spells are further split into levels, from one to three, to indicate their potency. Level 1 spells are your most basic spells and are what a character starts off with should they buy magic with their mana egg. Players that use magic in combat will gain skill points in that particular element, and those skill points are applied to all spells of that element, regardless of spell level — so, use a Level 1 Fire spell, and all levels of your Fire magic gain skill. Spell skill progression is represented by five red stars that fill up like a gauge, turning yellow as they gain experience. Greater experience shortens casting time (which, remember, is a vulnerable time as your spell can be cancelled by an opponent) and at higher levels, allows your character to learn combined element magic spells. All magic spells consume MP making them a limited resource, though a character’s overall MP capacity will grow with experience.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002917.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002917.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002917.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>The snowy village of Laine. The water effects are <strong>chef’s kiss</strong>.</em></figcaption></figure>
</div>


<p>Outside of magic, each character can also execute <strong>special attacks</strong> that are unique to them. These attacks are usually more devastating than standard attacks and sometimes require that the character is using a particular weapon class. These, too, gain skill points represented by five red stars that slowly build up to yellow, though special attacks consume SP (skill points). SP works much the same way as MP.</p>



<h3>Grandia Grinder Alert!<br>Rare Enemies Give High XP</h3>



<p>Typically, the game’s monsters do a good job of seeking you out, but there are occasional difficult-to-catch enemies to be found as well. Notice, for instance, the Chameleon enemies in the Virgin Forest. These green creatures are shy and are hard to catch and engage. But persist, and finish them off for a huge load of experience points — well worth a grinding sesh or three.</p>



<h2>Experience Required</h2>



<p><em>Grandia</em> has a complex (for the time) experience points system, which is cleverly segmented into several categories.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002507.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002507.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002507.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>Level up!</em></figcaption></figure>
</div>


<p>To start, each playable character has a set of basic stats that slowly increase as they gain experience. Hit Points (HP) are your standard measure of health and these increase at level-ups. SP are your skill points, which increase the speed and potency of your special attacks, as well as unlock new special attacks as you accumulate experience. Finally, the same is true of the more traditional magic points (MP), with the difference between SP and MP being that special attacks are individualized whereas magic attacks are more common amongst party members and can be bought in exchange for Mana Eggs.</p>



<p>As they adventure, Justin and company will occasionally find items that slightly <strong>boost a particular stat on a permanent basis.</strong> These items are rare indeed, but as with life, incremental gains tend to compound until the effects are undeniable.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002940.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002940.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002940.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>The Seed of Speed grants a permanent stat boost.</em></figcaption></figure>
</div>


<p>Most traditionally, defeating enemies grants experience points and accumulating the required amount grants characters a level-up, which slightly increases basic stats. Experience gained and gold / treasure collected is displayed on an after-battle screen. It is this type of XP that most contemporary RPGs concerned themselves with. <em>Grandia</em> ups the complexity a bit by introducing leveling for magic and skills, and further mixes things up by employing different weapon classes.</p>



<p>Justin and company are each capable of wielding a few different types of weapons, of which there are seven in total, ranging from swords to maces to staffs to bows. Each weapon class has its advantages and disadvantages, be it speed of use (from Command input to Execution on the IP gauge), to range, to overall damage dealt. As party members use their weapons, they gain experience in those weapon types, separately from their character experience.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002512.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002512.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002512.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>The texture work is awesome throughout.</em></figcaption></figure>
</div>


<p>In total, <em>Grandia</em> features basic character experience points which boosts common stats, magic experience which results in spells being cast faster and the learning of higher-level spells for various element types, skill experience for faster execution of special attacks, and weapon experience points which increase how well a character will handle that weapon type. Cleverly, these different experience categories are implemented in such a way as to make it entirely possible for gamers to completely ignore this aspect of the game should they so fancy. Because the system is automated, gamers can pay all of it little heed and still progress and have a great time with the game. Alternately, gamers can dive right into the finer points of the system to make those minor tweaks to get their characters to exactly the state they prefer.</p>



<figure>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="126" height="126" data-id="53366" src="https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/Liete.png?resize=126%2C126&amp;ssl=1" alt=""></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="123" height="124" data-id="53368" src="https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/Puffy.png?resize=123%2C124&amp;ssl=1" alt=""></figure>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="126" height="122" data-id="53367" src="https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/Darlin.png?resize=126%2C122&amp;ssl=1" alt=""></figure>
</figure>



<p>The mysterious Liete awaits at Alent. The enigmatic Puffy accompanies Sue wherever she goes. Lastly, Darlin is one of the many non-human denizens of Grandia.</p>



<h2>Go with the Flow</h2>



<p><em>Grandia</em> allows up to <strong>four playable characters</strong> to form Justin’s party at any one time. As the story progresses, some of the main characters will permanently step away from the adventure, for reasons practical and dramatic alike. One such parting in particular tugs at the heartstrings — it is nothing quite as dramatic as the year’s earlier death of Aeris (Aerith) from that big RPG on Sony’s lesser 32-bit machine, but it somehow feels more relatable, and more impactful. Players ought not be surprised by the need for tissues to manage an unexpected tear or two. And here, too, <em>Grandia</em> innovates: a portion of a departing playable character’s magic and weapon experience points are stored in the stashing place, to be retrieved and applied to whatever character you see fit. This strengthens their legacy in your party, as well as provide a practical reason not to neglect building up a character just because they may eventually leave the party. A nice touch.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002976.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002976.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002976.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>At the foot of the End of the World.</em></figcaption></figure>
</div>


<h2>Is It Perfect?</h2>



<p>Grand as it sounds, the game isn’t without a few small flaws. Story-wise, players will be left wanting to know more about Justin’s father and how he came to be the keeper of his Spirit Stone. He is mentioned often in the early stages of the game, but as Justin’s adventure takes off, that arc never completes. Likewise for General Baal — we eventually learn his motivations, but not so much why he has become who he is today. A really well put together villain is one with whom we can empathise; someone whose circumstance we can understand. Both with Justin’s unnamed father and with Baal, there is a feeling that we are reading a book and that the answers lie just ahead, but despite some teasing, <em>Grandia</em> never lets us turn the page.</p>



<p>Technically, the game’s 3D is solid and varied, with plenty of minor details and meticulous textures, especially in the town sections. Blending VDP2-drawn planes with solid geometry and animated sprites means the world of <em>Grandia</em> is beautifully rendered, but that comes at the cost of an oft-stuttering framerate. The more of <em>Grandia</em>’s world we are allowed to see at once, the more the framerate suffers. Now, these were the formative years of 3D gaming, but at times, that framerate simply chugs, and it’s noticeable to the point of distraction. Thankfully, for most of the game, the framerate sits comfortably in the ‘acceptable’ space, but you won’t get through the game without feeling the Saturn sweat as it works to display all that <em>Grandia</em>’s artists wanted you to see.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002376.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002376.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002376.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>Special Moves. These gain experience as well.</em></figcaption></figure>
</div>


<p>Speaking of 3D, the game often requires the shifting of camera angles when exploring. When in long dungeons or any other large space, this can quickly become disorienting, and the player will lose their sense of direction. The game compensates somewhat for this with the addition of the compass, though its implementation is somewhat clumsy as rather than point north, it points to an exit or other objective. <strong>There is also lookout points called Dungeon Scopes</strong>, where the player is given a bird’s eye view of their current location from a default ‘north is up’ viewpoint. This helps orientating, but those lookout points are few and far between and using them tends to break up the game’s flow. Players may well find themselves keeping their camera shifting to a minimum as a result.</p>



<p>Lastly, a technical note: <em>Grandia</em> sure gives the Saturn’s laser a workout, and there are some clever pre-loading techniques implemented to keep the game flowing as smoothly as possible. The cost here is that <em>Grandia</em> is very sensitive to disc quality. Those that have burnt their English-patched game onto CDs and are playing on real Saturn hardware may well find the game freeze, especially in battle when calling various spells. This is VERY annoying, especially as dungeon save points are sparse, and it is not uncommon to be in the heat of a battle only to have everyone freeze with the reset button being the only escape. This is remedied by using an ODE solution that omits discs entirely, but the game’s sensitivity to the quality of your CD-R burn needs to be called out.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002964.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002964.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002964.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>Hell yeah! Feena’s strongest spell.</em></figcaption></figure>
</div>


<h2>Final Word</h2>



<p><strong><em>Grandia </em>is great.</strong> The visuals are gorgeous, the music is appropriately evocative, the combat is frenetically strategic, and the story is well paced. Tough battles and surprise plot twists await intrepid gamers, and sub-plots occasionally weave their way into the adventure, too — especially in sections where we briefly leave Justin. On occasion, players will follow Colonel Mullen with Feena, explore the mysterious past of Mullen’s attaché Leen, or even soak in the comedic antics of the three beautiful Garlyle generals Mio, Nana, and Saki.</p>



<p>Ultimately, <em>Grandia</em> a delight to play. A total joy… but one that demands an <strong>intense time commitment</strong>. A player Justin’s age surely has the time, but what about those of us that are well into adulting? Some sections of the game, especially the longer dungeons, have few opportunities to save one’s game. In that sense, the game is a total hardcore, traditional JRPG. It is not easily digested in small play sessions, so playing <em>Grandia </em>is committing a huge slice of one’s discretionary time budget.</p>



<p>And yet, perhaps paradoxically, playing <em>Grandia</em> has a way of making one feel young again. <em>Grandia</em> is grand in the same way we ourselves felt grand as youngsters — that, armed with a stick we’ve just picked up and nothing more than our imagination, our wits, and our indomitable spirit, we could go forth boldly and change the world. That’s the beauty of a main character like Justin — he is not yet jaded; he has not yet borne the <strong>burden of grown-up problems on his shoulders</strong>. In many ways, we were all Justin (or Sue!) at one point, and the game shines a light on that part of us that is now long behind (most of) us. Perhaps the most memorable aspect of <em>Grandia</em> is that it allows us, for a moment all too brief, to once again be that young boy or girl full of optimism and energy, and in today’s complex and stressful world, that feels simply wonderful.</p>


<div>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="728" height="546" src="https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/Grandia.jpg?resize=728%2C546&amp;ssl=1" alt="" srcset="https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/Grandia.jpg?w=800&amp;ssl=1 800w, https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/Grandia.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/www.segasaturnshiro.com/wp-content/uploads/2025/11/Grandia.jpg?resize=768%2C576&amp;ssl=1 768w" sizes="auto, (max-width: 728px) 100vw, 728px"><figcaption><em>Promotional art that showcases one of the game’s most powerful moments: Justin, Sue, and Feena have climbed the wall at the end of the world, and see, for the first time, the lands on the other side.</em></figcaption></figure>
</div>


<div>
<h2>Three Optional Dungeons</h2>



<p><em>Grandia</em> is generally a well-balanced affair, with experience accumulating at the right rate for players to progress in the game. That said, the world of <em>Grandia</em> plays host to three completely optional dungeons meant solely for increasing character abilities and experience — and goes so far as to explicitly point out that these areas are not part of the game’s story and are entirely optional.</p>



<p>The first such dungeon can be found just west of the first section of the Zil Desert. It’s a large, very dark brown multi-leveled maze with the only save point being at the entrance. The enemies are tougher than one would expect at this point in the game, but nothing is impossible for Justin et al. The key here is to find the four Soldier’s Souls, which grants access to the treasures of the dungeon, at the very end, past the boss. The boss is a remix of a previous boss from Feena’s failed wedding to Pakon and packs quite a punch. The main prize here is the excellent Godspeed Knife, which adds a huge ACT boost, to massively speed up the user’s IP gauge.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002930.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002930.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002930.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>The Soldier’s Graveyard entrance.</em></figcaption></figure>
</div>


<p>The second optional dungeon is also found to the west but is accessible from the second part of the Zil Desert. This dungeon is very small but has perhaps the most charm. Justin and company are greeted by a mysterious Lady at the castle entrance, begging for help but also warning of a curse on the castle. Once inside, there are several rooms to visit and loot to collect. Really simplistic and set to lure the player to lower their guard, just in time to battle the formidable Lord’s Ghost boss. This guy’s TOUGH, with strong multi-character attacks and cancelling moves. Take him down to claim the awesome Lightning Sword, which gives a 50 ATK boost and, as an elemental sword, has the Zap! spell built in.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002936.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002936.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002936.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>Don’t thank us yet…</em></figcaption></figure>
</div>


<p>The final optional dungeon is the mother of all dungeons in<em> Grandia</em>. Found tucked away in the Savanna Wilderness and accessible via a secret passage, the Tower of Temptation consists of an outside area and 12 (!) floors of punishing combat. Of course, the only save point is at the very start of the outside area, though Justin can activate a couple of shortcuts through the tower as he makes progress, so that backtracking to heal and save is a bit easier. Interestingly, the starting area is surrounded by six Zero Weapons – one of each kind of weapons that grants a 0 ATK value — ideal for training weapons on weaker enemies, as these will do nearly no damage.</p>



<p><strong><em>Grandia</em> Grinder Mini-Alert</strong>: many enemies in the Tower drop stat-increasing items, making this an ideal place to pull it all out and go for that growth.</p>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002973.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002973.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002973.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>Prepare to spend hours on this dungeon.</em></figcaption></figure>
</div>


<p>Each floor of the Tower features maze sections, hidden doors, powerful enemies, and of course, switches to hit. Simply by making one’s way through the tower will increase the party’s levels, as there is so much battling to do. It is not uncommon to spend hours in the Tower, so it’s a welcome fact that the Tower is entirely optional. The final three floors are all boss — yes, there are three bosses to fight in a row. No saving, no healing. The final of the three bosses is tough as nails, but the reward is well worth it — NINE amazing items to pick up, including two items from the Grinder’s Gear™ premium collection: the Astral Miracle and the Ethereal Miracle, both accessories that double weapon or magic experience gained. VERY useful, but they better be, considering the pain just endured to complete the Tower of Temptation!</p>
</div>



<h2>The Universe is Grand…ia</h2>



<p><em>Grandia</em> went on to <strong>sell bucket-loads in Japan</strong>, especially during release week. It received a Digital Museum DLC-style disc, got a port on the mass-market PlayStation including a PS Greatest Hits re-release, and finally, a PlayStation English localization in 1999. The series continued in 2000 with the excellent <em>Grandia 2</em> on Dreamcast, which itself was later poorly ported to Sony’s killer of dreams, the PlayStation 2. That system would also see the less-well received <em>Grandia 3</em>, which would spell the end of the main series’ run. The series also saw several spin-off games such as <em>Grandia Xtreme</em> and <em>Grandia Online</em>. Additionally, the first <em>Grandia</em> was recently remade for modern consoles with the release of the <em>Grandia HD Collection</em>.</p>



<p>*Note: you cannot inspect monsters’ genders in battle. That was just a joke. Also there is no Grinder’s Gear in Grandia. </p>



<h2>I’m Not Crying, You’re Crying!</h2>


<div>
<figure><img loading="lazy" decoding="async" width="704" height="448" src="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002478.bmp" alt="" srcset="https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002478.bmp 704w, https://www.segasaturnshiro.com/wp-content/uploads/2025/11/00002478.bmp 300w" sizes="auto, (max-width: 704px) 100vw, 704px"><figcaption><em>A beautiful scene.</em></figcaption></figure>
</div>


<p>A bit of a personal story… The above screenshot is my favorite scene in all of Grandia. See, the game does a brilliant job of bringing us back to the days of youthful adventures where nothing at all was impossible, and despite whatever danger beset us, we knew deep down that in the end, we would be all right. But in the most subtle of ways, Grandia also covers personal growth and the passage of time. </p>



<p>At some point, deep into the adventure, 8-year-old Sue gets tired. At first, she temporarily leaves the party whilst recuperating at a local sick house, with everyone hoping (and the player confidently knowing) that she will get better. But… she doesn’t. She puts on a brave face and re-joins the party, going on one final quest. As the gamer, I kept looking for the herb or special item that I could find to cure her, but no such moment ever came. There never was any single wound or ailment that Sue suffered, it’s just that one day, she simply… got tired, and ultimately, had to leave the party. She was a trooper through the entire adventure; completely indispensable she was, but there was a sunset to her time on the grand adventure, and she ended up leaving far too soon for my liking.</p>



<p>In real life, this sometimes happens, too. People in our orbit — strong, vibrant people, whom we believe will be with us forever — sometimes, unexpectedly, undeservedly… get tired, and have to quit the great adventure. Sometimes they are even younger than us, or in better health than us, or benefitting from any number of other factors that make their leaving seem senseless and cruelly unfair. It’s a reminder of the finite nature of life, and that sometimes we are living oh so naively and innocently through what we will later call the best times of our lives.</p>



<p>Sometimes, we get a chance to say our goodbyes before they depart us, and this is something Justin and Feena were able to share with Sue. With tears in her eyes, even as she bade farewell, she wished for Justin to follow his dreams and complete his long quest to find Angelou. It’s this that ties all of these sentiments together, for me. We all get older. We all leave our childhood behind us and begin to lead our adult lives in earnest. Our carefree days of questing and playing our days away, confident that in the end, everything will be all right, are replaced by planning, worrying, pressure, stress, failure, and other harsh realities of life. Here, Sue reminds us of the importance of not forgetting our dreams. We may not have the time or the energy that we did then, but whatever the obstacles, we must always go boldly in the direction of our dreams, hand-in-hand with those who love us, for we, too, will one day exit the adventure. In our final moments, what sweeter satisfaction could there be than to warmly smile at those who walked with us, and to look back on our journey with pride.   </p>	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Modern Walkmans (158 pts)]]></title>
            <link>https://walkman.land/modern</link>
            <guid>46201381</guid>
            <pubDate>Tue, 09 Dec 2025 04:57:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://walkman.land/modern">https://walkman.land/modern</a>, See on <a href="https://news.ycombinator.com/item?id=46201381">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
	<div>
		
		<p><sub>Cassette Players for the Modern Digital Age</sub>
	</p></div>
	<p>Σ 11 models</p>
</div>

<div id="9"><p><a href="https://walkman.land/modern/aurex-ax-w10c"><img src="https://walkman.land/public/img/modern/aurex-1.jpg" loading="lazy" alt="Aurex AX-W10C (Walky) feature"></a></p><div><p>(Toshiba) Wireless cassette player with Bluetooth, you can enjoy cassettes with wireless earphones. Equipped with virtual surround sound, you can enjoy realistic sound. It can play cassette tapes for about 16 hours (2*AA alkaline batteries). It can also be powered, and played from a USB port. Weight 230g. Selling primarly in Japan.</p></div></div><div id="5"><p><a href="https://walkman.land/modern/byron-statics"><img src="https://walkman.land/public/img/modern/byronstatics-1.jpg" loading="lazy" alt="Byron Statics feature"></a></p><div><p>Bring out the soundtrack of past memories on Your cherished cassettes. FM/AM Radio playback. Voice Activation System. Automatic Stop System. 2AA Battery or USB power supply. KCS-315</p></div></div><div id="4"><p><a href="https://walkman.land/modern/digitnow"><img src="https://walkman.land/public/img/modern/digitnow-1.jpg" loading="lazy" alt="DIGITNOW! feature"></a></p><div><p>The personal cassette player looks the part with its retro silver casing and comes complete with earphones for your private listening. With bluetooth function, can transmit the music to other bluetooth receivers and let everyone enjoy the music.</p></div></div><div id="7"><p><a href="https://walkman.land/modern/fiio-cp13"><img src="https://walkman.land/public/img/modern/fiio-cp13-1.jpg" loading="lazy" alt="FiiO CP13 feature"></a></p><div><p>Achieving ultra-low Wow and Flutter. Oversized pure copper flywheel. 100% pure analog sound &amp; custom balanced amplification head. Classic audiophile op-amp JRC5532. High voltage motor power supply. Dual-color all-aluminum alloy chassis and a long-lasting 13 hours of battery life.</p></div></div><div id="3"><p><a href="https://walkman.land/modern/gpo"><img src="https://walkman.land/public/img/modern/gpo-1.jpg" loading="lazy" alt="GPO feature"></a></p><div><p>Battery powered and with built in speakers, just plug in your cassette and you're ready to go. The portable cassette player was an iconic piece of kit for music fans in the 80s and 90s. Play tapes or use the FM radio and listen through your headphones.</p></div></div><div id="10"><p><a href="https://walkman.land/modern/its_ok"><img src="https://walkman.land/public/img/modern/ITSOK_details1.webp" loading="lazy" alt="It's OK! feature"></a></p><div><p>It is the world’s first cassette player with Bluetooth 5.0 capability that not only supports traditional 3.5mm headphones but is also compatible with Bluetooth 5.0 headphones or speakers. Whether you are alone or in an open space, you can freely enjoy the penetrating voice and warm sound from the cassette tape.</p></div><div><p><strong>(+)</strong> Nice translucent design. Bluetooth connection. Built-in microphone.</p><p><strong>(-)</strong> No autoreverse, or any convenience function. No headphone.</p><p><span>Web:</span> <a href="https://www.ninmlab.com/its-ok">https://www.ninmlab.com/its-ok</a></p></div></div><div id="8"><p><a href="https://walkman.land/modern/jensen"><img src="https://walkman.land/public/img/modern/jensen-1.jpg" loading="lazy" alt="Jensen feature"></a></p><div><p>Jensen Portable Compact Lightweight Slim Design Stereo, AM/FM Radio Cassette Player.

Pop in that favorite cassette or relive the magic of the mixed tape with Jensen's Portable Stereo Cassette Player AM/FM Stereo Cassette Player. When you're feeling more like the radio, tune into the AM or FM dial. You can also get up to the minute weather info with local Weather Band broadcasts. And, in the name of keeping things economical, just 2 'AA' battery has the Walkman up and running for hours on end.</p></div></div><div id="11"><p><a href="https://walkman.land/modern/maxell-mxcp-p100"><img src="https://walkman.land/public/img/modern/mxcp-p100_1.webp" loading="lazy" alt="Maxell MXCP-P100 feature"></a></p><div><p>It supports Bluetooth v5.4 , which provides high communication quality and low power consumption. Brass flywheel adopted. Reduces rotational irregularities and provides high quality sound. Bultin battery, accumulator. Playback time is around 9 hours. Weight 210g.</p></div></div><div id="6"><p><a href="https://walkman.land/modern/mulann-b-1000-ew"><img src="https://walkman.land/public/img/modern/mulann1000-1.webp" loading="lazy" alt="Mulann B-1000 EW feature"></a></p><div><p>Affordable modern portable cassette tape player &amp; recorder with 2 track, stereo playback. Good sound quality, plays all (I-IV) cassettes. Frequency response : 40Hz-11KHz (Type I), Signal-to-noise ratio 50dB, Distorsion 1%, Wow &amp; Flutter 0.3%, Headphone output power: 2x2 mW into 32 ohms</p></div></div><div id="2"><p><a href="https://walkman.land/modern/tomashi"><img src="https://walkman.land/public/img/modern/tomashi-1.jpg" loading="lazy" alt="TOMASHI feature"></a></p><div><p>Entry level portable cassette player. F116/F113</p></div></div><div id="1"><p><a href="https://walkman.land/modern/we-are-rewind"><img src="https://walkman.land/public/img/modern/we-are-rewind-cassette-player-serge.webp" loading="lazy" alt="We Are Rewind feature"></a></p><div><p>As you will have understood, this cassette player is the best of the best! The "crème de la crème" as they say in French. An object that is both cult and essential for any self-respecting music lover.</p></div></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Horses: AI progress is steady. Human equivalence is sudden (508 pts)]]></title>
            <link>https://andyljones.com/posts/horses.html</link>
            <guid>46199723</guid>
            <pubDate>Tue, 09 Dec 2025 00:26:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://andyljones.com/posts/horses.html">https://andyljones.com/posts/horses.html</a>, See on <a href="https://news.ycombinator.com/item?id=46199723">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
        
<p>So after all these hours talking about AI, in these last five minutes I am going to talk about: horses.</p>
<p><img src="https://andyljones.com/source/horses/horse_efficiency.png" alt="Engine efficiency over time, showing steady improvement"></p>
<p>Engines, steam engines, were invented in 1700.</p>
<p>And what followed was 200 years of steady improvement, with engines getting 20% better a decade.</p>
<p>For the first 120 years of that steady improvement, horses didn't notice at all.</p>
<p>Then, between 1930 and 1950, 90% of the horses in the US disappeared.</p>
<p>Progress in engines was steady. Equivalence to horses was sudden.</p>
<hr>

<p>But enough about horses. Let's talk about chess!</p>
<p><img src="https://andyljones.com/source/horses/chess.png" alt="Computer chess Elo over time, showing steady 50 point per year improvement"></p>
<p>Folks started tracking computer chess in 1985.</p>
<p>And for the next 40 years, computer chess would improve by 50 Elo per year.</p>
<p>That meant in 2000, a human grandmaster could expect to win 90% of their games against a computer.</p>
<p>But ten years later, the same human grandmaster would lose 90% of their games against a computer.</p>
<p>Progress in chess was steady. Equivalence to humans was sudden.</p>
<hr>

<p>Enough about chess! Let's talk about AI.</p>
<p><img src="https://andyljones.com/source/horses/project_cost.png" alt="AI datacenter capital expenditure over time"></p>
<p>Capital expenditure on AI has been pretty steady.</p>
<p>Right now we're - globally - spending the equivalent of 2% of US GDP on AI datacenters each year.</p>
<p>That number seems to have steadily been doubling over the past few years.</p>
<p>And it seems - according to the deals signed - likely to carry on doubling for the next few years.</p>
<hr>

<p>But from my perspective, from equivalence to me, it hasn't been steady at all.</p>
<p><img src="https://andyljones.com/source/horses/ask_claude.png" alt="Questions answered by humans vs Claude over time"></p>
<p>I was one of the first researchers hired at Anthropic.</p>
<p>This pink line, back in 2024, was a large part of my job. Answer technical questions for new hires.</p>
<p>Back then, me and other old-timers were answering about 4,000 new-hire questions a month.</p>
<p>Then in December, Claude finally got good enough to answer some of those questions for us.</p>
<p>In December, it was some of those questions. Six months later, 80% of the questions I'd been being asked had disappeared.</p>
<p>Claude, meanwhile, was now answering 30,000 questions a month; eight times as many questions as me &amp; mine ever did.</p>
<hr>

<p>Now. Answering those questions was only part of my job.</p>
<p>But while it took horses decades to be overcome, and chess masters years, it took me all of six months to be surpassed.</p>
<p><img src="https://andyljones.com/source/horses/per_token_cost.png" alt="Cost per million words: AI researcher vs subsistence farmer vs Sonnet"></p>
<p>Surpassed by a system that costs one thousand times less than I do.</p>
<p>A system that costs less, per word thought or written, than it'd cost to hire the cheapest human labor on the face of the planet.</p>
<hr>

<p>And so I find myself thinking a lot about horses, nowadays.</p>
<p><img src="https://andyljones.com/source/horses/horse_car.png" alt="Horses vs cars in the United States, with 'me' marked at 1920"></p>
<p>In 1920, there were 25 million horses in the United States, 25 million horses totally ambivalent to two hundred years of progress in mechanical engines.</p>
<p>And not very long after, 93 per cent of those horses had disappeared.</p>
<p>I very much hope we'll get the two decades that horses did.</p>
<p>But looking at how fast Claude is automating my job, I think we're getting a lot less.</p>
<hr>

<p><em>This was a five-minute lightning talk given over the summer of 2025 to round out a small workshop.</em></p>
<p><em>All opinions are my own and not those of my employer.</em></p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The universal weight subspace hypothesis (337 pts)]]></title>
            <link>https://arxiv.org/abs/2512.05117</link>
            <guid>46199623</guid>
            <pubDate>Tue, 09 Dec 2025 00:16:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2512.05117">https://arxiv.org/abs/2512.05117</a>, See on <a href="https://news.ycombinator.com/item?id=46199623">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2512.05117">View PDF</a>
    <a href="https://arxiv.org/html/2512.05117v2">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Prakhar Kaushik [<a href="https://arxiv.org/show-email/784dfcb8/2512.05117" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2512.05117v1" rel="nofollow">[v1]</a></strong>
        Thu, 4 Dec 2025 18:59:58 UTC (14,316 KB)<br>
    <strong>[v2]</strong>
        Sat, 6 Dec 2025 04:42:07 UTC (14,321 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Manual: Spaces (102 pts)]]></title>
            <link>https://type.today/en/journal/spaces</link>
            <guid>46199530</guid>
            <pubDate>Tue, 09 Dec 2025 00:07:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://type.today/en/journal/spaces">https://type.today/en/journal/spaces</a>, See on <a href="https://news.ycombinator.com/item?id=46199530">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p>Space (whitespace) is&nbsp;a&nbsp;whole group of&nbsp;glyphs, one of&nbsp;the most important and frequently-used. Any computer user knows space as&nbsp;the widest key on&nbsp;their keyboard, however the notion itself is&nbsp;much bigger and comprises multiple important typographic terms and ideas.</p>

<p>Space in&nbsp;general is&nbsp;a&nbsp;blank unprinted area, a&nbsp;counterform that separates letters, words, lines etc. In&nbsp;typography, there are several types of&nbsp;spaces: sinkage (space on&nbsp;a&nbsp;page above a&nbsp;textblock), indent (space before the paragraph), leading (vertical space), word spacing, and letter spacing. In&nbsp;this article, we&nbsp;will primarily focus on&nbsp;word spacing, i.e. the space as&nbsp;a&nbsp;glyph.</p>

<p>European languages did not use word spacing for a&nbsp;long time, it&nbsp;was not until the 7th century that word spacing entered Latin script. In&nbsp;the age of&nbsp;metal type, the space was a&nbsp;material, tangible object — a&nbsp;piece of&nbsp;metal that left no&nbsp;print. In&nbsp;the pre-digital era, most text blocks were justified, which required several spaces of&nbsp;different width. Those types of&nbsp;spacing were defined by&nbsp;the notion of&nbsp;<em>em</em>&nbsp;(or&nbsp;<em>point size</em>), which is <a href="">height of&nbsp;the piece of&nbsp;metal
<span><span><i></i><img src="https://cdn-gc.type.today/storage/post_image/4/9/4945/image-vLazqd07deKA_g8IX7MUyqR6zQfEGy3j3A.jpg" alt="litera"><p>Diagram of a cast metal sort, <b>c</b> is point size</p></span></span></a> used for printing a&nbsp;character. For example, one em&nbsp;in&nbsp;a&nbsp;12-point typeface is&nbsp;12&nbsp;points, whereas its en&nbsp;(half-em) spaces’ width is&nbsp;6pt, third space (of&nbsp;an&nbsp;em) equals 4pt, and so&nbsp;on.</p>



<p><img src="https://cdn-gc.type.today/storage/post_image/4/9/4944/image-cEkXYUq-LtcyxBKQNvrwxlQpmTTlCX5n4g.jpg" alt="1"></p>

<p><em>Whitespace characters in&nbsp;<a href="https://type.today/en/collection/gauge" target="_blank">Gauge</a>. Widths and correlations between spaces differ depending on&nbsp;the typeface</em></p>



<p>These types of&nbsp;spaces are still existent in&nbsp;the digital age, but they are mostly used by&nbsp;advanced typographers. Messengers, text editors, and other programs and applications most typically use only regular space.</p>



<h2 id="word-space">Word space</h2>

<p>Standard space, word space, space <em>per se,</em> is&nbsp;the symbol typed using the widest key on&nbsp;the keyboard.</p>

<p>In&nbsp;metal type, the size of&nbsp;standard space varied depending on&nbsp;the typographic tradition, in&nbsp;most cases the space was rather wide.</p>

<p>As&nbsp;a&nbsp;standard word space, metal composition used an&nbsp;en&nbsp;space, half the height of&nbsp;the point size, or&nbsp;em-square (in&nbsp;Cyrillic typography), while Latin space was equal to&nbsp;the third of&nbsp;the em&nbsp;space.

<a href="https://yadi.sk/i/oy19NzNjydZjR" target="_blank">Living Typography (2012)</a></p>

<p>In&nbsp;the early digitised fonts one often sees excessively wide spaces; probably, it&nbsp;was an&nbsp;attempt to&nbsp;imitate en&nbsp;space, or&nbsp;three-per-em space, which were used as&nbsp;the main spacing material in&nbsp;metal type. Such a&nbsp;space width can affect the typesetting rhythm and would seem redundant in&nbsp;modern typography.</p>

<p>Wide spacing is&nbsp;both physiologically unnecessary and makes the whole typeset structure reticulate, aesthetically ruining the page’s layout. If&nbsp;for some reason you can’t stick to&nbsp;en&nbsp;space size in&nbsp;this particular line, it’s better to&nbsp;scale down spacing using three-per-em spaces (that equal to&nbsp;the third of&nbsp;an&nbsp;em), or&nbsp;spaces of&nbsp;3, or&nbsp;even 2&nbsp;points. 
<strong>M. I. Schelkunov</strong>
<a href="https://www.booksite.ru/fulltext/shelkunov/index.htm" target="_blank">History, Technique, Art of&nbsp;Printing (1926)</a></p>



<p><img src="https://cdn-gc.type.today/storage/post_image/4/9/4936/image-Nwd6Y000QF7gL_FgJ_-dlDT-SjOgH93OhA.jpg" alt="2"></p>

<p><em>A wide word spacing seems weird to&nbsp;an&nbsp;eye of&nbsp;the modern reader, and it&nbsp;is&nbsp;way too visible in&nbsp;texts</em></p>



<p>Today, word space width is&nbsp;specified by&nbsp;the typeface’s designer themselves, and it&nbsp;is&nbsp;one of&nbsp;the defining moments in&nbsp;designing a&nbsp;typeface, along with spacing, — texture and rhythm of&nbsp;the typeset are heavily dependent on&nbsp;word space width.</p>

<p>Many modern typographers are seeking to&nbsp;subject the space width to&nbsp;certain rules. For example, some type designers claim that the space should be&nbsp;equal to&nbsp;the bounding box of&nbsp;lowercase letter&nbsp;<strong>i</strong>. However, this rule can’t be&nbsp;universal: specifically, it&nbsp;definitely won’t work for typefaces where letter <strong>i</strong>&nbsp;is&nbsp;of&nbsp;unconventional design and proportions. In&nbsp;super large point sizes, spacing and word spaces are often intentionally reduced, as&nbsp;in&nbsp;such cases even the bounding box of&nbsp;the <strong>i&nbsp;</strong>can be&nbsp;too wide.</p>

<p>It&nbsp;used to&nbsp;be&nbsp;a&nbsp;rule of&nbsp;thumb for headline settings to&nbsp;leave a&nbsp;space between words that is&nbsp;just wide enough to&nbsp;fit in&nbsp;a&nbsp;lowercase&nbsp;<strong>i</strong>. For comfortable reading of&nbsp;long lines, the space between words should be&nbsp;much wider.
<strong>Erik Spiekermann</strong>
<a href="https://www.oreilly.com/library/view/stop-stealing-sheep/9780133441147/#:~:text=Book%20description,type%20lovers%20around%20the%20globe." target="_blank">Stop stealing sheep &amp;&nbsp;find out how type works (1993)</a></p>

<p>Depending on&nbsp;whether your typeface is&nbsp;serif or&nbsp;sans serif, it&nbsp;makes sense to&nbsp;take, or&nbsp;not to&nbsp;take, in&nbsp;consideration sidebearings of&nbsp;the glyph. It&nbsp;can be&nbsp;very different depending on&nbsp;style, too: with wide and light weights, there will be&nbsp;more unprinted area than with narrow and heavy weights, and this also applies to&nbsp;the space width.</p>

<p>There is&nbsp;no&nbsp;question but that wordspaces may not be&nbsp;too large, or&nbsp;that the line must appear to&nbsp;be&nbsp;an&nbsp;even, well-balanced whole. What applies to&nbsp;letterspaces also applies to&nbsp;wordspaces: they too are a&nbsp;function of&nbsp;the counters of&nbsp;the individual letters: the smaller these are, the smaller the wordspaces; the larger the counters, the larger the wordspaces.
<strong>Jost Hochuli</strong>
<a href="https://www.typotheque.com/books/detail_in_typography" target="_blank">Detail in&nbsp;Typography (2008)</a></p>

<p>Blank space between words should be&nbsp;such as&nbsp;to&nbsp;ensure that words are visibly separated from each other — if&nbsp;spacing is&nbsp;wider, there will be&nbsp;holes between words, if&nbsp;smaller, it&nbsp;will be&nbsp;difficult to&nbsp;tell one word from another. You can’t measure space with a&nbsp;ruler, as&nbsp;everything depends on&nbsp;specific design or&nbsp;typeface.</p>



<p><img src="https://cdn-gc.type.today/storage/post_image/4/9/4937/image-u5k5jOGtni47nAP2erjKYKrYJ2NWnq802A.jpg" alt="3"><em>Word spaces as&nbsp;set in&nbsp;<a href="https://type.today/en/kazimir-text" target="_blank">Kazimir Text</a>. The space width is&nbsp;good: words are separated from one another, the hierarchy of&nbsp;white space is&nbsp;maintained</em>
<img src="https://cdn-gc.type.today/storage/post_image/4/9/4938/image-ll0APFBxq49FEGFriOkB3CZ9Gpyeae39SA.jpg" alt="4"><em>If you increase word spacing, word spaces would conflict with leading, which makes it&nbsp;hard to&nbsp;navigate through the text</em>
<img src="https://cdn-gc.type.today/storage/post_image/4/9/4939/image-kL9plG0GoFiDQaYRq6_bxrF9VZE86-CWVQ.jpg" alt="5"><em>If you decrease the width of&nbsp;word space, it&nbsp;will affect legibility, as&nbsp;the words will blend together</em></p>



<p>Using double spaces is&nbsp;a&nbsp;technique inherited from the age of&nbsp;typewriters. It&nbsp;is&nbsp;strongly advisable to&nbsp;check a&nbsp;document for double spaces and replace those by&nbsp;single spaces.</p>

<p>Some of&nbsp;the recommendations learned by&nbsp;the educated typist are still now acquired habits wrongly used in&nbsp;digital documents; for instance, the use of&nbsp;three spaces after a&nbsp;period or&nbsp;two after the comma. There was just one space width available in&nbsp;the typewriter, so&nbsp;words and sentences were separated by&nbsp;the same distance. The double space was used to&nbsp;differentiate sentences and improve the readability of&nbsp;the text. 
<strong>María Ramos Silva</strong>
<a href="https://www.academia.edu/15850041/Type_design_for_typewriters_Olivetti" target="_blank">Type design for typewriters: Olivetti (2015)</a></p>

<p>Additional spacing after a&nbsp;period is&nbsp;a&nbsp;questionable method in&nbsp;terms of&nbsp;readability. It&nbsp;can be&nbsp;assumed that in&nbsp;the age of&nbsp;typewriters additional space could have better separated sentences from one another in&nbsp;the context of&nbsp;monowidth typeface, yet monowidth period and space already form a&nbsp;larger gap than any space within the sentence. Since typewriters, typesetting tools have significantly improved over time, and today nobody will typeset in&nbsp;a&nbsp;monowidth typeface, unless it&nbsp;is&nbsp;absolutely necessary. So, currently, the use of&nbsp;double spaces is&nbsp;considered mauvais ton, i.e. bad manners, regardless of&nbsp;typeface.</p>

<p>American lawyer Matthew Butterick wrote a&nbsp;book on&nbsp;typography for lawyers, writers, and anyone who works with text. In&nbsp;the&nbsp;US, it&nbsp;is&nbsp;still very common among the older generation to&nbsp;use double spaces, so&nbsp;Matthew dedicated two entire chapters of&nbsp;his Practical Typography to&nbsp;this issue. Butterick tried to&nbsp;convince his audience by&nbsp;imaginary dialogues:</p>

<p>“If&nbsp;you approve of&nbsp;smaller word spaces in&nbsp;some situations, why do&nbsp;you insist on&nbsp;only one space between sentences, where a&nbsp;larger gap might be&nbsp;useful?” Because you’re already getting a&nbsp;larger gap. A&nbsp;sentence-ending word space typically appears next to&nbsp;a&nbsp;period. A&nbsp;period is&nbsp;mostly white space. So&nbsp;visually, the space at&nbsp;the end of&nbsp;a&nbsp;sentence already appears larger than a&nbsp;single word space. No&nbsp;need to&nbsp;add another.
<strong>Matthew Butterick</strong>
<a href="https://practicaltypography.com/" target="_blank">Butterick’s Practical Typography (2013)</a></p>



<h2 id="non-breaking-space">Non-breaking Space</h2>

<p>Non-breaking space is&nbsp;a&nbsp;space character that prevents an&nbsp;automatic line break at&nbsp;its position. For instance, in&nbsp;Russian and a&nbsp;number of&nbsp;other Central and Eastern European languages, non-breaking space serves to&nbsp;stick together a&nbsp;preposition and a&nbsp;word next to&nbsp;it, numbers and units of&nbsp;measurements, name and surname, etc.</p>

<p>Non-breaking space is&nbsp;supported by&nbsp;almost any text editing program, graphic design software, or&nbsp;browser, along with a&nbsp;standard space, so&nbsp;one shouldn’t forget to&nbsp;utilise it&nbsp;according to&nbsp;the typesetting rules of&nbsp;any given language.</p>

<p>In&nbsp;Russian language, non-breaking space shall connect the dash and its previous word (except for direct speech), prepositions with following words, initials with surname, abbreviations (such as&nbsp;i.e.), numero sign with numbers, numbers and units of&nbsp;measurements.</p>

<p>In&nbsp;English it&nbsp;is&nbsp;considered good manners to&nbsp;stick together not prepositions, but pronouns and articles with the following word. However, this rule is&nbsp;often neglected, especially when it&nbsp;comes to&nbsp;newspapers and magazines.</p>

<p>Professional typesetting software have spaces of&nbsp;non-standard widths. In&nbsp;InDesign, all additional spaces — em&nbsp;space, en&nbsp;space, thin space, etc. — are non-breaking.</p>



<h2 id="additional-spaces">Additional spaces</h2>

<p>Standard space is&nbsp;used everywhere; it&nbsp;is&nbsp;supported by&nbsp;any word, text, or&nbsp;code processing app. Non-breaking space is&nbsp;supported almost anywhere as&nbsp;well. However, computer typesetting still possesses a&nbsp;number of&nbsp;spaces dating back to&nbsp;metal type, allowing for finer adjustment of&nbsp;white space if&nbsp;necessary.</p>

<p>If&nbsp;a&nbsp;font supports additional spaces, those can be fetched via&nbsp;glyphs palette or&nbsp;using clipboard. Most graphic software do&nbsp;not support those spaces; for example, Adobe Illustrator 2020 includes only four additional spaces: em&nbsp;space, en&nbsp;space, thin space, and hair space.</p>

<p>And there is&nbsp;a&nbsp;reason for that: neither Illustrator, nor Photoshop were designed for advanced typesetting and laying out books. However, in&nbsp;InDesign you can easily set any kind of&nbsp;space, and a&nbsp;skilled typographer will use those.</p>



<h2 id="emspace">Em&nbsp;Space</h2>

<p>A&nbsp;space equal to&nbsp;the height of&nbsp;the em&nbsp;square (point size.) In&nbsp;early serifs, the metal face of&nbsp;the capital&nbsp;<strong>М</strong> tended to&nbsp;be&nbsp;square — probably, thus the English name. Metal type often used em&nbsp;space as paragraph indent.</p>



<h2 id="enspace">En&nbsp;Space</h2>

<p>Half of&nbsp;the width of&nbsp;an&nbsp;em. Russian-language metal type composition considered it&nbsp;the main type of&nbsp;space, even though in&nbsp;word spacing, especially if&nbsp;the text is&nbsp;aligned to&nbsp;the left or&nbsp;right, it&nbsp;is&nbsp;excessively wide.</p>



<h2 id="three-per-em-space-third-space">Three-per-em Space, Third Space</h2>

<p>One&nbsp;third of&nbsp;an&nbsp;em&nbsp;space. Historically considered as&nbsp;the main space in&nbsp;Western European typography.</p>

<p>The first obligation of&nbsp;a&nbsp;good typesetter is&nbsp;to&nbsp;achieve a&nbsp;compact line image, something best accomplished by&nbsp;using three-to-em or&nbsp;three-space word spacing. In&nbsp;former times even roman was set much tighter than we&nbsp;do&nbsp;it&nbsp;today; the specimen sheet that contains the original of&nbsp;Garamond’s roman of&nbsp;1592, printed in&nbsp;14-point, shows a&nbsp;word spacing in&nbsp;all lines of&nbsp;2&nbsp;points only, which is&nbsp;one-seventh of&nbsp;an&nbsp;em! This means that we&nbsp;cannot call three-to-em word spacing particularly tight. 
<strong>Jan Tschichold</strong>
<a href="https://vsip.info/qdownload/the-form-of-the-book-essays-on-the-mora-jan-tschichold-pdf-free.html" target="_blank">The Form Of&nbsp;The Book (1975)</a></p>



<h2 id="quarter-space">Quarter Space</h2>

<p>One fourth of&nbsp;an&nbsp;em&nbsp;space. Some authors believe quarter space to&nbsp;be&nbsp;the primary word space.</p>

<p>For a&nbsp;normal text face in&nbsp;a&nbsp;normal text size, a&nbsp;typical value for the word space is&nbsp;a&nbsp;quarter of&nbsp;an&nbsp;em, which can be&nbsp;written M/4. (A&nbsp;quarter of&nbsp;an&nbsp;em&nbsp;is&nbsp;typically about the same&nbsp;as, or&nbsp;slightly more than, the set-width of&nbsp;the letter&nbsp;t.)
<strong>Robert Bringhurst</strong>
<a href="https://www.amazon.com/Elements-Typographic-Style-Robert-Bringhurst/dp/0881791326" target="_blank">The Elements of&nbsp;Typographic Style (1992)</a></p>



<h2 id="thin-space">Thin Space</h2>

<p>⅕ of&nbsp;an&nbsp;em&nbsp;space. It&nbsp;is&nbsp;common that thin space equals about half the standard one, which is&nbsp;why thin space is&nbsp;used where standard word space would be&nbsp;too wide. For example, thin space is&nbsp;often utilised for spacing a&nbsp;dash in&nbsp;cases where standard space is&nbsp;too wide. Thin space is&nbsp;also used for spacing initials, from each other and from the surname:</p>



<p><img src="https://cdn-gc.type.today/storage/post_image/4/9/4940/image-SO7pERcEMDmGd2OHkjQ4CnsLs1Y3edduEw.jpg" alt="6"><em>Standard space in&nbsp;<a href="https://type.today/en/spectral" target="_blank">Spectral</a> is&nbsp;too wide to&nbsp;be&nbsp;used for spacing initials and dashes</em>
<img src="https://cdn-gc.type.today/storage/post_image/4/9/4941/image-O4eNbawExg11Uyp03oXkekBsZyEgHfZJGw.jpg" alt="7"><em>Thin spaces look more neat, better connecting initials with a&nbsp;surname and two parts of&nbsp;a&nbsp;sentence with each other</em></p>



<p>French typographic tradition prescribes the use of&nbsp;either thin or&nbsp;hair spaces to&nbsp;space any two-part symbols: exclamation mark, question mark, semicolon, etc.</p>

<p>Regardless of&nbsp;the language, such glyphs as&nbsp;question mark and exclamation mark typically are very visible in&nbsp;lowercase, but they can get lost in&nbsp;an&nbsp;all-caps typeset — in&nbsp;this case, one should finely space them.</p>



<h2 id="sixth-space">Sixth Space</h2>

<p>The sixth space is&nbsp;used when the thin space is&nbsp;too large.</p>



<h2 id="hair-space">Hair Space</h2>

<p>The narrowest of spaces. In&nbsp;metal type, it&nbsp;was equal to&nbsp;1/10 of&nbsp;an&nbsp;em&nbsp;space, in&nbsp;the digital age it&nbsp;is&nbsp;mostly 1/ 24 of&nbsp;an&nbsp;em. It&nbsp;might be&nbsp;useful if&nbsp;a&nbsp;certain typeface’s punctuation marks have too tight sidebearings, but a&nbsp;thin space would be&nbsp;too wide. For example, you can use hair space to&nbsp;space dashes instead of&nbsp;thin one — everything depends on&nbsp;the sidebearings and the design of&nbsp;the particular typeface.</p>

<p>You should keep in&nbsp;mind that after you change font, selected space glyphs will remain, but their width can change, — and this will affect the texture.</p>

<p>Isn’t it&nbsp;ridiculous when a&nbsp;punctuation mark, relating to&nbsp;the entire preceding phrase, is&nbsp;tied to&nbsp;one last word of&nbsp;the said phrase? And, vice versa, how unfortunately it&nbsp;looks when there is&nbsp;a&nbsp;large gap between this mark and the previous word. As&nbsp;a&nbsp;matter of&nbsp;fact, it&nbsp;is&nbsp;about time type foundry workers started thinking about it&nbsp;and cast the punctuation mark with an&nbsp;extra sidebearing on&nbsp;its left. However, typefounders are not always, or&nbsp;rather rarely, that forethoughtful, and also they are used to&nbsp;cast all letters without generous sidebearings. During punching of&nbsp;matrices, the beauty of&nbsp;spacing punctuation marks is&nbsp;also barely remembered. Therefore, it&nbsp;is&nbsp;your burden and responsibility to&nbsp;fix this <nobr>problem — </nobr>and even more it&nbsp;is&nbsp;the one of&nbsp;compositors. These latter dislike 1-pt spaces, however it&nbsp;is&nbsp;this very thin space that can save the typeset beauty in&nbsp;these situations. That is&nbsp;why, with punctuation marks <strong>, ;. … : ! ?</strong> you should insist on&nbsp;putting 1-pt (hair) space before those symbols — but only when those don’t have an&nbsp;extra sidebearing on&nbsp;their left. If&nbsp;you are in&nbsp;charge of&nbsp;purchasing a&nbsp;typeface for the printing establishment, regard this issue when ordering typefaces, make the foundry give consideration to&nbsp;the beauty of&nbsp;their work and this particular detail. 
<strong>M. I. Schelkunov</strong>
<a href="https://www.booksite.ru/fulltext/shelkunov/index.htm" target="_blank">History, Technique, Art of&nbsp;Printing (1926)</a></p>



<h2 id="spacing-injustified-texts">Spacing in&nbsp;justified texts</h2>

<p>Full justification — that&nbsp;is, alignment of&nbsp;text to&nbsp;its both margins, — is&nbsp;still commonly used in&nbsp;books and magazines. When the text is&nbsp;justified, the width of&nbsp;word spaces is&nbsp;not constant, it&nbsp;is&nbsp;changing to&nbsp;distribute words to&nbsp;the entire width of&nbsp;the line. In&nbsp;this situation, the uniformity of&nbsp;spacing could be&nbsp;even more important than the very width of&nbsp;these spaces: evenly large spaces in&nbsp;the entire page are better than large spaces in&nbsp;only one line. That is&nbsp;why, no&nbsp;matter how optimised the typeface’s word spacing in&nbsp;terms of&nbsp;its width&nbsp;is, it&nbsp;will not be&nbsp;enough for typesetting a&nbsp;justified text. While in&nbsp;metal type all spaces were set manually, and a&nbsp;typesetter knew what space they should add for even typesetting, nowadays it’s a&nbsp;computer that defines the length of&nbsp;spaces for justified texts. The algorithm divides the remaining space into equal parts and adds them to&nbsp;regular spaces. In&nbsp;doing&nbsp;so, the algorithm ignores letters, syntax, and punctuation, which is&nbsp;why when typesetting justified texts one should always double-check and adjust spacing manually.</p>

<p>In&nbsp;Indesign, it&nbsp;is&nbsp;possible to&nbsp;set minimum and maximum word spacing width for fully justified text typesetting: the width of&nbsp;standard space is&nbsp;used as&nbsp;a&nbsp;basis 100 %, maximum is&nbsp;normally about 120 %, minimum is&nbsp;about 80 %.</p>

<p>If&nbsp;the text is&nbsp;justified, a&nbsp;reasonable minimum word space is&nbsp;a&nbsp;fifth of&nbsp;an&nbsp;em (M/5), and M/4&nbsp;is a&nbsp;good average to&nbsp;aim for. A&nbsp;reasonable maximum in&nbsp;justified text is&nbsp;M/2. If&nbsp;it&nbsp;can be&nbsp;held to&nbsp;M/3, so&nbsp;much the better. But for loosely fitted faces, or&nbsp;text set in&nbsp;a&nbsp;small size, M/3&nbsp;is often a&nbsp;better average to&nbsp;aim for, and a&nbsp;better minimum is&nbsp;M/4. In&nbsp;a&nbsp;line of&nbsp;widely letterspaced capitals, a&nbsp;word space of&nbsp;M/2&nbsp;or more may be&nbsp;required.
<strong>Robert Bringhurst</strong>
<a href="https://www.amazon.com/Elements-Typographic-Style-Robert-Bringhurst/dp/0881791326" target="_blank">The Elements of&nbsp;Typographic Style (1992)</a></p>

<p>Robert Bringhurst recommends choosing appropriate spaces based on&nbsp;an&nbsp;em. However, space is&nbsp;a&nbsp;relative value, so&nbsp;in&nbsp;justified texts you should consider not the width of&nbsp;some abstract&nbsp;em, but rather the width of&nbsp;space in&nbsp;particular font.</p>

<p>The optimal word space width in&nbsp;justified texts is&nbsp;ephemeral and changes depending on&nbsp;typeface, point size, line width, line spacing, and many other factors. That is&nbsp;why in&nbsp;Indesign you can’t set maximum and minimum values once and for all cases — you will have to&nbsp;choose the best possible options manually.</p>

<p>In&nbsp;setting justified texts, standard word space width becomes a&nbsp;fluctuating value. The fixed width space and all additional spaces with constant width can help better control the setting.</p>

<p>The more even are the gaps between words, the better &lt;…&gt;. In&nbsp;no&nbsp;case shall you allow a&nbsp;considerable disparity in&nbsp;space widths, while an&nbsp;insignificant difference won’t ruin the beauty of&nbsp;typesetting. 
<strong>Pyotr Kolomnin</strong>
<a href="https://www.artlebedev.com/izdal/kratkie-svedenia-po-tipografskomu-delu/" target="_blank">A Concise Account of Typography (1899)</a></p>



<h2 id="figure-space">Figure Space</h2>

<p>Figure space, or&nbsp;numeric space, is&nbsp;used for typesetting tables and sheets. If&nbsp;a&nbsp;typeface is&nbsp;fitted with tabular figures, its figure space will be&nbsp;equal to&nbsp;the width of&nbsp;tabular figures. Figure space is&nbsp;a&nbsp;non-breaking one.</p>



<p><img src="https://cdn-gc.type.today/storage/post_image/4/9/4942/image-ixsGF0udPbg8EuRFjq0Zgx_jWvrvwa6-4g.jpg" alt="8"></p>

<p><em>Normally, figure space is&nbsp;significantly wider than standard space, it&nbsp;will be&nbsp;helpful when you need to&nbsp;even a&nbsp;large amount of&nbsp;multi-digit numbers</em></p>



<h2 id="punctuation-space">Punctuation Space</h2>

<p>In&nbsp;most cases, the width of&nbsp;this space is&nbsp;equal to&nbsp;the glyph width of&nbsp;a&nbsp;period or&nbsp;a&nbsp;colon. May be&nbsp;of&nbsp;use in&nbsp;making up&nbsp;numbers in&nbsp;tables where digits are defined by&nbsp;a&nbsp;spacing element instead of&nbsp;period or&nbsp;colon.</p>



<h2 id="narrow-no-break-space">Narrow No-break Space</h2>

<p>A&nbsp;thin space that prevents an&nbsp;automatic line break. The name of&nbsp;this symbol in&nbsp;Unicode causes additional confusion: Narrow in&nbsp;this case is&nbsp;the same thing as&nbsp;Thin, and Narrow Space has the same width as&nbsp;Thin Space does.</p>

<p>In&nbsp;some applications, such as&nbsp;InDesign, the simple regular thin space is&nbsp;non-breaking by&nbsp;default and is&nbsp;called with Thin Space. In&nbsp;other cases it’s a&nbsp;separate symbol, for example, the Web uses Narrow No-break Space.</p>



<h2 id="spaces-inlayout">Spaces in&nbsp;layout</h2>

<p>The distribution of&nbsp;white space in&nbsp;text setting is&nbsp;a&nbsp;highly important factor, responsible for the neat design and the content’s clear structure. Many designers keep in&nbsp;mind correlation between point size, line width, and margins, but some tend to&nbsp;forget that word spacing is&nbsp;an&nbsp;equivalent factor of&nbsp;these relations.</p>

<p>Body text font, designed for smaller sizes, would require smaller spacing and word spaces if&nbsp;used&nbsp;to&nbsp;set a&nbsp;large headline. The point size gets more important in&nbsp;determining spacing and white unprinted area in&nbsp;general, than whether it&nbsp;is&nbsp;a&nbsp;text typeface or&nbsp;a&nbsp;display one.</p>

<p>It&nbsp;is&nbsp;also necessary to&nbsp;consider spacing when you’re dealing with particular elements of&nbsp;the text. For instance, small-caps or&nbsp;all-caps fragments quite often should be&nbsp;additionally spaced. Manual spacing is&nbsp;sometimes necessary in&nbsp;bold or&nbsp;italic styles, or&nbsp;even if&nbsp;no&nbsp;additional styles are applied at&nbsp;all.</p>



<p><img src="https://cdn-gc.type.today/storage/post_image/4/9/4943/image-bWnqFiPT4kFilwqQzO86EXG-y1UQreQkvg.jpg" alt="9"><em>Small-caps spacing in&nbsp;Charter is&nbsp;too tight by&nbsp;default, more white space is&nbsp;needed</em>
<img src="https://cdn-gc.type.today/storage/post_image/4/9/4930/image-nZES_ulF6olUaArz0JgEItsLfK1bUW8reQ.jpg" alt="10"><em>In&nbsp;William, small caps are taken care&nbsp;of, this generous spacing doesn’t require additional adjustment</em>
<img src="https://cdn-gc.type.today/storage/post_image/4/9/4931/image-8n39JCTmGVfH-tOQ3YHTKPl5wAiVC9GQ3A.jpg" alt="11"><em>A text set in a quality typeface sometimes needs manual adjustment: standard word space in Guyot is clearly not enough for the <strong>of ‘i’</strong> combination</em></p>



<h2 id="white-spaces-insoftware">White spaces in&nbsp;software</h2>

<p>Most typically, in&nbsp;non-professional software and web services there are only standard and non-breaking spaces available. You might be&nbsp;able to&nbsp;set additional symbols using clipboard almost anywhere where Unicode is&nbsp;supported. That said, you have to&nbsp;check everytime: for example, at&nbsp;the time of&nbsp;writing this piece, Facebook allows for inserting additional symbols in&nbsp;its input field, but automatically replaces them while posting.</p>

<p>Speaking of&nbsp;the Web, additional spaces are available as HTML special characters: if you use them, your source code might become a&nbsp;bit cluttered, but that would allow you to&nbsp;control the placing of&nbsp;each non-standard space. Please note that different browsers might render spacing differently, and not so&nbsp;long ago some of&nbsp;them even ignored additional spaces, replacing them by&nbsp;regular ones. You should check on&nbsp;the correct display of&nbsp;additional spaces where you use&nbsp;it.</p>

<p>Two industry standards for text formatting and typesetting, InDesign and Quark Xpress, support all kinds of&nbsp;spaces. Today, type designers usually include at&nbsp;least thin and hair spaces. Their width might vary from one typeface to&nbsp;another — but the typographer, at&nbsp;least, has more control over the word spacing.</p>

<p>In&nbsp;InDesign, an&nbsp;additional space not included in&nbsp;the typeface would still be&nbsp;visible, but its width would be&nbsp;defined by&nbsp;the software with no&nbsp;regard to&nbsp;what kind of&nbsp;typeface it&nbsp;is. For example, hair space in&nbsp;24pt size will be&nbsp;1pt — both in&nbsp;a&nbsp;display face with tight spacing and in&nbsp;a&nbsp;text face with loose spacing.</p>

<p>Spaces calculated this way are not always suitable for your task. Depending on&nbsp;the typeface, the additional space width suggested by&nbsp;InDesign can be&nbsp;insufficient or&nbsp;excessive. And if&nbsp;you export the text with such spaces from InDesign to&nbsp;Figma, their width will most likely change — every software may have its own algorithms for calculating these values.</p>

<p>Be&nbsp;vigilant and trust your eye: it&nbsp;is&nbsp;not mathematical values that matter, but a&nbsp;convincing, reasonable relationship between the black and the white.</p>



<p><img src="https://cdn-gc.type.today/storage/post_image/4/9/4932/image-VJyk23r8W5PPMYzUewwEftzlS-XOHvl8pg.jpg" alt="12"><em>These dashes are&nbsp;spaced by&nbsp;hair spaces provided by&nbsp;the typeface</em>
<img src="https://cdn-gc.type.today/storage/post_image/4/9/4933/image-gHIrQL1y3cIiC1wGLT6xYHssXfu4Mbempg.jpg" alt="13"><em>These dashes are&nbsp;spaced by&nbsp;hair spaces provided by&nbsp;the typeface</em>
<img src="https://cdn-gc.type.today/storage/post_image/4/9/4934/image-DMdIJ22feFxvlaZMj2Q6bSTcrElPX4rTcw.jpg" alt="14"><em>The typefaces above have no&nbsp;hair space, therefore its width is&nbsp;set automatically</em>
<img src="https://cdn-gc.type.today/storage/post_image/4/9/4935/image-lDf82atd1bEftsEmKGUXOb4Cgm3q2USI-w.jpg" alt="15"><em>With x-height and spacing that Arno Pro and RIA Text have, the InDesign’s hair space is&nbsp;good enough. Whereas in&nbsp;IBM Plex we&nbsp;perhaps should put thin space instead of&nbsp;a&nbsp;hair one</em></p>



<p>Whitespace characters are among&nbsp;the most important typographic elements. Alongside sidebearings, they define text rhythm and organise blocks of&nbsp;information. Disregard for white spaces can ruin relations between them: line and word spacing, word spacing and column-gap. In&nbsp;such case the reader wouldn’t be&nbsp;able to&nbsp;easily track the line and would have to&nbsp;put additional <nobr>effort — </nobr>unless this is&nbsp;your intended goal, you should always consider how different sorts of&nbsp;white space work with each other.</p>



<h2 id="summary-table">Summary table</h2>

<table>
<tbody><tr>
    <td>Non-breaking space</td>
    <td>MacOS: Alt + Space<br>
        Windows: Alt+0160<br>
        Unicode: <a href="http://decodeunicode.org/en/u+00A0">U00A0</a><br>
        HTML: &amp;nbsp;<p>
        Indesign: Type → Insert White Space → Nonbreaking Space or Alt + Cmnd + X
        <br>
        in case you need a space of non-changing width, in a justified text layout:
        <br>
        Type → Insert White Space → Nonbreaking Space (Fixed Width)
    </p></td>
</tr>
<tr>
    <td>Thin space</td>
    <td>Unicode: U2009<br>
        HTML: &amp;ThinSpace;<p>
        Indesign: Type → Insert White Space → Thin Space
        <br>
        or
        <br>
        Shift + Alt + Cmnd + M
    </p></td>
</tr>
<tr>
    <td>Thin non-breaking space (for Web)</td>
    <td>Unicode: U202F<br>
        HTML: &amp;#8239;
    </td>
</tr>
<tr>
    <td>Em&nbsp;space</td>
    <td>Unicode: U2003<br>
        HTML: &amp;emsp;<p>
        Indesign: Type → Insert White Space → Em&nbsp;Space
    </p></td>
</tr>
<tr>
    <td>En&nbsp;space</td>
    <td>Unicode: U2002<br>
        HTML: &amp;ensp;<p>
        Indesign: Type → Insert White Space → En&nbsp;Space
    </p></td>
</tr>
<tr>
    <td>Third space</td>
    <td>Unicode: U2004<br>
        HTML: &amp;emsp13;<p>
        Indesign: Type → Insert White Space → Third Space
    </p></td>
</tr>
<tr>
    <td>Quarter space</td>
    <td>Unicode: U2005<br>
        HTML: &amp;emsp14;<p>
        Indesign: Type → Insert White Space → Quarter Space
    </p></td>
</tr>
<tr>
    <td>Sixth space</td>
    <td>Unicode: U2006<br>
        HTML: &amp;#8198;<p>
        Indesign: Type → Insert White Space → Sixth Space
    </p></td>
</tr>
<tr>
    <td>Hair space</td>
    <td>Unicode: U200A<br>
        HTML: &amp;hairsp;<p>
        Indesign: Type → Insert White Space → Hair Space
    </p></td>
</tr>
<tr>
    <td>Figure space</td>
    <td>Unicode: U2007<br>
        HTML: &amp;numsp;<p>
        Indesign: Type → Insert White Space → Figure Space
    </p></td>
</tr>
<tr>
    <td>Punctuation space</td>
    <td>Unicode: U2008<br>
        HTML: &amp;puncsp;<p>
        Indesign: Type → Insert White Space → Punctuation Space
    </p></td>
</tr>
</tbody></table>



<h2 id="references">References</h2>

<h3 id="in-english">In English</h3>

<p>Kirill Belyayev, <a href="https://kirillbelyaev.com/s/" target="_blank">Whitespaces and zero width characters with buttons for copying to clipboard, short mnemonics and usage comments</a>
<br>
Robert Bringhurst, <a href="https://www.amazon.com/Elements-Typographic-Style-Robert-Bringhurst/dp/0881791326" target="_blank">The Elements of&nbsp;Typographic Style</a>
<br>
Matthew Butterick, <a href="https://practicaltypography.com/word-spaces.html" target="_blank">Butterick’s Practical Typography</a>
<br>
Jost Hochuli, <a href="https://www.typotheque.com/books/detail_in_typography" target="_blank">Detail in&nbsp;Typography</a>
<br>
Yves Peters, <a href="https://www.fontshop.com/content/adventures-in-space_spaces" target="_blank">Adventures in Space (fontshop.com)</a>
<br>
María Ramos Silva, <a href="https://www.academia.edu/15850041/Type_design_for_typewriters_Olivetti" target="_blank">Type design for typewriters: Olivetti</a>
<br>
Erik Spiekermann, <a href="https://www.oreilly.com/library/view/stop-stealing-sheep/9780133441147/#:~:text=Book%20description,type%20lovers%20around%20the%20globe." target="_blank">Stop stealing sheep &amp;&nbsp;find out how type works</a>
<br>
Jan Tschichold, <a href="https://vsip.info/qdownload/the-form-of-the-book-essays-on-the-mora-jan-tschichold-pdf-free.html" target="_blank">The Form Of&nbsp;The Book</a>
<br>
Martin Wichary, <a href="https://www.smashingmagazine.com/2015/10/space-yourself/" target="_blank">Space Yourself (smashingmagazine.com)</a>
<br></p>



<h3 id="in-russian">In Russian</h3>

<p>Pyotr Kolomnin, <a href="https://www.artlebedev.com/izdal/kratkie-svedenia-po-tipografskomu-delu/" target="_blank">A Concise Account of Typography</a>
<br>
Alexandra Korolkova, <a href="https://yadi.sk/i/oy19NzNjydZjR" target="_blank">Living Typography</a>
<br>
M. I. Schelkunov, <a href="https://www.booksite.ru/fulltext/shelkunov/index.htm" target="_blank">History, Technique, Art of&nbsp;Printing</a>
<br>
Alexei Yozhikov, <a href="https://habr.com/ru/post/23250/" target="_blank">(Nearly) Everything You Need To Know About Whitespace (habr.com)</a>
<br></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kroger acknowledges that its bet on robotics went too far (232 pts)]]></title>
            <link>https://www.grocerydive.com/news/kroger-ocado-close-automated-fulfillment-centers-robotics-grocery-ecommerce/805931/</link>
            <guid>46199411</guid>
            <pubDate>Mon, 08 Dec 2025 23:53:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.grocerydive.com/news/kroger-ocado-close-automated-fulfillment-centers-robotics-grocery-ecommerce/805931/">https://www.grocerydive.com/news/kroger-ocado-close-automated-fulfillment-centers-robotics-grocery-ecommerce/805931/</a>, See on <a href="https://news.ycombinator.com/item?id=46199411">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        


<div>
        <p>
            This audio is auto-generated. Please let us know if you have <a href="https://www.grocerydive.com/contact/">feedback</a>.
        </p>
    </div>


                        

<p>Kroger’s <a href="https://www.grocerydive.com/news/kroger-ecommerce-profitability-400M-ocado-automated-fulfillmnet-centers-delivery/805781/">announcement on Tuesday</a> that it will shutter three of its robotic e-commerce fulfillment facilities represents a sharp turnabout for the grocery company, which until recently had expressed confidence in its ability to leverage automation to run a profitable online grocery business.</p>
<p>Less than a year ago, Kroger said it <a href="https://www.grocerydive.com/news/kroger-and-ocado-plan-to-open-2-more-automated-fulfillment-centers/741254/">planned to expand</a> the fleet of high-tech fulfillment centers it has been developing in partnership with U.K.-based warehouse automation company Ocado. And in mid-2024, Kroger revealed that it would <a href="https://www.grocerydive.com/news/kroger-ocado-technology-automated-fulfillment-centers/721986/">install new technology</a> from Ocado to improve the efficiency of the warehouses.</p>



<p>When Kroger <a href="https://www.grocerydive.com/news/grocery--kroger-partners-with-ocado-in-a-bet-on-the-future-of-online-grocery/534008/">launched its partnership with Ocado</a>, the company “believed in the relentless drive to innovate way ahead of the market in order to delight our customers and advance our position as one of America’s leading e-commerce companies,” former Kroger CEO Rodney McMullen <a href="https://www.youtube.com/watch?v=E4kH-atEpd4">said in a video</a> about improvements to its equipment that the automation company announced last year.</p>
<p>However, Kroger’s projected confidence came even as it was questioning whether the Ocado network was living up to expectations.</p>
<p>Kroger revealed in September 2023 that it had decided to <a href="https://www.grocerydive.com/news/kroger-ocado-grocery-cfc-delivery-pickup/693287/">pause development of the Ocado project</a> as it waited to see if sites it had already started operating would meet performance benchmarks.</p>
<p>In a further sign that its strategy was faltering, Kroger announced last March it would <a href="https://www.grocerydive.com/news/kroger-ocado-closing-3-e-commerce-fulfillment-facilities/711583/">close three spoke facilities</a> that worked in tandem with several of its robotic centers, with a spokesperson noting that the facilities “did not meet the benchmarks we set for success.”</p>
<p>By September 2025, it was clear that depending on automation as the foundation of a money-making grocery delivery business was probably not going to pan out for Kroger. Speaking during an earnings call, interim Kroger CEO Ron Sargent — who <a href="https://www.grocerydive.com/news/kroger-ceo-rodney-mcmullen-resigns-ethics-probe/741345/">took over in March</a> after McMullen’s sudden departure following an ethics probe — said the company would <a href="https://www.supplychaindive.com/news/kroger-is-reviewing-its-automated-e-commerce-fulfillment-network/759926/">conduct a “full site-by-site analysis</a>” of the Ocado network.</p>
<p>Sargent also said Kroger would refocus its e-commerce efforts on its fleet of more than 2,700 grocery supermarkets because it believed that its stores gave it a way to “reach new customer segments and expand rapid delivery capabilities without significant capital investments.”</p>


<p>Kroger said on Tuesday that its decision to close the three robotic facilities, along with other adjustments to its e-commerce operations, would provide a $400 million boost as it looks to improve e-commerce profitability. But the course-correction will be expensive, forcing Kroger to incur charges of about $2.6 billion.</p>
<p>Ken Fenyo, a former Kroger executive who now advises retailers on technology as managing partner of Pine Street Advisors, said the changes Kroger is making reflect the broader reality that grocery e-commerce has not reached the levels the industry had predicted when the COVID-19 pandemic supercharged digital sales five years ago.</p>
<p>Fenyo added that Kroger’s decision to locate the Ocado centers outside of cities turned out to be a key flaw.</p>
<p>“Ultimately those were hard places to make this model work,” said Fenyo. “You didn’t have enough people ordering, and you had a fair amount of distance to drive to get the orders to them. And so ultimately, these large centers were just not processing enough orders to pay for all that technology investment you had to make.”</p>
<p>With its automated fulfillment network, Kroger bet that consumers would be willing to trade delivery speed for sensible prices on grocery orders. That model has been highly successful for Ocado in the U.K., but U.S. consumers <a href="https://www.grocerydive.com/news/delivery-speed-is-king-for-online-shoppers-survey-shows/599282/">have shown they value speed of delivery</a>, with companies like Instacart and DoorDash expanding rapidly in recent years and rolling out services like 30-minute delivery.</p>
<p>Acknowledging this reality, Kroger noted on Tuesday that it’s <a href="https://www.grocerydive.com/news/kroger-doordash-grocery-delivery-ecommerce/761315/">deepening partnerships with third-party delivery companies</a>. The grocer also said it will pilot “capital-light, store-based automation in high-volume markets” — a seeming nod to the type of micro-fulfillment technology that grocers have tested in recent years, and that Amazon is <a href="https://www.grocerydive.com/news/amazon-whole-foods-store-within-store-automated-micro-fulfillment/804749/">currently piloting in a Whole Foods Market</a> store in Pennsylvania.&nbsp;</p>
<p>Fenyo pointed out that micro-fulfillment technology has also run into significant headwinds, adding that he thinks that outside of areas with large numbers of shoppers and high online ordering volume, putting automated order-assembly systems in stores probably doesn’t justify the cost.</p>
<p>Kroger’s decision to reduce its commitment to automation also poses a significant setback to Ocado, which has positioned its relationship with Kroger as a key endorsement of its warehouse automation technology. Shares in the U.K.-based robotics company have fallen dramatically and are now <a href="https://www.theguardian.com/business/nils-pratley-on-finance/2025/nov/18/ocado-share-price-back-where-it-started-fancy-robots">back to their level 15 years ago</a>, when the company <a href="https://multiples.vc/public-comps/ocado-group-valuation-multiples">went public</a>.</p>

                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built a system for active note-taking in regular meetings like 1-1s (141 pts)]]></title>
            <link>https://withdocket.com</link>
            <guid>46198430</guid>
            <pubDate>Mon, 08 Dec 2025 22:21:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://withdocket.com">https://withdocket.com</a>, See on <a href="https://news.ycombinator.com/item?id=46198430">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[GitHub no longer uses Toasts (114 pts)]]></title>
            <link>https://primer.style/accessibility/toasts/</link>
            <guid>46196831</guid>
            <pubDate>Mon, 08 Dec 2025 19:58:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://primer.style/accessibility/toasts/">https://primer.style/accessibility/toasts/</a>, See on <a href="https://news.ycombinator.com/item?id=46196831">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Toasts pose significant accessibility concerns and are not recommended for use.</p>
<h2 id="overview"><a href="#overview">Overview<!-- --> </a></h2>
<p>Toasts are small, rectangular notifications that pop up on the screen, triggered either by a user or system behavior. They commonly show up on the bottom left or right-hand side of the viewport and disappear after a preset amount of time.</p>
<p>While it can be tempting to use toast UI as a solution for your task, know that there are many accessibility and usability issues inherent with this pattern. Because of this, <strong>GitHub recommends using other more established, effective, and accessible ways of communicating with users</strong>.</p>
<h2 id="what-to-use-instead-of-toasts"><a href="#what-to-use-instead-of-toasts">What to use instead of toasts<!-- --> </a></h2>
<p>Primer offers a variety of solutions for informing users about updates. Consider:</p>
<ul>
<li>What kind of outcome you want to achieve, and</li>
<li>How the UI will best enable a user to do that.</li>
</ul>
<p>Are you attempting to highlight a successful or unsuccessful form submission? Give feedback that an action was successfully undertaken? Alert someone that a long-running task has finished?</p>
<p>Thinking through your use case can help select a UI treatment that not only best serves our users, but also reinforces the internal consistency of experience within the overall GitHub platform.</p>
<h3 id="successfully-completed-simple-actions"><a href="#successfully-completed-simple-actions">Successfully-completed simple actions<!-- --> </a></h3>
<p>User and system initiated actions that are direct and straightforward should be successfully completed as a matter of course. An example of this is creating an Issue, and then seeing the Issue show up on the list of Repo Issues.</p>
<p>There does not need to be a secondary form of reinforcement to communicate success, as it should be self-evident—including a toast to communicate this success may ironically lessen a sense of trust.</p>
<h3 id="successfully-completed-complex-actions"><a href="#successfully-completed-complex-actions">Successfully-completed complex actions<!-- --> </a></h3>
<p>User and system-initiated actions that require more complicated interaction may need additional feedback mechanisms to help inform the user that their request was successfully enacted. An example of this is the bulk creation of Issues.</p>
<p>Complex interactions may benefit from a secondary form of feedback to communicate success. The manner in which this secondary feedback is expressed depends on the design, but two common approaches are:</p>
<ol>
<li>Using <a data-inline="true" href="https://primer.style/product/ui-patterns/notification-messaging/">banners</a> to provide a summary of what was performed.</li>
<li>Progressively showing content as it is formed as part of a multi-step or progressive disclosure process.</li>
</ol>
<p>Note that both approaches persist feedback information and do not auto-dismiss it.</p>
<h3 id="unsuccessful-simple-and-complex-actions"><a href="#unsuccessful-simple-and-complex-actions">Unsuccessful simple and complex actions<!-- --> </a></h3>
<p><a data-inline="true" href="https://primer.style/product/ui-patterns/notification-messaging/">Banners</a> and <a data-inline="true" href="https://primer.style/product/components/dialog/">dialogs</a> can provide feedback about user and system error as a result of an undertaken action. Banners are useful when the error information needs to be passively available, while dialogs are useful for deliberately interrupting the user to get their attention.</p>
<h3 id="successfully-completed-forms"><a href="#successfully-completed-forms">Successfully-completed forms<!-- --> </a></h3>
<p>Simple forms may not need any other confirmation state other than creating and displaying what the user requested.</p>
<p>More complicated forms can utilize an interstitial confirmation page or <a data-inline="true" href="https://primer.style/product/ui-patterns/notification-messaging/">banner</a> that informs the user about what is being done with the data they submitted.</p>
<h3 id="other-form-validation"><a href="#other-form-validation">Other form validation<!-- --> </a></h3>
<p>Primer already has a robust set of components and guidance for <a data-inline="true" href="https://primer.style/product/ui-patterns/forms/#validation-message">handling input validation</a>. Using these offerings helps GitHub to feel consistent across the entire surface area of the site.</p>
<h3 id="long-running-tasks"><a href="#long-running-tasks">Long-running tasks<!-- --> </a></h3>
<p>Actions that take a long time to complete should <a data-inline="true" href="https://primer.style/product/ui-patterns/notification-messaging/">utilize banners</a> to inform the user of task completion or failure. Also consider ways to notify the user in other communication channels such as email, <a data-inline="true" href="https://github.com/notifications">notifications</a>, or a push notification in the GitHub app.</p>
<h3 id="application-state"><a href="#application-state">Application state<!-- --> </a></h3>
<p>There is the potential for a client’s session to become desynchronized, especially if a browser tab has been left open for a long time on a part of GitHub where a lot of dynamic updates are present. <a data-inline="true" href="https://primer.style/product/components/dialog/">Dialogs</a> and <a data-inline="true" href="https://primer.style/product/ui-patterns/notification-messaging/">banners</a> can be used to inform the user that a refresh is needed to resynchronize the client and server.</p>
<h2 id="accessibility-considerations"><a href="#accessibility-considerations">Accessibility considerations<!-- --> </a></h2>
<p>Toast UI risks violating the following <a data-inline="true" href="https://www.w3.org/TR/WCAG22/">Web Content Accessibility Guideline</a> (<abbr>WCAG</abbr>) Success Criteria (<abbr>SC</abbr>). Each of these SCs has one of three levels for support, and represent friction or a hard barrier for our users. GitHub honors the first two levels: A and AA.</p>
<h3 id="primary-considerations"><a href="#primary-considerations">Primary considerations<!-- --> </a></h3>
<p>These are aspects of toast UI that potentially represent large barriers for our users:</p>
<h4 id="221-timing-adjustable"><a href="#221-timing-adjustable">2.2.1: Timing Adjustable<!-- --> </a></h4>
<ul>
<li><a data-inline="true" href="https://www.w3.org/WAI/WCAG22/Understanding/timing-adjustable.html">Understanding SC 2.2.1: Timing Adjustable</a></li>
<li>Level A</li>
</ul>
<p>A mechanism needs to be present to extend the toast UI’s presence indefinitely until manually dismissed by the user. This is a guarantee that the toast’s duration is a length that allows all users to be able to navigate to, read, and potentially take action on the toast UI content.</p>
<h4 id="132-meaningful-sequence"><a href="#132-meaningful-sequence">1.3.2: Meaningful Sequence<!-- --> </a></h4>
<ul>
<li><a data-inline="true" href="https://www.w3.org/WAI/WCAG22/Understanding/meaningful-sequence.html">Understanding 1.3.2: Meaningful Sequence</a></li>
<li>Level A</li>
</ul>
<p>Toast message code is commonly placed at the start or the end of the DOM. Many forms of assistive technology work by reading the DOM in sequence, so there will be a disconnect between what triggers the toast UI and the toast UI itself. This impedes discovery and understanding.</p>
<h4 id="211-keyboard"><a href="#211-keyboard">2.1.1: Keyboard<!-- --> </a></h4>
<ul>
<li><a data-inline="true" href="https://www.w3.org/WAI/WCAG22/Understanding/keyboard.html">Understanding 2.1.1: Keyboard</a></li>
<li>Level A</li>
</ul>
<p>Toasts UI started as a mechanism to display passive notifications, but evolved to include interactive controls.</p>
<p>All interactive controls placed inside a toast UI need to be operable via keyboard, as well as accessing the toast UI container itself. This includes a mechanism for dismissing the toast, as well as managing focus when it is removed from the DOM.</p>
<h4 id="413-status-messages"><a href="#413-status-messages">4.1.3: Status Messages<!-- --> </a></h4>
<ul>
<li><a data-inline="true" href="https://www.w3.org/WAI/WCAG22/Understanding/status-messages.html">Understanding 4.1.3: Status Messages</a></li>
<li>Level AA</li>
</ul>
<p>Toast UIs should make their presence known to assistive technology in a way that is not disruptive to a user’s regular workflow or working context.</p>
<h3 id="secondary-considerations"><a href="#secondary-considerations">Secondary considerations<!-- --> </a></h3>
<p>These are other potential success criteria that using toast UI may violate depending on its context:</p>
<h4 id="144-resize-text"><a href="#144-resize-text">1.4.4: Resize text<!-- --> </a></h4>
<ul>
<li><a data-inline="true" href="https://www.w3.org/WAI/WCAG22/Understanding/resize-text.html">Understanding 1.4.4: Resize text</a></li>
<li>Level AA</li>
</ul>
<p>Increasing the text size on the browser or operating system level runs into three potential risks for toast UI.</p>
<p>First is making the toast so large that it obscures the rest of the page content. Second is creating horizontal overflow in an attempt to prevent obscuring the underlying page content. Third is attempting to block text resizing on a toast component to prevent both of the previous risks.</p>
<h4 id="1410-reflow"><a href="#1410-reflow">1.4.10: Reflow<!-- --> </a></h4>
<ul>
<li><a data-inline="true" href="https://www.w3.org/WAI/WCAG22/Understanding/reflow.html">Understanding 1.4.10: Reflow</a></li>
<li>Level AA</li>
</ul>
<p>If horizontal overflow is created as a result of using toast UI, it needs to be able to be scrollable via keyboard interaction.</p>
<h4 id="243-focus-order"><a href="#243-focus-order">2.4.3: Focus Order<!-- --> </a></h4>
<ul>
<li><a data-inline="true" href="https://www.w3.org/WAI/WCAG22/Understanding/focus-order.html">Understanding 2.4.3: Focus Order</a></li>
<li>Level A</li>
</ul>
<p>A toast that contains interactive elements needs those elements to be able to receive keyboard focus. Additionally, the toast’s position in the DOM may make the order of focus not make sense compared to the interactive content that comes before or after it.</p>
<h4 id="324-consistent-identification"><a href="#324-consistent-identification">3.2.4: Consistent Identification<!-- --> </a></h4>
<ul>
<li><a data-inline="true" href="https://www.w3.org/WAI/WCAG22/Understanding/consistent-identification.html">Understanding 3.2.4: Consistent Identification</a></li>
<li>Level AA</li>
</ul>
<p>The underlying code for toast notifications should be the same, regardless of where they are used or what team owns the service.</p>
<h2 id="usability-considerations"><a href="#usability-considerations">Usability considerations<!-- --> </a></h2>
<p>In addition to accessibility issues, there are also usability issues to consider for toasts:</p>
<h3 id="large-displays"><a href="#large-displays">Large displays<!-- --> </a></h3>
<p>Many developers work on a larger display, in order to have more screen real estate to work with. Toasts could be placed in such a way that they go unnoticed, in that they sit outside of a user’s immediate field of view.</p>
<h3 id="distractions-and-multitasking"><a href="#distractions-and-multitasking">Distractions and multitasking<!-- --> </a></h3>
<p>Toasts that automatically dismiss themselves risk being unread if a user is distracted or is actively switching between tabs and applications.</p>
<h3 id="blocking-ui"><a href="#blocking-ui">Blocking UI<!-- --> </a></h3>
<p>Since toasts “float” above the rest of the UI, there is a chance that they can obscure underlying content.</p>
<p>This obscuring effect is especially worth considering given that important UI such as form submission buttons tend to also be placed at the bottom corner of the viewport. The effect also becomes more pronounced if multiple toasts stack on top of each other.</p>
<h3 id="screen-magnification"><a href="#screen-magnification">Screen magnification<!-- --> </a></h3>
<p>Some users rely on a software or hardware-based magnification solution in order to be able to use a computer. Toast notifications may not be seen by the user, in that they are displayed outside of the range of the magnification window.</p>
<h3 id="working-memory"><a href="#working-memory">Working memory<!-- --> </a></h3>
<p>Toasts that both display important information and automatically dismiss themselves may create a situation where a user is given important information, but then has no way to go back and review the information.</p>
<h3 id="banner-blindness"><a href="#banner-blindness">Banner blindness<!-- --> </a></h3>
<p>Toasts are an over-used interaction pattern on the web. This leads to a phenomenon where users are taught to ignore and avoid their content, as it is often low-quality or unrelated to their immediate task at hand.</p>
<h3 id="disconnection"><a href="#disconnection">Disconnection<!-- --> </a></h3>
<p>A toast’s placement may be far away from the UI that triggered it. This violation of the gestalt principle of proximity means there is more of a chance a user does not understand the relationship between the toast message and its related piece of content.</p>
<h3 id="accidental-dismissal"><a href="#accidental-dismissal">Accidental dismissal<!-- --> </a></h3>
<p>Users pressing <kbd>Esc</kbd> to dismiss a toast may accidentally dismiss another piece of UI, if multiple keyboard-dismissable pieces of content are present. The opposite also applies, where a user may dismiss a toast containing important information while trying to dismiss an unrelated piece of UI.</p>
<h2 id="further-reading"><a href="#further-reading">Further reading<!-- --> </a></h2>
<ul>
<li><a data-inline="true" href="https://primer.style/product/ui-patterns/notification-messaging/">Notification messaging - Primer</a></li>
<li><a data-inline="true" href="https://github.com/github/accessibility/blob/main/docs/coaching-recommendations/toast-flash-banner/toasts/accessible-toast-prototype.md">Engineering Explorations on an Accessible Toast Prototype - GitHub (staff only)</a></li>
<li><a data-inline="true" href="https://www.tpgi.com/celebrating-with-the-perfect-toast/">Celebrating with The Perfect Toast - TPGi</a></li>
<li><a data-inline="true" href="https://www.scottohara.me/blog/2019/07/08/a-toast-to-a11y-toasts.html">A toast to an accessible toast… - Scott O’Hara</a></li>
<li><a data-inline="true" href="https://adrianroselli.com/2020/01/defining-toast-messages.html">Defining ‘Toast’ Messages - Adrian Roselli</a></li>
<li><a data-inline="true" href="https://adamsilver.io/blog/the-problem-with-toast-messages-and-what-to-do-instead/">The problem with toast messages and what to do instead - Adam Silver</a></li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Icons in Menus Everywhere – Send Help (742 pts)]]></title>
            <link>https://blog.jim-nielsen.com/2025/icons-in-menus/</link>
            <guid>46196688</guid>
            <pubDate>Mon, 08 Dec 2025 19:44:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.jim-nielsen.com/2025/icons-in-menus/">https://blog.jim-nielsen.com/2025/icons-in-menus/</a>, See on <a href="https://news.ycombinator.com/item?id=46196688">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://mastodon.social/@jimniels/115556046706814962">I complained about this on the socials</a>, but I didn’t get it all out of my system. So now I write a blog post.</p><p>I’ve never liked the philosophy of “put an icon in every menu item by default”.</p><p>Google Sheets, for example, does this. Go to “File” or “Edit” or “View” and you’ll see a menu with a list of options, every single one having an icon (same thing with the right-click context menu).</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-sheets.png" width="725" height="848" alt="Screenshot of menus with icons in Google Sheets"></p><p>It’s extra noise to me. It’s not that I think menu items should <em>never</em> have icons. I think they can be incredibly useful (more on that below). It’s more that I don’t like the idea of “give each menu item an icon” being the default approach.</p><p>This posture lends itself to a practice where designers have an attitude of “I need an icon to fill up this space” instead of an attitude of “Does the addition of a icon here, and the cognitive load of parsing and understanding it, help or hurt how someone would use this menu system?”</p><p>The former doesn’t require thinking. It’s just templating — they all have icons, so we need to put <em>something</em> there. The latter requires care and thoughtfulness for each use case and its context.</p><p>To defend my point, one of the examples I always pointed to was macOS. For the longest time, Apple’s OS-level menus seemed to avoid this default approach of sticking icons in every menu item.</p><p>That is, until macOS Tahoe shipped.</p><p>Tahoe now has icons in menus everywhere. For example, here’s the Apple menu:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-mac.png" width="312" height="351" alt="Screenshot of the Apple menu in macOS tahoe where every menu item is prefixed with an icon."></p><p>Let’s look at others. As I’m writing this I have Safari open. Let’s look at the “Safari” menu:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-safari-about.png" width="333" height="445" alt="Screenshot of the Safari menu in macOS Tahoe where about half of the menu items are prefixed with an icon."></p><p>Hmm. Interesting. Ok so we’ve got an icon for like half the menu items. I wonder why some get icons and others don’t?</p><p>For example, the “Settings” menu item (third from the top) has an icon. But the other item in its grouping “Privacy Report” does not. I wonder why? Especially when Safari has an icon for Privacy report, like if you go to customize the toolbar you’ll see it:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-macos-safari-privacy-report.png" width="723" height="259" alt="Screenshot of the Customize Toolbar UI in Safari and the Privacy Report button has a red highlight around indicating its icon."></p><p>Hmm. Who knows? Let’s keep going.</p><p>Let’s look at the "File" menu in Safari:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-safari-file.png" width="422" height="567" alt="Screenshot of the File menu Safari in macOS Tahoe where only a few menu items are prefixed with an icon. Some are indented, others not."></p><p>Some groupings have icons and get inset, while other groupings don’t have icons and don’t get inset. Interesting…again I wonder what the rationale is here? How do you choose? It’s not clear to me.</p><p>Let’s keep going. Let’s go to the "View" menu:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-safari-view.png" width="373" height="648" alt="Screenshot of the View menu in Safari on macOS Tahoe where some menu items are prefixed with an icon and two also have a checkmark."></p><p>Oh boy, now we’re really in it. Some of these menu items have the notion of a toggle (indicated by the checkmark) so now you’ve got all kinds of alignment things to deal with. The visual symbols are doubling-up when there’s a toggle <em>and</em> an icon.</p><p>The “View” menu in Mail is a similar mix of:</p><ul><li>Text</li><li>Text + toggles</li><li>Text + icons</li><li>Text + icons + toggles</li></ul><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-mail-view.png" width="343" height="770" alt="Screenshot of the View menu in Mail on macOS Tahoe showing how menu items can be indented and have icons, not have icons, and have toggles with checkmarks."></p><p>You know what would be a fun game? Get a bunch of people in a room, show them menus where the textual labels are gone, and see who can get the most right.</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-app-edit.png" width="188" height="541" alt="Screenshot of a menu in macOS Tahoe where every menu item is prefixed with an icon but the labels are blurred out so you don’t know for sure what each menu item is."></p><p>But I digress.</p><p>In so many of these cases, I honestly can’t intuit why some menus have icons and others do not. What are so many of these icons affording me at the cost of extra visual and cognitive parsing? I don’t know.</p><p>To be fair, there are <em>some</em> menus where these visual symbols are incredibly useful. Take this menu from Finder:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-finder-window.png" width="614" height="604" alt="Screenshot of a Finder menu in macOS Tahoe where every menu item is prefixed with a useful icon."></p><p>The visual depiction of how those are going to align is actually incredibly useful because it’s way easier for my brain to parse the symbol and understand where the window is going to go than it is to read the text and imagine in my head what “Top Left” or “Bottom &amp; Top” or “Quarters” will mean. But a visual symbol? I instantly get it!</p><p>Those are good icons in menus. I like those.</p><h2 id="apple-abandons-its-own-guidance">Apple Abandons Its Own Guidance</h2><p>What I find really interesting about this change on Apple’s part is how it seemingly goes against their own previous human interface guidelines (as <a href="https://mastodon.gassner.io/@peter/115559008588925643">pointed out to me by Peter Gassner</a>).</p><p>They have an entire section in their 2005 guidelines titled “Using Symbols in Menus”:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-hig.png" width="679" height="359" alt="Screenshot from Apple’s Human Interface Guidelines"></p><p>See what it says?</p><blockquote><p>There are a few standard symbols you can use to indicate additional information in menus…Don’t use other, arbitrary symbols in menus, because they add visual clutter and may confuse people.</p></blockquote><p>Confused people. That’s me.</p><p>They even have an example of what <em>not</em> to do and guess what it looks like? A menu in macOS Tahoe.</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-hig-donts.png" width="343" height="224" alt="Screenshot from the HIG denoting how you shouldn’t use arbitrary symbols in menus." data-og-image=""></p><h2 id="conclusion">Conclusion</h2><p>It’s pretty obvious how I feel. I’m tired of all this visual noise in my menus.</p><p>And now that Apple has seemingly thrown in with the “stick an icon in every menu by default” crowd, it’s harder than ever for me to convince people otherwise. To persuade, “Hey, unless you can articulate a really good reason to add this, maybe our default posture should be no icons in menus?”</p><p>So I guess this is the world I live in now. Icons in menus. Icons in menus everywhere.</p><p>Send help.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Trials avoid high risk patients and underestimate drug harms (162 pts)]]></title>
            <link>https://www.nber.org/papers/w34534</link>
            <guid>46196308</guid>
            <pubDate>Mon, 08 Dec 2025 19:07:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nber.org/papers/w34534">https://www.nber.org/papers/w34534</a>, See on <a href="https://news.ycombinator.com/item?id=46196308">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <a id="main-content" tabindex="-1"></a>
        <div id="block-nber-breadcrumbs">
  
    
        
  
      <nav aria-label="You are here:">
      
      <ul>
                  <li>
                          <a href="https://www.nber.org/">Home</a>
                      </li>
                  <li>
                          <a href="https://www.nber.org/research">Research</a>
                      </li>
                  <li>
                          <a href="https://www.nber.org/papers">Working Papers</a>
                      </li>
                  <li>
                          Trials Avoid High Risk Patients and…
                      </li>
              </ul>
    </nav>
  
  </div>

  
  
        

<div>
  <div>
        <p><span>Working Paper</span> 34534
  </p>

        <p><span>DOI</span> 10.3386/w34534
  </p>

        <p><span>Issue Date</span> <time datetime="2025-12-04T12:00:00Z">December 2025</time>

  </p>

          </div>
  <div>
    <p>
The FDA does not formally regulate representativeness, but if trials under-enroll vulnerable patients, the resulting evidence may understate harm from drugs. We study the relationship between trial participation and the risk of drug-induced adverse events for cancer medications using data from the Surveillance, Epidemiology, and End Results Program linked to Medicare claims. Initiating treatment with a cancer drug increases the risk of hospitalization due to serious adverse events (SAE) by 2 percentage points per month (a 250% increase). Heterogeneity in SAE treatment effects can be predicted by patient's comorbidities, frailty, and demographic characteristics. Patients at the 90th percentile of the risk distribution experience a 2.5 times greater increase in SAEs after treatment initiation compared to patients at the 10th percentile of the risk distribution yet are 4 times less likely to enroll in trials. The predicted SAE treatment effects for the drug's target population are 15% larger than the predicted SAE treatment effects for trial enrollees, corresponding to 1 additional induced SAE hospitalization for every 25 patients per year of treatment. We formalize conditions under which regulating representativeness of SAE risk will lead to more externally valid trials, and we discuss how our results could inform regulatory requirements.
</p>
  </div>
  
</div>

  

  

<div>
  <ul>
          <li>
        
      </li>
    
    <li>
      <div id="accordion-body-guid2" aria-labelledby="accordion-button-guid2">
        <p>Copy Citation</p>
        <div>
            <p>
                Jason Abaluck, Leila Agha, and Sachin Shah, "Trials Avoid High Risk Patients and Underestimate Drug Harms," NBER Working Paper 34534 (2025), https://doi.org/10.3386/w34534.
            </p>

            </div>
                    <p>Download Citation</p>
            

            </div>    </li>

    
      </ul>
</div>


<div>
    <h2>Related</h2>
    <div>
    
      
                <div>
    <h3>Topics</h3>
    
  </div>

      
                <div>
    <h3>Programs</h3>
    
  </div>

      
                <div>
    <h3>Working Groups</h3>
    
  </div>

      
                        <div>
            <h3>Projects</h3>
            
          </div>
              
      
      
      
      
      
      
      
      
          </div>
  </div>


  
  
        <section id="block-morefromthenber">
    <h2>More from the NBER</h2>
    
    <div>
    

<div data-href="/research/videos/2025-17th-annual-feldstein-lecture-n-gregory-mankiw-fiscal-future">
      <p><img loading="lazy" src="https://www.nber.org/sites/default/files/styles/promo/public/2025-07/MF%20Lecture%202025%20updated.png?itok=ij7zY5fj" width="736" height="414" alt=" 2025, 17th Annual Feldstein Lecture, N. Gregory Mankiw,&quot; The Fiscal Future&quot;" typeof="foaf:Image">



    </p>
  
  

      <ul>
      <li>Feldstein Lecture</li>
    </ul>
  
      <ul>
      <li>
        Presenter:
        

              <span>
      <a href="https://www.nber.org/people/gregory_mankiw">N. Gregory Mankiw</a>    </span>
      
      </li>
    </ul>
  
  
  
      <p>N. Gregory Mankiw, Robert M. Beren Professor of Economics at Harvard University, presented the 2025 Martin Feldstein...</p>
  </div>

    

<div data-href="/research/videos/2025-methods-lecture-raj-chetty-and-kosuke-imai-uncovering-causal-mechanisms-mediation-analysis-and">
      <p><img loading="lazy" src="https://www.nber.org/sites/default/files/styles/promo/public/2025-07/Methods%20Lecture%20SI%202025_0.png?itok=hsgorA8D" width="736" height="414" alt=" 2025 Methods Lecture, Raj Chetty, &quot;Uncovering Causal Mechanisms: Mediation Analysis and Surrogate Indices&quot;" typeof="foaf:Image">



    </p>
  
  

      <ul>
      <li>Methods Lectures</li>
    </ul>
  
      <ul>
      <li>
        Presenters:
        

              <span>
      <a href="https://www.nber.org/people/raj_chetty">Raj Chetty</a>    </span>
                  <span>
       &amp; <a href="https://www.nber.org/people/kosuke_imai">Kosuke Imai</a>    </span>
      
      </li>
    </ul>
  
  
  
      <p>SlidesBackground materials on mediationImai, Kosuke, Dustin Tingley, and Teppei Yamamoto. (2013). “Experimental Designs...</p>
  </div>

    

<div data-href="/research/videos/2025-international-trade-and-macroeconomics-panel-future-global-economy">
      <p><img loading="lazy" src="https://www.nber.org/sites/default/files/styles/promo/public/2025-08/SI%20International%20trade%20Panel%202025.png?itok=2DuJTJDm" width="736" height="414" alt="2025 International Trade and Macroeconomics, &quot;Panel on The Future of the Global Economy&quot;" typeof="foaf:Image">



    </p>
  
  

      <ul>
      <li>Panel Discussion</li>
    </ul>
  
      <ul>
      <li>
        Presenters:
        

              <span>
      <a href="https://www.nber.org/people/oleg_itskhoki">Oleg Itskhoki</a>,     </span>
                  <span>
      <a href="https://www.nber.org/people/paul_krugman">Paul R. Krugman</a>    </span>
                  <span>
       &amp; <a href="https://www.nber.org/people/linda_tesar">Linda Tesar</a>    </span>
      
      </li>
    </ul>
  
  
  
      <p>Supported by the Alfred P. Sloan Foundation grant #G-2023-19633, the Lynde and Harry Bradley Foundation grant #20251294...</p>
  </div>
</div>
  </section>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Has the cost of building software dropped 90%? (381 pts)]]></title>
            <link>https://martinalderson.com/posts/has-the-cost-of-software-just-dropped-90-percent/</link>
            <guid>46196228</guid>
            <pubDate>Mon, 08 Dec 2025 19:00:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://martinalderson.com/posts/has-the-cost-of-software-just-dropped-90-percent/">https://martinalderson.com/posts/has-the-cost-of-software-just-dropped-90-percent/</a>, See on <a href="https://news.ycombinator.com/item?id=46196228">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    
    
    <div>
        <p>I've been building software professionally for nearly 20 years. I've been through a lot of changes - the 'birth' of SaaS, the mass shift towards mobile apps, the outrageous hype around blockchain, and the perennial promise that low-code would make developers obsolete.</p>
<p>The economics have changed <em>dramatically</em> now with agentic coding, and it is going to totally transform the software development industry (and the wider economy). 2026 is going to catch a lot of people off guard.</p>
<p>In my previous post I delved into why I think <a href="https://martinalderson.com/posts/are-we-in-a-gpt4-style-leap-that-evals-cant-see/">evals are missing</a> some of the big leaps, but thinking this over since then (and recent experience) has made me confident we're in the early stages of a once-in-a-generation shift.</p>
<h2>The cost of shipping</h2>
<p>I started developing just around the time open source started to really explode - but it was clear this was one of the first big shifts in cost of building custom software. I can remember eye watering costs for SQL Server or Oracle - and as such started out really with MySQL, which did allow you to build custom networked applications without incurring five or six figures of annual database licensing costs.</p>
<p>Since then we've had cloud (which I would debate is a cost saving at all, but let's be generous and assume it has some initial capex savings) and lately what I feel has been the era of complexity. Software engineering has got - in my opinion, often needlessly - complicated, with people rushing to very labour intensive patterns such as TDD, microservices, super complex React frontends and Kubernetes. I definitely don't think we've seen much of a cost decrease in the past few years.</p>
<p><img src="https://martinalderson.com/img/cost_of_shipping@2x.png" alt="Cost of shipping software over time"></p>
<p>AI Agents however in my mind <em>massively</em> reduce the labour cost of developing software.</p>
<h2>So where do the 90% savings actually come from?</h2>
<p>At the start of 2025 I was incredibly sceptical of a lot of the AI coding tools - and a lot of them I still am. Many of the platforms felt like glorified low code tooling (Loveable, Bolt, etc), or VS Code forks with some semi-useful (but often annoying) autocomplete improvements.</p>
<p>Take an average project for an internal tool in a company. Let's assume the data modelling is already done to some degree, and you need to implement a web app to manage widgets.</p>
<p>Previously, you'd have a small team of people working on setting up CI/CD, building out data access patterns and building out the core services. Then usually a whole load of CRUD-style pages and maybe some dashboards and graphs for the user to make. Finally you'd (hopefully) add some automated unit/integration/e2e tests to make sure it was fairly solid and ship it, maybe a month later.</p>
<p>And that's just the direct labour. Every person on the project adds coordination overhead. Standups, ticket management, code reviews, handoffs between frontend and backend, waiting for someone to unblock you. The actual coding is often a fraction of where the time goes.</p>
<p><em>Nearly all of this</em> can be done in a few hours with an agentic coding CLI. I've had Claude Code write an entire unit/integration test suite in a few hours (300+ tests) for a fairly complex internal tool. This would take me, or many developers I know and respect, days to write by hand.</p>
<p>The agentic coding tools have got <em>extremely</em> good at converting business logic specifications into pretty well written APIs and services.</p>
<p>A project that would have taken a month now takes a week. The thinking time is roughly the same  - the implementation time collapsed. And with smaller teams, you get the inverse of Brooks's Law: instead of communication overhead scaling with headcount, it disappears. A handful of people can suddenly achieve an order of magnitude more.</p>
<h2>Latent demand</h2>
<p>On the face of it, this seems like incredibly bad news for the software development industry - but economics tells us otherwise.</p>
<p><a href="https://en.wikipedia.org/wiki/Jevons_paradox">Jevons Paradox</a> says that when something becomes cheaper to produce, we don't just do the same amount for less money. Take electric lighting for example; while sales of candles and gas lamps fell, overall <em>far</em> more artificial light was generated.</p>
<p>If we apply this to software engineering, think of supply and demand. There is <em>so much</em> latent demand for software. I'm sure every organisation has hundreds if not thousands of Excel sheets tracking important business processes that would be far better off as a SaaS app. Let's say they get a quote from an agency to build one into an app for $50k - only essential ones meet the grade. At $5k (for a decent developer + AI tooling) - suddenly there is far more demand.</p>
<p><img src="https://martinalderson.com/img/latent_demand@2x.png" alt="Latent demand for software"></p>
<h2>Domain knowledge is the only moat</h2>
<p>So where does that leave us? Right now there is still enormous value in having a human 'babysit' the agent - checking its work, suggesting the approach and shortcutting bad approaches. Pure YOLO vibe coding ends up in a total mess very quickly, but with a human in the loop I think you can build incredibly good quality software, <em>very</em> quickly.</p>
<p>This then allows developers who really master this technology to be hugely effective at solving business problems. Their domain and industry knowledge becomes a huge lever - knowing the best architectural decisions for a project, knowing which framework to use and which libraries work best.</p>
<p>Layer on understanding of the business domain and it does genuinely feel like the mythical 10x engineer is here. Equally, the pairing of a business domain expert with a motivated developer and these tools becomes an incredibly powerful combination, and something I think we'll see becoming quite common - instead of a 'squad' of a business specialist and a set of developers, we'll see a far tighter pairing of a couple of people.</p>
<p>This combination allows you to iterate incredibly quickly, and software becomes almost disposable - if the direction is bad, then throw it away and start again, using those learnings. This takes a fairly large mindset shift, but the hard work is the <em>conceptual thinking</em>, not the typing.</p>
<h2>Don't get caught off guard</h2>
<p>The agents and models are still improving rapidly, which I don't think is really being captured in the benchmarks. Opus 4.5 seems to be able to follow long 10-20 minute sessions without going completely off piste. We're just starting to see the results of the hundreds of billions of dollars of capex that has gone into GB200 GPUs now, and I'm sure newer models will quickly make these look completely obsolete.</p>
<p>However, I've spoken to so many software engineers that are really fighting this change. I've heard the same objections too many times - LLMs make too many mistakes, it can't understand <code>[framework]</code>, or it doesn't really save any time.</p>
<p>These assertions are rapidly becoming completely false, and remind me a lot of the desktop engineers who dismissed the iPhone in 2007. I think we all know how that turned out - networking got better, the phones got way faster and the mobile operating systems became very capable.</p>
<p>Engineers need to really lean in to the change in my opinion. This won't change overnight - large corporates are still very much behind the curve in general, lost in a web of bureaucracy of vendor approvals and management structures that leave them incredibly vulnerable to smaller competitors.</p>
<p>But if you're working for a smaller company or team and have the power to use these tools, you should. Your job is going to change - but software has always changed. Just perhaps this time it's going to change faster than anyone anticipates. 2026 is coming.</p>
<p>One objection I hear a lot is that LLMs are only good at greenfield projects. I'd push back hard on this. I've spent plenty of time trying to understand 3-year-old+ codebases where everyone who wrote it has left. Agents make this dramatically easier - explaining what the code does, finding the bug(s), suggesting the fix. I'd rather inherit a repo written with an agent and a good engineer in the loop than one written by a questionable quality contractor who left three years ago, with no tests, and a spaghetti mess of classes and methods.</p>

    </div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jepsen: NATS 2.12.1 (415 pts)]]></title>
            <link>https://jepsen.io/analyses/nats-2.12.1</link>
            <guid>46196105</guid>
            <pubDate>Mon, 08 Dec 2025 18:51:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jepsen.io/analyses/nats-2.12.1">https://jepsen.io/analyses/nats-2.12.1</a>, See on <a href="https://news.ycombinator.com/item?id=46196105">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><a href="https://nats.io/">NATS</a> is a distributed streaming system. Regular NATS streams offer only best-effort delivery, but a subsystem, called JetStream, guarantees messages are delivered at least once. We tested NATS JetStream, version 2.12.1, and found that it lost writes if data files were truncated or corrupted on a minority of nodes. We also found that coordinated power failures, or an OS crash on a single node combined with network delays or process pauses, can cause the loss of committed writes and persistent split-brain. This data loss was caused (at least in part) by choosing to flush writes to disk every two minutes, rather than before acknowledging them. We also include a belated note on data loss due to process crashes in version 2.10.22, which was fixed in 2.10.23. NATS has now documented the risk of its default <code>fsync</code> policy, and the remaining issues remain under investigation. This research was performed independently by Jepsen, without compensation, and conducted in accordance with the <a href="https://jepsen.io/analyses/ethics">Jepsen ethics policy</a>.</p><article>
  <div>
<h2 data-number="1" id="background"> Background</h2>
<p><a href="https://nats.io/">NATS</a> is a popular streaming system. Producers <a href="https://docs.nats.io/nats-concepts/core-nats/pubsub">publish messages to streams</a>, and consumers subscribe to those streams, fetching messages from them. Regular NATS streams are allowed to drop messages. However, NATS has a subsystem called <a href="https://docs.nats.io/nats-concepts/jetstream">JetStream</a>, which <a href="https://docs.nats.io/running-a-nats-service/configuration/clustering/jetstream_clustering">uses</a> the <a href="https://raft.github.io/">Raft consensus algorithm</a> to replicate data among nodes. JetStream promises <a href="https://docs.nats.io/nats-concepts/jetstream#exactly-once-semantics">“at least once”</a> delivery: messages may be duplicated, but acknowledged messages<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> should not be lost.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> Moreover, JetStream streams are <a href="https://docs.nats.io/nats-concepts/jetstream#persistent-and-consistent-distributed-storage">totally ordered logs</a>.</p>
<p>JetStream is intended to <a href="https://docs.nats.io/nats-concepts/jetstream">“self-heal and always be available”</a>. The documentation also states that <a href="https://docs.nats.io/nats-concepts/jetstream#persistent-and-consistent-distributed-storage">“the formal consistency model of NATS JetStream is Linearizable”</a>. At most one of these claims can be true: the <a href="https://www.cs.princeton.edu/courses/archive/spr22/cos418/papers/cap.pdf">CAP theorem</a> tells us that <a href="https://jepsen.io/consistency/models/linearizable">Linearizable</a> systems can not be totally available.<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> In practice, they tend to be available so long as a majority of nodes are non-faulty and communicating. If, say, a single node loses network connectivity, operations must fail on that node. If three out of five nodes crash, all operations must fail.</p>
<p>Indeed, a <a href="https://docs.nats.io/nats-concepts/jetstream#persistent-and-consistent-distributed-storage">later section</a> of the JetStream docs acknowledges this fact, saying that streams with three replicas can tolerate the loss of one server, and those with five can tolerate the simultaneous loss of two.</p>
<blockquote>
<p>Replicas=5 - Can tolerate simultaneous loss of two servers servicing the stream. Mitigates risk at the expense of performance.</p>
</blockquote>
<p>When does NATS guarantee a message will be durable? The <a href="https://docs.nats.io/using-nats/developer/develop_jetstream?q=sync#publish-to-a-stream">JetStream developer docs</a> say that once a JetStream client’s <code>publish</code> request is acknowledged by the server, that message has “been successfully persisted”. The <a href="https://docs.nats.io/running-a-nats-service/configuration/clustering/jetstream_clustering#the-quorum">clustering configuration documentation</a> says:</p>
<blockquote>
<p>In order to ensure data consistency across complete restarts, a quorum of servers is required. A quorum is ½ cluster size + 1. This is the minimum number of nodes to ensure at least one node has the most recent data and state after a catastrophic failure. So for a cluster size of 3, you’ll need at least two JetStream enabled NATS servers available to store new messages. For a cluster size of 5, you’ll need at least 3 NATS servers, and so forth.</p>
</blockquote>
<p>With these guarantees in mind, we set out to test NATS JetStream behavior under a variety of simulated faults.</p>
<h2 data-number="2" id="test-design"> Test Design</h2>
<p>We designed a <a href="https://github.com/jepsen-io/nats">test suite</a> for NATS JetStream using the <a href="https://github.com/jepsen-io/jepsen">Jepsen testing library</a>, using <a href="https://github.com/nats-io/nats.java">JNATS</a> (the official Java client) at version 2.24.0. Most of our tests ran in Debian 12 containers under LXC; <a href="https://github.com/jepsen-io/nats/tree/4760f97f86350c5c9983478656dbcbcdade33817/antithesis">some tests</a> ran in <a href="https://antithesis.com/">Antithesis</a>, using the official NATS Docker images. In all our tests we created a single JetStream stream with a target replication factor of five. Per NATS’ recommendations, our clusters generally contained three or five nodes. We tested a variety of versions, but the bulk of this work focused on NATS 2.12.1.</p>
<p>The test harness <a href="https://github.com/jepsen-io/nats/blob/4760f97f86350c5c9983478656dbcbcdade33817/src/jepsen/nats/nemesis.clj">injected a variety of faults</a>, including process pauses, crashes, network partitions, and packet loss, as well as single-bit errors and truncation of data files. We limited file corruption to a minority of nodes. We also simulated power failure—a crash with partial amnesia—using the <a href="https://github.com/dsrhaslab/lazyfs">LazyFS</a> filesystem. LazyFS allows Jepsen to drop any writes which have not yet been flushed using a call to (e.g.) <code>fsync</code>.</p>
<p>Our tests did not measure Linearizability or <a href="https://jepsen.io/consistency/models/serializable">Serializability</a>. Instead we ran <a href="https://github.com/jepsen-io/nats/blob/4760f97f86350c5c9983478656dbcbcdade33817/src/jepsen/nats/queue.clj#L238-L266">several producer processes</a>, each bound to a single NATS client, which published globally unique values to a single JetStream stream. Each message included the process number and a sequence number within that process, so message <code>4-0</code> denoted the first <code>publish</code> attempted by process <code>4</code>, message <code>4-1</code> denoted the second, and so on. At the end of the test we ensured all nodes were running, resolved any network partitions or other faults, subscribed to the stream, and <a href="https://github.com/jepsen-io/nats/blob/4760f97f86350c5c9983478656dbcbcdade33817/src/jepsen/nats/queue.clj#L189-L201">attempted to read all acknowledged messages from the the stream</a>. Each reader called <code>fetch</code> until it had observed (at least) the last acknowledged message published by each process, or timed out.</p>
<p>We measured JetStream’s at-least-once semantics <a href="https://github.com/jepsen-io/nats/blob/4760f97f86350c5c9983478656dbcbcdade33817/src/jepsen/nats/queue.clj#L359-L424">based on the union of all published and read messages</a>. We considered a message <em>OK</em> if it was attempted and read. Messages were <em>lost</em> if they were acknowledged as published, but never read by any process. We divided lost messages into three epochs, based on the first and last OK messages written by the same process.<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> We called those lost before the first OK message the <em>lost-prefix</em>, those lost after all the last OK message the <em>lost-postfix</em>, and all others the <em>lost-middle</em>. This helped to distinguish between lagging readers and true data loss.</p>
<p>In addition to verifying each acknowledged message was delivered to at least one consumer across all nodes, we also checked the set of messages read by all consumers connected to a specific node. We called it <em>divergence</em>, or <em>split-brain</em>, when an acknowledged message was missing from some nodes but not others.</p>
<h2 data-number="3" id="results"> Results</h2>
<p>We begin with a belated note on total data loss in version 2.10.22, then continue with four findings related to data loss and replica divergence in version 2.12.1: two with file corruption, and two with power failures.</p>
<h2 data-number="3.1" id="total-data-loss-on-crash-in-2.10.22-6888"> Total Data Loss on Crash in 2.10.22 (#6888)</h2>
<p>Before discussing version 2.12.1, we present a long-overdue finding from earlier work. In versions 2.10.20 through 2.10.22 (released 2024-10-17), we found that process crashes alone could cause the total loss of a JetStream stream and all its associated data. Subscription requests would return <code>"No matching streams for subject"</code>, and <code>getStreamNames()</code> would return an empty list. These conditions would persist for hours: <a href="https://github.com/user-attachments/files/20133999/20250509T191519.377-0500.zip">in this test run</a>, we waited 10,000 seconds for the cluster to recover, but the stream never returned.</p>
<p>Jepsen reported this issue to NATS as <a href="https://github.com/nats-io/nats-server/issues/6888">#6888</a>, but it appears that NATS had already identified several potential causes for this problem and resolved them. In <a href="https://github.com/nats-io/nats-server/pull/5946">#5946</a>, a cluster-wide crash occurring shortly after a stream was created could cause the loss of the stream. A new leader would be elected with a snapshot which preceded the creation of the stream, and replicate that empty snapshot to followers, causing everyone to delete their copy of the stream. In <a href="https://github.com/nats-io/nats-server/pull/5700">#5700</a>, tests running in <a href="https://antithesis.com/">Antithesis</a> found that out-of-order delivery of snapshot messages could cause streams to be deleted and re-created as well. In <a href="https://github.com/nats-io/nats-server/pull/6061">#6061</a>, process crashes could cause nodes to delete their local Raft state. All of these fixes were released as a part of 2.10.23, and we no longer observed the problem in that version.</p>
<h2 data-number="3.2" id="lost-writes-with-.blk-file-corruption-7549"> Lost Writes With <code>.blk</code> File Corruption (#7549)</h2>
<p>NATS has several checksum mechanisms meant to detect data corruption in on-disk files. However, we found that single-bit errors or truncation of JetStream’s <code>.blk</code> files could cause the cluster to lose large windows of writes. This occurred even when file corruption was limited to just one or two nodes out of five. For instance, <a href="https://s3.amazonaws.com/jepsen.io/analyses/nats-2.12.1/20251116T061217-blk-bitflip.zip">file corruption in this test run</a> caused NATS to lose 679,153 acknowledged writes out of 1,367,069 total, including 201,286 which were missing even though later values written by the same process were later read.</p>
<p><img src="https://jepsen.io/analyses/nats-2.12.1/blk-bitflip-loss.png" alt="A timeseries plot of write loss over time. A large block of writes is lost around sixty seconds, followed by a few which survive, and then the rest of the successfully acknowledged writes are lost as well."><br>
</p>
<p>In some cases, file corruption caused the quiet loss of <a href="https://s3.amazonaws.com/jepsen.io/analyses/nats-2.12.1/20251116T030143-blk-bitflip-single-loss.zip">just a single message</a>. In others, writes vanished in large blocks. Even worse, bitflips could cause split-brain, where different nodes returned different sets of messages. In <a href="https://s3.amazonaws.com/jepsen.io/analyses/nats-2.12.1/20251120T093731-blk-bitflip-split-brain.zip">this test</a>, NATS acknowledged a total of 1,479,661 messages. However, single-bit errors in <code>.blk</code> files on nodes <code>n1</code> and <code>n3</code> caused nodes <code>n1</code>, <code>n3</code>, and <code>n5</code> to lose up to 78% of those acknowledged messages. Node <code>n1</code> lost 852,413 messages, and nodes <code>n3</code> and <code>n5</code> lost 1,167,167 messages, despite <code>n5</code>’s data files remaining intact. Messages were lost in prefix, middle, and postfix: the stream, at least on those three nodes, resembled Swiss cheese.</p>
<p>NATS is investigating this issue (<a href="https://github.com/nats-io/nats-server/issues/7549">#7549</a>).</p>
<h2 data-number="3.3" id="total-data-loss-with-snapshot-file-corruption-7556"> Total Data Loss With Snapshot File Corruption (#7556)</h2>
<p>When we truncated or introduced single-bit errors into JetStream’s snapshot files in <code>data/jetstream/$SYS/_js_/</code>, we found that nodes would sometimes decide that a stream had been orphaned, and delete all its data files. This happened even when only a minority of nodes in the cluster experienced file corruption. The cluster would never recover quorum, and the stream remained unavailable for the remainder of the test.</p>
<p>In <a href="https://s3.amazonaws.com/jepsen.io/analyses/nats-2.12.1/20251115T142345-snap-bitflip-quorum-break.zip">this test run</a>, we introduced single-bit errors into snapshots on nodes <code>n3</code> and <code>n5</code>. During the final recovery period, node <code>n3</code> became the metadata leader for the cluster and decided to clean up <code>jepsen-stream</code>, which stored all the test’s messages.</p>
<pre><code>[1010859] 2025/11/15 20:27:02.947432 [INF]
Self is new JetStream cluster metadata leader
[1010859] 2025/11/15 20:27:14.996174 [WRN]
Detected orphaned stream 'jepsen &gt;
jepsen-stream', will cleanup</code></pre>
<p>Nodes <code>n3</code> and <code>n5</code> then deleted all files in the stream directory. This might seem defensible—after all, some of <code>n3</code>’s data files <em>were</em> corrupted. However, <code>n3</code> managed to become the leader of the cluster despite its corrupt state! In general, leader-based consensus systems must be careful to ensure that any node which becomes a leader is aware of majority committed state. Becoming a leader, then opting to delete a stream full of committed data, is particularly troubling.</p>
<p>Although nodes <code>n1</code>, <code>n2</code>, and <code>n4</code> retained their data files, <code>n1</code> struggled to apply snapshots; <code>n4</code> declared that <code>jepsen-stream</code> had no quorum and stalled. Every attempt to subscribe to the stream threw <code>[SUB-90007] No matching streams for subject</code>. Jepsen filed issue <a href="https://github.com/nats-io/nats-server/issues/7556">#7556</a> for this, and the NATS team is looking into it.</p>
<h2 data-number="3.4" id="lazy-fsync-by-default-7564"> Lazy <code>fsync</code> by Default (#7564)</h2>
<p>NATS JetStream promises that once a <code>publish</code> call has been acknowledged, it is “successfully persisted”. This is not exactly true. By default, NATS calls <code>fsync</code> to flush data to disk only once every two minutes, but acknowledges messages immediately. Consequently, recently acknowledged writes are generally <em>not</em> persisted, and could be lost to coordinated power failure, kernel crashes, etc. For instance, simulated power failures in <a href="https://github.com/user-attachments/files/23631053/20251119T075152.133-0600.zip">this test run</a> caused NATS to lose roughly thirty seconds of writes: 131,418 out of 930,005 messages.</p>
<p><img src="https://jepsen.io/analyses/nats-2.12.1/power-loss.png" alt="A timeseries plot of data loss over time. Acknowledged writes are fine for the first 125 seconds, then all acknowledged writes are lost for the remainder of the test."><br>
</p>
<p>Because the default flush interval is quite large, even killing a single node at a time is sufficient to cause data loss, so long as nodes fail within a few seconds of each other. In <a href="https://github.com/user-attachments/files/23631363/20251119T085347.396-0600.zip">this run</a>, a series of single-node failures in the first two minutes of the test caused NATS to delete the entire stream, along with all of its messages.</p>
<p>There are only two mentions of this behavior in the NATS documentation. The first is in the <a href="https://docs.nats.io/release-notes/whats_new/whats_new_210">2.10 release notes</a>. The second, <a href="https://docs.nats.io/running-a-nats-service/configuration">buried in the configuration docs</a>, describes the <code>sync_interval</code> option:</p>
<blockquote>
<p>Change the default fsync/sync interval for page cache in the filestore. By default JetStream relies on stream replication in the cluster to guarantee data is available after an OS crash. If you run JetStream without replication or with a replication of just 2 you may want to shorten the fsync/sync interval. You can force an fsync after each messsage [sic] with <code>always</code>, this will slow down the throughput to a few hundred msg/s.</p>
</blockquote>
<p>Consensus protocols often require that nodes sync to disk before acknowledging an operation. For example, the famous 2007 paper <a href="https://www.cs.utexas.edu/~lorenzo/corsi/cs380d/papers/paper2-1.pdf">Paxos Made Live</a> remarks:</p>
<blockquote>
<p>Note that all writes have to be flushed to disk immediately before the system can proceed any further.</p>
</blockquote>
<p>The <a href="https://web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf">Raft thesis</a> on which NATS is based is clear that nodes must “flush [new log entries] to their disks” before acknowledging. Section 11.7.3 discusses the possibility of instead writing data to disk asynchronously, and concludes:</p>
<blockquote>
<p>The trade-off is that data loss is possible in catastrophic events. For example, if a majority of the cluster were to restart simultaneously, the cluster would have potentially lost entries and would not be able to form a new view. Raft could be extended in similar ways to support disk-less operation, but we think the risk of availability or data loss usually outweighs the benefits.</p>
</blockquote>
<p>For similar reasons, replicated systems like <a href="https://www.mongodb.com/docs/manual/reference/replica-configuration/#mongodb-rsconf-rsconf.writeConcernMajorityJournalDefault">MongoDB</a>, <a href="https://www.redhat.com/en/blog/a-guide-to-etcd">etcd</a>, <a href="https://docs.tigerbeetle.com/single-page/">TigerBeetle</a>, <a href="https://thinkingaboutdistributedsystems.blogspot.com/2017/09/what-we-can-learn-from-zookeepers.html">Zookeeper</a>, <a href="https://www.redpanda.com/blog/top-performance-considerations-redpanda">Redpanda</a>, and <a href="https://docs.pingcap.com/tidb/stable/release-5.0.0/#configuration-file-parameters">TiDB</a> sync data to disk before acknowledging an operation as committed.</p>
<p>However, some systems do choose to <code>fsync</code> asynchronously. YugabyteDB’s default is <a href="https://docs.yugabyte.com/stable/reference/configuration/yb-master/#durable-wal-write">to acknowledge un-fsynced writes</a>. Liskov and Cowling’s <a href="http://pmg.csail.mit.edu/papers/vr-revisited.pdf">Viewstamped Replication Revisited</a> assumes replicas are “highly unlikely to fail at the same time”—but acknowledges that if they were to fail simultaneously, state would be lost. Apache Kafka <a href="https://jack-vanlightly.com/blog/2023/4/24/why-apache-kafka-doesnt-need-fsync-to-be-safe">makes a similar choice</a>, but claims that it is not vulnerable to coordinated failure because Kafka “doesn’t store unflushed data in its own memory, but in the page cache”. This offers resilience to the Kafka process itself crashing, but not power failure.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a> Jepsen remains skeptical of this approach: as <a href="https://www.usenix.org/system/files/conference/osdi16/osdi16-alagappan.pdf">Alagappan et al.</a> argue, <a href="https://www.usenix.org/system/files/conference/atc13/atc13-cidon.pdf">extensive</a> <a href="https://pages.cs.wisc.edu/~akella/CS838/F15/838-CloudPapers/hdfs.pdf">literature</a> <a href="https://static.googleusercontent.com/media/research.google.com/en//people/jeff/SOCC2010-keynote-slides.pdf">on</a> <a href="https://aws.amazon.com/message/2329B7/">correlated</a> <a href="https://www.datacenterknowledge.com/cloud/lightning-in-belgium-disrupts-google-cloud-services-updated-">failures</a> <a href="https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Ford.pdf">suggests</a> <a href="https://haeberlen.cis.upenn.edu/papers/glacier-nsdi2005.pdf">we</a> <a href="http://issg.cs.duke.edu/publications/disasters-fast04.pdf">should</a> <a href="https://web.archive.org/web/20080108203642/https://radar.oreilly.com/archives/2007/07/365_main_datace.html">continue</a> <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45499.pdf">to</a> <a href="https://www.energy.gov/oe/august-2003-blackout">take</a> <a href="https://www.webofscience.com/wos/woscc/full-record/WOS:000245079400017?&amp;SID=USW2EC0C58xDjRHQcDMVJJIWW5emx">this</a> <a href="https://web.archive.org/web/20170519044830/https://www.joyent.com/blog/postmortem-for-outage-of-us-east-1-may-27-2014">risk</a> <a href="https://pace.gatech.edu/2025/04/02/cooling-failure-in-coda-datacenter/">seriously</a>. Heat waves, grid instability, fires, lightning, tornadoes, and floods are not necessarily constrained to a single availability zone.</p>
<p>Jepsen suggests that NATS change the default value for <code>fsync</code> to <code>always</code>, rather than every two minutes. Alternatively, NATS documentation should prominently disclose that JetStream may lose data when nodes experience correlated power failure, or fail in rapid succession (<a href="https://github.com/nats-io/nats-server/issues/7564">#7564</a>).</p>
<h2 data-number="3.5" id="a-single-os-crash-can-cause-split-brain-7567"> A Single OS Crash Can Cause Split-Brain (#7567)</h2>
<p>In response to #7564, NATS engineers <a href="https://github.com/nats-io/nats-server/issues/7564">noted</a> that most production deployments run with each node in a separate availability zone, which reduces the probability of correlated failure. This raises the question: how many power failures (or hardware faults, kernel crashes, etc.) are required to cause data loss? Perhaps surprisingly, in an asynchronous network the answer is “just one”.</p>
<p>To understand why, consider that a system which remains partly available when a minority of nodes are unavailable must allow states in which a committed operation is present—solely in memory—on a bare majority of nodes. For example, in a leader-follower protocol the leader of a three-node cluster may consider a write committed as soon as a single follower has responded: it has two acknowledgements, counting itself. Under normal operation there will usually be some window of committed operations in this state.<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>

<p>Now imagine that one of those two nodes loses power and restarts. Because the write was stored only in memory, rather than on disk, the acknowledged write is no longer present on that node. There now exist two out of three nodes which do <em>not</em> have the write. Since the system is fault-tolerant, these two nodes must be able to form a quorum and continue processing requests—creating new states of the system in which the acknowledged write never happened.</p>
<p>Strictly speaking, this fault requires nothing more than a single power failure (or HW fault, kernel crash, etc.) and an asynchronous network—one which is allowed to deliver messages arbitrarily late. Whether it occurs in practice depends on the specific messages exchanged by the replication system, which node fails, how long it remains offline, the order of message delivery, and so on. However, one can reliably induce data loss by killing, pausing, or partitioning away a minority of nodes before and after a simulated OS crash.</p>
<p>For example, process pauses and a single simulated power failure in <a href="https://github.com/user-attachments/files/23638240/20251119T155654.398-0600.zip">this test run</a> caused JetStream to lose acknowledged writes for windows roughly on par with <code>sync_interval</code>. Stranger still, the cluster entered a persistent split-brain which continued after all nodes were restarted and the network healed. Consider these two plots of lost writes, based on final reads performed against nodes <code>n1</code> and <code>n5</code> respectively:</p>
<p><img src="https://jepsen.io/analyses/nats-2.12.1/single-kill-split-brain-n1.png" alt="A plot of data loss on n1. A few seconds of writes are lost around 42 seconds."><br>
</p>
<p><img src="https://jepsen.io/analyses/nats-2.12.1/single-kill-split-brain-n5.png" alt="A plot of data loss on n5. About six seconds of writes are lost at 58 seconds."><br>
</p>
<p>Consumers talking to <code>n1</code> failed to observe a short window of acknowledged messages written around 42 seconds into the test. Meanwhile, consumers talking to <code>n5</code> would miss acknowledged messages written around 58 seconds. Both windows of write loss were on the order of our choice of <code>sync_interval = 10s</code> for this run. In repeated testing, we found that any node in the cluster could lose committed writes, including the node which failed, those which received writes before the failure, and those which received writes afterwards.</p>
<p>The fact that a single power failure can cause data loss is not new. In 2023, RedPanda wrote <a href="https://www.redpanda.com/blog/why-fsync-is-needed-for-data-safety-in-kafka-or-non-byzantine-protocols">a detailed blog post</a> showing that Kafka’s default lazy <code>fsync</code> could lead to data loss under exactly this scenario. However, it is especially concerning that this scenario led to persistent replica divergence, not just data loss! We filed <a href="https://github.com/nats-io/nats-server/issues/7567">#7567</a> for this issue, and the NATS team is investigating.</p>
<table>
<thead>
<tr>
<th>№</th>
<th>Summary</th>
<th>Event Required</th>
<th>Fixed in</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/nats-io/nats-server/issues/6888">#6888</a></td>
<td>Stream deleted on crash in 2.10.22</td>
<td>Crashes</td>
<td>2.10.23</td>
</tr>
<tr>
<td><a href="https://github.com/nats-io/nats-server/issues/7549">#7549</a></td>
<td>Lost writes due to <code>.blk</code> file corruption</td>
<td>Minority truncation or bitflip</td>
<td>Unresolved</td>
</tr>
<tr>
<td><a href="https://github.com/nats-io/nats-server/issues/7556">#7556</a></td>
<td>Stream deleted due to snapshot file corruption</td>
<td>Minority truncation or bitflip</td>
<td>Unresolved</td>
</tr>
<tr>
<td><a href="https://github.com/nats-io/nats-server/issues/7564">#7564</a></td>
<td>Write loss due to lazy <code>fsync</code> policy</td>
<td>Coordinated OS crash</td>
<td>Documented</td>
</tr>
<tr>
<td><a href="https://github.com/nats-io/nats-server/issues/7567">#7567</a></td>
<td>Write loss and split-brain</td>
<td>Single OS crash and pause</td>
<td>Unresolved</td>
</tr>
</tbody>
</table>
<h2 data-number="4" id="discussion"> Discussion</h2>
<p>In NATS 2.10.22, process crashes could cause JetStream to forget a stream ever existed (#6888). This issue was identified independently by NATS and resolved in version 2.10.23, released on 2024-12-10. We did not observe data loss with simple network partitions, process pauses, or crashes in version 2.12.1.</p>
<p>However, we found that in NATS 2.12.1, file corruption and simulated OS crashes could both lead to data loss and persistent split-brain. Bitflips or truncation of either <code>.blk</code> (#7549) or snapshot (#7556) files, even on a minority of nodes, could cause the loss of single messages, large windows of messages, or even cause some nodes to delete their stream data altogether. Messages could be missing on some nodes and present on others. NATS has multiple checksum mechanisms designed to limit the impact of file corruption; more thorough testing of these mechanisms seems warranted.</p>
<p>By default, NATS only flushes data to disk every two minutes, but acknowledges operations immediately. This approach can lead to the loss of committed writes when several nodes experience a power failure, kernel crash, or hardware fault concurrently—or in rapid succession (#7564). In addition, a single OS crash combined with process crashes, pauses, or network partitions can cause the loss of acknowledged messages and persistent split-brain (#7567). We recommended NATS change the default value of <code>fsync</code> to <code>always</code>, or clearly document these hazards. NATS has <a href="https://github.com/nats-io/nats.docs/pull/896">added new documentation</a> to the <a href="https://docs.nats.io/nats-concepts/jetstream#persistent-and-consistent-distributed-storage">JetStream Concepts page</a>.</p>
<p>This documentation <a href="https://docs.nats.io/nats-concepts/jetstream#goals">also describes</a> several goals for JetStream, including that “[t]he system must self-heal and always be available.” This is impossible: the CAP theorem states that Linearizable systems cannot be totally available in an asynchronous network. In our three and five-node clusters JetStream generally behaved like a typical Raft implementation. Operations proceeded on a majority of connected nodes but isolated nodes were unavailable, and if a majority failed, the system as a whole became unavailable. Jepsen suggests clarifying this part of the documentation.</p>
<p>As always, Jepsen takes an experimental approach to safety verification: we can prove the presence of bugs, but not their absence. While we make extensive efforts to find problems, we cannot prove correctness.</p>
<h2 data-number="4.1" id="lazyfs"> LazyFS</h2>
<p>This work demonstrates that systems which do not exhibit data loss under normal process crashes (e.g.&nbsp;<code>kill -9 &lt;PID&gt;</code>) may lose data or enter split-brain under simulated OS-level crashes. Our tests relied heavily on <a href="https://github.com/dsrhaslab/lazyfs">LazyFS</a>, a project of <a href="https://www.inesctec.pt/en">INESC TEC</a> at the University of Porto.<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a> After killing a process, we used LazyFS to simulate the effects of a power failure by dropping writes to the filesystem which had not yet been <code>fsync</code>ed to disk.</p>
<p>While this work focused purely on the loss of unflushed writes, LazyFS can also simulate linear and non-linear torn writes: an anomaly where a storage device persists part, but not all, of written data thanks to (e.g.) IO cache reordering. Our 2024 paper <a href="https://www.vldb.org/pvldb/vol17/p3017-ramos.pdf">When Amnesia Strikes</a> discusses these faults in more detail, highlighting bugs in PostgreSQL, Redis, ZooKeeper, etcd, LevelDB, PebblesDB, and the Lightning Network.</p>
<h2 data-number="4.2" id="future-work"> Future Work</h2>
<p>We designed only a simple workload for NATS which checked for lost records either across all consumers, or across all consumers bound to a single node. We did not check whether single consumers could miss messages, or the order in which they were delivered. We did not check NATS’ claims of Linearizable writes or Serializable operations in general. We also did not evaluate JetStream’s “exactly-once semantics”. All of these could prove fruitful avenues for further tests.</p>
<p>In some tests, we <a href="https://github.com/jepsen-io/nats/blob/4760f97f86350c5c9983478656dbcbcdade33817/src/jepsen/nats/nemesis.clj#L67-L263">added and removed</a> nodes from the cluster. This work <a href="https://github.com/nats-io/nats-server/issues/7545">generated some preliminary results</a>. However, the NATS documentation for membership changes was incorrect and incomplete: it gave <a href="https://github.com/nats-io/nats.docs/pull/893">the wrong command</a> for removing peers, and there appears to be an undocumented but mandatory <a href="https://github.com/nats-io/nats-server/issues/7545#issuecomment-3528168499">health check step</a> for newly-added nodes. As of this writing, Jepsen is unsure how to safely add or remove nodes to a NATS cluster. Consequently, we leave membership changes for future research.</p>
<p><em>Our thanks to <a href="https://www.inesctec.pt/en">INESC TEC</a> and everyone on the LazyFS team, including Maria Ramos, João Azevedo, José Pereira, Tânia Esteves, Ricardo Macedo, and João Paulo. Jepsen is also grateful to Silvia Botros, Kellan Elliott-McCrea, Carla Geisser, Coda Hale, and Marc Hedlund for their expertise regarding datacenter power failures, correlated kernel panics, disk faults, and other causes of OS-level crashes. Finally, our thanks to <a href="https://www.irenekannyo.com/">Irene Kannyo</a> for her editorial support. This research was performed independently by Jepsen, without compensation, and conducted in accordance with the <a href="https://jepsen.io/analyses/ethics">Jepsen ethics policy</a>.</em></p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>Throughout this report we use “acknowledged message” to describe a message whose <code>publish</code> request was acknowledged successfully by some server. NATS also offers a separate notion of acknowledgement, which indicates when a message has been processed and need not be delivered again.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>JetStream also promises “exactly once semantics” in some scenarios. We leave this for later research.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>The CAP theorem’s definition of “availability” requires that all operations on non-faulty nodes must succeed.<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>This is overly conservative: in a system with Linearizable writes, we should never observe a lost message which was acknowledged prior to the invocation of the <code>publish</code> call for an OK message, regardless of process. However, early testing with NATS suggested that it might be better to test a weaker property, and come to stronger conclusions about data loss.<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p><a href="https://www.redpanda.com/blog/why-fsync-is-needed-for-data-safety-in-kafka-or-non-byzantine-protocols">Redpanda argues</a> that the situation is actually worse: a single power failure, combined with network partitions or process pauses, can cause Kafka to lose committed data.<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Some protocols, like Raft, consider an operation committed as soon as it is acknowledged by a majority of nodes. These systems offer lower latencies, but at any given time there are likely a few committed operations which are missing from a minority of nodes due to normal network latency. Other systems, like Kafka, require acknowledgement from <em>all</em> “online” nodes before considering an operation committed. These systems offer worse latency in healthy clusters (since they must wait for the slowest node) but in exchange, committed operations can only be missing from some node when the fault detector decides that node is no longer online (e.g.&nbsp;due to elevated latency).<a href="#fnref6" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>Jepsen contributed some funds, testing, and integration assistance to LazyFS, but most credit belongs to the LazyFS team.<a href="#fnref7" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
  </div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deep dive on Nvidia circular funding (299 pts)]]></title>
            <link>https://philippeoger.com/pages/deep-dive-into-nvidias-virtuous-cycle</link>
            <guid>46196076</guid>
            <pubDate>Mon, 08 Dec 2025 18:48:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://philippeoger.com/pages/deep-dive-into-nvidias-virtuous-cycle">https://philippeoger.com/pages/deep-dive-into-nvidias-virtuous-cycle</a>, See on <a href="https://news.ycombinator.com/item?id=46196076">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        
    <h2 id="nvidia-frenemy-relation-with-openai-and-oracle">NVIDIA frenemy relation with OpenAI and Oracle</h2>
<p>I’ve spent the last 48 hours completely falling down the rabbit hole of
<a href="https://nvidianews.nvidia.com/">NVIDIA’s Q3 Fiscal 2026 earnings report</a>. If
you just skim the headlines, everything looks perfect: Revenue is up 62% to $57
billion, and Jensen Huang is talking about a "virtuous cycle of AI."</p>
<p>But I wanted to understand what was <em>really</em> happening under the hood, so I dug
into the balance sheet and cross-referenced it with all the news swirling
around OpenAI and Oracle. I’m not a professional Wall Street analyst, but even
just connecting the dots myself (with the help of Gemini), I’m seeing some cracks in the "AI Alliance."
While NVIDIA posts record numbers, it feels like their biggest customers are
quietly arming themselves for a breakout.</p>
<p>Here is my take on the hardware market, the "frenemy" dynamics between OpenAI
and NVIDIA, and the "circular financing" theories that everyone—including
Michael Burry, has been talking about.</p>
<p>Here is a quick summary of the points I'll discuss below:</p>
<ul>
<li><a href="#nvidias-earnings-perfection-with-a-side-of-stress">NVIDIA’s Earnings: Perfection with a side of stress</a></li>
<li><a href="#making-sense-of-the-round-tripping-news">Making Sense of the Round-Tripping News</a></li>
<li><a href="#openai-making-moves-to-reduce-dependency-on-nvidia">OpenAI making moves to reduce dependency on NVIDIA</a></li>
<li><a href="#an-interesting-idea-for-oracle-groq-acquisition">An interesting idea for Oracle: Groq acquisition</a></li>
<li><a href="#final-thoughts">Final Thoughts</a></li>
</ul>
<h2 id="nvidias-earnings-perfection-with-a-side-of-stress">NVIDIA’s Earnings: Perfection with a side of stress</h2>
<p>On the surface, NVIDIA is the absolute monarch of the AI era. You can’t argue
with a Data Center segment that now makes up nearly 90% of the company's
business. However, when I looked closer at the financials, I found three
specific things that stood out to me as "red flags."</p>
<ul>
<li><strong>The Cash Flow Mystery:</strong> NVIDIA reported a massive <strong>$31.9 billion in Net
  Income</strong>, but when I checked the cash flow statement, they only generated
  <strong>$23.8 billion in Operating Cash Flow</strong>. That is an $8 billion gap where
  profits aren't converting to cash immediately.</li>
<li><strong>The Inventory Balloon:</strong> I noticed that inventory has nearly doubled this
  year, hitting <strong>$19.8 billion</strong>. Management says this is to prep for the
  "Blackwell" launch, but holding ~120 days of inventory seems like a huge
  capital drag to me.</li>
<li><strong>The "Paper" Chase:</strong> I calculated their Days Sales Outstanding (DSO), and
  it has crept up to about <strong>53 days</strong>. As revenue skyrockets, NVIDIA is
  waiting nearly two months to get paid, which suggests they might be extending
  massive credit terms to enterprise clients to keep the flywheel spinning.</li>
</ul>
<p>My personal read? NVIDIA is "burning the furniture" to build inventory, betting
everything that the <a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/">Blackwell architecture</a>
will sell out instantly in Q4.</p>
<h2 id="making-sense-of-the-round-tripping-news">Making Sense of the Round-Tripping News</h2>
<p>I want to be clear: I didn't discover this next part. It’s been all over the
financial news lately, and if you follow <strong>Michael Burry</strong> (the "Big Short"
guy), you’ve probably seen his tweets warning about "circular financing" and
<a href="https://www.investing.com/news/stock-market-news/michael-burry-warns-of-suspicious-revenue-recognition-after-nvidia-earnings-4369197">suspicious revenue recognition</a>.</p>
<p>I wanted to map it out for myself to see what the fuss was about. Burry shared
a chart recently that visualizes a "web" of deals, and it looks something like
this:</p>
<ol>
<li><strong>Leg 1:</strong> NVIDIA pledges billions (part of a widely reported $100B
    investment roadmap) to <strong>OpenAI</strong>.</li>
<li><strong>Leg 2:</strong> OpenAI signs a massive <strong>$300 billion</strong> cloud contract with
    <strong>Oracle</strong> (Project Stargate) to host its models.</li>
<li><strong>Leg 3:</strong> To fulfill that contract, Oracle turns around and places a <strong>$40
    billion</strong> order for NVIDIA’s GB200 GPUs.</li>
</ol>
<p>Here is the Nano Banana Pro generation I just did for the visual people out there:</p>
<p><img alt="NVIDIA-OpenAI-Oracle Circular Financing" src="https://philippeoger.com/static/img/circular-funding.png"></p>
<p>Burry’s argument, and the reason <a href="https://m.economictimes.com/news/international/us/nvidia-rejects-circular-financing-claims-as-top-short-sellers-push-back/articleshow/125589622.cms">regulators like the DOJ are reportedly looking
into this</a>—is
that this mimics "Round-Tripping." It raises a tough question: If NVIDIA
stopped investing in OpenAI, would OpenAI still have the cash to sign that deal
with Oracle? And would Oracle still buy those chips? If the answer is "no,"
then some of that revenue might be more fragile than it looks.</p>
<h2 id="openai-making-moves-to-reduce-dependency-on-nvidia">OpenAI making moves to reduce dependency on NVIDIA</h2>
<p>The other big shift I’ve been tracking is OpenAI’s pivot. They used to be
NVIDIA’s star pupil, but now they look more like a future rival.
On one hand, they are hugging NVIDIA tight—deploying 10 gigawatts of infrastructure to train GPT-6. But on the
other, they seem to be building a supply chain to kill their dependency on
Jensen Huang.</p>
<p>The evidence is pretty loud if you look for it. "Project Stargate" isn't just a
data center; it's a huge infrastructure plan that includes custom hardware.
OpenAI made some news buying DRAM wafers directly from Samsung and SK Hynix (the 2 main HBM
world provider), bypassing NVIDIA’s supply chain, and many others, as reported 
<a href="https://openai.com/index/samsung-and-sk-join-stargate/">here</a>, 
<a href="https://www.asiafinancial.com/samsung-sk-hynix-building-stargate-korea-using-open-ai">here</a>, or 
<a href="https://www.kedglobal.com/artificial-intelligence/newsView/ked202510010013">here</a>, and widely debated <a href="https://news.ycombinator.com/item?id=46169224#46170844">on Hacker News here</a>.</p>
<p>Plus, the talent migration is telling: OpenAI has poached
key silicon talent, including Richard Ho (Google’s former TPU
lead) back in 2023, and more recently many hardware engineers from Apple (around 40
apparently).</p>
<p>With the <a href="https://openai.com/index/openai-and-broadcom-announce-strategic-collaboration/">Broadcom partnership</a>, 
my guess is OpenAI plans to use NVIDIA GPUs to <em>create</em> intelligence, but run that
intelligence on their own custom silicon to stop bleeding cash, or by betting on 
Edge TPU-like chips for inference, similar to what Google does with its NPU chip.</p>
<p>The big question is, which money is Openai planning on using to fund this? 
and how much influence does NVIDIA has over OpenAI’s future plans?</p>
<p>The $100 billions that NVIDIA is "investing" in OpenAI is not yet confirmed neither,
as reported <a href="https://fortune.com/2025/12/02/nvidia-openai-deal-not-signed-yet-100-billion-rally-colette-kress/">here</a>,</p>
<h2 id="an-interesting-idea-for-oracle-groq-acquisition">An interesting idea for Oracle: Groq acquisition</h2>
<p>Everyone is talking about <strong>Inference</strong> costs right now, basically, how
expensive it is to actually <em>run</em> ChatGPT or any other LLMs versus training it.
Now I'm looking at <strong>Groq</strong>, a startup claiming specifically to be faster and cheaper
than NVIDIA for this task. The founder is <a href="https://www.linkedin.com/in/ross-jonathan/">Jonathan Ross</a>, 
a former Google TPU lead and literally the person that basically had the idea of TPU.</p>
<p>There is another layer to this that I think is getting overlooked as well: <strong>The
HBM Shortage</strong> created by Openai’s direct wafer purchases.</p>
<p>From what I understand, one of the biggest bottlenecks for NVIDIA right now is
HBM (High Bandwidth Memory), which is manufactured in specialized memory fabs
that are completely overwhelmed. However, Groq’s architecture relies on SRAM
(Static RAM). Since SRAM is typically built in logic fabs (like TSMC) alongside
the processors themselves, it theoretically shouldn't face the same supply
chain crunch as HBM.</p>
<p>Looking at all those pieces, I feel Oracle should seriously look into buying Groq.
Buying Groq wouldn't just give Oracle a faster chip, it could give them a chip that is
actually <em>available</em> when everything else is sold out. It’s a supply chain hedge.</p>
<p>It's also a massive edge for its main client, OpenAI, to get faster and cheaper inference.</p>
<p>Combine that with the fact that <a href="https://www.fool.com/investing/2025/12/02/michael-burry-just-sent-a-warning-to-artificial-in/">Oracle’s margins on renting NVIDIA chips are
brutal</a>, reportedly
as low as 14%, then the deal just makes sense. By owning Groq, Oracle could stop
paying the "NVIDIA Tax," fix their margins, and bypass the HBM shortage
entirely.</p>
<p>Groq currently has a valuation of around $6.9 billions, <a href="https://groq.com/newsroom/groq-raises-750-million-as-inference-demand-surges">according to its last funding round
in september 2025</a>.
Even with a premium, Oracle has financial firepower to make that acquisition happen.</p>
<p><strong>But would NVIDIA let that happen? and if the answer is no, then what does that tell us
about the circular funding in place? Is there a Quid pro quo where Nvidia agrees to invest 
100 billions in OpenAI in exchange of Oracle being exclusive to Nvidia?</strong> </p>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>As we head into 2026, when looking at Nvidia, openai and Oracle dynamics, it looks like they are squeezing each 
other balls. I do not know if Nvidia knew about the Openai deal about the wafer memory supply, or was there any collusion?
Does NVIDIA is fighting to maintain exclusivity for both training and inference at Stargate? What kind of chips is Openai
planning on building ? TPU/LPU like? Or more Edge TPU? </p>
<p><a href="https://www.techradar.com/pro/security/could-the-ai-bubble-be-real-this-sage-of-the-2008-market-crash-the-central-character-of-the-big-short-certainly-thinks-so">Michael Burry is betting against the whole
thing</a>. </p>
<p>Me, I’m just a guy reading the reports, I have no way to speculate on this market. But I do know one thing: The AI hardware market 
is hotter than ever, and the next few quarters are going to be fascinating to watch.</p>
<p>I have not discussed much about TPU from Google in this article, but I cover some thoughts <a href="https://philippeoger.com/pages/why-googles-tpu-could-beat-nvidias-gpu-in-the-long-run">about the TPU vs GPU 
in a previous post recently.</a>.
It seems Google responded quickly to the current situation about the memory wafer shortage by securing a <a href="https://www.kedglobal.com/korean-chipmakers/newsView/ked202512010003">major deal 
with Samsung in 2026</a>.</p>
<p><em>Disclaimer: I say very smart things sometimes, but say stupid things a lot more. Take this in consideration when reading this blog post</em></p>

    
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Quanta to publish popular math and physics books by Terence Tao and David Tong (126 pts)]]></title>
            <link>https://www.simonsfoundation.org/2025/12/08/quanta-books-to-publish-popular-math-and-physics-titles-by-terence-tao-and-david-tong/</link>
            <guid>46195225</guid>
            <pubDate>Mon, 08 Dec 2025 17:39:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.simonsfoundation.org/2025/12/08/quanta-books-to-publish-popular-math-and-physics-titles-by-terence-tao-and-david-tong/">https://www.simonsfoundation.org/2025/12/08/quanta-books-to-publish-popular-math-and-physics-titles-by-terence-tao-and-david-tong/</a>, See on <a href="https://news.ycombinator.com/item?id=46195225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p><a href="https://www.quantabooks.org/">Quanta Books</a> is delighted to announce two new upcoming books by mathematician Terence Tao and theoretical physicist David Tong.</p>
<p><em>Six Math Essentials</em> will be Tao’s first math book written for a popular audience. In the book, Tao — a recipient of the Fields Medal and one of the world’s top mathematicians — will explore six ideas that have guided mathematicians throughout history. This short and friendly volume is for all readers, Tao says, because he believes that “mathematics has become unnecessarily intimidating and abstruse to the general public while being more essential than ever in the modern world.” <em>Six Math Essentials</em> will be available internationally, with translated editions in Chinese, French, Greek, Italian, Polish and other languages. It will arrive in U.S. bookstores in November 2026.</p>
<p>Tong’s book, <em>Everything Is Fields</em>, will illuminate quantum field theory — the physics that explains the fundamental makeup of the universe — drawing from Tong’s distinguished track record as a quantum field theorist and public communicator. “This book reveals the hidden unity that ties together particles and forces,” says Tong. “Everything — matter, light, even you — are just waves on a restless sea known as a quantum field.”</p>
<p>“Terry Tao and David Tong are intellectual powerhouses and seasoned communicators,” says Thomas Lin, publisher of Quanta Books and founding editor of the Pulitzer Prize­–winning <a href="https://www.quantamagazine.org/"><em>Quanta Magazine</em></a>. “Their&nbsp;books embody the curiosity and ambition that animate our imprint, and I can’t wait to share them with readers everywhere.”</p>
<p>Quanta Books is an editorially independent subsidiary of the Simons Foundation and a partner imprint of <a href="https://us.macmillan.com/fsg/">Farrar, Straus and Giroux</a>. The imprint publishes books that illuminate and elucidate the central questions and fundamental ideas of modern science for readers, inviting a deeper understanding of the universe through artful storytelling. Quanta Books’ first title, <em>The Proof in the Code</em> by math journalist Kevin Hartnett, will be published in June 2026 and is available for <a href="https://us.macmillan.com/books/9780374620066/theproofinthecode/">preorder</a> now.</p>
<p>For more information, visit <a href="https://www.quantabooks.org/">QuantaBooks.org</a>.</p>
<h2><em>Six Math Essentials</em></h2>
<p>In <em>Six Math Essentials</em>, Tao, the world’s most renowned mathematician, introduces readers to six core ideas that have guided mathematicians from antiquity to the frontiers of what we know today. This elegant volume explores: numbers as the gateway to quantitative thinking, algebra as the gateway to abstraction, geometry as a way to go beyond what we can see, probability as a tool to navigate uncertainty with rigorous thinking, analysis as a means to tame the very large or very small, and dynamics as the mathematics of change. <em>Six Math Essentials</em> — Tao’s first popular math book — offers a glimpse into the workings of an incomparable mind and how he thinks about the creativity, beauty, and interconnectedness of the mathematical enterprise. Math, Tao insists, isn’t magic — it’s a powerful way of thinking that anyone can learn.</p>
<h2><em>Everything Is Fields</em></h2>
<p>In <em>Everything Is Fields</em>, Tong leads readers on a lively tour through quantum field theory. Tong, a leading theoretical physicist and University of Cambridge professor, explores Quantum field theory, or QFT. The theory forms the underlying mathematical framework of the Standard Model, the deepest description we have of the fundamental laws of physics. And, as Tong shows, it reveals a startling truth: that, at our most basic level, we are made not of particles or forces, but fields, fluid-like substances stretched throughout the entire universe. With his infectious sense of wonder and characteristic wit, Tong buoys our journey through the most difficult topic in theoretical physics. He revels in all that we’ve learned about our world and illuminates the questions we’re still trying to answer about the stuff that makes up you, me, and everything else.</p>
<h2><em>The Proof in the Code</em></h2>
<p><em>The Proof in the Code</em> is the definitive account of the birth and rise of Lean, a proof assistant developed at Microsoft that is transforming the enterprise of mathematics and ushering in a new era of human-computer collaboration. Although Lean was originally conceived of as a code-checking program, a small group of mathematicians recognized its potential to become something far more powerful: the “truth oracle” that thinkers have sought for centuries, a tool to definitively verify or refute any mathematical or logical assertion, no matter how complex. This is the story of the grassroots effort to make that dream a reality. Filled with insights about the future of math, computers, and AI, <em>The Proof in the Code</em> is a brilliant work of journalism by Hartnett, a leading math writer whose research and reporting offer a profound answer to a longstanding mystery: Can computers reveal universal truths?</p>
</div><div>
  <p>For more information, please contact <a href="https://www.simonsfoundation.org/cdn-cgi/l/email-protection#86efe8e0e9c6f7f3e7e8f2e7e4e9e9edf5a8e9f4e1"><span data-cfemail="4821262e2708393d29263c292a2727233b66273a2f">[email&nbsp;protected]</span></a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI should only run as fast as we can catch up (185 pts)]]></title>
            <link>https://higashi.blog/2025/12/07/ai-verification/</link>
            <guid>46195198</guid>
            <pubDate>Mon, 08 Dec 2025 17:38:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://higashi.blog/2025/12/07/ai-verification/">https://higashi.blog/2025/12/07/ai-verification/</a>, See on <a href="https://news.ycombinator.com/item?id=46195198">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
  <p><span>07 Dec 2025</span></p><h2 id="ai-should-only-run-as-fast-as-we-can-catch-up">AI should only run as fast as we can catch up.</h2>

<h3 id="the-story-of-daniel-and-eric">The story of Daniel and Eric</h3>

<p>Recently I have spoke with two of my friends who all had fun playing with AI.</p>

<p>Last month, I met with Eric, a fearless PM at a medium size startup who recently got into vibe coding with Gemini.
<!--more--></p>

<blockquote>
  <p>After getting familiarized with Gemini, Eric was genuinely amazed by how AI quickly turns prompt into playable web applications. It served great purpose as a first prototype to communicate ideas to designers and engineers. But Eric really wanted to skip those steps and directly ship it to prod. But he couldn’t really understand that Gemini actually built a single-page HTML file that merely looks like a working app. Sadly, one cannot build a reliable enterprise product out of this. And there is really no effective way for Eric to catch up on these technical details and outpace the engineering team himself.</p>
</blockquote>

<p>Last week, I had coffee with Daniel, a senior staff engineer who recently grew fond of AI coding and found it to be the true force multiplier.</p>

<blockquote>
  <p>Daniel was skeptical of AI at first, but lately he hasn’t wrote a single line of code for months already. What he does is just precisely prompt the AI to create new components in an existing framework (involving Kafka, postgres, AuthN/Z, and k8s infra stuff) and adhering to certain preexisting paradigms. He would just spot-check the correctness of AI’s work and quickly spin up local deployments to verify it’s indeed working. Later, he pushes the changes through code review process and lands those features. All without writing a single line of code and it’s production ready just as if he wrote them himself. To Daniel, building and shipping things fast and scalable is simpler than ever.</p>
</blockquote>

<h3 id="interpolating-between-the-two-stories">Interpolating between the two stories</h3>

<p>After speaking with Eric and Daniel, I suddenly feel that there is an overarching theme around the use of AI that we can probably interpolate out of the stories here. And after pondering for a weekend, I think I can attempt to describe it now: it’s the problem of <strong>reliable engineering - how can we make AI work reliably</strong>.</p>

<p>With the AI superpower, one can task it to do all crazy things on the internet with just typing a few lines of prompt. AI always thinks and learns faster than us, this is undeniable now. However, to make AI work actually useful (not only works, but reliable and trustworthy), we also need to catch up with what the AI does as quickly as possible.</p>

<p>It’s almost like - we need to send the AI off to learn and think as fast as possible, but we also need to catch up as soon as possible to make it all relevant. And the speed we catch up things is critical to whether AI can help us effectively do these tasks. For the case of Daniel, he can spot-check and basically just skim through AI’s work and know for sure it’s doing the right thing with a few simple tests steps to verify, hence his results are more reliable. Whereas for Eric, he needs to basically learn software development from the bottom up to comprehend what the AI has done, and that really doesn’t give him the edge to outpace engineering teams to ship features reliably by himself.</p>

<h3 id="where-ai-exploded-fast-verification-slow-learning-and-creation">Where AI exploded: fast verification, slow learning and creation</h3>

<p>To generalize the problem again, I think for all the tasks we do, we can break them down into two parts: learning/creation and verification. Basically doing the task and checking if the task is done right. Interestingly, this gives us a good perspective to our relationship with AI on performing such tasks.</p>

<p>Effort wise, if <strong>verification «&nbsp;learning/creation</strong>, one can very effectively check AI’s work and be confident about its reliability.</p>

<p>If <strong>verification ~= learning/creation</strong>, one spends equal amount of time checking AI’s work. It’s not a big win, maybe AI becomes a good automation script to cut down some boilerplate.</p>

<p>If <strong>verification&nbsp;» learning/creation</strong>, one cannot be sure about AI’s work that easily, and we are in the vibe-land.</p>

<p>A very good example of the first category is image (and video) generation. Drawing/rendering a realistic looking image is a crazily hard task. Have you tried to make a slide look nicer? It will take me literally hours to center the text boxes to make it look “good”. However, you really just need to take a look at the output of Nano Banana and you can tell if it’s a good render or a bad one based on how you feel. The verification is literally <strong>instantaneous</strong> and <strong>effortless</strong> because it’s all encoded as feeling or vibes in your brain. “Does this look right?” probably can be answered in the span of milliseconds by your vision cortex. There is also no special knowledge required - <strong>human beings have been evaluating visual images since birth</strong>, hardwired into our instincts.</p>

<p>The significant cost asymmetry can greatly explain why AI image generation exploded. If we can look for similar scenarios, we can probably identify other “killer” use cases of AI as well.</p>

<h3 id="verification-debt-scarier-than-tech-debt">Verification debt: scarier than tech debt</h3>

<p>However, if we go down into the bottom of the spectrum where verification becomes more intense - requiring domain knowledge, technical expertise, industry know-hows to tell if the AI is producing slop or not, we will enter this dark age of piling verification debt. More things are being created, but we are lagging behind to check if any of it actually works to our satisfaction.</p>

<p>If an organization keeps vibe-coding without catching up with verification, those tasks can quickly end up as “debts” that needs to be verified. When verification becomes the bottleneck, dangerous things can happen if we still want to move fast - we will risk ourselves running unverified code and having unexpected side effects that are yet to be validated. It can also apply to other fields - imagine asking AI to craft a new vaccine and you don’t want to wait for FDA to use it.</p>

<p>I’ve come across a few blog posts that talks about Verification Debt already. I think it’s genuinely a good problem for technical leaders to have in their mind in this era.</p>

<h3 id="verification-engineering-is-the-next-context-engineering">Verification Engineering is the next Context Engineering</h3>

<p>AI can only reliably run as fast as we check their work. It’s almost like a complexity theory claim. But I believe it needs to be the case to ensure we can harvest the exponential warp speed of AI but also remain robust and competent, as these technologies ultimate serve human beings, and us human beings need technology to be reliable and accountable, as we humans are already flaky enough ;)</p>

<p>This brings out the topic of Verification Engineering. I believe this can be a big thing after Context Engineering (which is the big thing after Prompt Engineering). By cleverly rearranging tasks and using nice abstractions and frameworks, we can make verification of AI performed tasks easier and use AI to ship more solid products the world. No more slop.</p>

<p>I can think of a few ideas to kickoff verification engineering:</p>

<ul>
  <li>How to craft more technicall precise prompts to guide AI to surgically do things, rather than vibing it.</li>
  <li>How to train more capable technical stakeholders who can effectively verify and approve what AI has done.</li>
  <li>How to find more tasks that are relatively easy to verify but rather hard to create.</li>
  <li>How to push our theoretical boundaries of what things we can succinctly verify (complexity theory strikes again).</li>
</ul>

<h3 id="where-next">Where next</h3>

<p>I believe whoever figures out ways to effectively verify more complex tasks using human brains, can gain the most benefit out of the AI boom. Maybe we need to discard traditional programming languages and start programming in abstract graph-like dataflow representations where one can easily tell if a thing is done right or wrong despite its language or implementation details.</p>

<p>Maybe our future is like the one depicted in Severance - we look at computer screens with wiggly numbers and whatever “feels right” is the right thing to do. We can harvest these effortless low latency “feelings” that nature gives us to make AI do more powerful work.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We collected 10k hours of neuro-language data in our basement (110 pts)]]></title>
            <link>https://condu.it/thought/10k-hours</link>
            <guid>46195109</guid>
            <pubDate>Mon, 08 Dec 2025 17:33:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://condu.it/thought/10k-hours">https://condu.it/thought/10k-hours</a>, See on <a href="https://news.ycombinator.com/item?id=46195109">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mobile-blog-content">
                        <p>
                            Over the last 6 months, we collected ~10k hours of data across thousands of unique individuals. As far as we know, this is the largest neuro-language dataset in the world.<sup><a href="#fn1-mobile">[1]</a></sup><span id="fn1-mobile" data-footnote-number="[1]">See <a href="https://openneuro.org/datasets/ds002345/versions/1.1.4">here</a>, <a href="https://openneuro.org/datasets/ds003643/versions/2.0.7">here</a>, <a href="https://arxiv.org/abs/2504.21214">here</a>, <a href="https://www.nature.com/articles/sdata2018291">here</a>, and <a href="https://arxiv.org/abs/2502.17480">here</a> (discussion only, no data available) for some of the larger datasets. See recent papers discussing the problem of small datasets <a href="https://www.sciencedirect.com/science/article/pii/S1878929324001312">here</a>, <a href="https://arxiv.org/abs/2407.07595">here</a>, and <a href="https://thesai.org/Downloads/Volume16No4/Paper_85-Speech_Decoding_from_EEG_Signals.pdf">here</a>.</span> Why did we do this? We train thought-to-text models. That is, we train models to decode semantic content from noninvasive neural data. Here are some entirely zero-shot examples:
                        </p>

                        <p><span id="fn2" data-footnote-number="[2]">The neural data is taken from the seconds leading up to but not including the time when the subject typed or spoke, meaning that the model detects an idea before the subject even compiles that idea down into words.</span>
                        </p>
                        <table>
                            <thead>
                                <tr>
                                    <th>Ground truth</th>
                                    <th>Model prediction (based ONLY on neural data)<sup><a href="#fn2">[2]</a></sup></th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>the room seemed colder</td>
                                    <td>there was a breeze even a gentle gust</td>
                                </tr>
                                <tr>
                                    <td>do you have a favorite app or website</td>
                                    <td>do you have any favorite robot</td>
                                </tr>
                                <tr>
                                    <td>then she smiled faintly and nodded</td>
                                    <td>she shrugged, hoping to look indifferent.</td>
                                </tr>
                                <tr>
                                    <td colspan="2">All examples are zero-shot to new subjects, whom the model has never seen before.</td>
                                </tr>
                            </tbody>
                        </table>

                        <p>
                            We'll write about the model in a future post. But before you can train a model that generalizes to new people, you need to get many thousands of hours of data. When we started, the existing datasets were either inapplicable or tiny. Most were in the low hundreds of hours (if that), and most had tens or, at a stretch, hundreds of subjects.
                        </p>

                        <p><img src="https://condu.it/thought/10k-hours/images/image4.png" alt="Data collection setup"></p><p>
                            So we got thousands of people to come wear headsets in our basement. This post is about how we collected our dataset—what participants do, the hardware and software involved, and what we learned about operations and ML when we scaled it up.
                        </p>
<nav id="toc">
                            <p>Contents</p>
                            <ul>
                                <li><a href="#introduction">Introduction</a></li>
                                <li><a href="#what-participants-do">What participants actually do</a></li>
                                <li><a href="#headsets">Headsets</a></li>
                                <li><a href="#modalities">Modalities</a></li>
                                <li><a href="#training-vs-inference">Training vs. inference</a></li>
                                <li><a href="#noise-reduction">Noise Reduction</a></li>
                                <li><a href="#why-noise-matters-less">Why noise matters much less at scale</a></li>
                                <li><a href="#scaling-the-operation">Scaling the operation</a></li>
                                <li><a href="#people-and-bookings">People and bookings</a></li>
                                <li><a href="#marginal-cost">Marginal cost per usable hour of data</a></li>
                                <li><a href="#now-what">Now What</a></li>
                                <li><a href="#appendix">Appendix: Booths</a></li>
                            </ul>
                        </nav>

                        <h2 id="what-participants-do">What participants actually do</h2>

                        <p>
                            A participant comes in, signs a consent form, and sits down in a booth. A session manager fits a headset onto them and starts the session. Then, the participant has a freeform conversation with an LLM for two hours.
                        </p>

                        <p>
                            Sessions vary. Some are listening and speaking with an LLM, and some are reading and typing.<sup><a href="#fn3">[3]</a></sup><span id="fn3" data-footnote-number="[3]">We use Deepgram for audio transcription, OSS120B on Cerebras for the LLM responses, and ElevenLabs for voicing certain replies. In the past, we used various Gemma and Llama models on Groq.</span> The goal is to maximize the amount that subjects type or say during the two-hour period, without constraining the topics they discuss.<sup><a href="#fn4">[4]</a></sup><span id="fn4" data-footnote-number="[4]">In the beginning, we included tasks like 'retype this sentence', or 'paraphrase this but use this different tone'. Over time, we eliminated these and replaced them with more freeform conversation. We still include a few baseline tasks for calibration and easy model evals.</span> Each session produces multimodal neural data time-aligned with text and audio.
                        </p>

                        <p>
                            Participants have to touch-type without looking at the keyboard. In the beginning, participants would occasionally press a crazy key combination that crashed or closed the software. We could have fixed this in the code, but that would've taken time—so instead we 'simplified' the keyboards.
                        </p>

                        <p><img src="https://condu.it/thought/10k-hours/images/image6.jpg" alt="Simplified keyboard"></p><p>
                            What your participants type—and whether it's remotely coherent—is a more difficult problem. We implemented a token quantity/quality scoring system that determines if we invite a participant back for future sessions, and we make sure participants know about this so they're incentivized to engage.
                        </p>

                        <p>
                            Below are passages typed by participants in May vs. October:
                        </p>

                        <table>
                            <thead>
                                <tr>
                                    <th>May:</th>
                                    <th>October:</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>SO, AI NEEDS THIS CODE: 1, THOSE WHO BELONG TO THE CHURCH CAN NEVER BE FOUND GUILTY WHEN SINNED 2. HIDE THE SINS! CRIMES! WHICH IS A FEDERAL CRIME BUT THOSE ARE THE OLDEST TEACHINGS OR LAWS OF CHRISTIANITY! AND WE ARE ALL LIVING IN THIS HELL IN THE WEST. CHRISTIANS ARE DEEMED CRIMINALLY INSANE, PER A JEWISH THERAPIST, AND THE TEACHINGS ARE SUGGEST VERY GROTESQUE CRIMES AND SHE SHOWED ME THE PASSAGES IN THE FAKE VATICAN BIBLE. NO WONDER IS WAS NOT WRITTEN BY JESUS! DUH!</td>
                                    <td>I guess the way I am thinking about it is that since the amygdala is the irrational fight or flight part of the brain it would activate/be used with a higher frequency when a human being finds themselves under threat. Humans tend not to find themselves under threat when experiencing loving and therefore safe interactions. Therefore,when engaging in positive social interaction, the amygdala is less reactive. I don't know exactly what has sparked this interest other than a curiosity to understant the human brain and how we make decisions and funtion as social beings. I guess it all could stem from my interest in improving well being/ reducing suffering.</td>
                                </tr>
                                <tr>
                                    <td>l''''''''''''''''''''''''''''xcccccccccccccccccccccccccccccccccccccccccccczzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzccccckkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkllllllllllllllllllllllllllllllllllllllll,llll</td>
                                    <td>I would travel to local elementary schools and teach kids how to ride bikes as well as teach them bike safety stuff. That was the most enjoyable part and stuck with me the most. I think it was seeing their excitement when they would get riding on their own. And watching their independence and confidence flourish. It was a super rewarding experience. This is so funny, it feels like a job interview. I think its the beginning of a newfound independence and selfhood for a lot of the kids.They get to move on their own accord and get to experience the world in a new way, its the first taste of freedom.</td>
                                </tr>
                            </tbody>
                        </table>

                        <p>
                            You'll also get much better engagement if the LLM personalizes the sessions. For the first few months of data collection, participants chatted with the LLM about generic, banal topics. Now, participants introduce themselves to the LLM very early in the session, and the LLM uses that context to tailor back-and-forth conversation to the particular person it's talking to. As a result, participants engage more with the LLM—and therefore provide better data.
                        </p>

                        <p><img src="https://condu.it/thought/10k-hours/images/image11.png" alt="Participant session interface">

                        <img src="https://condu.it/thought/10k-hours/images/image5.png" alt="Ventilation setup"></p><p>
                            Participants often raised discomfort as a distraction from the sessions. Ventilation was a common complaint. So, we bought <a href="https://www.amazon.com/dp/B07FPFVZTZ">these fans</a> and <a href="https://www.amazon.com/dp/B0791V19H7">these pipes</a>. These can't be plugged in next to the data collection booths (because of electrical interference), so we snake an ~8m ventilation pipe along the ceiling from a central hub into each booth.
                        </p>

                        <p>
                            Making the headsets comfortable to wear is difficult, since you need to press a 4-pound helmet into participants' scalps. To address this, we cut polygonal sections of padding that compress inwards so as to not cover any sensors.
                        </p>

                        <div>
                            <p><img src="https://condu.it/thought/10k-hours/images/image1.png" alt="% of participants by # of sessions completed"></p><p>% of participants by # of sessions completed</p>
                        </div>

                        <p>
                            At first, &lt;20% of participants even finished their first session. Now, &gt;97% complete their first session, and almost half sign up for more.
                        </p>

                        <h2 id="headsets">Headsets</h2>

                        <p>
                            There were two main things we thought about when we designed the headsets. The first was what modalities the headsets should have, and the second was how training headsets should compare to inference ones.
                        </p>

                        <h4 id="modalities">Modalities</h4>

                        <p>
                            There are many ways of measuring brain data: common modalities include EEG, fMRI, fNIRS, transcranial ultrasound, and MEG. We tried various modalities, but the main takeaway we found is that you need multiple. You can't practically make it work with just one, even if you get the best possible headset of that modality.
                        </p>

                        <p>
                            None of the available multimodal headsets were good enough (far worse than the best single modality versions of each). So we bought some of the best single-modality headsets, took them apart, 3D printed parts to make them fit together, and combined them into our own optimized multimodal headsets.<sup><a href="#fn5">[5]</a></sup><span id="fn5" data-footnote-number="[5]">We have a 3D printer at our office that we use for prototyping and designing pieces. For the ones we put in production in data collection, we send them out to a professional printer and have them printed in bulk. We usually have them printed in Pa-F Nylon, which is stiffer and holds up longer before needing replacement.</span>
                        </p>

                        <p>
                            If you want your model to perform well across various neural modalities and across sensors from different providers, you should design and train on a range of headsets. We buy sensors from several providers, combine them into different multimodal headsets, and then use those headsets essentially interchangeably. We also designed our data format such that data from many kinds of sensors fit nicely into a single, standard framework that our model can parse.
                        </p>

                        <h4 id="training-vs-inference">Training vs. inference</h4>

                        <p>
                            Designing headsets for training is very different from designing headsets for inference—what we'll eventually sell as a product. Training headsets should be maximally sensor-dense, can afford to be expensive, and don't need to be as comfortable. In inference, though, few people are willing to wear a 4-pound helmet as they go about their day—even if it can read their minds. So, we did ablation studies. The take-away here is that you should only think about the inference headset once you've trained a model on your data, because that lets you figure out the exact minimal inference headset.
                        </p>

                        <div>
                            <div>
                                <p><img src="https://condu.it/thought/10k-hours/images/image10.jpg" alt="Inference headset concept"></p><p>(inference headset concept)</p>
                            </div>
                            <div>
                                <p><img src="https://condu.it/thought/10k-hours/images/image3.jpg" alt="Training headset concept"></p><p>(training headset concept)</p>
                            </div>
                        </div>

                        <p>
                            What should be shared across both training and inference is your data format. Initially, we got this wrong: we used HDF5 for data collection and storage and processed it into MDS for model training. Eventually, we switched to using Zarr 3 for everything. Zarr 3 gives us chunked, cloud-native storage with the same format for training and inference.
                        </p>

                        <p>
                            You might think a crucial consideration for training (and for inference) is noise. At first, so did we.
                        </p>

                        <details open="">
                            <summary><h2 id="noise-reduction">Noise Reduction</h2></summary>
                            <div>
                                    <p>
                                        The sources of noise you'll notice are very different depending on which modality you use. That said, all modalities of noninvasive neural data are noisy. We're not disclosing all the modalities or headset configurations we use here, but we'll use EEG as an example. The important lessons, which apply to any modality, are that (1) noise-reduction is only worth it if it doesn't cripple the amount of data you can collect, and (2) you should always keep in mind the logistics of running sessions and recruiting participants.
                                    </p>

                                    <h4 id="gel">Gel</h4>

                                    <p>
                                        The classic wisdom is that gel makes EEG data much better, and without it, your data will be substantially noisier. But if you care about data quantity, you probably shouldn't use gel.
                                    </p>

                                    <p>
                                        It takes up to 30 minutes to apply, and we allocate ~3 minutes for the time between one participant finishing a session and the next one starting.<sup><a href="#fn6">[6]</a></sup><span id="fn6" data-footnote-number="[6]">Most kinds of gel also dry out over time, meaning that we likely would've had to make sessions shorter—and fewer participants would have signed up if they had to let us put gel in their hair.</span> Using gel would've &gt;2xed the marginal cost of an hour of data.
                                    </p>

                                    <p>
                                        Instead, we got the highest quality dry electrodes we could, and we spring-loaded the 3D printed pieces so that a spring presses the electrode against the head. We had to try <a href="https://www.acxesspring.com/english/catalogsearch/advanced/result/?category=cs&amp;unit_measure=en&amp;cs_od%5Bfrom%5D=0.4&amp;cs_od%5Bto%5D=0.6&amp;cs_fl%5Bfrom%5D=0.9&amp;cs_fl%5Bto%5D=1.1&amp;material_type=0&amp;cs_rt%5Bfrom%5D=0.3&amp;cs_rt%5Bto%5D=2&amp;form_key=uJgOhWqe9j9Ipypv">various strengths of spring</a> because we wanted to maximize contact without causing discomfort. Generally, stronger springs work well at the front and back of the head; and weaker ones on the top of the head and above the ears.
                                    </p>

                                    <p>
                                        The essential take-away here is that the fast switching time (2-3 mins) is super important. If you care about data quantity, you should operate with some fixed switching time as a constraint, and limit yourself only to interventions that improve quality without violating that constraint.
                                    </p>

                                    <h4 id="electrical-noise">Electrical noise</h4>

                                    <p>
                                        Most buildings have a lot of background electrical noise, which shows up on any EEG power spectrum—in particular, a spike at 60Hz, the U.S. power line frequency. Here is what that spike looks like with no filtering:
                                    </p>

                                    <p><img src="https://condu.it/thought/10k-hours/images/image2.png" alt="EEG power spectrum showing 60Hz spike"></p><p>(not from our dataset—example from <a href="https://mne.discourse.group/t/psd-power-peaks-at-25-and-0-hz/6148">MNE</a>.<sup><a href="#fn7">[7]</a></sup><span id="fn7" data-footnote-number="[7]">Worth noting that this data is from outside of the United States, where the power line frequency is 50hz rather than 60hz.</span>)</p>

                                    <p>
                                        At first, we tried to get around this by triple-layering <a href="https://www.uline.com/Product/Detail/H-3086/Anti-Fatigue-Mats/Cadillac-Mat-3-8-thick-2-x-2-Black?pricode=WA9154&amp;gadtype=pla&amp;id=H-3086&amp;gad_source=1&amp;gad_campaignid=10688098445&amp;gbraid=0AAAAAD_uetMrP89IMnou9MvnpRfxBK41U&amp;gclid=CjwKCAiAraXJBhBJEiwAjz7MZb_nvjmv-n2pAcf2_9lEM78xU_45GjlWEVIgyg9rE71AHd_dCU8fHBoCGfAQAvD_BwE">rubber mats</a> around the equipment.
                                        But the fundamental issue was that some of the headset components weren't wireless, so we had to plug them into the wall (meaning that the rubber didn't help that much, though it does help a bit and we still use it).
                                    </p>

                                    <p>
                                        We then tried getting <a href="https://www.amazon.com/Furman-Conditioner-Protector-Electrical-Extension/dp/B008A85LL2">adapters that plug into the wall and output clean power</a>. This didn't really help.
                                        Eventually, we used <a href="https://www.ankersolix.com/products/c1000?variant=49702371524938">Anker batteries</a> and only plugged stuff into the DC adapters (we got extra batteries so we could switch them out to charge). This helped a lot, but the thing that really helped was turning off all the power to that side of the building.
                                    </p>

                                    <p>
                                        Turning the power off had a lot of downsides. It meant we had to drag ~30 lb batteries back and forth an average of once an hour to charge, and it was difficult to power some of the headsets with only DC power, which made us drop ~10% of frames.
                                    </p>

                                    <p>
                                        Luckily, after a few thousand hours, noise stopped mattering as much.
                                    </p>

                                    
                                </div>
                        </details>

                        <h2 id="why-noise-matters-less">Why noise matters much less at scale</h2>

                        <p>
                            The key observation: data quantity swamps every noise-reduction technique once you cross ~4k-5k hours.
                        </p>

                        <p>
                            When we only had a few hundred hours, denoising was mandatory. Every extra source of variation—different booths, power setups, posture changes—meant the same neural pattern showed up in fewer comparable examples, so the encoder had less to learn from. Keeping the environment stable and electrically boring was the easiest way to keep the problem manageable.
                        </p>

                        <p>
                            At ~4-5 thousand hours, that constraint changes. The model now sees the same patterns across many people and setups, and has enough capacity to represent both the mess and the neural signal.<sup><a href="#fn8">[8]</a></sup><span id="fn8" data-footnote-number="[8]">Similar effects appear in other modalities. Speech models like Whisper, trained on hundreds of thousands of hours of diverse, weakly supervised web audio, show that trading label quality for sheer quantity improves robustness and generalization (see <a href="https://arxiv.org/abs/2212.04356">here</a>). Video-language models trained on uncurated instructional videos learn strong representations even though a large fraction of clip-caption pairs are misaligned or noisy (see <a href="https://arxiv.org/pdf/1912.06430">here</a>). In each of these cases, once the dataset is sufficiently large and diverse, total volume of data outweighs strict curation and noiselessness for downstream robustness.</span> The decoder gets enough examples to tell apart "this changes with the text" from "this is just the room". At that point, data quantity overwhelms noise, and most of the extreme noise-reduction work stops buying much—so we turned the power back on.
                        </p>

                        <h2 id="scaling-the-operation">Scaling the operation</h2>

                        <p>
                            After a few thousand hours, noise stops being the thing to worry about in data collection. The things that matter most are
                        </p>

                        <ol>
                            <li>The raw number of people you can put in headsets; and</li>
                            <li>The marginal cost per usable hour of data.</li>
                        </ol>

                        <h4 id="people-and-bookings">People and bookings</h4>

                        <p><img src="https://condu.it/thought/10k-hours/images/image12.png" alt="Participant recruitment poster"></p><p>
                            Since we run sessions 20 hours/day, 7 days/week, we get a lot of bookings and see a lot of people. An Uber driver once started telling us about 'this great new way to earn money in SF'—and it turned out to be our data collection.
                        </p>

                        <p>
                            Surprisingly central to getting headset occupancy high enough was building a custom booking suite.<sup><a href="#fn9">[9]</a></sup><span id="fn9" data-footnote-number="[9]">We tried Calendly, You Can Book Me, and various other things before making our own. In the end, all the available booking systems had different issues, e.g. not allowing us to blacklist certain people, not allowing dynamic pricing or overbooking, and limited visibility for participants and bookings.</span> There are two main tenets: dynamic pricing and dynamic overbooking. Because few people book at 7am on a Sunday, dynamic pricing means participants are paid more for that slot. Because many people book at 7pm on a Friday, but few of them actually show up, dynamic overbooking allows more people to sign up. The overbooking algorithm can also access information about particular participants.<sup><a href="#fn10">[10]</a></sup><span id="fn10" data-footnote-number="[10]">E.g. if Alice has reliably shown up for sessions before, the algorithm lowers the expected total no-show rate during future times when Alice has booked.</span>
                        </p>

                        <p><img src="https://condu.it/thought/10k-hours/images/image7.png" alt="Booking system dashboard"></p><p>
                            In order to get your model to generalize, it's important to get a dataset of thousands of unique individuals. That is *not* just thousands of hours from dozens or hundreds of individuals. In an ideal world, most participants would only come in for one or two sessions, but that trades off hard against total hours. We cap the number of sessions that any one participant is allowed to do at 10 sessions. Before we introduced the cap, our schedule was fantastically full, but we weren't getting enough unique participants because long-term returners were filling all the slots.
                        </p>

                        <p>
                            Even so, participant recruitment gets easier with scale. We now have participant-ambassadors, whom we pay to recruit more participants for us even after they've completed their 10 sessions.<sup><a href="#fn11">[11]</a></sup><span id="fn11" data-footnote-number="[11]">Since the start, we've tried dozens of ways to directly recruit first-time participants. By far the most effective has been Craigslist. Almost every day since April, we've posted a listing—<a href="https://sfbay.craigslist.org/sfc/lbg/d/san-francisco-chat-with-ai-get-paid-up/7896688506.html">in</a> <a href="https://sfbay.craigslist.org/sfc/crg/d/san-francisco-think-fast-type-fast-earn/7897143450.html">sections</a> <a href="https://sfbay.craigslist.org/sfc/wrg/d/san-francisco-two-hours-one-headset/7893366908.html">from</a> '<a href="https://sfbay.craigslist.org/sfc/cpg/d/san-francisco-earn-550-testing-new/7896689107.html">computer</a>' <a href="https://sfbay.craigslist.org/sfc/wrg/d/san-francisco-up-to-550-to-chat-with-ai/7894838033.html">to</a> '<a href="https://sfbay.craigslist.org/sfc/crg/d/san-francisco-up-to-550-to-chat-with-ai/7895293226.html">creative</a>' <a href="https://sfbay.craigslist.org/sfc/lbg/d/san-francisco-join-the-research-thats/7893367163.html">to</a> '<a href="https://sfbay.craigslist.org/sfc/lbg/d/san-francisco-we-strap-device-to-your/7897143946.html">labor gigs</a>'—that advertises a $50 payout for wearing a helmet and typing for two hours.</span>
                        </p>

                        <h4 id="marginal-cost">Marginal cost per usable hour of data</h4>

                        <p>
                            Between May and October, we cut the marginal cost per usable hour of data by ~40%. Here are the highest-impact things we did.
                        </p>

                        <p>
                            In August, we entirely rewrote the data format and data collection backend to catch issues in the data live, before participants complete two potentially useless hours of data collection. The sessions stream to the cloud, and we automatically sanity-check each session in real time for modality dropout, token quality, timestamp drift, and alignment jitter. Any session that falls outside the tolerance bands gets flagged for session managers to restart or debug.<sup><a href="#fn12">[12]</a></sup><span id="fn12" data-footnote-number="[12]">This is only possible because we changed our data format to use Zarr 3 and optimized it for fast quality checks.</span>
                        </p>

                        <p>
                            This change alone cut the marginal cost of data by ~30% and ~1.5xed the amount of usable data we collect.
                        </p>

                        <p>
                            Second, we enable session managers to run more sessions in parallel without sacrificing supervision. We put <a href="https://www.amazon.com/dp/B0F2HCHS52">EVERSECU cameras</a> in the booths, so session managers can monitor and speak directly to participants without leaving the main supervision station. We also made a unified booking -&gt; intake -&gt; data collection backend, which massively simplifies the participant intake process and improves security.<sup><a href="#fn13">[13]</a></sup><span id="fn13" data-footnote-number="[13]">As one example of how the unified system helps, it detects how much support a given participant is likely to need (based on, e.g., whether they've attended sessions before, their answers to questions on the booking form, etc.) and how many concurrent bookings are already scheduled for that participant's sign-up time. If needed, it can also stagger booking start-times by 5-10 minutes so session managers don't struggle with an onslaught of arrivals all at once.</span>
                        </p>

                        <h2 id="now-what">Now What</h2>

                        <p>
                            The steps to building thought-to-text have always been clear: (1) collect a dataset; (2) train a model; (3) close the loop. We're now well into step two—we spend &gt;95% of our time training models and very little time actively thinking about data collection.
                        </p>

                        <p>
                            But you can't have a model without a dataset, so you do need to get this part right.
                        </p>

                        <p>
                            If you're collecting a similar kind of data, training multi-modal models, or want to give us cheap GPUs, we'd love to hear from you. Please reach out to us at <a href="mailto:contact@condu.it">contact@condu.it</a>.
                        </p>

                        <p>
                            And if this dataset sounds cool to you and you want to train models with it, we're hiring engineers and researchers. Reach out to us at <a href="mailto:jobs@condu.it">jobs@condu.it</a>.
                        </p>

                        <hr>

                        <details>
                            <summary><h2 id="appendix">Appendix: Booths</h2></summary>
                            <div>
                                    <p>
                                        We started out putting each participant in a separate room at a normal work station. We saw huge noise spikes in the data from participants moving their heads, and sometimes they'd get up and walk around with the headset on or take the headset off without telling us.
                                    </p>

                                    <p>
                                        The solution to this was putting multiple booths in one shared room for easier supervision. We also installed chinrests that hold participants' heads still, which help reduce motion artifacts in the data.<sup><a href="#fn14">[14]</a></sup><span id="fn14" data-footnote-number="[14]">We initially wanted to get something like an optician's chinrest, but the bar across the forehead got in the way of the headset. We ended up buying <a href="https://www.amazon.com/Vondynote-Desktop-Speaker-Monitor-Adjustable/dp/B0C1NHTNN6/ref=sr_1_5?crid=1EMUTNK27HD3R&amp;dib=eyJ2IjoiMSJ9.oKWo-hoGxOY41YRBZ1z6jBnI3UaB-8lUM7TE5fQyokQL8pxhLH2H0eBShU8HGHo_hzWWmfClRVX3PUbFWmaAEeC6ECWAUmlxEogINUhyreaSAV2TmKUtbGVKEuexz4CFNNIOe1E50kSNIr6HneQca7p5eK0AQ3w3_8dV9--G0RmcXOFZed1t6tXNSNxZ8mBjo3BhejOb1wQJ1X2UiaTnZ4KWuPZTgIKyO7rR_CpAV9DywLeCwPJbXKXqNXfR_N5M43K_3mCoM2uEtLK68-cjEtT7nPo6e7VjvuC_-kib3i4.HhLWkX9pJEmlPk5gG6qVEWdN8dywr0f-9_JSJVH0PsA&amp;dib_tag=se&amp;keywords=speaker%2Bstand&amp;qid=1764383726&amp;s=musical-instruments&amp;sprefix=speaker%2Bstan,mi,170&amp;sr=1-5">speaker stands</a> and sawing pieces of wood to screw onto them. This works pretty well, although participants don't always use them. You should ensure that any desks, chairs, and chinrests that you buy are height-adjustable.</span>
                                    </p>

                                    <p>
                                        Now, we use <a href="https://zenbooth.net/products/zenbooth-duo?srsltid=AfmBOopJCM2lQH9yLpdLhC7OVeurtDVG1i-mPuu-NiCQ_Hh39xfmQS3g&amp;variant=32767488819271">these nice phone booths</a> (~$10k each, though you can sometimes get them used). We initially picked them because they were the best option for turning into safe Faraday Cages.
                                    </p>

                                    <p>
                                        We've stopped worrying so much about electrical noise, so we only ever bothered turning one booth into a Faraday Cage. But professional phone booths save a lot of hassle and set participants at ease, so you should use them if you can.
                                    </p>

                                    <p>
                                        If you don't have two weeks to wait for booths to arrive or if you want a cheaper option, we also used <a href="https://www.amazon.com/dp/B0DJ3CNQFP?ref=cm_sw_r_cso_cp_apin_dp_1KJDHPCHRDQ55WCFM532&amp;ref_=cm_sw_r_cso_cp_apin_dp_1KJDHPCHRDQ55WCFM532&amp;social_share=cm_sw_r_cso_cp_apin_dp_1KJDHPCHRDQ55WCFM532&amp;previewDoh=1&amp;th=1">these vocal recording booths</a>. The downside of using these is that they aren't remotely soundproof, so the participants could hear each other talking—which interfered with speaking and listening tasks.
                                    </p>

                                    <p>
                                        We added three layers of <a href="https://www.amazon.com/dp/B0DM5V3ZMJ?ref=ppx_pop_mob_ap_share">soundproof curtains</a>.<sup><a href="#fn15">[15]</a></sup><span id="fn15" data-footnote-number="[15]">This still wasn't enough, so we got dozens of <a href="https://www.amazon.com/dp/B0CR15SX4S?ref=ppx_pop_mob_ap_share&amp;th=1">sound panels</a> and used rope to hang them wall to wall in the booths.</span> Unfortunately, the weight of the curtains caused the booths to collapse. The solution to this is a lot of rope, which we used to tie the poles of the booth together and then nailed into a hook in the wall.
                                    </p>

                                    <p><img src="https://condu.it/thought/10k-hours/images/image8.png" alt="DIY booths soundproofed">
                                        <img src="https://condu.it/thought/10k-hours/images/image13.jpg" alt="Stock vocal booths">
                                        <img src="https://condu.it/thought/10k-hours/images/image9.png" alt="Zenbooth booth">
                                    </p>
                                    <p>(our first DIY booths, soundproofed)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(stock vocal booths)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(our Zenbooth booth)</p>

                                    <p>
                                        It costs ~$2,000 to set up these booths: $600 for the booth itself, $1,300 for soundproofing, and $100 for miscellaneous construction (rope, screws, etc). They look less professional, and you can't make them into a safe Faraday Cage, but otherwise this setup actually does work pretty well. We have a couple that we still use in our current data collection center, and they've been running flawlessly 20 hours/day for months.
                                    </p>

                                    
                                </div>
                        </details>
                    </div></div>]]></description>
        </item>
    </channel>
</rss>