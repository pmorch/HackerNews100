<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 25 Nov 2024 15:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Why I have resigned from the Royal Society (117 pts)]]></title>
            <link>http://deevybee.blogspot.com/2024/11/why-i-have-resigned-from-royal-society.html</link>
            <guid>42234497</guid>
            <pubDate>Mon, 25 Nov 2024 09:03:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://deevybee.blogspot.com/2024/11/why-i-have-resigned-from-royal-society.html">http://deevybee.blogspot.com/2024/11/why-i-have-resigned-from-royal-society.html</a>, See on <a href="https://news.ycombinator.com/item?id=42234497">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-1326298114103223151" itemprop="description articleBody">
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2Y-vThaI32BMolboVeqFScPWCpnjWhhmMRnj_tIxkrmET28q_V0IK0kYMFC_eH9ptMiYOyPhMK3Dexs3gLkB6ClY3Po2CaiyINEPcxv8G88dGEBA8J1xTS5753holfA3TOyt58ymqZ0XDC7TeplvQeAQNCePpiaTsmZA3IgbxwxLlLBJvLNdjch7DSBc/s800/TRS_location_image_1.jpg"><img data-original-height="300" data-original-width="800" height="120" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2Y-vThaI32BMolboVeqFScPWCpnjWhhmMRnj_tIxkrmET28q_V0IK0kYMFC_eH9ptMiYOyPhMK3Dexs3gLkB6ClY3Po2CaiyINEPcxv8G88dGEBA8J1xTS5753holfA3TOyt58ymqZ0XDC7TeplvQeAQNCePpiaTsmZA3IgbxwxLlLBJvLNdjch7DSBc/s320/TRS_location_image_1.jpg" width="320"></a></p><p>The Royal Society is a venerable institution founded in 1660, whose original members included such eminent men as Christopher Wren, Robert Hooke, Robert Boyle and Isaac Newton. It promotes science in many ways: administering grants, advising government, holding meetings and lectures, and publishing expert reports on scientific matters of public importance. &nbsp;</p><div><p>There are currently around 1,800 Fellows and Foreign Members of the Royal Society, all elected through a <a href="https://royalsociety.org/fellows-directory/election" target="_blank">stringent and highly competitive process</a> which includes nomination by two Fellows of the Royal Society (FRS), detailed scrutiny of the candidate's achievements and publications, reports by referees, and consideration by a committee of experts in their broad area of research. &nbsp;Although most Fellows are elected on the basis of their scientific contributions, others are nominated on the basis of "<i>wider contributions to science, engineering or medicine through leadership, organisation, scholarship or communication</i>".</p><div><p>For many scientists, election to the Royal Society is the pinnacle of their scientific career. It establishes that their achievements are recognised as exceptional, and the title FRS brings immediate respect from colleagues. Of course, things do not always work out as they should. Some Fellows may turn out to have published fraudulent work, or go insane and start promoting crackpot ideas. Although there are procedures that allow a fellow to be expelled from the Royal Society, I have been told this has not happened for over 150 years. It seems that election as a Fellow of the Royal Society, like loss of virginity, is something that can't readily be reversed.</p><div><p>This brings us, then, to the case of Elon Musk, who was <a href="https://royalsociety.org/people/elon-musk-13829/" target="_blank">elected as a Fellow of the Royal Society in 2018</a>&nbsp;on the basis of his technological achievements, notably in space travel and electrical vehicle development. Unfortunately,  since that time, his interests have extended to <a href="https://www.nbcnews.com/tech/social-media/elon-musk-turned-x-trump-echo-chamber-rcna174321" target="_blank">using social media for political propaganda</a>, while at the same time battling what he sees as "<a href="https://eu.usatoday.com/story/tech/2024/07/22/elon-musk-jordan-peterson-interview/74506785007/" target="_blank">woke mind virus</a>" and <a href="https://www.nbcnews.com/tech/tech-news/elon-musk-worries-free-speech-advocates-calls-prosecute-researchers-cr-rcna179194" target="_blank">attacks on free speech</a>.  Whereas previously he seemed to agree with mainstream scientific opinion on issues such as climate change and medicine, over the past year or two, he's started promoting alternative ideas. &nbsp;&nbsp;</p><div><p>In summer of 2024, a number of FRSs became concerned at how Musk was using his social media platform (previously Twitter, now termed X) to stir up racial unrest and anti-government sentiment in the UK.  Notable tweets by him from this period included incendiary comments and frank misinformation, as documented in this <a href="https://www.theguardian.com/commentisfree/article/2024/aug/09/uk-far-right-riots-elon-musk-x" target="_blank">Guardian article</a>.&nbsp;</p><div><p>This led to a number of Fellows expressing dismay that Musk had been elected. There was no formal consultation of the Fellowship but via informal email contacts, a group of 74 Fellows formulated a letter of concern that was sent in early August to the President of the Royal Society, raising doubts as to whether he was "<i>a fit and proper person to hold the considerable honour of being a Fellow of the Royal Society".</i>&nbsp;The letter specifically mentioned the way Musk had used his platform on X to make unjustified and divisive statements that served to inflame right-wing thuggery and racist violence in the UK.&nbsp;</p><div><p>I gather that at this point the Royal Society Council opted to consult a top lawyer to determine whether Musk's behaviour breached their Code of Conduct.  The problem with this course of action is that if you are uncertain about doing something that seems morally right but may have consequences, then it is easy to find a lawyer who will advise against doing it. That's just how lawyers work. They're paid to rescue people from ethical impulses that may get them into trouble.  And, sure enough, the lawyer determined that Musk hadn't breached the Code of Conduct.  If you want to see if you agree, you can find the Code of Conduct <a href="https://royalsociety.org/about-us/how-we-are-governed/governance" target="_blank">here</a>.</p><p>Many of the signatories of the letter, including me, were unhappy with this response.  We set about assembling further evidence of behaviours incompatible with the Code of Conduct.  There is a lot of material, which can be broadly divided into two categories, depending on whether it relates to "Scientific conduct" or "Principles". &nbsp;</p><div><p>
  

On <b>Scientific conduct</b>, the most relevant points from the Code of Conduct are:</p><blockquote><p><i>
  
2.6. Fellows and Foreign Members shall carry out their scientific research with regard to the Society's statement on research integrity and to the highest standards.&nbsp;</i></p><p><i>2.10. Fellows and Foreign Members shall treat all individuals in the scientific enterprise collegially and with courtesy, including supervisors, colleagues, other Society Fellows and Foreign Members, Society staff, students and other early‐career colleagues, technical and clerical staff, and interested members of the public.&nbsp;</i></p><p><i>2.11. Fellows and Foreign Members shall not engage in any form of discrimination, harassment, or bullying.</i></p></blockquote><div><p>Most of those I've spoken to agree that a serious breach of these principles was in 2022, when <a href="https://x.com/elonmusk/status/1601894132573605888?s=20&amp;t=aLV0JH6LGhKMYKYed2T9Fw" target="_blank">Musk tweeted</a>: "<i>My pronouns are Prosecute/Fauci</i>", thereby managing to simultaneously offend the LGBTQ community, express an antivaxx sentiment, and put Fauci, <a href="https://www.theguardian.com/us-news/article/2024/jun/03/anthony-fauci-covid-19-threats-harassment" target="_blank">already under attack from antivaxxers</a>, at further risk.  Fauci was not a Fellow at the time these comments were made, but that should not matter given the scope of the statement is "<i>individuals in the scientific community</i>".  This incident was <a href="https://www.cbsnews.com/news/elon-musk-anthony-fauci-viral-tweet-backlash-health-experts/" target="_blank">covered by CBS News.</a></p><p>Now that the US election is over, Musk seems emboldened to ramp up his attacks. On 19th November 2024, he&nbsp;<a href="https://x.com/elonmusk/status/1858873877184627039?s=58&amp;t=RqV9ku4ADjOWHIF2HcuosQ" target="_blank">retweeted this</a> to his millions of followers, followed by a <a href="https://x.com/elonmusk/status/1859596285549920658?s=58&amp;t=RqV9ku4ADjOWHIF2HcuosQ" target="_blank">compilation of attacks</a> on Fauci on 21st November,</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-zZemJ-PPwsC6435Xfdf_jg-kbsqzKuam70Da9TDWjBk03RwGvKQMyV2s0dbcok7QmTUPxqIZz5B6BPh6J56pnk0wap3G3Fg9VSKaKVPgKu6bwKmUFLaReQFMcc-wk7T1O_LQOlayiVleniufKTA38fFtDlLWYKqlmqvzWoTW6HdNCQGgTMFyVdemK8s/s548/Screenshot%202024-11-24%20at%2010.43.21.png"><img data-original-height="548" data-original-width="467" height="320" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-zZemJ-PPwsC6435Xfdf_jg-kbsqzKuam70Da9TDWjBk03RwGvKQMyV2s0dbcok7QmTUPxqIZz5B6BPh6J56pnk0wap3G3Fg9VSKaKVPgKu6bwKmUFLaReQFMcc-wk7T1O_LQOlayiVleniufKTA38fFtDlLWYKqlmqvzWoTW6HdNCQGgTMFyVdemK8s/s320/Screenshot%202024-11-24%20at%2010.43.21.png" width="273"></a></p><p><b><i>Neuralink</i></b></p><p>There are also questions about the management of Musk's research project, Neuralink, which involves developing a brain-computer interface to help people who are paralysed.  While this is clearly a worthy goal, his approach to conducting research is characterised by refusal to let anyone interfere with how he does things. This has led to <a href="https://pcrm.widen.net/s/llzr7cg57q/request-for-glp-investigation-re-neuralink---with-enclosures---12.13.22" target="_blank">accusations of failure to adhere to regulatory procedures for Good Laboratory Practic</a>e. For instance, consider these quotes from <a href="https://spectrum.ieee.org/neuralink-human-trials" target="_blank">this article</a>:&nbsp;</p><div><div><blockquote><i>'I think what concerns people is that Neuralink could be cutting corners, and so far nobody has stopped them,' says Nick Ramsey, a clinical neuroscientist at University Medical Center Utrecht, in the Netherlands. &nbsp;</i><i>There’s an incredible push by Neuralink to bypass the conventional research world, and there’s little interaction with academics, as if they think that we’re on the wrong track—that we’re inching forward while they want to leap years forward.</i></blockquote></div><div><blockquote><i>In response to Musk's claim that no monkey had died because of Neuralink, the Physicians Committee for Responsible Medicine wrote to the SEC, claiming Musk’s comments were false. The group said it had obtained veterinary records from Neuralink’s experiments showing that at least 12 young, healthy monkeys were euthanized as a result of problems with Neuralink’s implant. The group alleged that Musk’s comments are misleading investors, and urged SEC regulators to investigate Musk and Neuralink for securities fraud.</i></blockquote></div><div><p>The problems with Neuralink do not stop with the ethics of the animals and the secrecy surrounding them.  In a <a href="https://doi.org/10.1038/d41586-024-00304-4" target="_blank">piece in Nature</a>, various scientists were interviewed about the first human trial that was conducted earlier this year. The main concern was lack of transparency. Human trials are usually recorded in <i>clinical.trials.gov</i>, which was set up precisely to make it easier to track if studies had followed a protocol.  Musk did not do this. His approach to the human trials again reflects his distaste for any regulations.  But the regulations are there for a purpose, and one would expect a Fellow of the Royal Society to abide by them; otherwise we end up with scandals such as <a href="https://www.panmacmillan.com/blogs/literary/theranos-elizabeth-holmes-john-carreyrou" target="_blank">Theranos</a>&nbsp;or the <a href="https://www.science.org/content/article/two-controversial-stem-cell-trials-could-harm-patients-critics-say" target="_blank">stem cell experiments</a> by Macchiarini and Birchall. The <a href="https://www.statnews.com/2024/07/08/neuralink-elon-musk-scientific-ethics-brain-computer-interface/" target="_blank">ethics of this kind of trial</a> also needs careful handling, especially in terms of the patient's understanding of possible adverse effects, their expectations of benefits, and the undertaking of researchers to provide long-term support for the prosthesis.</p><div><p>
  

If we turn to the more general issues that come under <b>Principles</b>, then the Code of Conduct states:&nbsp;</p><div><blockquote><i>Fellows and Foreign Members shall not act or fail to act in any way which would undermine the Society's mission or bring the Society into disrepute</i>.</blockquote></div><p>&nbsp;Here are some examples that I would regard as contrary to the Society's mission.</p><div><p><b><i>
  

 
Promoting vaccine hesitation</i></b></p><p>The Royal Society has done good work promoting public understanding of vaccines, as with<a href="https://royalsociety.org/blog/2021/01/why-we-know-vaccines-work/" target="_blank"> this blogpost</a> by Charles Bangham FRS.  In contrast, <a href="https://www.livemint.com/news/world/elon-musk-factchecked-after-misleading-post-on-covid-19-vaccine-efficacy-know-more-11697730755260.html" target="_blank">as described here</a>, Musk has promoted <a href="https://www.bbc.com/news/health-66313916" target="_blank">vaccine conspiracy theories</a> and anti-vaccine views on his platform. 
<a href="https://x.com/elonmusk/status/1817357502725407024" target="_blank">This Tweet</a> had 85 million views:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgg3N6oEahfkSpd0OHN-VK6TtLpOZMbP9r5VGszAJn4Vnha17tlUxfuZlhJf6SsHND_1mIO5OR__l5JYFqOVvLUdLj3yR_hCHs_G_yUeOiRcK_rsj89RE81ubkWmRQp3Y3cSWQhne03NMyDRHPY5jWZVhTbj_Hqtl2mEsL7RMz7U9H7K2My6TmzTODgjQY/s338/dead%20unvacc.png"><img data-original-height="338" data-original-width="295" height="320" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgg3N6oEahfkSpd0OHN-VK6TtLpOZMbP9r5VGszAJn4Vnha17tlUxfuZlhJf6SsHND_1mIO5OR__l5JYFqOVvLUdLj3yR_hCHs_G_yUeOiRcK_rsj89RE81ubkWmRQp3Y3cSWQhne03NMyDRHPY5jWZVhTbj_Hqtl2mEsL7RMz7U9H7K2My6TmzTODgjQY/s320/dead%20unvacc.png" width="279"></a></p><br><div><p><b><i>
  
 
Downplaying the climate emergency </i></b></p><p>In 2023 Musk <a href="https://www.theguardian.com/environment/2023/nov/20/elon-musk-green-credentials-clean-energy-climate-deniers" target="_blank">played down the seriousness of climate change</a>, and 2024 participated in a bizarre <a href="https://www.theguardian.com/environment/article/2024/aug/13/trump-musk-x-climate" target="_blank">interview with Donald Trump</a>, which dismayed climate experts.  Among the commenters was Michael Mann, who said “<i>It is sad that Elon Musk has become a climate change denier, but that’s what he is. He’s literally denying what the science has to say here</i>.”  Mann was elected as a Foreign Member of the Royal Society in 2024.</p><div><p><b><i>
  
 
Spreading deep fakes and misinformation on X</i></b></p><p>As recently as 2022, the Royal Society published<a href="https://royalsociety.org/-/media/policy/projects/online-information-environment/the-online-information-environment.pdf" target="_blank"> a report</a>&nbsp;in which Frank Kelly (FRS) <span>noted&nbsp;<span>the high priority that the Royal Society gives to accurate scientific communication:</span></span></p><div><div><blockquote><i>The Royal Society’s mission since it was established in 1660 has been to promote science for the benefit of humanity, and a major strand of that is to communicate accurately. But false information is interfering with that goal. It is accused of fuelling mistrust in vaccines, confusing discussions about tackling the climate crisis and influencing the
debate about genetically modified crops.&nbsp;</i></blockquote></div><div><blockquote><i>&nbsp;Musk’s reason for buying Twitter was to influence the social discourse. And influence he did—by using his enormous platform (203 million followers) to endorse Trump, spread disinformation about voter fraud and deep fakes of Kamala Harris, and amplify conspiracy theories about everything from vaccines to race replacement theory to misogyny.</i></blockquote><p>The most recent development is the announcement that Musk is to be co-director of the new Department of Government Efficiency (DOGE, an allusion to the cryptocurrency Dogecoin) in the Trump Administration, with a brief to cut waste and bureaucracy. The future for US science is starting to look bleak, with Musk being given unfettered powers to cut budgets to NIH and NASA, among others. &nbsp;<a href="https://x.com/america/status/1857228761915412814" target="_blank">This tweet</a>, which he endorsed, indicates that rather than using objective evidence, the cuts will fall on those who have criticized Trump, who will find bowdlerized summaries of their work used to generate public outrage. The tweet reads: &nbsp;"<i>Here’s what the U.S. Government wasted $900 Billion of your tax dollars on in 2023.  The Department of Government Efficiency (@DOGE) will fix this. America deserves leaders that prioritize sensible spending</i>" before presenting a chart listing items for cuts, with unsourced descriptions of expenditure, including:</p><div><div><ul><li><i>Dr Fauci's monkey business on NIH's "monkey island": &nbsp; $33,200,000&nbsp;</i></li><li><i>NIH's meth-head monkeys: &nbsp;portion of $12,000,000&nbsp;</i></li><li><i>Dr Fauci's transgender monkey study: $477,121</i></li></ul><p>I'm sad to say I agree with Alex Wild, Curator of Entomology at University of Texas Austin, who&nbsp;<a href="http://deevybee.blogspot.com/2024/11/%22https://bsky.app/profile/alexwild.bsky.social/post/3lbmvdzmxos2e" target="_blank">wrote</a>&nbsp;a few days ago: <i>"I hope federally funded scientists are preparing for large scale, bad faith attacks by Musk and his troll army. &nbsp;It’s pretty clear the DOGE operation is going to take snippets of grant proposals and papers, present them out of context, and direct weaponized harassment of individual people."</i></p></div><div><p><b>What next? &nbsp;</b></p><p>I've been told that in the light of the evolving situation, the Royal Society Council will look again at the case of Elon Musk.  In conversations I have had with them, they emphasise that they must adhere to their own procedures, which are specified in the Statutes, and which involve a whole series of stages of legal scrutiny, committee evaluation, discussion with the Fellow in question,  and ultimately a vote from the Fellowship, before a Fellow or Foreign Member could be expelled. While I agree that if you have a set of rules you should stick to them, I find the fact that nobody has been expelled for over 150 years telling. It does suggest that the Statutes are worded so that it is virtually impossible to do anything about Fellows who breach the Code of Conduct. In effect the Statutes serve a purpose of protecting the Royal Society from ever having to take action against one of its Fellows.</p><div><p>In the course of investigating this blogpost, I've become intimately familiar with the Code of Conduct, which requires me to "<i>treat all individuals in the scientific enterprise collegially and with courtesy, including ... foreign Members</i>".   I'm not willing to treat Elon Musk "<i>collegially and with courtesy</i>".  Any pleasure I may take in the distinction of the honour of an FRS is diminished by the fact it is shared with someone who appears to be modeling himself on a Bond villain, a man who has immeasurable wealth and power which he will use to threaten scientists who disagree with him. Accordingly, last week I resigned my FRS. I don't do this in the expectation of having any impact: in the context of over 350 years of Royal Society history, this is just a blip. I just feel far more comfortable to be dissociated from an institution that continues to honour this disreputable man.</p></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><p><span>Note: Comments will be accepted if they are by a named individual, civil, and on topic. They are moderated and there may be a delay before they appear online.&nbsp;</span></p></div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Judge's Investigation into Patent Troll Results in Criminal Referrals (105 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2024/11/judges-investigation-patent-troll-ip-edge-results-criminal-referrals</link>
            <guid>42234147</guid>
            <pubDate>Mon, 25 Nov 2024 07:55:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2024/11/judges-investigation-patent-troll-ip-edge-results-criminal-referrals">https://www.eff.org/deeplinks/2024/11/judges-investigation-patent-troll-ip-edge-results-criminal-referrals</a>, See on <a href="https://news.ycombinator.com/item?id=42234147">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p><span></span><span>In 2022, three companies with strange names and no clear business purpose beyond&nbsp; patent litigation filed dozens of lawsuits in Delaware federal court, accusing businesses of all sizes of patent infringement. Some of these complaints claimed patent rights over basic aspects of modern life; one, for example, involved a&nbsp; </span><a href="https://www.eff.org/deeplinks/2023/02/stupid-patent-month-clocking-work-app"><span>patent that pertains to the process of clocking in to work through an app</span></a><span>.</span></p>
<p><span>These companies–named Mellaconic IP, Backertop Licensing, and Nimitz Technologies–seemed to be typical examples of “patent trolls,”</span> <span>companies whose primary business is suing others over patents or demanding licensing fees rather than providing actual products or services.&nbsp;</span></p>
<p><span>However, the cases soon took an unusual turn. The Delaware federal judge overseeing the cases, U.S. District Judge Colm Connolly, sought more information about the patents and their ownership. One of the alleged owners was a food-truck operator who had been promised “passive income,” but was entitled to only a small portion of any revenue generated from the lawsuits. Another owner was the spouse of an attorney at IP Edge, the patent-assertion company linked to all three LLCs.&nbsp;</span></p>
<p><span>Following an extensive investigation, the judge determined that attorneys associated with these shell companies had violated legal ethics rules. He pointed out that the attorneys may have misled Hau Bui, the food-truck owner, about his potential liability in the case. Judge Connolly wrote:&nbsp;</span></p>
<blockquote><p><span>[T]he disparity in legal sophistication between Mr. Bui and the IP Edge and Mavexar actors who dealt with him underscore that counsel's failures to comply with the Model Rules of Professional Conduct while representing Mr. Bui and his LLC in the Mellaconic cases are not merely technical or academic.</span></p>
</blockquote>
<p><span>Judge Connolly also concluded that IP Edge, the patent-assertion company behind hundreds of patent lawsuits and linked to the three LLCs, was the “de facto owner” of the patents asserted in his court, but that it attempted to hide its involvement. He wrote, “IP Edge, however, has gone to great lengths to hide the ‘we’ from the world,” with "we" referring to IP Edge. Connolly further noted, “IP Edge arranged for the patents to be assigned to LLCs it formed under the names of relatively unsophisticated individuals recruited by [IP Edge office manager] Linh Deitz.”&nbsp;</span></p>
<p><span>The judge </span><a href="https://news.bloomberglaw.com/ip-law/judges-litigation-funding-probe-reveals-ip-edges-human-toll"><span>referred</span></a><span> three IP Edge attorneys to the </span><a href="https://www.bloomberglaw.com/public/desktop/document/NimitzTechnologiesLLCvCNETMediaIncDocketNo121cv01247DDelAug302021/12?doc_id=X1MU2O97VP39OFRLOSOF11VA4JP"><span>Supreme Court of Texas’ Unauthorized Practice of Law Committee</span></a><span> for engaging in “unauthorized practices of law in Texas.” Judge Connolly also sent a </span><a href="https://www.bloomberglaw.com/public/desktop/document/NimitzTechnologiesLLCvCNETMediaIncDocketNo121cv01247DDelAug302021/11?doc_id=X2DE7OM6SOT9VIOU8VQAC5CBUSF"><span>letter to the Department of Justice</span></a><span>, suggesting an investigation into “individuals associated with IP Edge LLC and its affiliate Maxevar LLC.”&nbsp;</span></p>
<h3><span>Patent Trolls Tried To Shut Down This Investigation</span></h3>
<p><span>The attorneys involved in this wild patent trolling scheme challenged Judge Connolly’s authority to proceed with his investigation. However, because transparency in federal courts is essential and applicable to all parties, including patent assertion entities, </span><a href="https://www.eff.org/deeplinks/2022/12/judges-investigation-patent-trolls-must-be-allowed-move-forward"><span>EFF and two other patent reform groups filed a brief in support of the judge’s investigation</span></a><span>. The brief argued that “[t]he public has a right—and need—to know who is controlling and benefiting from litigation in publicly-funded courts.” Companies targeted by the patent trolls, as well as the Chamber of Commerce, filed their own briefs supporting the investigation.&nbsp;</span></p>
<p><span>The </span><a href="https://www.eff.org/files/2022/12/08/44_-_order_denying_nimitz_writ.pdf"><span>appeals court sided with us</span></a><span>, </span><a href="https://www.eff.org/deeplinks/2022/12/victory-judges-critical-investigation-patent-troll-companies-can-move-forward"><span>upholding</span></a><span> Judge Connolly’s authority to proceed, which led to the referral of the involved attorneys to the disciplinary counsel of their respective bar associations.&nbsp;</span></p>
<p><span>After this damning ruling, one of the patent troll companies and its alleged owner made a final effort at appealing this outcome. In July of this year, the U.S. Court of Appeals for the Federal Circuit </span><a href="https://cafc.uscourts.gov/opinions-orders/23-2367.OPINION.7-16-2024_2350699.pdf"><span>ruled </span></a><span>that investigating Backertop Licensing LLC and ordering its alleged owner to testify was “an appropriate means to investigate potential misconduct involving Backertop.”&nbsp;</span></p>
<p><span>In EFF’s view, these types of investigations into the murky world of patent trolling are not only appropriate but should happen more often. Now that the appeals court has ruled, let’s take a look at what we learned about the patent trolls in this case.&nbsp;</span></p>
<h3><span>Patent Troll Entities Linked To French Government</span></h3>
<p><span>One of the patent trolling entities, Nimitz Technologies LLC, asserted a single patent, U.S. Patent No. 7,848,328, against 11 companies. When the judge required Nimitz’s supposed owner, a man named Mark Hall, to testify in court, Hall could not describe anything about the patent or explain how Nimitz acquired it. He didn’t even know the name of the patent (“Broadcast Content Encapsulation”). When asked what technology was covered by the patent, he said, “I haven’t reviewed it enough to know,” and when asked how he paid for the patent, Hall replied, “no money exchanged hands.”&nbsp;</span></p>
<p><span>The exchange between Hall and Judge Connolly went as follows:&nbsp;</span></p>
<blockquote><p><span>Q. S</span>o how do you come to own something if you never paid for it with money?</p>
<p>A. I wouldn't be able to explain it very well. That would be a better question for Mavexar.</p>
<p>Q. Well, you're the owner?</p>
<p>A. Correct.</p>
<p>Q. How do you know you're the owner if you didn't pay anything for the patent?</p>
<p>A. Because I have the paperwork that says I'm the owner.</p>
</blockquote>
<p><span>(</span><a href="https://www.eff.org/document/nimitz-technologies-v-cnet-et-al-memorandum-order"><span>Nov. 27, 2023 Opinion</span></a><span>, pages 8-9.)&nbsp;</span></p>
<p><span>The Nimitz patent originated from the Finnish cell phone company Nokia, which later&nbsp;assigned it and several other patents to France Brevets, a French sovereign investment fund, in 2013. France Brevets, in turn, assigned&nbsp;the patent to a US company called Burley Licensing LLC, an entity linked to IP Edge, in 2021. Hau Bui (the food truck owner) signed on behalf of Burley, and Didier Patry, </span><a href="https://www.privateequitywire.co.uk/didier-patry-appointed-ceo-france-brevets-sovereign-investment-fund/"><span>then the CEO of France Brevets</span></a><span>, signed on behalf of the French fund.&nbsp;</span></p>
<p><span>France Brevets was </span><a href="https://www.iam-media.com/article/france-brevets-public-sector-links-hindered-its-monetisation-efforts-says-ceo"><span>an investment fund formed in 2009</span></a><span> with €100 million in seed money from the French government to manage intellectual property. France Brevets was set to receive 35% of any revenue related to “monetizing and enforcement” of the patent, with Burley agreeing to file at least one patent infringement lawsuit within a year, and collect a “total minimum Gross Revenue of US $100,000” within 24 months, or the patent rights would be given back to France Brevets.&nbsp;</span></p>
<p><span>Burley Licensing LLC, run by IP Edge personnel, then created Nimitz Technologies LLC— a company with no assets except for the single patent. They obtained a mailing address for it from a Staples in Frisco, Texas, and assigned the patent to the LLC in August 2021, while the obligations to France Brevets remained unchanged until the fund </span><a href="https://www.iam-media.com/article/france-brevets-public-sector-links-hindered-its-monetisation-efforts-says-ceo"><span>shut down in 2022</span></a><span>.</span></p>
<h3><span>The Bigger Picture</span></h3>
<p><span></span><span>It’s troubling that patent lawsuits are often funded by entities with no genuine interest in innovation, such as private equity firms. However, it’s even more concerning when foreign government-backed organizations like France Brevets manipulate the US patent system for profit. In this case, a Finnish company sold its patents to a French government fund, which used US-based IP lawyers to file baseless lawsuits against American companies, including well-known establishments like Reddit and Bloomberg, as well as smaller</span><a href="https://www.tastemade.com/about"><span> ones like Tastemade</span></a><span> and </span><a href="https://www.crunchbase.com/organization/skillshare"><span>Skillshare</span></a><span>.</span></p>
<p><span>Judges should enforce rules requiring transparency about third-party funding in patent lawsuits. When ownership is unclear, it’s appropriate to insist that the real owners show up and testify—before dragging dozens of companies into court over dubious software patents.&nbsp;</span></p>
<p><span>Related documents:&nbsp;</span></p>
<ul>
<li><a href="https://www.eff.org/files/2024/09/09/1_22-cv-00413-cfc_34_primary_document.pdf"><span>Memorandum and Order</span></a><span> referring counsel to disciplinary bodies (Nov. 23, 2023)&nbsp;</span></li>
<li><a href="https://cafc.uscourts.gov/opinions-orders/23-2367.OPINION.7-16-2024_2350699.pdf"><span>Federal Circuit Opinion</span></a><span> affirming the order requiring Lori LaPray to appear “for testimony regarding potential fraud on the court,” as well as the District Court’s order of monetary sanction against Ms. LaPray for subsequently failing to appear</span></li>
</ul>


</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Day in the Life: The Global BGP Table (127 pts)]]></title>
            <link>https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/</link>
            <guid>42233565</guid>
            <pubDate>Mon, 25 Nov 2024 05:41:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/">https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/</a>, See on <a href="https://news.ycombinator.com/item?id=42233565">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>Much has been written and a lot of analysis performed on the global BGP table over the years, a significant portion by the inimitable <a href="https://bgp.potaroo.net/">Geoff Huston</a>. However this often focuses on is long term trends, like the growth of the routing table or the adoption of IPv6 , dealing with time frames of of months or years.</p><p>I was more interested in what was happening in the short term: what does it look like on the front line for those poor routers connected to the churning, foamy chaos of the interenet, trying their best to adhere to <a href="https://en.wikipedia.org/wiki/Robustness_principle">Postel’s Law</a>? What we’ll look at in this article is “a day in the life of the global BGP table”, exploring the intra-day shenanigans with an eye to finding some of the ridiculous things that go on out.</p><p>We’ll focus in on three key areas:</p><ul><li>General behaviour over the course of the day</li><li>Outlier path attributes</li><li>Flappy paths</li></ul><p>As you’ll see, we end up with more questions than answers, but I think that’s the hallmark of good exploratory work. Let’s dive in.</p><h2 id="let-the-yak-shaving-begin">Let the Yak Shaving Begin</h2><p>The first step, as always, is to get some data to work with. Parsing the debug outputs from various routers seemed like a recipe for disaster, so instead I did a little yak-shaving. I went back to a half-finished project BGP daemon I’d started writing years ago and got it into a working state. The result is <strong><a href="https://github.com/gregfoletta/bgpsee">bgpsee</a></strong>, a multi-threaded BGP peering tool for the CLI. Once peered with another router, all the BGP messages - OPENs, KEEPALIVES, and most importantly UPDATEs - are parsed and output as JSON.</p><p>For example, heres one of the BGP updates from the dataset we’re working with in this article:</p><div><pre><code data-lang="json">{
  <span>"recv_time"</span>: <span>1704483075</span>,
  <span>"id"</span>: <span>12349</span>,
  <span>"type"</span>: <span>"UPDATE"</span>,
  <span>"nlri"</span>: [ <span>"38.43.124.0/23"</span> ],
  <span>"withdrawn_routes"</span>: [],
  <span>"path_attributes"</span>: [
    {
      <span>"type"</span>: <span>"ORIGIN"</span>, <span>"type_code"</span>: <span>1</span>,
      <span>"origin"</span>: <span>"IGP"</span>
    },
    {
      <span>"type"</span>: <span>"AS_PATH"</span>, <span>"type_code"</span>: <span>2</span>,
      <span>"n_as_segments"</span>: <span>1</span>,
      <span>"path_segments"</span>: [
        {
          <span>"type"</span>: <span>"AS_SEQUENCE"</span>,
          <span>"n_as"</span>: <span>6</span>,
          <span>"asns"</span>: [ <span>45270</span>, <span>4764</span>, <span>2914</span>, <span>12956</span>, <span>27951</span>, <span>23456</span> ]
        }
      ]
    },
    {
      <span>"type"</span>: <span>"NEXT_HOP"</span>, <span>"type_code"</span>: <span>3</span>,
      <span>"next_hop"</span>: <span>"61.245.147.114"</span>
    },
    {
      <span>"type"</span>: <span>"AS4_PATH"</span>, <span>"type_code"</span>: <span>17</span>,
      <span>"n_as_segments"</span>: <span>1</span>,
      <span>"path_segments"</span>: [
        {
          <span>"type"</span>: <span>"AS_SEQUENCE"</span>,
          <span>"n_as"</span>: <span>6</span>,
          <span>"asns"</span>: [ <span>45270</span>,<span>4764</span>, <span>2914</span>, <span>12956</span>, <span>27951</span>, <span>273013</span> ]
        }
      ]
    }
  ]
}
</code></pre></div><p>Collected between 6/1/2024 and 7/1/2024, the full dataset consists of 464,673 BGP UPDATE messages received from a peer (many thanks to <a href="https://www.linkedin.com/in/andrew-vinton/">Andrew Vinton</a>) with a full BGP table. Let’s take a look at how this full table behaves over the course of the day.</p><h2 id="initial-send-number-of-v4-and-v6-paths">Initial Send, Number of v4 and v6 Paths</h2><p>When you first bring up a BGP peering with a router you get a big dump of of UPDATEs, what I’ll call the ‘first tranche’. It consists of all paths and associated network layer reachability information (NLRI, or more simply ‘routes’) in the router’s BGP table. After this first tranche the peering only receives UPDATEs for paths that have changed, or withdrawn routes which no longer have any paths. There’s no structural difference between the first tranche and the subsequent UPDATEs, except for the fact you received the first batch in the first 5 or so seconds of the peering coming up.</p><p>Here’s a breakdown of the number of distinct paths received in that first tranche, separated by IP version:</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-7-1.png" width="672">
It’s important to highlight that this is a count of BGP paths, <strong>not</strong> routes. Each path is a unique combination of path attributes with associated NLRI information attached, sent in a distinct BGP UPDATE message. There could be one, or one-thousand routes associated with each path. In this first tranche the total number of routes across all of these paths is 949483.</p><h2 id="a-garden-hose-or-a-fire-hose">A Garden Hose or a Fire Hose?</h2><p>That’s all we’ll look at in the first tranche, we’ll focus our attention from this point on to the rest of the updates received across the day. The updates aren’t sent as a real-time stream, but in bunches based on the <a href="https://datatracker.ietf.org/doc/html/rfc4271#section-10">Route Advertisement Interval</a> timer, which for this peering was 30 seconds. Here’s a time-series view of the number of updates received during the course of the day:</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-8-1.gif" alt="">
For IPv4 paths you’re looking on average at around 50 path updates every 30 seconds. For IPv6 it’s slightly lower, at around 47 path updates. While the averages are close, the variance is quite different, a standard deviation of 64.3 and 43 for v4 and v6 respectively.</p><p>Instead of looking at the total count of udpates, we can instead look at the total aggregate IP address change. We do this by adding up the total amount of IP addresses across all updates for every 30 second interval, then take the log2() of the sum. So for example: a /22, a /23 and a /24 would be \(log_2(2^{32-22} + 2^{32-23} + 2^{32-24})\)</p><p>Below is the log2() IPv4 address space, viewed as a time series and as a density plot. It shows that on average, every 30 seconds, around 2^16 IP addresses (i.e a /16) change paths in the global routing table, with 95% of time time the change in IP address space is between \(2^{20.75}\) (approx. a /11) and \(2^{13.85}\) (approx. a /18).</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-9-1.png" width="672"></p><p>What is apparent in both the path and IP space changes over time is that there is some sort of cyclic behaviour in the IPv4 updates. To determine the period of this cycle we can use an <a href="https://otexts.com/fpp3/acf.html">ACF</a> or autocorrelation plot. We calculate the correlation between the number of paths received at time \(y_t\) versus the number received at \(y_{t-{1,t-2,…,t-n}}\) lags. I’ve grouped the updates together into 1 minute intervals, so 1 lag = 1 minute.</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-10-1.png" width="672">
There is a strong correlation in the first 7 or so lags, which intuitively makes sense to me as path changes can create other path changes as they propagate around the world. But there also appears to be strong correlation at lags 40 and 41, indicating some cyclic behaviour every forty minutes. This gives us the first question which I’ll leave unanswered:</p><ul><li><em>What is causing the global IPv4 BGP table have a 40 minute cycle?</em>.</li></ul><h2 id="prepending-madness">Prepending Madness</h2><p>If you’re a network admin, there’s a couple of different ways you can influence how traffic enters your ASN. You can use longer network prefixes, but this doesn’t scale well and you’re not being a polite BGP citizen. You can use the MED attribute, but it’s non-transitive so it doesn’t work if you’re peered to multiple AS. The usual go-to is to modify the AS path length by prepending your own AS one or more times to certain peers, making that path less preferable.</p><p>In chaos of the global routing table, some people take this prepending too far. This has in the past caused <a href="https://blog.ipspace.net/2009/02/root-cause-analysis-oversized-as-paths/">large, global problems</a>. Let’s take a look at the top 50 AS path lengths for IPv4 and IPv6 updates respectively:</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-11-1.png" width="672">
What stands out is the difference between IPv4 and IPv6. The largest IPv4 path length is 105, which is still pretty ridiculous given the fact that the largest non-prepended path in this dataset has a length of 14. But compared to the IPv6 paths it’s outright sensible: top of the table for IPv6 comes in at a whopping 599 ASes! An AS path is actually made up of one or more <a href="https://datatracker.ietf.org/doc/html/rfc4271#section-5.1.2">AS sets or AS sequences</a>, each of which have a maximum length of 255. So it’s taken three AS sequences to announce those routes.</p><p>Here’s the longest IPv4 path in all it’s glory with its 105 ASNs. It originated from AS149381 “Dinas Komunikasi dan Informatika Kabupaten Tulungagung” in Indonesia.</p><pre><code>[1] "45270 4764 9002 136106 45305 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381 149381"
</code></pre><p>We see that around 6 hours and 50 minutes later they realise the error in their ways and announce a path with only four ASes, rather than 105:</p><div id="xqhmswhyyp"><table data-quarto-disable-processing="false" data-quarto-bootstrap="false"><thead><tr><th rowspan="1" colspan="1" scope="col" id="recv_time">recv_time</th><th rowspan="1" colspan="1" scope="col" id="time_difference">time_difference</th><th rowspan="1" colspan="1" scope="col" id="id">id</th><th rowspan="1" colspan="1" scope="col" id="as_path_length">as_path_length</th><th rowspan="1" colspan="1" scope="col" id="type">type</th><th rowspan="1" colspan="1" scope="col" id="nlri">nlri</th></tr></thead><tbody><tr><td headers="recv_time">2024-01-06 06:31:18</td><td headers="time_difference">NA</td><td headers="id">66121</td><td headers="as_path_length">105</td><td headers="type">UPDATE</td><td headers="nlri">103.179.250.0/24</td></tr><tr><td headers="recv_time">2024-01-06 13:21:35</td><td headers="time_difference">6.84</td><td headers="id">280028</td><td headers="as_path_length">4</td><td headers="type">UPDATE</td><td headers="nlri">103.179.250.0/24</td></tr></tbody>
</table></div><p>Here’s the largest IPv6 path, with its mammoth 599 prefixes; I’ll let you enjoy scrolling to the right on this one:</p><pre><code>[1] "45270 4764 2914 29632 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 8772 200579 200579 203868"
</code></pre><p>Interestingly it’s not the originator that’s prepending, but as8772 ‘NetAssist LLC’, an ISP out of Ukraine prepending to make paths to asn203868 (Rifqi Arief Pamungkas, again out of Indonesia) less preferable.</p><p>Why is there such a difference between the largest IPv4 and IPv6 path lengths? I had a couple of different theories, but then looked at the total number of ASNs in <em>all</em> positions for those top 50 longest paths, and it became apparent what was happening:</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-15-1.png" width="672">
Looks like they let the junior network admin at NetAssist on to the tools too early!</p><h2 id="path-attributes">Path Attributes</h2><p>Each BGP update consist of network layer reachability information (routes) and path attributes. For example AS_PATH, NEXT_HOP, etc. There are four kinds of attributes:</p><ol><li>Well-known mandatory</li><li>Well-known discretionary</li><li>Optional transitive</li><li>Optional non-transitive</li></ol><p><a href="https://datatracker.ietf.org/doc/html/rfc4271#section-5">Section 5</a> of RFC4271 has a good description of all of these.</p><p>What we can do is take a look at the number of attributes we’ve seen across all of our IPv4 paths, placing this on on a log scale to make it easier to view:</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-17-1.png" width="672"></p><p>The well-known mandatory attributes, ORIGIN, NEXT_HOP, and AS_PATH, are present in all updates, and have the same counts. There’s a few other common attributes (e.g.&nbsp;AGGREGATOR), and some less common ones (AS_PATHLIMIT and ATTR_SET). However some ASes have attached attribute 255 - the <a href="https://www.rfc-editor.org/rfc/rfc2042.html">reserved for development</a> attribute - to their updates.</p><p>At the time of receiving the updates my bgpsee daemon didn’t save value of these esoteric path attributes. But using <a href="https://routeviews.org/">routeviews.org</a> we can see that some ASes are still announcing paths with this attribute, and we can observe the raw bytes of its value:</p><pre><code>- AS265999 attrib. 255 value:       0000 07DB 0000 0001 0001 000A FF08 0000 0000 0C49 75B3
- AS10429 attrib. 255 value:        0000 07DB 0000 0001 0001 000A FF08 0000 0003 43DC 75C3
- AS52564 attrib. 255 valuue:       0000 07DB 0000 0001 0001 0012 FF10 0000 0000 0C49 75B3 0000 0000 4003 F1C9
</code></pre><p>Three different ISPs, all announcing paths with this strange path attribute, and raw bytes of the attribute having a similar structure.</p><p>This leads us to the second question which I’ll leave here unanswered:</p><ul><li><em>what vendor is deciding it’s a good idea to use this reserved for development attribute, and what are they using it for?</em>.</li></ul><h2 id="flippy-flappy-whos-having-a-bad-time">Flippy-Flappy: Who’s Having a Bad Time?</h2><p>Finally, let’s see who’s having a bad time: what are the top routes that are shifting paths or being withdrawn completely during the day. Here’s the top 10 active NLRIs with the number of times the route was included in an UPDATE:</p><div id="alweeynynt"><table data-quarto-disable-processing="false" data-quarto-bootstrap="false"><thead><tr><th rowspan="1" colspan="1" scope="col" id="nlri">nlri</th><th rowspan="1" colspan="1" scope="col" id="update_count">update_count</th></tr></thead><tbody><tr><td headers="nlri">140.99.244.0/23</td><td headers="update_count">2596</td></tr><tr><td headers="nlri">107.154.97.0/24</td><td headers="update_count">2583</td></tr><tr><td headers="nlri">45.172.92.0/22</td><td headers="update_count">2494</td></tr><tr><td headers="nlri">151.236.111.0/24</td><td headers="update_count">2312</td></tr><tr><td headers="nlri">205.164.85.0/24</td><td headers="update_count">2189</td></tr><tr><td headers="nlri">41.209.0.0/18</td><td headers="update_count">2069</td></tr><tr><td headers="nlri">143.255.204.0/22</td><td headers="update_count">2048</td></tr><tr><td headers="nlri">176.124.58.0/24</td><td headers="update_count">1584</td></tr><tr><td headers="nlri">187.1.11.0/24</td><td headers="update_count">1582</td></tr><tr><td headers="nlri">187.1.13.0/24</td><td headers="update_count">1580</td></tr></tbody>
</table></div><p>Looks like anyone on <strong>140.99.244.0/23</strong> was having a bad time during this day. This space is owned by a company called <a href="https://www.epicup.com/">EpicUp</a>… more like EpicDown! *groan*.</p><p>Graphing the updates and complete withdraws over the course of the day paints a bad picture</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-19-1.png" width="672">
The top graph looks like a straight line, but that’s because this route is present in almost every single 30 second block of updates. There are 2,879 30-second blocks and it’s present as either a different path or a withdrawn route in 2,637 of them, or 92.8%!</p><p>We know the routes is flapping, but <em>how</em> is it flapping, and who is to blame? The best way to visualise this is a graph, with the ASNs in all paths to that network as nodes and edges showing the pairs of ASNs in the paths. I’ve colourised the edges by how many updates were seen with each pair of ASes, binned into groups of 300:</p><p><img src="https://articles.foletta.org/post/2024-01-08-a-day-in-the-life-the-bgp-table/index_files/figure-html/unnamed-chunk-21-1.png" width="672">
What a mess! You can make out the primary path down the centre through NTT (2914) and Lumen/Level3 (3356), but for whatever reason (bad link? power outages? router crashing?) the path is moving between these tier 1 ISPS and others, including Arelion (1299) and PCCW (3419). While it’s almost impossible to identify the exact reason for the route flapping using this data only, what it does show is the amazing peering diversity of modern global networks, and the the resiliency of a 33 year old routing protocol.</p><h2 id="just-the-beginning">Just The Beginning</h2><p>There’s a big problem with a data set like this: there’s just too much to look at. I needed to keep a lid on it so this article didn’t balloon out to 30,000 words, but there’s another five rabbit holes I could have gone down. That’s not including the the questions I’ve left unanswered.</p><p>With the global BGP table, you’ve got a summary of an entire world encapsulated in a few packets. Your BGP updates could could be political unrest, natural phenomena like earthquakes or fires, or simply a network admin’s fat finger. You’ve got the economics of internet peering, and you’ve got the human element of different administrators with different capabilities coming together to bring up connectivity. And somehow it manages to work, well, most of the time. There’s something both bizarre and beautiful about seeing all of that humanity encapsulated and streamed as small little updates into your laptop.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How I configure my Git identities (379 pts)]]></title>
            <link>https://www.benji.dog/articles/git-config/</link>
            <guid>42233524</guid>
            <pubDate>Mon, 25 Nov 2024 05:32:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.benji.dog/articles/git-config/">https://www.benji.dog/articles/git-config/</a>, See on <a href="https://news.ycombinator.com/item?id=42233524">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><blockquote>
<p><strong>Note</strong>: I've had this post drafted for 3 YEARS!!! It's finally time to publish it.</p>
</blockquote>
<p>I like to mess with my <a href="https://github.com/benjifs/dotfiles">dotfiles</a> and every so often, I find out about a new way to do things and I spend more time than I should learning how to use it.</p>
<p>A few years ago I learned about <a href="https://git-scm.com/docs/git-config#_includes">includeIf</a> for including specific files if some condition was met for <code>git</code>. The example that I first saw was doing:</p>
<pre><code>[includeIf "gitdir:~/code/**"]
  path = ~/.config/git/personal
[includeIf "gitdir:~/work/**"]
  path = ~/.config/git/work
</code></pre>
<p>So that <code>~/.config/git/personal</code> is only included for <code>git</code> directories under <code>~/code</code> and <code>~/.config/git/work</code> is only included for directories under <code>~/work</code>. The contents of those included files varies but usually it contains your git identity, signing keys, etc. Here's an example of what that could look like:</p>
<pre><code>[user]
  name = benji
  email = benji@work.com
  signingkey = ~/.ssh/work.id_ed25519.pub
</code></pre>
<p>That works pretty well but I usually organize all my code in <code>~/workspace</code> regardless of whether its personal, <strong>work-1</strong>, <strong>work-2</strong>, etc. I wanted to be able to configure git depending on where that repo actually lives instead of where the directory is in my machine. Then I found out about <a href="https://git-scm.com/docs/git-config#Documentation/git-config.txt-codehasconfigremoteurlcode">hasconfig:remote.*.url:</a>!</p>
<p>This makes it so that I can configure git conditionally if the given remote URL exists for that directory I'm currently working in.</p>
<p>A few examples of what I do is:</p>
<pre><code>[includeIf "hasconfig:remote.*.url:git@github.com:orgname/**"]
  path = ~/.config/git/config-gh-org

[includeIf "hasconfig:remote.*.url:git@github.com:*/**"]
  path = ~/.config/git/config-gh

[includeIf "hasconfig:remote.*.url:git@gitlab.com:*/**"]
  path = ~/.config/git/config-gl

[includeIf "hasconfig:remote.*.url:git@git.sr.ht:*/**"]
  path = ~/.config/git/config-srht
</code></pre>
<p>Now if I'm in a directory where the remote matches <code>github.com:orgname/**</code> it would use <code>~/.config/git/config-gh-org</code>, otherwise it uses the general config file for any other GitHub repo.</p>
<hr>
<p>While that handles git identities, I still need to configure SSH keys separately to be able to <code>pull</code> and <code>push</code> to remotes. The simple version of my <code>~/.ssh/config</code> looks like this:</p>
<pre><code>Host gitlab.com
Hostname gitlab.com
User git
IdentityFile ~/.ssh/gitlab.id_ed25519

Host github.com
Hostname github.com
User git
IdentityFile ~/.ssh/github.id_ed25519
</code></pre>
<p>The only problem with this is that in order to use a different <code>IdentityFile</code> for the same <code>Hostname</code> so that I could use a different key for repos under <code>github.com/orgname</code>, I'd have to use a different value for <code>Host</code>. So in my case I would add the following to my <code>~/.ssh/config</code>:</p>
<pre><code>Host gh-work
Hostname github.com
User git
IdentityFile ~/.ssh/work.id_ed25519
</code></pre>
<p>Finally, to use that <code>Host</code> when I'm looking for a repo in <code>github.com/orgname</code>, I would add the following to my git config:</p>
<pre><code>[url "gh-work:orgname"]
  insteadOf = git@github.com:orgname
</code></pre>
<p>So when I <code>clone</code>, <code>pull</code>, or <code>push</code> a repo that's under my work's org account I can do:</p>
<pre><code>git clone git@github.com:orgname/project
</code></pre>
<p>and <code>insteadOf</code> would replace <code>github.com:orgname</code> with <code>gh-work:orgname</code> so that it uses the right info from my SSH config. It's a neat trick which I saw referenced in this <a href="https://www.kenmuse.com/blog/ssh-and-multiple-git-credentials/#git">article</a>.</p>
<hr>
<p>Are there any issues with this approach? Is there a better way to do this? I'm not sure so please let me know as I'd love to learn and I'll update this post accordingly.</p>
<h2>References</h2>
<ul>
<li><a href="https://fundor333.com/post/2021/advance-git-config-and-ssh-config/">https://fundor333.com/post/2021/advance-git-config-and-ssh-config/</a></li>
<li><a href="https://www.kenmuse.com/blog/ssh-and-multiple-git-credentials/#git">https://www.kenmuse.com/blog/ssh-and-multiple-git-credentials/#git</a></li>
<li><a href="https://garrit.xyz/posts/2023-10-13-organizing-multiple-git-identities">https://garrit.xyz/posts/2023-10-13-organizing-multiple-git-identities</a></li>
<li><a href="https://stevenharman.net/configure-ssh-keys-for-multiple-github-accounts">https://stevenharman.net/configure-ssh-keys-for-multiple-github-accounts</a></li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wildlife monitoring technologies used to intimidate and spy on women (140 pts)]]></title>
            <link>https://www.cam.ac.uk/research/news/wildlife-monitoring-technologies-used-to-intimidate-and-spy-on-women-study-finds</link>
            <guid>42232289</guid>
            <pubDate>Mon, 25 Nov 2024 01:24:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cam.ac.uk/research/news/wildlife-monitoring-technologies-used-to-intimidate-and-spy-on-women-study-finds">https://www.cam.ac.uk/research/news/wildlife-monitoring-technologies-used-to-intimidate-and-spy-on-women-study-finds</a>, See on <a href="https://news.ycombinator.com/item?id=42232289">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Remotely operated camera traps, sound recorders and drones are increasingly being used in conservation science to monitor wildlife and natural habitats, and to keep watch on protected natural areas.</p>

<p>But Cambridge researchers studying a forest in northern India have found that the technologies are being deliberately misused by local government and male villagers to keep watch on women without their consent.</p>

<p>Cambridge researcher Dr Trishant Simlai spent 14 months interviewing 270 locals living around the Corbett Tiger Reserve, a national park in northern India, including many women from nearby villages.</p>

<p>His report, published today in the journal <a href="https://doi.org/10.17863/CAM.111664"><em>Environment and Planning F</em></a>, reveals how forest rangers in the national park deliberately fly drones over local women to frighten them out of the forest, and stop them collecting natural resources despite it being their legal right to do so.</p>

<p>The women, who previously found sanctuary in the forest away from their male-dominated villages, told Simlai they feel watched and inhibited by camera traps, so talk and sing much more quietly. This increases the chance of surprise encounters with potentially dangerous wildlife like elephants and tigers. One woman he interviewed has since been killed in a tiger attack.</p>

<p>The study reveals a worst-case scenario of deliberate human monitoring and intimidation. But the researchers say people are being unintentionally recorded by wildlife monitoring devices without their knowledge in many other places - even national parks in the UK.&nbsp;</p>

<p>“Nobody could have realised that camera traps put in the Indian forest to monitor mammals actually have a profoundly negative impact on the mental health of local women who use these spaces,” said Dr Trishant Simlai, a researcher in the University of Cambridge’s Department of Sociology and lead author of the report.</p>

<p>“These findings have caused quite a stir amongst the conservation community. It’s very common for projects to use these technologies to monitor wildlife, but this highlights that we really need to be sure they’re not causing unintended harm,” said Professor Chris Sandbrook, Director of the University of Cambridge’s Masters in Conservation Leadership programme, who was also involved in the report.</p>

<p>He added: “Surveillance technologies that are supposed to be tracking animals can easily be used to watch people instead – invading their privacy and altering the way they behave.”</p>

<p>Many areas of conservation importance overlap with areas of human use. The researchers call for conservationists to think carefully about the social implications of using remote monitoring technologies – and whether less invasive methods like surveys could provide the information they need instead.</p>

<p><strong><em>Intimidation and deliberate humiliation</em></strong></p>

<p>The women living near India’s Corbett Tiger Reserve use the forest daily in ways that are central to their lives: from gathering firewood and herbs to sharing life’s difficulties through traditional songs.</p>

<p>Domestic violence and alcoholism are widespread problems in this rural region and many women spend long hours in forest spaces to escape difficult home situations.</p>

<p>The women told Simlai that new technologies, deployed under the guise of wildlife monitoring projects, are being used to intimidate and exert power over them - by monitoring them too.&nbsp;</p>

<p>“A photograph of a woman going to the toilet in the forest – captured on a camera trap supposedly for wildlife monitoring - was circulated on local Facebook and WhatsApp groups as a means of deliberate harassment,” said Simlai.&nbsp;</p>

<p>He added: “I discovered that local women form strong bonds while working together in the forest, and they sing while collecting firewood to deter attacks by elephants and tigers. When they see camera traps they feel inhibited because they don’t know who’s watching or listening to them – and as a result they behave differently - often being much quieter, which puts them in danger.”</p>

<p>In places like northern India, the identity of local women is closely linked to their daily activities and social roles within the forest. The researchers say that understanding the various ways local women use forests is vital for effective forest management strategies.</p>

<p><em><strong>Reference: </strong>Simlai, T. et al: ‘<a href="https://doi.org/10.17863/CAM.111664">The Gendered Forest: Digital Surveillance Technologies for Conservation and Gender-Environment relationships</a>.’ November 2024. DOI:10.17863/CAM.111664</em><br>
&nbsp;</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RFC 35140: HTTP Do-Not-Stab (2023) (708 pts)]]></title>
            <link>https://www.5snb.club/posts/2023/do-not-stab/</link>
            <guid>42232040</guid>
            <pubDate>Mon, 25 Nov 2024 00:43:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.5snb.club/posts/2023/do-not-stab/">https://www.5snb.club/posts/2023/do-not-stab/</a>, See on <a href="https://news.ycombinator.com/item?id=42232040">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      






<p>Date: March 7, 2111</p>
<h2 id="abstract">Abstract</h2>
<p>This document defines the syntax and semantics of the <code>Do-Not-Stab</code> header, a proposed HTTP header
that allows users to indicate to a website their preferences about being stabbed. It also provides
a standard for how services should comply with such user preferences, if they wish to.</p>

<ul>
<li><code>[REDACTED]</code> (Google)</li>
<li><code>[REDACTED]</code> (Google)</li>
<li><code>[REDACTED]</code> (Google)</li>
<li><code>[REDACTED]</code> (Google)</li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>Over the last 50 years, advancements in peripherals have allowed websites to stab users. A number
of industries have popped up to provide SaaS (Stabbings as a Service). Some users have expressed
discomfort when a knife is plunged into their chest, and this header allows those users to express
their personal preferences.</p>
<p>A user preference can, of course, be ignored by bad actors. However, most stabbings are not done by
malicious actors, they are simply law-abiding companies which will gladly stop stabbing you if you
ask. This standard provides a method for a user to easily opt-out of all stabbings, except those
mandated by law, and ones that the company wants to do anyways.</p>
<h2 id="syntax">Syntax</h2>
<p>The header has only one form, <code>Do-Not-Stab: 1</code>. This is because the lack of a header indicates a
clear preference that the user wants to be stabbed.</p>
<h2 id="defaults">Defaults</h2>
<p>A user-agent MUST NOT adopt <code>Do-Not-Stab: 1</code> as the default preference. If a user-agent were to do
this, web services SHOULD ignore the preference and stab the user anyways.</p>
<p>This is because user-agents are in no position to determine if a user wants to be stabbed or not,
this must be an explicit choice that the user makes.</p>
<h2 id="enforcement">Enforcement</h2>
<p>Microsoft has committed to supporting the <code>Do-Not-Stab</code> header inside the EEA (European Economic
Area). Outside of the EEA, support for the header is still in-progress, and you may get stabbed,
even with the header set. If you are in a country that leaves the EEA, you may get stabbed.</p>
<h2 id="exceptions">Exceptions</h2>
<p>Exceptions to the <code>Do-Not-Stab</code> header are accepted when commercial interests outweigh safety
concerns. These include, but are not limited to</p>
<ul>
<li>Stabbing users who have consented to being stabbed (even if they don’t know they consented)</li>
<li>Stabbings requested by a government. Websites SHOULD NOT try to challenge the legality of any
stabbings requested, the user probably deserved it.</li>
<li>Stabbings that are probably not going to kill the user.</li>
<li>Shareholders wanted it</li>
</ul>
<hr>

<p>seriously, what the fuck is with companies nowadays demanding that they be told to not do the
things they know they shouldn’t be doing anyways? why is microsoft respecting the user’s choice
only in the EEA? because they only <em>have</em> to there. extremely funny how they were also the ones to
set Do-Not-Track by default in IE, thereby getting everyone to ignore it for IE. because companies
are god damn children and must be told no explicitly by every person individually. it’s a fucking
wonder that DNT even got in as a general option and wasn’t mandated to be set per-origin, making it
even more fucking useless than it is.</p>
<p><a href="https://blogs.windows.com/windows-insider/2023/11/16/previewing-changes-in-windows-to-comply-with-the-digital-markets-act-in-the-european-economic-area/">https://blogs.windows.com/windows-insider/2023/11/16/previewing-changes-in-windows-to-comply-with-the-digital-markets-act-in-the-european-economic-area/</a></p>
<p>it’s fucking depressing when even the fucking bare minimum form of regulation is followed to the
letter and no more, because every company out there fucking hates you and would sell you out to
make a bit more money if they legally could. and even if they couldn’t, who’s going to stop them?</p>
<p>“We and our 756 partners process personal data[…]” wow big polycule this website is in, there’s no
fucking way they actually need to work with that many fucking companies, what the shit? adtech is a
scourge on humanity and serves zero fucking purpose.</p>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SQLiteStudio: Create, edit, browse SQLite databases (255 pts)]]></title>
            <link>https://sqlitestudio.pl/</link>
            <guid>42232000</guid>
            <pubDate>Mon, 25 Nov 2024 00:36:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sqlitestudio.pl/">https://sqlitestudio.pl/</a>, See on <a href="https://news.ycombinator.com/item?id=42232000">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <header>
  <a href="https://sqlitestudio.pl/gallery/"><img src="https://sqlitestudio.pl/img/front_shot.jpg"></a>
  
  <p>Create, edit, browse SQLite databases.</p>
  <a href="https://api.github.com/repos/pawelsalawa/sqlitestudio/zipball/3.4.6">Download <span></span></a>
  <a href="https://sqlitestudio.pl/donate/">Donate <span></span></a>
</header>

<div>
    <div>
        <div>
          <h2>3.4.6 released!</h2>
          <p>It's a hotfix release to address urgent problem of "black SQL code line" that appeared in 3.4.5. It also gets two more issues resolved.</p>
          <p><a href="https://sqlitestudio.pl/news/#159">Read More →</a>
        </p></div>
        <div><p>
          Posted on 23 November 2024
          <a href="https://sqlitestudio.pl/news/">More news →</a>
        </p></div>
      </div>
	<div>
		<a href="https://solidnaksiegowa.com/">
			<p><img src="https://sqlitestudio.pl/img/solidnaksiegowa.png">
			</p>
		</a>
	</div>
</div>



<div>
  <div>
        <h4>Feature rich</h4>
        <p>Powerful, yet light and fast.</p>
        <p><a href="https://sqlitestudio.pl/features/">Learn more</a>
      </p></div>
<div>
        <h4>Open Source</h4>
        <p>It's released under GPL license and is free to use for any purpose.</p>
        
      </div>
<div>
        <h4>Cross-platform</h4>
        <p>Runs on Windows, Linux and MacOS X.</p>
        
      </div>
<div>
        <h4>Portable</h4>
        <p>No need to install or uninstall. Just download, decompress and run.</p>
        
      </div>

</div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Two Factions of C++ (235 pts)]]></title>
            <link>https://herecomesthemoon.net/2024/11/two-factions-of-cpp/</link>
            <guid>42231489</guid>
            <pubDate>Sun, 24 Nov 2024 23:21:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://herecomesthemoon.net/2024/11/two-factions-of-cpp/">https://herecomesthemoon.net/2024/11/two-factions-of-cpp/</a>, See on <a href="https://news.ycombinator.com/item?id=42231489">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>
<div id="content"><div>
<figure data-imgstate="dither">
<img alt="Zero Ranger." data-dither="/2024/11/two-factions-of-cpp/images/dithers/not-bad-at-all_dithered.png" data-original="https://herecomesthemoon.net/2024/11/two-factions-of-cpp/images/not-bad-at-all_hu130da105ae916814d5129db49a9c3717_341311_800x800_fit_q90_h2_box_3.webp" loading="lazy" src="https://herecomesthemoon.net/2024/11/two-factions-of-cpp/images/dithers/not-bad-at-all_dithered.png">
<div>
<figcaption>
<span> Zero Ranger. </span>
<svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect>
<rect height="24.28" width="24.28" x="37.93" y="37.86"></rect>
<rect height="24.28" width="24.28" x="62.21" y="13.58"></rect>
<rect height="24.28" width="24.28" x="13.51" y="62.14"></rect>
<rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg>
<p><span>
             Toggle original/dithered image
          </span>
</p>
</figcaption>
</div>
</figure>
</div>
<p>There seems to be a lot of fighting and arguing over the future of C++.</p>
<p>On Reddit and a certain orange website, definitely, but also surely at the official C++ standard committee meetings. You don’t need to look very far.</p>
<h2 id="the-absolute-state-of-c">The Absolute State (of C++)</h2>
<p>It looks like we’re in the following situation:</p>
<ul>
<li>C++’s Evolution Working Group (EWG) just <a href="https://github.com/cplusplus/papers/issues/2121#issuecomment-2494153010" target="_blank">achieved consensus</a> on adopting <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p3466r0.pdf" target="_blank">P3466 R0 - (Re)affirm design principles for future C++ evolution</a>:
<ul>
<li>This means no ABI breaks, retain link compatibility with C and previous C++.</li>
<li>It also means no ‘viral annotations’ (no lifetime annotations, for example).<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></li>
<li>It heavily doubles down on a set of incompatible goals, ie. no ABI break and the zero-overhead-principle.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></li>
<li>Whether this is good or bad, it is a (literal) doubling down on the current trajectory of the C++ language.</li>
</ul>
</li>
</ul>
<p>In the meantime:</p>
<ul>
<li>The US government wants people to stop using C++:
<ul>
<li><a href="https://www.cisa.gov/resources-tools/resources/product-security-bad-practices" target="_blank">The CISA</a></li>
<li><a href="https://media.defense.gov/2022/Nov/10/2003112742/-1/-1/0/CSI_SOFTWARE_MEMORY_SAFETY.PDF" target="_blank">The NSA</a></li>
<li><a href="https://www.whitehouse.gov/wp-content/uploads/2024/02/Final-ONCD-Technical-Report.pdf" target="_blank">The White House, apparently.</a></li>
<li>No, really. Various branches of the US government have released papers, reports, recommendation to warn the industry against usage of memory-unsafe languages.</li>
</ul>
</li>
<li>All sorts of big tech players are adopting Rust:
<ul>
<li>Microsoft is apparently <a href="https://www.theregister.com/2023/04/27/microsoft_windows_rust/" target="_blank">rewriting core-libraries in Rust</a>.</li>
<li>Google seems to <a href="https://security.googleblog.com/2021/04/rust-in-android-platform.html" target="_blank">be committing to Rust</a>, and in fact started working on a <a href="https://github.com/google/crubit" target="_blank">bidirectional C++/Rust interop tool</a>.</li>
<li>AWS is <a href="https://aws.amazon.com/blogs/devops/why-aws-is-the-best-place-to-run-rust/" target="_blank">using Rust</a>.</li>
<li>etc.</li>
</ul>
</li>
<li>Speaking of big tech, did you notice that <a href="https://herbsutter.com/2024/11/11/a-new-chapter-and-a-pivotal-year-for-cpp/" target="_blank">Herb Sutter is leaving Microsoft</a>, and that it seems like <a href="https://www.reddit.com/r/cpp/comments/1gkdr6e/msvc_c23_support/" target="_blank">MSVC is slow to implement C++23 features, and asking the community for prioritization</a>.</li>
<li>The infamous <a href="https://cor3ntin.github.io/posts/abi/" target="_blank">Prague ABI-vote</a> happened (tl;dr: “C++23 will not break ABI, it’s unclear if it ever will.”), Google supposedly significantly lowered its participation in the C++ development process, and instead started to work on <a href="https://www.youtube.com/watch?v=omrY53kbVoA" target="_blank">their own C++ successor language</a>. They even have a <a href="https://github.com/carbon-language/carbon-lang/blob/trunk/docs/project/difficulties_improving_cpp.md" target="_blank">summary</a> outlining all of the issues they had trying to improve C++.<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></li>
<li><a href="https://thephd.dev/finally-embed-in-c23" target="_blank">Stories</a> of people trying their best to participate in the C++-standard committee process across multiple years, only to be chewed up and spit out are widely known and shared throughout the community. (A feature landing in <em>C</em> first doesn’t help either.)</li>
<li>Modules are still not implemented. <a href="https://arewemodulesyet.org/" target="_blank">Are we modules yet?</a> <img alt="image" src="https://herecomesthemoon.net/2024/11/two-factions-of-cpp/images/modules.png"></li>
<li><a href="https://isocpp.org/files/papers/P3081R0.pdf" target="_blank">‘Safety Profiles’</a> are still in a weird state with no existing implementation, trying to retrofit <em>some</em> amount of safety onto existing C++ code while minimizing changes to existing code. Sean Baxter himself <a href="https://www.circle-lang.org/draft-profiles.html" target="_blank">took a stance</a> against profiles, and described C++ as “underspecified”.</li>
</ul>
<p>I don’t know about you, but if I were to look at all of this <em>as an outsider</em>, it sure would look as if C++ is <em>basically falling apart</em>, and as if a vast amount of people lost faith in the ability of C++’s committee to <em>somehow</em> stay on top of this.<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></p>
<h2 id="two-cultures">Two Cultures</h2>
<p>People seem to be looking for other solutions.</p>
<p>Say, Google. Google evidently lost faith in ’the process’ ever since the ABI-vote. This isn’t a loss of faith in the language itself, Google has one of the largest C++ codebases in the world, and it has served them incredibly well. It’s a loss of faith in the language’s ability to evolve as pressure mounts from different angles (potential government regulations, competing languages, a desire for better performance and safety guarantees from key players, etc.).</p>
<p>So what’s the problem? Why doesn’t C++ just…change?</p>
<p>Well, figuring that out is easy. Just <a href="https://isocpp.org/files/papers/P3081R0.pdf" target="_blank">look at what Herb Sutter said in his paper on profiles</a>:</p>
<blockquote>
<p>“<strong>We must minimize the need to change existing code.</strong> For adoption in existing code, decades of experience has consistently shown that most customers with large code bases cannot and will not change even 1% of their lines of code in order to satisfy strictness rules, not even for safety reasons unless regulatory requirements compel them to do so.” – Herb Sutter</p>
</blockquote>
<p>Cool. Is anyone surprised by this? I don’t think so.</p>
<p>Now, let’s contrast this with <a href="https://isocpp.org/wiki/faq/wg21#chandler-carruth" target="_blank">Chandler Carruth’s biography on the WG21 member page</a>:</p>
<blockquote>
<p>I led the design of <strong>C++ tooling and automated refactoring systems built on top of Clang</strong> and now part of the Clang project. […]<br>
Within Google, I led the effort to scale the <strong>automated Clang-based refactoring</strong> tools up to our entire codebase, over 100 million lines of C++ code. We can analyze and apply refactorings across the entire codebase in 20 minutes.</p>
</blockquote>
<p>Oh. Do you see it? (Yes you do, I highlighted it.)</p>
<p>It’s “automated migration tooling”. Except it’s not <em>just</em> that, automated migration tooling is just the peak, the single brightly glowing example.</p>
<p>We’re basically seeing a conflict between two starkly different camps of C++-users:</p>
<ul>
<li>Nimble, modern, highly capable tech corporations that understand that their code is an asset. (This isn’t strictly <em>big</em> tech. Any sane greenfield C++ startup will also fall into this category.)</li>
<li>Everyone else. Every ancient corporation where people are still fighting over how to indent their code, and some young engineer is begging management to allow him to set up a linter.</li>
</ul>
<p>One of these groups will be capable of handling a migration <em>somewhat</em> gracefully, and it’s the group that is capable of <strong>building their C++ stack from versioned source</strong>, not the group that still uses ancient pre-built libraries from 1998.</p>
<p>In practice, of course, this is a gradient. I can only imagine how much sweat, tears, bills and blood must’ve flown to turn big tech codebases from terrifying balls of mud into semi-manageable, buildable, linted, properly versioned, slightly-less-terrifying balls of mud.</p>
<p>With the bias of hindsight, it’s easy to think of all of this as inevitable: There was a clear disconnect between the needs of corporations such as Google (who use highly modern C++, have automated tooling and testing, and modern infrastructure), and the (very strong) desire for backwards compatibility.</p>
<p>To go out on a limb, the notion of a single, dialect-free and unified C++ seems like it’s been <strong>dead for years</strong>.<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> We have, at the very least, two major flavors of C++:</p>
<ul>
<li>Any <em>remotely</em> modern C++. Probably at least C++17. <code>uniqe_ptr</code>, <code>constexpr</code>, lambdas, <code>optional</code>. Everything can be built from versioned source using some sort of dedicated, clean and unified build process that’s at least slightly more sophisticated than raw CMake, and sort of looks like it just works if you squint a little. Some sort of static analyzers, formatter, linter. <em>Any</em> sort of agreement that keeping a codebase clean and modern is worthwhile.</li>
<li>Anything that’s not that. Any C++ that’s been sitting in ancient, dusted-up servers of a medium-sized bank. Any C++ that relies on some utterly ancient chunk of compiled code, whose source has been lost, and whose original authors are unreachable. Any C++ that sits deployed on pet-type servers, to the point that spinning it up anywhere else would take an engineer a full month just to figure out all of the implicit dependencies, configs, and environment variables. Any codebase which is primarily classified as a cost-center.</li>
</ul>
<p>You’ll notice that the main difference isn’t about C++ itself at all. The difference is <em>tooling</em> and the ability to build from versioned source in any clean, well-defined manner. Ideally, even the ability to <em>deploy</em> without needing to remember that one flag or environment variable the previous guy usually set to keep everything from imploding.</p>
<p>A lot of people will tell you that tooling isn’t the responsibility of the C++ standard committee, and <em>they are right</em>. Tooling isn’t the responsibility of the C++ standard committee, <em>because the C++ standard committee abdicates any responsibility for it</em> (it focuses on specifications for the C++ language, <em>not</em> on concrete implementations). This is by design, and it’s hard to blame them considering the legacy baggage. C++ is a standard unifying different implementations.</p>
<p>That said, if there’s <em>one</em> thing which Go got right, it’s that tooling matters. C++, in comparison, is from a prehistoric age before linters were invented. C++ has no unified build system, it has nothing even close to a unified package management system, it is incredibly hard to parse and analyze (this is <em>terrible</em>  for tooling), and is fighting a horrifying uphill battle against Hyrum’s Law for every change that needs to be made.</p>
<p>There’s a massive, growing rift between those two factions, and I honestly don’t see it closing anytime soon. The C++ committee seems pretty committed (committeed, if you will) to maintaining backwards compatibility, no matter the cost.</p>
<h2 id="consequences">Consequences</h2>
<p>This is why profiles are the way they are: Safety Profiles are <em>not</em> intended to solve the problems of modern, tech-savvy C++ corporations. They’re intended to bring improvements <em>without</em> requiring any changes to old code.</p>
<p>Likewise, modules. You’re intended to be able to “just” import a header file as a module, and there should not be any sort of backwards compatibility issues.</p>
<p>Of course, <em>everyone</em> loves features which can just be dropped-in and bring improvements without requiring any changes to old code. But it’s pretty clear that these features are designed (first and foremost) with the goal of ’legacy C++’ in mind. Any feature that would require a migration from legacy C++ is a non-starter for the C++ committee since, as Herb Sutter said, you essentially cannot expect people to migrate.</p>
<p>This is something which I try to keep in mind when I look at C++ papers: There’s two large audiences here. One is that of modern C++, the other is that of legacy C++. These two camps disagree fiercely, and many papers are written with the needs of one specific group in mind.</p>
<p>The C++-committee is trying to keep this rift from widening. That’s, presumably, why anything in the direction of <a href="https://safecpp.org/draft.html" target="_blank">Safe C++ by Sean Baxter</a> is a non-starter for them. This is a radical, sweeping change that could create a fundamentally new way of writing C++.</p>
<p>Of course, there’s also the question of whether specific C++ standard committee members are just being very, very stubborn, and grasping at straws to prevent an evolution which they personally aesthetically disagree with.</p>
<p>Far be it from to accuse anyone, but it wouldn’t be the first time I heard that the C++ committee applied double standards such as: “We expect a full, working implementation across several working compilers from you if you’d like to see this proposal approved, but we’re still happy to commit to certain vast projects (eg. modules, profiles) that have no functioning proof of concept implementation.”</p>
<p>If <em>this</em> were the case (I genuinely cannot say) then I really wouldn’t know for how much longer C++ could continue going down that road without a much more dramatic split.</p>
<p>And all of that that is not even getting into the massive can of worms and problems that’d be caused by breaking ABI compatibility.</p>
</div>

</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bluesky is on the verge of overtaking Threads in all the ways that matter (274 pts)]]></title>
            <link>https://mashable.com/article/bluesky-gaining-ground-on-competitor-meta-threads</link>
            <guid>42231148</guid>
            <pubDate>Sun, 24 Nov 2024 22:23:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mashable.com/article/bluesky-gaining-ground-on-competitor-meta-threads">https://mashable.com/article/bluesky-gaining-ground-on-competitor-meta-threads</a>, See on <a href="https://news.ycombinator.com/item?id=42231148">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        

        
    <p>Meta-owned Threads started November with 5 times the daily app users of Bluesky. That number is now down to just 1.5.</p>
    

    
</div><section data-ga-module="content_body">
                        <div>
                <p><img src="https://helios-i.mashable.com/imagery/articles/05ga38nPz2tMnT4fT0Plssj/hero-image.fill.size_1248x702.v1732394007.jpg" alt="The image shows the logo of Bluesky" width="1248" height="702" srcset="https://helios-i.mashable.com/imagery/articles/05ga38nPz2tMnT4fT0Plssj/hero-image.fill.size_400x225.v1732394007.jpg 400w, https://helios-i.mashable.com/imagery/articles/05ga38nPz2tMnT4fT0Plssj/hero-image.fill.size_800x450.v1732394007.jpg 800w, https://helios-i.mashable.com/imagery/articles/05ga38nPz2tMnT4fT0Plssj/hero-image.fill.size_1248x702.v1732394007.jpg 1600w" sizes="(max-width: 1280px) 100vw, 1280px"></p><p><span>Credit: Matteo Della Torre/NurPhoto via Getty Images</span>
                            </p>
                        </div>

    
    
    
            <article id="article" data-autopogo="">
                                    <p><a href="https://mashable.com/article/bluesky-fake-accounts-problem" target="_self" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body">Bluesky</a>, the formerly Jack Dorsey-affiliated, decentralized answer to Elon Musk's X is closing the gap with Threads at breakneck speed. The browser version of Bluesky surpassed Threads in total usage weeks ago, but now the Bluesky app has exploded to 3.5 million daily active users, putting it just 1.5 times behind Meta’s Threads — an impressive feat considering the Threads app had 5x Bluesky's active users at the start of the month.</p>
<p>The momentum shift has been nothing short of seismic, especially in the wake of the November 5 election. According to Similarweb data <a href="https://www.ft.com/content/e1b52147-c171-4902-8bce-204ba0905912" target="_blank" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body" title="(opens in a new window)">reported by the<em> Financial Times</em></a>, Bluesky’s user base has ballooned by 300 percent since Election Day. Journalists, academics, and companies are fleeing Elon Musk’s chaotic X (formerly Twitter) in droves, and Bluesky is quickly becoming their platform of choice.</p><p>Why Bluesky over Threads? Meta CEO Mark Zuckerberg’s decision to <a href="https://mashable.com/article/instagram-threads-political-content" target="_self" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body">downplay political content</a> on Threads appears to have turned off many users seeking vibrant public discourse. Critics see it as an <a href="https://thehill.com/policy/technology/4934534-trump-zuckerberg-politics-facebook/" target="_blank" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body" title="(opens in a new window)">attempt to stay in President-elect Donald Trump’s good graces</a>, effectively neutering the platform’s potential as a forum for political and cultural debate. </p><section x-data="window.newsletter()" x-init="init()" data-ga-impression="" data-ga-category="newsletters" data-ga-module="incontent_nl_signup" data-ga-label="mashablelightspeed">
        <p>
            Mashable Light Speed
        </p>
        
        
    </section>
<p>Bluesky has quickly become the go-to platform for what commentator Max Read has <a href="https://maxread.substack.com/p/does-bluesky-have-the-juice" target="_blank" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body" title="(opens in a new window)">called</a> the "Politically Engaged Email Job Blob" — the same cohort that helped transform early Twitter into the cultural juggernaut it once was.</p><p>That said, Bluesky is still very much a work in progress. Its rapid growth has brought its share of headaches, including outages, glitches, and <a href="https://mashable.com/article/bluesky-fake-accounts-problem" target="_self" data-ga-click="1" data-ga-label="$text" data-ga-item="text-link" data-ga-module="content_body">scammers</a>. As more users flock to this latest "Twitter replacement," expect some inevitable growing pains along the way.</p>

                                        
                    </article>
    
    
    
            <div>
            <div>
                    <p><img src="https://helios-i.mashable.com/imagery/authors/03tgQJc7HbwhuDaGMuSAh8J/image.fill.size_100x100.v1673717191.jpg" alt="Headshot of a Black man" width="100" height="100" loading="lazy"></p><div>
                        
                        <p>Assistant Editor, General Assignments</p>
                    </div>
                </div>
            <div>
                <p>Currently residing in Chicago, Illinois, Chance Townsend is the General Assignments Editor at Mashable covering tech, video games, dating apps, digital culture, and whatever else comes his way. He has a Master's in Journalism from the University of North Texas and is a proud orange cat father. His writing has also appeared in PC Mag and <em>Mother Jones</em>.</p><p>In his free time, he cooks, loves to sleep, and finds great enjoyment in Detroit sports. If you have any stories, tips, recipes, or wanna talk shop about the Lions/Tigers/Pistons/Red Wings you can reach him at <a href="https://mashable.com/cdn-cgi/l/email-protection" data-cfemail="36555e57585553184259415845535852764c5f50505b53525f571855595b">[email&nbsp;protected]</a></p>
            </div>
        </div>
        
        
                    </section><section data-ga-module="content_body">
        <section>
                                    <hr>
            
                            
                            <div data-module="content-list" data-ga-module="recommendation-recirc" data-ga-element="content-stripe" data-ga-action="content-stripe">
                            
                                                    <hr>
                                            
                                                    <hr>
                                            
                                                    <hr>
                                            
                                                    <hr>
                                            <div data-ga-position="5">
    
    <a data-ga-click="" data-ga-item="image" data-ga-label="X users are fleeing to Bluesky: Here’s a quick-start guide on how to sign up" href="https://mashable.com/article/getting-started-on-bluesky-guide">
        <p><img src="https://helios-i.mashable.com/imagery/articles/00hLKqqdMxBC9PKtD18sBCb/hero-image.fill.size_220x133.v1731628618.jpg" alt="Bluesky logo" width="220" height="133" loading="lazy">


        </p>
        <p><img src="https://helios-i.mashable.com/imagery/articles/00hLKqqdMxBC9PKtD18sBCb/hero-image.fill.size_220x220.v1731628618.jpg" alt="Bluesky logo" width="220" height="220" loading="lazy">


        </p>
    </a>
</div>
                                                    </div>
                                </section>

    
    
    </section><div x-data="window.newsletter()" x-init="init()" data-ga-impression="" data-ga-category="newsletters" data-ga-module="footer_nl_signup" data-ga-label="Top Stories">
    

    <p>
        This newsletter may contain advertising, deals, or affiliate links. Subscribing to a newsletter indicates your consent to our <a href="https://www.ziffdavis.com/terms-of-use" target="_blank" rel="noopener" title="(opens in a new window)">Terms of Use</a> and <a href="https://www.ziffdavis.com/ztg-privacy-policy" target="_blank" rel="noopener" title="(opens in a new window)">Privacy Policy</a>. You may unsubscribe from the newsletters at any time.
    </p>
    
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[This website is hosted on Bluesky (586 pts)]]></title>
            <link>https://danielmangum.com/posts/this-website-is-hosted-on-bluesky/</link>
            <guid>42230392</guid>
            <pubDate>Sun, 24 Nov 2024 20:43:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danielmangum.com/posts/this-website-is-hosted-on-bluesky/">https://danielmangum.com/posts/this-website-is-hosted-on-bluesky/</a>, See on <a href="https://news.ycombinator.com/item?id=42230392">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        <p>Well, not this one. But <a href="https://porcini.us-east.host.bsky.network/xrpc/com.atproto.sync.getBlob?did=did:plc:j22nebhg6aek3kt2mex5ng7e&amp;cid=bafkreic5fmelmhqoqxfjz2siw5ey43ixwlzg5gvv2pkkz7o25ikepv4zeq">this
one</a>
is! How? Let’s take a closer look at <a href="https://bsky.app/">Bluesky</a> and the <a href="https://atproto.com/">AT
Protocol</a> that underpins it.</p>
<blockquote>
<p>Note: I communicated with the Bluesky team prior to the publishing of this
post. While the functionality described is not the intended use of the
application, it is known behavior and does not constitue a vulnerability
disclosure process. My main motivation for reaching out to them was because I
like the folks and don’t want to make their lives harder.</p>
</blockquote>



<p><img src="https://danielmangum.com/static/website_on_bsky_0.png" alt="website-on-bsky-0">
</p>
<p>Being able to host a website on Bluesky really has very little to do with
Bluesky itself. I happen to use Bluesky for hosting my <a href="https://atproto.com/guides/glossary#pds-personal-data-server">Personal Data Server
(PDS)</a>, but all of
the APIs leveraged in uploading the site contents are defined at the AT Protocol
level and implemented by a PDS. Bluesky offers access to my PDS via their <a href="https://docs.bsky.app/docs/advanced-guides/entryway">PDS
entryway</a>, which allows for
the many (have you heard that they are <a href="https://thehill.com/policy/technology/4999675-bluesky-ceo-1m-people-a-day-joining-in-past-week/">growing by a million users per
day</a>?)
PDS instances they run to be exposed via the <code>bsky.social</code> domain. That being
said, individual PDS instances can be accessed directly, and if you clicked the
link at the top of this post to access the Bluesky hosted website, then you have
already visited mine at <code>porcini.us-east.host.bsky.network</code>.</p>
<p>Most social applications, and many applications in general for that matter,
broadly have two primary types of content:
<a href="https://atproto.com/guides/glossary#record">records</a> and
<a href="https://atproto.com/guides/glossary#blob">blobs</a>. Records are the core entity
types that users create. They generally have some defined structure and
metadata, and they may reference other records or content. Blobs are typically
larger unstructured data, such as media assets, that may be uploaded by a user,
but are exposed via a record referencing them. For example, on Bluesky a user
may upload an image, then create a post that references it. From an end-user
perspective, these two operations appear to be one action, but they are
typically decoupled at the API level.</p>
<p>This decoupling is described in detail in the AT Protocol <a href="https://atproto.com/specs/blob">blob
specification</a>.</p>
<blockquote>
<p>Blob files are uploaded and distributed separately from records. Blobs are
authoritatively stored by the account’s PDS instance, but views are commonly
served by CDNs associated with individual applications (“AppViews”), to reduce
traffic on the PDS. CDNs may serve transformed (resized, transcoded, etc)
versions of the original blob.</p>
</blockquote>
<p>Later on, the specification details how blob lifecylce is to be managed.</p>
<blockquote>
<p>Blobs must be uploaded to the PDS before a record can be created referencing
that blob. Note that the server does not know the intended Lexicon when
receiving an upload, so can only apply generic blob limits and restrictions at
initial upload time, and then enforce Lexicon-defined limits later when the
record is created.</p>
</blockquote>
<p>Reading this section is what initially got my wheels turning. While Bluesky has
a limited set of media asset types that can be referenced by posts, posts are
just one record type that is defined by the Bluesky
<a href="https://atproto.com/specs/lexicon">lexicon</a> (<code>app.bsky.*</code>). Records, on the
other hand, are defined in the AT Protocol lexicon (<code>com.atproto.*</code>) and are
designed to accommodate creating any <em>type</em> of record defined by any lexicon.
Because different types of blobs may be relevant for other lexicons, the
specification highlights that restrictions cannot be enforced at time of upload.</p>
<p>Instead blobs are not made available until they are referenced, at which point
the validation can be performed based on the lexicon of the record type.</p>
<blockquote>
<p>After a successful upload, blobs are placed in temporary storage. They are not
accessible for download or distribution while in this state. Servers should
“garbage collect” (delete) un-referenced temporary blobs after an appropriate
time span (see implementation guidelines below). Blobs which are in temporary
storage should not be included in the <code>listBlobs</code> output.</p>
</blockquote>
<blockquote>
<p>The upload blob can now be referenced from records by including the returned
blob metadata in a record. When processing record creation, the server
extracts the set of all referenced blobs, and checks that they are either
already referenced, or are in temporary storage. Once the record creation
succeeds, the server makes the blob publicly accessible.</p>
</blockquote>
<p>However, applying validation does not mean that Bluesky’s restrictions will
necessarily be applied. A record that references a blob could very well be of a
type defined by a different lexicon, or, as we’ll see later on, part of a
<a href="https://docs.bsky.app/docs/advanced-guides/custom-schemas#sub-schemas">sub-schema</a>
enabled by an open union in the Bluesky lexicon. Let’s see how this works in
practice.</p>
<p>In order to perform data creation operations against a PDS, an access token must
be acquired for authentication. The
<a href="https://docs.bsky.app/docs/api/com-atproto-server-create-session"><code>com.atproto.server.createSession</code></a>
<a href="https://atproto.com/guides/glossary#xrpc">XRPC</a> method can be used to exchange
user credentials for a token. In the following <code>curl</code> command, I used
<code>danielmangum.com</code> as <code>$BSKY_HANDLE</code> and my password as <code>$BSKY_PWD</code>.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>curl -X POST 'https://bsky.social/xrpc/com.atproto.server.createSession' \
</span></span><span><span>-H 'Content-Type: application/json' \
</span></span><span><span>-d '{"identifier": "'"$BSKY_HANDLE"'", "password": "'"$BSKY_PWD"'"}'
</span></span></code></pre></div><p>The response includes an <code>accessJWT</code> field, which will be used as <code>$ACCESS_JWT</code>
in subsequent operations. As described in the blob specification, a blob must be
uploaded prior to it being referenced. I wanted to verify that the blob was not
present in the
<a href="https://docs.bsky.app/docs/api/com-atproto-sync-list-blobs"><code>com.atproto.sync.listBlobs</code></a>
output, or accessible via the
<a href="https://docs.bsky.app/docs/api/com-atproto-sync-get-blob"><code>com.atproto.sync.getBlob</code></a>
methods immediately after upload, so I checked how many blobs were currently
being returned.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>curl -s 'https://bsky.social/xrpc/com.atproto.sync.listBlobs?did='"$DID"'' \
</span></span><span><span>-H 'Authorization: Bearer '"$ACCESS_JWT"'' | jq -r '.cids | length'
</span></span></code></pre></div><p>The <a href="https://atproto.com/specs/did">decentralized identifier (<code>$DID</code>)</a> used
above can be obtained from the <code>createSession</code> output as well. It is the
underlying identifier for an account. Every Bluesky handle <a href="https://bsky.social/about/blog/4-28-2023-domain-handle-tutorial">resolves to a
DID</a>.</p>
<p>The
<a href="https://docs.bsky.app/docs/api/com-atproto-repo-upload-blob"><code>com.atproto.repo.uploadBlob</code></a>
method is used to upload a blob to a repository. The content of the website is a
simple <code>index.html</code> file.</p>
<div><pre tabindex="0"><code data-lang="html"><span><span>&lt;<span>h1</span>&gt;This Website is Hosted on Bluesky&lt;/<span>h1</span>&gt;
</span></span><span><span>
</span></span><span><span>&lt;<span>p</span>&gt;
</span></span><span><span>This website is just a blob uploaded to Bluesky via the API. Curious about how
</span></span><span><span>this works? Check out the write-up on &lt;<span>a</span>
</span></span><span><span><span>href</span>=<span>"https://danielmangum.com/posts/this-website-is-hosted-on-bluesky/"</span>&gt;danielmangum.com&lt;/<span>a</span>&gt;.
</span></span><span><span>&lt;/<span>p</span>&gt;
</span></span></code></pre></div><p>To upload it, I used the following command.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>curl -X POST 'https://bsky.social/xrpc/com.atproto.repo.uploadBlob' \
</span></span><span><span>-H 'Authorization: Bearer '"$ACCESS_JWT"'' \
</span></span><span><span>-H 'Content-Type: text/html' \
</span></span><span><span>--data-binary '@index.html'
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="json"><span><span>{
</span></span><span><span>  <span>"blob"</span>: {
</span></span><span><span>    <span>"$type"</span>: <span>"blob"</span>,
</span></span><span><span>    <span>"ref"</span>: {
</span></span><span><span>      <span>"$link"</span>: <span>"bafkreic5fmelmhqoqxfjz2siw5ey43ixwlzg5gvv2pkkz7o25ikepv4zeq"</span>
</span></span><span><span>    },
</span></span><span><span>    <span>"mimeType"</span>: <span>"text/html"</span>,
</span></span><span><span>    <span>"size"</span>: <span>268</span>
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div><p>The returned <code>$link</code> can be used as the <a href="https://atproto.com/guides/glossary#cid-content-id">Content Identifier
(<code>cid</code>)</a> when fetching the
blob via the <code>getBlob</code> method. However, according to the specification, because
this blob has to be referenced, it shouldn’t be visible. I checked to see if I
could access it with the following command.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>curl -L 'https://bsky.social/xrpc/com.atproto.sync.getBlob?did='"$DID"'&amp;cid='"$LINK"'' 
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="json"><span><span>{
</span></span><span><span>  <span>"error"</span>: <span>"InternalServerError"</span>,
</span></span><span><span>  <span>"message"</span>: <span>"Internal Server Error"</span>
</span></span><span><span>}
</span></span></code></pre></div><p>Not the error I was expecting, but it looks like I indeed cannot access it. I
was also able to determine that it had not beed added to the <code>listBlobs</code> output.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>curl -s 'https://bsky.social/xrpc/com.atproto.sync.listBlobs?did='"$DID"'' \
</span></span><span><span>-H 'Authorization: Bearer '"$ACCESS_JWT"'' | jq -r '.cids | length'
</span></span></code></pre></div><p>Blobs can be referenced in <a href="https://github.com/bluesky-social/atproto/blob/b5c6bce9703faa7a8eb7f629d34f173116cb37b8/lexicons/app/bsky/feed/post.json"><code>app.bsky.feed.post</code>
records</a>
on Bluesky by including an <a href="https://docs.bsky.app/docs/advanced-guides/posts#images-embeds">embedded
image</a>. However,
the <a href="https://github.com/bluesky-social/atproto/blob/b5c6bce9703faa7a8eb7f629d34f173116cb37b8/lexicons/app/bsky/embed/images.json#L23"><code>app.bsky.embed.image</code>
schema</a>
retricts the <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/MIME_types">MIME
type</a> to those
prefixed with <code>image/*</code>. We can see this validation in action if we try to
create a post with an embedded image.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>curl -X POST 'https://bsky.social/xrpc/com.atproto.repo.createRecord' \
</span></span><span><span>-H 'Authorization: Bearer '"$ACCESS_JWT"'' \
</span></span><span><span>-H 'Content-Type: application/json' \
</span></span><span><span>-d '{
</span></span><span><span>  "repo": "danielmangum.com",
</span></span><span><span>  "collection": "app.bsky.feed.post",
</span></span><span><span>  "record": {
</span></span><span><span>    "$type": "app.bsky.feed.post",
</span></span><span><span>    "text": "testing123",
</span></span><span><span>    "createdAt": "2024-11-23T05:49:35.422015Z",
</span></span><span><span>    "embed": {
</span></span><span><span>      "$type": "app.bsky.embed.images",
</span></span><span><span>      "images": [
</span></span><span><span>        {
</span></span><span><span>          "alt": "that is not an image that is a website!",
</span></span><span><span>          "image": {
</span></span><span><span>            "$type": "blob",
</span></span><span><span>            "ref": {
</span></span><span><span>              "$link": "bafkreidphtuvbzublyzacxukmmk2ikiur5ahme75fegokbhh26o4wfzvry"
</span></span><span><span>            },
</span></span><span><span>            "mimeType": "text/html",
</span></span><span><span>            "size": 21
</span></span><span><span>          }
</span></span><span><span>        }
</span></span><span><span>      ]
</span></span><span><span>    }
</span></span><span><span>  }
</span></span><span><span>}'
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="json"><span><span>{
</span></span><span><span>  <span>"error"</span>: <span>"InvalidMimeType"</span>,
</span></span><span><span>  <span>"message"</span>: <span>"Wrong type of file. It is text/html but it must match image/*."</span>
</span></span><span><span>}
</span></span></code></pre></div><p>For completeness, I also tried specifying the <code>mimeType</code> as <code>image/jpeg</code> and
verified that the PDS also validates that the blob reference MIME type matches
the blob.</p>
<div><pre tabindex="0"><code data-lang="json"><span><span>{
</span></span><span><span>  <span>"error"</span>: <span>"InvalidMimeType"</span>,
</span></span><span><span>  <span>"message"</span>: <span>"Referenced Mimetype does not match stored blob. Expected: text/html, Got: image/jpeg"</span>
</span></span><span><span>}
</span></span></code></pre></div><p>However, the <a href="https://atproto.com/specs/data-model#blob-type"><code>blob</code> <code>$type</code> is part of the AT Protocol data
model</a> and not specific to
Bluesky. Because Bluesky’s PDS implementation is open source, we can see exactly
how a <a href="https://github.com/bluesky-social/atproto/blob/b5c6bce9703faa7a8eb7f629d34f173116cb37b8/packages/lexicon/src/blob-refs.ts#L26"><code>BlobRef</code> is
defined</a>.</p>
<div><pre tabindex="0"><code data-lang="ts"><span><span><span>export</span> <span>class</span> BlobRef {
</span></span><span><span>  <span>public</span> original: <span>JsonBlobRef</span>
</span></span><span><span>
</span></span><span><span>  <span>constructor</span>(
</span></span><span><span>    <span>public</span> ref: <span>CID</span>,
</span></span><span><span>    <span>public</span> mimeType: <span>string</span>,
</span></span><span><span>    <span>public</span> size: <span>number</span>,
</span></span><span><span>    original?: <span>JsonBlobRef</span>,
</span></span><span><span>  ) {
</span></span><span><span>    <span>this</span>.original = original ?? {
</span></span><span><span>      $type: <span>'blob'</span>,
</span></span><span><span>      ref,
</span></span><span><span>      mimeType,
</span></span><span><span>      size,
</span></span><span><span>    }
</span></span><span><span>  }
</span></span></code></pre></div><p>We can also see exactly how the PDS <a href="https://github.com/bluesky-social/atproto/blob/b5c6bce9703faa7a8eb7f629d34f173116cb37b8/packages/pds/src/repo/prepare.ts#L282">identifies blobs in a
record</a>.</p>
<div><pre tabindex="0"><code data-lang="ts"><span><span><span>export</span> <span>const</span> findBlobRefs = (
</span></span><span><span>  val: <span>LexValue</span>,
</span></span><span><span>  path: <span>string</span>[] = [],
</span></span><span><span>  layer = <span>0</span>,
</span></span><span><span>): FoundBlobRef[] =&gt; {
</span></span><span><span>  <span>if</span> (layer &gt; <span>32</span>) {
</span></span><span><span>    <span>return</span> []
</span></span><span><span>  }
</span></span><span><span>  <span>// walk arrays
</span></span></span><span><span><span></span>  <span>if</span> (<span>Array</span>.isArray(val)) {
</span></span><span><span>    <span>return</span> val.flatMap((item) =&gt; findBlobRefs(item, path, layer + <span>1</span>))
</span></span><span><span>  }
</span></span><span><span>  <span>// objects
</span></span></span><span><span><span></span>  <span>if</span> (val &amp;&amp; <span>typeof</span> val === <span>'object'</span>) {
</span></span><span><span>    <span>// convert blobs, leaving the original encoding so that we don't change CIDs on re-encode
</span></span></span><span><span><span></span>    <span>if</span> (val <span>instanceof</span> BlobRef) {
</span></span><span><span>      <span>return</span> [
</span></span><span><span>        {
</span></span><span><span>          ref: <span>val</span>,
</span></span><span><span>          path,
</span></span><span><span>        },
</span></span><span><span>      ]
</span></span><span><span>    }
</span></span><span><span>    <span>// retain cids &amp; bytes
</span></span></span><span><span><span></span>    <span>if</span> (CID.asCID(val) || val <span>instanceof</span> Uint8Array) {
</span></span><span><span>      <span>return</span> []
</span></span><span><span>    }
</span></span><span><span>    <span>return</span> <span>Object</span>.entries(val).flatMap(([key, item]) =&gt;
</span></span><span><span>      findBlobRefs(item, [...path, key], layer + <span>1</span>),
</span></span><span><span>    )
</span></span><span><span>  }
</span></span><span><span>  <span>// pass through
</span></span></span><span><span><span></span>  <span>return</span> []
</span></span><span><span>}
</span></span></code></pre></div><p>The important thing to notice is that identifying blob references does require
the presence of a lexicon schema. <code>findBlobRefs</code> recursively navigates a
<code>LexValue</code> and looks for <code>$type: blob</code>. In order to support new lexicons over
time, the PDS needs to be able to handle lexicons that it doesn’t know about.
Because blobs are a fundamental component of so many applications, these new
lexicons also need to be able to leverage them. To put this into action, I
attempted to create a record of type <code>com.danielmangum.hack.website</code>, which
included a reference to the uploaded HTML blob.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>curl -X POST 'https://bsky.social/xrpc/com.atproto.repo.createRecord' \
</span></span><span><span>-H 'Authorization: Bearer '"$ACCESS_JWT"'' \
</span></span><span><span>-H 'Content-Type: application/json' \
</span></span><span><span>-d '{
</span></span><span><span>  "repo": "danielmangum.com",
</span></span><span><span>  "collection": "com.danielmangum.hack.website",
</span></span><span><span>  "record": {
</span></span><span><span>    "$type": "com.danielmangum.hack.website",
</span></span><span><span>    "website": {
</span></span><span><span>      "$type": "blob",
</span></span><span><span>      "ref": {
</span></span><span><span>        "$link": "bafkreic5fmelmhqoqxfjz2siw5ey43ixwlzg5gvv2pkkz7o25ikepv4zeq"
</span></span><span><span>      },
</span></span><span><span>      "mimeType": "text/html",
</span></span><span><span>      "size": 268
</span></span><span><span>    }
</span></span><span><span>  }
</span></span><span><span>}'
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="json"><span><span>{
</span></span><span><span>  <span>"uri"</span>: <span>"at://did:plc:j22nebhg6aek3kt2mex5ng7e/com.danielmangum.hack.website/3lbnguuzckm2u"</span>,
</span></span><span><span>  <span>"cid"</span>: <span>"bafyreicjcptshc7lmgb7abxlvcb5fmqqjdj6neie23szyum7rcaowmm5qm"</span>,
</span></span><span><span>  <span>"commit"</span>: {
</span></span><span><span>    <span>"cid"</span>: <span>"bafyreid6apjjy56xoyenxmg5xv356twh22n3hayecoxlf6mflpltlzpuwu"</span>,
</span></span><span><span>    <span>"rev"</span>: <span>"3lbnguuzmd42u"</span>
</span></span><span><span>  },
</span></span><span><span>  <span>"validationStatus"</span>: <span>"unknown"</span>
</span></span><span><span>}
</span></span></code></pre></div><p>It worked! We can see in the response that the PDS was unable to validate the
record (<code>validationStatus: unknown</code>) because it does not know about the
<code>com.danielmangum.hack.*</code> lexicon. Nevertheless, it will agree to persist the
record. The next step was to check whether the referenced blob had been
persisted.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>curl -s 'https://bsky.social/xrpc/com.atproto.sync.listBlobs?did='"$DID"'' \
</span></span><span><span>-H 'Authorization: Bearer '"$ACCESS_JWT"'' | jq -r '.cids | length'
</span></span></code></pre></div><p>It looked like it had as the count had increased by 1. Fetching the blob
directly would tell us for sure. Importantly, <code>getBlob</code> <em>does not require
passing the <code>$ACCESS_JWT</code></em> because unauthenticated parties need to be able to
fetch blobs to process alongside records that reference them.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>curl 'https://bsky.social/xrpc/com.atproto.sync.getBlob?did='"$DID"'&amp;cid=bafkreic5fmelmhqoqxfjz2siw5ey43ixwlzg5gvv2pkkz7o25ikepv4zeq'
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="fallback"><span><span>{
</span></span><span><span>  "error": "Redirecting",
</span></span><span><span>  "message": "Redirecting to new blob location"
</span></span><span><span>}
</span></span></code></pre></div><p>Adding <code>-L</code> to the command enables following redirects.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>curl -L 'https://bsky.social/xrpc/com.atproto.sync.getBlob?did='"$DID"'&amp;cid=bafkreic5fmelmhqoqxfjz2siw5ey43ixwlzg5gvv2pkkz7o25ikepv4zeq'
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="html"><span><span>&lt;<span>h1</span>&gt;This Website is Hosted on Bluesky&lt;/<span>h1</span>&gt;
</span></span><span><span>
</span></span><span><span>&lt;<span>p</span>&gt;
</span></span><span><span>This website is just a blob uploaded to Bluesky via the API. Curious about how
</span></span><span><span>this works? Check out the write-up on &lt;<span>a</span>
</span></span><span><span><span>href</span>=<span>"https://danielmangum.com/posts/this-website-is-hosted-on-bluesky/"</span>&gt;danielmangum.com&lt;/<span>a</span>&gt;.
</span></span><span><span>&lt;/<span>p</span>&gt;
</span></span></code></pre></div><p>Examining the redirect response, we can see that we are being directed directly
to my PDS.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>&lt; HTTP/2 302 
</span></span><span><span>&lt; date: Sun, 24 Nov 2024 13:58:21 GMT
</span></span><span><span>&lt; content-type: application/json; charset=utf-8
</span></span><span><span>&lt; content-length: 68
</span></span><span><span>&lt; location: https://porcini.us-east.host.bsky.network/xrpc/com.atproto.sync.getBlob?did=did:plc:j22nebhg6aek3kt2mex5ng7e&amp;cid=bafkreic5fmelmhqoqxfjz2siw5ey43ixwlzg5gvv2pkkz7o25ikepv4zeq
</span></span><span><span>&lt; x-powered-by: Express
</span></span><span><span>&lt; access-control-allow-origin: *
</span></span><span><span>&lt; ratelimit-limit: 3000
</span></span><span><span>&lt; ratelimit-remaining: 2997
</span></span><span><span>&lt; ratelimit-reset: 1732456898
</span></span><span><span>&lt; ratelimit-policy: 3000;w=300
</span></span><span><span>&lt; etag: W/"44-1je7JKzDJZFd5iRtOI+IS+zlOOE"
</span></span><span><span>&lt; vary: Accept-Encoding
</span></span></code></pre></div><p>Opening the <a href="https://porcini.us-east.host.bsky.network/xrpc/com.atproto.sync.getBlob?did=did:plc:j22nebhg6aek3kt2mex5ng7e&amp;cid=bafkreic5fmelmhqoqxfjz2siw5ey43ixwlzg5gvv2pkkz7o25ikepv4zeq"><code>location</code>
URL</a>
in the browser presents the website as expected, And just like that, we have a
website hosted on Bluesky! While this is not <em>really</em> the intended use of blobs
on Bluesky specifically, it <em>could</em> be a legitimate use case in the future.
Records that reference website content, code, or other binary artifacts are a
possibility on the AT Protocol. That being said, if a service like Bluesky is
running PDS instances on behalf of users, this <strong>effectively equates to free
(albiet unreliable) arbiratry file hosting</strong>, which has implications beyond just
racking up large storage and egress data fees. Returning back to the blobs
specification, there is an additional section on <a href="https://atproto.com/specs/blob#security-considerations">Security
Considerations</a>.</p>
<blockquote>
<p>Serving arbitrary user-uploaded files from a web server raises many content
security issues. For example, cross-site scripting (XSS) of scripts or SVG
content form the same “origin” as other web pages. It is effectively mandatory
to enable a Content Security Policy (LINK:
<a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP">https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP</a>) for the <code>getBlob</code>
endpoint. It is effectively not supported to dynamically serve assets directly
out of blob storage (the <code>getBlob</code> endpoint) directly to browsers and web
applications. Applications must proxy blobs, files, and assets through an
independent CDN, proxy, or other web service before serving to browsers and
web agents, and such services are expected to implement security precautions.</p>
</blockquote>
<p>Bluesky does <a href="https://github.com/bluesky-social/atproto/blob/b5c6bce9703faa7a8eb7f629d34f173116cb37b8/packages/pds/src/api/com/atproto/sync/getBlob.ts#L8">apply recommended CSP headers to the endpoint in the
handler</a>,
which guards against some of the issues described.</p>
<div><pre tabindex="0"><code data-lang="ts"><span><span>      res.setHeader(<span>'x-content-type-options'</span>, <span>'nosniff'</span>)
</span></span><span><span>      res.setHeader(<span>'content-security-policy'</span>, <span>`default-src 'none'; sandbox`</span>)
</span></span></code></pre></div><p>There is also a <a href="https://github.com/bluesky-social/atproto/blob/b5c6bce9703faa7a8eb7f629d34f173116cb37b8/packages/pds/src/config/config.ts#L28">default size limit on blob of 5
MB</a>.</p>
<div><pre tabindex="0"><code data-lang="ts"><span><span>    blobUploadLimit: <span>env.blobUploadLimit</span> ?? <span>5</span> * <span>1024</span> * <span>1024</span>, <span>// 5mb
</span></span></span></code></pre></div><p>Images, the most common blob type on the Bluesky application, are expectedly not
served directly from PDS instances, but from the Bluesky CDN. For example, the
following URL points to the feed thumbnail version of an image I recently
uploaded as part of a post.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:j22nebhg6aek3kt2mex5ng7e/bafkreie5ci75iujpv34slnh3o4b7xcuxklpcguzed6qqmi4eaagn6cg4ve@jpeg
</span></span></code></pre></div><p>A different URL provides the full size version.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>https://cdn.bsky.app/img/feed_fullsize/plain/did:plc:j22nebhg6aek3kt2mex5ng7e/bafkreie5ci75iujpv34slnh3o4b7xcuxklpcguzed6qqmi4eaagn6cg4ve@jpeg
</span></span></code></pre></div><p>However, the post that references the image just includes the <code>cid</code>. The
application itself needs to be aware of how images are served from the CDN.</p>
<div><pre tabindex="0"><code data-lang="json"><span><span>{
</span></span><span><span>  <span>"$type"</span>: <span>"app.bsky.feed.post"</span>,
</span></span><span><span>  <span>"createdAt"</span>: <span>"2024-11-12T14:18:44.263Z"</span>,
</span></span><span><span>  <span>"embed"</span>: {
</span></span><span><span>    <span>"$type"</span>: <span>"app.bsky.embed.images"</span>,
</span></span><span><span>    <span>"images"</span>: [
</span></span><span><span>      {
</span></span><span><span>        <span>"alt"</span>: <span>"Title image for blog post \"USB On-The-Go on the ESP32-S3\" on danielmangum.com."</span>,
</span></span><span><span>        <span>"aspectRatio"</span>: {
</span></span><span><span>          <span>"height"</span>: <span>1080</span>,
</span></span><span><span>          <span>"width"</span>: <span>1920</span>
</span></span><span><span>        },
</span></span><span><span>        <span>"image"</span>: {
</span></span><span><span>          <span>"$type"</span>: <span>"blob"</span>,
</span></span><span><span>          <span>"ref"</span>: {
</span></span><span><span>            <span>"$link"</span>: <span>"bafkreie5ci75iujpv34slnh3o4b7xcuxklpcguzed6qqmi4eaagn6cg4ve"</span>
</span></span><span><span>          },
</span></span><span><span>          <span>"mimeType"</span>: <span>"image/jpeg"</span>,
</span></span><span><span>          <span>"size"</span>: <span>869901</span>
</span></span><span><span>        }
</span></span><span><span>      }
</span></span><span><span>    ]
</span></span><span><span>  },
</span></span><span><span>  <span>"facets"</span>: [
</span></span><span><span>    {
</span></span><span><span>      <span>"features"</span>: [
</span></span><span><span>        {
</span></span><span><span>          <span>"$type"</span>: <span>"app.bsky.richtext.facet#link"</span>,
</span></span><span><span>          <span>"uri"</span>: <span>"https://danielmangum.com/posts/usb-otg-esp32s3/"</span>
</span></span><span><span>        }
</span></span><span><span>      ],
</span></span><span><span>      <span>"index"</span>: {
</span></span><span><span>        <span>"byteEnd"</span>: <span>261</span>,
</span></span><span><span>        <span>"byteStart"</span>: <span>229</span>
</span></span><span><span>      }
</span></span><span><span>    }
</span></span><span><span>  ],
</span></span><span><span>  <span>"langs"</span>: [
</span></span><span><span>    <span>"en"</span>
</span></span><span><span>  ],
</span></span><span><span>  <span>"text"</span>: <span>"ICYMI: This weekend I wrote about USB On-The-Go on the ESP32-S3. OTG allows devices to also act as USB hosts. I dive into how the USB PHY is configured, and demonstrate connecting two ESP32-S3's, as well as a Raspberry Pi Pico.\n\ndanielmangum.com/posts/usb-ot..."</span>
</span></span><span><span>}
</span></span></code></pre></div><p>The logic is present in <a href="https://github.com/bluesky-social/atproto/blob/b5c6bce9703faa7a8eb7f629d34f173116cb37b8/packages/bsky/src/image/uri.ts#L12">the
<code>ImageUriBuilder</code></a>,
which <a href="https://github.com/bluesky-social/atproto/blob/b5c6bce9703faa7a8eb7f629d34f173116cb37b8/packages/bsky/src/index.ts#L64">will use a CDN if one is
configured</a>.</p>
<div><pre tabindex="0"><code data-lang="ts"><span><span>    <span>const</span> imgUriBuilder = <span>new</span> ImageUriBuilder(
</span></span><span><span>      config.cdnUrl || <span>`</span><span>${</span>config.publicUrl<span>}</span><span>/img`</span>,
</span></span><span><span>    )
</span></span></code></pre></div><p>So why does Bluesky provide direct unauthenticated access to the PDS <code>getBlobs</code>
endpoint? Once again illustrating the beauty of open source, there is an <a href="https://github.com/bluesky-social/atproto/issues/523">issue
describing the original
motivation</a>. In it, image
labeling and user content export, as well as additional future use cases, are
enumerated. There is also a
<a href="https://github.com/bluesky-social/atproto/issues/523#issuecomment-1450545120">mention</a>
of the possibility of users hotlinking content and Bluesky for free hosting, so
these issues are clearly top-of-mind. The <a href="https://github.com/bluesky-social/atproto/pull/606">original
implementation</a> did not
include the proper security headers, but they were <a href="https://github.com/bluesky-social/atproto/commit/3d1b3b367575f2b4d2856a4795b1ca94a9f16554">subsequently
added</a>.</p>
<p>Traditional social platforms can place more restrictions on blobs at time of
upload because there is a limited set of valid content. The extensibility of
Bluesky and the AT Protocol, which is what differentiates it from traditional
networks, also necessitates more complexity. However, I, and clearly the awesome
folks building Bluesky, think it’s clearly worth it.</p>
<h2 id="bonus-content">
  Bonus Content
  <a href="#bonus-content">
    
    <span>Link to heading</span>
  </a>
</h2>
<p>I mentioned sub-schemas and open unions earlier in this post. The
<code>app.bsky.feed.post</code> type includes a union for valid embeds. Per the AT Protocol
lexicon specification, unions are open unless explicitly marked as <code>closed</code>.</p>
<blockquote>
<p>By default unions are “open”, meaning that future revisions of the schema
could add more types to the list of refs (though can not remove types). This
means that implementations should be permissive when validating, in case they
do not have the most recent version of the Lexicon. The <code>closed</code> flag
(boolean) can indicate that the set of types is fixed and can not be extended
in the future.</p>
</blockquote>
<p>The embed union is not marked as closed.</p>
<div><pre tabindex="0"><code data-lang="json"><span><span>          <span>"embed"</span><span>:</span> {
</span></span><span><span>            <span>"type"</span>: <span>"union"</span>,
</span></span><span><span>            <span>"refs"</span>: [
</span></span><span><span>              <span>"app.bsky.embed.images"</span>,
</span></span><span><span>              <span>"app.bsky.embed.video"</span>,
</span></span><span><span>              <span>"app.bsky.embed.external"</span>,
</span></span><span><span>              <span>"app.bsky.embed.record"</span>,
</span></span><span><span>              <span>"app.bsky.embed.recordWithMedia"</span>
</span></span><span><span>            ]
</span></span><span><span>          }<span>,</span>
</span></span></code></pre></div><p>Therefore, posts can be created with an embed <code>$type</code> that is not enumerated.
For example, I could also persist the website HTML via <a href="https://bsky.app/profile/danielmangum.com/post/3lbpfxwnjoq23">making a post on
Bluesky</a> with a
custom embed.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>curl -X POST 'https://bsky.social/xrpc/com.atproto.repo.createRecord' \
</span></span><span><span>-H 'Authorization: Bearer '"$ACCESS_JWT"'' \
</span></span><span><span>-H 'Content-Type: application/json' \
</span></span><span><span>-d '{
</span></span><span><span>  "repo": "danielmangum.com",
</span></span><span><span>  "collection": "app.bsky.feed.post",
</span></span><span><span>  "record": {
</span></span><span><span>    "$type": "app.bsky.feed.post",
</span></span><span><span>    "text": "This post embeds a website.",
</span></span><span><span>    "createdAt": "2024-11-23T05:49:35.422015Z",
</span></span><span><span>    "embed": {
</span></span><span><span>      "$type": "com.danielmangum.hack.sites",
</span></span><span><span>      "sites": [
</span></span><span><span>        {
</span></span><span><span>          "site": {
</span></span><span><span>            "$type": "blob",
</span></span><span><span>            "ref": {
</span></span><span><span>              "$link": "bafkreic5fmelmhqoqxfjz2siw5ey43ixwlzg5gvv2pkkz7o25ikepv4zeq"
</span></span><span><span>            },
</span></span><span><span>            "mimeType": "text/html",
</span></span><span><span>            "size": 268
</span></span><span><span>          }
</span></span><span><span>        }
</span></span><span><span>      ]
</span></span><span><span>    }
</span></span><span><span>  }
</span></span><span><span>}'
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="fallback"><span><span>{
</span></span><span><span>  "uri": "at://did:plc:j22nebhg6aek3kt2mex5ng7e/app.bsky.feed.post/3lbpfxwnjoq23",
</span></span><span><span>  "cid": "bafyreidnlyhcvlzl5hc3btih6ly5anjld6ss4bgocyichnm72cpnjuzsvu",
</span></span><span><span>  "commit": {
</span></span><span><span>    "cid": "bafyreibv77m3bdyywmotn7ncbbrqv6pv7irzmw27bzklt4tppgsoodarma",
</span></span><span><span>    "rev": "3lbpfxwo57q23"
</span></span><span><span>  },
</span></span><span><span>  "validationStatus": "valid"
</span></span><span><span>}
</span></span></code></pre></div><p>In the Bluesky application, the embed is silently ignored.</p>



<p><img src="https://danielmangum.com/static/website_on_bsky_1.png" alt="website-on-bsky-1">
</p>
<p>However, the content is persisted and the reference is included in the post
record, so a different application could choose to start rendering the embed.</p>
<div><pre tabindex="0"><code data-lang="json"><span><span>{
</span></span><span><span>  <span>"$type"</span>: <span>"app.bsky.feed.post"</span>,
</span></span><span><span>  <span>"createdAt"</span>: <span>"2024-11-23T05:49:35.422015Z"</span>,
</span></span><span><span>  <span>"embed"</span>: {
</span></span><span><span>    <span>"$type"</span>: <span>"com.danielmangum.hack.sites"</span>,
</span></span><span><span>    <span>"sites"</span>: [
</span></span><span><span>      {
</span></span><span><span>        <span>"site"</span>: {
</span></span><span><span>          <span>"$type"</span>: <span>"blob"</span>,
</span></span><span><span>          <span>"ref"</span>: {
</span></span><span><span>            <span>"$link"</span>: <span>"bafkreic5fmelmhqoqxfjz2siw5ey43ixwlzg5gvv2pkkz7o25ikepv4zeq"</span>
</span></span><span><span>          },
</span></span><span><span>          <span>"mimeType"</span>: <span>"text/html"</span>,
</span></span><span><span>          <span>"size"</span>: <span>268</span>
</span></span><span><span>        }
</span></span><span><span>      }
</span></span><span><span>    ]
</span></span><span><span>  },
</span></span><span><span>  <span>"text"</span>: <span>"This post embeds a website."</span>
</span></span><span><span>}
</span></span></code></pre></div><p>In my opinion, this is one of the most interesting features of lexicons because
it allows for “micro-extensions” that build on existing use cases (e.g.
“microblogging”). For example, I for one would love a world in which small code
snippets could be embedded in my posts and run in a
<a href="https://webassembly.org/">WebAssembly</a> sandbox by other users. But that’s a
post for another day.</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SQLite: Outlandish Recursive Query Examples (190 pts)]]></title>
            <link>https://www.sqlite.org/lang_with.html#outlandish_recursive_query_examples</link>
            <guid>42230384</guid>
            <pubDate>Sun, 24 Nov 2024 20:42:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sqlite.org/lang_with.html#outlandish_recursive_query_examples">https://www.sqlite.org/lang_with.html#outlandish_recursive_query_examples</a>, See on <a href="https://news.ycombinator.com/item?id=42230384">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<p>
The WITH Clause
</p>
<details>
<summary>Table Of Contents</summary>

</details>
</div>




<h2 id="overview"><span>1. </span>Overview</h2>
<p><b><a href="https://www.sqlite.org/syntax/with-clause.html">with-clause:</a></b>
</p>
 


<p>Common Table Expressions or CTEs act like temporary <a href="https://www.sqlite.org/lang_createview.html">views</a> that exist
only for the duration of a single SQL statement.  There are two kinds of
common table expressions: "ordinary" and "recursive". Ordinary 
common table expressions are helpful for making
queries easier to understand by factoring
subqueries out of the main SQL statement.
Recursive common table expressions
provide the ability to do hierarchical or
recursive queries of trees and graphs, a capability
that is not otherwise available in the SQL language.

</p><p>All common table expressions (ordinary and recursive) are 
created by prepending a WITH clause in front of a <a href="https://www.sqlite.org/lang_select.html">SELECT</a>, <a href="https://www.sqlite.org/lang_insert.html">INSERT</a>, <a href="https://www.sqlite.org/lang_delete.html">DELETE</a>,
or <a href="https://www.sqlite.org/lang_update.html">UPDATE</a> statement.  A single WITH clause can specify one or more
common table expressions, some of which are ordinary and some of which
are recursive.

<a name="ordinarycte"></a>

</p><h2 id="ordinary_common_table_expressions"><span>2. </span>Ordinary Common Table Expressions</h2>

<p>An ordinary common table expression works as if it were a <a href="https://www.sqlite.org/lang_createview.html">view</a> that
exists for the duration of a single statement.  Ordinary common table
expressions are useful for factoring out subqueries and making the overall
SQL statement easier to read and understand.

</p><p>A WITH clause can contain ordinary common table expressions even if
it includes the RECURSIVE keyword.  The use of RECURSIVE does not force
common table expressions to be recursive.

<a name="recursivecte"></a>

</p><h2 id="recursive_common_table_expressions"><span>3. </span>Recursive Common Table Expressions</h2>

<p>A recursive common table expression can be used to write a query that
walks a tree or graph.  A recursive common table expression has the same
basic syntax as an ordinary common table expression, but with the following
additional attributes:

</p><ol>
<li> The "<a href="https://www.sqlite.org/syntax/select-stmt.html">select-stmt</a>" must be a <a href="https://www.sqlite.org/lang_select.html#compound">compound select</a>.  That is to say,
     the CTE body must be two or more individual SELECT statements
     separated by compound operators like UNION, UNION ALL, INTERSECT,
     or EXCEPT.
</li><li> One or more of the individual SELECT statements that make up
     the compound must be
     "recursive".  A SELECT statement is recursive if
     its FROM clause contains exactly one reference to
     the CTE table (the table named on the left-hand side of the
     AS clause).
</li><li> One or more of the SELECT statements in the compound must be
     non-recursive.
</li><li> All non-recursive SELECT statements must occur before any
     recursive SELECT statements.
</li><li> The recursive SELECT statements must be separated from the
     non-recursive SELECT statements
     and from each other by the UNION or UNION ALL operators.
     If there are two or more recursive SELECT statements, they all must
     be separated from each other using the same operator that separates
     the first recursive SELECT from the last non-recursive SELECT statement.
</li><li> Recursive SELECT statements may not use
     <a href="https://www.sqlite.org/lang_aggfunc.html">aggregate functions</a> or <a href="https://www.sqlite.org/windowfunctions.html">window functions</a>.     
</li></ol>

<p>To put it another way, a recursive common table expression must
look something like the following:

</p><p><b><a href="https://www.sqlite.org/syntax/recursive-cte.html">recursive-cte:</a></b>
</p>
 <div id="x91789db1">
 <p><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 874.291 72.36">
<circle cx="5" cy="17" r="3.6" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></circle>
<polygon points="32,17 20,21 20,12" style="fill:rgb(0,0,0)"></polygon>
<path d="M9,17L26,17" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<path d="M32,32L169,32L169,2L32,2Z" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<text x="100" y="17" text-anchor="middle" fill="rgb(0,0,0)" dominant-baseline="central">cte-table-name</text>
<polygon points="192,17 180,21 180,12" style="fill:rgb(0,0,0)"></polygon>
<path d="M169,17L186,17" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<path d="M207,32L211,32A15 15 0 0 0 226 17A15 15 0 0 0 211 2L207,2A15 15 0 0 0 192 17A15 15 0 0 0 207 32Z" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<text x="209" y="17" text-anchor="middle" fill="rgb(0,0,0)" dominant-baseline="central">AS</text>
<polygon points="249,17 237,21 237,12" style="fill:rgb(0,0,0)"></polygon>
<path d="M226,17L243,17" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<path d="M264,32A15 15 0 0 0 279 17A15 15 0 0 0 264 2A15 15 0 0 0 249 17A15 15 0 0 0 264 32Z" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<text x="264" y="17" text-anchor="middle" font-weight="bold" fill="rgb(0,0,0)" dominant-baseline="central">(</text>
<polygon points="302,17 291,21 291,12" style="fill:rgb(0,0,0)"></polygon>
<path d="M279,17L296,17" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<path d="M317,32L403,32A15 15 0 0 0 418 17A15 15 0 0 0 403 2L317,2A15 15 0 0 0 302 17A15 15 0 0 0 317 32Z" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<text x="360" y="17" text-anchor="middle" fill="rgb(0,0,0)" dominant-baseline="central">initial-select</text>
<polygon points="459,55 448,59 448,50" style="fill:rgb(0,0,0)"></polygon>
<path d="M418,17 L 425,17 Q 433,17 433,32 L 433,40 Q 433,55 443,55 L 454,55" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<path d="M474,70L518,70A15 15 0 0 0 533 55L533,55A15 15 0 0 0 518 39L474,39A15 15 0 0 0 459 55L459,55A15 15 0 0 0 474 70Z" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<text x="496" y="55" text-anchor="middle" fill="rgb(0,0,0)" dominant-baseline="central">UNION</text>
<polygon points="556,55 545,59 545,50" style="fill:rgb(0,0,0)"></polygon>
<path d="M533,55L550,55" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<path d="M571,70L586,70A15 15 0 0 0 601 55L601,55A15 15 0 0 0 586 39L571,39A15 15 0 0 0 556 55L556,55A15 15 0 0 0 571 70Z" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<text x="579" y="55" text-anchor="middle" fill="rgb(0,0,0)" dominant-baseline="central">ALL</text>
<polygon points="643,17 631,21 631,12" style="fill:rgb(0,0,0)"></polygon>
<path d="M601,55 L 609,55 Q 616,55 616,40 L 616,32 Q 616,17 627,17 L 637,17" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<path d="M658,32L773,32A15 15 0 0 0 788 17A15 15 0 0 0 773 2L658,2A15 15 0 0 0 643 17A15 15 0 0 0 658 32Z" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<text x="715" y="17" text-anchor="middle" fill="rgb(0,0,0)" dominant-baseline="central">recursive-select</text>
<polygon points="811,17 800,21 800,12" style="fill:rgb(0,0,0)"></polygon>
<path d="M788,17L805,17" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<path d="M826,32A15 15 0 0 0 841 17A15 15 0 0 0 826 2A15 15 0 0 0 811 17A15 15 0 0 0 826 32Z" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<text x="826" y="17" text-anchor="middle" font-weight="bold" fill="rgb(0,0,0)" dominant-baseline="central">)</text>
<polygon points="864,17 853,21 853,12" style="fill:rgb(0,0,0)"></polygon>
<path d="M841,17L859,17" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<circle cx="868" cy="17" r="3.6" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></circle>
<polygon points="459,17 448,21 448,12" style="fill:rgb(0,0,0)"></polygon>
<path d="M418,17L454,17" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<path d="M474,32L518,32A15 15 0 0 0 533 17A15 15 0 0 0 518 2L474,2A15 15 0 0 0 459 17A15 15 0 0 0 474 32Z" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
<text x="496" y="17" text-anchor="middle" fill="rgb(0,0,0)" dominant-baseline="central">UNION</text>
<path d="M533,17L631,17" style="fill:none;stroke-width:2.16;stroke:rgb(0,0,0);"></path>
</svg>
</p>
<p><b><a href="https://www.sqlite.org/syntax/cte-table-name.html">cte-table-name:</a></b>
</p>
 
</div>


<p>In the diagram above, <span>initial-select</span> means one or more
non-recursive SELECT statements and <span>recursive-select</span> means
one or more recursive SELECT statements.  The most common case is for there
to be exactly one <span>initial-select</span> and exactly one
<span>recursive-select</span> but more than one of each is allowed.</p>

<p>Call the table named by the <a href="https://www.sqlite.org/syntax/cte-table-name.html">cte-table-name</a> in a recursive
common table expression the "recursive table".
In the <a href="https://www.sqlite.org/syntax/recursive-cte.html">recursive-cte</a> bubble diagram above, the recursive
table must appear exactly once in the FROM clause of each
top-level SELECT statement in the <span>recursive-select</span>
and must not appear anywhere else in either the
<span>initial-select</span> or the
<span>recursive-select</span>, including subqueries.
The <span>initial-select</span> may be
a <a href="https://www.sqlite.org/lang_select.html#compound">compound select</a>, but it may not include an ORDER BY, LIMIT, or OFFSET.
The <span>recursive-select</span> may also be a <a href="https://www.sqlite.org/lang_select.html#compound">compound select</a> with
the restriction that all elements of that compound must be separated by
the same UNION or UNION ALL operator that separates
<span>initial-select</span> from <span>recursive-select</span>.
The <span>recursive-select</span> is allowed to include an
ORDER BY, LIMIT, and/or OFFSET but may not use
<a href="https://www.sqlite.org/lang_aggfunc.html">aggregate functions</a> or <a href="https://www.sqlite.org/windowfunctions.html">window functions</a>.

</p><p>The ability for the <span>recursive-select</span> to be a compound
was added in <a href="https://www.sqlite.org/releaselog/3_34_0.html">version 3.34.0</a> (2020-12-01).  In earlier versions of
SQLite, the <span>recursive-select</span> could only be a single
simple SELECT statement.</p>

<p>The basic algorithm for computing the content of the recursive table
is as follows:

</p><ol>
<li> Run the <span>initial-select</span> and add the results to a queue.
</li><li> While the queue is not empty:
<ol type="a">
<li> Extract a single row from the queue.
</li><li> Insert that single row into the recursive table
</li><li> Pretend that the single row just extracted is the only
     row in the recursive table and run the recursive-select,
     adding all results to the queue.
</li></ol>
</li></ol>

<p>The basic procedure above may modified by the following additional rules:

</p><ul>
<li><p>
  If a UNION operator connects the <span>initial-select</span> with the
  <span>recursive-select</span>, then only add rows to the queue if 
  no identical row has
  been previously added to the queue.  Repeated rows are discarded before being
  added to the queue even if the repeated rows have already been extracted
  from the queue by the recursion step.  If the operator is UNION ALL,
  then all rows generated by both the <span>initial-select</span> and the
  <span>recursive-select</span> are always added to the queue even if
  they are repeats.
  When determining if a row is repeated, NULL values compare
  equal to one another and not equal to any other value.
</p></li><li><p>
  The LIMIT clause, if present, determines the maximum number of rows that
  will ever be added to the recursive table in step 2b.
  Once the limit is reached, the recursion stops.
  A limit of zero means that no rows are ever added to the
  recursive table, and a negative limit means an unlimited number of rows
  may be added to the recursive table.
</p></li><li><p>
  The OFFSET clause, if it is present and has a positive value N, prevents the
  first N rows from being added to the recursive table.
  The first N rows are still processed
  by the <span>recursive-select</span> — they
  just are not added to the recursive table.  Rows are not counted toward
  fulfilling the LIMIT until all OFFSET rows have been skipped.
</p></li><li><p>
  If an ORDER BY clause is present, it determines the order in which rows
  are extracted from the queue in step 2a.  If there is no ORDER BY clause,
  then the order in which rows are extracted is undefined.  (In the current
  implementation, the queue becomes a FIFO if the ORDER BY clause is omitted,
  but applications should not depend on that fact since it might change.)
</p></li></ul>

<h2 id="recursive_query_examples"><span>3.1. </span>Recursive Query Examples</h2>

<p>The following query returns all integers between 1 and 1000000:

</p><blockquote><pre>WITH RECURSIVE
  cnt(x) AS (VALUES(1) UNION ALL SELECT x+1 FROM cnt WHERE x&lt;1000000)
SELECT x FROM cnt;
</pre></blockquote>

<p>Consider how this query works.  The initial-select
runs first and returns a single row
with a single column "1".  This one row is added to the queue.  In
step 2a, that one row is extracted from the queue and added to "cnt".
Then the recursive-select is run in accordance with step 2c generating
a single new row with value "2" to add to the queue.  The queue still
has one row, so step 2 repeats.  The "2" row is extracted and added to the
recursive table by steps 2a and 2b.  Then the row containing 2 is used 
as if it were the complete content of the recursive table and the 
recursive-select is run again, resulting in a row with value "3" being added
to the queue.  This repeats 999999 times until finally at step 2a the
only value on the queue is a row containing 1000000.  That row is
extracted and added to the recursive table.  But this time, the
WHERE clause causes the recursive-select to return no rows, so the
queue remains empty and the recursion stops.

</p><p><b>Optimization note:</b>
In the discussion above, statements like "insert the row into
the recursive table" should be understood conceptually, not literally.
It sounds as if SQLite is accumulating a huge table
containing one million rows, then going back and scanning that table
from top to bottom to generate the result.  What really happens
is that the query optimizer sees that values in the
"cnt" recursive table are only used once.  So as each row is added to
the recursive table, that row is immediately returned as a result of the main
SELECT statement and then discarded.  SQLite does <em>not</em> accumulate
a temporary table containing a million rows.  Very little memory is
needed to run the above example.  However, if the example had used
UNION instead of UNION ALL, then SQLite would have had to keep around
all previously generated content in order to check for duplicates.
For this reason, programmers should strive to use UNION ALL instead
of UNION when feasible.

</p><p>Here is a variation on the previous example:

</p><blockquote><pre>WITH RECURSIVE
  cnt(x) AS (
     SELECT 1
     UNION ALL
     SELECT x+1 FROM cnt
      LIMIT 1000000
  )
SELECT x FROM cnt;
</pre></blockquote>

<p>There are two differences in this variation.  The initial-select is
"SELECT 1" instead of "VALUES(1)".  But those are just different
syntaxes for saying exactly the same thing.  The other change is that the
recursion is stopped by a LIMIT rather than a WHERE clause.  The use of
LIMIT means that when the one-millionth row is added to the "cnt" table
(and returned by the main SELECT, thanks to the query optimizer)
then the recursion stops immediately regardless of how many rows might be
left in the queue.  On more complex queries, it can sometimes be
difficult to ensure that the WHERE clause will eventually cause the
queue to drain and the recursion to terminate.  But the LIMIT clause will
always stop the recursion.  So it is good practice to always include a
LIMIT clause as a safety if an upper bound on the size of the recursion 
is known.

<a name="rcex2"></a>

</p><h2 id="hierarchical_query_examples"><span>3.2. </span>Hierarchical Query Examples</h2>

<p>Consider a table that describes the members of an organization as
well as the chain-of-command within that organization:

</p><blockquote><pre>CREATE TABLE org(
  name TEXT PRIMARY KEY,
  boss TEXT REFERENCES org,
  height INT,
  -- other content omitted
);
</pre></blockquote>

<p>Every member in the organization has a name, and most members have
a single boss.  (The head of the whole organization has a NULL
"boss" field.) The rows of the "org" table form a tree.

</p><p>Here is a query that computes the average height over everyone
in Alice's organization, including Alice:

</p><blockquote><pre>WITH RECURSIVE
  works_for_alice(n) AS (
    VALUES('Alice')
    UNION
    SELECT name FROM org, works_for_alice
     WHERE org.boss=works_for_alice.n
  )
SELECT avg(height) FROM org
 WHERE org.name IN works_for_alice;
</pre></blockquote>

<p>The next example uses two 
common table expressions in a single WITH clause.  
The following table records a family tree:

</p><blockquote><pre>CREATE TABLE family(
  name TEXT PRIMARY KEY,
  mom TEXT REFERENCES family,
  dad TEXT REFERENCES family,
  born DATETIME,
  died DATETIME -- NULL if still alive
  -- other content
);
</pre></blockquote>

<p>The "family" table is similar to the earlier "org" table except that 
now there are two parents to each member.
We want to know all living ancestors of Alice, from oldest to youngest.
An ordinary common table expression, "parent_of", is defined first.  That
ordinary CTE is a view that can be used to find all parents of any
individual.  That ordinary CTE is then used in the "ancestor_of_alice"
recursive CTE.  The recursive CTE is then used in the final query:

</p><blockquote><pre>WITH RECURSIVE
  parent_of(name, parent) AS
    (SELECT name, mom FROM family UNION SELECT name, dad FROM family),
  ancestor_of_alice(name) AS
    (SELECT parent FROM parent_of WHERE name='Alice'
     UNION ALL
     SELECT parent FROM parent_of JOIN ancestor_of_alice USING(name))
SELECT family.name FROM ancestor_of_alice, family
 WHERE ancestor_of_alice.name=family.name
   AND died IS NULL
 ORDER BY born;
</pre></blockquote>

<h2 id="queries_against_a_graph"><span>3.3. </span>Queries Against A Graph</h2>

<p>Suppose you have an undirected graph where each node is
identified by an integer and edges are defined by a table like
this:

</p><blockquote><pre>CREATE TABLE edge(aa INT, bb INT);
CREATE INDEX edge_aa ON edge(aa);
CREATE INDEX edge_bb ON edge(bb);
</pre></blockquote>

<p>The indexes are not required, but they do help performance
for large graphs.
To find all nodes of the graph that are connected to
node 59, use a query similar to the following:

</p><blockquote><pre>WITH RECURSIVE nodes(x) AS (
   SELECT 59
   UNION
   SELECT aa FROM edge JOIN nodes ON bb=x
   UNION
   SELECT bb FROM edge JOIN nodes ON aa=x
)
SELECT x FROM nodes;
</pre></blockquote>

<p>
The <span>initial-select</span> in this case is the simple query
"SELECT 59".  This establishes the base case.  The
<span>recursive-select</span> consists of the other two
SELECT statements.  The first recursive SELECT follows edges
in the bb-to-aa direction and the second recursive SELECT follows
edges in the aa-to-bb direction.  UNION is used instead of
UNION ALL to prevent the recursion from entering an infinite
loop if the graph contains cycles.
</p>

<p>Here is a real-world example of using a graph query against a
directed graph:
A version control system (VCS) will typically store the evolving
versions of a project as a directed acyclic graph (DAG).  Call each
version of the project a "checkin".  A single
checkin can have zero or more parents.  Most checkins (except the
first) have a single parent, but in the case of a merge, a checkin
might have two or three or more parents.  A schema to keep track of
checkins and the order in which they occur might look something like
this:

</p><blockquote><pre>CREATE TABLE checkin(
  id INTEGER PRIMARY KEY,
  mtime INTEGER -- timestamp when this checkin occurred
);
CREATE TABLE derivedfrom(
  xfrom INTEGER NOT NULL REFERENCES checkin, -- parent checkin
  xto INTEGER NOT NULL REFERENCES checkin,   -- derived checkin
  PRIMARY KEY(xfrom,xto)
);
CREATE INDEX derivedfrom_back ON derivedfrom(xto,xfrom);
</pre></blockquote>

<p>This graph is acyclic.  And we assume that the mtime of every
child checkin is no less than the mtime of all its parents.  But
unlike the earlier examples, this graph might have multiple paths of
differing lengths between any two checkins.

</p><p>We want to know the twenty most recent ancestors in time (out of
the thousands and thousands of ancestors in the whole DAG) for
checkin "@BASELINE".  (A query similar to this is used
by the <a href="http://www.fossil-scm.org/">Fossil</a> VCS to
show the N most recent ancestors of a checkin.  For example:
<a href="https://www.sqlite.org/src/timeline?p=trunk&amp;n=30">https://www.sqlite.org/src/timeline?p=trunk&amp;n=30</a>.)

</p><blockquote><pre>WITH RECURSIVE
  ancestor(id,mtime) AS (
    SELECT id, mtime FROM checkin WHERE id=@BASELINE
    UNION
    SELECT derivedfrom.xfrom, checkin.mtime
      FROM ancestor, derivedfrom, checkin
     WHERE ancestor.id=derivedfrom.xto
       AND checkin.id=derivedfrom.xfrom
     ORDER BY checkin.mtime DESC
     LIMIT 20
  )
SELECT * FROM checkin JOIN ancestor USING(id);
</pre></blockquote>

<p>
The "ORDER BY checkin.mtime DESC" term in the recursive-select makes
the query run much faster by preventing it from following
branches that merge checkins
from long ago.  The ORDER BY forces the recursive-select to focus
on the most recent checkins, the ones we want.  Without the ORDER BY
on the recursive-select, one would be forced to compute the complete set of
thousands of ancestors, sort them all by mtime, then take the top twenty.
The ORDER BY essentially sets up a priority queue that
forces the recursive query to look at the most recent ancestors first,
allowing the use of a LIMIT clause to restrict the scope of the
query to just the checkins of interest.

<a name="withorderby"></a>

</p><h2 id="controlling_depth_first_versus_breadth_first_search_of_a_tree_using_order_by"><span>3.4. </span>Controlling Depth-First Versus Breadth-First Search Of a Tree
Using ORDER BY</h2>

<p>An ORDER BY clause on the recursive-select can be used to control
whether the search of a tree is depth-first or breadth-first.  To
illustrate, we will use a variation on the "org" table from an example
above, without the "height" column, and with some real data inserted:

</p><blockquote><pre>CREATE TABLE org(
  name TEXT PRIMARY KEY,
  boss TEXT REFERENCES org
) WITHOUT ROWID;
INSERT INTO org VALUES('Alice',NULL);
INSERT INTO org VALUES('Bob','Alice');
INSERT INTO org VALUES('Cindy','Alice');
INSERT INTO org VALUES('Dave','Bob');
INSERT INTO org VALUES('Emma','Bob');
INSERT INTO org VALUES('Fred','Cindy');
INSERT INTO org VALUES('Gail','Cindy');
</pre></blockquote>

<p>Here is a query to show the tree structure in a breadth-first pattern:

</p><blockquote><pre>WITH RECURSIVE
  under_alice(name,level) AS (
    VALUES('Alice',0)
    UNION ALL
    SELECT org.name, under_alice.level+1
      FROM org JOIN under_alice ON org.boss=under_alice.name
     ORDER BY 2
  )
SELECT substr('..........',1,level*3) || name FROM under_alice;
</pre></blockquote>

<p>The "ORDER BY 2" (which means the same as "ORDER BY under_alice.level+1")
causes higher levels in the organization chart (with smaller "level" values)
to be processed first, resulting in a breadth-first search.  The output is:

</p><blockquote><pre>Alice
...Bob
...Cindy
......Dave
......Emma
......Fred
......Gail
</pre></blockquote>

<p>But if we change the ORDER BY clause to add the "DESC" modifier, that will
cause lower levels in the organization (with larger "level" values) to be
processed first by the recursive-select, resulting in a depth-first search:

</p><blockquote><pre>WITH RECURSIVE
  under_alice(name,level) AS (
    VALUES('Alice',0)
    UNION ALL
    SELECT org.name, under_alice.level+1
      FROM org JOIN under_alice ON org.boss=under_alice.name
     ORDER BY 2 <b>DESC</b>
  )
SELECT substr('..........',1,level*3) || name FROM under_alice;
</pre></blockquote>

<p>The output of this revised query is:

</p><blockquote><pre>Alice
...Bob
......Dave
......Emma
...Cindy
......Fred
......Gail
</pre></blockquote>

<p>When the ORDER BY clause is omitted from the recursive-select, the
queue behaves as a FIFO, which results in a breadth-first search.


<a name="mandelbrot"></a>

</p><h2 id="outlandish_recursive_query_examples"><span>3.5. </span>Outlandish Recursive Query Examples</h2>

<p>The following query computes an approximation of the Mandelbrot Set
and outputs the result as ASCII-art:

</p><blockquote><pre>WITH RECURSIVE
  xaxis(x) AS (VALUES(-2.0) UNION ALL SELECT x+0.05 FROM xaxis WHERE x&lt;1.2),
  yaxis(y) AS (VALUES(-1.0) UNION ALL SELECT y+0.1 FROM yaxis WHERE y&lt;1.0),
  m(iter, cx, cy, x, y) AS (
    SELECT 0, x, y, 0.0, 0.0 FROM xaxis, yaxis
    UNION ALL
    SELECT iter+1, cx, cy, x*x-y*y + cx, 2.0*x*y + cy FROM m 
     WHERE (x*x + y*y) &lt; 4.0 AND iter&lt;28
  ),
  m2(iter, cx, cy) AS (
    SELECT max(iter), cx, cy FROM m GROUP BY cx, cy
  ),
  a(t) AS (
    SELECT group_concat( substr(' .+*#', 1+min(iter/7,4), 1), '') 
    FROM m2 GROUP BY cy
  )
SELECT group_concat(rtrim(t),x'0a') FROM a;
</pre></blockquote>

<p>In this query, the "xaxis" and "yaxis" CTEs define the grid of points for
which the Mandelbrot Set will be approximated.  Each row in the
"m(iter,cx,cy,x,y)" CTE means that after "iter" iterations, the Mandelbrot
iteration starting at cx,cy has reached point x,y.  The number of iterations
in this example is limited to 28 (which severely limits the resolution of
the computation, but is sufficient for low-resolution ASCII-art output).
The "m2(iter,cx,cy)" CTE holds the maximum number of iterations reached when
starting at point cx,cy.
Finally, each row in the "a(t)" CTE holds a string 
which is a single line of the output ASCII-art.
The SELECT statement at the end just queries the "a" CTE to
retrieve all lines of ASCII-art, one by one.

</p><p>Running the query above in an SQLite <a href="https://www.sqlite.org/cli.html">command-line shell</a> results
in the following output:

</p><blockquote><pre>                                    ....#
                                   ..#*..
                                 ..+####+.
                            .......+####....   +
                           ..##+*##########+.++++
                          .+.##################+.
              .............+###################+.+
              ..++..#.....*#####################+.
             ...+#######++#######################.
          ....+*################################.
 #############################################...
          ....+*################################.
             ...+#######++#######################.
              ..++..#.....*#####################+.
              .............+###################+.+
                          .+.##################+.
                           ..##+*##########+.++++
                            .......+####....   +
                                 ..+####+.
                                   ..#*..
                                    ....#
                                    +.
</pre></blockquote>

<p>This next query solves a Sudoku puzzle.  The state of the puzzle is
defined by an 81-character string formed by reading entries from the
puzzle box row by row from left to right and then from top to bottom.
Blank squares in the puzzle are denoted by a "." character.  
Thus the input string:

</p><blockquote>
53..7....6..195....98....6.8...6...34..8.3..17...2...6.6....28....419..5....8..79
</blockquote>

<p>Corresponds to a puzzle like this:

</p><blockquote>
<table>
<tbody><tr><td>5</td><td>3</td><td> </td><td> </td><td>7</td><td> </td><td> </td><td> </td><td>
</td></tr><tr><td>6</td><td> </td><td> </td><td>1</td><td>9</td><td>5</td><td> </td><td> </td><td>
</td></tr><tr><td> </td><td>9</td><td>8</td><td> </td><td> </td><td> </td><td> </td><td>6</td><td>
</td></tr><tr><td>8</td><td> </td><td> </td><td> </td><td>6</td><td> </td><td> </td><td> </td><td>3
</td></tr><tr><td>4</td><td> </td><td> </td><td>8</td><td> </td><td>3</td><td> </td><td> </td><td>1
</td></tr><tr><td>7</td><td> </td><td> </td><td> </td><td>2</td><td> </td><td> </td><td> </td><td>6
</td></tr><tr><td> </td><td>6</td><td> </td><td> </td><td> </td><td> </td><td>2</td><td>8</td><td>
</td></tr><tr><td> </td><td> </td><td> </td><td>4</td><td>1</td><td>9</td><td> </td><td> </td><td>5
</td></tr><tr><td> </td><td> </td><td> </td><td> </td><td>8</td><td> </td><td> </td><td>7</td><td>9
</td></tr></tbody></table>
</blockquote>

<p>This is the query that solves the puzzle:

</p><blockquote><pre>WITH RECURSIVE
  input(sud) AS (
    VALUES('53..7....6..195....98....6.8...6...34..8.3..17...2...6.6....28....419..5....8..79')
  ),
  digits(z, lp) AS (
    VALUES('1', 1)
    UNION ALL SELECT
    CAST(lp+1 AS TEXT), lp+1 FROM digits WHERE lp&lt;9
  ),
  x(s, ind) AS (
    SELECT sud, instr(sud, '.') FROM input
    UNION ALL
    SELECT
      substr(s, 1, ind-1) || z || substr(s, ind+1),
      instr( substr(s, 1, ind-1) || z || substr(s, ind+1), '.' )
     FROM x, digits AS z
    WHERE ind&gt;0
      AND NOT EXISTS (
            SELECT 1
              FROM digits AS lp
             WHERE z.z = substr(s, ((ind-1)/9)*9 + lp, 1)
                OR z.z = substr(s, ((ind-1)%9) + (lp-1)*9 + 1, 1)
                OR z.z = substr(s, (((ind-1)/3) % 3) * 3
                        + ((ind-1)/27) * 27 + lp
                        + ((lp-1) / 3) * 6, 1)
         )
  )
SELECT s FROM x WHERE ind=0;
</pre></blockquote>

<p>The "input" CTE defines the input puzzle.
The "digits" CTE defines a table that holds all digits between 1 and 9.
The work of solving the puzzle is undertaken by the "x" CTE.
An entry in x(s,ind) means that the 81-character string "s" is a valid
sudoku puzzle (it has no conflicts) and that the first unknown character
is at position "ind", or ind==0 if all character positions are filled in.
The goal, then, is to compute entries for "x" with an "ind" of 0.

</p><p>The solver works by adding new entries to the "x" recursive table.
Given prior entries, the recursive-select tries to fill in a single new
position with all values between 1 and 9 that actually work in that
position.  The complicated "NOT EXISTS" subquery is the magic that
figures out whether or not each candidate "s" string is a valid
sudoku puzzle or not.

</p><p>The final answer is found by looking for a string with ind==0.
If the original sudoku problem did not have a unique solution, then
the query will return all possible solutions.  If the original problem
was unsolvable, then no rows will be returned.  In this case, the unique
answer is:

</p><blockquote>
534678912672195348198342567859761423426853791713924856961537284287419635345286179
</blockquote>

<p>The solution was computed in less than 300 milliseconds on a modern
workstation.

<a name="mathint"></a>

</p><h2 id="materialization_hints"><span>4. </span>Materialization Hints</h2>

<p>
The "AS MATERIALIZED" and "AS NOT MATERIALIZED" forms of a common table expression
are non-standard SQL syntax copied from PostgreSQL.  Using MATERIALIZED or
NOT MATERIALIZED after the AS keyword provides non-binding hints to the query
planner about how the CTE should be implemented.

</p><p>
If the MATERIALIZED phrase is used, then <span>select-stmt</span> will
be materialized into an ephemeral table that is held in memory or in a
temporary disk file.  That ephemeral table will then be used in place of the
CTE table name whenever the CTE table name appears in the subsequent SQL.
Because the <span>select-stmt</span> is evaluated immediately,
the opportunity to apply optimizations such as
<a href="https://www.sqlite.org/optoverview.html#flattening">query flattening</a> or the <a href="https://www.sqlite.org/optoverview.html#pushdown">push-down optimization</a>, is lost.
This loss of optimization is a feature, not a bug.  Developers are able
to use the MATERIALIZED keyword as an "optimization fence" to more tightly
control the behavior of the SQLite query planner.  SQLite copied the idea of
using MATERIALIZED as an optimization fence from PostgreSQL.

</p><p>
If the NOT MATERIALIZED phrase is used, then <span>select-stmt</span>
is substituted as a subquery in place of every occurrence of the CTE
table name.  Optimizations such as <a href="https://www.sqlite.org/optoverview.html#flattening">flattening</a> and
<a href="https://www.sqlite.org/optoverview.html#pushdown">push-down</a> are then applied to the subquery as if
the subquery had by used in directly.  In spite of its name, the NOT MATERIALIZED
phrase does not prohibit the use of materialization.  The query planner
is still free to implement the subquery using materialization if
it feels that is the best solution.  The true meaning of NOT MATERIALIZED
is closer to "TREAT LIKE ANY ORDINARY VIEW OR SUBQUERY".  

</p><p>
If neither hint is present, then SQLite is free to choose whatever
implementation strategy it thinks will work best.  This is the recommended
approach.  <i>Do not use the MATERIALIZED or NOT MATERIALIZED keywords on
a common table expression unless you have a compelling reason to do so.</i>

</p><p>
The MATERIALIZED and NOT MATERIALIZED hints are only available in
SQLite version 3.35.0 (2021-03-12) and later.

</p><h2 id="limitations_and_caveats"><span>5. </span>Limitations And Caveats</h2>

<ul>
<li><p>
The WITH clause cannot be used within a <a href="https://www.sqlite.org/lang_createtrigger.html">CREATE TRIGGER</a>.
</p></li><li><p>
The WITH clause must appear at the beginning of a top-level <a href="https://www.sqlite.org/lang_select.html">SELECT</a> statement
or at the beginning of a subquery.  The WITH clause cannot be prepended to
the second or subsequent SELECT statement of a <a href="https://www.sqlite.org/lang_select.html#compound">compound select</a>.
</p></li><li><p>
The SQL:1999 spec requires that the RECURSIVE keyword follow WITH in any
WITH clause that includes a recursive common table expression.  However, for
compatibility with SqlServer and Oracle, SQLite does not enforce this rule.
</p></li></ul>
<p><small><i>This page last modified on  <a href="https://sqlite.org/docsrc/honeypot" id="mtimelink" data-href="https://sqlite.org/docsrc/finfo/pages/lang_with.in?m=5365d4ff94">2024-01-29 11:00:27</a> UTC </i></small></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pushing AMD's Infinity Fabric to Its Limit (177 pts)]]></title>
            <link>https://chipsandcheese.com/p/pushing-amds-infinity-fabric-to-its</link>
            <guid>42230355</guid>
            <pubDate>Sun, 24 Nov 2024 20:39:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/p/pushing-amds-infinity-fabric-to-its">https://chipsandcheese.com/p/pushing-amds-infinity-fabric-to-its</a>, See on <a href="https://news.ycombinator.com/item?id=42230355">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>I recently wrote code to test memory latency under load, seeking to reproduce data in various presentations with bandwidth on the X axis and latency on the Y axis. Ampere pretty much described how that was done during their Hot Chips 2024 presentation. To achieve the same results in a semi-automated fashion, I run a latency test thread while also running a variable number of threads that generate bandwidth load.</p><blockquote><p>We run a single latency sensitive application and gradually add bandwidth hungry applications.</p><p>Matthew Erler, during the AmpereOne presentation at Hot Chips 2024</p></blockquote><p>Getting good data from the test required more care than I anticipated. Some AMD chips were particularly sensitive to thread placement. Certain core affinity choices would result in dramatic latency spikes, while others showed latency being very well controlled even with similar or higher achieved bandwidth. I worked around this issue when I wrote the Broadwell article. But now, it’s time for a more detailed dig into AMD’s system topology and the various bottlenecks it may present.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6dad193f-4091-413f-9d9c-35c723e9b107_2239x1267.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6dad193f-4091-413f-9d9c-35c723e9b107_2239x1267.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6dad193f-4091-413f-9d9c-35c723e9b107_2239x1267.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6dad193f-4091-413f-9d9c-35c723e9b107_2239x1267.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6dad193f-4091-413f-9d9c-35c723e9b107_2239x1267.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6dad193f-4091-413f-9d9c-35c723e9b107_2239x1267.png" width="1456" height="824" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6dad193f-4091-413f-9d9c-35c723e9b107_2239x1267.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:824,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1045915,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6dad193f-4091-413f-9d9c-35c723e9b107_2239x1267.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6dad193f-4091-413f-9d9c-35c723e9b107_2239x1267.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6dad193f-4091-413f-9d9c-35c723e9b107_2239x1267.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6dad193f-4091-413f-9d9c-35c723e9b107_2239x1267.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Loaded latency graph during Ampere’s presentation at Hot Chips 2024</figcaption></figure></div><p>AMD chips since Zen have used multiple levels of interconnects to create a modular system, letting AMD hit high core counts quickly and cheaply. Several Zen cores share a L3 cache within a cluster, called a Core Complex (CCX). CCX-es access the rest of the system through AMD’s Infinity Fabric, a flexible interconnect that lets AMD adapt system topology to their needs. Since Zen 2, that meant putting CPU cores on Core Complex Dies (CCDs). CCDs connect to a separate IO die, which talks to system memory and slower components like PCIe, SATA, and USB. That creates a hub and spoke model, and let AMD push core counts higher than Intel.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59e4812e-bfdf-4511-b353-ed07f76d17a1_688x544.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59e4812e-bfdf-4511-b353-ed07f76d17a1_688x544.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59e4812e-bfdf-4511-b353-ed07f76d17a1_688x544.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59e4812e-bfdf-4511-b353-ed07f76d17a1_688x544.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59e4812e-bfdf-4511-b353-ed07f76d17a1_688x544.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59e4812e-bfdf-4511-b353-ed07f76d17a1_688x544.png" width="688" height="544" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/59e4812e-bfdf-4511-b353-ed07f76d17a1_688x544.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:544,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59e4812e-bfdf-4511-b353-ed07f76d17a1_688x544.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59e4812e-bfdf-4511-b353-ed07f76d17a1_688x544.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59e4812e-bfdf-4511-b353-ed07f76d17a1_688x544.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59e4812e-bfdf-4511-b353-ed07f76d17a1_688x544.png 1456w" sizes="100vw"></picture></div></a><figcaption>IFOP link from AMD’s Zen 2 platform (Matisse/Rome), from an ISSCC 2020 paper</figcaption></figure></div><p>CCDs connect to the IO die using an Infinity Fabric On-Package (IFOP) interface. A CCD’s IFOP link provides 32 bytes per cycle of read bandwidth and 16 bytes per cycle of write bandwidth, at the Infinity Fabric clock (FCLK). FCLK is typically far lower than L3 and core clocks. In later Zen systems with faster DDR5, one IFOP may not have enough bandwidth to saturate DDR5 bandwidth. Past that potential bandwidth limit, DDR memory has not been able to provide enough bandwidth to handle what all cores can demand in a high core count system.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a518cf-e8c2-49b0-b773-676d40524707_688x477.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a518cf-e8c2-49b0-b773-676d40524707_688x477.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a518cf-e8c2-49b0-b773-676d40524707_688x477.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a518cf-e8c2-49b0-b773-676d40524707_688x477.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a518cf-e8c2-49b0-b773-676d40524707_688x477.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a518cf-e8c2-49b0-b773-676d40524707_688x477.png" width="688" height="477" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/10a518cf-e8c2-49b0-b773-676d40524707_688x477.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:477,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a518cf-e8c2-49b0-b773-676d40524707_688x477.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a518cf-e8c2-49b0-b773-676d40524707_688x477.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a518cf-e8c2-49b0-b773-676d40524707_688x477.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a518cf-e8c2-49b0-b773-676d40524707_688x477.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>Of course, there can be other points of contention too. On Zen 2, multiple CCX-es can contend for one IFOP interface. Here, I’ll look at how pushing bandwidth limits at multiple points affects a latency sensitive thread contending for the same shared resource. Instead of presenting data with latency on one axis and bandwidth on another, I’m going to plot latency and bandwidth as two separate series to show how latency is affected by core count.</p><p>Zen 4 is AMD’s outgoing CPU generation, and makes for a convenient testing platform because it’s my daily driver desktop. As a recent member of the Zen line, it has one octa-core CCX per CCD. A single Zen 4 core can read from a 3 GB array at about 50 GB/s, so it can guzzle an incredible amount of memory bandwidth compared to prior Zen generations. That should make any bottlenecks easy to see. I’m using a typical DDR5 configuration with moderately spec-ed DDR5-5600.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b432db2-eb50-4454-9f30-a23fa0834d54_1743x394.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b432db2-eb50-4454-9f30-a23fa0834d54_1743x394.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b432db2-eb50-4454-9f30-a23fa0834d54_1743x394.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b432db2-eb50-4454-9f30-a23fa0834d54_1743x394.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b432db2-eb50-4454-9f30-a23fa0834d54_1743x394.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b432db2-eb50-4454-9f30-a23fa0834d54_1743x394.png" width="1456" height="329" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5b432db2-eb50-4454-9f30-a23fa0834d54_1743x394.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:329,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:75663,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b432db2-eb50-4454-9f30-a23fa0834d54_1743x394.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b432db2-eb50-4454-9f30-a23fa0834d54_1743x394.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b432db2-eb50-4454-9f30-a23fa0834d54_1743x394.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b432db2-eb50-4454-9f30-a23fa0834d54_1743x394.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Under minimal load, my system has 82-83 ns of DRAM latency. A latency test thread quickly sees worse latency as bandwidth demand from other cores starts filling up queues throughout the memory subsystem. Just a couple bandwidth test threads are enough to push the CCD’s memory subsystem to its limits.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5fecc6-679b-4983-be0e-347869a45700_1231x683.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5fecc6-679b-4983-be0e-347869a45700_1231x683.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5fecc6-679b-4983-be0e-347869a45700_1231x683.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5fecc6-679b-4983-be0e-347869a45700_1231x683.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5fecc6-679b-4983-be0e-347869a45700_1231x683.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5fecc6-679b-4983-be0e-347869a45700_1231x683.png" width="1231" height="683" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0e5fecc6-679b-4983-be0e-347869a45700_1231x683.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:683,&quot;width&quot;:1231,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:151846,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5fecc6-679b-4983-be0e-347869a45700_1231x683.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5fecc6-679b-4983-be0e-347869a45700_1231x683.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5fecc6-679b-4983-be0e-347869a45700_1231x683.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5fecc6-679b-4983-be0e-347869a45700_1231x683.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Up to 7 bandwidth test threads and the latency test thread, all pinned to the non-VCache CCD</figcaption></figure></div><p>Increasing thread count makes latency skyrocket, probably as more cores cause more contention for queue capacity. Latency shoots past 400 ns when fighting with five bandwidth threads.</p><p>Shoving the bandwidth load to the other CCD dramatically improves latency. I see a weird latency spike when CCD0 has one core demanding bandwidth, and CCD1 is running the latency test. Loading more cores on CCD0 curiously brings latency down, even as achieved bandwidth inches up. I wonder if AMD is detecting active core count, and starts reserving queue entries or otherwise throttles per-core bandwidth consumption if enough cores require high bandwidth.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd03d6c1e-536d-4d12-a0b9-dfdb2e963379_1334x677.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd03d6c1e-536d-4d12-a0b9-dfdb2e963379_1334x677.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd03d6c1e-536d-4d12-a0b9-dfdb2e963379_1334x677.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd03d6c1e-536d-4d12-a0b9-dfdb2e963379_1334x677.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd03d6c1e-536d-4d12-a0b9-dfdb2e963379_1334x677.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd03d6c1e-536d-4d12-a0b9-dfdb2e963379_1334x677.png" width="1334" height="677" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d03d6c1e-536d-4d12-a0b9-dfdb2e963379_1334x677.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:677,&quot;width&quot;:1334,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:185630,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd03d6c1e-536d-4d12-a0b9-dfdb2e963379_1334x677.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd03d6c1e-536d-4d12-a0b9-dfdb2e963379_1334x677.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd03d6c1e-536d-4d12-a0b9-dfdb2e963379_1334x677.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd03d6c1e-536d-4d12-a0b9-dfdb2e963379_1334x677.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Bandwidth improves alongside latency. In fact, the CCD running eight bandwidth test threads achieves nearly 64 GB/s. AMD seems to get excellent bandwidth efficiency out of the IFOP interface when the bandwidth test threads aren’t fighting with the latency thread. Taken together, those two observations suggest AMD’s dual CCD setup can act as a QoS mechanism of sorts. Containing bandwidth hungry code within one CCD can let latency sensitive code on the other CCD to proceed with minimal impact.</p><p>To test the whole system, I switched up core loading order to alternate between CCDs when adding bandwidth test threads. That lets me use both IFOP links, hopefully maximizing memory bandwidth. Achieved bandwidth of course is higher, and latency remains well under control with a couple of bandwidth test threads in play. I also achieve maximum bandwidth with one bandwidth test thread running on each CCD.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe2c59dc-4e07-4c7c-a335-4c1615e21662_1226x668.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe2c59dc-4e07-4c7c-a335-4c1615e21662_1226x668.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe2c59dc-4e07-4c7c-a335-4c1615e21662_1226x668.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe2c59dc-4e07-4c7c-a335-4c1615e21662_1226x668.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe2c59dc-4e07-4c7c-a335-4c1615e21662_1226x668.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe2c59dc-4e07-4c7c-a335-4c1615e21662_1226x668.png" width="1226" height="668" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fe2c59dc-4e07-4c7c-a335-4c1615e21662_1226x668.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:668,&quot;width&quot;:1226,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:161110,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe2c59dc-4e07-4c7c-a335-4c1615e21662_1226x668.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe2c59dc-4e07-4c7c-a335-4c1615e21662_1226x668.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe2c59dc-4e07-4c7c-a335-4c1615e21662_1226x668.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe2c59dc-4e07-4c7c-a335-4c1615e21662_1226x668.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>But the situation rapidly gets out of control as I spawn more bandwidth test threads. We’re probably looking at contention at both the CCD and memory controller level. Latency delays at both layers seems to be additive, and the latency test thread really gets the worst of it when it has to fight with more than 10 bandwidth hungry threads.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F163b3770-ade9-4c4b-81db-5102af0a055c_1452x483.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F163b3770-ade9-4c4b-81db-5102af0a055c_1452x483.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F163b3770-ade9-4c4b-81db-5102af0a055c_1452x483.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F163b3770-ade9-4c4b-81db-5102af0a055c_1452x483.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F163b3770-ade9-4c4b-81db-5102af0a055c_1452x483.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F163b3770-ade9-4c4b-81db-5102af0a055c_1452x483.png" width="1452" height="483" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/163b3770-ade9-4c4b-81db-5102af0a055c_1452x483.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:483,&quot;width&quot;:1452,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:134611,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F163b3770-ade9-4c4b-81db-5102af0a055c_1452x483.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F163b3770-ade9-4c4b-81db-5102af0a055c_1452x483.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F163b3770-ade9-4c4b-81db-5102af0a055c_1452x483.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F163b3770-ade9-4c4b-81db-5102af0a055c_1452x483.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>At this point, the system also started behaving strangely. For example, bringing up the “Details” tab in Task manager took an agonizingly long time, even though my test only loaded up one thread per physical core. Thankfully, I think it’s a rather extreme and non-typical workload.</p><p>Observing latency from software is simple, but I can get more information by asking the hardware what’s going on. Zen 4’s L3 cache has performance monitoring facilities. One of its capabilities is randomly sampling L3 misses and tracking their latency.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c97319-773e-4350-8d92-e2bf698def9c_1272x835.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c97319-773e-4350-8d92-e2bf698def9c_1272x835.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c97319-773e-4350-8d92-e2bf698def9c_1272x835.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c97319-773e-4350-8d92-e2bf698def9c_1272x835.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c97319-773e-4350-8d92-e2bf698def9c_1272x835.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c97319-773e-4350-8d92-e2bf698def9c_1272x835.png" width="1272" height="835" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d3c97319-773e-4350-8d92-e2bf698def9c_1272x835.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:835,&quot;width&quot;:1272,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:548561,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c97319-773e-4350-8d92-e2bf698def9c_1272x835.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c97319-773e-4350-8d92-e2bf698def9c_1272x835.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c97319-773e-4350-8d92-e2bf698def9c_1272x835.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c97319-773e-4350-8d92-e2bf698def9c_1272x835.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Event documented in AMD’s Zen 4 Processor Programming Reference (PPR)</figcaption></figure></div><p>While this performance monitoring event provides an idea of average latency just as my C and assembly code does, they don’t measure exactly the same thing. Software can only observe load-to-use latency. That includes latency all the way from address generation within the core to getting the data from somewhere in the memory hierarchy. AMD uses the mnemonic “XiSampledLatency” to describe their event. “Xi” is a component in Zen’s L3 cache complex that interfaces with the rest of the system. Likely, it stands for “eXternal Interface”. It probably has a set of queues to track outstanding requests. Sampling latency would be as simple as noting how long a queue entry remained allocated.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3ce53f6-c9b7-459f-af6c-ee36539607fa_328x572.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3ce53f6-c9b7-459f-af6c-ee36539607fa_328x572.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3ce53f6-c9b7-459f-af6c-ee36539607fa_328x572.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3ce53f6-c9b7-459f-af6c-ee36539607fa_328x572.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3ce53f6-c9b7-459f-af6c-ee36539607fa_328x572.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3ce53f6-c9b7-459f-af6c-ee36539607fa_328x572.jpeg" width="328" height="572" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b3ce53f6-c9b7-459f-af6c-ee36539607fa_328x572.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:572,&quot;width&quot;:328,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3ce53f6-c9b7-459f-af6c-ee36539607fa_328x572.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3ce53f6-c9b7-459f-af6c-ee36539607fa_328x572.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3ce53f6-c9b7-459f-af6c-ee36539607fa_328x572.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3ce53f6-c9b7-459f-af6c-ee36539607fa_328x572.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure from AMD’s ISSCC paper on Zen 1, showing the L3 cache complex. Xi is likely eXternal Interface. DP might be Datapath, STM = State Macros, LRU = least recently used metadata</figcaption></figure></div><p>Because this event is likely implemented in the Xi, it only measures latency after a L3 miss. DRAM latency as seen from software will include latency introduced at many other layers. That includes address generation, and checking each cache layer before a L3 miss is discovered.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57acdd50-f1c7-45c3-9b22-00528393338f_2425x299.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57acdd50-f1c7-45c3-9b22-00528393338f_2425x299.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57acdd50-f1c7-45c3-9b22-00528393338f_2425x299.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57acdd50-f1c7-45c3-9b22-00528393338f_2425x299.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57acdd50-f1c7-45c3-9b22-00528393338f_2425x299.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57acdd50-f1c7-45c3-9b22-00528393338f_2425x299.png" width="1456" height="180" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/57acdd50-f1c7-45c3-9b22-00528393338f_2425x299.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:180,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:165943,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57acdd50-f1c7-45c3-9b22-00528393338f_2425x299.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57acdd50-f1c7-45c3-9b22-00528393338f_2425x299.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57acdd50-f1c7-45c3-9b22-00528393338f_2425x299.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57acdd50-f1c7-45c3-9b22-00528393338f_2425x299.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Therefore, latency seen by the Xi should be lower than latency seen by software. Still, this event is useful for seeing how the memory subsystem behaves after a L3 miss. Data from the Xi roughly correlates with software observations at the start of my full system bandwidth test, when CC1 runs a latency test and CCD0 runs a single thread generating bandwidth load. Software sees 190 ns of latency, while L3 performance monitoring on CCD1 sees 166 ns.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F849d72b7-8cf1-443b-9d88-4c3448f47905_1716x827.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F849d72b7-8cf1-443b-9d88-4c3448f47905_1716x827.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F849d72b7-8cf1-443b-9d88-4c3448f47905_1716x827.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F849d72b7-8cf1-443b-9d88-4c3448f47905_1716x827.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F849d72b7-8cf1-443b-9d88-4c3448f47905_1716x827.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F849d72b7-8cf1-443b-9d88-4c3448f47905_1716x827.png" width="1456" height="702" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/849d72b7-8cf1-443b-9d88-4c3448f47905_1716x827.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:702,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:278212,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F849d72b7-8cf1-443b-9d88-4c3448f47905_1716x827.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F849d72b7-8cf1-443b-9d88-4c3448f47905_1716x827.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F849d72b7-8cf1-443b-9d88-4c3448f47905_1716x827.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F849d72b7-8cf1-443b-9d88-4c3448f47905_1716x827.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>X axis = elapsed time during test run. Latency test thread on CCCD1. I alternate between CCDs when adding bandwidth test threads</figcaption></figure></div><p>Interestingly, performance monitoring data from the other CCD suggests Zen 4 prioritized the bandwidth hungry thread at the expense of the latency sensitive one. As a sanity check, L3 miss bandwidth from the CCD hosting the bandwidth test thread is 59 GB/s, almost exactly matching what I calculated from software.</p><p>Once I spawn more bandwidth test threads, performance monitoring data suggests average latency rises to around 200 ns. However, software observations from the latency test thread sees latency go beyond 700 ns. Requests from the latency test thread account for a small minority of traffic passing through the memory subsystem, so it makes sense that the average latency as seen by the Xi doesn’t reflect my measurements.</p><p>Zen 5 is the latest and greatest member of AMD’s Zen line. It uses the same IO die as Zen 4, but the CCDs have changed. Cheese (George) has set the system up with very fast DDR5, and is running Infinity Fabric at a slightly higher clock to boot.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea2dfb4-e0f9-4c0c-8c0e-0b3ec3186750_1725x383.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea2dfb4-e0f9-4c0c-8c0e-0b3ec3186750_1725x383.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea2dfb4-e0f9-4c0c-8c0e-0b3ec3186750_1725x383.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea2dfb4-e0f9-4c0c-8c0e-0b3ec3186750_1725x383.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea2dfb4-e0f9-4c0c-8c0e-0b3ec3186750_1725x383.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea2dfb4-e0f9-4c0c-8c0e-0b3ec3186750_1725x383.png" width="1456" height="323" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2ea2dfb4-e0f9-4c0c-8c0e-0b3ec3186750_1725x383.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:323,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:72151,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea2dfb4-e0f9-4c0c-8c0e-0b3ec3186750_1725x383.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea2dfb4-e0f9-4c0c-8c0e-0b3ec3186750_1725x383.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea2dfb4-e0f9-4c0c-8c0e-0b3ec3186750_1725x383.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea2dfb4-e0f9-4c0c-8c0e-0b3ec3186750_1725x383.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>I wouldn’t call this a typical setup. DDR5-8000 kits are expensive. AMD’s reviewer guide recommends 6000 MT/s as a sweet spot. Yet this configuration provides a look into how Infinity Fabric performs with a ton of memory bandwidth available. I shouldn’t come anywhere close to DRAM bandwidth limits from within a single CCD. And indeed, latency is much better as I push to the IFOP’s bandwidth limits. Latency also starts off lower under high load, probably thanks to the very fast DRAM configuration.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4877ab40-01fe-4306-8e7e-cbfdcfe05711_1179x724.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4877ab40-01fe-4306-8e7e-cbfdcfe05711_1179x724.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4877ab40-01fe-4306-8e7e-cbfdcfe05711_1179x724.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4877ab40-01fe-4306-8e7e-cbfdcfe05711_1179x724.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4877ab40-01fe-4306-8e7e-cbfdcfe05711_1179x724.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4877ab40-01fe-4306-8e7e-cbfdcfe05711_1179x724.png" width="1179" height="724" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4877ab40-01fe-4306-8e7e-cbfdcfe05711_1179x724.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:724,&quot;width&quot;:1179,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:148059,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4877ab40-01fe-4306-8e7e-cbfdcfe05711_1179x724.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4877ab40-01fe-4306-8e7e-cbfdcfe05711_1179x724.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4877ab40-01fe-4306-8e7e-cbfdcfe05711_1179x724.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4877ab40-01fe-4306-8e7e-cbfdcfe05711_1179x724.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Contention within a single CCX still increases latency, but not to the same extent as with Zen 4. Zen 5 cores can also individually gobble down tons of bandwidth just like its predecessor. Perhaps CCX-level changes play a role. At Hot Chips 2024, AMD showed a slide suggesting each Zen 5 CCX has a pair of XIs. The two XIs together likely have more queue entries available than on Zen 4, which the slide also hints at.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07482216-437d-4cf9-b9a5-bba72e96e49e_2225x1162.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07482216-437d-4cf9-b9a5-bba72e96e49e_2225x1162.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07482216-437d-4cf9-b9a5-bba72e96e49e_2225x1162.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07482216-437d-4cf9-b9a5-bba72e96e49e_2225x1162.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07482216-437d-4cf9-b9a5-bba72e96e49e_2225x1162.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07482216-437d-4cf9-b9a5-bba72e96e49e_2225x1162.png" width="1456" height="760" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/07482216-437d-4cf9-b9a5-bba72e96e49e_2225x1162.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:760,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:692869,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07482216-437d-4cf9-b9a5-bba72e96e49e_2225x1162.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07482216-437d-4cf9-b9a5-bba72e96e49e_2225x1162.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07482216-437d-4cf9-b9a5-bba72e96e49e_2225x1162.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07482216-437d-4cf9-b9a5-bba72e96e49e_2225x1162.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>That probably decreases the chance of bandwidth hungry threads monopolizing all queue entries and starving out a latency sensitive one. Moreover, IFOP bandwidth covers just 55% of DDR5 bandwidth in this setup, compared to 71.4% on my Zen 4 system. Lower load on the memory controller gives it more room to manage DRAM inefficiencies like bus turnarounds, refreshes, or bank conflicts. I suspect Zen 5’s better behavior comes down to a combination of both factors.</p><p>As with Zen 4, CCD boundaries can insulate a latency sensitive thread from bandwidth hungry code. On this Zen 5 system. faster memory and faster Infinity Fabric clocks make everything better overall. More significantly, the latency spike observed on Zen 4 with one bandwidth thread is gone.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a191018-9f52-424f-bc35-a3ea7177fdc2_1186x727.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a191018-9f52-424f-bc35-a3ea7177fdc2_1186x727.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a191018-9f52-424f-bc35-a3ea7177fdc2_1186x727.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a191018-9f52-424f-bc35-a3ea7177fdc2_1186x727.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a191018-9f52-424f-bc35-a3ea7177fdc2_1186x727.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a191018-9f52-424f-bc35-a3ea7177fdc2_1186x727.png" width="1186" height="727" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0a191018-9f52-424f-bc35-a3ea7177fdc2_1186x727.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:727,&quot;width&quot;:1186,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:144694,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a191018-9f52-424f-bc35-a3ea7177fdc2_1186x727.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a191018-9f52-424f-bc35-a3ea7177fdc2_1186x727.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a191018-9f52-424f-bc35-a3ea7177fdc2_1186x727.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a191018-9f52-424f-bc35-a3ea7177fdc2_1186x727.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>On Zen 4, that spike must have been caused by something within Infinity Fabric. After all, the latency and bandwidth test threads can’t fight for the same XI or IFOP if they’re on different CCDs. Even though Zen 5 uses the same IO die, AMD may have tweaked their traffic management policies to more fairly service cores with varying memory access patterns.</p><p>The Ryzen 9 9950X and its fast memory setup continues to impress as I load both CCDs. Even as memory bandwidth passes 100 GB/s, latency remains well under control. Those DDR5-8000 memory sticks appear to cost $250 for a 48 GB kit. For that much money, you better get top-notch performance.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa88c98db-0df8-4358-81ff-f776761c0b93_1363x767.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa88c98db-0df8-4358-81ff-f776761c0b93_1363x767.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa88c98db-0df8-4358-81ff-f776761c0b93_1363x767.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa88c98db-0df8-4358-81ff-f776761c0b93_1363x767.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa88c98db-0df8-4358-81ff-f776761c0b93_1363x767.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa88c98db-0df8-4358-81ff-f776761c0b93_1363x767.png" width="1363" height="767" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a88c98db-0df8-4358-81ff-f776761c0b93_1363x767.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:767,&quot;width&quot;:1363,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:177791,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa88c98db-0df8-4358-81ff-f776761c0b93_1363x767.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa88c98db-0df8-4358-81ff-f776761c0b93_1363x767.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa88c98db-0df8-4358-81ff-f776761c0b93_1363x767.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa88c98db-0df8-4358-81ff-f776761c0b93_1363x767.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Again, I suspect AMD tweaked something to improve latency under load. Crazy 700 ns measurements from Zen 4 are gone. I’m not pushing as close to DDR5 bandwidth limits on Zen 4, but Zen 4’s performance monitoring data suggests latency shouldn’t have been too far above 200 ns.</p><p>Zen 2 may be a bit dated, but it did debut AMD’s modern chiplet setup. More interestingly, it has two quad core CCX-es per CCD. That lets me look at CCX-level and CCD-level bottlenecks separately.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b8669d8-6891-42d7-a5b0-d1de651aa6f2_1770x537.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b8669d8-6891-42d7-a5b0-d1de651aa6f2_1770x537.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b8669d8-6891-42d7-a5b0-d1de651aa6f2_1770x537.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b8669d8-6891-42d7-a5b0-d1de651aa6f2_1770x537.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b8669d8-6891-42d7-a5b0-d1de651aa6f2_1770x537.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b8669d8-6891-42d7-a5b0-d1de651aa6f2_1770x537.png" width="1456" height="442" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7b8669d8-6891-42d7-a5b0-d1de651aa6f2_1770x537.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:442,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:99030,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b8669d8-6891-42d7-a5b0-d1de651aa6f2_1770x537.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b8669d8-6891-42d7-a5b0-d1de651aa6f2_1770x537.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b8669d8-6891-42d7-a5b0-d1de651aa6f2_1770x537.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b8669d8-6891-42d7-a5b0-d1de651aa6f2_1770x537.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Unlike Zen 4 and Zen 5, I’m running Zen 2 with matched FCLK and DRAM speeds. Thus one CCD’s IFOP bandwidth matches DRAM bandwidth. Zen 2 achieves about 84.4% of theoretical DRAM bandwidth from a single CCX. That’s a larger percentage than Zen 4 (71.4%) or Zen 5 (55%). Of course both later generations achieve better absolute bandwidth.</p><p>Latency starts at 71.7 ns, and increases to 142.77 ns when three bandwidth hungry threads share the same CCX. But the latency test thread running on one CCX is reasonably well insulated from bandwidth load on another CCX, even if both CCX-es are on the same CCD. That leads me to think the CCX’s XI may be a more significant bottleneck than the IFOP link downstream.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1200f261-12ea-4970-be56-10a0391bc8f6_1758x884.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1200f261-12ea-4970-be56-10a0391bc8f6_1758x884.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1200f261-12ea-4970-be56-10a0391bc8f6_1758x884.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1200f261-12ea-4970-be56-10a0391bc8f6_1758x884.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1200f261-12ea-4970-be56-10a0391bc8f6_1758x884.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1200f261-12ea-4970-be56-10a0391bc8f6_1758x884.png" width="1456" height="732" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1200f261-12ea-4970-be56-10a0391bc8f6_1758x884.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:732,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:309778,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1200f261-12ea-4970-be56-10a0391bc8f6_1758x884.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1200f261-12ea-4970-be56-10a0391bc8f6_1758x884.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1200f261-12ea-4970-be56-10a0391bc8f6_1758x884.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1200f261-12ea-4970-be56-10a0391bc8f6_1758x884.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Creating bandwidth demand across both CCX-es within a CCD drives latency up. That’s no surprise because there’s now contention at both the CCX’s XI, and at the IFOP. Still, Zen 2 doesn’t do too badly. 285 ns of latency isn’t great, but it’s better than Zen 4’s 400 ns from a single CCD. Zen 5 is better than both at ~151 ns for a comparable CCD-level contention test.</p><p>I suspect Zen 2 does better than Zen 4 because Zen 2 cores individually can’t consume as much bandwidth. DRAM latency is high, which means you need a lot of in-flight requests queued up to sustain high DRAM bandwidth. A Zen 2 core can only sustain enough in-flight requests to achieve 24-25 GB/s of DRAM bandwidth. That’s well short of Infinity Fabric or DRAM bandwidth limits, so the latency test thread has a good chance of finding free queue entries for its own requests.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb80cf55f-a743-45c5-b26c-c9fc91c52a77_2688x582.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb80cf55f-a743-45c5-b26c-c9fc91c52a77_2688x582.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb80cf55f-a743-45c5-b26c-c9fc91c52a77_2688x582.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb80cf55f-a743-45c5-b26c-c9fc91c52a77_2688x582.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb80cf55f-a743-45c5-b26c-c9fc91c52a77_2688x582.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb80cf55f-a743-45c5-b26c-c9fc91c52a77_2688x582.png" width="1456" height="315" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b80cf55f-a743-45c5-b26c-c9fc91c52a77_2688x582.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:315,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:242145,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb80cf55f-a743-45c5-b26c-c9fc91c52a77_2688x582.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb80cf55f-a743-45c5-b26c-c9fc91c52a77_2688x582.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb80cf55f-a743-45c5-b26c-c9fc91c52a77_2688x582.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb80cf55f-a743-45c5-b26c-c9fc91c52a77_2688x582.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Zen 2 can benefit from CCD-level isolation too, just like Zen 4 and Zen 5. Like Zen 5, Zen 2 doesn’t see a latency spike with a single bandwidth hungry thread. However, I doubt there’s any sophisticated traffic management going on here. Again, a single thread isn’t able to sustain enough L3 misses to monopolize downstream queues.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382645c7-b0fa-4df1-b101-142f16ebf2e9_1692x590.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382645c7-b0fa-4df1-b101-142f16ebf2e9_1692x590.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382645c7-b0fa-4df1-b101-142f16ebf2e9_1692x590.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382645c7-b0fa-4df1-b101-142f16ebf2e9_1692x590.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382645c7-b0fa-4df1-b101-142f16ebf2e9_1692x590.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382645c7-b0fa-4df1-b101-142f16ebf2e9_1692x590.png" width="1456" height="508" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/382645c7-b0fa-4df1-b101-142f16ebf2e9_1692x590.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:508,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:114685,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382645c7-b0fa-4df1-b101-142f16ebf2e9_1692x590.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382645c7-b0fa-4df1-b101-142f16ebf2e9_1692x590.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382645c7-b0fa-4df1-b101-142f16ebf2e9_1692x590.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382645c7-b0fa-4df1-b101-142f16ebf2e9_1692x590.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Stepping back, Zen 2’s DDR4 controller is doing an excellent job of scheduling memory requests under high load. Despite being pushed closer to its bandwidth limits, the Ryzen 9 3950X is able to keep latency under control. In the bandwidth on CCD1, latency tested from CCD0 scenario, the 3950X maintains better latency than the 7950X3D.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c6a4482-8c42-400a-9090-096dc2a1b713_2188x1090.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c6a4482-8c42-400a-9090-096dc2a1b713_2188x1090.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c6a4482-8c42-400a-9090-096dc2a1b713_2188x1090.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c6a4482-8c42-400a-9090-096dc2a1b713_2188x1090.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c6a4482-8c42-400a-9090-096dc2a1b713_2188x1090.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c6a4482-8c42-400a-9090-096dc2a1b713_2188x1090.png" width="1456" height="725" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2c6a4482-8c42-400a-9090-096dc2a1b713_2188x1090.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:725,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:415810,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c6a4482-8c42-400a-9090-096dc2a1b713_2188x1090.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c6a4482-8c42-400a-9090-096dc2a1b713_2188x1090.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c6a4482-8c42-400a-9090-096dc2a1b713_2188x1090.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c6a4482-8c42-400a-9090-096dc2a1b713_2188x1090.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Loading both CCDs does increase latency, but it’s better than drawing all DRAM bandwidth through one CCD’s IFOP. Even though one IFOP interface has enough bandwidth to saturate the DDR4 controller, using both IFOP interfaces together provides better latency. That’s likely because I’m only pushing DDR4 bandwidth limits at this point, rather than pushing both DDR4 and a single IFOP to its limits.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4849b382-0022-4d2a-985e-3aba6a50136e_1790x567.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4849b382-0022-4d2a-985e-3aba6a50136e_1790x567.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4849b382-0022-4d2a-985e-3aba6a50136e_1790x567.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4849b382-0022-4d2a-985e-3aba6a50136e_1790x567.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4849b382-0022-4d2a-985e-3aba6a50136e_1790x567.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4849b382-0022-4d2a-985e-3aba6a50136e_1790x567.png" width="1456" height="461" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4849b382-0022-4d2a-985e-3aba6a50136e_1790x567.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:461,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:178845,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4849b382-0022-4d2a-985e-3aba6a50136e_1790x567.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4849b382-0022-4d2a-985e-3aba6a50136e_1790x567.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4849b382-0022-4d2a-985e-3aba6a50136e_1790x567.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4849b382-0022-4d2a-985e-3aba6a50136e_1790x567.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Those observations suggest contention within a CCX is most problematic, though contention over an IFOP interface can slightly increase latency too.</p><p><span>Zen 2 also has a pair of XI performance monitoring events for tracking average L3 miss latency. However, Zen 2 does a more straightforward measurement in cycles, rather than randomly sampling requests. The PPR tells you to divide the latency event by request count to get latency in cycles. Basically, it’s telling you to solve </span><a href="https://en.wikipedia.org/wiki/Little%27s_law" rel="">Little’s Law</a><span>. Working backwards, the latency event is incrementing with the XI’s queue occupancy per cycle.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eac121a-a0f6-4526-9182-4c42fd633071_1610x462.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eac121a-a0f6-4526-9182-4c42fd633071_1610x462.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eac121a-a0f6-4526-9182-4c42fd633071_1610x462.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eac121a-a0f6-4526-9182-4c42fd633071_1610x462.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eac121a-a0f6-4526-9182-4c42fd633071_1610x462.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eac121a-a0f6-4526-9182-4c42fd633071_1610x462.png" width="1456" height="418" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9eac121a-a0f6-4526-9182-4c42fd633071_1610x462.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:418,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:286479,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eac121a-a0f6-4526-9182-4c42fd633071_1610x462.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eac121a-a0f6-4526-9182-4c42fd633071_1610x462.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eac121a-a0f6-4526-9182-4c42fd633071_1610x462.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eac121a-a0f6-4526-9182-4c42fd633071_1610x462.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>From AMD’s Zen 2 PPR. There’s a corresponding event that you can divide by to get L3 miss latency in cycles. latency = queue occupancy / request count. So the latency event tracks queue occupancy</figcaption></figure></div><p>Looking at the queue occupancy figure on its own shows average queue occupancy around 59-61, which is suspiciously close to 64. Unfortunately AMD’s L3 performance counters don’t support count masking, but the average figure could mean each CCX’s XI has 64 queue entries. If so, two CCX-es together would have 128 XI queue entries.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca48acf-1d13-4787-bdc2-0a34d5260ce3_1531x783.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca48acf-1d13-4787-bdc2-0a34d5260ce3_1531x783.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca48acf-1d13-4787-bdc2-0a34d5260ce3_1531x783.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca48acf-1d13-4787-bdc2-0a34d5260ce3_1531x783.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca48acf-1d13-4787-bdc2-0a34d5260ce3_1531x783.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca48acf-1d13-4787-bdc2-0a34d5260ce3_1531x783.png" width="1456" height="745" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fca48acf-1d13-4787-bdc2-0a34d5260ce3_1531x783.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:745,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:261124,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca48acf-1d13-4787-bdc2-0a34d5260ce3_1531x783.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca48acf-1d13-4787-bdc2-0a34d5260ce3_1531x783.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca48acf-1d13-4787-bdc2-0a34d5260ce3_1531x783.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca48acf-1d13-4787-bdc2-0a34d5260ce3_1531x783.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>At Hot Chips 33, AMD presented a slide indicating the XI for Zen 3’s merged, octa-core CCX has 192 queue entries.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79db7ee1-b952-43bd-bede-5cbc88668b10_2235x1234.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79db7ee1-b952-43bd-bede-5cbc88668b10_2235x1234.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79db7ee1-b952-43bd-bede-5cbc88668b10_2235x1234.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79db7ee1-b952-43bd-bede-5cbc88668b10_2235x1234.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79db7ee1-b952-43bd-bede-5cbc88668b10_2235x1234.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79db7ee1-b952-43bd-bede-5cbc88668b10_2235x1234.png" width="1456" height="804" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/79db7ee1-b952-43bd-bede-5cbc88668b10_2235x1234.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:804,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:801950,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79db7ee1-b952-43bd-bede-5cbc88668b10_2235x1234.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79db7ee1-b952-43bd-bede-5cbc88668b10_2235x1234.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79db7ee1-b952-43bd-bede-5cbc88668b10_2235x1234.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79db7ee1-b952-43bd-bede-5cbc88668b10_2235x1234.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>AMD’s slide from Hot Chips 33 showing 192 pending misses from the CCX to memory</figcaption></figure></div><p>With Zen 5, AMD may have 320 XI queue entries per CCX, likely 160 entries in each of the CCD’s two XI blocks.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb690006f-1f90-47eb-a311-124c8ac02e97_1981x1113.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb690006f-1f90-47eb-a311-124c8ac02e97_1981x1113.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb690006f-1f90-47eb-a311-124c8ac02e97_1981x1113.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb690006f-1f90-47eb-a311-124c8ac02e97_1981x1113.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb690006f-1f90-47eb-a311-124c8ac02e97_1981x1113.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb690006f-1f90-47eb-a311-124c8ac02e97_1981x1113.webp" width="1456" height="818" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b690006f-1f90-47eb-a311-124c8ac02e97_1981x1113.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:818,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:108096,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb690006f-1f90-47eb-a311-124c8ac02e97_1981x1113.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb690006f-1f90-47eb-a311-124c8ac02e97_1981x1113.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb690006f-1f90-47eb-a311-124c8ac02e97_1981x1113.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb690006f-1f90-47eb-a311-124c8ac02e97_1981x1113.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Unfortunately, I haven’t found any information on Zen 4’s XI queue capacity. Perhaps Zen 4 increased the number of queue entries, but not by enough to accommodate Zen 4’s massive jump in memory level parallelism capabilities.</p><blockquote><p>Both the L2 and the L3 received larger miss queues to support more outstanding requests</p><p>Kai Troester, at AMD’s Hot Chips 2023 presentation on Zen 4</p></blockquote><p>If so, that would explain some of the weird behavior I see on Zen 4. Queue entries of course aren’t free, and larger queues cost both area and power. AMD could have made a sensible tradeoff on Zen 4 if users rarely run into those limitations. AMD likely evaluated many programs and decided on a sensible tradeoff. I don’t have the time or resources to do what a full time employee can, but I can go through a few examples.</p><p>Here, I’m running Cyberpunk 2077’s built-in benchmark at 1080P. I ran the benchmark twice with the game pinned to different CCDs, which should make performance monitoring data easier to interpret. On the non-VCache CCD, the game sees 10-15 GB/s of L3 miss traffic. It’s not a lot of bandwidth over a 1 second interval, but bandwidth usage may not be constant over that sampling interval. Short spikes in bandwidth demand may be smoothed out by queues throughout the memory subsystem, but longer spikes (still on the nanosecond scale) can fill those queues and increase access latency. Some of that may be happening in Cyberpunk 2077, as performance monitoring data indicates L3 miss latency is often above the 90 ns mark.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec392124-61a4-4d7f-a463-c99c2b1edcce_2295x1268.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec392124-61a4-4d7f-a463-c99c2b1edcce_2295x1268.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec392124-61a4-4d7f-a463-c99c2b1edcce_2295x1268.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec392124-61a4-4d7f-a463-c99c2b1edcce_2295x1268.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec392124-61a4-4d7f-a463-c99c2b1edcce_2295x1268.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec392124-61a4-4d7f-a463-c99c2b1edcce_2295x1268.png" width="1456" height="804" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ec392124-61a4-4d7f-a463-c99c2b1edcce_2295x1268.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:804,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:447584,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec392124-61a4-4d7f-a463-c99c2b1edcce_2295x1268.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec392124-61a4-4d7f-a463-c99c2b1edcce_2295x1268.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec392124-61a4-4d7f-a463-c99c2b1edcce_2295x1268.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec392124-61a4-4d7f-a463-c99c2b1edcce_2295x1268.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Pinning Cyberpunk 2077 to the VCache die significantly reduces L3 miss traffic, showing the value of having three times as much L3 capacity on hand. L3 misses are serviced with lower latency too. Less memory traffic reduces queuing delays, throughout the memory subsystem. Thus VCache has a secondary benefit of reducing average DRAM latency. It’s a potent combination, and one that’s reflected by the benchmark’s output. Cyberpunk 2077’s benchmark averaged 122.34 FPS when pinned to the non-VCache die, and 153.98 FPS when pinned to the VCache die. Despite clocking lower, the VCache die delivered 25.8% better performance.</p><p>Stepping back, neither scenario sees the game push against bandwidth limits anywhere in the memory subsystem. Latency in both cases is well under control, and baseline latency has more of an impact on performance than latency incurred from approaching bandwidth limits.</p><p>GHPC is a tank game, and presents another example. Patterns are similar, though with lower bandwidth demands. Again VCache shows its worth by servicing more memory requests on-die. And again, reducing load on the memory subsystem past L3 reduces average L3 miss latency.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7406a09d-705e-4974-9bd6-f9eed2f90437_2305x1273.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7406a09d-705e-4974-9bd6-f9eed2f90437_2305x1273.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7406a09d-705e-4974-9bd6-f9eed2f90437_2305x1273.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7406a09d-705e-4974-9bd6-f9eed2f90437_2305x1273.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7406a09d-705e-4974-9bd6-f9eed2f90437_2305x1273.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7406a09d-705e-4974-9bd6-f9eed2f90437_2305x1273.png" width="1456" height="804" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7406a09d-705e-4974-9bd6-f9eed2f90437_2305x1273.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:804,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:469264,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7406a09d-705e-4974-9bd6-f9eed2f90437_2305x1273.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7406a09d-705e-4974-9bd6-f9eed2f90437_2305x1273.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7406a09d-705e-4974-9bd6-f9eed2f90437_2305x1273.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7406a09d-705e-4974-9bd6-f9eed2f90437_2305x1273.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Baldur’s Gate 3 is a role playing game where you can roll dice and throw things. Bandwidth demands vary wildly from second to second, but sampled memory latency remains well under control. We don’t get anywhere close to the 200 ns that would suggest a bandwidth bottleneck.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc81f6dde-7d9a-4a54-9ef6-3c35374e95c1_2291x1163.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc81f6dde-7d9a-4a54-9ef6-3c35374e95c1_2291x1163.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc81f6dde-7d9a-4a54-9ef6-3c35374e95c1_2291x1163.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc81f6dde-7d9a-4a54-9ef6-3c35374e95c1_2291x1163.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc81f6dde-7d9a-4a54-9ef6-3c35374e95c1_2291x1163.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc81f6dde-7d9a-4a54-9ef6-3c35374e95c1_2291x1163.png" width="1456" height="739" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c81f6dde-7d9a-4a54-9ef6-3c35374e95c1_2291x1163.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:739,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:731748,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc81f6dde-7d9a-4a54-9ef6-3c35374e95c1_2291x1163.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc81f6dde-7d9a-4a54-9ef6-3c35374e95c1_2291x1163.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc81f6dde-7d9a-4a54-9ef6-3c35374e95c1_2291x1163.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc81f6dde-7d9a-4a54-9ef6-3c35374e95c1_2291x1163.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Again, Zen 4’s memory subsystem isn’t under significant pressure. VCache continues to do an excellent job, bringing L3 hitrate from 31.65% to 79.46%. But even without VCache, there’s plenty of spare Infinity Fabric and DDR5 bandwidth to go around.</p><p>RawTherapee is a free and open source raw file conversion program. Enthusiast cameras can record raw 12 or 14-bit sensor data instead of processed JPEGs. Raw files give photographers much more editing headroom to make exposure and white balance adjustments. They also let the editor make conscious tradeoffs between preserving detail and reducing noise. However, image processing can be computationally intensive. Here, I’m converting a few 45.7 megapixel D850 raw files to JPEGs, with exposure and noise reduction applied.</p><p>I didn’t pin RawTherapee to a CCD because image processing is a parallel task that benefits from high core counts (unlike most games). Instead, I’m logging data for both CCDs simultaneously. RawTherapee has spiky bandwidth demand – enough to fill queues, but often not long enough to run for longer than my 1 second sampling interval.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f3e3404-3853-4566-96df-01faf94f3667_2300x1166.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f3e3404-3853-4566-96df-01faf94f3667_2300x1166.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f3e3404-3853-4566-96df-01faf94f3667_2300x1166.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f3e3404-3853-4566-96df-01faf94f3667_2300x1166.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f3e3404-3853-4566-96df-01faf94f3667_2300x1166.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f3e3404-3853-4566-96df-01faf94f3667_2300x1166.png" width="1456" height="738" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8f3e3404-3853-4566-96df-01faf94f3667_2300x1166.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:738,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1216403,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f3e3404-3853-4566-96df-01faf94f3667_2300x1166.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f3e3404-3853-4566-96df-01faf94f3667_2300x1166.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f3e3404-3853-4566-96df-01faf94f3667_2300x1166.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f3e3404-3853-4566-96df-01faf94f3667_2300x1166.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>That’s where the sampled latency figure gives valuable insight. Latency spikes to over 200 ns, indicating the memory subsystem is pushed to its limits.</p><p>Not all multithreaded programs stress the memory subsystem though. I played Baldur’s Gate 3 while running video encoding jobs in the background. L3 miss traffic is significant but not overwhelming. Latency remained under control, and the game held 60 FPS most of the time.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a9440b-2fef-455f-a126-04db5f41fa2b_2180x979.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a9440b-2fef-455f-a126-04db5f41fa2b_2180x979.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a9440b-2fef-455f-a126-04db5f41fa2b_2180x979.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a9440b-2fef-455f-a126-04db5f41fa2b_2180x979.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a9440b-2fef-455f-a126-04db5f41fa2b_2180x979.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a9440b-2fef-455f-a126-04db5f41fa2b_2180x979.png" width="1456" height="654" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/58a9440b-2fef-455f-a126-04db5f41fa2b_2180x979.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:654,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1482264,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a9440b-2fef-455f-a126-04db5f41fa2b_2180x979.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a9440b-2fef-455f-a126-04db5f41fa2b_2180x979.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a9440b-2fef-455f-a126-04db5f41fa2b_2180x979.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a9440b-2fef-455f-a126-04db5f41fa2b_2180x979.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Video encoding can demand a lot of bandwidth, but the Ryzen 9 7950X3D’s L3 cache contains enough of it to avoid contention at the XI, Infinity Fabric, or DRAM controller levels. Off-core traffic exceed 85 GB/s over some sampling intervals, so a hypothetical Zen 4 setup with no L3 cache would suffer heavily from DRAM and Infinity Fabric bottlenecks. For perspective, here’s a bandwidth plot with L3 hit bandwidth included.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17e4f664-8258-4fab-b821-1ee086edae9b_2199x979.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17e4f664-8258-4fab-b821-1ee086edae9b_2199x979.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17e4f664-8258-4fab-b821-1ee086edae9b_2199x979.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17e4f664-8258-4fab-b821-1ee086edae9b_2199x979.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17e4f664-8258-4fab-b821-1ee086edae9b_2199x979.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17e4f664-8258-4fab-b821-1ee086edae9b_2199x979.png" width="1456" height="648" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/17e4f664-8258-4fab-b821-1ee086edae9b_2199x979.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:648,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1438861,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17e4f664-8258-4fab-b821-1ee086edae9b_2199x979.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17e4f664-8258-4fab-b821-1ee086edae9b_2199x979.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17e4f664-8258-4fab-b821-1ee086edae9b_2199x979.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17e4f664-8258-4fab-b821-1ee086edae9b_2199x979.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Long ago, chips like AMD’s Llano or Excavator only had 1 MB L2 caches and no L3. A large L3 cache takes significant die area and adds complexity, so I understand why AMD omitted it on certain products. But I can only imagine how hard even a fast DDR5 setup would be pushed by a hypothetical desktop chip with 16 cores, 1 MB of L2 per core, and no L3. Any interconnect sitting between the cores and memory would be heavily loaded too. Of course, such a setup doesn’t exist for good reason.</p><p>AMD’s successful Zen line stands on top of a scalable system architecture with multiple interconnect levels. But designing a scalable architecture is hard. Several blocks at one level may have to be fed through a choke point in the next level. If many cores are asking for as much bandwidth as they can get their hands on, queues can start filling and cause delays. That risks a “noisy neighbor” problem where latency sensitive code is penalized by bandwidth heavy code running elsewhere on the system.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F293ddd95-5e23-4b8d-83e5-b34b0a631057_2583x1436.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F293ddd95-5e23-4b8d-83e5-b34b0a631057_2583x1436.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F293ddd95-5e23-4b8d-83e5-b34b0a631057_2583x1436.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F293ddd95-5e23-4b8d-83e5-b34b0a631057_2583x1436.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F293ddd95-5e23-4b8d-83e5-b34b0a631057_2583x1436.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F293ddd95-5e23-4b8d-83e5-b34b0a631057_2583x1436.png" width="1456" height="809" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/293ddd95-5e23-4b8d-83e5-b34b0a631057_2583x1436.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:809,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1906858,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F293ddd95-5e23-4b8d-83e5-b34b0a631057_2583x1436.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F293ddd95-5e23-4b8d-83e5-b34b0a631057_2583x1436.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F293ddd95-5e23-4b8d-83e5-b34b0a631057_2583x1436.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F293ddd95-5e23-4b8d-83e5-b34b0a631057_2583x1436.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Delays at level in the memory subsystem are additive. A request held up for a dozen cycles waiting for an XI queue entry will be dozens of cycles late to the party when fighting for IFOP cycles. Any additional delays at the IFOP will mean the request goes through various Infinity Fabric components later, and so on. Zen 4 appears to be most severely affected by compounding delays, probably because AMD let individual cores consume way more bandwidth than before. But as performance counters and observations on Zen 2 show, AMD’s Infinity Fabric and memory controller do a good job of maintaining reasonable latency under load. CCX-level contention seems to cause the worst of the loaded latency spikes I saw.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca77403-8a53-4ac9-9954-4a36b2195088_2569x1438.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca77403-8a53-4ac9-9954-4a36b2195088_2569x1438.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca77403-8a53-4ac9-9954-4a36b2195088_2569x1438.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca77403-8a53-4ac9-9954-4a36b2195088_2569x1438.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca77403-8a53-4ac9-9954-4a36b2195088_2569x1438.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca77403-8a53-4ac9-9954-4a36b2195088_2569x1438.png" width="1456" height="815" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fca77403-8a53-4ac9-9954-4a36b2195088_2569x1438.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:815,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:727589,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca77403-8a53-4ac9-9954-4a36b2195088_2569x1438.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca77403-8a53-4ac9-9954-4a36b2195088_2569x1438.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca77403-8a53-4ac9-9954-4a36b2195088_2569x1438.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffca77403-8a53-4ac9-9954-4a36b2195088_2569x1438.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>For the most part, these limitations aren’t seen in typical client applications. Games can be latency sensitive, but the ones I tested don’t create enough bandwidth demand to stress parts of the memory subsystem. Even well threaded productivity workloads may not push bandwidth limits, as AMD’s large L3 cache can contain a lot of memory accesses. Some workloads, like RawTherapee, are both difficult to cache and well threaded. I wouldn’t advise running such a workload alongside a game or another latency sensitive program. Still, Zen 5 shows AMD is paying some attention to ensuring a good baseline level of performance for latency sensitive tasks, even when the memory subsystem is very busy.</p><p>If time permits, I intend to test some Intel chips too. Many Intel chips basically have a single level interconnect, with a mesh or ring stop connecting cores to L3, DRAM, an iGPU, and possibly other blocks too. My first impression is that Intel’s Comet Lake behaves much like AMD’s Zen 2, though without the CCX and CCD-level contention points. Loading all cores brings latency up to 233.8 ns, worse than Zen 2 with all cores loaded, but better than Zen 2 with all bandwidth load and the latency test stuck on a single CCD. Eventually, I plan to play with some cloud server instances too.</p><p><span>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;</span><a href="https://www.patreon.com/ChipsandCheese" rel="">Patreon</a><span>&nbsp;or our&nbsp;</span><a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ" rel="">PayPal</a><span>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;</span><a href="https://discord.gg/TwVnRhxgY2" rel="">Discord</a><span>.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Starlink Direct to Cell (627 pts)]]></title>
            <link>https://www.starlink.com/business/direct-to-cell</link>
            <guid>42230103</guid>
            <pubDate>Sun, 24 Nov 2024 20:08:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.starlink.com/business/direct-to-cell">https://www.starlink.com/business/direct-to-cell</a>, See on <a href="https://news.ycombinator.com/item?id=42230103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="backgroundWrapper1"><shui-background-image _nghost-sc290=""><div _ngcontent-sc290=""><shui-factory><!----></shui-factory><shui-order-call-to-action _nghost-sc297="" id="ubiquitousCoverageBlock"><div _ngcontent-sc297=""><p>UBIQUITOUS COVERAGE</p><p>Starlink satellites with Direct to Cell capabilities enable ubiquitous access to texting, calling, and browsing wherever you may be on land, lakes, or coastal waters. Direct to Cell will also connect IoT devices with common LTE standards.</p></div></shui-order-call-to-action><shui-key-features-wrapper-helper _nghost-sc324="" id="dtcFeatures"><div _ngcontent-sc324=""><shui-key-features _ngcontent-sc324="" _nghost-sc323=""><shui-feature-badge _ngcontent-sc323="" alignment="center" _nghost-sc311=""></shui-feature-badge><!----><!----><shui-feature-badge _ngcontent-sc323="" alignment="center" _nghost-sc311=""><div _ngcontent-sc311=""><p>Voice &amp; Data</p><p>Starting 2025</p></div></shui-feature-badge><!----><!----><shui-feature-badge _ngcontent-sc323="" alignment="center" _nghost-sc311=""></shui-feature-badge><!----><!----><!----></shui-key-features></div></shui-key-features-wrapper-helper><shui-image-block _nghost-sc296="" id="dtcDiagram"><div _ngcontent-sc296=""><div _ngcontent-sc296=""><!----><!----><picture _ngcontent-sc296="" draggable="false"><!----><source _ngcontent-sc296="" media="(min-width: 959.9px)" srcset="https://api.starlink.com/public-files/quad_picture.png"><!----><!----><img _ngcontent-sc296="" loading="lazy" alt="" draggable="false" src="https://api.starlink.com/public-files/quad_picture.png"><!----></picture><!----></div><div _ngcontent-sc296=""><h2 _ngcontent-sc296="">STAY CONNECTED</h2><p _ngcontent-sc296=""> Direct to Cell works with existing LTE phones wherever you can see the sky. No changes to hardware, firmware, or special apps are required, providing seamless access to text, voice, and data. </p><!----><!----></div><!----><!----></div></shui-image-block><!----></div></shui-background-image></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WireGuard: Beyond the most basic configuration (306 pts)]]></title>
            <link>https://sloonz.github.io/posts/wireguard-beyond-basic-configuration/</link>
            <guid>42229299</guid>
            <pubDate>Sun, 24 Nov 2024 18:08:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sloonz.github.io/posts/wireguard-beyond-basic-configuration/">https://sloonz.github.io/posts/wireguard-beyond-basic-configuration/</a>, See on <a href="https://news.ycombinator.com/item?id=42229299">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p>Last week I wanted to replace my OpenVPN setup with WireGuard. The
basics were well-documented, going beyond the basics was a bit trickier.
Let me teach you want I learned.</p><h2 id="the-basics">The basics</h2><p>But first, let’s summarize the basics. I have a server with a hosting
provider that I want to use as a VPN server. I won’t delve into details
here, since there are so many great explanations on the web already
(<a href="https://www.wireguard.com/quickstart/#nat-and-firewall-traversal-persistence">here</a>,
<a href="https://www.wireguard.com/netns/">here</a>,
<a href="https://volatilesystems.org/wireguard-in-a-separate-linux-network-namespace.html">here</a>
or <a href="https://wiki.archlinux.org/title/WireGuard">here</a>), let’s just
make a quick summary of a simple setup, as a base for discussing the
(slightly) more advanced usages I had to configure myself:</p><ol><li><p>Generate a keypair (private key/public key) for the server.</p></li><li><p>Generate a keypair (private key/public key) for each client.</p></li><li><p>Pick a network for the VPN (for me: <code>10.100.0.0/16</code>), an IP for the
server (<code>10.100.0.1</code>) and the clients (<code>10.100.0.2</code>, <code>10.100.0.3</code>, etc.)</p></li><li><p>Create the configuration for the server</p></li></ol><pre tabindex="0"><code>[Interface]
Address = 10.100.0.1/24
PrivateKey = (redacted)
ListenPort = 51820
PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o ens0 -j MASQUERADE
PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o ens0 -j MASQUERADE

[Peer]
PublicKey = (redacted)
AllowedIPs = 10.100.0.2/32

[Peer]
PublicKey = (redacted)
AllowedIPs = 10.100.0.3/32
</code></pre><ol start="5"><li><p>Start the server, <code>wg up /etc/wireguard/wg0.conf</code></p></li><li><p>Create the configuration for the client</p></li></ol><pre tabindex="0"><code>[Interface]
PrivateKey = (redacted)

[Peer]
PublicKey = (redacted)
Endpoint = my-server.example.com:51820
AllowedIPs = 0.0.0.0/0, ::/0
</code></pre><ol start="7"><li>Start the client: <code>wg up /etc/wireguard/wg0.conf</code></li></ol><p>(optionally, not pictured here: create a network namespace for your VPN,
so your main connection still has a direct access to the internet, but
you can put applications that want the VPN in the VPN network namespace).</p><h2 id="nat">NAT</h2><p>Some applications (looking at you, BitTorrent client) do not play well
behind a NAT. Unfortunately, your VPN (wireguard or not) acts as a
NAT. One widely used method to work around those issues is UPnP.</p><p>UPnP solves two issues:</p><ul><li><p>Your computer does not know its public address on the internet (from
the point of view of an external system) ; behind a VPN, you public
address is the address of the VPN server, not the address assigned
to you by your ISP (your VPN software knows that address — but the
rest of the system generally has absolutely no knowledge of it). And
if that address is not known by your peer-to-peer software, it cannot
communicate it to other peers.</p></li><li><p>Even if that address is known and correctly communicated to a peer,
if you listen to a port (for example, TCP 8043), the peer will try to
reach you on that port, but on your VPN server IP. For that connection to
actually reach your computer, your VPN software will have to set up a port
forwarding rule (from VPN server 8043 to your ISP-assigned IP address,
port 8043 — in a real setup, the two ports may actually differ, but
let’s keep it simple for that explanation). UPnP provides a way to
do that.</p></li></ul><p>Let’s show that (obviously) our simple WireGuard-based VPN setup
does not provide UPnP (<code>external-ip</code> is a tool provided by <code>miniupnpc</code>,
an UPnP client):</p><pre tabindex="0"><code>$ external-ip
No IGD UPnP Device found on the network !
</code></pre><p>That was expected. Wireguard, being a very simple kernel module, does
not come with batteries included in the form of a UPnP server. We will
have to do it manually. Thankfully, it is pretty straightforward:</p><ol><li><p>Install <code>miniupnpd</code> (on the server, obviously).</p></li><li><p>Configure <code>miniupnpd</code>.</p></li><li><p>Add <code>PostUp = systemctl start miniupnpd</code> and <code>PostDown = systemctl stop miniupnpd</code> in your wireguard configuration file.</p></li></ol><p>The only non-trivial step here is configuring <code>miniupnpd</code>. All the
action lies in <code>/etc/miniupnpd/miniupnpd.conf</code>. Here is what you have
to configure:</p><ul><li><p><code>ext_ifname=ens0</code>: this is the internet-facing interface of your server
(it may be different from <code>ens0</code>).</p></li><li><p><code>listening_ip=wg0</code>: this is the wireguard network interface on your
server.</p></li><li><p><code>uuid=06df7440-dbac-404c-9c07-0b0dbfca609e</code>: use <code>uuidgen</code> to generate
one. Or you can steal mine, it doesn’t matter, since everything happens
in a private, non-routable network.</p></li><li><p><code>allow 1024-65535 10.100.0.0/16 1024-65535</code>: this is where you specify
your wireguard network (in my basic setup <code>10.100.0.0/16</code>).</p></li></ul><p>Let’s check that it works:</p><pre tabindex="0"><code>$ external-ip
(redacted, but it correctly returned my server IP)
$ upnpc -n 10.100.0.2 8043 8043 tcp 300
external (redacted:server-ip):8043 TCP is redirected to internal 10.100.0.2:8043 (duration=300)
$ socat TCP-LISTEN:8043 STDIO
</code></pre><p>And on another machine:</p><pre tabindex="0"><code>$ socat TCP:(redacted:server-ip):8043 STDIO
</code></pre><p>You can see that the two <code>socat</code> instances can communicate with each
other, passing through your VPN.</p><h2 id="ipv6">IPv6</h2><p>You know what’s even better than supporting UPnP to work around the
issues introduced that NAT ? Not having NAT. And the good news is,
with IPv6, you actually can.</p><p>The few tutorials who actually explains how to setup IPv6 for a
WireGuard-based VPN usually mirror the IPv4 setup: assign a private,
non-routable network to it (<code>10.100.0.0/16</code> for IPv4 get translated
to something like <code>fd00:dead:beef::/48</code> for IPv6), assign IP addresses
in this network to the server and the clients, and add an <code>ip6tables</code>
masquerade action.</p><p>We’re not going to do that. We can do better, and we will do better.</p><p>The first thing to notice is that my hosting provider has assigned to me
a whole /48 network for my account (<code>2001:aaaa:bbbb::/48</code>), and a /56
(<code>2001:aaaa:bbbb:1000::1/56</code>) for my server. We can take advantage of
that to assign different publicly routable IPv6 addresses to our clients,
instead of assigning private, non-routable addresses.</p><p>Let’s start with the server configuration. Let’s add IPv6. We assign
the /80 subnetwork <code>2001:aaaa:bbbb:1000:cafe::/80</code> to VPN network. I’ll
only list added configuration lines, not repeating existing ones:</p><pre tabindex="0"><code>[Interface]
Address = 2001:aaaa:bbbb:1000:cafe::1/80

[Peer]
AllowedIPs = 2001:aaaa:bbbb:1000:cafe::2/128

[Peer]
AllowedIPs = 2001:aaaa:bbbb:1000:cafe::3/128
</code></pre><p>Client-side, this is not much more complicated:</p><pre tabindex="0"><code>[Interface]
Address = 2001:aaaa:bbbb:1000:cafe::2/128
</code></pre><p>Just one sanity check: on your server, <code>ip -6 route get 2001:aaaa:bbbb:1000:cafe::2</code> must return the WireGuard interface
(<code>wg0</code>). If not, you will have to give a lower metric to <code>wg0</code> in your
routes. But you can now, on your client, directly listen to a port:</p><pre tabindex="0"><code>$ socat TCP6-LISTEN:8043
</code></pre><p>and it will be accessible from the public internet without any UPnP setup:</p><pre tabindex="0"><code>$ socat TCP6:[2001:aaaa:bbbb:1000:cafe::2]:8043
</code></pre><p>Also, the IP address on the device of your default route will be
<code>2001:aaaa:bbbb:1000:cafe::2</code>, meaning no need for UPnP to dectect your
public, routable IPv6: your VPN interface IP (which is private in the
IPv4 case, but now public for IPv6) is also your public IP.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mark–Scavenge: Waiting for Trash to Take Itself Out (133 pts)]]></title>
            <link>https://inside.java/2024/11/22/mark-scavenge-gc/</link>
            <guid>42229107</guid>
            <pubDate>Sun, 24 Nov 2024 17:40:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://inside.java/2024/11/22/mark-scavenge-gc/">https://inside.java/2024/11/22/mark-scavenge-gc/</a>, See on <a href="https://news.ycombinator.com/item?id=42229107">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="entry"><p><em>This blog post summarises a new garbage collection algorithm called Mark-Scavenge, which highlights how using reachability as a proxy for liveness in moving GCs leads to unnecessary data movement and how we can address this. This work is from the latest paper within the research collaboration between Oracle and Uppsala University. 
The full paper can be found <a href="https://dl.acm.org/doi/10.1145/3689791">on the ACM website</a>.</em></p>

<p>Modern garbage collection assumes that the weak generational hypothesis holds and that most objects die young [4]. To that end, the heap is partitioned by age into regions.
If the weak generational hypothesis holds, most objects in a young region will become garbage at almost any given time. Thus, within the young generation, a garbage collection algorithm 
that operates on live objects will be more efficient than one that operates on garbage objects, as the number of live objects will be substantially fewer.
Consider the example below where the GC needs to reclaim memory.</p>

<p>
    <img src="https://inside.java/images/blog/gc/reclaim-memory.png" width="100%">
    </p><h5> Figure 1: Reclaiming Memory Through Garbage Collection</h5>


<p>Since regions A and C have four live objects and region B has only two, selecting the sparsely populated region B for evacuation is the most efficient, as you only get memory back if you empty the whole region.
In this example, we evacuate (move) objects e and f to region D, and after updating the incoming pointers, the entire region B is now free.
So far so good, but what does it mean when a GC considers something to be live? According to <em>The Garbage Collection Handbook</em> [1]:</p>

<blockquote>
  <p>An object is said to be live if it will be accessed at some time in the future execution of the mutator.</p>
</blockquote>

<p>Alas, such information about an object’s “true liveness” is only available to those who can predict the future, and for correctness, they must never mispredict an object as dead when it is not.
Instead of such perfect predictions, GCs typically approximate liveness using pointer reachability [1]. However, using reachability may result in moving objects to reclaim memory can lead to “wasted work”. The wasted work stems from GC moving objects that while reachable will never be touched again by the system.</p>

<p>In OpenJDK, our object of study, all GCs approximate liveness as reachability. Serial and G1, for example, do this in the context of scavenging. Scavenging means traversing the heap from the roots and relocating reachable objects upon discovery.
ZGC uses separate <em>Mark</em> and <em>Evacuate</em> phases to discover and copy objects. An object is marked as live on discovery and later copied if found live during marking.</p>

<p>We set out to study to what extent using reachability as liveness causes the copying of effectively-garbage objects. As far as we know, we are the first to publish a study of this kind.
Our initial hypothesis was that collectors based on separate <em>Mark and Evacuate</em> phases would do worse because liveness is computed long before the liveness information is acted upon.
This delay, notably absent in scavenging, might cause the liveness-information to become stale. On the other hand, a computed live map, after marking, permits only evacuating sparsely populated regions, meaning that evacuation can be selective, focusing on pages that give a higher return on investment (e.g., copying a handful of objects to make the entire page free).
Ultimately, it was only possible to understand the amount of wasted work by running experiments.</p>

<p>
    <img src="https://inside.java/images/blog/gc/mark-evacuate-scavenge.png" width="100%">
    </p><h5> Figure 2: Mark-Evacuate | Scavenge Phases</h5>


<p>We started measuring wasted work in ZGC because it had the most fitting infrastructure we needed: load-barriers.
Without the load-barrier, we could not easily have detected object accesses – i.e., whether the program used the objects eventually.
We instrumented ZGC to identify young objects that were reachable during <em>Mark</em> phase but never subsequently accessed before becoming unreachable in the next GC cycle. 
This identifies GC relocation which is wasted work.
We then measured how many of these objects were moved by the GC. We refer to this as wasted work: the movement of actually-dead objects.</p>

<p>We ran the DaCapo benchmark suite [2] and SPECjbb2015 [3] using our modified ZGC and collected information about the true liveness (use) of evacuated objects. 
Each bar in the 4th group – ZGC (barrier) – in the plot below represents a particular benchmark and measures the percentage of unnecessary objects evacuated because they were never again touched and became unreachable by the next GC. 
The data shows a very high rate of objects being evacuated unnecessarily due to the pointer reachability over-approximation of liveness and the delay between marking and evacuation.</p>

<p>
    <img src="https://inside.java/images/blog/gc/benchmarks.png" width="100%">
    </p><h5> Figure 3: Benchmarks with Our Modified ZGC</h5>


<p>We repeated the G1 and Serial GC experiments with a less precise measurement due to the absence of a load barrier in these GCs. 
The results are similar, and not dramatically different from the precise load-barrier based measurements (ZGC barrier). 
While objects are evacuated immediately upon discovery in scavenging, all reachable objects are evacuated, leading to considerable wasted work. 
Our data shows that despite the time between marking discovery and copying in scavenging being shorter than in <em>Mark–Evacuate</em>, it does not necessarily reduce wasted work. 
Our conclusion is naively using reachability as a proxy for liveness in the context of a moving GC leads to considerable wasted work.</p>

<p>The more concurrent a GC is, the greater its freedom to schedule when it does work. 
However, a concurrent GC needs to predict and match the program’s allocation rate because it does not stop the program during garbage collection. 
However, to withstand misprediction, it also requires a “headroom” – a buffer or available memory – that can accommodate allocation spikes while the GC is working to free up memory. 
The headroom is typically unused, but it is a necessary safety precaution.</p>

<p>We propose a new GC algorithm to reduce wasted work by delaying evacuation until the next GC cycle to increase the chances of objects becoming unreachable. 
We call this algorithm <em>Mark–Scavenge</em>. <em>Mark-Scavenge</em> performs reachability discovery similarly to <em>Mark-Evacuate</em> and selects only sparsely populated regions for evacuation. 
Unlike <em>Mark-Evacuate</em>, there is no immediate evacuation after marking. This means that the only new memory that becomes available after marking are the regions that turned out to be completely empty. 
Our insight is that the headroom can serve allocations after marking. New headroom can be created on demand from sparsely populated regions using standard evacuation since we have liveness information for these regions. 
If this does not happen, objects in these regions have had an extra GC cycle to become unreachable, and if that happens, the GC does not need to evacuate those objects as they are dead. 
If an object is still reachable in the next GC cycle, it is evacuated in a scavenge fashion and relocated upon discovery. 
Thus, <em>Mark–Scavenge</em> combines the best elements of <em>Scavenging</em> and <em>Mark–Evacuate</em>: the ability to selectively evacuate and no delay between discovery and evacuation after – this is new to <em>Mark–Scavenge</em> – a delay.</p>

<p>
    <img src="https://inside.java/images/blog/gc/mark-scavenge.png" width="100%">
    </p><h5> Figure 4: Mark-Scavenge - Delay Evacuation Further for Less Wasted Work </h5>


<p>We implemented <em>Mark-Scavenge</em> by replacing <em>Mark-Evacuate</em> for the young collection in generational ZGC. The plot below shows how <em>Mark-Scavenge</em> manages to eliminate a lot of the wasted work.</p>

<p>
    <img src="https://inside.java/images/blog/gc/wasted-work-benchmarks.png" width="100%">
    </p><h5> Figure 5: Benchmarks on Wasted Work</h5>


<p>The most striking result is (up to) a 91% reduction in the relocation of dead objects. If the GC is run with as much memory as is needed to avoid allocation stalls, the cost of the wasted work is largely hidden. 
However, if we run on a heavily loaded machine (e.g., the allocation rate is temporarily spiked), the performance benefits become significant: 2-4%. Our best results are from SPECjbb2015, where critical-jOPS (latency) is improved by 2–4% and max-jOPS (throughput) is improved by 2–8%. 
For a configuration that stresses the machine, the latency for Cassandra is reduced by 20% in the 99th, 99.9th, and 99.99th percentiles.</p>

<p>Our results show that equating liveness with reachability causes significant copying of garbage data in moving collectors. On heavily loaded machines, this negatively impacts performance, as demonstrated by our <em>Mark-Scavenge</em> implementation in ZGC. 
The insights of this work will enable garbage collection engineers and researchers to improve the state-of-the-art further.</p>

<h2 id="references">References</h2>

<p>[1] Jones et al., The Garbage Collection Handbook (2023)</p>

<p>[2] <a href="https://github.com/dacapobench/dacapobench/releases/tag/v23.11-chopin">https://github.com/dacapobench/dacapobench/releases/tag/v23.11-chopin</a></p>

<p>[3] <a href="https://www.spec.org/jbb2015/">https://www.spec.org/jbb2015/</a></p>

<p>[4] <a href="https://doi.org/10.1145/390010.808261">Ungar, David. “Generation scavenging: A non-disruptive high performance storage reclamation algorithm”</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made an ls alternative for my personal use (103 pts)]]></title>
            <link>https://github.com/triyanox/lla</link>
            <guid>42229003</guid>
            <pubDate>Sun, 24 Nov 2024 17:23:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/triyanox/lla">https://github.com/triyanox/lla</a>, See on <a href="https://news.ycombinator.com/item?id=42229003">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto"><code>lla</code> - A modern alternative to ls</h2><a id="user-content-lla---a-modern-alternative-to-ls" aria-label="Permalink: lla - A modern alternative to ls" href="#lla---a-modern-alternative-to-ls"></a></p>
<p dir="auto"><code>lla</code> is a high-performance, extensible alternative to the traditional <code>ls</code> command, written in Rust. It offers enhanced functionality, customizable output, and a plugin system for extended capabilities.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/triyanox/lla/blob/main/lla.png?raw=true"><img src="https://github.com/triyanox/lla/raw/main/lla.png?raw=true" alt="lla in action"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>Efficient file listing</strong>: Optimized for speed, even in large directories</li>
<li><strong>Multiple view modes</strong>:
<ul dir="auto">
<li>Default view</li>
<li>Long format (<code>-l</code>)</li>
<li>Tree view (<code>-t</code>)</li>
<li>Recursive listing (<code>-R</code>)</li>
</ul>
</li>
<li><strong>Advanced sorting</strong>:
<ul dir="auto">
<li>Alphabetical (default)</li>
<li>File size (<code>-s size</code>)</li>
<li>Modification date (<code>-s date</code>)</li>
</ul>
</li>
<li><strong>Flexible filtering</strong>: Filter by filename or extension (<code>-f, --filter</code>)</li>
<li><strong>Customizable recursion</strong>: Set maximum depth for subdirectory traversal</li>
<li><strong>Extensible plugin system</strong>: Develop and integrate custom functionality</li>
<li><strong>Color-coded output</strong>: Easily distinguish file types and permissions</li>
<li><strong>Git integration</strong>: Show git status for files (with plugin)</li>
<li><strong>File categorization</strong>: Categorize files by type (with plugin)</li>
<li><strong>Keyword search</strong>: Search file contents for specified keywords (with plugin)</li>
<li><strong>File hash display</strong>: Show file hashes (with plugin)</li>
<li><strong>Code complexity analysis</strong>: Analyze code complexity (with plugin)</li>
<li><strong>File size visualization</strong>: Visualize file sizes (with plugin)</li>
<li><strong>Duplicate file detection</strong>: Identify duplicate files (with plugin)</li>
<li><strong>Directory metadata</strong>: Display detailed directory information (with plugin)</li>
<li><strong>File metadata</strong>: Show extended file metadata (with plugin)</li>
<li><strong>Last git commit info</strong>: Display information about the last git commit (with plugin)</li>
</ul>
<p dir="auto">and more!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">From crates.io</h3><a id="user-content-from-cratesio" aria-label="Permalink: From crates.io" href="#from-cratesio"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">For NetBSD users</h3><a id="user-content-for-netbsd-users" aria-label="Permalink: For NetBSD users" href="#for-netbsd-users"></a></p>

<p dir="auto">(we see you, netbsd. we appreciate you.)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">First you need to initialize the configuration file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="lla init
lla config # to view the config file"><pre>lla init
lla config <span><span>#</span> to view the config file</span></pre></div>
<p dir="auto">Then you can start using <code>lla</code>:</p>
<div data-snippet-clipboard-copy-content="lla [OPTIONS] [DIRECTORY]"><pre><code>lla [OPTIONS] [DIRECTORY]
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core Options</h3><a id="user-content-core-options" aria-label="Permalink: Core Options" href="#core-options"></a></p>
<ul dir="auto">
<li><code>-l, --long</code>: Use long listing format</li>
<li><code>-R, --recursive</code>: List subdirectories recursively</li>
<li><code>-t, --tree</code>: Display files in a tree structure</li>
<li><code>-s, --sort &lt;CRITERIA&gt;</code>: Sort by "name", "size", or "date"</li>
<li><code>-f, --filter &lt;PATTERN&gt;</code>: Filter files by name or extension</li>
<li><code>-d, --depth &lt;DEPTH&gt;</code>: Set maximum recursion depth</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Plugin Management</h3><a id="user-content-plugin-management" aria-label="Permalink: Plugin Management" href="#plugin-management"></a></p>
<ul dir="auto">
<li><code>--enable-plugin &lt;NAME&gt;</code>: Enable a specific plugin</li>
<li><code>--disable-plugin &lt;NAME&gt;</code>: Disable a specific plugin</li>
<li><code>--plugins-dir &lt;PATH&gt;</code>: Specify custom plugins directory</li>
<li><code>--plugin-arg &lt;ARG&gt;</code>: Pass arguments to enabled plugins</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Plugin Actions</h3><a id="user-content-plugin-actions" aria-label="Permalink: Plugin Actions" href="#plugin-actions"></a></p>
<p dir="auto"><code>lla</code> supports plugin-specific actions, allowing you to interact with plugins directly:</p>
<div data-snippet-clipboard-copy-content="lla plugin --name <PLUGIN_NAME> --action <ACTION_NAME> [--args <ARG1> <ARG2> ...]"><pre><code>lla plugin --name &lt;PLUGIN_NAME&gt; --action &lt;ACTION_NAME&gt; [--args &lt;ARG1&gt; &lt;ARG2&gt; ...]
</code></pre></div>
<ul dir="auto">
<li><code>--name &lt;PLUGIN_NAME&gt;</code>: Specify the name of the plugin</li>
<li><code>--action &lt;ACTION_NAME&gt;</code>: Specify the action to perform</li>
<li><code>--args &lt;ARG1&gt; &lt;ARG2&gt; ...</code>: Provide arguments for the action (optional)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Utility Commands</h3><a id="user-content-utility-commands" aria-label="Permalink: Utility Commands" href="#utility-commands"></a></p>
<ul dir="auto">
<li><code>lla install</code>: Install plugins
<ul dir="auto">
<li><code>--git &lt;URL&gt;</code>: Install from a Git repository</li>
<li><code>--dir &lt;PATH&gt;</code>: Install from a local directory</li>
</ul>
</li>
<li><code>lla list-plugins</code>: Display all available plugins</li>
<li><code>lla init</code>: Initialize configuration file</li>
<li><code>lla config</code>: View configuration file</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto"><code>lla</code> uses a TOML configuration file located at <code>~/.config/lla/config.toml</code>. Initialize with default settings:</p>
<div dir="auto" data-snippet-clipboard-copy-content="lla init
lla config # to view the config file"><pre>lla init
lla config <span><span>#</span> to view the config file</span></pre></div>
<p dir="auto">Example configuration:</p>
<div dir="auto" data-snippet-clipboard-copy-content="default_sort = &quot;name&quot;
default_format = &quot;default&quot;
enabled_plugins = [&quot;git_status&quot;, &quot;file_hash&quot;]
plugins_dir = &quot;/home/user/.config/lla/plugins&quot;
default_depth = 3"><pre><span>default_sort</span> = <span><span>"</span>name<span>"</span></span>
<span>default_format</span> = <span><span>"</span>default<span>"</span></span>
<span>enabled_plugins</span> = [<span><span>"</span>git_status<span>"</span></span>, <span><span>"</span>file_hash<span>"</span></span>]
<span>plugins_dir</span> = <span><span>"</span>/home/user/.config/lla/plugins<span>"</span></span>
<span>default_depth</span> = <span>3</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install Plugins</h2><a id="user-content-install-plugins" aria-label="Permalink: Install Plugins" href="#install-plugins"></a></p>
<p dir="auto">You can install plugins from a local directory or from a Git repository.</p>
<p dir="auto">You can find official plugins <a href="https://github.com/triyanox/lla/blob/main/plugins.md">here</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">From Git</h3><a id="user-content-from-git" aria-label="Permalink: From Git" href="#from-git"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="lla install --git <github_url>"><pre>lla install --git <span>&lt;</span>github_url<span>&gt;</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">From Local Directory</h3><a id="user-content-from-local-directory" aria-label="Permalink: From Local Directory" href="#from-local-directory"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Plugin Development</h2><a id="user-content-plugin-development" aria-label="Permalink: Plugin Development" href="#plugin-development"></a></p>
<p dir="auto">Develop custom plugins to extend <code>lla</code>'s functionality. Plugins are dynamic libraries that implement the <code>Plugin</code> trait from the <code>lla_plugin_interface</code> crate.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Plugin Structure</h3><a id="user-content-plugin-structure" aria-label="Permalink: Plugin Structure" href="#plugin-structure"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Create a new Rust library:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo new --lib my_lla_plugin"><pre>cargo new --lib my_lla_plugin</pre></div>
</li>
<li>
<p dir="auto">Add dependencies to <code>Cargo.toml</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[dependencies]
lla_plugin_interface = &quot;*&quot;

[lib]
crate-type = [&quot;cdylib&quot;]"><pre>[<span>dependencies</span>]
<span>lla_plugin_interface</span> = <span><span>"</span>*<span>"</span></span>

[<span>lib</span>]
<span>crate-type</span> = [<span><span>"</span>cdylib<span>"</span></span>]</pre></div>
</li>
<li>
<p dir="auto">Implement the <code>Plugin</code> trait:</p>
</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="use lla_plugin_interface::{Plugin, DecoratedEntry, EntryDecorator, CliArg};

pub struct MyPlugin;

impl Plugin for MyPlugin {
    fn name(&amp;self) -> &amp;'static str {
        &quot;my_plugin&quot;
    }

    fn version(&amp;self) -> &amp;'static str {
        env!(&quot;CARGO_PKG_VERSION&quot;)
    }

    fn description(&amp;self) -> &amp;'static str {
        env!(&quot;CARGO_PKG_DESCRIPTION&quot;)
    }

    fn cli_args(&amp;self) -> Vec<CliArg> {
        vec![
            CliArg {
                name: &quot;my-option&quot;.to_string(),
                short: Some('m'),
                long: Some(&quot;my-option&quot;.to_string()),
                help: &quot;Description of my option&quot;.to_string(),
                takes_value: true,
            }
        ]
    }

    fn handle_cli_args(&amp;self, args: &amp;[String]) {
        // Handle CLI arguments passed to the plugin
    }

    fn perform_action(&amp;self, action: &amp;str, args: &amp;[String]) -> Result<(), String> {
        match action {
            &quot;my-action&quot; => {
                // Perform custom action
                Ok(())
            }
            _ => Err(format!(&quot;Unknown action: {}&quot;, action)),
        }
    }
}

impl EntryDecorator for MyPlugin {
    fn decorate(&amp;self, entry: &amp;mut DecoratedEntry) {
        // Add custom fields or modify entry
    }

    fn format_field(&amp;self, entry: &amp;DecoratedEntry, format: &amp;str) -> Option<String> {
        // Return formatted string for display
    }

    fn supported_formats(&amp;self) -> Vec<&amp;'static str> {
        vec![&quot;default&quot;, &quot;long&quot;, &quot;tree&quot;]
    }
}

lla_plugin_interface::declare_plugin!(MyPlugin);"><pre><span>use</span> lla_plugin_interface<span>::</span><span>{</span><span>Plugin</span><span>,</span> <span>DecoratedEntry</span><span>,</span> <span>EntryDecorator</span><span>,</span> <span>CliArg</span><span>}</span><span>;</span>

<span>pub</span> <span>struct</span> <span>MyPlugin</span><span>;</span>

<span>impl</span> <span>Plugin</span> <span>for</span> <span>MyPlugin</span> <span>{</span>
    <span>fn</span> <span>name</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> -&gt; <span>&amp;</span><span>'</span>static <span>str</span> <span>{</span>
        <span>"my_plugin"</span>
    <span>}</span>

    <span>fn</span> <span>version</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> -&gt; <span>&amp;</span><span>'</span>static <span>str</span> <span>{</span>
        <span>env</span><span>!</span><span>(</span><span>"CARGO_PKG_VERSION"</span><span>)</span>
    <span>}</span>

    <span>fn</span> <span>description</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> -&gt; <span>&amp;</span><span>'</span>static <span>str</span> <span>{</span>
        <span>env</span><span>!</span><span>(</span><span>"CARGO_PKG_DESCRIPTION"</span><span>)</span>
    <span>}</span>

    <span>fn</span> <span>cli_args</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> -&gt; <span>Vec</span><span>&lt;</span><span>CliArg</span><span>&gt;</span> <span>{</span>
        <span>vec</span><span>!</span><span>[</span>
            <span>CliArg</span> <span>{</span>
                name: <span>"my-option"</span>.to_string<span>(</span><span>)</span>,
                short: <span>Some</span><span>(</span><span>'m'</span><span>)</span>,
                long: <span>Some</span><span>(</span><span>"my-option"</span>.to_string<span>(</span><span>)</span><span>)</span>,
                help: <span>"Description of my option"</span>.to_string<span>(</span><span>)</span>,
                takes_value: <span>true</span>,
            <span>}</span>
        <span>]</span>
    <span>}</span>

    <span>fn</span> <span>handle_cli_args</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>args</span><span>:</span> <span>&amp;</span><span>[</span><span>String</span><span>]</span><span>)</span> <span>{</span>
        <span>// Handle CLI arguments passed to the plugin</span>
    <span>}</span>

    <span>fn</span> <span>perform_action</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>action</span><span>:</span> <span>&amp;</span><span>str</span><span>,</span> <span>args</span><span>:</span> <span>&amp;</span><span>[</span><span>String</span><span>]</span><span>)</span> -&gt; <span>Result</span><span>&lt;</span><span>(</span><span>)</span><span>,</span> <span>String</span><span>&gt;</span> <span>{</span>
        <span>match</span> action <span>{</span>
            <span>"my-action"</span> =&gt; <span>{</span>
                <span>// Perform custom action</span>
                <span>Ok</span><span>(</span><span>(</span><span>)</span><span>)</span>
            <span>}</span>
            _ =&gt; <span>Err</span><span>(</span><span>format</span><span>!</span><span>(</span><span>"Unknown action: {}"</span>, action<span>)</span><span>)</span><span>,</span>
        <span>}</span>
    <span>}</span>
<span>}</span>

<span>impl</span> <span>EntryDecorator</span> <span>for</span> <span>MyPlugin</span> <span>{</span>
    <span>fn</span> <span>decorate</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>entry</span><span>:</span> <span>&amp;</span><span>mut</span> <span>DecoratedEntry</span><span>)</span> <span>{</span>
        <span>// Add custom fields or modify entry</span>
    <span>}</span>

    <span>fn</span> <span>format_field</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>entry</span><span>:</span> <span>&amp;</span><span>DecoratedEntry</span><span>,</span> <span>format</span><span>:</span> <span>&amp;</span><span>str</span><span>)</span> -&gt; <span>Option</span><span>&lt;</span><span>String</span><span>&gt;</span> <span>{</span>
        <span>// Return formatted string for display</span>
    <span>}</span>

    <span>fn</span> <span>supported_formats</span><span>(</span><span>&amp;</span><span>self</span><span>)</span> -&gt; <span>Vec</span><span>&lt;</span><span>&amp;</span><span>'</span>static <span>str</span><span>&gt;</span> <span>{</span>
        <span>vec</span><span>!</span><span>[</span><span>"default"</span>, <span>"long"</span>, <span>"tree"</span><span>]</span>
    <span>}</span>
<span>}</span>

lla_plugin_interface<span>::</span>declare_plugin!<span>(</span><span>MyPlugin</span><span>)</span><span>;</span></pre></div>
<ol start="4" dir="auto">
<li>
<p dir="auto">Build your plugin:</p>

</li>
<li>
<p dir="auto">Install the plugin:</p>
<div dir="auto" data-snippet-clipboard-copy-content="lla install --dir /path/to/my_lla_plugin"><pre>lla install --dir /path/to/my_lla_plugin</pre></div>
<p dir="auto">or</p>
<div dir="auto" data-snippet-clipboard-copy-content="lla install --git <git_repo>"><pre>lla install --git <span>&lt;</span>git_repo<span>&gt;</span></pre></div>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Plugin Interface</h3><a id="user-content-plugin-interface" aria-label="Permalink: Plugin Interface" href="#plugin-interface"></a></p>
<p dir="auto">The <code>lla_plugin_interface</code> crate provides the following key components:</p>
<ul dir="auto">
<li><code>Plugin</code> trait: Core interface for plugin functionality</li>
<li><code>EntryDecorator</code> trait: Methods for decorating and formatting file entries</li>
<li><code>DecoratedEntry</code> struct: Represents a file entry with metadata and custom fields</li>
<li><code>CliArg</code> struct: Defines command-line arguments for the plugin</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Long format, sorted by size, showing only .rs files
lla -ls size -f .rs

# Enable git status plugin
lla --enable-plugin git_status

# Enable multiple plugins
lla --enable-plugin git_status categorizer

# Disable git status plugin
lla --disable-plugin git_status

# Disable multiple plugins
lla --disable-plugin git_status categorizer

# Set keywords for the keyword search plugin using plugin action
lla plugin --name keyword_search --action set-keywords --args &quot;TODO&quot; &quot;FIXME&quot; &quot;BUG&quot;

# Show current keywords for the keyword search plugin
lla plugin --name keyword_search --action show-keywords

# Use the keyword search plugin with the set keywords
lla --enable-plugin keyword_search"><pre><span><span>#</span> Long format, sorted by size, showing only .rs files</span>
lla -ls size -f .rs

<span><span>#</span> Enable git status plugin</span>
lla --enable-plugin git_status

<span><span>#</span> Enable multiple plugins</span>
lla --enable-plugin git_status categorizer

<span><span>#</span> Disable git status plugin</span>
lla --disable-plugin git_status

<span><span>#</span> Disable multiple plugins</span>
lla --disable-plugin git_status categorizer

<span><span>#</span> Set keywords for the keyword search plugin using plugin action</span>
lla plugin --name keyword_search --action set-keywords --args <span><span>"</span>TODO<span>"</span></span> <span><span>"</span>FIXME<span>"</span></span> <span><span>"</span>BUG<span>"</span></span>

<span><span>#</span> Show current keywords for the keyword search plugin</span>
lla plugin --name keyword_search --action show-keywords

<span><span>#</span> Use the keyword search plugin with the set keywords</span>
lla --enable-plugin keyword_search</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Please feel free to submit pull requests, report bugs, and suggest features.</p>
<ol dir="auto">
<li>Fork the repository</li>
<li>Create your feature branch (<code>git checkout -b feature/new-feature</code>)</li>
<li>Commit your changes (<code>git commit -m 'Add some new-feature'</code>)</li>
<li>Push to the branch (<code>git push origin feature/new-feature</code>)</li>
<li>Open a Pull Request</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License - see the <a href="https://github.com/triyanox/lla/blob/main/LICENSE">LICENSE</a> file for details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Those correction notices, in full (123 pts)]]></title>
            <link>https://statmodeling.stat.columbia.edu/2024/11/24/those-correction-notices-in-full-yes-its-possible-to-directly-admit-and-learn-from-error/</link>
            <guid>42228866</guid>
            <pubDate>Sun, 24 Nov 2024 17:00:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://statmodeling.stat.columbia.edu/2024/11/24/those-correction-notices-in-full-yes-its-possible-to-directly-admit-and-learn-from-error/">https://statmodeling.stat.columbia.edu/2024/11/24/those-correction-notices-in-full-yes-its-possible-to-directly-admit-and-learn-from-error/</a>, See on <a href="https://news.ycombinator.com/item?id=42228866">Hacker News</a></p>
Couldn't get https://statmodeling.stat.columbia.edu/2024/11/24/those-correction-notices-in-full-yes-its-possible-to-directly-admit-and-learn-from-error/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Senators say TSA's facial recognition program is out of control (167 pts)]]></title>
            <link>https://gizmodo.com/senators-say-tsas-facial-recognition-program-is-out-of-control-heres-how-to-opt-out-2000528310</link>
            <guid>42228795</guid>
            <pubDate>Sun, 24 Nov 2024 16:44:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/senators-say-tsas-facial-recognition-program-is-out-of-control-heres-how-to-opt-out-2000528310">https://gizmodo.com/senators-say-tsas-facial-recognition-program-is-out-of-control-heres-how-to-opt-out-2000528310</a>, See on <a href="https://news.ycombinator.com/item?id=42228795">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              
              
              <p>A bipartisan group of 12 senators has urged the Transportation Security Administration’s inspector general to investigate the agency’s use of facial recognition, saying it poses a significant threat to privacy and civil liberties.</p>

 <p>Their <a href="https://www.merkley.senate.gov/wp-content/uploads/Merkley-Letter-to-IG-FINAL.pdf">letter</a> comes just before one of the busiest travel periods of the year when millions of Americans are expected to pass through the nation’s airports.</p> <p>“This technology will soon be in use at hundreds of major and mid-size airports without an independent evaluation of the technology’s precision or an audit of whether there are sufficient safeguards in place to protect passenger privacy,” the senators wrote.</p>

 <p>The letter was signed by Jeffrey Merkley (D-OR), John Kennedy (R-LA), Ed Markey (D-MA), Ted Cruz (R-TX), Roger Marshall (R-Kansas), Ron Wyden (D-OR),&nbsp; Steve Daines (R-MT), Elizabeth Warren (D-MA), Bernie Sanders (I-VT), Cynthia Lummis (R-WY), Chris Van Hollen (D-MD), and Peter Welch (D-VT).</p> <p>While the TSA’s facial recognition program is currently optional and only in a few dozen airports, the agency announced in June that it plans to expand the technology to more than 430 airports. And the senators’ letter quotes a talk given by TSA Administrator David Pekoske in 2023 in which he said “we will get to the point where we require biometrics across the board.”</p>

 <p>“While the TSA claims facial recognition is optional, it is confusing and intimidating to opt out of TSA’s facial recognition scans, and our offices have received numerous anecdotal reports of Transportation Security Officers (TSOs) becoming belligerent when a traveler askes to opt out, or simply being unaware of that right,” the senators wrote. They added that in some airports the signage instructing flyers to step in front of a camera is prominently displayed while signs advising passengers of their right to opt out of face scan is “strategically placed in inconspicuous locations.”</p> <p>In an <a href="https://www.merkley.senate.gov/wp-content/uploads/2024_05_02_LTR-TSA-Freeze-to-Leadership.pdf">earlier letter</a> sent by many of the same senators to Senate Majority Leader Chuck Schumer (D-NY) and Minority Leader Mitch McConnell (R-KY), the senators said that the TSA has not produced any evidence in response to congressional inquiries showing that the implementation of facial recognition has led to the discovery of more fraudulent identity documents. Meanwhile, the TSA <a href="https://www.washingtonpost.com/technology/2023/07/11/tsa-airport-security-facial-recognition/">has said</a> the systems have a three percent false negative rate—how often they fail to properly match a person to their image in the database—which would equate to 68,000 failures daily if the technology was spread across all airports.</p>

 <p>The latest letter urges the TSA’s inspector general to evaluate the agency’s facial recognition program to determine whether it’s resulted in a meaningful reduction in passenger delays, assess whether it’s prevented anyone on no-fly lists from boarding a plane, and identify how frequently it results in identity verification errors.</p> <p>To opt out of a face scan at an airport, a traveler need only say that they decline facial recognition. They can then proceed normally through security by presenting an identification document, such as a driver’s license or passport.</p>

 <p><strong><em>Correction:</em></strong><em> An earlier version of this story quoted from the wrong letter sent by senators regarding the TSA’s facial recognition problem and incorrectly identified the senators who had signed the latest letter to the TSA’s inspector general.</em></p>
                          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Howstuffworks.com creator, Marshall Brain, has died (306 pts)]]></title>
            <link>https://www.wral.com/news/local/nc-state-marshall-brain-dies-november-2024/</link>
            <guid>42228759</guid>
            <pubDate>Sun, 24 Nov 2024 16:39:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wral.com/news/local/nc-state-marshall-brain-dies-november-2024/">https://www.wral.com/news/local/nc-state-marshall-brain-dies-november-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=42228759">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <section>
				
        
				
					<p>A North Carolina State University faculty member and creator of the popular website "How Stuff Works" has died.</p>
				
        <p><span>
            Updated
            <formatted-timestamp iso-date="2024-11-24T14:52:56.373Z">
              ${new Date(updatedAt).toLocaleString() || updatedAt}
            </formatted-timestamp>
					</span>
          
        </p>
      </section>

      

      <section>

        <div>
					
          <figure>
						
							<div>
								<video id="hero-video" controls="" poster="https://images.wral.com/d5e9f942-c3e4-42f0-b449-f6b85291bf11?w=964&amp;h=542">
									
										<source src="https://media-hls.wral.com/vodhttporigins3/_definst_/smil:amazons3/cbcnm-static-web-content/home/web/wral/public/asset/news/local/2024/11/22/21735146/3263999-nc_state-DMID1-65213aki2.smil/playlist.m3u8" type="application/x-mpegURL">
									
									<track kind="captions">
								</video>
								
								<div>
									<p>NC State faculty member found dead in office</p>
								</div>
							</div>
							
						

						
					</figure>
					
          
          
            
          

        </div>

				<div>
					
						<p>A North Carolina State University faculty member and creator of the popular website "How Stuff&nbsp;Works" has died.</p>
						
						
						
					
						<p>Marshall Brain died inside his office Wednesday on N.C. State’s centennial campus.</p>
						
							
						
						
						
					
						<p>While&nbsp;the university would not confirm any details related to his death, sources close to Brain said he died by suicide.</p>
						
						
						
					
						<p>The N.C. State Police Department said it is no longer investigating, and foul play is not suspected.</p>
						
						
						
					
						<p>Brain contributed several articles to <a href="http://www.wral.com/">WRAL.com</a> over the years, <a href="https://www.wral.com/story/marshall-brain-anatomy-of-a-failed-doomsday-scenario/20997204/" target="_blank">including an editorial piece</a>. He’s also author of "<a href="https://www.amazon.com/Doomsday-Book-Science-Humanitys-Greatest/dp/1454939966/" target="_blank" rel="noopener">The Doomsday Book: The Science Behind Humanity’s Greatest Threats."</a></p>
						
						
						
					
						<h2><a href="https://www.wral.com/20833852/">Get Help: County-by-county guide to mental health and crisis resources in NC</a></h2>
						
						
						
					
						<p>If you're having suicidal thoughts or a mental health crisis,<i> </i><a href="https://www.ncdhhs.gov/divisions/mental-health-developmental-disabilities-and-substance-use-services/crisis-services/988-suicide-crisis-lifeline">call or text 988</a> or call the <a href="https://www.nami.org/Advocacy/Policy-Priorities/Responding-to-Crises/National-Hotline-for-Mental-Health-Crises-and-Suicide-Prevention">National Suicide Prevention Lifeline</a> at 800-273-8255. Veterans can press “1” after dialing<i> </i><a href="https://www.ncdhhs.gov/divisions/mental-health-developmental-disabilities-and-substance-use-services/crisis-services/988-suicide-crisis-lifeline">988</a> to connect directly to the Veterans Crisis Lifeline. For texts, veterans should continue to text the Veterans Crisis Lifeline short code: 838255.</p>
						
						
							
							
						
						
					
					
				</div>

        <div>
					
					
					
					
						<p>
							© 2024  Copyright Capitol Broadcasting Company
						</p>
					
        </div>

				<!-- RevContent -->
				


      </section>

      
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linux 6.13 will report the number of hung tasks since boot (117 pts)]]></title>
            <link>https://www.phoronix.com/news/Linux-6.13-Non-MM</link>
            <guid>42228672</guid>
            <pubDate>Sun, 24 Nov 2024 16:20:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phoronix.com/news/Linux-6.13-Non-MM">https://www.phoronix.com/news/Linux-6.13-Non-MM</a>, See on <a href="https://news.ycombinator.com/item?id=42228672">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="LINUX KERNEL" src="https://www.phoronix.com/assets/categories/linuxkernel.webp" width="100" height="100"></p><p>
Following <a href="https://www.phoronix.com/news/Linux-6.13-MM-Patches">all of the MM patches earlier this week</a> sent in by Andrew Morton, on Sunday morning he sent out all of the non-MM patches that he manages for the Linux kernel. Notable for Linux 6.13 with this pull request is presenting the hung task counter as well as finishing off the folio conversion in the NILFS2 code.
</p><p>
Among the patches in the non-MM pull request is adding the code to detect the count of the number of hung tasks since boot time. Back when the idea and patches were first floated this work was covered on Phoronix within <a href="https://www.phoronix.com/news/Linux-hung_task_detect_count">Linux Working On A Counter To Keep Track Of The Number Of Hung Tasks Since Boot</a>.
</p><p>
Long story short this adds <strong>/proc/sys/kernel/hung_task_detect_count</strong> for indicating the number of hung task warnings since the system/server was booted. This is intended to help out particularly with Linux servers as a health metric for administrators to gauge if there may be software/hardware problems at play if encountering a number of unexpected hung tasks. It's a simple metric but until now there hasn't been any convenient "hung_task_detect_count" report readily available.
</p><p><img src="https://www.phoronix.net/image.php?id=2024&amp;image=hung_task_detect_count" alt="hung_task_detect_count"></p>
<p>Also part of today's non-MM pull are clean-ups to the resource management code and finishing off the folio conversion for the NILFS2 file-system.
</p><p>
See the <a href="https://lore.kernel.org/lkml/20241124020841.3a96d9e26be9ce4d5810d0b5@linux-foundation.org/">non-MM pull request</a> for the full list of patches.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Career Ending Mistake (325 pts)]]></title>
            <link>https://bitfieldconsulting.com/posts/career</link>
            <guid>42228538</guid>
            <pubDate>Sun, 24 Nov 2024 15:56:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bitfieldconsulting.com/posts/career">https://bitfieldconsulting.com/posts/career</a>, See on <a href="https://news.ycombinator.com/item?id=42228538">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="23" id="block-yui_3_17_2_1_1667929911209_9668"><p>This isn’t about the time I inadvertently shut down one of Britain’s nuclear power stations, an entirely true story for which the world is nevertheless not yet prepared. Nor is it about the poor junior developer who <a href="https://np.reddit.com/r/cscareerquestions/comments/6ez8ag/accidentally_destroyed_production_database_on/">accidentally destroyed the production database</a> on their first day (they’ll be fine, bless them).</p>
<p>Instead, I want to talk about another kind of career ending mistake, one that affects more than just the unlucky few. Indeed, it’s a mistake we’re probably all making right now. And that’s <em>not planning the end of our careers</em>.</p>
<h3 id="the-end">The end</h3>
<p>By “the end”, I don’t necessarily mean picking your retirement date. This is really about the aim or goal of your career. Where will you be when you realise that this is where you’ve always wanted to be?</p>
<p>If you love what you’re doing now and don’t ever want to change jobs, great: you’ve reached the end of your career, even if it plays out over many decades. If you don’t love it, though, and that’s much more likely, then it’s worth asking what job you <em>would</em> love, and how you’re going to get it.</p>
<p>So, where do you want to end up? And is that where you’re currently heading? If not, what should you do about it?</p>
<h3 id="out-of-control">Out of control</h3>
<p>The word “career” means, among other things, “to rush about wildly”, or, of a vehicle, “to move fast and in a way that is out of control”. Isn’t that apt? And doesn’t it describe the career experience of most of us pretty well?</p>
<blockquote>
<p><em>The indispensable first step to getting what you want is this: decide what you want.</em><br>
 —Saying</p>
</blockquote>
<p>Most of us, in fact, don’t really know what we want to do with our working lives until we’re more or less doing it. By the time we’ve made our minds up where we want to go, we may already be a long way down the wrong track.</p>
<p>It’s not surprising, then, that many of us find ourselves in less than fully satisfying jobs, with doubtful or non-existent prospects for advancement. In all honesty, if we <em>are</em> happy in our jobs, it’s probably more often a matter of luck than of design.</p>
<p>Maybe rather than rushing around wildly, we should give a little thought to what we want to achieve in our careers. Rather than just sit back and hope we get the career we want, maybe we should actively plan and take steps to bring it about.</p>
<p>As software engineers, we’re constantly making detailed, elaborate plans for computers to execute; that’s what software is, after all. Isn’t it kind of weird that we rarely give a moment’s thought to the program we want to write for ourselves?</p>
<h3 id="choose-your-destination">Choose your destination</h3>
<p>The best time to start a pension is always twenty years ago, and career planning is a bit like that, too. By the time you realise you need it, you usually wish you’d started a lot earlier. On the other hand, it would be foolish to let that prevent you from starting at all. Even a late, sketchy, and uncertain plan is way better than no plan.</p>
<p>So what would a career plan look like? And when should we start making it?</p>
<p>The first phase of your career is probably too early to make serious plans, and any decisions you make at this stage are rarely critical: there’s plenty of room to experiment and make mistakes.</p>
<p>In the final phase, by contrast, you have fewer options, and there’s not much time to pull off a significant change of direction. The autopilot tends to lock in and steer you further down the glide path you’re already on.</p>
<p>You should be planning, in other words, to enter the final phase of your career in the right position, at the right level, and at the right time to land where you want to. So where is that, exactly?</p>
<h3 id="three-kinds-of-careers">Three kinds of careers</h3>
<p>Your career is likely to encompass lots of different things. Over the years, you’ll do all sorts of different jobs at different companies, and the perfect career destination for you may end up being something you can’t even imagine right now.</p>
<p>To help us think about it, though, let’s divide the possibilities, very broadly, into three groups:</p>
<ol type="1">
<li>Seniority</li>
<li>Management</li>
<li>Independence</li>
</ol>
<p>In the rest of this article, we’ll take a closer look at each of these destination airports, check out the local weather conditions, and ask what kind of flight plan would get us there.</p>
<p>So, what kind of career do you want to have?</p>
</div><div data-block-type="23" id="block-yui_3_17_2_1_1667929911209_12674"><h2 id="seniority">Seniority</h2>
<p>The first kind of career destination we’ll consider is <em>senior
individual contributor</em> (IC), which usually means something like
“high-level engineer”. A senior IC doesn’t run their own firm, or work
independently. They’re at the top of their game technically, and may be
a technical leader for others, but they spend the majority of their time
<em>doing</em> the work, not managing other people who do.</p>
<h3 id="what-does-a-senior-ic-do">What does a senior IC do?</h3>
<p>The exact ranks and job titles for senior engineers vary from one
company to the next, and there may be many levels for you to achieve
within a given company. But in terms of your <em>eventual</em> career
goal, let’s say we’re talking about the top level you can reach while
still being primarily an engineer.</p>
<p>In a small company, that might simply be something like “senior
developer”, or even team leader. In larger companies, the pinnacle of
engineering pay and responsibilities might be called something like
“principal engineer” or even “distinguished engineer” (sounds good,
doesn’t it?).</p>
<p>This isn’t an executive position: you won’t have your own washroom,
or a seat on the board. On the other hand, you’ll have plenty of money,
status, and authority, and unlike your colleagues in management, you
won’t spend all day in meetings. You’ll be at the highest level of your
profession, and getting well paid to excel at the work you love.</p>
<blockquote>
<p><em>Your work is going to fill a large part of your life, and the
only way to be truly satisfied is to do what you believe is great work.
And the only way to do great work is to love what you do.</em><br>
—Steve Jobs, <a href="https://news.stanford.edu/2005/06/14/jobs-061505/">Stanford
commencement address</a></p>
</blockquote>
<h3 id="what-they-dont-do">What they don’t do</h3>
<p>There are a few limitations, though. You probably won’t get to choose
<em>what</em> to work on, and you may not agree with all the decisions
of the powers that be. In fact, it’s practically certain you won’t.
After all, you know more about the subject matter than they do. If, in
the end, you can’t live with those decisions, you can go work for
another company, but of course you’ll find the same dynamic there.</p>
<p>A senior IC role appeals to those who want to stay technical and keep
their hands on the keyboard, or at least the mouse. You don’t have the
time-consuming, cross-disciplinary responsibilities of a business owner,
or the political challenges of a manager. You get to do all the fun
stuff: the building and making. And you can do it until you choose to
retire—hopefully in financial comfort.</p>
<h3 id="how-to-get-there">How to get there</h3>
<p>Find out what the situation is where you work. If senior ICs are a
thing in your company, talk to them. Ask them for advice. How did they
get where they are? What were the steps along the way? And is it what
they thought it would be?</p>
<p>Talk to your boss. Do they know this is what you want? What are they
prepared to do to help you get it? And what will they expect from you in
return? Establishing this dialogue is important, but it’s not enough:
you need to keep it open throughout your employment. Check in with them
often on how you’re doing, and what more you can do to help them help
<em>you</em>.</p>
<p>Don’t wait for annual reviews. A year is too long to wait to find out
that you’ve made no progress towards your career goal. Some people
blithely assume that if they keep doing what they’re doing for long
enough and don’t screw up, they’ll eventually make senior IC.</p>
<p>Well, maybe. But a more reliable strategy is to take personal control
of your skills development, and start taking it seriously.</p>
<h3 id="seniority-means-mastery">Seniority means mastery</h3>
<p>The most senior engineers in any company are the most accomplished,
the most experienced, and those who have the most to teach others. If
you want to reach this level, you’ll need to become a master of your
chosen craft.</p>
<p>Don’t wait for the company to train you. They prefer to promote
people who don’t need training. Instead, once you’ve found the craft you
love, you’ll need to dedicate yourself to mastering it.</p>
<p>Start seeing your current work not as a simple exchange of your time
for their money, but rather as an opportunity to find out what you’re
good at, and to get better at it. A professional coach can help you
figure this out, support your learning, and keep your eye on the
ball.</p>
<blockquote>
<p><em>Manhood is patience. Mastery is nine times patience.</em><br>
—Ursula K. Le Guin, <a href="https://amzn.to/3o7CwKW">“A Wizard of
Earthsea”</a></p>
</blockquote>
<h2 id="management">Management</h2>
<p>While very senior engineers can be well-compensated, their roles are
usually not the most senior in the <em>company</em>. If you want to rise
even higher in pay and responsibility while staying within the firm, you
may need to think about leaving your technical work behind and switching
to <em>management</em>.</p>
<p>Management is perhaps the default destination for many tech careers.
Provided you can stay in a company long enough, the natural path to
further advancement may be for you to become a manager.</p>
<p>If you don’t change direction, in other words, you may end up where
you’re heading. But <em>is</em> that where you want to go?</p>
<h3 id="what-does-a-manager-do">What does a manager do?</h3>
<p>Engineering managers need a solid foundation of technical competence,
to be sure, but the work itself is primarily about leading, supervising,
hiring, and developing the skills of other technical people. It turns
out those are all skills, too, and relatively rare ones.</p>
<p>Managing people is hard; much harder than programming. Computers just
do what you tell them, whether that’s right or wrong (usually wrong).
Anyone can get good at programming, if they’re willing to put in enough
time and effort. I’m not sure anyone can get good at managing, and most
don’t. Most managers are <em>terrible</em>.</p>
<p>That’s quite a sweeping statement, I know. (Prove me wrong, managers,
prove me wrong.) But, really, would a car mechanic last long in the job
if they couldn’t fit a tyre, or change a spark plug? Would a doctor
succeed if they regularly amputated the wrong leg? We would hope not.
But many managers are just as incompetent, in their own field, and yet
they seem to get away with it.</p>
<h3 id="being-a-great-manager">Being a great manager</h3>
<p>Good managers, then, like good teachers, are rare, but all the more
precious for it. If you’ve ever had a really good boss, you’ll remember
them all your life, and, if you’re lucky, emulate them. (You’ll remember
the really bad ones, too.) And just because managers don’t cut code or
solder chips, it doesn’t mean they don’t have a big influence on the
success of projects and companies.</p>
<p>Indeed, managers can have an outsize influence on events. I’m sure we
can all cite examples of promising projects that sank without trace
because of a disastrous manager. I’m less sure that there are many
examples of inspirational managers rescuing doomed projects from the
brink, though it does happen.</p>
<p>If you want to become a great manager, which I think is the only kind
worth being, start practising now. Learn people skills, communication,
collaboration, psychology. Work on understanding the things that make
different kinds of people tick. Manage <em>yourself</em> excellently. If
you can’t organise yourself, how do you expect to be responsible for a
team?</p>
<h3 id="getting-started">Getting started</h3>
<p>Study your own manager. If they do the job well, figure out why (and
talk to them about it). If they’re a shambles, figure out what they’re
doing wrong, and decide how you’d do better.</p>
<p>A great manager understands what’s happening with each person in
their team, and can be there to eliminate problems and roadblocks almost
before they happen. Why shouldn’t you start doing this kind of thing
right now, rather than waiting to be told? Sometimes the de facto leader
of a team is simply the person that everyone turns to when they have a
problem they can’t solve on their own. If that’s you, you may already be
on the road to becoming a memorable manager—for the right reasons.</p>
<h2 id="independence">Independence</h2>
<p>Being independent means working for yourself, most likely in your own
company, and maybe with others working for you, but also maybe not. Just
being a one-person company doesn’t necessarily make you truly
independent, though. For example, are you a consultant or a
contractor?</p>
<p>While the client tells a contractor what to do, a consultant tells
the <em>client</em> what to do. The difference matters. A consultant is
independent; a contractor is not.</p>
<h3 id="the-pros-and-cons-of-independent-working">The pros and cons of
independent working</h3>
<p>Running your own business, or otherwise being an independent worker,
is great for those who like it. I do, and I was never really happy
working for someone else. I couldn’t wait to strike out on my own. I was
probably a pretty mediocre employee for that reason, among others. And
there was the little <em>nuclear</em> incident, of course.</p>
<p>On the other hand, not everyone wants the hassle of figuring out how
to market their business and pitch clients, or the headache of handling
accounts and taxes. And not everyone can manage on an irregular,
unpredictable income, especially if they have a family to support. You
don’t get vacations, insurance, or sick pay. On the plus side, you own a
business.</p>
<h3 id="when-its-time-to-leave">When it’s time to leave</h3>
<p>If you do want to swap your stable job and decent salary for the joys
of career independence, the transition needs a little careful planning.
It would be unwise, for example, to just quit one day in a fit of pique,
<em>then</em> start wondering how you’re going to make rent the first
few months. Timing is important.</p>
<blockquote>
<p>DENPOK: <em>Lao Tzu teaches: the best fighter is never angry. More
important than the blow is knowing <strong>when</strong> to strike.
Like, perhaps, after we experience the executive whitewater rafting trip
in Coeur d’Alene.</em><br>
—<a href="https://amzn.to/3uVthBs">“Silicon Valley”</a></p>
</blockquote>
<p>And to make money on your own, you’ll need to be <em>excellent</em>
at what you do. There’s no one else to pick up the slack. In a big
company, you can learn on the job. When you run your own company, you’d
better already know your trade.</p>
<h3 id="testing-the-waters">Testing the waters</h3>
<p>If you can make the time, it’s a good idea to dip your toe into
independent working by doing a few small, one-off side gigs. You’ll gain
experience and some satisfied clients, making it easier to go fully
independent when you want to. And if it turns out that you don’t enjoy
the experience of working for yourself, it’s better to find that out
<em>before</em> you rage-quit your job, isn’t it?</p>
<p>Of course, we don’t always leave our jobs through choice. Layoffs are
a fact of life in a volatile industry. You may not see the rocks coming;
companies tend to fail gradually, then suddenly. But an unexpected
transition to “funemployment” needn’t be a disaster. If you’ve always
dreamed of being independent but somehow never quite worked up the
courage to jump ship voluntarily, then being made to walk the plank
could be just the spur you need.</p>
<h2 id="making-the-choice">Making the choice</h2>
<p>I hope I’ve encouraged you to think about where your career is going,
where you <em>want</em> to go, and what you can do to get there. Of
course, you may not yet know how you want to spend the bulk of your
career. That’s okay, and completely normal.</p>
<p>But you can <em>think</em> about it, even if it’s too early to come
to any firm decisions. And you’re not limited to just <em>one</em> of
these potential destinations: many successful careers combine seniority,
management, and independence in some way.</p>
<p>For example, you could be a full-time or part-time manager, and also
run your own business on the side. Or you could achieve independence by
being a roving consultant within a large organisation, while still
technically being a senior engineer. Or you could combine all three by
being the chief executive <em>and</em> the director of engineering in
your own firm. There are many possibilities.</p>
<h3 id="figuring-out-who-you-are">Figuring out who you are</h3>
<p>You can inquire of yourself what things you value, and how your
working life could contribute to them. As your experience and knowledge
of the world grows, ideas may start to slot into place for you about
what you want to do and be.</p>
<blockquote>
<p><em>Tell me, what is it you plan to do</em><br>
<em>With your one wild and precious life?</em><br>
—Mary Oliver, <a href="https://amzn.to/3uVsUGY">“The Summer Day”</a></p>
</blockquote>
<p>Your future may not, in fact, lie in the tech industry. That’s all
right, too. I have more than one friend who, despite achieving
considerable success as an engineer, has decided that this isn’t really
what they want to do in the long term.</p>
<p>If you want to quit and be a doctor instead, or a schoolteacher, a <a href="https://xkcd.com/2124/">spaaaaaace</a> engineer, a woodworker, or
simply wander the world like a <a href="https://www.youtube.com/watch?v=B7YDcLP2DeY">badass</a> righting
wrongs, go to it. <a href="https://bitfieldconsulting.com/posts/time-lords">Don’t waste any more of
your one wild and precious life</a> careering down a blind alley.</p>
<h3 id="making-small-course-corrections">Making small course
corrections</h3>
<p>Once you do have a sense of where you want to go, it can help guide
your choices. Even if you don’t know exactly what your perfect job looks
like, you may start to feel that you won’t be truly happy until you’re
independent, or a senior IC, or a manager. You can steer away from
things that would limit your options in those areas, and instead seek
out companies, fields, or sectors where you’ll have the best chance of
achieving the career you want.</p>
<p>That’s not to say you should have a detailed map of every step that
you plan to take (“make junior VP by Q4 2035”). As engineers, we already
know that a too-rigid plan rarely survives contact with reality.
Instead, assume life will throw all kinds of crazy and unexpected things
at you. Plan to be flexible, and to change your plans.</p>
<h3 id="its-about-the-planning-not-the-plan">It’s about the planning,
not the plan</h3>
<p>You can’t stop the waves, as the saying goes, but you can <a href="https://bitfieldconsulting.com/posts/tao-of-go">learn to surf</a>. Chance favours the prepared
mind. Never underestimate the role of serendipity. The perfect
opportunity may show up just when you least expect it, but if you’ve
never thought about what you want, how will you recognise it?</p>
<p>The time to start planning for the end of your career is now. It’s
never too early, and it’s also never too late, provided, of course, that
you don’t have your own little <em>incident</em>. <a href="https://bitfieldconsulting.com/books/code">Let’s be careful out there</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Full LLM training and evaluation toolkit (241 pts)]]></title>
            <link>https://github.com/huggingface/smollm</link>
            <guid>42228472</guid>
            <pubDate>Sun, 24 Nov 2024 15:44:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/huggingface/smollm">https://github.com/huggingface/smollm</a>, See on <a href="https://news.ycombinator.com/item?id=42228472">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">SmolLM2</h2><a id="user-content-smollm2" aria-label="Permalink: SmolLM2" href="#smollm2"></a></p>
<p dir="auto">SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device.</p>
<p dir="auto">You can find our most capable model <strong>🤏 SmolLM2-1.7B-Instruct</strong> <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct" rel="nofollow">here</a>.</p>
<p dir="auto"><strong>New: Introducing <a href="https://huggingface.co/datasets/HuggingFaceTB/smoltalk" rel="nofollow">SmolTalk</a>, the SFT dataset of SmolLM2 🚀</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/919b15524c95e8c27a14584c356023d8a4e234a595ebbc841abdae2e111c9549/68747470733a2f2f63646e2d75706c6f6164732e68756767696e67666163652e636f2f70726f64756374696f6e2f75706c6f6164732f3631633134313334326161633736346365313635346534332f79343568494d4e52455737775f58704859425f30712e706e67"><img src="https://camo.githubusercontent.com/919b15524c95e8c27a14584c356023d8a4e234a595ebbc841abdae2e111c9549/68747470733a2f2f63646e2d75706c6f6164732e68756767696e67666163652e636f2f70726f64756374696f6e2f75706c6f6164732f3631633134313334326161633736346365313635346534332f79343568494d4e52455737775f58704859425f30712e706e67" alt="Evaluation Results" width="600" data-canonical-src="https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/y45hIMNREW7w_XpHYB_0q.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ol dir="auto">
<li><a href="#usage">Usage</a>
<ul dir="auto">
<li><a href="#transformers">Transformers</a></li>
<li><a href="#chat-in-trl">Chat in TRL</a></li>
<li><a href="#local-applications">Local applications</a></li>
</ul>
</li>
<li><a href="#smol-tools">Smol-tools</a></li>
<li><a href="#pre-training">Pre-training</a></li>
<li><a href="#fine-tuning">Fine-tuning</a></li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#synthetic-data-pipelines">Synthetic data pipelines</a></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Our most powerful model is <code>SmolLM2-1.7B-Instruct</code>, which you can use as an assistant with <code>transformers</code>, <code>trl</code>, or using quantized versions with tools like <code>llama.cpp</code>, <code>MLX</code>, and <code>transformers.js</code>. For lighter applications, you can also use the smaller models <code>SmolLM2-360M</code> and<code>SmolLM2-135M</code>, which are suitable for on-device usage and can be integrated similarly.
All available in this <a href="https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9" rel="nofollow">collection</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Transformers</h3><a id="user-content-transformers" aria-label="Permalink: Transformers" href="#transformers"></a></p>

<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoModelForCausalLM, AutoTokenizer
checkpoint = &quot;HuggingFaceTB/SmolLM2-1.7B-Instruct&quot;

device = &quot;cuda&quot; # for GPU usage or &quot;cpu&quot; for CPU usage
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=&quot;auto&quot;)`
model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)

messages = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Write a 100-word article on 'Benefits of Open-Source in AI research&quot;}]
input_text=tokenizer.apply_chat_template(messages, tokenize=False)
inputs = tokenizer.encode(input_text, return_tensors=&quot;pt&quot;).to(device)
outputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)
print(tokenizer.decode(outputs[0]))"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>
<span>checkpoint</span> <span>=</span> <span>"HuggingFaceTB/SmolLM2-1.7B-Instruct"</span>

<span>device</span> <span>=</span> <span>"cuda"</span> <span># for GPU usage or "cpu" for CPU usage</span>
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>checkpoint</span>)
<span># for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map="auto")`</span>
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>checkpoint</span>).<span>to</span>(<span>device</span>)

<span>messages</span> <span>=</span> [{<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"Write a 100-word article on 'Benefits of Open-Source in AI research"</span>}]
<span>input_text</span><span>=</span><span>tokenizer</span>.<span>apply_chat_template</span>(<span>messages</span>, <span>tokenize</span><span>=</span><span>False</span>)
<span>inputs</span> <span>=</span> <span>tokenizer</span>.<span>encode</span>(<span>input_text</span>, <span>return_tensors</span><span>=</span><span>"pt"</span>).<span>to</span>(<span>device</span>)
<span>outputs</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>inputs</span>, <span>max_new_tokens</span><span>=</span><span>50</span>, <span>temperature</span><span>=</span><span>0.2</span>, <span>top_p</span><span>=</span><span>0.9</span>, <span>do_sample</span><span>=</span><span>True</span>)
<span>print</span>(<span>tokenizer</span>.<span>decode</span>(<span>outputs</span>[<span>0</span>]))</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Chat in TRL</h3><a id="user-content-chat-in-trl" aria-label="Permalink: Chat in TRL" href="#chat-in-trl"></a></p>
<p dir="auto">You can also use the TRL CLI to chat with the model from the terminal:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install trl
trl chat --model_name_or_path HuggingFaceTB/SmolLM2-1.7B-Instruct --device cpu"><pre>pip install trl
trl chat --model_name_or_path HuggingFaceTB/SmolLM2-1.7B-Instruct --device cpu</pre></div>
<p dir="auto">You can find more details on how to leverage the model for use cases such as text summarization, text rewriting and function calling in the model card: <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct" rel="nofollow">https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Local applications</h3><a id="user-content-local-applications" aria-label="Permalink: Local applications" href="#local-applications"></a></p>
<p dir="auto">You can use the models locally with frameworks like <code>llama.cpp</code>, <code>MLX</code>, and <code>transformers.js</code>, which support SmolLM2.
All models are available in this <a href="https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9" rel="nofollow">collection</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Smol-tools</h2><a id="user-content-smol-tools" aria-label="Permalink: Smol-tools" href="#smol-tools"></a></p>
<p dir="auto">A collection of lightweight AI-powered tools built with LLaMA.cpp and small language models. These tools are designed to run locally on your machine without requiring expensive GPU resources.
Further instructions on how to use the tools can be found in the <a href="https://github.com/huggingface/smollm/blob/main/smol_tools/README.md">smol-tools README</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Pre-training</h2><a id="user-content-pre-training" aria-label="Permalink: Pre-training" href="#pre-training"></a></p>
<p dir="auto">You can find scripts for launching pre-training with <a href="https://github.com/huggingface/nanotron/">nanotron</a> under <a href="https://github.com/huggingface/smollm/blob/main/pre-training/README.md">pre-training</a>, we share the exact configs for training SmolLM1 and will upload SmolLM2's configs soon.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Fine-tuning</h2><a id="user-content-fine-tuning" aria-label="Permalink: Fine-tuning" href="#fine-tuning"></a></p>
<p dir="auto">You can find an example script to finetune SmolLM2 using <code>TRL</code> and <code>PEFT</code> in the <code>finetuning</code> folder. We also link to our post-training scripts for SmolLM2 using the alignement handbook.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Evaluation</h2><a id="user-content-evaluation" aria-label="Permalink: Evaluation" href="#evaluation"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9390426af7c12fcab4f192c7f8d3414e3d22451cfe9b8241dc2e6d7331d0b493/68747470733a2f2f63646e2d75706c6f6164732e68756767696e67666163652e636f2f70726f64756374696f6e2f75706c6f6164732f3631633134313334326161633736346365313635346534332f542d63484a564137464261493063674441707a456a2e706e67"><img src="https://camo.githubusercontent.com/9390426af7c12fcab4f192c7f8d3414e3d22451cfe9b8241dc2e6d7331d0b493/68747470733a2f2f63646e2d75706c6f6164732e68756767696e67666163652e636f2f70726f64756374696f6e2f75706c6f6164732f3631633134313334326161633736346365313635346534332f542d63484a564137464261493063674441707a456a2e706e67" alt="image/png" data-canonical-src="https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/T-cHJVA7FBaI0cgDApzEj.png"></a></p>
<p dir="auto">You can find more detailed evaluation of each model size in the model cards in this <a href="https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9" rel="nofollow">collection</a>.
We use <a href="https://github.com/huggingface/lighteval">lighteval</a> for all our evaluations, for more details refer to the <a href="https://github.com/huggingface/smollm/blob/main/evaluation/README.md">evaluation README</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Synthetic data pipelines</h2><a id="user-content-synthetic-data-pipelines" aria-label="Permalink: Synthetic data pipelines" href="#synthetic-data-pipelines"></a></p>
<p dir="auto">We released <a href="https://huggingface.co/datasets/HuggingFaceTB/smoltalk" rel="nofollow">SmolTalk</a> the SFT dataset used for building SmolLM2 instruct models. It was created with <a href="https://github.com/argilla-io/distilabel">distilabel</a> and you can check and execute the synthetic data pipelines in <a href="https://github.com/huggingface/smollm/blob/main/distilabel_pipelines/README.md">distilabel_pipelines README</a></p>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f7469fe83fb142b50df98d220431dc4c487f70ad946d2defeafa09be9e67b36d/68747470733a2f2f63646e2d75706c6f6164732e68756767696e67666163652e636f2f70726f64756374696f6e2f75706c6f6164732f3631633134313334326161633736346365313635346534332f4a4c5445626e7342515f71593033326d78467a67432e706e67"><img src="https://camo.githubusercontent.com/f7469fe83fb142b50df98d220431dc4c487f70ad946d2defeafa09be9e67b36d/68747470733a2f2f63646e2d75706c6f6164732e68756767696e67666163652e636f2f70726f64756374696f6e2f75706c6f6164732f3631633134313334326161633736346365313635346534332f4a4c5445626e7342515f71593033326d78467a67432e706e67" width="800" data-canonical-src="https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/JLTEbnsBQ_qY032mxFzgC.png"></a></p><p dir="auto"><em>Comparison of models finetuned on SmolTalk and Orca AgentInstruct 1M. For more details, refer to the <a href="https://huggingface.co/datasets/HuggingFaceTB/smoltalk" rel="nofollow">dataset card</a>.</em></p>
</div>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>