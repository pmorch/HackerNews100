<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 11 Jul 2023 15:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Claude 2 (139 pts)]]></title>
            <link>https://www.anthropic.com/index/claude-2</link>
            <guid>36680755</guid>
            <pubDate>Tue, 11 Jul 2023 13:35:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/index/claude-2">https://www.anthropic.com/index/claude-2</a>, See on <a href="https://news.ycombinator.com/item?id=36680755">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <main data-taxi="">
            
    			    		    		
	
        <article data-taxi-view="default" data-handle="postDetail">

            <div>

                <div>

                        <div>

                                                        
                                                        
                                

                            
                            

                            <p>

                                                                Jul 11, 2023

                                
                                                                        
                                        <span>●</span> 

                                        
                                                                                
                                                                                
                                            4 min
                                            
                                        
                                        read

                                    
                                
                            </p>

                            
                                
                                
                            
                        </div>

                        <div>
                            
                            <figure><img src="https://www-images.anthropic.com/production/images/Claude2_Blog_V1-1.png?w=2880&amp;h=1620&amp;auto=compress%2Cformat&amp;fit=crop&amp;dm=1688849489&amp;s=acaed1a9be1af257c7c9dc30c3c13838" alt="Claude 2"></figure>
<p>We are pleased to announce <a href="https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf">Claude 2</a>, our new model. Claude 2 has improved performance, longer responses, and can be accessed via API as well as a new public-facing beta website, <a href="https://claude.ai/" target="_blank" rel="noreferrer noopener">claude.ai</a>. We have heard from our users that Claude is easy to converse with, clearly explains its thinking, is less likely to produce harmful outputs, and has a longer memory. We have made improvements from our previous models on coding, math, and reasoning. For example, our latest model scored <strong>76.5%</strong> on the multiple choice section of the Bar exam, up from 73.0% with Claude 1.3. When compared to college students applying to graduate school, Claude 2 scores above the 90th percentile on the GRE reading and writing exams, and similarly to the median applicant on quantitative reasoning.</p>
<p>Think of Claude as a friendly, enthusiastic colleague or personal assistant who can be instructed in natural language to help you with many tasks. The Claude 2 API for businesses is being offered for the same price as Claude 1.3. Additionally, anyone in the US and UK can start using our <a href="https://claude.ai/" target="_blank" rel="noreferrer noopener">beta chat experience</a> today.</p>
<p>As we work to improve both the performance and safety of our models, we have increased the length of Claude’s input and output. Users can input up to&nbsp;<a href="https://www.anthropic.com/index/100k-context-windows" target="_blank" rel="noreferrer noopener">100K tokens</a> in each prompt, which means that Claude can work over hundreds of pages of technical documentation or even a book. Claude can now also write longer documents - from memos to letters to stories up to a few thousand tokens - all in one go.</p>

<figure><p><iframe src="https://player.vimeo.com/video/844014740?h=141c021b45&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&amp;title=0&amp;byline=0&amp;portrait=0" frameborder="0" title="Claude 2 | Long Input &amp; Output"></iframe></p> </figure>

<p>In addition, our latest model has greatly improved coding skills. Claude 2 scored a <strong>71.2%</strong> up from 56.0% on the <a href="https://github.com/openai/human-eval" target="_blank" rel="noreferrer noopener">Codex HumanEval</a>, a Python coding test. On GSM8k, a large set of grade-school math problems, Claude 2 scored <strong>88.0%</strong> up from 85.2%. We have an exciting roadmap of capability improvements planned for Claude 2 and will be slowly and iteratively deploying them in the coming months.</p>

<figure><p><iframe src="https://player.vimeo.com/video/844019370?h=d1e5de8aa0&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&amp;title=0&amp;byline=0&amp;portrait=0" frameborder="0" title="Claude 2 | Live Coding Data Visualization"></iframe></p> </figure>

<p>We've been iterating to improve the underlying safety of Claude 2, so that it is more harmless and harder to prompt to produce offensive or dangerous output. We have an internal red-teaming evaluation that scores our models on a large representative set of harmful prompts, using an automated test while we also regularly check the results manually. In this evaluation, Claude 2 was <strong>2x better</strong> at giving harmless responses compared to Claude 1.3. Although no model is immune from jailbreaks, we’ve used a variety of safety techniques (which you can read about <a href="https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback" target="_blank" rel="noreferrer noopener">here</a> and <a href="https://www.anthropic.com/index/the-capacity-for-moral-self-correction-in-large-language-models" target="_blank" rel="noreferrer noopener">here</a>), as well as <a href="https://www.anthropic.com/index/red-teaming-language-models-to-reduce-harms-methods-scaling-behaviors-and-lessons-learned" target="_blank" rel="noreferrer noopener">extensive red-teaming</a>, to improve its outputs.</p>
<p>Claude 2 powers our chat experience, and is generally available in the US and UK. We are working to make Claude more globally available in the coming months. You can now <a href="https://claude.ai/" target="_blank" rel="noreferrer noopener">create an account</a> and start talking to Claude in natural language, asking it for help with any tasks that you like. Talking to an AI assistant can take some trial and error, so <a href="https://console.anthropic.com/docs/prompt-design" target="_blank" rel="noreferrer noopener">read up on our tips</a> to get the most out of Claude.</p>
<p>We are also currently working with thousands of businesses who are using the Claude API. One of our partners is <strong>Jasper</strong>, a generative AI platform that enables individuals and teams to scale their content strategies. They found that Claude 2 was able to go head to head with other state of the art models for a wide variety of use cases, but has particular strength for long form low latency uses. "We are really happy to be among the first to offer Claude 2 to our customers, bringing enhanced semantics, up-to-date knowledge training, improved reasoning for complex prompts, and the ability to effortlessly remix existing content with a 3X larger context window," said Greg Larson, VP of Engineering at Jasper. "We are proud to help our customers stay ahead of the curve through partnerships like this one with Anthropic."</p>
<p><strong>Sourcegraph</strong> is a code AI platform that helps customers write, fix, and maintain code. Their <a href="https://www.youtube.com/watch?v=pRTTdlgDd8g" target="_blank" rel="noreferrer noopener">coding assistant Cody</a> uses Claude 2’s improved reasoning ability to give even more accurate answers to user queries while also passing along more codebase context with up to 100K context windows. In addition, Claude 2 was trained on more recent data, meaning it has knowledge of newer frameworks and libraries for Cody to pull from. “When it comes to AI coding, devs need fast and reliable access to context about their unique codebase and a powerful LLM with a large context window and strong general reasoning capabilities,” says Quinn Slack, CEO &amp; Co-founder of Sourcegraph. “The slowest and most frustrating parts of the dev workflow are becoming faster and more enjoyable. Thanks to Claude 2, Cody’s helping more devs build more software that pushes the world forward.”</p>
<p>We welcome your feedback as we work to responsibly deploy our products more broadly. Our chat experience is an open beta launch, and users should be aware that Claude – like all current models – can generate inappropriate responses. AI assistants are most useful in everyday situations, like serving to summarize or organize information, and should not be used where physical or mental health and well-being are involved. Please let us know if you’d like to talk to Claude in a <a href="https://docs.google.com/forms/d/1DlDkGmcxHYhuJ277R7x2g-ZqCDUWeLyL88IO0culKxE/edit?usp=drive_web" target="_blank" rel="noreferrer noopener">currently unsupported area</a>, or if you are a <a href="https://anthropic.com/earlyaccess" target="_blank" rel="noreferrer noopener">business who would like to start working with Claude</a>.</p>

                            

                        </div>

                    </div>

                
                
                                
                    




	




	<div>

				<p>
					<h3>Related</h3>
				</p>

				
			</div>


                
            </div>

        </article>

    
        </main>

        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PhotoPrism: Browse Your Life in Pictures (157 pts)]]></title>
            <link>https://github.com/photoprism/photoprism</link>
            <guid>36679368</guid>
            <pubDate>Tue, 11 Jul 2023 11:18:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/photoprism/photoprism">https://github.com/photoprism/photoprism</a>, See on <a href="https://news.ycombinator.com/item?id=36679368">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">PhotoPrism: Browse Your Life in Pictures</h2>
<p dir="auto"><a href="https://docs.photoprism.app/license/agpl/" rel="nofollow"><img src="https://camo.githubusercontent.com/91cf42a4ef5c3686cb1dfafaae6e4603ed5a8554325a6de54b5c7162a0c3ce4f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4147504c2d626c75652e737667" alt="License: AGPL" data-canonical-src="https://img.shields.io/badge/license-AGPL-blue.svg"></a>
<a href="https://www.photoprism.app/about/team" rel="nofollow"><img src="https://camo.githubusercontent.com/cf074a9310c3dfd11cf49b5b10ce639cef12daa0d4118ceece4262e93b008f65/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f70686f746f707269736d2f70686f746f707269736d2e737667" alt="GitHub contributors" data-canonical-src="https://img.shields.io/github/contributors/photoprism/photoprism.svg"></a>
<a href="https://docs.photoprism.app/" rel="nofollow"><img src="https://camo.githubusercontent.com/1efc9ca56e4f0730955703c40ea0e732e40ae1e32616ad49f854980d95fc699f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f726561642d746865253230646f63732d3461613038372e737667" alt="Documentation" data-canonical-src="https://img.shields.io/badge/read-the%20docs-4aa087.svg"></a>
<a href="https://link.photoprism.app/chat" rel="nofollow"><img src="https://camo.githubusercontent.com/aa75dfdb5fb7f254023c4f8ff450da5be0e218b91f6cf2a0cdfd6e9f1f50ce95/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6f6e2532306769747465722d3461613038372e737667" alt="Community Chat" data-canonical-src="https://img.shields.io/badge/chat-on%20gitter-4aa087.svg"></a>
<a href="https://link.photoprism.app/discussions" rel="nofollow"><img src="https://camo.githubusercontent.com/cf6348d81c02f9a25e45e05189b443baa82bf89c9336b96fdc613898fa39ba22/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61736b2d2532306f6e2532306769746875622d3464366139312e737667" alt="GitHub Discussions" data-canonical-src="https://img.shields.io/badge/ask-%20on%20github-4d6a91.svg"></a>
<a href="https://floss.social/@photoprism" rel="nofollow"><img src="https://camo.githubusercontent.com/d2c1bc5971701f4350ee252a74acfeb965443d4cc9a46a2bc97b9a32d622d976/68747470733a2f2f646c2e70686f746f707269736d2e6170702f696d672f6261646765732f62616467652d6d6173746f646f6e2e737667" alt="Mastodon" data-canonical-src="https://dl.photoprism.app/img/badges/badge-mastodon.svg"></a>
<a href="https://link.photoprism.app/twitter" rel="nofollow"><img src="https://camo.githubusercontent.com/d0a3db64808823a02c3c1fe0917c8b9f9dfcd0b5da2dc9eeff7855b71b18e8f4/68747470733a2f2f646c2e70686f746f707269736d2e6170702f696d672f6261646765732f62616467652d747769747465722e737667" alt="Twitter" data-canonical-src="https://dl.photoprism.app/img/badges/badge-twitter.svg"></a></p>
<p dir="auto">PhotoPrism® is an AI-Powered Photos App for the <a href="https://en.wikipedia.org/wiki/Decentralized_web" rel="nofollow">Decentralized Web</a>.
It makes use of the latest technologies to tag and find pictures automatically without getting in your way.
You can run it at home, on a private server, or in the cloud.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5e03a87e47aad26ad7248b8b43eac6471fe96f7b655ac2e532697692753c3ff8/68747470733a2f2f646c2e70686f746f707269736d2e6170702f696d672f75692f6465736b746f702d3130303070782e6a7067"><img src="https://camo.githubusercontent.com/5e03a87e47aad26ad7248b8b43eac6471fe96f7b655ac2e532697692753c3ff8/68747470733a2f2f646c2e70686f746f707269736d2e6170702f696d672f75692f6465736b746f702d3130303070782e6a7067" alt="" data-canonical-src="https://dl.photoprism.app/img/ui/desktop-1000px.jpg"></a></p>
<p dir="auto">To get a first impression, you are welcome to play with our <a href="https://try.photoprism.app/" rel="nofollow">public demo</a>. Be careful not to upload any private pictures.</p>
<h2 tabindex="-1" dir="auto">Feature Overview</h2>
<p dir="auto"><strong>Our mission is to provide the most user- and privacy-friendly solution to keep your pictures organized and accessible.</strong> That's why PhotoPrism was built from the ground up to run wherever you need it, without compromising freedom, privacy, or functionality:</p>
<ul dir="auto">
<li>Browse <a href="https://docs.photoprism.app/user-guide/organize/browse/" rel="nofollow">all your photos</a> and <a href="https://try.photoprism.app/library/videos" rel="nofollow">videos</a> without worrying about <a href="https://docs.photoprism.app/user-guide/settings/library/" rel="nofollow">RAW conversion, duplicates or video formats</a></li>
<li>Easily find specific pictures using <a href="https://try.photoprism.app/library/browse?view=cards&amp;q=flower%20color%3Ared" rel="nofollow">powerful search filters</a></li>
<li>Recognizes <a href="https://try.photoprism.app/library/people" rel="nofollow">the faces of your family and friends</a></li>
<li><a href="https://try.photoprism.app/library/labels" rel="nofollow">Automatic classification</a> of pictures based on their content and location</li>
<li><a href="https://try.photoprism.app/library/live" rel="nofollow">Play Live Photos</a> by hovering over them in <a href="https://try.photoprism.app/library/albums" rel="nofollow">albums</a> and <a href="https://try.photoprism.app/library/browse?view=cards&amp;q=type%3Alive" rel="nofollow">search results</a></li>
<li>Since the <a href="https://try.photoprism.app/" rel="nofollow">User Interface</a> is a <a href="https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps" rel="nofollow">Progressive Web App</a>,
it provides a native app-like experience, and you can conveniently install it on the home screen of all major operating systems and mobile devices</li>
<li>Includes four high-resolution <a href="https://try.photoprism.app/library/places" rel="nofollow">World Maps</a> to bring back the memories of your favorite trips</li>
<li>Metadata is extracted and merged from Exif, XMP, and other sources such as Google Photos</li>
<li>Many more image properties like <a href="https://try.photoprism.app/library/browse?view=cards&amp;q=color:red" rel="nofollow">Colors</a>, <a href="https://try.photoprism.app/library/browse?view=cards&amp;q=mono%3Atrue" rel="nofollow">Chroma</a>, and <a href="https://try.photoprism.app/library/review" rel="nofollow">Quality</a> can be searched as well</li>
<li>Use <a href="https://link.photoprism.app/photosync" rel="nofollow">PhotoSync</a> to securely backup iOS and Android phones in the background</li>
<li>WebDAV clients such as Microsoft's Windows Explorer and Apple's Finder <a href="https://docs.photoprism.app/user-guide/sync/webdav/" rel="nofollow">can connect directly</a> to PhotoPrism, allowing you to open, edit, and delete files from your computer as if they were local</li>
</ul>
<p dir="auto">Being completely <a href="https://link.photoprism.app/membership" rel="nofollow"><strong>self-funded and independent</strong></a>, we can promise you that we will <a href="https://www.photoprism.app/privacy" rel="nofollow">never sell your data</a> and that we will <a href="https://www.photoprism.app/terms" rel="nofollow">always be transparent</a> about our software and services. Your data will never be shared with Google, Amazon, Microsoft or Apple unless you intentionally upload files to one of their services. <g-emoji alias="lock" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f512.png">🔒</g-emoji></p>
<h2 tabindex="-1" dir="auto">Getting Started</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bf392858e9357b5ffbc941d30fc21923a0729f45f5960e83eebbabf85de140ef/68747470733a2f2f7777772e70686f746f707269736d2e6170702f757365722f70616765732f30312e686f6d652f30332e5f73637265656e73686f74732f6970686f6e652d6d6170732d6879627269642d35343070782e706e67"><img width="25%" src="https://camo.githubusercontent.com/bf392858e9357b5ffbc941d30fc21923a0729f45f5960e83eebbabf85de140ef/68747470733a2f2f7777772e70686f746f707269736d2e6170702f757365722f70616765732f30312e686f6d652f30332e5f73637265656e73686f74732f6970686f6e652d6d6170732d6879627269642d35343070782e706e67" data-canonical-src="https://www.photoprism.app/user/pages/01.home/03._screenshots/iphone-maps-hybrid-540px.png"></a></p>
<p dir="auto">Step-by-step installation instructions for our self-hosted <a href="https://www.photoprism.app/get" rel="nofollow">community edition</a> can be found
on <a href="https://docs.photoprism.app/getting-started/" rel="nofollow">docs.photoprism.app</a> -
all you need is a Web browser and <a href="https://docs.docker.com/get-docker/" rel="nofollow">Docker</a> to run the server.
It is available for Mac, Linux, and Windows.</p>
<p dir="auto">The <a href="https://docs.photoprism.app/release-notes/" rel="nofollow">stable version</a> and development
preview have been built into a single <a href="https://link.photoprism.app/docker-hub" rel="nofollow">multi-arch image</a> for 64-bit AMD, Intel,
and ARM processors. That means, <a href="https://docs.photoprism.app/getting-started/raspberry-pi/" rel="nofollow">Raspberry Pi</a> 3 / 4 owners can pull
from the same repository, enjoy the exact same functionality, and can follow the regular
<a href="https://docs.photoprism.app/getting-started/docker-compose/" rel="nofollow">installation instructions</a>
after going through a short list of <a href="https://docs.photoprism.app/getting-started/raspberry-pi/" rel="nofollow">requirements</a>.</p>
<p dir="auto">Existing users are advised to update their <code>docker-compose.yml</code> config based on our examples
available at <a href="https://dl.photoprism.app/docker/" rel="nofollow">dl.photoprism.app/docker</a>.</p>
<h2 tabindex="-1" dir="auto">Support Our Mission <g-emoji alias="gem" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f48e.png">💎</g-emoji></h2>
<p dir="auto"><strong>PhotoPrism is 100% self-funded and independent.</strong> Your <a href="https://link.photoprism.app/membership" rel="nofollow">continued support</a> helps us <a href="https://www.photoprism.app/oss/faq#what-functionality-is-generally-available" rel="nofollow">provide more features to the public</a>, release <a href="https://docs.photoprism.app/release-notes/" rel="nofollow">regular updates</a>, and remain independent!</p>
<p dir="auto">Our members <a href="https://www.photoprism.app/kb/personal" rel="nofollow">enjoy additional features</a>, including access to <a href="https://try.photoprism.app/library/places" rel="nofollow">interactive world maps</a>, and can join our private chat room to <a href="https://www.photoprism.app/about/team" rel="nofollow">connect with our team</a>. We currently have the following membership options:</p>
<ul dir="auto">
<li>You can <a href="https://link.photoprism.app/membership" rel="nofollow">sign up directly on our website</a> and pay with credit card or SEPA through Stripe, so you don't need to <a href="https://www.photoprism.app/kb/activation" rel="nofollow">link an external account</a> and can easily upgrade or downgrade at any time</li>
<li>Alternatively, <a href="https://link.photoprism.app/patreon" rel="nofollow">Patreon</a> also supports PayPal, additional currencies, and lets you choose between monthly and annual billing for all tiers</li>
</ul>
<p dir="auto">If you currently support us through <a href="https://link.photoprism.app/sponsor" rel="nofollow">GitHub Sponsors</a>, you can also <a href="https://my.photoprism.app/register" rel="nofollow">register on our website</a> and use the <em>Activate GitHub Sponsors Membership</em> button to link your account. For details on this and how to <a href="https://www.patreon.com/pledges" rel="nofollow">link your Patreon account</a>, see our <a href="https://www.photoprism.app/kb/activation" rel="nofollow">Activation Guide</a>.</p>
<p dir="auto">You are <a href="https://www.photoprism.app/contact" rel="nofollow">welcome to contact us</a> for change requests, membership questions, and business partnerships.</p>
<p dir="auto"><a href="https://www.photoprism.app/kb/membership" rel="nofollow">View Membership FAQ ›</a> <a href="https://link.photoprism.app/membership" rel="nofollow">Sign Up ›</a></p>
<h3 tabindex="-1" dir="auto">Why Your Support Matters</h3>
<ul dir="auto">
<li>Your continued support helps us provide regular updates and remain independent, so we can fulfill our mission and protect your privacy</li>
<li>Sustained funding is key to quickly releasing new features requested by you and other community members</li>
<li>Being self-funded and independent, we can personally promise you that we will never sell your data and that we will always be transparent about our software and services</li>
</ul>
<p dir="auto">Please also leave <a href="https://github.com/photoprism/photoprism/stargazers">a star</a> on GitHub if you like this project. It provides additional motivation to keep going.</p>
<p dir="auto"><strong>A big thank you to all current and past sponsors, whose generous support has been and continues to be essential to the success of the project!</strong></p>
<p dir="auto"><a href="https://github.com/photoprism/photoprism/blob/develop/SPONSORS.md">View Sponsors ›</a> <a href="https://docs.photoprism.app/credits/" rel="nofollow">View Credits ›</a></p>
<h2 tabindex="-1" dir="auto">Getting Support</h2>
<p dir="auto">Visit <a href="https://docs.photoprism.app/user-guide/" rel="nofollow">docs.photoprism.app/user-guide</a> to learn how to <a href="https://docs.photoprism.app/user-guide/sync/webdav/" rel="nofollow">sync</a>, <a href="https://docs.photoprism.app/user-guide/library/" rel="nofollow">organize</a>, and <a href="https://docs.photoprism.app/user-guide/share/" rel="nofollow">share</a> your pictures. If you need help installing our software at home, you are welcome to post your question in <a href="https://link.photoprism.app/discussions" rel="nofollow">GitHub Discussions</a> or ask in our <a href="https://link.photoprism.app/chat" rel="nofollow">Community Chat</a>.
Common problems can be quickly diagnosed and solved using our <a href="https://docs.photoprism.app/getting-started/troubleshooting/" rel="nofollow">Troubleshooting Checklists</a>. Eligible <a href="https://link.photoprism.app/membership" rel="nofollow">members</a> are also welcome to email us for technical support and advice.</p>
<h2 tabindex="-1" dir="auto">Upcoming Features and Enhancements</h2>
<p dir="auto">Our <a href="https://link.photoprism.app/roadmap" rel="nofollow">Project Roadmap</a> shows what tasks are in progress and what features will be implemented next. You are invited to give ideas you like a thumbs-up, so we know what's most popular.</p>
<p dir="auto">Be aware that we have a zero-bug policy and do our best to help users when they need support or have other questions. This comes at a price though, as we can't give exact release dates for new features. Our team receives many more requests than can be implemented, so we want to emphasize that we are in no way obligated to implement the features, enhancements, or other changes you request. We do, however, appreciate your feedback and carefully consider all requests.</p>
<p dir="auto"><strong>Because sustained funding is key to quickly releasing new features, we encourage you to support our mission by <a href="https://link.photoprism.app/sponsor" rel="nofollow">signing up as a sponsor</a> or purchasing a <a href="https://www.photoprism.app/teams" rel="nofollow">commercial license</a>. Ultimately, that's what's best for the product and the community.</strong></p>
<h2 tabindex="-1" dir="auto">GitHub Issues <g-emoji alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png">⚠️</g-emoji></h2>
<p dir="auto">We kindly ask you not to report bugs via GitHub Issues <strong>unless you are certain to have found a fully reproducible and previously unreported issue</strong> that must be fixed directly in the app. Thank you for your careful consideration!</p>
<ul dir="auto">
<li>When reporting a problem, always include the software versions you are using and other information about your environment such as <a href="https://docs.photoprism.app/getting-started/troubleshooting/browsers/" rel="nofollow">browser, browser plugins</a>, operating system, <a href="https://docs.photoprism.app/getting-started/troubleshooting/performance/#storage" rel="nofollow">storage type</a>, <a href="https://docs.photoprism.app/getting-started/troubleshooting/performance/#memory" rel="nofollow">memory size</a>, and <a href="https://docs.photoprism.app/getting-started/troubleshooting/performance/#server-cpu" rel="nofollow">processor</a></li>
<li>Note that all issue <strong>subscribers receive an email notification</strong> from GitHub whenever a new comment is added, so these should only be used for sharing important information and not for discussions, questions or expressing personal opinions</li>
<li><a href="https://www.photoprism.app/contact" rel="nofollow">Contact us</a> or <a href="https://link.photoprism.app/discussions" rel="nofollow">a community member</a> if you need help, it could be a local configuration problem, or a misunderstanding in how the software works</li>
<li>This gives our team the opportunity to <a href="https://docs.photoprism.app/getting-started/troubleshooting/" rel="nofollow">improve the docs</a> and provide best-in-class support to you, instead of handling unclear/duplicate bug reports or triggering a flood of notifications by responding to comments</li>
</ul>
<h2 tabindex="-1" dir="auto">Connect with the Community</h2>
<p dir="auto">Follow us on <a href="https://link.photoprism.app/twitter" rel="nofollow">Twitter</a> and join the <a href="https://link.photoprism.app/chat" rel="nofollow">Community Chat</a>
to get regular updates, connect with other users, and discuss your ideas. Our <a href="https://www.photoprism.app/code-of-conduct" rel="nofollow">Code of Conduct</a> explains the "dos and don’ts" when interacting with other community members.</p>
<p dir="auto">Feel free to contact us at <a href="mailto:hello@photoprism.app">hello@photoprism.app</a> with anything that is on your mind. We appreciate your feedback! Due to the high volume of emails we receive, our team may be unable to get back to you immediately. We do our best to respond within five business days or less.</p>
<h2 tabindex="-1" dir="auto">Every Contribution Makes a Difference</h2>
<p dir="auto">We welcome <a href="https://github.com/photoprism/photoprism/blob/develop/CONTRIBUTING.md">contributions</a> of any kind, including blog posts, tutorials, testing, writing documentation, and pull requests. Our <a href="https://docs.photoprism.app/developer-guide/" rel="nofollow">Developer Guide</a> contains all the information necessary for you to get started.</p>
<hr>
<p dir="auto"><em>PhotoPrism® is a <a href="https://www.photoprism.app/trademark" rel="nofollow">registered trademark</a>. By using the software and services we provide, you agree to our <a href="https://www.photoprism.app/terms" rel="nofollow">Terms of Service</a>, <a href="https://www.photoprism.app/privacy" rel="nofollow">Privacy Policy</a>, and <a href="https://www.photoprism.app/code-of-conduct" rel="nofollow">Code of Conduct</a>. Docs are <a href="https://link.photoprism.app/github-docs" rel="nofollow">available</a> under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nofollow">CC BY-NC-SA 4.0 License</a>; <a href="https://github.com/photoprism/photoprism/blob/develop/assets/README.md">additional terms</a> may apply.</em></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Big Tech can transfer Europeans’ data to US in win for Facebook and Google (107 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/07/big-tech-can-transfer-europeans-data-to-us-in-win-for-facebook-and-google/</link>
            <guid>36678834</guid>
            <pubDate>Tue, 11 Jul 2023 10:14:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/07/big-tech-can-transfer-europeans-data-to-us-in-win-for-facebook-and-google/">https://arstechnica.com/tech-policy/2023/07/big-tech-can-transfer-europeans-data-to-us-in-win-for-facebook-and-google/</a>, See on <a href="https://news.ycombinator.com/item?id=36678834">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      Intercontinental data flows    —
</h4>
            
            <h2 itemprop="description">EU-US data pact approved; privacy advocates to appeal because of US surveillance.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/07/getty-us-eu-flags-data-800x450.jpg" alt="Illustration of European and US flags superimposed with ones and zeroes to represent data.">
      <figcaption><p>Getty Images | BeeBright</p></figcaption>  </figure>

  




<!-- cache hit 21:single/related:ffa4b76ed636b6cf119e8ea5bd4eeefc --><!-- empty -->
<p>The European Commission today decided it is safe for personal data to be transferred from the European Union to US-based companies, handing a victory to firms like Facebook and Google despite protests from privacy advocates who worry about US government surveillance.</p>
<p>The <a href="https://ec.europa.eu/commission/presscorner/detail/en/ip_23_3721">commission announced</a> that it "adopted its adequacy decision for the <a href="https://commission.europa.eu/document/fa09cbad-dd7d-4684-ae60-be03fcb0fddf_en">EU-US Data Privacy Framework</a>," concluding "that the United States ensures an adequate level of protection—comparable to that of the European Union—for personal data transferred from the EU to US companies under the new framework. On the basis of the new adequacy decision, personal data can flow safely from the EU to US companies participating in the Framework, without having to put in place additional data protection safeguards."</p>
<p>In May, Facebook-owner Meta was fined 1.2 billion euros for violating the General Data Protection Regulation (GDPR) with transfers of personal data to the United States and was <a href="https://arstechnica.com/tech-policy/2023/05/facebook-ordered-to-pay-e1-2-billion-fine-and-stop-storing-eu-user-data-in-us/">ordered</a> to stop storing European Union user data in the US within six months. But Meta said at the time that if the pending data-transfer pact "comes into effect before the implementation deadlines expire, our services can continue as they do today without any disruption or impact on users."</p>
<p>The data-transfer deal "is expected to face a legal challenge from European privacy advocates, who have long said that the US needs to make substantial changes to surveillance laws," a <a href="https://www.wsj.com/articles/eu-approves-data-transfer-deal-with-u-s-averting-potential-halt-in-flows-7a149c9">Wall Street Journal report</a> said today. "Transfers of data from Europe to the US have been in question since an EU court ruled in 2020 that a previous deal allowing trans-Atlantic data flows was illegal because the US didn't give EU individuals an effective way to challenge surveillance of their data by the US government."</p>
<h2>US to monitor compliance</h2>
<p>The EC's announcement said the new framework has "binding safeguards to address all the concerns raised by the European Court of Justice, including limiting access to EU data by US intelligence services to what is necessary and proportionate, and establishing a Data Protection Review Court (DPRC), to which EU individuals will have access." The new court "will be able to order the deletion" of data that is found to have been collected in violation of the new rules.</p>                                            
                                                        
<p>The framework will be administered and monitored by the US Department of Commerce and the "US Federal Trade Commission will enforce US companies' compliance," the EC announcement said. EU residents who challenge data collection will have free access to "independent dispute resolution mechanisms and an arbitration panel."</p>
<p>The US and EC <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2022/03/25/fact-sheet-united-states-and-european-commission-announce-trans-atlantic-data-privacy-framework/">agreed</a> on the data privacy framework in March 2022, and the US <a href="https://www.commerce.gov/news/press-releases/2023/07/statement-us-secretary-commerce-gina-raimondo-european-union-us-data">announced last week</a> that it has fulfilled its commitments for implementing the deal.</p>
<p>Google supported the agreement, <a href="https://blog.google/outreach-initiatives/public-policy/trans-atlantic-data-privacy-framework-building-long-term/">writing last year</a> that the "US government has now committed to systems that will enable independent and meaningful redress for people in the EU, strengthen the guardrails and proportionality of US intelligence collection, and ensure effective oversight of these new privacy and civil liberties standards in ways that address the concerns articulated by the Court of Justice of the European Union." Google also said the agreement provides "a reliable and durable foundation for the future of Internet services on both sides of the Atlantic."</p>
<p>US companies can join the EU-US framework "by committing to comply with a detailed set of privacy obligations, for instance the requirement to delete personal data when it is no longer necessary for the purpose for which it was collected, and to ensure continuity of protection when personal data is shared with third parties," the European Commission said.</p>
<h2>Privacy activist plans appeal</h2>
<p>Previous data agreements known as Safe Harbor and Privacy Shield were struck down by European courts, the WSJ noted. "Max Schrems, an Austrian lawyer and privacy activist who led the legal challenges to the earlier agreements, said he plans to challenge the latest deal, too," the report said.</p>
<p>"We would need changes in US surveillance law to make this work and we simply don't have it," Schrems was quoted as saying.</p>
<p>European Parliament member Birgit Sippel, who is in Germany's Social Democratic Party, said the "framework does not provide any meaningful safeguards against indiscriminate surveillance conducted by US intelligence agencies," <a href="https://www.nytimes.com/2023/07/10/technology/us-eu-data-privacy-deal.html">according to The New York Times</a>.</p>
<p>The EC approval of the deal was lauded by the Computer &amp; Communications Industry Association, which represents Amazon, Apple, eBay, Google, Meta, Twitter, and other tech companies. "Today's decision means that EU and US businesses will soon have full legal certainty again to transfer personal data across the Atlantic... Data flows are vital to transatlantic trade and the EU-US economic relationship, which is worth €5.5 trillion per year. Nevertheless, the two economies had been left without guidelines for data transfers after an EU Court ruling invalidated the previous framework back in 2020," the group <a href="https://ccianet.org/news/2023/07/eu-countries-seal-data-transfer-deal-with-united-states-after-years-of-uncertainty/">said</a>.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Privatisation has been a costly failure in Britain (407 pts)]]></title>
            <link>https://www.economist.com/by-invitation/2023/07/10/mathew-lawrence-on-why-privatisation-has-been-a-costly-failure-in-britain</link>
            <guid>36678375</guid>
            <pubDate>Tue, 11 Jul 2023 09:20:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/by-invitation/2023/07/10/mathew-lawrence-on-why-privatisation-has-been-a-costly-failure-in-britain">https://www.economist.com/by-invitation/2023/07/10/mathew-lawrence-on-why-privatisation-has-been-a-costly-failure-in-britain</a>, See on <a href="https://news.ycombinator.com/item?id=36678375">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><h2>The energy transition further strengthens the argument for state ownership, says the think-tank head</h2></section><div data-body-id="cp1"><p data-caps="initial"><span data-caps="initial">T</span><small>HE BRITISH</small> economy has been subject to a giant experiment: privatisation on a scale more extensive than in almost any other <small>OECD</small> country. Perhaps most strikingly, following the lead of Augusto Pinochet’s Chile, in 1989 the Conservative government privatised the water industry in England and Wales. This outlier status remains to this day: the majority of water infrastructure in other countries is held and managed by the public. To see the disastrous effects of this experiment, one need only look at England’s crisis-ridden water companies—or brave a swim in an English river flooded with sewage.</p><p>Emblematic of these failures is Thames Water, England’s largest water company. Having accumulated debts of £14bn ($18bn), in part the legacy of a leveraged buy-out by Macquarie, an Australian investment group, it is now precariously exposed to higher interest rates. But the problem extends beyond one company. </p><p>At the time of privatisation, the water industry as a whole carried no debt, partly because £15bn had been paid off by the government beforehand. Since then the sector has accumulated £53bn in debt while distributing £72bn to shareholders, the majority of whom are international investors. English water-bill payers, who have no choice but to use their local monopoly provider, are essentially taxed without representation for the benefit of using this essential service. </p><p>The practice of loading a company with debt to distribute cash to shareholders and managers is especially pronounced among private-equity owners in the water industry. However, the tension between generating strong returns for investors in uncompetitive monopoly conditions and providing high-quality, affordable infrastructure to the public is as real for publicly listed companies as it is for those owned by private equity. </p><p>The effects of this tension are clear. For example, South East Water—thousands of whose customers were left without running water this summer—spent more on dividends and servicing its debt than on infrastructure in the two years to March 2022. Water bills for Britain as a whole have increased by around 360%, more than double the rate of inflation, since privatisation. Over that time, annual capital investment by the ten largest water and sewage companies has fallen by some 15%, according to research by the <i>Financial Times </i>(<i>FT</i>). Bills are lower in Scotland, whose water company is government-owned, even though it has invested 35% more per household per year than English firms.</p><p>The case for privatisation rested on two claims, reiterated by Michael Howard, <a href="https://www.economist.com/by-invitation/2023/07/06/thames-water-may-be-troubled-but-privatisation-has-served-britain-well-argues-michael-howard">writing recently</a> for <i>The Economist</i>. First, the bracing force of competition would improve utility management and hence overall outcomes. Second, it would unlock access to private capital to fund much-needed investment that would otherwise be held back by short-termism and scarcity—supposedly endemic to public ownership. </p><p>Neither claim stands up to scrutiny. Private management and weak regulation have delivered a crisis-ridden sector and chronic underinvestment. Water companies have made clear that investment will be funded mainly by increased bills, undermining the idea that private capital is essential for investment.</p><p>Dogmatic adherence to privatisation in the face of its sustained failure suggests ideology, not pragmatism, was the motivation. Whether key infrastructure is publicly or privately owned, investment in its construction and maintenance will always ultimately be borne by taxpayers, bondholders and water customers. The question is whether we want to add to those costs the weight of shareholder payouts and the extra interest that private owners have to pay over sovereign borrowers. If your answer is, like mine, no, then the solution is obvious: the public ownership that prevails in most other water systems around the world. </p><p>Rather than defending a failing status quo, a better approach would be to use existing law to hold responsible those who have wrung the industry dry, and then to change the law’s purpose and water companies’ governance and ownership to put the public interest first—treating water as a human right and organising water networks as a critical public service. </p><p>The case for public ownership should rest on a sober assessment of how best to deliver fundamental public goods and services. Public ownership—which can operate at multiple levels, from the national to the municipal—is clearly not appropriate for every sector. But it allows the pursuit of a broad range of objectives beyond profit-maximisation. </p><p>If the argument for public ownership is strong in water, it is even more powerful when it comes to the biggest socio-economic challenge confronting humanity: the energy transition. In the span of a decade we must undertake an unprecedented investment and divestment sprint to deliver a clean electricity system, which private ownership and market co-ordination are ill-equipped to deliver. As Derek Brower put it in a recent column in the <i>FT</i>: “The sheer scale of the physical infrastructure that must be revamped, demolished or replaced is almost beyond comprehension. Governments, not BlackRock, will have to lead this new Marshall Plan.” </p><p>From bringing new renewable-energy generation online, to dramatically expanding a backlogged grid and storage, to phasing out fossil-fuel assets, the discrete elements of grid decarbonisation aren’t discrete at all: they must be synchronised to limit physical bottlenecks and inflationary pressures. Private investment, hindered by the profit imperative and, in many quarters, a preference for liquid assets over longer-term capital investments, cannot deliver a smooth wholesale transformation of the electricity system. Public ownership offers greater affordability, more effective co-ordination and greater democratic oversight. The argument for it is built on necessity, not nostalgia. </p><p>In his recent book, “The Death of Consensus”, Phil Tinline traced how the power of political nightmares has driven the transformation of Britain’s economy. The case for private ownership drew strength from a spectre that, advocates argued, haunted British life: sclerotic service under nationalised utilities that stood as both cause and symptom of wider national decline. Yet the age of privatisation has now produced its own nightmare: of raw sewage gushing into rivers as international shareholders profit, utility bills rising relentlessly as infrastructure creaks and, now, the potential collapse of leading water and energy companies under the weight of their own fragile, extractive business models. It is time we woke up and exorcised the ghost.<span data-ornament="ufinish">■</span></p><p><i>Mathew Lawrence is the founder and director of Common Wealth, a think-tank, and the co-author of “Owning the Future” (2022). </i></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SUSE is forking RHEL (393 pts)]]></title>
            <link>https://www.suse.com/news/SUSE-Preserves-Choice-in-Enterprise-Linux/</link>
            <guid>36678079</guid>
            <pubDate>Tue, 11 Jul 2023 08:41:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.suse.com/news/SUSE-Preserves-Choice-in-Enterprise-Linux/">https://www.suse.com/news/SUSE-Preserves-Choice-in-Enterprise-Linux/</a>, See on <a href="https://news.ycombinator.com/item?id=36678079">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div>
<h4>LUXEMBOURG</h4>

<p>Today <a href="http://www.suse.com/">SUSE</a>, the company behind Rancher, NeuVector, and SUSE Linux Enterprise (SLE) and a global leader in enterprise open source solutions, announced it is forking publicly available Red Hat Enterprise Linux (RHEL) and will develop and maintain a RHEL-compatible distribution available to all without restrictions. Over the next few years, SUSE plans to invest more than $10 million into this project.&nbsp;&nbsp;</p>
</div>

<p>Dirk-Peter van Leeuwen, CEO of SUSE, said, “For decades, collaboration and shared success have been the building blocks of our open source community. We have a responsibility to defend these values. This investment will preserve the flow of innovation for years to come and ensures that customers and community alike are not subjected to vendor lock-in and have genuine choice tomorrow as well as today.”&nbsp;<span>&nbsp;</span></p>

<p>SUSE remains fully committed to investing in its highly regarded Linux solutions such as SLE and openSUSE that countless satisfied enterprise customers and the community rely on. At the same time, it acknowledges that enterprises and the open source community deserve choice and freedom from vendor lock-in. SUSE has a long history in empowering and supporting users with mixed Linux environments.&nbsp;&nbsp;</p>

<p>SUSE is committed to working with the open source community to develop a long-term, enduring compatible alternative for RHEL and CentOS users. SUSE plans to contribute this project to an open source foundation, which will provide ongoing free access to alternative source code.&nbsp;&nbsp;</p>

<p>“This collaborative effort demonstrates SUSE’s deep-rooted commitment to fostering innovation and nurturing community-driven development, and it reinforces the fundamental values of open source software. We invite the community to actively engage and collaborate in shaping the future of this essential software,” said Dr. Thomas Di Giacomo, Chief Technology and Product Officer, SUSE. “We firmly believe this new RHEL-compatible Linux distribution, together with SUSE’s portfolio, will help the community and customers navigate unprecedented advancements in enterprise Linux, cloud computing, containerization, edge, AI/ML and other emerging technologies.”&nbsp;&nbsp;</p>

<p>“The enterprise Linux community requires standardization, stability, and consistency,” said Gregory Kurtzer, CEO of <a href="https://www.ciq.com/?utm_source=web&amp;utm_medium=press-release&amp;utm_campaign=suse&amp;utm_content=rhel-fork">CIQ</a> and Founder of <a href="https://www.rockylinux.org/">Rocky Linux</a>. “CIQ is bringing stability to our partners, customers, and community, by building a broad coalition of like-minded companies, organizations, and individuals. SUSE has embodied the core principles and spirit of open source; CIQ is thrilled to collaborate with SUSE on advancing an open enterprise Linux standard.”&nbsp;&nbsp;</p>

<div>
<p>To read more about how this strengthens Liberty Linux and how to get involved, go <a href="https://www.suse.com/c/at-suse-we-make-choice-happen/">here</a>.&nbsp;&nbsp;&nbsp;</p>



<p><strong>About SUSE&nbsp;</strong></p>
</div>

<p>SUSE is a global leader in innovative, reliable and secure enterprise-grade open source solutions, relied upon by more than 60% of the Fortune 500 to power their mission-critical workloads. We specialize in Business-critical Linux, Enterprise Container Management and Edge solutions, and collaborate with partners and communities to empower our customers to innovate everywhere – from the data center to the cloud, to the edge and beyond.&nbsp;</p>

<p>SUSE puts the “open” back in open source, giving customers the ability to tackle innovation challenges today and the freedom to evolve their strategy and solutions tomorrow. The company employs more than 2,000 people globally. SUSE is listed on the Frankfurt Stock Exchange.&nbsp;</p>

<div>


<p><strong>Forward-Looking Statements&nbsp;</strong></p>
</div>

<p>Any statements in this press release about future expectations, plans and prospects for the company, including statements containing the words “aims,” “targets,” “will,” “believes,” “anticipates,” “plans,” “expects,” and similar expressions, may constitute forward-looking statements and should be read with caution. Actual results may differ materially from those indicated by such forward-looking statements as a result of various important factors, including competitive landscape, development of customer deals, reliance upon customer relationships, management of growth and acquisitions, the possibility of undetected software issues, the risks of impacts of the Covid-19 pandemic and economic downturns, pricing pressures and the viability of the Internet. In addition, any forward-looking statements included herein represent views as of the date of this press release and these views could change. The Company does not have any obligation to update its forward-looking statements. These forward-looking statements are subject to change and should not be relied upon as representing the Company’s views as of any date other than the date of this press release.&nbsp;</p>

<div>
<p><img alt="RHEL Response " data-align="center" data-entity-type="" data-entity-uuid="" height="306" src="https://links.imagerelay.com/cdn/3404/ql/0211f0a7329140f5a410e96767dec868/AdobeStock_156220876.jpeg"></p>

<p>Copyright 2023 SUSE LLC. All rights reserved. SUSE and the SUSE logo are registered trademarks of SUSE LLC in the United States and other countries. All third-party trademarks are the property of their respective owners.&nbsp;</p>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-Prompt-Engineer (248 pts)]]></title>
            <link>https://github.com/mshumer/gpt-prompt-engineer</link>
            <guid>36677034</guid>
            <pubDate>Tue, 11 Jul 2023 06:16:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mshumer/gpt-prompt-engineer">https://github.com/mshumer/gpt-prompt-engineer</a>, See on <a href="https://news.ycombinator.com/item?id=36677034">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">gpt-prompt-engineer</h2>
<p dir="auto"><a href="https://twitter.com/mattshumer_" rel="nofollow"><img src="https://camo.githubusercontent.com/3f2ef3da38c79ef4c01ff068c1062e0c06200161a767b4015ce0372de4180d96/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f6d6174747368756d65725f3f7374796c653d736f6369616c" alt="Twitter Follow" data-canonical-src="https://img.shields.io/twitter/follow/mattshumer_?style=social"></a> <a href="https://colab.research.google.com/github/mshumer/gpt-prompt-engineer/blob/main/gpt_prompt_engineer.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open Main Version In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a> <a href="https://colab.research.google.com/drive/16NLMjqyuUWxcokE_NF6RwHD8grwEeoaJ?usp=sharing" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open Classification Version In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></p>
<h2 tabindex="-1" dir="auto">Overview</h2>
<p dir="auto">Prompt engineering is kind of like alchemy. There's no clear way to predict what will work best. It's all about experimenting until you find the right prompt. <code>gpt-prompt-engineer</code> is a tool that takes this experimentation to a whole new level.</p>
<p dir="auto"><strong>Simply input a description of your task and some test cases, and the system will generate, test, and rank a multitude of prompts to find the ones that perform the best.</strong></p>
<h2 tabindex="-1" dir="auto">Features</h2>
<ul dir="auto">
<li>
<p dir="auto"><strong>Prompt Generation</strong>: Using GPT-4 and GPT-3.5-Turbo, <code>gpt-prompt-engineer</code> can generate a variety of possible prompts based on a provided use-case and test cases.</p>
</li>
<li>
<p dir="auto"><strong>Prompt Testing</strong>: The real magic happens after the generation. The system tests each prompt against all the test cases, comparing their performance and ranking them using an ELO rating system.</p>
</li>
</ul>
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/41550495/250947004-f8171cff-1703-40ca-b9fd-f0aa24d07110.png"><img width="1563" alt="Screen Shot 2023-07-04 at 11 41 54 AM" src="https://user-images.githubusercontent.com/41550495/250947004-f8171cff-1703-40ca-b9fd-f0aa24d07110.png"></a>
<ul dir="auto">
<li>
<p dir="auto"><strong>ELO Rating System</strong>: Each prompt starts with an ELO rating of 1200. As they compete against each other in generating responses to the test cases, their ELO ratings change based on their performance. This way, you can easily see which prompts are the most effective.</p>
</li>
<li>
<p dir="auto"><strong>Classification Version</strong>: The <code>gpt-prompt-engineer -- Classification Version</code> notebook is designed to handle classification tasks. It evaluates the correctness of a test case by matching it to the expected output ('true' or 'false') and provides a table with scores for each prompt.</p>
</li>
</ul>
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/41550495/252494291-d5c9f2a8-97fa-445d-9c38-dec744f77854.png"><img width="1607" alt="Screen Shot 2023-07-10 at 5 22 24 PM" src="https://user-images.githubusercontent.com/41550495/252494291-d5c9f2a8-97fa-445d-9c38-dec744f77854.png"></a>
<ul dir="auto">
<li><strong>Weights &amp; Biases Logging</strong>: Optional logging to <a href="https://wandb.ai/site" rel="nofollow">Weights &amp; Biases</a> of your configs such as temperature and max tokens, the system and user prompts for each part, the test cases used and the final ranked ELO rating for each candidate prompt. Set <code>use_wandb</code> to <code>True</code> to use. Only available in the main <code>gpt-prompt-engineer</code> notebook for now.</li>
</ul>
<h2 tabindex="-1" dir="auto">Setup</h2>
<ol dir="auto">
<li>
<p dir="auto"><a href="https://colab.research.google.com/github/mshumer/gpt-prompt-engineer/blob/main/gpt_prompt_engineer.ipynb" rel="nofollow">Open the notebook in Google Colab</a> or in a local Jupyter notebook. For classification, use <a href="https://colab.research.google.com/drive/16NLMjqyuUWxcokE_NF6RwHD8grwEeoaJ?usp=sharing" rel="nofollow">this one.</a></p>
</li>
<li>
<p dir="auto">Add your OpenAI API key to the line <code>openai.api_key = "ADD YOUR KEY HERE"</code>.</p>
</li>
<li>
<p dir="auto">If you have GPT-4 access, you're ready to move on. If not, change <code>CANDIDATE_MODEL='gpt-4'</code> to <code>CANDIDATE_MODEL='gpt-3.5-turbo'</code>. If you're using the classification version, and don't have GPT-4 access, change <code>model='gpt-4'</code> in the second cell to `model='gpt-3.5-turbo'.</p>
</li>
</ol>
<h2 tabindex="-1" dir="auto">How to Use</h2>
<ol dir="auto">
<li>Define your use-case and test cases. The use-case is a description of what you want the AI to do. Test cases are specific prompts that you would like the AI to respond to. For example:</li>
</ol>
<div data-snippet-clipboard-copy-content="description = &quot;Given a prompt, generate a landing page headline.&quot; # this style of description tends to work well

test_cases = [
    {
        'prompt': 'Promoting an innovative new fitness app, Smartly',
    },
    {
        'prompt': 'Why a vegan diet is beneficial for your health',
    },
    {
        'prompt': 'Introducing a new online course on digital marketing',
    },
    {
        'prompt': 'Launching a new line of eco-friendly clothing',
    },
    {
        'prompt': 'Promoting a new travel blog focusing on budget travel',
    },
    {
        'prompt': 'Advertising a new software for efficient project management',
    },
    {
        'prompt': 'Introducing a new book on mastering Python programming',
    },
    {
        'prompt': 'Promoting a new online platform for learning languages',
    },
    {
        'prompt': 'Advertising a new service for personalized meal plans',
    },
    {
        'prompt': 'Launching a new app for mental health and mindfulness',
    }
]"><pre><code>description = "Given a prompt, generate a landing page headline." # this style of description tends to work well

test_cases = [
    {
        'prompt': 'Promoting an innovative new fitness app, Smartly',
    },
    {
        'prompt': 'Why a vegan diet is beneficial for your health',
    },
    {
        'prompt': 'Introducing a new online course on digital marketing',
    },
    {
        'prompt': 'Launching a new line of eco-friendly clothing',
    },
    {
        'prompt': 'Promoting a new travel blog focusing on budget travel',
    },
    {
        'prompt': 'Advertising a new software for efficient project management',
    },
    {
        'prompt': 'Introducing a new book on mastering Python programming',
    },
    {
        'prompt': 'Promoting a new online platform for learning languages',
    },
    {
        'prompt': 'Advertising a new service for personalized meal plans',
    },
    {
        'prompt': 'Launching a new app for mental health and mindfulness',
    }
]
</code></pre></div>
<p dir="auto">For the classification version, your test cases should be in the format:</p>
<div data-snippet-clipboard-copy-content="test_cases = [
    {
        'prompt': 'I had a great day!',
        'output': 'true'
    },
    {
        'prompt': 'I am feeling gloomy.',
        'output': 'false'
    },
    // add more test cases here
]"><pre><code>test_cases = [
    {
        'prompt': 'I had a great day!',
        'output': 'true'
    },
    {
        'prompt': 'I am feeling gloomy.',
        'output': 'false'
    },
    // add more test cases here
]
</code></pre></div>
<ol start="3" dir="auto">
<li>
<p dir="auto">Choose how many prompts to generate. Keep in mind, this can get expensive if you generate many prompts. 10 is a good starting point.</p>
</li>
<li>
<p dir="auto">Call <code>generate_optimal_prompt(description, test_cases, number_of_prompts)</code> to generate a list of potential prompts, and test and rate their performance. For the classification version, just run the last cell.</p>
</li>
<li>
<p dir="auto">The final ELO ratings will be printed in a table, sorted in descending order. The higher the rating, the better the prompt.</p>
</li>
</ol>
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/41550495/250948262-324f90b8-c0ee-45fd-b219-6c44d9aa281b.png"><img width="1074" alt="Screen Shot 2023-07-04 at 11 48 45 AM" src="https://user-images.githubusercontent.com/41550495/250948262-324f90b8-c0ee-45fd-b219-6c44d9aa281b.png"></a>
<p dir="auto">For the classification version, the scores for each prompt will be printed in a table (see the image above).</p>
<h2 tabindex="-1" dir="auto">Contributions are welcome! Some ideas:</h2>
<ul dir="auto">
<li>have a number of different system prompt generators that create different styles of prompts, to cover more ground (ex. examples, verbose, short, markdown, etc.)</li>
<li>automatically generate the test cases</li>
<li>expand the classification version to support more than two classes using tiktoken</li>
</ul>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">This project is <a href="https://github.com/your_username/your_repository/blob/master/LICENSE">MIT</a> licensed.</p>
<h2 tabindex="-1" dir="auto">Contact</h2>
<p dir="auto">Matt Shumer - <a href="https://twitter.com/mattshumer_" rel="nofollow">@mattshumer_</a></p>
<p dir="auto">Project Link: <a href="https://github.com/mshumer/gpt-prompt-engineer/blob/main/url">https://github.com/mshumer/gpt-prompt-engineer</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Laser, a new game played on a chess board (165 pts)]]></title>
            <link>https://playlaser.xyz</link>
            <guid>36676415</guid>
            <pubDate>Tue, 11 Jul 2023 04:27:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://playlaser.xyz">https://playlaser.xyz</a>, See on <a href="https://news.ycombinator.com/item?id=36676415">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header><span data-svelte-h="svelte-ee3vl7">LASER</span> </header> <div><p><span data-svelte-h="svelte-pxsjkp">RULES</span></p> <p><span>Movement</span></p><div data-svelte-h="svelte-h8xg07"><p><span>Laser&nbsp;&nbsp;</span><span>Moves horizontal and vertical, shoots laser diagonally until the edge of the board or a Wall, destroying all pieces in its path. Cannot capture pieces.</span></p></div> <div data-svelte-h="svelte-1raf1hq"><p><span>Wall&nbsp;&nbsp;</span><span>Same movement as a rook in chess, stops the Laser and cannot be destroyed by it</span></p></div> <div data-svelte-h="svelte-t1meyh"><p><span>King&nbsp;&nbsp;</span><span>Same movement as chess</span></p></div> <div data-svelte-h="svelte-23b521"><p><span>Knight&nbsp;&nbsp;</span><span>Same movement as chess</span></p></div> <div data-svelte-h="svelte-ca1shm"><p><span>Pawn&nbsp;&nbsp;</span><span>Moves diagonally, takes horizontally and vertically</span></p></div> <p><span>Winning</span></p><p><span>Take your opponent's King</span> or <span>move one of your pawns to the seven squares in the opposite corner</span>, for black that's a1, a2, a3, a4, b1, c1, d1 and for white that's h8, h7, h6, h5, g8, f8, e8.</p> <p><span>Gameplay</span></p><p>No checks or checkmates, the game continues until one of the win conditions. The game is a draw if both kings are lasered at the same time. Black moves first.
</p></div>   </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[After 30 Years, Linux Finally Hits 3% Market Share (506 pts)]]></title>
            <link>https://linuxiac.com/linux-hits-3-percent-market-share/</link>
            <guid>36676103</guid>
            <pubDate>Tue, 11 Jul 2023 03:29:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://linuxiac.com/linux-hits-3-percent-market-share/">https://linuxiac.com/linux-hits-3-percent-market-share/</a>, See on <a href="https://news.ycombinator.com/item?id=36676103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p><strong><strong>Linux enthusiasts rejoice! After a long journey, according to StatCounter’s data, by June 2023, Linux has achieved a 3% desktop market share.</strong></strong></p>



<p>Linux has a long history that dates back <a href="https://linuxiac.com/linux-birthday/">more than 30 years</a>. However, it has never been as popular among regular computer users as other operating systems such as Microsft’s Windows or Apple’s macOS.</p>



<p>Of course, for many years, Linux has emerged as a dominant force in the realm of server operating systems. Due to its exceptional performance, stability, reliability, and security, it has been widely adopted in server/cloud/IoT environments.</p>



<p>However, these days, Linux is no longer limited to these environments alone; it is rapidly gaining momentum as an operating system of choice for many desktop users, especially developers.</p>



<p>And the most recent figures confirm this, giving all advocates of Linux and open source in general reason to rejoice.</p>





<p><a href="https://gs.statcounter.com/os-market-share/desktop/worldwide" target="_blank" rel="noreferrer noopener">According to StatCounter</a>, a web analytics company, by June 2023, Linux has reached a 3% market share in the desktop segment. This is a remarkable achievement considering its fierce competition from other operating systems.</p>



<figure><a href="https://linuxiac.b-cdn.net/wp-content/uploads/2023/07/lunux-marketshare-statcounter.jpg"><img decoding="async" width="1024" height="838" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMTIwIDkxNyIgd2lkdGg9IjExMjAiIGhlaWdodD0iOTE3IiBkYXRhLXU9Imh0dHBzJTNBJTJGJTJGbGludXhpYWMuY29tJTJGd3AtY29udGVudCUyRnVwbG9hZHMlMkYyMDIzJTJGMDclMkZsdW51eC1tYXJrZXRzaGFyZS1zdGF0Y291bnRlci5qcGciIGRhdGEtdz0iMTEyMCIgZGF0YS1oPSI5MTciIGRhdGEtYmlwPSIiPjwvc3ZnPg==" data-spai="1" alt="Desktop Operating System Market Share Worldwide" srcset=" " sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Desktop Operating System Market Share Worldwide</figcaption></figure>



<p>While someone may seem the figure modest, it signifies a growing acceptance and recognition of the power and versatility of Linux.</p>



<p>In any case, the achievement of a 3% market share by Linux is undoubtedly a cause for celebration among its dedicated community. It reflects the growing recognition of Linux’s strengths and the efforts to overcome its historical barriers.</p>



<p>Moreover, with the continued development and innovation within the Linux ecosystem, its market share will continue growing in the coming years.</p>



<p>The growing importance of cloud computing and the rise of server infrastructure have also contributed to Linux’s success. Still, the main reason for reaching this figure is the operating system’s growing popularity among desktop users.</p>



<p>With exceptionally easy-to-use and entirely user-centric <a href="https://linuxiac.com/best-7-linux-distro-releases-for-desktop-in-2022/">Linux desktop distributions</a>, the operating system is no longer what it was 20 years ago – a complex equation available only to highly technically enlightened hackers.</p>


<h2 id="h-linux-growing-popularity-among-desktop-users">Linux Growing Popularity among Desktop Users</h2>


<figure><a href="https://linuxiac.b-cdn.net/wp-content/uploads/2023/07/linux-desktop.jpg"><img decoding="async" width="1024" height="640" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxNjgwIDEwNTAiIHdpZHRoPSIxNjgwIiBoZWlnaHQ9IjEwNTAiIGRhdGEtdT0iaHR0cHMlM0ElMkYlMkZsaW51eGlhYy5jb20lMkZ3cC1jb250ZW50JTJGdXBsb2FkcyUyRjIwMjMlMkYwNyUyRmxpbnV4LWRlc2t0b3AuanBnIiBkYXRhLXc9IjE2ODAiIGRhdGEtaD0iMTA1MCIgZGF0YS1iaXA9IiI+PC9zdmc+" data-spai="1" alt="Linux with GNOME Desktop." srcset=" " sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Linux with GNOME Desktop.</figcaption></figure>



<p>Yes, I know. Over the last 10+ years, each one has often been heralded as “Linux on the Desktop,” although it turns out that’s not quite the case. But still, we’re close to that point now. And for good reasons.</p>


<h3 id="free-lightweight-amp-customizable">Free, Lightweight &amp; Customizable</h3>


<p>The main appealing aspect of Linux for desktop users is its lightweight nature, free from corporate bloatware, and especially the limitless customization options.</p>



<p>It allows users to tailor their desktop environment to suit their preferences and workflow. With a vast selection of desktop environments like GNOME, KDE, Xfce, and many others, users can choose the one that best aligns with their needs.</p>


<h3 id="valuing-user-privacy">Valuing User Privacy</h3>


<p>Another important factor driving Linux’s growing popularity among desktop users is privacy. Compared to other mainstream operating systems, Linux generally collects no user data.</p>



<p>While <a href="https://linuxiac.com/fedora-40-plans-to-use-telemetry/">some distributions may try to collect basic telemetry data</a> for improvement purposes, the level of data collection is typically minimal and can be disabled or opted out of entirely. This aspect appeals to privacy-conscious individuals who prefer more control over their personal information.</p>


<h3 id="linux-is-a-developers-dream-come-true">Linux is a Developer’s Dream Come True</h3>


<p>Linux has long been the operating system of choice for developers worldwide, and its allure continues to grow.</p>



<p>First and foremost, Linux’s open-source nature empowers developers with unparalleled freedom. They can access and modify the source code, customize their environments, and contribute to the community, fostering collaboration and innovation.</p>



<p>Furthermore, performance is also a crucial factor. Linux’s efficiency, scalability, and ability to run on diverse hardware architectures make it ideal for resource-intensive tasks.</p>



<p>Lastly, its command-line interface and powerful scripting capabilities offer flexibility and automation, streamlining development workflows.</p>


<h2 id="bottom-line">Bottom Line</h2>


<p>So, as Linux enthusiasts rejoice, it is essential to remember that the journey does not end here. Linux has proven its worth, and its rise to a 3% desktop market share is a testament to its resilience and adaptability in the desktop field.</p>



<p>With ongoing advancements and increased support from the Open Source community and businesses, Linux is poised to become an even more formidable player in the world of operating systems.</p>




		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-4 details leaked? (455 pts)]]></title>
            <link>https://threadreaderapp.com/thread/1678545170508267522.html</link>
            <guid>36675934</guid>
            <pubDate>Tue, 11 Jul 2023 03:00:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://threadreaderapp.com/thread/1678545170508267522.html">https://threadreaderapp.com/thread/1678545170508267522.html</a>, See on <a href="https://news.ycombinator.com/item?id=36675934">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-controller="mentions">

<div>
<p><a href="https://threadreaderapp.com/user/Yampeleg"><img src="https://pbs.twimg.com/profile_images/1505912788031623170/GC7LMHNp_bigger.jpg" alt="Yam Peleg Profile picture" data-controller="twitter-profile" data-twtrid="634339745" data-action="error->twitter-profile#error"></a>
</p>

</div> 
<div id="tweet_1" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545170508267522" dir="auto"><p>
GPT-4's details are leaked. </p><p>

It is over.</p><p>

Everything is here: <a data-preview="true" href="https://twitter.com/i/web/status/1678545170508267522">twitter.com/i/web/status/1…</a>
<sup><i></i></sup></p></div>
<div id="tweet_2" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545175210074112" dir="auto"><p>
Parameters count:</p><p>

GPT-4 is more than 10x the size of GPT-3. We believe it has a total of ~1.8 trillion parameters across 120 layers.
<sup><i></i></sup></p></div>
<div id="tweet_3" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545177282052098" dir="auto"><p>
Mixture Of Experts - Confirmed.</p><p>

OpenAI was able to keep costs reasonable by utilizing a mixture of experts (MoE) model.<br>
They utilizes 16 experts within their model, each is about ~111B parameters for MLP. 2 of these experts are routed to per forward pass.
<sup><i></i></sup></p></div>
<div id="tweet_4" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545179840589836" dir="auto"><p>
MoE Routing:</p><p>

While the literature talks a lot about advanced routing algorithms for choosing which experts to route each token to, OpenAI’s is allegedly quite simple, for the current GPT-4 model.</p><p>

There roughly ~55B shared parameters for attention.
<sup><i></i></sup></p></div>
<div id="tweet_5" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545182990512129" dir="auto"><p>
Inference:</p><p>

Each forward pass inference (generation of 1 token) only utilizes ~280B parameters and ~560 TFLOPs. This contrasts with the ~1.8 trillion parameters and ~3,700 TFLOP that would be required per forward pass of a purely dense model.
<sup><i></i></sup></p></div>
<div id="tweet_6" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545185108631552" dir="auto"><p>
Dataset:</p><p>

GPT-4 is trained on ~13T tokens.</p><p>

These are not unique tokens, they count the epochs as more tokens as well.</p><p>

Epoch number: 2 epochs for text-based data and 4 for code-based data.</p><p>

There is millions of rows of instruction fine-tuning data from ScaleAI &amp; internally.
<sup><i></i></sup></p></div>
<div id="tweet_7" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545188363329539" dir="auto"><p>
GPT-4 32K</p><p>

There was an 8k context length (seqlen) for the pre-training phase. The 32k seqlen version of GPT-4 is based on fine-tuning of the 8k after the pre-training.
<sup><i></i></sup></p></div>
<div id="tweet_8" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545190892470272" dir="auto"><p>
Batch Size:</p><p>

The batch size was gradually ramped up over a number of days on the cluster, but by the end, OpenAI was using a batch size of 60 million! This, of course, is “only” a batch size of 7.5 million tokens per expert due to not every expert seeing all tokens.
<sup><i></i></sup></p></div>
<p>
For the real batch size:<br>
Divide this number by the seq len to get the real batch size. just stop with this misleading numbers already.
<sup><i></i></sup>
</p>
<div id="tweet_10" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545195506233344" dir="auto"><p>
Parallelism Strategies</p><p>

To parallelize across all their A100s GPUs They utilized 8-way tensor parallelism as that is the limit for NVLink.</p><p>

Beyond that, they are using 15-way pipeline parallelism.</p><p>

(likely used ZeRo Stage 1. It is possible they used block-level FSDP)
<sup><i></i></sup></p></div>
<div id="tweet_11" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545197792206851" dir="auto"><p>
Training Cost</p><p>

OpenAI’s training FLOPS for GPT-4 is ~2.15e25, on ~25,000 A100s for 90 to 100 days at about 32% to 36% MFU.</p><p>

Part of this extremely low utilization is due to an absurd number of failures requiring checkpoints that needed to be restarted from.
<sup><i></i></sup></p></div>
<div id="tweet_12" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545200325558272" dir="auto"><p>
If their cost in the cloud was about $1 per A100 hour, the training costs for this run alone would be about $63 million.</p><p>

(Today, the pre-training could be done with ~8,192 H100 in ~55 days for $21.5 million at $2 per H100 hour.)
<sup><i></i></sup></p></div>
<div id="tweet_13" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545202477146113" dir="auto"><p>
Mixture of Expert Tradeoffs</p><p>

There are multiple MoE tradeoffs taken: For example, MoE is incredibly difficult to deal with on inference because not every part of the model is utilized on every token generation.
<sup><i></i></sup></p></div>
<div id="tweet_14" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545205442621440" dir="auto"><p>
This means parts may sit dormant when other parts are being used. When serving users, this really hurts utilization rates.</p><p>

Researchers have shown that using 64 to 128 experts achieves better loss than 16 experts, but that’s purely research.
<sup><i></i></sup></p></div>
<p>
There are multiple reasons to go with fewer experts. One reason for OpenAI choosing 16 experts is because more experts are difficult to generalize at many tasks. More experts can also be more difficult to achieve convergence with.
<sup><i></i></sup>
</p>
<p>
With such a large training run, OpenAI instead chose to be more conservative on the number of experts.
<sup><i></i></sup>
</p>
<div id="tweet_17" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545212581265409" dir="auto"><p>
GPT-4 Inference Cost</p><p>

GPT-4 costs 3x that of the 175B parameter Davinchi.<br>
This is largely due to the larger clusters required for GPT-4 and much lower utilization achieved.
<sup><i></i></sup></p></div>
<p>
AN estimate of it's costs is $0.0049 cents per 1k tokens for 128 A100s to inference GPT-4 8k seqlen and $0.0021 cents per 1k tokens for 128 H100’s to inference GPT-4 8k seqlen. It should be noted, we assume decent high utilization, and keeping batch sizes high.
<sup><i></i></sup>
</p>
<div id="tweet_19" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545217631272960" dir="auto"><p>
Multi-Query Attention</p><p>

OpenAI are using MQA just like everybody else.<br>
Because of that only 1 head is needed and memory capacity can be significantly reduced for the KV cache. Even then, the 32k seqlen GPT-4 definitely cannot run on 40GB A100s, and the 8k is capped on max bsz.
<sup><i></i></sup></p></div>
<div id="tweet_20" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545219774562304" dir="auto"><p>
Continuous batching</p><p>

OpenAI implements both variable batch sizes and continuous batching. This is so as to allow some level of maximum latency as well optimizing the inference costs.
<sup><i></i></sup></p></div>
<div id="tweet_21" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545222815424512" dir="auto"><p>
Vision Multi-Modal</p><p>

It is a separate vision encoder from the text encoder, with cross-attention. The architecture is similar to Flamingo. This adds more parameters on top of the 1.8T of GPT-4. It is fine-tuned with another ~2 trillion tokens, after the text only pre-training.
<sup><i></i></sup></p></div>
<p>
On the vision model, OpenAI wanted to train it from scratch, but it wasn’t mature enough, so they wanted to derisk it by starting with text.
<sup><i></i></sup>
</p>
<p>
One of the primary purposes of this vision capability is for autonomous agents able to read web pages and transcribe what’s in images and video.
<sup><i></i></sup>
</p>
<div id="tweet_24" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545231300403200" dir="auto"><p>
Some of the data they train on is joint data (rendered LaTeX/text), screen shots of web page, youtube videos: sampling frames, and run Whisper around it to get transcript.</p><p>

[Dont want to say "I told you so" but..]
<sup><i></i></sup></p></div>
<div id="tweet_25" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678546324105330689" dir="auto"><p>
Speculative Decoding</p><p>

OpenAI might be using speculative decoding on GPT-4's inference. (not sure 100%)</p><p>

The idea is to use a smaller faster model to decode several tokens in advance, and then feeds them into a large oracle model as a single batch.
<sup><i></i></sup></p></div>
<p>
If the small model was right about its predictions – the larger model agrees and we can decode several tokens in a single batch.
<sup><i></i></sup>
</p>
<p>
But if the larger model rejects the tokens predicted by the draft model then the rest of the batch is discarded. And we continue with the larger model.
<sup><i></i></sup>
</p>
<p>
The conspiracy theory that the new GPT-4 quality had been deteriorated might be simply because they are letting the oracle model accept lower probability sequences from the speculative decoding model.
<sup><i></i></sup>
</p>
<div id="tweet_29" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678547812177330180" dir="auto"><p>
Inference Architecture</p><p>

The inference runs on a cluster of 128 GPUs.</p><p>

There are multiple of these clusters in multiple datacenters in different locations.</p><p>

It is done in 8-way tensor parallelism and 16-way pipeline parallelism.</p><p>

Each node of 8 GPUs has only ~130B parameters, or… <a data-preview="true" href="https://twitter.com/i/web/status/1678547812177330180">twitter.com/i/web/status/1…</a>
<sup><i></i></sup></p></div>
<p>
The model has 120, so it fits in 15 different nodes.<br>
[Possibly the there are less layers on the first node since it needs to also compute the embeddings]
<sup><i></i></sup>
</p>
<div id="tweet_31" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678549234612674560" dir="auto"><p>
According to these numbers: OpenAI should have trained on 2x the tokens if they were trying to go by chinchilla's optimal.</p><p>

[let alone surpass it like we do]</p><p>

This goes to show that they are struggling to get high quality data.
<sup><i></i></sup></p></div>
<div id="tweet_32" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678550338075336706" dir="auto"><p>
Why no FSDP?</p><p>

A possible reason for this could be that some of the hardware infra they secured is of an older generation. </p><p>

This is pretty common at local compute clusters as the organisation usually upgrade the infra in several "waves" to avoid a complete pause of operation.… <a data-preview="true" href="https://twitter.com/i/web/status/1678550338075336706">twitter.com/i/web/status/1…</a>
<sup><i></i></sup></p></div>
<div id="tweet_33" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678553044219224064" dir="auto"><p>
Dataset Mixture</p><p>

They trained on 13T tokens.<br>
CommonCrawl &amp; RefinedWeb are both 5T.</p><p>

Remove the duplication of tokens from multiple epochs and we get to a much reasonable number of "unaccounted for" tokens: The "secret" data.
<sup><i></i></sup></p></div>
<div id="tweet_34" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678553048501633024" dir="auto"><p>
Which by this point we already get rumors that parts of it came from twitter, reddit &amp; youtube.</p><p>

[Rumors that start to become lawsuits] </p><p>

Some speculations are:<br>
- LibGen (4M+ books)<br>
- Sci-Hub (80M+ papers)<br>
- All of GitHub
<sup><i></i></sup></p></div>
<div id="tweet_35" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678553050296856577" dir="auto"><p>
My own opinion:</p><p>

The missing dataset it a custom dataset of college textbooks collected by hand for as much courses as possible.</p><p>

This is very easy to convert to txt file and than with self-instruct into instruction form.
<sup><i></i></sup></p></div>
<div id="tweet_36" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678553055673872387" dir="auto"><p>
This creates the "illusion" that GPT-4 "is smart" no matter who use it.</p><p>

Computer scientist? sure! it can help you with your questions about P!=NP<br>
Philosophy major? It can totally talk to you about epistemology.</p><p>

Don't you see?<br>
It was trained on the textbooks. It is so obvious.
<sup><i></i></sup></p></div>
<div id="tweet_37" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678553507236835328" dir="auto"><p>
There are also papers that try to extract by force memorized parts of books from GPT-4 to understand what it trained on. </p><p>

There are some books it knows so well that it had seen them for sure.</p><p>

Moreover, If i remember correctly: It even know the unique ids of project Euler exes.
<sup><i></i></sup></p></div>
<p>• • •</p>
<p><span>
Missing some Tweet in this thread? You can try to
<a id="force-click" href="#" data-category="refresh" data-action="1678545170508267522">force a refresh</a>
</span>
</p>
　
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Easy HTTPS for your private networks (237 pts)]]></title>
            <link>https://www.getlocalcert.net/</link>
            <guid>36674224</guid>
            <pubDate>Mon, 10 Jul 2023 23:05:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.getlocalcert.net/">https://www.getlocalcert.net/</a>, See on <a href="https://news.ycombinator.com/item?id=36674224">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <h2>ACME Client Configuration</h2>

<div>
  <div>
    <h3>Using Caddy</h3>

    <p>
    Modern, all-in-one web server
    </p>

    <div><p>
    Edit Caddyfile:
    </p><!-- https://github.com/caddy-dns/acmedns -->
    <pre><code>
<yoursubdomain>.localhostcert.net {
  tls {
    dns acmedns credentials.json
  }
  respond "Hello"
}
</yoursubdomain></code></pre>

    <p><a href="https://docs.getlocalcert.net/acme-clients/caddy/">See full example</a>

    </p></div>
  </div>

  <div>
    <h3>Using traefik</h3>
    <p>
    Cloud-native application proxy
    </p>
    <div><p>
    Edit configuration:
    </p><pre><code>
certificatesResolvers:
  myresolver:
    acme:
      dnsChallenge:
        provider: acme-dns</code></pre>

    <p><a href="https://docs.getlocalcert.net/acme-clients/traefik/">See full example</a>
    </p></div>
  </div>

</div>
<div>

  <div>
    <h3>Using cert-manager</h3>

    <p>
    Certificate automation for Kubernetes
    </p>

    <div><p>
    Edit configuration:
    </p><pre><code>
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: example-issuer
spec:
  acme:
    solvers:
    - dns01:
        acmeDNS:
          host: https://api.getlocalcert.net/api/v1/acme-dns-compat
          accountSecretRef:
            name: acme-dns
            key: credentials.json</code></pre>

    <p><a href="https://docs.getlocalcert.net/acme-clients/cert-manager/">See full example</a>

    </p></div>
  </div>

  <div>
    <h3>Using acme.sh</h3>

    <p>
    Get started quickly
    </p>

    <div>
    <pre><code>
$ export ACMEDNS_BASE_URL=https://api.getlocalcert.net/api/v1/acme-dns-compat
$ export ACMEDNS_USERNAME=yourApiKeyId
$ export ACMEDNS_PASSWORD=yourApiKeySecret
$ export ACMEDNS_SUBDOMAIN=yoursubdomain
$ ./acme.sh --issue \
            --dns dns_acmedns \
            -d yoursubdomain.localhostcert.net</code></pre>
    <p><a href="https://docs.getlocalcert.net/acme-clients/acme-sh/">See full example</a>
    </p></div>
  </div>



</div>

<div>

  <div>
    <h3>Using LEGO</h3>

    <p>
    A client build by Let's Encrypt
    </p>

    <div>
    <pre><code>
$ export ACME_DNS_API_BASE=https://api.getlocalcert.net/api/v1/acme-dns-compat
$ export ACME_DNS_STORAGE_PATH=credentials.json
$ lego --email you@example.com \
       --dns acme-dns \
       --domains yoursubdomain.localhostcert.net \
       run</code></pre>
    <p><a href="https://docs.getlocalcert.net/acme-clients/lego/">See full example</a>
    </p></div>
    
  </div>

  <div>
    <h3>Certify The Web</h3>

    <p>
    Manage certificates on Windows and IIS
    </p>

    <div>
      <pre><code>
1. Select acme-dns as the DNS update method.
2. Enter https://api.getlocalcert.net/api/v1/acme-dns-compat as the server
3. Click Request Certificate
4. Skip the CNAME step, you won't need it
      </code></pre>
      <p><a href="https://docs.getlocalcert.net/acme-clients/certify-the-web/">See full example</a>
    </p></div>
  </div>

</div>

    <section>
      <h2>
        Ready to set up your free domain name?
      </h2>
      
      <p>
      <a href="https://console.getlocalcert.net/">Get Started</a>
      </p>
    </section>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brian Eno albums available in Dolby Atmos and Spatial (128 pts)]]></title>
            <link>https://www.udiscovermusic.com/news/brian-eno-albums-dolby-atmos-first-time/</link>
            <guid>36674069</guid>
            <pubDate>Mon, 10 Jul 2023 22:49:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.udiscovermusic.com/news/brian-eno-albums-dolby-atmos-first-time/">https://www.udiscovermusic.com/news/brian-eno-albums-dolby-atmos-first-time/</a>, See on <a href="https://news.ycombinator.com/item?id=36674069">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mvp-main-body-wrap">
<!--hide-mobile-newmenu-tabs--><article id="mvp-article-wrap" itemscope="" itemtype="http://schema.org/NewsArticle">
			<meta itemscope="" itemprop="mainEntityOfPage" itemtype="https://schema.org/WebPage" itemid="https://www.udiscovermusic.com/news/brian-eno-albums-dolby-atmos-first-time/">
        	
			<div>
					
					<header id="mvp-post-head">
                      
												  <span><p>The titles from Eno’s catalog now available include ‘Taking Tiger Mountain (By Strategy)’ and ‘Another Green World.’</p>
</span>
																			<!--mvp-author-info-wrap-->
											</header>
					
					<div id="mvp-post-content-top">								
					
					
						<div id="mvp-post-feat-img" itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
																				
												<p><img width="1000" height="600" src="https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-1000x600.jpg" alt="Brian-Eno-Albums-Dolby-Atmos" srcset="https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-1000x600.jpg 1000w, https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-300x180.jpg 300w, https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-1024x614.jpg 1024w, https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-768x461.jpg 768w, https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-590x354.jpg 590w, https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-400x240.jpg 400w, https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy.jpg 1440w" sizes="(max-width: 1000px) 100vw, 1000px"></p><meta itemprop="url" content="https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-1000x600.jpg">
													<meta itemprop="width" content="1000">
													<meta itemprop="height" content="600">
											
																							
                                            </div><!--mvp-post-feat-img-->
                                        					
										<p>Brian Eno - Photo: Cecily Eno</p>
					
					

					<div id="mvp-content-main">
	                    
<p>Seen classic titles from <a href="https://www.udiscovermusic.com/artist/brian-eno/"><strong>Brian Eno’s</strong></a> catalog are now available in Dolby Atmos for the first time, These new releases range from his early solo work, <a href="https://www.udiscovermusic.com/stories/rediscover-eno-taking-tiger-mountain/"><strong><em>Taking Tiger Mountain</em></strong></a> and <a href="https://www.udiscovermusic.com/stories/brian-eno-another-green-world-new-sound/"><strong><em>Another Green World</em></strong></a> to some of the artist’s most renowned output of recent times including <em>Small Craft On A Milk Sea</em> and <em>The Ship</em>, which will be presented for the first time as a live orchestral arrangement in concert this coming October.</p>
<p><strong><a href="https://shop.udiscovermusic.com/collections/brian-eno" target="_blank" rel="noopener">Shop the best of Brian Eno’s discography on vinyl and more</a>.</strong></p>
<p>Brian Eno says, “What’s interesting about 3D music is the possibility of making an immersive space which is capable of sustaining much more detail than a 2 dimensional space (such as that presented by stereo listening). Our ears aren’t as directional as our eyes, but they are still capable of locating sounds in space, and listening in 3 dimensions this becomes a whole new compositional possibility. It allows a listener the experience of ‘exploring’ the musical space in much more intricate ways. It’s the step from 2-dimensional sound-painting to 3-dimensional sound-sculpting.”</p>
		
<p>Additionally, Brian Eno’s most recent album, the critically acclaimed vocal album <a href="https://www.udiscovermusic.com/news/brian-eno-foreverandevernomore-out-now/"><em><strong>FOREVERANDEVERNOMORE</strong></em></a> and the album’s instrumental accompaniment, <a href="https://www.udiscovermusic.com/news/brian-eno-forever-voiceless/"><em><strong>Forever Voiceless</strong></em></a> will also be available in Dolby Atmos and Spatial audio.</p>
<p>In June, Eno announced details of his<a href="https://www.udiscovermusic.com/news/brian-eno-solo-tour-ships/"><strong> first-ever solo tour</strong></a>, which will take him across Europe later this year. The electronic and ambient music legend has previously toured as part of <a href="https://www.udiscovermusic.com/artist/roxy-music/"><strong>Roxy Music</strong></a> and with other artists – including most recently alongside his brother in 2021 – as well as at festivals and his own one-off shows.</p>
<p>His first solo tour will kick off with two shows at the Venice Biennale Musica on October 21 before heading to Berlin, Paris, and Utrecht. It will wrap up on October 30 with another pair of shows at London’s Royal Festival Hall. <a href="https://linktr.ee/brianeno?utm_source=Email&amp;utm_medium=Email&amp;utm_campaign=BrianEnoBrianEnoalbumsavailableinDolbyAtmosandSpatialforthefirsttime070723&amp;utm_content=UMGUK42415-1012847&amp;vvsa_consumer_id=35467988&amp;vvsa_tracking=_vvsa_tyiMfxQa525130" target="_blank" rel="noopener"><strong>Tickets are still available to purchase</strong></a>, though the shows in Venice and Utrecht have now sold out.</p>
<p>Eno’s new live show, Ships, will be based around his 2016 album<em> The Ship</em> and was originally commissioned by La Biennale di Venezia. He will perform together with the Baltic Sea Philharmonic, orchestrated and conducted by Kristin Järvi. The performance will also feature actor Peter Serafinowicz and Eno’s long-time collaborators Leo Abrahams (guitar) and Peter Chilvers (programming/keyboards).</p>
<p><strong><a href="https://brianeno.lnk.to/StreamEM!UMGUK42415-1012847?vvsa_consumer_id=35467988&amp;vvsa_tracking=_vvsa_JWfcz8cD525129" target="_blank" rel="noopener">Listen to Brian Eno in Dolby Atmos and Spatial audio</a>.</strong></p>
		
						
					<!--US / UK English-->
                      
						
	

					</div><!--mvp-content-main-->
						
					</div><!--mvp-post-content-top-->
					
					<!--mvp-post-content-bottom-->

	</div><!--mvp-main-box-->
	
	
</article><!--mvp-article-wrap-->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MicroVM by QEMU (124 pts)]]></title>
            <link>https://qemu.readthedocs.io/en/latest/system/i386/microvm.html</link>
            <guid>36673945</guid>
            <pubDate>Mon, 10 Jul 2023 22:35:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qemu.readthedocs.io/en/latest/system/i386/microvm.html">https://qemu.readthedocs.io/en/latest/system/i386/microvm.html</a>, See on <a href="https://news.ycombinator.com/item?id=36673945">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <nav data-toggle="wy-nav-shift">
      
    </nav>

    <section data-toggle="wy-nav-shift">

      
      <nav aria-label="top navigation">
        
          <i data-toggle="wy-nav-top"></i>
          <a href="https://qemu.readthedocs.io/en/latest/index.html">QEMU</a>
        
      </nav>


      <div id="microvm-virtual-platform-microvm" itemprop="articleBody" role="main" itemscope="itemscope" itemtype="http://schema.org/Article">

<p><code><span>microvm</span></code> is a machine type inspired by <code><span>Firecracker</span></code> and
constructed after its machine model.</p>
<p>It’s a minimalist machine type without <code><span>PCI</span></code> nor <code><span>ACPI</span></code> support,
designed for short-lived guests. microvm also establishes a baseline
for benchmarking and optimizing both QEMU and guest operating systems,
since it is optimized for both boot time and footprint.</p>
<div id="supported-devices">
<h2>Supported devices<a href="#supported-devices" title="Permalink to this headline">¶</a></h2>
<p>The microvm machine type supports the following devices:</p>
<ul>
<li>ISA bus</li>
<li>i8259 PIC (optional)</li>
<li>i8254 PIT (optional)</li>
<li>MC146818 RTC (optional)</li>
<li>One ISA serial port (optional)</li>
<li>LAPIC</li>
<li>IOAPIC (with kernel-irqchip=split by default)</li>
<li>kvmclock (if using KVM)</li>
<li>fw_cfg</li>
<li>Up to eight virtio-mmio devices (configured by the user)</li>
</ul>
</div>
<div id="limitations">
<h2>Limitations<a href="#limitations" title="Permalink to this headline">¶</a></h2>
<p>Currently, microvm does <em>not</em> support the following features:</p>
<ul>
<li>PCI-only devices.</li>
<li>Hotplug of any kind.</li>
<li>Live migration across QEMU versions.</li>
</ul>
</div>
<div id="using-the-microvm-machine-type">
<h2>Using the microvm machine type<a href="#using-the-microvm-machine-type" title="Permalink to this headline">¶</a></h2>
<div id="machine-specific-options">
<h3>Machine-specific options<a href="#machine-specific-options" title="Permalink to this headline">¶</a></h3>
<p>It supports the following machine-specific options:</p>
<ul>
<li>microvm.x-option-roms=bool (Set off to disable loading option ROMs)</li>
<li>microvm.pit=OnOffAuto (Enable i8254 PIT)</li>
<li>microvm.isa-serial=bool (Set off to disable the instantiation an ISA serial port)</li>
<li>microvm.pic=OnOffAuto (Enable i8259 PIC)</li>
<li>microvm.rtc=OnOffAuto (Enable MC146818 RTC)</li>
<li>microvm.auto-kernel-cmdline=bool (Set off to disable adding virtio-mmio devices to the kernel cmdline)</li>
</ul>
</div>
<div id="boot-options">
<h3>Boot options<a href="#boot-options" title="Permalink to this headline">¶</a></h3>
<p>By default, microvm uses <code><span>qboot</span></code> as its BIOS, to obtain better boot
times, but it’s also compatible with <code><span>SeaBIOS</span></code>.</p>
<p>As no current FW is able to boot from a block device using
<code><span>virtio-mmio</span></code> as its transport, a microvm-based VM needs to be run
using a host-side kernel and, optionally, an initrd image.</p>
</div>
<div id="running-a-microvm-based-vm">
<h3>Running a microvm-based VM<a href="#running-a-microvm-based-vm" title="Permalink to this headline">¶</a></h3>
<p>By default, microvm aims for maximum compatibility, enabling both
legacy and non-legacy devices. In this example, a VM is created
without passing any additional machine-specific option, using the
legacy <code><span>ISA</span> <span>serial</span></code> device as console:</p>
<div><pre><span></span>$ qemu-system-x86_64 -M microvm \
   -enable-kvm -cpu host -m 512m -smp 2 \
   -kernel vmlinux -append "earlyprintk=ttyS0 console=ttyS0 root=/dev/vda" \
   -nodefaults -no-user-config -nographic \
   -serial stdio \
   -drive id=test,file=test.img,format=raw,if=none \
   -device virtio-blk-device,drive=test \
   -netdev tap,id=tap0,script=no,downscript=no \
   -device virtio-net-device,netdev=tap0
</pre></div>
<p>While the example above works, you might be interested in reducing the
footprint further by disabling some legacy devices. If you’re using
<code><span>KVM</span></code>, you can disable the <code><span>RTC</span></code>, making the Guest rely on
<code><span>kvmclock</span></code> exclusively. Additionally, if your host’s CPUs have the
<code><span>TSC_DEADLINE</span></code> feature, you can also disable both the i8259 PIC and
the i8254 PIT (make sure you’re also emulating a CPU with such feature
in the guest).</p>
<p>This is an example of a VM with all optional legacy features
disabled:</p>
<div><pre><span></span>$ qemu-system-x86_64 \
   -M microvm,x-option-roms=off,pit=off,pic=off,isa-serial=off,rtc=off \
   -enable-kvm -cpu host -m 512m -smp 2 \
   -kernel vmlinux -append "console=hvc0 root=/dev/vda" \
   -nodefaults -no-user-config -nographic \
   -chardev stdio,id=virtiocon0 \
   -device virtio-serial-device \
   -device virtconsole,chardev=virtiocon0 \
   -drive id=test,file=test.img,format=raw,if=none \
   -device virtio-blk-device,drive=test \
   -netdev tap,id=tap0,script=no,downscript=no \
   -device virtio-net-device,netdev=tap0
</pre></div>
</div>
<div id="triggering-a-guest-initiated-shut-down">
<h3>Triggering a guest-initiated shut down<a href="#triggering-a-guest-initiated-shut-down" title="Permalink to this headline">¶</a></h3>
<p>As the microvm machine type includes just a small set of system
devices, some x86 mechanisms for rebooting or shutting down the
system, like sending a key sequence to the keyboard or writing to an
ACPI register, doesn’t have any effect in the VM.</p>
<p>The recommended way to trigger a guest-initiated shut down is by
generating a <code><span>triple-fault</span></code>, which will cause the VM to initiate a
reboot. Additionally, if the <code><span>-no-reboot</span></code> argument is present in the
command line, QEMU will detect this event and terminate its own
execution gracefully.</p>
<p>Linux does support this mechanism, but by default will only be used
after other options have been tried and failed, causing the reboot to
be delayed by a small number of seconds. It’s possible to instruct it
to try the triple-fault mechanism first, by adding <code><span>reboot=t</span></code> to the
kernel’s command line.</p>
</div>
</div>
</div>

    </section>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shortening the Let's Encrypt chain of trust (493 pts)]]></title>
            <link>https://letsencrypt.org/2023/07/10/cross-sign-expiration.html</link>
            <guid>36673793</guid>
            <pubDate>Mon, 10 Jul 2023 22:19:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://letsencrypt.org/2023/07/10/cross-sign-expiration.html">https://letsencrypt.org/2023/07/10/cross-sign-expiration.html</a>, See on <a href="https://news.ycombinator.com/item?id=36673793">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<article>
		<p>When Let’s Encrypt first launched, we needed to ensure that our certificates were widely trusted. To that end, we arranged to have our intermediate certificates <a href="https://letsencrypt.org/2015/10/19/lets-encrypt-is-trusted.html">cross-signed by IdenTrust’s DST Root CA X3</a>. This meant that all certificates issued by those intermediates would be trusted, even while our own ISRG Root X1 wasn’t yet. During subsequent years, our Root X1 became <a href="https://letsencrypt.org/docs/certificate-compatibility/">widely trusted</a> on its own.&nbsp;</p>
<p>Come late 2021, our cross-signed intermediates and DST Root CA X3 itself were expiring. And while all up-to-date browsers at that time trusted our root, <a href="https://letsencrypt.org/2020/11/06/own-two-feet.html">over a third of Android devices</a> were still running old versions of the OS which would suddenly stop trusting websites using our certificates. That breakage would have been too widespread, so we arranged for a new cross-sign – this time <a href="https://letsencrypt.org/2020/12/21/extending-android-compatibility.html">directly onto our root</a> rather than our intermediates – which would outlive DST Root CA X3 itself. This stopgap allowed those old Android devices to continue trusting our certificates for three more years.</p>
<p>On September 30th, 2024, that cross-sign too will expire.</p>
<p>In the last three years, the percentage of Android devices which trust our ISRG Root X1 has risen from 66% to 93.9%. That percentage will increase further over the next year, especially as Android releases version 14, which has the ability to <a href="https://www.xda-developers.com/android-14-root-certificates-updatable">update its trust store without a full OS update</a>. In addition, dropping the cross-sign will reduce the number of certificate bytes sent in a TLS handshake by over 40%. Finally, it will significantly reduce our operating costs, allowing us to focus our funding on continuing to improve your privacy and security.</p>
<p>For these reasons, we will not be getting a new cross-sign to extend compatibility any further.</p>
<p>The transition will roll out as follows:</p>
<ul>
<li>
<p>On <strong>Thursday, Feb 8th, 2024</strong>, we will stop providing the cross-sign by default in requests made to our /acme/certificate API endpoint. For most Subscribers, this means that your ACME client will configure a chain which terminates at ISRG Root X1, and your webserver will begin providing this shorter chain in all TLS handshakes. The longer chain, terminating at the soon-to-expire cross-sign, will still be available as an alternate chain which you can configure your client to request.</p>
</li>
<li>
<p>On <strong>Thursday, June 6th, 2024</strong>, we will stop providing the longer cross-signed chain entirely. This is just over <a href="https://letsencrypt.org/2015/11/09/why-90-days.html">90 days</a> (the lifetime of one certificate) before the cross-sign expires, and we need to make sure subscribers have had at least one full issuance cycle to migrate off of the cross-signed chain.</p>
</li>
<li>
<p>On <strong>Monday, September 30th, 2024</strong>, the cross-signed certificate will expire. This should be a non-event for most people, as any client breakages should have occurred over the preceding six months.</p>
</li>
</ul>
<p><img alt="Infographic of the distribution of installed Android versions, showing that 93.9% of the population is running Android 7.1 or above." src="https://letsencrypt.org/images/2023.07.10-android-version-distribution.png">
</p>
<p><strong>If you use Android 7.0 or earlier</strong>, you may need to take action to ensure you can still access websites secured by Let’s Encrypt certificates. We recommend installing and using <a href="https://www.mozilla.org/en-US/firefox/browsers/mobile/android/">Firefox Mobile</a>, which uses its own trust store instead of the Android OS trust store, and therefore trusts ISRG Root X1.</p>
<p><strong>If you are a site operator</strong>, you should keep an eye on your website usage statistics and active user-agent strings during Q2 and Q3 of 2024. If you see a sudden drop in visits from Android, it is likely because you have a significant population of users on Android 7.0 or earlier. We encourage you to provide the same advice to them as we provided above.</p>
<p><strong>If you are an ACME client author</strong>, please make sure that your client correctly downloads and installs the certificate chain provided by our API during every certificate issuance, including renewals. Failure modes we have seen in the past include a) never downloading the chain at all and only serving the end-entity certificate; b) never downloading the chain and instead serving a hard-coded chain; and c) only downloading the chain at first issuance and not re-downloading during renewals. Please ensure that your client does not fall into any of these buckets.</p>
<p>We appreciate your understanding and support, both now and in the years to come as we provide safe and secure communication to everyone who uses the web. If you have any questions about this transition or any of the other work we do, please ask on our <a href="https://community.letsencrypt.org/">community forum</a>.</p>
<p>We’d like to thank IdenTrust for their years of partnership. They played an important role in helping Let’s Encrypt get to where we are today and their willingness to arrange a stopgap cross sign in 2021 demonstrated a true commitment to creating a secure Web.&nbsp;</p>
<p>We depend on contributions from our supporters in order to provide our services. If your company or organization can help our work by becoming a <a href="https://www.abetterinternet.org/sponsor/">sponsor</a> of Let’s Encrypt please email us at <a href="mailto:sponsor@letsencrypt.org">sponsor@letsencrypt.org</a>. We ask that you make an <a href="https://letsencrypt.org/donate/">individual contribution</a> if it is within your means.</p>

	</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Self-hosted photo and video backups directly from your mobile phone (672 pts)]]></title>
            <link>https://github.com/immich-app/immich</link>
            <guid>36673224</guid>
            <pubDate>Mon, 10 Jul 2023 21:28:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/immich-app/immich">https://github.com/immich-app/immich</a>, See on <a href="https://news.ycombinator.com/item?id=36673224">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><div dir="auto"> 
    
  <p><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/7d08b7e3dec312341d94a4aff5add03658295489c509aa6a7af1d911a7b93e33/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e7376673f636f6c6f723d334635314235267374796c653d666f722d7468652d6261646765266c6162656c3d4c6963656e7365266c6f676f436f6c6f723d303030303030266c6162656c436f6c6f723d656365636563" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg?color=3F51B5&amp;style=for-the-badge&amp;label=License&amp;logoColor=000000&amp;labelColor=ececec"></a>
  <a href="https://discord.gg/D8JsnBEuKb" rel="nofollow">
    <img src="https://camo.githubusercontent.com/9951ac4f9dbf9adbd35d470c4668d875f6ffa8769bdb3f97d5b353ec154f5dac/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3937393131363632333837393336383735352e7376673f6c6162656c3d446973636f7264266c6f676f3d446973636f7264267374796c653d666f722d7468652d6261646765266c6f676f436f6c6f723d303030303030266c6162656c436f6c6f723d656365636563" data-canonical-src="https://img.shields.io/discord/979116623879368755.svg?label=Discord&amp;logo=Discord&amp;style=for-the-badge&amp;logoColor=000000&amp;labelColor=ececec">
  </a></p></div>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/immich-app/immich/blob/main/design/immich-logo.svg"><img src="https://github.com/immich-app/immich/raw/main/design/immich-logo.svg" width="150" title="Login With Custom URL"></a>
</p>
<h3 tabindex="-1" dir="auto">Immich - High performance self-hosted photo and video backup solution</h3>
<br>
<a href="https://immich.app/" rel="nofollow">
<img src="https://github.com/immich-app/immich/raw/main/design/immich-screenshots.png" title="Main Screenshot">
</a>

<p dir="auto">
  <a href="https://github.com/immich-app/immich/blob/main/README_zh_CN.md">中文</a>
  <a href="https://github.com/immich-app/immich/blob/main/README_tr_TR.md">Türkçe</a>
  <a href="https://github.com/immich-app/immich/blob/main/README_ca_ES.md">Català</a>
</p>
<h2 tabindex="-1" dir="auto">Disclaimer</h2>
<ul dir="auto">
<li><g-emoji alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png">⚠️</g-emoji> The project is under <strong>very active</strong> development.</li>
<li><g-emoji alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png">⚠️</g-emoji> Expect bugs and breaking changes.</li>
<li><g-emoji alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png">⚠️</g-emoji> <strong>Do not use the app as the only way to store your photos and videos!</strong></li>
</ul>
<h2 tabindex="-1" dir="auto">Content</h2>
<ul dir="auto">
<li><a href="https://immich.app/docs" rel="nofollow">Official Documentation</a></li>
<li><a href="https://github.com/orgs/immich-app/projects/1">Roadmap</a></li>
<li><a href="#demo">Demo</a></li>
<li><a href="#features">Features</a></li>
<li><a href="https://immich.app/docs/overview/introduction" rel="nofollow">Introduction</a></li>
<li><a href="https://immich.app/docs/install/requirements" rel="nofollow">Installation</a></li>
<li><a href="https://immich.app/docs/overview/support-the-project" rel="nofollow">Contribution Guidelines</a></li>
<li><a href="#support-the-project">Support The Project</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto">You can find the main documentation, including installation guides, at <a href="https://immich.app/" rel="nofollow">https://immich.app/</a>.</p>
<h2 tabindex="-1" dir="auto">Demo</h2>
<p dir="auto">You can access the web demo at <a href="https://demo.immich.app/" rel="nofollow">https://demo.immich.app</a></p>
<p dir="auto">For the mobile app, you can use <code>https://demo.immich.app/api</code> for the <code>Server Endpoint URL</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="The credential
email: demo@immich.app
password: demo"><pre>The credential
email: demo@immich.app
password: demo</pre></div>
<div data-snippet-clipboard-copy-content="Spec: Free-tier Oracle VM - Amsterdam - 2.4Ghz quad-core ARM64 CPU, 24GB RAM"><pre><code>Spec: Free-tier Oracle VM - Amsterdam - 2.4Ghz quad-core ARM64 CPU, 24GB RAM
</code></pre></div>
<h2 tabindex="-1" dir="auto">Features</h2>
<table>
<thead>
<tr>
<th>Features</th>
<th>Mobile</th>
<th>Web</th>
</tr>
</thead>
<tbody>
<tr>
<td>Upload and view videos and photos</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Auto backup when the app is opened</td>
<td>Yes</td>
<td>N/A</td>
</tr>
<tr>
<td>Selective album(s) for backup</td>
<td>Yes</td>
<td>N/A</td>
</tr>
<tr>
<td>Download photos and videos to local device</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Multi-user support</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Album and Shared albums</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Scrubbable/draggable scrollbar</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Support raw formats</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Metadata view (EXIF, map)</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Search by metadata, objects, faces, and CLIP</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Administrative functions (user management)</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Background backup</td>
<td>Yes</td>
<td>N/A</td>
</tr>
<tr>
<td>Virtual scroll</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>OAuth support</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>API Keys</td>
<td>N/A</td>
<td>Yes</td>
</tr>
<tr>
<td>LivePhoto backup and playback</td>
<td>iOS</td>
<td>Yes</td>
</tr>
<tr>
<td>User-defined storage structure</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Public Sharing</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Archive and Favorites</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Global Map</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Partner Sharing</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Facial recognition and clustering</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Memories (x years ago)</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Offline support</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Read-only gallery</td>
<td>Yes</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">Support the project</h2>
<p dir="auto">I've committed to this project, and I will not stop. I will keep updating the docs, adding new features, and fixing bugs. But I can't do it alone. So I need your help to give me additional motivation to keep going.</p>
<p dir="auto">As our hosts in the <a href="https://selfhosted.show/79?t=1418" rel="nofollow">selfhosted.show - In the episode 'The-organization-must-not-be-name is a Hostile Actor'</a> said, this is a massive undertaking of what the team and I are doing. And I would love to someday be able to do this full-time, and I am asking for your help to make that happen.</p>
<p dir="auto">If you feel like this is the right cause and the app is something you are seeing yourself using for a long time, please consider supporting the project with the option below.</p>
<h2 tabindex="-1" dir="auto">Donation</h2>
<ul dir="auto">
<li><a href="https://github.com/sponsors/alextran1502">Monthly donation</a> via GitHub Sponsors</li>
<li><a href="https://github.com/sponsors/alextran1502?frequency=one-time&amp;sponsor=alextran1502">One-time donation</a> via GitHub Sponsors</li>
<li><a href="https://liberapay.com/alex.tran1502/" rel="nofollow">Librepay</a></li>
<li><a href="https://www.buymeacoffee.com/altran1502" rel="nofollow">buymeacoffee</a></li>
<li>Bitcoin: 1FvEp6P6NM8EZEkpGUFAN2LqJ1gxusNxZX</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gandi.net updates pricing, increases rates by up to 1000% (114 pts)]]></title>
            <link>https://chatting.neocities.org/posts/2023-gandi-pricing</link>
            <guid>36673108</guid>
            <pubDate>Mon, 10 Jul 2023 21:20:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chatting.neocities.org/posts/2023-gandi-pricing">https://chatting.neocities.org/posts/2023-gandi-pricing</a>, See on <a href="https://news.ycombinator.com/item?id=36673108">Hacker News</a></p>
<div id="readability-page-1" class="page">

<h5 id="july-10-2023">July 10, 2023</h5>
<hr>
<p>According to <a href="https://chatting.neocities.org/documents/gandi-pricing-email.html">an email</a> sent to its customers in mid-June, Gandi.net, a major European domain name registration and web hosting company, is <a href="https://gandi.link/eur-prices2023">increasing</a> prices across its entire suite of products.</p>
<p>For domain names, the new prices will usually amount to a 25 to 50% increase, depending on the extension. For example, .com domain prices will jump by 34%, .net by 45%, .org by 32%, etc.</p>
<p>A much steeper price hike is awaiting web hosting customers, who will see their costs almost double in some cases. In particular, the Advanced web hosting plan will be raised from €8.25/month to €15.00/month, an 82% surge.</p>
<p>But by far the most egregious is the email service.</p>
<p>Gandi had already started to inform customers on their intention to <a href="https://news.ycombinator.com/item?id=35080777">discontinue</a> their offer of 2 free mailboxes included with every domain name registration, including for users who had pre-paid for multiple years.</p>
<p>Now, Gandi will raise the prices of the Standard mailbox plan from €0.35/month to €3.99/month per mailbox, <b>a 1040% increase</b>. The Premium plan will go from €1.75/month to €5.99/month, a 242% increase.</p>
<p>In its announcement, Gandi appears to justify these price increases by describing the product as “new and improved”, citing increased storage (<i>editor’s note: this does not apply to Premium plan customers</i>) and other nebulous commitments, such as “better anti-spam protection”, and “stronger security”.</p>
<p>The price hikes will come into effect on July 13.</p>
<p><b>It is unclear how with the action of dropping extreme price increases on customers, backed by dubious justifications, and with barely a month’s notice can be reconciled with Gandi’s famous and long-standing <a href="https://www.gandi.net/en/no-bullshit">“No Bullshit” policy</a>.</b></p>
<p>At the time of writing, Gandi has still not updated its website (apart from the <a href="https://www.gandi.net/en/domain/email">email hosting page</a>, which does feature the new prices) to inform potential new customers of the upcoming changes.</p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Stripe is holding 50% for 9 month (114 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36673091</link>
            <guid>36673091</guid>
            <pubDate>Mon, 10 Jul 2023 21:18:53 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36673091">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="36674721"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36674721" href="https://news.ycombinator.com/vote?id=36674721&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>Some staff members from Stripe already replied that they are looking into this. But if you take a look at reviews on Trustpilot this (withholding funds) seems to be a very common complain, which is terrifying.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36674461"><td></td></tr>
                      <tr id="36674703"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36674703" href="https://news.ycombinator.com/vote?id=36674703&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>Every time one of these Stripe posts shows up, I always wonder what'll happen when Edwin retires or moves on.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36674963"><td></td></tr>
                  <tr id="36675493"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36675493" href="https://news.ycombinator.com/vote?id=36675493&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>What is the alternative to stripe? I was going to set it up for a new service in making but with all these posts I'm not so sure.<p>A quick google brings up paddle, is that going to have the same issues?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36675243"><td></td></tr>
            <tr id="36675031"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36675031" href="https://news.ycombinator.com/vote?id=36675031&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>9 month is long. I thought credit card companies only hold for 3 months at the most so not sure why Stripe needs it 3x longer</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36675695"><td></td></tr>
                  <tr id="36675561"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36675561" href="https://news.ycombinator.com/vote?id=36675561&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>Stop using Stripe, this is what you sign up for when you use them. PayPal is not much better. Use Adyen, or a processor that doesn't have to squeeze every dollar out of the transaction like Google Payments.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36675789"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36675789" href="https://news.ycombinator.com/vote?id=36675789&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>This is a regulatory problem not a business problem. Payment processors need to understand a client's risk profile and what business they do in order to prevent money laundering, and in practice, regulators only care about clients that have a certain volume.<p>Paypal, Stripe, and any other major payment processor have developed a strategy of letting users use the service with no restrictions until they hit that volume then they begin doing things like this.</p><p>Ultimately, this is caused by regulation and needs to be fixed by regulation, but it won't because practices like this are extremely lucrative.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36674629"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36674629" href="https://news.ycombinator.com/vote?id=36674629&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>Thank you for submitting your support request through the Stripe back channel support network.  Your ticket will be reviewed by nobody, unless you happen to be extremely lucky and win the Hacker News lottery.  Best of luck!</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36675273"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36675273" href="https://news.ycombinator.com/vote?id=36675273&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>I'm sorry your request has been denied by some schmuck you slighted in 3rd year of uni who works at stripe.<p>Too bad there is no incentive for companies to put in place adequate volume to deal with necessary customer support.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36675487"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36675487" href="https://news.ycombinator.com/vote?id=36675487&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>The shareholders have proclaimed that such a strategy is illegal due to fiduciary duty /s</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="36676060"><td></td></tr>
            <tr id="36675998"><td></td></tr>
            <tr id="36673940"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36673940" href="https://news.ycombinator.com/vote?id=36673940&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>There are a bunch of factors—we look at your refund and chargeback rates, then your revenue and how much you have in your Stripe balance. For macro factors, the card networks may also sense an uptick in refund requests depending on the product/service you're selling.<p>That said, I see your email into our support team and we're taking another look at this now.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36674267"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36674267" href="https://news.ycombinator.com/vote?id=36674267&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>It's a bit horrifying it takes a disgruntled HN post and <i>the pure chance</i> you happened across it. It's also discomforting how comfortable Stripe employees like yourself think holding 50% of someone's earning for any amount of time is acceptable. I really don't care if Stripe is HN's baby, this is awful customer service and gives me great pause in recommending or using Stripe.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36674329"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36674329" href="https://news.ycombinator.com/vote?id=36674329&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>I feel like these posts should be disallowed. Seems like Stripe is using HN as a free help forum, and it's customers have accepted that this is the place to go when you need to talk to a human. I don't think HN should encourage it, and I don't know why we all play along. I've seen a solid handful or two of these posts, and most of the time users aren't looking for discussion, only enough attention to get the Stripe team to act. My opinion only, obviously I sympathize with OP whose just trying to get their money and this seems to be the tried and true way to get a response, but I definitely don't think it adds anything positive to the site other than a recurring warning to other would-be Stripe customers.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36675014"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36675014" href="https://news.ycombinator.com/vote?id=36675014&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>&gt; other than a recurring warning to other would-be Stripe customers<p>that seems like an exceptionally high-value positive in a site with a large number of users who need payment services.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36674498"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36674498" href="https://news.ycombinator.com/vote?id=36674498&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>HN should encourage it, because startup founders read HN. They should think twice risking their business existence, after seeing this benevolent approach.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36674557"><td></td></tr>
                <tr id="36675455"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36675455" href="https://news.ycombinator.com/vote?id=36675455&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>Sunlight only works reasonably well for waterborne pathogens, and only when you've got time.<p>As with disinfecting anything, and especially for societal issues, we need more than sunlight alone.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36674851"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36674851" href="https://news.ycombinator.com/vote?id=36674851&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>Tell that to Edward Snowden, Julian Assange, or one of the billions of working families all over the world that <i>everyone</i> knows is getting f*cked over on a constant basis.<p>Sunlight is not a disinfectant. Better to try gasoline.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="36675244"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36675244" href="https://news.ycombinator.com/vote?id=36675244&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>These types of companies are often frauds (anything involving large amounts charged for travel, or housing of some sort). A bunch of <i>big</i> charges comes in, and then months later there are massive chargebacks and the company in question no longer exists. Stripe is left holding the bag. Their only option with these large charges for services to be delivered in the future is to not release funds until they are quite certain there won't be a charge back.<p>For every instance of what appears to be bad behavior by a payments company, consider the angle of the fraudster. They are really screwing things up for everybody.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36675298"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36675298" href="https://news.ycombinator.com/vote?id=36675298&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>In a working payment system, the scenario where someone initiates a valid dispute nine months after the fact would be extremely rare: it would be limited to a few types of physical card theft where the card owner is unable to report the card missing for some relatively unusual reason. We just don’t have a working payment system, so unfortunately this stuff is incredibly common and everyone has to suffer because of it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36675346"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36675346" href="https://news.ycombinator.com/vote?id=36675346&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>There are genuine cases (travel and lodging are the obvious ones) where people tend to pay for things months in advance. I don't see it going away. If you can come up with a cheap, effective, convenient payment method to cover these things, you are on a winner. I don't think it's possible because the time horizon creates risk, there is no avoiding it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="36674505"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36674505" href="https://news.ycombinator.com/vote?id=36674505&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>The alternative is likely not being able to work with the business at all due to their risk profile and chargeback rates.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36675032"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36675032" href="https://news.ycombinator.com/vote?id=36675032&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>That seems... better? At least you know what you're in for (ie. you can only take crypto or go with a risk-taking payment processor that has an explicit net 120 payout schedule), so you can plan your business around it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="36674159"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36674159" href="https://news.ycombinator.com/vote?id=36674159&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>As always, why bother having real support when you can instead wait for paying customer to bitch publicly on HN?<p>Sickening.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36674289"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36674289" href="https://news.ycombinator.com/vote?id=36674289&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>Sickening is hardly the word for a comms lead of Stripe being the first reply to a post about a customers issue, complete with some informative context and saying they will double check the issue.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36674376"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36674376" href="https://news.ycombinator.com/vote?id=36674376&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>Not the place for many reasons. As an example this discussion should not be publicly indexable whenever a customer Googles this same problem in the future.<p>Basic Marketing and Customer Care Management.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36674378"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36674378" href="https://news.ycombinator.com/vote?id=36674378&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>Are you new here? This website seems to be the only way to get human help from stripe. THAT is sickening.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36674531"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36674531" href="https://news.ycombinator.com/vote?id=36674531&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>What would you say the percentage of the total support requests that Stripe receives per day is compared to issues raised on HN?</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="36675291"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36675291" href="https://news.ycombinator.com/vote?id=36675291&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>When you hold on to someone's money for an extremely long time, do you earn any interest on that money?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36676021"><td></td></tr>
                  <tr id="36674192"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36674192" href="https://news.ycombinator.com/vote?id=36674192&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>Just wanted to say I appreciate seeing the responsiveness here. First comment on the thread even. Always better if it doesn’t reach this point, but it’s nice to know you can get a human’s attention one way or another.<p>Hoping to @edwinwee will share the outcome for other businesses who might need to evaluate Stripe.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36674562"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36674562" href="https://news.ycombinator.com/vote?id=36674562&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>You know what would be better? Maybe a support email? Or this crazy thing called a 1-800-number? The fact that Stripe de facto uses HN as their support forum and that people are lauding them for it is pretty laughable.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36674386"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36674386" href="https://news.ycombinator.com/vote?id=36674386&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>Search this website. Outcome is always the same: something resolved just this once. No procedural changes made. Stripe remains unreachable unless you get enough upvotes on HN</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36674849"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36674849" href="https://news.ycombinator.com/vote?id=36674849&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>The outcome isn't always the same.<p>It's 50/50:</p><p>- The poster was doing something shady and upon further investigation it turns out the situation is warranted</p><p>- The poster was indeed let down by a broken support system and Stripe staff chimes in to save the day
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36675253"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36675253" href="https://news.ycombinator.com/vote?id=36675253&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>The outcome is not always, or perhaps even often, the same.<p>The last time I paid attention to one of these complaints, someone was using stripe to sell cellphone accessories (and yes, at least on my Stripe application, they asked what I was selling.)</p><p>After selling accessories for a while, he then used it to sell a minivan, and had a tantrum when Stripe -- quite reasonably -- said this flags all the flags, and Stripe was going to hold onto the money until they were sure things were kosher.</p><p>Getting dollars before multiple months post transaction basically leaves someone holding a bag of risk, and if you act shady AF, you can't be surprised if Stripe declines to hold that bag of risk for you.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A hash array-mapped trie implementation in C (159 pts)]]></title>
            <link>https://github.com/mkirchner/hamt</link>
            <guid>36672957</guid>
            <pubDate>Mon, 10 Jul 2023 21:06:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mkirchner/hamt">https://github.com/mkirchner/hamt</a>, See on <a href="https://news.ycombinator.com/item?id=36672957">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">libhamt</h2>
<p dir="auto">A hash array-mapped trie (HAMT) implementation in C99. A HAMT is a data
structure that can be used to efficiently implement
<a href="https://en.wikipedia.org/wiki/Persistent_data_structure" rel="nofollow"><em>persistent</em></a> associative arrays (aka maps,
dicts) and sets, see the <a href="#introduction">Introduction</a>. The implementation here
loosely follows Bagwell's 2000 paper<a href="https://lampwww.epfl.ch/papers/idealhashtrees.pdf" rel="nofollow">[1]</a>, with a focus on
code clarity.</p>
<p dir="auto">What prompted the somewhat detailed writeup was the realization that there is
not a lot of in-depth documentation for HAMTs beyond the original Bagwell
paper[<a href="https://lampwww.epfl.ch/papers/idealhashtrees.pdf" rel="nofollow">1</a>]. Some of the more helpful posts are <a href="http://blog.higher-order.net/2009/09/08/understanding-clojures-persistenthashmap-deftwice.html" rel="nofollow">Karl Krukow's
intro to Clojure's <code>PersistentHashMap</code></a>, <a href="https://github.com/chaelim/HAMT">C. S. Lim's
C++ template implementation</a>, <a href="https://blog.acolyer.org/2015/11/27/hamt/" rel="nofollow">Adrian Coyler's morning paper
post</a> and the original <a href="https://michael.steindorfer.name/publications/oopsla15.pdf" rel="nofollow">Steindoerfer/Vinju compressed HAMT
article it summarizes</a>. The rest mostly seems to be
all bits and pieces and this document is an attempt to (partially) improve that
situation.</p>
<p dir="auto"><em>FIXME: Complete docs (removal, persistence, path copying)</em></p>
<h2 tabindex="-1" dir="auto">Quickstart</h2>
<p dir="auto">To build the library and run the tests:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ git clone git@github.com:mkirchner/hamt.git
$ cd hamt
$ make
$ make test"><pre>$ git clone git@github.com:mkirchner/hamt.git
$ <span>cd</span> hamt
$ make
$ make <span>test</span></pre></div>
<p dir="auto">In order to use <code>libhamt</code> in your own projects, copy <code>include/hamt.h</code> and
<code>src/hamt.c</code> in your own source tree and build from there.</p>
<h3 tabindex="-1" dir="auto">Benchmarks</h3>
<p dir="auto">For basic performance comparison with AVL and red-black trees (from <code>libavl</code>)
and the HashTree from GLib, see <a href="https://github.com/mkirchner/hamt-bench">the benchmarking repo</a>.</p>
<h2 tabindex="-1" dir="auto">Introduction</h2>
<p dir="auto">A <em>hash array mapped trie (HAMT)</em> is a data structure that can be used to
implement <a href="https://en.wikipedia.org/wiki/Associative_array" rel="nofollow">associative arrays</a> (aka maps) and
<a href="https://en.wikipedia.org/wiki/Set_(abstract_data_type)" rel="nofollow">sets</a>.</p>
<p dir="auto">Structurally, HAMTs are <a href="https://en.wikipedia.org/wiki/Hash_tree_(persistent_data_structure)" rel="nofollow">hash trees</a> that combine favorable
characteristics of <a href="https://en.wikipedia.org/wiki/Hash_table" rel="nofollow">hash tables</a> and array mapped
<a href="https://en.wikipedia.org/wiki/Trie" rel="nofollow">tries</a>, namely almost hash table-like time complexity
guarantees<a href="https://lampwww.epfl.ch/papers/idealhashtrees.pdf" rel="nofollow">[1]</a> (O(log<sub>32</sub>n)) and economic use of memory.</p>
<p dir="auto">An additional benefit, and a key motivation for the work presented here, is that
augmentation of HAMTs with path copying and garbage collection allows for a
straightforward and efficient implementation of <a href="https://en.wikipedia.org/wiki/Persistent_data_structure" rel="nofollow">persistent</a>
versions of maps and sets.</p>
<p dir="auto">The remaining documentation starts with a description of the <code>libhamt</code> API and
two examples that demonstrate the use of a HAMT as an ephemeral and persistent
data structure, respectively. It then details the implementation: starting from
the foundational data structures and the helper code required for hash
exhaustion and table management, we cover search, insertion, removal, and
iterators. The final implementation section introduces path copying and explains
the changes required to support persistent insert and remove operations. It
closes with an outlook and an appendix.</p>
<h2 tabindex="-1" dir="auto">API</h2>
<h2 tabindex="-1" dir="auto">HAMT lifecycle</h2>
<p dir="auto">The core data type exported in the <code>libhamt</code> interface is <code>struct hamt</code>. In order to
create a <code>struct hamt</code> instance, one must call <code>hamt_create()</code>, which requires a
hash function of type <code>hamt_key_hash_fn</code> to hash keys, a comparison function of
type <code>hamt_cmp_fn</code> to compare keys, and a pointer to a <code>hamt_allocator</code> instance.
<code>hamt_delete()</code> deletes <code>struct hamt</code> instances that were created with <code>hamt_create()</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="/* The libhamt core data structure is a handle to a hash array-mapped trie */

/* Function signature definitions for key comparison and hashing */
typedef int (*hamt_cmp_fn)(const void *lhs, const void *rhs);
typedef uint32_t (*hamt_key_hash_fn)(const void *key, const size_t gen);

/* API functions for lifecycle management */
struct hamt *hamt_create(hamt_key_hash_fn key_hash, hamt_cmp_fn key_cmp, struct hamt_allocator *ator);
void hamt_delete(struct hamt *);"><pre><span>/* The libhamt core data structure is a handle to a hash array-mapped trie */</span>

<span>/* Function signature definitions for key comparison and hashing */</span>
<span>typedef</span> <span>int</span> (<span>*</span><span>hamt_cmp_fn</span>)(<span>const</span> <span>void</span> <span>*</span><span>lhs</span>, <span>const</span> <span>void</span> <span>*</span><span>rhs</span>);
<span>typedef</span> <span>uint32_t</span> (<span>*</span><span>hamt_key_hash_fn</span>)(<span>const</span> <span>void</span> <span>*</span><span>key</span>, <span>const</span> <span>size_t</span> <span>gen</span>);

<span>/* API functions for lifecycle management */</span>
<span>struct</span> <span>hamt</span> <span>*</span><span>hamt_create</span>(<span>hamt_key_hash_fn</span> <span>key_hash</span>, <span>hamt_cmp_fn</span> <span>key_cmp</span>, <span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>);
<span>void</span> <span>hamt_delete</span>(<span>struct</span> <span>hamt</span> <span>*</span>);</pre></div>
<p dir="auto">The <code>hamt_key_hash_fn</code> takes a <code>key</code> and a generation <code>gen</code>. The expectation is
that the supplied hash function returns different hashes for the same key but
different generations. Depending on the choice of hash function this can be
implemented using <code>gen</code> as a seed or modifying a copy of <code>key</code> on the fly.
See the <a href="#examples">examples</a> section for a <code>murmur3</code>-based implementation and
the <a href="#hashing">hashing</a> section for more information on suitable hash functions.</p>
<h3 tabindex="-1" dir="auto">Memory management</h3>
<p dir="auto"><code>libhamt</code> exports its internal memory management API through the <code>hamt_allocator</code>
struct. The struct specifies the functions that the HAMT implementation uses to
allocate, re-allocate and deallocate system memory. The API provides a default
<code>hamt_allocator_default</code> which refers to the standard <code>malloc()</code>, <code>realloc()</code>
and <code>free()</code> functions.</p>
<div dir="auto" data-snippet-clipboard-copy-content="struct hamt_allocator {
    void *(*malloc)(const size_t size);
    void *(*realloc)(void *chunk, const size_t size);
    void (*free)(void *chunk);
};

extern struct hamt_allocator hamt_allocator_default;"><pre><span>struct</span> <span>hamt_allocator</span> {
    <span>void</span> <span>*</span>(<span>*</span><span>malloc</span>)(<span>const</span> <span>size_t</span> <span>size</span>);
    <span>void</span> <span>*</span>(<span>*</span><span>realloc</span>)(<span>void</span> <span>*</span><span>chunk</span>, <span>const</span> <span>size_t</span> <span>size</span>);
    <span>void</span> (<span>*</span><span>free</span>)(<span>void</span> <span>*</span><span>chunk</span>);
};

<span>extern</span> <span>struct</span> <span>hamt_allocator</span> <span>hamt_allocator_default</span>;</pre></div>
<p dir="auto">Exporting the <code>libhamt</code> memory management API enables library clients to make
use of alternate memory management solutions, most notably of garbage collection
solutions (e.g. the <a href="https://www.hboehm.info/gc/" rel="nofollow">Boehm-Demers-Weiser GC</a>) which are required when
using the HAMT as a persistent data structure (see the <a href="#example-2-garbage-collected-persistent-hamts">structural sharing
example</a>).</p>
<h2 tabindex="-1" dir="auto">Query</h2>
<div dir="auto" data-snippet-clipboard-copy-content="size_t hamt_size(const struct hamt *trie);
const void *hamt_get(const struct hamt *trie, void *key);"><pre><span>size_t</span> <span>hamt_size</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>);
<span>const</span> <span>void</span> <span>*</span><span>hamt_get</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>);</pre></div>
<p dir="auto">The <code>hamt_size()</code> function returns the size of the HAMT in O(1). Querying the
HAMT (i.e. searching a key) is done with <code>hamt_get()</code> which takes a pointer to a
key and returns a result in O(log<sub>32</sub> n) - or <code>NULL</code> if the key does
not exist in the HAMT.</p>
<h3 tabindex="-1" dir="auto">Iterators</h3>
<p dir="auto">The API also provides key/value pair access through the <code>hamt_iterator</code> struct.</p>
<div dir="auto" data-snippet-clipboard-copy-content="size_t hamt_size(const struct hamt *trie);
const void *hamt_get(const struct hamt *trie, void *key);"><pre><span>size_t</span> <span>hamt_size</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>);
<span>const</span> <span>void</span> <span>*</span><span>hamt_get</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>);</pre></div>
<p dir="auto">Iterators are tied to a specific HAMT and are created using the
<code>hamt_it_create()</code> function, passing the HAMT instance the iterator should refer
to. Iterators can be advanced with the <code>hamt_it_next()</code> function and as long as
<code>hamt_it_valid()</code> returns <code>true</code>, the <code>hamt_it_get_key()</code> and
<code>hamt_it_get_value()</code> functions will return the pointers to the current
key/value pair. In order to delete an existing and/or exhausted iterator, call
<code>hamt_it_delete()</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct hamt_iterator_impl *hamt_iterator;

hamt_iterator hamt_it_create(const struct hamt *trie);
void hamt_it_delete(hamt_iterator it);
bool hamt_it_valid(hamt_iterator it);
hamt_iterator hamt_it_next(hamt_iterator it);
const void *hamt_it_get_key(hamt_iterator it);
const void *hamt_it_get_value(hamt_iterator it);"><pre><span>typedef</span> <span>struct</span> <span>hamt_iterator_impl</span> <span>*</span><span>hamt_iterator</span>;

<span>hamt_iterator</span> <span>hamt_it_create</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>);
<span>void</span> <span>hamt_it_delete</span>(<span>hamt_iterator</span> <span>it</span>);
<span>bool</span> <span>hamt_it_valid</span>(<span>hamt_iterator</span> <span>it</span>);
<span>hamt_iterator</span> <span>hamt_it_next</span>(<span>hamt_iterator</span> <span>it</span>);
<span>const</span> <span>void</span> <span>*</span><span>hamt_it_get_key</span>(<span>hamt_iterator</span> <span>it</span>);
<span>const</span> <span>void</span> <span>*</span><span>hamt_it_get_value</span>(<span>hamt_iterator</span> <span>it</span>);</pre></div>
<p dir="auto">Iterators maintain state about their traversal path and changes to the HAMT
that an iterator refers to implicitly invalidate the iteration (i.e. undefined
behavior).</p>
<p dir="auto">The order in which iterators return the key value pairs is fully defined by
the structure of the trie, which, in turn, is completely defined by the choice
of hash function and (where applicable) seed.</p>
<h2 tabindex="-1" dir="auto">Insert &amp; Remove</h2>
<p dir="auto"><code>libhamt</code> supports ephemeral and
<a href="https://en.wikipedia.org/wiki/Persistent_data_structure" rel="nofollow">persistent</a> (aka not ephemeral) HAMTs through two different interfaces:
<code>hamt_set()</code> and <code>hamt_remove()</code> for ephemeral use, and their <code>p</code>-versions
<code>hamt_pset()</code> and <code>hamt_premove()</code> for persistent use.</p>
<h3 tabindex="-1" dir="auto">Ephemeral modification</h3>
<div dir="auto" data-snippet-clipboard-copy-content="const void *hamt_set(struct hamt *trie, void *key, void *value);
void *hamt_remove(struct hamt *trie, void *key);"><pre><span>const</span> <span>void</span> <span>*</span><span>hamt_set</span>(<span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>, <span>void</span> <span>*</span><span>value</span>);
<span>void</span> <span>*</span><span>hamt_remove</span>(<span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>);</pre></div>
<p dir="auto"><code>hamt_set()</code> takes a pair of <code>key</code> and <code>value</code> pointers and adds the pair to the HAMT,
returning a pointer to the <code>value</code>. If the <code>key</code> already exists, <code>hamt_set()</code>
updates the pointer to the <code>value</code>.</p>
<p dir="auto"><code>hamt_remove()</code> takes a <code>key</code> and removes the key/value pair with the
respective <code>key</code> from the HAMT, returning a pointer to the <code>value</code> that was
just removed. If the <code>key</code> does not exist, <code>hamt_remove()</code> returns <code>NULL</code>.</p>
<h3 tabindex="-1" dir="auto">Persistent HAMTs</h3>
<p dir="auto">The semantics of persistent HAMTs are different from their ephemeral
counterparts: since every modification creates a new version of a HAMT, the
modificiation functions return a new HAMT. Modification of a persistent HAMT
therefore requires a reassignment idiom if the goal is modification only:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const struct hamt *h = hamt_create(...)
...
/* Set a value and drop the reference to the old HAMT; the GC
 * will take care of cleaning up remaining unreachable allocations.
 */
h = hamt_pset(h, some_key, some_value);
..."><pre><span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>h</span> <span>=</span> <span>hamt_create</span>(...)
...
<span>/* Set a value and drop the reference to the old HAMT; the GC</span>
<span> * will take care of cleaning up remaining unreachable allocations.</span>
<span> */</span>
<span>h</span> <span>=</span> <span>hamt_pset</span>(<span>h</span>, <span>some_key</span>, <span>some_value</span>);
...</pre></div>
<p dir="auto">This seems wasteful at first glance but the respective functions implement structural
sharing such that the overhead is limited to <em>~log<sub>32</sub>(N)</em> nodes (where <em>N</em> is the
number of nodes in the graph).</p>
<div dir="auto" data-snippet-clipboard-copy-content="const struct hamt *hamt_pset(const struct hamt *trie, void *key, void *value);
const struct hamt *hamt_premove(const struct hamt *trie, void *key);"><pre><span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>hamt_pset</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>, <span>void</span> <span>*</span><span>value</span>);
<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>hamt_premove</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>);</pre></div>
<p dir="auto"><code>hamt_pset()</code> inserts or updates the <code>key</code> with <code>value</code> and returns an opaque
handle to the new HAMT. The new HAMT is guaranteed to contain the new
key/value pair.</p>
<p dir="auto"><code>hamt_premove()</code> attempts to remove the value with the key <code>key</code>. It is <em>not</em>
an error if the key does not exist; the new HAMT is guaranteed to not contain
the key <code>key</code>.</p>
<h2 tabindex="-1" dir="auto">Examples</h2>
<h3 tabindex="-1" dir="auto">Example 1: ephemeral HAMT w/ standard allocation</h3>
<div dir="auto" data-snippet-clipboard-copy-content="#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#include &quot;hamt.h&quot;
#include &quot;murmur3.h&quot;


static uint32_t hash_string(const void *key, const size_t gen)
{
    return murmur3_32((uint8_t *)key, strlen((const char *)key), gen);
}

int main(int argn, char *argv[])
{
    enum { N = 5; };
    struct {
        char *country;
        char *capital;
    } cities[N] = {
        {&quot;Germany&quot;, &quot;Berlin&quot;},
        {&quot;Spain&quot;, &quot;Madrid&quot;},
        {&quot;Italy&quot;, &quot;Rome&quot;},
        {&quot;France&quot;, &quot;Paris&quot;},
        {&quot;Romania&quot;, &quot;Bucharest&quot;}
        /* ... */
    };

    struct hamt *t;

    /* create table */
    t = hamt_create(hash_string, strcmp, &amp;hamt_allocator_default);
    /* load table */
    for (size_t i = 0; i < N; i++) {
        hamt_set(t, cities[i].country, cities[i].capital);
    }

    /* query table */
    for (size_t i = 0; i < N; i++) {
        printf(&quot;%s has capital %s\n&quot;, cities[i].country,
                                      hamt_get(t, cities[i].country));
    }
    /* cleanup */
    hamt_delete(t);
    return 0;
}"><pre><span>#include</span> <span>&lt;stdint.h&gt;</span>
<span>#include</span> <span>&lt;stdio.h&gt;</span>
<span>#include</span> <span>&lt;stdlib.h&gt;</span>
<span>#include</span> <span>&lt;string.h&gt;</span>

<span>#include</span> <span>"hamt.h"</span>
<span>#include</span> <span>"murmur3.h"</span>


<span>static</span> <span>uint32_t</span> <span>hash_string</span>(<span>const</span> <span>void</span> <span>*</span><span>key</span>, <span>const</span> <span>size_t</span> <span>gen</span>)
{
    <span>return</span> <span>murmur3_32</span>((<span>uint8_t</span> <span>*</span>)<span>key</span>, <span>strlen</span>((<span>const</span> <span>char</span> <span>*</span>)<span>key</span>), <span>gen</span>);
}

<span>int</span> <span>main</span>(<span>int</span> <span>argn</span>, <span>char</span> <span>*</span><span>argv</span>[])
{
    <span>enum</span> { <span>N</span> <span>=</span> <span>5</span>; };
    <span>struct</span> {
        <span>char</span> <span>*</span><span>country</span>;
        <span>char</span> <span>*</span><span>capital</span>;
    } <span>cities</span>[<span>N</span>] <span>=</span> {
        {<span>"Germany"</span>, <span>"Berlin"</span>},
        {<span>"Spain"</span>, <span>"Madrid"</span>},
        {<span>"Italy"</span>, <span>"Rome"</span>},
        {<span>"France"</span>, <span>"Paris"</span>},
        {<span>"Romania"</span>, <span>"Bucharest"</span>}
        <span>/* ... */</span>
    };

    <span>struct</span> <span>hamt</span> <span>*</span><span>t</span>;

    <span>/* create table */</span>
    <span>t</span> <span>=</span> <span>hamt_create</span>(<span>hash_string</span>, <span>strcmp</span>, <span>&amp;</span><span>hamt_allocator_default</span>);
    <span>/* load table */</span>
    <span>for</span> (<span>size_t</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>N</span>; <span>i</span><span>++</span>) {
        <span>hamt_set</span>(<span>t</span>, <span>cities</span>[<span>i</span>].<span>country</span>, <span>cities</span>[<span>i</span>].<span>capital</span>);
    }

    <span>/* query table */</span>
    <span>for</span> (<span>size_t</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>N</span>; <span>i</span><span>++</span>) {
        <span>printf</span>(<span>"%s has capital %s\n"</span>, <span>cities</span>[<span>i</span>].<span>country</span>,
                                      <span>hamt_get</span>(<span>t</span>, <span>cities</span>[<span>i</span>].<span>country</span>));
    }
    <span>/* cleanup */</span>
    <span>hamt_delete</span>(<span>t</span>);
    <span>return</span> <span>0</span>;
}</pre></div>
<h3 tabindex="-1" dir="auto">Example 2: Garbage-collected persistent HAMTs</h3>
<p dir="auto">The key to making use of structural sharing is to provide <code>libhamt</code> with a
<code>struct hamt_allocator</code> instance that implements garbage collection.</p>
<p dir="auto">The example below uses the the <a href="https://www.hboehm.info/gc/" rel="nofollow">Boehm-Demers-Weiser</a> GC. For
GC installation, compilation and linking instructions, please refer to the GC
documentation.</p>
<p dir="auto">In brief, the Boehm GC provides a <code>gc.h</code> include file and drop-in replacements
for the standard memory management functions, including <code>malloc</code>, <code>realloc</code>
and <code>free</code>.</p>
<p dir="auto">The following snippet illustrates the required changes:</p>
<div dir="auto" data-snippet-clipboard-copy-content="...
#include &quot;gc.h&quot;  /* Boehm-Demers-Weiser GC */

...

inline void nop(void *_) { return; }

int main(int argc, char *argv[]) {
    ...
    /*
    Set up garbage collection. We set the function pointer for `free` to
    NULL to avoid explicit freeing of memory.
    */
    struct hamt_allocator gc_alloc = {GC_malloc, GC_realloc, nop};
    const struct hamt *t = hamt_create(hash_string, strcmp, &amp;gc_alloc);
    ...
}"><pre>...
<span>#include</span> <span>"gc.h"</span>  <span>/* Boehm-Demers-Weiser GC */</span>

...

<span>inline</span> <span>void</span> <span>nop</span>(<span>void</span> <span>*</span><span>_</span>) { <span>return</span>; }

<span>int</span> <span>main</span>(<span>int</span> <span>argc</span>, <span>char</span> <span>*</span><span>argv</span>[]) {
    ...
    <span>/*</span>
<span>    Set up garbage collection. We set the function pointer for `free` to</span>
<span>    NULL to avoid explicit freeing of memory.</span>
<span>    */</span>
    <span>struct</span> <span>hamt_allocator</span> <span>gc_alloc</span> <span>=</span> {<span>GC_malloc</span>, <span>GC_realloc</span>, <span>nop</span>};
    <span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>t</span> <span>=</span> <span>hamt_create</span>(<span>hash_string</span>, <span>strcmp</span>, <span>&amp;</span><span>gc_alloc</span>);
    ...
}</pre></div>
<p dir="auto">We set the <code>gc_alloc.free</code> function pointer to point to <code>nop()</code>, a
no-operation function. This is necessary to ensure that we rely on the garbage
collector. If we were to provide a pointer to <code>GC_free()</code> (i.e. GC's drop-in
replacement for the <code>free()</code> function), we would still implement explicit
deallocation, just with a different free function.</p>
<h3 tabindex="-1" dir="auto">Example 3: Iterators</h3>
<p dir="auto">The following snipped illustrates how to create, test, exhaust and dispose of
an iterator. We first create the iterator using <code>hamt_it_create()</code>, jump into
a <code>while</code> loop and advance the iterator using <code>hamt_it_next()</code> while the
iterator is valid. In every interation we print the current key/value pair to
<code>stdout</code>. Once we exit the loop, we clean up using <code>hamt_it_delete()</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="    ...
    struct hamt *t = hamt_create(hash_string, strcmp, &amp;hamt_allocator_default);

    /* load table */
    ...

    /* create iterator */
    hamt_iterator it = hamt_it_create(t);
    while (hamt_it_valid(it)) {
        printf(&quot;(%s, %s)\n&quot;, (char *)hamt_it_get_key(it),
                             (char *)hamt_it_get_value(it));
        hamt_it_next(it);
    }
    /* clean up */
    hamt_it_delete(it);

    ...
    hamt_delete(t);
    ..."><pre>    ...
    <span>struct</span> <span>hamt</span> <span>*</span><span>t</span> <span>=</span> <span>hamt_create</span>(<span>hash_string</span>, <span>strcmp</span>, <span>&amp;</span><span>hamt_allocator_default</span>);

    <span>/* load table */</span>
    ...

    <span>/* create iterator */</span>
    <span>hamt_iterator</span> <span>it</span> <span>=</span> <span>hamt_it_create</span>(<span>t</span>);
    <span>while</span> (<span>hamt_it_valid</span>(<span>it</span>)) {
        <span>printf</span>(<span>"(%s, %s)\n"</span>, (<span>char</span> <span>*</span>)<span>hamt_it_get_key</span>(<span>it</span>),
                             (<span>char</span> <span>*</span>)<span>hamt_it_get_value</span>(<span>it</span>));
        <span>hamt_it_next</span>(<span>it</span>);
    }
    <span>/* clean up */</span>
    <span>hamt_it_delete</span>(<span>it</span>);

    ...
    <span>hamt_delete</span>(<span>t</span>);
    ...</pre></div>
<p dir="auto">This concludes the description of the <code>libhamt</code> interface and we now move on
to detailed implementation notes.</p>
<h2 tabindex="-1" dir="auto">Implementation</h2>
<h2 tabindex="-1" dir="auto">Prelude: Setup</h2>
<h3 tabindex="-1" dir="auto">Project structure</h3>
<p dir="auto">The <code>hamt</code> source tree has the following structure:</p>
<div data-snippet-clipboard-copy-content="hamt/
  build/         Out-of-source build destination
  include/       Header files that are part of the interface
  src/           Source and header files
  test/          Test and utility headers &amp; sources
  Makefile"><pre><code>hamt/
  build/         Out-of-source build destination
  include/       Header files that are part of the interface
  src/           Source and header files
  test/          Test and utility headers &amp; sources
  Makefile
</code></pre></div>
<p dir="auto">Sources are organized in three folders: the <code>include</code> folder, for all header
files that are part of the public interface; the <code>src</code> folder, for the
actual implementation and private header files; and the <code>test</code> folder, for all
test code, including headers and sources for testing utilities (e.g. data
loading and benchmarking functions).</p>
<p dir="auto">The build process is governed by a single <code>Makefile</code> in the project root
directory.</p>
<h3 tabindex="-1" dir="auto">Programming Style</h3>
<h3 tabindex="-1" dir="auto">Building the project</h3>
<p dir="auto">To build the library and run the tests:</p>

<h2 tabindex="-1" dir="auto">Design</h2>
<h3 tabindex="-1" dir="auto">Introduction</h3>
<p dir="auto"><strong>Hash tables.</strong> A common and practical answer to efficient value retrieval
from a collection given a key is to "use a <em>hash table</em>".  This is good
advice. <em>Hash tables</em> provide insert, modification, and retrieval in amortized
constant average time, using space linear in the number of elements they
store.  They have been the subject of intensive research and
optimization and are part of <a href="https://www.amazon.com/Algorithms-4th-Robert-Sedgewick/dp/032157351X" rel="nofollow">every</a>
<a href="https://www.amazon.com/Introduction-Algorithms-3rd-MIT-Press/dp/0262033844/ref=zg_bs_491298_1/147-2375898-2942653?pd_rd_i=0262033844&amp;psc=1" rel="nofollow">introductory</a> CS textbook.  Chances are that the
standard library of the languange at hand contains a readily available, tried
and tested implementation.</p>
<p dir="auto">For instance, <code>std::unordered_set</code> and <code>std::unordered_map</code> (and their
<code>*_multiset</code> cousins) are hash table implementations for C++ <sup id="user-content-ac_hash_table_cpp"><a href="#fn_hash_table_cpp">1</a></sup>; for C, multiple
<a href="https://en.wikipedia.org/wiki/C_standard_library" rel="nofollow">libc</a> implementations (e.g. <a href="https://en.wikipedia.org/wiki/Glibc" rel="nofollow">glibc</a>, <a href="https://www.musl-libc.org/" rel="nofollow">musl</a>,
<a href="https://en.wikipedia.org/wiki/C_standard_library#BSD_libc" rel="nofollow">BSD libc</a>) provide POSIX-compliant <code>hsearch</code> facilities,
GNOME's <a href="https://en.wikipedia.org/wiki/GLib" rel="nofollow">GLib</a>
and others provide <a href="https://gitlab.gnome.org/GNOME/glib/-/blob/main/glib/ghash.c" rel="nofollow">hash table</a> implementations<sup id="user-content-ac_hash_table_c"><a href="#fn_hash_table_c">2</a></sup>. Python has the <code>dict</code> type
for associative arrays which <a href="https://stackoverflow.com/a/9022835" rel="nofollow">is implemented as a hash
table</a><sup id="user-content-ac_hash_table_python"><a href="#fn_hash_table_python">3</a></sup>.  Java has
<code>Hashtable</code>, <code>HashMap</code>, and <code>HashSet</code> <sup id="user-content-ac_hash_table_java"><a href="#fn_hash_table_java">4</a></sup> and JavaScript has
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map" rel="nofollow"><code>Map</code></a>.</p>
<p dir="auto">One property of the classical hash table implementations is that they do not
provide support for <em>persistence</em> (in the sense of <a href="https://en.wikipedia.org/wiki/Persistent_data_structure" rel="nofollow">persistent data
structures</a>, not persistent storage). They are a
<a href="https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/ValueOfValues.md">place-oriented</a> solution to associative storage and
make destructive modifications to the data structure when the data changes
(note that this is independent of any particular conflict resolution and
capacity maintenance strategies).</p>
<p dir="auto">Persistent associative containers require a different approach.</p>
<p dir="auto"><strong>Persistent data structures.</strong> <em>(Full) persistence</em> is the property of a data
structure to always preserve (all) previous versions if itself under
modification. The property is related to
<a href="https://en.wikipedia.org/wiki/Immutable_object" rel="nofollow">immutability</a>: from the perspective of the client,
every update yields a new copy, making instances practically immutable. This
is a huge conceptual change: if data structures are immutable, functions using
these data structures are pure (i.e. side effect-free). That in turn enables
<a href="https://en.wikipedia.org/wiki/Value_semantics" rel="nofollow">value semantics</a>, <a href="https://en.wikipedia.org/wiki/Referential_transparency" rel="nofollow">referential
transparency</a>, and, consequently, substantial
reduction in programming complexity when dealing with paralellism and
synchronization (see e.g. Rich Hickey's presentations on <a href="https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/ValueOfValues.md"><em>The Value of
Values</em></a> and <a href="https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/AreWeThereYet.md"><em>Are We There
Yet?</em></a>).</p>
<p dir="auto">The catch is that classical hash tables set a high bar in terms of time and
space performance characteristics, and persistent data structures need to
approximate that bar.</p>
<p dir="auto"><strong>Efficient persistence.</strong> Persistent associative data structures need to
minimize the memory overhead introduced by value
semantics (i.e. returning copies as opposed to modified originals) and, at
the same time, provide practically average constant time insert, retrieve and
delete capabilities to minimize the performance gap to classical hash tables.</p>
<p dir="auto">It turns out that the data structure of choice to tackle these challenges is a
<em>tree</em>. Trees support efficient <a href="https://en.wikipedia.org/wiki/Persistent_data_structure#Trees" rel="nofollow"><em>structural
sharing</em></a> strategies for efficient memory management
and, if they are <em>balanced</em> and have <em>large branching factors</em>, provide
O(log<sub>k</sub> N) average performance guarantees.</p>
<p dir="auto"><em>Persistent hash array-mapped tries</em> are, in essence, a sophisticated,
practical implementation of such a data structure.</p>
<h3 tabindex="-1" dir="auto">Persistent Hash Array-Mapped Tries</h3>
<p dir="auto">One way to understand hash array-mapped tries is to look at them as an
evolution of <em>k</em>-ary trees (Fig. 1) that follows from a series of real-world
tradeoffs.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mkirchner/hamt/blob/main/doc/img/hamt-trees.png"><img src="https://github.com/mkirchner/hamt/raw/main/doc/img/hamt-trees.png" width="600"></a>
</p>
<p dir="auto"><b>Figure 1:</b> *k*-ary tree, hash tree, and
hash array-mapped trie.</p>
<p dir="auto">In classic <em>k</em>-ary trees Ⓐ,  Internal and leaf nodes have
different types: internal nodes point to <em>n</em> internal or leaf nodes and leaf
nodes hold or point to data (i.e. the keys/value pairs). In their basic form,
<em>n</em>-ary trees (just like binary trees) are not balanced and their performance
characteristics can easily degrade from <em>O(log<sub>k</sub> n)</em> to <em>O(n)</em>
for degenerate input sequences.</p>
<p dir="auto">One approach to balanced trees are explicit implementations of
tree rebalancing (as in e.g. <a href="https://en.wikipedia.org/wiki/Red%E2%80%93black_tree" rel="nofollow">Red-black
trees</a>, <a href="https://en.wikipedia.org/wiki/AVL_tree" rel="nofollow">AVL trees</a>, or
<a href="https://en.wikipedia.org/wiki/B-tree" rel="nofollow">B-trees</a>).</p>
<p dir="auto">Another option is to use a <a href="https://en.wikipedia.org/wiki/Hash_tree_(persistent_data_structure)" rel="nofollow"><em>hash tree</em></a> Ⓑ: like the name
implies, it uses the <em>hash</em> of the key, interpreted as a sequence of <em>b</em>-bit
groups, to detetermine the location of the leaf node that stores the key/value
pair. The group size <em>b</em> determines the branching factor 2<sup><i>b</i></sup>,
i.e. for <em>b</em>=5, every node can have 2<sup>5</sup>=32 child nodes.
Instead of implementing explicit tree rebalancing, hash trees rely on the
distributional properties of a (good) hash function to place nodes uniformly.
While this saves some effort for rebalancing, note that hash trees <em>do</em>
require a strategy to deal with <em>hash exhaustion</em>, a topic covered below.</p>
<p dir="auto">The challenge with vanilla hash trees is that they reserve space for <em>k</em>
children in every internal node. If the tree is sparsely populated this will
cause significant memory overhead and impact performance due to cache misses.</p>
<p dir="auto">For that reason, HAMTs implement <em>array mapping</em> Ⓒ: instead of reserving space
for <em>n</em> pointers to children in each internal node, the parent node stores a
bitmap that indicates which children are present and the actual node only
allocates the memory required to refer to its children. This is an important
optimization that makes trees with a high branching factor more memory
efficient and cache-friendly.</p>
<p dir="auto">In order to implement a <em>persistent</em> map or set, every modification operation
must return a modified copy and maintain the source data structure. And
returning actual copies is prohibitively expensive in time and memory.</p>
<p dir="auto">This, finally, is where HAMTs really shine and the true reason why we build
them in the first place.</p>
<p dir="auto">HAMTs are trees and trees are compatible with
<a href="https://en.wikipedia.org/wiki/Persistent_data_structure" rel="nofollow">structural sharing</a> strategies. Common
techniques are copy-on-write, fat nodes, <a href="https://en.wikipedia.org/wiki/Persistent_data_structure#Techniques_for_preserving_previous_versions" rel="nofollow">path
copying</a>, and there are <a href="https://www.cs.cmu.edu/~sleator/papers/another-persistence.pdf" rel="nofollow">complex
combinations of the previous three</a>. Path copying is
simple, efficient and general and therefore the technique of choice for
<code>libhamt</code>: Instead of returning an actual copy of the tree during an insert,
update or delete operations, we follow the search path to the item in
question, maintaining a path copy with all the nodes along the way, make our
modification along this path and return it to the caller.</p>
<p dir="auto">Note that enabling persistence <em>requires</em> the use of a garbage collection
strategy. Under stanard <code>malloc()</code> memory management, there is no way for
the HAMT nodes to know how many descendants of a HAMT refer to them.</p>
<h3 tabindex="-1" dir="auto">Documentation structure and implementation strategy</h3>
<p dir="auto">In the following we will address these concepts in turn: we first define the
foundational data structure used to build a tree and introduce the concept of
an <em>anchor</em>. We then dive into hash functions and the <em>hash state management</em>
required to make hashing work for trees of arbitrary depths and in the
presence of hash collisions. We then turn to <em>table management</em>,
introducing a set of functions used to create, modify, query and dispose of
mapped arrays.  With these pieces in place, we are ready to implement the
insert/update, query, and delete functions for non-persistent HAMTs. And
lastly, we introduce the concept of path copying and close with the
implementation of persistent insert/update and delete functions for HAMTs.</p>
<h3 tabindex="-1" dir="auto">Foundational data structures</h3>

<p dir="auto"><code>libhamt</code> uses different types to implement internal and leaf nodes.</p>
<p dir="auto">Leaf nodes contain two fields, called <code>value</code> and <code>key</code> (the rationale for the
reverse ordering of the two fields will become evident shortly).</p>
<div dir="auto" data-snippet-clipboard-copy-content="struct {
    void *value;
    void *key;
} kv;"><pre><span>struct</span> {
    <span>void</span> <span>*</span><span>value</span>;
    <span>void</span> <span>*</span><span>key</span>;
} <span>kv</span>;</pre></div>
<p dir="auto">Both fields are
defined as <code>void*</code> pointers to support referring to arbitrary data types via
type casting
<sup id="user-content-ac_cpp_virtual_method_table"><a href="#fn_cpp_virtual_method_table">5</a></sup>.</p>
<p dir="auto"><code>libhamt</code>'s internal nodes are where the magic happens, based on Bagwell's <em><a href="https://lampwww.epfl.ch/papers/idealhashtrees.pdf" rel="nofollow">Ideal Hash
Trees</a></em> paper and according to the design principles
outlined above.</p>
<p dir="auto">With a branching factor <em>k</em>, internal nodes have at most <em>k</em> successors but
can be sparsely populated. To allow for a memory-efficient representation,
internal nodes have a pointer <code>ptr</code> that points to a fixed-size, right-sized
<em>array</em> of child nodes (also known as a <em>table</em>) and a <em>k</em>-bit <code>index</code> bitmap field that
keeps track of the size and occupancy of that array.</p>
<p dir="auto"><code>libhamt</code> uses <em>k</em>=32 and because <code>index</code> is a 32-bit bitmap field, the number
of one-bits in <code>index</code> yields the size of the array that <code>ptr</code> points to (also
known as the <em>population count</em> or <code>popcount()</code> of <code>index</code>).</p>
<p dir="auto">This suggests an initial (incomplete) definition along the following lines:</p>
<div dir="auto" data-snippet-clipboard-copy-content="struct {
    struct T *ptr;  /* incomplete */
    uint32_t index;
} table;"><pre><span>struct</span> {
    <span>struct</span> <span>T</span> <span>*</span><span>ptr</span>;  <span>/* incomplete */</span>
    <span>uint32_t</span> <span>index</span>;
} <span>table</span>;</pre></div>
<p dir="auto">The specification of <code>T</code> must provide the ability for that datatype to point to
internal and external nodes alike, using only a single pointer type.
A solution is to wrap the two types into a <code>union</code> (and then to wrap
the <code>union</code> into a <code>typedef</code> for convenience):</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct hamt_node {
    union {
        struct {
            void *value;
            void *key;
        } kv;
        struct {
            struct hamt_node *ptr;
            uint32_t index;
        } table;
    } as;
} hamt_node;"><pre><span>typedef</span> <span>struct</span> <span>hamt_node</span> {
    <span>union</span> {
        <span>struct</span> {
            <span>void</span> <span>*</span><span>value</span>;
            <span>void</span> <span>*</span><span>key</span>;
        } <span>kv</span>;
        <span>struct</span> {
            <span>struct</span> <span>hamt_node</span> <span>*</span><span>ptr</span>;
            <span>uint32_t</span> <span>index</span>;
        } <span>table</span>;
    } <span>as</span>;
} <span>hamt_node</span>;</pre></div>
<p dir="auto">With this structure, given a pointer <code>hamt_node *p</code> to a <code>hamt_node</code>
instance, <code>p-&gt;as.kv</code> addresses the leaf node, and <code>p-&gt;as.table</code> addresses the
internal node and <code>p-&gt;as.kv.value</code>, <code>p-&gt;as.kv.key</code>, <code>p-&gt;as.table.ptr</code>, and
<code>p-&gt;as.table.index</code> provide access to the respective fields.</p>
<p dir="auto">To maintain sanity, we define the following convenience macros:</p>
<div dir="auto" data-snippet-clipboard-copy-content="#define TABLE(node) node->as.table.ptr
#define INDEX(node) node->as.table.index
#define VALUE(node) node->as.kv.value
#define KEY(node)   node->as.kv.key"><pre><span>#define</span> <span>TABLE</span>(<span>node</span>) node-&gt;as.table.ptr
<span>#define</span> <span>INDEX</span>(<span>node</span>) node-&gt;as.table.index
<span>#define</span> <span>VALUE</span>(<span>node</span>) node-&gt;as.kv.value
<span>#define</span> <span>KEY</span>(<span>node</span>)   node-&gt;as.kv.key</pre></div>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mkirchner/hamt/blob/main/doc/img/hamtnode-table.png"><img src="https://github.com/mkirchner/hamt/raw/main/doc/img/hamtnode-table.png" width="450"></a>
</p>
<p dir="auto"><b>Figure 2:</b>
Memory structure of an internal node. If <code>node</code> is a pointer
to an internal node, <code>TABLE(node)</code> (or, equivalently, <code>
node-&gt;as.table.ptr</code>) points to the first field of the successor table.
</p>
<h3 tabindex="-1" dir="auto">The Anchor</h3>
<p dir="auto">The <code>libhamt</code> codebase makes liberal use of the concept of an <em>anchor</em>.  An
<em>anchor</em> is a <code>hamt_node*</code> pointer to an internal node (i.e.
<code>is_value(VALUE(anchor))</code> evaluates to false). An <code>anchor</code> provides access to
all information relevant to manage the table of child nodes: <code>INDEX(anchor)</code>
returns the bitmap that encodes the array mapping, applying a popcount to the
bitmap gives the size of the table and indexing is implemented using partial
popcounts. Table elements can be accessed through
<code>TABLE(anchor)[i]</code>, where <code>i</code> must be in the valid range.</p>
<h3 tabindex="-1" dir="auto">Pointer tagging</h3>
<p dir="auto">The definition of <code>hamt_node</code> enables the construction of trees with a mix of
internal and leaf nodes. What the definition does not provide, is a way to
determine if a concrete <code>hamt_node*</code> pointer points to an internal or a leaf
node. One solution would be to specify an <code>enum</code> that indicates the type
(i.e. <code>NODE_LEAF</code>, etc.) and to add a <code>type</code> field to <code>struct hamt_node</code>.  While
valid, this would also increase the size of the struct by 50% just to maintain
a single bit of information. Luckily, there is a more memory-efficient
solution: pointer tagging.</p>
<p dir="auto">Since pointers need to be word-aligned, that leaves the lower 3 bits of all
pointers on 64-bit architectures always set to zero. It is possible to make
use of these bits under two conditions: (1) we know we are looking at a
pointer (the bottom three bits for the integer 1 are zero, too); and (2) we
carefully mask the bits in question whenever we actually use the pointer
(since it would point to the wrong location otherwise). The first is not a
problem since we own the code; the second requires diligence and some helper
functions:</p>
<div dir="auto" data-snippet-clipboard-copy-content="#define HAMT_TAG_MASK 0x3
#define HAMT_TAG_VALUE 0x1
#define tagged(__p) (hamt_node *)((uintptr_t)__p | HAMT_TAG_VALUE)
#define untagged(__p) (hamt_node *)((uintptr_t)__p &amp; ~HAMT_TAG_MASK)
#define is_value(__p) (((uintptr_t)__p &amp; HAMT_TAG_MASK) == HAMT_TAG_VALUE)"><pre><span>#define</span> <span>HAMT_TAG_MASK</span> 0x3
<span>#define</span> <span>HAMT_TAG_VALUE</span> 0x1
<span>#define</span> <span>tagged</span>(<span>__p</span>) (hamt_node *)((uintptr_t)__p | HAMT_TAG_VALUE)
<span>#define</span> <span>untagged</span>(<span>__p</span>) (hamt_node *)((uintptr_t)__p &amp; ~HAMT_TAG_MASK)
<span>#define</span> <span>is_value</span>(<span>__p</span>) (((uintptr_t)__p &amp; HAMT_TAG_MASK) == HAMT_TAG_VALUE)</pre></div>
<p dir="auto">In order to mark a leaf node as such, we set <code>key</code> as usual and tag the value
pointer before assigning it to <code>value</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="    p->as.kv.key = key_ptr;
    p->as.kv.value = tagged(value_ptr);"><pre>    <span>p</span><span>-&gt;</span><span>as</span>.<span>kv</span>.<span>key</span> <span>=</span> <span>key_ptr</span>;
    <span>p</span><span>-&gt;</span><span>as</span>.<span>kv</span>.<span>value</span> <span>=</span> <span>tagged</span>(<span>value_ptr</span>);</pre></div>
<p dir="auto">Given a pointer to a leaf (e.g. a search result), we untag <code>value</code> before
returning it:</p>
<div dir="auto" data-snippet-clipboard-copy-content="    ...
    if (status == SEARCH_SUCCESS) {
        return untagged(p->as.kv.value);
    }
    ..."><pre>    ...
    <span>if</span> (<span>status</span> <span>==</span> <span>SEARCH_SUCCESS</span>) {
        <span>return</span> <span>untagged</span>(<span>p</span><span>-&gt;</span><span>as</span>.<span>kv</span>.<span>value</span>);
    }
    ...</pre></div>
<p dir="auto">And, in order to determine what we are looking at, we use <code>is_value</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="    if (is_value(p->as.kv.value)) {
        /* this is a leaf */
        ...
    } else {
        /* this is an internal node */
        ...
    }"><pre>    <span>if</span> (<span>is_value</span>(<span>p</span><span>-&gt;</span><span>as</span>.<span>kv</span>.<span>value</span>)) {
        <span>/* this is a leaf */</span>
        ...
    } <span>else</span> {
        <span>/* this is an internal node */</span>
        ...
    }</pre></div>
<p dir="auto">Pointer tagging is the reason why the <code>value</code> and <code>key</code>
fields in the <code>struct kv</code> struct are ordered the way they are.
The <code>union</code> in <code>hamt_node</code> causes the
memory locations of the <code>struct kv</code> and <code>struct table</code> structs to overlap. Since
the <code>table.index</code> field is <em>not</em> a pointer (and the bottom-three-bits-are-zero
guarantee does not apply), its storage location cannot be used for pointer
tagging, leaving the <code>table.ptr</code> to the task. Putting <code>kv.value</code> first,
aligns the value field with <code>table.ptr</code>. The reverse order would work, but the
<code>kv.key</code> pointer is dereferenced much more often in the code and so it is more
convenient to use <code>kv.value</code>.</p>
<h2 tabindex="-1" dir="auto">Array mapping</h2>
<p dir="auto">The principal idea behing array mapping is to project a sparse bitmap index
onto the index of a dense array, where the size of the array corresponds to
the number of non-zero bits in the bitmap index.</p>
<p dir="auto">Given <code>hamt_node *p</code> is a valid pointer to a node, <code>INDEX(p)</code> corresponds to a
sparse bitmap index. The dense array is located at <code>TABLE(p)</code> and its size is
determined by the <a href="https://en.wikipedia.org/wiki/Hamming_weight" rel="nofollow"><em>population count</em></a> of <code>INDEX(p)</code>.</p>
<p dir="auto">The mapping itself is conceptually trivial: to determine the dense index for
every non-zero bit in the bitmap index, count the number of non-zero bits to
the right of it. In other words, the first set bit goes to index 0, the second
to index 1, and so forth.</p>
<p dir="auto">Efficiently implementing population counting (also known as the hamming
weight) of a bitset is <a href="https://en.wikipedia.org/wiki/Hamming_weight" rel="nofollow">not trivial</a>. <code>libhamt</code> falls back on a
GCC/Clang intrinsic:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static inline int get_popcount(uint32_t n) { return __builtin_popcount(n); }"><pre><span>static</span> <span>inline</span> <span>int</span> <span>get_popcount</span>(<span>uint32_t</span> <span>n</span>) { <span>return</span> <span>__builtin_popcount</span>(<span>n</span>); }</pre></div>
<p dir="auto">With <code>get_popcount()</code> available, determining the position (i.e. dense index)
for a sparse index in a bitmap reduces to calculating the population count of
the bitmap masked off above the sparse index:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static inline int get_pos(uint32_t sparse_index, uint32_t bitmap)
{
    return get_popcount(bitmap &amp; ((1 << sparse_index) - 1));
}"><pre><span>static</span> <span>inline</span> <span>int</span> <span>get_pos</span>(<span>uint32_t</span> <span>sparse_index</span>, <span>uint32_t</span> <span>bitmap</span>)
{
    <span>return</span> <span>get_popcount</span>(<span>bitmap</span> <span>&amp;</span> ((<span>1</span> &lt;&lt; <span>sparse_index</span>) <span>-</span> <span>1</span>));
}</pre></div>
<p dir="auto">Lastly, to determine if a node has a child at a particular index <code>index</code>, we
check if the bit at that index is set in the bitmap:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static inline bool has_index(const hamt_node *anchor, size_t index)
{
    return INDEX(anchor) &amp; (1 << index);
}"><pre><span>static</span> <span>inline</span> <span>bool</span> <span>has_index</span>(<span>const</span> <span>hamt_node</span> <span>*</span><span>anchor</span>, <span>size_t</span> <span>index</span>)
{
    <span>return</span> <span>INDEX</span>(<span>anchor</span>) <span>&amp;</span> (<span>1</span> &lt;&lt; <span>index</span>);
}</pre></div>
<h2 tabindex="-1" dir="auto">Hashing</h2>
<p dir="auto">A <a href="https://en.wikipedia.org/wiki/Hash_function" rel="nofollow"><em>hash function</em></a> is a function that takes data of
arbitrary size and maps it to a fixed-size value (often machine word sizes).
<em>Good</em> hash functions are fast to compute and produce <em>uniform</em> output, they
map their inputs as evenly as possible over the output range.  If it is
practically infeasible to invert the mapping (i.e. determine which hash
corresponds to which input value), the hash function is called a <a href="https://en.wikipedia.org/wiki/Cryptographic_hash_function" rel="nofollow">cryptographic
hash function</a>.</p>
<p dir="auto">For the purpose of implementing a HAMT, cryptographical security is not a
design goal. However, the uniformity of the hash function has direct impact on
the balance of the tree: it is the hash that pre-determines all key positions
in the fully populated tree and it is its distribution properties that
determines the number of collisions (and hence depth extensions) we introduce.</p>
<p dir="auto"><code>libhamt</code> does not force clients to use a particular hash function. The
libary exposes a hash function signature of the form</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef uint32_t (*hamt_key_hash_fn)(const void *key, const size_t gen);"><pre><span>typedef</span> <span>uint32_t</span> (<span>*</span><span>hamt_key_hash_fn</span>)(<span>const</span> <span>void</span> <span>*</span><span>key</span>, <span>const</span> <span>size_t</span> <span>gen</span>);</pre></div>
<p dir="auto">and expects users to provide a suitable function pointer as part of the call to
<code>hamt_create()</code> which, among other parameters, takes a hash function:</p>
<div dir="auto" data-snippet-clipboard-copy-content="/* ... see below for a practical definition of my_keyhash_string */

    struct hamt *t = hamt_create(my_keyhash_string, my_keycmp_string,
                                 &amp;hamt_allocator_default);"><pre><span>/* ... see below for a practical definition of my_keyhash_string */</span>

    <span>struct</span> <span>hamt</span> <span>*</span><span>t</span> <span>=</span> <span>hamt_create</span>(<span>my_keyhash_string</span>, <span>my_keycmp_string</span>,
                                 <span>&amp;</span><span>hamt_allocator_default</span>);</pre></div>
<p dir="auto">There are multiple <a href="https://theoryofcomputing.org/articles/v009a030/v009a030.pdf" rel="nofollow">good, practical choices</a>
for the HAMT.  Per default <code>libhamt</code> includes its <a href="https://github.com/mkirchner/hamt/blob/main/src/murmur3.c">own</a>,
<a href="https://github.com/mkirchner/hamt/blob/62a24e5501d72d5fb505d3c642113015f46904d3/test/test_hamt.c#L92">tested</a> implementation of 32-bit
<a href="https://en.wikipedia.org/wiki/MurmurHash" rel="nofollow">MurmurHash3</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="/* from include/murmur3.h */

uint32_t murmur3_32(const uint8_t *key, size_t len, uint32_t seed);"><pre><span>/* from include/murmur3.h */</span>

<span>uint32_t</span> <span>murmur3_32</span>(<span>const</span> <span>uint8_t</span> <span>*</span><span>key</span>, <span>size_t</span> <span>len</span>, <span>uint32_t</span> <span>seed</span>);</pre></div>
<p dir="auto">This declares the <em>murmur</em> hash function. In its standard form <code>murmur3_32</code>
takes a pointer <code>key</code> to byte-sized objects, a count of <code>len</code> that speficies
the number of bytes to hash and a random seed <code>seed</code>.</p>
<p dir="auto">In order to use murmur3 as a <code>hamt</code> hash function, we need to wrap it into a
helper function:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static uint32_t my_keyhash_string(const void *key, const size_t gen)
{
    uint32_t hash = murmur3_32((uint8_t *)key, strlen((const char *)key), gen);
    return hash;
}"><pre><span>static</span> <span>uint32_t</span> <span>my_keyhash_string</span>(<span>const</span> <span>void</span> <span>*</span><span>key</span>, <span>const</span> <span>size_t</span> <span>gen</span>)
{
    <span>uint32_t</span> <span>hash</span> <span>=</span> <span>murmur3_32</span>((<span>uint8_t</span> <span>*</span>)<span>key</span>, <span>strlen</span>((<span>const</span> <span>char</span> <span>*</span>)<span>key</span>), <span>gen</span>);
    <span>return</span> <span>hash</span>;
}</pre></div>
<p dir="auto">Here, the wrapper makes use of <code>strlen(3)</code>, assuming valid C strings as keys.
Note the use of <code>gen</code> as a seed for the hash (see below for the hash exhaustion
discussion).</p>
<p dir="auto">Here is a full example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="#include &quot;murmur3.h&quot;

/* ... */

static uint32_t my_keyhash_string(const void *key, const size_t gen)
{
    uint32_t hash = murmur3_32((uint8_t *)key, strlen((const char *)key), gen);
    return hash;
}

/* ... */

    struct hamt *t = hamt_create(my_keyhash_string, my_keycmp_string,
                                 &amp;hamt_allocator_default);
"><pre><span>#include</span> <span>"murmur3.h"</span>

<span>/* ... */</span>

<span>static</span> <span>uint32_t</span> <span>my_keyhash_string</span>(<span>const</span> <span>void</span> <span>*</span><span>key</span>, <span>const</span> <span>size_t</span> <span>gen</span>)
{
    <span>uint32_t</span> <span>hash</span> <span>=</span> <span>murmur3_32</span>((<span>uint8_t</span> <span>*</span>)<span>key</span>, <span>strlen</span>((<span>const</span> <span>char</span> <span>*</span>)<span>key</span>), <span>gen</span>);
    <span>return</span> <span>hash</span>;
}

<span>/* ... */</span>

    <span>struct</span> <span>hamt</span> <span>*</span><span>t</span> <span>=</span> <span>hamt_create</span>(<span>my_keyhash_string</span>, <span>my_keycmp_string</span>,
                                 <span>&amp;</span><span>hamt_allocator_default</span>);</pre></div>
<h3 tabindex="-1" dir="auto">Hash exhaustion: hash generations and state management</h3>
<p dir="auto">For a hash trie, the number of elements in the trie is limited by the total number
of hashes that fits into a 32-bit <code>uint32_t</code>, i.e. 2^32-1. Since the HAMT only
uses 30 bits (in 6 chunks of 5 bits), the number of unique keys in the trie is
limited to 2<sup>30</sup>-1 = 1,073,741,823 keys.
At the same time, since every layer of the
tree uses 5 bits of the hash, the trie depth is limited to 32/5 = 6 layers.
Neither the hard limit to the number of elements in the trie,
nor the inability to build a trie beyond depth of 6 are desirable properties.</p>
<p dir="auto">To address both issues, <code>libhamt</code> recalculates the hash with a different seed every
6 layers. This requires a bit of state management and motivates the
existence of the <code>hash_state</code> data type and functions that operate on it:</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct hash_state {
    const void *key;
    hamt_key_hash_fn hash_fn;
    uint32_t hash;
    size_t depth;
    size_t shift;
} hash_state;"><pre><span>typedef</span> <span>struct</span> <span>hash_state</span> {
    <span>const</span> <span>void</span> <span>*</span><span>key</span>;
    <span>hamt_key_hash_fn</span> <span>hash_fn</span>;
    <span>uint32_t</span> <span>hash</span>;
    <span>size_t</span> <span>depth</span>;
    <span>size_t</span> <span>shift</span>;
} <span>hash_state</span>;</pre></div>
<p dir="auto">The struct maintains the pointers <code>key</code> to the key that is being hashed and
<code>hash_fn</code> to the hash function used to calculate the current hash <code>hash</code>. At
the same time, it tracks the current depth <code>depth</code> in the tree (this is the
<em>hash generation</em>) and the bitshift <code>shift</code> of the current 5-bit hash chunk.</p>
<p dir="auto">The interface provides two functions: the means to step from the current 5-bit
hash to the next in <code>hash_next()</code>; and the ability query the current index of a
key at the current trie depth in <code>hash_get_index()</code>.</p>
<p dir="auto"><code>hash_next()</code> takes a pointer to a <code>hash_state</code> instance and steps that instance
from the current to the next chunk. Taking a step involves increasing the
<code>depth</code> and <code>shift</code>, and initiating a rehash if the <code>shift</code> indicates
that the hash has been exhausted:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static inline hash_state *hash_next(hash_state *h)
{
    h->depth += 1;
    h->shift += 5;
    if (h->shift > 25) {
        h->hash = h->hash_fn(h->key, h->depth / 5);
        h->shift = 0;
    }
    return h;
}"><pre><span>static</span> <span>inline</span> <span>hash_state</span> <span>*</span><span>hash_next</span>(<span>hash_state</span> <span>*</span><span>h</span>)
{
    <span>h</span><span>-&gt;</span><span>depth</span> <span>+=</span> <span>1</span>;
    <span>h</span><span>-&gt;</span><span>shift</span> <span>+=</span> <span>5</span>;
    <span>if</span> (<span>h</span><span>-&gt;</span><span>shift</span> <span>&gt;</span> <span>25</span>) {
        <span>h</span><span>-&gt;</span><span>hash</span> <span>=</span> <span>h</span><span>-&gt;</span><span>hash_fn</span>(<span>h</span><span>-&gt;</span><span>key</span>, <span>h</span><span>-&gt;</span><span>depth</span> / <span>5</span>);
        <span>h</span><span>-&gt;</span><span>shift</span> <span>=</span> <span>0</span>;
    }
    <span>return</span> <span>h</span>;
}</pre></div>
<p dir="auto">The index of a hash at its current depth corresponds to the decimal
representation of the current chunk. To determine the current chunk,
we right-shift the hash by <code>h-&gt;shift</code> to right-align the desired
LSB and then mask with <code>0x11111</code> which equals <code>0x1f</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static inline uint32_t hash_get_index(const hash_state *h)
{
    return (h->hash >> h->shift) &amp; 0x1f;
}"><pre><span>static</span> <span>inline</span> <span>uint32_t</span> <span>hash_get_index</span>(<span>const</span> <span>hash_state</span> <span>*</span><span>h</span>)
{
    <span>return</span> (<span>h</span><span>-&gt;</span><span>hash</span> &gt;&gt; <span>h</span><span>-&gt;</span><span>shift</span>) <span>&amp;</span> <span>0x1f</span>;
}</pre></div>
<h2 tabindex="-1" dir="auto">Table management</h2>
<p dir="auto">In order to facilitate memory management for tables (aka the internal nodes),
<code>libhamt</code> defines a set of helper functions. Each of these functions takes a
<code>hamt_allocator</code> and calls the user-supplied allocation, re-allocation and
deallocation functions as appropriate.</p>
<p dir="auto">We start by defining a simple memory abstraction (it would also be correct to use real functions
instead of preprocessor macros for this):</p>
<div dir="auto" data-snippet-clipboard-copy-content="#define mem_alloc(ator, size) (ator)->malloc(size)
#define mem_realloc(ator, ptr, size) (ator)->realloc(ptr, size)
#define mem_free(ator, ptr) (ator)->free(ptr)"><pre><span>#define</span> <span>mem_alloc</span>(<span>ator</span>, <span>size</span>) (ator)-&gt;malloc(size)
<span>#define</span> <span>mem_realloc</span>(<span>ator</span>, <span>ptr</span>, <span>size</span>) (ator)-&gt;realloc(ptr, size)
<span>#define</span> <span>mem_free</span>(<span>ator</span>, <span>ptr</span>) (ator)-&gt;free(ptr)</pre></div>
<p dir="auto">This will make it easier to add optimizations (e.g. table caching) in the
future. On top of these macros, table lifecycle management is accomplished
with a few dedicated allocation and de-allocation functions.</p>
<h3 tabindex="-1" dir="auto">Simple allocation and deallocation</h3>
<p dir="auto"><code>table_allocate()</code> allocates tables with size <code>size</code> and returns a pointer to
the newly allocated table.</p>
<div dir="auto" data-snippet-clipboard-copy-content="hamt_node *table_allocate(struct hamt_allocator *ator, size_t size)
{
    return (hamt_node *)mem_alloc(ator, (size * sizeof(hamt_node)));
}"><pre><span>hamt_node</span> <span>*</span><span>table_allocate</span>(<span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>, <span>size_t</span> <span>size</span>)
{
    <span>return</span> (<span>hamt_node</span> <span>*</span>)<span>mem_alloc</span>(<span>ator</span>, (<span>size</span> <span>*</span> <span>sizeof</span>(<span>hamt_node</span>)));
}</pre></div>
<p dir="auto"><code>table_free()</code> deallocates the allocation referenced by <code>ptr</code>. It also
supports taking a <code>size</code> parameter for future extension (e.g. provide a hint
for allocation pool management) that is currently ignored by the underlying
<code>mem_free()</code> implementation.</p>
<div dir="auto" data-snippet-clipboard-copy-content="void table_free(struct hamt_allocator *ator, hamt_node *ptr, size_t size)
{
    mem_free(ator, ptr);
}
"><pre><span>void</span> <span>table_free</span>(<span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>, <span>hamt_node</span> <span>*</span><span>ptr</span>, <span>size_t</span> <span>size</span>)
{
    <span>mem_free</span>(<span>ator</span>, <span>ptr</span>);
}</pre></div>
<h3 tabindex="-1" dir="auto">Specialized table resize operations</h3>
<p dir="auto">While it is possible to implement table re- and right-sizing with the
two functions introduced above, it makes a lot of sense to provide specialized
functionality for the key allocation/de-allocation use cases: extending,
shrinking and gathering a table.</p>
<p dir="auto"><strong>Table extension.</strong> Since the tables in a HAMT are right-sized to minimize
memory overhead, item insertion must necessarily add an additional row to an
existing table. As illustrated in figure 3, the table extension function takes an anchor for an existing
table, allocates a new table with increased size, copies over the exsiting
entries (leaving a gap at the appropriate position for the new row), assigns
the new key and value to the fields in the new row, updates the anchor with
the new memory location of the table and the new index, and eventually frees the
memory of the old table.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mkirchner/hamt/blob/main/doc/img/table-extend.png"><img src="https://github.com/mkirchner/hamt/raw/main/doc/img/table-extend.png" width="450"></a>
</p>
<p dir="auto"><b>Figure 3:</b>
Extending a table creates a new copy of the existing table with an additional
row for the new node.
</p>
<p dir="auto">Looking at the code, this is implemented in verbatim in the <code>table_extend()</code>
function. <code>table_extend()</code> takes an <code>anchor</code> pointer to a table of
size <code>n_rows</code>, then uses the allocator <code>ator</code> to create a new table of size <code>n_rows + 1</code>
with an empty row at position <code>pos</code> and the bitmap index bit <code>index</code> set. It
uses <code>memcpy()</code> to copy memory ranges into the the appropriate positions in
the new allocation, frees the old table and assignes the new table <code>ptr</code> and
<code>index</code> in the anchor:</p>
<div dir="auto" data-snippet-clipboard-copy-content="hamt_node *table_extend(struct hamt_allocator *ator, hamt_node *anchor,
                       size_t n_rows, uint32_t index, uint32_t pos)
{
    hamt_node *new_table = table_allocate(ator, n_rows + 1);
    if (!new_table)
        return NULL;
    if (n_rows > 0) {
        /* copy over table */
        memcpy(&amp;new_table[0], &amp;TABLE(anchor)[0], pos * sizeof(hamt_node));
        /* note: this works since (n_rows - pos) == 0 for cases
         * where we're adding the new k/v pair at the end and memcpy(a, b, 0)
         * is a nop */
        memcpy(&amp;new_table[pos + 1], &amp;TABLE(anchor)[pos],
               (n_rows - pos) * sizeof(hamt_node));
    }
    table_free(ator, TABLE(anchor), n_rows);
    TABLE(anchor) = new_table;
    INDEX(anchor) |= (1 << index);
    return anchor;
}"><pre><span>hamt_node</span> <span>*</span><span>table_extend</span>(<span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>, <span>hamt_node</span> <span>*</span><span>anchor</span>,
                       <span>size_t</span> <span>n_rows</span>, <span>uint32_t</span> <span>index</span>, <span>uint32_t</span> <span>pos</span>)
{
    <span>hamt_node</span> <span>*</span><span>new_table</span> <span>=</span> <span>table_allocate</span>(<span>ator</span>, <span>n_rows</span> <span>+</span> <span>1</span>);
    <span>if</span> (!<span>new_table</span>)
        <span>return</span> <span>NULL</span>;
    <span>if</span> (<span>n_rows</span> <span>&gt;</span> <span>0</span>) {
        <span>/* copy over table */</span>
        <span>memcpy</span>(<span>&amp;</span><span>new_table</span>[<span>0</span>], <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>0</span>], <span>pos</span> <span>*</span> <span>sizeof</span>(<span>hamt_node</span>));
        <span>/* note: this works since (n_rows - pos) == 0 for cases</span>
<span>         * where we're adding the new k/v pair at the end and memcpy(a, b, 0)</span>
<span>         * is a nop */</span>
        <span>memcpy</span>(<span>&amp;</span><span>new_table</span>[<span>pos</span> <span>+</span> <span>1</span>], <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>pos</span>],
               (<span>n_rows</span> <span>-</span> <span>pos</span>) <span>*</span> <span>sizeof</span>(<span>hamt_node</span>));
    }
    <span>table_free</span>(<span>ator</span>, <span>TABLE</span>(<span>anchor</span>), <span>n_rows</span>);
    <span>TABLE</span>(<span>anchor</span>) <span>=</span> <span>new_table</span>;
    <span>INDEX</span>(<span>anchor</span>) |= (<span>1</span> &lt;&lt; <span>index</span>);
    <span>return</span> <span>anchor</span>;
}</pre></div>
<p dir="auto"><strong>Shrinking a table.</strong> Shrinking a table is the inverse operation of table
extension: since we maintain right-sized tables as an invariant, we need to
adjust table sizes the moment the client deletes a key/value pair from the
HAMT.</p>
<p dir="auto">Figure 4 illustrates the concept: given an anchor, the shrinking function
returns a new table with the specified row removed.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mkirchner/hamt/blob/main/doc/img/table-shrink.png"><img src="https://github.com/mkirchner/hamt/raw/main/doc/img/table-shrink.png" width="450"></a>
</p>
<p dir="auto"><b>Figure 4:</b>
Shrinking a table creates a new copy of the table with the specified row
removed.
</p>
<p dir="auto">In the code, this is what <code>table_shrink()</code> does. In the same way as
<code>table_extend()</code> the function takes a pointer <code>ator</code> to the global allocator,
a pointer <code>anchor</code> to the current anchor, the size of the current tables as
<code>n_rows</code>, and the pair of one-hot bitmap index <code>index</code> and storage array
position <code>pos</code>. And, in analogy to table extension, the function allocation a
right-sized table, copies the data to keep using range copies with <code>memcpy()</code>,
frees up the old table and updates the anchor to reflect the changes.</p>
<div dir="auto" data-snippet-clipboard-copy-content="hamt_node *table_shrink(struct hamt_allocator *ator, hamt_node *anchor,
                       size_t n_rows, uint32_t index, uint32_t pos)
{
    hamt_node *new_table = NULL;
    uint32_t new_index = 0;
    if (n_rows > 0) {
        new_table = table_allocate(ator, n_rows - 1);
        if (!new_table)
            return NULL;
        new_index = INDEX(anchor) &amp; ~(1 << index);
        memcpy(&amp;new_table[0], &amp;TABLE(anchor)[0], pos * sizeof(hamt_node));
        memcpy(&amp;new_table[pos], &amp;TABLE(anchor)[pos + 1],
               (n_rows - pos - 1) * sizeof(hamt_node));
    }
    table_free(ator, TABLE(anchor), n_rows);
    INDEX(anchor) = new_index;
    TABLE(anchor) = new_table;
    return anchor;
}"><pre><span>hamt_node</span> <span>*</span><span>table_shrink</span>(<span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>, <span>hamt_node</span> <span>*</span><span>anchor</span>,
                       <span>size_t</span> <span>n_rows</span>, <span>uint32_t</span> <span>index</span>, <span>uint32_t</span> <span>pos</span>)
{
    <span>hamt_node</span> <span>*</span><span>new_table</span> <span>=</span> <span>NULL</span>;
    <span>uint32_t</span> <span>new_index</span> <span>=</span> <span>0</span>;
    <span>if</span> (<span>n_rows</span> <span>&gt;</span> <span>0</span>) {
        <span>new_table</span> <span>=</span> <span>table_allocate</span>(<span>ator</span>, <span>n_rows</span> <span>-</span> <span>1</span>);
        <span>if</span> (!<span>new_table</span>)
            <span>return</span> <span>NULL</span>;
        <span>new_index</span> <span>=</span> <span>INDEX</span>(<span>anchor</span>) <span>&amp;</span> ~(<span>1</span> &lt;&lt; <span>index</span>);
        <span>memcpy</span>(<span>&amp;</span><span>new_table</span>[<span>0</span>], <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>0</span>], <span>pos</span> <span>*</span> <span>sizeof</span>(<span>hamt_node</span>));
        <span>memcpy</span>(<span>&amp;</span><span>new_table</span>[<span>pos</span>], <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>pos</span> <span>+</span> <span>1</span>],
               (<span>n_rows</span> <span>-</span> <span>pos</span> <span>-</span> <span>1</span>) <span>*</span> <span>sizeof</span>(<span>hamt_node</span>));
    }
    <span>table_free</span>(<span>ator</span>, <span>TABLE</span>(<span>anchor</span>), <span>n_rows</span>);
    <span>INDEX</span>(<span>anchor</span>) <span>=</span> <span>new_index</span>;
    <span>TABLE</span>(<span>anchor</span>) <span>=</span> <span>new_table</span>;
    <span>return</span> <span>anchor</span>;
}</pre></div>
<p dir="auto"><strong>Table gathering.</strong> As we are deleting entries from the HAMT, we may end up
with the table structure shown in Figure 5: a table in which one of the
entries is a single-row table. What we want to do in these cases is to replace
the table entry in <code>TABLE(anchor)[1]</code> with the key/value pair from
<code>TABLE(TABLE(anchor)[1])</code> and <em>gather</em> the one-row table into its parent.
While this comes at additional computational cost upon delete, it maintains
the logarithmic depth properties as the HAMT changes its size.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mkirchner/hamt/blob/main/doc/img/table-gather.png"><img src="https://github.com/mkirchner/hamt/raw/main/doc/img/table-gather.png" width="450"></a>
</p>
<p dir="auto"><b>Figure 5:</b>
Gathering pulls a one-row-sized table into its parent table (essentially
converting an internal node into a leaf node).
</p>
<p dir="auto">The code is straightforward: we take the allocator <code>alloc</code>, the <code>anchor</code>
pointer, and the position <code>pos</code> of the single-row table inside the parent
table, copy over the key and value from the child table to the parent
(maintaining a temporary handle on the child) and then free the child table:</p>
<div dir="auto" data-snippet-clipboard-copy-content="hamt_node *table_gather(struct hamt_allocator *ator, hamt_node *anchor,
                       uint32_t pos)
{
    int n_rows = get_popcount(INDEX(anchor));
    hamt_node *table = TABLE(anchor);
    KEY(anchor) = table[pos].as.kv.key;
    VALUE(anchor) = table[pos].as.kv.value; /* already tagged */
    table_free(ator, table, n_rows);
    return anchor;
}
"><pre><span>hamt_node</span> <span>*</span><span>table_gather</span>(<span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>, <span>hamt_node</span> <span>*</span><span>anchor</span>,
                       <span>uint32_t</span> <span>pos</span>)
{
    <span>int</span> <span>n_rows</span> <span>=</span> <span>get_popcount</span>(<span>INDEX</span>(<span>anchor</span>));
    <span>hamt_node</span> <span>*</span><span>table</span> <span>=</span> <span>TABLE</span>(<span>anchor</span>);
    <span>KEY</span>(<span>anchor</span>) <span>=</span> <span>table</span>[<span>pos</span>].<span>as</span>.<span>kv</span>.<span>key</span>;
    <span>VALUE</span>(<span>anchor</span>) <span>=</span> <span>table</span>[<span>pos</span>].<span>as</span>.<span>kv</span>.<span>value</span>; <span>/* already tagged */</span>
    <span>table_free</span>(<span>ator</span>, <span>table</span>, <span>n_rows</span>);
    <span>return</span> <span>anchor</span>;
}</pre></div>
<p dir="auto"><strong>Table duplication.</strong> Lastly, table duplication. This will be required for path
copying when we implement persistency and it is so straightforward that there
is no diagram: given an anchor, <code>table_dup()</code> determines the size of the table
that the anchor points to, allocates the required memory and performs a range
copy using <code>memcpy()</code> to duplicate the table contents:</p>
<div dir="auto" data-snippet-clipboard-copy-content="hamt_node *table_dup(struct hamt_allocator *ator, hamt_node *anchor)
{
    int n_rows = get_popcount(INDEX(anchor));
    hamt_node *new_table = table_allocate(ator, n_rows);
    if (new_table) {
        memcpy(&amp;new_table[0], &amp;TABLE(anchor)[0], n_rows * sizeof(hamt_node));
    }
    return new_table;
}"><pre><span>hamt_node</span> <span>*</span><span>table_dup</span>(<span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>, <span>hamt_node</span> <span>*</span><span>anchor</span>)
{
    <span>int</span> <span>n_rows</span> <span>=</span> <span>get_popcount</span>(<span>INDEX</span>(<span>anchor</span>));
    <span>hamt_node</span> <span>*</span><span>new_table</span> <span>=</span> <span>table_allocate</span>(<span>ator</span>, <span>n_rows</span>);
    <span>if</span> (<span>new_table</span>) {
        <span>memcpy</span>(<span>&amp;</span><span>new_table</span>[<span>0</span>], <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>0</span>], <span>n_rows</span> <span>*</span> <span>sizeof</span>(<span>hamt_node</span>));
    }
    <span>return</span> <span>new_table</span>;
}</pre></div>
<h2 tabindex="-1" dir="auto">Putting it all together</h2>
<p dir="auto">The following subsections detail the implementations of search, insertion and
removal of key/value pairs in our HAMT implementation. Note that, while the
implementations shown here have been thoroughly tested and are deemed correct,
they may have been replaced by faster or more capable implementations in the
actual <code>libhamt</code> source. An attempt is being made to keep this section up to
date with the actual implementation but the choice here is in favor of
conceptual clarity and will not necessarily cover every implementation detail.
PRs welcome.</p>
<h3 tabindex="-1" dir="auto">Example data</h3>
<table>
<thead>
<tr>
<th>key</th>
<th>key hash</th>
<th>binary key hash</th>
<th>5-bit ints</th>
</tr>
</thead>
<tbody>
<tr>
<td>"0"</td>
<td>d271c07f</td>
<td><code>11 01001 00111 00011 10000 00011 11111</code></td>
<td>[ 31  3 16  3  7 9 ]</td>
</tr>
<tr>
<td>"2"</td>
<td>0129e217</td>
<td><code>00 00000 10010 10011 11000 10000 10111</code></td>
<td>[ 23 16 24 19 18 0 ]</td>
</tr>
<tr>
<td>"4"</td>
<td>e131cc88</td>
<td><code>11 10000 10011 00011 10011 00100 01000</code></td>
<td>[  8  4 19 3 19 16 ]</td>
</tr>
<tr>
<td>"7"</td>
<td>23ea8628</td>
<td><code>00 10001 11110 10101 00001 10001 01000</code></td>
<td>[  8 17 1 21 30 17 ]</td>
</tr>
<tr>
<td>"8"</td>
<td>bd920017</td>
<td><code>10 11110 11001 00100 00000 00000 10111</code></td>
<td>[ 23 0  0  4 25 30 ]</td>
</tr>
</tbody>
</table>
<h3 tabindex="-1" dir="auto">Search: internal API</h3>
<p dir="auto">Search plays a double role: finding a HAMT entry is a fundamental part of the
HAMT interface (exposed by <code>hamt_get()</code>); and the first step in the insert and remove
functions is finding the anchors to operate on.</p>
<p dir="auto">It is therefore desirable to approach the search implementation from a
more generic perspective such that we do not need to re-invent the
wheel for each of these use cases. We therefore define an internal search
function</p>
<div dir="auto" data-snippet-clipboard-copy-content="static ... search_recursive(...);"><pre><span>static</span> ... <span>search_recursive</span>(...);</pre></div>
<p dir="auto">that is called from internal and the API functions alike. As the
name implies, we implement search in a recursive manner (this is for clarity;
conversion to an iterative solution is straightforward).</p>
<p dir="auto">When we search for a key in the HAMT, there are two fundamental outcomes: the
key is either there, or it is not (note that these are exactly the semantics
of the user-facing <code>hamt_get()</code> function: it either returns a pointer to the
value stored under the key or it returns <code>NULL</code>). However, looking more
closely, searches can fail for two reasons: the search can be unsuccessful
because a key does not exist in the HAMT <em>or</em> it can be unsuccessful because
there is a key value pair that happens to have the same partial hash but a
different key (i.e. there is a hash collission or the hash has not been
sufficiently exhausted to differentiate between the two keys).  And each of
these three cases is meaningful (the latter two corresponding directly to the
two different insertion strategies described below).</p>
<p dir="auto">A good approach here is to define a ternary return value (as opposed to
the usual, binary use-<code>NULL</code>-as-a-failure-indicator approach that is often
prevalent in C code) to allow us to signal each of these cases clearly.</p>
<p dir="auto">We create a suitable three-value <code>enum</code> called <code>search_status</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef enum {
    SEARCH_SUCCESS,
    SEARCH_FAIL_NOTFOUND,
    SEARCH_FAIL_KEYMISMATCH
} search_status;"><pre><span>typedef</span> <span>enum</span> {
    <span>SEARCH_SUCCESS</span>,
    <span>SEARCH_FAIL_NOTFOUND</span>,
    <span>SEARCH_FAIL_KEYMISMATCH</span>
} <span>search_status</span>;</pre></div>
<p dir="auto">where <code>SEARCH_SUCCESS</code> indicates that the key in question was
found, <code>SEARCH_FAIL_NOTFOUND</code> indicates a search failure due to a missing key,
and <code>SEARCH_FAIL_KEYMISMATCH</code> signals a hash conflict.</p>
<p dir="auto">In order to return the result of a search (and not only its status), we
introduce a search result data type that is a bit more heavy-weight:</p>
<div dir="auto" data-snippet-clipboard-copy-content="struct search_result {
    search_status status;
    hamt_node *anchor;
    hamt_node *value;
    hash_state *hash;
};"><pre><span>struct</span> <span>search_result</span> {
    <span>search_status</span> <span>status</span>;
    <span>hamt_node</span> <span>*</span><span>anchor</span>;
    <span>hamt_node</span> <span>*</span><span>value</span>;
    <span>hash_state</span> <span>*</span><span>hash</span>;
};</pre></div>
<p dir="auto">Here, <code>anchor</code> always points to the anchor at which the search was terminated;
if the search was successful, <code>value</code> points to the table row that holds the
key/value pair with matching key; if it was unsuccessful with a key mismatch,
<code>value</code> points to the mismatching key/value pair; and if it was unsuccessful
because the key did not exist, <code>value</code> equals <code>NULL</code>. Depending on the depth
that the search reached, we may have hit hash exhaustion and the hash may have
been recalculated, so we are returning this here, too.</p>
<p dir="auto">Given <code>struct search_result</code>, the return value of <code>search_recursive()</code>
becomes:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static struct search_result search_recursive(...)
{
    // ...
}"><pre><span>static</span> <span>struct</span> <span>search_result</span> <span>search_recursive</span>(...)
{
    <span>// ...</span>
}</pre></div>
<p dir="auto">With these prerequisites out of the way, we can tackle the actual search
algorithm:</p>
<div data-snippet-clipboard-copy-content="    search_recursive(anchor, hash, eq, key, ...):
        if the current 5-bit sub-hash is a valid index in the current table: 
            if the index refers to a key/value pair:
                if the key matches the search key:
                    return SEARCH_SUCCESS
                else:
                    return SEARCH_FAIL_KEYMISMATCH
            else (i.e. it refers to a sub-table):
                search_recursive(sub-table, hash_next(hash), eq, key)
        else:
            return SEARCH_FAIL_NOTFOUND"><pre><code>    search_recursive(anchor, hash, eq, key, ...):
        if the current 5-bit sub-hash is a valid index in the current table: 
            if the index refers to a key/value pair:
                if the key matches the search key:
                    return SEARCH_SUCCESS
                else:
                    return SEARCH_FAIL_KEYMISMATCH
            else (i.e. it refers to a sub-table):
                search_recursive(sub-table, hash_next(hash), eq, key)
        else:
            return SEARCH_FAIL_NOTFOUND
</code></pre></div>
<p dir="auto">The basic idea is to start from the root of the HAMT and then, at every level,
test if the curret sub-hash of the key is present in the current sub-trie. If
not, bail and report failure immediately. If yes, check if the entry refers to
a key/value pair or to another table. If this is true as well, check if the
keys match and return success or failure accordingly. If the entry refers to
a sub-table, repeat the search at the level of the sub-table.</p>
<p dir="auto">With the conceptual approach lined out, let's get into the implementation
details.
We start with deriving the table index for the current search level from the
hash. This is accomplished using
<code>hash_get_index()</code>, which encapsulates the bit-fiddling required to extract
the correct 5-bit hash for the current search level and returns the index as
an unsigned integer.</p>
<div dir="auto" data-snippet-clipboard-copy-content="static search_result search_recursive(hamt_node *anchor,
                                      hash_state *hash,
                                      hamt_cmp_fn cmp_eq,
                                      const void *key, ...)
{
    uint32_t expected_index = hash_get_index(hash);
    ...
}"><pre><span>static</span> <span>search_result</span> <span>search_recursive</span>(<span>hamt_node</span> <span>*</span><span>anchor</span>,
                                      <span>hash_state</span> <span>*</span><span>hash</span>,
                                      <span>hamt_cmp_fn</span> <span>cmp_eq</span>,
                                      <span>const</span> <span>void</span> <span>*</span><span>key</span>, ...)
{
    <span>uint32_t</span> <span>expected_index</span> <span>=</span> <span>hash_get_index</span>(<span>hash</span>);
    ...
}</pre></div>
<p dir="auto">The code then checks if the <code>expected_index</code> exists in the current table:</p>
<div dir="auto" data-snippet-clipboard-copy-content="    ...
    if (has_index(anchor, expected_index)) {
    ...
    }"><pre>    ...
    <span>if</span> (<span>has_index</span>(<span>anchor</span>, <span>expected_index</span>)) {
    ...
    }</pre></div>
<p dir="auto">Here, <code>has_index()</code> is a simple helper function that checks if
the <code>INDEX(anchor)</code> bitfield has the bit set at <code>expected_index</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static inline bool has_expected_index(const hamt_node *anchor, size_t expected_index)
{
    return INDEX(anchor) &amp; (1 << expected_index);
}"><pre><span>static</span> <span>inline</span> <span>bool</span> <span>has_expected_index</span>(<span>const</span> <span>hamt_node</span> <span>*</span><span>anchor</span>, <span>size_t</span> <span>expected_index</span>)
{
    <span>return</span> <span>INDEX</span>(<span>anchor</span>) <span>&amp;</span> (<span>1</span> &lt;&lt; <span>expected_index</span>);
}</pre></div>
<p dir="auto">If <code>has_index()</code> evaluates to false, the key does not exist in the HAMT and we
can immediately fail the search and return the result:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    uint32_t expected_index = hash_get_index(hash);
    if (has_index(anchor, expected_index)) {
        ...
        ... 
        ...
    }
    search_result result = {.status = SEARCH_FAIL_NOTFOUND,
                            .anchor = anchor,
                            .value = NULL,
                            .hash = hash};
    return result;
}"><pre>{
    <span>uint32_t</span> <span>expected_index</span> <span>=</span> <span>hash_get_index</span>(<span>hash</span>);
    <span>if</span> (<span>has_index</span>(<span>anchor</span>, <span>expected_index</span>)) {
        ...
        ... 
        ...
    }
    <span>search_result</span> <span>result</span> <span>=</span> {.<span>status</span> <span>=</span> <span>SEARCH_FAIL_NOTFOUND</span>,
                            .<span>anchor</span> <span>=</span> <span>anchor</span>,
                            .<span>value</span> <span>=</span> <span>NULL</span>,
                            .<span>hash</span> <span>=</span> <span>hash</span>};
    <span>return</span> <span>result</span>;
}</pre></div>
<p dir="auto">If <code>has_index()</code> evaluates to true, we find the array index using
<code>get_pos()</code> (see above), store it into <code>pos</code> and then acquire a pointer to the
<code>next</code> node by addressing <code>pos</code> indices into the <code>anchor</code>'s table.</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    ...
    if (has_index(anchor, expected_index)) {
        /* If yes, get the compact index to address the array */
        int pos = get_pos(expected_index, INDEX(anchor));
        /* Index into the table */
        hamt_node *next = &amp;TABLE(anchor)[pos];
        ...
    }
    ...
}"><pre>{
    ...
    <span>if</span> (<span>has_index</span>(<span>anchor</span>, <span>expected_index</span>)) {
        <span>/* If yes, get the compact index to address the array */</span>
        <span>int</span> <span>pos</span> <span>=</span> <span>get_pos</span>(<span>expected_index</span>, <span>INDEX</span>(<span>anchor</span>));
        <span>/* Index into the table */</span>
        <span>hamt_node</span> <span>*</span><span>next</span> <span>=</span> <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>pos</span>];
        ...
    }
    ...
}</pre></div>
<p dir="auto">If the <code>next</code> node is not a value, we advance the hash state and recurse the
search. If it is, we compare the keys and return success or failure
accordingly:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
        ...
        /* Index into the table */
        hamt_node *next = &amp;TABLE(anchor)[pos];
        /* Are we looking at a value or another level of tables? */
        if (is_value(VALUE(next))) {
            if ((*cmp_eq)(key, KEY(next)) == 0) {
                /* Found: keys match */
                search_result result = {.status = SEARCH_SUCCESS,
                                        .anchor = anchor,
                                        .value = next,
                                        .hash = hash};
                return result;
            }
            /* Not found: same hash but different key */
            search_result result = {.status = SEARCH_FAIL_KEYMISMATCH,
                                    .anchor = anchor,
                                    .value = next,
                                    .hash = hash};
            return result;
        } else {
            /* For table entries, recurse to the next level */
            return search_recursive(next, hash_next(hash), cmp_eq, key);
        }"><pre>{
        ...
        <span>/* Index into the table */</span>
        <span>hamt_node</span> <span>*</span><span>next</span> <span>=</span> <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>pos</span>];
        <span>/* Are we looking at a value or another level of tables? */</span>
        <span>if</span> (<span>is_value</span>(<span>VALUE</span>(<span>next</span>))) {
            <span>if</span> ((<span>*</span><span>cmp_eq</span>)(<span>key</span>, <span>KEY</span>(<span>next</span>)) <span>==</span> <span>0</span>) {
                <span>/* Found: keys match */</span>
                <span>search_result</span> <span>result</span> <span>=</span> {.<span>status</span> <span>=</span> <span>SEARCH_SUCCESS</span>,
                                        .<span>anchor</span> <span>=</span> <span>anchor</span>,
                                        .<span>value</span> <span>=</span> <span>next</span>,
                                        .<span>hash</span> <span>=</span> <span>hash</span>};
                <span>return</span> <span>result</span>;
            }
            <span>/* Not found: same hash but different key */</span>
            <span>search_result</span> <span>result</span> <span>=</span> {.<span>status</span> <span>=</span> <span>SEARCH_FAIL_KEYMISMATCH</span>,
                                    .<span>anchor</span> <span>=</span> <span>anchor</span>,
                                    .<span>value</span> <span>=</span> <span>next</span>,
                                    .<span>hash</span> <span>=</span> <span>hash</span>};
            <span>return</span> <span>result</span>;
        } <span>else</span> {
            <span>/* For table entries, recurse to the next level */</span>
            <span>return</span> <span>search_recursive</span>(<span>next</span>, <span>hash_next</span>(<span>hash</span>), <span>cmp_eq</span>, <span>key</span>);
        }</pre></div>
<p dir="auto">That concludes the implementation of the recursive search function and the
complete implementation looks like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static search_result search_recursive(hamt_node *anchor, hash_state *hash,
                                      hamt_cmp_fn cmp_eq, const void *key)
{
    /* Determine the expected index in table */
    uint32_t expected_index = hash_get_index(hash);
    /* Check if the expected index is set */
    if (has_index(anchor, expected_index)) {
        /* If yes, get the compact index to address the array */
        int pos = get_pos(expected_index, INDEX(anchor));
        /* Index into the table */
        hamt_node *next = &amp;TABLE(anchor)[pos];
        /* Are we looking at a value or another level of tables? */
        if (is_value(VALUE(next))) {
            if ((*cmp_eq)(key, KEY(next)) == 0) {
                /* Found: keys match */
                search_result result = {.status = SEARCH_SUCCESS,
                                        .anchor = anchor,
                                        .value = next,
                                        .hash = hash};
                return result;
            }
            /* Not found: same hash but different key */
            search_result result = {.status = SEARCH_FAIL_KEYMISMATCH,
                                    .anchor = anchor,
                                    .value = next,
                                    .hash = hash};
            return result;
        } else {
            /* For table entries, recurse to the next level */
            return search_recursive(next, hash_next(hash), cmp_eq, key);
        }
    }
    /* Not found: expected index is not set, key does not exist */
    search_result result = {.status = SEARCH_FAIL_NOTFOUND,
                            .anchor = anchor,
                            .value = NULL,
                            .hash = hash};
    return result;
}"><pre><span>static</span> <span>search_result</span> <span>search_recursive</span>(<span>hamt_node</span> <span>*</span><span>anchor</span>, <span>hash_state</span> <span>*</span><span>hash</span>,
                                      <span>hamt_cmp_fn</span> <span>cmp_eq</span>, <span>const</span> <span>void</span> <span>*</span><span>key</span>)
{
    <span>/* Determine the expected index in table */</span>
    <span>uint32_t</span> <span>expected_index</span> <span>=</span> <span>hash_get_index</span>(<span>hash</span>);
    <span>/* Check if the expected index is set */</span>
    <span>if</span> (<span>has_index</span>(<span>anchor</span>, <span>expected_index</span>)) {
        <span>/* If yes, get the compact index to address the array */</span>
        <span>int</span> <span>pos</span> <span>=</span> <span>get_pos</span>(<span>expected_index</span>, <span>INDEX</span>(<span>anchor</span>));
        <span>/* Index into the table */</span>
        <span>hamt_node</span> <span>*</span><span>next</span> <span>=</span> <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>pos</span>];
        <span>/* Are we looking at a value or another level of tables? */</span>
        <span>if</span> (<span>is_value</span>(<span>VALUE</span>(<span>next</span>))) {
            <span>if</span> ((<span>*</span><span>cmp_eq</span>)(<span>key</span>, <span>KEY</span>(<span>next</span>)) <span>==</span> <span>0</span>) {
                <span>/* Found: keys match */</span>
                <span>search_result</span> <span>result</span> <span>=</span> {.<span>status</span> <span>=</span> <span>SEARCH_SUCCESS</span>,
                                        .<span>anchor</span> <span>=</span> <span>anchor</span>,
                                        .<span>value</span> <span>=</span> <span>next</span>,
                                        .<span>hash</span> <span>=</span> <span>hash</span>};
                <span>return</span> <span>result</span>;
            }
            <span>/* Not found: same hash but different key */</span>
            <span>search_result</span> <span>result</span> <span>=</span> {.<span>status</span> <span>=</span> <span>SEARCH_FAIL_KEYMISMATCH</span>,
                                    .<span>anchor</span> <span>=</span> <span>anchor</span>,
                                    .<span>value</span> <span>=</span> <span>next</span>,
                                    .<span>hash</span> <span>=</span> <span>hash</span>};
            <span>return</span> <span>result</span>;
        } <span>else</span> {
            <span>/* For table entries, recurse to the next level */</span>
            <span>return</span> <span>search_recursive</span>(<span>next</span>, <span>hash_next</span>(<span>hash</span>), <span>cmp_eq</span>, <span>key</span>);
        }
    }
    <span>/* Not found: expected index is not set, key does not exist */</span>
    <span>search_result</span> <span>result</span> <span>=</span> {.<span>status</span> <span>=</span> <span>SEARCH_FAIL_NOTFOUND</span>,
                            .<span>anchor</span> <span>=</span> <span>anchor</span>,
                            .<span>value</span> <span>=</span> <span>NULL</span>,
                            .<span>hash</span> <span>=</span> <span>hash</span>};
    <span>return</span> <span>result</span>;
}</pre></div>
<h3 tabindex="-1" dir="auto">Search: external API</h3>
<p dir="auto">The external API for search is <code>hamt_get(trie, key)</code> which takes a <code>trie</code>
and attempts to find (and return) a key/value pair specified by <code>key</code>. Its
implementation uses <code>search_recursive()</code> from above:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const void *hamt_get(const struct hamt *trie, void *key)
{
    hash_state *hash = &amp;(hash_state){.key = key,
                                     .hash_fn = trie->key_hash,
                                     .hash = trie->key_hash(key, 0),
                                     .depth = 0,
                                     .shift = 0};
    search_result sr = search_recursive(trie->root, hash, trie->key_cmp, key,
                                        NULL, trie->ator);
    if (sr.status == SEARCH_SUCCESS) {
        return untagged(sr.VALUE(value));
    }
    return NULL;
}"><pre><span>const</span> <span>void</span> <span>*</span><span>hamt_get</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>)
{
    <span>hash_state</span> <span>*</span><span>hash</span> <span>=</span> <span>&amp;</span>(<span>hash_state</span>){.<span>key</span> <span>=</span> <span>key</span>,
                                     .<span>hash_fn</span> <span>=</span> <span>trie</span><span>-&gt;</span><span>key_hash</span>,
                                     .<span>hash</span> <span>=</span> <span>trie</span><span>-&gt;</span><span>key_hash</span>(<span>key</span>, <span>0</span>),
                                     .<span>depth</span> <span>=</span> <span>0</span>,
                                     .<span>shift</span> <span>=</span> <span>0</span>};
    <span>search_result</span> <span>sr</span> <span>=</span> <span>search_recursive</span>(<span>trie</span><span>-&gt;</span><span>root</span>, <span>hash</span>, <span>trie</span><span>-&gt;</span><span>key_cmp</span>, <span>key</span>,
                                        <span>NULL</span>, <span>trie</span><span>-&gt;</span><span>ator</span>);
    <span>if</span> (<span>sr</span>.<span>status</span> <span>==</span> <span>SEARCH_SUCCESS</span>) {
        <span>return</span> <span>untagged</span>(<span>sr</span>.<span>VALUE</span>(<span>value</span>));
    }
    <span>return</span> <span>NULL</span>;
}</pre></div>
<p dir="auto">In order to use <code>search_recursive()</code>, it is necessary to set up the hash state
management, initializing it with the <code>key</code>, the hashed <code>key</code>, and starting
search from level <code>0</code> (corresponding to a shift of <code>0</code>). If the search is
not successful, the function returns <code>NULL</code>, if it is successful, it passes
a <code>void</code> pointer to the value that corresponds to <code>key</code>. Note the <em>untagging</em>
of the <code>value</code> field since we're using it as a <em>tagged pointer</em> to indicate
field types.</p>
<h3 tabindex="-1" dir="auto">Insert: internal functions</h3>
<p dir="auto"><code>libhamt</code> does not support an explicit insertion function; all insertions into
the HAMT are <em>upserts</em>, i.e. after calling <code>hamt_set()</code> the API guarantees
that the requested key/value pair exists, irrespective of potential previous
entries that may have had the same key but a different value.</p>
<p dir="auto">The internal function that implements this behavior is <code>set()</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static const hamt_node *set(struct hamt *h, hamt_node *anchor, hamt_key_hash_fn hash_fn,
                            hamt_cmp_fn cmp_fn, void *key, void *value)"><pre><span>static</span> <span>const</span> <span>hamt_node</span> <span>*</span><span>set</span>(<span>struct</span> <span>hamt</span> <span>*</span><span>h</span>, <span>hamt_node</span> <span>*</span><span>anchor</span>, <span>hamt_key_hash_fn</span> <span>hash_fn</span>,
                            <span>hamt_cmp_fn</span> <span>cmp_fn</span>, <span>void</span> <span>*</span><span>key</span>, <span>void</span> <span>*</span><span>value</span>)</pre></div>
<p dir="auto"><code>set()</code> takes a HAMT, an anchor in that HAMT, hashing and comparison
functions as well as a key/value pair. After initializing the hash state, the
function makes use of <code>search_recursive</code> to find the specified <code>key</code>. It deals
with three different search outcomes: (1) if the search is successful, the
value of <code>key</code> gets replaced with the new <code>value</code>; (2) if the search is
unsuccessful because the key does not exist, it attempts to insert a new
key/value pair at the appropriate position; and (3) if the search fails due to
a key mismatch (i.e. there is an entry at the expected hash position but its
key does not equal <code>key</code>), it extends the hash trie until the new key/value
pair can be placed correctly. Cases (2) and (3) are covered by the
<code>insert_kv()</code> and <code>insert_table()</code> helper functions, respectively.</p>
<div dir="auto" data-snippet-clipboard-copy-content="static const hamt_node *set(struct hamt *h, hamt_node *anchor, hamt_key_hash_fn hash_fn,
                            hamt_cmp_fn cmp_fn, void *key, void *value)
{
    hash_state *hash = &amp;(hash_state){.key = key,
                                     .hash_fn = hash_fn,
                                     .hash = hash_fn(key, 0),
                                     .depth = 0,
                                     .shift = 0};
    search_result sr =
        search_recursive(anchor, hash, cmp_fn, key, NULL, h->ator);
    const hamt_node *inserted;
    switch (sr.status) {
    case SEARCH_SUCCESS:
        sr.VALUE(value) = tagged(value);
        inserted = sr.value;
        break;
    case SEARCH_FAIL_NOTFOUND:
        if ((inserted = insert_kv(sr.anchor, sr.hash, key, value, h->ator)) !=
            NULL) {
            h->size += 1;
        }
        break;
    case SEARCH_FAIL_KEYMISMATCH:
        if ((inserted = insert_table(sr.value, sr.hash, key, value, h->ator)) !=
            NULL) {
            h->size += 1;
        }
        break;
    }
    return inserted;
}"><pre><span>static</span> <span>const</span> <span>hamt_node</span> <span>*</span><span>set</span>(<span>struct</span> <span>hamt</span> <span>*</span><span>h</span>, <span>hamt_node</span> <span>*</span><span>anchor</span>, <span>hamt_key_hash_fn</span> <span>hash_fn</span>,
                            <span>hamt_cmp_fn</span> <span>cmp_fn</span>, <span>void</span> <span>*</span><span>key</span>, <span>void</span> <span>*</span><span>value</span>)
{
    <span>hash_state</span> <span>*</span><span>hash</span> <span>=</span> <span>&amp;</span>(<span>hash_state</span>){.<span>key</span> <span>=</span> <span>key</span>,
                                     .<span>hash_fn</span> <span>=</span> <span>hash_fn</span>,
                                     .<span>hash</span> <span>=</span> <span>hash_fn</span>(<span>key</span>, <span>0</span>),
                                     .<span>depth</span> <span>=</span> <span>0</span>,
                                     .<span>shift</span> <span>=</span> <span>0</span>};
    <span>search_result</span> <span>sr</span> <span>=</span>
        <span>search_recursive</span>(<span>anchor</span>, <span>hash</span>, <span>cmp_fn</span>, <span>key</span>, <span>NULL</span>, <span>h</span><span>-&gt;</span><span>ator</span>);
    <span>const</span> <span>hamt_node</span> <span>*</span><span>inserted</span>;
    <span>switch</span> (<span>sr</span>.<span>status</span>) {
    <span>case</span> <span>SEARCH_SUCCESS</span>:
        <span>sr</span>.<span>VALUE</span>(<span>value</span>) <span>=</span> <span>tagged</span>(<span>value</span>);
        <span>inserted</span> <span>=</span> <span>sr</span>.<span>value</span>;
        <span>break</span>;
    <span>case</span> <span>SEARCH_FAIL_NOTFOUND</span>:
        <span>if</span> ((<span>inserted</span> <span>=</span> <span>insert_kv</span>(<span>sr</span>.<span>anchor</span>, <span>sr</span>.<span>hash</span>, <span>key</span>, <span>value</span>, <span>h</span><span>-&gt;</span><span>ator</span>)) <span>!=</span>
            <span>NULL</span>) {
            <span>h</span><span>-&gt;</span><span>size</span> <span>+=</span> <span>1</span>;
        }
        <span>break</span>;
    <span>case</span> <span>SEARCH_FAIL_KEYMISMATCH</span>:
        <span>if</span> ((<span>inserted</span> <span>=</span> <span>insert_table</span>(<span>sr</span>.<span>value</span>, <span>sr</span>.<span>hash</span>, <span>key</span>, <span>value</span>, <span>h</span><span>-&gt;</span><span>ator</span>)) <span>!=</span>
            <span>NULL</span>) {
            <span>h</span><span>-&gt;</span><span>size</span> <span>+=</span> <span>1</span>;
        }
        <span>break</span>;
    }
    <span>return</span> <span>inserted</span>;
}</pre></div>
<p dir="auto">If the call to <code>search_recursive()</code> fails with <code>SEARCH_FAIL_NOTFOUND</code>, we know
that there is a free row in the table of <code>sr.anchor</code>. To insert the new
<code>key</code>/<code>value</code> pair, we calculate the position of the <code>key</code> in the current
table: it extracts the 0-31 index position for the current key and stores it
into <code>ix</code>, extends the existing <code>INDEX(anchor)</code> index bitmap to include the
new key by setting the <code>ix</code>-th bit, and then calculates the dense index
position of the new entry via <code>get_pos()</code>. It then uses <code>table_extend()</code> to
extend the table to the correct size and populates the <code>key</code> and <code>value</code>
entries to reflect the new key/value pair. Note the pointer tagging on the
value field to mark it as a key/value row in the table (as opposed to a row
that points to a sub-table).</p>
<div dir="auto" data-snippet-clipboard-copy-content="static const hamt_node *insert_kv(hamt_node *anchor, hash_state *hash,
                                  void *key, void *value,
                                  struct hamt_allocator *ator)
{
    /* calculate position in new table */
    uint32_t ix = hash_get_index(hash);
    uint32_t new_index = INDEX(anchor) | (1 << ix);
    int pos = get_pos(ix, new_index);
    /* extend table */
    size_t n_rows = get_popcount(INDEX(anchor));
    anchor = table_extend(ator, anchor, n_rows, ix, pos);
    if (!anchor)
        return NULL;
    hamt_node *new_table = TABLE(anchor);
    /* set new k/v pair */
    new_table[pos].as.kv.key = key;
    new_table[pos].as.kv.value = tagged(value);
    /* return a pointer to the inserted k/v pair */
    return &amp;new_table[pos];
}"><pre><span>static</span> <span>const</span> <span>hamt_node</span> <span>*</span><span>insert_kv</span>(<span>hamt_node</span> <span>*</span><span>anchor</span>, <span>hash_state</span> <span>*</span><span>hash</span>,
                                  <span>void</span> <span>*</span><span>key</span>, <span>void</span> <span>*</span><span>value</span>,
                                  <span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>)
{
    <span>/* calculate position in new table */</span>
    <span>uint32_t</span> <span>ix</span> <span>=</span> <span>hash_get_index</span>(<span>hash</span>);
    <span>uint32_t</span> <span>new_index</span> <span>=</span> <span>INDEX</span>(<span>anchor</span>) | (<span>1</span> &lt;&lt; <span>ix</span>);
    <span>int</span> <span>pos</span> <span>=</span> <span>get_pos</span>(<span>ix</span>, <span>new_index</span>);
    <span>/* extend table */</span>
    <span>size_t</span> <span>n_rows</span> <span>=</span> <span>get_popcount</span>(<span>INDEX</span>(<span>anchor</span>));
    <span>anchor</span> <span>=</span> <span>table_extend</span>(<span>ator</span>, <span>anchor</span>, <span>n_rows</span>, <span>ix</span>, <span>pos</span>);
    <span>if</span> (!<span>anchor</span>)
        <span>return</span> <span>NULL</span>;
    <span>hamt_node</span> <span>*</span><span>new_table</span> <span>=</span> <span>TABLE</span>(<span>anchor</span>);
    <span>/* set new k/v pair */</span>
    <span>new_table</span>[<span>pos</span>].<span>as</span>.<span>kv</span>.<span>key</span> <span>=</span> <span>key</span>;
    <span>new_table</span>[<span>pos</span>].<span>as</span>.<span>kv</span>.<span>value</span> <span>=</span> <span>tagged</span>(<span>value</span>);
    <span>/* return a pointer to the inserted k/v pair */</span>
    <span>return</span> <span>&amp;</span><span>new_table</span>[<span>pos</span>];
}</pre></div>
<p dir="auto">When the call to <code>search_recursive()</code> in <code>set()</code> fails with
<code>SEARCH_FAIL_KEYMISMATCH</code>, the situation is different: there is another entry
(either a key/value pair or a reference to a sub-table) in the HAMT that
currently occupies a transitionary trie location for <code>key</code>. This is expected
to happen regularly: keys are always inserted with the shortest possible trie
path that resolves hashing conflicts between <em>existing</em> keys. As more and more
entries are added to the HAMT, these paths necessarily must increase in
length. This situation is handled by <code>insert_table()</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static const hamt_node *insert_table(hamt_node *anchor, hash_state *hash,
                                     void *key, void *value,
                                     struct hamt_allocator *ator)
{
    /* Collect everything we know about the existing value */
    hash_state *x_hash =
        &amp;(hash_state){.key = KEY(anchor),
                      .hash_fn = hash->hash_fn,
                      .hash = hash->hash_fn(KEY(anchor), hash->depth / 5),
                      .depth = hash->depth,
                      .shift = hash->shift};
    void *x_value = VALUE(anchor); /* tagged (!) value ptr */
    /* increase depth until the hashes diverge, building a list
     * of tables along the way */
    hash_state *next_hash = hash_next(hash);
    hash_state *x_next_hash = hash_next(x_hash);
    uint32_t next_index = hash_get_index(next_hash);
    uint32_t x_next_index = hash_get_index(x_next_hash);
    while (x_next_index == next_index) {
        TABLE(anchor) = table_allocate(ator, 1);
        INDEX(anchor) = (1 << next_index);
        next_hash = hash_next(next_hash);
        x_next_hash = hash_next(x_next_hash);
        next_index = hash_get_index(next_hash);
        x_next_index = hash_get_index(x_next_hash);
        anchor = TABLE(anchor);
    }
    /* the hashes are different, let's allocate a table with two
     * entries to store the existing and new values */
    TABLE(anchor) = table_allocate(ator, 2);
    INDEX(anchor) = (1 << next_index) | (1 << x_next_index);
    /* determine the proper position in the allocated table */
    int x_pos = get_pos(x_next_index, INDEX(anchor));
    int pos = get_pos(next_index, INDEX(anchor));
    /* fill in the existing value; no need to tag the value pointer
     * since it is already tagged. */
    TABLE(anchor)[x_pos].as.kv.key = (void *)x_hash->key;
    TABLE(anchor)[x_pos].as.kv.value = x_value;
    /* fill in the new key/value pair, tagging the pointer to the
     * new value to mark it as a value ptr */
    TABLE(anchor)[pos].as.kv.key = key;
    TABLE(anchor)[pos].as.kv.value = tagged(value);

    return &amp;TABLE(anchor)[pos];
}"><pre><span>static</span> <span>const</span> <span>hamt_node</span> <span>*</span><span>insert_table</span>(<span>hamt_node</span> <span>*</span><span>anchor</span>, <span>hash_state</span> <span>*</span><span>hash</span>,
                                     <span>void</span> <span>*</span><span>key</span>, <span>void</span> <span>*</span><span>value</span>,
                                     <span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>)
{
    <span>/* Collect everything we know about the existing value */</span>
    <span>hash_state</span> <span>*</span><span>x_hash</span> <span>=</span>
        <span>&amp;</span>(<span>hash_state</span>){.<span>key</span> <span>=</span> <span>KEY</span>(<span>anchor</span>),
                      .<span>hash_fn</span> <span>=</span> <span>hash</span><span>-&gt;</span><span>hash_fn</span>,
                      .<span>hash</span> <span>=</span> <span>hash</span><span>-&gt;</span><span>hash_fn</span>(<span>KEY</span>(<span>anchor</span>), <span>hash</span><span>-&gt;</span><span>depth</span> / <span>5</span>),
                      .<span>depth</span> <span>=</span> <span>hash</span><span>-&gt;</span><span>depth</span>,
                      .<span>shift</span> <span>=</span> <span>hash</span><span>-&gt;</span><span>shift</span>};
    <span>void</span> <span>*</span><span>x_value</span> <span>=</span> <span>VALUE</span>(<span>anchor</span>); <span>/* tagged (!) value ptr */</span>
    <span>/* increase depth until the hashes diverge, building a list</span>
<span>     * of tables along the way */</span>
    <span>hash_state</span> <span>*</span><span>next_hash</span> <span>=</span> <span>hash_next</span>(<span>hash</span>);
    <span>hash_state</span> <span>*</span><span>x_next_hash</span> <span>=</span> <span>hash_next</span>(<span>x_hash</span>);
    <span>uint32_t</span> <span>next_index</span> <span>=</span> <span>hash_get_index</span>(<span>next_hash</span>);
    <span>uint32_t</span> <span>x_next_index</span> <span>=</span> <span>hash_get_index</span>(<span>x_next_hash</span>);
    <span>while</span> (<span>x_next_index</span> <span>==</span> <span>next_index</span>) {
        <span>TABLE</span>(<span>anchor</span>) <span>=</span> <span>table_allocate</span>(<span>ator</span>, <span>1</span>);
        <span>INDEX</span>(<span>anchor</span>) <span>=</span> (<span>1</span> &lt;&lt; <span>next_index</span>);
        <span>next_hash</span> <span>=</span> <span>hash_next</span>(<span>next_hash</span>);
        <span>x_next_hash</span> <span>=</span> <span>hash_next</span>(<span>x_next_hash</span>);
        <span>next_index</span> <span>=</span> <span>hash_get_index</span>(<span>next_hash</span>);
        <span>x_next_index</span> <span>=</span> <span>hash_get_index</span>(<span>x_next_hash</span>);
        <span>anchor</span> <span>=</span> <span>TABLE</span>(<span>anchor</span>);
    }
    <span>/* the hashes are different, let's allocate a table with two</span>
<span>     * entries to store the existing and new values */</span>
    <span>TABLE</span>(<span>anchor</span>) <span>=</span> <span>table_allocate</span>(<span>ator</span>, <span>2</span>);
    <span>INDEX</span>(<span>anchor</span>) <span>=</span> (<span>1</span> &lt;&lt; <span>next_index</span>) | (<span>1</span> &lt;&lt; <span>x_next_index</span>);
    <span>/* determine the proper position in the allocated table */</span>
    <span>int</span> <span>x_pos</span> <span>=</span> <span>get_pos</span>(<span>x_next_index</span>, <span>INDEX</span>(<span>anchor</span>));
    <span>int</span> <span>pos</span> <span>=</span> <span>get_pos</span>(<span>next_index</span>, <span>INDEX</span>(<span>anchor</span>));
    <span>/* fill in the existing value; no need to tag the value pointer</span>
<span>     * since it is already tagged. */</span>
    <span>TABLE</span>(<span>anchor</span>)[<span>x_pos</span>].<span>as</span>.<span>kv</span>.<span>key</span> <span>=</span> (<span>void</span> <span>*</span>)<span>x_hash</span><span>-&gt;</span><span>key</span>;
    <span>TABLE</span>(<span>anchor</span>)[<span>x_pos</span>].<span>as</span>.<span>kv</span>.<span>value</span> <span>=</span> <span>x_value</span>;
    <span>/* fill in the new key/value pair, tagging the pointer to the</span>
<span>     * new value to mark it as a value ptr */</span>
    <span>TABLE</span>(<span>anchor</span>)[<span>pos</span>].<span>as</span>.<span>kv</span>.<span>key</span> <span>=</span> <span>key</span>;
    <span>TABLE</span>(<span>anchor</span>)[<span>pos</span>].<span>as</span>.<span>kv</span>.<span>value</span> <span>=</span> <span>tagged</span>(<span>value</span>);

    <span>return</span> <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>pos</span>];
}</pre></div>
<p dir="auto"><code>insert_table()</code> works in three stages: (1) it initiatlizes the <code>hash_state</code>
for the current anchor; (2) creates a series of single-entry tables until the
hashes of the current and new keys diverge; and (3) finally creates a new
table of size 2 that holds the old entry as well as the new key/value pair.</p>
<h3 tabindex="-1" dir="auto">Insert: external API</h3>
<p dir="auto">The implementation of the external API for inserting and updating values in
the HAMT is straighforward:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const void *hamt_set(struct hamt *trie, void *key, void *value)
{
    const hamt_node *n =
        set(trie, trie->root, trie->key_hash, trie->key_cmp, key, value);
    return VALUE(n);
}"><pre><span>const</span> <span>void</span> <span>*</span><span>hamt_set</span>(<span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>, <span>void</span> <span>*</span><span>value</span>)
{
    <span>const</span> <span>hamt_node</span> <span>*</span><span>n</span> <span>=</span>
        <span>set</span>(<span>trie</span>, <span>trie</span><span>-&gt;</span><span>root</span>, <span>trie</span><span>-&gt;</span><span>key_hash</span>, <span>trie</span><span>-&gt;</span><span>key_cmp</span>, <span>key</span>, <span>value</span>);
    <span>return</span> <span>VALUE</span>(<span>n</span>);
}</pre></div>
<p dir="auto"><code>hamt_set()</code> uses a vanilla call to the internal <code>set()</code> function and returns
a pointer to the value of the new key.</p>
<h3 tabindex="-1" dir="auto">Remove</h3>
<h3 tabindex="-1" dir="auto">Iterators</h3>
<h2 tabindex="-1" dir="auto">Persistent data structures and structural sharing</h2>
<h3 tabindex="-1" dir="auto">Path copying</h3>
<h3 tabindex="-1" dir="auto">Insert</h3>
<h3 tabindex="-1" dir="auto">Remove</h3>
<h2 tabindex="-1" dir="auto">Appendix</h2>
<h2 tabindex="-1" dir="auto">Unit testing</h2>
<p dir="auto">For testing, <code>hamt</code> uses a variant of <a href="http://www.jera.com/techinfo/jtns/jtn002.html" rel="nofollow">John Brewer's <code>minunit</code> testing
framework</a>. Minunit is extremely minimalistic and its
header-only implementation easily fits on a single page:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// test/minunit.h
#ifndef MINUNIT_H
#define MINUNIT_H

#define MU_ASSERT(test, message)                                               \
    do {                                                                       \
        if (!(test))                                                           \
            return message;                                                    \
    } while (0)
#define MU_RUN_TEST(test)                                                      \
    do {                                                                       \
        char *message = test();                                                \
        mu_tests_run++;                                                        \
        if (message)                                                           \
            return message;                                                    \
    } while (0)

#define MU_TEST_CASE(name) static char *name()
#define MU_TEST_SUITE(name) static char *name()

extern int mu_tests_run;

#endif /* !MINUNIT_H */"><pre><span>// test/minunit.h</span>
<span>#ifndef</span> <span>MINUNIT_H</span>
<span>#define</span> <span>MINUNIT_H</span>

<span>#define</span> <span>MU_ASSERT</span>(<span>test</span>, <span>message</span>)                                               \
    do {                                                                       \
        if (!(test))                                                           \
            return message;                                                    \
    } while (0)
<span>#define</span> <span>MU_RUN_TEST</span>(<span>test</span>)                                                      \
    do {                                                                       \
        char *message = test();                                                \
        mu_tests_run++;                                                        \
        if (message)                                                           \
            return message;                                                    \
    } while (0)

<span>#define</span> <span>MU_TEST_CASE</span>(<span>name</span>) static char *name()
<span>#define</span> <span>MU_TEST_SUITE</span>(<span>name</span>) static char *name()

<span>extern</span> <span>int</span> <span>mu_tests_run</span>;

<span>#endif</span> <span>/* !MINUNIT_H */</span></pre></div>
<p dir="auto">With <code>minunit</code>, every unit test is a <code>MU_TEST_CASE</code> We use <code>MU_ASSERT</code> to test
the test invariants.  Test cases are grouped into <code>MU_TEST_SUITE</code>s as
sequential calls to <code>MU_RUN_TEST</code>.  When an assertion fails, the <code>return</code>
statement in <code>MU_ASSERT</code> short-circuts test execution and returns a non-null
pointer to the respective <code>message</code> (generally a static string). This, in turn,
causes <code>MU_RUN_TEST</code> to issue a <code>return</code> call with the string pointer,
short-circuting the remaining test suite. The header also declares a global
variable <code>mu_tests_run</code> that keeps track of the total number of executed
tests.</p>
<p dir="auto">The following listing illustrates the basic structure of unit test
implementations with <code>minunit</code>, check the <a href="https://github.com/mkirchner/hamt/blob/main/test/test_hamt.c">actual tests</a> for
a full listing.</p>
<div dir="auto" data-snippet-clipboard-copy-content="// test/test_hamt.c
#include &quot;minunit.h&quot;
#include &quot;../src/hamt.c&quot;

int mu_tests_run = 0;

MU_TEST_CASE(test_dummy)
{
    /* do something here */
    MU_ASSERT(0 == 0, &quot;Oops X-{&quot;);
    return 0;
}

MU_TEST_SUITE(test_suite)
{
    /* Add tests here */
    MU_RUN_TEST(test_dummy);
    /*
     * ... many more ...
     */
    return 0;
}

int main()
{
    printf(&quot;---=[ Hash array mapped trie tests\n&quot;);
    char *result = test_suite();
    if (result != 0) {
        printf(&quot;%s\n&quot;, result);
    } else {
        printf(&quot;All tests passed.\n&quot;);
    }
    printf(&quot;Tests run: %d\n&quot;, tests_run);
    return result != 0;
}"><pre><span>// test/test_hamt.c</span>
<span>#include</span> <span>"minunit.h"</span>
<span>#include</span> <span>"../src/hamt.c"</span>

<span>int</span> <span>mu_tests_run</span> <span>=</span> <span>0</span>;

<span>MU_TEST_CASE</span>(<span>test_dummy</span>)
{
    <span>/* do something here */</span>
    <span>MU_ASSERT</span>(<span>0</span> <span>==</span> <span>0</span>, <span>"Oops X-{"</span>);
    <span>return</span> <span>0</span>;
}

<span>MU_TEST_SUITE</span>(<span>test_suite</span>)
{
    <span>/* Add tests here */</span>
    <span>MU_RUN_TEST</span>(<span>test_dummy</span>);
    <span>/*</span>
<span>     * ... many more ...</span>
<span>     */</span>
    <span>return</span> <span>0</span>;
}

<span>int</span> <span>main</span>()
{
    <span>printf</span>(<span>"---=[ Hash array mapped trie tests\n"</span>);
    <span>char</span> <span>*</span><span>result</span> <span>=</span> <span>test_suite</span>();
    <span>if</span> (<span>result</span> <span>!=</span> <span>0</span>) {
        <span>printf</span>(<span>"%s\n"</span>, <span>result</span>);
    } <span>else</span> {
        <span>printf</span>(<span>"All tests passed.\n"</span>);
    }
    <span>printf</span>(<span>"Tests run: %d\n"</span>, <span>tests_run</span>);
    <span>return</span> <span>result</span> <span>!=</span> <span>0</span>;
}</pre></div>
<p dir="auto">Note that the test setup <code>include</code>s the <code>hamt.c</code> implementation file. This is a
common trick used in unit testing to gain easy access to testing <code>static</code>
functions that would otherwise be inaccessible since they are local to the
<code>hamt.c</code> compilation unit. This requires some care in
the Makefile setup in order to avoid symbol duplication.</p>
<h2 tabindex="-1" dir="auto">Footnotes</h2>
<p dir="auto"><b id="user-content-fn_hash_table_cpp">[1]</b>
The <code>std::unordered_*</code> methods implement open hashing (aka separate chaining),
with the hash table being an array of buckets, each pointing to the head of a
linked list. This is a deliberate and reasonable compromise for general use;
gaining an order of magnitude of speed improvements for specialized use cases
(e.g. append-only, guaranteed high-quality hash functions) is possible. See
<a href="https://stackoverflow.com/a/31113618" rel="nofollow">this stackoverflow post</a> for a summary of the <a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2003/n1456.html" rel="nofollow">standard
proposal</a>.
<a href="#ac_hash_table_cpp"><g-emoji alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">↩</g-emoji></a></p>
<p dir="auto"><b id="user-content-fn_hash_table_c">[2]</b>
<code>musl</code> provides a <code>hsearch</code> implementation that uses closed hashing with
quadratic probing for conflict resolution. The
<a href="https://git.musl-libc.org/cgit/musl/tree/src/search/hsearch.c" rel="nofollow">documentation</a> states that they use powers of two for
table sizing which seems wrong due to the impact on the modulo (table sizes
should ideally be prime). The GLib <code>GHashTable</code> has surprisingly little
documentation in its implementation details but <a href="https://gitlab.gnome.org/GNOME/glib/-/blob/main/glib/ghash.c" rel="nofollow">appears to be
using</a> a separate chaining approach similar to the C++
solution.
<a href="#ac_hash_table_c"><g-emoji alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">↩</g-emoji></a></p>
<p dir="auto"><b id="user-content-fn_hash_table_python">[3]</b> Python's <code>dict</code> implementation uses
closed hashing (aka open addressing) with pseudo-random probing to mitigate
the poor hashing properties of standard python <code>hash()</code> function for some data
types (from <a href="https://stackoverflow.com/a/9022835" rel="nofollow">here</a>). Python keeps the load factor below
0.66; this avoids gradual performance degradation associated w/ high load
factors in closed hashing but comes at increased memory footprint. The
<a href="https://github.com/python/cpython/blob/main/Objects/dictobject.c">codebase</a> was refactored to split the actual data from the
hash table in 3.6, resulting in better memory efficiency and GC friendliness
(see <a href="https://morepypy.blogspot.com/2015/01/faster-more-memory-efficient-and-more.html" rel="nofollow">here</a> and <a href="https://mail.python.org/pipermail/python-dev/2012-December/123028.html" rel="nofollow">here</a>).
<a href="#ac_hash_table_python"><g-emoji alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">↩</g-emoji></a></p>
<p dir="auto"><b id="user-content-fn_hash_table_java">[4]</b> Java provides <code>Hashtable&lt;K,V&gt;</code> and
<code>HashMap&lt;K,V&gt;</code>, both of which implement <code>Map</code> and <code>Collection</code> interfaces; in
addition, <code>Hashtable</code> is synchronized. The <code>HashSet</code> type internally uses a
<code>HashMap</code>. <code>Hashtable</code> and <code>HashMap</code> implement open hashing
(separate chaining) with a default load factor of 0.75; The OpenJDK
implementation of <code>HashMap</code> converts
between linked list and tree representations in the hash buckets, depending on
bucket size, see <a href="https://github.com/openjdk/jdk17/blob/74007890bb9a3fa3a65683a3f480e399f2b1a0b6/src/java.base/share/classes/java/util/HashMap.java">the source</a>.
<a href="#ac_hash_table_java"><g-emoji alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">↩</g-emoji></a></p>
<p dir="auto"><b id="user-content-fn_cpp_virtual_method_table">[5]</b>
There are alternative approaches to enable (somewhat) typesafe templating in
C, mainly by implementing what basically amounts to virtual method tables
using the C preprocessor. See e.g. <a href="https://stackoverflow.com/questions/10950828/simulation-of-templates-in-c-for-a-queue-data-type/11035347" rel="nofollow">here</a> for a useful stackoverflow
summary or <a href="http://blog.pkh.me/p/20-templating-in-c.html" rel="nofollow">here</a> for a more in-depth treatise.
<a href="#ac_cpp_virtual_method_table"><g-emoji alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">↩</g-emoji></a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: The full source code and assets for my custom game engine and game (116 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36672183</link>
            <guid>36672183</guid>
            <pubDate>Mon, 10 Jul 2023 20:04:29 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36672183">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="36679012"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36679012" href="https://news.ycombinator.com/vote?id=36679012&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>More of a general nooby question: what would be a good way to read through code like this? Where to start? How to understand what's going on and when? etc.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36679331"><td></td></tr>
                  <tr id="36674742"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36674742" href="https://news.ycombinator.com/vote?id=36674742&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>You should probably give credit to Casey / Handmade Hero with regards to your windows platform code. Well its kinda scattered around, so probably just credit in general.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36676453"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36676453" href="https://news.ycombinator.com/vote?id=36676453&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>True, Casey/HH was a huge inspiration and the project started from following the first few days of Handmade Hero. I should mention it.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36675541"><td></td></tr>
                <tr id="36677434"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36677434" href="https://news.ycombinator.com/vote?id=36677434&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><p><span>Here 'from scratch' probably means 'from fundamentals', e.g. without a prefab game engine, as opposed to 'taking no inspiration or lessons from anyone else or using any library code written by other people'.<p>Though that would be one hell of a project.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36677483"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36677483" href="https://news.ycombinator.com/vote?id=36677483&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><p><span>I mentioned in a couple of places that I did use other people's file loading libraries (mainly Sean Barrett's excellent stb libs).<p>“If you wish to make an apple pie from scratch, you must first invent the universe” :)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36678074"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36678074" href="https://news.ycombinator.com/vote?id=36678074&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><p><span>&gt; “If you wish to make an apple pie from scratch, you must first invent the universe” :)<p>This is something I posted on Casey's first handmade video, together with some counter arguments to his rant on the shortcomings of c++ and windows. He has since disabled comments.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                    <tr id="36675969"><td></td></tr>
                <tr id="36676445"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36676445" href="https://news.ycombinator.com/vote?id=36676445&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>The HTTPS certificate expired on my privacy policy page and Google delists the app when that happens. After you renew they require that you resubmit the app and I didn't bother since there were barely any downloads then.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36678562"><td></td></tr>
                        <tr id="36673476"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36673476" href="https://news.ycombinator.com/vote?id=36673476&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>I work in tools for game devs and it's great to see more open-source work like this it's very hard to find good modern open-source game examples.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36676492"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36676492" href="https://news.ycombinator.com/vote?id=36676492&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>Thanks, it's not the cleanest code and for example my immediate mode GUI is nowhere near IMGUI but theres still plenty to look at.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36675685"><td></td></tr>
                <tr id="36676506"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36676506" href="https://news.ycombinator.com/vote?id=36676506&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>Thanks! Finishing such a large project feels great and you learn a lot when you have to implement so much you take for granted when using things other people have built</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36679448"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36679448" href="https://news.ycombinator.com/vote?id=36679448&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>Yes, congratulations on finishing the game! Also, thank you very much for sharing the story of making it. I'm more inspired than ever to keep dev logs for the little side projects I work on. It's important to keep track of progress, reflect on where you were early on, and see how far you've come.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36679689"><td></td></tr>
                              <tr id="36674045"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36674045" href="https://news.ycombinator.com/vote?id=36674045&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><p><span>Any idea what you would have to do to get it running on iOS?<p>Did you release it? If so, how is it doing compared to your expectations? (I looked for it on the Play store but couldn't find it)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36676471"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36676471" href="https://news.ycombinator.com/vote?id=36676471&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><p><span>You would have to write the platform code for iOS similarly to how the Android and Windows is written. The initial idea was to release it on iOS as well but at the end I didn't want to invest in buying a Mac, the license and the time to port it.<p>Edit: As for the release, I think it barely broke a couple of hundred downloads. I didn't do any real marketing and had no real goal of really getting the game out there as it was a learning experience for me first and foremost.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36675869"><td></td></tr>
                  <tr id="36672219"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36672219" href="https://news.ycombinator.com/vote?id=36672219&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>I'm also making a game from scratch and reinventing wheels. What is the processing that you are doing for GLES? I see the GLSL_PREPROCESS in the GLSL.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36672236"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36672236" href="https://news.ycombinator.com/vote?id=36672236&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>I wanted to include common code for all shaders in a simple way, but I was too lazy write a proper parser. So instead I used the C++ preprocessor and macros to include things from common shader code and combine them in the build step.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36674193"><td></td></tr>
                <tr id="36676533"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36676533" href="https://news.ycombinator.com/vote?id=36676533&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>I could have definitely used that and I'm surprised I didn't realise it when researching how to include shader code. Luckily my setup required very little work and with my system I could share snippets between GLES and GL.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="36674356"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36674356" href="https://news.ycombinator.com/vote?id=36674356&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><p><span>if youre wondering why its not on the play store<p>&gt; The game has been since delisted on the Play Store, however you can download the APK here.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36676508"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36676508" href="https://news.ycombinator.com/vote?id=36676508&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>Yeah the privacy policy page had an expired certificate and Google delists the app automatically when that happens.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[2048 Bit RSA and the Year 2030 (227 pts)]]></title>
            <link>https://articles.59.ca/doku.php?id=em:20482030</link>
            <guid>36672115</guid>
            <pubDate>Mon, 10 Jul 2023 19:58:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://articles.59.ca/doku.php?id=em:20482030">https://articles.59.ca/doku.php?id=em:20482030</a>, See on <a href="https://news.ycombinator.com/item?id=36672115">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page">
                                                            <!-- wikipage start -->
                    <!-- TOC START -->
<div id="dw__toc">
<h3>Table of Contents</h3>

</div>
<!-- TOC END -->


<p>
In the course of some recent work I developed the impression that 2048 RSA was quite secure. Canada<sup><a href="#fn__1" id="fnt__1">1)</a></sup> (my country of residence) and others
<sup><a href="#fn__2" id="fnt__2">2)</a></sup> are currently strongly suggesting that 2048 bit RSA should be considered potentially insecure after the year 2030 and that the minimum length considered secure should be then be 3072 bits. That is only 7 years from now (2023).
</p>

<h2 id="where_did_the_2030_cutoff_come_from">Where did the 2030 cutoff come from?</h2>
<div>

<p>
I am reasonably certain that the ideas here came from an influential paper released in 2004 by Arjen K. Lenstra<sup><a href="#fn__3" id="fnt__3">3)</a></sup> that showed this year in a table. Here is a simplified version of the table:
</p>
<div><table>
	<thead>
	<tr>
		<th> Modulus Bit Length </th><th> Conservative Year </th><th> Optimistic Year </th>
	</tr>
	</thead>
	<tbody><tr>
		<td> 1024 </td><td> 2006 </td><td> 2006 </td>
	</tr>
	<tr>
		<td> 1280 </td><td> 2014 </td><td> 2017 </td>
	</tr>
	<tr>
		<td> 1536 </td><td> 2020 </td><td> 2025 </td>
	</tr>
	<tr>
		<td> 2048 </td><td> 2030 </td><td> 2040 </td>
	</tr>
	<tr>
		<td> 3072 </td><td> 2046 </td><td> 2065 </td>
	</tr>
	<tr>
		<td> 4096 </td><td> 2060 </td><td> 2085 </td>
	</tr>
	<tr>
		<td> 8192 </td><td> 2100 </td><td> 2142 </td>
	</tr>
</tbody></table></div>

<p>
<em><sub>Common RSA modulus bit-length life spans. Table 4 from: <a href="https://infoscience.epfl.ch/record/164539/files/NPDF-32.pdf" target="_tab" title="https://infoscience.epfl.ch/record/164539/files/NPDF-32.pdf" rel="ugc nofollow noopener">Key Lengths</a></sub></em>
</p>

<p>
So we look at the 2048 bit row, decide we do not feel all that optimistic, and then choose 2030 as the cutoff date. Simple. Straightforward. Definite. Great for long term planning…
</p>

<p>
Lenstra's prediction was largely based on two observations:
</p>
<ul>
<li><p> That the performance of computing technology was doubling every 18 months.</p>
</li>
<li><p> That the increase of factoring efficiency due to software improvement was doubling every 18 months.</p>
</li>
</ul>

<p>
Combining the two observations means that as of 2004, the capability to attack RSA 2048 was doubling every 9 months. Then 2004 to 2030 is 26 years or 312 months or 35 doublings. That is a capability increase of 2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2 or 2<sup>35</sup> in exponential notation which works out to a predicted capability increase of 34,359,738,368 times as of 2030. So around 34 billion (34×10<sup>9</sup>) times. This figure can be interpreted as how far 2048 bit RSA encryption was out of reach as of 2004. This is what that increase looks like on a plot:
</p>

<p>
<a href="https://articles.59.ca/lib/exe/detail.php?id=em%3A20482030&amp;media=em:9m0430.svg" target="_tab" title="em:9m0430.svg" rel="noopener"><img src="https://articles.59.ca/lib/exe/fetch.php?media=em:9m0430.svg" loading="lazy" alt=""></a><br>

<em><sub>Doubling every 9 months from 2004 to 2030</sub></em>
</p>

<p>
This sort of curve and this sort of increase is called “exponential” after exponential notation. It is well known that any physical quantity can not increase in this way past a certain point. There is a common and ancient story that illustrates this principle in a way quite appropriate to this discussion:
</p>

<p>
Someone is to be rewarded. As a reward, they ask that the following process be completed and that they should then be rewarded with the resultant number of wheat/rice grains.
</p>

<p>
Place a single grain on the first square of a chessboard. Place double that number of grains (2) on the next empty square. Then double <em>that</em> number of wheat grains (4) on the next empty square. If you fill all 64 squares of the chessboard, doubling each time, you end up with an amount of wheat some thousands of times more than the entire yearly production of wheat on the entire planet. Obviously this process can never be completed. The exponential nature of the increase precludes that<sup><a href="#fn__4" id="fnt__4">4)</a></sup>. An appropriate quote:
</p>
<blockquote><p>
Exponentials can't go on forever, because they will gobble up everything.<br>
</p></blockquote>

<p>
<sub>Carl Sagan, <em>Billions and Billions: Thoughts On Life And Death At the Brink Of The Millennium</em></sub>
</p>

<p>
Having seen that we have a predicted exponential increase of RSA factoring<sup><a href="#fn__5" id="fnt__5">5)</a></sup> capability here, an important task will be to determine if a limit has yet occurred that would prevent that predicted increase.
</p>

</div>

<h2 id="how_did_things_actually_go">How did things actually go?</h2>
<div>

<p>
The best we can do here is to look at the length of the numbers, of the sort relevant to RSA, that have been actually factored. So far, the largest number is 829 bits long<sup><a href="#fn__6" id="fnt__6">6)</a></sup> which was factored in 2020. Late in 2003, a 576 bit number was factored. Here is a plot of those factoring achievements, including the ones between them:
</p>

<p>
<a href="https://articles.59.ca/lib/exe/detail.php?id=em%3A20482030&amp;media=em:factoring.svg" target="_tab" title="em:factoring.svg" rel="noopener"><img src="https://articles.59.ca/lib/exe/fetch.php?media=em:factoring.svg" loading="lazy" alt=""></a>
</p>

<p>
I ignored the impression that the increase in factoring capability seems to have levelled off after 2009 and added a linear extrapolation based on the record factorizations over the entire predicted interval. The result (1024 bits by 2030) was still quite underwhelming. One would expect a more definite trend if the capability was increasing exponentially. The increasing trend is not enough to make us think that 2048 bit RSA would be under threat by 2030.
</p>

<p>
If the actual factoring demonstrations <em>had</em> shown a more definite upward trend then we could assume that the researchers were simply funded at a lower level than, say, some over funded state run signals intelligence agency and that there was the possibility of a genuine threat. But as it is, this is just inconclusive for our purposes. There are any number of reasons that less time and fewer resources might be allocated to these factoring demonstrations. We have to find another way to approach this question.
</p>

</div>

<h2 id="how_are_the_basic_assumptions_holding_up_so_far">How are the basic assumptions holding up so far?</h2>
<p>
Let's consider the two basic assumptions that the prediction was based on:
</p>

<h3 id="factoring_algorithm_performance">Factoring Algorithm Performance</h3>
<div>

<p>
The assumption is that the increase of factoring efficiency due to software improvement will double every 18 months. 
</p>

<p>
It isn't normally possible to predict the rate of software efficiency increase. It is safe to assume that there will be <em>some</em> increase of performance possible for a particular software system, but not how much or when. Software performance improvement, very generally, seems to have three phases:
</p>
<div><table>
	<thead>
	<tr>
		<th>Phase              </th><th> Performance Increase </th><th> Relative Complexity </th>
	</tr>
	</thead>
	<tbody><tr>
		<td>Low hanging fruit  </td><td>Large                 </td><td>Low                  </td>
	</tr>
	<tr>
		<td>High hanging fruit </td><td>Low                   </td><td>High                 </td>
	</tr>
	<tr>
		<td>New algorithm      </td><td>Small to very large   </td><td>—                  </td>
	</tr>
</tbody></table></div>

<p>
So improvement is first easy in the “low hanging fruit” phase and progressively harder in the “high hanging fruit” phase. The cost of this improvement is increased complexity. At some point someone might invent a new algorithm and the cycle can continue. The improvement that comes with a new algorithm can be very large; perhaps even enough to count as exponential if it happens more than once.
</p>

<p>
That is exactly what happened in the case of factoring algorithms in the '80s and '90s<sup><a href="#fn__7" id="fnt__7">7)</a></sup>. The <em>continued fraction factoring algorithm</em> was followed by the <em>quadratic sieve factoring algorithm</em> which was in turn followed by the <em>number field sieve</em> (NFS) algorithm. At the time of Lenstra's paper it seemed that his <em>elliptic curve method</em> might exceed the capability of the NFS algorithm.
</p>

<p>
For the 19 years from 2004 and 2023 and an assumption of 18 month doubling we end up with a predicted increase of algorithm performance of 6,502 times. It is hard to find definite figures on actual algorithm performance increase in this interval as it is mixed in with hardware performance. The CADO-NFS<sup><a href="#fn__8" id="fnt__8">8)</a></sup> system claims a performance increase of 3.2 from version 1.1 to version 2.3.0 at the 512 bit (RSA-155) level. The researchers responsible for the most recent factoring record<sup><a href="#fn__9" id="fnt__9">9)</a></sup> (829 bits) claimed a performance increase of 3-4 times. Even combining these improvements (I am not sure that is appropriate) we end up with a performance increase of 12 times which is well short of the predicted 6,502 times.
</p>

<p>
The elliptic curve method never ended up exceeding the NFS algorithm for factoring at the scale of RSA 2048. The NFS algorithm was never exceeded by any other invented algorithm. As a result the NFS algorithm is still the best available 27 years after its invention. That seems to be the reason for the severe slowdown in algorithm performance increase.
</p>

<p>
The history of computing shows that there is normally a quick burst of progress related to algorithmic performance when some particular task becomes feasible followed by a long period of very gradual improvement. The popular example of sorting is a good example. The mergesort, quicksort and heapsort algorithms were invented in 1945, 1959 and 1964 respectively. They are still in routine use today for sorting large lists of values. In retrospect it very much seems that the same sort of thing happened with respect to factoring algorithms. The complexity here seems to be high (CADO-NFS has hundreds of thousands of lines of code) so we are probably in the “high hanging fruit” phase. At this point there seems to be no reason to expect the predicted exponential increase in factoring algorithm performance (165,140 times) required to threaten 2048 bit RSA by the year 2030.
</p>

</div>

<h3 id="computing_performance">Computing Performance</h3>
<div>

<p>
The prediction is based in the assumption that the available computing performance will double every 18 months.
</p>

<p>
This assumption is popularly known as “Moore's Law”<sup><a href="#fn__10" id="fnt__10">10)</a></sup>. It has become fashionable to speculate that Moore's law is now dead. That is both true and false.
</p>

<p>
It turns out that there are two versions of Moore's law. The 18 month doubling refers to the increase in available computing performance and is appropriate to our discussion here. The original Moore's law refers to the number of <a href="https://en.wikipedia.org/wiki/Transistor" target="_tab" title="https://en.wikipedia.org/wiki/Transistor" rel="noopener">transistors</a> available on a substrate of a particular size and is now generally accepted to mean a doubling every 24 months. The 24 month law related to the number of transistors is still alive (but the rate of examples is decreasing rapidly). The 18 month performance law is the one that is dead. That's the one that the 2030 prediction is based on…
</p>

<p>
If you double the number of transistors on a substrate of a particular size then if the power consumption of those new transistors is half the power consumption of the old transistors then the result will be a substrate that produces the same amount of heat. <a href="https://en.wikipedia.org/wiki/Dennard_scaling" target="_tab" title="https://en.wikipedia.org/wiki/Dennard_scaling" rel="noopener">Dennard scaling</a> explains why this used to be the case. Otherwise, each doubling of transistors would produce more heat in the same area. That heat would quickly become unmanageable. The 18 month performance version of Moore's law depends on Dennard scaling to be practical. But Dennard scaling no longer works and as a result Moore's law 18 is dead.
</p>

<p>
This graph shows an example based on Intel processors:
</p>

<p>
<a href="https://articles.59.ca/lib/exe/detail.php?id=em%3A20482030&amp;media=em:dennard.png" target="_tab" title="em:dennard.png" rel="noopener"><img src="https://articles.59.ca/lib/exe/fetch.php?w=640&amp;tok=b6207e&amp;media=em:dennard.png" loading="lazy" alt="" width="640"></a><br>

<em><sub>From: <a href="https://www.extremetech.com/computing/116561-the-death-of-cpu-scaling-from-one-core-to-many-and-why-were-still-stuck" target="_tab" title="https://www.extremetech.com/computing/116561-the-death-of-cpu-scaling-from-one-core-to-many-and-why-were-still-stuck" rel="ugc nofollow noopener">The death of CPU scaling: From one core to many -- and why we're still stuck</a><br>

… which was from: <a href="http://www.gotw.ca/publications/concurrency-ddj.htm" target="_tab" title="http://www.gotw.ca/publications/concurrency-ddj.htm" rel="ugc nofollow noopener">The Free Lunch Is Over - A Fundamental Turn Toward Concurrency in Software</a></sub></em>
</p>

<p>
Note the logarithmic scale. The rising, roughly straight line for the number of transistors indicates that some sort of exponential increase is occurring. The qualities we are interested in, clock speed and performance per clock, have levelled off. The power consumption has levelled off because of physical limitations associated with removing heat from a substrate. Because of the end of Dennard scaling, performance is limited by the power efficiency of the transistors and the amount of heat that can be removed from the substrate. We can't make the transistors smaller in a way that would make them faster because making transistors smaller greatly decreases their power efficiency. That is mostly because of <a href="https://en.wikipedia.org/wiki/Quantum_tunnelling" target="_tab" title="https://en.wikipedia.org/wiki/Quantum_tunnelling" rel="noopener">quantum tunnelling</a> which turned out to be a fundamental physical limit on the performance of the highest performance computing technology we have.
</p>

<p>
There it is. A physical limit that prevents an exponential increase. In retrospect, it can be seen that this limit was already preventing an exponential increase in 2004 when the prediction was made. Unfortunate timing for the one making the prediction but it makes our task here easier. There was no exponential growth of computing performance from 2004 to 2023. It is very unlikely that such growth will reappear in the 7 years before 2030.
</p>

</div>

<h2 id="where_are_we_now">Where Are We Now?</h2>
<div>

<p>
How do things look for breaking 2048 bit RSA right now in 2023?
</p>

<p>
The best available algorithm known, usable with the most powerful computers we know how to build, is NFS. So we would use the NFS algorithm.
</p>

<p>
How much computing power could be brought to bear? As a exorbitant example we could use the Bitcoin mining network. The Bitcoin mining network is a distributed network devoted to breaking a cryptographic function with a brute force search. So it would seem to be a fairly good example to use with respect to breaking RSA which is the same sort of thing.
</p>

<p>
Bitcoin mining is a process that makes money for the entity running the mining system. This financial incentive has created a situation where the mining network has expanded to what might seem a ridiculous extent. The incentive is very sensitive to the cost of electricity. As a result the mining systems are designed to be as power efficient as humanly possible. The end of Dennard scaling is very relevant here. The troublesome heat starts as expensive electricity. Even with this desperate quest for energy efficiency, it is estimated that the Bitcoin mining network consumed 1/200th (0.5%) of all the electricity generated on the entire planet<sup><a href="#fn__11" id="fnt__11">11)</a></sup>in 2021. This makes the network a good upper limit on what might be done in secret. If some over funded national signals intelligence agency built that much processing power we would be able to tell just by checking their power bill. Electricity consumption at the level of an entire country would be impossible to hide.
</p>

<p>
Let's imagine that we could magically repurpose the processing power of the entire Bitcoin mining network for breaking a single 2048 bit RSA key. This will require us to relate what the network is currently doing to the NFS algorithm. I will use the “apples to apples” relation developed in RFC3766<sup><a href="#fn__12" id="fnt__12">12)</a></sup>. It's based on the situation in 2004 but there does not seem to be a better one available. The operations that the Bitcoin network performs would seem to take roughly the same amount of processing as the operations used as a reference in RFC3766<sup><a href="#fn__13" id="fnt__13">13)</a></sup>. By RFC3766, breaking 2048 bit RSA would require 9.01×10<sup>30</sup> cryptographic operations. The Bitcoin mining network recently achieved a rate of 1.24×10<sup>28</sup> operations/year<sup><a href="#fn__14" id="fnt__14">14)</a></sup>.
</p>

<p>
So using the power of the largest amount of computing ever dedicated to breaking cryptographic operations in history, it would take  9.01×10<sup>30</sup>/1.24×10<sup>28</sup> years to break one RSA key. That works out to 727 years. If we could magically create enough physical hardware to break a RSA key in a year then we would need to come up with 727/200 or 3.6 times the amount of electricity currently generated on the planet to run that hardware.
</p>

<p>
This only compares the amount of computing power required to break 2048 bit RSA vs the amount of computing power embodied by the Bitcoin mining network. The operations that the Bitcoin network do require very little memory/RAM. The NFS algorithm on the other hand requires a tremendous amount of memory. In 2004, R. D. Silverman pushed back against the idea that 1024 bit RSA was at imminent risk of compromise. As part of that argument he pointed out that the NFS algorithm has a phase (matrix reduction) that requires a large amount of computing power tightly coupled to a large amount of memory. On the basis of that observation he predicted that 1024 bit RSA would not be publicly broken before the 2030s. So far that prediction seems on track. In passing he mentioned that 2048 bit RSA would require 10<sup>18</sup> bytes of memory for the irreducible matrix reduction step<sup><a href="#fn__15" id="fnt__15">15)</a></sup>. That's a million terabytes, an unimaginable amount of memory to be found in one place, much less tightly coupled to enough processing power to provide any practical benefit. Speaking of memory, the speed of DRAM has lagged to be something like a hundred times slower than processing and sieving is likely very cache unfriendly. So the idea that the processing power of the Bitcoin network could break a single 2048 bit RSA key in a mere 727 years is hopelessly optimistic.
</p>

<p>
It is clear that 2048 bit RSA is vastly out of the scope of current computing technology running the best available algorithm (NFS)…
</p>

</div>

<h2 id="the_future">The future</h2>
<div>

<p>
It appears that a fundamental algorithmic breakthrough would be a prerequisite to threaten the security of RSA 2048. How likely is such a breakthrough?
</p>

<p>
Factoring, prime numbers and the relationship between the two subjects have fascinated humanity for a very long time. The fundamental idea of sieving as a fast method of finding a lot of primes has been around for thousands of years<sup><a href="#fn__16" id="fnt__16">16)</a></sup> (Number Field <em>Sieve</em>, Quadratic <em>Sieve</em>).
</p>

<p>
When creating advanced factoring algorithms there is a vast amount of historical knowledge available. This is very well trodden ground. I think this reduces the chances of surprising insights and reduces the advantage to those working in secret.
</p>

</div>

<h3 id="quantum_computing">Quantum computing</h3>
<div>

<p>
Quantum computing of the sort intended to break RSA involves a breakthrough in both computing and algorithms. Normally some sort of new computing technology is invented and then algorithms are designed to enable that technology to do something useful. The quantum computing threat to RSA is different. We now have the algorithm (Shor's) but the computer to run it on only exists in our imagination.
</p>

<p>
If someone were to invent such a computer then RSA 2048 would be immediately and trivially breakable. RSA 3072 would also be trivially breakable. The same applies to RSA 4096 and 8192. The threat here is admittedly hypothetical, but this still serves as an example of a situation where routine key length extension is of no real value. By repeatedly increasing the size of keys we are betting that a breakthrough will be exactly strong enough to break the superseded key length and not break the current key length. That seems unlikely.
</p>

<p>
New threats tend to take a different form than old threats…
</p>

</div>

<h2 id="conclusion">Conclusion</h2>
<div>

<p>
The assumptions that the 2030 date for increasing RSA key length were based on turned out to be invalid. A check of current capability confirms this. There seems to be no rational reason to increase RSA key sizes past 2048 starting in the year 2030. We don't have any reason to increase RSA key sizes at <em>any</em> time based on our current understanding.
</p>
<blockquote><p>
Although this type of estimates is the best that can be done at this point, it should be understood that actual factoring capabilities may follow an entirely different pattern. Any prediction more than a few decades away about security levels is wishful thinking. The figures in the tables should be properly interpreted, namely as today’s best estimates that may have to be revised tomorrow. Anyone using factoring based asymmetric cryptosystems should constantly monitor and stay ahead of the developments in the research community.</p></blockquote>

<p>
The proceeding quoted text is found with the previously shown “Common RSA modulus bit-length life spans” table in the original paper. If you don't believe me you should still believe Dr. A. K. Lenstra, the one that came up with the double exponential prediction in the first place. We should enact a continuous process of watchful waiting. Any policy changes should be done as a response to changes in the algorithmic and computing performance landscape.
</p>

<p>
This is important because an aspect like a key length is often deeply embedded in the systems protected by the associated cryptography. Rooting out and changing such aspects is normally very expensive in the sorts of systems where cryptographic protection is provided. Where this is not possible at the software level then wholesale equipment replacement is required. The time and resources expended on pointless key length increases can be more profitably used elsewhere. Longer RSA keys require more processing power to process so that pointless keylength increases waste computing resources and power as well.
</p>

</div>

<h2 id="other_systems">Other systems</h2>
<p>
Up to this point this article was made exclusively about RSA to improve readability. Let's use the ideas developed here to briefly examine some other popular cryptographic systems. I will use the NIST recommendations as the practical example<sup><a href="#fn__17" id="fnt__17">17)</a></sup><sup><a href="#fn__18" id="fnt__18">18)</a></sup>. The period for all of these NIST recommendations is from 2019 to 2030 (11 years). The current keysize recommendation became effective as of 2019 and the next one becomes effective as of 2030.
</p>

<h3 id="discrete_logarithm">Discrete logarithm</h3>
<div>

<p>
Current group size: 2048 bits. Next group size: 3072 bits.
</p>

<p>
This is most often seen as the basis of the <a href="https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange" target="_tab" title="https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange" rel="noopener">Diffie–Hellman key exchange</a> system.
</p>

<p>
The best known algorithm to attack discrete logarithm systems is also a version of NFS. The difficulty vs RSA is slightly harder where the RSA key size is the same as the discrete logarithm group size. So the proceeding discussion about the pointlessness of increasing RSA key sizes directly applies to discrete logarithm systems.
</p>

</div>

<h3 id="elliptic_curve_based">Elliptic curve based</h3>
<div>

<p>
Current key size: 224 bits. Next key size: 256 bits.
</p>

<p>
The rule of thumb for elliptic curves is that 2 extra key bits doubles the difficulty. That's (256-224)/2=16 difficulty doublings over the 11 year period. So an implicit assumption that the capability available for breaking elliptic curves will double every 11*12/16=8.25 months. That's a bit faster than the 9 month double exponential assumption that in turn comes from the assumption that available processing power and algorithmic capability are each doubling every 18 months. We know that that is not true for processing power. I have not been able to find any indication of a current or upcoming breakthrough in software methods for breaking elliptic curves, but I am not really qualified to judge that. So I will have to leave this here. In the absence of any evidence of a algorithmic breakthrough then the increase from 224 bits to 256 bits would be unnecessary.
</p>

<p>
Elliptic curve based systems can use much shorter key lengths than other systems. Pointlessly increasing elliptic curve key length would be tragic. If the current assumptions are maintained that would mean we would see a recommendation for more than 256 bit elliptic curve keys for the 2040 decade.
</p>

</div>

<h3 id="symmetric_encryption">Symmetric encryption</h3>
<div>

<p>
Current key size: 112 bits. Next key size: 128 bits.
</p>

<p>
Some examples of symmetric encryption schemes are: <a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard" target="_tab" title="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard" rel="noopener">AES</a>, <a href="https://en.wikipedia.org/wiki/Salsa20#ChaCha_variant" target="_tab" title="https://en.wikipedia.org/wiki/Salsa20#ChaCha_variant" rel="noopener">ChaCha20</a> and <a href="https://en.wikipedia.org/wiki/Camellia_(cipher)" target="_tab" title="https://en.wikipedia.org/wiki/Camellia_(cipher)" rel="noopener">Camellia</a>.
</p>

<p>
One extra key bit doubles the difficulty here. That's 128-112=16 difficulty doublings over the 11 year period. So an implicit assumption that the capability available for breaking elliptic curves will double every 11*12/16=8.25 months. That's a bit faster than the 9 month double exponential assumption that in turn comes from the assumption that available processing power and algorithmic capability are each doubling every 18 months. We know that that is not true for processing power.
</p>

<p>
The idea that the algorithmic capability against symmetric encryption might be doubling every 18 months is fairly surprising. A regular increase here is not something that is normally assumed. Perhaps there was some sort of “debt” with respect to key length that we are making up for in this time period. It might be good to apply the Bitcoin thought experiment as previously seen in this article as a sort of sanity check.
</p>

<p>
The number of cryptographic operations required to use brute force to break a 112 bit key in a symmetric system is 2<sup>112</sup>=5.19×10<sup>33</sup> operations. We will make the reasonable assumption that one Bitcoin operation would take roughly the same time as one symmetric encryption operation. Then it would take 5.19×10<sup>33</sup>/1.24×10<sup>28</sup>=418,548 years. Reducing this to a year would require 418,548/200=2092 times the current total planetary electricity production<sup><a href="#fn__19" id="fnt__19">19)</a></sup>.
</p>

<p>
It does not seem reasonable to increase minimum symmetric encryption key size past 112 bits after 2030.
</p>

<p>
<a href="https://articles.59.ca/doku.php?id=em:index" title="em:index" data-wiki-id="em:index">Encrypted Messaging index</a><br>

<a href="https://articles.59.ca/doku.php?id=start" title="start" data-wiki-id="start">Home</a>
</p>

</div>


                    <!-- wikipage stop -->
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: What tech newsletters are you currently subscribing to? (115 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36671105</link>
            <guid>36671105</guid>
            <pubDate>Mon, 10 Jul 2023 18:45:30 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36671105">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="36671719"><td></td></tr>
            <tr id="36671454"><td></td></tr>
            <tr id="36671714"><td></td></tr>
            <tr id="36676841"><td></td></tr>
            <tr id="36672214"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36672214" href="https://news.ycombinator.com/vote?id=36672214&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><p><span>off topic, how do you find time to follow and read all those newsletters?<p>I tried to follow couple of them, after a while it felt like everyday I am reading some newsletter. At this moment I completely stopped, because messed up my time management skills after having kids. Need to recover
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36672303"><td></td></tr>
            <tr id="36673872"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36673872" href="https://news.ycombinator.com/vote?id=36673872&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><p><span>They all get filtered straight in to a folder. No notification when they arrive.<p>Then I just go through it every couple weeks either when im waiting around on my phone or when I find myself wasting time on reddit or hn but dont quite have it in my to jump back in to dev work just yet.</p><p>Between topics not being interesting to me, overlap in covered topics by each newsletter, and overlap with things I've already seen here; the vast majority of content that hits that folder doesn't ever actually get read. I'm yet to find any source of news, tech newsletter or otherwise, that warrants reading on a daily basis. Closest would be the weather.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36672451"><td></td></tr>
                <tr id="36672594"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36672594" href="https://news.ycombinator.com/vote?id=36672594&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><br><div>
                  <p><span>Honest question; do people actually go back and read the stuff the intended to read later? I have never been successful with that strategy, and have to bookmark folders to prove it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36673042"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36673042" href="https://news.ycombinator.com/vote?id=36673042&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><br><div>
                  <p><span>It’s mostly aspirational for me but I still do it and am happy 10% of the time I actually circle back and read the thing.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36672817"><td></td></tr>
                  <tr id="36672629"><td></td></tr>
                        <tr id="36671555"><td></td></tr>
            <tr id="36672049"><td></td></tr>
            <tr id="36674020"><td></td></tr>
            <tr id="36671899"><td></td></tr>
            <tr id="36672334"><td></td></tr>
                <tr id="36672404"><td></td></tr>
                  <tr id="36671726"><td></td></tr>
                <tr id="36671850"><td></td></tr>
                <tr id="36671985"><td></td></tr>
                        <tr id="36672163"><td></td></tr>
            <tr id="36675534"><td></td></tr>
            <tr id="36671456"><td></td></tr>
                <tr id="36671529"><td></td></tr>
                  <tr id="36676121"><td></td></tr>
            <tr id="36671920"><td></td></tr>
                <tr id="36677004"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36677004" href="https://news.ycombinator.com/vote?id=36677004&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><br><div>
                  <p><span>Yep Ben Evans’ newsletter is great. If one only would want to subscribe to one single weekly newsletter about the tech industry, I’d recommend that one.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36673273"><td></td></tr>
            <tr id="36673182"><td></td></tr>
            <tr id="36676356"><td></td></tr>
            <tr id="36671483"><td></td></tr>
            <tr id="36671782"><td></td></tr>
            <tr id="36672410"><td></td></tr>
            <tr id="36671943"><td></td></tr>
            <tr id="36674358"><td></td></tr>
            <tr id="36672504"><td></td></tr>
            <tr id="36672267"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36672267" href="https://news.ycombinator.com/vote?id=36672267&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><p><span>Chips and cheese (in depth hardware analysis)<p>Asianometry (just go subscribe, it's brilliant)</p><p>and as others mentioned, the graphics programming newsletter
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36672380"><td></td></tr>
            <tr id="36672413"><td></td></tr>
                <tr id="36672430"><td></td></tr>
                  <tr id="36671476"><td></td></tr>
            <tr id="36672311"><td></td></tr>
            <tr id="36671574"><td></td></tr>
            <tr id="36672193"><td></td></tr>
            <tr id="36671926"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36671926" href="https://news.ycombinator.com/vote?id=36671926&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><p><span>The questions is: what are you looking for?<p>- General tech news?</p><p>- Career tips/tricks for devs?</p><p>- Deep dives on functional topics?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36672000"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36672000" href="https://news.ycombinator.com/vote?id=36672000&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><br><div>
                  <p><span>Since everything I subscribe to has already been named, I'll ask a question: anyone got any blogs/newsletters/<i>etc.</i> geared toward LISPy topics that they can recommend?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36671940"><td></td></tr>
            <tr id="36671621"><td></td></tr>
            <tr id="36673118"><td></td></tr>
            <tr id="36671730"><td></td></tr>
            <tr id="36672087"><td></td></tr>
            <tr id="36672770"><td></td></tr>
            <tr id="36671800"><td></td></tr>
            <tr id="36671533"><td></td></tr>
            <tr id="36672456"><td></td></tr>
            <tr id="36672927"><td></td></tr>
            <tr id="36675301"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FreeShip Plus in Lazarus – an open-source software for boat and hull design (122 pts)]]></title>
            <link>https://github.com/markmal/freeship-plus-in-lazarus</link>
            <guid>36670328</guid>
            <pubDate>Mon, 10 Jul 2023 17:58:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/markmal/freeship-plus-in-lazarus">https://github.com/markmal/freeship-plus-in-lazarus</a>, See on <a href="https://news.ycombinator.com/item?id=36670328">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content"><pre># freeship-plus-in-lazarus

To download binary packages ( .zip and .deb files) please proceed to Releases page 
<a href="https://github.com/markmal/freeship-plus-in-lazarus/releases">https://github.com/markmal/freeship-plus-in-lazarus/releases</a>

FREE!ship Plus in Lazarus is further development of the FREE!ship Plus (by <a href="http://www.hydronship.net/" rel="nofollow">http://www.hydronship.net</a> ) 
Windows program with free source code FREE!ship v3.x under license GNU GPL. 
This FREE!ship Plus application is migrated into free open source Lazarus / Free Pascal environment 
to promote further development in various platforms and for various platforms (OS and architectures).

FREE!ship Plus is designed for the full parametric analysis of resistance and power prediction for 
a ship and other calculations of hydrodynamics of vessels and underwater vehicles. FREE!ship Plus 
allows the designer to simulate and analyze condition of balance of a complex completely hull - 
rudders - keels - engine - propellers in different regimes and of service conditions of a ship. 
The analyzable system includes a hull, appendages, a propeller and the engine (i.e. resistance, 
power, a thrust and a torque), and also various service conditions (heaving, a wind, a shallow-water 
effect, a regime of tow / pushing, etc...) refer to <a href="http://www.hydronship.net/" rel="nofollow">http://www.hydronship.net</a> for full description.
</pre></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tuition costs have risen 710% since 1983 (1200% since 1980). Inflation: 194% (681 pts)]]></title>
            <link>https://statecraft.beehiiv.com/p/student-loan-debt-forgiveness</link>
            <guid>36669253</guid>
            <pubDate>Mon, 10 Jul 2023 17:03:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://statecraft.beehiiv.com/p/student-loan-debt-forgiveness">https://statecraft.beehiiv.com/p/student-loan-debt-forgiveness</a>, See on <a href="https://news.ycombinator.com/item?id=36669253">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-blocks"><p><i><b>Quick updates before we begin: </b></i></p><p><i><b>New Perks for Subscribers! </b></i><i>Starting today, we’ve added the following perks: </i></p><div><ul><li><p><i>Enabled comment section (leave feedback or suggest a topic!)</i></p><ul><li><p><i>We’ve already received suggestions for AI and climate topics, and those are in the works!</i></p></li></ul></li><li><p><i>Sub-only posts</i></p></li><li><p><i>Access to our full archive</i></p></li><li><p><i>Early access to interactive web and video projects</i></p></li></ul></div><p><i>Lastly, Statecraft is now available on </i><a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL25ld3MuZ29vZ2xlLmNvbS9wdWJsaWNhdGlvbnMvQ0FBcUJ3Z0tNTGFjMFFzdzliZm9Bdz9jZWlkPUNBOmVuJm9jPTMiLCJwb3N0X2lkIjoiODU0YzFlMDgtOTQyMi00ZjZiLWE0ODItNTI3M2Y0YjM5ZDM5IiwicHVibGljYXRpb25faWQiOiJkMWI4MGM5My1lODI4LTRhOTQtOTlkNC1lYzgyYTE0MzAyOGEiLCJ2aXNpdF90b2tlbiI6IjQ3Njc2MDU4LTEwMzctNDY4MC05YjcwLTBjYjIxNjQyN2RiZSIsImlhdCI6MTY4OTAxMjAwMy4zODEsImlzcyI6Im9yY2hpZCJ9.YMZWkJXYt0PLse4X9xGH-asL9J3XH3LbYflfs0C5J1g" target="_blank" rel="noopener noreferrer nofollow"><i>Google News</i></a><i>! </i>🎉<i>&nbsp;</i></p><p> The outstanding student loan debt in the United States is <b>1.78 trillion dollars</b>. For reference, the GDPs of developed countries like South Korea, Australia, and oil-rich Saudi Arabia are less than $1.7T respectively. The explosion in tuition costs and associated debt accumulation is undeniably a looming economic crisis. It’s a crisis that requires a basket of policy solutions to transform higher education from the bottom up - including, yes, cancellation of some federal student loans. But while the fight to cancel student loan debt <a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy53aGl0ZWhvdXNlLmdvdi9icmllZmluZy1yb29tL3N0YXRlbWVudHMtcmVsZWFzZXMvMjAyMi8wOC8yNC9mYWN0LXNoZWV0LXByZXNpZGVudC1iaWRlbi1hbm5vdW5jZXMtc3R1ZGVudC1sb2FuLXJlbGllZi1mb3ItYm9ycm93ZXJzLXdoby1uZWVkLWl0LW1vc3QvP3V0bV9zb3VyY2U9c3RhdGVjcmFmdC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZS13ZXJlbi10LXJlYWR5LWZvci1hLTcxMC1yaXNlLWluLXR1aXRpb24tY29zdHMiLCJwb3N0X2lkIjoiODU0YzFlMDgtOTQyMi00ZjZiLWE0ODItNTI3M2Y0YjM5ZDM5IiwicHVibGljYXRpb25faWQiOiJkMWI4MGM5My1lODI4LTRhOTQtOTlkNC1lYzgyYTE0MzAyOGEiLCJ2aXNpdF90b2tlbiI6IjQ3Njc2MDU4LTEwMzctNDY4MC05YjcwLTBjYjIxNjQyN2RiZSIsImlhdCI6MTY4OTAxMjAwMy4zODIsImlzcyI6Im9yY2hpZCJ9.TbC4ohDc8GWT4aCRNVhqLlvS81GLNaPsRD2whAgXK1E" target="_blank" rel="noopener noreferrer nofollow">continues</a>, we think it would be especially worthwhile to explore some data and highlight potential solutions for this debt burden. </p><p><h3>📈 How We Got Here</h3></p><p> Tuition cost and fee inflation have outpaced CPI Inflation 4-to-1 over the last 40 years: </p><div><p><img alt="" src="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/33ae477a-f0ea-4d56-a664-710b346bf8cf/inflation_capture.png"></p><p><small><p>Animated versions of these charts on Threads (or Twitter): @theArmanMadani</p></small></p></div><p> The resulting $1.78T of debt includes $1.65T that is federally owned (93%). 45M Americans have federal student loan debt. Roughly half of all the debt belongs to individuals with graduate degrees while the other half belongs to individuals with undergraduate degrees; though it’s worth noting that there are fewer graduate students overall so the debt held per student is higher for graduates: </p><div><p><img alt="" src="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/98cda4db-b39d-47e8-98f9-5dc3f95d832e/TotalDebtCapture.jpg"></p><p><small><p>Animated versions of these charts on Threads (or Twitter): @theArmanMadani</p></small></p></div><p><b>Student enrollment in higher ed institutions has </b><a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5zdGF0aXN0YS5jb20vc3RhdGlzdGljcy8xODM5OTUvdXMtY29sbGVnZS1lbnJvbGxtZW50LWFuZC1wcm9qZWN0aW9ucy1pbi1wdWJsaWMtYW5kLXByaXZhdGUtaW5zdGl0dXRpb25zLz91dG1fc291cmNlPXN0YXRlY3JhZnQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Utd2VyZW4tdC1yZWFkeS1mb3ItYS03MTAtcmlzZS1pbi10dWl0aW9uLWNvc3RzIiwicG9zdF9pZCI6Ijg1NGMxZTA4LTk0MjItNGY2Yi1hNDgyLTUyNzNmNGIzOWQzOSIsInB1YmxpY2F0aW9uX2lkIjoiZDFiODBjOTMtZTgyOC00YTk0LTk5ZDQtZWM4MmExNDMwMjhhIiwidmlzaXRfdG9rZW4iOiI0NzY3NjA1OC0xMDM3LTQ2ODAtOWI3MC0wY2IyMTY0MjdkYmUiLCJpYXQiOjE2ODkwMTIwMDMuMzgyLCJpc3MiOiJvcmNoaWQifQ.syv1ZnkHSR_7B0ialU7w22IbaGYPuInRtz-3KPUDXic" target="_blank" rel="noopener noreferrer nofollow"><b>risen consistently since 2000</b></a>. With higher demand for college degrees - including out-of-state demand for state universities - tuition has risen precipitously. Rising enrollment is also associated with a changing <a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5zdGF0aXN0YS5jb20vdG9waWNzLzc3MS9lbXBsb3ltZW50Lz91dG1fc291cmNlPXN0YXRlY3JhZnQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Utd2VyZW4tdC1yZWFkeS1mb3ItYS03MTAtcmlzZS1pbi10dWl0aW9uLWNvc3RzI3RvcGljT3ZlcnZpZXciLCJwb3N0X2lkIjoiODU0YzFlMDgtOTQyMi00ZjZiLWE0ODItNTI3M2Y0YjM5ZDM5IiwicHVibGljYXRpb25faWQiOiJkMWI4MGM5My1lODI4LTRhOTQtOTlkNC1lYzgyYTE0MzAyOGEiLCJ2aXNpdF90b2tlbiI6IjQ3Njc2MDU4LTEwMzctNDY4MC05YjcwLTBjYjIxNjQyN2RiZSIsImlhdCI6MTY4OTAxMjAwMy4zODIsImlzcyI6Im9yY2hpZCJ9.0u-HWi65MIPQbv9rF94ngToxvo3-H3xf_iibpMZwDIU" target="_blank" rel="noopener noreferrer nofollow">US labor profile</a>; for example, manufacturing jobs were eclipsed by “business and professional services” jobs as well as healthcare, education, and retail jobs. One troubling manifestation of the demand for higher degrees can be seen during the 2008-09 Recession as newly unemployed individuals scrambled for retraining. From 2008 through 2009, enrollment in private for-profit colleges rose disproportionately compared to private non-profit and public colleges. Private for-profit colleges have <a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2VkdWNhdGlvbmRhdGEub3JnL3N0dWRlbnQtbG9hbi1kZWZhdWx0LXJhdGU_dXRtX3NvdXJjZT1zdGF0ZWNyYWZ0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlLXdlcmVuLXQtcmVhZHktZm9yLWEtNzEwLXJpc2UtaW4tdHVpdGlvbi1jb3N0cyIsInBvc3RfaWQiOiI4NTRjMWUwOC05NDIyLTRmNmItYTQ4Mi01MjczZjRiMzlkMzkiLCJwdWJsaWNhdGlvbl9pZCI6ImQxYjgwYzkzLWU4MjgtNGE5NC05OWQ0LWVjODJhMTQzMDI4YSIsInZpc2l0X3Rva2VuIjoiNDc2NzYwNTgtMTAzNy00NjgwLTliNzAtMGNiMjE2NDI3ZGJlIiwiaWF0IjoxNjg5MDEyMDAzLjM4MiwiaXNzIjoib3JjaGlkIn0.-8EXNxtP3mX6u68fZeH1Y6Dbz7SOxEOYlb5VcgJofFE" target="_blank" rel="noopener noreferrer nofollow">extraordinarily high rates of default</a>, near 16% of borrowers default within 3 years. </p><p><img alt="" src="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/ccaaffe9-1806-4cae-8fad-4bbb1da00da6/image.png"></p><p> During this period of heightened demand, a more obvious inflationary effect took place: <b><a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5uZWEub3JnL2Fkdm9jYXRpbmctZm9yLWNoYW5nZS9uZXctZnJvbS1uZWEvc3RhdGUtZnVuZGluZy1oaWdoZXItZWR1Y2F0aW9uLXN0aWxsLWxhZ2dpbmc_dXRtX3NvdXJjZT1zdGF0ZWNyYWZ0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlLXdlcmVuLXQtcmVhZHktZm9yLWEtNzEwLXJpc2UtaW4tdHVpdGlvbi1jb3N0cyM6fjp0ZXh0PVdoZW4lMjBzdGF0ZSUyMGxhd21ha2VycyUyMHR1cm4lMjB0aGVpcixkb3duJTJDJTIwc3R1ZGVudCUyMHR1aXRpb24lMjBnb2VzJTIwdXAuIiwicG9zdF9pZCI6Ijg1NGMxZTA4LTk0MjItNGY2Yi1hNDgyLTUyNzNmNGIzOWQzOSIsInB1YmxpY2F0aW9uX2lkIjoiZDFiODBjOTMtZTgyOC00YTk0LTk5ZDQtZWM4MmExNDMwMjhhIiwidmlzaXRfdG9rZW4iOiI0NzY3NjA1OC0xMDM3LTQ2ODAtOWI3MC0wY2IyMTY0MjdkYmUiLCJpYXQiOjE2ODkwMTIwMDMuMzgyLCJpc3MiOiJvcmNoaWQifQ.VIfYccx1zSUkzhZU--9xiKvePMPUk1LdVSt0esoPxrI" target="_blank" rel="noopener noreferrer nofollow">state funds were cut</a></b><b> for public colleges on a per-student basis </b>in the run-up and fallout of the Recession. </p><p><h3>💸 What Now</h3></p><p> [WARNING: I include my opinion in this section, I look forward to a lively discussion in the comment section] </p><p> There are short and long-term solutions needed to address the debt problem. </p><p> Short-term, students need recourse. Defaulting on loans creates a cascade of consequences that necessarily harm borrowers earning potential for years (e.g. colleges can withholding proof of attendance, decline in credit score, wage garnishments, etc.) There are loans that are highly likely to default regardless after the COVID moratorium on payments ends. These loans should be forgiven/cancelled. The fallout of default en masse - especially amongst low and middle-income degree holders - would present a much greater cost to the economy than the cost of cancellation. Additionally, controlling what information can be reported to credit bureaus (like medical debt) and regulating what information colleges can withhold would also reduce negative income effects caused by default (e.g. a private for-profit institution cannot withhold proof of attendance in the event of default). </p><p> Long-term, controlling the underlying cost of higher ed itself is an imperative. Federal support for and expansion of programs like <a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5jY2Njby5lZHUvQWJvdXQtVXMvQ2hhbmNlbGxvcnMtT2ZmaWNlL0RpdmlzaW9ucy9FZHVjYXRpb25hbC1TZXJ2aWNlcy1hbmQtU3VwcG9ydC9TdHVkZW50LVNlcnZpY2UvV2hhdC13ZS1kby9DYWxpZm9ybmlhLVByb21pc2U_dXRtX3NvdXJjZT1zdGF0ZWNyYWZ0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlLXdlcmVuLXQtcmVhZHktZm9yLWEtNzEwLXJpc2UtaW4tdHVpdGlvbi1jb3N0cyIsInBvc3RfaWQiOiI4NTRjMWUwOC05NDIyLTRmNmItYTQ4Mi01MjczZjRiMzlkMzkiLCJwdWJsaWNhdGlvbl9pZCI6ImQxYjgwYzkzLWU4MjgtNGE5NC05OWQ0LWVjODJhMTQzMDI4YSIsInZpc2l0X3Rva2VuIjoiNDc2NzYwNTgtMTAzNy00NjgwLTliNzAtMGNiMjE2NDI3ZGJlIiwiaWF0IjoxNjg5MDEyMDAzLjM4MiwiaXNzIjoib3JjaGlkIn0.46i8nRok0CXsqfgPv_peIiPNn1SIoVbsi1ofW1vypas" target="_blank" rel="noopener noreferrer nofollow">The California Promise</a> - which provides free tuition for California residents who attend CA community colleges - would eliminate the burden for some new graduates. Apportioning more existing or raising new tax revenue to invest in underfunded private colleges (e.g. HBCUs) and public colleges can reduce tuition costs by supplanting/exceeding the aforementioned state funding cuts as well. </p><p> This is a complex issue which requires comprehensive policy solutions. But one of the reasons student debt has received the attention it has is because of the opportunity it presents. An opportunity to extend a stimulus to low/middle income earners in the short-term and transform the higher education system in the long-term. </p><p><i>Thank you for reading this article! If you enjoyed it, consider sharing Statecraft with a friend. You can also connect with us on Threads, Twitter or Instagram. We’re working on our first long-form video content as well, so if you want to see what we produce, head over to YouTube or TikTok. Links below:</i></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Invisible Details of Interaction Design (142 pts)]]></title>
            <link>https://rauno.me/craft/interaction-design</link>
            <guid>36669249</guid>
            <pubDate>Mon, 10 Jul 2023 17:02:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rauno.me/craft/interaction-design">https://rauno.me/craft/interaction-design</a>, See on <a href="https://news.ycombinator.com/item?id=36669249">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-stage=""><p>Design can feel like there's no science to it — only feel and intuition. Even researchers have trouble grounding interaction design practices in science, inherently treating them as a mysterious black box. <sup>1</sup> While from my own experience that's partly true, I have been trying to deconstruct and dig out the <i>why</i> behind great displays of interaction design.</p><p>Searching the Internet for depth on interaction design yields a plethora of recycled content obsessing over user personas, storyboards, and Venn diagrams labeled with "UI" and "UX". Besides a few exceptional talks, actual substance and insight reveal themselves to those willing to fanatically dig for them. Either through studying obscure, long-winded research papers or by maniacally replaying hundreds of slow motion screen recordings.</p><p>Sitting down and just thinking hard does not magically produce valuable discoveries either. The essence of the word "interaction" implies a relationship between a human and an environment. In my experience, great revelations surface from making something — filling your headspace with a problem — and then going for a synthesising daydreaming walk to stir the pot.</p><p>This writing is not a tutorial nor a collection of guidelines. But rather an observation on the invisible details of a few interactions that I use often but rarely think about. Besides recreating interfaces, I found this exercise in reflection to be another great way to build a stronger design intuition and vocabulary.</p><h3 data-heading="true" id="metaphors">Metaphors</h3><p>What even is interaction design? Here's how I think about it through the lens of technology. Interaction design is an artform to make experiences that fluidly respond to human intent. When does a swipe trigger an action? Do gestures retain momentum? What happens if a finger is covering content? How can we predict intent based on context? Executing well on details like these make products feel like a natural extension of ourselves.<!-- --> <sup>2</sup></p><p>But it's not an artform in the same way as painting or composing music. There's a unique human component to interaction design. Why? Because ultimately people are trying to get stuff done using a product. Beauty in form and composition is not enough. There's an inherent satisfaction apparent in striking a holistic balance between form <i>and </i>function.</p><p>Great interaction design rewards learning by reusing metaphors. You can use most touch interfaces with just two gestures: tapping and swiping. For example, on iOS the only gesture you're explicitly taught how to do is swiping up to open:</p><p>Now you've learned swiping which unlocks control over many other parts of the interface. The sliding motion also tells you that the interface is composed of stacked layers, like a deck of cards. Knowing so, you might be enthused to try swiping down on the screen to discover more controls. Conceptually, the interface is further implicitly teaching you that swiping <i>down</i> reveals layers of system functionality. This knowledge compounds as you delve deeper into the Apple ecosystem.</p><p>We can stretch interaction design metaphors even further. Why does swiping horizontally navigate between pages? Because that's how we've intuitively interfaced with books for thousands of years.<!-- --> </p><p>Great interactions are modeled after properties from the real world, like interruptability. This sounds kinda silly because, duh, obviously flipping a page in a book is interruptible. But imagine if it were an animation that you had to wait for!</p><p>Pinching is another gesture that we've come to intuitively associate with zooming. Simply put, zooming is an act of precision — adjusting the amount of detail visible.</p><p>We can think of pinching akin to movements that require intricate motor skills, like picking up tiny objects or working with spices. Naturally, we pinch our fingers for higher precision:</p><p>Notice how pinching on a touch screen works in reverse. You start with your fingers together, and then move them apart for precision. But to grab something, you bring fingers together from afar. This is because the interface needs to first establish an anchor point from where the zooming originates.</p><p><img alt="Two app icons are in focus, both of them have the bottom 10 points duplicated and stretched for an elongated effect." loading="lazy" width="1920" height="1400" decoding="async" data-nimg="1" srcset="https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fpinch-anchor.09bc6fa5.png&amp;w=1920&amp;q=75 1x, https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fpinch-anchor.09bc6fa5.png&amp;w=3840&amp;q=75 2x" src="https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fpinch-anchor.09bc6fa5.png&amp;w=3840&amp;q=75"></p><p>Scientifically or intuitively, there are hundreds of design decisions made by someone obsessesing over the tiniest margins so that when they work, no one has to think about. And many of them tap into our instinctive behaviors.</p><h3 data-heading="true" id="kinetic-physics">Kinetic Physics</h3><p>The lock screen sliding up establishes that, in essence, it's just an overlay that can be dismissed by swiping up, and within that framing so is an app. That means you also now know how to dismiss an application.</p><p>Let's take a look at how dismissing an app morphs into the Dynamic Island. Notice how the gesture retains the momentum and angle at which it was thrown. It's never perfectly centered or consistent in timing.</p><p>This movement builds on our natural sense of physics from the real world, like how swiping a playing card would feel. Although the movement of the playing card exhibits less bounce since it's conceptually lighter and does not magnetically morph into something.<!-- --> </p><h3 data-heading="true" id="swipe-gestures">Swipe Gestures</h3><p>When does a swipe trigger an action? It seems trivial: you press down, move a little, and then finally trigger an action <i>after</i> releasing the finger. After building a few touch interactions myself using SwiftUI, I realised that might not always be the case. Sometimes we expect the action to be triggered <i>whilst</i> <!-- -->swiping.</p><p>Lightweight actions, such as displaying overlays, feel more natural to trigger during the swipe after an arbitrary amount of distance. For example, with a single gesture, I can immediately grok the overlaying surface, understand that it gives me a search input, and then dismiss if it's not what I want. Waiting for the gesture to end would feel like a drag here.</p><p>Here's an example from the MercuryOS SwiftUI prototype I was working on. It feels expected to trigger an action when elements moving during the gesture reach their logical, final position. Notice how the screen is unlocked after both the titles snap to their position, and then locked with a single gesture without releasing the finger. Again, waiting for the gesture to end before unlocking would make the interface feel broken and provide less affordance.</p><p>Now, let's look at examples where triggering an action requires explicit intent. The iOS App Switcher will never dismiss an app before the gesture ends. No matter the distance or the fact that the app is partially off-screen:</p><p>This makes sense to me because dismissing an app is destructive, and it wouldn't feel nice if the app were to dismiss in the middle of the swipe. What if I were change my mind half-way through and accidentally reached the threshold for dismissing? I could potentially lose some important progress in an app! To make sure the interface responds to intent, triggering on gesture end, regardless of distance, feels right here.</p><p>Here's another example where despite swiping an adequate amount of distance for the view to be fully visible and snap, it doesn't until the gesture ends. This makes it lightweight to briefly peek at another screen when scanning for an app, without committing to it, and quickly interrupt the gesture by changing direction.</p><h3 data-heading="true" id="responsive-gestures">Responsive Gestures</h3><p>Truly fluid gestures are immediately responsive. As mentioned above, gestures can have an explicit trigger threshold, but this does not mean simply performing an animation 0 → 1 would feel great.</p><p>For example, a naive implementation for pinching a card would exponentially zoom in after a certain threshold:</p><p>Pinching an adequate amount to animate would not feel exactly broken here. But the interface gives zero affordance or confidence that the card is even pinchable with a lower velocity. Neither does this feel satisfying to perform.</p><p>It feels a lot better by feeling the scale delta applying immediately, and then performing an animation past a given threshold:</p><p>For some reason navigating iOS Settings does not feel as responsive as the App Switcher. A layer slides over from the right which tells me that it can be dismissed by swiping left. But if you happen to mistap, then swiping back immediately does not interrupt the animation. You have to wait for it to end.</p><h3 data-heading="true" id="spatial-consistency">Spatial Consistency</h3><p>The Dynamic Island has this nice interaction where on tap the application slides out under from the Island to cover the screen:</p><p>But if the Island is <i>expanded</i> which conceptually tells the interface my intent is to receive <i>more</i> <!-- -->detail, the application does not slide out from the Island. Instead, it launches from the icon, if its visible. Alternatively, the application slides in from the right:</p><p>I can only assume that by launching Spotify from the icon, it is a lot more clear where the audio is playing from. Say you had three music apps on the same row. Through motion this helps establish a relationship between the audio player and its source.</p><p>Similarly, if the app slides in from the right, it communicates where it is spatially — in the App Switcher. By moving in from the right, not left, it also signifies that the app is now first on the stack of apps in the switcher.</p><p>However, the native Clock app will never open from its icon. It always jumps out from the Island, even when expanded:</p><p>This seems to support the theory above. Because the Island timer module is only specific to one app, and there can't be another with the same Island, there's no need to make it clear where it's from.</p><h3 data-heading="true" id="fluid-morphing">Fluid Morphing</h3><p>We're all familiar with the beautifully fluid, interruptible gestures of iOS to quickly navigate apps. Swiping up morphs the full screen app into its icon:</p><p>A curious detail here is that the icon is intentionally stretched from the bottom to fill the frame as it fludily morphs its shape from a vertical rectangle to a uniform square. It's a tiny bit more obvious what happens when looking at the non-standard GitHub icon:</p><p><img alt="Two app icons are in focus, both of them have the bottom 10 points duplicated and stretched for an elongated effect." loading="lazy" width="1920" height="1080" decoding="async" data-nimg="1" srcset="https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fiphone-close-crop.0334862c.png&amp;w=1920&amp;q=75 1x, https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fiphone-close-crop.0334862c.png&amp;w=3840&amp;q=75 2x" src="https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fiphone-close-crop.0334862c.png&amp;w=3840&amp;q=75"></p><p>This technique does assume that app icons adhere to the guidelines outlined by Apple. The Bluesky icon ignores the recommended safe zone and as a result the bottom ~10pt of the icon is cropped, duplicated, and stretched, resulting in this weird repeating image effect:</p><p><img alt="The Bluesky app icon is in focus. It is an image of a blue sky with clouds. The bottom 10 points are duplicated and stretched, but because its an image, the result is an odd stretching image effect." loading="lazy" width="1920" height="1080" decoding="async" data-nimg="1" srcset="https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fiphone-close-bsky.720d6ba8.png&amp;w=1920&amp;q=75 1x, https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fiphone-close-bsky.720d6ba8.png&amp;w=3840&amp;q=75 2x" src="https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fiphone-close-bsky.720d6ba8.png&amp;w=3840&amp;q=75"></p><p>In practice this does not feel completely off, but definitely not as great:</p><h3 data-heading="true" id="frequency-novelty">Frequency &amp; Novelty</h3><p>As a designer, I love to animate everything. Object permanence, creating a focal point, and delight are all good reasons for doing so. But it's not so obvious when <i>not</i> to animate something.</p><p>Sometimes we can get away with not animating mouse or keyboard interactions, without it feeling jarring. There is an inherent disconnect between input from peripheral devices and what happens on the screen. Pressing a key feels less visceral, and more mechanical than touching the screen.</p><p>A good example for this would be command menus. It's tempting to throw an opacity and scale fade on the overlay.<!-- --> <!-- -->But if we for a moment consider the interaction frequency being hundreds of times a day, it does start to feel more like cognitive burden after seeing the same animation for the hundredth time. <sup>3</sup></p><p>When so commonly executed, the interaction novelty is also diminished. It doesn't feel like you're doing anything peculiar, deserving of a special flourish.</p><p>A case in point: I was working on a bookmarking tool (<a href="https://bmrks.com/" target="_blank" rel="noopener noreferrer">bmrks.com</a>) and intuitively felt great about animating the active indicator and elements being added and removed from the list:</p><p>After a couple of days they began to feel sluggish. Despite making the motion even snappier, my perceived performance was making me feel like I have to wait too much when interfacing with the keyboard. I removed motion from core interactions and suddenly felt like I was moving much faster:</p><p>Context (right-click) menus on macOS also appear without motion. Used thousands of times a day, with very low novelty and high frequency. Despite being a mouse interaction, it feels right to not animate the menu appearing:</p><p>Interestingly enough, the menu subtly fades out. On closer inspection, the selected item briefly blinks the accent color (pink) to provide assurance that the element was successfully selected. I can only assume that the menu fading out makes this feel more graceful and intentional than abruptly disappearing after the blink:</p><p>Another good example is the App Switcher on macOS which gets a lot of mileage for heavy keyboard users. The overlay never animates which makes moving between apps feel snappy:<!-- --> </p><p>Furthermore, if the time delta between pressing Command and Tab is low enough, the previously active window receives focus immediately without showing the menu:<!-- --> </p><h3 data-heading="true" id="fidgetability">Fidgetability</h3><p>Wonderful interactions don't have to be entirely practical. We've all been in math class, either biting our lips or repetitively clicking a pencil while crunching numbers. Behaviors like this are considered fidgeting. In other words, repetitive movements that apparently help release situational stress, or even enhance concentration. Although there is no scientific research that supports this claim <sup>4</sup>, fidgeting does regardless feel like a part of intentional interaction design.</p><p>Fidgetability could also be an after-thought, or a happy side-effect. However, the AirPods case is uncannily satisfying to play with. Assuming it to be a coincidence would be very generous.</p><p>Apple Pencil is a more obvious candidate to intentionally design to be fidgetable. The tip of the pencil is unscrewable which means it can be replaced eventually. Oddly enough, twisting the tip and rotating the pencil body provides satisfying friction to casually play with while thinking.</p><p>Now here's a crazy one that I would not bet my money on being intentional. Although it is dope.</p><h3 data-heading="true" id="scroll-landmarks">Scroll Landmarks</h3><p>On macOS you can always find the pointer by shaking the mouse. This interaction feels wonderful because it taps into the frustration and natural reaction that people feel when losing track of the pointer.</p><p>Something similar quite often happens to me on mobile when browsing long-form content. I've scrolled down half-way, and while reading I want to quickly recall something from above. But then I feel awkward scrolling back up because I will lose my precious scroll position and reading progress.</p><p>I made a tiny prototype where double tapping the scrollbar will place a landmark for the current scroll position. Now I could freely navigate around the page and double tap the landmark to get back to where I was before.</p><p>This feels familiar to use because the scrollbar is already interactive on touch. If you didn't know, long-pressing the scrollbar would make it draggable which is much faster to scroll quickly.</p><p>This reminds me of an old minimap prototype I made. Inspired by games where you always have a bird's eye view of the surrounding environment. Why not have a similar heads up display for navigating a page?</p><h3 data-heading="true" id="touch-content-visiblity">Touch Content Visiblity</h3><p>On touch interfaces, sometimes a finger might obfuscate what's happening on the screen which makes it hard to perform gestures at pixel-level precision. Commonly, the interface would then render a temporary representation of what's underneath the finger.</p><p>For example, on iOS when pressing down and dragging to move the text caret, a magnifying loupe will appear above the touch point. However, whenever the finger moves downwards and no longer covers the caret, the loupe disappears.</p><p>A similar detail is used for the keyboard. Pressing a key will show an enlarged key which gives you confidence that the interface understood what you meant.</p><p>It doesn't always make sense to mirror the obfuscated region. For example, sliders can be tiny and disappear under the touch of the thumb. It helps to ensure that the dragging gesture does not cancel when moving away from the slider and still pressing down:</p><p>Although seeking video is mostly a visual interaction, there's an unintelligible level of discomfort apparent when interacting with an element that you can't see.</p><p>Here's a more obvious example where it's critical to understand contents of the menu: </p><h3 data-heading="true" id="implicit-input">Implicit Input</h3><p>Forever long we've been peeling back the layers between humans and computing. Touch input elevated the relationship by introducing gestures and haptics. Soon applications will no longer be bound by the constraints of a fixed screen.</p><p>The keyboard, mouse, touch, voice are all explicit inputs. They feel like a natural extension of ourselves<!-- --> <sup>2</sup> when dialed into perfection. But isn't the mother of all inputs no input at all? When an interface makes use of context as input and can infer what you're trying to do without asking, it truly feels magical.</p><p>For example, by looking at the screen, Apple Maps will show the active route navigation without unlocking. Apple Wallet will increase the brightness when presenting a pass for scanning. Spotify will adjust the interface to be more accessible while driving.</p><p>Some custom iOS apps will blur the contents of the app when opening the App Switcher. At first, I figured it was just a performance optimization. But it turns out that it's a deliberate attempt to conceal possibly sensitive data, like medical records or a bank statement.</p><h3 data-heading="true" id="fittss-law">Fitts's Law</h3><p>Fitts's Law states that the time to click on something depends on distance and size. <sup>5</sup> The bigger the target, and the closer it is to where your pointer is, the better.</p><p>Operating systems make use of "magic corners" on the edges of the screen because the target area is infinitely large. <sup>6</sup> For example, on macOS, you can configure what happens when the pointer moves to a corner. You could show the Launchpad from the top-left corner:</p><p>The target size is infinite because the pointer can't overshoot past the corner, so the precision required for this interaction is very low. Reaching for any corner becomes a quick flick of the mouse. This is also why operating systems place commonly used menus, like the Apple menu, in corners.</p><p>Radial menus are an exemplary case of Fitts's Law. They spawn around the pointer making the size and distance towards any target the same for all actions. Over time, muscle memory will kick in and even make it possible to select an action based purely on distance and direction.</p><p>Here's a radial menu you can try:</p><div><p>Hold and rotate from anywhere</p></div><h3 data-heading="true" id="scrolling">Scrolling</h3><p>On most operating systems, you can scroll any scrollable region, even if the window itself is not active. This is great, except when another window scrolls unintentionally.</p><p>With the Magic Mouse I can scroll on a window, then move the pointer over a second window to click or find something, and the scroll events will not register on the second window. This feels great to me.</p><p>However, with any traditional mouse, like the Logitech MX Master 3, the scrolling on the first window is cancelled and hijacked by the second window. And it's really frustrating when this happens daily:</p><p>With the Magic Mouse, scrolling is cancelled explicitly by focusing another window:</p><p>Pointing devices like the Magic Trackpad and Magic Mouse also unlock direct manipulation for desktop computing. Besides the obvious ones like swiping between apps, it's also possible to directly manipulate sliders by scrolling, all with a single interaction:</p><h3 data-heading="true" id="closing-thoughts">Closing Thoughts</h3><p>For me, understanding and articulating why something feels right does not come as intuitively as<!-- --> <i>designing</i> something to feel right. But they are two sides of the same coin. There must be a reason. It can be as simple as a particular spring curve or something more innate, like metaphors. Analyzing and making sense of design details beyond just "it feels nice" helps nurture taste, amplify level of execution, and grow appreciation for how hard the pursuit of excellence is.</p><h3 data-heading="true" id="acknowledgments">Acknowledgments</h3><p>Thanks to <a href="https://twitter.com/pacocoursey" target="_blank" rel="noopener noreferrer">Paco</a>,<!-- --> <a href="https://twitter.com/almonk" target="_blank" rel="noopener noreferrer">Alasdair</a>,<!-- --> <a href="https://twitter.com/emilkowalski_" target="_blank" rel="noopener noreferrer">Emil</a>,<!-- --> <a href="https://twitter.com/thomaspaulmann" target="_blank" rel="noopener noreferrer">Thomas</a> for reading early drafts and their insights and feedback.</p><p>No artificial intelligence was used to generate content for this article.</p><h3 data-heading="true" id="resources">Resources</h3><ol><li><a href="https://summit.sfu.ca/_flysystem/fedora/sfu_migrate/15215/2011_CHI_Understanding_Goodman_vy-edited.pdf" target="_blank" rel="noopener noreferrer">E. Goodman, E. Stolterman, R. Wakkary. Understanding Interaction Design Practices (2011)</a></li><li><a href="https://developer.apple.com/videos/play/wwdc2018/803/" target="_blank" rel="noopener noreferrer">C. Karunamuni, N. Vries, M. Alonso. Designing Fluid Interfaces (2018)</a></li><li><a href="https://brandur.org/interfaces" target="_blank" rel="noopener noreferrer">Brandur. Learning From Terminals to Design the Future of User Interfaces (2017)</a></li><li><a href="https://digscholarship.unco.edu/cgi/viewcontent.cgi?article=1669&amp;context=dissertations" target="_blank" rel="noopener noreferrer">S. L. Kriescher. The Effects of Fidgets on Attention and Learning of College Students (2020)</a></li><li><a href="https://en.wikipedia.org/wiki/Fitts%27s_law" target="_blank" rel="noopener noreferrer">Paul Morris Fitts. The information capacity of the human motor system in controlling the amplitude of movement (1954)</a></li><li><a href="http://www.particletree.com/features/visualizing-fittss-law/" target="_blank" rel="noopener noreferrer">Kevin Hale. Visualizing Fitts's Law (2010)</a></li><li><a href="https://andymatuschak.org/files/papers/Apple%20Human%20Interface%20Guidelines%201987.pdf" target="_blank" rel="noopener noreferrer">Apple Human Interface Guidelines (1987)</a></li><li><a href="https://www.youtube.com/watch?v=76b3c_ssyPQ&amp;ab_channel=Figma" target="_blank" rel="noopener noreferrer">Rasmus Andersson. The curious case of user interfaces (2023)</a></li><li><a href="https://museapp.com/podcast/17-rethink-the-os/" target="_blank" rel="noopener noreferrer">Metamuse. Rethink the OS with Jason Yuan (2020)</a></li><li><a href="https://www.mercuryos.com/" target="_blank" rel="noopener noreferrer">Jason Yuan. MercuryOS (2019)</a></li><li><a href="http://paulgraham.com/greatwork.html#f2n" target="_blank" rel="noopener noreferrer">Paul Graham. How to Do Great Work (2023)</a></li><li><a href="https://brianlovin.com/app-dissection" target="_blank" rel="noopener noreferrer">App Dissection. Brian Lovin</a></li></ol></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lima: A nice way to run Linux VMs on Mac (444 pts)]]></title>
            <link>https://jvns.ca/blog/2023/07/10/lima--a-nice-way-to-run-linux-vms-on-mac/</link>
            <guid>36668964</guid>
            <pubDate>Mon, 10 Jul 2023 16:44:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jvns.ca/blog/2023/07/10/lima--a-nice-way-to-run-linux-vms-on-mac/">https://jvns.ca/blog/2023/07/10/lima--a-nice-way-to-run-linux-vms-on-mac/</a>, See on <a href="https://news.ycombinator.com/item?id=36668964">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
     

<p>Hello! Here’s a new entry in the “cool software julia likes” section.</p>

<p>A little while ago I started using a Mac, and one of my biggest
frustrations with it is that often I need to run Linux-specific software. For
example, the <a href="https://jvns.ca/blog/2021/09/24/new-tool--an-nginx-playground/">nginx playground</a> I
posted about the other day only works on Linux because it uses Linux namespaces (via <code>bubblewrap</code>)
to sandbox nginx. And I’m working on another playground right now that uses bubblewrap too.</p>

<p>This post is very short, it’s just to say that Lima seems nice and much simpler
to get started with than Vagrant.</p>

<h3 id="enter-lima">enter Lima!</h3>

<p>I was complaining about this to a friend, and they mentioned
<a href="https://lima-vm.io/">Lima</a>, which stands for <strong>Li</strong>nux on <strong>Ma</strong>c. I’d heard
of <a href="https://github.com/abiosoft/colima">colima</a> (another way to run Linux
containers on Mac), but I hadn’t realized that Lima also just lets you run VMs.</p>

<p>It was surprisingly simple to set up. I just had to:</p>

<ol>
<li>Install Lima (I did <code>nix-env -iA nixpkgs.lima</code> but you can also install it with <code>brew install lima</code>)</li>
<li>Run <code>limactl start default</code> to start the VM</li>
<li>Run <code>lima</code> to get a shell</li>
</ol>

<p>That’s it! By default it mounts your home directory as read-only inside the VM</p>

<p>There’s a config file in <code>~/.lima/default/lima.yaml</code>, but I haven’t needed to change it yet.</p>

<h3 id="some-nice-things-about-lima">some nice things about Lima</h3>

<p>Some things I appreciate about Lima (as opposed to Vagrant which I’ve used in the past and found kind of frustrating) are:</p>

<ol>
<li>it provides a default config</li>
<li>it automatically downloads a Ubuntu 22.04 image to use in the VM (which is what I would have probably picked anyway)</li>
<li>it mounts my entire home directory inside the VM, which I really like as a default choice (it feels very seamless)</li>
</ol>

<p>I think the paradigm of “I have a single chaotic global Linux VM which I use
for all my projects” might work better for me than super carefully configured
per-project VMs. Though I’m sure that you can have carefully configured
per-project VMs with Lima too if you want, I’m just only using the <code>default</code> VM.</p>

<h3 id="problem-i-don-t-know-how-to-mount-directories-read-write">problem: I don’t know how to mount directories read-write</h3>

<p>I wanted to have my entire home directory mounted read-only, but have some
subdirectories (like <code>~/work/nginx-playground</code>) mounted read-write. I did some
research and here’s what I found:</p>

<ul>
<li>a comment on <a href="https://github.com/lima-vm/lima/issues/873">this github issue</a> says that you can use <a href="https://github.com/lima-vm/lima/blob/master/docs/vmtype.md#vz">mountType: “virtiofs” and vmType: “vz”</a> to mount subdirectories of your home directory read-write</li>
<li>the Lima version packaged in nix 23.05 doesn’t seem to support <code>vmType: vz</code> (though I could be wrong about this)</li>
</ul>

<p>Maybe I’ll figure out how to mount directories read-write later, I’m not too
bothered by working around it for now.</p>

<h3 id="why-not-use-containers">why not use containers?</h3>

<p>I wanted a VM and not a Linux container because:</p>

<ol>
<li>the playground runs on a VM in production, not in a container, and generally
it’s easier to develop in a similar environment to production</li>
<li>all of my playgrounds use Linux namespaces, and I don’t know how to create a
namespace inside a container. Probably you can but I don’t feel like
figuring it out and it seems like an unnecessary distraction.</li>
<li>on Mac you need to run containers inside a Linux VM anyway, so I’d rather
use a VM directly and not introduce another unnecessary layer</li>
</ol>

<h3 id="that-s-all">that’s all!</h3>

<p>Some other notes:</p>

<ul>
<li>It looks like Lima works on Linux too</li>
<li>a bunch of people on Mastodon also said <a href="https://github.com/abiosoft/colima">colima</a> (built on top of Lima) is a nice Docker alternative on Mac for running Linux containers</li>
<li>there’s also an new alternative Linux container runtime for Mac called <a href="https://docs.orbstack.dev/">Orb Stack</a>, which I know nothing about</li>
</ul>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple VisionOS Simulator streaming wirelessly to Meta Quest headset (385 pts)]]></title>
            <link>https://github.com/zhuowei/VisionOSStereoScreenshots/tree/alvr</link>
            <guid>36668732</guid>
            <pubDate>Mon, 10 Jul 2023 16:30:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/zhuowei/VisionOSStereoScreenshots/tree/alvr">https://github.com/zhuowei/VisionOSStereoScreenshots/tree/alvr</a>, See on <a href="https://news.ycombinator.com/item?id=36668732">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">visionOS Simulator to ALVR / Meta Quest wireless streaming</h2>
<p dir="auto">Streams the visionOS Simulator to a Meta Quest wirelessly with <a href="https://github.com/alvr-org/ALVR">ALVR</a> installed.</p>
<p dir="auto">Tested with Xcode 15 beta 2 / macOS 14 beta 2 on Apple Silicon, Meta Quest (original).</p>
<h3 tabindex="-1" dir="auto">Usage</h3>
<ol dir="auto">
<li>
<p dir="auto">First, sideload <a href="https://github.com/alvr-org/ALVR-nightly/releases/tag/v21.0.0-dev00%2Bnightly.2023.07.06">ALVR Nightly 2023.07.06</a> onto your Meta Quest.</p>
<p dir="auto">(This does not currently work with stable ALVR)</p>
</li>
<li>
<p dir="auto">Start the visionOS Simulator from Xcode.</p>
</li>
<li>
<p dir="auto">Download and extract <a href="https://github.com/zhuowei/VisionOSStereoScreenshots/releases">alvr_visionos_streaming.zip</a>.</p>
</li>
<li>
<p dir="auto">Inject the streaming library into the Simulator:</p>
<div data-snippet-clipboard-copy-content="./inject.sh
# this resprings the simulator"><pre><code>./inject.sh
# this resprings the simulator
</code></pre></div>
</li>
<li>
<p dir="auto">Open ALVR on your Meta Quest: if all goes well, the visionOS interface should stream into your headset.</p>
</li>
<li>
<p dir="auto">To configure streaming settings, you can use the ALVR dashboard (./alvr_dashboard). See <a href="https://github.com/alvr-org/ALVR">ALVR's documentation</a> for more info.</p>
</li>
<li>
<p dir="auto">You can't control the Simulator using the Quest's controllers yet (I'm looking into it).</p>
<p dir="auto">For now, use the computer's mouse/keyboard to control the Simulator.</p>
<p dir="auto">You probably want to enable a visible mouse cursor inside the Simulator (Settings -&gt; Accessibility -&gt; Pointer Control)</p>
</li>
</ol>
<h3 tabindex="-1" dir="auto">How it works</h3>
<p dir="auto">This hooks CompositorService APIs inside backboardd so that it renders to our own textures instead of to the simulator screen. We then pass these textures to ALVR's server, which encodes them and streams them to the headset.</p>
<h3 tabindex="-1" dir="auto">What's next</h3>
<ul dir="auto">
<li>Enable passthrough</li>
<li>Hook up Quest controllers / eye gaze?</li>
</ul>
<h3 tabindex="-1" dir="auto">Credits</h3>
<p dir="auto">Thank you so much to <a href="https://mastodon.social/@ShinyQuagsire" rel="nofollow">@ShinyQuagsire</a>: he <a href="https://mastodon.social/@ShinyQuagsire/110670442474420349" rel="nofollow">released</a> the <a href="https://github.com/shinyquagsire23/XRGyroControls_OpenXR">first ever tool</a> for streaming the visionOS Simulator to a Quest headset (via wired Quest Link), and helped me figure out how to port this to work wirelessly using ALVR.</p>
<p dir="auto">Thanks to <a href="https://infosec.exchange/@jjtech" rel="nofollow">@JJTech</a> and <a href="https://mastodon.social/@keithahern" rel="nofollow">@keithahern</a> for figuring out how the visionOS Simulator handles input.</p>
<p dir="auto">Thanks to <a href="https://github.com/alvr-org/ALVR">the ALVR developers</a> for making an amazing cross-platform VR streaming system.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Study: 87% of classic video games are not legally available (476 pts)]]></title>
            <link>https://gamehistory.org/87percent/</link>
            <guid>36668472</guid>
            <pubDate>Mon, 10 Jul 2023 16:15:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gamehistory.org/87percent/">https://gamehistory.org/87percent/</a>, See on <a href="https://news.ycombinator.com/item?id=36668472">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-content" role="main">

	
<article id="post-24002">
	
	<!-- .cover-header -->

	<div id="post-inner">

		
<p>The Video Game History Foundation, in partnership with the Software Preservation Network, has conducted <a href="https://doi.org/10.5281/zenodo.7996492">the first ever study</a> on the commercial availability of classic video games, and the results are bleak. <strong>87% of classic video games released in the United States are critically endangered</strong>.</p>



<p>Imagine if the only way to watch <em>Titanic</em> was to find a used VHS tape, and maintain your own vintage equipment so that you could still watch it. And what if no library, not even the Library of Congress, could do any better — they could keep and digitize that VHS of <em>Titanic</em>, but you’d have to go all the way there to watch it. It sounds crazy, but that’s the reality we live in with video games, a $180 billion industry, while the games and their history disappear. </p>



<div><figure><img decoding="async" width="1024" height="1024" src="https://gamehistory.org/wp-content/uploads/2023/07/8-1-1024x1024.png" alt="" srcset="https://gamehistory.org/wp-content/uploads/2023/07/8-1-1024x1024.png 1024w, https://gamehistory.org/wp-content/uploads/2023/07/8-1-300x300.png 300w, https://gamehistory.org/wp-content/uploads/2023/07/8-1-740x740.png 740w, https://gamehistory.org/wp-content/uploads/2023/07/8-1-768x768.png 768w, https://gamehistory.org/wp-content/uploads/2023/07/8-1.png 1080w" sizes="(max-width: 1024px) 100vw, 1024px"></figure><div>
<p>Just 13% of video game history is being represented in the current marketplace. In fact, no period of video game history defined in this study even cracked <em>20% representation</em>.</p>



<p>Figure 1: Availability rate of historical games, by period, between 1960 and 2009. (n = 1500, ±2.5%, 95% CI) Curious about our methodology? Check out our <a href="https://gamehistory.org/study-explainer">study explainer blog pos</a><a href="https://gamehistory.org/?p=24007">t</a>!</p>
</div></div>



<p>For accessing <em>nearly 9 in 10</em> classic games, there are few options: seek out and maintain vintage collectible games and hardware, travel across the country to visit a library, or… piracy. None of those options are desirable, which means that most video games are inaccessible to all but the most diehard and dedicated fans. That’s pretty grim!</p>



<p>This is where libraries and archives&nbsp;<em>should</em>&nbsp;come in. Anyone&nbsp;<em>should</em>&nbsp;be able to easily explore, research and play classic video games, in the same way that they can read classic novels, listen to classic albums, and watch classic movies. But outdated copyright laws are preventing institutions like ours from doing our jobs.</p>


<div>
<figure><img decoding="async" loading="lazy" src="https://gamehistory.org/wp-content/uploads/2023/07/Historical-Games-Simple.png" alt="" width="454" height="454" srcset="https://gamehistory.org/wp-content/uploads/2023/07/Historical-Games-Simple.png 1080w, https://gamehistory.org/wp-content/uploads/2023/07/Historical-Games-Simple-300x300.png 300w, https://gamehistory.org/wp-content/uploads/2023/07/Historical-Games-Simple-1024x1024.png 1024w, https://gamehistory.org/wp-content/uploads/2023/07/Historical-Games-Simple-740x740.png 740w, https://gamehistory.org/wp-content/uploads/2023/07/Historical-Games-Simple-768x768.png 768w" sizes="(max-width: 454px) 100vw, 454px"><figcaption><em>Availability rate of historical games. (</em>n<em> = 1500, ±2.5%, 95% CI)</em><br><em>Random list of games pulled from MobyGames</em>.</figcaption></figure></div>


<p><strong>Goal of this study:</strong> Get expanded exemptions for libraries and organizations preserving video games, which are currently far more limited than their ability to preserve books, movies, audio, etc.</p>



<p><strong>How this study helps:</strong> The <a href="https://arstechnica.com/gaming/2023/03/why-game-archivists-are-dreading-this-months-3ds-wii-u-eshop-shutdown/">video game industry’s main lobbying group</a> has successfully argued to the US Copyright Office that the industry already does enough to preserve its own history commercially, and that additional protections for preservation institutions would hurt their bottom line. We proved them wrong: the industry has actually only managed to make 13% of its history available, and <a href="https://www.youtube.com/watch?v=HLWY7fCXUwE">it’s unlikely to get better</a>.</p>



<p><em>The argument that these games are commercially available is what’s keeping video games behind in the preservation world.</em></p>











<p>The next <a href="https://www.copyright.gov/1201/">rulemaking proceeding</a> under the Digital Millennium Copyright Act (“DMCA”), Title 17, section 1201, of the&nbsp;<em>United States Code</em> is scheduled for 2024. We’re hopeful that this study will incite change, and that video game preservation will become stronger — before we lose more.</p>



<p><em>Survey of the Video Game Reissue Market in the United States was conducted for the Video Game History Foundation and the Software Preservation Network by VGHF Library Director Phil Salvador, published July 2023.</em></p>



<p><mark>Want to learn more?</mark> Check out our <a href="http://gamehistory.org/study-explainer">explainer blog post</a> for a more in-depth look at this study. A bonus episode of our podcast about this study and its findings will be available soon. You can <a href="https://doi.org/10.5281/zenodo.7996492">read the full study here</a>.</p>



<hr>



<h4>Special thanks to our volunteers:</h4>





		</div><!-- .post-inner -->

	
	<!-- .pagination-single -->

	
</article><!-- .post -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First U.S. ban on sale of cellphone location data might be coming (510 pts)]]></title>
            <link>https://www.wsj.com/articles/first-u-s-ban-on-sale-of-cellphone-location-data-might-be-coming-fbe47e53</link>
            <guid>36667848</guid>
            <pubDate>Mon, 10 Jul 2023 15:28:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/articles/first-u-s-ban-on-sale-of-cellphone-location-data-might-be-coming-fbe47e53">https://www.wsj.com/articles/first-u-s-ban-on-sale-of-cellphone-location-data-might-be-coming-fbe47e53</a>, See on <a href="https://news.ycombinator.com/item?id=36667848">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
This copy is for your personal, non-commercial use only. Distribution and use of this material are governed by
our Subscriber Agreement and by copyright law. For non-personal use or to order multiple copies, please contact
Dow Jones Reprints at 1-800-843-0008 or visit www.djreprints.com.
</p><p>https://www.wsj.com/articles/first-u-s-ban-on-sale-of-cellphone-location-data-might-be-coming-fbe47e53</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Keep Linux Open and Free–We Can’t Afford Not To (190 pts)]]></title>
            <link>https://www.oracle.com/news/announcement/blog/keep-linux-open-and-free-2023-07-10/</link>
            <guid>36667582</guid>
            <pubDate>Mon, 10 Jul 2023 15:11:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oracle.com/news/announcement/blog/keep-linux-open-and-free-2023-07-10/">https://www.oracle.com/news/announcement/blog/keep-linux-open-and-free-2023-07-10/</a>, See on <a href="https://news.ycombinator.com/item?id=36667582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div data-trackas="rc25"><p><span>Press Release</span></p><p><strong> <!-- -->By Edward Screven, Chief Corporate Architect and Wim Coekaerts, Head of Oracle Linux Development<!-- -->—<!-- -->July 10, 2023<!-- --> </strong></p></div>
<div>
<p>Oracle has been part of the Linux community for 25 years. Our goal has remained the same over all those years: help make Linux the best server operating system for everyone, freely available to all, with high-quality, low-cost support provided to those who need it.</p>

<p>Our Linux engineering team makes significant contributions to the kernel, file systems, and tools. We push all that work back to mainline so that every Linux distribution can include it. We are proud those contributions are part of the reason Linux is now so very capable, benefiting not just Oracle customers, but all users.</p>

<p>In 2006, we launched what is now called Oracle Linux, a RHEL compatible distribution and support offering that is used widely, and powers Oracle’s engineered systems and our cloud infrastructure. We chose to be RHEL compatible because we did not want to fragment the Linux community. Our effort to remain compatible has been enormously successful. In all the years since launch, we have had almost no compatibility bugs filed. Customers and ISVs can switch to Oracle Linux from RHEL without modifying their applications, and we certify Oracle software products on RHEL even though they are built and tested on Oracle Linux only, never on RHEL.</p>

<p>While Oracle and IBM have compatible Linux distributions, we have very different ideas about our responsibilities as open source stewards and about operating under the GPLv2. Oracle has always made Oracle Linux binaries and source freely available to all. We do not have subscription agreements that interfere with a subscriber’s rights to redistribute Oracle Linux. On the other hand, IBM subscription agreements specify that you’re in breach if you use those subscription services to exercise your GPLv2 rights. And now, as of June 21, IBM no longer publicly releases RHEL source code.</p>

<p>Why did IBM make this change? Well, if you read IBM’s <a href="https://www.redhat.com/en/blog/red-hats-commitment-open-source-response-gitcentosorg-changes">blog</a> attempting to explain its rationale, it boils down to this:</p>

<blockquote><q>At Red Hat, thousands of people spend their time writing code to enable new features, fixing bugs, integrating different packages and then supporting that work for a long time … We have to pay the people to do that work.</q></blockquote>

<p>Interesting. IBM doesn’t want to continue publicly releasing RHEL source code because it has to pay its engineers? That seems odd, given that Red Hat as a successful independent open source company chose to publicly release RHEL source and pay its engineers for many years before IBM acquired Red Hat in 2019 for $34 billion.</p>

<p>The blog goes on to mention CentOS. It is no surprise CentOS was top of mind for the author attempting to justify withholding RHEL source. CentOS had been a very popular free RHEL compatible distribution. In December 2020, IBM effectively killed it as a free alternative to RHEL. Two new alternatives to RHEL have sprung up in CentOS’s place: AlmaLinux and Rocky Linux. Now, by withholding RHEL source code, IBM has directly attacked them.</p>

<p>And perhaps that is the real answer to the question of why: eliminate competitors. Fewer competitors means more revenue opportunity for IBM.</p>

<p>As for Oracle, we will continue pursuing our goal for Linux as transparently and openly as we always have while minimizing fragmentation. We will continue to develop and test our software products on Oracle Linux. Oracle Linux will continue to be RHEL compatible to the extent we can make it so. In the past, Oracle’s access to published RHEL source has been important for maintaining that compatibility. From a practical standpoint, we believe Oracle Linux will remain as compatible as it has always been through release 9.2, but after that, there may be a greater chance for a compatibility issue to arise. If an incompatibility does affect a customer or ISV, Oracle will work to remediate the problem.</p>

<p>We want to emphasize to Linux developers, Linux customers, and Linux distributors that Oracle is committed to Linux freedom. Oracle makes the following promise: as long as Oracle distributes Linux, Oracle will make the binaries and source code for that distribution publicly and freely available. Furthermore, Oracle welcomes downstream distributions of every kind, community and commercial. We are happy to work with distributors to ease that process, work together on the content of Oracle Linux, and ensure Oracle software products are certified on your distribution.</p>

<p>By the way, if you are a Linux developer who disagrees with IBM’s actions and you believe in Linux freedom the way we do, we are hiring.</p>

<p>One observation for ISVs: IBM’s actions are not in your best interest. By killing CentOS as a RHEL alternative and attacking AlmaLinux and Rocky Linux, IBM is eliminating one way your customers save money and make a larger share of their wallet available to you. If you don’t yet support your product on Oracle Linux, we would be happy to show you how easy that is. Give your customers more choice.</p>

<p>Finally, to IBM, here’s a big idea for you. You say that you don’t want to pay all those RHEL developers? Here’s how you can save money: just pull from us. Become a downstream distributor of Oracle Linux. We will happily take on the burden.</p>
</div>
<!-- --> <!-- -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Feature Flags: Theory vs. Reality (119 pts)]]></title>
            <link>https://bpapillon.com/post/feature-flags-theory-vs-reality/</link>
            <guid>36667469</guid>
            <pubDate>Mon, 10 Jul 2023 15:02:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bpapillon.com/post/feature-flags-theory-vs-reality/">https://bpapillon.com/post/feature-flags-theory-vs-reality/</a>, See on <a href="https://news.ycombinator.com/item?id=36667469">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        

<figure>
  <img src="https://bpapillon.com/communismtheoryhomer.gif" alt="A discussion about DevOps in 2023">
  <figcaption>A discussion about DevOps in 2023</figcaption>
</figure>

<h2 id="the-rise-of-feature-flags-in-devops"><strong>The Rise of Feature Flags in DevOps</strong></h2>

<p>During the second half of the 2010s, the DevOps movement gained massive momentum throughout the SaaS industry. Riding this wave and buoyed by the marketing efforts of LaunchDarkly, feature toggles rapidly became an essential tool for engineering operations used by practically every SaaS company.</p>

<p>In the DevOps context, feature flags are especially great for continuous deployment. By decoupling release from deployment, they allow marketing and engineering to operate independently. This flexibility enables engineers to continuously deploy features in progress.</p>

<p>However, a few downsides were clear even from the beginning. To name a few:</p>

<ul>
<li><strong>They add complexity to the code</strong>, including additional testing overhead.</li>
<li><strong>It’s difficult to verify</strong> whether all of your changes are actually “behind” a feature flag as intended.</li>
<li><strong>Feature toggle checks aren’t free</strong>; it’s more logic, which means more possible bugs, and the check may take time if a database or external service is involved.</li>
</ul>

<p>Due to these shortcomings, the <a href="https://martinfowler.com/articles/feature-toggles.html">conventional wisdom</a> has always been that feature flags should be short-lived and kept to a minimum:</p>

<blockquote>
<p>Savvy teams view the Feature Toggles in their codebase as inventory which comes with a carrying cost and seek to keep that inventory as low as possible.</p>
</blockquote>

<h2 id="reality-check"><strong>Reality Check!</strong></h2>

<figure>
  <img src="https://bpapillon.com/sideshow-bob-rakes.gif" alt="A software developer interacts with their feature management system">
  <figcaption>A software developer interacts with their feature management system</figcaption>
</figure>

<p>As adoption of the feature toggle pattern has spread throughout the industry, a few common problems can be observed.</p>

<h3 id="1-release-follow-through-and-ever-expanding-complexity"><strong>1. Release follow-through and ever-expanding complexity</strong></h3>

<p>If feature flags are meant to be short-lived, then there must be a process for reviewing them and cleaning them up at the appropriate time. This often takes the form of a manual process, such as filing a ticket for a future sprint.</p>

<p>As a general rule of thumb, any process that relies on humans to remember things is not going to be a reliable one, and for most companies, these processes end up being spotty and inconsistent.</p>

<p>A few common issues I’ve seen:</p>

<ul>
<li><strong>Zombie flags</strong>

<ul>
<li>In our haste to move on to the next big feature, we often forget to rip out the release flag. This leads to higher “carrying cost”, taking many forms: the readability of the code, the number of automated tests that continually execute each branch of the feature flag logic, and so forth. Over time, this makes our code base harder to work on and leads to annoyance, wasted time, and bugs.</li>
</ul></li>
<li><strong>Unfinished business</strong>

<ul>
<li>Perhaps we forget to finish rolling out our flag, or maybe some customer segments never even get the new feature. This leads to unexpected support tickets months later, at a time when everyone in the organization thought that the rollout was long finished.</li>
</ul></li>
<li><strong>Ghost flags</strong>

<ul>
<li>Sometimes, even when we remember to rip out the feature flag, there may be a communication breakdown with other users, who end up futilely toggling a control that has no effect.</li>
</ul></li>
</ul>

<h3 id="2-unintended-or-inappropriate-usage"><strong>2. Unintended or inappropriate usage</strong></h3>

<p>Even with perfect follow-through, a flag meant for release purposes might end up being implemented for long-term feature access. This may seem like a time-saving win, but it results in third-party services becoming load-bearing for cases beyond their intended purpose.</p>

<p>Imagine even a very short outage by the service provider. If we only rely on that service provider for release flags, we probably live with our hardcoded fallback values; but, if we’re using it for something like a permission or entitlement check, there is now essentially an outage of our own.</p>

<p>Many teams understand this danger, and respond by implementing separate systems for short-lived and long-lived flags; the former may use a managed service, while the latter may be a home-grown system that stores its data in the main application database.</p>

<p>Even though we now have a more correct tool for each job, we still end up with a lot of problems in practice:</p>

<ul>
<li><strong>Lack of portability &amp; promotion</strong>

<ul>
<li>We’ve mentioned that release flags can sometimes take on a second life as other types of toggles. With this model, if a release flag starts to make sense as a long-lived flag later, there’s no way to promote it to the more appropriate system without making code changes.</li>
</ul></li>
<li><strong>User confusion &amp; frustration</strong>

<ul>
<li>Across these two systems (short and long-lived), there may be a broad base of user types: engineering, product, customer success, marketing, ops. It’s unlikely that these groups share a consistent mental model for the systems, and people tend to just refer to all of it as “flags”. Many users may be unclear as to why there are two separate systems at all, and feel frustrated that they just have to remember which flag lives in which system.</li>
</ul></li>
<li><strong>Lack of context &amp; readability</strong>

<ul>
<li>Exacerbating the user confusion issue, most flags are exposed as a list of keys (e.g. “new_onboarding_flow”, “widgets_v2”) that provide very little context about what these flags do or how they’re meant to be used.</li>
<li>Without talking to someone, reading code, or separately maintaining documentation, it’s very difficult to know much about the flags, which frustrates non-technical and technical stakholders alike.</li>
</ul></li>
<li><strong>Testing headaches</strong>

<ul>
<li>Developers have to accommodate multiple systems in testing; for example, if you’re using mocks or stubs in your tests to simulate the behavior of these systems, you now need twice as many of these.</li>
</ul></li>
<li><strong>Feature gaps</strong>

<ul>
<li>A long-lived flag service that is homegrown may be able to provide better guarantees with regards to latency and availability; however, the short-lived flag service will almost always be more feature-rich because it’s often provided by a managed service. This reality may incentivize its use over the more reliable system in certain cases.</li>
</ul></li>
</ul>

<h2 id="accepting-reality">Accepting Reality</h2>

<p>After observing the feature flag experiment in the wild for some time, I’ve come to the conclusion that the conventional wisdom to limit the number and age of feature flags in your code base is wise, but unrealistic. The fast-paced and cross-functional nature of modern software development create dynamics that are too hard to overcome with best intentions and best practices.</p>

<p>Furthermore, there’s a disconnect between “feature management” as a term of industry and the actual “feature management” that goes on. This term, and the tools that are sold within this market, generally refer only to DevOps use cases like rollout and experimentation. However, we clearly do a lot more managing of features than this.</p>

<ul>
<li>Every time a new customer signs up, are their features not being managed?</li>
<li>When they upgrade to a more expensive plan?</li>
<li>When a sales negotiation results in a bespoke enterprise plan?</li>
<li>When a customer success rep enables an add-on?</li>
</ul>

<p>These are all feature management, but none of them fall into the definition of “feature management” that our tooling and DevOps culture wants to support.</p>

<p>As engineers, it’s time to change our framing of feature management to better align with the businesses we operate in, but to do this, we need new tools.</p>

<h2 id="taming-complexity">Taming complexity</h2>

<figure>
  <img src="https://bpapillon.com/homer-grill-fail.gif" alt="A software developer evaluates their adherence to feature management best practices">
  <figcaption>A software developer evaluates their adherence to feature management best practices</figcaption>
</figure>

<p>A software developer evaluates their adherence to feature management best practices</p>

<p>If we accept that we as engineers are powerless to contain the spread of feature management, and perhaps that we are holding back our businesses to the extent we try to do so, then we need to stop relying on manual processes for hygiene and maintenance.</p>

<p>Let’s imagine what capabilities we might need a new feature management tool to have in order to accomplish this. A few possibilities:</p>

<ul>
<li>Long-lived and short-lived use cases coexist within the same tool, but are clearly delineated in its interfaces.

<ul>
<li>Short-lived flags might come with additional metadata, such as an expiration date by which we expect the flag should no longer be in use.</li>
</ul></li>
<li>Flags should have an owner, either an individual user or perhaps a user role or group.</li>
<li>Users should be able to add meaning to flags after the fact via metadata and grouping.</li>
<li>If a flag changes purpose, say from a release flag to an entitlement check, we can simply update this in the tool. Such a change would be tracked in an audit log.</li>
<li>Policies can be set; for example, short-lived flags must be removed or graduated within a specified amount of time, or perhaps certain metadata (like expiration date) can be made required for certain types of flags.</li>
<li>Flags can easily be used in relation to one another; for example, one flag might be required in order for another flag to be enabled, or two flags might be incompatible with one another for code reasons. The tool should make it easy to configure such invariants.</li>
</ul>

<p>With capabilities like this in place, the tool could start to automate some of the maintenance processes that are currently manual.</p>

<p>For example:</p>

<ul>
<li>Auditing a codebase for out-of-date flags could be done via a static analysis tool in CI.</li>
<li>We could fail builds or notify engineering or product managers if certain assumptions are not met.</li>
<li>Flag owners could receive notifications when flags they are responsible for are out of compliance, or a ticket could automatically be filed in the ticketing system.</li>
</ul>

<p>If we can automate these processes, then we finally might have a system that holds up to the chaos of the modern software development process and fights back against ever-growing complexity.</p>

<h2 id="onward">Onward!</h2>

<figure>
  <img src="https://bpapillon.com/twirling-towards-freedom.gif" alt="Leaving the old expectations behind as we move to a new framing of feature management">
  <figcaption>Leaving the old expectations behind as we move to a new framing of feature management</figcaption>
</figure>

<p>It’s high time that we take another look at what “feature management” means in our industry.  If we accept a more expansive view that aligns with how our businesses want to be managing features and build the tools needed to support this, we can free ourselves from the need to adhere to best practices that have proved unrealistic.</p>

<p>If we were to have a tool like this that better suited the natural complexity of feature management, then this would be a great start. However, there are more considerations, such as the architecture of such a tool, that I will explore in upcoming posts.</p>

        
      </div></div>]]></description>
        </item>
    </channel>
</rss>