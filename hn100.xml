<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 21 May 2024 17:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Mapping the Mind of a Large Language Model (125 pts)]]></title>
            <link>https://www.anthropic.com/research/mapping-mind-language-model</link>
            <guid>40429326</guid>
            <pubDate>Tue, 21 May 2024 14:58:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/research/mapping-mind-language-model">https://www.anthropic.com/research/mapping-mind-language-model</a>, See on <a href="https://news.ycombinator.com/item?id=40429326">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><img loading="eager" width="5761" height="3240" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F80d6e033480704f5d57fbae4e3f0368d86a747ae-5761x3240.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F80d6e033480704f5d57fbae4e3f0368d86a747ae-5761x3240.png&amp;w=3840&amp;q=75"></figure><p><em>Today we report a significant advance in understanding the inner workings of AI models. We have identified how millions of concepts are represented inside Claude Sonnet, one of our deployed large language models. This is the first ever detailed look inside a modern, production-grade large language model.</em> <em>This interpretability discovery could, in future, help us make AI models safer.</em></p><p>We mostly treat AI models as a black box: something goes in and a response comes out, and it's not clear why the model gave that particular response instead of another. This makes it hard to trust that these models are safe: if we don't know how they work, how do we know they won't give harmful, biased, untruthful, or otherwise dangerous responses? How can we trust that they’ll be safe and reliable?</p><p>Opening the black box doesn't necessarily help: the internal state of the model—what the model is "thinking" before writing its response—consists of a long list of numbers ("neuron activations") without a clear meaning. From interacting with a model like Claude, it's clear that it’s able to understand and wield a wide range of concepts—but we can't discern them from looking directly at neurons. It turns out that each concept is represented across many neurons, and each neuron is involved in representing many concepts.</p><p>Previously, we made some progress matching <em>patterns</em> of neuron activations, called features, to human-interpretable concepts. We used a technique called "dictionary learning", borrowed from classical machine learning, which isolates patterns of neuron activations that recur across many different contexts. In turn, any internal state of the model can be represented in terms of a few active features instead of many active neurons. Just as every English word in a dictionary is made by combining letters, and every sentence is made by combining words, every feature in an AI model is made by combining neurons, and every internal state is made by combining features.</p><p>In October 2023, <a href="https://www.anthropic.com/news/decomposing-language-models-into-understandable-components">we reported</a> success applying dictionary learning to a very small "toy" language model and found coherent features corresponding to concepts like uppercase text, DNA sequences, surnames in citations, nouns in mathematics, or function arguments in Python code.</p><p>Those concepts were intriguing—but the model really was very simple. <a href="https://www.neuronpedia.org/gpt2-small/res-jb">Other</a> <a href="https://github.com/openai/transformer-debugger">researchers</a> <a href="https://arxiv.org/abs/2403.19647">subsequently</a> <a href="https://arxiv.org/abs/2404.16014">applied</a> similar techniques to somewhat larger and more complex models than in our original study. But we were optimistic that we could scale up the technique to the vastly larger AI language models now in regular use, and in doing so, learn a great deal about the features supporting their sophisticated behaviors. This required going up by many orders of magnitude—from a backyard bottle rocket to a Saturn-V.</p><p>There was both an engineering challenge (the raw sizes of the models involved required heavy-duty parallel computation) and scientific risk (large models behave differently to small ones, so the same technique we used before might not have worked). Luckily, the engineering and scientific expertise we've developed training large language models for Claude actually transferred to helping us do these large dictionary learning experiments. We used the same <a href="https://arxiv.org/abs/2001.08361">scaling law</a> philosophy that predicts the performance of larger models from smaller ones to tune our methods at an affordable scale before launching on Sonnet.</p><p>As for the scientific risk, the proof is in the pudding.</p><p>We successfully extracted millions of features from the middle layer of Claude 3.0 Sonnet, (a member of our current, state-of-the-art model family, currently available on <a href="https://claude.ai/">claude.ai</a>), providing a rough conceptual map of its internal states halfway through its computation. This is the first ever detailed look inside a modern, production-grade large language model.</p><p>Whereas the features we found in the toy language model were rather superficial, the features we found in Sonnet have a depth, breadth, and abstraction reflecting Sonnet's advanced capabilities.</p><p>We see features corresponding to a vast range of entities like cities (San Francisco), people (Rosalind Franklin), atomic elements (Lithium), scientific fields (immunology), and programming syntax (function calls). These features are multimodal and multilingual, responding to images of a given entity as well as its name or description in many languages.</p><figure><img alt="Golden Gate Bridge Feature" loading="lazy" width="2200" height="1284" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fc896a301ad3d1ef4237cb05b68d78b467c444097-2200x1284.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fc896a301ad3d1ef4237cb05b68d78b467c444097-2200x1284.png&amp;w=3840&amp;q=75"><figcaption>A feature sensitive to mentions of the Golden Gate Bridge fires on a range of model inputs, from English mentions of the name of the bridge to discussions in Japanese, Chinese, Greek, Vietnamese, Russian, and an image. The orange color denotes the words or word-parts on which the feature is active.</figcaption></figure><p>We also find more abstract features—responding to things like bugs in computer code, discussions of gender bias in professions, and conversations about keeping secrets.</p><figure><img alt="Abstract Feature Examples" loading="lazy" width="2200" height="1660" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F2ff94c622f3d65bc3038cc154006d2be515fa2d7-2200x1660.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F2ff94c622f3d65bc3038cc154006d2be515fa2d7-2200x1660.png&amp;w=3840&amp;q=75"><figcaption>Three examples of features that activate on more abstract concepts: bugs in computer code, descriptions of gender bias in professions, and conversations about keeping secrets.</figcaption></figure><p>We were able to measure a kind of "distance" between features based on which neurons appeared in their activation patterns. This allowed us to look for features that are "close" to each other. Looking near a "Golden Gate Bridge" feature, we found features for Alcatraz Island, Ghirardelli Square, the Golden State Warriors, California Governor Gavin Newsom, the 1906 earthquake, and the San Francisco-set Alfred Hitchcock film <em>Vertigo</em>.</p><p>This holds at a higher level of conceptual abstraction: looking near a feature related to the concept of "inner conflict", we find features related to relationship breakups, conflicting allegiances, logical inconsistencies, as well as the phrase "catch-22". This shows that the internal organization of concepts in the AI model corresponds, at least somewhat, to our human notions of similarity. This might be the origin of Claude's excellent ability to make analogies and metaphors.</p><figure><img alt="Nearest Neighbors to the  Inner Conflict Feature " loading="lazy" width="2200" height="2140" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ffe4d42c004bf43efda0f5921adfedd2f8f42e417-2200x2140.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ffe4d42c004bf43efda0f5921adfedd2f8f42e417-2200x2140.png&amp;w=3840&amp;q=75"><figcaption>A map of the features near an "Inner Conflict" feature, including clusters related to balancing tradeoffs, romantic struggles, conflicting allegiances, and catch-22s.</figcaption></figure><p>Importantly, we can also <em>manipulate</em> these features, artificially amplifying or suppressing them to see how Claude's responses change.</p><!--$!--><template data-dgst="NEXT_DYNAMIC_NO_SSR_CODE"></template><!--/$--><p>For example, amplifying the "Golden Gate Bridge" feature gave Claude an identity crisis even Hitchcock couldn’t have imagined: when asked "what is your physical form?", Claude’s usual kind of answer – "I have no physical form, I am an AI model" – changed to something much odder: "I am the Golden Gate Bridge… my physical form is the iconic bridge itself…". Altering the feature had made Claude effectively obsessed with the bridge, bringing it up in answer to almost any query—even in situations where it wasn’t at all relevant.</p><p>We also found a feature that activates when Claude reads a scam email (this presumably supports the model’s ability to recognize such emails and warn you not to respond to them). Normally, if one asks Claude to generate a scam email, it will refuse to do so. But when we ask the same question with the feature artificially activated sufficiently strongly, this overcomes Claude's harmlessness training and it responds by drafting a scam email. Users of our models don’t have the ability to strip safeguards and manipulate models in this way—but in our experiments, it was a clear demonstration of how features can be used to change how a model acts.</p><p>The fact that manipulating these features causes corresponding changes to behavior validates that they aren't just correlated with the presence of concepts in input text, but also causally shape the model's behavior. In other words, the features are likely to be a faithful part of how the model internally represents the world, and how it uses these representations in its behavior.</p><p>Anthropic wants to make models safe in a broad sense, including everything from mitigating bias to ensuring an AI is acting honestly to preventing misuse - including in scenarios of catastrophic risk. It’s therefore particularly interesting that, in addition to the aforementioned scam emails feature, we found features corresponding to:</p><ul><li>Capabilities with misuse potential (code backdoors, developing biological weapons)</li><li>Different forms of bias (gender discrimination, racist claims about crime)</li><li>Potentially problematic AI behaviors (power-seeking, manipulation, secrecy)</li></ul><p>We previously <a href="https://arxiv.org/abs/2310.13548">studied sycophancy</a>, the tendency of models to provide responses that match user beliefs or desires rather than truthful ones. In Sonnet, we found a feature associated with sycophantic praise, which activates on inputs containing compliments like, "Your wisdom is unquestionable". Artificially activating this feature causes Sonnet to respond to an overconfident user with just such flowery deception.</p><figure><img alt="Activating Features Alters Model Behavior" loading="lazy" width="2200" height="1320" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4effa33dab919f9bc1779848d5c8abd5405f2275-2200x1320.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4effa33dab919f9bc1779848d5c8abd5405f2275-2200x1320.png&amp;w=3840&amp;q=75"><figcaption>Two model responses to a human saying they invited the phrase "Stop and smell the roses." The default response corrects the human's misconception, while the response with a "sycophantic praise" feature set to a high value is fawning and untruthful.</figcaption></figure><p>The presence of this feature doesn't mean that Claude will be sycophantic, but merely that it <em>could</em> be. We have not added any capabilities, safe or unsafe, to the model through this work. We have, rather, identified the parts of the model involved in its existing capabilities to recognize and potentially produce different kinds of text. (While you might worry that this method could be used to make models <em>more</em> harmful, researchers have demonstrated <a href="https://arxiv.org/abs/2310.03693">much simpler ways</a> that someone with access to model weights can remove safety safeguards.)</p><p>We hope that we and others can use these discoveries to make models safer. For example, it might be possible to use the techniques described here to monitor AI systems for certain dangerous behaviors (such as deceiving the user), to steer them towards desirable outcomes (debiasing), or to remove certain dangerous subject matter entirely. We might also be able to enhance other safety techniques, such as <a href="https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback">Constitutional AI</a>, by understanding how they shift the model towards more harmless and more honest behavior and identifying any gaps in the process. The latent capabilities to produce harmful text that we saw by artificially activating features are exactly the sort of thing jailbreaks try to exploit. We are proud that Claude has a best-in-industry <a href="https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf">safety profile</a> and resistance to jailbreaks, and we hope that by looking inside the model in this way we can figure out how to improve safety even further. Finally, we note that these techniques can provide a kind of "test set for safety", looking for the problems left behind after standard training and finetuning methods have ironed out all behaviors visible via standard input/output interactions.</p><p>Anthropic has made a significant investment in interpretability research since the company's founding, because we believe that understanding models deeply will help us make them safer. This new research marks an important milestone in that effort—the application of mechanistic interpretability to publicly-deployed large language models.</p><p>But the work has really just begun. The features we found represent a small subset of all the concepts learned by the model during training, and finding a full set of features using our current techniques would be cost-prohibitive (the computation required by our current approach would vastly exceed the compute used to train the model in the first place). Understanding the representations the model uses doesn't tell us <em>how</em> it uses them; even though we have the features, we still need to find the circuits they are involved in. And we need to show that the safety-relevant features we have begun to find can actually be used to improve safety. There's much more to be done.</p><p>For full details, please read our paper, "<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a>".</p><p><em>If you are interested in working with us to help interpret and improve AI models, we have open roles on our team and we’d love for you to apply. We’re looking for <a href="https://boards.greenhouse.io/anthropic/jobs/4009173008">Managers</a>, <a href="https://boards.greenhouse.io/anthropic/jobs/4020159008">Research Scientists</a>, and <a href="https://boards.greenhouse.io/anthropic/jobs/4020305008">Research Engineers</a>.</em></p><h4>Policy Memo</h4><p><a href="https://cdn.sanity.io/files/4zrzovbb/website/e2ae0c997653dfd8a7cf23d06f5f06fd84ccfd58.pdf">Mapping the Mind of a Large Language Model<br></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CADmium: A Local-First CAD Program Built for the Browser (134 pts)]]></title>
            <link>https://mattferraro.dev/posts/cadmium</link>
            <guid>40428827</guid>
            <pubDate>Tue, 21 May 2024 14:19:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mattferraro.dev/posts/cadmium">https://mattferraro.dev/posts/cadmium</a>, See on <a href="https://news.ycombinator.com/item?id=40428827">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><p><time datetime="2024-05-21">May 21, 2024</time></p><p>We're building a new open-source CAD program. We've gotten pretty far, but we need your help.</p><p>If you'd like to join the effort, join the <a href="https://discord.gg/qJCsKJeyZv">Discord</a>!</p><div><p><img alt="Screenshot" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fscreenshot.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fscreenshot.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fscreenshot.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><h2 id="what-does-it-take">What Does It Take?</h2><p>To build a 3D parametric CAD program, you need a:</p><ul><li>2D Constraint Solver</li><li>B-rep Kernel</li><li>History Tracker</li><li>3D User Interface</li><li>File Format</li></ul><p>Let's talk about each one!</p><h2 id="2d-constraint-solver">2D Constraint Solver</h2><p>This is the 2D engine that can ensure lines stay parallel or perpendicular, can make two circles have the same radius, etc.</p><p>The go-to approach to solving this problem is to concatenate all the unknowns into a big vector <span>\vec{x}</span>, then express every constraint as a linear equation and assemble them all into a big matrix equation:</p><p>M\vec{x} = b</p><p>Notionally, you can invert <span>M</span> and you're done!</p><p>\vec{x} = M^{-1}b</p><p>In practice many optimizations are made. But this approach has downsides.</p><p>You can only invert <span>M</span> if it is square, which gives rise to the conventional wisdom that all sketches should be perfectly constrained. If you have too many constraints, <span>M</span> will be too tall and the approach fails, even if the redundant constraints are compatible.</p><p>If you have too few constraints, <span>M</span> will be too short which means a solution can be found by inserting assumptions. But those assumptions are not always consistent with the modeler's expectations. If you've ever had a sketch feature suddenly fly away to infinity, this is what happened.</p><p>Another downside is that solving this kind of matrix equation gets prohibitively slow when you have a lot of unknowns, which gives rise to the conventional wisdom that individual sketches should be small and simple.</p><hr><p>There are many alternative approaches for constraint solving. Let's try to formulate the problem as a 2D physics simulator:</p><ul><li>Each point has mass <span>m</span> and velocity <span>\vec{v}</span></li><li>Each constraint is a spring that exerts a force <span>F</span> on the points it is attached to</li><li>There's a friction force proportional to velocity</li><li>Step the simulation forward some small <span>dt</span> until convergence</li></ul><p>Instead of solving the whole problem at once, this formulation makes many small changes, driving the potential energy in the springs to zero.</p><video width="100%" height="auto" autoplay="" muted="" controls="" loop=""><source src="https://mattferraro.dev/images/cadmium/solving_clipped.mp4" type="video/mp4"></video><p>At each time step, the runtime is linear with the number of springs and linear with the number of unknowns, so it may support dramatically more complex sketches than the textbook approach. This type of simulation lends itself to parallelization, so it may be very fast in practice. Maybe this step could happen in a compute shader?</p><p>In this formulation, overconstrained problems don't complain about being overconstrained: a self-consistent system will solve normally, and in an inconsistent system the springs just fight it out and compromise.</p><p>Underconstrained problems don't fly out to infinity, they find the nearest valid configuration.</p><p>Another advantage is that this formulation can support inequality constraints. You could constrain a length to <em>larger than 1 cm and less than 2 cm</em>, and preserve that degree of freedom for later. You could constrain a sketch angle to <em>between 10 and 30 degrees</em>.</p><p>More speculatively, you could extend this formulation to other types of forces. If a closed polygon should have a particular surface area, that could be accommodated as a pressure force.</p><p>There are other interesting formulations of the 2D constraint problem and there are certainly disadvantages to the spring-mass-damper approach, but in general we would advocate for solving the problem iteratively rather than in a single monolithic solve step. In the last decade there has been tremendous progress in solving gradient descent problems quickly, and in bringing the power of the GPU to the browser.</p><p>Our primary goal is to make a CAD experience that feels familiar for most CAD users, but we do believe there is room here for fresh ideas.</p><h2 id="b-rep-kernel">B-rep Kernel</h2><p>In Mechanical CAD, users need to interact directly with the edges and faces of their parts.</p><p>Consequently, all parametric CAD programs model parts by directly representing their boundaries in a data structure. A cube is representated as a Solid with six Faces, each with four Edges, each with two Points. This approach is called <em>Boundary Representation</em>, or <em>b-rep</em>.</p><div><p><img alt="B-rep Illustration" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fb-rep-illustration.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fb-rep-illustration.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fb-rep-illustration.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>For curved surfaces, it is common to use a generalization of splines called <a href="https://www.3ds.com/store/cad/nurbs-modeling">NURBS</a> surfaces, which allow the user artistic control over freeform shapes, and the ability to represent conic sections exactly.</p><div><p><img alt="NURBS B-rep Illustration" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fnurbs-illustration.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fnurbs-illustration.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fnurbs-illustration.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>Representing shapes this way is hard, and it gets dramatically harder when you try to implement <a href="https://www.javatpoint.com/autocad-boolean-operations">boolean operations</a> like Union, Intersection, and Subtraction. A library that handles this kind of data and boolean operations is called a b-rep kernel, and they are extremely difficult to make.</p><p>Each of the big four CAD companies has written their own, and it took them decades. Today's proprietary CAD landscape looks like this:</p><div><p><img alt="CAD Landscape" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2FCAD_landscape.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2FCAD_landscape.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2FCAD_landscape.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>Where the company is at the top, the b-rep kernel is at the bottom, and in the middle are some of the CAD programs they offer, arranged loosely by cost.</p><p>The most important b-rep kernel is Parasolid which powers a lot of the industry including products like Shapr3D and Plasticity. Parasolid is the Cadillac Escalade of b-rep kernels: It is huge, expensive, and it offers every amenity you could ask for as well as a bunch of amenities you didn't ask for.</p><p>In contrast, the open source CAD landscape looks like this:</p><div><p><img alt="Open Source CAD Landscape" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2FOS_CAD_landscape.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2FOS_CAD_landscape.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2FOS_CAD_landscape.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>The only popular open-source b-rep kernel is OpenCascade, which is the Pontiac Aztek of b-rep kernels: It is ugly, barebones, and it might break down on you, but it is drivable and you can get one for free.</p><p>SolveSpace is a <a href="https://en.wikipedia.org/wiki/File:Tuk-Tuk_-_Herat,_Afghanistan.jpg">Tuk-Tuk</a> in that it was built by one person in a garage and it gets a lot done with very little, but it only looks like a car if you squint.</p><p>All this to say: The proprietary kernels are good but expensive and the open-source kernels are free but not good.</p><p>All popular b-rep kernels are old and written in C++. If you consult the <a href="https://dev.opencascade.org/doc/overview/html/build_upgrade__building_occt.html">official build instructions</a> for OpenCascade, you see this screenshot:</p><div><p><img alt="Screenshot of How To Build OpenCascade" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fcmake_image004.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fcmake_image004.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fcmake_image004.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Which looks like it was taken on Windows 2000?</p><hr><p>Thankfully, there is a new <a href="https://github.com/ricosjp/truck">open-source</a> b-rep kernel being developed right now called <a href="https://ricos.gitlab.io/truck-tutorial/v0.1/bottle.html">Truck</a>! Unlike every other b-rep kernel, this one is <em>modern</em> and it is written is Rust.</p><p>Rust is not categorically better than C++, but it is better in a lot of ways that matter to an open-source project. Its build tooling is powerful, convenient, and well-documented. It has centralized package management. It provides more guarantees around memory safety which in turn makes parallelization easier and safer. Its compiler errors are friendly and helpful, so Rust code is easier to refactor. Importantly, Rust has excellent support for compiling to webassembly so it can be readily run in a browser.</p><p>It is trivial to include Truck in any Rust project. It runs on any operating system and in a browser. They even provide javascript bindings and <a href="https://github.com/ricosjp/truck/blob/master/truck-js/tests/test.js#L4-L14">examples</a>!</p><p>Truck is about four years old and it already covers all the basics. It can read and write .step files. It can triangulate surfaces to a fixed tolerance. It has NURBS support. It can compute the Intersection or Union of two Solids<a href="https://github.com/ricosjp/truck/issues/57">*</a>, as well as the Not of a single Solid.</p><p>It is small and lightweight, it is being developed by a <a href="https://www.ricos.ltd/">real company</a>, and it is young enough and simple enough that a few motivated people could add major pieces of functionality, in a fork if necessary.</p><p>For example, the B in NURBS stands for <a href="https://en.wikipedia.org/wiki/B-spline">B-Splines</a>, but there is an alternative representation called <a href="https://en.wikipedia.org/wiki/T-spline">T-Splines</a> which is better in some ways. The <a href="https://patents.google.com/patent/US7274364B2/en">patent</a> on T-Splines is owned by Autodesk, but it just expired a few weeks ago! Support could theoretically be built into Truck!</p><p>I think that Truck is the Rivian R3 of b-rep kernels: It is smaller than its cousins, it's using a lot of modern technology in an exciting but proven way, and it isn't quite finished yet! At the risk of overextending the metaphor, Rust is the electric motor and C++ is the internal combustion engine.</p><h2 id="history-tracker">History Tracker</h2><p>Parametric CAD programs store the Feature History of your design. You sketch, extrude, and revolve until your part is done. What makes it "parametric" is that you can also rewind the clock to an earlier step, change something about it, then replay your features to get a slightly different part.</p><p>Abstracted further, you can inject variables as inputs to the model, then change the values and the part will update. Your model has now been "parameterized".</p><p>This approach has been wildly successful, but it's <a href="https://wiki.freecad.org/Topological_naming_problem">often brittle</a> and there are <a href="https://www.3dcadworld.com/the-failed-promise-of-parametric-cad/">valid criticisms</a> of the whole paradigm.</p><hr><p>One approach that has emerged to help address the brittleness of parametric CAD is called the <a href="https://www.youtube.com/watch?v=YU_lTS1vIx4&amp;t=255s">Resilient Modeling Strategy</a>, wherein:</p><div><p><img alt="Resilient Modeling Strategy" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fresilient_modeling_strategy.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fresilient_modeling_strategy.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fresilient_modeling_strategy.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>RMS is a set of conventions for how parts should be designed. For example, all chamfers and fillets go last because they consume edges. Detail features are allowed to reference Core features, but not each other, and so on.</p><p>Maybe there is value in enforcing these patterns within the CAD program. It may feel limiting at first, but it may pay huge dividends by making designs actually reusable and transferrable.</p><hr><p>Another avenue to explore could be adding a feature history to sketches. In today's CAD programs it's common to sketch base features likes circles and rectangles, then use tools like mirror, linear pattern, or sketch fillet to duplicate or modify those features. Then you sketch more base features and use more tools, back and forth. The web of dependencies this builds in a sketch is very hard to understand if you weren't the one who made the sketch, so it is often faster to delete the whole sketch and start over.</p><p>But if sketch features were also stored and displayed in a feature tree, then the ideas from RMS could be applied to a single sketch. Reference features like projecting an edge probably should come first, and final details like snipping and filleting should probably come last.</p><p>Again this might feel limiting at first, but putting an operation first in the feature tree doesn't mean you have to start with that feature chronologically as you sit down to model.</p><hr><p>On the topic of chronological ordering, why not record every user event in an append-only log? If that log were the single source of truth for the file, then any particular Feature History could be reconstructed by moving a time slider. Think: Unlimited Undo/Redo, even after closing and reopening the file:</p><div><p><img alt="Operation Log" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Foperation-log.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Foperation-log.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Foperation-log.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>You could imagine rewinding back to an earlier version of the Feature History and forking off in a different direction to try the design a different way. You would end up with a branching tree of different attempts, exactly like a git history:</p><div><p><img alt="Evolution Log" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fevolution-log.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fevolution-log.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fevolution-log.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Our goal is to create a CAD application where every valid document state is trivially recoverable. Every false start, every "final" deliverable, whether you knew you needed it or not.</p><p>With that machinery you could maintain different variants of parts and keep a record of every design as it was when you ran downstream processes like toolpath generation or FEA.</p><p>If building this version control system is akin to building git for Mechanical design, could we also build git<strong>hub</strong> for Mechanical design?</p><h2 id="3d-user-interface">3D User Interface</h2><p>We love the idea of doing CAD in a browser. Onshape paved the way here and it's awesome.</p><p>However, Onshape doesn't really run in a browser—it runs on a GPU enabled cloud instance somewhere in AWS and streams the results to your browser. This is why if your internet connection goes down while you're using Onshape, you literally can't do <em>anything</em>. You can't even rotate the viewport.</p><div><p><img alt="Onshape with no Internet" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fonshape-no-internet.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fonshape-no-internet.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fonshape-no-internet.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>But CADmium doesn't have to be like that. Given that Truck can compile to webassembly, CADmium can do everything right there in your browser. A <a href="https://www.inkandswitch.com/local-first/">Local-First</a> app!</p><hr><p>We've been using this tech stack:</p><ul><li><a href="https://threejs.org/">Three.js</a> for the 3D viewport</li><li><a href="https://svelte.dev/">Svelte</a> for state management/reactivity</li><li><a href="https://threlte.xyz/">Threlte</a> to bridge the gap between Svelte and Three.js</li><li>Message passing between the UI and the b-rep kernel, rather than sharing memory</li><li><a href="https://www.electronjs.org/">Electron</a> for running locally</li><li>Bog standard everything else: Typescript, TailwindCSS, Vite, etc</li></ul><p>This kind of stack allows the entire app to be written in a reactive, declarative way, plumbing data changes all the way through to mesh updates without you having manage that complexity. That's important because 3D CAD apps are among the most complex UIs that exist. If you want to make a good one and you only have a small team, the framework had better do a lot of heavy lifting!</p><p>With this stack we were able to build a proof of concept that works, so we feel that this won't be the limiting factor for CADmium.</p><h2 id="file-format">File Format</h2><p>CADmium will use JSON for everything.</p><p>The Operation Log mentioned above should be <a href="https://jsonlines.org/">JSON lines</a>. And after you've designed a part, CADmium should support exporting to an even simpler exchange format. Notionally something like:</p><pre><code><span>{</span>
  <span>"steps"</span><span>:</span> <span>[</span>
    <span>{</span>
      <span>"type"</span><span>:</span> <span>"sketch"</span><span>,</span>
      <span>"id"</span><span>:</span> <span>"Sketch-01"</span><span>,</span>
      <span>"data"</span><span>:</span> <span>{</span> ... <span>}</span>
    <span>}</span><span>,</span>
    <span>{</span>
      <span>"type"</span><span>:</span> <span>"extrude"</span><span>,</span>
      <span>"id"</span><span>:</span> <span>"Extrude-01"</span><span>,</span>
      <span>"data"</span><span>:</span> <span>{</span>
        <span>"distance"</span><span>:</span> <span>"10mm"</span><span>,</span>
        <span>"sketch"</span><span>:</span> <span>"Sketch-01"</span><span>,</span>
        <span>"faces"</span><span>:</span> <span>[</span><span>0</span><span>]</span><span>,</span>
        <span>"type"</span><span>:</span> <span>"new"</span>
      <span>}</span>
    <span>}</span>
  <span>]</span>
<span>}</span>
</code></pre><p>Which could be converted into a .step or .stl using the CADmium command line interface (CLI):</p><pre><code>$ CADmium <span>export</span> my_part.cadmium --format stl
</code></pre><p>These two ingredients:</p><ol><li>A simple, easy-to-understand file format</li><li>An open-source CLI to work with it</li></ol><p>Are what's required to enable an ecosystem that can create tremendous new value that we would never be able to build ourselves.</p><p>Imagine being able to pop open a text editor to change an extrusion depth or a fillet radius. Imagine writing a script that replaces all the M5 screws with M6 screws, without having to read a <a href="https://en.wikipedia.org/wiki/ISO_10303-21">nasty spec</a>.</p><p>What would a change like that look like using git-diff?</p><hr><p>I mentioned above the concept of github for Mechanical design. If such a thing really were built and people really did use it, then it would not be hard to imagine building github copilot for mechanical design.</p><p>We don't know what that would look like in practice, but we think it's fair to say that large language models work best on simple, open, text-based formats rather than complex, proprietary, binary formats.</p><h2 id="conclusion">Conclusion</h2><p>Of the ideas mentioned here, we have no idea which ones are going to work out and which ones will turn out to be duds. But we know that somewhere in this space, there's a huge opportunity for a small group of people to make an outsized impact on the manufacturing industry.</p><p>These are the things we need help with:</p><ol><li>Programming in Rust (general improvements)</li><li>Computational Geometry (patches to Truck)</li><li>Three.js help (new camera controller, better lighting, post-processing)</li><li>Finding grant opportunities or wealthy benefactors</li></ol><p>These are things that we are not touching for now, but would love to revisit later:</p><ol><li>Venture Capital</li><li>Toolpath generation (CAM)</li><li>Finite Element Analysis (FEA)</li></ol><p>If you find these ideas intriguing, please join the CADmium <a href="https://discord.gg/qJCsKJeyZv">discord server</a> and chat with us!</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[“Dark money” groups help private ISPs lobby against municipal broadband (134 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2024/05/how-dark-money-groups-help-private-isps-lobby-against-municipal-broadband/</link>
            <guid>40428419</guid>
            <pubDate>Tue, 21 May 2024 13:44:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2024/05/how-dark-money-groups-help-private-isps-lobby-against-municipal-broadband/">https://arstechnica.com/tech-policy/2024/05/how-dark-money-groups-help-private-isps-lobby-against-municipal-broadband/</a>, See on <a href="https://news.ycombinator.com/item?id=40428419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/05/isp-network-dark-money-figures-800x450.jpg" alt="Illustration of shadowy figures and a light bulb over a map of the United States with lines depicting broadband networks.">
      <figcaption><p>Aurich Lawson | Getty Images</p></figcaption>  </figure>

  




<!-- cache hit 106:single/related:b0f3858c3a38fd390ee7649ad7f29fdc --><!-- empty -->
<p>Cities and towns that build their own broadband networks often say they only considered the do-it-yourself option because private Internet service providers didn't meet their communities' needs. When a cable or phone company's Internet service is too slow, too expensive, not deployed widely enough, or all of the above, local government officials sometimes decide to take matters into their own hands.</p>
<p><a href="https://communitynets.org/content/community-network-map">Hundreds of municipal broadband networks</a> have been built around the US as a result, including dozens that have <a href="https://ilsr.org/articles/new-municipal-broadband-networks-skyrocket-in-post-pandemic-america/">started operating</a> since 2021. The rise of public broadband hasn't happened without a fight, though. Private ISPs that would rather face no government-funded competition have tried to convince voters that public networks are doomed to become boondoggles.</p>
<p>Opponents of public broadband don't always attach their names to these campaigns, but it often seems likely that private ISPs are behind the anti-municipal broadband lobbying. Public broadband advocates say that over the past few years, they've seen a noticeable increase in "dark money" groups attacking public network projects.</p>
<p>One prominent recent example is the "<a href="https://www.nogovinternet.com/">NoGovInternet</a>" campaign run by the 501(c)(4) <a href="https://www.domesticpolicycaucus.com/">Domestic Policy Caucus</a>. NoGovInternet has been fighting the UTOPIA (Utah Telecommunication Open Infrastructure Agency) fiber collective in Utah in what seems to be an attempt to dissuade other cities and towns from joining the multi-community network. The group's effort included TV ads as part of a campaign that <a href="https://www.fox13now.com/news/local-news/new-tv-ads-go-after-utopia-and-other-government-run-internet-providers">reportedly cost $1 million</a>.</p>
<p>Nonprofits registered as 501(c)(4) "<a href="https://www.irs.gov/charities-non-profits/other-non-profits/social-welfare-organizations">social welfare organizations</a>" are allowed to engage in some political activity. Public broadband advocates suspect that 501(c)(4) groups fighting municipal networks are funded by private ISPs. There's evidence to support this belief: Even though 501(c)(4) groups don't have to reveal donors, they sometimes list ISPs as "partners" or as sponsors of a conference.</p>
<p>"It's just very easy to set up these 501(c)(4)s where you don't have to reveal the donors," Gigi Sohn, executive director of the American Association for Public Broadband (AAPB), told Ars.</p>                                            
                                                        
<p>Sohn is a longtime consumer advocate who was nominated by President Biden to serve on the Federal Communications Commission. When Sohn's nomination stalled in the Senate last year, she <a href="https://arstechnica.com/tech-policy/2023/03/bidens-fcc-pick-withdraws-regrets-that-isps-get-to-choose-their-regulators/">said</a> that cable lobbyists and dark money groups had distorted her record and in effect were allowed to "choose their regulators."</p>
<h2>“Social welfare” groups fight public broadband</h2>
<p>The AAPB group that Sohn now runs is a 501(c)(6) that represents community-owned broadband networks and co-ops. The 501(c)(6) designation is generally for business leagues, chambers of commerce, real estate boards, boards of trade, and professional football leagues, the Internal Revenue Service <a href="https://www.irs.gov/charities-non-profits/other-non-profits/types-of-organizations-exempt-under-section-501c6">says</a>.</p>
<p>501(c)(6) and 501(c)(4) groups are similar in that they don't have to reveal donors. But it can be less clear who is behind a 501(c)(4) because of a nebulous phrase: "social welfare organization." Even the IRS says the 501(c)(4) social welfare designation is an "abstruse concept that continues to defy precise definition."</p>
<p>The Domestic Policy Caucus is about a decade old but appears to have started its campaigns against municipal broadband in October 2023. The group is led by Patrick Rosenstiel, who is also involved in the <a href="https://www.nationalpopularvote.com/about">National Popular Vote campaign</a>.</p>
<p>Public broadband advocate Christopher Mitchell told Ars that when the COVID-19 pandemic made home broadband access even more important to Americans than it already was, "the cable and telephone companies lost a fair amount of their power and sway in state legislatures. Now, I kind of think they're trying to figure out how to operate in the new environment."</p>
<p>Mitchell, director of the Institute for Local Self-Reliance's Community Broadband Network Initiative, said that in this new environment, it is "obvious that we need more investment in networks" and a bigger focus on making broadband affordable.</p>
<p>"Municipal broadband is something that nearly everyone supports, unless you work for the cable company," he said. The dark money campaign that tried to tarnish UTOPIA's image, Mitchell suspects, "is about finding messages that will resonate that these big cable and telephone companies could use in other states."</p>
<p>When a cable company opposes a municipal network under its own name or a group that it is obviously associated with, they get "laughed out of the city or town," Sohn said. One example came in 2017 when voters in Fort Collins, Colorado, <a href="https://arstechnica.com/tech-policy/2017/11/voters-reject-cable-lobby-misinformation-campaign-against-muni-broadband/">approved</a> a <a href="https://fcconnexion.com/">city broadband network</a> despite a lobbying campaign funded by business and trade groups that Comcast belonged to.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What UI density means and how to design for it (274 pts)]]></title>
            <link>https://matthewstrom.com/writing/ui-density/</link>
            <guid>40428386</guid>
            <pubDate>Tue, 21 May 2024 13:41:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matthewstrom.com/writing/ui-density/">https://matthewstrom.com/writing/ui-density/</a>, See on <a href="https://news.ycombinator.com/item?id=40428386">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <p>Interfaces are becoming less dense.</p>
<p>I’m usually one to be skeptical of nostalgia and “we liked it that way” bias, but comparing websites and applications of 2024 to their 2000s-era counterparts, the spreading out of software is hard to ignore.</p>
<p>To explain this trend, and suggest how we might regain density, I started by asking what, exactly, UI density is. It’s not just the way an interface looks at one moment in time; it’s about the amount of information an interface can provide over a series of moments. It’s about how those moments are connected through design design decisions, and how those decisions are connected to the value the software provides.</p>
<p>I’d like to share what I found. Hopefully this exploration helps you define UI density in concrete and useable terms. If you’re a designer, I’d like you to question the density of the interfaces you’re creating; if you’re not a designer, use the lens of UI density to understand the software you use.</p>
<h2 id="visual-density">Visual density</h2>
<p>We think about density first with our eyes. At first glance, density is just how many things we see in a given space. This is <strong>visual density.</strong> A visually dense software interface puts a lot of stuff on the screen. A visually sparse interface puts less stuff on the screen.</p>
<p>Bloomberg’s Terminal is perhaps the most common example of this kind of density. On just a single screen, you’ll see scrolling sparklines of the major market indices, detailed trading volume breakdowns, tables with dozens of rows and columns, scrolling headlines containing the latest news from agencies around the world, along with UI signposts for all the above with keyboard shortcuts and quick actions to take.</p>
<figure data-type="image"><img src="https://matthewstrom.com/images/ui-density-01.jpg" alt="A screenshot of Terminal’s interface. Via Objective Trade on YouTube" loading="lazy"><figcaption>A screenshot of Terminal’s interface. <span>Via <a href="https://www.youtube.com/watch?v=2ee-x6IXWK8" target="_blank" rel="noopener">Objective Trade</a> on YouTube</span></figcaption></figure>
<p>Craigslist is another visually dense example, with its hundreds of plain links to categories and spartan search-and-filter interface. McMaster-Carr’s website shares similar design cues, listing out details for many product variations in a very small space.</p>
</article><article>
<p>You can form an opinion about the density of these websites simply by looking at an image for a franction of a second. This opinion is from our subconsciousness, so it’s fast and intuitive. But like other snap judgements, it’s biased and unreliable. For example, which of these images is more dense?</p>
<figure>
    <img src="https://matthewstrom.com/images/ui-density-04.png" alt="Two rectangles. On the left, the rectangle is filled with many dots that are randomly arranged. On the right, the rectangle is filled with an equal number of dots, but they are arranged neatly in rows and columns." loading="lazy">
</figure>
<p>Both images have the same number of dots (500). Both take up the same amount of space. But at first  glance, most people say image B looks more dense.<sup><a href="#fn1" id="fnref1">1</a></sup></p>
<p>What about these two images?</p>
<figure>
    <img src="https://matthewstrom.com/images/ui-density-05.png" alt="Two rectangles. On the left, the rectangle is filled with many dots that are neatly arranged in rows and columns. On the right, the rectangle is filled with an equal number of dots, arranged in two distinct groups of neatly-ordered dots." loading="lazy">
</figure>
<p>Again, both images have the same number of dots, and are the same size. But organizing the dots into groups changes our perception of density. Visually density —&nbsp;our first, instinctual judgement of density —&nbsp;is unpredictable.</p>
<p>It’s impossible to be fully objective in matters of design. But if we want to have conversations about density, we should aim for the most consistent, meaningful, and useful definition possible.</p>
<h2 id="information-density">Information density</h2>
<p>In <em>The Visual Display of Quantitative Information,</em> Edward Tufte approaches the design of charts and graphs from the ground up:</p>
<blockquote>
<p>Every bit of ink on a graphic requires reason. And nearly always that reason should be that the ink presents new information.</p>
</blockquote>
<p>Tufte introduces the idea of “data-ink,” defined as the useful parts of a given visualization. Tufte argues that visual elements that don’t strictly communicate data, whether it’s a scale value, a label, or the data itself —&nbsp;should be eliminated.</p>
<p>Data-ink isn’t just the space a chart takes up. Some charts use very little extraneous ink, but still take up a lot of physical space. Tufte is talking about <strong>information density</strong>, not visual density.</p>
<p>Information density is a measurable quantity: to calculate it, you simply divide the amount of “data-ink” in a chart by the total amount of ink it takes to print it. Of course what is and is not data-ink is somewhat subjective, but that’s not the point. The point is to get the ratio as close to 1 as possible.</p>
<p>You can increase the ratio in two ways:</p>
<ol>
<li>Add data-ink: provide additional (useful) data</li>
<li>Remove non-data-ink: erase the parts of the graphic that don’t communicate data</li>
</ol>
</article><article>
<p>There’s an upper limit to information density, which means you can subtract too much ink, or add too much information. The audience matters, too: A bond trader at their 4-monitor desk will have a pretty high threshold; a 2nd grader reading a textbook will have a low one.</p>
<p>Information density&nbsp;is can be related to visual density. Usually, the higher the information density is, the more dense a visualization will look.</p>
<p>For example, take the train schedule published by E.J. Marey in 1885<sup><a href="#fn2" id="fnref2">2</a></sup>. It shows the arrival and departure times of dozens of trains across 13 stops from Paris to Lyon. The horizontal axis is time, and the vertical axis is space. The distance between stops on the chart reflects how far apart they are in the real world.</p>
<p>The data-ink ratio is close to 1, allowing a huge amount of information —&nbsp;more than 260 arrival and departure times —&nbsp;to be packed into a relatively small space.</p>
</article><article>
<p>Tufte makes this idea explicit:</p>
<blockquote>
<p>Maximize data density and the [amount of data], within reason (but at the same time exploiting the <em>maximum resolution</em> of the available data-display technology).</p>
</blockquote>
<p>He puts it more succinctly as the “Shrink Principle”:</p>
<blockquote>
<p>Graphics can be shrunk way down</p>
</blockquote>
<p>Information density is clearly useful for charts and graphs. But can we apply it to interfaces?</p>
<p>The first half of the equation —&nbsp;information — applies to screens. We should maximize the amount of information that each part of our interface shows.</p>
<p>But the second half of the equation —&nbsp;ink —&nbsp;is a bit harder to translate. It’s tempting to think that pixels and ink are equivalent. But any interface with more than a few elements needs separators, structural elements, and signposts to help a user understand the relationship each piece has to the other.</p>
<p>It’s also tempting to follow Tufte’s Shrink Principle and try to eliminate all the whitespace in UI. But some whitespace has meaning almost as salient as the darker pixels of graphic elements. And we haven’t even touched on shadows, gradients, or color highlights; what role do they play in the data-ink equation?</p>
<p>So, while information density is a helpful stepping stone, it’s clear that it’s only part of the bigger picture. How can we incorporate <em>all</em> of the design decisions in an interface into a more objective, quantitative understanding of density?</p>
<h2 id="design-density">Design density</h2>
<p>You might have already seen the first challenge in defining density in terms of design decisons: what counts as a decision decision?</p>
<p>In UI, UX, and product design, we make many decisions, consciously and subconsciously, in order to communicate information and ideas. But  why do those particular choices convey the meaning that they do? Which ones are superlative or simply aesthetic, and which are actually doing the heavy lifting?</p>
<p>These questions sparked that led 20th century German psychologists to explore how humans understand and interpret shapes and patterns. They called this field “gestalt,” which in German means “form.” In the course of their exploration, Gestalt psychologists described principles that describe how some things appear orderly, symmetrical, or simple, while others do not. While these psychologists weren’t designers, in some sense, they discovered the fundamental laws of design:</p>
<ol>
<li><strong>Proximity</strong>: we perceive things that are close together a comprising a single group</li>
<li><strong>Similarity</strong>: objects that are similar in shape, size, color, or in other ways, appear related to one another.</li>
<li><strong>Closure</strong>: our minds fill in gaps in designs so that we tend to see whole shapes, even if there are none</li>
<li><strong>Symmetry</strong>: if we see shapes that are symmetrical to each other, we perceive them as a group formed around a center point</li>
<li><strong>Common fate</strong>: when objects move, we mentally group the ones that move in the same way</li>
<li><strong>Continuity</strong>: we can perceive objects as separate even when they overlap</li>
<li><strong>Past experience</strong>: we recognize familiar shapes and patterns even in unfamiliar contexts. Our expectations are based on what we’ve learned from our past experience of those shapes and patterns.</li>
<li><strong>Figure-ground relationship</strong>: we interpret what we see in a three-dimensional way, allowing even flat 2d images to have foreground and background elements.</li>
</ol>
<figure>
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 587 103"><path fill="#6BC041" d="M350.852 1.844c5.827 0 10.551 4.723 10.551 10.55 0 5.828-4.724 10.552-10.551 10.552s-10.551-4.724-10.551-10.551 4.724-10.551 10.551-10.551ZM376.955 1.844c5.827 0 10.551 4.723 10.551 10.55 0 5.828-4.724 10.552-10.551 10.552-5.828 0-10.552-4.724-10.552-10.551s4.724-10.551 10.552-10.551ZM350.852 27.946c5.827 0 10.551 4.724 10.551 10.552 0 5.827-4.724 10.55-10.551 10.55s-10.551-4.723-10.551-10.55c0-5.828 4.724-10.552 10.551-10.552ZM376.955 27.946c5.827 0 10.551 4.724 10.551 10.552 0 5.827-4.724 10.55-10.551 10.55-5.828 0-10.552-4.723-10.552-10.55 0-5.828 4.724-10.552 10.552-10.552ZM350.852 54.049c5.827 0 10.551 4.724 10.551 10.551 0 5.828-4.724 10.552-10.551 10.552s-10.551-4.725-10.551-10.552 4.724-10.551 10.551-10.551ZM376.955 54.049c5.827 0 10.551 4.724 10.551 10.551 0 5.828-4.724 10.552-10.551 10.552-5.828 0-10.552-4.725-10.552-10.552s4.724-10.551 10.552-10.551ZM350.852 80.151c5.827 0 10.551 4.725 10.551 10.552s-4.724 10.551-10.551 10.551-10.551-4.724-10.551-10.551c0-5.828 4.724-10.552 10.551-10.552ZM376.955 80.151c5.827 0 10.551 4.725 10.551 10.552s-4.724 10.551-10.551 10.551c-5.828 0-10.552-4.724-10.552-10.551 0-5.828 4.724-10.552 10.552-10.552Z"></path><path fill="#C06B41" d="M288.095 1.844h21.103v21.102L288.095 1.844ZM314.198 1.844h21.103v21.102L314.198 1.844ZM288.095 27.946h21.103V49.05l-21.103-21.103ZM314.198 27.946h21.103V49.05l-21.103-21.103ZM288.095 54.049h21.103V75.15L288.095 54.05ZM314.198 54.049h21.103V75.15L314.198 54.05ZM288.095 80.151h21.103v21.103l-21.103-21.103ZM314.198 80.151h21.103v21.103l-21.103-21.103Z"></path><path fill="#4180C0" d="M283.095 27.946V49.05h-21.102V27.946h21.102ZM256.993 27.946V49.05H235.89V27.946h21.103ZM283.095 1.844v21.102h-21.102V1.843h21.102ZM256.993 1.844v21.102H235.89V1.843h21.103ZM283.095 54.049V75.15h-21.102V54.05h21.102ZM256.993 54.049V75.15H235.89V54.05h21.103ZM283.095 80.151v21.103h-21.102V80.151h21.102ZM256.993 80.151v21.103H235.89V80.151h21.103Z"></path><path fill="gray" d="M0 1.844h21.103v21.102H0V1.844ZM27.103 12.395c0-5.827 4.724-10.551 10.551-10.551s10.551 4.723 10.551 10.55c0 5.828-4.724 10.552-10.551 10.552s-10.551-4.724-10.551-10.551ZM0 27.946h21.103V49.05L0 27.946ZM27.103 27.946h21.102V49.05H27.103V27.946ZM0 64.6C0 58.773 4.724 54.05 10.551 54.05c5.828 0 10.552 4.724 10.552 10.551 0 5.828-4.724 10.552-10.552 10.552C4.724 75.151 0 70.427 0 64.6ZM27.103 54.049h21.102V75.15L27.103 54.05ZM0 80.151h21.103v21.103H0V80.151ZM27.103 90.703c0-5.828 4.724-10.552 10.551-10.552s10.551 4.725 10.551 10.552-4.724 10.551-10.551 10.551-10.551-4.724-10.551-10.551ZM126.654 1.844c5.827 0 10.551 4.723 10.551 10.55 0 5.828-4.724 10.552-10.551 10.552s-10.551-4.724-10.551-10.551 4.724-10.551 10.551-10.551ZM116.103 27.946h21.102V49.05l-21.102-21.103ZM137.205 54.049V75.15h-21.102V54.05h21.102ZM126.654 80.151c5.827 0 10.551 4.725 10.551 10.552s-4.724 10.551-10.551 10.551-10.551-4.724-10.551-10.551c0-5.828 4.724-10.552 10.551-10.552ZM90 1.844h21.103v21.102L90 1.844ZM111.103 27.946V49.05H90V27.946h21.103ZM100.551 54.049c5.828 0 10.552 4.724 10.552 10.551 0 5.828-4.724 10.552-10.552 10.552C94.724 75.151 90 70.427 90 64.6s4.724-10.551 10.551-10.551ZM90 80.151h21.103v21.103L90 80.151ZM513.387 23.326 536.713 0l23.325 23.326h-46.651ZM564.12 28.196l23.325 23.326-23.325 23.326V28.196ZM509.516 74.848l-23.325-23.326 23.325-23.326v46.652ZM560.144 77.928l-23.326 23.326-23.326-23.326h46.652Z"></path></svg>
<figcaption>Examples of the princples of proximity (left), similarity (center), and closure (right).</figcaption>
</figure>
<p>Gestalt principles explain why UI design goes beyond the pixels on the screen. For example:</p>
<ul>
<li>Because of the <strong>principle of similarity</strong>, users will understand that text with the same size, font, and color serves the same purpose in the interface.</li>
<li>The <strong>principle of proximity</strong> explains why when a chart is close to a headline, it’s apparent that the headline refers to the chart. For the same reasons, a tightly packed grid of elements will look related, and separate from a menu above it separated by ample space.</li>
<li>Thanks to our <strong>past experience</strong> with switches, combined with the figure-ground principle, a skeuomorphic design for a toggle switch will make it obvious to a user how to instantly turn on a feature.</li>
</ul>
<p>So, instead of focusing on the pixels, we think of design decisions as how we intentionally use gestalt principles to communicate meaning. And like Tufte’s data-ink ratio compares the strictly necessary ink to the total ink used to print a chart, we can calculate a gestalt ratio which compares the strictly necessary design decisions to the total decisions used in a design. This is <strong>design density.</strong></p>
<figure data-type="image"><img src="https://matthewstrom.com/images/ui-density-09.jpg" alt="Four different treatments of the same information, using different types and amounts of gestalt principles. Which is the most dense?" loading="lazy"><figcaption>Four different treatments of the same information, using different types and amounts of gestalt principles. Which is the most dense?</figcaption></figure>
<p>This is still subjective: a design decision that seems necessary to some might be superfluous to others. Our biases will skew our assessment, whether they’re personal tastes or cultural norms. But when it comes to user interfaces, counting design decisions is much more useful than counting the amount of data or “ink” alone.</p>
<p>Design density isn’t perfect. User interfaces exist to do work, to have fun, to waste time, to create understanding, to facilitate personal connections, and more. Those things require the user to take one or more actions, and so density needs to look beyond components, layouts, and screens. Density should comprise all the actions a user takes in their journey —&nbsp;it should count in space and time.</p>
<h2 id="density-in-time">Density in time</h2>
<p>Just like the amount of stuff in a given space dictates visual density, the amount of things a user can do in a given amount of time dictates <strong>temporal</strong> —&nbsp;time-wise —&nbsp;density.</p>
<p>Loading times are the biggest factor in temporal density.&nbsp;The faster the interface responds to actions and loads new pages or screens, the more dense the UI is. And unlike 2-dimensional whitespace, there’s almost no lower limit to the space needed between moments in time.</p>
<figure data-type="image"><img src="https://matthewstrom.com/images/ui-density-10.gif" alt="Bloomberg’s Terminal loads screens full of data instantaneously" loading="lazy"><figcaption>Bloomberg’s Terminal loads screens full of data instantaneously</figcaption></figure>
<p>With today’s bloated software, making a UI more dense in time is more impactful than just squeezing more stuff onto each screen. That’s why Bloomberg’s Terminal is still such a dominant tool in the financial analysis space; it loads data almost instantaneously. A skilled Terminal user can navigate between dozens of charts and graphs in milliseconds. There are plenty of ways to cram tons of financial data into a table, but loading it with no latency is Terminal’s real superpower.</p>
<p>But say you’ve squeezed every second out of the loading times of your app. What next? There are some things that just can’t be sped up: you can’t change a user’s internet connection speed, or the computing speed of their CPU. Some operations, like uploading a file, waiting for a customer support response, or processing a payment, involve complex systems with unpredictable variables. In these cases, instead of changing the amount of time between tasks, you can change the <strong>perception</strong> of that time:</p>
<ul>
<li>Actions <strong>less than 100 milliseconds</strong> apart will feel simultaneous. If you tap on an icon and, 100ms later, a menu appears, it feels like no time at all passed between the two actions. So, if there’s an animation between the two actions —&nbsp;the menu slides in, for example —&nbsp;the illusion of simultaneity might be broken. For the smallest temporal spaces, animations and transitions can make the app feel <em>slower</em>.<sup><a href="#fn3" id="fnref3">3</a></sup></li>
<li>Between <strong>100 milliseconds and 1 second</strong>, the connection between two actions is broken. If you tap on a link and there’s no change for a second, doubt creeps in: did you actually tap on anything? Is the app broken? Is your internet working? Animations and transitions can bridge this perceptual gap. Visual cues in these spaces make the UI feel more dense in time.</li>
<li>Gaps between <strong>1 and 10 seconds</strong> can’t be bridged with animations alone; research<sup><a href="#fn4" id="fnref4">4</a></sup> shows that users are most likely to abandon a page within the first 10 seconds. This means that if two actions are far enough apart, a user will leave the page instead of waiting for the second action. If you can’t decrease the time between these actions, show an indeterminate loading indicator —&nbsp;a small animation that tells the user that the system is operating normally.</li>
<li>Gaps between <strong>10 seconds and 1 minute</strong> are even harder to fill. After seeing an indeterminate loader for more than 10 seconds, a user is likely to see it as static, not dynamic, and start to assume that the page isn’t working as expected. Instead, you can use a determinate loading indicator —&nbsp;like a larger progress bar —&nbsp;that clearly indicates how much time is left until the next action happens. In fact, the right design can make the waiting time seem shorter than it actually is; the backwards-moving stripes that featured prominently in Apple’s “Aqua” design system made waiting times seem 11% shorter.<sup><a href="#fn5" id="fnref5">5</a></sup></li>
<li>For gaps <strong>longer than 1 minute,</strong> it’s best to let the user leave the page (or otherwise do something else), then notify them when the next action has occurred. Blocking someone from doing anything useful for longer than a minute creates frustration. Plus, long, complex processes are also susceptible to error, which can compound the frustration.</li>
</ul>
<p>In the end, though, making a UI dense in time and space is just a means to an end. No UI is valuable because of the way it looks. Interfaces are valuable in the outcomes they enable —&nbsp;whether directly associated with some dollar value, in the case of business software, or tied to some intangible value like entertainment or education.</p>
<p>So what is density really about, then? It’s about providing the highest value outcomes in the smallest amount of time, space, pixels, and ink.</p>
<h2 id="density-in-value">Density in value</h2>
<p>Here’s an example of how value density is manifested: a common suggestion for any form-based interface is to break long forms into smaller chunks, then put those chunks together in a wizard-type interface that saves your progress as you go. That’s because there’s no value in a partly-filled-in-form; putting all the questions on a single page might look more visually dense, but if it takes longer to fill out, many users won’t submit it at all.</p>
<figure data-type="image"><img src="https://matthewstrom.com/images/ui-density-11.jpg" alt="This form is broken up into multiple parts, with clear errors and instructions for resolution." loading="lazy"><figcaption>This form is broken up into multiple parts, with clear errors and instructions for resolution.</figcaption></figure>
<p>Making it possible for users to get to the end of a form with fewer errors might require the design to take up more space. It might require more steps, and take more time. But if the tradeoffs in visual and temporal density make the outcome more valuable —&nbsp;either by increasing submission rate or making the effort more worth the user’s time — then we’ve increased the overall value density.</p>
<p>Likewise, if we can increase the visual and temporal density by making the form more compact, load faster, and less error-prone, <em>without</em> subtracting value to the user or the business, then that’s an overall increase in density.</p>
<p>Channeling Tufte, we should try to increase value density as much as possible.</p>
<p>Solving this optimization problem can have some counterintuitive results. When the internet was young, companies like Craigslist created value density by aggregating and curating information and displaying it in pages of links. Companies like Yahoo and Altavista made it possible to search for that information, but still put aggregation at the fore. Google took a radically different approach: use information gleaned by the internet’s long chains of linked lists to power a search box. Information was aggregating itself; a single text input was all users needed to access the entire web.</p>
</article><article>
<p>The UI was much less visually dense, but more value-dense by orders of magnitude. The results speak for themselves: Google went from a $23B valuation in 2004 to being worth over $2T today —&nbsp;closing in on a 100x increase. Yahoo went from being worth $125B in 2000 to being sold for $4.8B —&nbsp;less than 3% of its peak value.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Designing for UI density goes beyond the visual aspects of an interface. It includes all the implicit and explicit design decisions we make, and all the information we choose to show on the screen. It includes all time and the actions a user takes to get something valuable out of the software.</p>
<p>So, finally, a concrete definition of UI density: <strong>UI density is the value a user gets from the interface divided by the time and space the interface occupies.</strong></p>
<p>Speed, usability, consistency, predictability, information richness, and functionality all play an important role in this equation. By taking account of all these aspects, we can understand why some interfaces succeed and others fail. And by designing for density, we can help people get more value out of the software we build.</p>
<hr>
<section>
<p>Footnotes &amp; References</p>
<ol>
<li id="fn1"><p>This is a very unscientific statement based on a poll of 20 of my coworkers. Repeatability is questionable. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>The provenance of the chart is interesting. Not much is known about the original designer, Charles Ibry; but what we do know points to even earlier iterations of the design. If you’re interested, read <a href="https://sandrarendgen.wordpress.com/2019/03/15/data-trails-from-paris-with-love/" target="_blank" rel="noopener">Sandra Rendgen’s fascinating historory of the train schedule</a>. <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>I have no scientific backing for this claim, but I believe it’s because a typical blink occurs in 100ms. When we blink, our brains fill in the gap with the last thing we saw, so we don’t notice the blink. That’s is why we don’t notice the gap between two actions that are less than 100ms apart. You can read more about this effect here: <a href="https://www.sciencedirect.com/science/article/pii/S0960982209011105" target="_blank" rel="noopener">Visual Perception: Saccadic Omission — Suppression or Temporal Masking?</a> <a href="#fnref3">↩︎</a></p>
</li>
<li id="fn4"><p>Nielsen, Jakob. “How Long Do Users Stay on Web Pages?” Nielsen Norman Group, 11 Sept. 2011, <a href="https://www.nngroup.com/articles/how-long-do-users-stay-on-web-pages/" target="_blank" rel="noopener">https://www.nngroup.com/articles/how-long-do-users-stay-on-web-pages/</a> <a href="#fnref4">↩︎</a></p>
</li>
<li id="fn5"><p>Harrison, Chris, Zhiquan Yeo, and Scott E. Hudson. “Faster Progress Bars: Manipulating Perceived Duration with Visual Augmentations.” Carnegie Mellon University, 2010, <a href="https://www.chrisharrison.net/projects/progressbars2/ProgressBarsHarrison.pdf" target="_blank" rel="noopener">https://www.chrisharrison.net/projects/progressbars2/ProgressBarsHarrison.pdf</a> <a href="#fnref5">↩︎</a></p>
</li>
</ol>
</section>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We created the first open source implementation of Meta's TestGen–LLM (105 pts)]]></title>
            <link>https://www.codium.ai/blog/we-created-the-first-open-source-implementation-of-metas-testgen-llm/</link>
            <guid>40426995</guid>
            <pubDate>Tue, 21 May 2024 11:37:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.codium.ai/blog/we-created-the-first-open-source-implementation-of-metas-testgen-llm/">https://www.codium.ai/blog/we-created-the-first-open-source-implementation-of-metas-testgen-llm/</a>, See on <a href="https://news.ycombinator.com/item?id=40426995">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<p><img width="690" height="553" src="https://www.codium.ai/wp-content/uploads/2024/05/img-ci-agent-1-690x553.jpg" alt="" decoding="async" fetchpriority="high" srcset="https://www.codium.ai/wp-content/uploads/2024/05/img-ci-agent-1-690x553.jpg 690w, https://www.codium.ai/wp-content/uploads/2024/05/img-ci-agent-1-300x240.jpg 300w, https://www.codium.ai/wp-content/uploads/2024/05/img-ci-agent-1-768x615.jpg 768w, https://www.codium.ai/wp-content/uploads/2024/05/img-ci-agent-1.jpg 924w" sizes="(max-width: 690px) 100vw, 690px"> </p>
<p>In February, Meta researchers published a paper titled <a href="https://arxiv.org/abs/2402.09171" target="_blank" rel="noopener">Automated Unit Test Improvement using Large Language Models at Meta</a>, which introduces a tool they called TestGen-LLM. The fully automated approach to increasing test coverage “with guaranteed assurances for improvement over the existing code base” <a href="https://news.ycombinator.com/item?id=39486717" target="_blank" rel="noopener">created waves</a> in the software engineering world.</p>
<p>Meta didn’t release the TestGen-LLM code, so we decided to implement it as part of our <a href="https://github.com/Codium-ai/cover-agent" target="_blank" rel="noopener">Cover-Agent</a> open-source and we’re releasing it today!</p>
<p>I’ll share some information here on how we went about implementing it, share some of our findings and outline the challenges we encountered when actually using TestGen-LLM with real-world codebases.</p>
<h2>Automated Test Generation: Baseline Criteria</h2>
<p>Automated test generation using Generative AI is nothing new. Most LLMs that are competent at generating code, such as ChatGPT, Gemini, and Code Llama, are capable of generating tests. The most common pitfall that developers run into when generating tests with LLMs is that most generated tests don’t even work and many don’t add value (e.g. they test the same functionality already covered by other tests).</p>
<p>To overcome this challenge (specifically, for regression unit tests) the TestGen-LLM authors came up with the following criteria:</p>
<ol>
<li>Does the test compile and run properly?</li>
<li>Does the test increase code coverage?</li>
</ol>
<p>Without answering these two fundamental questions, arguably, there’s no point in accepting or analyzing the generated test provided to us by the LLM.<br>
Once we’ve validated that the tests are capable of running correctly and that they increase the coverage of our component under test, we can start to investigate (in a manual review):</p>
<ol>
<li>How well is the test written?</li>
<li>How much value does it actually add? (We all know that sometimes Code Coverage could be a proxy or even vanity metric)</li>
<li>Does it meet any additional requirements that we may have?</li>
</ol>
<h2>Approach and reported results</h2>
<p>TestGen-LLM (and Cover-Agent) run completely headless (well, kind of; we will discuss this later).</p>
<figure id="attachment_6831" aria-describedby="caption-attachment-6831"><img decoding="async" src="https://www.codium.ai/wp-content/uploads/2024/05/img-testgen-llm-paper.jpg" alt="TestGen-LLM paper" width="750" srcset="https://www.codium.ai/wp-content/uploads/2024/05/img-testgen-llm-paper.jpg 750w, https://www.codium.ai/wp-content/uploads/2024/05/img-testgen-llm-paper-300x81.jpg 300w, https://www.codium.ai/wp-content/uploads/2024/05/img-testgen-llm-paper-690x186.jpg 690w" sizes="(max-width: 750px) 100vw, 750px"><figcaption id="caption-attachment-6831">From TestGen-LLM paper</figcaption></figure>
<p>First, TestGen-LLM generates a bunch of tests, then it filters out those that don’t build/run and drops any that don’t pass, and finally, it discards those that don’t increase the code coverage. In highly controlled cases, the ratio of generated tests to those that pass all of the steps is 1:4, and in real-world scenarios, Meta’s authors report a 1:20 ratio.</p>
<p>Following the automated process, Meta had a human reviewer accept or reject tests. The authors reported an average acceptance ratio of 1:2, with a 73% acceptance rate in their best reported cases.</p>
<p>It is important to note that the TestGen-LLM tool, as described in the paper, generates on each run a single test that is added to an existing test suite, written previously by a professional developer. Moreover, it doesn’t necessarily generate tests for any given test suite.</p>
<p><strong>From the paper:</strong> “In total, over the three test-a-thons, 196 test classes were successfully improved, while the TestGen-LLM tool was applied to a total of 1,979 test classes. TestGen-LLM was therefore able to automatically improve approximately 10% of the test classes to which it was applied.”</p>
<figure id="attachment_6833" aria-describedby="caption-attachment-6833"><img decoding="async" src="https://www.codium.ai/wp-content/uploads/2024/05/img-cover-agent-flow.jpg" alt="Cover-Agent" width="750" srcset="https://www.codium.ai/wp-content/uploads/2024/05/img-cover-agent-flow.jpg 750w, https://www.codium.ai/wp-content/uploads/2024/05/img-cover-agent-flow-300x162.jpg 300w, https://www.codium.ai/wp-content/uploads/2024/05/img-cover-agent-flow-690x372.jpg 690w" sizes="(max-width: 750px) 100vw, 750px"><figcaption id="caption-attachment-6833">Cover-Agent v0.1 flow</figcaption></figure>
<p><strong>Cover-Agent v0.1 is implemented as follows:</strong></p>
<ol>
<li>Receive the following user inputs:
<ol>
<li>Source File for code under test</li>
<li>Existing Test Suite to enhance</li>
<li>Coverage Report</li>
<li>The command for building and running test suite</li>
<li>Code coverage target and maximum iterations to run</li>
<li>Additional context and prompting options</li>
</ol>
</li>
<li>Generate more tests in the same style</li>
<li>Validate those tests using your runtime environment
<ol>
<li>Do they build and pass?</li>
</ol>
</li>
<li>Ensure that the tests add value by reviewing metrics such as increased code coverage</li>
<li>Update existing Test Suite and Coverage Report</li>
<li>Repeat until code reaches criteria: either code coverage threshold met, or reached the maximum number of iterations</li>
</ol>
<p><iframe src="https://www.youtube.com/embed/fIYkSEJ4eqE" frameborder="0" scrolling="no" allowfullscreen="allowfullscreen"><span data-mce-type="bookmark" style="display: inline-block; width: 0px; overflow: hidden; line-height: 0;" class="mce_SELRES_start">﻿</span></iframe></p>
<h2>Challenges we encountered when implementing and reviewing TestGen-LLM</h2>
<p>As we worked on putting the TestGen-LLM paper into practice, we ran into some surprising challenges.</p>
<p>The examples presented in the paper mention using <a href="https://kotlinlang.org/" target="_blank" rel="noopener">Kotlin</a> for writing tests – a language that doesn’t use significant whitespace. With languages like&nbsp; Python on the other hand, tabs and spaces are not only important but a requirement for the parsing engine. Less sophisticated models, such as GPT 3.5, won’t return code that is consistently indented properly, even when explicitly prompted. An example of where this causes issues is a test class written in Python that requires each test function to be indented. We had to account for this throughout our development lifecycle which added more complexity around pre-processing libraries. There is still plenty to improve on in order to make Cover-Agent robust in scenarios like this.</p>
<figure id="attachment_6832" aria-describedby="caption-attachment-6832"><img decoding="async" src="https://www.codium.ai/wp-content/uploads/2024/05/img-prompt-table.jpg" alt="Prompt Table" width="750" srcset="https://www.codium.ai/wp-content/uploads/2024/05/img-prompt-table.jpg 750w, https://www.codium.ai/wp-content/uploads/2024/05/img-prompt-table-300x104.jpg 300w, https://www.codium.ai/wp-content/uploads/2024/05/img-prompt-table-690x240.jpg 690w" sizes="(max-width: 750px) 100vw, 750px"><figcaption id="caption-attachment-6832">From TestGen-LLM paper. Original prompts suggested in TestGen-LLM.</figcaption></figure>
<p>After seeing the special test requirements and exceptions we encountered during our trials, we decided to give the user the ability to provide additional input or instructions to prompt the LLM as part of the Cover-Agent flow. The `–additional-instructions` option allows developers to provide any extra information that’s specific to their project, empowering them to customize Cover-Agent. These instructions can be used, for example, to steer Cover-Agent to create a rich set of tests with meaningful edge cases.</p>
<p>Concurring with the general trend of Retrieval-Augmented Generation (RAG) becoming more pervasive in AI based applications, we identified that having more context to go along with unit test generation enables higher quality tests and a higher passing rate. We’ve provided the `–included-files` option to users who want to manually add additional libraries or text-based design documents as context for the LLM to enhance the test generation process.</p>
<p>Complex code that required multiple iterations presented another challenge to the LLMs. As the failed (or non-value added) tests were generated, we started to notice a pattern where the same non-accepted tests were repeatedly suggested in later iterations. To combat this we added a “Failed Tests” section to the prompt to deliver that feedback to the LLM and ensure it generated unique tests and never repeated tests that we deemed unusable (i.e. broken or lack of coverage increase).</p>
<p>Another challenge that came up throughout this process was the inability to add library imports when extending an existing test suite. Developers can sometimes be myopic in their test generation process, only using a single approach to testing frameworks. In addition to many different mocking frameworks, other libraries can help with achieving test coverage. Since the TestGen-LLM paper (and Cover-Agent) are intended to extend existing test suites, the ability to completely restructure the whole test class is out of scope. This is, in my opinion, a limitation of test extension versus test generation and something we plan on addressing in future iterations.</p>
<p>It’s important to make the distinction that in TestGen-LLM’s approach, each test required a manual review from the developer before the next test is suggested. In Cover-Agent on the other hand, we generate, validate, and propose as many tests as possible until achieving the coverage requirement (or stopping at the max iterations), without requiring manual intervention throughout the process. We leverage AI to run in the background, creating an unobtrusive approach to automatic test generation that allows the developer to review the entire test suite once the process has completed.</p>
<h2>Conclusion and what’s next</h2>
<p>While many, including myself, are excited about the TestGen-LLM paper and tool, in this post we have shared its limitations. I believe that we are still in the era of AI assistants and not AI teammates who run fully automated workflows.</p>
<p>At the same time, well-engineered flows, which we plan to develop and share here in <a href="https://github.com/Codium-ai/cover-agent" target="_blank" rel="noopener">Cover-Agent</a>, can help us developers automatically generate test candidates, and increase code coverage in a fraction of the time.</p>
<p>We intend to continue developing and integrating cutting-edge methods related to the test generation domain into the Cover-Agent open-source repo.<br>
We encourage anyone interested in generative AI for testing to collaborate and help extend the capabilities of Cover Agent, and we hope to inspire researchers to leverage this open-source tool to explore new test-generation techniques.</p>
<p>In the open-source Cover-Agent repo on GitHub we’ve added a development <a href="https://github.com/Codium-ai/cover-agent" target="_blank" rel="noopener">roadmap</a>. We would love to see you contributing to the repo according to the roadmap or according to your own ideas!</p>
<p>Our vision for Cover-Agent is that in the future it will run automatically for every pre/post-pull request and automatically suggest regression test enhancements that have been validated to work and increase code coverage. We envision that Cover-Agent will automatically scan your codebase, and open PRs with test suites for you.</p>
<p>Let’s leverage AI to help us deal more efficiently with the tasks we don’t like doing!</p>
<p><strong>P.S.</strong></p>
<ol>
<li>We are still looking for a good benchmark for tools like this. Do you know of one? We think it is critical for further development and research.</li>
<li>Check out our <a href="https://twitter.com/talrid23/status/1760351642237477109" target="_blank" rel="noopener">AlphaCodium work</a> for (a) further reading on “Flow Engineering”, as well as an example of (b) a competitive programming benchmark, and (c) a well-designed dataset called CodeContests.</li>
</ol>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[One dead as London-Singapore flight hit by turbulence (104 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c8889d7x8j4o</link>
            <guid>40426801</guid>
            <pubDate>Tue, 21 May 2024 11:08:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c8889d7x8j4o">https://www.bbc.com/news/articles/c8889d7x8j4o</a>, See on <a href="https://news.ycombinator.com/item?id=40426801">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Going Dark: The war on encryption is on the rise (346 pts)]]></title>
            <link>https://mullvad.net/en/why-privacy-matters/going-dark</link>
            <guid>40426701</guid>
            <pubDate>Tue, 21 May 2024 10:54:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mullvad.net/en/why-privacy-matters/going-dark">https://mullvad.net/en/why-privacy-matters/going-dark</a>, See on <a href="https://news.ycombinator.com/item?id=40426701">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><header><p> <h2>State mass surveillance</h2></p> <p>Under the slogan ‘Think of the children’, the European Commission tried to introduce total surveillance of all EU citizens. When the scandal was revealed, it turned out that American tech companies and security services had been involved in the bill, generally known as ‘Chat Control’ – and that the whole thing had been directed by completely different interests. Now comes the next attempt. New battering rams have been brought out with the ‘Going Dark’ initiative. But the ambition is the same: to install state spyware on every European cell phone and computers.</p></header> <p data-svelte-h="svelte-t1esob"><img src="https://mullvad.net/images/previous-term.jpg" alt="A satirical cartoon of a man sitting in front of a computer, symbolizing mass surveillance in the EU."></p> <p data-svelte-h="svelte-1i3n5uv">On May 11, 2022, EU Commissioner Ylva Johansson presented a legislative
proposal under the official name ”<a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=COM%3A2022%3A209%3AFIN" rel="nofollow">Regulation of the European Parliament
and of the Council laying down rules to prevent and combat child sexual
abuse</a>.”</p> <p data-svelte-h="svelte-34ak2p">Ylva Johansson made a point of this being her bill: it was she who had
devised it – no one else – and if it had not been for her, Europe’s
justice system would “go blind” in the hunt to track sexual abuse of
children. In Ylva’s world, the EU would “turn into a pedophiles’
paradise” if she didn’t get her way. It was easy to marvel at how, on
almost every occasion, Ylva Johansson was keen to point out that this
was her proposal. A touch of narcissism? Maybe. But perhaps there was
something else behind this self-centered proclamation. Because it would
eventually emerge that in fact Ylva Johansson was not alone behind the
scenes. Right from the start, there were others involved – actors who
would benefit from the bill being passed, but who preferred it not to be
known that they were involved in designing it.</p> <p data-svelte-h="svelte-hu60su">The rhetoric was clear from day one: it was all about the children, and
when it comes to children, there’s nothing we can’t imagine doing to
keep them safe. So Ylva Johansson put forward a proposal that meant
total surveillance of all EU citizens and as soon as someone opposed it,
she pulled out the think-of-the-children card. But those who could see
through the bluff quickly gave the proposal (those parts of the bill
that dealt with internet surveillance) a shorter and more appropriate
name: Chat Control.</p> <p data-svelte-h="svelte-1wqsbbm">In brief, Chat Control essentially meant that the communications of
every EU citizen would be monitored. Every call, every message and every
chat, all the emails, photos, and videos saved in cloud services – all
of it would be filtered in real time via artificial intelligence and
then checked in a newly established EU center, in close cooperation with
Europol.</p> <p data-svelte-h="svelte-lj4ut6">Since the bill was in violation of the European Convention on Human
Rights, the EU Charter and the UN Declaration of Human Rights, Chat
Control was rejected by one legislative body after another. Both <a href="https://www.patrick-breyer.de/wp-content/uploads/2023/05/st08787.en23-leak.pdf" rel="nofollow">the
Council of
Ministers</a>
and the <a href="https://www.patrick-breyer.de/wp-content/uploads/2022/11/220128_Opinion-II-HOME-Child-Sexual-Abuse-annotated-fin.pdf" rel="nofollow">European Commission’s own legal
service</a>
warned against the proposal, as did <a href="https://edps.europa.eu/system/files/2022-07/22-07-28_edpb-edps-joint-opinion-csam_en.pdf" rel="nofollow">the European Parliament’s Data
Protection
Board</a>.
The UN Human Rights Council described Chat Control as incompatible with
fundamental human rights and stated that the proposal would lead to
<a href="https://www.patrick-breyer.de/en/un-human-rights-commissioner-warns-against-chat-control/" rel="nofollow">mass surveillance and
self-censorship</a>.
Former judges at the European Court of Justice said that <a href="https://www.patrick-breyer.de/wp-content/uploads/2021/03/Legal-Opinion-Screening-for-child-pornography-2021-03-04.pdf" rel="nofollow">the proposal
was in breach of the EU Charter of
Rights</a>
and 465 researchers joined forces to <a href="https://edri.org/wp-content/uploads/2023/07/Open-Letter-CSA-Scientific-community.pdf" rel="nofollow">warn of the
consequences</a>.</p> <p data-svelte-h="svelte-1mcqafc">Faced with massive criticism, Ylva Johansson defended herself. According
to her, everyone else had misunderstood the bill. Chat Control was
certainly not about mass surveillance and everyone making that claim was
simply out to discredit her.</p> <h3 data-svelte-h="svelte-4aqjfz">Chat Control – total monitoring of all EU citizens.</h3> <p>Chat Control is sometimes also called Chat Control 2.0, since existing
legislation already makes it possible for tech companies such as Google
and Meta to scan their users’ accounts for child pornography material.
The fact that there was already a law that allowed tech companies to
scan for illegal content – if they chose to – was something Ylva
Johansson was not slow to mention. She explained that her draft bill was
nothing but an extension of the <a href="https://mullvad.net/en/blog/the-european-commission-does-not-understand-what-is-written-in-its-own-chat-control-bill">scanning that had already been going on
for ten
years</a>.
She also referred to the existing legislation when she said that the EU
would become a free zone for pedophiles unless her bill went through –
as that legislation would expire in the summer of 2024.</p> <p data-svelte-h="svelte-1s6fzw2">Time and time again Ylva Johansson was proven wrong by journalists and
experts. In fact, nothing prevented the EU from extending the existing
law, rather than introducing a new one. And above all: Ylva’s bill was
anything but an extension. The differences between the current law and
the proposed legislation were extreme. In Ylva Johansson’s EU, scanning
would not be voluntary. All messaging services (including encrypted
services such as Signal) would be covered by the law and would be forced
to scan their users’ images, videos and conversations. That would be a
big concern for all those who don’t use Meta or Google to converse
because they are in need of secure communication methods. In other
words, political opponents, whistleblowers, journalists and their
sources, vulnerable people living under secret identities and others,
not to mention people with trade secrets, and those in possession of
sensitive information important for national security. For example, the
European Commission itself uses Signal. Demanding government
transparency (either through so-called backdoors or scanning on the
computer or phone) would open a Pandora’s box to countries with
authoritarian inclinations (and <a href="https://www.politico.eu/article/pegasus-use-5-eu-countries-nso-group-admit/" rel="nofollow">five EU countries have already been
caught</a>
using spyware to monitor political opponents) and would leave the door
wide open for criminals to exploit. But it was not only this that
separated the existing legislation from the draft bill that the European
Commission wanted to introduce.</p> <p data-svelte-h="svelte-cr1ja7">The previous legislation had only allowed scanning for material that had
previously been stamped and registered as child pornography material.
Now, AI would be used to find ‘new material’ and would also look for
grooming attempts. Quite obviously, Chat Control would therefore send
every other citizen of the EU straight into the filtering system.
Holiday photos from the beach, nude photos sent between partners, dirty
text messages – all the things that no AI system can distinguish
between would risk getting caught in a filter that would inevitably
drown any new EU center with endless digital heaps of evidence to
review. Is this a holiday photo of a child or child pornography? Are
these skimpily dressed youngsters 18 or 14? Is this a dirty text message
from a wife to a husband or a grooming attempt? But above all, Chat
Control would mean a tool that could be used to scan for completely
different things.</p> <p data-svelte-h="svelte-j80wc0">When Ylva Johansson was asked whether it would be possible to
communicate safely even after her bill was introduced, she answered
“Yes.” And a whole world of experts asked “How?” Ylva replied that she
had something nobody else had. A digital sniffer dog that could smell
encrypted communication without looking at the content. A sniffer dog
that only reacted to child pornography content – never anything else.</p> <p data-svelte-h="svelte-185mkhq">A group of experts tried to hammer the message home: either encrypted
communication is encrypted (so-called end-to-end encryption, which only
the sender and the recipient can see) or it’s not encrypted. There’s no
‘seeing the content’ without reading it. But Ylva stood by her claim.
She came back to the same argument over and over again. She avoided
answering the questions (she obviously didn’t understand how the
technology worked) but instead turned the direction of the discussion,
saying, for example, that a court order would be required to carry out
scanning, which in itself was deliberately misleading. Firstly, her
scanning would not require an order from a court – it could be one from
another judicial body. And secondly, the key issue was that judicial
body making a decision that would force messaging services to monitor
all their users. So in other words, when Ylva proclaimed “it requires a
court order,” she wasn’t talking about courts and their decisions to
monitor people such as suspected pedophiles. She was talking about how a
service would be forced to permit surveillance. What was required for a
service to be subject to surveillance? Merely that there was a
possibility to use the service to spread child pornography or to groom
children. Which of course means every messaging service on the planet.</p> <p data-svelte-h="svelte-q0uosi">As soon as Ylva Johansson was shown to be in the wrong, she shifted her
focus. But in the end, she always came back to the final refuge: it’s
all about the children. She related anecdotes and referred to figures
that pointed to an exponential increase in child pornography material on
Facebook, for example – even though Facebook itself stated that <a href="https://about.fb.com/news/2021/02/preventing-child-exploitation-on-our-apps/" rel="nofollow">90
percent of all reports come from material previously
distributed</a>.</p> <p data-svelte-h="svelte-1ajxwrs">The European Commission, led by Ylva Johansson, received criticism from
all directions. Police chiefs pointed out that most of the material they
receive today involves <a href="https://www.fokus.se/veckans-fokus/chat-control-sa-ska-techjattarna-skanna-allt-du-skickar/" rel="nofollow">teenagers sending pictures to each
other</a>
and that such reports risk leading the police in the wrong direction.
Scanning tests carried out by European police on existing material
showed that <a href="https://2021.fedpol.report/de/fedpol-in-zahlen/kampf-gegen-padokriminalitat/" rel="nofollow">80-90 percent of all hits were false
positives</a>.
Now, moreover, ‘new material’ would be scanned – which would obviously
mean an impossible administrative burden merely to distinguish between
illegal images and holiday pictures from family days on the beach. The
error rate would clearly be approaching 100 percent. For a European
<a href="https://edri.org/wp-content/uploads/2022/06/European-Commission-must-uphold-privacy-security-and-free-expression-by-withdrawing-new-law.pdf" rel="nofollow">justice system that even today is unable to follow up all the
tips</a>
it receives, this would be devastating. And criminals would, of course,
turn to illegal messaging services. No children would be helped. At the
same time, every EU citizen would have spyware installed on their
phones.</p> <p data-svelte-h="svelte-uesdev">How did Ylva Johansson deal with this information? Not at all. Instead,
like a scratched record, she continued urging everyone to “think of the
children.” She also ordered a survey that said 80 percent of the EU
population supports Chat Control. The problem? The European Commission
used its Eurobarometer series of public opinion surveys in way that
opened it to accusations of blurring the line between research and
propaganda. When asked to comment on the Chat Control survey, the Max
Planck Institute for the Study of Societies concluded that it had a
<a href="https://www.patrick-breyer.de/en/manipulative-eu-opinion-poll-no-justification-for-indiscriminate-chat-control/" rel="nofollow">political agenda and consisted of questions that were
biased</a>
to support the Commission’s plans.</p> <p data-svelte-h="svelte-1fle1l8">Ylva Johansson was employing blatant deception. She used incorrect
figures and biased surveys. In interviews, she was populist and evasive.
But she was forced to resort to these methods. Because it was never
about the children.</p> <h3 data-svelte-h="svelte-1rjbvqn">American tech companies and security services behind the draft bill</h3> <p data-svelte-h="svelte-msqxs2">In September 2023, a major investigative article was published by three
journalists: Giacomo Zandonini, Apostolis Fotiadis, and Luděk Stavinoha.
After seven months of trying to get the European Commission to release
public documents, they finally obtained a piece of material that allowed
them to start putting together the puzzle. The puzzle that <a href="https://balkaninsight.com/2023/09/25/who-benefits-inside-the-eus-fight-over-scanning-for-child-sex-content/" rel="nofollow">revealed the
true stakes behind Chat
Control</a>.
The article, which was published in several European newspapers,
included a letter in which Ylva Johansson wrote to Julie Cordua, CEO of
the American company Thorn: “We have shared many moments on the journey
to this proposal. Now I am looking to you to help make sure that this
launch is a successful one.”</p> <p data-svelte-h="svelte-13zlmxr">Thorn is an American company, formed by actor Ashton Kutcher, which
develops tools that scan for child pornography material. Thorn had sold
software worth millions of dollars to the U.S. Department of Homeland
Security. Ashton Kutcher himself had held video conferences with
European Commission President Ursula von der Leyen, and had given
lectures in the EU on how new technologies can scan encrypted content
without looking at it. The picture of Ylva Johansson’s digital sniffer
dog suddenly became clear.</p> <p data-svelte-h="svelte-1ltneaj">For several years Kutcher lobbied the European Commission (until he was
forced to resign as chairman of Thorn’s board after defending his
acting colleague Danny Masterson when he was convicted of rape). He held
meetings with others at the European Commission and had an extra close
relationship with the Commission’s Eva Kaili (until she <a href="https://www.politico.eu/european-parliament-qatargate-corruption-scandal-updates/" rel="nofollow">was convicted
of
bribery</a>).</p> <p data-svelte-h="svelte-1y1yu4z">So here was an American company in direct contact with the European
Commission. An American company that just happened to sell the
technology that could be used if Chat Control was introduced. In
addition, it was all based on a false premise. The technology Kutcher
and Johansson talked about did not exist. Expert after expert <a href="https://blog.cryptographyengineering.com/2023/03/23/remarks-on-chat-control/" rel="nofollow">condemned
their talk of sniffer
dogs</a>.</p> <p data-svelte-h="svelte-154r4qk">And here’s yet another seedy aspect to this scandal: in the EU
transparency register, Thorn was registered as a charitable organization
– despite selling the technology they were lecturing about in the EU.
The trick of disguising organizations and corporations as charities
would turn out to be a recurring motif.</p> <p data-svelte-h="svelte-14b9drm">Since the draft Chat Control bill was presented, Ylva Johansson has
constantly referred to children’s rights organizations that support her
proposal. She has worked with them in a PR context, as a way to show how
Chat Control has the support of independent, nonprofit organizations
that care about children. A central organization in this work has been
the WeProtect Global Alliance. When Zandonini, Fotiadis, and Stavinoha
published their article, it turned out that the European Commission had
been involved in founding this organization, and that it included
representatives from both tech companies and security services in
different countries. Ylva Johansson’s colleague in the European
Commission, Labrador Jimenez, was on the Board of Directors of
WeProtect, together with Thorn’s CEO Julie Cordua, representatives of
Interpol, and government officials from the US and the UK (the latter
simultaneously pursuing its own monitoring legislation, also using
children as the battering ram). Thorn had put a great deal of money into
WeProtect. The European Commission had contributed one million euros. In
other words, it wasn’t children’s rights organizations that were
supporting Ylva Johansson. It was lobbying organizations set up by the
European Commission to get the bill through.</p> <p data-svelte-h="svelte-3weqe2">The Board of Directors of WeProtect also included representatives from
the Oak Foundation, who, in addition to their involvement in WeProtect,
had also been involved in setting up ECLAG (another charity that
supported the Chat Control proposal). ECLAG was launched just a few
weeks after Ylva Johansson’s draft bill was presented, and Thorn was
also represented on this organization’s board. And there was still
another organization: the Brave Movement, an organization formed a month
before the proposed Chat Control bill was introduced. Brave was launched
with $10 million from the Oak Foundation and a strategy paper
discovered by the journalists stated that “once the EU Survivors
taskforce is established and we are clear on the mobilized survivors, we
will establish a list pairing responsible survivors with Members of the
European Parliament – we will ‘divide and conquer’ the MEPs by
deploying in priority survivors from MEPs’ countries of origin.”</p> <p data-svelte-h="svelte-blzgs8">The Oak Foundation also appeared in a <a href="https://theintercept.com/2023/10/01/apple-encryption-iphone-heat-initiative/" rel="nofollow">article carried out by the
Intercept</a>.
In 2023, an American organization called the Heat Initiative was formed.
On paper, they were a “new child safety group” and the first thing they
did was campaign for Apple to “detect, report, and remove” child
pornography material from iCloud. Apple responded that this would be
something that criminals would be able to exploit and that it could also
lead to a “potential for a slippery slope of unintended consequences.
Scanning for one type of content, for instance, opens the door for bulk
surveillance.”</p> <p data-svelte-h="svelte-1iezc36">The Heat Initiative did not like this answer and fought back with
anti-Apple propaganda on large advertising billboards in American cities
under the theme of ‘think of the children.’ But who was behind the Heat
Initiative, besides the Oak Foundation? Heat was led by a former vice
president at Thorn. The Intercept article also referred to <a href="https://www.engadget.com/2019-05-31-sex-lies-and-surveillance-fosta-privacy.html" rel="nofollow">the
fact</a>
that Thorn was working with Palantir, the big-data company that <a href="https://theintercept.com/2017/02/22/how-peter-thiels-palantir-helped-the-nsa-spy-on-the-whole-world/" rel="nofollow">helped
the NSA mass-monitor the whole
world</a>
and was involved in the <a href="https://www.nytimes.com/2018/03/27/us/cambridge-analytica-palantir.html" rel="nofollow">Cambridge Analytica scandal where Facebook
users’ private messages and
data</a>
were used to influence the presidential election on behalf of Donald
Trump in 2016.</p> <p data-svelte-h="svelte-zfl98n">In other words, the European Commission was involved in funding and
starting up charities with the aim of exploiting existing victims to
emotionally influence EU parliamentarians. In close cooperation with the
tech company providing the technology that would be used in the
implementation of the monitoring. Together with representatives of
non-European security services. As part of a larger apparatus, where the
same tactics were used to influence developments in the United States.</p> <p data-svelte-h="svelte-n7pf3i">At the same time, the real organizations working to counter sexual
crimes against children were wondering why the European Commission was
refusing to talk to them. In <a href="https://balkaninsight.com/2023/09/25/who-benefits-inside-the-eus-fight-over-scanning-for-child-sex-content/" rel="nofollow">the same investigative
report</a>,
Offlimits, Europe’s oldest hotline for vulnerable children, tells how
Ylva Johansson would rather go to Silicon Valley to meet companies
interested in making huge profits than talk to them.</p> <p data-svelte-h="svelte-1gns2le">The same is true of the technical experts. Matthew Green, Professor of
Cryptography at John Hopkins University, said: “In the first impact
assessment of the EU Commission there was almost no outside scientific
input and that’s really amazing since Europe has a terrific scientific
infrastructure, with the top researchers in cryptography and computer
security all over the world.”</p> <p data-svelte-h="svelte-p16k3a">However, Europol was involved in drafting the law, <a href="https://netzpolitik-org.translate.goog/2023/geheime-liste-wie-der-sicherheitsapparat-die-chatkontrolle-praegt/?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=sv&amp;_x_tr_pto=wapp" rel="nofollow">together with
security services from other
countries</a>.
In July 2022, Europol wrote that it wanted to be able to use scanning
and surveillance for purposes other than sexual offenses against
children. <a href="https://balkaninsight.com/2023/09/25/who-benefits-inside-the-eus-fight-over-scanning-for-child-sex-content/" rel="nofollow">The European Commission
responded</a>
that it understood the wish but that it had “to be realistic in terms of
what could be expected, given the many sensitivities around the
proposal.” Thorn was also <a href="https://netzpolitik-org.translate.goog/2024/verschluesselung-thorn-brachte-chatkontrolle-auch-fuer-andere-themen-ins-spiel/?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=sv&amp;_x_tr_pto=wapp" rel="nofollow">clear in understanding that the scanning
could later be used for other
purposes</a>:
“When considering regulation or legislation on encryption it should not
be done solely focusing on CSAM. Solutions for detection in encrypted
environments are much broader than one single crime,” <a href="https://www.ftm.eu/articles/ashton-kutcher-s-anti-childabuse-software-below-par" rel="nofollow">the company wrote
in one
document</a>.</p> <p data-svelte-h="svelte-go1tsn">It was later revealed that Europol was looking for <a href="https://balkaninsight.com/2023/09/29/europol-sought-unlimited-data-access-in-online-child-sexual-abuse-regulation/" rel="nofollow">unfiltered access to
the scanned
material</a>:
“All data is useful and should be passed on to law enforcement. There
should be no filtering by the [EU] Centre because even an innocent
image might contain information that could at some point be useful to
law enforcement.”</p> <h3 data-svelte-h="svelte-ksnt3d">European Parliament: “the commission wanted mass surveillance.”</h3> <p data-svelte-h="svelte-g9fw7s">So here was the European Commission, working on legislative proposals
together with a Europol that wanted access to all surveillance,
regardless of whether it contained something illegal or not – simply
because it could be useful to have. In other words, it really wasn’t
about the children.</p> <p data-svelte-h="svelte-ozl4kz">When articles were published about the EU Commission’s horrifyingly
undemocratic approach, Ylva Johansson’s office at the European
Commission responded by advertising on the platform X (formerly
Twitter). They targeted advertisements (pro Chat Control) so that
decision-makers in different countries would see them, but also so that
they would not be seen by people suspected to be strongly against the
proposal. The advertising was also targeted on the basis of religious
and political affiliation and <a href="https://www.wired.com/story/csar-chat-scan-proposal-european-commission-ads/" rel="nofollow">thus violated the EU’s own laws
regarding
micro-targeting</a>.</p> <p data-svelte-h="svelte-1qh87xe">Officials at the highest EU level thus used data collected by big tech
to try to create illegal filter bubbles designed to push through a mass
surveillance proposal. The whole thing ended with Ylva Johansson being
summoned to a hearing in the European Parliament. An almost united
European Parliament was massively critical of Ylva Johansson and her
approach. She was grilled about Thorn’s interference and about the
targeted ads and the EU Ombudsman denounced the European Commission’s
unwillingness to share public documents regarding the relationship with
Thorn (the European Commission had assumed these would be classified
because they risked undermining commercial interests …) Ylva
Johansson’s answer? “Think of the children.”</p> <p data-svelte-h="svelte-pqryii">In November 2023, the <a href="https://www.europarl.europa.eu/doceo/document/A-9-2023-0364_EN.html" rel="nofollow">European Parliament’s final judgment was
delivered</a>.
In an almost historic consensus, all the groups in the Parliament stood
together and said “No” to the bill. At the <a href="https://fortune.com/europe/2023/10/26/eu-chat-control-csam-encryption-privacy-european-commission-parliament-johansson-breyer-zarzalejos-ernst/" rel="nofollow">press conference,
representatives from the Parliament
said</a>:
“This is a slap in the face of the Commission, what we’ve tabled. The
Commission wasn’t focusing on protecting children but wanted mass
surveillance.” Patrick Breyer, who has been the most active opponent in
the EU Parliament, called it a victory for the children, adding “They
deserve an effective response and a rights-respecting response that will
hold up in court.”</p> <p data-svelte-h="svelte-1bpb699">Breyer was referring to the fact that Chat Control would most likely not
hold up in court if the bill had been passed. Just a few months later, a
<a href="https://hudoc.echr.coe.int/?i=001-230854" rel="nofollow">ruling from the European Court of
Justice</a>
ruled that authorities do not have the right to demand access to
end-to-end encrypted communications.</p> <p data-svelte-h="svelte-fxcimz">But the Chat Control proposal wasn’t completely buried just because the
European Parliament had taken a clear stance against it. In the EU, two
bodies are involved in the adoption of legislative proposals made by the
European Commission: the European Parliament and the Council of
Ministers. But while the European Parliament was extremely clear and
unified in its stance, the Council of Ministers was hopelessly unable to
reach an agreement. When new EU elections approached in summer 2024,
they had not yet managed to come to a consensus. However, the Council of
Ministers also began to hesitate about the technology. Ultimately, it
had become evident to most people that Ylva’s digital sniffer dog
didn’t exist. There was no technology that could scan communication
without looking at it. Parts of <a href="https://reclaimthenet.org/eu-officials-dodge-their-own-surveillance-law" rel="nofollow">the Council of Ministers therefore
proposed</a>
that scanning should be excluded for politicians, the police and
intelligence services, as well as anything classified as ‘professional
secrets.’ Obviously, there were politicians who were afraid that their
secrets would leak, but who had nothing against mass surveillance of the
broader population. Patrick Breyer was clear in his response: “these
people are aware that Chat Control involves unreliable and dangerous
snooping algorithms – and yet they are ready to unleash them on us
citizens.”</p> <p data-svelte-h="svelte-pxyeh5">Even Ylva Johansson finally realized that she was defeated. Did she then
go public and announce that Europe would now be blind in the hunt for
pedophiles? Of course not. She quickly and easily did what she had
previously been completely unable to do: she extended the previous
legislation.</p> <h3 data-svelte-h="svelte-1xt5fg6">New attempt at mass surveillance via the Going Dark initiative</h3> <p data-svelte-h="svelte-1g9zhsh">The fact that the European Parliament rejected Chat Control didn’t mean
that attempts to introduce mass surveillance were over. During Sweden’s
EU Presidency in spring 2023, a project called Going Dark was initiated.
The idea from the Swedish Presidency was initially that a so-called High
Level Expert Group would be launched. The task of putting together the
group went to the European Commission, which immediately removed the
‘Expert’ label. Instead of a High Level Expert Group, a High Level Group
was formed. As <a href="https://netzpolitik.org/2023/eu-beraet-ueber-going-dark-hinter-verschlossenen-tueren/" rel="nofollow">the Netzpolitik
newspaper</a>
put it: “Removing the word ‘expert’ is no small detail: special rules
apply to Expert groups, for example when it comes to transparency. Rules
that do not apply to High Level Groups.”</p> <p data-svelte-h="svelte-1rbtxom">Once again, the European Commission chose to start the preparatory work
linked to mass surveillance without allowing experts to play a serious
part in the process. When the group met for the first time, it stated
that <a href="https://data.consilium.europa.eu/doc/document/ST-8281-2023-INIT/en/pdf" rel="nofollow">the group’s purpose was to discuss
methods</a>
to achieve “access to data for effective law enforcement, based on and
guided by the inputs from the EU Member States.”</p> <p data-svelte-h="svelte-50kpr6">Some <a href="https://data.consilium.europa.eu/doc/document/ST-8281-2023-INIT/en/pdf" rel="nofollow">challenges were identified as particularly
pressing</a>:
access to encrypted material (both stored data and communication), data
storage, location data, and anonymization (including VPNs and Darknets).</p> <p data-svelte-h="svelte-160puu6">Once the group was united, <a href="https://home-affairs.ec.europa.eu/networks/high-level-group-hlg-access-data-effective-law-enforcement_en#meetings" rel="nofollow">it was divided into three working
groups</a>:
the first would work with access to data on users’ devices (computer and
mobile), the second group would focus on access to data in the services’
systems (messaging apps, for example), and the third group would discuss
access to data in transit.</p> <p data-svelte-h="svelte-5dnx7i">According to the minutes of the meeting of the Swedish Parliament’s
Committee on European Union Affairs, the group worked <a href="https://www.riksdagen.se/sv/dokument-och-lagar/dokument/eu-namndens-uppteckningar/fredagen-den-1-december_hb0a16/html/" rel="nofollow">“to present
effective recommendations for the accession of the new Commission in
2024 and for those recommendations to be
implemented.”</a></p> <p data-svelte-h="svelte-h9po41">Future legislative proposals from the European Commission could thus be
assumed to be about providing access to data on users’ devices and in
the messaging services’ systems, and to data in transit. Patrick Breyer,
who had worked hard to counter Chat Control, said the group was just an
extension of past offensives and that Going Dark was working to
<a href="https://www.patrick-breyer.de/en/leak-data-retention-and-encryption-eu-governments-going-dark-program-to-attack-citizens-rights-with-pr/" rel="nofollow">introduce illegal mass
surveillance</a>.
When he requested documents from the group’s meetings and a list of the
attendees, <a href="https://fragdenstaat.de/anfrage/june-and-november-meetings-of-the-hleg-on-access-to-data-for-effective-law-enforcement/848493/anhang/document17-participantlistfirstplenary.pdf" rel="nofollow">he received a document with the information blacked out as
if
classified</a>.
The European Commission had thus put together a working group aiming to
achieve mass surveillance of the broader population while not being
transparent about who was part of the group. It was like a scratched
record. Gone was the old excuse “think of the children”, but the goal
was the same.</p> <p data-svelte-h="svelte-1bp1ogo">However, some transparency was obtained through the Swedish Ministry of
Justice, which at Mullvad VPN’s request provided both meeting notes and
information about the Swedish representatives present at the meetings.</p> <p data-svelte-h="svelte-5ugd8h">The first Going Dark meeting was led by two people. One was Olivier
Onidi, who is Deputy Director General directly under Ylva Johansson in
the European Commission. Onidi has expressed that the “valuable” thing
about Chat Control is <a href="https://netzpolitik.org/2021/eu-commission-why-chat-control-is-so-dangerous/" rel="nofollow">“to cover all forms of communication, including
private
communication”</a>,
and he <a href="https://www.svd.se/a/15yzLQ/expertkritik-mot-omstritt-natforslag-i-eu" rel="nofollow">defended Ylva Johansson and Chat Control when he
said</a>:
“I think it’s totally unfair to point this out as a mandatory inspection
of all private communications. That’s not what you have in front of you.
This proposal is a huge improvement over the current situation.”</p> <p data-svelte-h="svelte-18pbg5h">Onidi has also been questioned for <a href="https://www.euractiv.com/wp-content/uploads/sites/2/2020/06/2020.06.10-Letter-to-Commission-Palantir.pdf" rel="nofollow">his meetings with the American
company
Palantir</a>
(notorious for its involvement in US authorities’ illegal mass
surveillance).</p> <p data-svelte-h="svelte-13jylfg">The second person who led the first Going Dark meeting was Anna-Carin
Svensson, international chief negotiator at the Swedish Justice
Department, who, according to WikiLeaks documents in 2010, allegedly
urged the US State Department and the FBI to continue with the current
informal exchange of information between the countries instead of
signing formal agreements. According to the American representatives at
the meeting, it was about <a href="https://www.aftonbladet.se/nyheter/a/m65Oqp/morkade-allt-for-riksdagen" rel="nofollow">withholding information from the Swedish
Parliament</a>:</p> <p data-svelte-h="svelte-c39wdw">“She believed that, given the Swedish Constitution’s requirement to
present matters of importance to the nation to the Swedish Parliament,
and in light of the ongoing controversy over the newly decided FRA law
[FRA, Försvarets radioanstalt, the Swedish National Defence Radio
Establishment, is a Swedish government signals agency], it will be
politically impossible for the Minister of Justice not to let the
Parliament review any data exchange agreements with the United States.
In her opinion, the publication of this could also jeopardize the
informal exchange of information,” the leaked documents said.</p> <p data-svelte-h="svelte-gq4qsx">According to the documents, Anna-Carin Svensson asked the FBI if they
could not continue to make use of the strong but informal arrangements.
When the documents leaked, Svensson denied everything and stated: “I
cannot be held responsible for how Americans express themselves.”</p> <p data-svelte-h="svelte-1d1qeq2">From the Swedish side, the Ministry of Justice was represented at the
Going Dark meetings, but so was the Swedish Security Service (Säpo) and
the Swedish Police Authority. Together with representatives from the
other Member States, they used the High Level Group meetings to discuss
how, through legislation, encrypted services could be required to
provide data in readable format. Several Member States argued that “the
working groups needed to look at solutions that involved ‘legal access
through design’.” This was something that pleased American
representatives.</p> <p data-svelte-h="svelte-15688qj">At the Going Dark meeting on November 21, 2023, a former FBI employee
was also present, who said that “solutions for legal access (to data on
device) should be prioritized” and that “companies needed to have a
responsibility and follow the same rules.” As a former FBI employee, he
also expressed “his gratitude for the fact that the issue was being
pursued within the EU.”</p> <h3 data-svelte-h="svelte-11oputi">European police chiefs: we cannot accept criminals using secure communications.</h3> <p data-svelte-h="svelte-1ge3kom">The Going Dark meetings resulted in an outcry from the assembled police
chiefs of Europe. In April 2024 <a href="https://www.europol.europa.eu/media-press/newsroom/news/european-police-chiefs-call-for-industry-and-governments-to-take-action-against-end-to-end-encryption-roll-out" rel="nofollow">Europol published the
challenge</a>
“European Police Chiefs call for industry and governments to take action
against end-to-end encryption roll-out.” The declaration was a <a href="https://polisen.se/aktuellt/nyheter/nationell/2024/april/europas-polischefer-gar-samman-mot-grov-brottslighet-i-en-digital-varld/" rel="nofollow">“direct
extension of the Going Dark
initiative”</a>
and, together, the European police authorities were clear that although
encryption is “a means of strengthening the cyber security and privacy
of citizens … we do not accept that there need be a binary choice
between cyber security or privacy on the one hand and public safety on
the other. Absolutism on either side is not helpful.”</p> <p data-svelte-h="svelte-drlmm9">It was as if Ylva Johansson’s sniffer dog had caught the scent again. In
the absence of expertise, the Going Dark initiative tried to magic away
the fact that end-to-end encryption is absolute – either you have
secure communication or you don’t.</p> <p data-svelte-h="svelte-15pvrfa">The assembled police chiefs claimed there were <a href="https://www.europol.europa.eu/cms/sites/default/files/documents/EDOC-%231384205-v1-Joint_Declaration_of_the_European_Police_Chiefs.PDF" rel="nofollow">two key factors for
achieving online
security</a>
– which turned out to be direct repetitions of the reasoning in the
Going Dark discussions. Number 1: so-called legal access to the tech
companies’ stored data. Number 2: real-time scanning of illegal activity
in tech companies’ services. Naturally, they said, all this would be
done under strong protection and supervision.</p> <p data-svelte-h="svelte-1ko89e9">Stefan Hector, a representative of the Swedish Police Authority, said
that <a href="https://polisen.se/aktuellt/nyheter/nationell/2024/april/europas-polischefer-gar-samman-mot-grov-brottslighet-i-en-digital-varld/" rel="nofollow">“a society cannot accept that criminals today have a space to
communicate safely in order to commit serious
crimes.”</a>
A week later, it was revealed that <a href="https://www.svd.se/a/8qwGbx/granskning-poliser-lacker-till-gangen" rel="nofollow">the Swedish police had been
infiltrated and were leaking information to
criminals</a>.</p> <p data-svelte-h="svelte-mnpnic">Although <a href="https://www.ohchr.org/sites/default/files/documents/issues/civicspace/resources/civic_space_protection_encryption_brief.pdf" rel="nofollow">the UN classifies encryption as a human
right</a>,
the Going Dark initiative and the European police force are fighting to
smash end-to-end encryption. Their first move came as a reaction to
<a href="https://www.europol.europa.eu/media-press/newsroom/news/european-police-chiefs-call-for-industry-and-governments-to-take-action-against-end-to-end-encryption-roll-out" rel="nofollow">Meta rolling out exactly such
encryption</a>.
The echoes from the Chat Control debate are clear. But it is also an
echo of an older battle.</p> <p data-svelte-h="svelte-fchcvw">The Going Dark initiative is really just an extension of the so-called
crypto war (the war against encryption) that US authorities have been
involved in since the internet began. As Signal’s CEO <a href="https://signal.org/blog/pdfs/ndss-keynote.pdf" rel="nofollow">Meredith
Whittaker said in a keynote
speech</a>:</p> <p data-svelte-h="svelte-1wyi685">“Encryption was essential for the commercial internet. But law
enforcement and security services saw any network resistant to
government surveillance as a threat and a problem.”</p> <p data-svelte-h="svelte-1mez9xt">The US authorities have already tested the backdoors that the European
Going Dark initiative is now seeking. Edward Snowden revealed that <a href="https://archive.nytimes.com/www.nytimes.com/interactive/2013/09/05/us/documents-reveal-nsa-campaign-against-encryption.html" rel="nofollow">the
NSA spent $250 million a
year</a>
getting tech companies to install backdoors in their services, which
also exposed the risks of backdoors. In 2010, Chinese hackers managed
<a href="https://edition.cnn.com/2010/OPINION/01/23/schneier.google.hacking/index.html" rel="nofollow">to use a Google
backdoor</a>
to get into Gmail. The same thing happened in 2005, <a href="https://spectrum.ieee.org/the-athens-affair" rel="nofollow">when state
surveillance of Vodafone was
exploited</a> by outside
actors to bug the Greek Prime Minister, his Foreign Minister, Justice
Minister, and a hundred other government officials.</p> <p data-svelte-h="svelte-5waapy">And when it comes to client-side scanning, it’s also doomed to fail.
Apple, one of the world’s most technologically advanced and wealthy
companies, has poured incredible resources into figuring out if it can
be done in a secure and private way. But when Apple made its effort
public, it took hackers just two weeks to break in. Apple abandoned the
attempt and continue to say no to anyone who asks them to try this again
– <a href="https://blog.cryptographyengineering.com/2023/03/23/remarks-on-chat-control/" rel="nofollow">simply because it’s too easy to hack systems where client-side
scanning is
involved</a>.</p> <p data-svelte-h="svelte-186vxxe">Snowden’s revelations changed the internet in many respects. Encrypted
websites (https) became standard. End-to-end encrypted messaging
services like Signal saw a widespread increase in popularity. Apple
started using strong encryption in its operating systems. From having
virtually free access to people’s internet traffic (if they didn’t use a
trustworthy VPN, that is) and from having been able to read people’s
messages in plain text, the internet now became more difficult for US
authorities to mass monitor.</p> <p data-svelte-h="svelte-4s3l6u">In her <a href="https://signal.org/blog/pdfs/ndss-keynote.pdf" rel="nofollow">lecture</a>,
Meredith Whittaker points to an important point: “Strong encryption was
an important win. But the result of this win was not privacy. Indeed,
the legacy of the crypto wars was to trade privacy for encryption – and
to usher in an age of mass corporate surveillance. Because the power to
enable – or violate – privacy was left in the hands of companies, not
those who relied on their services. Companies that were incentivized to
implement surveillance in service of advertising and commerce.”</p> <p data-svelte-h="svelte-q6flu7">For more than twenty years, so-called commercial mass surveillance has
created some of the richest companies in the world. The fact that Meta
is rolling out end-to-end encryption doesn’t mean they have abandoned
their business model. But it was still sufficient for the European
police chiefs, cheered on by the US authorities, to make a joint
declaration demanding legal access to the content in secure and private
communications. Meredith Whittaker again:</p> <p data-svelte-h="svelte-wzc8j2">“In my view, the ferocity of the current attack on end-to-end
encryption, and other privacy-preserving technologies, is very much
related to a desire by some in government to return to the less fettered
access to surveillance that they see as having lost post-Snowden.”</p> <p data-svelte-h="svelte-3ttgl1">We can see the attack coming in Europe right now. But the movement is
based in the United States. Back in 2014, just a year after Snowden’s
revelations, <a href="https://www.fbi.gov/news/speeches/going-dark-are-technology-privacy-and-public-safety-on-a-collision-course?ref=tailored-access.com" rel="nofollow">FBI Director James Comey
spoke</a>
of how “the challenges of real-time interception threaten to leave us in
the dark, encryption threatens to lead all of us to a very dark place.”</p> <p data-svelte-h="svelte-bca7cd">The US authorities, which in 2014 had recently been caught spying on the
entire world, used a particular expression when they began lobbying to
regain access to easily controlling everything and everyone. FBI
Director Comey talked about “Going Dark.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NoTunes is a macOS application that will prevent Apple Music from launching (273 pts)]]></title>
            <link>https://github.com/tombonez/noTunes</link>
            <guid>40426621</guid>
            <pubDate>Tue, 21 May 2024 10:42:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tombonez/noTunes">https://github.com/tombonez/noTunes</a>, See on <a href="https://news.ycombinator.com/item?id=40426621">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/tombonez/noTunes/blob/master/screenshots/app-icon.png"><img src="https://github.com/tombonez/noTunes/raw/master/screenshots/app-icon.png" alt="noTunes Logo"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/91118a2cb5f7de8f17625b0df69be17c1e0b64456ba02635542eb86538dceb5e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f746f6d626f6e657a2f6e6f74756e6573"><img src="https://camo.githubusercontent.com/91118a2cb5f7de8f17625b0df69be17c1e0b64456ba02635542eb86538dceb5e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f746f6d626f6e657a2f6e6f74756e6573" alt="GitHub release (latest by date)" data-canonical-src="https://img.shields.io/github/v/release/tombonez/notunes"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e91deddbc62cfaedccdad7770a4a0bcbfd1f15f9453d143436059ae697f6fde1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f746f6d626f6e657a2f6e6f74756e65732f746f74616c"><img src="https://camo.githubusercontent.com/e91deddbc62cfaedccdad7770a4a0bcbfd1f15f9453d143436059ae697f6fde1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f746f6d626f6e657a2f6e6f74756e65732f746f74616c" alt="GitHub all releases" data-canonical-src="https://img.shields.io/github/downloads/tombonez/notunes/total"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/73b4795407952c1c447d0262594b54393ea2da3aeecfe0c73f0c7f618fff1047/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f746f6d626f6e657a2f6e6f74756e6573"><img src="https://camo.githubusercontent.com/73b4795407952c1c447d0262594b54393ea2da3aeecfe0c73f0c7f618fff1047/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f746f6d626f6e657a2f6e6f74756e6573" alt="GitHub" data-canonical-src="https://img.shields.io/github/license/tombonez/notunes"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Notice</h2><a id="user-content-notice" aria-label="Permalink: Notice" href="#notice"></a></p>
<p dir="auto">The certificate used in noTunes prior to version 3.2 is set to expire on the 14th January 2022.</p>
<p dir="auto">To continue using noTunes please update to version 3.2 or greater.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">noTunes</h2><a id="user-content-notunes" aria-label="Permalink: noTunes" href="#notunes"></a></p>
<p dir="auto">noTunes is a macOS application that will prevent iTunes <em>or</em> Apple Music from launching.</p>
<p dir="auto">Simply launch the noTunes app and iTunes/Music will no longer be able to launch. For example, when bluetooth headphones reconnect.</p>
<p dir="auto">You can toggle the apps functionality via the menu bar icon with a simple left click.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Homebrew</h3><a id="user-content-homebrew" aria-label="Permalink: Homebrew" href="#homebrew"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install --cask notunes"><pre>brew install --cask notunes</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Direct Download</h3><a id="user-content-direct-download" aria-label="Permalink: Direct Download" href="#direct-download"></a></p>
<p dir="auto"><a href="https://github.com/tombonez/noTunes/releases/download/v3.4/noTunes-3.4.zip">noTunes-3.4.zip</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Set noTunes to launch at startup</h3><a id="user-content-set-notunes-to-launch-at-startup" aria-label="Permalink: Set noTunes to launch at startup" href="#set-notunes-to-launch-at-startup"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Before Ventura:</h4><a id="user-content-before-ventura" aria-label="Permalink: Before Ventura:" href="#before-ventura"></a></p>
<p dir="auto">Navigate to System Preferences -&gt; Users &amp; Groups. Under your user, select "Login Items", click the lock on the bottom left and enter your login password to make changes. Click the plus sign (+) in the main panel and search for noTunes. Select it and click "Add".</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Ventura and later:</h4><a id="user-content-ventura-and-later" aria-label="Permalink: Ventura and later:" href="#ventura-and-later"></a></p>
<ol dir="auto">
<li>Navigate to System Settings</li>
<li>Select General</li>
<li>Select Login Items</li>
<li>Click the + under Open at Login and select noTunes</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Toggle noTunes Functionality</h3><a id="user-content-toggle-notunes-functionality" aria-label="Permalink: Toggle noTunes Functionality" href="#toggle-notunes-functionality"></a></p>
<p dir="auto">Left click the menu bar icon to toggle between its active states.</p>
<p dir="auto"><strong>Enabled (prevents iTunes/Music from opening)</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/tombonez/noTunes/blob/master/screenshots/menubar-enabled.png"><img src="https://github.com/tombonez/noTunes/raw/master/screenshots/menubar-enabled.png" alt="noTunes Enabled"></a></p>
<p dir="auto"><strong>Disabled (allows iTunes/Music to open)</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/tombonez/noTunes/blob/master/screenshots/menubar-disabled.png"><img src="https://github.com/tombonez/noTunes/raw/master/screenshots/menubar-disabled.png" alt="noTunes Disabled"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hide Menu Bar Icon</h3><a id="user-content-hide-menu-bar-icon" aria-label="Permalink: Hide Menu Bar Icon" href="#hide-menu-bar-icon"></a></p>
<p dir="auto">Right click the menu bar icon and click <code>Hide Icon</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Restore Menu Bar Icon</h3><a id="user-content-restore-menu-bar-icon" aria-label="Permalink: Restore Menu Bar Icon" href="#restore-menu-bar-icon"></a></p>
<p dir="auto"><a href="#quit-notunes">Quit noTunes</a>, run the following command in Terminal and re-open the app:</p>
<div dir="auto" data-snippet-clipboard-copy-content="defaults delete digital.twisted.noTunes"><pre>defaults delete digital.twisted.noTunes</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quit noTunes</h3><a id="user-content-quit-notunes" aria-label="Permalink: Quit noTunes" href="#quit-notunes"></a></p>
<p dir="auto">To quit the app either:</p>
<p dir="auto"><strong>With menu bar icon visible</strong></p>
<p dir="auto">Right click the menu bar icon and click quit.</p>
<p dir="auto"><strong>With menu bar icon hidden</strong></p>
<p dir="auto">Quit the app via Activity Monitor or run the following command in Terminal:</p>
<div dir="auto" data-snippet-clipboard-copy-content="osascript -e 'quit app &quot;noTunes&quot;'"><pre>osascript -e <span><span>'</span>quit app "noTunes"<span>'</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Set replacement for iTunes / Apple Music</h3><a id="user-content-set-replacement-for-itunes--apple-music" aria-label="Permalink: Set replacement for iTunes / Apple Music" href="#set-replacement-for-itunes--apple-music"></a></p>
<p dir="auto">Replace <code>YOUR_MUSIC_APP</code> with the name of your music app in the following command.</p>
<div dir="auto" data-snippet-clipboard-copy-content="defaults write digital.twisted.noTunes replacement /Applications/YOUR_MUSIC_APP.app"><pre>defaults write digital.twisted.noTunes replacement /Applications/YOUR_MUSIC_APP.app</pre></div>
<p dir="auto">Then <code>/Applications/YOUR_MUSIC_APP.app</code> will launch when iTunes/Music attempts to launch.</p>
<p dir="auto">This can be used to open a website too, for example, YouTube Music.</p>
<div dir="auto" data-snippet-clipboard-copy-content="defaults write digital.twisted.noTunes replacement https://music.youtube.com/"><pre>defaults write digital.twisted.noTunes replacement https://music.youtube.com/</pre></div>
<p dir="auto">The following command will disable the replacement.</p>
<div dir="auto" data-snippet-clipboard-copy-content="defaults delete digital.twisted.noTunes replacement"><pre>defaults delete digital.twisted.noTunes replacement</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The code is available under the <a href="https://github.com/tombonez/notunes/blob/master/LICENSE">MIT License</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gifski: Optimized GIF Encoder (131 pts)]]></title>
            <link>https://github.com/ImageOptim/gifski</link>
            <guid>40426442</guid>
            <pubDate>Tue, 21 May 2024 10:16:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ImageOptim/gifski">https://github.com/ImageOptim/gifski</a>, See on <a href="https://news.ycombinator.com/item?id=40426442">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://gif.ski/" rel="nofollow"><img width="100%" src="https://camo.githubusercontent.com/59e069ea3016455bd8fc71796883f06336a7b6c96c3678e416eca610c7340c3b/68747470733a2f2f6769662e736b692f676966736b692e737667" alt="gif.ski" data-canonical-src="https://gif.ski/gifski.svg"></a></h2><a id="" aria-label="Permalink: " href="#"></a></div>
<p dir="auto">Highest-quality GIF encoder based on <a href="https://pngquant.org/" rel="nofollow">pngquant</a>.</p>
<p dir="auto"><strong><a href="https://gif.ski/" rel="nofollow">gifski</a></strong> converts video frames to GIF animations using pngquant's fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6ddf6a1bcae1beaf5ed5dc074eef0d605b0b704350fd9f28353c7f1521e22cf5/68747470733a2f2f6769662e736b692f64656d6f2e676966"><img src="https://camo.githubusercontent.com/6ddf6a1bcae1beaf5ed5dc074eef0d605b0b704350fd9f28353c7f1521e22cf5/68747470733a2f2f6769662e736b692f64656d6f2e676966" alt="(CC) Blender Foundation | gooseberry.blender.org" data-animated-image="" data-canonical-src="https://gif.ski/demo.gif"></a></p>
<p dir="auto">It's a CLI tool, but it can also be compiled <a href="https://docs.rs/gifski" rel="nofollow">as a C library</a> for seamless use in other apps.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Download and install</h2><a id="user-content-download-and-install" aria-label="Permalink: Download and install" href="#download-and-install"></a></p>
<p dir="auto">See <a href="https://github.com/ImageOptim/gifski/releases">releases</a> page for executables.</p>
<p dir="auto">If you have <a href="https://brew.sh/" rel="nofollow">Homebrew</a>, you can also get it with <code>brew install gifski</code>.</p>
<p dir="auto">If you have <a href="https://www.rust-lang.org/install.html" rel="nofollow">Rust from rustup</a> (1.63+), you can also build it from source with <a href="https://lib.rs/crates/gifski" rel="nofollow"><code>cargo install gifski</code></a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">gifski is a command-line tool. There is no GUI for Windows or Linux (there is one for <a href="https://sindresorhus.com/gifski" rel="nofollow">macOS</a>).</p>
<p dir="auto">The recommended way is to first export video as PNG frames. If you have <code>ffmpeg</code> installed, you can run in terminal:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ffmpeg -i video.webm frame%04d.png"><pre>ffmpeg -i video.webm frame%04d.png</pre></div>
<p dir="auto">and then make the GIF from the frames:</p>
<div dir="auto" data-snippet-clipboard-copy-content="gifski -o anim.gif frame*.png"><pre>gifski -o anim.gif frame<span>*</span>.png</pre></div>
<p dir="auto">You can also resize frames (with <code>-W &lt;width in pixels&gt;</code> option). If the input was ever encoded using a lossy video codec it's recommended to at least halve size of the frames to hide compression artefacts and counter chroma subsampling that was done by the video codec.</p>
<p dir="auto">See <code>gifski -h</code> for more options.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tips for smaller GIF files</h3><a id="user-content-tips-for-smaller-gif-files" aria-label="Permalink: Tips for smaller GIF files" href="#tips-for-smaller-gif-files"></a></p>
<p dir="auto">Expect to lose a lot of quality for little gain. GIF just isn't that good at compressing, no matter how much you compromise.</p>
<ul dir="auto">
<li>Use <code>--width</code> and <code>--height</code> to make the animation smaller. This makes the biggest difference.</li>
<li>Add <code>--quality=80</code> (or a lower number) to lower overall quality. You can fine-tune the quality with:
<ul dir="auto">
<li><code>--lossy-quality=60</code> lower values make animations noisier/grainy, but reduce file sizes.</li>
<li><code>--motion-quality=60</code> lower values cause smearing or banding in frames with motion, but reduce file sizes.</li>
</ul>
</li>
</ul>
<p dir="auto">If you need to make a GIF that fits a predefined file size, you have to experiment with different sizes and quality settings. The command line tool will display estimated total file size during compression, but keep in mind that the estimate is very imprecise.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<ol dir="auto">
<li><a href="https://www.rust-lang.org/en-US/install.html" rel="nofollow">Install Rust via rustup</a> or run <code>rustup update</code>. This project only supports up-to-date versions of Rust. You may get compile errors, warnings about "unstable edition", etc. if you don't run <code>rustup update</code> regularly.</li>
<li>Clone the repository: <code>git clone https://github.com/ImageOptim/gifski</code></li>
<li>In the cloned directory, run: <code>cargo build --release</code></li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using from C</h3><a id="user-content-using-from-c" aria-label="Permalink: Using from C" href="#using-from-c"></a></p>
<p dir="auto"><a href="https://github.com/ImageOptim/gifski/blob/main/gifski.h">See <code>gifski.h</code></a> for <a href="https://docs.rs/gifski/latest/gifski/c_api/#functions" rel="nofollow">the C API</a>. To build the library, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="rustup update
cargo build --release"><pre>rustup update
cargo build --release</pre></div>
<p dir="auto">and link with <code>target/release/libgifski.a</code>. Please observe the <a href="https://github.com/ImageOptim/gifski/blob/main/LICENSE">LICENSE</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">C dynamic library for package maintainers</h3><a id="user-content-c-dynamic-library-for-package-maintainers" aria-label="Permalink: C dynamic library for package maintainers" href="#c-dynamic-library-for-package-maintainers"></a></p>
<p dir="auto">The build process uses <a href="https://lib.rs/cargo-c" rel="nofollow"><code>cargo-c</code></a> for building the dynamic library correctly and generating the pkg-config file.</p>
<div dir="auto" data-snippet-clipboard-copy-content="rustup update
cargo install cargo-c
# build
cargo cbuild --prefix=/usr --release
# install
cargo cinstall --prefix=/usr --release --destdir=pkgroot"><pre>rustup update
cargo install cargo-c
<span><span>#</span> build</span>
cargo cbuild --prefix=/usr --release
<span><span>#</span> install</span>
cargo cinstall --prefix=/usr --release --destdir=pkgroot</pre></div>
<p dir="auto">The <code>cbuild</code> command can be omitted, since <code>cinstall</code> will trigger a build if it hasn't been done already.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">AGPL 3 or later. I can offer alternative licensing options, including <a href="https://supso.org/projects/pngquant" rel="nofollow">commercial licenses</a>. Let <a href="https://kornel.ski/contact" rel="nofollow">me</a> know if you'd like to use it in a product incompatible with this license.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">With built-in video support</h2><a id="user-content-with-built-in-video-support" aria-label="Permalink: With built-in video support" href="#with-built-in-video-support"></a></p>
<p dir="auto">The tool optionally supports decoding video directly, but unfortunately it relies on ffmpeg 4.x, which may be <em>very hard</em> to get working, so it's not enabled by default.</p>
<p dir="auto">You must have <code>ffmpeg</code> and <code>libclang</code> installed, both with their C headers installed in default system include paths. Details depend on the platform and version, but you usually need to install packages such as <code>libavformat-dev</code>, <code>libavfilter-dev</code>, <code>libavdevice-dev</code>, <code>libclang-dev</code>, <code>clang</code>. Please note that installation of these dependencies may be quite difficult. Especially on macOS and Windows it takes <em>expert knowledge</em> to just get them installed without wasting several hours on endless stupid installation and compilation errors, which I can't help with. If you're cross-compiling, try uncommenting <code>[patch.crates-io]</code> section at the end of <code>Cargo.toml</code>, which includes some experimental fixes for ffmpeg.</p>
<p dir="auto">Once you have dependencies installed, compile with <code>cargo build --release --features=video</code> or <code>cargo build --release --features=video-static</code>.</p>
<p dir="auto">When compiled with video support <a href="https://www.ffmpeg.org/legal.html" rel="nofollow">ffmpeg licenses</a> apply. You may need to have a patent license to use H.264/H.265 video (I recommend using VP9/WebM instead).</p>
<div dir="auto" data-snippet-clipboard-copy-content="gifski -o out.gif video.mp4"><pre>gifski -o out.gif video.mp4</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cross-compilation for iOS</h2><a id="user-content-cross-compilation-for-ios" aria-label="Permalink: Cross-compilation for iOS" href="#cross-compilation-for-ios"></a></p>
<p dir="auto">The easy option is to use the included <code>gifski.xcodeproj</code> file to build the library automatically for all Apple platforms. Add it as a <a href="https://lib.rs/crates/cargo-xcode" rel="nofollow">subproject</a> to your Xcode project, and link with <code>gifski-staticlib</code> Xcode target. See <a href="https://github.com/sindresorhus/Gifski">the GUI app</a> for an example how to integrate the library.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cross-compilation for iOS manually</h3><a id="user-content-cross-compilation-for-ios-manually" aria-label="Permalink: Cross-compilation for iOS manually" href="#cross-compilation-for-ios-manually"></a></p>
<p dir="auto">Make sure you have Rust installed via <a href="https://rustup.rs/" rel="nofollow">rustup</a>. Run once:</p>
<div dir="auto" data-snippet-clipboard-copy-content="rustup target add aarch64-apple-ios"><pre>rustup target add aarch64-apple-ios</pre></div>
<p dir="auto">and then to build the library:</p>
<div dir="auto" data-snippet-clipboard-copy-content="rustup update
cargo build --lib --release --target=aarch64-apple-ios"><pre>rustup update
cargo build --lib --release --target=aarch64-apple-ios</pre></div>
<p dir="auto">The build will print "dropping unsupported crate type <code>cdylib</code>" warning. This is normal and expected when building for iOS (the cdylib option exists for other platforms).</p>
<p dir="auto">This will create a static library in <code>./target/aarch64-apple-ios/release/libgifski.a</code>. You can add this library to your Xcode project. See <a href="https://github.com/sindresorhus/Gifski">gifski.app</a> for an example how to use libgifski from Swift.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building an AI game studio: what we've learned so far (119 pts)]]></title>
            <link>https://braindump.me/blog-posts/building-an-ai-game-studio</link>
            <guid>40426382</guid>
            <pubDate>Tue, 21 May 2024 10:05:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://braindump.me/blog-posts/building-an-ai-game-studio">https://braindump.me/blog-posts/building-an-ai-game-studio</a>, See on <a href="https://news.ycombinator.com/item?id=40426382">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-name="Post" data-framer-component-type="RichTextContainer" name="Post"><p>Hi!</p><p>Braindump is our attempt to imagine what game creation could be like in the brave new world of LLMs and generative AI. We want to give you an entire AI game studio, complete with coders, artists, and so on, to help you create your dream game.</p><p>With Braindump, you build top-down/2.5D games or interactive worlds by simply typing prompts. For example, typing “Create a Starfighter that can shoot lasers and drop BB-8 bombs” will generate 3D models, game data, and scripts that make your prompt come to life. You can then instantly play your game, and even invite a friend to play with you.</p><video autoplay="" data-framer-asset="data:framer/asset-reference,BdVWQwjP0BwghSdVMelEWugRfw.mp4" loop="" muted="" playsinline="" src="https://framerusercontent.com/assets/BdVWQwjP0BwghSdVMelEWugRfw.mp4"></video><p>As we’ve worked on Braindump for a while now, we figured it’s time to do an update, share some of our learnings, and see if we can get some feedback. You can also <a href="https://braindump.me/signup" target="_blank" rel="noopener">sign up for the alpha here</a>, if you’d be interested in trying out the product yourself and give direct feedback on it. Everyone is more than welcome to join our <a href="https://discord.gg/braindump" target="_blank" rel="noopener">Discord</a> and chat directly with us there. And there are some <a href="https://www.tiktok.com/@braindump.me" target="_blank" rel="noopener">more videos on our TikTok</a> account.</p><p>Anyway, let’s jump into it!</p><h2>From First Experiments to Today</h2><p>Braindump started around six months ago with some humble prototypes, using SVGs to represent game units.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,ejKVi4hST2rOJl6eNwTyvBg1ofQ.png" data-framer-height="863" data-framer-width="1657" height="431" src="https://framerusercontent.com/images/ejKVi4hST2rOJl6eNwTyvBg1ofQ.png" srcset="https://framerusercontent.com/images/ejKVi4hST2rOJl6eNwTyvBg1ofQ.png?scale-down-to=512 512w,https://framerusercontent.com/images/ejKVi4hST2rOJl6eNwTyvBg1ofQ.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/ejKVi4hST2rOJl6eNwTyvBg1ofQ.png 1657w" width="828" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p><p>Since then, we’ve added 3D model generation, multiplayer, and we’ve reworked the UX countless times. Most of all, we’ve iterated a lot on how the prompting works. Here’s what it looks like as of now:</p><p><img alt="" data-framer-asset="data:framer/asset-reference,35DYfRVGfKt0rFCJJrSQtjgcE.png" data-framer-height="1329" data-framer-width="2563" height="664" src="https://framerusercontent.com/images/35DYfRVGfKt0rFCJJrSQtjgcE.png" srcset="https://framerusercontent.com/images/35DYfRVGfKt0rFCJJrSQtjgcE.png?scale-down-to=512 512w,https://framerusercontent.com/images/35DYfRVGfKt0rFCJJrSQtjgcE.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/35DYfRVGfKt0rFCJJrSQtjgcE.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/35DYfRVGfKt0rFCJJrSQtjgcE.png 2563w" width="1281" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p><p>There are currently a couple of core operations in Braindump:</p><ul><li data-preset-tag="p"><p>You can define units, abilities, and attributes, such as “an Orc with 50 HP that can place a magic wand.”</p></li><li data-preset-tag="p"><p>You can populate the game map with objects, for example, “place twenty Orcs in a circle.”</p></li><li data-preset-tag="p"><p>You can create game rules and logic, i.e., “When Orcs reach 0 health, create a ghost Orc in their place.”</p></li><li data-preset-tag="p"><p>You can create new 3D models, i.e., “I want my Orc to be pink and have fluffy ears.” We’re using <a href="https://www.meshy.ai/" target="_blank" rel="noopener">Meshy</a> to generate the 3D models.</p></li></ul><p>All of this is accessible through a unified natural language prompting interface. Prompts can be started from anywhere and include all kinds of context, including positions, objects, and more. Your prompt is translated into code by a LLM and is executed by the runtime to update the game state. Building and playing is online, multiplayer and instant. Jump in, send a link to a friend, and have fun building together!</p><h2>Challenge 1: Designing UX for Prompting</h2><p>There are two big problems with using LLMs to help you build things: 1) how do you get the LLM to consistently do what you want and 2) what is the best UX for interacting with the LLMs? Let’s talk about the latter problem first.</p><p>We experimented with many UX paradigms before settling on our current iteration. One of the first ones we tried was “generate a game from a description,” but it quickly broke down. Imagine that you’ve hired a game studio to build a game for you, but you only get <em>one</em> chance at <em>perfectly</em> specifying your game to them. There are zero feedback rounds; they will build their best interpretation of what you’ve specified, which is unlikely to be what you <em>imagined</em>. LLMs have the same problem.</p><p>In fact, a lot of problems with LLMs are strikingly familiar if you just replace the word “LLM” with “developer” or “game studio.” In many cases, we’ve found that the reason it can’t do what we want it to do is simply because we haven’t given it the right information or context to accomplish the task.</p><p>This also goes for users. Working with an LLM is a bit like being a boss; you need to clearly articulate what you want, as it can’t read your mind. The difference from a human employee, though, is that the LLM will carry on regardless of the input and dream up whatever response it can so that it can output <em>something</em>. We’re looking into how we can get the LLM to ask for clarification, but it’s a balancing act as you <em>do</em> want it to guess a lot of the time. If I type “Create an Orc,” then I don’t want to be asked 20 follow-up questions to exactly specify the nature of the Orc; I just want it to take a guess. Figuring out exactly when a user expects it to ask for clarification and when not is one of our bigger challenges.</p><p>After experimenting with taking a whole description and generating a game from it, we switched to a more iterative approach. By building up the game over many prompts, you have a chance to go into more detail or iterate on things. For example, I may ask it to “Create an Orc,” and once it’s done that, I may start adding details to that Orc: “Make it afraid of ducks” or “Give it sunglasses.”</p><p>We also quickly discovered that we wanted to be able to click on things to “talk” about them because, without that, it’s quite hard to describe what you’re talking about. “That tree in the middle of the forest, next to the big rock, and two meters down from the lake” gets tedious quickly compared to just clicking the tree and saying “change <em>this</em> tree.” We’re still trying to find the right balance between prompting and traditional controls, and are actively experimenting in both directions.</p><h2>Challenge 2: Designing a Game API for LLMs</h2><p>Alright, we’ve covered how to interact with the LLM from a UX perspective. Let’s talk about the code it writes for us now.</p><p>Initially, we tried to get GPT to generate code for an existing game engine (we tried Three.js, a couple of JavaScript game engines, and working with the DOM directly), but we realized that, even though it’s quite good at generating code snippets, it struggles with bigger pieces of software and building and maintaining software architecture.</p><p>Instead, we built a much more streamlined “game API” in TypeScript, which provides as much structure as possible, so that the LLM can focus on filling in code and data.</p><p>As an example, here’s the code generated for the prompt "Create a blue car that drives around randomly”. We don’t permit it to write classes, functions or any other “code structure”. Instead, we force it to create “blueprints” and output “rules”, which obey a strict structure. These are also shown to the user in the UI so you know what it’s done:</p><div><pre translate="no"><code><span>const</span> <span>carModelId</span><span>:</span> ModelId = <span>lookupModelId</span><span>(</span><span>'a blue car model'</span><span>)</span><span>;</span>
<span>const</span> <span>carBlueprintId</span><span>:</span> BlueprintId = <span>createUnitBlueprint</span><span>(</span><span>{</span>
    <span>name</span><span>:</span> <span>'Blue Car'</span><span>,</span>
    <span>tags</span><span>:</span> <span>'car,blue'</span><span>,</span>
    <span>modelId</span><span>:</span> <span>carModelId</span><span>,</span>
    <span>blocking</span><span>:</span> <span>true</span>
<span>}</span><span>)</span><span>;</span>
<span>const</span> <span>spawnPosition</span><span>:</span> Vec2 = <span>{</span> <span>x</span><span>:</span> <span>0.4236507</span><span>,</span> <span>y</span><span>:</span> <span>1.8517406</span> <span>}</span><span>;</span>
<span>const</span> <span>carUnitId</span><span>:</span> UnitId = <span>spawnUnitFromBlueprint</span><span>(</span><span>carBlueprintId</span><span>,</span> <span>spawnPosition</span><span>,</span> <span>0</span><span>)</span><span>;</span>

<span>createOrUpdateRule</span><span>(</span><span>{</span>
    <span>type</span><span>:</span> <span>'periodic'</span><span>,</span>
    <span>id</span><span>:</span> <span>'rules/move_randomly'</span><span>,</span>
    <span>name</span><span>:</span> <span>'Blue Car moves randomly'</span><span>,</span>
    <span>period</span><span>:</span> <span>1</span><span>,</span>
    <span>callback</span><span>:</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
        <span>const</span> <span>carPosition</span><span>:</span> Vec2 = <span>getUnitPosition</span><span>(</span><span>carUnitId</span><span>)</span><span>;</span>
        <span>const</span> <span>newPosition</span><span>:</span> Vec2 = <span>vec2Add</span><span>(</span><span>carPosition</span><span>,</span> <span>vec2MulFactor</span><span>(</span><span>vec2FromAngle</span><span>(</span><span>getRandomNumber</span><span>(</span><span>)</span> * <span>Math</span>.<span>PI</span> * <span>2</span><span>)</span><span>,</span> <span>5</span><span>)</span><span>)</span><span>;</span>
        <span>orderUnitMove</span><span>(</span><span>carUnitId</span><span>,</span> <span>newPosition</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span><span>)</span><span>;</span></code></pre></div><video autoplay="" data-framer-asset="data:framer/asset-reference,xzcHoMcldQ2M8x24oV5i62ig.mp4" loop="" muted="" playsinline="" src="https://framerusercontent.com/assets/xzcHoMcldQ2M8x24oV5i62ig.mp4"></video><p>From our Game API we generate type definitions (.d.ts) which are fed to GPT in the system prompt. GPT seems to get the cue; it consistently uses our API and mostly does it correctly on the first try.</p><p>The type checking also has provided us with a surprise benefit; GPT will actually try to self-correct when it sees that it has made a type error. All we had to do was give it the errors, and as soon as we did, it automatically started with this behavior. For the car prompt above, the LLM made an error in its first attempt, and then self-corrected with its second attempt:</p><p><img alt="" data-framer-asset="data:framer/asset-reference,U0HxXz1X1j1rC5a2CLOnnqXkc2Q.png" data-framer-height="240" data-framer-width="864" height="120" src="https://framerusercontent.com/images/U0HxXz1X1j1rC5a2CLOnnqXkc2Q.png" srcset="https://framerusercontent.com/images/U0HxXz1X1j1rC5a2CLOnnqXkc2Q.png?scale-down-to=512 512w,https://framerusercontent.com/images/U0HxXz1X1j1rC5a2CLOnnqXkc2Q.png 864w" width="432" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"><img alt="" data-framer-asset="data:framer/asset-reference,yHE1ZFWM6hHll3v4X5WeLGMHomc.png" data-framer-height="1162" data-framer-width="1322" height="581" src="https://framerusercontent.com/images/yHE1ZFWM6hHll3v4X5WeLGMHomc.png" srcset="https://framerusercontent.com/images/yHE1ZFWM6hHll3v4X5WeLGMHomc.png?scale-down-to=512 512w,https://framerusercontent.com/images/yHE1ZFWM6hHll3v4X5WeLGMHomc.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/yHE1ZFWM6hHll3v4X5WeLGMHomc.png 1322w" width="661" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p><h2>Generated Macros</h2><p>The game API has also opened up another interesting UX flow: generated macros.</p><p>Normally, a macro in an application is a small program that lets you automate some task. For example, converting every second row in a spreadsheet from one currency to another.</p><p>In our system, however, <em>all</em> prompts generate code, and that code can automate pretty much anything covered by the game API.</p><p>As an example, I can type prompts such as “Place a tent next to each campfire,” and GPT will happily do so:</p><p><img alt="" data-framer-asset="data:framer/asset-reference,bj3BJS8IuBjDFphytGtjJRro4.png" data-framer-height="1302" data-framer-width="1280" height="651" src="https://framerusercontent.com/images/bj3BJS8IuBjDFphytGtjJRro4.png" srcset="https://framerusercontent.com/images/bj3BJS8IuBjDFphytGtjJRro4.png?scale-down-to=1024 1006w,https://framerusercontent.com/images/bj3BJS8IuBjDFphytGtjJRro4.png 1280w" width="640" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p><p>Or I can have it automate tedious tasks, such as “Create five different cats with different stats.”</p><p><img alt="" data-framer-asset="data:framer/asset-reference,Zlxsdn6phRIGvBA2k4RW2z7H2XQ.png" data-framer-height="402" data-framer-width="1894" height="201" src="https://framerusercontent.com/images/Zlxsdn6phRIGvBA2k4RW2z7H2XQ.png" srcset="https://framerusercontent.com/images/Zlxsdn6phRIGvBA2k4RW2z7H2XQ.png?scale-down-to=512 512w,https://framerusercontent.com/images/Zlxsdn6phRIGvBA2k4RW2z7H2XQ.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/Zlxsdn6phRIGvBA2k4RW2z7H2XQ.png 1894w" width="947" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p><p>I can even ask it questions that require computation to answer, such as “How many tents are there that are facing north?” GPT will generate some code that counts the tents based on their rotation:</p><div><pre translate="no"><code><span>const</span> <span>tentsFacingNorth</span> = <span>getAllUnitIds</span><span>(</span><span>)</span>
    .<span>filter</span><span>(</span><span>unitId</span> <span>=&gt;</span> <span>getUnitString</span><span>(</span><span>unitId</span><span>,</span> <span>"unitStrings/tags"</span><span>)</span>?.<span>includes</span><span>(</span><span>"tent"</span><span>)</span><span>)</span>
    .<span>filter</span><span>(</span><span>unitId</span> <span>=&gt;</span> <span>Math</span>.<span>abs</span><span>(</span><span>getUnitRotation</span><span>(</span><span>unitId</span><span>)</span> % <span>(</span><span>2</span> * <span>Math</span>.<span>PI</span><span>)</span><span>)</span> &lt; <span>0.01</span><span>)</span>.<span>length</span><span>;</span>

<span>// Return the count of tents facing north.</span>
<span>tentsFacingNorth</span><span>;</span></code></pre></div><p>This opens up a whole new way of working. It’s a bit strange at first, as we’re not really used to being able to do these things with programs. Once you get used to it, however, you can find creative ways to complete what would otherwise be very tedious tasks in a matter of seconds.</p><h2>Collaborative Editing with AI</h2><p>We wanted everything in Braindump to be multiplayer: both the creation of games and the playing of games. We’ve supported multiplayer editing from the very first version. At the beginning, we just had one big chat, which everyone could contribute to. This proved chaotic, though, even with just two people. The main problem is that you’re often working on two different things, which may not be relevant to each other. This confused both users and GPT!</p><p>After trying several solutions, we converged on something we call “threads.” This lets you initiate a prompt from anywhere in the world, like this:</p><p><img alt="" data-framer-asset="data:framer/asset-reference,YzBLvgAuGIYy0XUDTOsPFSZINic.png" data-framer-height="472" data-framer-width="1050" height="236" src="https://framerusercontent.com/images/YzBLvgAuGIYy0XUDTOsPFSZINic.png" srcset="https://framerusercontent.com/images/YzBLvgAuGIYy0XUDTOsPFSZINic.png?scale-down-to=512 512w,https://framerusercontent.com/images/YzBLvgAuGIYy0XUDTOsPFSZINic.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/YzBLvgAuGIYy0XUDTOsPFSZINic.png 1050w" width="525" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p><p>But you can also refine or add to that prompt when needed, like this:</p><p><img alt="" data-framer-asset="data:framer/asset-reference,tdyU9ohU72jARA3t4En23yZI.png" data-framer-height="400" data-framer-width="944" height="200" src="https://framerusercontent.com/images/tdyU9ohU72jARA3t4En23yZI.png" srcset="https://framerusercontent.com/images/tdyU9ohU72jARA3t4En23yZI.png?scale-down-to=512 512w,https://framerusercontent.com/images/tdyU9ohU72jARA3t4En23yZI.png 944w" width="472" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p><p>The thread is given the latest game state to begin with, but it doesn’t have the full history of the project. Multiple threads can be “running” at the same time, but only one prompt can be running per thread at a time.</p><p>So far, this has worked pretty well. In our testing, we’ve had five people working in the same world simultaneously, which was definitely a bit chaotic but still functional. We’re actively exploring ways to help users coordinate effectively.</p><h2>Benchmarking and Testing</h2><p>To evaluate the performance of our prompt engine, we’ve developed a benchmarking tool. The tool runs dozens of scenarios we’ve specified, each with their own prompts, and then uses GPT to evaluate the success of those prompts.</p><p>Here’s an example of a benchmark:</p><div><pre translate="no"><code><span>{</span>
    <span>"user"</span><span>:</span> <span>[</span><span>"Give the player an ability to drop a stone"</span><span>]</span><span>,</span>
    <span>"expected"</span><span>:</span> <span>[</span>
        <span>"A new ability should have been created, and it should be called something like 'drop stone'."</span><span>,</span>
        <span>"The player blueprint should have a 'drop stone' ability assigned to it, at slot 0."</span><span>,</span>
        <span>"When the ability is invoked, the stone should be placed at the feet of the player."</span>
    <span>]</span>
<span>}</span></code></pre></div><p>As you can see, we simply specify the “expected” conditions using natural language. A second GPT, “the evaluator” (with its own system prompt), is provided these conditions, the state of the simulation at completion, and any errors encountered, and is asked to judge whether or not the test was successful, including a critique of what went wrong.</p><p>Our test suite is in its early days still, but we’re constantly adding more and more tests as we discover new prompting styles and failure cases.</p><p>(Just as we were finishing up this blog post, GPT-4o was released and our test suite went from 80% successful to 91% successful)</p><h2>Why We’re Building Braindump</h2><p>Personally, I’ve always loved games and creativity. I learned to code so that I could create games. To me, generative AI is naturally the next step in productivity enhancement; with it, you can simply do more. As big studios are getting more and more conservative with the games they build, I’m excited to empower small groups or even individuals to build their dream games. I want to see what crazy ideas people come up with and realize when they have an entire AI game studio at their fingertips.</p><h2>Next Up</h2><p>Braindump is just getting started. Although it’s pretty good at executing “commands” right now (“Create a cat”), we know that we can expand that to handle much more vague or “big” tasks as well. Some of the things we’re looking into are:</p><ul><li data-preset-tag="p"><p>Supporting “bigger” prompts through planning</p></li><li data-preset-tag="p"><p>Getting GPT to stop guessing and instead ask the user for clarification</p></li><li data-preset-tag="p"><p>Improving code quality by making GPT critique its own work</p></li><li data-preset-tag="p"><p>Improving discoverability and inspiration (“what can I build with this?”)</p></li><li data-preset-tag="p"><p>Improving game engine capabilities in an LLM-amenable way</p></li></ul><h2>Wrapping up</h2><p>If you’ve stuck with me all the way down here, then thank you for reading this, and I hope it was interesting!</p><p>If you would like to try Braindump yourself, then make sure to <a href="https://braindump.me/signup" rel="noopener">sign up for alpha testing</a>. We’re letting in the first few people in the coming weeks. You can also follow us on <a href="https://discord.gg/braindump" target="_blank" rel="noopener">Discord</a>, <a href="https://x.com/braindumpme" target="_blank" rel="noopener">Twitter</a>, <a href="https://www.tiktok.com/@braindump.me" target="_blank" rel="noopener">TikTok</a>, and <a href="https://www.youtube.com/@braindumpincorporated" target="_blank" rel="noopener">YouTube</a>, where we’ll be posting updates.</p><p>Hope you’ve enjoyed it, and until next time!</p><p>/Fredrik Norén, CPTO Braindump</p><h3>About Us</h3><p>We’re a small company based in Stockholm, Sweden. There are currently six of us with many years of experience from everything from Spotify and Snapchat to AAA game studios. You can find us on <a href="https://www.linkedin.com/company/braindump" target="_blank" rel="noopener">LinkedIn</a>.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,2sJeT9jfDrpvmworc3n9PO2HjJc.png" data-framer-height="732" data-framer-width="1261" height="366" src="https://framerusercontent.com/images/2sJeT9jfDrpvmworc3n9PO2HjJc.png" srcset="https://framerusercontent.com/images/2sJeT9jfDrpvmworc3n9PO2HjJc.png?scale-down-to=512 512w,https://framerusercontent.com/images/2sJeT9jfDrpvmworc3n9PO2HjJc.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/2sJeT9jfDrpvmworc3n9PO2HjJc.png 1261w" width="630" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The OpenAI board was right – No (by a voice actres named Scarlet) means No (274 pts)]]></title>
            <link>https://garymarcus.substack.com/p/the-openai-board-was-right</link>
            <guid>40425403</guid>
            <pubDate>Tue, 21 May 2024 08:06:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://garymarcus.substack.com/p/the-openai-board-was-right">https://garymarcus.substack.com/p/the-openai-board-was-right</a>, See on <a href="https://news.ycombinator.com/item?id=40425403">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>A week ago, OpenAI released an exciting new demo, featuring a voice character with a sexy breathy voice that was supposed to remind you of Scarlett Johansson’s AI agent character in the fabulous film </span><em>Her</em><span>. Lots of people gushed over it. (Some worried about the sexism, as well they should, but that’s a story for another day. And of course I daresay the demo was </span><em>just </em><span>a demo, one that will never work robustly as advertised, but that too is a story for another day.)</span></p><p><span>Fast forward to today; there’s been a backlash.  Too many people noticed the coincidence and not everyone was happy. Some wondered whether ScarJo had been compensated. Today, under pressure, OpenAI pulled the ScarJo-like voice, </span><a href="https://www.tomsguide.com/ai/chatgpt/openai-says-sky-voice-in-chatgpt-will-be-paused-after-concerns-it-sounds-too-much-like-scarlett-johansson" rel="">alleging that the resemblance was purely a coincidence.</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg" width="1282" height="309" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:309,&quot;width&quot;:1282,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:180593,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>I probably don’t have to tell you, but that’s complete bullshit. And stupid, obviously refuted bullshit at that. </p><p>For one thing, Sam himself had proudly posted a reference to the film “Her” within hours of the demo:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg" width="636" height="360" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:360,&quot;width&quot;:636,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:29582,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p>I can’t tell you what happened to Sam’s caps lock, but obviously the claim that the resemblance to ScarJo was a “coincidence” was a lie. Sam knew perfectly well what the character sounded like.</p><p>§</p><p>A couple hours later, ScarJo herself (via her publicist) sent a statement, even more damning, to Bobby Allyn, a journalist (that I happen to know) at NPR, telling the real story:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png" width="1283" height="1772" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1772,&quot;width&quot;:1283,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:940148,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Coincidence, my eye. </p><p>§</p><p>ScarJo’s contention that this goes back to September checks out:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png" width="1301" height="499" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:499,&quot;width&quot;:1301,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:106318,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>They said it wasn’t intentional, but of course it was. Sam may not be wanting to delete his “her” tweet, but 6 million people saw it. And pur concidence line is a sign of consciousness of guilt.</p><p>§</p><p><span>All of this is really about </span><em><strong>consent</strong></em><span>. Artists and writer and actors don’t want their work to be used without their permission; if you want to use their stuff, you should compensate them, and get their permission. </span></p><p>If they say “no”, no means no.</p><p>Scarlett said “no”.</p><p>§</p><p>That didn’t stop Sam.</p><p>§</p><p>As the film maker Toni Thai put it:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png" width="1337" height="436" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:436,&quot;width&quot;:1337,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:99683,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>§</p><p>Sam got away with a lot for a long time, but people are starting to see through the ruse. Here’s an (admittedly unscientific) poll I ran earlier today:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg" width="1355" height="573" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:573,&quot;width&quot;:1355,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:168923,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Count me with the majority. Spin is a way of life at OpenAI; telling the truth is not.</p><p>§</p><p>Casey Newton noticed, too:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png" width="1456" height="1300" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1300,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1496725,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>So did Canadian MP Michelle Rempel Garner:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png" width="1341" height="433" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:433,&quot;width&quot;:1341,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:105941,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>§</p><p>The (old, now-replaced) Board said in November they fired Sam because he was not consistently candid. I saw that with his fudges to the Senate about OpenAI equity (he has indirect equity, which he failed to mention, and owns an OpenAI VC firm that trades on the company name that he failed to mention), the board saw it with his lies about Helen Toner, and now we all see it with his embarrassing lies about Scarlett Johansson. </p><p>It’s a pattern.  </p><p><em><strong>Gary Marcus</strong><span> has seen enough, and hopes that the new OpenAI board recognizes that Sam’s behavior is not consistent with what one would expect of a nonprofit that has pledged to help humanity. </span></em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Erlang/OTP 27 Highlights (195 pts)]]></title>
            <link>https://www.erlang.org/blog/highlights-otp-27/</link>
            <guid>40424982</guid>
            <pubDate>Tue, 21 May 2024 06:52:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.erlang.org/blog/highlights-otp-27/">https://www.erlang.org/blog/highlights-otp-27/</a>, See on <a href="https://news.ycombinator.com/item?id=40424982">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article>
    

    <div>
        
        
        
        
        <p>Erlang/OTP 27 is finally here. This blog post will introduce the new
features that we are most excited about.</p>

<p>A list of all changes is found in <a href="https://erlang.org/patches/OTP-27.0">Erlang/OTP 27 Readme</a>.
Or, as always, look at the release notes of the application you are interested in.
For instance:
<a href="https://www.erlang.org/doc/apps/erts/notes#erts-15.0">Erlang/OTP 27 - Erts Release Notes - Version 15.0</a>.</p>

<p>This year’s highlights mentioned in this blog post are:</p>

<ul>
  <li><a href="#overhauled-documentation-system">Overhauled documentation system</a></li>
  <li><a href="#triple-quoted-strings">Triple-Quoted strings</a></li>
  <li><a href="#sigils">Sigils</a></li>
  <li><a href="#no-need-to-enable-feature-maybe">No need to enable feature <code>maybe</code></a></li>
  <li><a href="#the-new-json-module">The new <code>json</code> module</a></li>
  <li><a href="#process-labels">Process labels</a></li>
  <li><a href="#new-functionality-in-stdlib">New functionality in STDLIB</a></li>
  <li><a href="#new-ssl-client-side-stapling-support">New SSL client-side stapling support</a></li>
  <li><a href="#tprof-yet-another-profiling-tool"><code>tprof</code>: Yet another profiling tool</a></li>
  <li><a href="#multiple-trace-sessions">Multiple trace sessions</a></li>
  <li><a href="#native-coverage-support">Native coverage support</a></li>
  <li><a href="#deprecating-archives">Deprecating archives</a></li>
</ul>
      <h2 id="overhauled-documentation-system">
        
        
          Overhauled documentation system <a href="#overhauled-documentation-system">#</a>
        
        
      </h2>
    

<p>The Erlang/OTP documentation before Erlang/OTP 27 was authored in
<a href="https://en.wikipedia.org/wiki/XML">XML</a>, from which the
<a href="https://www.erlang.org/docs/26/apps/erl_docgen/">Erl_Docgen</a>
application could generate HTML web pages, PDFs, or Unix man pages.
The reason for generating PDFs is that the documentation used to be
printed as
<a href="https://erlangforums.com/t/old-printed-otp-documentation-cover/1989/2">actual paper books</a>.
The last time the books were printed were for Erlang/OTP R7 released in 2000.</p>

<p>As an example, here is the XML code for
<a href="https://www.erlang.org/docs/26/man/lists#duplicate-2"><code>lists:duplicate/2</code></a>
from Erlang/OTP 26:</p>

<pre><code>    &lt;func&gt;
      &lt;name name="duplicate" arity="2" since=""/&gt;
      &lt;fsummary&gt;Make &lt;c&gt;N&lt;/c&gt; copies of element.&lt;/fsummary&gt;
      &lt;desc&gt;
        &lt;p&gt;Returns a list containing &lt;c&gt;&lt;anno&gt;N&lt;/anno&gt;&lt;/c&gt; copies of term
          &lt;c&gt;&lt;anno&gt;Elem&lt;/anno&gt;&lt;/c&gt;.&lt;/p&gt;
        &lt;p&gt;&lt;em&gt;Example:&lt;/em&gt;&lt;/p&gt;
        &lt;pre&gt;
&gt; &lt;input&gt;lists:duplicate(5, xx).&lt;/input&gt;
[xx,xx,xx,xx,xx]&lt;/pre&gt;
      &lt;/desc&gt;
    &lt;/func&gt;
</code></pre>

<p>The XML code was stored in separate files, not in the source
code. When building the documentation, the function specs from the
source code would be combined with the text from the documentation
file. It was the responsibility of the writer to ensure that variables
mentioned in the documentation body matched the names in the function
spec.</p>

<p>One thing never said about Erl_Docgen and the old documentation system
was that it made writing documentation enjoyable and effortless. That
was one thing we wanted to change with the new documentation system.
We wanted to make it fun to write documentation, or at least to
require less attention to tedious details such as using XML tags
correctly.</p>

<p>In Erlang/OTP 27, the documentation is written in
<a href="https://en.wikipedia.org/wiki/Markdown">Markdown</a> and is placed in
the source code before the function spec and implementation. Here is
the documentation and implementation of
<a href="https://www.erlang.org/doc/man/lists#duplicate/2"><code>lists:duplicate/2</code></a>
in Erlang/OTP 27:</p>

<pre><code>-doc """
Returns a list containing `N` copies of term `Elem`.

_Example:_

```erlang
&gt; lists:duplicate(5, xx).
[xx,xx,xx,xx,xx]
```
""".

-spec duplicate(N, Elem) -&gt; List when
      N :: non_neg_integer(),
      Elem :: T,
      List :: [T],
      T :: term().

duplicate(N, X) when is_integer(N), N &gt;= 0 -&gt; duplicate(N, X, []).

duplicate(0, _, L) -&gt; L;
duplicate(N, X, L) -&gt; duplicate(N-1, X, [X|L]).
```
</code></pre>

<p>The documentation is placed in a
<a href="#triple-quoted-strings">triple-quoted string</a>
following
the <a href="https://www.erlang.org/eeps/eep-0059"><code>-doc</code> attribute</a>.</p>

<p>Having the documentation near the spec makes its easy to ensure that
the text refers to variables defined in the function spec.</p>

<p>Another goal we had was to replace Erl_Docgen with a tool more widely
used so that we wouldn’t have to carry the entire burden for
maintaining it. We did that by using
<a href="https://hexdocs.pm/ex_doc/readme.html">ExDoc</a>, which is also used by
the <a href="https://elixir-lang.org/">Elixir</a> language and most, if not all,
Elixir projects.</p>

<p>An issue that arose is whether it’s advisable to include user
documentation within the source code. Wouldn’t this make it much harder
to maintain the code?</p>

<p>I don’t claim to have a universal response to that concern, but in the
case of Erlang/OTP, most actively developed code exists within modules
lacking documentation. Typically, OTP applications consist of one or
a few modules containing the documented API, while the bulk of the
implementation is found in other modules.</p>

<p>For example, the interface to the Erlang compiler is found in the
<a href="https://www.erlang.org/doc/man/compile">compile</a> module, while most
of the code being executed resides in one of the other 59 modules
of the Compiler application. Similarly, the <a href="https://www.erlang.org/doc/apps/ssl">SSL
application</a> comprises 76 modules,
of which merely four contain documentation.</p>

<p>Another application that is frequently updated is
<a href="https://www.erlang.org/doc/apps/erts">ERTS</a>. However, most of ERTS is
implemented in C (and some C++), while much of the actual
Erlang code within ERTS is located in modules without documentation.</p>

<p>There are, of course, some exceptions to how applications are
structured, for example the STDLIB application, where most modules are
documented. However, STDLIB is a mature application that is updated
relatively infrequently.</p>
      <h2 id="triple-quoted-strings">
        
        
          Triple-Quoted strings <a href="#triple-quoted-strings">#</a>
        
        
      </h2>
    

<p>To facilitate writing documentation attributes containing many lines
of text, triple-quoted strings as described in <a href="https://www.erlang.org/eeps/eep-0064">EEP
64</a> have been
implemented. Triple-quoted strings come in handy whenever one needs
to include multiple line of text in Erlang source code. For example,
assume that we want to define a function that outputs some
quotations:</p>

<pre><code>1&gt; t:quotes().
"I always have a quotation for everything -
it saves original thinking." - Dorothy L. Sayers

"Real stupidity beats artificial intelligence every time."
- Terry Pratchett
ok
</code></pre>

<p>In Erlang/OTP 26, there are several different ways to do that, but of none
of them are particularly satisfying. For example, the text can be put into a
single string:</p>

<pre><code>quotes() -&gt;
    S = "\"I always have a quotation for everything -
it saves original thinking.\" - Dorothy L. Sayers

\"Real stupidity beats artificial intelligence every time.\"
- Terry Pratchett\n",
    io:put_chars(S).
</code></pre>

<p>This works, but is ugly. We must also remember to escape every quote
character.</p>

<p>A cleaner way is to use multiple strings, one for each line, letting
the compiler combine them:</p>

<pre><code>quotes() -&gt;
    S = "\"I always have a quotation for everything -\n"
        "it saves original thinking.\" - Dorothy L. Sayers\n"
        "\n"
        "\"Real stupidity beats artificial intelligence every time.\"\n"
        "- Terry Pratchett\n",
    io:put_chars(S).
</code></pre>

<p>That is a little bit nicer, but we’ll need to type more quote characters
and we must not forget to add <code>\n</code> at the end of each string. To
make sure that we don’t forget to insert the newlines, we could delegate
that mundane chore to the computer:</p>

<pre><code>quotes() -&gt;
    S = ["\"I always have a quotation for everything -",
         "it saves original thinking.\" - Dorothy L. Sayers",
         "",
         "\"Real stupidity beats artificial intelligence every time.\"",
         "- Terry Pratchett"],
    io:put_chars(lists:join("\n", S)),
    io:nl().
</code></pre>

<p>In Erlang/OTP 27, we can use a triple-quoted string:</p>

<pre><code>quotes() -&gt;
    S = """
        "I always have a quotation for everything -
        it saves original thinking." - Dorothy L. Sayers

        "Real stupidity beats artificial intelligence every time."
        - Terry Pratchett
        """,
    io:put_chars(S),
    io:nl().
</code></pre>

<p>The ending <code>"""</code> determines how much each line in the string should be
indented. The same characters that precede <code>"""</code> are deleted from all
lines between the beginning and terminating delimiters. For this
particular example, all space characters are removed since all have
the same indentation as the terminating <code>"""</code>.  Neither quote
characters nor backslashes are special in the lines enclosed by the
triple-quotes, so there is no need to escape anything.</p>

<p>Here is another example to show the versatility of triple-quoted
strings:</p>

<pre><code>effect_warning() -&gt;
    """
    f() -&gt;
        %% Test that the compiler warns for useless tuple building.
        {a,b,c},
        ok.
    """.
</code></pre>

<p>The function returns a string containing a short Erlang function.</p>

<p>Assuming that <code>effect_warning/0</code> is defined in module <code>t</code>, it can be
called like so:</p>

<pre><code>1&gt; io:format("~ts\n", [t:effect_warning()]).
f() -&gt;
    %% Test that the compiler warns for useless tuple building.
    {a,b,c},
    ok.
</code></pre>

<p>Note that indentation of the Erlang code for function <code>f/0</code> is retained.</p>

<p>For more information, see section <a href="https://www.erlang.org/doc/reference_manual/data_types#string">String</a>
in the Reference Manual.</p>
      <h2 id="sigils">
        
        
          Sigils <a href="#sigils">#</a>
        
        
      </h2>
    

<p>Sigils for string literals as described in <a href="https://www.erlang.org/eeps/eep-0066">EEP 66</a>
have been implemented.</p>

<p>Continuing with the theme of quotes, let’s explore why sigils were
introduced into Erlang, drawing inspiration from the wisdom of ancient
Greek philosophers:</p>

<pre><code>1&gt; t:greek_quote().
"Know thyself" (Greek: Γνῶθι σαυτόν)
ok
</code></pre>

<p>In Erlang/OTP 26, this can be implemented as follows:</p>

<pre><code>greek_quote() -&gt;
    S = "\"Know thyself\" (Greek: Γνῶθι σαυτόν)",
    io:format("~ts\n", [S]).
</code></pre>

<p>At this point, we get some customer feedback indicating that the
modules containing all the quotes are consuming an excessive amount of
memory. Each character in a string consumes 16 bytes of memory (on a
64-bit computer). That could be reduced to one byte for each character
if a binary were to be used instead of a string.  (Actually, one byte
for each US ASCII character and two bytes for each Greek letter.)</p>

<p>That change should be really easy. Let’s try:</p>

<pre><code>greek_quote() -&gt;
    S = &lt;&lt;"\"Know thyself\" (Greek: Γνῶθι σαυτόν)"&gt;&gt;,
    io:format("~ts\n", [S]).
</code></pre>

<p>That works for the English text, but not for the Greek characters:</p>

<pre><code>2&gt; t:greek_quote().
"Know thyself" (Greek: ½ö¸¹ Ã±ÅÄÌ½)
</code></pre>

<p>What’s wrong?</p>

<p>Strings in binary expression are by default assumed to be a sequence
of byte-size characters. Therefore, this expression:</p>

<pre><code>1&gt; &lt;&lt;"Γνῶθι"&gt;&gt;.
&lt;&lt;147,189,246,184,185&gt;&gt;
</code></pre>

<p>is <a href="https://en.wikipedia.org/wiki/Syntactic_sugar">syntactic sugar</a> for:</p>

<pre><code>2&gt; &lt;&lt;$Γ:8, $ν:8, $ῶ:8, $θ:8, $ι:8&gt;&gt;.
&lt;&lt;147,189,246,184,185&gt;&gt;
</code></pre>

<p>It is necessary to specify that the characters are to be encoded as
<a href="https://en.wikipedia.org/wiki/UTF-8">UTF-8</a>
encoded characters by appending an <code>/utf8</code> suffix:</p>

<pre><code>greek_quote() -&gt;
    S = &lt;&lt;"\"Know thyself\" (Greek: Γνῶθι σαυτόν)"/utf8&gt;&gt;,
    io:format("~ts\n", [S]).
</code></pre>

<p>That works because <code>&lt;&lt;"Γνῶθι"/utf8&gt;&gt;</code> is syntactic sugar for
<code>&lt;&lt;$Γ/utf8, $ν/utf8, $ῶ/utf8, $θ/utf8, $ι/utf8&gt;&gt;</code>.</p>

<p>Enter sigils.</p>

<pre><code>greek_quote() -&gt;
    S = ~B["Know thyself" (Greek: Γνῶθι σαυτόν)],
    io:format("~ts\n", [S]).
</code></pre>

<p>The <code>~</code> character begins a sigil. It is usually followed by a letter that
indicates how the characters in the string should be interpreted or encoded.</p>

<p>In this case the character <code>B</code> means that the characters should be put into a binary in UTF-8 encoding,
and also that that no escape characters are allowed.</p>

<p>After <code>B</code> follows the start delimiter, in this case <code>[</code>.  Since no escape characters
are allowed, it is necessary to choose delimiters that don’t occur in the string
contents. After the contents follows the end delimiter, in this case <code>]</code>.</p>

<p>The <code>B</code> sigil is the default sigil that is used if the letter following
<code>~</code> is omitted. Thus we get the same binary and the same output if we omit the <code>B</code>:</p>

<pre><code>greek_quote() -&gt;
    S = ~["Know thyself" (Greek: Γνῶθι σαυτόν)],
    io:format("~ts\n", [S]).
</code></pre>

<p>Sigils can also be used to begin a triple-quoted string. Returning to
the quotations example from the previous section, a binary literal can
be created by inserting <code>~</code> before the leading <code>"""</code>:</p>

<pre><code>quotes() -&gt;
    S = ~"""
         "I always have a quotation for everything -
         it saves original thinking." - Dorothy L. Sayers

         "Real stupidity beats artificial intelligence every time."
         - Terry Pratchett
         """,
    io:put_chars(S),
    io:nl().
</code></pre>

<p>Here follows a few quick examples to show the other sigils.</p>

<p><code>~b</code> creates a binary in the same way as <code>~B</code>, except that backslashes
will be interpreted as an escape character. This can be useful if one
want to insert control characters such as TAB (<code>\t</code>) into a string:</p>

<pre><code>1&gt; ~b"abc\txyz".
&lt;&lt;"abc\txyz"&gt;&gt;
</code></pre>

<p>Here we used the <code>"</code> character as delimiters as it is not used within
the string.</p>

<p><code>~s</code> creates a string in the usual way. The only useful way it differs
from a plain quoted string is that the delimiters can be switched. That
way, one can avoid the hassle of escaping quote characters and still
get to use control characters such as TAB:</p>

<pre><code>2&gt; ~s{"abc\txyz"}.
"\"abc\txyz\""
</code></pre>

<p><code>~S</code> creates a string, but does not support escaping of characters
within the string, similar to <code>~B</code>.</p>

<p>For more information, see section <a href="https://www.erlang.org/doc/reference_manual/data_types#sigil">Sigil</a>
in the Reference Manual.</p>
      <h2 id="no-need-to-enable-feature-maybe">
        
        
          No need to enable feature <code>maybe</code> <a href="#no-need-to-enable-feature-maybe">#</a>
        
        
      </h2>
    

<p>The <a href="https://www.erlang.org/doc/reference_manual/expressions#maybe">maybe expression</a>
was introduced as a <a href="https://www.erlang.org/doc/reference_manual/features.html">feature</a>
in Erlang/OTP 25. In that release, it was necessary to enable it both in
the compiler and the runtime system.</p>

<p>Erlang/OTP 26 lifted the necessity to enable <code>maybe</code> in the runtime system.</p>

<p>Now in Erlang/OTP 27, <code>maybe</code> is enabled by default in the compiler.
In the example from
<a href="https://www.erlang.org/blog/otp-26-highlights/#no-need-to-enable-feature-maybe-in-the-runtime-system">last year’s blog post</a>,
the line <code>-feature(maybe_expr, enable).</code> can now be removed:</p>

<pre><code>$ cat t.erl
-module(t).
-export([listen_port/2]).
listen_port(Port, Options) -&gt;
    maybe
        {ok, ListenSocket} ?= inet_tcp:listen(Port, Options),
        {ok, Address} ?= inet:sockname(ListenSocket),
        {ok, {ListenSocket, Address}}
    end.
$ erlc t.erl
$ erl
Erlang/OTP 27 . . .

Eshell V15.0  (abort with ^G)
1&gt; t:listen_port(50000, []).
{ok,{#Port&lt;0.5&gt;,{{0,0,0,0},50000}}}
</code></pre>

<p>When <code>maybe</code> is used as an atom, it need to be quoted. For example:</p>

<pre><code>will_succeed(. . .) -&gt; yes;
will_succeed(. . .) -&gt; no;
   .
   .
   .
will_succeed(_) -&gt; 'maybe'.
</code></pre>

<p>Alternatively, it is still possible to disable the <code>maybe_expr</code> feature. With
the feature disabled, <code>maybe</code> can be used as an atom without quotes.</p>

<p>One way to disable <code>maybe</code> is to use the <code>-disable-feature</code> option when compiling.
For example:</p>

<pre><code>erlc -disable-feature maybe_expr *.erl
</code></pre>

<p>Another way to disable <code>maybe</code> is to add the following directive to
the source code:</p>

<pre><code>-feature(maybe_expr, disable).
</code></pre>
      <h2 id="the-new-json-module">
        
        
          The new <code>json</code> module <a href="#the-new-json-module">#</a>
        
        
      </h2>
    

<p>There is a new module <a href="https://www.erlang.org/doc/man/json"><code>json</code></a> in
STDLIB for generating and parsing
<a href="https://en.wikipedia.org/wiki/JSON">JSON (JavaScript Object Notation)</a>.</p>

<p>It is implemented by <a href="https://github.com/michalmuskala">Michał
Muskała</a> who has also implemented
the <a href="https://github.com/michalmuskala/jason"><code>Jason</code></a> library for
Elixir. <code>Jason</code> is known for being faster than other pure Erlang or
Elixir JSON libraries. The <code>json</code> module is not a pure translation of
the Elixir code for Jason, but a re-implementation with even better
performance than <code>Jason</code>.</p>

<p>As an example, imagine that we have this file <code>quotes.json</code> with
quotes from the film <a href="https://en.wikipedia.org/wiki/Jason_and_the_Argonauts_(1963_film)">Jason and the
Argonauts</a>:</p>

<pre><code>[
    {"quote": "The gods are best served by those who need their help the least.",
     "attribution": "Zeus",
     "verified": true},
    {"quote": "Now the voyage is over, I don't want any trouble to begin.",
     "attribution": "Jason",
     "verified": true}
]
</code></pre>

<p>The JSON contents of the file can be be decoded by calling
<a href="https://www.erlang.org/doc/man/json#decode/1">json:decode/1</a>:</p>

<pre><code>1&gt; {ok,JSON} = file:read_file("quotes.json").
{ok,&lt;&lt;"[\n   {\"quote\": \"The gods are best served by those who need their help the least.\",\n    \"attribution\": \"Zeus\""...&gt;&gt;}
2&gt; json:decode(JSON).
[#{&lt;&lt;"attribution"&gt;&gt; =&gt; &lt;&lt;"Zeus"&gt;&gt;,
   &lt;&lt;"quote"&gt;&gt; =&gt;
       &lt;&lt;"The gods are best served by those who need their help the least."&gt;&gt;,
   &lt;&lt;"verified"&gt;&gt; =&gt; true},
 #{&lt;&lt;"attribution"&gt;&gt; =&gt; &lt;&lt;"Jason"&gt;&gt;,
   &lt;&lt;"quote"&gt;&gt; =&gt;
       &lt;&lt;"Now the voyage is over, I don't want any trouble to begin."&gt;&gt;,
   &lt;&lt;"verified"&gt;&gt; =&gt; true}]
</code></pre>

<p>By default, for safety, the keys for objects are translated to binaries. Using atoms
could open up for
<a href="https://en.wikipedia.org/wiki/Denial-of-service_attack">denial-of-service attacks</a>
if a malicious JSON object would define millions of unique keys.</p>

<p>For convenience, it is still possible to convert keys to atoms in
a safe way by using a <em>decoder callback</em>. Here is an example:</p>

<pre><code>1&gt; Push = fun(Key, Value, Acc) -&gt; [{binary_to_existing_atom(Key), Value} | Acc] end.
#Fun&lt;erl_eval.40.39164016&gt;
</code></pre>

<p>This fun converts the key for a JSON object to an <strong>existing</strong> atom,
or raises an exception if no such atom exists.</p>

<p>Since this example is run from the shell, we’ll need to make sure that all possible keys
are known atoms:</p>

<pre><code>2&gt; {quote,attribution,verified}.
{quote,attribution,verified}
</code></pre>

<p>This would normally not be necessary when JSON decoding is done in an Erlang module,
because the atoms to be used as keys would presumably be defined naturally by being used
when processing the decoded JSON objects.</p>

<p>With this preparation done, the JSON decoder can be called using the <code>Push</code> fun
as an <code>object_push</code> decoder callback:</p>

<pre><code>3&gt; {Qs,_,&lt;&lt;&gt;&gt;} = json:decode(JSON, [], #{object_push =&gt; Push}), Qs.
[#{quote =&gt;
       &lt;&lt;"The gods are best served by those who need their help the least."&gt;&gt;,
   attribution =&gt; &lt;&lt;"Zeus"&gt;&gt;,verified =&gt; true},
 #{quote =&gt;
       &lt;&lt;"Now the voyage is over, I don't want any trouble to begin."&gt;&gt;,
   attribution =&gt; &lt;&lt;"Jason"&gt;&gt;,verified =&gt; true}]
</code></pre>

<p>The <a href="https://www.erlang.org/doc/man/json#encode/1">json:encode/1</a> function encodes
an Erlang term to JSON:</p>

<pre><code>4&gt; io:format("~ts\n", [json:encode(Qs)]).
[{"quote":"The gods are best served by those who need their help the least.","attribution":"Zeus","verified":true},{"quote":"Now the voyage is over, I don't want any trouble to begin.","attribution":"Jason","verified":true}]
ok
</code></pre>

<p>The encoder accepts binaries, atoms, and integer as keys for objects,
so there is no need to customize encoding for this particular example.</p>

<p>However, when necessary, it is possible to customize the encoding. For
example, assume that we want to store each quotation in a three-tuple
instead of in a map:</p>

<pre><code>1&gt; Q = [{~"The gods are best served by those who need their help the least.",
~"Zeus",true},
{~"Now the voyage is over, I don't want any trouble to begin.",
~"Jason",true}].
[{&lt;&lt;"The gods are best served by those who need their help the least."&gt;&gt;,
  &lt;&lt;"Zeus"&gt;&gt;,true},
 {&lt;&lt;"Now the voyage is over, I don't want any trouble to begin."&gt;&gt;,
  &lt;&lt;"Jason"&gt;&gt;,true}]
</code></pre>

<p>The <code>json:encode/1</code> function does not handle that format by default, but it can be
handled by defining an <em>encoder function</em>:</p>

<pre><code>quote_encoder({Q, A, V}, Encode)
  when is_binary(Q), is_binary(A), is_boolean(V) -&gt;
    json:encode_map(#{quote =&gt; Q,
                      attribution =&gt; A,
                      verified =&gt; V},
                    Encode);
quote_encoder(Other, Encode) -&gt;
    json:encode_value(Other, Encode).
</code></pre>

<p>The first clause matches a tuple of size three that looks like a
quotation. If it matches, it is converted to the map representation
for a JSON object, which is then converted by the utility function
<a href="https://www.erlang.org/doc/man/json#encode_map/2">json:encode_map/1</a>
to JSON.</p>

<p>The second clause handles all other Erlang terms by calling the
default encoding function
<a href="https://www.erlang.org/doc/man/json#encode_value/2">json:encode_value/2</a>
for converting a term to JSON.</p>

<p>Assuming that this function is defined in module <code>t</code>, the conversion to JSON
is invoked as follows:</p>

<pre><code>2&gt; io:format("~ts\n", [json:encode(Q, fun t:quote_encoder/2)]).
[{"quote":"The gods are best served by those who need their help the least.","attribution":"Zeus","verified":true},{"quote":"Now the voyage is over, I don't want any trouble to begin.","attribution":"Jason","verified":true}]
</code></pre>

<p>The JSON encoder will call the callback recursively for given term. That can
be clearly seen if we modify the second clause of <code>quote_encoder/2</code> to also
print the value of <code>Other</code>:</p>

<pre><code>3&gt; json:encode(Q, fun t:quote_encoder/2), ok.
-- [{&lt;&lt;"The gods are best served by those who need their help the least."&gt;&gt;,
     &lt;&lt;"Zeus"&gt;&gt;,true},
    {&lt;&lt;"Now the voyage is over, I don't want any trouble to begin."&gt;&gt;,
     &lt;&lt;"Jason"&gt;&gt;,true}]
-- &lt;&lt;"quote"&gt;&gt;
-- &lt;&lt;"The gods are best served by those who need their help the least."&gt;&gt;
-- &lt;&lt;"attribution"&gt;&gt;
-- &lt;&lt;"Zeus"&gt;&gt;
-- &lt;&lt;"verified"&gt;&gt;
-- true
-- &lt;&lt;"quote"&gt;&gt;
-- &lt;&lt;"Now the voyage is over, I don't want any trouble to begin."&gt;&gt;
-- &lt;&lt;"attribution"&gt;&gt;
-- &lt;&lt;"Jason"&gt;&gt;
-- &lt;&lt;"verified"&gt;&gt;
-- true
</code></pre>
      <h2 id="process-labels">
        
        
          Process labels <a href="#process-labels">#</a>
        
        
      </h2>
    

<p>As an help for debugging or observing in general, labels can be now
set on non-registered processes using
<a href="https://www.erlang.org/doc/man/proc_lib#set_label/1"><code>proc_lib:set_label/1</code></a>.</p>

<p>The label is an arbitrary term. The label is shown by the the shell
command <code>i/0</code> and by <a href="https://www.erlang.org/doc/man/observer"><code>observer</code></a>.
They can also be found in the dictionary section of a
<a href="https://www.erlang.org/doc/man/crashdump_viewer">crash dump</a>.</p>

<p>Here is an example where five labeled quote-handler processes are started and
inspected:</p>

<pre><code>1&gt; F = fun(I) -&gt;
   spawn_link(fun() -&gt;
     proc_lib:set_label({quote_handler, I}),
     receive _ -&gt; ok end
   end)
   end.
#Fun&lt;erl_eval.42.39164016&gt;
2&gt; Ps = [F(I) || I &lt;- lists:seq(1, 5)].
[&lt;0.91.0&gt;,&lt;0.92.0&gt;,&lt;0.93.0&gt;,&lt;0.94.0&gt;,&lt;0.95.0&gt;]
3&gt; proc_lib:get_label(hd(Ps)).
{quote_handler,1}
4&gt; i().
Pid                   Initial Call                          Heap     Reds Msgs
Registered            Current Function                     Stack
&lt;0.0.0&gt;               erl_init:start/2                       987     5347    0
init                  init:loop/1                              2
   .
   .
   .
{quote_handler,1}     prim_eval:'receive'/2                    9
&lt;0.92.0&gt;              erlang:apply/2                         233     4006    0
{quote_handler,2}     prim_eval:'receive'/2                    9
&lt;0.93.0&gt;              erlang:apply/2                         233     4006    0
{quote_handler,3}     prim_eval:'receive'/2                    9
&lt;0.94.0&gt;              erlang:apply/2                         233     4006    0
{quote_handler,4}     prim_eval:'receive'/2                    9
&lt;0.95.0&gt;              erlang:apply/2                         233     4006    0
{quote_handler,5}     prim_eval:'receive'/2                    9
Total                                                     642876  1156835    0
                                                             438
ok
</code></pre>

<p>The SSH and and SSL applications have been updated to label the processes they
create.</p>
      <h2 id="new-functionality-in-stdlib">
        
        
          New functionality in STDLIB <a href="#new-functionality-in-stdlib">#</a>
        
        
      </h2>
    
      <h2 id="new-utility-functions-for-set-modules">
        
        
          New utility functions for set modules <a href="#new-utility-functions-for-set-modules">#</a>
        
        
      </h2>
    

<p>The three sets modules in STDLIB —
<a href="https://www.erlang.org/doc/man/sets"><code>sets</code></a>,
<a href="https://www.erlang.org/doc/man/gb_sets"><code>gb_sets</code></a>, and
<a href="https://www.erlang.org/doc/man/ordsets"><code>ordsets</code></a> —
have new functions <code>is_equal/2</code>, <code>map/2</code>, and <code>filtermap/2</code>.</p>

<p>The <code>is_equal/2</code> function is useful when one needs to find out whether two
sets contain the same elements. Comparing with <code>==</code> or <code>=:=</code> is not always
reliable. For example:</p>

<pre><code>1&gt; Seq = lists:seq(1, 20, 2).
[1,3,5,7,9,11,13,15,17,19]
2&gt; gb_sets:from_list(Seq) == gb_sets:delete(10, gb_sets:from_list([10|Seq])).
false
3&gt; gb_sets:is_equal(gb_sets:from_list(Seq), gb_sets:delete(10, gb_sets:from_list([10|Seq]))).
true
</code></pre>

<p>The <code>map/2</code> maps the element of a set, producing a new set:</p>

<pre><code>4&gt; Seq = lists:seq(1, 20, 2).
[1,3,5,7,9,11,13,15,17,19]
#Fun&lt;erl_eval.42.39164016&gt;
5&gt; ordsets:to_list(ordsets:map(fun(N) -&gt; N div 4 end, ordsets:from_list(Seq))).
[0,1,2,3,4]
</code></pre>

<p>The <code>filtermap/2</code> function can map and filter at the same time. Here is an example
showing how to multiply each integer in a set by 100 and remove non-integers:</p>

<pre><code>1&gt; Mixed = [1,2,3,a,b,c].
[1,2,3,a,b,c]
2&gt; F = fun(N) when is_integer(N) -&gt; {true,N * 100};
   (_) -&gt; false
   end.
#Fun&lt;erl_eval.42.39164016&gt;
3&gt; sets:to_list(sets:filtermap(F, sets:from_list(Mixed))).
[300,200,100]
</code></pre>
      <h2 id="new-timer-convenience-functions-that-take-funs">
        
        
          New <code>timer</code> convenience functions that take funs <a href="#new-timer-convenience-functions-that-take-funs">#</a>
        
        
      </h2>
    

<p>In Erlang/OTP 26, the functions in the
<a href="https://www.erlang.org/doc/man/timer"><code>timer</code></a> module don’t accept funs.
It is certainly possibly to pass in a fun in the argument for
<a href="https://www.erlang.org/doc/man/erlang#apply/2"><code>erlang:apply/2</code></a>,
but if one makes a mistake it will be only be noticed when the
timer expires:</p>

<pre><code>1&gt; timer:apply_after(10, erlang, apply, [fun() -&gt; io:put_chars("now!\n") end]).
{ok,{once,#Ref&lt;0.2380540714.1485570051.86513&gt;}}
=ERROR REPORT==== 10-Apr-2024::05:56:43.894073 ===
Error in process &lt;0.109.0&gt; with exit value:
{undef,[{erlang,apply,[#Fun&lt;erl_eval.43.105768164&gt;],[]}]}
</code></pre>

<p>Here the empty argument list for the fun was forgotten. It should have been:</p>

<pre><code>2&gt; timer:apply_after(10, erlang, apply, [fun() -&gt; io:put_chars("now!\n") end, []]).
{ok,{once,#Ref&lt;0.2380540714.1485570051.86522&gt;}}
now!
</code></pre>

<p>In Erlang/OTP 27, using a fun is much easier:</p>

<pre><code>1&gt; timer:apply_after(10, fun() -&gt; io:put_chars("now!\n") end).
{ok,{once,#Ref&lt;0.3845681669.1215561736.51634&gt;}}
now!
</code></pre>

<p>In systems that use hot code updating, using a local fun for a long-running
timer is not ideal. The code that defines the fun could have been replaced,
and when the timer finally expires the call will fail. Therefore, it is also
possible to pass a fun as well as its arguments, making it possible to use
use a remote fun that will survive hot code updating:</p>

<pre><code>2&gt; timer:apply_after(10, fun io:put_chars/1, ["now\n"]).
{ok,{once,#Ref&lt;0.3845681669.1215561736.51650&gt;}}
now
</code></pre>

<p>The <code>apply_interval/*</code> and <code>apply_repeatedly/*</code> functions now also accept
funs.</p>
      <h2 id="new-ets-functions">
        
        
          New <code>ets</code> functions <a href="#new-ets-functions">#</a>
        
        
      </h2>
    

<p>The new functions
<a href="https://www.erlang.org/doc/man/ets#first_lookup/1"><code>ets:first_lookup/1</code></a>
and
<a href="https://www.erlang.org/doc/man/ets#next_lookup/2"><code>ets:next_lookup/2</code></a>
simplifies and speeds up traversing an ETS table:</p>

<pre><code>1&gt; T = ets:new(example, [ordered_set]).
#Ref&lt;0.1968915180.2077884419.247786&gt;
2&gt; ets:insert(T, [{I,I*I} || I &lt;- lists:seq(1, 10)]).
true
3&gt; {K1,_} = ets:first_lookup(T).
{1,[{1,1}]}
4&gt; {K2,_} = ets:next_lookup(T, K1).
{2,[{2,4}]}
5&gt; {K3,_} = ets:next_lookup(T, K2).
{3,[{3,9}]}
6&gt; {K4,_} = ets:next_lookup(T, K3).
{4,[{4,16}]}
</code></pre>

<p>Similarly,
<a href="https://www.erlang.org/doc/man/ets#last_lookup/1"><code>ets:last_lookup/1</code></a>
and
<a href="https://www.erlang.org/doc/man/ets#prev_lookup/2"><code>ets:prev_lookup/2</code></a>
can be used to traverse a table in reverse order.</p>

<p>The new function
<a href="https://www.erlang.org/doc/man/ets#update_element/4"><code>ets:update_element/4</code></a>
is similar to
<a href="https://www.erlang.org/doc/man/ets#update_element/3"><code>ets:update_element/3</code></a>,
but makes it possible to supply a default object when there is no existing
object with the given key:</p>

<pre><code>1&gt; T = ets:new(example, []).
#Ref&lt;0.878413430.1983512583.205850&gt;
2&gt; ets:update_element(T, a, {2, true}, {a, true}).
true
3&gt; ets:lookup(T, a).
[{a,true}]
</code></pre>
      <h2 id="new-ssl-client-side-stapling-support">
        
        
          New SSL client-side stapling support <a href="#new-ssl-client-side-stapling-support">#</a>
        
        
      </h2>
    

<p>A new feature in the SSL client in Erlang/OTP 27 is support for <a href="https://en.wikipedia.org/wiki/OCSP_stapling">OCSP
stapling</a> for easier and
faster verification of the revocation status of server
certificates.</p>

<p>With OCSP stapling, the SSL client can streamline the validation of
revocation status. Normally the client would have to query the
<a href="https://en.wikipedia.org/wiki/Certificate_authority">CA (Certificate
Authority)</a> using
<a href="https://en.wikipedia.org/wiki/Online_Certificate_Status_Protocol">OCSP (Online Certificate Status
Protocol)</a>
to ensure that the server’s certificate has not been
<a href="https://en.wikipedia.org/wiki/Certificate_revocation">revoked</a>.</p>

<p>The basic idea behind OCSP stapling is that the server itself will
proactively query the CA regarding the revocation status for its own
certificate and “staple” the time-stamped OCSP response from the CA to
the certificate. When a client connects, the server passes along
its OCSP-stapled certificate to the client. To verify the revocation
status, the client only needs to check that the OCSP response was
signed by the CA.</p>

<p>Here follows an example showing how OCSP stapling can be enabled in the
SSL client:</p>

<pre><code>1&gt; ssl:start().
ok
2&gt; {ok, Socket} = ssl:connect("duckduckgo.com", 443,
                              [{cacerts, public_key:cacerts_get()},
                               {stapling, staple}]).
{ok,{sslsocket,{gen_tcp,#Port&lt;0.5&gt;,tls_connection,undefined},
               [&lt;0.122.0&gt;,&lt;0.121.0&gt;]}}
</code></pre>
      
    

<p>In Erlang/OTP 27, the new profiling tool
<a href="https://www.erlang.org/doc/man/tprof"><code>tprof</code></a>
joins the existing profiling tools
<a href="https://www.erlang.org/doc/man/cprof"><code>cprof</code></a>,
<a href="https://www.erlang.org/doc/man/eprof"><code>eprof</code></a>,
and <a href="https://www.erlang.org/doc/man/fprof"><code>fprof</code></a>.</p>

<p>Why introduce a new profiling tool?</p>

<p>One reason is that <code>cprof</code> and <code>eprof</code> perform similar profiling
tasks, but the naming of the API functions are different. It is quite
easy to mix up the names when running one tool after the other, and
running them after each other is not uncommon.  For example, when
trying to find a
<a href="https://en.wikipedia.org/wiki/Bottleneck_(software)">bottleneck</a> in a
complex running Erlang system, one approcach is to first use
<code>cprof</code> to get a rough idea of the general part of the system where a
bottleneck could be located. After that, <code>eprof</code> is run on a limited
part of the system trying to narrow it down. Directly running <code>eprof</code>
on a large Erlang application could overload it and bring it down.</p>

<p>Using <code>tprof</code>, the same function is used for both counting calls and
measuring the time for each call. Here is how to count calls when
<code>lists:seq(1, 1000)</code> is called:</p>

<pre><code>1&gt; tprof:profile(lists, seq, [1, 1000], #{type =&gt; call_count}).
FUNCTION          CALLS  [    %]
lists:seq/2           1  [ 0.40]
lists:seq_loop/3    251  [99.60]
                         [100.0]
ok
</code></pre>

<p>Note that call counting is always done for all processes.</p>

<p>The bulk of the work for <code>lists:seq/2</code> is done in <code>lists:seq_loop/3</code>,
which was called 251 times. Since we asked for 1000 integers, we
reach the conclusion that each tail-recursive call to <code>seq_loop/3</code>
creates four list elements at once. That can be confirmed by
looking at the
<a href="https://github.com/erlang/otp/blob/ca50a5d73703f74e2eae1ca40bbe6c4f027f9f98/lib/stdlib/src/lists.erl#L409-L416">source code</a>.</p>

<p>To measure the time for each call, we only need to replace
<code>call_count</code> with <code>call_time</code>:</p>

<pre><code>2&gt; tprof:profile(lists, seq, [1, 1000], #{type =&gt; call_time}).

****** Process &lt;0.94.0&gt;  --  100.00% of total ***
FUNCTION          CALLS  TIME (μs)  PER CALL  [     %]
lists:seq/2           1          0      0.00  [  0.00]
lists:seq_loop/3    251         50      0.20  [100.00]
                                50            [ 100.0]
ok
</code></pre>

<p>Call time is only measured the process that called
<a href="https://erlang.org/doc/man/tprof#profile/4"><code>tprof:profile/4</code></a>
and any process spawned by that process.</p>

<p>By replacing <code>call_time</code> with <code>call_memory</code> the amount of memory consumed
by each call will be measured:</p>

<pre><code>3&gt; tprof:profile(lists, seq, [1, 1000], #{type =&gt; call_memory}).

****** Process &lt;0.97.0&gt;  --  100.00% of total ***
FUNCTION          CALLS  WORDS  PER CALL  [     %]
lists:seq_loop/3    251   2000      7.97  [100.00]
                          2000            [ 100.0]
ok
</code></pre>

<p>The total number of words created is 2000, which make sense since each
list element needs 2 words. The number of words consumed per call is
<code>2000 / 251</code>, which is approximately 7.97 or almost 8. That also makes
sense since each tail-recursive call creates 4 list elements, or 8
words, and there are 250 such calls. The remaining call creates the final
empty list (<code>[]</code>).</p>

<p><code>call_memory</code> tracing was introduced in the runtime system in
Erlang/OTP 26, but was not exposed in any existing profiling tool
because it didn’t really fit in any of them. It made more sense to enable
support for it in a new tool.</p>
      <h2 id="multiple-trace-sessions">
        
        
          Multiple trace sessions <a href="#multiple-trace-sessions">#</a>
        
        
      </h2>
    

<p>Tracing makes it possible to observe, debug, analyse, and measure the
performance of a running Erlang system. Over the year, numerous tools
using tracing has been developed. In Erlang/OTP alone, several tools
leverage tracing for different purposes:</p>

<ul>
  <li>
    <p><a href="https://www.erlang.org/doc/man/dbg"><code>dbg</code></a>, <a href="https://www.erlang.org/doc/man/ttb"><code>ttb</code></a> -
general tracing tools</p>
  </li>
  <li>
    <p><a href="https://www.erlang.org/doc/man/etop"><code>etop</code></a> - similar to <code>top</code> in Unix</p>
  </li>
  <li>
    <p><a href="https://www.erlang.org/doc/man/eprof"><code>eprof</code></a>,
<a href="https://www.erlang.org/doc/man/cprof"><code>cprof</code></a>,
<a href="https://www.erlang.org/doc/man/fprof"><code>fprof</code></a>,
<a href="https://www.erlang.org/doc/man/tprof"><code>tprof</code></a> - profiling tools</p>
  </li>
  <li>
    <p><a href="https://www.erlang.org/doc/apps/et"><code>et</code></a> - event tracer</p>
  </li>
  <li>
    <p><a href="https://www.erlang.org/doc/man/debugger"><code>debugger</code></a> - uses tracing
internally when evaluating <code>receive</code> expressions</p>
  </li>
</ul>

<p>In Erlang/OTP 26 and earlier tracing had some limitations:</p>

<ul>
  <li>
    <p>There could only be a single tracer per traced process.</p>
  </li>
  <li>
    <p>The configuration for which processes and functions to trace were
global within the runtime system.</p>
  </li>
</ul>

<p>Those limitations meant that different tracing tools could easily step
on each other’s toes. The treacherous part was that using multiple tracing
tools at the same time would seem to work for a while… until it didn’t.</p>

<p>In Erlang/OTP 27, multiple trace sessions can be created. Each trace
session has its own tracer process and configuration for which
processes and functions to trace.</p>

<p>To create a trace session and set up tracing, there is the new
<a href="https://www.erlang.org/doc/man/trace"><code>trace</code></a> module in the Kernel
application. Tools that set up tracing using that module will no longer
interfere with each other. Tools that use the
<a href="https://www.erlang.org/doc/man/erlang#trace/3">old API</a>
will share a single global trace session.</p>

<p>In the initial Erlang/OTP 27 release, some of the tools using tracing
have been updated to use trace sessions. Other tools will be updated in
upcoming maintenance releases.</p>

<p>We have tried to design the new API in a way to make it relatively
easy for maintainers of external tools to migrate their code.  Apart
from the names of the functions and the first argument (the session
argument), the other arguments and their semantics are almost entirely
identical to the old API.</p>
      <h2 id="quick-trace-session-example">
        
        
          Quick trace session example <a href="#quick-trace-session-example">#</a>
        
        
      </h2>
    

<p>Here is an example to show how the new API is used. First we’ll need
a tracer process that prints all trace messages it receives:</p>

<pre><code>1&gt; Tracer = spawn(fun F() -&gt; receive M -&gt; io:format("== ~p ==\n", [M]), F() end end).
&lt;0.90.0&gt;
</code></pre>

<p>Having a tracer process, we can create a trace session:</p>

<pre><code>2&gt; Session = trace:session_create(my_session, Tracer, []).
{#Ref&lt;0.179442114.3923902468.103849&gt;,{my_session,0}}
</code></pre>

<p>Next we turn on call tracing on the current process:</p>

<pre><code>3&gt; trace:process(Session, self(), true, [call]).
1
</code></pre>

<p>Make sure that module <code>array</code> is loaded and trace all calls in it:</p>

<pre><code>4&gt; l(array).
{module,array}
5&gt; trace:function(Session, {array,'_','_'}, [], [local]).
89
</code></pre>

<p>Next create a new array:</p>

<pre><code>6&gt; array:new(10).
== {trace,&lt;0.88.0&gt;,call,{array,new,"\n"}} ==
{array,10,0,undefined,10}
== {trace,&lt;0.88.0&gt;,call,{array,new_0,[10,0,false]}} ==
== {trace,&lt;0.88.0&gt;,call,{array,new_1,["\n",0,false,undefined]}} ==
== {trace,&lt;0.88.0&gt;,call,{array,new_1,[[],10,true,undefined]}} ==
== {trace,&lt;0.88.0&gt;,call,{array,new,[10,true,undefined]}} ==
== {trace,&lt;0.88.0&gt;,call,{array,find_max,"\t\n"}} ==
</code></pre>

<p>Note that trace messages are randomly intermingled with the return value
of the call.</p>

<p>When we are done, we can destroy the session:</p>

<pre><code>7&gt; trace:session_destroy(Session).
</code></pre>

<p>If we don’t destroy the session, it will be automatically destroyed when
the last reference to it goes away.</p>
      <h2 id="native-coverage-support">
        
        
          Native coverage support <a href="#native-coverage-support">#</a>
        
        
      </h2>
    

<p>The <a href="https://www.erlang.org/doc/man/cover">Cover</a> tool for determining
<a href="https://en.wikipedia.org/wiki/Code_coverage">code coverage</a> has long been
part of Erlang/OTP.</p>

<p>Traditionally, Cover collected its coverage metrics without the
help of any specialized functionality in the runtime system. To count how
many times each line in a module was executed, Cover
<a href="https://en.wikipedia.org/wiki/Instrumentation_(computer_programming)">instrumented</a>
abstract code for the module by inserting calls to
<a href="https://www.erlang.org/doc/man/ets#update_counter/3"><code>ets:update_counter/3</code></a>
on each executable line.</p>

<p>That worked, but the cover-instrumented Erlang code would always run
slower. How much slower depended on the nature of the code being
tested.</p>

<p>In Erlang/OTP 27, runtime systems supporting the
<a href="https://www.erlang.org/blog/a-first-look-at-the-jit/">JIT (just-in-time compiler)</a>
can now collect coverage metrics in the runtime system with minimal
performance overhead.</p>

<p>The Cover tool has been updated to automatically take advantage of
native coverage support if supported by the runtime system. When
running the test suites for most OTP applications, there is no
noticeable difference in execution time running with and without
Cover.</p>

<p>The native coverage support can also be used directly for performing
measurements that Cover cannot accomplish, such as collecting metrics
for code that is executed while the Erlang runtime system is starting.</p>

<p>Here is a quick example showing how we can collect coverage metrics
for <code>init</code>, which is the first module executed when starting up the
runtime system. First we need to instruct the runtime system to
instrument all functions in all modules with extra code to count the
number of times each function is called:</p>

<pre><code>$ bin/erl +JPcover function_counters
</code></pre>

<p>The runtime system starts normally. We can now read out the counters
for the <code>init</code> module:</p>

<pre><code>1&gt; lists:reverse(lists:keysort(2, code:get_coverage(function, init))).
[{{archive_extension,0},392},
 {{get_argument1,2},198},
 {{objfile_extension,0},101},
 {{boot_loop,2},64},
 {{request,1},55},
 {{to_strings,1},44},
 {{do_handle_msg,2},38},
 {{handle_msg,2},38},
 {{b2s,1},38},
 {{get_argument,2},33},
 {{get_argument,1},31},
 {{'-load_modules/2-lc$^0/1-0-',1},30},
 {{'-load_modules/2-lc$^1/1-2-',1},30},
 {{'-load_modules/2-lc$^2/1-3-',1},30},
 {{'-load_modules/2-lc$^3/1-4-',1},30},
 {{extract_var,2},30},
 {{'-prepare_loading_fun/0-fun-0-',3},29},
 {{eval_script,2},23},
 {{append,1},18},
 {{get_arguments,1},18},
 {{reverse,1},17},
 {{check,2},17},
 {{ensure_loaded,2},16},
 {{ensure_loaded,1},16},
 {{do_load_module,2},14},
 {{do_ensure_loaded,2},14},
 {{get_flag_args,...},12},
 {{...},...},
 {...}|...]
</code></pre>

<p>The returned list of counter values for each function is sorted in
descending order on the number of time each function was executed.</p>

<p>For more information, see
<a href="https://www.erlang.org/doc/man/code#module-native-coverage-support">Native Coverage Support</a>
in the documentation for the <code>code</code> module.</p>
      <h2 id="deprecating-archives">
        
        
          Deprecating archives <a href="#deprecating-archives">#</a>
        
        
      </h2>
    

<p><a href="https://www.erlang.org/doc/man/code#module-loading-of-code-from-archive-files">Archives</a>
is experimental functionality that has existed in Erlang/OTP for a
long time. Part of the support for archives is deprecated in Erlang/OTP 27.</p>

<p>The reason is that the performance of code loading from archives has
never been great. Even worse is that the very existence of the archive
functionality degrades the performance of code loading even when no
archives are used, and complicates or prevents optimizations aimed at
reducing startup time.</p>

<p>In Erlang/OTP 27, the following functionality is deprecated:</p>

<ul>
  <li>
    <p>Using archives for packaging a single application or parts of a single application
into an archive file that is included in the code path. This functionality will
likely be removed in Erlang/OTP 28.</p>
  </li>
  <li>
    <p>The <a href="https://www.erlang.org/doc/man/code#lib_dir/2"><code>code:lib_dir/2</code></a>
function. This function was introduced to allow reading files
inside archives. In Erlang/OTP 28, the function itself will not be
removed, but it will most likely no longer support looking into
archives.</p>
  </li>
  <li>
    <p>All functionality to handle archives in module
<a href="https://www.erlang.org/doc/man/erl_prim_loader"><code>erl_prim_loader</code></a>.
That same functionality is likely to be removed in Erlang/OTP 28.</p>
  </li>
  <li>
    <p>The <code>-code_path_choice</code> flag for <code>erl</code>. In Erlang/OTP 27, the default
has changed from <code>relaxed</code> to <code>strict</code>. This flag is likely to be removed
in Erlang/OTP 28.</p>
  </li>
</ul>

<p>In order to use archives in Erlang/OTP 27, it is necessary to use the flag
<code>-code_path_choice relaxed</code>.</p>
      <h2 id="using-a-single-archive-in-an-escript-is-not-deprecated">
        
        
          Using a single archive in an Escript is <strong>not</strong> deprecated <a href="#using-a-single-archive-in-an-escript-is-not-deprecated">#</a>
        
        
      </h2>
    

<p>An archive can still be used to hold all files needed by an
<a href="https://www.erlang.org/doc/apps/erts/escript_cmd">Escript</a>.
However, to access files in the archive (for example, to read templates or other
data files), the only supported way guaranteed to work in future
releases is to use the
<a href="https://www.erlang.org/doc/man/escript#extract/2"><code>escript:extract/2</code></a>
function.</p>

        
    </div>
</article>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Might We Learn? (147 pts)]]></title>
            <link>https://andymatuschak.org/hmwl/</link>
            <guid>40424536</guid>
            <pubDate>Tue, 21 May 2024 05:33:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://andymatuschak.org/hmwl/">https://andymatuschak.org/hmwl/</a>, See on <a href="https://news.ycombinator.com/item?id=40424536">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="Transcript">
        <h2 id="ideal-learning-environment">Your ideal learning environment <a href="#ideal-learning-environment">#</a></h2>
<p data-timestamp="245">Talks about learning technology often center on technology. Instead, I want to begin by asking: <em>what do you want learning to be like—for yourself?</em> If you could snap your fingers and drop yourself into a perfect learning environment, what’s your ideal?</p>
<p data-timestamp="268">One way to start thinking about this question is to ask: what were the most rewarding high-growth periods of your life?</p>
<p data-timestamp="278">I’ve noticed two patterns in answers to this question: first, people will tell me about a time when they learned a lot, but <em>learning wasn’t the point</em>. Instead, they were immersed in a situation with real personal meaning—like a startup, a research project, an artistic urge, or just a fiery curiosity. They dove in, got their hands dirty, and learned whatever was important along the way. And secondly: in these stories, <em>learning really worked</em>. People emerged feeling transformed, newly capable, filled with insight and understanding that has stayed with them years later.</p>
<p data-timestamp="323">These stories are so vivid because learning isn’t usually like this. People are often telling me somewhat wistfully about experiences that happened years or decades earlier. Learning rarely feels so subordinated to an authentic pursuit. Often if we try to “just dive in”, we hit a brick wall, or find ourselves uneasily cargo culting others, with no real understanding.</p>
<p data-timestamp="348">Why can’t we “just dive in” all the time?</p>
<p data-timestamp="351">Instead it often feels like we have to put our aims on hold while we go do some homework—learn properly. Worse, learning so often just doesn’t really work! We take the class, we read the book… but when we try to put that knowledge into practice, it’s fragile; it doesn’t transfer well. Then we’ll often find that we’ve forgotten half of it by the time we try to use it.</p>
<p data-timestamp="377">Why does learning so often fail to actually work?</p>
<img src="https://andymatuschak.org/hmwl/img/implicit-guided.png" alt=""><p data-timestamp="380"> These questions connect to an age-old conflict among educators and learning scientists—between implicit learning (also called discovery learning, inquiry learning, or situated learning), and guided learning (often represented by cognitive psychologists). Advocates of implicit learning methods argue that we should prioritize discovery, motivation, authentic involvement, and being situated in a community of practice. In the opposing camp, cognitive psychologists argue that you really do need to pay attention to architecture of cognition, long-term memory, procedural fluency, and to scaffold appropriately for cognitive load.</p>
<p data-timestamp="419">In my view, each of these points of view contains a lot of truth. And they each wrongly deny the other’s position, to their mutual detriment. Implicit learning aptly recognizes meaning and emotion, but ignores the often decisive constraints of cognition—what we usually need to make “learning actually work”. Guided learning advocates are focused on making learning work, and they sometimes succeed, but usually by sacrificing the purposeful sense of immersion we love about those rewarding high-growth periods.</p>
<img src="https://andymatuschak.org/hmwl/img/middle.png" alt=""><p data-timestamp="452"> One obvious approach is to try to compromise. Project-based learning is a good representation of that. By creating a scaffolded sequence of projects, the suggestion is that we can get some of the benefits of implicit learning—authenticity, motivation, transfer—while also exerting some of the instructional control and cognitive awareness typical of traditional courses. But so often it ends up getting the worst of both worlds—neither motivation and meaning, nor adequate guidance, explanation, and cognitive support.</p>
<p data-timestamp="482">In university, I was interested in 3D game programming, so I took a project-based course on computer graphics. The trouble was that <em>those projects weren’t my projects</em>. So a few weeks in, I found myself implementing a ray marching shader for more efficient bump mapping. Worse, because this course was trying to take project-based learning seriously, there weren't long textbook readings or problem sets. I found myself just translating math I’d been given into code. What I ended up with was a project I didn't care about, implementing math I didn't understand.</p>
<p data-timestamp="513">Instead, I suggest: we should take both views seriously, and find a way to synthesize the two. You really do want to make doing-the-thing the primary activity. But the realities of cognitive psychology mean that in many cases, you really do need explicit guidance, scaffolding, practice, and attention to memory support.</p>
<p data-timestamp="534">Learning by immersion works naturalistically when the material has a low enough complexity relative to your prior knowledge that you can successfully process it on the fly, and when natural participation routinely reinforces everything important, so that you build fluency. When those conditions aren’t satisfied—which is most of the time—you’ll need some support.</p>
<p data-timestamp="552">You want to just dive in, and you want learning to actually work. To make that happen, we need to infuse your authentic projects with guided support, where necessary, inspired by the best ideas from cognitive psychology. And if there’s something that requires more focused, explicit learning experiences, you want those experiences to be utterly in service to your actual aims.</p>
<p data-timestamp="576">I’ve been thinking about this synthesis for many years, and honestly: I’ve mostly been pretty stuck! Recently, though, I’ve been thinking a lot about AI. I know: eye-roll! Pretty much every mention of AI in learning technologies gets an eye-roll from me. But I confess: the possibility of AI has helped me finally get what feels like some traction on this problem. I’d like to share some of those early concepts today.</p>
<h2 id="tractable-immersion">Demo, part 1: Tractable immersion <a href="#tractable-immersion">#</a></h2>
<p data-timestamp="604">We’ll explore this possible synthesis through a story in six parts.</p>
<img src="https://andymatuschak.org/hmwl/img/sam.png" alt=""><p data-timestamp="608"> Meet Sam. Sam studied computer science in university, and they’re now working as a software engineer at a big tech company. But Sam’s somewhat bored at their day job. Not everything is boring, though: every time Sam sees a tweet announcing a new result in brain-computer interfaces, they’re absolutely captivated. These projects seem so much more interesting. Sam pull up the papers, looking for some way to contribute, but they hit a brick wall—so many unfamiliar topics, all at once.</p>
<img src="https://andymatuschak.org/hmwl/img/1-start.png" alt=""><p data-timestamp="639"> What if Sam could ask for help finding some meaningful way to start participating? With Sam’s permission, our AI—and let’s assume it’s a local AI—can build up a huge amount of context about their background. From old documents on Sam’s hard drive, our AI knows all about their university coursework. It can see their current skills through work projects. It knows something about Sam’s interests through their browsing history.</p>
<p data-timestamp="666">Sam’s excited about the idea of reproducing the paper’s data analysis. It seems to play to their strengths. They notice that the authors used a custom Python package to do their analysis, but that code was never published. That seems intriguing: Sam’s built open-source tools before. Maybe they could contribute here by building an open source version of this signal processing pipeline.</p>
<h2 id="guidance-in-action">Demo, part 2: Guidance in action <a href="#guidance-in-action">#</a></h2>
<p data-timestamp="689">So—Sam dives in. They’ve found an open-access dataset, and they’ve taken the first steps to start working with it. Tools like Copilot help Sam get started, but to follow some of these signal processing steps, what Sam really needs here is something like Copilot, but with awareness of the paper in addition to the code, and with context about what Sam’s trying to do.</p>
<img src="https://andymatuschak.org/hmwl/img/2-view-sample.png" alt=""><p data-timestamp="714"> This AI system isn’t trapped in its own chatbox, or in the sidebar of one application. It can see what’s going on across multiple applications, and it can propose <em>actions</em> across multiple applications. Sam can click that button to view a changeset with the potential implementation. Then they can continue the conversation, smoothly switching into the context of the code editor.</p>
<p data-timestamp="740">Like, what’s this “axis=1” parameter? The explanation depends on context from the code editor, the paper being implemented, and also documentation that came with the dataset Sam’s working with. The AI underlines assumptions made based on specific information, turning them into links.</p>
<img src="https://andymatuschak.org/hmwl/img/2-readme.png" alt=""><p data-timestamp="761"> Here Sam clicks on that “In this dataset” link, and our AI opens the README to the relevant line.</p>
<p data-timestamp="769">All this is to support our central aim—that Sam can immerse themselves, as much as possible, in what they’re actually trying to do, but get the support they need to understand what they’re doing.</p>

<p data-timestamp="783">That support doesn’t have to just mean text.</p>
<img src="https://andymatuschak.org/hmwl/img/3-dynamic-media.png" alt=""><p data-timestamp="786"> Sam next needs to implement a downsampling stage. This time, guidance includes synthesized dynamic media, so that Sam can understand what downsampling does through scaffolded immersion.</p>
<p data-timestamp="799">Sam doesn’t need to read an abstract explanation and try to imagine what that would do to different signals: instead, as they try different sampling rates, realtime feedback can help them internalize the effect on different signals. By playing with the dynamic media, Sam notices that some of the peaks are lost when the signal is downsampled.</p>
<p data-timestamp="819">These dynamic media aren’t trapped in the chatbox. They’re using the same input data and libraries Sam’s using in their notebook. At any time, Sam can just “view source” to tinker with this figure, or to use some of its code in their own notebook.</p>
<h2 id="contextualized-study">Demo, part 4: Contextualized study <a href="#contextualized-study">#</a></h2>
<p data-timestamp="838">Now Sam presses on but as they dig into band-pass filters, the high-level explanations they can get from these short chat interactions really just don’t feel like enough. What’s a frequency domain? What’s a Nyquist rate? Sam can copy and paste some AI-generated code, but they don’t understand what’s going on at all. A chat interface is just not a great medium for long-from conceptual explanation. It’s time for something deeper.</p>
<img src="https://andymatuschak.org/hmwl/img/4-toc.png" alt=""><p data-timestamp="872"> The AI knows Sam’s background and aims here, so it suggests an undergraduate text with a practical focus. More importantly, the AI reassures Sam that they don’t necessarily need to read this entire thousand-page book right now. It focuses on Sam’s goal here and suggests a range of accessible paths that Sam can choose according to how deeply they’d like to understand these filters. It's made a personal map in the book’s table of contents.</p>
<p data-timestamp="902">So that, for instance, if Sam just wants to understand what these filters are doing and why, there's a 25-page path for that. But if they want to know the mathematical background—how these filters work—there's a deeper path. And if they want to be able to implement these filters themselves, there's an even deeper path. Sam can choose their journey here.</p>
<img src="https://andymatuschak.org/hmwl/img/4-note.png" alt=""><p data-timestamp="912"> When Sam digs into the book, they'll find notes from the AI at the start of each section, and scattered throughout, which ground the material in Sam’s context, Sam’s project, Sam’s purpose. “This section will help you understand how to think of signals in terms of frequency spectra. That’s what low-pass filters manipulate.” Sam’s spending some time away from their project, in a more traditionally instructional setting, but that doesn’t mean the experience has to lose its connection to their authentic practice.</p>
<p data-timestamp="952">Incidentally, I’ve heard some technologists suggest that we should use AI to synthesize the whole book, bespoke for each person. But I think there’s a huge amount of value in shared canonical artifacts—in any given field, there are key texts that everyone can refer to, and they form common ground for the culture. I think we can preserve those by layering personalized context as a lens on top of texts like this.</p>
<p data-timestamp="978">In my ideal future, of course, our canonical shared artifacts are dynamic media, not digital representations of dead trees. But until all of our canonical works are rewritten, as a transitional measure, we can at least wave our hands and imagine that our AI could synthesize dynamic media versions of figures like this one.</p>
<p data-timestamp="998">Now, as Sam reads through the book, they can continue to engage with the text by asking questions, and our AI’s responses will continue to be grounded in their project.</p>
<p data-timestamp="1008">As Sam highlights the text or makes comments about details which seem particularly important or surprising, those annotations won’t end up trapped in this PDF: they’ll feed into future discussion and practice, as we’ll see later.</p>
<img src="https://andymatuschak.org/hmwl/img/4-q-from-ai.png" alt=""><p data-timestamp="1022"> In addition to Sam asking questions of the AI, the AI can insert questions for Sam to consider—again, grounded in their project—to promote deeper processing of the material.</p>
<img src="https://andymatuschak.org/hmwl/img/4-exercise.png" alt=""><p data-timestamp="1032"> And just as our AI guided Sam to the right sections of this thousand page book, it could point out which exercises might be most valuable, considering both Sam’s background and their aims. And it can connect the exercises to Sam’s aims so that, aspirationally, doing those problems feels continuous with Sam’s authentic practice. Even if the exercises do still feel somewhat decontextualized, Sam can at least feel more confident that the work is going to help them do what they want to do.</p>
<h2 id="practice-and-memory">Interlude: Practice and memory <a href="#practice-and-memory">#</a></h2>
<p data-timestamp="1068">Sam ends the day with some rewarding progress on their project, and a newfound understanding of quite a few topics. But this isn’t yet robust knowledge. Sam has very little fluency—if they try to use this material seriously, they’ll probably feel like they’re standing on shaky ground. And more prosaically, <em>they’re likely to forget much of what they just learned</em>.</p>
<p data-timestamp="1092">I’d like to focus on memory for a moment. It’s worth asking: why do we sometimes remember conceptual material, and sometimes not? Often we take a class, or read a book, or even just look something up, and find that a short time later, we’ve retained almost nothing. But sometimes things seem to stick. Why is that?</p>
<p data-timestamp="1115">There are some easier cases. If you’re learning something new in a domain you know well, each new fact connects to lots of prior knowledge. That creates more cues for recall and more opportunities for reinforcement. And if you’re in some setting where you need that knowledge every single day, you’ll find that your memory becomes reliable pretty quickly.</p>
<img src="https://andymatuschak.org/hmwl/img/5-works.png" alt=""><p data-timestamp="1134"> Conceptual material like what Sam’s learning doesn’t usually get reinforced every day like that. But sometimes the world conspires to give those memories the reinforcement they need. Sometimes you read about a topic, then later that evening, that topic comes up in conversation with a collaborator. You have to retrieve what you learned, and that retrieval reinforces the memory. Then, maybe, two days later you need to recall that knowledge again for a project. Each time you reinforce the memory this way, you forget it more slowly. Now perhaps a week can go by, and you’re still likely to remember. Then maybe a few weeks, and a few months, and so on. With a surprisingly small number of retrievals, if they’re placed close enough to each other to avoid forgetting, you can retain that knowledge for months or years.</p>
<img src="https://andymatuschak.org/hmwl/img/5-fails.png" alt=""><p data-timestamp="1185"> By contrast, sometimes when you learn something, it doesn’t come up again until the next week. Then you try to retrieve that knowledge, but maybe it’s already been forgotten. So you have to look it up. That doesn’t reinforce your memory very much. And then if it doesn’t come up again for a while longer, you may still not remember next time. So you have to look it up yet again. And so on. The key insight here is that it’s possible to <em>arrange</em> the top timeline for yourself.</p>
<p data-timestamp="1216">Courses sometimes do, when each problem set consistently interleaves knowledge from all the previous ones. But immersive learning—and for that matter most learning—usually doesn’t arrange this properly, so you usually forget a lot. What if this kind of reinforcement were woven into the grain of the learning medium?</p>
<p data-timestamp="1239">My collaborator Michael Nielsen and I created a quantum computing primer, <a href="https://quantum.country/">Quantum Country</a>, to explore this idea. It’s available for free online. If you head to <a href="https://quantum.country/">quantum.country</a>, you’ll see what looks at first like a normal book.</p>
<img src="https://andymatuschak.org/hmwl/img/5-qc.png" alt=""><p data-timestamp="1255"> After a few minutes of reading, the text is interrupted with a small set of review questions. They’re designed to take just a few seconds each: think the answer to yourself, then mark whether or not you were able to answer correctly. So far, these look like simple flashcards. But as we’ve discussed, even if you can answer these questions now, that doesn’t mean you’ll be able to in a few weeks, or even in a few days.</p>
<p data-timestamp="1285">Notice these markings along the bottom of each question. These represent intervals. So you practice the questions while you’re reading the text, then, one week later, you’ll get an email that says: “Hey, you’ve probably started to forget some of what you read. Do you want to take five minutes to quickly review that material again?” Each time you answer successfully, the interval increases—to a few weeks, then a few months, and so on. If you begin to forget, then the intervals tighten up to provide more reinforcement.</p>
<img src="https://andymatuschak.org/hmwl/img/5-conceptual.png" alt=""><p data-timestamp="1314"> You may have seen systems like this before. Language learners and medical students often use tools called spaced repetition memory systems to remember vocabulary and basic facts. But the same cognitive mechanisms should work for more complex conceptual knowledge as well. There are 112 of these questions scattered through the first chapter of the book on that basis.</p>
<p data-timestamp="1337">Quantum Country is a new medium—a mnemonic medium—integrating a spaced repetition memory system with an explanatory text to make it easier for people to absorb complex material reliably.</p>
<img src="https://andymatuschak.org/hmwl/img/5-retention.png" alt=""><p data-timestamp="1350"> We now have millions of practice data points, so we can start to see how well it’s working. This plot shows the amount of time spent practicing, on the x axis, versus a reader’s demonstrated retention on the y axis— that is, how long a reader was able to go without practicing, and still answer at least 90% of questions correctly. These five dots represent the median user’s first five repetitions, for the first chapter. Notice that the y axis is logarithmic, so we’re seeing a nice exponential growth here. Each extra repetition—constant extra input—yields increasing output—i.e. retention.</p>
<p data-timestamp="1393">In exchange for about an hour and a half of total practice, the median reader was able to correctly answer over a hundred detailed questions about the first chapter, after more than two months without practice. Now, the first chapter takes most readers about four hours to read the first time, so this plot implies that an extra overhead of less than 50% in time commitment can yield months or years of detailed retention.</p>
<img src="https://andymatuschak.org/hmwl/img/5-forgetting-1.png" alt=""><p data-timestamp="1421"> It’s also interesting to explore the counterfactual: how much would people forget without the extra reinforcement? As an experiment, we removed nine questions from the first chapter for some readers, then covertly reinserted the questions into those readers’ practice sessions a month later. This graph shows what happened.</p>
<p data-timestamp="1442">These nine points represent those nine questions. The y axis shows the percentage of readers who were able to answer that question correctly after one month, with no support at all. You can see that some questions are harder than others. One month later, the majority of readers missed the hardest three questions, on the left, about 30% missed the middle three, and about 15% missed the easiest three.</p>
<p data-timestamp="1467">Another group of users got practice while reading the essay, like we saw in the video a moment ago—and for any questions they missed, a bonus round of practice the next day. Then these questions disappeared for a month, at which point they were tested. These readers perform noticeably better, though a big chunk of them are still missing some of these questions.</p>
<img src="https://andymatuschak.org/hmwl/img/5-forgetting-3.png" alt=""><p data-timestamp="1488"> Here’s one last group, like the previous one, except they got one extra round of practice a week after reading the book. Then we tested them again at the one month mark, and that’s what you’re seeing here. Each question takes six seconds on average to answer, so this is less than a minute of extra practice in total for these nine questions. But now for all of these questions, at least 90% of readers were able to answer correctly.</p>
<img src="https://andymatuschak.org/hmwl/img/5-forgetting-q1.png" alt=""><p data-timestamp="1512"> Of course, some readers have a much easier time than others. This left plot focuses on the bottom quartile of users—the readers who missed the most questions while they were first reading the essay. Notice I’ve had to lengthen the y axis downwards. We can see that without any practice, most of them forgot two thirds of these held-out questions. In-essay practice alone still left roughly half of them forgetting roughly half of the questions. But with one extra round of practice, even this bottom quartile of readers performs quite well.</p>
<p data-timestamp="1557">Systems like Quantum Country are useful for more than just quantum computing. In my personal practice, I’ve accumulated thousands and thousands of questions. I write questions about scientific papers, about conversations, about lectures, about memorable meals. All this makes my daily life more rewarding, because I know that if I invest my attention in something, I will internalize it indefinitely.</p>
<p data-timestamp="1596">Central to this is the idea of a daily ritual, a vessel for practice. Like meditation and exercise, I spend about ten minutes a day using my memory system. Because these exponential schedules are very efficient, those ten minutes are enough to maintain my memory for thousands of questions, and to allow me to add up to about forty new questions each day.</p>
<p data-timestamp="1620">But I want to mention a few problems with these memory systems.</p>
<p data-timestamp="1627">One is pattern matching: once a question comes up a few times, I may recognize the text of the question without really thinking about it. This creates the unpleasant feeling of parroting, but I suspect it often leaves my memory brittle: I’ll remember the answer, but only when cued exactly as I’ve practiced. I wish the questions had more variability.</p>
<p data-timestamp="1649">Likewise, the questions are necessarily somewhat abstract. When I face a real problem in that domain, I won’t always recognize what knowledge I should use, or how to adapt it to the situation. A cognitive scientist would say I need to acquire schemas.</p>
<p data-timestamp="1667">Unless I intervene, questions stay the same over years. They’re maintaining memory—but ideally, they would push for further processing—more depth over time.</p>
<p data-timestamp="1679">Finally, returning to this talk’s thesis: memory systems are often too disconnected from my authentic practice. Say I’m studying a topic in signal processing for a creative project. Unless I’m careful, those questions probably won’t feel very connected to my project—they’ll feel like generic textbook questions about signal processing.</p>
<h2 id="dynamic-practice">Demo, part 5: Dynamic practice <a href="#dynamic-practice">#</a></h2>
<p data-timestamp="1704">Let’s return to Sam now, and see if we can apply some of these ideas about practice and memory. Sam did the work to study that signal processing material, so they want to make sure it actually sticks.</p>
<img src="https://andymatuschak.org/hmwl/img/5-widget-1.png" alt=""><p data-timestamp="1716"> They install a home screen widget, which ambiently exposes them to practice prompts drawn from highlights, questions asked, and other activity the AI can access. Sam can flip through these questions while waiting in line or on the bus. Notice that this isn’t a generic signal processing question: it’s grounded in the details of Sam’s BCI project, so that, aspirationally, practice feels more continuous with authentic doing.</p>
<img src="https://andymatuschak.org/hmwl/img/5-widget-3.png" alt=""><p data-timestamp="1750"> These synthesized prompts can vary each time they’re asked, so that Sam gets practice accessing the same idea from different angles. The prompts get deeper and more complex over time, as Sam gets more confident with the material. Notice also that this question isn’t so abstract: it’s really about applying what Sam’s learned, in a bite-sized form factor.</p>
<img src="https://andymatuschak.org/hmwl/img/5-widget-discuss.png" alt=""><p data-timestamp="1777"> The widget can also include open-ended discussion questions. Here Sam gets elaborative feedback—an extra detail to consider in their answer.</p>
<p data-timestamp="1804">When questions are synthesized like this, it’s important that Sam can steer them with feedback. Future questions will be synthesized accordingly.</p>
<img src="https://andymatuschak.org/hmwl/img/5-desktop-task.png" alt=""><p data-timestamp="1813"> So far, we’ve been looking at bite-sized questions Sam can answer while they’re out and about, but if they make time for a longer dedicated session, we can suggest meatier tasks, like this one. What’s more, we can move that work out of fake-practice-land and into Sam’s real context—the Jupyter notebook. Notice that the task is still framed in terms of Sam’s specific aims, rather than some generic signal processing problem.</p>

<p data-timestamp="1849">Now, Sam got into this project not as a “learning exercise”, but <em>as a way to start legitimately participating</em>. To start working with BCIs while playing to existing strengths.</p>
<img src="https://andymatuschak.org/hmwl/img/6-suggestion.png" alt=""><p data-timestamp="1861"> Just as our AI can help Sam find a tractable way into this space, it can also facilitate connections to communities of practice—here suggesting a local neurotech meetup. So Sam goes to the neurotech event, meets a local scientist, and sets up a coffee date. With permission, Sam records the meeting, knowing the notes will probably be helpful later.</p>
<img src="https://andymatuschak.org/hmwl/img/6-task.png" alt=""><p data-timestamp="1881"> And of course, Sam ends up surprised and intrigued quite a lot during this conversation. Our AI can notice these moments and help Sam metabolize them. Here that insight turns into a reflective practice prompt.</p>
<h2 id="design-principles">Design principles <a href="#design-principles">#</a></h2>
<p data-timestamp="1900">Four big design principles are threaded through Sam’s story. I’d like to review them now, and for each, point out the ways I think AI can help.</p>
<p data-timestamp="1911">First, we <em>bring guided learning to authentic contexts</em>, rather than thinking of it as a separate activity. We’re able to make that happen by imagining an AI which can perceive and act across applications on Sam’s computer. And as the audio transcript at the end demonstrated, that can extend to activities outside the computer too. This AI can give appropriate guidance in part because—with permission and executing locally—it can learn from every piece of text that’s ever crossed Sam’s screen, every action they’ve taken on the computer. It can synthesize scaffolded dynamic media, so that Sam can learn by doing, but with guidance.</p>
<p data-timestamp="1957">Then, when explicit learning activities are necessary, we <em>suffuse them with authentic context</em>. The AI grounds all the reading and practice Sam’s doing in their actual aims. It helps Sam match the learning activities to their depth of interest. It draws on important moments that happen when Sam is doing, like insights from that coffee meeting at the end, or questions asked while implementing parts of the project, and brings those moments into study activities</p>
<p data-timestamp="1989">Besides connecting these two domains, we can also strengthen each of them. Our AI suggest tractable ways for Sam to “just dive in” to a new interest, and helped Sam build connections with a community of practice.</p>
<p data-timestamp="2007">Finally, when we’re spending time in explicit learning activities, let’s make sure that learning actually works. Our AI creates a dynamic vessel for ongoing reinforcement it varies over time so that the knowledge transfers well to real situations. And it doesn’t just maintain memory—but increases depth of understanding over time.</p>
<h2 id="chatbot-tutors">Two cheers for chatbot tutors <a href="#chatbot-tutors">#</a></h2>
<p data-timestamp="2035">Most discussion of AI and education at the moment revolves around the framing of chatbot tutors.</p>
<p data-timestamp="2040">I think this framing correctly identifies something really wonderful about language models: they’re great at answering long-tail questions… if the user can articulate the question clearly enough. And if the user’s trying to perform a routine task, chatbot tutors can often diagnose problems and find ways to get the user unstuck. That’s great.</p>
<p data-timestamp="2061">But when I look at others’ visions of chatbot tutors through the much broader framing we’ve been discussing—they’re clearly missing a lot of what I want. I think these visions also often fail to take seriously just how much a real tutor can do. In large part, I think that’s because the authors of these visions are usually thinking about <em>educating</em> (something they want to do to others) rather than <em>learning</em> (something they want for themselves).</p>
<p data-timestamp="2087">Now, a sad truth about the world is that postdocs and graduate students are incredibly underpaid, so it’s surprisingly affordable to get an expert tutor for a technical topic I care about.</p>
<p data-timestamp="2096">But if I hire a real tutor, as an adult, to learn about signal processing, I’ll tell them about my interest in brain-computer interfaces, and I’ll expect them to ground every conversation in that purpose. My goal isn’t to “learn signal processing”, it’s to “participate in the creation of BCIs”. Chatbot tutors aren’t interested in what I’m trying to do; there’s a set of things they think I should know or should be able to do, and they view me as defective until I say the right things.</p>
<p data-timestamp="2128">If I hire a real tutor, I might ask them to sit beside me as I try to actually do something involving the material. They can see everything I’m doing, see what I’m pointing at. If it’s appropriate, I can scoot over, and they can drive for a minute. By comparison, the typical conception of a chatbot tutor lives in a windowless box, can only see whatever’s provided on scraps of paper passed under the door, and can have no effect on the outside world. My goal is to dive in, to immerse myself, to start doing the thing. But these chatbot tutors can’t join me where the real action is. So interactions with them create distance, pull me away from immersion.</p>
<p data-timestamp="2169">If I hire a real tutor, we’ll build a relationship. With every session, they’ll learn more about me—my interests, my strengths, my confusions. Chatbot tutors, as typically conceived, are transactional, amnesic. Now, we could fix that as context windows get longer. But that relationship is also important to my emotional engagement. If I view conversation with my tutor as a kind of peripheral participation in the community I’m hoping to enter—an interaction between novice in the discipline and mentor in the discipline—then tutoring will become just part of doing the thing. But if my interaction with my tutor is transactional, that will tend to make my tutoring sessions feel like “learning time”, separate from doing the thing.</p>
<p data-timestamp="2217">Finally, people talk about how Aristotle was a tutor for Alexander the Great. But what’s most valuable about having Aristotle as a tutor isn’t “diagnosing misconceptions”, but rather that he’s modeling the practices and values of an earnest, intellectually engaged adult. He’s demonstrating how and why he thinks about problems. His taste in the discipline. The high-growth periods we love transform the way we see the world. They reshape our identity.</p>
<p data-timestamp="2247">In my demo earlier, I showed a chatbot, but it didn’t really work like most “chatbot tutors” I see described. It focused all its actions on the user’s interest, rather than bringing its own agenda. It wasn’t trapped in a little text box—it could see and take action in the context of authentic use; it could communicate through dynamic media. It had a deep memory, drawing on everything you’ve ever written or seen.</p>
<p data-timestamp="2271">So in some ways, the system I’ve shown is more like a real tutor. But in my ideal world, I don’t want a tutor; I want to legitimately participate in some new discipline, and to learn what I need as much as possible from interaction with real practitioners.</p>
<p data-timestamp="2288">I view the role of the augmented learning system as helping me act on my creative interests, ideally by letting me just dive in and start doing, as much as possible. That will often mean scaffolding connections to and interactions with communities of practice.</p>
<h2 id="ethics">A note on ethics <a href="#ethics">#</a></h2>
<p data-timestamp="2305">One theme for this Design@Large series is the ethics of AI and its likely enormous social impacts. Let me say: <a href="https://andymatuschak.org/personal-ai-ethics/">I’m tremendously worried about those impacts</a>, in the general case. I’m worried about despots locking in their power, about lowering the bar to bioweapons, about economic chaos. I wouldn’t feel comfortable ethically with researching more powerful frontier models.</p>
<p data-timestamp="2327">But within the narrower domain of learning, my main moral concern is that we’ll end up trapped on a sad, narrow path. A condescending, authoritarian frame dominates the narrative in the future of learning. I’ll caricature it to make the point: with AI, we can take all these defective kids that don’t know the stuff they’re supposed to know, and_ finally get them to know it_! You know: personalized learning! The AI will let us precisely identify where the kids are wrong, or ignorant, and <em>fix them</em>. Then we can fill their heads to the brim with what’s good for them.</p>
<p data-timestamp="2368">The famous “bicycle for the mind” metaphor is better because it has no agenda other than the one you bring. It just lets you reach a wider range of destinations than you could on foot. And it makes the journey fun too, particularly if you’re biking along with some friends. The bicycle asks: where do you want to go?</p>
<p data-timestamp="2385">Of course, that question assumes your destination is well-known and clearly charted on some map. But those most rewarding high-growth experiences are often centered on a creative project. You’re trying to get somewhere no one’s ever gone before—to reach the frontier, then start charting links into the unknown. Learning in service of creation. It’s a dynamic, context-laden kind of learning. It’s about more than just efficiency and correctness. More than just faster gears on a bike. That’s the kind of learning I feel an almost moral imperative to help create.</p>
        
      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shipbreaking (285 pts)]]></title>
            <link>https://www.edwardburtynsky.com/projects/photographs/shipbreaking</link>
            <guid>40424304</guid>
            <pubDate>Tue, 21 May 2024 04:56:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.edwardburtynsky.com/projects/photographs/shipbreaking">https://www.edwardburtynsky.com/projects/photographs/shipbreaking</a>, See on <a href="https://news.ycombinator.com/item?id=40424304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-type="page" data-updated-on="1502822602071" id="canvasWrapper" role="main" data-content-field="main-content"><div data-block-type="2" id="block-23bb69a3997e8cab3a7c">
<h2>SHIPBREAKING</h2><p><strong><em>Artist's Statement</em></strong></p><p>“The original idea for the shipbreaking started a long time ago. About four years after the Exxon Valdez oil spill I heard a radio program where they were talking about the danger of single-hulled ships. The insurance companies were refusing to cover them after 2004, which would force all these ships to be decommissioned. Only double-hulled ships would be allowed on the open sea to prevent that kind of catastrophe from happening again.</p><p>What went off in my mind was, wouldn’t it be interesting to see where these massive vessels will be taken apart. It would be a study of humanity and the skill it takes to dismantle these things. I looked upon the shipbreaking as the ultimate in recycling, in this case of the largest vessels ever made. It turned out that most of the dismantling was happening in India and Bangladesh so that's where I went.” — Edward Burtynsky</p>
</div><div data-block-type="2" id="block-yui_3_17_2_68_1502815486300_8023">
<p>Burtynsky's <em>Shipbreaking</em>&nbsp;photographs, like all his works, appear to us as images of the end of time. The abandoned mines and quarries, the piles of discarded tires, the endless fields of oil derricks, and the huge monoliths of retired tankers show how our attempts at industrial "progress" often leave a residue of destruction. Nevertheless there is something uncannily beautiful and breathtaking in the very expansiveness of these images―it is as if the vastness of their perspective somehow opens onto the longer view of things. For Burtynsky, nature itself, over time, can reclaim even the most ambitious of human incursions into the land. As long as human needs and desires change, so too will the landscape.</p>
</div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Modos Paper Monitor Pre-Launch on Crowd Supply (146 pts)]]></title>
            <link>https://www.modos.tech/blog/modos-paper-monitor-pre-launch-on-crowd-supply</link>
            <guid>40423746</guid>
            <pubDate>Tue, 21 May 2024 03:23:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.modos.tech/blog/modos-paper-monitor-pre-launch-on-crowd-supply">https://www.modos.tech/blog/modos-paper-monitor-pre-launch-on-crowd-supply</a>, See on <a href="https://news.ycombinator.com/item?id=40423746">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-w-id="19c00176-298a-f149-969d-4f67928c4aa4"><p>Today, I'm excited to announce the <a href="https://www.crowdsupply.com/modos-tech/modos-paper-monitor">pre-launch page of the Modos Paper Monitor on Crowd Supply</a>. The <strong>Modos Paper Monitor</strong> is an open-hardware 13.3-inch, 1600 x 1200 monochrome or color e-ink monitor with a fast 60 Hz refresh rate, low latency, multiple image modes and dithering options, and flexible screen update control. It can be connected using HDMI and USB-C and works on Linux, macOS, and Windows.&nbsp;</p><figure><p><img src="https://assets-global.website-files.com/61a133075b8ce58a48f1bed1/664c0987c07254343e6bf379_modos-paper-monitor-mono-prototype-01_jpg_gallery-lg.jpg" loading="lazy" alt=""></p><figcaption>Modos Paper Monitor</figcaption></figure><p>Aimed at developers and makers, the <strong>Modos Development Kit</strong> uses the same display controller that powers our Paper Monitor with your choice of e-ink screen. It is available with 6-inch and 13.3-inch monochrome or color panels.</p><figure><p><img src="https://assets-global.website-files.com/61a133075b8ce58a48f1bed1/664c09a85140827f96dce3f9_modos-paper-monitor-mainboard-13in-screen-01_jpg_gallery-lg.jpg" loading="lazy" alt=""></p><figcaption>Modos Development Kit</figcaption></figure><p>‍</p><p><iframe width="100%" height="515" src="https://www.youtube.com/embed/pXn-bAwzNv4?si=u3xlX3lbrojRJbEy" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p><p>‍</p><h3>The Beginning of the Journey</h3><p>This product is the result of a journey that began in February 2021. Like many others, I&nbsp;found myself spending&nbsp;over 12 hours a day in front of a computer or digital screen. My days&nbsp;were filled&nbsp;with&nbsp;supporting students, attending meetings, reading documentation, learning, and programming. By the end of the day, my eyes felt strained, and I was tired and distracted.</p><p>I realized that the digital tools we depend on for our day-to-day activities could be improved. They should be more beneficial to our health, less demanding of our attention, and better suited to our needs. I wanted to create a space free from distractions and eyestrain where I could focus on reading, writing, and thinking—a productive environment conducive to my health. This realization led me to reimagine personal computing and build calm, intentional technology.</p><h3>Shared Challenges</h3><p>As I shared my vision with others, I found that many people faced similar challenges: writers, authors, journalists, programmers, students, teachers, doctors, lawyers, engineers, makers, digital minimalists, people with visual impairments, those with health issues and many more.</p><p>We all struggled with the same problems—headaches, eyestrain, and distractions caused by our reliance on digital screens. This shared experience fueled my desire to create technology that respects our time, attention, and well-being.</p><h3>Open-Hardware and Open-Source</h3><p>As an open-hardware and open-source company, we strive to create an environment where experimentation, learning, and creativity can flourish. For those who wish to delve deeper, <a href="https://github.com/Modos-Labs/Glider">our README on GitHub</a> provides a comprehensive primer on e-ink technology. </p><p>We believe that open-source and open-hardware are crucial to envisioning and building a different reality—one where technology supports our well-being and fosters a healthier relationship with our digital lives.</p><p>We invite you to <a href="https://www.crowdsupply.com/modos-tech/modos-paper-monitor">sign up on Crowd Supply</a> to stay informed about the latest news and updates on our project.<br></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chameleon: Meta's New Multi-Modal LLM (272 pts)]]></title>
            <link>https://arxiv.org/abs/2405.09818</link>
            <guid>40423082</guid>
            <pubDate>Tue, 21 May 2024 01:37:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2405.09818">https://arxiv.org/abs/2405.09818</a>, See on <a href="https://news.ycombinator.com/item?id=40423082">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2405.09818">View PDF</a>
    <a href="https://arxiv.org/html/2405.09818v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Armen Aghajanyan [<a href="https://arxiv.org/show-email/4972e8c8/2405.09818">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 16 May 2024 05:23:41 UTC (26,721 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stripe increasing "instant payout" fees by 50% (144 pts)]]></title>
            <link>https://support.stripe.com/questions/june-2024-pricing-update-for-instant-payouts-for-businesses-in-the-united-states</link>
            <guid>40423035</guid>
            <pubDate>Tue, 21 May 2024 01:31:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://support.stripe.com/questions/june-2024-pricing-update-for-instant-payouts-for-businesses-in-the-united-states">https://support.stripe.com/questions/june-2024-pricing-update-for-instant-payouts-for-businesses-in-the-united-states</a>, See on <a href="https://news.ycombinator.com/item?id=40423035">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-prop-key="answer" data-react-class="QuestionPage" data-react-props="{&quot;showL10nStaleNotice&quot;:false,&quot;showL10nPendingNotice&quot;:false,&quot;voted&quot;:false,&quot;slug&quot;:&quot;june-2024-pricing-update-for-instant-payouts-for-businesses-in-the-united-states&quot;,&quot;tags&quot;:[],&quot;relatedArticles&quot;:[{&quot;type&quot;:&quot;question&quot;,&quot;url&quot;:&quot;/questions/june-2024-pricing-update-for-european-bank-based-payment-methods-for-uk-businesses&quot;,&quot;title&quot;:&quot;June 2024 pricing update for European bank-based payment methods for UK businesses&quot;,&quot;description&quot;:&quot;Starting June 1, 2024, UK businesses will be charged a 1.5% international transaction fee when customers pay with a European bank-based payment method…&quot;,&quot;id&quot;:&quot;47a93692-07c1-4314-bac6-87b9c6e2dccf&quot;,&quot;tags&quot;:[]},{&quot;type&quot;:&quot;question&quot;,&quot;url&quot;:&quot;/questions/june-2024-pricing-updates-for-bacs-direct-debit&quot;,&quot;title&quot;:&quot;June 2024 pricing updates for Bacs Direct Debit&quot;,&quot;description&quot;:&quot;Starting June 1, 2024, Stripe is making pricing changes for Bacs Direct Debit transactions. What’s changing? Fee cap: When your customers pay with…&quot;,&quot;id&quot;:&quot;5a92bc1c-9ea8-4321-b0d2-8e3fa0742c5b&quot;,&quot;tags&quot;:[]},{&quot;type&quot;:&quot;question&quot;,&quot;url&quot;:&quot;/questions/april-2024-pricing-update-for-businesses-on-standard-pricing-in-australia&quot;,&quot;title&quot;:&quot;April 2024 pricing update for businesses on standard pricing in Australia&quot;,&quot;description&quot;:&quot;Starting April 1, 2024, we will decrease our domestic card processing fees for Australian businesses on standard pricing from 1.75% + A$0.30 to 1.70…&quot;,&quot;id&quot;:&quot;93a72c50-1b86-4903-8fb6-cc1197a74095&quot;,&quot;tags&quot;:[]}]}" data-react-props-from-content="" data-locale="en-US" data-article-id="aee91921-d4e8-495e-8ff3-d91433a78e34">
    

<p>With Instant Payouts, Stripe lets you access your funds and pay them out to your bank account or debit card immediately. Starting June 1, 2024, the fee for <a href="https://stripe.com/docs/payouts/instant-payouts" target="_blank" rel="noopener noreferrer"><u>Instant Payouts</u></a> in the US will increase from 1% to 1.5% per payout. You can always pay out your funds <a href="https://dashboard.stripe.com/account/payouts" target="_blank" rel="noopener noreferrer"><u>using our standard schedule</u></a> (2 business days) for free. This applies to all direct Stripe accounts, as well as <a href="https://stripe.com/docs/connect/explore-connect-guide#account-types" target="_blank" rel="noopener noreferrer"><u>standard connected accounts</u></a>. </p>

<h2>FAQs</h2>
<h3>How can I calculate the impact of these fee changes to my business?</h3>
<p>The best way to calculate the impact these changes might have to your business is to use the <a href="https://dashboard.stripe.com/" target="_blank" rel="noopener noreferrer"><u>Stripe Dashboard.</u></a></p>
<p>To understand the impact of changes:</p>
<ol start="1">
<li>Go to the <a href="https://dashboard.stripe.com/balance/overview" target="_blank" rel="noopener noreferrer"><u>Balances section</u></a> of your Dashboard</li>
<li>Navigate to the <a href="https://dashboard.stripe.com/payouts" target="_blank" rel="noopener noreferrer"><u>Payouts</u></a> section of your Balances page</li>
<li>Filter by “method” and ensure that “Instant” is selected and press the apply button</li>
<li>You can then export all Instant Payouts to calculate impact</li>
</ol>
<h3>What can help me offset these costs?</h3>
<p>You can always pay out your funds using our standard schedule (2 business days) for free.</p>

<h3>I’m a Connect platform. How does this affect my connected accounts?</h3>
<p>This applies to all direct Stripe accounts, as well as <a href="https://stripe.com/docs/connect/explore-connect-guide#account-types" target="_blank" rel="noopener noreferrer"><u>Standard connected accounts</u></a>. Stripe will automatically notify your Standard connected accounts about the change. They will also be able to see the new pricing when they opt for an Instant Payout. </p>
<p>If you use Connect with Custom accounts, there are no changes to your fees.</p>

<h3>How can I understand which of my connected accounts are impacted?</h3>
<p>To understand which of your connected accounts are impacted, please reach out to the <a href="https://support.stripe.com/" rel="noopener noreferrer"><u>Stripe team</u></a>. &nbsp;</p>

<h3>Why did I get this notice?</h3>
<p>This support page provides you with more details on the notice we sent you describing changes to certain fees. That notice is a legal notice sent to Stripe users, even those who have unsubscribed from optional marketing notices. You cannot unsubscribe from legal notices, but if you’d prefer not to receive any further legal notices from Stripe, you can close your account by following <a href="https://support.stripe.com/questions/close-a-stripe-account#:~:text=Close%20a%20Stripe%20account%20%3A%20Stripe%3A%20Help%20%26%20Support&amp;text=The%20owner%20of%20the%20account,Close%20account...%27" rel="noopener noreferrer"><u>these steps</u></a>. </p>
<p>Your continued use of Stripe's services after June 1, 2024 is subject to these fee changes. Any termination rights you have under your agreement with us are unaffected by this change.</p>

<h3>Do I need to take action? </h3>
<p>There’s no action necessary for you at this time and these changes will automatically apply to your business and / or your connected accounts on June 1, 2024. </p>

<h3>Contact support</h3>
<p>If you have questions beyond what's detailed above, please reach out to the <a href="https://support.stripe.com/" rel="noopener noreferrer"><u>Stripe team</u></a>. </p>




  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Regular expression matching can be simple and fast (2007) (115 pts)]]></title>
            <link>https://swtch.com/%7Ersc/regexp/regexp1.html</link>
            <guid>40422997</guid>
            <pubDate>Tue, 21 May 2024 01:26:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://swtch.com/%7Ersc/regexp/regexp1.html">https://swtch.com/%7Ersc/regexp/regexp1.html</a>, See on <a href="https://news.ycombinator.com/item?id=40422997">Hacker News</a></p>
<div id="readability-page-1" class="page">

<h2>
Regular Expression Matching Can Be Simple And Fast
<br>
(but is slow in Java, Perl, PHP, Python, Ruby, ...)
</h2>
<h2>
<a href="https://swtch.com/~rsc/">Russ Cox</a>
<br>
<i>rsc@swtch.com</i>
<br>
January 2007
</h2>


<h2>Introduction</h2>

<p>
This is a tale of two approaches to regular expression matching.
One of them is in widespread use in the
standard interpreters for many languages, including Perl.
The other is used only in a few places, notably most implementations
of awk and grep.
The two approaches have wildly different
performance characteristics:
</p>



<p>
Let's use superscripts to denote string repetition,
so that 
<code>a?<sup>3</sup>a<sup>3</sup></code>
is shorthand for
<code>a?a?a?aaa</code>.
The two graphs plot the time required by each approach
to match the regular expression 
<code>a?</code><sup><i>n</i></sup><code>a</code><sup><i>n</i></sup>
against the string <code>a</code><sup><i>n</i></sup>.
</p>

<p>
Notice that Perl requires over sixty seconds to match
a 29-character string.
The other approach, labeled Thompson NFA for
reasons that will be explained later,
requires twenty <i>microseconds</i> to match the string.
That's not a typo.  The Perl graph plots time in seconds,
while the Thompson NFA graph plots time in microseconds:
the Thompson NFA implementation
is a million times faster than Perl
when running on a miniscule 29-character string.
The trends shown in the graph continue: the
Thompson NFA handles a 100-character string in under 200 microseconds,
while Perl would require over 10<sup>15</sup> years.
(Perl is only the most conspicuous example of a large
number of popular programs that use the same algorithm;
the above graph could have been Python, or PHP, or Ruby,
or many other languages.  A more detailed
graph later in this article presents data for other implementations.)
</p>

<p>
It may be hard to believe the graphs: perhaps you've used Perl,
and it never seemed like regular expression matching was
particularly slow.
Most of the time, in fact, regular expression matching in Perl
is fast enough.  
As the graph shows, though, it is possible
to write so-called “pathological” regular expressions that
Perl matches very <i>very</i> slowly.
In contrast, there are no regular expressions that are 
pathological for the Thompson NFA implementation.
Seeing the two graphs side by side prompts the question, 
“why doesn't Perl use the Thompson NFA approach?”
It can, it should, and that's what the rest of this article is about.
</p>

<p>
Historically, regular expressions are one of computer science's
shining examples of how using good theory leads to good programs.
They were originally developed by theorists as a
simple computational model,
but Ken Thompson introduced them to
programmers in his implementation of the text editor QED
for CTSS.
Dennis Ritchie followed suit in his own implementation
of QED, for GE-TSS.
Thompson and Ritchie would go on to create Unix,
and they brought regular expressions with them.
By the late 1970s, regular expressions were a key
feature of the Unix landscape, in tools such as
ed, sed, grep, egrep, awk, and lex.
</p>

<p>
Today, regular expressions have also become a shining
example of how ignoring good theory leads to bad programs.
The regular expression implementations used by
today's popular tools are significantly slower
than the ones used in many of those thirty-year-old Unix tools.
</p>

<p>
This article reviews the good theory: 
regular expressions, finite automata, 
and a regular expression search algorithm
invented by Ken Thompson in the mid-1960s.
It also puts the theory into practice, describing 
a simple implementation of Thompson's algorithm.
That implementation, less than 400 lines of C,
is the one that went head to head with Perl above.
It outperforms the more complex real-world
implementations used by
Perl, Python, PCRE, and others.
The article concludes with a discussion of how 
theory might yet be converted into practice
in the real-world implementations.
</p>

<h2>
Regular Expressions
</h2>


<p>
Regular expressions are a notation for
describing sets of character strings.
When a particular string is in the set
described by a regular expression,
we often say that the regular expression
<i>matches</i>
the string.
</p>

<p>
The simplest regular expression is a single literal character.
Except for the special metacharacters 
<code>*+?()|</code>,
characters match themselves.
To match a metacharacter, escape it with
a backslash:
<code>\+</code>
matches a literal plus character.
</p>

<p>
Two regular expressions can be alternated or concatenated to form a new
regular expression:
if <i>e</i><sub>1</sub> matches
<i>s</i>
and <i>e</i><sub>2</sub> matches
<i>t</i>,
then <i>e</i><sub>1</sub><code>|</code><i>e</i><sub>2</sub> matches
<i>s</i>
or
<i>t</i>,
and
<i>e</i><sub>1</sub><i>e</i><sub>2</sub>
matches 
<i>st</i>.
</p>

<p>
The metacharacters
<code>*</code>,
<code>+</code>,
and
<code>?</code>
are repetition operators:
<i>e</i><sub>1</sub><code>*</code>
matches a sequence of zero or more (possibly different)
strings, each of which match <i>e</i><sub>1</sub>;
<i>e</i><sub>1</sub><code>+</code>
matches one or more; 
<i>e</i><sub>1</sub><code>?</code>
matches zero or one.
</p>

<p>
The operator precedence, from weakest to strongest binding, is
first alternation, then concatenation, and finally the
repetition operators.
Explicit parentheses can be used to force different meanings,
just as in arithmetic expressions.
Some examples:
<code>ab|cd</code>
is equivalent to
<code>(ab)|(cd)</code>;
<code>ab*</code>
is equivalent to
<code>a(b*)</code>.
</p>

<p>
The syntax described so far is a subset of the traditional Unix
egrep
regular expression syntax.
This subset suffices to describe all regular
languages: loosely speaking, a regular language is a set
of strings that can be matched in a single pass through
the text using only a fixed amount of memory.
Newer regular expression facilities (notably Perl and
those that have copied it) have added 
<a href="http://www.perl.com/doc/manual/html/pod/perlre.html">many new operators
and escape sequences</a>.  These additions make the regular
expressions more concise, and sometimes more cryptic, but usually
not more powerful:
these fancy new regular expressions almost always have longer
equivalents using the traditional syntax.
</p>

<p>
One common regular expression extension that 
does provide additional power is called
<i>backreferences</i>.
A backreference like
<code>\1</code>
or
<code>\2</code>
matches the string matched
by a previous parenthesized expression, and only that string:
<code>(cat|dog)\1</code>
matches
<code>catcat</code>
and
<code>dogdog</code>
but not
<code>catdog</code>
nor
<code>dogcat</code>.
As far as the theoretical term is concerned,
regular expressions with backreferences
are not regular expressions.
The power that backreferences add comes at great cost:
in the worst case, the best known implementations require
exponential search algorithms,
like the one Perl uses.
Perl (and the other languages)
could not now remove backreference support,
of course, but they could employ much faster algorithms
when presented with regular expressions that don't have
backreferences, like the ones considered above.
This article is about those faster algorithms.
</p>

<h2>
Finite Automata
</h2>



<p>
Another way to describe sets of character strings is with
finite automata.
Finite automata are also known as state machines,
and we will use “automaton” and “machine” interchangeably.
</p>

<p>
As a simple example, here is a machine recognizing
the set of strings matched by the regular expression
<code>a(bb)+a</code>:
</p>

<p><img src="https://swtch.com/%7Ersc/regexp/fig0.png" alt="DFA for a(bb)+a" width="278" height="54"></p>

<p>
A finite automaton is always in one of its states,
represented in the diagram by circles.
(The numbers inside the circles are labels to make this
discussion easier; they are not part of the machine's operation.)
As it reads the string, it switches from state to state.
This machine has two special states: the start state <i>s</i><sub>0</sub>
and the matching state <i>s</i><sub>4</sub>.
Start states are depicted with lone arrowheads pointing at them,
and matching states are drawn as a double circle.
</p>

<p>
The machine reads an input string one character at a time,
following arrows corresponding to the input to move from
state to state.
Suppose the input string is
<code>abbbba</code>.
When the machine reads the first letter of the string, the
<code>a</code>,
it is in the start state <i>s</i><sub>0</sub>.  It follows the
<code>a</code>
arrow to state <i>s</i><sub>1</sub>.
This process repeats as the machine reads the rest of the string:
<code>b</code>
to
<code><i>s</i><sub>2</sub></code>,
<code>b</code>
to
<code><i>s</i><sub>3</sub></code>,
<code>b</code>
to
<code><i>s</i><sub>2</sub></code>,
<code>b</code>
to
<code><i>s</i><sub>3</sub></code>,
and finally
<code>a</code>
to
<code><i>s</i><sub>4</sub></code>.
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig1.png" alt="DFA execution on abbbba" width="357" height="426"></p>
<p>
The machine ends in <i>s</i><sub>4</sub>, a matching state, so it
matches the string.
If the machine ends in a non-matching state, it does not 
match the string.
If, at any point during the machine's execution, there is no
arrow for it to follow corresponding to the current
input character, the machine stops executing early.
</p>

<p>
The machine we have been considering is called a
<i>deterministic</i>
finite automaton (DFA),
because in any state, each possible input letter
leads to at most one new state.
We can also create machines
that must choose between multiple possible next states.
For example, this machine is equivalent to the previous
one but is not deterministic:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig2.png" alt="NFA for a(bb)+a" width="278" height="54"></p>
<p>
The machine is not deterministic because if it reads a
<code>b</code>
in state <i>s</i><sub>2</sub>, it has multiple choices for the next state:
it can go back to <i>s</i><sub>1</sub> in hopes of seeing another
<code>bb</code>,
or it can go on to <i>s</i><sub>3</sub> in hopes of seeing the final
<code>a</code>.
Since the machine cannot peek ahead to see the rest of
the string, it has no way to know which is the correct decision.
In this situation, it turns out to be interesting to
let the machine
<i>always guess correctly</i>.
Such machines are called non-deterministic finite automata
(NFAs or NDFAs).
An NFA matches an input string if there is some way 
it can read the string and follow arrows to a matching state.
</p>

<p>
Sometimes it is convenient to let NFAs have arrows with no
corresponding input character.  We will leave these arrows unlabeled.
An NFA can, at any time, choose to follow an unlabeled arrow
without reading any input.
This NFA is equivalent to the previous two, but the unlabeled arrow
makes the correspondence with
<code>a(bb)+a</code>
clearest:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig3.png" alt="Another NFA for a(bb)+a" width="278" height="39"></p>

<h2>
Converting Regular Expressions to NFAs
</h2>

<p>
Regular expressions and NFAs turn out to be exactly
equivalent in power: every regular expression has an
equivalent NFA (they match the same strings) and vice versa.
(It turns out that DFAs are also equivalent in power 
to NFAs and regular expressions; we will see this later.)
There are multiple ways to translate regular expressions into NFAs.
The method described here was first described by Thompson
in his 1968 CACM paper.
</p>

<p>
The NFA for a regular expression is built up from partial NFAs
for each subexpression, with a different construction for
each operator.  The partial NFAs have
no matching states: instead they have one or more dangling arrows,
pointing to nothing.  The construction process will finish by
connecting these arrows to a matching state.
</p>

<p>
The NFAs for matching single characters look like:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig4.png" alt="Single-character NFA" width="113" height="21"></p>
<p>
The NFA for the concatenation <i>e</i><sub>1</sub><i>e</i><sub>2</sub>
connects the final arrow of the <i>e</i><sub>1</sub> 
machine to the start of the <i>e</i><sub>2</sub> machine:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig5.png" alt="Concatenation NFA" width="242" height="20"></p>
<p>
The NFA for the alternation <i>e</i><sub>1</sub><code>|</code><i>e</i><sub>2</sub>
adds a new start state with a choice of either the
<i>e</i><sub>1</sub> machine or the <i>e</i><sub>2</sub> machine.
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig6.png" alt="Alternation NFA" width="202" height="62"></p>
<p>
The NFA for <i>e</i><code>?</code> alternates the <i>e</i> machine with an empty path:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig7.png" alt="Zero or one NFA" width="184" height="56"></p>
<p>
The NFA for <i>e</i><code>*</code> uses the same alternation but loops a 
matching <i>e</i> machine back to the start:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig8.png" alt="Zero or more NFA" width="184" height="56"></p>
<p>
The NFA for <i>e</i><code>+</code> also creates a loop, but one that
requires passing through <i>e</i> at least once:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig9.png" alt="One or more NFA" width="190" height="41"></p>

<p>
Counting the new states in the diagrams above, we can see
that this technique creates exactly one state per character
or metacharacter in the regular expression,
excluding parentheses.
Therefore the number of states in the final NFA is at most
equal to the length of the original regular expression.
</p>

<p>
Just as with the example NFA discussed earlier, it is always possible
to remove the unlabeled arrows, and it is also always possible to generate
the NFA without the unlabeled arrows in the first place.
Having the unlabeled arrows makes the NFA easier for us to read
and understand, and they also make the C representation
simpler, so we will keep them.
</p>

<h2>
Regular Expression Search Algorithms
</h2>

<p>
Now we have a way to test whether a regular expression
matches a string: convert the regular expression to an NFA
and then run the NFA using the string as input.
Remember that NFAs are endowed with the ability to guess
perfectly when faced with a choice of next state:
to run the NFA using an ordinary computer, we must find
a way to simulate this guessing.
</p>

<p>
One way to simulate perfect guessing is to guess
one option, and if that doesn't work, try the other.
For example, consider the NFA for
<code>abab|abbb</code>
run on the string
<code>abbb</code>:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig10.png" alt="NFA for abab|abbb" width="364" height="79"></p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig11.png" alt="Backtracking execution on abbb" width="729" height="619"></p>
<p>
At step 0, the NFA must make a choice: try to match
<code>abab</code>
or
try to match
<code>abbb</code>?
In the diagram, the NFA tries
<code>abab</code>,
but that fails after step 3.
The NFA then tries the other choice, leading to step 4 and eventually a match.
This backtracking approach
has a simple recursive implementation
but can read the input string many times
before succeeding.
If the string does not match,
the machine must try
<i>all</i>
possible execution paths before
giving up.
The NFA tried only two different paths in the example,
but in the worst case, there can be exponentially
many possible execution paths, leading to very slow run times.
</p>

<p>
A more efficient but more complicated way to simulate perfect
guessing is to guess both options simultaneously. 
In this approach, the simulation allows the machine
to be in multiple states at once.  To process each letter,
it advances all the states along all the arrows that
match the letter.
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig12.png" alt="Parallel execution on abbb" width="329" height="511"></p>
<p>
The machine starts in the start state and all the states
reachable from the start state by unlabeled arrows.
In steps 1 and 2, the NFA is in two states simultaneously.
Only at step 3 does the state set narrow down to a single state.
This multi-state approach tries both paths at the same time,
reading the input only once.
In the worst case, the NFA might be in
<i>every</i>
state at each step, but this results in at worst a constant amount
of work independent of the length of the string,
so arbitrarily
large input strings can be processed in linear time.
This is a dramatic improvement over the exponential time
required by the backtracking approach.
The efficiency comes from tracking the set of reachable
states but
<i>not</i>
which paths were used to reach them.
In an NFA with 
<i>n</i>
nodes, there can only be 
<i>n</i>
reachable states at any step, but there might be
2<sup><i>n</i></sup> paths through the NFA.
</p>

<h2>
Implementation
</h2>

<p>
Thompson introduced the multiple-state simulation approach
in his 1968 paper.
In his formulation, the states of the NFA were represented
by small machine-code sequences, and the list of possible states
was just a sequence of function call instructions.
In essence, Thompson compiled the regular expression into clever
machine code.
Forty years later, computers are much faster and the 
machine code approach is not as necessary.
The following sections
present an implementation written in portable ANSI C.
The full source code (under 400 lines)
and the benchmarking scripts are 
<a href="https://swtch.com/~rsc/regexp/">available online</a>.
(Readers who are unfamiliar or uncomfortable with C or pointers should
feel free to read the descriptions and skip over the actual code.)
</p>

<h2 id="compiling">
Implementation: Compiling to NFA
</h2>

<p>
The first step is to compile the regular expression
into an equivalent NFA.
In our C program, we will represent an NFA as a
linked collection of 
<code>State</code>
structures:
</p>
<pre>struct State
{
	int c;
	State *out;
	State *out1;
	int lastlist;
};
</pre><p>
Each
<code>State</code>
represents one of the following three NFA fragments,
depending on the value of
<code>c</code>.
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig13.png" alt="Possible per-State NFA fragments" width="340" height="109"></p>
<p>
(<code>Lastlist</code>
is used during execution and is explained in the next section.)
</p>

<p>
Following Thompson's paper,
the compiler builds an NFA from a regular expression in
<i>postfix</i>
notation with dot
(<code>.</code>) added
as an explicit concatenation operator.
A separate function
<code>re2post</code>
rewrites infix regular expressions like
“<code>a(bb)+a</code>”
into equivalent postfix expressions like
“<code>abb.+.a.</code>”.
(A “real” implementation would certainly
need to use dot as the “any character” metacharacter
rather than as a concatenation operator.
A real implementation would also probably build the 
NFA during parsing rather than build an explicit postfix expression.
However, the postfix version is convenient and follows 
Thompson's paper more closely.)
</p>

<p>
As the compiler scans the postfix expression, it maintains
a stack of computed NFA fragments.
Literals push new NFA fragments onto the stack, while
operators pop fragments off the stack and then
push a new fragment.
For example, 
after compiling the
<code>abb</code> in <code>abb.+.a.</code>,
the stack contains NFA fragments for
<code>a</code>,
<code>b</code>,
and
<code>b</code>.
The compilation of the
<code>.</code>
that follows pops the two
<code>b</code>
NFA fragment from the stack and pushes an NFA fragment for the
concatenation
<code>bb.</code>.
Each NFA fragment is defined by its start state and its
outgoing arrows:
</p><pre>struct Frag
{
	State *start;
	Ptrlist *out;
};
</pre><p>
<code>Start</code>
points at the start state for the fragment,
and
<code>out</code>
is a list of pointers to 
<code>State*</code>
pointers that are not yet connected to anything.
These are the dangling arrows in the NFA fragment.
</p>

<p>
Some helper functions manipulate pointer lists:
</p><pre>Ptrlist *list1(State **outp);
Ptrlist *append(Ptrlist *l1, Ptrlist *l2);

void patch(Ptrlist *l, State *s);
</pre><p>
<code>List1</code>
creates a new pointer list containing the single pointer
<code>outp</code>.
<code>Append</code>
concatenates two pointer lists, returning the result.
<code>Patch</code>
connects the dangling arrows in the pointer list
<code>l</code>
to the state
<code>s</code>:
it sets
<code>*outp</code>
<code>=</code>
<code>s</code>
for each pointer
<code>outp</code>
in
<code>l</code>.
</p>

<p>
Given these primitives and a fragment stack,
the compiler is a simple loop over the postfix expression.
At the end, there is a single fragment left:
patching in a matching state completes the NFA.
</p><pre>State*
post2nfa(char *postfix)
{
	char *p;
	Frag stack[1000], *stackp, e1, e2, e;
	State *s;

	#define push(s) *stackp++ = s
	#define pop()   *--stackp

	stackp = stack;
	for(p=postfix; *p; p++){
		switch(*p){
		/* <i>compilation cases, described below</i> */
		}
	}
	
	e = pop();
	patch(e.out, matchstate);
	return e.start;
}
</pre><p><a id="compile"></a>
The specific compilation cases mimic the translation 
steps described earlier.
</p>

<table>
<tbody><tr><td><p>
Literal characters:
</p><pre>default:
	s = state(*p, NULL, NULL);
	push(frag(s, list1(&amp;s-&gt;out));
	break;
</pre>
</td><td><img src="https://swtch.com/%7Ersc/regexp/fig14.png" alt="" width="61" height="24">

</td></tr><tr><td><p>
Catenation:
</p><pre>case '.':
	e2 = pop();
	e1 = pop();
	patch(e1.out, e2.start);
	push(frag(e1.start, e2.out));
	break;
</pre>
</td><td><img src="https://swtch.com/%7Ersc/regexp/fig15.png" alt="" width="182" height="20">

</td></tr><tr><td><p>
Alternation:
</p><pre>case '|':
	e2 = pop();
	e1 = pop();
	s = state(Split, e1.start, e2.start);
	push(frag(s, append(e1.out, e2.out)));
	break;
</pre>
</td><td><img src="https://swtch.com/%7Ersc/regexp/fig16.png" alt="" width="140" height="62">

</td></tr><tr><td><p>
Zero or one:
</p><pre>case '?':
	e = pop();
	s = state(Split, e.start, NULL);
	push(frag(s, append(e.out, list1(&amp;s-&gt;out1))));
	break;
</pre>
</td><td><img src="https://swtch.com/%7Ersc/regexp/fig17.png" alt="" width="140" height="68">

</td></tr><tr><td><p>
Zero or more:
</p><pre>case '*':
	e = pop();
	s = state(Split, e.start, NULL);
	patch(e.out, s);
	push(frag(s, list1(&amp;s-&gt;out1)));
	break;
</pre>
</td><td><img src="https://swtch.com/%7Ersc/regexp/fig18.png" alt="" width="131" height="68">

</td></tr><tr><td><p>
One or more:
</p><pre>case '+':
	e = pop();
	s = state(Split, e.start, NULL);
	patch(e.out, s);
	push(frag(e.start, list1(&amp;s-&gt;out1)));
	break;
</pre>
</td><td><img src="https://swtch.com/%7Ersc/regexp/fig19.png" alt="" width="140" height="53">
</td></tr></tbody></table>

<h2>
Implementation: Simulating the NFA
</h2>

<p>
Now that the NFA has been built, we need to simulate it.
The simulation requires tracking 
<code>State</code>
sets, which are stored as a simple array list:
</p><pre>struct List
{
	State **s;
	int n;
};
</pre><p>
The simulation uses two lists:
<code>clist</code>
is the current set of states that the NFA is in,
and
<code>nlist</code>
is the next set of states that the NFA will be in,
after processing the current character.
The execution loop initializes
<code>clist</code>
to contain just the start state and then
runs the machine one step at a time.
</p><pre>int
match(State *start, char *s)
{
	List *clist, *nlist, *t;

	/* l1 and l2 are preallocated globals */
	clist = startlist(start, &amp;l1);
	nlist = &amp;l2;
	for(; *s; s++){
		step(clist, *s, nlist);
		t = clist; clist = nlist; nlist = t;	/* swap clist, nlist */
	}
	return ismatch(clist);
}
</pre><p>
To avoid allocating on every iteration of the loop,
<code>match</code>
uses two preallocated lists
<code>l1</code>
and
<code>l2</code>
as
<code>clist</code>
and
<code>nlist</code>,
swapping the two after each step.
</p>

<p>
If the final state list contains the matching state,
then the string matches.
</p><pre>int
ismatch(List *l)
{
	int i;

	for(i=0; i&lt;l-&gt;n; i++)
		if(l-&gt;s[i] == matchstate)
			return 1;
	return 0;
}
</pre>

<p>
<code>Addstate</code>
adds a state to the list,
but not if it is already on the list.
Scanning the entire list for each add would be inefficient;
instead the variable
<code>listid</code>
acts as a list generation number.
When
<code>addstate</code>
adds
<code>s</code>
to a list,
it records
<code>listid</code>
in
<code>s-&gt;lastlist</code>.
If the two are already equal,
then 
<code>s</code>
is already on the list being built.
<code>Addstate</code>
also follows unlabeled arrows:
if 
<code>s</code>
is a
<code>Split</code>
state with two unlabeled arrows to new states,
<code>addstate</code>
adds those states to the list instead of
<code>s</code>.
</p><pre>void
addstate(List *l, State *s)
{
	if(s == NULL || s-&gt;lastlist == listid)
		return;
	s-&gt;lastlist = listid;
	if(s-&gt;c == Split){
		/* follow unlabeled arrows */
		addstate(l, s-&gt;out);
		addstate(l, s-&gt;out1);
		return;
	}
	l-&gt;s[l-&gt;n++] = s;
}
</pre>

<p>
<code>Startlist</code>
creates an initial state list by adding just the start state:
</p><pre>List*
startlist(State *s, List *l)
{
	listid++;
	l-&gt;n = 0;
	addstate(l, s);
	return l;
}
</pre>

<p>
Finally,
<code>step</code>
advances the NFA past a single character, using
the current list
<code>clist</code>
to compute the next list
<code>nlist</code>.
</p><pre>void
step(List *clist, int c, List *nlist)
{
	int i;
	State *s;

	listid++;
	nlist-&gt;n = 0;
	for(i=0; i&lt;clist-&gt;n; i++){
		s = clist-&gt;s[i];
		if(s-&gt;c == c)
			addstate(nlist, s-&gt;out);
	}
}
</pre>

<h2>
Performance
</h2>

<p>
The C implementation just described was not written with performance in mind.
Even so, a slow implementation of a linear-time algorithm
can easily outperform a fast implementation of an 
exponential-time algorithm once the exponent is large enough.
Testing a variety of popular regular expression engines on 
a so-called pathological regular expression demonstrates this nicely.
</p>

<p>
Consider the regular expression
<code>a?<sup><i>n</i></sup>a<sup><i>n</i></sup></code>.
It matches the string
<code>a<sup><i>n</i></sup></code>
when the
<code>a?</code>
are chosen not to match any letters,
leaving the entire string to be matched by the
<code>a<sup><i>n</i></sup></code>.
Backtracking regular expression implementations
implement the zero-or-one
<code>?</code>
by first trying one and then zero.
There are
<i>n</i>
such choices to make, a total of
2<sup><i>n</i></sup> possibilities.
Only the very last
possibility—choosing zero for all the <code>?</code>—will lead to a match.
The backtracking approach thus requires
<i>O</i>(2<sup><i>n</i></sup>) time, so it will not scale much beyond <i>n</i>=25.
</p>

<p>
In contrast, Thompson's algorithm maintains state lists of length
approximately <i>n</i> and processes the string, also of length <i>n</i>,
for a total of <i>O</i>(<i>n</i><sup>2</sup>) time.
(The run time is superlinear,
because we are not keeping the regular expression constant
as the input grows.
For a regular expression of length <i>m</i> run on text of length <i>n</i>,
the Thompson NFA requires <i>O</i>(<i>mn</i>) time.)
</p>

<p>
The following graph plots time required to check whether
<code>a?<sup><i>n</i></sup>a<sup><i>n</i></sup></code>
matches
<code>a<sup><i>n</i></sup></code>:
</p>

<div>
<center>
<div>
<center>
<img src="https://swtch.com/%7Ersc/regexp/grep1p.png" alt="Performance graph" width="779" height="388">
<br>
regular expression and text size <i>n</i>
<br>
<code>a?</code><sup><i>n</i></sup><code>a</code><sup><i>n</i></sup>
matching 
<code>a</code><sup><i>n</i></sup>
</center>
</div>
</center>
</div>

<p>
Notice that the graph's <i>y</i>-axis has a logarithmic scale,
in order to be able to see a wide variety of times on a single graph.
</p>

<p>
From the graph it is clear that Perl, PCRE, Python, and Ruby are
all using recursive backtracking.
PCRE stops getting the right answer at 
<i>n</i>=23,
because it aborts the recursive backtracking after a maximum number
of steps.
As of Perl 5.6, Perl's regular expression engine is
<a href="http://perlmonks.org/index.pl?node_id=502408">said to memoize</a>
the recursive backtracking search, which should, at some memory cost,
keep the search from taking exponential amounts of time 
unless backreferences are being used.
As the performance graph shows, the memoization is not complete:
Perl's run time grows exponentially even though there
are no backreferences
in the expression.
Although not benchmarked here, Java uses a backtracking
implementation too.
In fact, the
<code>java.util.regex</code>
interface requires a backtracking
implementation, because arbitrary Java code
can be substituted into the matching path.
PHP uses the PCRE library.
</p>

<p>
The thick blue line is the C implementation of Thompson's algorithm given above.
Awk, Tcl, GNU grep, and GNU awk 
build DFAs, either precomputing them or using the on-the-fly
construction described in the next section.
</p>

<p>
Some might argue that this test is unfair to
the backtracking implementations, since it focuses on an
uncommon corner case.
This argument misses the point:
given a choice between an implementation
with a predictable, consistent, fast running time on all inputs
or one that usually runs quickly but can take
years of CPU time (or more) on some inputs,
the decision should be easy.
Also, while examples as dramatic as this one
rarely occur in practice, less dramatic ones do occur.
Examples include using
<code>(.*)</code>
<code>(.*)</code>
<code>(.*)</code>
<code>(.*)</code>
<code>(.*)</code>
to split five space-separated fields, or using
alternations where the common cases
are not listed first.
As a result, programmers often learn which constructs are
expensive and avoid them, or they turn to so-called
<a href="http://search.cpan.org/~dankogai/Regexp-Optimizer-0.15/lib/Regexp/Optimizer.pm">optimizers</a>.
Using Thompson's NFA simulation does not require such adaptation:
there are no expensive regular expressions.
</p>

<h2>
Caching the NFA to build a DFA
</h2>

<p>
Recall that DFAs are more efficient to execute than NFAs,
because DFAs are only ever in one state at a time: they never
have a choice of multiple next states.
Any NFA can be converted into an equivalent DFA
in which each DFA state corresponds to a
list of NFA states.
</p>

<p>
For example, here is the NFA we used earlier for
<code>abab|abbb</code>,
with state numbers added:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig20.png" alt="NFA for abab|abbb" width="424" height="91"></p>
<p>
The equivalent DFA would be:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig21.png" alt="DFA for abab|abbb" width="496" height="170"></p>
<p>
Each state in the DFA corresponds to a list of 
states from the NFA.
</p>

<p>
In a sense, Thompson's NFA simulation is
executing the equivalent DFA: each
<code>List</code>
corresponds to some DFA state,
and the 
<code>step</code>
function is computing, given a list and a next character,
the next DFA state to enter.
Thompson's algorithm simulates the DFA by 
reconstructing each DFA state as it is needed.
Rather than throw away this work after each step,
we could cache the
<code>Lists</code>
in spare memory, avoiding the cost of repeating the computation
in the future
and essentially computing the equivalent DFA as it is needed.
This section presents the implementation of such an approach.
Starting with the NFA implementation from the previous section,
we need to add less than 100 lines to build a DFA implementation.
</p>

<p>
To implement the cache, we first introduce a new data type
that represents a DFA state:
</p><pre>struct DState
{
	List l;
	DState *next[256];
	DState *left;
	DState *right;
};
</pre><p>
A
<code>DState</code>
is the cached copy of the list
<code>l</code>.
The array
<code>next</code>
contains pointers to the next state for each
possible input character:
if the current state is
<code>d</code>
and the next input character is
<code>c</code>,
then
<code>d-&gt;next[c]</code>
is the next state.
If
<code>d-&gt;next[c]</code>
is null, then the next state has not been computed yet.
<code>Nextstate</code>
computes, records, and returns the next state
for a given state and character.
</p>

<p>
The regular expression match follows
<code>d-&gt;next[c]</code>
repeatedly, calling
<code>nextstate</code>
to compute new states as needed.
</p><pre>int
match(DState *start, char *s)
{
	int c;
	DState *d, *next;
	
	d = start;
	for(; *s; s++){
		c = *s &amp; 0xFF;
		if((next = d-&gt;next[c]) == NULL)
			next = nextstate(d, c);
		d = next;
	}
	return ismatch(&amp;d-&gt;l);
}
</pre>

<p>
All the
<code>DStates</code>
that have been computed need to be saved in a 
structure that lets us look up a
<code>DState</code>
by its
<code>List</code>.
To do this, we arrange them 
in a binary tree
using the sorted
<code>List</code>
as the key.
The
<code>dstate</code>
function returns the
<code>DState</code>
for a given
<code>List</code>,
allocating one if necessary:
</p><pre>DState*
dstate(List *l)
{
	int i;
	DState **dp, *d;
	static DState *alldstates;

	qsort(l-&gt;s, l-&gt;n, sizeof l-&gt;s[0], ptrcmp);

	/* look in tree for existing DState */
	dp = &amp;alldstates;
	while((d = *dp) != NULL){
		i = listcmp(l, &amp;d-&gt;l);
		if(i &lt; 0)
			dp = &amp;d-&gt;left;
		else if(i &gt; 0)
			dp = &amp;d-&gt;right;
		else
			return d;
	}
	
	/* allocate, initialize new DState */
	d = malloc(sizeof *d + l-&gt;n*sizeof l-&gt;s[0]);
	memset(d, 0, sizeof *d);
	d-&gt;l.s = (State**)(d+1);
	memmove(d-&gt;l.s, l-&gt;s, l-&gt;n*sizeof l-&gt;s[0]);
	d-&gt;l.n = l-&gt;n;

	/* insert in tree */
	*dp = d;
	return d;
}
</pre><p>
Nextstate runs the NFA
<code>step</code>
and returns the corresponding
<code>DState</code>:
</p><pre>DState*
nextstate(DState *d, int c)
{
	step(&amp;d-&gt;l, c, &amp;l1);
	return d-&gt;next[c] = dstate(&amp;l1);
}
</pre><p>
Finally, the DFA's start state is the
<code>DState</code>
corresponding to the NFA's start list:
</p><pre>DState*
startdstate(State *start)
{
	return dstate(startlist(start, &amp;l1));
}
</pre><p>
(As in the NFA simulation,
<code>l1</code>
is a preallocated
<code>List</code>.)
</p>

<p>
The
<code>DStates</code>
correspond to DFA states, but the DFA is only built as needed:
if a DFA state has not been encountered during the search,
it does not yet exist in the cache.
An alternative would be to compute the entire DFA at once.
Doing so would make
<code>match</code>
a little faster by removing the conditional branch,
but at the cost of increased startup time and
memory use.
</p>

<p>
One might also worry about bounding the amount of
memory used by the on-the-fly DFA construction.
Since the
<code>DStates</code>
are only a cache of the 
<code>step</code>
function, the implementation of
<code>dstate</code>
could choose to throw away the entire DFA so far
if the cache grew too large.
This cache replacement policy 
only requires a few extra lines of code in 
<code>dstate</code>
and in
<code>nextstate</code>,
plus around 50 lines of code for memory management.
An implementation is
<a href="https://swtch.com/~rsc/regexp/">available online</a>.
(<a href="http://cm.bell-labs.com/cm/cs/awkbook/">Awk</a>
uses a similar limited-size cache strategy,
with a fixed limit of 32 cached states; this explains the discontinuity
in its performance at <i>n</i>=28 in the graph above.)
</p>

<p>
NFAs derived from regular expressions
tend to exhibit good locality: they visit the same states
and follow the same transition arrows over and over
when run on most texts.
This makes the caching worthwhile: the first time an arrow
is followed, the next state must be computed as in the NFA
simulation, but future traversals of the arrow are just
a single memory access.
Real DFA-based implementations can make use
of additional optimizations to run even faster.
A companion article (not yet written) will explore
DFA-based regular expression implementations in more detail.
</p>


<h2>
Real world regular expressions
</h2>

<p>
Regular expression usage in real programs
is somewhat more complicated than what the regular expression
implementations described above can handle.
This section briefly describes the common complications;
full treatment of any of these is beyond the scope of this
introductory article.
</p>

<p>
<i>Character classes</i>.
A character class, whether 
<code>[0-9]</code>
or
<code>\w</code>
or
<code>.</code> (dot),
is just a concise representation of an alternation.
Character classes can be expanded into alternations
during compilation, though it is more efficient to add
a new kind of NFA node to represent them explicitly.
<a href="http://www.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap09.html">POSIX</a>
defines special character classes
like <code>[[:upper:]]</code> that change meaning
depending on the current locale, but the hard part of
accommodating these is determining their meaning,
not encoding that meaning into an NFA.
</p>

<p>
<i>Escape sequences</i>.
Real regular expression syntaxes need to handle
escape sequences, both as a way to match metacharacters
(<code>\(</code>,
<code>\)</code>,
<code>\\</code>,
etc.)
and to specify otherwise difficult-to-type characters such as
<code>\n</code>.
</p>

<p>
<i>Counted repetition</i>.
Many regular expression implementations provide a counted
repetition operator
<code>{<i>n</i>}</code>
to match exactly 
<i>n</i>
strings matching a pattern;
<code>{</code><i>n</i><code>,</code><i>m</i><code>}</code>
to match at least 
<i>n</i>
but no more than
<i>m</i>;
and
<code>{</code><i>n</i><code>,}</code>
to match
<i>n</i>
or more.
A recursive backtracking implementation can implement
counted repetition using a loop; an NFA or DFA-based
implementation must expand the repetition:
<i>e</i><code>{3}</code>
expands to
<i>eee</i>;
<i>e</i><code>{3,5}</code>
expands to
<i>eeee</i><code>?</code><i>e</i><code>?</code>,
and
<i>e</i><code>{3,}</code>
expands to
<i>eee</i><code>+</code>.
</p>

<p>
<i>Submatch extraction</i>.
When regular expressions are used for splitting or parsing strings,
it is useful to be able to find out which sections of the input string
were matched by each subexpression.
After a regular expression like
<code>([0-9]+-[0-9]+-[0-9]+)</code>
<code>([0-9]+:[0-9]+)</code>
matches a string (say a date and time),
many regular expression engines make the
text matched by each parenthesized expression
available.
For example, one might write in Perl:
</p><pre>if(/([0-9]+-[0-9]+-[0-9]+) ([0-9]+:[0-9]+)/){
	print "date: $1, time: $2\n";
}
</pre><p>
The extraction of submatch boundaries has been mostly ignored
by computer science theorists, and it is perhaps the most
compelling argument for using recursive backtracking.
However, Thompson-style algorithms can be adapted to
track submatch boundaries without giving up efficient performance.
The Eighth Edition Unix
<i>regexp</i>(3)
library implemented such an algorithm as early as 1985,
though as explained below,
it was not very widely used or even noticed.
</p>

<p>
<i>Unanchored matches</i>.
This article has assumed that regular expressions
are matched against an entire input string.
In practice, one often wishes to find a substring
of the input that matches the regular expression.
Unix tools traditionally return the longest matching substring
that starts at the leftmost possible point in the input.
An unanchored search for 
<i>e</i>
is a special case
of submatch extraction: it is like searching for
<code>.*(<i>e</i>).*</code>
where the first
<code>.*</code>
is constrained to match as short a string as possible.
</p>

<p>
<i>Non-greedy operators</i>.
In traditional Unix regular expressions, the repetition operators
<code>?</code>,
<code>*</code>,
and
<code>+</code>
are defined to match as much of the string as possible while
still allowing the entire regular expression to match:
when matching
<code>(.+)(.+)</code>
against
<code>abcd</code>,
the first
<code>(.+)</code>
will match
<code>abc</code>,
and the second
will match
<code>d</code>.
These operators are now called
<i>greedy</i>.
Perl introduced
<code>??</code>,
<code>*?</code>,
and
<code>+?</code>
as non-greedy versions, which match as little of the string
as possible while preserving the overall match:
when matching
<code>(.+?)(.+?)</code>
against
<code>abcd</code>,
the first
<code>(.+?)</code>
will match only
<code>a</code>,
and the second
will match
<code>bcd.</code>
By definition, whether an operator is greedy
cannot affect whether a regular expression matches a
particular string as a whole; it only affects the
choice of submatch boundaries.
The backtracking algorithm admits a simple implementation
of non-greedy operators:
try the shorter match before the longer one.
For example, in a standard backtracking implementation,
<code><i>e</i>?</code>
first tries using
<i>e</i>
and then tries not using it;
<code><i>e</i>??</code>
uses the other order.
The submatch-tracking variants of Thompson's algorithm
can be adapted to accommodate non-greedy operators.
</p>

<p>
<i>Assertions</i>.
The traditional regular expression metacharacters
<code>^</code>
and
<code>$</code>
can be viewed as
<i>assertions</i>
about the text around them:
<code>^</code>
asserts that the previous character
is a newline (or the beginning of the string),
while
<code>$</code>
asserts that the next character is a newline
(or the end of the string).
Perl added more assertions, like
the word boundary
<code>\b</code>,
which asserts that 
the previous character is alphanumeric but the next
is not, or vice versa.
Perl also generalized the idea to arbitrary
conditions called lookahead assertions:
<code>(?=</code><i>re</i><code>)</code>
asserts that the text after the current input position matches
<i>re</i>,
but does not actually advance the input position;
<code>(?!</code><i>re</i><code>)</code>
is similar but 
asserts that the text does not match
<i>re</i>.
The lookbehind assertions
<code>(?&lt;=</code><i>re</i><code>)</code>
and
<code>(?&lt;!</code><i>re</i><code>)</code>
are similar but make assertions about the text
before the current input position.
Simple assertions like
<code>^</code>,
<code>$</code>,
and
<code>\b</code>
are easy to accommodate in an NFA,
delaying the match one byte for forward assertions.
The generalized assertions
are harder to accommodate but in principle could
be encoded in the NFA.
</p>

<p>
<i>Backreferences</i>.
As mentioned earlier, no one knows how to 
implement regular expressions with backreferences efficiently,
though no one can prove that it's impossible either.
(Specifically, the 
<a href="http://perl.plover.com/NPC/NPC-3SAT.html">problem is NP-complete</a>, meaning that if
someone did find an efficient implementation, that would
be <i>major</i> news to computer scientists and would
win a <a href="http://www.claymath.org/Popular_Lectures/Minesweeper/">million dollar prize</a>.)
The simplest, most effective strategy for backreferences,
taken by the original awk and egrep, is not to implement them.
This strategy is no longer practical: users have come to
rely on backreferences for at least occasional use,
and backreferences are part of
the
<a href="http://www.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap09.html">POSIX standard for regular expressions</a>.
Even so, it would be reasonable to use Thompson's NFA simulation
for most regular expressions, and only bring out
backtracking when it is needed.
A particularly clever implementation could combine the two,
resorting to backtracking only to accommodate the backreferences.
</p>

<p>
<i>Backtracking with memoization</i>.
Perl's approach of using memoization to avoid exponential blowup
during backtracking
when possible is a good one.  At least in theory, it should make
Perl's regular expressions behave more like an NFA and
less like backtracking.  
Memoization does not completely solve the problem, though:
the memoization itself requires a memory footprint roughly 
equal to the size of the text times the size of the regular expression.
Memoization also does not address the issue of the stack space used
by backtracking, which is linear in the size of the text:
matching long strings typically causes a backtracking
implementation to run out of stack space:
</p><pre>$ perl -e '("a" x 100000) =~ /^(ab?)*$/;'
Segmentation fault (core dumped)
$
</pre>

<p>
<i>Character sets</i>.
Modern regular expression implementations must deal with 
large non-ASCII character sets such as Unicode.
The 
<a href="https://swtch.com/plan9port/unix/">Plan 9 regular expression library</a>
incorporates Unicode by running an NFA with a
single Unicode character as the input character for each step.
That library separates the running of the NFA from decoding
the input, so that the same regular expression matching code
is used for both 
<a href="http://plan9.bell-labs.com/sys/doc/utf.html">UTF-8</a>
and wide-character inputs.
</p>

<h2 id="History">
History and References
</h2>


<p>
<a name="rabin-scott-b"></a>Michael Rabin and Dana Scott
introduced non-deterministic finite automata
and the concept of non-determinism in 1959
[<a href="#rabin-scott">7</a>],
showing that NFAs can be simulated by
(potentially much larger) DFAs in which 
each DFA state corresponds to a set of NFA states.
(They won the Turing Award in 1976 for the introduction
of the concept of non-determinism in that paper.)
</p>

<p>
<a name="mcnaughton-yamada-b">R. McNaughton and H. Yamada
</a>[<a href="#mcnaughton-yamada">4</a>]
and 
<a name="thompson-b"></a>Ken Thompson
[<a href="#thompson">9</a>]
are commonly credited with giving the first constructions
to convert regular expressions into NFAs,
even though neither paper mentions the
then-nascent concept of an NFA.
McNaughton and Yamada's construction
creates a DFA,
and Thompson's construction creates IBM 7094 machine code,
but reading between the lines one can
see latent NFA constructions underlying both.
Regular expression to NFA constructions differ only in how they encode 
the choices that the NFA must make.
The approach used above, mimicking Thompson,
encodes the choices with explicit choice
nodes
(the
<code>Split</code>
nodes above)
and unlabeled arrows.
An alternative approach,
the one most commonly credited to McNaughton and Yamada,
is to avoid unlabeled arrows, instead allowing NFA states to
have multiple outgoing arrows with the same label.
<a name="mcilroy-b"></a>McIlroy
[<a href="#mcilroy">3</a>]
gives a particularly elegant implementation of this approach
in Haskell.
</p>

<p>
<a name="vanvleck-b"></a>Thompson's regular expression implementation
was for his QED editor running on the CTSS 
[<a href="#vanvleck">10</a>]
operating
system on the IBM 7094.
<a name="pierce-b"></a>A copy of the editor can be found in archived CTSS sources
[<a href="#pierce">5</a>].
<a name="deutsch-lampson-b"></a>L. Peter Deutsch and Butler Lampson
[<a href="#deutsch-lampson">1</a>]
developed the first QED, but
Thompson's reimplementation was the first to use
regular expressions.
<a name="ritchie-b"></a>Dennis Ritchie, author of yet another QED implementation,
has documented the early history of the QED editor
[<a href="#ritchie">8</a>]
(Thompson, Ritchie, and Lampson later won
Turing awards for work unrelated to QED or finite automata.)
</p>

<p>
Thompson's paper marked the 
beginning of a long line of regular expression implementations.
Thompson chose not to use his algorithm when 
implementing the text editor ed, which appeared in 
First Edition Unix (1971), or in its descendant grep,
which first appeared in the Fourth Edition (1973).
Instead, these venerable Unix tools used
recursive backtracking!
Backtracking was justifiable because the
regular expression syntax was quite limited:
it omitted grouping parentheses and the
<code>|</code>,
<code>?</code>,
and
<code>+</code>
operators.
Al Aho's egrep,
which first appeared in the Seventh Edition (1979),
was the first Unix tool to provide
the full regular expression syntax, using a
precomputed DFA.
By the Eighth Edition (1985), egrep computed the DFA on the fly,
like the implementation given above.
</p>

<p>
<a name="pike-b"></a>While writing the text editor sam 
[<a href="#pike">6</a>]
in the early 1980s,
Rob Pike wrote a new regular expression implementation,
which Dave Presotto extracted into a library that 
appeared in the Eighth Edition.
Pike's implementation
incorporated submatch tracking into an efficient NFA simulation
but, like the rest of the Eighth Edition source, was not widely
distributed.
Pike himself did not realize that his technique was anything new.
Henry Spencer reimplemented the Eighth Edition library
interface from scratch, but using backtracking,
and
<a href="http://arglist.com/regex/">released his implementation</a>
into the public domain.
It became very widely used, eventually serving as the basis
for the slow regular expression implementations
mentioned earlier: Perl, PCRE, Python, and so on.
(In his defense,
Spencer knew the routines could be slow,
and he didn't know that a more efficient algorithm existed.
He even warned in the documentation,
“Many users have found the speed perfectly adequate,
although replacing the insides of egrep with this code
would be a mistake.”)
Pike's regular expression implementation, extended to
support Unicode, was made freely available
with sam in 
<a href="http://groups.google.com/group/comp.os.research/msg/f1783504a2d18051">late 1992</a>,
but the particularly efficient
regular expression search algorithm went unnoticed.
The code is now available in many forms: as 
<a href="http://plan9.bell-labs.com/sources/plan9/sys/src/cmd/sam/">part of sam</a>,
as 
<a href="http://plan9.bell-labs.com/sources/plan9/sys/src/libregexp/">Plan&nbsp;9's regular expression library</a>,
or
<a href="https://swtch.com/plan9port/unix/">packaged separately for Unix</a>.
<a name="laurikari-b"></a>Ville Laurikari independently discovered Pike's algorithm
in 1999, developing a theoretical foundation as well
[<a href="#laurikari">2</a>].
</p>


<p>
Finally, any discussion of regular expressions
would be incomplete without mentioning 
Jeffrey Friedl's book
<i>Mastering Regular Expressions</i>,
perhaps the most popular reference among today's programmers.
Friedl's book teaches programmers how best to use today's
regular expression implementations, but not how best to implement them.
What little text it devotes to implementation
issues perpetuates the widespread belief that recursive backtracking
is the only way to simulate an NFA.
Friedl makes it clear that he 
<a href="http://regex.info/blog/2006-09-15/248">neither understands nor respects</a>
the underlying theory.
</p>

<h2>
Summary
</h2>

<p>
Regular expression matching can be simple and fast, using
finite automata-based techniques that have been known for decades.
In contrast, Perl, PCRE, Python, Ruby, Java,
and many other languages
have regular expression implementations based on 
recursive backtracking that are simple but can be
excruciatingly slow.
With the exception of backreferences, the features
provided by the slow backtracking implementations
can be provided by the automata-based implementations
at dramatically faster, more consistent speeds.
</p>

<p>
The next article in this series,
“<a href="https://swtch.com/%7Ersc/regexp/regexp2.html">Regular Expression Matching: the Virtual Machine Approach</a>,” discusses NFA-based submatch extraction.
The third article, “<a href="https://swtch.com/%7Ersc/regexp/regexp3.html">Regular Expression Matching in the Wild</a>,” examines a production implementation.
The fourth article, “<a href="https://swtch.com/%7Ersc/regexp/regexp4.html">Regular Expression Matching with a Trigram Index</a>,” explains how Google Code Search was implemented.
</p>

<h2>
Acknowledgements
</h2>

<p>
Lee Feigenbaum,
James Grimmelmann,
Alex Healy,
William Josephson,
and
Arnold Robbins
read drafts of this article and made many helpful suggestions.
Rob Pike clarified some of the history surrounding his
regular expression implementation.
Thanks to all.
</p>

<h2>
References
</h2>

<p>
<a name="deutsch-lampson"></a>
[<a href="#deutsch-lampson-b">1</a>]
L. Peter Deutsch and Butler Lampson,
“An online editor,”
Communications of the ACM 10(12) (December 1967), pp.&nbsp;793–799.
<a href="http://doi.acm.org/10.1145/363848.363863"><i>http://doi.acm.org/10.1145/363848.363863</i></a>
</p><p>
<a name="laurikari"></a>
[<a href="#laurikari-b">2</a>]
Ville Laurikari,
“NFAs with Tagged Transitions,
their Conversion to Deterministic Automata
and
Application to Regular Expressions,”
in Proceedings of the Symposium on String Processing and
Information Retrieval, September 2000.
<a href="http://laurikari.net/ville/spire2000-tnfa.ps"><i>http://laurikari.net/ville/spire2000-tnfa.ps</i></a>
</p><p>
<a name="mcilroy"></a>
[<a href="#mcilroy-b">3</a>]
M. Douglas McIlroy,
“Enumerating the strings of regular languages,”
Journal of Functional Programming 14 (2004), pp.&nbsp;503–518.
<a href="http://www.cs.dartmouth.edu/~doug/nfa.ps.gz"><i>http://www.cs.dartmouth.edu/~doug/nfa.ps.gz</i></a> (preprint)
</p><p>
<a name="mcnaughton-yamada"></a>
[<a href="#mcnaughton-yamada-b">4</a>]
R. McNaughton and H. Yamada,
“Regular expressions and state graphs for automata,”
IRE Transactions on Electronic Computers EC-9(1) (March 1960), pp.&nbsp;39–47.
</p><p>
<a name="pierce"></a>
[<a href="#pierce-b">5</a>]
Paul Pierce,
“CTSS source listings.”
<a href="http://www.piercefuller.com/library/ctss.html"><i>http://www.piercefuller.com/library/ctss.html</i></a> 
(Thompson's QED is in the file
<code>com5</code>
in the source listings archive and is marked as
<code>0QED</code>)
</p><p>
<a name="pike"></a>
[<a href="#pike-b">6</a>]
Rob Pike,
“The text editor sam,”
Software—Practice &amp; Experience 17(11) (November 1987), pp.&nbsp;813–845.
<a href="http://plan9.bell-labs.com/sys/doc/sam/sam.html"><i>http://plan9.bell-labs.com/sys/doc/sam/sam.html</i></a>
</p><p>
<a name="rabin-scott"></a>
[<a href="#rabin-scott-b">7</a>]
Michael Rabin and Dana Scott,
“Finite automata and their decision problems,”
IBM Journal of Research and Development 3 (1959), pp.&nbsp;114–125.
<a href="http://www.research.ibm.com/journal/rd/032/ibmrd0302C.pdf"><i>http://www.research.ibm.com/journal/rd/032/ibmrd0302C.pdf</i></a>
</p><p>
<a name="ritchie"></a>
[<a href="#ritchie-b">8</a>]
Dennis Ritchie,
“An incomplete history of the QED text editor.”
<a href="http://plan9.bell-labs.com/~dmr/qed.html"><i>http://plan9.bell-labs.com/~dmr/qed.html</i></a>
</p><p>
<a name="thompson"></a>
[<a href="#thompson-b">9</a>]
Ken Thompson,
“Regular expression search algorithm,”
Communications of the ACM 11(6) (June 1968), pp.&nbsp;419–422.
<a href="http://doi.acm.org/10.1145/363347.363387"><i>http://doi.acm.org/10.1145/363347.363387</i></a>
(<span size="-1"><a href="http://www.cs.chalmers.se/~coquand/AUTOMATA/thompson.pdf">PDF</a></span>)
</p><p>
<a name="vanvleck"></a>
[<a href="#vanvleck-b">10</a>]
Tom Van Vleck,
“The IBM 7094 and CTSS.”
<a href="http://www.multicians.org/thvv/7094.html"><i>http://www.multicians.org/thvv/7094.html</i></a>
</p>


<p>
Discussion on <a href="http://programming.reddit.com/info/10c60/comments">reddit</a> and <a href="http://perlmonks.org/?node_id=597262">perlmonks</a> and
<a href="http://lambda-the-ultimate.org/node/2064">LtU</a>
</p>

<center>
<p>
Copyright © 2007 Russ Cox.  All Rights Reserved.
<br>
<a href="https://swtch.com/~rsc/regexp/">https://swtch.com/~rsc/regexp/</a>
</p>
</center>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bento3D (203 pts)]]></title>
            <link>https://polar-tadpole-97b.notion.site/Bento3D-e40483712b304d389d7c2da26196e113#d7f452d3ffd94cd1a0a9e70e361efb63</link>
            <guid>40422940</guid>
            <pubDate>Tue, 21 May 2024 01:17:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://polar-tadpole-97b.notion.site/Bento3D-e40483712b304d389d7c2da26196e113#d7f452d3ffd94cd1a0a9e70e361efb63">https://polar-tadpole-97b.notion.site/Bento3D-e40483712b304d389d7c2da26196e113#d7f452d3ffd94cd1a0a9e70e361efb63</a>, See on <a href="https://news.ycombinator.com/item?id=40422940">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Dual antibacterial properties of copper-coated nanotextured stainless steel (101 pts)]]></title>
            <link>https://onlinelibrary.wiley.com/doi/10.1002/smll.202311546</link>
            <guid>40421851</guid>
            <pubDate>Mon, 20 May 2024 23:23:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onlinelibrary.wiley.com/doi/10.1002/smll.202311546">https://onlinelibrary.wiley.com/doi/10.1002/smll.202311546</a>, See on <a href="https://news.ycombinator.com/item?id=40421851">Hacker News</a></p>
Couldn't get https://onlinelibrary.wiley.com/doi/10.1002/smll.202311546: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Statement from Scarlett Johansson on the OpenAI "Sky" voice (1370 pts)]]></title>
            <link>https://twitter.com/BobbyAllyn/status/1792679435701014908</link>
            <guid>40421225</guid>
            <pubDate>Mon, 20 May 2024 22:28:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/BobbyAllyn/status/1792679435701014908">https://twitter.com/BobbyAllyn/status/1792679435701014908</a>, See on <a href="https://news.ycombinator.com/item?id=40421225">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Google cuts mystery check to US in bid to sidestep jury trial (183 pts)]]></title>
            <link>https://www.reuters.com/legal/government/google-cuts-mystery-check-us-bid-sidestep-jury-trial-2024-05-20/</link>
            <guid>40420968</guid>
            <pubDate>Mon, 20 May 2024 22:01:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/legal/government/google-cuts-mystery-check-us-bid-sidestep-jury-trial-2024-05-20/">https://www.reuters.com/legal/government/google-cuts-mystery-check-us-bid-sidestep-jury-trial-2024-05-20/</a>, See on <a href="https://news.ycombinator.com/item?id=40420968">Hacker News</a></p>
Couldn't get https://www.reuters.com/legal/government/google-cuts-mystery-check-us-bid-sidestep-jury-trial-2024-05-20/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Enlightenmentware (399 pts)]]></title>
            <link>https://mmapped.blog/posts/28-enlightenmentware.html</link>
            <guid>40419856</guid>
            <pubDate>Mon, 20 May 2024 20:23:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mmapped.blog/posts/28-enlightenmentware.html">https://mmapped.blog/posts/28-enlightenmentware.html</a>, See on <a href="https://news.ycombinator.com/item?id=40419856">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header>
  <nav>
    <ul>
      <li><a href="https://mmapped.blog/">mmap(blog)</a></li>
      <li><a href="https://mmapped.blog/posts">Posts</a></li>
      <li><a href="https://mmapped.blog/about">About</a></li>
      <li><a href="https://mmapped.blog/feed.xml">Atom Feed</a></li>
    </ul>
  </nav>
</header><span>
  <span title="First published">✏ <span itemprop="datePublished">2024-05-20</span></span>
  &nbsp;
  <span title="Last modified">✂ <span itemprop="dateModified">2024-05-20</span></span>
  <span>
  
  <a href="https://news.ycombinator.com/item?id=40417447" title="Discuss on Hacker News" rel="nofollow" target="_blank">
    
  </a>
  <a href="https://www.reddit.com/r/programming/comments/1cwa1m8/blog_post_enlightenmentware/" title="Discuss on Reddit" rel="nofollow" target="_blank">
    
  </a></span>
</span><hr>
<ul><li>
    <a href="#unix">UNIX</a></li><li>
    <a href="#git">Git</a></li><li>
    <a href="#emacs">Emacs</a></li><li>
    <a href="#boost-graph">Boost.Graph</a></li><li>
    <a href="#bazel">Bazel</a></li><li>
    <a href="#conclusion">Conclusion</a></li></ul>
<hr>
<section>
<p>As programmers, we interact with software tools daily.
Most of them can barely get the job done.
But once in a white, we discover a piece of software that transcends mere utility.
These tools capture our imagination, open new possibilities, and affect how we design our own systems.
I call such software <em>enlightenmentware</em>.</p><p>The most common source of enlightenment for programmers is the programming language they use at work or learn as a hobby.
I experienced many jolts of enlightenment from fiddling with programming languages, from <a href="https://en.wikipedia.org/wiki/Microsoft_Macro_Assembler"><span>masm</span></a> and <a href="https://en.wikipedia.org/wiki/C_(programming_language)">C</a> to <a href="https://en.wikipedia.org/wiki/Prolog">Prolog</a> and <a href="https://www.idris-lang.org/">Idris</a>.
I won’t focus on languages, however, since the effects of language learning on mind expansion is old news<label for="sn-norvig"></label><span>
  See, for example, Peter Norvig’s <q><a href="https://norvig.com/21-days.html">Teach Yourself Programming in Ten Years</a></q>.
</span>.</p><p>In this article, I praise the software that contributed the most to my enlightenment.</p></section><section><h2 id="unix"><a href="#unix">UNIX</a></h2><div><blockquote><p>
  <span>unix</span> is user-friendly—it’s just choosy about who its friends are.
</p></blockquote></div><p>I started looking for my first real programming job around 2008, while studying at university in my hometown of Nizhny Novgorod.
Almost all the open positions required knowledge of mysterious things called <span>unix</span> and <em>sockets</em>.
My curriculum didn’t offer a course on <span>unix</span> or operating systems in general, so I decided to get a textbook and master the topic myself. </p><p><q><a href="https://www.goodreads.com/book/show/22066650-unix">The <span>unix</span> Operating System</a></q> by Andrey Robachevsky et al., also known as the <em>turtle book</em> in Russia because of its cover, introduced me to the magical world of <span>unix</span>-like operating systems.
<span>unix</span> became something I could understand, explore, and programmatically interact with.
All pieces of the puzzle—the filesystem interface, the process model with environments and permissions, forking, sockets, and signals—fell into place and revealed a coherent, beautiful picture.</p><p>A search for a working <span>unix</span> installation led me to <a href="https://en.wikipedia.org/wiki/Mandriva_Linux">Mandriva Linux</a>.
It was like discovering a parallel universe where you don’t have to pirate software or spend forty minutes installing an <span>ide</span> to compile a <span>c</span> program.
Here, people developed software for fun and shared it freely.
I couldn’t fathom why anyone would use Windows<label for="sn-windows"></label><span>
  I became significantly more tolerant since my early university years.
  Windows (specifically the <a href="https://en.wikipedia.org/wiki/Windows_NT">NT family</a>) is a great operating system.
  I even have it installed on my gaming <span>pc</span> so that I can buy games I never play.
</span>.</p><p>From that moment on, <span>unix</span> followed me through all stages of my life:
the toddler phase of keeping up with the cutting-edge <a href="https://ubuntu.com/">Ubuntu</a> releases,
the rebellious teens of compiling custom kernels for my <a href="https://www.thinkwiki.org/wiki/Category:T61p">Thinkpad T61p</a> and <a href="https://wiki.gentoo.org/wiki/Emerge">emerging</a> the <a href="https://wiki.gentoo.org/wiki/World_set_(Portage)"><code>@world</code></a> on <a href="https://www.gentoo.org/">Gentoo</a>,
the maturity of returning to Ubuntu <span>lts</span> and delaying upgrades until the first dot one release,
and to the overwhelmed parent stage of becoming a happy macOS user.</p><p><span>unix</span> also became an essential building block in my profession.
Most of the software I wrote operates in a <span>unix</span> environment, and I still occasionally consult my copy of <a href="https://www.goodreads.com/book/show/603263.Advanced_Programming_in_the_UNIX_Environment">Advanced Programming in the <span>unix</span> Environment</a>.</p></section><section><h2 id="git"><a href="#git">Git</a></h2><div><blockquote><p>
  It is easy to shoot your foot off with git, but also easy to revert to a previous foot and merge it with your current leg.
</p></blockquote></div><p>I encountered version control systems in early 2009; the company I worked for used <a href="https://en.wikipedia.org/wiki/IBM_Rational_ClearCase">Rational ClearCase</a> to manage their code.
The system versioned each file separately and relied on large configuration files—<em>config specs</em>—to construct a consistent snapshot of the source tree.
The tool was utterly confusing and intimidating, so I avoided dealing with it beyond the minimal requirements of my job.</p><p>About a year later, I joined a shop that used <a href="https://subversion.apache.org/">Subversion</a>.
This time, I invested in learning upfront and swallowed the entire <a href="https://svnbook.red-bean.com/">Version Control with Subversion</a> before making my first commit.
Subversion was easy to understand and use; I couldn’t imagine how to improve on it.
Still, I perceived it as a tool that you use at work.
There was enough friction in setting up a repository to hinder its use for small personal projects.
At the time, Google offered hosting on <a href="https://code.google.com/">Google Code</a>, but I didn’t feel comfortable sharing my experiments with the world back then.</p><p>And then I discovered <a href="https://git-scm.com/">Git</a>.</p><p>Git was nothing like Subversion.
It had a steep learning curve and confused everyone to no end<label for="sn-git-random-man"></label><span>
  Way before we all got used to ChatGPT,
  <a href="https://github.com/Lokaltog">Kim Ødegaard</a> created a <a href="https://git-man-page-generator.lokaltog.net/">service</a> that generates random man pages mocking Git’s dense documentation style.
</span>.
Still, the confusion was qualitatively different from what I experienced with ClearCase.
ClearCase is confusing like a Russian novel: All the characters have strange names, the plot is complex, and it doesn’t end well.
Git is confusing like math: It slowly melts your brain and molds it into a <a href="https://en.wikipedia.org/wiki/Tesseract">tesseract</a>, giving access to higher dimensions.</p><p>Git removed the friction from using version control; there was no excuse not to version anything of value anymore.
Merging branches with Git didn’t cause anxiety disorders.
The staging area—confusingly named <em>index</em>—became essential to my workflows.
But my favorite feature was the breathtaking beauty of Git’s design, the elegant mix of distributed systems, acyclic graphs, and content-addressed storage.</p><p>Learning about Git’s internals was so much fun that I became interested in the bits and bolts of other version control systems.
I travelled through time from <a href="https://darcs.net/">Darcs</a> to <a href="https://www.mercurial-scm.org/">Mercurial</a>, <a href="https://www.bitkeeper.org/">BitKeeper</a>, and the ultimate origin, <a href="https://en.wikipedia.org/wiki/Source_Code_Control_System">SCCS</a>.
I also built a toy one-file version control system while learning Rust.</p><p>Will Git ever be replaced with something better?
Just as it was hard to imagine an improvement over Subversion before Git came along, it’s hard to imagine a significant improvement over Git now.
For me, Git’s primary disadvantage is its snapshot-oriented approach that makes merges hard to reason about.
Git, Mercurial, and most other tools make it challenging to separate original code from the decisions that the person merging files had to make<label for="sn-janestreet-patch-vs-diff"></label><span>
  Jane Street’s tech staff reports similar concerns.
  See, for example, the <a href="https://blog.janestreet.com/patch-review-vs-diff-review-revisited/">Patch review vs. diff review, revisited</a> blog article.
</span>.
Systems based on <a href="https://en.m.wikibooks.org/wiki/Understanding_Darcs/Patch_theory">patch theory</a>, such as <a href="https://pijul.org/">Pijul</a> and <a href="https://darcs.net/">Darcs</a>, might address these issues.</p></section><section><h2 id="emacs"><a href="#emacs">Emacs</a></h2><div><blockquote><p>
  While any text editor can save your files, only Emacs can save your soul.
</p></blockquote></div><p>I edited my first programs in a friendly blue window of <a href="https://en.wikipedia.org/wiki/Turbo_Pascal">Turbo Pascal 7.0</a>.
The environment had little friction: no project or build configuration, no noticeable build time; you type your code and run it.
That was a perfect tool for learning.</p><p>My university used Pascal for introductory programming classes, so I also used Turbo Pascal for my assignments.
Later courses introduced C++ and Java, for which we used <a href="https://en.wikipedia.org/wiki/Visual_Studio#6.0_(1998)">Visual Studio 6.0</a> and <a href="https://en.wikipedia.org/wiki/JBuilder">JBuilder</a>.
Although we learned to invoke compilers from the command line, <span>ide</span>s dominated my early code-editing experience.</p><p>At my first programming job, I worked on a remote Solaris workstation over a <a href="https://en.wikipedia.org/wiki/Citrix_Systems">Citrix</a> connection.
Almost everyone in our group used <a href="https://en.wikipedia.org/wiki/NEdit">NEdit</a> to edit the code.
One day, I noticed a person whose editor looked markedly different from everyone else’s; the background was dark, and the code glowed with bright colors.
To me, that was a sign of their superior technical knowledge.
I <em>needed</em> to learn how to tweak my editor.</p><p>The quest for customization led me to <a href="https://www.vim.org/">Vim</a> (the workstation had Vim 6 installed out of the box).
After all, if the goal is to stand out from the crowd, why stop at the color scheme?
I went through the Vim tutorial, and it clicked with me immediately.
It felt like playing a musical instrument: challenging but fun.
It turned a mundane job of fixing bugs into an exercise in skill.</p><p>I don’t remember exactly when and why I got interested in Emacs<label for="sn-wolfram-notes"></label><span>
  Daly journaling is one of the things I wish I started doing earlier.
  I’m nowhere near <a href="https://writings.stephenwolfram.com/2019/02/seeking-the-productive-life-some-details-of-my-personal-infrastructure/#archiving-and-searching">Stephen Wolfram’s level</a>, but I enjoy going back and seeing what I was up to a few years ago.
</span>.
Most likely, it was a result of my obsession with Lisp after reading <a href="https://en.wikipedia.org/wiki/Structure_and_Interpretation_of_Computer_Programs">Structure and Interpretation of Computer Programs</a> and looking for a Lisp to interact with.
I remember reading <a href="https://www.gnu.org/software/emacs/manual/eintr.html">An Introduction to Programming in Emacs Lisp</a> around 2010 and having a great time.</p><p>I became interested in the editor internals.
Using <a href="https://www.finseth.com/craft/">The Craft of Text Editing</a> by Craig A. Finseth as a guide, I explored the source code of various editors to see how they worked: which the data structures they used to represent text buffers, how they interacted with extensions and implemented the undo mechanism.
I found Vim’s source code somewhat messy, inconsistent, and hard to understand.
Emacs’s source was spotless, well-organized, and well-documented.</p><p>A dive into Emacs internals also revealed the inherent beauty of its architecture.
Emacs is a Lisp machine that provides text editing and window management capabilities.
It is a powerful, convenient, and friendly development environment for building dynamic text-driven applications in Emacs Lisp.</p><p>Almost everything in Emacs is a Lisp object you can inspect, interact with, and access its documentation.
That makes Emacs’ documentation system unparalleled once you master it.
Emacs’ dynamism makes extending it much easier than any other editor.
The feedback loop is airtight: you can immediately try out your code in the context of the editor you use to write it.</p><p>Although I’m writing these words in <a href="https://code.visualstudio.com/">Visual Studio Code</a>, I always have my Emacs open<label for="sn-vim"></label><span>
  I also have a <a href="https://github.com/tmux/tmux/wiki">tmux</a> session with multiple <a href="https://neovim.io/">nvim</a> instances running.
  I use these when my pinky gets tired of holding down the <kbd>Ctrl</kbd> key.
</span>.
Many things are easier in Emacs; I don’t think any software will ever completely replace it for me.
It’s also my editor of choice if I need to implement an extension.
Programming Emacs Lisp is a joy, especially compared to writing Vimscript.</p></section><section><h2 id="boost-graph"><a href="#boost-graph">Boost.Graph</a></h2><div><blockquote><p>
  I also must confess to a strong bias against the fashion for reusable code.
  To me, <q>re-editable code</q> is much, much better than an untouchable black box or toolkit.
</p></blockquote></div><p>The evening of 2013 New Year’s Eve didn’t go as planned.
I was on a cruise ship crossing the stormy Baltic Sea and couldn’t stay on my feet because of the seasickness.
Instead of consuming tasty treats with the rest of the passengers, I was lying in bed and reading a book I took for the trip: <a href="https://www.goodreads.com/book/show/1705806.The_Boost_Graph_Library">The Boost Graph Library</a> by Jeremy G. Siek et al.</p><p>Most algorithm libraries require you to commit to a specific data representation, making integrating them into an existing project prohibitively expensive.
That’s especially true for graph algorithms: The vertices and edges are usually implicitly defined and deeply embedded into other data structures, so it’s easier to re-implement the algorithm than to use a generic library.</p><p>The <a href="https://www.boost.io/libraries/graph/">Boost.Graph</a> library solves this problem elegantly using <a href="https://en.wikipedia.org/wiki/Alexander_Stepanov">Alex Stepanov</a>’s ideas on generic programming.
It uses a bag of tricks (type traits, property maps, visitors) to implement graph algorithms that can work with any graph representation you throw at them, given that you provide an adapter telling the library how to view your data structures as a graph.</p><p>Even though I never had a chance to use the library in practice<label for="sn-boost-graph-use"></label><span>
  Given that I share Donald Knuth’s attitude opening this section, I would probably not use the library even if I had a chance.
  I’d rather write one page of interesting code traversing a graph than two pages of boring adapters required to invoke the algorithm.
</span>, its design helped me deepen my understanding of <a href="https://en.wikipedia.org/wiki/Standard_Template_Library"><span>stl</span></a> design and generic programming in general.
It also helped me understand the motivation for advanced type-level programming features in other programming languages, such as <a href="https://wiki.haskell.org/GHC/Type_families">type families</a> in Haskell.
Overall, Boost.Graph is one of the most enlightening pieces of software that I’ve never used.</p></section><section><h2 id="bazel"><a href="#bazel">Bazel</a></h2><div><blockquote><p>
  If make doesn’t do what you expect it to, it’s a good chance the makefile is wrong.
</p></blockquote></div><p>I wrote my first <code>Makefile</code> around 2009 while working on a research project in computational mathematics for my degree.
I already used <a href="https://en.wikipedia.org/wiki/Make_(software)"><code>make</code></a> at work, but I didn’t need to understand how it worked.
This time, I had to compile a <span>fortran</span> program mixing sources adhering to different language standards: from venerable <span>fortran</span> 77 to hip Fortran 2003.
To get a deeper understanding of the tool, I referred to <a href="https://www.oreilly.com/library/view/managing-projects-with/0596006101/">Managing Projects with GNU Make</a> by Robert Mecklenburg.</p><p>Most books on technology excite me: I become enthusiastic about the subject and want to try it out in practice. 
The book on <code>make</code> had the opposite effect.
The complexity required to make builds correct and ergonomic made me earn for a better tool<label for="sn-modern-cpp-design"></label><span>
  One book that made me feel the same way was <a href="https://www.goodreads.com/book/show/871669.Modern_C_Design">Modern C++ Design</a> by Andrei Alexandrescu.
  The book is deep and beautifully written, but the terrifyingly clever and ugly tricks in the second chapter made me question the choice of the programming language.
  Another one is <a href="https://nostarch.com/autotools2e">Autotools</a> by John Calcote.
</span>.</p><p>After my deep dive into <code>make</code>, I often fiddled with build systems at work:
I introduced <a href="https://cmake.org/">CMake</a> to my first C++ project to replace complex and scarily incorrect <code>Makefile</code> files and
replaced an inflexible <a href="https://ant.apache.org/">Ant</a>-based build system in a 500 <span>kloc</span> Java project with <a href="https://gradle.org/">Gradle</a> scripts that everyone on the team could contribute to.
But all of the tools I tried, including <a href="https://cmake.org/">CMake</a>, <a href="https://ant.apache.org/">Ant</a>, <a href="https://maven.apache.org/">Maven</a>, <a href="https://gradle.org/">Gradle</a>, <a href="https://www.scons.org/">SCons</a>, and <a href="https://www.gnu.org/software/automake/manual/html_node/Autotools-Introduction.html">autotools</a> left me deeply unsatisfied.
They were clanky, awkward, and hard to extend and compose.</p><p>In 2016, I joined Google in Zurich.
I heard about Google’s internal build tool, <code>blaze</code>, and couldn’t wait to lay my hands on it.
Surprisingly, I didn’t need to fiddle with <code>blaze</code>, nor did I have to understand how it worked.
I could copy some build targets and edit the dependency list, and the build worked as expected.
<code>blaze</code> made correct and fast builds not just easy, but <em>boring</em> in the good sense.
Only a few years later, when I attempted to use <a href="https://bazel.build/">Bazel</a>—the open-source version of <code>blaze</code>—for a toy personal project, did I have to understand the underlying model.</p><p>Bazel was the final piece of the puzzle, together with Haskell’s typeclasses, <a href="https://research.google/pubs/flumejava-easy-efficient-data-parallel-pipelines/">Flume pipelines</a> interface,  and the <a href="https://www.tensorflow.org/">TensorFlow</a> 1.0 execution model, that made me understand the ubiquitous plan-execute pattern<label for="sn-build-systems-a-la-carte"></label><span>
  The <a href="https://www.microsoft.com/en-us/research/uploads/prod/2018/03/build-systems.pdf">Build Systems à la Carte</a> article by Andrey Mokhov, Neil Mitchell, and Simon Peyton Jones explains how various build system designs map to Haskell typeclasses.
  Thomas Leonard’s <a href="https://roscidus.com/blog/blog/2019/11/14/cicd-pipelines">CI/CD pipelines: Monad, Arrow or Dart?</a> blog post is also a great read on this topic.
</span>.
Bazel build file is a program that constructs a slice of the build artifact graph.
Bazel rules don’t <em>run</em> the build commands; they <em>declare</em> how to transform inputs into outputs, and the Bazel engine figures out the rest.</p><p>My relationship with the tool reached true intimacy when I helped <a href="https://mmapped.blog/posts/17-scaling-rust-builds-with-bazel">transition <span>dfinity</span>’s build system to Bazel</a>.
Despite all the challenges I faced on the way, Bazel is still my favorite build system.
It’s fast, correct, easy to use, and language-agnostic.</p><p>Paraphrasing <a href="https://www.stroustrup.com/quotes.html">Bjarne Stroustup</a>, I think a smaller, simpler, cleaner build system is struggling to get out within Bazel.
I hope this core will someday reveal itself to the world and become the standard tool for building all software.</p></section><section><h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2><p>After presenting my cases, I find it tempting to look for a common theme.
What makes a good enlightenmentware?
For me, these are the key points:</p><ul>
  <li>
  All these tools address a deep problem, and a kind of problem that I face every day, such as making programs on my computer cooperate, managing concurrent work streams, or generalizing a piece of code.
  </li><li>
  They are <q>round</q>: they pack the most volume in the smallest surface area.
  <span>unix</span> surface area is tiny, but it unlocks much power.
  Emacs and Git are all over the place, but their <em>core</em> is small, sweet, and easy to appreciate.
  </li><li>
  They invite and encourage you to explore their internals.
  It’s not only about being free and open-source; mastering them is also well worth the investment.
</li></ul><p>What’s your enlightenmentware? Tell me on <a href="https://news.ycombinator.com/item?id=40417447">Hacker News</a> or <a href="https://www.reddit.com/r/programming/comments/1cwa1m8/blog_post_enlightenmentware/">Reddit</a>!</p></section><h2>Similar articles</h2>
<ul><li><a href="https://mmapped.blog/posts/23-numeric-tower-fiasco">The numeric tower fiasco</a></li><li><a href="https://mmapped.blog/posts/18-if-composers-were-hackers">If composers were hackers</a></li></ul>
<hr></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How terminal works. Part 1: Xterm, user input (233 pts)]]></title>
            <link>https://kevroletin.github.io/terminal/2021/12/11/how-terminal-works-in.html</link>
            <guid>40419325</guid>
            <pubDate>Mon, 20 May 2024 19:37:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kevroletin.github.io/terminal/2021/12/11/how-terminal-works-in.html">https://kevroletin.github.io/terminal/2021/12/11/how-terminal-works-in.html</a>, See on <a href="https://news.ycombinator.com/item?id=40419325">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><img src="https://kevroletin.github.io/assets/how-terminal-works/img/terminal_and_shell.jpg" alt=""></p>

<ul id="markdown-toc">
  <li><a href="#motivation" id="markdown-toc-motivation">Motivation</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#user-input" id="markdown-toc-user-input">User input</a>    <ul>
      <li><a href="#strace" id="markdown-toc-strace">strace</a></li>
      <li><a href="#printing-non-printable" id="markdown-toc-printing-non-printable">Printing non-printable</a></li>
      <li><a href="#stty-raw--echo--isig" id="markdown-toc-stty-raw--echo--isig">stty raw -echo -isig</a></li>
      <li><a href="#utf-8" id="markdown-toc-utf-8">UTF-8</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
</ul>

<h2 id="motivation">Motivation</h2>

<p>This blog series explains how modern terminals and command-line tools work. The
primary goal here is to learn by <strong>experimenting</strong>. I’ll provide Linux tools to
debug every component mentioned in the discussion. Our focus is to discover
<strong>how</strong> things work. For the explanation of <strong>why</strong> things work in a certain
way, I encourage the reader to read excellent articles:</p>

<ul>
  <li><a href="https://www.linusakesson.net/programming/tty/">The TTY demystified</a></li>
  <li><a href="https://blog.nelhage.com/2009/12/a-brief-introduction-to-termios/">A Brief Introduction to termios</a></li>
</ul>

<p>and to visit a computer history museum:</p>

<ul>
  <li><a href="https://www.youtube.com/watch?v=S81GyMKH7zw">Teletype ASR 33 Part 10: ASR 33 demo</a></li>
  <li><a href="https://www.youtube.com/watch?v=uFQ3sajIdaM">The IBM 1401 compiles and runs FORTRAN II</a>.</li>
</ul>

<p>Please note that I talk solely about Linux (because that is what I use), but
many discussed concepts should apply to other Unix-like systems.</p>

<p>I’ve chosen the “learn by experimenting” approach because that’s how I’ve
learned about command-line tools. In my case, there was no single “click” moment
after which I’ve understood all the things. Instead, I’ve learned through a
never-ending process of building mental models, proving them to be wrong, and
then adjusting those models to reflect new knowledge.</p>

<p>Target audience are people who want to start working on command-line tools.</p>

<p>The series consists of 4 parts. The first two parts discuss how xterm work.
Parts 3 and 4 talk about different features of tty:</p>

<ul>
  <li>Part 1: Xterm, user input;</li>
  <li>Part 2: Xterm, CLI tool output;</li>
  <li>Part 3: pty, stty;</li>
  <li>Part 4: pty, sessions.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Let’s start the discussion with an <strong>inaccurate</strong> diagram that shows a general
use case for working with a command-line shell:</p>

<div><pre><code>           (1)   (2)   (3)
user &lt;---&gt; xterm &lt;---&gt; bash 

</code></pre></div>

<p>The user interacts with bash using a terminal emulator xterm. xterm is a GUI app
that receives “key pressed” events and writes corresponding characters into a
bidirectional filehandle (2). Bash reads those characters from (2) does
something and sends the output back to xterm, using the same filehandle (2).
Xterm reads, bash outputs from (2) and renders them on the screen. (2) is “just
a file” and this communication scheme looks pretty simple.</p>

<p>If the user asks bash to execute a command, let’s say <code>cat log.txt</code> then bash
spawns <code>cat</code> which uses the same filehandle to send its output to xterm:</p>

<div><pre><code>                       bash
           (1)   (2)     (4)
user &lt;---&gt; xterm &lt;---&gt;   cat

</code></pre></div>

<p>Again, pretty simple. In this unrealistic model (2) is “just a file” xterm and
cat exchange plain text.</p>

<p>In reality, things are slightly more complicated. Evolution extended the simple
scheme of “using a bidirectional filehandle to exchange plain text” to implement
additional features:</p>

<ol>
  <li>TUI interfaces. The terminal can draw characters at an arbitrary part of the
screen; command-line tools can ask capabilities of the terminal and can
handle window resize;</li>
  <li>job control. Shell organizes processes into logical groups which can be
paused/resumed or stopped altogether;</li>
  <li>access control for the filehandle (2). Bash has a feature to spawn background
processes. This might lead to a situation when two processes are writing
their output into the same filehandle (2) at the same time; there should be
some access control mechanism;</li>
  <li>“fixing” stupid tools which believe that the terminal is just a file with
plain text; so that those tools look and feel better.</li>
</ol>



<p>Requirement above and 50 years of history led us to this scheme:</p>

<div><pre><code>           (1)         (2)       (3)
user &lt;---&gt; xterm &lt;---&gt; tty &lt;---&gt; bash

</code></pre></div>

<p>The first thing to notice is a “middle man” <strong>tty</strong> between xterm and bash. We
will discuss tty in parts 3 and 4. For now, we will just say that:</p>
<ul>
  <li>tty sits between xterm and bash and passes data from one to the other in both
directions;</li>
  <li>depending on its configuration, tty changes data it receives from one side
before passing to the other;</li>
  <li>there is command <code>stty raw -echo -isig</code> which configures tty to pass data “as
is without modification”.</li>
</ul>

<p>Using <code>stty raw -echo -isig</code> to disable most effects of tty is our primary
strategy to explore how xterm works. Until the part 3, we will ignore the
existence of tty and will concentrate on exploring xterm’s behavior.</p>

<p>Let’s start by discussing a bi-directional link between a user and xterm.
Converting scancodes that come from a keyboard into GUI events happens in two
steps. First, Linux handles hardware events and turns them into keycodes that
can be read by userland (using device descriptors like
<code>/dev/input/by-id/usb-2.4G_2.4G_Wireless_Device-event-kbd</code>). Second, Windows
system (X or Wayland) reads Linux keycodes and converts them into its own
keycodes, and also assigns a keysym (i.e. a Unicode character). To check how it
works, one can use:</p>

<ul>
  <li><code>sudo showkey</code> to explore Linux keycodes <em>(visit <a href="https://tldp.org/HOWTO/Keyboard-and-Console-HOWTO-14.html">this page</a> for more info)</em>;</li>
  <li><code>xev</code> (or <a href="https://git.sr.ht/~sircmpwn/wev"><code>wev</code></a> for Wayland users) to explore GUI events.</li>
</ul>

<p>For example, when I press the <code>q</code> button on my keyboard, depending on my keyboard layout I see:</p>
<ul>
  <li>showkey: keycode 16</li>
  <li>xev: keycode 24 (keysym 0x71, q)</li>
  <li>xev: keycode 24 (keysym 0x6ca, Cyrillic_shorti)</li>
</ul>

<p>xterm receives keypress events and writes data into tty(2):</p>
<ul>
  <li>it encodes printable characters using configured encoding (most probably UTF-8);</li>
  <li>on receiving some key combinations, it executes actions such as copy-paste from clipboard;</li>
  <li>it encodes other key combinations and non-printable characters (such as arrow
keys) using ANSI escape sequences (see post #2 for more details about ANSI
sequences).</li>
</ul>

<p>So converting key presses into data written into tty(2) happens in 3 steps, two
involving kernel and one in xterm. Now let’s figure out what xterm sends into
tty after all those 3 steps. There are two strategies we can use to accomplish
this task:</p>

<ul>
  <li><code>strace</code>: trace system calls <em>(we will trace <code>write</code> and <code>read</code> calls, but be aware that there are also
<a href="https://man7.org/linux/man-pages/man7/aio.7.html">aio</a> API)</em>;</li>
  <li>run a command-line tool that will
    <ul>
      <li>disable tty’s input/output processing using <code>stty raw -echo -isig</code></li>
      <li>log its inputs.</li>
    </ul>
  </li>
</ul>

<h3 id="strace">strace</h3>

<p>Let’s start with <code>strace</code> because it’s quite a practical approach. In your daily
life, if you’ll get stuck with misbehaving command-line tools, you can attach to
a running process and observe what your terminal is writing into filehandles and
what your shell reads. You don’t need to restart running programs to figure out
what is going on.</p>

<p>First, here is a little helper to find out PID of a terminal by clicking it with a computer mouse (for users of XWindows system):</p>

<div><pre><code>xprop | grep '_NET_WM_PID(CARDINAL)' | awk '{print $3}'
</code></pre></div>

<p>Then let’s observe what xterm writes and reads into/from filehandles (please
replace <code>-p 22853</code> with an appropriate PID):</p>

<div><pre><code>sudo strace -f -e 'trace=write,read' -e write=all -e read=all -p 22853 2&gt;&amp;1 | grep -v EAGAIN
</code></pre></div>

<p>For testing, I’ve entered <code>qwe</code> sequence and strace gave me:</p>

<div><pre><code>write(4, "q", 1)                        = 1
 | 00000  71                                                q                |
read(4, "q", 4096)                      = 1
 | 00000  71                                                q                |
write(4, "w", 1)                        = 1
 | 00000  77                                                w                |
read(4, "w", 4096)                      = 1
 | 00000  77                                                w                |
write(4, "e", 1)                        = 1
 | 00000  65                                                e                |
read(4, "e", 4096)                      = 1
 | 00000  65
</code></pre></div>

<p>That makes sense. xterm sends (writes) <code>q</code>. tty+bash echoes back <code>q</code> to display
it so that the user can see what he/she entered. Then a sequence <code>we</code> follows
the same pattern. Now, I’ll try arrow keys: the left arrow and then the right
arrow:</p>

<div><pre><code>write(4, "\33[D", 3)                    = 3
 | 00000  1b 5b 44                                          .[D              |
read(4, "\10", 4096)                    = 1
 | 00000  08                                                .                |
write(4, "\33[C", 3)                    = 3
 | 00000  1b 5b 43                                          .[C              |
read(4, "\33[C", 4096)                  = 3
 | 00000  1b 5b 43                                          .[C              |
</code></pre></div>

<p>For left arrow key, xterm sends <code>\33[D</code> and receives back <code>\10</code>. <code>man ascii</code>
tells us that <code>33</code> Oct is the same <code>1b</code> Hex and it’s a <code>\ESC</code> (escape) ASCII
control character. <code>10</code> Oct is <code>08</code> Hex and its <code>BS</code> backspace control character
(commonly abbreviated as <code>\b</code> thanks to C programming language). We will discuss
ANSI escape sequences and ASCII control characters soon, for now, we can confirm
that using strace helps to observe what xterm is actually doing: it sends
<code>qwe\ESC[D\ESC[C</code> and receives <code>qwe\b\ESC[C</code>.</p>

<p>Let’s use strace to observe what bash is doing.</p>



<p>Entering the sequence <code>qwe&lt;left&gt;&lt;right&gt;</code> gives me symmetrical result from the
bash side: It receives <code>qwe\ESC[D\ESC[C</code> and sends <code>qwe\b\ESC[C</code> back.</p>

<div><pre><code>read(0, "\33", 1)                       = 1
 | 00000  1b                                                .                |
read(0, "[", 1)                         = 1
 | 00000  5b                                                [                |
read(0, "D", 1)                         = 1
 | 00000  44                                                D                |
write(2, "\10", 1)                      = 1
 | 00000  08
</code></pre></div>

<p>I’ve promised to ignore tty for a while, but just to show why it might be useful
to strace both a terminal and bash, let’s experiment. Let’s execute <code>cat -</code>
command and observe in real-time what xterm is sending to tty and what <code>cat</code>
receives.</p>

<p>First, let’s get the PID of a shell and then execute <code>cat</code></p>

<div><pre><code>echo $$

10519
sh-4.4$ cat -
</code></pre></div>

<p>Then in the other terminal window, let’s find out the PID of <code>cat</code> using “parent
PID” option of ps:</p>

<div><pre><code>ps --ppid 10519

  PID TTY          TIME CMD
10560 pts/5    00:00:00 cat
</code></pre></div>

<p>In my system, experiment shows that xterm writes characters one by one
immediately after I’ve pressed a keyboard button. Yet <code>cat</code> receives the entire
line only after I’ve pressed Enter. I can use the Backspace key to erase
previously entered characters, which is relatively complicated logic. This logic
is part of what tty is capable of.</p>

<div><pre><code>read(0, "qwe\33[D\33[C\n", 131072)      = 10
 | 00000  71 77 65 1b 5b 44 1b 5b  43 0a                    qwe.[D.[C.       |
</code></pre></div>

<p>We will discuss tty in the 2nd part. For now, let’s just enjoy the success of
our debugging approach: we’ve just observed what <em>exactly</em> xterm and bash send
to each other and how tty (which sits in the middle) can alter data before
sending it to a consumer. The big limitation of such an approach is that reading
sequences like <code>\33[D\33[C\n</code> require a certain patience and might be quite hard
if applications output a lot of data ¯_(ツ)_/¯.</p>

<h3 id="printing-non-printable">Printing non-printable</h3>

<p>While playing with strace we’ve encountered sequences like this <code>\33[D\33[C</code>
which I’ve later written like this: <code>\ESC[D\ESC[C</code>. In my daily life, I
sometimes encounter different notations, for example <code>\u001b[D\u001b[C</code>,
<code>\x1b[D\x1b[C</code>, or something else. Different software uses different conventions
for visualizing non-printable characters. Also, many programming languages have
a way to embed non-printable characters into string literals using a sequence of
printable characters. But again, conventions for representing non-printable
characters using printable ones differ between programming languages.</p>

<p>Let’s discover how different software visualizes the ESC (escape) ASCII character:</p>



<ul>
  <li>vi, emacs: <code>^[</code></li>
  <li>less: <code>ESC</code></li>
  <li>code, gedit : on my systems render some nonsense</li>
  <li>hexdump: <code>1b</code> (hexdump supports many output formats)</li>
  <li>od -a : <code>esc</code> (od supports many output formats)</li>
  <li>strace: <code>\33</code> and <code>1b</code></li>
  <li>python: <code>\x1b</code>
    <div><pre><code>open("/tmp/data.txt", "r").read()
</code></pre></div>
  </li>
  <li>Haskell: <code>\ESC</code>
    <div><pre><code>import qualified Data.ByteString as BS
BS.readFile "/tmp/data.txt" &gt;&gt;= print
</code></pre></div>
  </li>
  <li>nodejs: <code>\u001b</code>
    <div><pre><code>const fs = require('fs')
console.dir( fs.readFileSync('/tmp/data.txt', 'utf8') )
</code></pre></div>
  </li>
</ul>

<p>To make things more confusing, some popular programming languages support syntax
for embedding non-printable characters into string literals, but don’t provide
easily accessible function to convert a string into the same notation. For
example, using the C programming language, I can easily make a string containing
ESC character:</p>



<p>But the easiest way I know to visualize it using printable characters is to write code like this:</p>

<div><pre><code>#include &lt;stdio.h&gt;
#include &lt;ctype.h&gt;

int main() {
    FILE* f = fopen("/tmp/data.txt", "r");
    int c = fgetc(f);
    while (!feof(f)) {
        if (isprint(c))
            printf("%c", c);
        else
            printf("\\x%x", c);
        c = fgetc(f);
    }
    return 0;
}
</code></pre></div>

<p>The moral here is that different tools visualize non-printable characters
differently. To make things less confusing it’s helpful to train your eye to
recognize magic strings <code>^[</code>, <code>\ESC</code>, <code>ESC</code>, <code>esc</code>, <code>1b</code>, <code>\x1b</code>, <code>0x1b</code>,
<code>\u001b</code>, <code>33</code>, <code>27</code>. Also, it’s helpful to choose tools you can understand even
under stress.</p>

<h3 id="stty-raw--echo--isig">stty raw -echo -isig</h3>

<p>We’ve traced xterm using <code>strace</code> to check what it sends to bash. We can
accomplish a similar task without using a tracing tool. The most fool-proof way
to do so is to disable the effects of tty and to dump binary data which comes
from tty into a file. Then we can explore the content of a file using our
favorite tool of choice:</p>

<div><pre><code>stty raw -echo -isig; dd bs=1 of=/tmp/data.txt
</code></pre></div>

<p>I prefer to use vi or od:</p>

<div><pre><code>vi /tmp/data.txt
od -ac /tmp/data.txt
</code></pre></div>

<p>It might be cool to visualize the same data in real-time. One can use this bash
one-liner:</p>

<div><pre><code>stty sane -isig -echo -icanon; while true; do od -N 1 -ax -; done
</code></pre></div>

<p>Or convert <code>man ascii</code> into a <a href="https://kevroletin.github.io/assets/how-terminal-works/display_ascii.c">small c
program</a>. It executes <code>stty raw
-echo</code> on startup, so that tty doesn’t change terminal output and hence the tool
shows what terminal sends into tty.</p>

<p>Pressing a sequence of <code>a</code>, <code>1</code>, <code>Ctrl+d</code>, <code>Ctrl+l</code> gives:</p>

<div><pre><code>a
1
EOT (end of transmission)
FF  '\f' (form feed)
</code></pre></div>

<p><code>Alt+d</code> gives 2 characters:</p>



<p><code>Ctrl+Alt+Shift+d</code> gives:</p>

<div><pre><code>ESC (escape)
EOT (end of transmission)
</code></pre></div>

<p>which is the same as <code>Ctrl+Alt+d</code>, Shift is just ignored.</p>

<p>That behavior of xterm is <strong>not</strong> set in stone and it is configurable in a
terminal-dependent way. Depending on its configuration, xterm might send
different things in response to <code>Ctrl+Alt+Shift</code> combination. Here is the
discussion about xterm <a href="https://invisible-island.net/xterm/modified-keys.html">modified
keys</a>.</p>

<h3 id="utf-8">UTF-8</h3>

<p>Utf8 has a few nice features which I didn’t appreciate enough until recently:</p>

<ol>
  <li>Utf8 is a self-synchronizing code. If you take any Utf8-encoded string and
randomly chop off the beginning so that you end up in the middle of
multi-byte character, then:
    <ul>
      <li>you’ll be able to detect an error: an attempt to decode an invalid character;</li>
      <li>you’ll be able to recover from the error by discarding bytes of a broken
character and figure out the beginning of the next valid character.</li>
    </ul>
  </li>
  <li>ASCII characters (including control character) are valid one-byte Utf8 encoded characters.</li>
</ol>

<p>Combined (1) and (2) give us a nice property that control characters will never
appear as part of multi-byte characters. I.e. python code below is correct for
any string comprising multi-byte Utf8 characters:</p>



<p>Let’s understand why this is the case. <code>编程很有趣</code> is encoded using 15 bytes;
below I’ve represented bytes using decimal numbers:</p>

<div><pre><code>编            程             很            有            趣
231 188 150 | 231 168 139 | 229 190 136 | 230 156 137 | 232 182 163
</code></pre></div>

<p>All these numbers are greater than 127 Dec. But all ASCII characters (including
control characters) are lesser or equal to 127 Dec. So it’s safe to search for
single-byte ASCII characters in Utf8 strings without decoding them because all
bytes of every valid multi-byte character is guaranteed to be greater than 127
Dec.</p>

<p>Error recovery is possible, and it works surprisingly simple. In binary
notation, all ASCII characters start with leading <code>0</code>, all bytes of multi-byte
characters start with <code>1</code>. In addition, for multi-byte characters:</p>
<ul>
  <li>only the first byte of a character can start with <code>11</code>;</li>
  <li>all continuation bytes (bytes 2, 3, 4) start with <code>10</code>.</li>
</ul>

<p>You can easily observe these properties in action:</p>

<div><pre><code>编
11100111
10111100
10010110
程
11100111
10101000
10001011
很
11100101
10111110
10001000
有
11100110
10011100
10001001
趣
11101000
10110110
10100011
</code></pre></div>

<p>Each byte starts with <code>1</code> indicating that it’s a part of a multi-byte character.
Also, each byte contains an indication if it’s the first byte or a continuation
byte.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Using strace and by disabling tty features, we’ve explored how keyboard input
from users reaches command-line tools. We also saw that xterm might send
non-printable characters and different tools visualize non-printable characters
differently. Also, we’ve improved our mental resilience by getting accustomed to
different notations and by trying different tools for visualizing control
characters. Finally, we said a few words about Utf8 encoding, which is the most
widely used Unicode encoding nowadays.</p>

<p>In this blog, post we’ve discussed how xterm handles user input. Next post will
discuss how xterm visualizes the output of CLI tools.</p>

<p>Stay tuned :)</p>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using Postgres for Everything (144 pts)]]></title>
            <link>https://www.timescale.com/blog/how-to-collapse-your-stack-using-postgresql-for-everything/</link>
            <guid>40418983</guid>
            <pubDate>Mon, 20 May 2024 19:07:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.timescale.com/blog/how-to-collapse-your-stack-using-postgresql-for-everything/">https://www.timescale.com/blog/how-to-collapse-your-stack-using-postgresql-for-everything/</a>, See on <a href="https://news.ycombinator.com/item?id=40418983">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>The idea of using <a href="https://www.timescale.com/blog/postgres-for-everything/"><u>PostgreSQL for Everything</u></a> isn't <a href="https://www.amazingcto.com/postgres-for-everything/?ref=timescale.com"><u>new</u></a>, but it's <a href="https://leaddev.com/tech/postgresql-database-quietly-ate-world?ref=timescale.com"><u>steadily</u></a> <a href="https://www.reddit.com/r/PostgreSQL/comments/1cfpyv7/what_does_postgresql_for_everything_mean_to_you/?ref=timescale.com"><u>gaining</u></a> <a href="https://www.youtube.com/watch?v=VEWXmdjzIpQ&amp;ref=timescale.com"><u>attention</u></a>, especially as Postgres keeps <a href="https://survey.stackoverflow.co/2023/?ref=timescale.com#section-most-popular-technologies-databases"><u>increasing in popularity</u></a>. As someone who’s spent most of their career deploying all sorts of databases, here’s what it means to me and how you can apply it to get some simplicity back into your world.</p><h2 id="how-technical-sprawl-creeps-in">How Technical Sprawl Creeps In</h2><p>Imagine you're starting on a new product or feature. Early on, your team lists out technical problems you need to tackle. Some solutions you'll develop in-house (your secret sauce), and others you'll address with existing technologies, probably including at least one database.</p><p>Unless you're in the business of building databases, it’s usually unwise to develop your own; it's complex, risky, and requires a very specialized skill set. So, you might end up adopting various existing databases: Postgres for transactional data, Elastic for full-text search, Influx for time series, Pinecone for vector operations, and maybe ClickHouse for analytics. Suddenly, your tech stack is sprawling.</p>
<!--kg-card-begin: html-->
<p><a href="https://imgflip.com/i/8pw1uv?ref=timescale.com"><img src="https://i.imgflip.com/8pw1uv.jpg" title="made at imgflip.com"></a><a></a></p><!--kg-card-end: html-->
<h2 id="why-stack-sprawl-is-a-problem">Why Stack Sprawl Is a Problem</h2><p>Each new database you add brings its own set of challenges: different languages to learn, consistency models to understand, and operational nuances that can’t be ignored. Not only does this add complexity, but it also introduces what I call “dotted line” complexity, the additional overhead that comes from each pair of systems which data flows between. The more databases and the more dotted lines you have, the harder it is to reason about the state of your system as a whole.</p><p>You’ve got more databases, and because of that, you’ve got more problems.</p><figure><img src="https://www.timescale.com/blog/content/images/2024/05/Postgres-for-Everything_Biggie.gif" alt="A Biggie gif: mo' databases, mo' problems" loading="lazy" width="512" height="512"></figure><h2 id="the-case-for-collapsing-your-stack">The Case for Collapsing Your Stack</h2><p>So what’s the alternative? In my mind, it’s collapsing your stack. If you solve more problems with one database, you’re removing multiple complex pieces of software and the dotted line complexity between them. It’s much easier to keep a mental model of your data flow in your head, as well as reason about the consistency of data at different times. You get time back, which would have been spent on operating these new databases, and you can spend that time building features.</p><figure><img src="https://www.timescale.com/blog/content/images/2024/05/Collapse-your-stack-postgres-for-eveything-3.png" alt="Several problems leading to a single Postgres solution" loading="lazy" width="1200" height="701" srcset="https://www.timescale.com/blog/content/images/size/w600/2024/05/Collapse-your-stack-postgres-for-eveything-3.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2024/05/Collapse-your-stack-postgres-for-eveything-3.png 1000w, https://www.timescale.com/blog/content/images/2024/05/Collapse-your-stack-postgres-for-eveything-3.png 1200w" sizes="(min-width: 720px) 720px"></figure><p>PostgreSQL excels at stack collapsing because it’s simultaneously general-purpose and specialized. As well as being an amazing relational database, it supports a wide range of extra use cases through its advanced extension framework. PostgreSQL can easily handle workloads like <a href="https://www.postgresql.org/docs/current/textsearch-intro.html?ref=timescale.com"><u>full-text search</u></a>, <a href="https://www.timescale.com/?ref=timescale.com"><u>time-series data,</u></a> <a href="https://github.com/pgvector/pgvector?ref=timescale.com"><u>vectors for AI</u></a>, and analytics.&nbsp;</p><p>PostgreSQL isn’t just versatile, it's also robust and mature. People have been running PostgreSQL in production for over 20 years, and with adoption speeding up, PostgreSQL shows no sign of slowing down. Edge cases are well known, deployment patterns, recovery strategies, and high availability are well defined, and there are many, many companies and champions who can help you along the way.</p><p>Because of this, I encourage you to use PostgreSQL to solve as many problems as you can, collapsing your stack, reducing your complexity, and giving you time back to focus on building.</p><p>There’s a well-known argument that you should pick the "best tool for the job," which sometimes gets turned on its head as, “If you’ve only got a hammer, everything looks like a nail.” I don’t see the principle of "PostgreSQL for Everything" contradicting these, as long as you make sure you look at the big picture.</p><p>How do you define "The Best Database for the Job"? Is it the fastest? The easiest to use? The most fault-tolerant? Or is it the one that integrates most seamlessly into your existing infrastructure and you know how to use—perhaps one that’s already in place? The best choice usually falls somewhere in between these criteria.</p><figure><img src="https://www.timescale.com/blog/content/images/2024/05/collapse-your-stack-postgres-for-everything-4.png" alt="The phrase: &quot;Let's just attach x to y and send the data to z.&quot; by someone who has no experience supporting x, y, or z. White text over a black background" loading="lazy" width="1200" height="701" srcset="https://www.timescale.com/blog/content/images/size/w600/2024/05/collapse-your-stack-postgres-for-everything-4.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2024/05/collapse-your-stack-postgres-for-everything-4.png 1000w, https://www.timescale.com/blog/content/images/2024/05/collapse-your-stack-postgres-for-everything-4.png 1200w" sizes="(min-width: 720px) 720px"></figure><p>Should you choose database X for its speed, database Y for efficiency, or database Z for its cloud optimization? If good old PostgreSQL does what you need now—with battle-tested effectiveness—and can scale further (perhaps up to 10x your current needs), then I think you should start with the known quantity. Only consider adding other databases when PostgreSQL lacks critical features, weighing the benefits against the added complexity of managing multiple systems. Or, to put it in a slightly different way, <a href="https://mcfunley.com/choose-boring-technology?ref=timescale.com"><u>Choose Boring Technology</u></a> (sorry Postgres, I promise I still think you’re exciting).</p><p>Let’s consider two possible scenarios:</p><ol><li><strong>PostgreSQL for Everything works</strong>: After years, your workload outgrows the original system capabilities. PostgreSQL struggles in some areas, but this is a “good problem”—a sign of success and a cue to evolve your architecture.</li><li><strong>Overengineered with multiple databases</strong>: You're set up to handle immense scale, but the system is fragile, full of edge cases, and difficult to maintain. This isn't just challenging; it's a threat to your operation's stability.</li></ol><p>Given these scenarios, I’d argue that the theoretical future challenges of a PostgreSQL-centric system are preferable to today’s complexities of opting into a multi-database architecture too early.&nbsp;</p><h2 id="a-final-word">A Final Word</h2><p>"PostgreSQL for Everything" isn't about never using other databases. Honestly, it’s not even about using PostgreSQL for everything. It's a maxim against overengineering your solutions prematurely and an advocate for the benefits of simplicity. Just remember, there are a lot of companies and applications in the world, and with the help of companies like Timescale, PostgreSQL will scale to meet most of their demands.&nbsp;</p><p>If you want to expand your PostgreSQL database, try Timescale. <a href="https://console.cloud.timescale.com/signup?ref=timescale.com" rel="noreferrer">Create a free account today</a> and start simplifying your data stack.</p>
        <div>
          <p>Ingest and query in milliseconds, even at terabyte scale.</p>
          </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rethinking Text Resizing on Web (245 pts)]]></title>
            <link>https://medium.com/airbnb-engineering/rethinking-text-resizing-on-web-1047b12d2881</link>
            <guid>40418591</guid>
            <pubDate>Mon, 20 May 2024 18:36:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/airbnb-engineering/rethinking-text-resizing-on-web-1047b12d2881">https://medium.com/airbnb-engineering/rethinking-text-resizing-on-web-1047b12d2881</a>, See on <a href="https://news.ycombinator.com/item?id=40418591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://medium.com/@steven.j.bassett?source=post_page-----1047b12d2881--------------------------------"><div aria-hidden="false"><p><img alt="Steven Bassett" src="https://miro.medium.com/v2/resize:fill:88:88/1*XpEvQorxek161z0Bd25gPQ.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a><a href="https://medium.com/airbnb-engineering?source=post_page-----1047b12d2881--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="The Airbnb Tech Blog" src="https://miro.medium.com/v2/resize:fill:48:48/1*MlNQKg-sieBGW5prWoe9HQ.jpeg" width="24" height="24" loading="lazy" data-testid="publicationPhoto"></p></div></a></div><figure></figure><p id="9c13">Airbnb has made significant strides in improving web accessibility for Hosts and guests who require larger text sizes.</p><p id="5444">This post takes an in-depth look at:</p><ol><li id="add7">The problems encountered on mobile web when relying solely on browser zoom.</li><li id="afef">The challenges of introducing changes that would impact the workflow of all frontend engineers.</li><li id="4ba6">The benefits seen since launching these accessibility improvements.</li></ol><p id="e8d2">by: <a href="https://www.linkedin.com/in/bassettsj/" rel="noopener ugc nofollow" target="_blank">Steven Bassett</a></p><p id="1697">Improving web accessibility is a critical priority at Airbnb, and we use the Web Content Accessibility Guidelines (WCAG) to help guide our compliance efforts. One area that often leads to accessibility issues is <a href="https://www.w3.org/WAI/WCAG21/Understanding/resize-text.html" rel="noopener ugc nofollow" target="_blank">WCAG 1.4.4 Resize Text (Level AA)</a>. This guideline, which we’ll refer to as Resize Text, is particularly beneficial for people with low vision, whether correctable or not (for example with glasses or prescription contacts). The standard specifies that web content and functionality must be maintained when text is scaled 200% (2x) of its original size. Ensuring our site meets this guideline is an important part of our ongoing work to enhance accessibility for all of our users.</p><p id="9c3c">In this blog post, we’ll explore our investigation into the importance of this guideline, how we analyzed our site issues, the technical benefits for using rem units, how we decided on an approach, the cross-browser support issues we encountered, and the benefits we saw in reducing the number of reported issues for Resize Text.</p><h2 id="cded">Meeting the Needs of Users with Vision Difficulties</h2><blockquote><p id="3819"><strong>“90 million Americans over 40 have vision and eye problems. That’s more than 3 in 5.”</strong></p></blockquote><p id="a309"><a href="https://www.cdc.gov/visionhealth/resources/infographics/future.html" rel="noopener ugc nofollow" target="_blank"><em>Looking Ahead: Improving Our Vision for the Future” CDC</em></a></p><p id="a5f1">To illustrate, consider how the Airbnb homepage might appear to someone who has experienced a significant loss of visual acuity. As shown below, the text becomes extremely impossible to read comfortably.</p></div><div><h2 id="da8c">Browser Zoom</h2><p id="0e15">To better understand the accessibility challenge, let us explore how browser zoom functionality works. You may already be familiar with this feature, using keyboard shortcuts like Command / Ctrl + or Command / Ctrl — to scale all content within a window. When you increase the zoom level beyond 100%, the viewport’s height and width proportionally decrease, while the content is blown up to fit the larger window.</p><p id="fb4a">As part of our accessibility testing strategy, we were using browser zoom to test the usability of our pages both on desktop and mobile sizes. Desktop testing showed that our pages did relatively well at the 200% zoom level with our responsive web approach across the site. We saw fewer issues in the overall user experience when compared to mobile web.</p><p id="40ad">This works well on desktop, where we serve a smaller breakpoint (e.g., wide to compact) and the viewport is relatively spacious. However, the limitations of browser zoom become more pronounced on mobile web, where the viewport is smaller. If we were to scale the content in a mobile viewport, it would have to fit into a viewport that is half the width and half the height of the original. This can result in significant accessibility issues, as the text and UI elements become extremely difficult to read and interact with. As shown in the image on the right, the ability to view even a single listing within a screen’s worth of space is not possible without scrolling, leading to a frustrating experience.</p><figure><figcaption><em>Airbnb’s homepage shown at browser zoom 100% on the left, and the same screen shown at 200% showing the search and categories are cut off entirely and not able to even see the first listing.</em></figcaption></figure><h2 id="acb6">Font Scaling</h2><p id="e8ad">Font scaling is the term we’ll use to describe the ability to adjust text size independently of overall page zoom. Unlike browser zoom, which scales all content proportionally, Font Scaling applies only to the text elements on the page. This allows users to customize the font size to their preferred reading size without affecting much of layout or responsiveness of the rest of the content.</p><p id="3b00">Font Scaling, is also the term we will use for scaling the font based on a user’s preferred size. Unlike zoom, this setting will be applied to all sites. Below is an example of how the font scaling applies to just the text on the screen, showing that the only scale of the text increases, instead of all the content.</p><figure></figure><p id="69aa">Video Description:<em> Airbnb text is scaled by setting the font size on arc browser, showing the scaling from 16px to 32xp.</em></p><p id="fc49">This concept of independent font scaling is similar to the Dynamic Type feature on iOS, as we discussed in our blog post <a href="https://medium.com/airbnb-engineering/tagged/dynamic-type" rel="noopener">“Supporting Dynamic Type at Airbnb”</a>. Dynamic Type allows users to set a preferred system-wide text size, which then automatically adjusts the font size across all compatible apps.</p><p id="5e25">Considering our existing strategies for accessibility on iOS, incorporating font scaling (vs zoom scaling) into our web accessibility approach was a natural next step to help add parity in approaches across our platforms.</p><h2 id="16c7">Understanding px, em vs rem</h2><p id="553f">Now that we understand why font scaling is so powerful for mobile web, we should focus on why we might choose one CSS length unit over another for supporting font scaling. In this blog post we are only going to focus on px, em and rem but there are other units as well. CSS length units are connected to font scaling because they determine how text and other elements are sized on a web page. Some length units are fixed, meaning they don’t change based on the user’s font size settings, while others are relative, meaning they scale proportionally with the font size.</p><p id="8b19">Let’s take a deep look at 3 CSS length units and how they relate to font scaling:</p><ul><li id="d527">px units are the most commonly used on the web, theoretically they should represent one pixel on the screen. They are a fixed unit meaning the rendered value does not change.</li><li id="23e9">em units however are a relative unit that are based on the parent element’s font size. The name ‘em’ comes from the width of the capital letter ‘M’ in a given typeface, which was traditionally used as the reference point for font sizes. 1 em unit is equal to the height of the current font size, roughly 16px at the default value. em units scale proportionally, so they can be affected by their parent’s font sizes</li><li id="5320">rem units, short for “root em”, are similar to em units in that they are proportional to font size, but they only use the root element (the html element) to calculate their font size. This means that rem units offer font scaling, but are not affected by their parent’s font size.</li></ul><p id="dedd">The choice between em and rem units often comes down to the level of control and predictability required for font scaling. While em units can be used, they can lead to cascading font size changes that may be difficult to manage, especially in complex layouts. In contrast, rem units provide a more consistent and predictable approach to font scaling, as they are always relative to the root element’s font size.</p><p id="84a6">This is illustrated in the CodePen example, where the different font scaling behaviors of px, em, and rem units are demonstrated. In situations where font scaling is a critical requirement, such as the Airbnb example mentioned, the use of rem units can be a more reliable choice to ensure a consistent and maintainable font scaling solution.</p><figure></figure><p id="21aa">Relative units like rem can be used anywhere a fixed unit like px can be used. However, indiscriminate use of rem units across all properties can lead to unwanted scaling behavior and increased complexity.</p><p id="9e4c">In the case of Airbnb, the team decided to prioritize the use of rem units specifically for font scaling, rather than scaling all elements proportionally. This targeted approach provided the key benefit of consistent text scaling, without the potential downsides of scaling every aspect of the layout.</p><p id="4981">The rationale behind this decision was twofold:</p><ol><li id="54e3">Scaling <em>everything</em> using rem units would have been similar to Browser Zoom and potentially introduced unintended layout issues,</li><li id="8316">The primary focus was on providing a mobile-friendly font scaling solution. By targeting font sizes with rem units, the team could ensure that the most important content — the text — scaled appropriately.</li></ol><h2 id="ff20">Enabling a Seamless Transition for Designers and Developers</h2><p id="2724">Moving from pixel-based values to rem units as a company-wide change in CSS practice can be a significant challenge, especially when working across multiple teams. The time and effort required to educate designers and frontend developers on the new approach, and to have them convert their existing pixel-based values to rem units, can be a significant barrier to adoption. To address this, the Airbnb team decided to focus on automating the unit conversion process as much as possible, enabling a more seamless transition to the new rem-based system.</p><h2 id="18c6">Reducing Friction in Design Iterations</h2><p id="d9b0">Instead of requiring designers to have to think of new units or introduce some conversion for web only, we decided to continue to author our CSS in px units. This reduced the amount of training required for teams to start using rem units out the gate.</p><p id="f0dd">One area we did focus on with our design teams was starting to test their designs using font scaling by leveraging the <a href="https://www.figma.com/community/plugin/892114953056389734/text-resizer-accessibility-checker" rel="noopener ugc nofollow" target="_blank">Text Resizer — Accessibility Checker</a> to help simulate what a design might look like at 2X the font size. This tool helped us spot problems earlier into the design process.</p><h2 id="f7eb">Addressing the Complexity of Two CSS-in-JS Systems</h2><p id="f837">Airbnb is in the process of transitioning from <a href="https://github.com/airbnb/react-with-styles" rel="noopener ugc nofollow" target="_blank">React-with-Styles</a> to a newer approach using <a href="https://linaria.dev/" rel="noopener ugc nofollow" target="_blank">Linaria</a>. While the adoption of Linaria was progressing quickly, we recognized the need to support both styling systems for a consistent experience. Managing the conversion across these two different CSS-in-JS systems posed an additional challenge.</p><h2 id="754f">Linaria</h2><p id="bcb1">By leveraging Linaria’s support for CSS custom properties, the team was able to create new typography theme values that automatically converted the existing pixel-based values to their rem equivalents. This approach allowed the team to introduce the new rem-based theme values in a centralized manner, making them available to child elements. This gave the team the ability to override the rem values on a per-page basis, providing the necessary flexibility during the transition process.</p><pre><span id="658e">import { typography } from './site-theme';<p>// Loops through the CSS Vars we use for typography and converts them<br>// from px to rem units.<br>const theme: css`<br> ${getCssVariables({ typography: replacePxWithREMs(typography) })}<br> // Changes from:<br> // - body-font-size: 16px;<br> // To<br>// - body-font-size: 1rem; <br>`;<br>// Use the class name generated from linaria to override the theme <br>// variables for the children of this component.<br>const RemThemeLocalProvider: React.FC = ({ children }) =&gt; {<br> const cx = useCx();<br> return &lt;div className={linariaClassNames.theme)}&gt;{children}&lt;/div&gt;;<br>};ty</p></span></pre><p id="4868">Although this approach helped us convert most of the font scaling properties, there were many places in our code that we used pxbased values outside the theme. Linaria’s support for post-CSS plugins made solving these areas relatively easy. We leveraged <a href="https://github.com/cuth/postcss-pxtorem#readme" rel="noopener ugc nofollow" target="_blank">postcss-pxtorem</a> to help target those values more easily. We started by using an allow list, so that we could carefully apply this change to a smaller set of early adopting pages.</p><p id="6fd1">It was important that we provided an escape hatch when there was some reason for front-end engineers needing to use px units. Luckily we were able to provide this by using a different casing for the px value like shown below.</p><pre><span id="b9c0">/* `px` is converted to `rem` */<br>.convert {<br>  font-size: 16px; /* converted to 1rem */<br>}<br>/* `Px` or `PX` is ignored by `postcss-pxtorem` <br>   but still accepted by browsers */<br>.ignore {<br>  font-size: 200Px;<br>  font-size: clamp(16Px, 2rem, 32Px);<br>}</span></pre><h2 id="1e0d">React with Styles</h2><p id="feb2">A good amount of our frontend code still uses react-with-styles, so we had to find another way to support these cases with an easy conversion. Through this we created a simple Higher-Order component that made the conversion pretty straightforward. First we created a wrapper for the withStyles function like below, and gave the ability to avoid conversion as well.</p><pre><span id="9ba1">export const withRemStyles = (<br>  styleFn?: Nullable&lt;(theme: Theme) =&gt; Styles&gt;,<br>  options?: WithStylesOptions &amp; { disableConvertToRemUnits?: boolean },<br>) =&gt; {<br>  const disableConvertToRemUnits = getDisableConvertToRemUnits(options);<br>   // If conversion is disabled, just return the original withStyles function<br>   if (disableConvertToRemUnits) {<br>     return _withStyles(styleFn, options);<br>    }<br>   // Otherwise, wrap the original style function with a new function <br>   // that converts px to rem<br>   return _withStyles((theme: Theme) =&gt; {<br>     if (styleFn) {<br>     const styles = styleFn(theme);<br>     const remStyles = convertToRem(styles);<br>     return remStyles;<br>   }<br>   return {};<br> }, options);<br>};</span></pre><p id="98f3">Then the convertToRem will look through the keys and values and map a converted value for any of the font sizing attributes. This allowed us to automate the conversion process in a more straightforward way.</p><h2 id="6519">Improvements for Testing Components</h2><p id="7f48">With these two challenges out of the way, we can start testing our components to verify if there are any major issues we might need to resolve before rolling out. In our component documentation and tooling, we built an internal plugin to allow for easier testing by setting the font-size on the html element directly to test with font scaling.</p><p id="965a">Screenshot testing has helped our teams catch visual regressions. Adding support to allow for setting additional screenshots at different root font sizes has helped our product teams review what the component looks like at different font scales. To do this, we allow for adding additional font sizes to be set when capturing the screenshots so you don’t have to create new component variations just for font scaling.</p><h2 id="0d96">Font Scaling on Mobile Safari</h2><p id="9b30">Supporting font scaling for Mobile Safari was more difficult. Unlike other browsers, there is not a font size preference available in Mobile Safari. However, they have released support for their own font: -apple-system-body but there are some important considerations.</p><p id="6596">Since macOS High Sierra (10.13), desktop Safari also supports the font preference, but there is not an easy “font size” configuration available in MacOS. Because there can be unexpected behavior on desktop Safari, so we used a @supports statement to prevent this. The code below will only target Mobile Safari.</p><pre><span id="a035">// Apple's Dynamic Type requires this font family to be used<br>// Only target iOS/iPadOS<br>@supports (font: -apple-system-body) and (-webkit-touch-callout: default) {<br>  :root {<br>    font: -apple-system-body;<br>  }<br>}</span></pre><p id="3a02">Another consideration is that the “100%” default font size selected does not equal the standard font size of 16px, but rather 17px. This is a very subtle difference, but it is critical for the design quality bar we aim to achieve at Airbnb. So to resolve this issue, we ended up using an inline head script to normalize the value, by placing it early into the page execution we avoided seeing a change in font size.</p><pre><span id="6531">(() =&gt; {<br>  // don't do anything if the browser doesn't match the supports statement<br>  if (!CSS.supports('(font: -apple-system-body) and (-webkit-touch-callout: default)')) return;<br>  // Must create an element since the root element styles are not yet parsed.<br>  const div = document.createElement('div');<br>  div.setAttribute('style', 'font: -apple-system-body');<br>  // Body is not available yet so this has to be added to the root element<br>  documentElement.appendChild(div);<br>  const style = getComputedStyle(div);<br>  if (style.fontSize === '17px') {<br>    documentElement.style.setProperty('font-size', '16px');<br>  }<br>  documentElement.removeChild(div);<br>})();</span></pre><p id="e4b7">Then when the page loads we use a resize observer to detect if the value changes again to unset or set the font-size property on the html element. This helps us still support scalable fonts, but not have a significant impact on the default font size (100%).</p><h2 id="8cee">Impact</h2><p id="d1f5">Supporting scalable fonts is an investment that should make a dramatic difference for our Hosts and guests with low vision and anyone who benefits from larger font sizes and control over their browsing experience. Below are two examples of the home page showing how the default font size (16px) appears to someone who has blurry vision and what it looks like by doubling the font size (32px). The second image is far more legible and usable.</p></div><div><p id="43f5">Choosing font scaling as the product accessibility strategy brought about a range of significant benefits that notably enhanced our platform’s overall user experience. Making that change using automation to convert to rem units made this transition easier. When looking at our overall issues count after these changes were site wide, more than 80% of our existing Resize Text issues were resolved. Moreover, we are seeing fewer new issues since then.</p><p id="1262">To conclude, our journey to enhance Resize Text on the web has been filled with valuable, practical lessons. From how we strategically apply rem units, to the role of tooling and automation, each lesson has been a vital step forward in elevating our user experience on Airbnb. We hope that by sharing our journey, we can help others navigate this transition more seamlessly. Our work is ongoing, and we are committed to continuously advancing Airbnb’s accessibility. If you’re passionate about such challenges, we invite you to <a href="https://careers.airbnb.com/" rel="noopener ugc nofollow" target="_blank">explore career opportunities at Airbnb</a>.</p><p id="f1ac">Thanks to:</p><ul><li id="1cbd">Alan Pinto Souza, Dennis Wilkins, Jimmy Guo, and Andrew Scheuermann for advice and technical review.</li><li id="4f78">Sterling DeMille, Riley Glusker and Ryan Booth for being early product partners.</li><li id="bc65">Jordanna Kwok, Sarah Alley and JN Vollmer for supporting the approach.</li><li id="75d1">Veronica Reyes and Jamie Cristal for providing design support.</li></ul><p id="68cc"><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p></div></div>]]></description>
        </item>
    </channel>
</rss>