<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 07 Jan 2024 13:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Someone bought 26.9 BTC on Binance and sent it to Satoshi's dead wallet (126 pts)]]></title>
            <link>https://www.blockchain.com/explorer/transactions/btc/d7db4f96a4059c8906b953677ce533493d7b9da0f854a21b99f5772910dd0a31</link>
            <guid>38900049</guid>
            <pubDate>Sun, 07 Jan 2024 10:28:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.blockchain.com/explorer/transactions/btc/d7db4f96a4059c8906b953677ce533493d7b9da0f854a21b99f5772910dd0a31">https://www.blockchain.com/explorer/transactions/btc/d7db4f96a4059c8906b953677ce533493d7b9da0f854a21b99f5772910dd0a31</a>, See on <a href="https://news.ycombinator.com/item?id=38900049">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next" data-reactroot=""><div><a href="https://blockchain.com/" target="_self" rel=""><div><p><span>Blockchain<span>.com</span></span></p></div></a><div><a href="https://www.blockchain.com/explorer"><div><p><span>Home</span></p></div></a><a href="https://www.blockchain.com/explorer/prices"><div><p><span>Prices</span></p></div></a><a href="https://www.blockchain.com/explorer/charts"><div><p><span>Charts</span></p></div></a><a href="https://www.blockchain.com/explorer/nfts"><div><p><span>NFTs</span></p></div></a><a href="https://www.blockchain.com/wallet?utm_campaign=expmarketing_createwallet" target="_self" rel=""><div><p><span>Buy</span></p></div></a><div><p><span>More</span></p></div></div><div><a href="https://www.blockchain.com/explorer"><div><p><span>Home</span></p></div></a><a href="https://www.blockchain.com/explorer/prices"><div><p><span>Prices</span></p></div></a><a href="https://www.blockchain.com/explorer/charts"><div><p><span>Charts</span></p></div></a><a href="https://www.blockchain.com/explorer/nfts"><div><p><span>NFTs</span></p></div></a><a href="https://www.blockchain.com/explorer/defi"><div><p><span>DeFi</span></p></div></a><a href="https://www.blockchain.com/learning-portal/" target="_self" rel=""><div><p><span>Academy</span></p></div></a><a href="https://www.blockchain.com/explorer/news"><div><p><span>News</span></p></div></a><a href="https://www.blockchain.com/explorer/api"><div><p><span>Developers</span></p></div></a><a href="https://www.blockchain.com/wallet?utm_campaign=expmarketing_createwallet" target="_self" rel=""><div><p><span>Wallet</span></p></div></a><a href="https://exchange.blockchain.com/?utm_campaign=expmarketing_getstarted" target="_self" rel=""><div><p><span>Exchange</span></p></div></a><a href="https://www.blockchain.com/explorer/assets/btc"><div><p><span>Bitcoin</span></p></div></a><a href="https://www.blockchain.com/explorer/assets/eth"><div><p><span>Ethereum</span></p></div></a><a href="https://www.blockchain.com/explorer/assets/bch"><div><p><span>Bitcoin Cash</span></p></div></a></div></div><div><div><div><a href="https://blockchain.com/" target="_self" rel=""><div><p><span>Blockchain<span>.com</span></span></p></div></a><div><div><a href="http://wallet.blockchain.com/" target="_self" rel=""></a></div></div></div><div><div><form></form></div><div><a href="http://wallet.blockchain.com/" target="_self" rel=""></a></div></div></div><main><div><section><div><div><p>TX</p></div><div><p>Amount</p><p>Fee</p><p>From</p><p>To</p></div></div><section><section><div><p>Advanced Details</p></div><div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></section><div><div></div><div><div><p>To</p></div><div><p>From</p></div></div></div></section></section><div><p>Explore top crypto assets.</p><div></div></div></div></main></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DNS Toys (2022) (232 pts)]]></title>
            <link>https://www.dns.toys/</link>
            <guid>38899290</guid>
            <pubDate>Sun, 07 Jan 2024 07:29:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dns.toys/">https://www.dns.toys/</a>, See on <a href="https://news.ycombinator.com/item?id=38899290">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<section>
		

		<p>
			dns.toys is a DNS server that takes creative liberties with the DNS
			protocol to offer handy utilities and services
			that are easily accessible via the command line.
		</p>

		<p>
			Copy and run the below commands to try it out.
		</p>
	</section>

	<section>
		<h2>World time</h2>
		<code>
			<p>dig mumbai.time @dns.toys</p>
			<p>dig newyork.time @dns.toys</p>
			<p>dig paris/fr.time @dns.toys</p>
		</code>
		<p>Pass city names without spaces suffixed with <code>.time</code>. Pass two letter country codes separated by slash optionally.</p>

		<br>
		<h3>Timezone conversion</h3>
		<p>Pass YYYY-MM-DD<strong>T</strong>HH:MM-$fromCity-$toCity (two letter country codes separated by slash optionally).</p>

		<code>
			<p>dig 2023-05-28T14:00-mumbai-paris/fr.time @dns.toys</p>
		</code>
	</section>

	<section>
		<h2>Weather</h2>
		<code>
			<p>dig mumbai.weather @dns.toys</p>
			<p>dig newyork.weather @dns.toys</p>
			<p>dig amsterdam/nl.weather @dns.toys</p>
		</code>
		<p>
			Pass city names without spaces suffixed with <code>.weather</code>.
			Pass two letter country codes optionally.
			This service is powered by <a href="https://www.yr.no/en">yr.no</a>
		</p>
	</section>

	<section>
		<h2>Unit conversion</h2>
		<code>
			<p>dig 42km-mi.unit @dns.toys</p>
			<p>dig 32GB-MB.unit @dns.toys</p>
		</code>
		<p>$Value$FromUnit-$ToUnit. To see all 70 available units,
			<code>dig unit @dns.toys</code>
		</p>
	</section>

	<section>
		<h2>Currency conversion (forex)</h2>
		<code>
			<p>dig 100USD-INR.fx @dns.toys</p>
			<p>dig 50CAD-AUD.fx @dns.toys</p>
		</code>
		<p>$Value$FromCurrency-$ToCurrency. Daily rates are from <a href="https://exchangerate.host/">exchangerate.host</a>.</p>
	</section>

	<section>
		<h2>IP echo</h2>
		<code>
			<p>dig -4 ip @dns.toys</p>
		</code>
		<p>Echo your IPv4 address.</p>
		<code>
			<p>dig -6 ip @dns.toys</p>
		</code>
		<p>Echo your IPv6 address.</p>
	</section>

	<section>
		<h2>Number to words</h2>
		<code>
			<p>dig 987654321.words @dns.toys</p>
		</code>
		<p>Convert numbers to English words.</p>
	</section>

	<section>
		<h2>Usable CIDR Range</h2>
		<code>
			<p>dig 10.0.0.0/24.cidr @dns.toys</p>
			<p>dig 2001:db8::/108.cidr @dns.toys</p>
		</code>
		<p>Parse CIDR notation to find out first and last usable IP address in the subnet.</p>
	</section>

	<section>
		<h2>Number base conversion</h2>
		<code>
			<p>dig 100dec-hex.base @dns.toys</p>
			<p>dig 755oct-bin.base @dns.toys</p>
		</code>
		<p>Converts a number from one base to another. Supported bases are hex, dec, oct and bin.</p>
	</section>

	<section>
		<h2>Pi</h2>
		<code>
			<p>dig pi @dns.toys</p>
			<p>dig pi -t txt @dns.toys</p>
			<p>dig pi -t aaaa @dns.toys</p>
		</code>
		<p>Print digits of Pi. Yep.</p>
	</section>

	<section>

		<h2>English dictionary</h2>
		<code>
			<p>dig fun.dict @dns.toys</p>
			<p>dig big-time.dict @dns.toys</p>
		</code>
		<p>Get dictionary definitions for English words. Powered by <a href="https://wordnet.princeton.edu/">WordNet®</a>. Replace spaces with dashes.</p>
	</section>

	<section>
		<h2>Rolling dice</h2>
		<code>
			<p>dig 1d6.dice @dns.toys</p>
			<p>dig 3d20/2.dice @dns.toys</p>
		</code>
		<p>The number of dice to roll, followed by <code>d</code>, followed by the number of sides for each dice, like in tabletop RPG games.</p>
		<p>Optionally suffix by <code>/$number</code> to add it to the total.
			For example, a DnD roll like 2d20+3 is written as 2d20/3.</p>
	</section>

	<section>
		<h2>Tossing coin</h2>
		<code>
			<p>dig coin @dns.toys</p>
			<p>dig 2.coin @dns.toys</p>
		</code>
		<p>Number of coins to toss.</p>
	</section>

	<section>
		<h2>Random number generation</h2>
		<code>
			<p>dig 1-100.rand @dns.toys</p>
			<p>dig 30-200.rand @dns.toys</p>
		</code>
		<p>Generate a random number in a specified range (inclusive of the range values).</p>
	</section>

	<section>
		<h2>Epoch/Unix timestamp conversion</h2>
		<code>
			<p>dig 784783800.epoch @dns.toys</p>
		</code>
		<p>Convert an epoch/unix timestamp into a human readable date. Supports Unix timestamps in s, ms, µs and ns. </p>
	</section>

	<section>
		<h2>Calculate aerial distance</h2>
		<code>
			<p>dig A12.9352,77.6245/12.9698,77.7500.aerial @dns.toys</p>
		</code>
		<p>Calculate aerial distance between a lat-long pair</p>
	</section>

	<section>
		<h2>Generate UUIDs</h2>
		<code>
			<p>dig 5.uuid @dns.toys</p>
		</code>
		<p>Generate N UUIDs (v4).</p>
	</section>

	<section>
		<h2>Help</h2>
		<code>
			<p>dig help @dns.toys</p>
		</code>
		<p>Lists available services.</p>
	</section>

	<section>
		<h2>Shortcut function</h2>
		<div>
			<h3>Bash</h3>
			<p>
				Add this bash alias to your <code>~/.bashrc</code> file.
				The <code>+</code> args show cleaner output from dig.
			</p>
			<p><code>
				<p>alias dy="dig +short @dns.toys"</p>
			</code></p><h3>Fish</h3>
			<p>
				Add this to your fish config file.
			</p>
			<p><code>
				<p>alias dy="dig +noall +answer +additional $argv @dns.toys"</p>
			</code></p><h3>Zsh</h3>
			<p>
				Add this zsh alias to your <code>~/.zshrc</code> file.
				The <code>+</code> args show cleaner output from dig.
			</p>
			<p><code>
				<p>alias dy="dig +short @dns.toys"</p>
			</code></p><p>Then, use the dy command as a shortcut.</p>
			<p><code>
				<p>dy berlin.time</p>
				<p>dy mumbai.weather</p>
				<p>dy 100USD-INR.fx</p>
			</code>
		</p></div>
	</section>

	<section>
		<h2>Why?</h2>
		Why not? For fun. I spend a lot of time on the terminal and doing quick unit
		conversions, weather checks etc. without having to open a clunky search
		page is useful. 
	</section>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linear CEO alleges Carta mishandled sensitive cap table data (126 pts)]]></title>
            <link>https://twitter.com/karrisaarinen/status/1743824345334714587</link>
            <guid>38899001</guid>
            <pubDate>Sun, 07 Jan 2024 06:22:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/karrisaarinen/status/1743824345334714587">https://twitter.com/karrisaarinen/status/1743824345334714587</a>, See on <a href="https://news.ycombinator.com/item?id=38899001">Hacker News</a></p>
Couldn't get https://twitter.com/karrisaarinen/status/1743824345334714587: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Pocketbase: Open-source back end in one file (200 pts)]]></title>
            <link>https://pocketbase.io/</link>
            <guid>38898934</guid>
            <pubDate>Sun, 07 Jan 2024 06:08:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pocketbase.io/">https://pocketbase.io/</a>, See on <a href="https://news.ycombinator.com/item?id=38898934">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>






<div data-wave-pattern-credits="https://www.freepik.com/author/garrykillian"><div><p>Open Source backend</p>
            <p>for your next <strong>SaaS</strong> and <strong>Mobile app</strong></p>
            <p><strong>in 1 file</strong></p></div>

        <div><p><i></i>
                <span>Realtime database</span></p>
            <p><i></i>
                <span>Authentication</span></p>
            <p><i></i>
                <span>File storage</span></p>
            <p><i></i>
                <span>Admin dashboard</span></p></div>

        <figure>

            <div><p><img data-gopher-credits="https://github.com/marcusolsson/gophers" src="data:image/png;base64,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" alt="Gopher" width="69" height="90"></p>
                </div>

            <img src="data:image/png;base64,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" alt="PocketBase dashboard preview" width="1106" height="626">

            </figure></div>



<div><section><h2>Ready to use out of the box</h2>

        <div><nav>
                
                
                
                <a href="https://pocketbase.io/docs"><span>Explore all features</span>
                    <i></i></a></nav>

            <div>
                <p><code><!-- HTML_TAG_START --><span>// JavaScript SDK</span>
<span>import</span> PocketBase <span>from</span> <span>'pocketbase'</span><span>;</span>

<span>const</span> pb <span>=</span> <span>new</span> <span>PocketBase</span><span>(</span><span>'http://127.0.0.1:8090'</span><span>)</span><span>;</span>

<span>...</span>

<span>// list and search for 'example' collection records</span>
<span>const</span> list <span>=</span> <span>await</span> pb<span>.</span><span>collection</span><span>(</span><span>'example'</span><span>)</span><span>.</span><span>getList</span><span>(</span><span>1</span><span>,</span> <span>100</span><span>,</span> <span>{</span>
    <span>filter</span><span>:</span> <span>'title != "" &amp;&amp; created &gt; "2022-08-01"'</span><span>,</span>
    <span>sort</span><span>:</span> <span>'-created,title'</span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>// or fetch a single 'example' collection record</span>
<span>const</span> record <span>=</span> <span>await</span> pb<span>.</span><span>collection</span><span>(</span><span>'example'</span><span>)</span><span>.</span><span>getOne</span><span>(</span><span>'RECORD_ID'</span><span>)</span><span>;</span>

<span>// delete a single 'example' collection record</span>
<span>await</span> pb<span>.</span><span>collection</span><span>(</span><span>'example'</span><span>)</span><span>.</span><span>delete</span><span>(</span><span>'RECORD_ID'</span><span>)</span><span>;</span>

<span>// create a new 'example' collection record</span>
<span>const</span> newRecord <span>=</span> <span>await</span> pb<span>.</span><span>collection</span><span>(</span><span>'example'</span><span>)</span><span>.</span><span>create</span><span>(</span><span>{</span>
    <span>title</span><span>:</span> <span>'Lorem ipsum dolor sit amet'</span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>// subscribe to changes in any record from the 'example' collection</span>
pb<span>.</span><span>collection</span><span>(</span><span>'example'</span><span>)</span><span>.</span><span>subscribe</span><span>(</span><span>'*'</span><span>,</span> <span>function</span> <span>(</span><span>e</span><span>)</span> <span>{</span>
    console<span>.</span><span>log</span><span>(</span>e<span>.</span>record<span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span>

<span>// stop listening for changes in the 'example' collection</span>
pb<span>.</span><span>collection</span><span>(</span><span>'example'</span><span>)</span><span>.</span><span>unsubscribe</span><span>(</span><span>)</span><span>;</span><!-- HTML_TAG_END --></code>
</p></div></div></section>

    <section><h2>Integrate nicely with your favorite frontend stack</h2>
        <p><a href="https://github.com/pocketbase/dart-sdk" target="_blank" rel="noreferrer noopener"><img src="https://pocketbase.io/images/flutter_logo.svg?v2" alt="Flutter logo" width="40" height="50"></a>
            <a href="https://github.com/pocketbase/js-sdk" target="_blank" rel="noreferrer noopener"><img src="https://pocketbase.io/images/svelte_logo.svg?v2" alt="Svelte logo" width="41" height="50"></a>
            <a href="https://github.com/pocketbase/js-sdk" target="_blank" rel="noreferrer noopener"><img src="https://pocketbase.io/images/vue_logo.svg?v2" alt="Vue logo" width="53" height="46"></a>
            <a href="https://github.com/pocketbase/js-sdk" target="_blank" rel="noreferrer noopener"><img src="https://pocketbase.io/images/react_logo.svg?v2" alt="React logo" width="57" height="51"></a>
            <a href="https://github.com/pocketbase/js-sdk" target="_blank" rel="noreferrer noopener"><img src="https://pocketbase.io/images/angular_logo.svg?v2" alt="Angular logo" width="47" height="50"></a></p></section></div>




			
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia Unveils RTX 5880 Graphics Card with 14,080 CUDA Cores and 48GB VRAM (106 pts)]]></title>
            <link>https://www.nvidia.com/en-us/design-visualization/rtx-5880/</link>
            <guid>38898388</guid>
            <pubDate>Sun, 07 Jan 2024 04:19:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nvidia.com/en-us/design-visualization/rtx-5880/">https://www.nvidia.com/en-us/design-visualization/rtx-5880/</a>, See on <a href="https://news.ycombinator.com/item?id=38898388">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <div id="container-403ed265bb" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">

         
           <div>
            
            <picture data-nv-lazyload="" data-srcset-mobile="/content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-uf1080-p.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-uf1080-p@2x.jpg 2x" data-srcset-tablet="/content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l440-t.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l440-t@2x.jpg 2x" data-srcset-laptop="/content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l580-l.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l580-l@2x.jpg 2x" data-srcset-desktop="/content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l580-d.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l580-d@2x.jpg 2x" srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l580-d.jpg, https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l580-d@2x.jpg 2x">
                <source data-source-mobile="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(max-width: 639px)">
                <source data-source-tablet="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:640px) and (max-width:1023px)">
                <source data-source-laptop="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:1024px) and (max-width:1349px)">
                <source data-source-desktop="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:1350px)">
                <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" alt="NVIDIA RTX 5880 Ada Generation Graphics Card" title="NVIDIA RTX 5880 Ada Generation Graphics Card" id="image-container-403ed265bb" onload="window.initLazyLoadingImages('container-403ed265bb');">
            </picture>
            

            
            </div>
              

         


    	

        <div id="container-6432dbecc1" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    
<div id="nv-text-a145a9d24d">
				<p><span><span>Performance for endless possibilities.</span></span></p>
			</div>

<div id="container-3c5cf013ac" data-title-style="manual" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
        
    

        
    <h2 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Where to Buy
    </h2>

        
    <p>Find an NVIDIA design and visualization partner.</p>

        
	
    
    
    
    

    </div>

    
</div>
        
    </div>
<div id="l80-subnav">
    <ul>
      
      <li data-in-page-nav-item-index="0">
        <a href="#introduction">Introduction</a>
      </li>
    
      
      <li data-in-page-nav-item-index="1">
        <a href="#highlights">Highlights</a>
      </li>
    
      
      <li data-in-page-nav-item-index="2">
        <a href="#features">Features</a>
      </li>
    
      
      <li data-in-page-nav-item-index="3">
        <a href="#workloads">Workloads</a>
      </li>
    
      
      <li data-in-page-nav-item-index="4">
        <a href="#specifications">Specs</a>
      </li>
    
      
      <li data-in-page-nav-item-index="5">
        <a href="#get-started">Get Started</a>
      </li>
    </ul>
    <div>
      
      <ul>
        <li data-in-page-nav-item-index="0">
          <a href="#introduction">Introduction</a>
        </li>
      
        <li data-in-page-nav-item-index="1">
          <a href="#highlights">Highlights</a>
        </li>
      
        <li data-in-page-nav-item-index="2">
          <a href="#features">Features</a>
        </li>
      
        <li data-in-page-nav-item-index="3">
          <a href="#workloads">Workloads</a>
        </li>
      
        <li data-in-page-nav-item-index="4">
          <a href="#specifications">Specs</a>
        </li>
      
        <li data-in-page-nav-item-index="5">
          <a href="#get-started">Get Started</a>
        </li>
      </ul>
    </div>
    <div>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 21.4 5" height="24" width="24" fill="#FFFFFF">
        <path d="M12613.6,1800.5a2.654,2.654,0,1,0-2.6,2.5A2.575,2.575,0,0,0,12613.6,1800.5Zm2.7,0a2.708,2.708,0,1,0,2.7-2.5A2.6,2.6,0,0,0,12616.3,1800.5Zm8.1,0a2.654,2.654,0,1,0,2.6-2.5A2.575,2.575,0,0,0,12624.4,1800.5Z" transform="translate(-12608.3 -1798)"></path>
      </svg>
      <ul>
        <li data-in-page-nav-item-index="0">
          <a href="#introduction">Introduction</a>
        </li>
      
        <li data-in-page-nav-item-index="1">
          <a href="#highlights">Highlights</a>
        </li>
      
        <li data-in-page-nav-item-index="2">
          <a href="#features">Features</a>
        </li>
      
        <li data-in-page-nav-item-index="3">
          <a href="#workloads">Workloads</a>
        </li>
      
        <li data-in-page-nav-item-index="4">
          <a href="#specifications">Specs</a>
        </li>
      
        <li data-in-page-nav-item-index="5">
          <a href="#get-started">Get Started</a>
        </li>
      </ul>
    </div>
    
  </div>
<div id="introduction" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-131023db86">
    	<p>
	    	<h2>
		    	The Ultimate Performance for Professionals
	    	</h2>
    	</p>
     </div>
<div id="nv-text-bdf62596b8">
				<p><span>The NVIDIA RTX™ 5880 Ada Generation delivers the features, capabilities, and performance to meet the challenges of today’s professional workflows. Built on the NVIDIA Ada Lovelace GPU architecture, the RTX 5880 combines third-generation RT Cores, fourth-generation Tensor Cores, and next-gen CUDA® cores with 48GB of graphics memory for unprecedented rendering, graphics, and compute performance. NVIDIA RTX 5880-powered workstations provide what you need to succeed in today’s ultra-challenging business environment.</span></p>
			</div>

    
</div>
<div id="highlights" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-e64d0726d9">
    	<p>
	    	<h2>
		    	Highlights
	    	</h2>
    	</p>
     </div>
<div id="nv-title-ead035950e">
    	<p>
	    	<h2>
		    	Industry-Leading Performance
	    	</h2>
    	</p>
     </div>
<div id="container-fb9e6599d2" data-cmp-is="nv-container">
	        
	        <div id="container-59bc9b08f2" data-cmp-is="nv-container">
    	<p>
	    	<h3>
		    	Single-Precision Performance
	    	</h3>
    	</p>
     </div>
<div id="container-864620bb7f" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    	<p>
	    	<h3>
		    	RT Core Performance
	    	</h3>
    	</p>
     </div>
<div id="container-744d177b07" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    	<p>
	    	<h3>
		    	Tensor Performance
	    	</h3>
    	</p>
     </div>

	        
        </div>
<div id="nv-text-642d773f5b">
				<p><span><span><sup>1</sup> Peak rates based on GPU Boost Clock.</span></span></p>
			</div>


    
</div>
<div id="features" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-1eebf163f7">
    	<p>
	    	<h2>
		    	Features
	    	</h2>
    	</p>
     </div>
<div id="nv-title-4734ca1be9">
    	<p>
	    	<h2>
		    	Powered by the NVIDIA Ada Lovelace Architecture
	    	</h2>
    	</p>
     </div>

<div id="teaser-img" data-cmp-is="nv-container">
	        
	        <div id="nv-teaser-397597a4c4" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/icons/m48-gpu-chip.svg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/icons/m48-gpu-chip.svg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/icons/m48-gpu-chip.svg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/icons/m48-gpu-chip.svg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/icons/m48-gpu-chip.svg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/icons/m48-gpu-chip.svg" data-rendition-web.1280.1280="/content/dam/icons/m48-gpu-chip.svg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/icons/m48-gpu-chip.svg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        NVIDIA Ada Lovelace Architecture-Based CUDA Cores
    </h3>

        
    <p>2X the speed of the previous generation for single-precision floating-point (FP32) operations provides significant performance improvements for graphics and simulation workflows on the desktop, such as complex 3D computer-aided design (CAD) and computer-aided engineering (CAE).</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-573c25ce97" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/icons/m48-render.svg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/icons/m48-render.svg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/icons/m48-render.svg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/icons/m48-render.svg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/icons/m48-render.svg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/icons/m48-render.svg" data-rendition-web.1280.1280="/content/dam/icons/m48-render.svg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/icons/m48-render.svg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="Two" data-titlerowlaptop="Three" data-titlerowtablet="One">
        Third-Generation RT Cores
    </h3>

        
    <p>With up to 2X the throughput over the previous generation, third-generation RT Cores deliver massive speedups for workloads like photorealistic rendering of movie content, architectural design evaluations, and virtual prototyping of product designs. This technology also accelerates the rendering of ray-traced motion blur with greater visual accuracy.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-22374c70b9" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg/jcr:content/renditions/cq5dam.thumbnail.48.48.jpeg" data-rendition-thumbnail.600.338="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg" data-rendition-web.1280.1280="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="Two" data-titlerowlaptop="Two" data-titlerowtablet="One">
        Fourth-Generation Tensor Cores
    </h3>

        
    <p>Fourth-generation Tensor Cores support acceleration of the FP8 precision data type and provide independent floating-point and integer data paths to speed up execution of mixed floating-point and integer calculations.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-dc60e692ca" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/icons/m48-ram-memory.svg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/icons/m48-ram-memory.svg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/icons/m48-ram-memory.svg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/icons/m48-ram-memory.svg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/icons/m48-ram-memory.svg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/icons/m48-ram-memory.svg" data-rendition-web.1280.1280="/content/dam/icons/m48-ram-memory.svg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/icons/m48-ram-memory.svg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        48GB of GPU Memory
    </h3>

        
    <p>With 48GB GDDR6 memory, RTX 5880 gives data scientists, engineers, and creative professionals the large memory needed to work with large datasets and workloads like rendering, data science, and simulation.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-58f79ceec4" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/icons/m48-gpu-encoding.svg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/icons/m48-gpu-encoding.svg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/icons/m48-gpu-encoding.svg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/icons/m48-gpu-encoding.svg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/icons/m48-gpu-encoding.svg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/icons/m48-gpu-encoding.svg" data-rendition-web.1280.1280="/content/dam/icons/m48-gpu-encoding.svg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/icons/m48-gpu-encoding.svg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        AV1 Encoders
    </h3>

        
    <p>Eighth-generation dedicated hardware encoder (NVENC) with AV1 encoding unlocks new opportunities for streamers, broadcasters, and video conferencing. It’s 40% more efficient than H.264, allowing users streaming at 1080p to increase their resolution to 1440p while running at the same bit rate and quality.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-aba6273ef9" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/icons/m48-virtual-pc-cloud-computer.svg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/icons/m48-virtual-pc-cloud-computer.svg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/icons/m48-virtual-pc-cloud-computer.svg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/icons/m48-virtual-pc-cloud-computer.svg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/icons/m48-virtual-pc-cloud-computer.svg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/icons/m48-virtual-pc-cloud-computer.svg" data-rendition-web.1280.1280="/content/dam/icons/m48-virtual-pc-cloud-computer.svg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/icons/m48-virtual-pc-cloud-computer.svg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Virtualization-Ready
    </h3>

        
    <p>Support for <a href="https://www.nvidia.com/en-us/design-visualization/virtual-workstation/">NVIDIA RTX Virtual Workstation (vWS) software</a> allows a personal workstation to be repurposed into multiple high-performance virtual workstation instances, letting remote users share resources to drive high-end design, and compute workloads.</p>

        
	
    
    
    
    

    </div>

	        
        </div>


    
</div>
<div id="workloads" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-2b96a72d51">
    	<p>
	    	<h2>
		    	Workloads
	    	</h2>
    	</p>
     </div>
<div id="nv-title-717ac37f39">
    	<p>
	    	<h2>
		    	Unlock the Power of NVIDIA RTX for Your Workloads
	    	</h2>
    	</p>
     </div>

<div data-numcards="4" id="rtx5880-workloads">
    
      <div id="card-2" data-gradient-start="rgba(17,17,17,0.7)" data-gradient-end="rgba(17,17,17,0)" data-background-text-color="#FFFFFF" data-index="0" data-imgcredit-text-color="#CCCCCC">
        <div>
          <h4>3D Modeling and Rendering</h4>
          
          <div>
              <p>Create awe-inspiring 3D visuals with RTX 5880. Harness the power of the latest CUDA cores and RT Cores to drive real-time design, intricate geometry, and lifelike textures. Empower your artistic genius and immerse yourself in crafting photorealistic 3D masterpieces.</p>
            </div>
        </div>
        <figure>
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i1of4-d.jpg" alt="3D visuals with NVIDIA RTX 5880">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i1of4-l.jpg" alt="3D visuals with NVIDIA RTX 5880">
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i1of4-t.jpg" alt="3D visuals with NVIDIA RTX 5880">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i1of4-p.jpg" alt="3D visuals with NVIDIA RTX 5880">
		      <figcaption></figcaption>
        </figure>
        </div>
    
      <div id="card-4" data-gradient-start="rgba(17,17,17,0.7)" data-gradient-end="rgba(17,17,17,0)" data-background-text-color="#FFFFFF" data-index="1" data-imgcredit-text-color="#CCCCCC">
        <div>
          <h4>NVIDIA Omniverse</h4>
          
          <div>
              <p>For the most complex Omniverse workloads, the RTX 5880 enables accelerated ray-traced and path-traced rendering of materials, physically accurate simulations, and generating photorealistic 3D synthetic data.</p>
            </div>
        </div>
        <figure>
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/workloads-omniverse-fg-i4of6-d.jpg" alt="NVIDIA Omniverse">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/workloads-omniverse-fg-i4of6-l.jpg" alt="NVIDIA Omniverse">
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/workloads-omniverse-fg-i4of6-t.jpg" alt="NVIDIA Omniverse">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/workloads-omniverse-fg-i4of6-p.jpg" alt="NVIDIA Omniverse">
		      <figcaption></figcaption>
        </figure>
        </div>
    
      <div id="card-5" data-gradient-start="rgba(17,17,17,0.7)" data-gradient-end="rgba(17,17,17,0)" data-background-text-color="#FFFFFF" data-index="2" data-imgcredit-text-color="#CCCCCC">
        <div>
          <h4>Video Content and Streaming</h4>
          
          <div>
              <p>Experience a new era of GPU-accelerated video content creation and streaming. With RTX 5880, harness the power of three encode and three decode engines combined with fourth-gen Tensor Cores to unlock limitless possibilities in video production and&nbsp;<a href="https://www.nvidia.com/en-us/industries/media-and-entertainment/professional-broadcast/">broadcast</a>.</p>
            </div>
        </div>
        <figure>
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i3of4-d.jpg" alt="AI-powered video content creation and streaming">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i3of4-l.jpg" alt="AI-powered video content creation and streaming">
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i3of4-t.jpg" alt="AI-powered video content creation and streaming">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i3of4-p.jpg" alt="AI-powered video content creation and streaming">
		      <figcaption></figcaption>
        </figure>
        </div>
    
      <div id="card-6" data-gradient-start="rgba(17,17,17,0.7)" data-gradient-end="rgba(17,17,17,0)" data-background-text-color="#FFFFFF" data-index="3" data-imgcredit-text-color="#CCCCCC">
        <div>
          <h4>Data Visualization and Simulation</h4>
          
          <div>
              <p>Accomplish compute-intensive data science tasks faster. With 48GB of GPU memory, the RTX 5880 makes it possible to examine large datasets interactively without cutting down the size of the data or reducing fidelity.</p>
            </div>
        </div>
        <figure>
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i4of4-d.jpg" alt="Compute-intensive data science tasks faster.">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i4of4-l.jpg" alt="Compute-intensive data science tasks faster.">
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i4of4-t.jpg" alt="Compute-intensive data science tasks faster.">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i4of4-p.jpg" alt="Compute-intensive data science tasks faster.">
		      <figcaption></figcaption>
        </figure>
        </div>
    
  </div>

    
</div>
<div id="performance" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-f125866496">
    	<p>
	    	<h2>
		    	Performance
	    	</h2>
    	</p>
     </div>
<div id="nv-title-311fdb2599">
    	<p>
	    	<h2>
		    	Performance for Endless Possibilities
	    	</h2>
    	</p>
     </div>
<div id="nv-text-ce9aacb821">
				<p><span>Designed to meet the challenges of today’s professional workflows, the RTX 6000 provides unprecedented performance for rendering, AI, graphics, and compute workloads. Delivering up to 10X higher performance than the previous generation, the RTX 6000 helps you take your work to new heights.</span></p>
			</div>
<div id="container-0fde2f61c9" data-cmp-is="nv-container">
	        
	        <div id="container-94de67496f" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-d3e08d5752">
    	<p>
	    	<h3>
		    	Graphics
	    	</h3>
    	</p>
     </div>

<div id="nv-text-6fd52df93e">
				<p><span><span>3840x2160 resolution, SPECviewperf 2020 geomean.</span></span></p>
			</div>

    
</div>
<div id="container-e1160389ff" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-099372c4be">
    	<p>
	    	<h3>
		    	Rendering 
	    	</h3>
    	</p>
     </div>

<div id="nv-text-53ebe9a256">
				<p><span><span>1920x1080 resolution, Chaos V-Ray v5.0.</span></span></p>
			</div>

    
</div>
<div id="container-69b81098e8" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-1259499045">
    	<p>
	    	<h3>
		    	Generative AI
	    	</h3>
    	</p>
     </div>

<div id="nv-text-acb2477100">
				<p><span><span>RTX 6000 Ada Generation vs RTX A6000 image generation, 512x512 Stable Diffusion webUI v1.3.1.</span></span></p>
			</div>

    
</div>
<div id="container-321c29de5a" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-abb07af2f7">
    	<p>
	    	<h3>
		    	Omniverse
	    	</h3>
    	</p>
     </div>

<div id="nv-text-de1d3ede79">
				<p><span><span>NVIDIA Omniverse performance for real-time rendering at 4K with NVIDIA Deep Learning Super Sampling (DLSS) 3.</span></span></p>
			</div>

    
</div>
<div id="container-2c697d00b2" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-749ff90553">
    	<p>
	    	<h3>
		    	AI Inference
	    	</h3>
    	</p>
     </div>

<div id="nv-text-943434eb9c">
				<p><span><span>TensorRT, ResNet-50 V1.5 Inference, precision: mixed.</span></span></p>
			</div>

    
</div>
<div id="container-c5559848a4" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-d1e648e5bc">
    	<p>
	    	<h3>
		    	AI Training
	    	</h3>
    	</p>
     </div>

<div id="nv-text-77f7fe24ef">
				<p><span><span>PyTorch, BERT Large Pre-Training, precision: mixed.</span></span></p>
			</div>

    
</div>

	        
        </div>
<div id="nv-text-3f803d4e88">
				<p><span><span>Performance testing with RTX 6000 Ada Generation and RTX A6000 GPUs and Intel Core i9-12900K. Performance subject to change.</span></span></p>
			</div>

    
</div>
<div id="specifications" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-d012c15180">
    	<p>
	    	<h2>
		    	Specifications
	    	</h2>
    	</p>
     </div>
<div id="nv-title-ae7a3b9671">
    	<p>
	    	<h2>
		    	NVIDIA RTX 5880
	    	</h2>
    	</p>
     </div>




    
</div>
<div id="news-events" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-4f3671cf67">
    	<p>
	    	<h2>
		    	News and Events
	    	</h2>
    	</p>
     </div>

<div aria-atomic="false" aria-live="polite" id="success-stories-carousel" role="group" aria-roledescription="carousel" data-cmp-is="carousel" data-cmp-autoplay="" data-cmp-delay="5000" data-cmp-autoscroll="false" data-cmp-scroll-direction="left" data-cmp-scroll-delay="5000" data-cmp-scroll-disabled="false">
         <div>
            <div id="success-stories-carousel-item-e865de9f76-tabpanel" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none" role="tabpanel" aria-roledescription="slide" aria-label="Slide 1 of 2" data-cmp-slide-no="1" data-cmp-hook-carousel="item">

         
           <div>
            

            
            <picture>
                <source srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-p.jpg, https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-p@2x.jpg 2x" media="(max-width: 639px)">
                <source srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-t.jpg, https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-t@2x.jpg 2x" media="(min-width:640px) and (max-width:1023px)">
                <source srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-l.jpg, https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-l@2x.jpg 2x" media="(min-width:1024px) and (max-width:1349px)">
                <source srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-d.jpg, https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-d@2x.jpg 2x" media="(min-width:1350px)">
                <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-d.jpg" srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-d@2x.jpg 2x" id="image-container-e865de9f76" onload="setContainerHeight('container-e865de9f76')">
            </picture>
            
            </div>
              

         


    	

        <div id="container-7c52602917" data-cmp-is="nv-container">
    
    <div id="nv-title-3844c4db24">
    	<p>
	    	<h3>
		    	The Next Generation of Workstations Is Here
	    	</h3>
    	</p>
     </div>
<div id="nv-text-30c6aee7b5">
				<p><span>Intel and AMD CPUs, along with NVIDIA GPUs, usher in the next generation of OEM workstation platforms. These new workstations, powered by the latest Intel® Xeon® W and AMD Threadripper processors, <a href="https://www.nvidia.com/en-us/design-visualization/rtx-5880/">NVIDIA RTX 6000 Ada Generation</a> GPUs, and <a href="https://www.nvidia.com/en-us/networking/ethernet-adapters/">NVIDIA ConnectX®</a> smart network interface cards, bring unprecedented performance for creative and technical professionals.</span></p>
			</div>


    
</div>
        
    </div>
<div id="success-stories-carousel-item-514cb18dad-tabpanel" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none" role="tabpanel" aria-roledescription="slide" aria-label="Slide 2 of 2" data-cmp-slide-no="2" data-cmp-hook-carousel="item">

         
           <div>
            
            <picture data-nv-lazyload="" data-srcset-mobile="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-p.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-p@2x.jpg 2x" data-srcset-tablet="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-t.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-t@2x.jpg 2x" data-srcset-laptop="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-l.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-l@2x.jpg 2x" data-srcset-desktop="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-d.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-d@2x.jpg 2x" srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-d.jpg, https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-d@2x.jpg 2x">
                <source data-source-mobile="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(max-width: 639px)">
                <source data-source-tablet="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:640px) and (max-width:1023px)">
                <source data-source-laptop="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:1024px) and (max-width:1349px)">
                <source data-source-desktop="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:1350px)">
                <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" id="image-container-514cb18dad" onload="window.initLazyLoadingImages('container-514cb18dad');">
            </picture>
            

            
            </div>
              

         


    	

        <div id="container-5e8acd74f1" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    
<div id="nv-title-2927340625">
    	<p>
	    	<h3>
		    	NVIDIA RTX 6000 Ada Generation With Omniverse Enterprise
	    	</h3>
    	</p>
     </div>
<div id="nv-text-f5e192fe87">
				<p>NVIDIA RTX 6000 Ada Generation GPUs bundled with NVIDIA Omniverse™ Enterprise are now available, providing a turn-key, real-time collaboration solution for advanced design, visualization, and simulation projects.</p>
			</div>


    
</div>
        
    </div>

         </div>

    

        <!--
        <div class="cmp-carousel__actions">
            <button data-sly-test="true"
                    class="cmp-carousel__action cmp-carousel__action--pause"
                    type="button"
                    aria-label="Pause"
                    data-cmp-hook-carousel="pause">
                <span class="cmp-carousel__action-icon"></span>
                <span class="cmp-carousel__action-text">Pause</span>
            </button>
            <button data-sly-test="true"
                    class="cmp-carousel__action cmp-carousel__action--play cmp-carousel__action--disabled"
                    type="button"
                    aria-label="Play"
                    data-cmp-hook-carousel="play">
                <span class="cmp-carousel__action-icon"></span>
                <span class="cmp-carousel__action-text">Play</span>
            </button>
        </div>
        -->
      <ol role="tablist" aria-label="Choose a slide to display" data-cmp-hook-carousel="indicators">
            <li id="success-stories-carousel-item-e865de9f76-tab" role="tab" aria-controls="success-stories-carousel-item-e865de9f76-tabpanel" aria-label="Slide 1" data-cmp-hook-carousel="indicator">slide-1</li>
<li id="success-stories-carousel-item-514cb18dad-tab" role="tab" aria-controls="success-stories-carousel-item-514cb18dad-tabpanel" aria-label="Slide 2" data-cmp-hook-carousel="indicator">slide-2</li>

        </ol>
    </div>


    
</div>
<div id="resources" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="resources-title-1">
    	<p>
	    	<h2>
		    	Resources
	    	</h2>
    	</p>
     </div>

<div id="resources-title-2">
    	<p>
	    	<h2>
		    	Take a Deeper Dive Into NVIDIA RTX 6000
	    	</h2>
    	</p>
     </div>
<div id="resources-tabs" data-cmp-is="tabs" data-cmp-id="resources-tabs">
     <div>
          
          
    <ol role="tablist" aria-multiselectable="false">
        
        
        <li role="tab" id="resources-tabs-item-077913460b-tab" aria-controls="resources-tabs-item-077913460b-tabpanel" tabindex="0" data-cmp-hook-tabs="tab">
            <p>Videos</p>
        </li>
    
        
        
        <li role="tab" id="resources-tabs-item-edc9c5ea8b-tab" aria-controls="resources-tabs-item-edc9c5ea8b-tabpanel" tabindex="-1" data-cmp-hook-tabs="tab">
            <p>Blogs</p>
        </li>
    
        
        
        <li role="tab" id="resources-tabs-item-00331ef0f5-tab" aria-controls="resources-tabs-item-00331ef0f5-tabpanel" tabindex="-1" data-cmp-hook-tabs="tab">
            <p>Community</p>
        </li>
    </ol>

          
    </div>
    <div id="resources-tabs-item-077913460b-tabpanel" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none" role="tabpanel" aria-labelledby="resources-tabs-item-077913460b-tab" tabindex="0" data-cmp-hook-tabs="tabpanel">

         
           <div>
            
            <picture data-nv-lazyload="" data-srcset-mobile="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb770_550-p.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb580_440-p@2x.jpg 2x" data-srcset-tablet="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb770_550-t.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb580_440-t@2x.jpg 2x" data-srcset-laptop="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb770_550-l.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb580_440-l@2x.jpg 2x" data-srcset-desktop="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb770_550-d.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb580_440-d@2x.jpg 2x" srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb770_550-d.jpg, https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb580_440-d@2x.jpg 2x">
                <source data-source-mobile="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(max-width: 639px)">
                <source data-source-tablet="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:640px) and (max-width:1023px)">
                <source data-source-laptop="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:1024px) and (max-width:1349px)">
                <source data-source-desktop="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:1350px)">
                <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" id="image-tab-slide-1" onload="window.initLazyLoadingImages('tab-slide-1');">
            </picture>
            

            
            </div>
              

         


    	

        <div id="videos-slide-container" data-cmp-is="nv-container">
    
    <div id="container-1dc4debc02" data-cmp-is="nv-container">
    
    

<div id="nv-title-b4a71013c1">
    	<p>
	    	<h3>
		    	Accurately Reflect Real-World Conditions With Ease
	    	</h3>
    	</p>
     </div>
<div id="nv-text-f6de73eef0">
				<p><span>GPU-accelerated aerodynamic simulation enhanced with NVIDIA RTX 6000 takes product development to the next level. The RTX 6000 enables you to design, simulate, and optimize products–using the most powerful 3D design and simulation tools–faster and more interactively than ever before.</span></p>
			</div>



    
</div>

<div id="container-be8a1d3536" data-cmp-is="nv-container">
	        
	        <div id="nv-teaser-5b71d9b0bc" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Envision Endless Possibilities with NVIDIA RTX
    </h3>

        
    <p>Unlock endless possibilities with NVIDIA RTX to create stunning visuals that can revolutionize workflows for professionals. See how AI breakthroughs in design, engineering, and simulation bring your vision to life faster with photorealistic detail. Experience the power of NVIDIA RTX and redefine what comes next.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-21b16a8db4" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        3D Volumetric Time-Lapse with Forager
    </h3>

        
    <p>To bring the forest to life, Forager developed MycoMachine, the first time-lapse photogrammetry rig to capture a library of mushrooms growing over time and play them back as hyperrealistic real-time animations. The NVIDIA RTX 6000 Ada helps Forager tackle the computationally intense challenges presented during reconstruction, clean up, and real-time playback.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-15161af4d7" data-title-style="dynamic" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Live Graphics Production with Vizrt
    </h3>

        
    <p>Gerhard Lang, CTO of Vizrt Group talks about how Viz Engine 5 uses NVIDIA RTX 6000 Ada’s latest generation RT Cores, Tensor Cores, and DLSS to track talent, upscale footage, and create the necessary reflections and shadows to ground a subject inside a virtual environment.</p>

        
	
    
    
    
    

    </div>

	        
        </div>

    
</div>
        
    </div>
<div id="resources-tabs-item-edc9c5ea8b-tabpanel" data-cmp-is="nv-container" role="tabpanel" aria-labelledby="resources-tabs-item-edc9c5ea8b-tab" tabindex="0" data-cmp-hook-tabs="tabpanel">
	        
	        <div id="nv-teaser-39b150a72c" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        A Perfect Pair: Adidas and Covision Media Use AI, NVIDIA RTX to Create Photorealistic 3D Content
    </h3>

        
    <p>Covision’s AI-based 3D technology helps businesses scan thousands of products, creating photorealistic 3D images, videos, and AR experiences for websites and mobile apps.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-25c754bc8a" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Design Speed Takes the Lead: Trek Bicycle Competes in Tour de France With Bikes Developed Using NVIDIA GPUs
    </h3>

        
    <p>The team uses RTX technology to accelerate product design, iterate more quickly, and run realistic computational fluid dynamics simulations to build world-class bicycles.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-e01797e367" data-title-style="dynamic" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        3D Artist Brings Ride and Joy to Automotive Designs With Real-Time Renders Using NVIDIA RTX
    </h3>

        
    <p>David Baylis uses the powerful ray-tracing features and large GPU memory of the NVIDIA RTX A6000 to create stunning visuals with sharp details, realistic lighting, and bouncing reflections.</p>

        
	
    
    
    
    

    </div>

	        
        </div>
<div id="resources-tabs-item-00331ef0f5-tabpanel" data-cmp-is="nv-container" role="tabpanel" aria-labelledby="resources-tabs-item-00331ef0f5-tab" tabindex="0" data-cmp-hook-tabs="tabpanel">
	        
	        <div id="nv-teaser-727cf96136" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Building Realistic Renders and 4D Sequences of Construction Projects With NVIDIA RTX
    </h3>

        
    <p>Explore how Layton Construction uses NVIDIA RTX 6000 Ada Generation GPUs to enhance visualization workflows, creating photo-realistic renders and animations that can capture every stage of a construction project.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-943186efba" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        NVIDIA RTX Ambassador David Baylis - Podcast With Allan McKay
    </h3>

        
    <p>Listen to RTX Ambassador David Baylis share his incredible insights on innovations in ray tracing and AI, the future of VFX careers in the era of machine learning, how to get started in Unreal, and why learning 3D is a lifelong occupation.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-6b40d02045" data-title-style="dynamic" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Sony Pictures Animation Bringing 2D and 3D Worlds Together With NVIDIA Omniverse Enterprise
    </h3>

        
    <p>Built on <a href="https://www.nvidia.com/en-us/omniverse/enterprise/">NVIDIA Omniverse Enterprise</a> and powered by NVIDIA RTX, Sony Pictures Entertainment developed FlixiVerse, allowing 2D artists to effortlessly step into the world of 3D. Learn how they are accelerating creative workflows with more iterations and easier transitions into 3D environments.</p>

        
	
    
    
    
    

    </div>

	        
        </div>

    
</div>


    
</div>
<div id="get-started" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-33e67b86ec">
    	<p>
	    	<h2>
		    	Get Started
	    	</h2>
    	</p>
     </div>
<div id="container-afe5a0d17e" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-teaser-16d96ca65a" data-title-style="manual">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Ready to Purchase?
    </h3>

        
    <p><span>Talk with an NVIDIA design and visualization partner.</span></p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-b7f6a809cd" data-title-style="manual">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Need Help Selecting the Right Product or Partner?
    </h3>

        
    <p>Talk to an NVIDIA product specialist about your professional needs.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-91aee16bfa" data-title-style="manual">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Get the Latest on NVIDIA RTX
    </h3>

        
    <p>Sign up for the latest news, updates, and more from NVIDIA.</p>

        
	
    
    
    
    

    </div>

    
</div>

    
</div>
<div data-form-page-path="/content/forms/af/nvidia-forms/industries/power-and-utilities-email-subscribe-form" id="notify-me">
		<p id="aem-form-title"> <h2><span>Sign Up To Be Notified On Availability</span></h2>

		</p>
		
		
		
		
		
    




		
		
		







    
        
        
    
    



































    








    
    
        
        
        
        
            
            
            
                




            
            


        
            
            
            
                







    
        
        
    
    







    
    
        

    


            
            
    










    




		
    




	</div>
<div id="specs">
    	<p>
	    	<h2>
		    	NVIDIA RTX 5880 Quick Specs
	    	</h2>
    	</p>
     </div>

    
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Teachable Machine (215 pts)]]></title>
            <link>https://teachablemachine.withgoogle.com/</link>
            <guid>38898104</guid>
            <pubDate>Sun, 07 Jan 2024 03:20:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://teachablemachine.withgoogle.com/">https://teachablemachine.withgoogle.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38898104">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[US unemployment has been under 4% for the longest streak since the Vietnam War (102 pts)]]></title>
            <link>https://www.npr.org/2024/01/05/1222714145/jobs-report-december-labor-wages</link>
            <guid>38897475</guid>
            <pubDate>Sun, 07 Jan 2024 01:33:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npr.org/2024/01/05/1222714145/jobs-report-december-labor-wages">https://www.npr.org/2024/01/05/1222714145/jobs-report-december-labor-wages</a>, See on <a href="https://news.ycombinator.com/item?id=38897475">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storytext">
      <div id="res1222714859">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s400-c85.webp 400w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s600-c85.webp 600w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s800-c85.webp 800w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s900-c85.webp 900w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1200-c85.webp 1200w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1800-c85.webp 1800w" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s600-c85.jpg 600w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s900-c85.jpg 900w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1200-c85.jpg 1200w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1800-c85.jpg 1800w" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                The U.S. job market held up well in 2023, despite rising interest rates. Employers added 2.7 million jobs last year and unemployment remained under 4% throughout the year.
                <b aria-label="Image credit">
                    
                    Mario Tama/Getty Images
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Mario Tama/Getty Images
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1200.jpg">
        </picture>
    </div>
<div>
        <p>The U.S. job market held up well in 2023, despite rising interest rates. Employers added 2.7 million jobs last year and unemployment remained under 4% throughout the year.</p>
        <p><span aria-label="Image credit">
            
            Mario Tama/Getty Images
            
        </span>
    </p></div>
   </div>
   <p>The U.S. job market capped off a strong year in December, as employers continued hiring at a solid pace.</p>   <p>Employers added 216,000 jobs last month, according to the Labor Department. The unemployment rate held steady at 3.7%.</p>   <p>Unemployment has now been under 4% for almost two years — the longest streak of rock-bottom jobless rates since the Vietnam War. </p>   <p>"The labor market ended 2023 on a solid footing," said Nela Richardson, chief economist for the payroll processing company ADP. "We'll see what 2024 will bring."  </p>   <p>December's job gains were concentrated in government and health care. Retailers added 17,000 jobs, suggesting a solid finish to the holiday shopping season. </p>   
   
   
<!-- END ID="RES1223052238" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <h3>Job growth has been resilient despite Fed's brutal interest rate increases</h3>   <p>For all of 2023, employers added 2.7 million jobs. That's a slowdown from the two previous years, when the economy was red-hot, rapidly rebounding from pandemic layoffs. But last year's job growth was still stronger than every other year since 2015.</p>   <p>The job market has proven to be resilient despite the Federal Reserve's aggressive push to combat inflation with higher interest rates. Even sensitive industries where the cost of borrowing is elevated continued to add jobs last year. Construction companies added 17,000 jobs in December. </p>   
   
<!-- END ID="RES1223052384" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>Nancy McNamara completed a building trades internship in October and quickly secured a job with a busy weatherization contractor in Rutland, Vt.</p>   <p>"I feel like every time we're at a job site, he's getting a call from someone else," McNamara said. "He's booked right up through — I don't even know when."</p>   <p>McNamara is eager to learn new construction skills and has gotten training offers from a carpenter and a drywall contractor. </p>   <p>"I like being tired at the end of the day and feeling like I accomplished something," she said. "With work like this, that's exactly how I feel."</p>   
   
<!-- END ID="RES1223051787" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <h3>Hotels, restaurants still hasn't recovered to pre-pandemic levels</h3>   <p>The leisure and hospitality sector — which includes restaurants and hotels — added 40,000 jobs last month but overall employment in the sector still hasn't quite recovered to pre-pandemic levels.</p>   <p>Government employment was also slow to bounce back from the pandemic, but strong government hiring in 2023 finally closed that gap. </p>   <p>Wages are rising, but not as fast as they were earlier in the year. Average wages in December were up 4.1% from a year ago. Slower wage growth puts less upward pressure on prices, which should be reassuring to inflation watchdogs at the Fed. </p>   
   <p>"There's very little risk of a wage-price spiral that will push up inflation in 2024," Richardson said. </p>   <p>The good news for workers is that wages have been climbing faster than prices in recent months, so the average paycheck stretches further. </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Carta CEO's response to the unsolicited outreach to their customers' investors (181 pts)]]></title>
            <link>https://twitter.com/henrysward/status/1743794996732735679</link>
            <guid>38897363</guid>
            <pubDate>Sun, 07 Jan 2024 01:15:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/henrysward/status/1743794996732735679">https://twitter.com/henrysward/status/1743794996732735679</a>, See on <a href="https://news.ycombinator.com/item?id=38897363">Hacker News</a></p>
Couldn't get https://twitter.com/henrysward/status/1743794996732735679: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Original Age of Empires 2 dev talks about its usage of assembly code (223 pts)]]></title>
            <link>https://old.reddit.com/r/aoe2/comments/18ysttu/aoe_is_written_in_assembly_is_this_actually_true_o/</link>
            <guid>38896896</guid>
            <pubDate>Sat, 06 Jan 2024 23:59:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/aoe2/comments/18ysttu/aoe_is_written_in_assembly_is_this_actually_true_o/">https://old.reddit.com/r/aoe2/comments/18ysttu/aoe_is_written_in_assembly_is_this_actually_true_o/</a>, See on <a href="https://news.ycombinator.com/item?id=38896896">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>[–]<a href="https://old.reddit.com/user/ES_MattP">ES_MattP</a><span title="ES/FE">ES/FE</span><span></span> <span title="127">127 points</span><span title="128">128 points</span><span title="129">129 points</span> <time title="Sat Jan 6 17:16:52 2024 UTC" datetime="2024-01-06T17:16:52+00:00">13 hours ago</time><time title="last edited 6 hours ago" datetime="2024-01-07T00:56:45+00:00">*</time>&nbsp;(6 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_kgltqrg21n"><div><p>I guess I can clarify this, since I wrote all the assembly code used in AoE / Aok, along with many other parts of those games.</p>

<p>There were about ~13,000 lines of x86 32-bit assembly code written in total.</p>

<p>The vast majority, about ~11,500 lines worth, was in the 'drawing core', which drew SLP sprites in a variety of ways (mirrored, stippled, clipped).  The code for this was in a separate .asm file, which was "compiled" by Microsoft Macro Assembler 6.1 into a .obj file.</p>

<p>There were maybe a dozen other uses of assembly in the C++ code, as the compilers at the time supported 'inline assembly' in the middle of functions.  These blocks of asm code were added using the <strong>__asm {  }</strong> keyword.  Microsoft Visual C++ 4.2 to 6.0 compilers were the ones used to compile the games as originally shipped.    The inline assembly in other functions was generally used to speed up operations like scanning areas of the map for targeting and accessing compressed look up tables, which had bit-level operations that could be reduced in assembly, but c++ couldn't express as well.</p>

<p>The use of assembly in the drawing core resulting in a ~10x sprite drawing speed improvement over the C++ reference implementations, and AoE's drawing core was notably faster than competitors like StarCraft, which is why the default resolution 'out of the box' for AoE was 800 by 600, when nearly all our competition was 640 x 480 resolution - we could scroll the screen and fill it with sprites as fast or faster even though we had twice as many pixels-ish in the game world area. </p>

<p>The design of the drawing core used a number of techniques that could not easily (or at all) be expressed in C++ and took into account things like the cache architecture and UV pipes of the first Pentium processors.  A key speedup technique AoE used was realized during discussions I had with iD software programmer and optimization guru Michael Abrash over lunch at Tia's Mexican Restaurant in Mesquite, TX.   Other speedups came from being able eliminating function call overhead and hand-managing register usage.  "register starvation" was a real issue for CPUs in the pre-Pentium Pro era and languages like C++ were hurt by it.</p>

<p>Additional assembly code was added in Age of Kings to add the clipped outlines of obscured units.</p>

<p>The assembly code remained in use in AoK:HD edition (a 32-bit game)</p>

<p>I re-wrote the assembly functions into C++ for both Definitive Editions, as they are 64-bit programs, and inline assembly was never supported by the 64-bit C++ compiler, and the vastly improved register sets and compiler optimizations made it un-necessary.   Additionally, sprite drawing in the definitive editions is multi-threaded, and will use up to 4 cores for that task alone.</p>

<p>I hope that clears things up.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/aoe2/comments/18ysttu/aoe_is_written_in_assembly_is_this_actually_true_o/kgltqrg/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li>report</li><li>reply</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On building a semantic search engine (160 pts)]]></title>
            <link>https://vickiboykis.com/2024/01/05/retro-on-viberary/</link>
            <guid>38896879</guid>
            <pubDate>Sat, 06 Jan 2024 23:57:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vickiboykis.com/2024/01/05/retro-on-viberary/">https://vickiboykis.com/2024/01/05/retro-on-viberary/</a>, See on <a href="https://news.ycombinator.com/item?id=38896879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Viberary is a side project that I worked on in 2023, which does semantic search for books by vibe. It was hosted at [viberary.pizza.]</p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/0c8fddcb-3bb1-4b67-8d0e-7b2462c6d561" width="600"></figure><p>I’m shutting down the running app and putting the codebase in maintenance mode because:</p><ul><li>A lot of what I want to continue to do there (i.e. changing embedding models, modifying training data) involves building out more complex infra: a model store, a feature store, data management, evaluation infra, and all of that’s going to take longer than I have</li><li>There’s a lot of maintenance that needs to happen for a running app (Python dependencies, etc. ) I.e. <a href="https://blog.professorbeekums.com/all-code-is-debt/">all code is technical debt.</a></li><li>Cost! I don’t want to maintain an app that is currently losing $100+ a month to maintenance costs unless I’m also planning to make money from it. I’m not planning to, but I have learned a LOT from this project and I have loved building and sharing it.</li><li>I have a new project idea I’d like to work on, so I need to make space for it.</li></ul><p>There were SO many, SO many things I learned from this project. Most of them are outlined in the post below, so read on. But, if you want a list of high-level bullets:</p><ul><li>The project HAS to be something you’re interested in. You will not work on it otherwise. I love books and I want to be recommended books, and I had a keen understanding of this problem space before I started.</li><li>Start as simple as you can, but no simpler. You should be able to test anything you deploy locally without external dependencies. You need to be able to go fast at the beginning, otherwise you’ll lose interest.</li><li>At the same time, you will not know what’s simple unless you try something, anything.</li><li>Simple means most of the code you write should be your library logic, <a href="https://vickiboykis.com/2022/12/05/the-cloudy-layers-of-modern-day-programming/">not glue code between cloud components.</a></li><li>Docker on new Mac M1+ architectures that have to be ported to Linux is really annoying but fixable.</li><li>Knowing <a href="https://newsletter.vickiboykis.com/archive/when-you-write-a-web-server-but-you-get-served/">nginx</a> well can save you a ton of time</li><li>Sometimes you don’t need large language models, BERT works just fine</li><li>Evaluating the results of unsupervised ranking and retrieval is really hard and no one has solved this problem yet</li><li>Digital Ocean has an amazing product suite that just works for small and medium-size projects</li><li>The satisfaction of shipping products that you’ve build is unparalleled</li></ul><p>For much, much more, read on!</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/learningtired.png" width="600"></figure><h3 id="august-5-2023">August 5, 2023</h3><p><em>TL;DR</em>: Viberary is a side project that I created to find books by <strong>vibe</strong>. I built it to satisfy an itch to do <a href="https://vickiboykis.com/2020/06/09/getting-machine-learning-to-production/">ML side projects</a> and navigate the current boundary between search and recommendations. It’s a production-grade complement to <a href="http://vickiboykis.com/what_are_embeddings/">my recent deep dive into embeddings.</a></p><p>This project is a lot of fun, but conclusively proves to me what I’ve known all along about myself: reaching MLE (machine learning enlightenment) is the cyclical process of working through modeling, engineering,and UI concerns, and connecting everything together - <a href="https://vickiboykis.com/2021/09/23/reaching-mle-machine-learning-enlightenment/">the system in production is the reward.</a>
And, like any production-grade system, machine learning is not magic. Even if the data outputs are not deterministic, it takes thoughtful engineering and design
choices to build any system like this, something that I think gets overlooked these days in the ML community.</p><p>I hope with this write-up to not only remind myself of what I did, but outline what it takes to build a production Transformer-based machine learning application, even a small one with a pre-trained model, and hope it serves as a resource and reference point.</p><hr><p>Viberary’s machine learning architecture is a <a href="https://blog.reachsumit.com/posts/2023/03/two-tower-model/">two-tower</a> semantic retrieval model that encodes the user search query and the Goodreads book corpus using the
<a href="https://www.sbert.net/docs/pretrained-models/msmarco-v3.html">Sentence Transformers pretrained asymmetric MSMarco Model</a>.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/viberary_arch.png" width="600"></figure><p>The training data is generated locally by <a href="https://github.com/veekaybee/viberary/blob/main/src/model/generate_training_data.py">proessing JSON in DuckDB</a> and the model is converted to ONNX for performant inference, with <a href="https://github.com/veekaybee/viberary/blob/main/src/model/generate_embeddings.ipynb">corpus embeddings learned on AWS P3 instances</a> against the same model and stored in Redis. Retrieval happens using the <a href="https://redis.io/docs/interact/search-and-query/">Redis Search</a> set with the <a href="https://arxiv.org/abs/1603.09320">HNSW algorithm</a> to search on cosine similarity. Results are served through a Flask API running four <a href="https://gunicorn.org/">Gunicorn</a> workers and served to a <a href="https://getbootstrap.com/">Bootstrap front-end.</a> using Flask’s ability to statically reder <a href="https://jinja.palletsprojects.com/en/3.1.x/">Jinja templates</a>. There is no Javascript dependencies internal to the project.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/tactical_app.png" width="600"></figure><p>It’s served from two <a href="https://www.digitalocean.com/products/droplets">Digital Ocean droplets</a> behind a <a href="https://www.digitalocean.com/products/load-balancer">Digital Ocean load balancer</a> and <a href="https://vicki.substack.com/p/when-you-write-a-web-server-but-you">Nginx</a>, as a Dockerized application with networking spun up through Docker compose between the web server and Redis Docker image, with data persisted to <a href="https://docs.digitalocean.com/products/volumes/">external volumes in DigitalOcean</a>, with [Digital Ocean] serving as the domain registrar and load balancer router.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/physical_arch_2.png" width="600"></figure><p>The deployable code artifact is generated through <a href="https://github.com/veekaybee/viberary/tree/main/.github/workflows">GitHub actions</a> on the main branch of the repo and then I manually refresh the docker image on the droplets through a set of Makefile commands. This all works fairly well at this scale for now.</p><h2 id="what-is-semantic-search">What is semantic search?</h2><hr><p>Viberary is a semantic search engine for books. It finds books based on ✨vibe✨. This is in contrast to traditional search engines, which work by performing lexical keyword
matching on terms like exact
keyword matches by genre, author, and title - as an example, if you type in “Nutella” into the search engine, it will try to find all documents that specifically have the word “Nutella” in the document.</p><p>Traditional search engines, including Elasticsearch/OpenSearch do this lookup efficiently by building <a href="https://en.wikipedia.org/wiki/Inverted_index">an inverted
index</a>, a data structure that creates a
key/value pair where the key is the term and the value is a collection of all the documents that match the term and performing retrieval from the inverted index. Retrieval performance from an inverted index can vary depending on how it’s implemented, but it is <code>O(1)</code> in the best case, making it an efficient data structure.</p><p>A commonc classic retrieval method from an inverted index is <a href="https://en.wikipedia.org/wiki/Okapi_BM25">BM25</a>, which is based on <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a> and calculates a relevance score for each element in the inverted index. The retrieval mechanism first selects all the documents with the keyword from the index, the calculates a relevance score, then ranks the documents based on the relevance score.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/inverted_index.png" width="400"></figure><p>Semantic search, in contrast, looks for near-meanings based on, as <a href="https://www.manning.com/books/ai-powered-search">“AI-Powered Search”</a> calls it, “things, not strings.” <a href="https://www.manning.com/books/relevant-search">In other words,</a></p><p>“Wouldn’t it be nice if you could search for a term like “dog” and pull back documents that contain terms like “poodle, terrier, and beagle,” even if those document happen to not use the word “dog?”</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/semantic_search.png" width="400"></figure><p>Semantic search is a vibe. A vibe can be hard to define, but generally it’s more of a feeling of association
than something concrete: a mood, a color, or a phrase. Viberary will not give you exact matches for “Nutella”, but if you type in “chocolately hazlenut goodness”, the expectation is that you’d get back Nutella, and probably also “cake” and “Ferrerro Rocher”.</p><p>Typically today, search engines will implement a number of both keyword-based and semantic approaches in a solution known as hybrid search. Semantic search includes methods like learning to rank, blending several retrieval models, query expansion which looks to enhance search results by adding synonyms to the original query, contextual search based on the user’s history and location, and vector similarity search, which looks to use NLP to help project the user’s query in a vector space.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/search_tree.png" width="600"></figure><p>The problem of semantic search is one researchers and companies have been grappling with for decades in the field known as information retrieval, which started with roots in library science. <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/334.pdf">The paper introducing Google in 1998</a> even discusses the problems with keyword-only search,</p><p>Netflix was one of the first companies that started doing vibe-based content exploration when it <a href="https://www.netflix.com/tudum/articles/netflix-secret-codes-guide">came up with a list of over 36,000
genres</a> like “Gentle British
Reality TV” and “WitchCraft and the Dark Arts” in the 2010s. They <a href="https://www.theatlantic.com/technology/archive/2014/01/how-netflix-reverse-engineered-hollywood/282679/">used large teams of people</a> to watch
movies and tag them with metadata. The process was so detailed that taggers received a 36-page document that “taught them how to rate movies on their sexually suggestive content, goriness, romance levels, and even narrative elements like plot conclusiveness.”</p><p>These labels were then incorporated into Netflix’s <a href="https://netflixtechblog.com/system-architectures-for-personalization-and-recommendation-e081aa94b5d8">recommendation architectures</a> as features for training data.</p><p>It can be easier to incorporate these kinds of features into recommendations than search because the process of recommendation is the process of implicitly learning user preferences through data about the user and offering them suggestions of content or items to purchase based on their past history, as well as the history of users across the site, or based on the properties of the content itself. As such, <a href="https://www.nngroup.com/articles/recommendation-guidelines/">recommender interfaces often include lists of suggestions</a> like <code>"you might like.."</code> or <code>"recommended for you"</code>, or <code>"because you interacted with X.."</code></p><p>Search, on the other hand, is an activity where the user expects their query to match results exactly, so users have specific expectations of modern search interfaces:</p><ol><li>They are <a href="http://glinden.blogspot.com/2006/11/marissa-mayer-at-web-20.html">extremely responsive and low-latency</a></li><li>Results are accurate and we get what we need in the first page</li><li>We use text boxes the same way <a href="https://arxiv.org/pdf/2301.08613.pdf">we have been conditioned</a> to use Google Search over the past 30 years in the SERP (search engine results page)</li></ol><p>As a result, in some ways, there is a tension between what makes traditional search interface and semantic search successful respectively, because semantic search is in that gray area between search and recommendations and traditional search expects exact results for exact queries. These are important aspects to keep in mind when designing conversational or semantic search interfaces. For more on this, <a href="https://www.theverge.com/2023/5/20/23731397/neeva-search-engine-google-shutdown">check out this recent article on Neeva.</a></p><p>Many search engines today, Google included, use a blend of traditional keyword search and semantic search to offer both direct results and related content, and with the explosion of generative AI and chat-based search and recommendation interfaces, this <a href="https://docs.google.com/presentation/d/12aoYVaqus600NEuWASw_eF9xSDXGUMzGedAftfqBCCE/edit">division is becoming even blurrier.</a></p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/searchandrec.png" width="600"></figure><h2 id="why-semantically-search-books">Why semantically search books?</h2><hr><p>I love reading, particularly fiction. I am always reading something. Check out my past reviews
<a href="https://vickiboykis.com/essays/2022-01-02-favorite-books/">2021</a>,
<a href="https://vickiboykis.com/essays/2021-04-16-favorite-books/">2020</a>, <a href="https://vickiboykis.com/essays/2020-01-01-books/">2019</a>,
and you get the idea. As a reader, I am always looking for something good to read. Often, I’ll get
recommendations by browsing sites like <a href="https://lithub.com/">LitHub</a>, but sometimes I’m in the mood for a particular
genre, or, more specifically a feeling that a book can capture. For example, after finishing <a href="https://www.richardpowers.net/the-overstory/">“The Overstory” by Richard Powers</a>, I was in the mood for more sprawling multi-generational epics
on arcane topics (I know so much about trees now!)</p><p>But you can’t find curated, quality collections of recommendations like this unless a human who reads a lot puts a list like this together. One of my favorite formats of book recommendations <a href="https://themorningnews.org/article/greetings-from-the-biblioracle">is
Biblioracle</a>, where readers
send John Warner, an extremely well-read novelist, a list of the last five books they’ve read and he recommends their next read
based on their reading preferences.</p><p>Given the recent rise in interest of semantic search and vector databases, as well as <a href="http://vickiboykis.com/what_are_embeddings/">the paper I just finished on embeddings</a>, I thought it would interesting if I could create a book search engine that gets at least somewhat close to what book nerd recommending humans can provide out of the box.</p><p>I started out by formulating the machine learning task as a recommendation problem: given that you know something about either a user or the item, can you generate a list of similar items that other users like the user has liked? We can either do this through collaborative filtering, which looks at previous user-item interactions, or content filtering, which looks purely at metadata of the items and returns similar items. Given that I have no desire to get deep into user data collection, with the exception of search queries and search query result lists, which I currently do log to see if I can fine-tune the model or offer suggestions at query time, collaborative filtering was off the table from the start.</p><p>Content-based filtering, i.e. looking at a book’s metadata rather than particular actions around a piece of content, would work well here for books. However, for content-based filtering, we also need information about the user’s preferences, which, again, I’m not storing.</p><p>What I realized is that the user would have to provide the query context to seed the recommendations, and that we don’t know anything about the user. At this point, based <a href="https://md.ekstrandom.net/blog/2015/10/search-and-recsys">on this heuristic,</a> it starts to become a search problem.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/searchrecscontext.png" width="600"></figure><p>An additional consideration was that recommendation surfaces are also traditionally rows of cards or lists that are loaded when the user is logged in, something that I don’t also don’t have and don’t want to implement from the front-end perspective. I’d like the user to be able to enter their own search query.</p><p>This idea eventually evolved into the thinking that, given my project constraints and preferences, what I had was really a semantic search problem aimed specifically at a non-personalized way of surfacing books.</p><p>After a <a href="https://vickiboykis.com/2022/11/10/how-i-learn-machine-learning/">literature search,</a>, what I found was <a href="https://arxiv.org/pdf/2006.02282.pdf">a great paper</a> that formulates the exact problem I wanted to solve, only in an ecommerce setting.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/dpsr.png" width="600"></figure><p>Their problem was more complicated in that, in addition to semantic search they also had to personalize it, and they also had to learn a model from scratch based on the data that they had, but the architecture was one that I could follow in my project, and the simplified online serving half was what I would be implementing.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/dpsr2.png" width="600"></figure><h2 id="architecting-semantic-search">Architecting Semantic Search</h2><hr><p>There are several stages to building semantic search that are related to some of the stages in <a href="https://medium.com/nvidia-merlin/recommender-systems-not-just-recommender-models-485c161c755e">a traditional four-stage recommender system</a>:</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/model_steps.png" width="600"></figure><ol><li>Data Collection</li><li>Modeling and generating embeddings</li><li>Indexing the embeddings</li><li>Model Inference, inclduing filtering</li></ol><p>and a fifth stage that’s often not included in search/recsys architectures but that’s just as important, Search/Conversational UX design.</p><p>Most <a href="https://eugeneyan.com/writing/system-design-for-discovery/">search and recommendation architectures</a> share a foundational set of commonalities that we’ve been developing for years. It’s interesting to note that <a href="https://dl.acm.org/doi/pdf/10.1145/138859.138867">Tapestry</a>, one of the first industrial recommender systems created in the 1990s to collaboratively filter emails, has an extremely similar structure to any search and recommendation system today, including components for indexing and filtering.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/tapestry.png" width="600"></figure><p>We start by collecting and processing a large set of documents. Our goal in information retrieval is to find the documents that are relevant to us, for any given definition of relevant. We update these collections of documents to be searchable at scale via an indexing function. We select a candidate set of relevant documents through either heuristics or machine learning. In our case, we do it by finding compressed numerical representations of text that are similar to the ones that we type into the query box. We generate these representations using an embedding space that’s created with deep learning models in the transformer family.</p><p>Then, once we find a candidate list of ~50 items that are potentially relevant to the query, we filter them and finally rank them, presenting them to the user through a front-end.</p><p>There are a number of related concerns that are not at all in this list but which make up the heart of machine learning projects: iteration on clean data, evaluation metrics both for online and offline testing, monitoring model performance in production over time, keeping track of model artifacts in model stores, exploratory data analysis, creating business logic for filtering rules, user testing, and much, much more. In the interest of time, I decided to forgo some of these steps as long as they made sense for the project.</p><h2 id="project-architecture-decisions">Project Architecture Decisions</h2><hr><p>Given this architecture and my time constraints, I constrained myself in several ways on this project. First, I wanted to a project that was well-scoped and had a UI component so that I was incentivized to ship it, because the worst ML project is the one that remains unshipped. <a href="https://mitchellh.com/writing/building-large-technical-projects">As Mitch writes</a>, you have an incentive to move forward if you have something tangible to show to yourself and others.</p><p>Second, I wanted to explore new technologies while also being careful of not wasting <a href="https://mcfunley.com/choose-boring-technology">my innovation tokens</a>. In other words, I wanted to <a href="https://normconf.com/">build something normcore</a>, i.e. using the right tool for the right job, and <a href="https://vicki.substack.com/p/you-dont-need-kafka">not going overboard.</a>. I wasn’t going to start with LLMs or Kubernetes or Flink or MLOps. I was going to start by writing simple Python classes and adding where I needed to as pain points became evident.</p><p>The third factor was to try to ignore <a href="https://vickiboykis.com/2022/11/10/how-i-learn-machine-learning/">the hype blast of the current ML ecsystem</a>, which comes out with a new model and a new product and a new wrapper for the model for the product every day. It wasn’t easy. It is extremely hard to ignore the noise and just build, particularly given all the discourse around LLMs and now in society at large.</p><p>Finally, I wanted to build everything as a traditional self-contained app with various components that were <a href="https://vickiboykis.com/2023/06/29/naming-things/">easy to understand</a>, and reusable components across the app. The architecture as it stands looks like this:</p><p>I wish I could say that I planned all of this out in advance, and the project that I eventually shipped was exactly what I had envisioned. But, like with any engineering effort, I had a bunch of false starts and dead ends. I started out <a href="https://vickiboykis.com/2022/12/05/the-cloudy-layers-of-modern-day-programming/">using Big Cloud</a>, a strategic mistake that cost me a lot of time and frustration because I couldn’t easily introspect the cloud components. This slowed down development cycles. I eventually moved to local data processing using DuckDB, but <a href="https://vickiboykis.com/2023/01/17/welcome-to-the-jungle-we-got-fun-and-frames/">it still look a long time to make this change and get to data understanding</a>, as is typically the case in any data-centric project.</p><p>Then, I spent a long time <a href="https://github.com/veekaybee/viberary/releases/tag/v0.0.1">working through creating baseline models in Word2Vec</a> so I could get some context for baseline text retrieval methods in the pre-Transformer era. Finally, in going from local development to production, I hit <a href="https://vickiboykis.com/2023/07/18/what-we-dont-talk-about-when-we-talk-about-building-ai-apps/">a bunch of different snags</a>, most of them related to making Docker images smaller, thinking about the size of the machine I’d need for infrence, Docker networking, load testing traffic, and, a long time on correctly routing Nginx behind a load balancer.</p><p>Generally, though, I’m really happy with this project, <a href="https://normconf.com/">guided by the spirit of Normconf</a> and all the great normcore ML engineering ideas <a href="https://vickiboykis.com/2022/12/22/everything-i-learned-about-accidentally-running-a-successful-tech-conference/">I both put in and took away from</a> people in the field looking to build practical solutions.</p><h2 id="tech-stack">Tech Stack</h2><hr><p>My project tech stack, as it now stands is primarily Python developed in <a href="https://gifted-bohr-74bf66.netlify.app/">virtual environments</a> with <code>requirements.txt</code> with:</p><ul><li>Original data in <strong>gzipped JSON</strong> files hosted locally not under version control</li><li>These files are rrocessed using the Python client for <strong>DuckDB</strong></li><li>Encoding of documents into model embeddings with <strong>SBERT</strong>, <a href="https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search">specifically the MS-Marco Asymmetric model</a></li><li>A <strong>Redis</strong> instance that indexes the embeddings into a special search index for retrieval</li><li>A <strong>Flask</strong> API that has a search query route that encodes the query with the same MSMarco model and then runs <strong>HNSW</strong> lookup in realtime against the Redis search index</li><li>A <strong>Bootstrap UI</strong> that returns the top 10 ranked results</li><li>Redis and Flask encapsulated in a networked <strong>docker compose</strong> configuration via <strong>Dockerfile</strong>, depending on the architecture (arm or AMD)</li><li>a <strong>Makefile</strong> that does a bunch of routine things around the app like reindexing the embeddigns and bringing up the app</li><li><strong>Nginx</strong> on the hosting server to reverse-proxy requests from the load balancer</li><li><strong>pre-commit</strong> for formatting and linting</li><li><strong>Locust</strong> for load testing</li><li>a logging module for capturing queries and outputs</li><li>and tests in <strong>pytest</strong></li></ul><hr><ul><li>PyCharm for development, <a href="https://www.jetbrains.com/help/pycharm/docker.html">including in Docker via bind mounts</a></li><li>iterm2</li><li>VSCode for specifically writing the documentation, it’s nicer than PyCharm for this</li><li><a href="https://whimsical.com/">Whimsical for charts</a></li><li>Docker Desktop for Mac (considered briefly switching to Podman but haven’t yet)</li></ul><h2 id="training-data">Training Data</h2><hr><p>The original book data comes from <a href="https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/books">UCSD Book Graph</a>, which scraped it from Goodreads for research papers in 2017-2019.</p><p>The data is stored in several gzipped-JSON files:</p><ul><li><a href="https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/books">books</a> - detailed meta-data about 2.36M books</li><li><a href="https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/reviews?authuser=0">reviews</a> - Complete 15.7m reviews (~5g):15M records with detailed review text</li></ul><p>Sample row: Note these are all encoded as strings!</p><p>There is a lot of good stuff in this data! So, like any good data scientist, I initially <a href="https://github.com/veekaybee/viberary/blob/main/src/notebooks/03_duckdb_eda.ipynb">did some data exploration</a> to get a feel for the data I had at hand. I wanted to know how full the dataset was, how many missing data I had, what language most of the reviews are in, and other things that will help understand what the model’s embedding space looks like.</p><p>The data input generally looks like this:</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/input_data.png" width="600"></figure><p>Then, I constructed several tables that I’d need to send to the embeddings model to generate embeddings for the text. I did this all in DuckDB. The final relationships between the tables look like this:</p><p>The <code>sentence</code> column which concatenates <code>review_text || goodreads_auth_ids.title || goodreads_auth_ids.description</code> is the most important because it’s this one that is used as a representation of the document to the embedding model and the one we use to generate numerical representations and look up similarity between the input vector.</p><p>There are a couple of things to note about the data. First, it’s from 2019 so the recency on the recommendations from the data won’t be great, but it should do fairly well on classical books. Second, since <a href="https://debugger.medium.com/goodreads-is-retiring-its-current-api-and-book-loving-developers-arent-happy-11ed764dd95">Goodreads no longer has an API</a>, it’s impossible to get this updated in any kind of reasonable way. It’s possible that future iterations of Viberary will use something like <a href="https://openlibrary.org/">Open Library</a>, but this will involve a lot of foundational data work. Third, there is a strong English-language bias in this data, which means we might not be able to get good results in other languages at query time if we want to make Viberary international.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/eng.png" width="600"></figure><p>Finally, in looking at the data available per column, it looks like we have a pretty full set of data available for author, title, ratings, and description (lower percent means less null values per column) which means we’ll be able to use most of our data for representing the corpus as embeddings.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/percent_data.png" width="600"></figure><h2 id="the-model">The Model</h2><hr><p><strong>If you want to understand more of the context behind this section, read <a href="https://raw.githubusercontent.com/veekaybee/what_are_embeddings/main/embeddings.pdf">my embeddings paper.</a></strong></p><p>Viberary uses <a href="https://www.sbert.net/">Sentence Transformers</a>, a modified version of <a href="https://jalammar.github.io/illustrated-bert/">the BERT architecture</a> that reduces computational overhead for deriving embeddings for sentence pairs <a href="https://arxiv.org/pdf/1908.10084.pdf">in a much more operationally efficient way</a> than the original BERT model, making it easy to generate sentence-level embeddings that can be compared relatively quickly using cosine similarity.</p><p>This fits our use case because our input documents are several sentences long, and our query will be a keyword like search of at most 10 or 11 words, much like a short sentence.</p><p>BERT stands for Bi-Directional Encoder and was released 2018, based on a paper written by Google as a way to solve common natural language tasks like sentiment analysis, question-answering, and text summarization. BERT is a transformer model, also based on the attention mechanism, but its architecture is such that it only includes the encoder piece. Its most prominent usage is in Google Search, where it’s the algorithm powering surfacing relevant search results. In the blog post they released on including BERT in search ranking in 2019, Google specifically discussed adding context to queries as a replacement for keyword-based methods as a reason they did this. BERT works as a masked language model, which means it works by removing words in the middle of sentences and guessing the probability that a given word fills in the gap. The B in Bert is for bi- directional, which means it pays attention to words in both ways through scaled dot-product attention. BERT has 12 transformer layers. It uses WordPiece, an algorithm that segments words into subwords, into tokens. To train BERT, the goal is to predict a token given its context, or the tokens surrounding it.</p><p>The output of BERT is latent representations of words and their context — a set of embeddings. BERT is, essentially, an enormous parallelized Word2Vec that remembers longer context windows. Given how flexible BERT is, it can be used for a number of tasks, from translation, to summarization, to autocomplete. Because it doesn’t have a decoder component, it can’t generate text, which paved the way for GPT models to pick up where BERT left off.</p><p>However, this architecture doesn’t work well for parallelizing sentence similarity, which is where sentence-transformers comes in.</p><p>Given a sentence, <code>a</code>, and a second sentence, <code>b</code>, from an input, upstream model with BERT or similar variations as its source data and model weights, we’d like to learn a model whose output is a similarity score for two sentences. In the process of generating that score, the intermediate layers of that model give us embeddings for subsentences and words that we can then use to encode our query and corpus and do semantic similarity matching.</p><p>Given two input sentences, we pass them through the sentence transformer network and uses mean-pooling (aka averaging) all the embeddings of words/subwords in the sentence, then compares the final <a href="https://github.com/UKPLab/sentence-transformers/blob/master/docs/training/overview.md">embedding using cosine similarity, a common distance measure that performs well for multidimensional vector spaces</a></p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/pooling.png" width="600"></figure><p>Sentence Transformers has a number of pre-trained models that are on this architecutre, the most common of which is <code>sentence-transformers/all-MiniLM-L6-v2</code>, which <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">maps sentences and paragraphs</a> into a 384-dimension vector space. This means that each sentence is encoded in a vector of 384 values.</p><p>The initial results of this model were <a href="https://github.com/veekaybee/viberary/releases/tag/v.0.0.4">just so-so</a>, so I had to decide whether to use a different model or tune this one. The different model I considered was the series of <a href="https://www.sbert.net/docs/pretrained-models/msmarco-v3.html">MSMarco models</a> , which were trained based on sample Bing searches. This was closer to what I wanted. Additionally, <a href="https://www.sbert.net/examples/applications/semantic-search/README.html">the search task was asymmetric</a>, which meant that the model accounted for the fact that the corpus vector would be longer than the query vector.</p><p>I chose <code>msmarco-distilbert-base-v3</code>, which is middle of the pack in terms of performance, and critically, is also tuned for cosine similarity lookups, instead of dot product, another similarity measure that takes into account both magnitude and direction. Cosine similarity only considers direction rather than size, making cosine similarity more suited for information retrieval with text because it’s not as affected by text length, and additionally, it’s more efficent at handling sparse representations of data.</p><p>There was a problem, however, because the vectors for this series of models was twice as long, at <code>768</code> dimensions per embedding vector. The longer a vector is, the more computationally intensive it is to work with, increasing, with the runtime and the memory requirement grows quadratic with the input length. However, the longer it is, the more information about the original input it compresses, so there is always a fine-lined tradeoff between being able to encode more information and faster inference, which is critical in search applications.</p><p>Learning embeddings was tricky not only in selecting the correct model, but also because everyone in the entire universe is using GPUs right now.</p><p>I first tried Colab, but soon found that, even at the paid tier, my instances would mysteriously get shut down or downgraded, particularly on Friday nights, when everyone is doing side projects.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/colab.png" width="600"></figure><p>I then tried Paperspace but found its UI hard to navigate, although, ironically, recently it’s been purchased by Digital Ocean which I always loved and have become even more a fan of over the course of this project. I settled on doing the training on AWS since I already have an account and, in doing PRs for PyTorch, <a href="https://vickiboykis.com/2022/07/26/how-to-prepare-an-aws-test-image-for-pytorch/">had already configured EC2 instances for deep learning.</a></p><p>The process turned out to be much less painless than I anticipated, with the exception that P3 instances run out very quickly due to everyone training on them. <a href="https://github.com/veekaybee/viberary/blob/main/src/model/generate_embeddings.ipynb">But it only took about 20 minutes to generate embeddings for my model</a>, which is a really fast feedback loop as far as ML is concerned. I then wrote that data out to a snappy-compressed parquet file that I then load manually to the server where inference is performed.</p><h2 id="redis-and-indexing">Redis and Indexing</h2><hr><p>Once I learned embeddings for the model, I needed to store them somewhere for use at inference time. Once the user inputs a query, that query is transformed also into an embedding representation using the same model, and then the KNN lookup happens. There are about <a href="https://thenewstack.io/vector-databases-long-term-memory-for-artificial-intelligence/">five million options now for storing embeddings</a> for all kinds of operations.</p><p>Some are better, some are worse, it all depends on your criteria. Here were my criteria:</p><ul><li>an existing technology I’d worked with before</li><li>something I could host on my own and introspect</li><li>something that provided blazing-fast inference</li><li>a software package where the documentation tells you <code>O(n)</code> performance time of <a href="https://redis.io/docs/data-types/hashes/">all its constitutent data structures</a></li></ul><p>I’m kidding about the last one but it’s one of the things I love about the Redis documentation. Since I’d previously worked with Redis as a cache, already knew it to be highly reliable and relatively simple to use, as well as plays well with high-traffic web apps and available packaged in Docker, which I would need for my next step to production, I went with <a href="https://redis.io/docs/interact/search-and-query/">Redis Search</a>, which offers storage and inference out of the box, as well as frequently updated Python modules.</p><p>Redis Search is an add-on to Redis that you can load as part of the <a href="https://github.com/RediSearch/RediSearch">redis-stack-server Docker image</a>.</p><p>It offers vector similarity search by indexing vectors stored as fields in Redis hash data structures, which are just field-value pairs like you might see in a dictionary or associative array. The common Redis commands for working with hashes are <code>HSET</code> and <code>HGET</code>, and <a href="https://github.com/veekaybee/viberary/blob/main/src/index/indexer.py">we can first HSET our embeddings</a> and then create an index with a schema on top of them. An important point is that we only want to <a href="https://github.com/veekaybee/viberary/blob/main/src/index/index_embeddings.py">create the index schema after we <code>HSET</code> the embeddings</a>, otherwise performance degrades significantly.</p><p>For our learned embeddings which encompass ~800k documents, this process takes about ~1 minute.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/indexing.png" width="600"></figure><h2 id="lookups-and-requestresponse">Lookups and Request/Response</h2><hr><p>Now that we have the data in Redis, we can perform lookups within the request-response cycle. The process looks like this:</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/request_response.png" width="600"></figure><p>Since we’ll be doing this in the context of a web app, we write a small <a href="https://github.com/veekaybee/viberary/tree/main/src/api">Flask application</a> that has several routes and captures the associated static files of the home page, the search box, and images, and takes a user query, runs it through the created search index object after cleaning the query, and returns a result:</p><p>that data gets passed into the model through a KNN Search object which takes a Redis connection and a config helper object:</p><p>The <a href="https://github.com/veekaybee/viberary/blob/main/src/search/knn_search.py#L13">search class</a> is where most of the real work happens. First, the user query string is parsed and sanitized, although in theory, in BERT models, you should be able to send the text as-is, since BERT was originally trained on data that does not do text clean-up and parsing, like traditional NLP does.</p><p>Then, that data is rewritten into the Python dialect for the Redis query syntax. The search syntax is can be a little hard to work with originally, both in the Python API and on the Redis CLI, so I spent a lot of time playing around with this and figuring out what works best, as well as tuning the hyperparameters passed in <a href="https://github.com/veekaybee/viberary/blob/9f55493e0c8f77c0727df9c0e9191033469e468a/config.yml#L24">from the config file</a>, such as the number of results, the vector size, and the float type (very important to make sure all these hyperparameters are correct given the model and vector inputs, or none of this works correctly.)</p><p><a href="https://github.com/RediSearch/RediSearch">HNSW is the algorithm, initially written at Twitter, implemented in Redis</a> that actually peforms the query to find <a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods">approximate nearest neighbors</a> based on cosine similarity. <a href="https://arxiv.org/abs/1603.09320">It looks for an approximate solution</a> to the k-nearest neighbors problem by formulating nearest neighbors as a graph search problem to be able to find nearest neighbors at scale. Naive solutions here would mean comparing each element to each other element, a process which computationally scales linearly with the number of elements we have. HNSW bypasses this problem by using skip list data structures to create multi-level linked lists to keep track of nearest neighbors. During the navigation process, HNSW traverses through the layers of the graph to find the shortest connections, leading to finding the nearest neighbors of a given point.</p><p>It then returns the closest elements, ranked by cosine similarity. In our case, it returns the document whose 768-dimension vector most closely matches the 768-dimension vector generated by our model at query time.</p><p>The final piece of this is filtering and ranking. We sort by cosine similarity descending, but then also by the number of reviews - we want to return not only books that are relevant to the query, but books that are high-quality, where number of reviews is (questionably) a proxy for the fact that people have read them. If we wanted to experiment with this, we could return by cosine similarity and then by nubmer of stars, etc. There are numerous ways to fine-tune.</p><h2 id="getting-the-ui-right">Getting the UI Right</h2><p>Once we get the results from the API, we get back is a list of elements that include the title, author, cosine similarity, and link to the book. It’s now our job to present this to the user, and to give them confidence that these are good results. Additionally, the results should be able to prompt them to build a query.</p><p>Research has found, and perhaps your personal experience has proven, that it’s hard to stare into a text box and know what to search for, particularly if
the dataset is new to you. Additionally, <a href="https://arxiv.org/ftp/arxiv/papers/2307/2307.01135.pdf">the UX of the SERP page matters greatly.</a> That’s why generative AI products, such as Bard and OpenAI often have prompts or ideas of how to use that open-ended search box.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/gpt.png" width="600"></figure><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/bard.png" width="600"></figure><p>The hard part for me was in getting users to understand how to write a successful vibe query that focused on semantic rather than direct search. I started out with a fairly simple results page that had the title and the rank of the results.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/viberary/main/assets/cats.png" width="600"></figure><p>It became clear that this was not satisfactory: there was no way to reference the author or to look up the book, and the ranking was confusing, particularly to non-developers who were not used to zero indexing. I then iterated to including the links to the books so that people could introspect the results.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/vibe2.jpeg" width="600"></figure><p>I removed the ranking because it felt more confusing and took up more computational power to include it, and additionally people generally understand that best search results are at the top. Finally, I added button suggestions for types of queries to write. I did this by looking at the list of Netflix original categories to see if I could create some of my own, and also by asking friends who had tested the app.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/netflix_cats.png" width="600"></figure><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/vibe3.jpeg" width="600"></figure><p>On top of all of this, I worked to make the site load quickly both on web and mobile, since most people are mobile-first when accessing sites in 2023. And finally, I changed the color to a lighter pink to be more legible. This concludes the graphic design is my passion section of this piece.</p><h2 id="digitalocean-docker-and-production">DigitalOcean, Docker, and Production</h2><hr><p>Now that this all worked in a development environment, it was time to scale it for production. My top requirements included being able to develop locally quickly and reproduce that environment almost exactly on my production instances, a fast build time for CI/CD and for Docker images, the ability to horizontally add more nodes if I needed to but <a href="https://www.youtube.com/watch?app=desktop&amp;v=9BXMWDXiugg">not mess with autoscaling or complicated AWS solutions</a>, and <a href="https://www.youtube.com/watch?v=kx-SeGbkNPU&amp;list=PLYXaKIsOZBsu3h2SSKEovRn7rGy7wkUAV&amp;index=5">smaller Docker images than is typical</a> for
AI apps, <a href="https://vickiboykis.com/2023/07/18/what-we-dont-talk-about-when-we-talk-about-building-ai-apps/">which can easily balloon to 10 GB with Cuda GPU-based layers.</a>. Since my dataset is fairly small and the app itself worked fairly well locally, I decided to stick with CPU-based operations for the time being, at least until I get to a volume of traffic where it’s a problem.</p><p>Another concern I had was that, halfway through the project (never do this), I got a new Macbook M2 machine, which meant <a href="https://www.youtube.com/watch?v=I4wkCSd7iMM">a whole new world of pain</a> in shipping code consistently between <code>arm</code> and <code>intel</code> architectures.</p><p>My deployment story works like this. The web app is developed in a Docker container that I have symlinked via bind mounts to my local directory so that I write code in PyCharm and changes are reflected in the Docker container. The web docker container is networked to Redis via Docker’s internal network. The web app is available at 8000 on the host machine, and, in production in Nginx, proxies port 80 so we can reach the main domain without typing in ports and hit Viberary. In the app dockerfile, I want to make sure to have the fastest load time possible, so I follow <a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/">Docker best practices</a> of having the layers that change the most last, caching, and mounting files into the Docker image so I’m not constantly copying data.</p><p>The docker image base for the web is <code>bitnami:pytorch</code> and it installs requirements via <code>requirements.txt</code></p><p>I have two Dockerfiles, one local and one for production. The production is linked from the <code>docker-compose</code> file and correctly builds on the Digital Ocean server. The local one is linked from the <code>docker-compose.override</code> file, which is excluded from version control, but which works only locally, so that each environment gets the proper build directives.</p><p>The Docker compose takes this Dockerfile and networks it to the Redis container.</p><p>All of this is run through a Makefile that has commands to build, serve, spin down, and run onnx model creation from the root of the directory. Once I’m happy with my code, I push a branch to GitHub where github actions runs basic tests and linting on code that should, in theory, already be checked since I have <code>precommit</code> set up. <a href="https://github.com/veekaybee/viberary/blob/main/.pre-commit-config.yaml">The pre-commit hook</a> lints it and cleans everything up, including black, ruff, and isort, before I even push to a branch.</p><p>Then, once the branch passes, I merge into main. The main branch does tests and pushes the latest git commit to the Digital Ocean server. I then manually go to the server, bring down the old docker image and spin up the new one, and the code changes are live.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/deploy.png" width="600"></figure><p>Finally, on the server, I have a very scientific shell script that helps me configure each additional machine. Since I only needed to do two, it’s fine that it’s fairly manual at the moment.</p><p>Finally everything is routed to port 80 via nginx, which I configured on each DigitalOcean droplet that I created. I load balanced two droplets behind a load balancer, pointing to the same web address, a domain I bought from Amazon’s Route 53. I eventually had to transfer the domain to Digital Ocean, because it’s easier to manage SSL and HTTPS on the load balancer when all the machines are on the same provider.</p><p>{% gist fc6a1b345c82ec4967e9dc3c4d8bba4f %}</p><p>Now, we have a working app. The final part of this was load testing, which I did with <a href="https://locust.io/">Python’s Locust library</a>, which provides a nice interface for running any type of code against any endpoint that you specify. One thing that I realized as I was load testing was that my model was slow, and search expects instant results, so I converted it to an <a href="https://blog.vespa.ai/stateful-model-serving-how-we-accelerate-inference-using-onnx-runtime/">ONNX artifact</a> and had to change the related code, as well.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/locust.jpeg" width="600"></figure><p>Finally, I wrote a small logging module that propogates across the app and keeps track of everything in the docker compose logs.</p><h2 id="key-takeaways">Key Takeaways</h2><ul><li><p><strong>Getting to a testable prototype is key</strong>. I did all my initial exploratory work locally in Jupyter notebooks, <a href="https://github.com/veekaybee/viberary/blob/main/src/notebooks/05_duckdb_0.7.1.ipynb">including working with Redis</a>, so I could see the data output of each cell. I <a href="https://vickiboykis.com/2021/11/07/the-programmers-brain-in-the-lands-of-exploration-and-production/">strongly believe</a> working with a REPL will get you the fastest results immediately. Then, when I had a strong enough grasp of all my datatypes and data flow, I immediately moved the code into object-oriented, testable modules. Once you know you need structure, you need it immediately because it will allow you to develop more quickly with reusable, modular components.</p></li><li><p><strong>Vector sizes and models are important</strong>. If you don’t watch your hyperparameters, if you pick the wrong model for your given machine learning task, the results are going to be bad and it won’t work at all.</p></li><li><p><strong>Don’t use the cloud if you don’t have to</strong>. I’m using DigitalOcean, which is really, really, really nice for medium-sized companies and projects and is often overlooked over AWS and GCP. I’m very versant in cloud, but it’s nice to not have to use BigCloud if you don’t have to and to be able to do a lot more with your server directly. DigitalOcean has reasonable pricing, reasonable servers, and a few extra features like monitoring, load balancing, and block storage that are nice coming from BigCloud land, but don’t overwhlem you with choices. They also recently acquired <a href="https://www.paperspace.com/">Paperspace</a>, which I’ve used before to train models, so should have GPU integration.</p></li><li><p><strong>DuckDB</strong> is becoming a stable tool for work up to 100GB locally. There are a lot of issues that still need to be worked out because it’s a growing project. For example, for two months I couldn’t use it for my JSON parsing because it didn’t have regex features that I was looking for, which were added in 0.7.1, so use with caution. Also, since it’s embedded, you can only run one process at a time which means you can’t run both command line queries and notebooks. But it’s a really neat tool for quickly munging data.</p></li><li><p><strong>Docker still takes time</strong> I spent a great amount of time on Docker. Why is Docker different than my local environment? How do I get the image to build quickly and why is my image now 3 GB? What do people do with CUDA libraries (exclude them if you don’t think you need them initially, it turns out). I spent a lot of time making sure this process worked well enough for me to not get frustrated rebuilding hundreds of times. Relatedly, <strong>Do not switch laptop architectures in the middle of a project</strong> .</p></li><li><p><strong>Deploying to production is magic</strong>, even when you’re a very lonely team of one, and as such <a href="https://vickiboykis.com/2021/06/20/the-ritual-of-the-deploy/">is filled with a lot of unknown variables</a>, so make your environments as absolutely reproducible as possible.</p></li></ul><p>And finally,</p><ul><li>True semantic search is very hard and involves a lot of algorithmic fine-tuning, both in the machine learning, and in the UI, and in deployment processes. People have been fine-tuning Google for years and years. Netflix had thousands of labelers. <a href="https://vicki.substack.com/p/what-we-talk-about-when-we-talk-about">Each company has teams of engineers working on search and recommendations</a> to steer the algorithms in the right direction. Just take a look at the company formerly known as Twitter’s algo stack. It’s fine if the initial results are not that great.</li></ul><p>The important thing is to keep benchmarking the current model against previous models and to keep iterating and keep on building.</p><h2 id="citations">Citations</h2><h2 id="resources">Resources</h2><ul><li><a href="https://www.manning.com/books/relevant-search">Relevant Search by Turnbull and Berryman</a></li><li><a href="https://corise.com/course/search-fundamentals">Corise Search Course</a> and Search with Machine Learning - I’ve taken these, and have nothing to sell except the fact that Grant and Daniel are aweosme. <a href="https://github.com/gsingers/search_fundamentals_course">Code is here.</a></li><li><a href="https://vickiboykis.com/what_are_embeddings/">What Are Embeddings</a> - during the process of writing this I came up with a lot of sources included in the site and bibliography</li><li><a href="https://arxiv.org/abs/2006.02282">Towards Personalized and Semantic Retrieval: An End-to-End Solution for E-commerce Search via Embedding Learning</a></li><li><a href="https://arxiv.org/pdf/2010.06467.pdf">Pretrained Transformers for Text Ranking: BERT and Beyond</a></li><li><a href="https://www.youtube.com/playlist?list=PLSg1mducmHTPZPDoal4m59pPxxsceXF-y">Advanced IR Youtube Series</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Libwebsockets: pure C library for http, websockets, MQTT (141 pts)]]></title>
            <link>https://github.com/warmcat/libwebsockets</link>
            <guid>38896096</guid>
            <pubDate>Sat, 06 Jan 2024 22:17:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/warmcat/libwebsockets">https://github.com/warmcat/libwebsockets</a>, See on <a href="https://news.ycombinator.com/item?id=38896096">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a href="https://libwebsockets.org/git/libwebsockets" rel="nofollow"><img src="https://camo.githubusercontent.com/cc104d8b14cf92dc75824b09daacec37af1b696e2c63ccc80f7ede1bfec5b9a6/68747470733a2f2f6c6962776562736f636b6574732e6f72672f7361692f7374617475732f6c6962776562736f636b657473" alt="CI status" data-canonical-src="https://libwebsockets.org/sai/status/libwebsockets"></a> <a href="https://scan.coverity.com/projects/3576" rel="nofollow"><img src="https://camo.githubusercontent.com/27f2ec5ff7550588149f273c9ebe15356186ad5da1403efd8a68cb6c43280b2d/68747470733a2f2f7363616e2e636f7665726974792e636f6d2f70726f6a656374732f333537362f62616467652e737667" alt="Coverity Scan Build Status" data-canonical-src="https://scan.coverity.com/projects/3576/badge.svg"></a> <a href="https://bestpractices.coreinfrastructure.org/projects/2266" rel="nofollow"><img src="https://camo.githubusercontent.com/baa021ad8613d12b1efcee900fdabeb3b308742ebe6513e5963b42e52d378858/68747470733a2f2f626573747072616374696365732e636f7265696e6672617374727563747572652e6f72672f70726f6a656374732f323236362f6261646765" alt="CII Best Practices" data-canonical-src="https://bestpractices.coreinfrastructure.org/projects/2266/badge"></a> <a href="https://www.codacy.com/app/lws-team/libwebsockets?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=warmcat/libwebsockets&amp;utm_campaign=Badge_Grade" rel="nofollow"><img src="https://camo.githubusercontent.com/e8674ede74f29075a080b65e4870e98ea2875230ed56fdd8946f14824ac413c9/68747470733a2f2f6170692e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f3134346662313935613833303436653438346137356338623463366366633939" alt="Codacy Badge" data-canonical-src="https://api.codacy.com/project/badge/Grade/144fb195a83046e484a75c8b4c6cfc99"></a> <a href="https://lgtm.com/projects/g/warmcat/libwebsockets/alerts/" rel="nofollow"><img src="https://camo.githubusercontent.com/2166be031424fb4ccffcb441f56d15c9f7fbc041cd6908d95abb6a057e84a6c3/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f616c657274732f672f7761726d6361742f6c6962776562736f636b6574732e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138" alt="Total alerts" data-canonical-src="https://img.shields.io/lgtm/alerts/g/warmcat/libwebsockets.svg?logo=lgtm&amp;logoWidth=18"></a> <a href="https://lgtm.com/projects/g/warmcat/libwebsockets/context:cpp" rel="nofollow"><img src="https://camo.githubusercontent.com/3926ba0b7d53bcf6803d58e0b015d571ac226aa900b65df49df1444a079006c9/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f6370702f672f7761726d6361742f6c6962776562736f636b6574732e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138" alt="Language grade: C/C++" data-canonical-src="https://img.shields.io/lgtm/grade/cpp/g/warmcat/libwebsockets.svg?logo=lgtm&amp;logoWidth=18"></a> <a href="https://lgtm.com/projects/g/warmcat/libwebsockets/context:javascript" rel="nofollow"><img src="https://camo.githubusercontent.com/a09fcd112f9f0510b98371a40fdbe8522cfc34503789421d78db50e9fd305d9f/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f6a6176617363726970742f672f7761726d6361742f6c6962776562736f636b6574732e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138" alt="Language grade: JavaScript" data-canonical-src="https://img.shields.io/lgtm/grade/javascript/g/warmcat/libwebsockets.svg?logo=lgtm&amp;logoWidth=18"></a></p>
<h2 tabindex="-1" dir="auto">Libwebsockets</h2>
<p dir="auto">Libwebsockets is a simple-to-use, MIT-license, pure C library providing client and server
for <strong>http/1</strong>, <strong>http/2</strong>, <strong>websockets</strong>, <strong>MQTT</strong> and other protocols in a security-minded,
lightweight, configurable, scalable and flexible way.  It's easy to build and
cross-build via cmake and is suitable for tasks from embedded RTOS through mass
cloud serving.</p>
<p dir="auto">It supports a lot of lightweight ancilliary implementations for things like JSON,
CBOR, JOSE, COSE, and supports OpenSSL and MbedTLS v2 and v3 out of the box for everything.
It's very gregarious when it comes to event loop sharing, supporting libuv, libevent, libev,
sdevent, glib and uloop, as well as custom event libs.</p>
<p dir="auto"><a href="https://libwebsockets.org/git/libwebsockets/tree/minimal-examples" rel="nofollow">100+ independent minimal examples</a> for various scenarios, CC0-licensed
(public domain) for cut-and-paste, allow you to get started quickly.</p>
<p dir="auto"><a href="https://libwebsockets.org/git/libwebsockets/tree/READMEs" rel="nofollow">There are a lot of READMEs</a> on a variety of topics.</p>
<p dir="auto"><a href="https://libwebsockets.org/sai/" rel="nofollow">We do a huge amount of CI testing per push</a>, currently 582 builds on 30 platforms.
<a href="https://warmcat.com/2021/08/21/Sai-CI.html" rel="nofollow">You can see the lws CI rack and read about how lws-based Sai is used to coordinate all the testing</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/warmcat/libwebsockets/blob/main/doc-assets/lws-overview.png"><img src="https://github.com/warmcat/libwebsockets/raw/main/doc-assets/lws-overview.png" alt="overview"></a></p>
<h2 tabindex="-1" dir="auto">News</h2>
<h2 tabindex="-1" dir="auto">HTML + CSS + JPEG + PNG display stack in lws</h2>
<p dir="auto">Want to drive your EPD or TFT / OLED display using HTML + CSS?  Only got an ESP32?</p>
<p dir="auto">Want remote JPEGs, PNGs, HTML, RGBA composition, gamma, error diffusion if needed?</p>
<p dir="auto">Realtime render into a line buffer because you don't have enough heap for a framebuffer?</p>
<p dir="auto"><a href="https://libwebsockets.org/git/libwebsockets/tree/READMEs/README.html-parser.md" rel="nofollow">Take a look here...</a></p>
<h2 tabindex="-1" dir="auto">Perl binding for lws available</h2>
<p dir="auto">Thanks to Felipe Gasper, there's now a <a href="https://metacpan.org/pod/Net::Libwebsockets" rel="nofollow">perl binding for lws available at metacpan</a>,
this uses the recent generic event loop support in lws to have lws as a guest on an existing perl event loop.</p>
<h2 tabindex="-1" dir="auto">Lws examples switching to Secure Streams</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/warmcat/libwebsockets/blob/main/doc-assets/ss-api1.png"><img src="https://github.com/warmcat/libwebsockets/raw/main/doc-assets/ss-api1.png" alt="Secure Streams direct"></a></p>
<p dir="auto"><strong>Secure Streams</strong> support in lws was introduced a couple of years ago, it's a
higher-level interface to lws <code>wsi</code>-level apis that simplifies connectivity by
segregating connection policy like protocol and endpoint information into a
separate <a href="https://github.com/warmcat/libwebsockets/blob/main/minimal-examples/client/hello_world/example-policy.json">JSON policy file</a>, and just having the <a href="https://github.com/warmcat/libwebsockets/blob/main/minimal-examples/clients/hello_world/hello_world-ss.c">code deal with payloads</a>; as many
details of the wire protocol as possible are hidden or moved to the policy, so
user code is almost identical even if the wire protocol changes.</p>
<p dir="auto">The user code just asks to create a SS by "streamtype name", it is created
according to the details (protocol, endpoint, etc) under the same name in the
policy.</p>
<p dir="auto">Key policy entries like endpoint can contain <code>${metadata-name}</code> string
substitutions to handle runtime adaptations via metadata.  h1, h2, ws and mqtt
are supported.</p>
<p dir="auto">As a layer on top of the <code>wsi</code> apis, SS provides a higher-level way to access
the existing wsi-level capabilities, both kinds of API will remain supported.
Secure Streams are longer-lived than a single wsi, so an SS can coordinate
retries by itself.  SS-based user code is typically significantly smaller and
more maintainable than wsi layer.</p>
<p dir="auto">In main branch I have moved the older examples into <code>./minimal-examples-lowlevel</code>
and am starting to port more cases from there into SS-based examples.</p>
<h3 tabindex="-1" dir="auto">Comparison between wsi and SS level lws usage</h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th>"low-level" wsi way</th>
<th>Secure Streams way</th>
</tr>
</thead>
<tbody>
<tr>
<td>Create context</td>
<td>code</td>
<td>same</td>
</tr>
<tr>
<td>Loop support, sul scheduler</td>
<td>default, event libs</td>
<td>same</td>
</tr>
<tr>
<td>Supports comms mode</td>
<td>Client, Server, Raw</td>
<td>same</td>
</tr>
<tr>
<td>Supports protocols</td>
<td>h1, h2, ws, mqtt (client)</td>
<td>same</td>
</tr>
<tr>
<td>TLS support</td>
<td>mbedtls (including v3), openssl (including v3), wolfssl, boringssl, libressl</td>
<td>same</td>
</tr>
<tr>
<td>Serializable, proxiable, muxable, transportable</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Auto-allocated per-connection user object</td>
<td>pss specified in lws_protocols</td>
<td>Specified in ss info struct</td>
</tr>
<tr>
<td>Connection User API</td>
<td>Protocol-specific lws_protocols cbs (&gt; 100)</td>
<td>SS API (rx, tx, state callbacks only)</td>
</tr>
<tr>
<td>Sending adaptation</td>
<td>lws_callback_on_writeable()  + WRITEABLE</td>
<td>lws_ss_request_write() + tx() cb</td>
</tr>
<tr>
<td>Sending buffer</td>
<td>User-chosen + malloc'd partial handling</td>
<td>SS-provided, no partials</td>
</tr>
<tr>
<td>Create vhosts</td>
<td>code</td>
<td><strong>JSON policy</strong></td>
</tr>
<tr>
<td>TLS validation</td>
<td>cert bundle or code</td>
<td><strong>JSON policy</strong>, or cert bundle</td>
</tr>
<tr>
<td>Connection retry / backoff</td>
<td>code</td>
<td><strong>JSON policy</strong>, Auto</td>
</tr>
<tr>
<td>Nailing up</td>
<td>code</td>
<td><strong>JSON policy</strong>, Auto</td>
</tr>
<tr>
<td>Endpoint and protocol details</td>
<td>spread around the code</td>
<td><strong>JSON policy</strong></td>
</tr>
<tr>
<td>Protocol selection, pipeline / stream sharing</td>
<td>code</td>
<td><strong>JSON policy</strong></td>
</tr>
<tr>
<td>ws subprotocol selection</td>
<td>code</td>
<td><strong>JSON policy</strong></td>
</tr>
<tr>
<td>ws binary / text</td>
<td>code</td>
<td><strong>JSON policy</strong></td>
</tr>
<tr>
<td>Protocol-specific metadata</td>
<td>Protocol-specific apis in code (eg, lws_hdr)</td>
<td><strong>JSON policy</strong>, generic metadata apis in code</td>
</tr>
<tr>
<td>Connection validity rules</td>
<td>struct</td>
<td><strong>JSON policy</strong>, Auto</td>
</tr>
<tr>
<td>Stream as Long Poll</td>
<td>code</td>
<td><strong>JSON policy</strong></td>
</tr>
<tr>
<td>Auth</td>
<td>code</td>
<td><strong>JSON policy</strong> + automatic rotation if provider supported, else code</td>
</tr>
</tbody>
</table>
<h3 tabindex="-1" dir="auto">Serialized Secure Streams</h3>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/warmcat/libwebsockets/blob/main/doc-assets/ss-api2.png"><img src="https://github.com/warmcat/libwebsockets/raw/main/doc-assets/ss-api2.png" alt="Secure Streams direct"></a></p>
<p dir="auto">Secure Streams APIs are also <strong>serializable</strong>, the exact same client code can
fulfil the connection directly in the same process as you would expect, or
forward the actions, metadata and payloads to an <a href="https://github.com/warmcat/libwebsockets/blob/main/minimal-examples/ssproxy/ssproxy-socket">SS Proxy</a> that owns the policy
over a Unix Domain or TCP socket connection to be fulfilled centrally.  This
allows, eg, h2 streams from different processes sharing a single connection.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/warmcat/libwebsockets/blob/main/doc-assets/ss-api3.png"><img src="https://github.com/warmcat/libwebsockets/raw/main/doc-assets/ss-api3.png" alt="Secure Streams direct"></a></p>
<p dir="auto">The serialized SS can also travel over generic transports like UART, an <a href="https://github.com/warmcat/libwebsockets/blob/main/minimal-examples/embedded/pico/pico-sspc-binance">example
is provided implementing the Binance example on an RPi Pico</a> with a UART transport
to a <a href="https://github.com/warmcat/libwebsockets/blob/main/minimal-examples/ssproxy/ssproxy-custom-transport-uart">UART transport SS proxy</a>, where the pico itself has no network stack, tls, compression or
wss stack, but can send and receive to and from the endpoint as if it did.</p>
<p dir="auto">The optional <code>lws_trasport_mux</code> is used to interpose between the UART transport
and the SSPC layer, allowing a single pipe to carry many separate SS connections.</p>
<p dir="auto">The user SS code is identical however it is transported, muxed and fulfilled.</p>
<h2 tabindex="-1" dir="auto">v4.3 is released</h2>
<p dir="auto">See the <a href="https://libwebsockets.org/git/libwebsockets/tree/changelog" rel="nofollow">changelog</a></p>
<h2 tabindex="-1" dir="auto">Lws work retrospective</h2>
<p dir="auto">The initial commit for lws will have been 11 years ago come Oct 28 2021, it's been a lot of work.
There are a total of 4.3K patches, touching 800KLOC cumulatively (this is not the size in the
repo, but over the years, how many source lines were changed by patches).</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/warmcat/libwebsockets/blob/main/doc-assets/work.png"><img src="https://github.com/warmcat/libwebsockets/raw/main/doc-assets/work.png" alt="overview"></a></p>
<p dir="auto">Gratifyingly, it turns out over the years, ~15% of that was contributed by 404 contributors: that's not so bad.
Thanks a lot to everyone who has provided patches.</p>
<p dir="auto">Today at least tens of millions of devices and product features rely on lws to
handle their communications including several from FAANG; Google now include lws
as part of Android sources.</p>
<h2 tabindex="-1" dir="auto">Support</h2>
<p dir="auto">This is the libwebsockets C library for lightweight websocket clients and
servers.  For support, visit</p>
<p dir="auto"><a href="https://libwebsockets.org/" rel="nofollow">https://libwebsockets.org</a></p>
<p dir="auto">and consider joining the project mailing list at</p>
<p dir="auto"><a href="https://libwebsockets.org/mailman/listinfo/libwebsockets" rel="nofollow">https://libwebsockets.org/mailman/listinfo/libwebsockets</a></p>
<p dir="auto">You can get the latest version of the library from git:</p>
<ul dir="auto">
<li><a href="https://libwebsockets.org/git" rel="nofollow">https://libwebsockets.org/git</a></li>
</ul>
<p dir="auto">Doxygen API docs for development: <a href="https://libwebsockets.org/lws-api-doc-main/html/index.html" rel="nofollow">https://libwebsockets.org/lws-api-doc-main/html/index.html</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ten Noteworthy AI Research Papers of 2023 (121 pts)]]></title>
            <link>https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023</link>
            <guid>38896027</guid>
            <pubDate>Sat, 06 Jan 2024 22:11:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023">https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023</a>, See on <a href="https://news.ycombinator.com/item?id=38896027">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>This year has felt distinctly different. I've been working in, on, and with machine learning and AI for over a decade, yet I can't recall a time when these fields were as popular and rapidly evolving as they have been this year.</p><p>To conclude an eventful 2023 in machine learning and AI research, I'm excited to share 10 noteworthy papers I've read this year. My personal focus has been more on large language models, so you'll find a heavier emphasis on large language model (LLM) papers than computer vision papers this year.</p><p>I resisted labeling this article "Top AI Research Papers of 2023" because determining the "best" paper is subjective. The selection criteria were based on a mix of papers I either particularly enjoyed or found impactful and worth noting. (The sorting order is a recommended reading order, not an ordering by perceived quality or impact.)</p><p><strong>By the way, if you scroll down to the end of this article, you'll find a little surprise. Thanks for all your support, and I wish you a great start to the new year!</strong></p><p><span>With </span><em><strong><a href="https://arxiv.org/abs/2304.01373" rel="">Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling</a></strong></em><span>, the researchers originally released 8 LLMs ranging from 70M to 12B parameters (with both weights and data publicly released, which is rare).</span></p><p>But in my opinion, the standout feature of this paper is that they also released the training details, analyses, and insights (some of them shown in the annotated figure below).&nbsp;</p><p>Here are some questions that the Pythia paper addresses:</p><ol><li><p>Does pretraining on duplicated data (i.e., training for &gt;1 epoch) make a difference? It turns out that deduplication does not benefit or hurt performance.</p></li><li><p>Does training order influence memorization? Unfortunately, it turns out that it does not. "Unfortunately," because if this was true, we could mitigate undesirable verbatim memorization issues by reordering the training data.</p></li><li><p>Does pretrained term frequency influence task performance? Yes, few-shot accuracy tends to be higher for terms that occur more frequently.</p></li><li><p>Does increasing the batch size affect training efficiency and model convergence? Doubling the batch size halves the training time but doesn't hurt convergence.</p></li></ol><p><span>Today, only six months later, the LLMs are by no means groundbreaking. However, I am including this paper because it not only tries to answer interesting questions about training settings but is also a positive example regarding details and transparency. Moreover, the small LLMs in the &lt;1B range are nice templates for small studies and tinkering, or starters for pretraining experiments (here's a link to their </span><a href="https://github.com/EleutherAI/pythia" rel="">GitHub repository</a><span>).&nbsp;</span></p><p>My wish for 2024 is that we see more studies like this and well-written papers in the coming year!</p><p><em><strong><a href="https://arxiv.org/abs/2307.09288" rel="">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></strong></em><span> is the follow-up paper to Meta's popular first Llama paper.&nbsp;</span></p><p><span>Llama 2 models, which range from 7B to 70B parameters, are one of the reasons this paper made it onto this list: these are still among the most capable and widely used openly available models. Worth noting is that the </span><a href="https://github.com/facebookresearch/llama/blob/main/LICENSE" rel="">Llama 2 license</a><span> also permits use in commercial applications (see the </span><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" rel="">Request to Access page</a><span> for details).</span></p><p>On the model side, what differentiates the Llama 2 suite from many other LLMs is that the models come as standard pretrained models and chat models that have been finetuned via reinforcement learning with human feedback (RLHF, the method used to create ChatGPT) to follow human instructions similar to ChatGPT — RLHF-finetuned models are still rare.</p><p>For more details on RLHF and how it's used in Llama 2, see my more comprehensive standalone article below.</p><div data-component-name="DigestPostEmbed"><a href="https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives" target="_blank" rel="noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe06a33c8-cdbd-4f5e-8380-86fb71a075c8_2216x1232.png"><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe06a33c8-cdbd-4f5e-8380-86fb71a075c8_2216x1232.png" sizes="100vw" alt="LLM Training: RLHF and Its Alternatives" width="140" height="140"></picture></div></a></div><p>Next to the fact that Llama 2 models are widely used and come with RLHF instruction-finetuned variants, the other reason I decided to include the paper on this list is the accompanying in-depth 77-page research report.</p><p>Here, the authors also nicely illustrated the evolution of the Llama 2 70B Chat models, tracing their journey from the initial supervised finetuning (SFT-v1) to the final RLHF finetuning stage with PPO (RLHF-v5). The chart reflects consistent improvements in both the harmlessness and helpfulness axes, as shown in the annotated plots below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png" width="1456" height="712" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:712,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em><span>Annotated figure from Llama 2 paper (</span><a href="https://arxiv.org/abs/2307.09288" rel="">https://arxiv.org/abs/2307.09288</a><span>) showing the performance progression from the first iteration of the supervised finetuned model (SFT-1) to the final RLHF-finetuned chat model (RLHF-v5).</span></em></figcaption></figure></div><p><br><span>Even though models such as Mistral-8x7B (more later), DeepSeek-67B, and YI-34B top the larger Llama-2-70B models in public benchmarks, Llama 2 still remains a common and popular choice when it comes to openly available LLMs and developing methods on top of it.&nbsp;</span></p><p>Furthermore, even though some benchmarks indicate that there may be better models, one of the bigger challenges this year has been the trustworthiness of benchmarks. For instance, how do we know that the models haven't been trained on said benchmarks and the scores aren't inflated? In classic machine learning, when someone proposed a new gradient boosting model, it was relatively easy to reproduce the results and check. Nowadays, given how expensive and complex it is to train LLMs (and the fact that most researchers either don't disclose the architecture or the training data details), it is impossible to tell.&nbsp;</p><p>To conclude, it's refreshing to see Meta doubling down on open source even though every other major company is now rolling out its own proprietary large language models (Google's Bard and Gemini, Amazon's Q, and Twitter/X's Grok, and OpenAI's ChatGPT).&nbsp;</p><p><em><strong><a href="https://arxiv.org/abs/2305.14314" rel="">QLoRA: Efficient Finetuning of Quantized LLMs</a></strong></em><span> has been one of the favorite techniques in the LLM research and finetuning community this year because it makes the already popular LoRA (low-rank adaptation) technique more memory efficient. In short, this means that you can fit larger models onto smaller GPUs.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png" width="616" height="261.46153846153845" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:618,&quot;width&quot;:1456,&quot;resizeWidth&quot;:616,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>A short visual summary of regular LoRA</em></figcaption></figure></div><p>QLoRA stands for quantized LoRA (low-rank adaptation). The standard LoRA method modifies a pretrained LLM by adding low-rank matrices to the weights of the model's layers. These matrices are smaller and, therefore, require fewer resources to update during finetuning.</p><p>In QLoRA, these low-rank matrices are quantized, meaning their numerical precision is reduced. This is done by mapping the continuous range of values in these matrices to a limited set of discrete levels. This process reduces the model's memory footprint and computational demands, as operations on lower-precision numbers are less memory-intensive</p><p><span>According to the </span><a href="https://arxiv.org/abs/2305.14314" rel="">QLoRA paper</a><span>, QLoRA reduces the memory requirements of a 65B Llama model to fit onto a single 48 GB GPU (like an A100). The 65B Guanaco model, obtained from quantized 4-bit training of 65B Llama, maintains full 16-bit finetuning task performance, reaching 99.3% of the ChatGPT performance after only 24 hours of finetuning.</span></p><p>I've also run many QLoRA experiments this year and found QLoRA a handy tool for reducing GPU memory requirements during finetuning. There's a trade-off, though: the extra quantization step results in an additional computation overhead, meaning the training will be a bit slower than regular LoRA.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png" width="1456" height="340" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:340,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em><span>Excerpt from my LoRA &amp; QLoRA experiments that I wrote about previously </span><a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms" rel="">here</a></em></figcaption></figure></div><p>LLM finetuning remains as relevant as ever as researchers and practitioners aim to create custom LLMs. And I appreciate techniques like QLoRA that help make this process more accessible by lowering the GPU memory-requirement barrier.</p><p><span>Looking at all the papers published this year, </span><em><strong><a href="https://arxiv.org/abs/2303.17564" rel="">BloombergGPT: A Large Language Model for Finance</a></strong></em><span> may look like an odd choice for a top-10 list because it didn't result in a groundbreaking new insight, methodology, or open-source model.&nbsp;</span></p><p>I include it because it's an interesting case study where someone pretrained a relatively large LLM on a domain-specific dataset. Moreover, the description was pretty thorough, which is becoming increasingly rare. This is especially true when it comes to papers with authors employed at companies -- one of the trends this year was that major companies are becoming increasingly secretive about architecture or dataset details to preserve trade secrets in this competitive landscape (PS: I don't fault them for that).</p><p>Also, BloombergGPT made me think of all the different ways we can pretrain and finetune models on domain-specific data, as summarized in the figure below (note that this was not explored in the BloombergGPT paper, but it would be interesting to see future studies on that).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg" width="1456" height="1126" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1126,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>The different ways of pretraining and finetuning LLMs.</em></figcaption></figure></div><p>In short, BloombergGPT is a 50-billion parameter language model for finance, trained on 363 billion tokens from finance data and 345 billion tokens from a general, publicly available dataset. For comparison, GPT-3 is 3.5x larger (175 billion parameters) but was trained on 1.4x fewer tokens (499 billion).</p><p>Why did the authors use an architecture with "only" 50 billion parameters since GPT-3 is 3.5x larger? That's easier to answer. They adopted the Chinchilla scaling laws and found this to be a good size given the available size of the finance data.</p><p>Is it worth (pre)training the LLM on the combined dataset from scratch? Based on the paper, the model performs really well in the target domain. However, we don't know whether it's better than a) further pretraining a pretrained model on domain-specific data or b) finetuning a pretrained model on domain-specific data.</p><p>Despite the little criticism above, overall, this is an interesting paper that serves as an interesting case study and example for domain-specific LLMs; plus, it leaves room for further research on pretraining versus finetuning to instill knowledge into an LLM.</p><p><span>(PS: For those curious about a comparison to finetuning, as </span><a href="https://x.com/rohanpaul_ai/status/1738474868214235163?s=20" rel="">Rohan Paul shared</a><span> with me, the "small" </span><a href="https://arxiv.org/abs/2309.09530" rel="">AdaptLLM-7B</a><span> model outperforms BloombergGPT on one dataset and nearly matches its performance on three other finance datasets. Although BloombergGPT appears to be slightly better overall, it's worth noting that training AdaptLLM-7B cost about $100, in contrast to BloombergGPT's multi-million dollar investment.)</span></p><p><span>Before discussing the </span><em><strong><a href="https://arxiv.org/abs/2305.18290" rel="">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></strong></em><span> paper, let's take a short step back and discuss the method it aims to replace, Reinforcement Learning from Human Feedback (RLHF).</span></p><p><span>RLHF is the main technique behind ChatGPT and Llama 2 Chat models. In RLHF, which I described in more detail in a </span><a href="https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives" rel="">separate article</a><span>, we use a multi-step procedure:</span></p><ol><li><p>Supervised finetuning: The model is initially trained on a dataset containing instructions and the desired responses.</p></li><li><p>Reward modeling: Human raters provide feedback on the model's outputs. This feedback is used to create a reward model, which learns to predict what kinds of outputs are to be preferred.</p></li><li><p>Proximal policy optimization (PPO): The model generates outputs, and the reward model scores each output. The PPO algorithm uses these scores to adjust the model's policy toward&nbsp;</p></li></ol><p>generating higher-quality outputs. (This is a reinforcement learning algorithm used to finetune the model's policy.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png" width="348" height="250.34418604651162" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:928,&quot;width&quot;:1290,&quot;resizeWidth&quot;:348,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Example of two training examples from a dataset for the supervised instruction finetuning step. Note that the "input" is optional.</em></figcaption></figure></div><p>While RLHF is popular and effective, as we've seen with ChatGPT and Llama 2, it's also pretty complex to implement and finicky.&nbsp;</p><p><a href="https://arxiv.org/abs/2305.18290" rel="">The Direct Preference Optimization (DPO) paper</a><span> introduces an algorithm that optimizes language models to align with human preferences </span><strong>without</strong><span> explicit reward modeling or reinforcement learning. Instead, DPO uses a simple classification objective.</span></p><p>In DPO, we still keep the supervised finetuning step (step 1 above), but we replace steps 2 and 3 with a single step to further finetune the model on the preference data. In other words, DPO skips the reward model creation required by RLHF entirely, which significantly simplifies the finetuning process.</p><p><span>How well does it work? There haven't been many models trained with DPO until very recently. (This makes sense because DPO is also a relatively recent method.) However, one recent example is the Zephyr 7B model described in </span><em><a href="https://arxiv.org/abs/2310.16944" rel="">Zephyr: Direct Distillation of LM Alignment</a></em><span>. Zephyr-7B is based on a Mistral-7B base LLM that has been finetuned using DPO. (There will be more on Mistral later.)</span></p><p><span>As the performance tables below reveal, the 7B-parameter Zephyr model outperformed all other models in its size class at the time of its release. Even more impressively, Zephyr-7B even surpassed the 10 times larger 70B-parameter Llama 2 chat model on the conversational </span><a href="https://arxiv.org/abs/2306.05685" rel="">MT-Bench</a><span> benchmark as well.</span><br></p><p>In summary, the appeal of the DPO paper lies in the simplicity of its method. The scarcity of chat models trained using RLHF, with Llama 2 as a notable exception, can likely be attributed to the complexity of the RLHF approach. Given this, I think it's reasonable to anticipate an increase in the adoption of DPO models in the coming year.</p><p><span>I must admit that the </span><em><strong><a href="https://arxiv.org/abs/2310.06825" rel="">Mistral 7B paper</a></strong></em><span> wasn't among my favorites due to its brevity. However, the model it proposed was quite impactful.</span></p><p>I decided to include the paper on this list because the Mistral 7B model was not only very popular upon release, but also served as the base model, leading to the development of two other notable models: Zephyr 7B and the latest Mistral Mixture of Experts (MoE) approach. These models are good examples of the trend I foresee for small LLMs in (at least) the early half of 2024.</p><p>Before we discuss the Zephyr 7B and Mistral MoE models, let's briefly talk about Mistral 7B itself.</p><p><span>In short, The Mistral 7B paper introduces a compact yet powerful language model that, despite its relatively modest size of 7 billion tokens, outperforms its larger counterparts, such as the 13B Llama 2 model, in various benchmarks. (Next to the two-times larger </span><a href="https://github.com/QwenLM/Qwen" rel="">Qwen 14B</a><span>, Mistral 7B was also the base model used in the winning solutions of this year's </span><a href="https://llm-efficiency-challenge.github.io/leaderboard" rel="">NeurIPS LLM Finetuning &amp; Efficiency challenge</a><span>.)</span></p><p>Why exactly it is so good is unclear, but it might likely be due to its training data. Neither Llama 2 nor Mistral discloses the training data, so we can only speculate.</p><p><span>Architecture-wise, the model shares group-query attention with Llama 2. While being very similar to Llama 2, one interesting addition to the Mistral architecture is sliding window attention to save memory and improve computational throughput for faster training. (Sliding window attention was previously proposed in </span><a href="https://arxiv.org/abs/1904.10509" rel="">Child et al. 2019</a><span> and </span><a href="https://arxiv.org/abs/2004.05150" rel="">Beltagy et al. 2020</a><span>.)</span></p><p>The sliding window attention mechanism used in Mistral is essentially a fixed-sized attention block that allows a current token to attend only a specific number of previous tokens (instead of all previous tokens), which is illustrated in the figure below.</p><p>In the specific case of 7B Mistral, the attention block size is 4096 tokens, and the researchers were training the model with up to 100k token context sizes. To provide a&nbsp; concrete example, in regular self-attention, a model at the 50,000th token can attend all previous 49,999 tokens. In sliding window self-attention, the Mistral model can only attend tokens 45,904 to 50,000 (since 50,000 - 4,096 = 45,904).&nbsp;</p><p>However, sliding window attention is mainly used to improve computational performance. The fact that Mistral outperforms larger Llama 2 models is likely not because of sliding window attention but rather despite sliding window attention.</p><p>One reason Mistral 7B is an influential model is that it served as the base model for Zephyr 7B, as mentioned earlier in the DPO section. Zephyr 7B, the first popular model trained with DPO to outperform other alternatives, has potentially set the stage for DPO to become the preferred method for finetuning chat models in the coming months.</p><p><span>Another noteworthy model derived from Mistral 7B is the recently released </span><a href="https://mistral.ai/news/mixtral-of-experts/" rel="">Mistral Mixture of Experts (MoE) model</a><span>, also known as Mixtral-8x7B. This model matches or exceeds the performance of the larger Llama-2-70B on several public benchmarks.</span></p><p><span>For more benchmarks, also see the official </span><a href="https://mistral.ai/news/mixtral-of-experts/" rel="">Mixtral blog post announcement</a><span>. The team also released a Mixtral-8x7B-Instruct model that has been finetuned with DPO (but as of this writing there are no benchmarks comparing it to Llama-2-70-Chat, the RLHF-finetuned model).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png" width="286" height="246.01421800947867" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d5e379db-3f34-4733-854a-f90987181118_844x726.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:726,&quot;width&quot;:844,&quot;resizeWidth&quot;:286,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Mixtral architecture overview based on the param.json file that the Mistral team originally shared via a magnet link on social media</em></figcaption></figure></div><p><span>GPT-4 is also rumored to be an MoE consisting of 16 submodules. Each of these 16 submodules is rumored to have 111 billion parameters (for reference, GPT-3 has 175 billion parameters). If you read my </span><a href="https://magazine.sebastianraschka.com/p/ai-and-open-source-in-2023" rel="">AI and Open Source in 2023 article</a><span> approximately two months ago, I mentioned that "It will be interesting to see if MoE approaches can lift open-source models to new heights in 2024". It looks like Mixtral started this trend early, and I am sure that this is just the beginning.</span></p><p>If you are new to MoE models, here's a short explanation.</p><p>The figure above shows the architecture behind the Switch Transformer, which uses 1 expert per token with 4 experts in total. Mixtral-8x-7B, on the other hand, consists of 8 experts and uses 2 experts per token.</p><p>Why MoEs? Combined, the 8 experts in a 7B model like Mixtral are still ~56B parameters. Actually, it's less than 56B, because the MoE approach is only applied to the FFN (feed forward network, aka fully-connected) layers, not the self-attention weight matrices. So, it's likely closer to 40-50B parameters.</p><p>Note that the router reroutes the tokens such that only &lt;14B parameters (2x &lt;7B, instead of all &lt;56B) are used at a time for the forward pass, so the training (and especially inference) will be faster compared to the traditional non-MoE approach.</p><p><span>If you want to learn more about MoEs, here's a reading list recommended by </span><a href="https://twitter.com/sophiamyang" rel="">Sophia Yang</a><span>:&nbsp;</span></p><ul><li><p><a href="https://arxiv.org/abs/1701.06538" rel="">The Sparsely-Gated Mixture-of-Experts Layer (2017)</a></p></li><li><p><a href="https://arxiv.org/abs/2006.16668" rel="">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (2020)</a><span>&nbsp;</span></p></li><li><p><a href="https://arxiv.org/abs/2211.15841" rel="">MegaBlocks: Efficient Sparse Training with Mixture-of-Experts (2022)</a><span>&nbsp;</span></p></li><li><p><a href="https://arxiv.org/abs/2305.14705" rel="">Mixture-of-Experts Meets Instruction Tuning (2023)</a></p></li></ul><p><span>Furthermore, if you are interested in trying MoE LLMs, also check out the </span><a href="https://github.com/XueFuzhao/OpenMoE" rel="">OpenMoE</a><span> repository, which implemented and shared MoE LLMs earlier this year.</span></p><p>Mistral 7B, Zephyr 7B, and Mixtral-8x7B are excellent examples of the progress made in 2023 with small yet capable models featuring openly available weights. Another notable model, a runner-up on my favorite papers list, is Microsoft's phi series. </p><p>The secret sauce of phi is training on high-quality data (referred to as “textbook quality data”) obtained by filtering web data.</p><p>Released in stages throughout 2023, the phi models include phi-1 (1.3B parameters), phi-1.5 (1.3B parameters), and phi-2 (2.7B parameters). The latter, released just two weeks ago, is already said to match or outperform Mistral 7B, despite being only half its size.</p><p>For more information about the phi models, I recommend the following resources:</p><ul><li><p><a href="https://arxiv.org/abs/2306.11644" rel="">Textbooks Are All You Need</a><span> -- the phi-1 paper</span></p></li><li><p><a href="https://arxiv.org/abs/2309.05463" rel="">Textbooks Are All You Need II: phi-1.5 Technical Report</a></p></li><li><p><a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/" rel="">The Phi-2: The Surprising Power of Small Language Models</a><span> announcement</span></p></li></ul><p><em><strong><a href="https://arxiv.org/abs/2311.11045" rel="">Orca 2: Teaching Small Language Models How to Reason</a></strong></em><span> is a relatively new paper, and time will tell whether it has a lasting impact on how we train LLMs in the upcoming months or years.&nbsp;</span></p><p>I decided to include it because it combines several concepts and ideas.&nbsp;</p><p>One is the idea of distilling data from large, capable models such as GPT-4 to create a synthetic dataset to train small but capable LLMs. This idea was described in the Self-Instruct paper, which came out last year. Earlier this year, Alpaca (a Llama model finetuned on ChatGPT outputs) really popularized this approach.</p><p>How does this work? In a nutshell, it's a 4-step process:</p><ol><li><p>Seed task pool with a set of human-written instructions (175 in this case) and sample instructions;</p></li><li><p>Use a pretrained LLM (like GPT-3) to determine the task category;</p></li><li><p>Given the new instruction, let a pretrained LLM generate the response;</p></li><li><p>Collect, prune, and filter the responses before adding them to the task pool.</p></li></ol><p><span>The other idea may not be surprising but worth highlighting: high-quality data is important for finetuning. For instance, the </span><a href="https://arxiv.org/abs/2305.11206" rel="">LIMA paper</a><span> proposed a human-generated high-quality dataset consisting of only 1k training examples that can be used to finetuning to outperform the same model finetuned on 50k ChatGPT-generated responses.</span></p><p>Unlike previous research that heavily relied on imitation learning to replicate outputs from larger models, Orca 2 aims to teach "small" (i.e., 7B and 13B) LLMs various reasoning techniques (like step-by-step reasoning, recall-then-generate, etc.) and to help them determine the most effective strategy for each task. This approach has led Orca 2 to outperform similar-sized models noticeably and even achieve results comparable to models 5-10 times larger.</p><p><span>While we haven't seen any extensive studies on this, the Orca 2 approach might also be able to address the issue of using synthetic data that was highlighted in the </span><a href="https://arxiv.org/abs/2305.15717" rel="">The False Promise of Imitating Proprietary LLMs</a><span> paper. Here, the researchers investigated the finetuning weaker language models to imitate stronger proprietary models like ChatGPT, using examples such as Alpaca and Self-Instruct. Initially, the imitation models showed promising results, performing well in following instructions and receiving competitive ratings from crowd workers compared to ChatGPT. However, more follow-up evaluations revealed that these imitation models only seemed to perform well to a human observer but often generated factually incorrect responses.</span></p><p>In recent years, I've almost exclusively worked with large language transformers or vision transformers (ViTs) due to their good performance.&nbsp;</p><p>Switching gears from language to computer vision papers for the last three entries, what I find particularly appealing about transformers for computer vision is that pretrained ViTs are even easier to finetune than convolutional neural networks. (I summarized a short hands-on talk at CVPR earlier this year here: https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training).&nbsp;</p><p><span>To my surprise, I stumbled upon the </span><em><strong><a href="https://arxiv.org/abs/2310.16764" rel="">ConvNets Match Vision Transformers at Scale</a></strong></em><span> paper showing that convolutional neural networks (CNNs) are in fact, competitive with ViTs when given access to large enough datasets.</span></p><p>Here, researchers invested compute budgets of up to 110k TPU hours to do a fair comparison between ViTs and CNNs. The outcome was that when CNNs are pretrained with a compute budget similar to what is typically used for ViTs, they can match the performance of ViTs. For this, they pretrained on 4 billion labeled images from JFT and subsequently finetuned the models on ImageNet.</p><p>Object recognition and segmentation in images and videos, along with classification and generative modeling, are the main research fields in computer vision.&nbsp;</p><p>To briefly highlight the difference between these two tasks: object detection about predicting bounding boxes and the associated labels; segmentation classifies each pixel to distinguish between foreground and background objects.&nbsp;</p><p><span>Meta's </span><em><strong><a href="https://arxiv.org/abs/2304.02643" rel="">Segment Anything</a></strong></em><span> paper is a notable milestone for open source and image segmentation research. The paper introduces a new task, model, and dataset for image segmentation. The accompanying image datasets the largest segmentation dataset to date with over 1 billion masks on 11 million images.&nbsp;</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg" width="592" height="304.1717791411043" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:670,&quot;width&quot;:1304,&quot;resizeWidth&quot;:592,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em><span>The Segment Anything Model (SAM) is designed for efficient, prompt-based image segmentation. Annotated screenshot from the Segment Anything paper, </span><a href="https://arxiv.org/abs/2304.02643" rel="">https://arxiv.org/abs/2304.02643</a></em></figcaption></figure></div><p>However, what's rare and especially laudable is that the researchers used licensed and privacy-respecting images, so the model can be open-sourced without major copyright concerns.</p><p>The Segment Anything Model (SAM) consists of three main components, as summarized in the annotated figure above.</p><p>In slightly more details, the three components can be summarized as follows:</p><ol><li><p>An image encoder utilizing a masked autoencoder based on a pretrained vision transformer (ViT) that can handle high-resolution inputs. This encoder is run once per image and can be applied before prompting the model.</p></li><li><p>A prompt encoder that handles two types of prompts: sparse (points, boxes, text) and dense (masks). Points and boxes are represented by positional encodings combined with learned embeddings for each prompt type. And free-form text uses an off-the-shelf text encoder from CLIP. Dense prompts, i.e., masks, are embedded using convolutions and summed element-wise with the image embedding.</p></li><li><p>A mask decoder maps the image embedding, prompt embeddings, and an output token to a mask. This is a decoder-style transformer architecture that computes the mask foreground probability at each image location.</p></li></ol><p><span>Image segmentation is important for applications like self-driving cars, medical imaging, and many others. In the short amount of 6 months, the paper has already been </span><a href="https://scholar.google.com/scholar?cites=15741444728855576863&amp;as_sdt=5,39&amp;sciodt=0,39&amp;hl=en" rel="">cited more than 1500 times</a><span>, and there have already been many projects that have been built on top of this paper.</span></p><p><em><strong><a href="https://arxiv.org/abs/2311.10709" rel="">Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning</a></strong></em><span> is another notable computer vision project from Meta's research division.&nbsp;</span></p><p>Emu is a text-to-video model that can generate entire videos from text prompts.&nbsp;</p><p>While it's not the first model for impressive text-to-video generation, it compares very favorably to previous works.</p><p>As the authors note, the Emu architecture setup is relatively simple compared to previous approaches. One of the main ideas here is that Emu factorizes the generation process into two steps: first, generating an image based on text (using a diffusion model), then creating a video conditioned on both the text and the generated image (using another diffusion model).&nbsp;</p><p>2022 has been a big year for text-to-image models like DALL-E 2, Stable Diffusion, and Midjourney. While text-to-image models remain very popular in 2023 (even though LLMs got most of the attention throughout the year), I think that text-to-video models are just about to become more prevalent in online communities in the upcoming year.&nbsp;</p><p>Since I am not an image or video designer, I don't have use cases for these tools at the moment; however, text-to-image and text-to-video models are nonetheless interesting to watch as a general measure of progress regarding computer vision.</p><p><span>I've been coding and writing a new book since last summer, and I am excited to share that the </span><a href="http://mng.bz/amjo" rel="">first chapters are now available via Manning's early access program</a><span>.</span></p><p><span>In </span><em><strong><a href="http://mng.bz/amjo" rel="">Build a Large Language Model (from Scratch)</a></strong></em><span>, you will code an LLM step-by-step using PyTorch to gain a thorough understanding of its inner workings.&nbsp;</span></p><p>The book covers everything from coding the data input pipeline to implementing attention mechanisms from scratch and pretraining and finetuning the LLM. Each stage is accompanied by clear text, diagrams, and examples.</p><p><em><strong>I hope you have a good start to the new year!</strong></em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chess-GPT's Internal World Model (119 pts)]]></title>
            <link>https://github.com/adamkarvonen/chess_llm_interpretability</link>
            <guid>38895623</guid>
            <pubDate>Sat, 06 Jan 2024 21:23:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/adamkarvonen/chess_llm_interpretability">https://github.com/adamkarvonen/chess_llm_interpretability</a>, See on <a href="https://news.ycombinator.com/item?id=38895623">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">chess_llm_interpretability</h2>
<p dir="auto">This evaluates LLMs trained on PGN format chess games and evaluates board understand, similar to the Othello World paper.</p>
<p dir="auto">This repo can train, evaluate, and visualize linear probes on LLMs that have been trained to play chess with PGN strings. For example, we can visualize where the model "thinks" the white pawns are. On the left, we have the actual white pawn location. In the middle, we clip the probe outputs to turn the heatmap into a more binary visualization. On the right, we have the full gradient of model beliefs, and we can see it's extremely confident that no white pawns are on either side's back rank.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/adamkarvonen/chess_llm_interpretability/blob/main/images/pawn_probe.png"><img src="https://github.com/adamkarvonen/chess_llm_interpretability/raw/main/images/pawn_probe.png" alt=""></a></p>
<p dir="auto">I trained linear probes on the model's ability to estimate player ELO as it's predicting the next character. Here we can see a graph of ELO classification accuracy per layer of the LLM.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/adamkarvonen/chess_llm_interpretability/blob/main/images/accuracy_per_layer_elo.png"><img src="https://github.com/adamkarvonen/chess_llm_interpretability/raw/main/images/accuracy_per_layer_elo.png" alt=""></a></p>
<p dir="auto">For more information, refer to this <a href="https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html" rel="nofollow">post</a>.</p>
<h2 tabindex="-1" dir="auto">Setup</h2>
<p dir="auto">Create a Python environment with Python 3.10.</p>
<div data-snippet-clipboard-copy-content="pip install -r requirements.txt
python model_setup.py"><pre><code>pip install -r requirements.txt
python model_setup.py
</code></pre></div>
<p dir="auto">Then click "Run All" on <code>lichess_data_filtering.ipynb</code>.
To visualise probe outputs, check out <code>probe_output_visualization.ipynb</code>.</p>
<p dir="auto">To train a linear probe or test a saved probe on the test set, set these two variables at the bottom of <code>train_test_chess.py</code>:
RUN_TEST_SET = True
USE_PIECE_BOARD_STATE = True</p>
<p dir="auto">Then run <code>python train_test_chess.py</code>.</p>
<h2 tabindex="-1" dir="auto">References</h2>
<p dir="auto">Much of my linear probing was developed using Neel Nanda's linear probing code as a reference. Here are the main references I used:</p>
<p dir="auto"><a href="https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Othello_GPT.ipynb" rel="nofollow">https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Othello_GPT.ipynb</a>
<a href="https://colab.research.google.com/github/likenneth/othello_world/blob/master/Othello_GPT_Circuits.ipynb" rel="nofollow">https://colab.research.google.com/github/likenneth/othello_world/blob/master/Othello_GPT_Circuits.ipynb</a>
<a href="https://www.neelnanda.io/mechanistic-interpretability/othello" rel="nofollow">https://www.neelnanda.io/mechanistic-interpretability/othello</a>
<a href="https://github.com/likenneth/othello_world/tree/master/mechanistic_interpretability">https://github.com/likenneth/othello_world/tree/master/mechanistic_interpretability</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Studies suggest that relying on will power to break habits is hopeless (2019) (163 pts)]]></title>
            <link>https://www.newyorker.com/magazine/2019/10/28/can-brain-science-help-us-break-bad-habits</link>
            <guid>38895342</guid>
            <pubDate>Sat, 06 Jan 2024 20:54:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newyorker.com/magazine/2019/10/28/can-brain-science-help-us-break-bad-habits">https://www.newyorker.com/magazine/2019/10/28/can-brain-science-help-us-break-bad-habits</a>, See on <a href="https://news.ycombinator.com/item?id=38895342">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Several years ago, I bought a smartphone and soon came to love it. Being able to send an e-mail, look up a fact, or buy something no matter where I was meant a previously unimaginable gain in productivity. Every time I got an e-mail, the phone emitted a ping and I would deal with whatever it was, priding myself on my efficiency. Texts arrived with the tones of a French horn and were similarly dispatched. Soon, I was reaching for the device every time it made a sound, like Pavlov’s dog salivating when it heard a bell. This started to interfere with work and conversations. The machine had seemed like a miraculous servant, but gradually I became its slave.</p><p>I’d always prided myself on my will power. Like most people who’ve made it through medical training—with its early mornings and its long shifts when your friends are partying—I had an established track record of delaying gratification. It didn’t matter. When I tried switching the phone to silent, I ended up checking it perhaps even more often, <em>just in case</em> there was something to deal with. The only time I managed to resist was during Shabbos, when I don’t read e-mail. But I’d be watching the clock, counting the hours till I could turn the thing on. For the first time, I could imagine what it’s like to be a smoker craving a cigarette. Checking the smartphone had become a bad habit that I couldn’t break.</p><p>Habits, good and bad, have long fascinated philosophers and policymakers. Aristotle, in the Nicomachean Ethics, surveyed existing notions of virtue and offered this summary: “Some thinkers hold that it is by nature that people become good, others that it is by habit, and others that it is by instruction.” He concluded that habits were responsible. Cicero called habit “second nature,” a phrase that we still use. And when Alexander Hamilton, in Federalist Paper No. 27, considered how to create citizens who would obey the federal laws of the newly formed republic, he used another proverbial phrase: “Man is very much a creature of habit.” If federal law permeated matters at the state level, it would seem part of everyday life. “The more it circulates through those channels and currents in which the passions of mankind naturally flow, the less will it require the aid of the violent and perilous expedients of compulsion,” he wrote.</p><p>In the modern era, habits have become a significant area of scientific inquiry. Psychologists have explored the genesis of habitual behavior and its impact on health and happiness. William James, echoing Aristotle, wrote, “All our life, so far as it has definite form, is but a mass of habits,—practical, emotional, and intellectual&nbsp;.&nbsp;.&nbsp;. bearing us irresistibly toward our destiny.”</p><p>Few of us like to think of ourselves in such passive terms. What about will power? Marketers flatter our sense of agency with slogans like “Just Do It” (Nike) and “Declare Your Path” (New Balance). Much popular psychology, too, bolsters our belief in self-control. In the famous Stanford marshmallow experiment, devised by Walter Mischel, in the nineteen-sixties, children were seated alone in front of a marshmallow and were scored on whether they resisted gobbling it down. The resulting determination of a child’s level of “executive function” supposedly distinguishes life’s winners and losers, predicting such things as performance on the SAT, duration of relationships, and career success. But how can that be, if we’re just creatures of habit?</p><p>In “<a data-offer-url="https://www.amazon.com/Good-Habits-Bad-Science-Positive/dp/1250159075" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Good-Habits-Bad-Science-Positive/dp/1250159075&quot;}" href="https://www.amazon.com/Good-Habits-Bad-Science-Positive/dp/1250159075" rel="nofollow noopener" target="_blank">Good Habits, Bad Habits</a>” (Farrar, Straus &amp; Giroux), the social psychologist Wendy Wood refutes both James’s determinism and glib exhortations to be proactive, and seeks to give the general reader more realistic ideas for how to break habits. Drawing on her work in the field, she sees the task of sustaining positive behaviors and quelling negative ones as involving an interplay of decisions and unconscious factors. Our minds, Wood explains, have “multiple separate but interconnected mechanisms that guide behavior.” But we are aware only of our decision-making ability—a phenomenon known as the “introspection illusion”—and that may be why we overestimate its power. The executive functions that make will power possible give us, she writes, “the sense of agency that we recognize as ‘me.’&nbsp;” But that comes at a cost in terms of effort. To go about our lives, we need to make some behaviors automatic.</p><p>Functional MRI scans have given researchers a peek into the respective neural networks that are active during rote and conscious tasks. A brain scan of someone learning a task shows activity in the prefrontal cortex and the hippocampus, networks associated with decision-making and executive control. With repetition of a task, brain activity moves into areas of the putamen and the basal ganglia, deep in what Wood calls “the rudimentary machinery of our minds.” There, a task is turned into a habit.</p><p>These more primitive areas of the brain demand less of our mental energy. Whole sequences of actions become linked, a process known as “chunking.” When we get into a car and drive off, we don’t need to think about the separate actions of buckling a seat belt, turning on the ignition, putting the car in drive, checking the mirrors and the blind spot, and pressing the gas pedal. All these steps, chunked into a single unit in the memory, are triggered by the environmental cue of getting into your car. This frees us up to concentrate on what most requires conscious attention. We can think about where we’re going or the day’s tasks, and keep an eye out for anything unusual on the road.</p><p>Wood’s research originally focussed not on habits but on persistence. For “one-off, occasional behaviors,” like getting a flu shot, conscious decisions were all that was required. For behaviors involving repetition, though, habits were crucial. William James estimated that “ninety-nine hundredths or, possibly, nine hundred and ninety-nine thousandths of our activity is purely automatic and habitual.” This was a guess; Wood, however, devised a study to quantify just how often people act out of habit. Using a research technique known as experience sampling, she had participants spend two days recording what they did while they were doing it. Results varied across the groups studied, but the basic finding was that our actions are habitual forty-three per cent of the time.</p><p>This explains why conscious knowledge is not in itself enough to change behavior, and why public-health initiatives that educate people about healthy choices tend to fail. In 1991, the National Cancer Institute determined that only eight per cent of Americans were aware of the recommendation to eat at least five servings of fruit and vegetables daily. A national campaign was declared: 5 a Day for Better Health. Six years later, thirty-nine per cent of Americans knew about five servings a day, a nearly fivefold increase, but actual diets had barely changed. In 2007, government officials tried again, launching a program called Fruits &amp; Veggies—More Matters. Even so, by 2018 only twelve per cent of Americans ate the recommended two servings of fruit daily, and only nine per cent ate three servings of vegetables. Simply informing us of what’s good for us doesn’t work, because so much of our eating, cooking, and shopping is governed by habit.</p><p>In Mischel’s marshmallow experiment, only a quarter of the subjects were able to resist eating the marshmallow for fifteen minutes. This implies that a large majority of us lack the self-control required to succeed in life. But a less discussed part of the study suggests a way of circumventing our frailty. The researchers compared the results of two situations: in one, children could see the marshmallow in front of them; in the other, they knew that it was there but couldn’t see it. On average, the children lasted only six minutes when presented with visible temptation but could manage ten minutes if the treat was hidden. For Wood, this outcome shows that self-control is “not so much an inherent disposition but instead a reflection of the situation we are in.” A few tweaks to our environment may enable us to emulate people who seem more disciplined.</p><p>A study of self-control among college students bears out this hypothesis. The students were told to report every time they thought, “Oops, I shouldn’t do this”—for instance, when they stayed up too late, overslept, overate, or procrastinated. They were most successful at adopting productive behaviors not when they resolved to do better, or distracted themselves from temptation, but when they altered their environment. Instead of studying on a couch in a dorm, with a TV close by, they went to the library. They ate better when they removed junk food from the dorm refrigerator. “Successful self-control,” Wood writes, “came from essentially covering up the marshmallow.”</p><p>Even people who score high on self-control questionnaires may owe their apparent virtue to situational factors rather than to sheer fortitude. A study of such people in Germany found that they reported resisting temptation surprisingly rarely. “They were living their lives in a way that hid the marshmallow almost all the time,” Wood writes. This observation leads to the crux of her book’s thesis: the path to breaking bad habits lies not in resolve but in restructuring our environment in ways that sustain good behaviors. Wood cites the psychologist Kurt Lewin, who argued that behavior was influenced by “a constellation of forces” analogous to gravity or to the fluid dynamics that make a river run faster or slower. Those forces work depending on where you are, who’s around you, the time of day, and your recent actions. We achieve situational control, paradoxically, not through will power but by finding ways to take will power out of the equation.</p><p>The central force for eliminating bad habits, according to Wood, is “friction”: if we can make bad habits more inconvenient, then inertia can carry us in the direction of virtue, without ever requiring us to be strong. She cites the ways in which increased friction has produced a decline in smoking: laws that ban it in restaurants, bars, airplanes, and trains; taxes that have helped triple the price of cigarettes in the U.S. in the past twenty years; the purge of cigarettes from vending machines, and of tobacco ads from TV and the radio.</p><p>Meanwhile, however, businesses all around us try to reduce friction. A cashier taking an order at McDonald’s is scripted to ask, “Would you like fries with that?” This simple question encourages us to eat more fat and carbs. Binge-watching on Netflix or Hulu is facilitated by the way that the next episode starts automatically as the credits roll on the previous one. Wood talks to M.&nbsp;Keith Chen, a former head of economic research for Uber, who explains that the app was designed to minimize friction. “The phone’s GPS knows where you are,” he says. “You don’t even need to think about it.&nbsp;.&nbsp;.&nbsp;. You get out without handling cash.”</p><p>The tendency of companies to act as our enablers was extensively examined in Charles Duhigg’s best-seller “<a data-offer-url="https://www.amazon.com/Power-Habit-What-Life-Business/dp/081298160X" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Power-Habit-What-Life-Business/dp/081298160X&quot;}" href="https://www.amazon.com/Power-Habit-What-Life-Business/dp/081298160X" rel="nofollow noopener" target="_blank">The Power of Habit</a>” (2012). Like Wood, Duhigg, who when he wrote the book was a reporter at the <em>Times</em>, notes ways that the fast-food industry designs prompts to make us consume more. McDonald’s standardizes the appearance of its restaurants, in order to trigger habitual eating routines. The foods at many chains are specifically engineered to deliver bursts of salt and fat that immediately light up the reward centers of the brain.</p><p>Examining corporate efforts to capitalize on habit formation, Duhigg describes the work of an early-twentieth-century advertising guru, Claude&nbsp;C. Hopkins, whose campaign for Pepsodent toothpaste is said to have established toothbrushing as habitual among Americans. When Pepsodent first appeared, in 1915, few people bothered to brush their teeth, and a leading dental researcher of the time pronounced all toothpastes useless. Hopkins focussed his marketing message on the film of plaque that covers our teeth; in 1917, his newspaper ads proclaimed it “the basic cause of all tooth troubles.” In fact, plaque can be temporarily removed simply by eating an apple, and toothpastes of the time didn’t remove any more of it than brushing without toothpaste did. Nevertheless, Hopkins set about amping up the dangers of plaque and telling the public that Pepsodent was the only way to get rid of it. “Just run your tongue across your teeth,” another ad read. “<em>You’ll feel a film</em>—that’s what makes your teeth look ‘off color’ and invites decay.” In just a few years, Pepsodent had become one of the best-known products in the world.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Everything NPM Package (169 pts)]]></title>
            <link>https://socket.dev/blog/when-everything-becomes-too-much</link>
            <guid>38894445</guid>
            <pubDate>Sat, 06 Jan 2024 19:22:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://socket.dev/blog/when-everything-becomes-too-much">https://socket.dev/blog/when-everything-becomes-too-much</a>, See on <a href="https://news.ycombinator.com/item?id=38894445">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="ltr"><p>Happy 2024, folks! Just when we thought we'd seen it all, an npm user named PatrickJS, aka <a href="https://socket.dev/npm/user/gdi2290">gdi2290</a>, threw us a curveball. He (<a target="_blank" rel="noopener noreferrer" href="https://uncenter.dev/posts/npm-install-everything/">along with a group of contributors</a>) kicked off the year with a bang, launching a troll campaign that uploaded an npm package aptly named <a href="https://socket.dev/npm/package/everything"><code>everything</code></a>. This package, true to its name, depends on every other public npm package, creating millions of transitive dependencies.</p><h3>The Chaos Unleashed</h3><p>The <code>everything</code> package and its 3,000+ sub-packages have caused a <a href="https://socket.dev/glossary/denial-of-service-dos">Denial of Service (DOS)</a> for anyone who installs it. We're talking about storage space running out and system resource exhaustion.</p><p>But that's not all. The creator took their prank to the next level by setting up http://everything.npm.lol, showcasing the chaos they unleashed. They even included a meme from Skyrim, adding some humor (or mockery, depending on your perspective) to the situation.</p><h4><code>everything</code>'s <code>package.json</code> file</h4><pre><code lang="json">{
  "name": "everything",
  "version": "3.0.0",
  "description": "npm install everything",
  "main": "index.js",
  "contributors": [
    "PatrickJS &lt;github@patrickjs.com&gt;",
    "uncenter &lt;hi@uncenter.dev&gt;",
    "ChatGPT &lt;chatgpt@openai.com&gt;",
    "trash &lt;trash@trash.dev&gt;",
    "Hacksore &lt;sean@boult.me&gt;"
  ],
  "scripts": {},
  "keywords": [
    "everything",
    "allthethings",
    "everymodule"
  ],
  "license": "MIT",
  "homepage": "https://github.com/everything-registry/everything",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/everything-registry/everything.git"
  },
  "dependencies": {
    "@everything-registry/chunk-0": "0.1.0",
    "@everything-registry/chunk-1": "0.1.0",
    "@everything-registry/chunk-2": "0.1.0",
    "@everything-registry/chunk-3": "0.1.0",
    "@everything-registry/chunk-4": "0.1.0"
  }
}</code></pre><h3>Echoes of the Past</h3><p>This isn't the first time we've seen such a stunt. Last year, the <a href="https://socket.dev/npm/package/no-one-left-behind/overview/2018.2.10"><code>no-one-left-behind</code></a> package by <a href="https://socket.dev/npm/user/zalastax">Zalastax</a> attempted something similar. It was removed, but then reemerged under a different scope with over 33,000 sub-packages. It's like playing whack-a-mole with npm packages!</p><p>It’s also reminiscent of a package called “hoarders” that used to directly depend on every module on npm (approximately 20,000 in 2012). It was published by software engineer Josh Holbrook, created to be “node.js's most complete utility grab bag.”</p><p>In an effort to maintain a secure and reliable ecosystem for JavaScript developers, <a target="_blank" rel="noopener noreferrer" href="https://github.com/jfhbrook/hoarders#history">hoarders was effectively “cancelled”</a> by Isaac Schlueter (creator of the npm package manager) after a year, due to the strain it caused on the registry's database.</p><h3>Unintended Consequences</h3><p>The "everything" package, with its 5 sub-packages and thousands of dependencies, has essentially locked down the ability for authors to unpublish their packages. This situation is due to npm's policy shift following the infamous <a target="_blank" rel="noopener noreferrer" href="https://qz.com/646467/how-one-programmer-broke-the-internet-by-deleting-a-tiny-piece-of-code">"left-pad" incident in 2016</a>, where a popular package <a href="https://socket.dev/npm/package/left-pad"><code>left-pad</code></a> was removed, grinding development to a halt across much of the developer world. In response, npm tightened its <a target="_blank" rel="noopener noreferrer" href="https://docs.npmjs.com/policies/unpublish">rules around unpublishing</a>, specifically preventing the unpublishing of any package that is used by another package.</p><p>Ironically, this policy trapped PatrickJS in his own web. Upon realizing the impact of his prank, he attempted to remove the <code>everything</code> package but was unable to do so. He reached out to the npm support team for help, but the damage was done.</p><p>PatrickJS wrote this apology on GitHub in a <a target="_blank" rel="noopener noreferrer" href="https://github.com/everything-registry/everything/issues/17">since-removed GitHub issue</a>:</p><blockquote>Hi all! First, just want to apologize about any difficulties this package has caused. We are working to resolve the issues and we have contacted NPM regarding support with this matter (see below). We appreciate your patience.<p>The major issue here is that when a package depends on another package at a specific version, that version cannot be unpublished. We've since realized there is an issue with "star" versions - a.k.a depending on any/all versions of another package ( "package-xyz": "*" ) - any version of that package is now unable to unpublish. As I previously mentioned, we've reached out to npm and are hoping they can either A) allow folks to unpublish when the packages that depend on them use a "star" version, B) not permit star versions in published packages going forward, or as a last resort, C) remove our npm organization entirely (and remove all of the packages that are blocking unpublishing). As far as we can tell, there is simply nothing we can do on our own - we can't unpublish the packages ourselves (because other packages depend on them) and publishing a new version over them doesn't change anything.</p></blockquote><p>However, we now see that while <code>everything</code> remains on the registry, the <code>@everything-registry</code> scoped packages have been made private, potentially offering a resolution.</p><h3>The Ripple Effect</h3><p>This whole saga is more than just a digital prank. It highlights the <a href="https://socket.dev/blog/inside-node-modules">ongoing challenges</a> in package management within the npm ecosystem. For developers, it's a reminder of the cascading effects of dependencies and the importance of mindful package creation, maintenance, and consumption.</p><p><img alt=" " loading="lazy" src="https://cdn.sanity.io/images/cgdhsj6q/production/14461d25dd2f7a1cf456a840fe3bc1e98670e3e0-2162x1902.png?w=1600&amp;fit=max&amp;auto=format"></p><p>As we navigate the open source world, incidents like the <code>everything</code> package remind us of the delicate balance between freedom and responsibility in open-source software.</p><p>Install <a href="https://socket.dev/github-app">Socket for GitHub</a> to stay secure this year, and let's see what the rest of 2024 has in store for us!</p><p><em>h/t <a target="_blank" rel="noopener noreferrer" href="https://twitter.com/ax_sharma">Ax Sharma</a></em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ultima (2012) (174 pts)]]></title>
            <link>https://www.filfre.net/2012/02/ultima-part-1/</link>
            <guid>38894397</guid>
            <pubDate>Sat, 06 Jan 2024 19:19:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.filfre.net/2012/02/ultima-part-1/">https://www.filfre.net/2012/02/ultima-part-1/</a>, See on <a href="https://news.ycombinator.com/item?id=38894397">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
														<p>When we left Richard Garriott, California Pacific had just released his first game, <em>Akalabeth</em>, a substantial windfall for the 19-year-old university student. In between classes and SCA events, he spent his sophomore year at the University of Texas writing a new, much more ambitious game, which CP published just as the spring semester of 1981 was wrapping up. I think we can best proceed by just diving right into the game that retroactively came to be known as <em>Ultima I</em>.</p>
<p>Like <em>Zork</em>, making <em>Ultima</em> available here presented a bit of an ethical dilemma for me. You can actually now <a href="https://track.adtraction.com/t/t?a=1578845458&amp;as=1821351176&amp;t=2&amp;tk=1&amp;url=https://www.gog.com/en/game/ultima_1_2_3">buy the first three <em>Ultima</em> games</a> again via GOG.com, a service I can hardly applaud enough for keeping deep catalog works like these in print in a form easily runnable on modern PCs. However, the version they sell is the Origin Systems remake from 1986. It’s much more polished and playable than the original that Garriott wrote in BASIC on his Apple II Plus, but it’s of course also something of an anachronism for a digital antiquarian like me. So, I’m going to go ahead and offer here <a href="https://www.filfre.net/misc/ultima.zip">the original California Pacific <em>Ultima</em></a> as Apple II disk images along with the original accompanying documentation, at least until someone tells me not to. If you’re following closely along with my journey into the game, or want to do some digital archaeology of your own, have at it. If, on the other hand, you’re a bit less hardcore but your interest is piqued enough to want to give <em>Ultima</em> a shot, by all means go for the much more playable and accessible version you’ll find on Good Old Games — no emulator required.</p>
<p><a href="https://www.filfre.net/wp-content/uploads/2012/02/u1-artwork.jpg"><img fetchpriority="high" decoding="async" title="Ultima box art" src="https://www.filfre.net/wp-content/uploads/2012/02/u1-artwork-235x300.jpg" alt="" width="235" height="300" srcset="https://www.filfre.net/wp-content/uploads/2012/02/u1-artwork-235x300.jpg 235w, https://www.filfre.net/wp-content/uploads/2012/02/u1-artwork.jpg 457w" sizes="(max-width: 235px) 100vw, 235px"></a></p>
<p>By the standards of later <em>Ultima</em>s, the packaging of <em>Ultima I</em> is spartan: the two disks, a very to-the-point 10-page manual, and a player reference card that, oddly, includes important information not included in the manual (and vice versa). No lengthy books of lore, no cloth maps, no ankh medallions. Yet by the standards of its time, in which games were just transitioning from Ziploc baggies to more professional packaging (a symptom of the slowly encroaching professionalization of the industry as a whole), it’s a fairly generous production. More ephemerally, this first <em>Ultima</em> experience <em>feels</em> like the CRPG experience that so many fans would come to know over the next decade, the era <a href="http://www.amazon.com/Dungeons-Desktops-History-Computer-Role-playing/dp/1568814119/ref=sr_1_1?ie=UTF8&amp;qid=1328894843&amp;sr=8-1">Matt Barton calls</a> the “Golden Age” of the CRPG: a <em>big</em> experience promising many hours of adventure from its garishly illustrated outside to the multiple disks found inside. (In fact, <em>Ultima</em> is the earliest game I know of to spill across more than a single disk side.) And that impression stems from more than just the details of its presentation. If <em>Zork</em> in some sense perfected the text adventure by hitting upon a robust approach to interactive fiction that still persists to this day, <em>Ultima</em>, one could argue, did much of the same for the CRPG. Like <em>Zork</em>, <em>Ultima</em> is perhaps the first example of its form that one might actually want to play today just to, you know, <em>play</em>. So let’s boot our Apple II and have at it, shall we?</p>
<p><a href="https://www.filfre.net/wp-content/uploads/2012/02/93.png"><img decoding="async" title="Ultima" src="https://www.filfre.net/wp-content/uploads/2012/02/93.png" alt="" width="280" height="192"></a></p>
<p>Until very shortly before its release, <em>Ultima</em> was not called <em>Ultima</em>, but rather <em>Ultimatum</em>. We can see evidence of this by listing the directories of the disks themselves; the file holding the title screen you see above is still titled “PIC.ULTIMATUM.” Why choose that name? Like so much in Garriott’s early games, simply because it sounded cool; certainly this title has no more bearing on the game’s plot, such as it is, than does the name <em>Ultima</em>. The change was made when Garriott and California Pacific discovered that there was already <a href="http://www.boardgamegeek.com/boardgame/3677/ultimatum">a tabletop war game in print</a> under the name <em>Ultimatum</em>. Wishing to avoid confusion and legal complications, it was Al Remmers of CP who suggested that they shorten the name to simply <em>Ultima</em> because, once again, it sounded cool. (Later apologists’ attempts to construe the name as a reference to the semi-mythical classical land of <a href="http://en.wikipedia.org/wiki/Ultima_Thule">ultima Thule</a> are about as convincing as their attempts to construct a coherent narrative arc out of the random smorgasbord of plot and setting of the first three <em>Ultima</em> games.) Remmers, you may remember, also suggested that Garriott take his occasional nickname Lord British as his nom de plume, drumming up a promotional campaign for <em>Akalabeth</em> depicting Lord British as a reclusive and enigmatic genius. It’s ironic that Remmers, a guy that Garriott didn’t know that well and with whom he would soon have an ugly falling out, essentially created the two brand names for which Garriott will forever be remembered, while he himself faded quickly into obscurity. It’s also emblematic of the uncanny luck that seemed to follow young Garriott around, luck which brought various older and (possibly) wiser men to further his career almost in spite of themselves. Remember also John Mayer, his ComputerLand manager who convinced him to sell <em>Akalabeth</em> in the store and by some accounts was responsible for bringing it to the attention of Remmers and CP…</p>
<p><a href="https://www.filfre.net/wp-content/uploads/2011/12/Akalabeth-1980_000000000.png"><img decoding="async" title="Akalabeth" src="https://www.filfre.net/wp-content/uploads/2011/12/Akalabeth-1980_000000000.png" alt="" width="280" height="192"></a> <a href="https://www.filfre.net/wp-content/uploads/2012/02/94.png"><img decoding="async" title="Ultima" src="https://www.filfre.net/wp-content/uploads/2012/02/94.png" alt="" width="280" height="192"></a></p>
<p>Just like <em>Akalabeth</em>, <em>Ultima</em> — shown on the right in the comparison above — dumps us into an overhead view of the outdoor landscape after we create our character. Unlike in <em>Akalabeth</em>, we now have monsters to contend with out here as well as in the dungeons. And if <em>Ultima</em> is still not exactly a graphical extravaganza, things sure do look a whole lot better than before, thanks largely to the game’s major technical innovation: tile graphics.</p>
<p><em>Ultima</em>‘s world is a pretty big one, spanning four continents each many times the length and width of a single screen. At a resolution of 280 X 160, trying to draw all of this at the level of individual pixels would be untenable, both technically (even two disk sides couldn’t possibly store that much information) and practically (Garriott was just one guy, and not really an artist either; nor was the the Apple II’s library of graphics software terribly mature by this point). The solution was to draw the world using a collection of pre-rendered tiles, each 14 X 16 pixels. Each screen is thus formed from 200 of these tiles, in rows of 20 and columns of 10, laid together in a process that would feel kind of similar to doing a jigsaw puzzle or playing a tile-laying board game like <em>Carcassonne</em>. <em>Ultima</em>‘s world map is represented on the computer as just a grid of numbers specifying which tile should be slotted into which position by the graphics engine. It’s often claimed that <em>Ultima</em> represents the very first application of this technique that was soon everywhere in videogames of the 1980s, one that still crops up more than you might expect even today. Being a skeptical bastard by nature, I do wonder that no one thought of it in even the relatively brief history of videogames prior to <em>Ultima</em>; it does seem a fairly obvious approach, after all. On the other hand, I can’t point to a specific example that would give me grounds to really challenge the claim. As always, post ’em (or comment ’em) if you got ’em.</p>
<p><em>Ultima</em>‘s tile-graphics engine was not so much the work of Garriott as of a friend of his who was the only other person to have a significant role in the game’s design and implementation: Ken W. Arnold (<em>not</em> the Ken Arnold who created <em>Rogue</em>). A neighborhood chum of Garriott’s, Arnold worked at the same ComputerLand store where Garriott spent that fateful summer of 1980. The two sketched out the initial plan for the game together when Garriott, excited by the sale of <em>Akalabeth</em> to California Pacific and beginning to realize he could make money at this stuff, began work on <em>Ultima</em> even before leaving again for university. Arnold not only invented the tile graphics scheme but also handled the technical implementation, writing an assembly-language routine to fetch the tiles and rapidly paint them onto the screen as the player moves about the world. This routine, along with another to generate the game’s simple combat sound effects, were the only parts of <em>Ultima</em> not to be written in BASIC. Garriott, unlike Arnold, had not yet learned assembly language, and thus implemented everything else in BASIC after leaving Arnold, Houston, and ComputerLand to return to university in Austin.</p>
<p>Even with the tile system, creating <em>Ultima</em>‘s graphics was a challenge. From <em>The Official Book of Ultima</em>:</p>
<blockquote><p>“We had to actually enter all the shapes in hex,” Garriott says, detailing the primitive process. First he and Arnold would draw them out on graph paper, then convert the graphs to binary, which in turn had to be reversed because the pixels appeared on the screen backwards. After converting it into hex, they entered the tile as data, stored it on disk, and then ran it to see if it looked right on the screen. “We had no editors or anything, so it was a very painful thing.”</p></blockquote>
<p>Indeed, one suspects that, even in the context of 1980-81, easier ways could have been devised. Put another way, young Richard and Ken had not yet learned the value of making programs to make programs. Still, stories like the above illustrate one of the most remarkable things about these early games of Garriott’s: they were created by a self-taught kid who literally figured things out as he went along, working on a single Apple II and with none of the technical background or resources of an Infocom or even an On-Line Systems or Muse to call upon. Their ramshackle technological underpinnings may be less elegant than the Z-Machine, but they are in their own way just as remarkable. In a very real sense it’s amazing that <em>Ultima</em> exists at all.</p>
<p>The world all of their labor lets us explore is based upon Garriott’s latest <em>Dungeons and Dragons</em> campaign world circa mid-1980, which he called Sosaria; he literally transcribed his <em>D&amp;D</em> maps right into the game. As we’ll soon see, Sosaria is not exactly the most coherent of milieu. A person could also say that gameplay has not progressed all that much beyond <em>Akalabeth</em>: we still move around the wilderness map to visit towns (for our shopping needs), castles (for quests), and dungeons (for critter bashing). One is reminded once again of Garriott’s joking comment that he spent some 15 years making the same game again and again. The person who said Garriott hadn’t progressed much would be pretty unfair, however, because much has changed here too. Virtually everything is now bigger and more fleshed out, and there’s a big overarching quest to solve. In fact, the whole philosophy of the game has moved from the <em>Akalabeth</em> approach of being a relatively short, replayable experience to the extended, save-game-enabled epic journey CRPG fans would soon come to associate with the name <em>Ultima</em>.</p>
							
							
														
													</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Adventures of Linux Userspace at Meta [video] (116 pts)]]></title>
            <link>https://media.ccc.de/v/all-systems-go-2023-193-adventures-of-linux-userspace-at-meta</link>
            <guid>38894208</guid>
            <pubDate>Sat, 06 Jan 2024 19:02:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://media.ccc.de/v/all-systems-go-2023-193-adventures-of-linux-userspace-at-meta">https://media.ccc.de/v/all-systems-go-2023-193-adventures-of-linux-userspace-at-meta</a>, See on <a href="https://news.ycombinator.com/item?id=38894208">Hacker News</a></p>
<div id="readability-page-1" class="page">

<div>
<ol>
<li>
<a href="https://media.ccc.de/b">
browse
</a>
</li>
<li>
<span></span>
<a href="https://media.ccc.de/b/conferences">
conferences
</a>
</li>
<li>
<span></span>
<a href="https://media.ccc.de/b/conferences/all_systems_go">
all_systems_go
</a>
</li>
<li>
<span></span>
<a href="https://media.ccc.de/b/conferences/all_systems_go/asg2023">
asg2023
</a>
</li>
<li>
<span></span>
event
</li>
</ol>
</div>

<div>

<p>
<span></span>
<a href="https://media.ccc.de/search?p=Anita+Zhang">Anita Zhang</a>

</p>
<p>
Playlists:
<a href="https://media.ccc.de/v/all-systems-go-2023-193-adventures-of-linux-userspace-at-meta/playlist">'asg2023' videos starting here</a>
/
<a data-method="get" href="https://media.ccc.de/v/all-systems-go-2023-193-adventures-of-linux-userspace-at-meta/audio">audio</a></p>
<!-- %h3 About -->
<p>The Linux Userspace team at Meta aims to make significant contributions to upstream userspace projects, while also ensuring that Meta is able to leverage those improvements. In this talk we'll give an overview of the team and brief history of how it was formalized. Then we'll dive deeper into some of the efforts we've worked on with the open source community and features we've adopted internally. Come if you enjoy hearing about systemd, BPF, distributions, and more!</p>

<h3>Download</h3>
<div>
<p>
<h4>Audio</h4>
</p>

</div>
<!-- %h3 Embed/Share -->

<h3>Tags</h3>

</div>





</div>]]></description>
        </item>
        <item>
            <title><![CDATA[FAA orders grounding of more than 170 Boeing 737 Max 9s (795 pts)]]></title>
            <link>https://www.cnbc.com/2024/01/06/boeing-737-max-9-grounding-after-alaska-airlines-door-blows-midflight.html</link>
            <guid>38893909</guid>
            <pubDate>Sat, 06 Jan 2024 18:36:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/01/06/boeing-737-max-9-grounding-after-alaska-airlines-door-blows-midflight.html">https://www.cnbc.com/2024/01/06/boeing-737-max-9-grounding-after-alaska-airlines-door-blows-midflight.html</a>, See on <a href="https://news.ycombinator.com/item?id=38893909">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-6" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-6-2"><div id="ArticleBody-InlineImage-107355129" data-test="InlineImage"><p>Passenger oxygen masks hang from the roof next to a missing window and a portion of a side wall of an Alaska Airlines Flight 1282, which had been bound for Ontario, California and suffered depressurization soon after departing, in Portland, Oregon, U.S., on Jan. 5, 2024, in this picture obtained from social media.</p><p>Instagram/@strawberrvy | Instagram/@strawberrvy Via Reute</p></div><div><p>The Federal Aviation Administration on Saturday ordered airlines to ground more than 170 <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/BA/">Boeing</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> 737 Max 9 aircraft for inspections, a day after after a panel on one blew out in the middle of an <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-2"><a href="https://www.cnbc.com/quotes/ALK/">Alaska Airlines</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> flight.</p><p>The emergency airworthiness directive will affect about 171 planes worldwide and applies to U.S. airlines and carriers operating in U.S. territory.</p><p><a href="https://www.cnbc.com/2024/01/06/alaska-airlines-grounds-boeing-737-max-9-fleet-after-section-blows-out-midair.html">Alaska Airlines Flight 1282</a> was bound for Ontario, California when it returned to Portland, Oregon shortly after takeoff on Friday after a pressurization issue was detected. No serious injuries were reported on the flight, according to federal safety officials. The flight returned to Portland, Oregon, shortly after takeoff on Friday after a pressurization issue was reported.</p><p>Images and video of Alaska's <a href="https://www.cnbc.com/quotes/BA/">Boeing</a>&nbsp;737 Max 9 shared on social media showed a gaping hole on the side of the plane and passengers using oxygen masks before it returned to Portland.</p><p>"Safety will continue to drive our decision-making as we assist the NTSB's investigation into Alaska Airlines Flight 1282," FAA Administrator Mike Whitaker said in a statement.</p><p>Alaska Airlines overnight said it would ground its fleet of Boeing 737 Max 9 planes. Alaska said on Saturday morning, local time, that it had completed inspections on more than a quarter of its 737 Max 9 fleet "with no concerning findings."</p><p>"Aircraft will return to service as their inspections are completed with our full confidence," the Seattle-based airline said.</p><p>The National Transportation Safety Board sent a team to Portland on Saturday to investigate the incident.</p><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-5"><a href="https://www.cnbc.com/quotes/UAL/">United Airlines</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, the largest operator of the planes in the U.S., was preparing to ground dozens of its <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-6"><a href="https://www.cnbc.com/quotes/BA/">Boeing</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> 737 Max 9 aircraft for inspections, CNBC reported earlier Saturday.</p><p>United has close to 80 of the planes in its fleet, though some of the jets have recently undergone in-depth, routine inspections.</p><p>The FAA said the inspections will take between four and eight hours per plane, the agency said.</p><p>The Boeing 737 Max 9 is a larger version of Boeing's best-selling jetliner, the 737 Max 8. Max planes were grounded worldwide in 2019 after two fatal crashes within five months. The U.S. lifted its flight ban of the jets in late 2020 after software and training updates.</p><p>Large-scale groundings of aircraft by the FAA or other aviation authorities are rare.</p><p>The Boeing 737 Max 9 has an emergency exit door cut behind the wings for use in dense seating cabin configurations, like those used by budget airlines, according to Flightradar24.</p><p>"The doors are not activated on Alaska Airlines aircraft and are permanently 'plugged,'" Flightradar23 said.</p><p>Boeing didn't to comment beyond its statement when asked about the sealed emergency exit door. <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-7"><a href="https://www.cnbc.com/quotes/SPR/">Spirit AeroSystems</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, which makes the fuselages for the planes, referred CNBC to Boeing when asked about the incident.</p><p>"Safety is our top priority and we deeply regret the impact this event has had on our customers and their passengers," Boeing said in a statement on Saturday. "We agree with and fully support the FAA's decision to require immediate inspections of 737-9 airplanes with the same configuration as the affected airplane."</p><p>The company said it is supporting the NTSB's investigation.</p><p>There are 215 Boeing 737 Max 9 planes in service worldwide, according to aviation-data firm Cirium.</p><p>Late last year, Boeing urged airlines to&nbsp;<a href="https://www.cnbc.com/2023/12/28/boeing-urges-inspections-of-737-max-planes-for-possible-loose-bolt.html">inspect aircraft</a>&nbsp;for a "possible" loose bolt in the rudder control system, the latest in a series of manufacturing flaws on Boeing jets that have prompted additional inspections.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FAA grounds 171 Boeing planes after mid-air blowout (175 pts)]]></title>
            <link>https://www.bbc.com/news/world-us-canada-67903655</link>
            <guid>38893811</guid>
            <pubDate>Sat, 06 Jan 2024 18:26:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/world-us-canada-67903655">https://www.bbc.com/news/world-us-canada-67903655</a>, See on <a href="https://news.ycombinator.com/item?id=38893811">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header></header><div data-component="video-block"><figure><figcaption><span>Media caption, </span><p>Watch: 'Trip from hell': On board flight during mid-air blow out</p></figcaption></figure></div><div data-component="text-block"><p><b>The US airline regulator has ordered the grounding of some Boeing 737 Max 9 jets after part of one plane fell off during an Alaska Airlines flight.</b></p></div><div data-component="text-block"><p>The Federal Aviation Administration (FAA) said the inspections would affect 171 planes.</p></div><div data-component="text-block"><p>On Friday the Alaska Airlines flight had to make an emergency landing after take-off from the US state of Oregon.</p></div><div data-component="text-block"><p>United Airlines says it has carried out the inspections required by the FAA on some of its 79 Boeing 737 Max 9 planes.</p></div><div data-component="text-block"><p>Removing some of the aircraft from service is expected to cause about 60 cancellations on Saturday, the airline said in a statement.</p></div><div data-component="text-block"><p>Earlier, the FAA said it would "order the temporary grounding of certain Boeing 737 Max 9 aircraft operated by US airlines or in US territory". </p></div><div data-component="text-block"><p>Required inspections will take around four to eight hours per aircraft, it said.</p></div><div data-component="text-block"><p>In Friday's incident, the Alaska Airlines flight from Portland, Oregon to Ontario, California, had reached 16,000ft (4,876m) when it began its emergency descent, according to flight tracking data.</p></div><div data-component="text-block"><p>The airline, carrying 177 passengers and crew, landed safely back in Portland.</p></div><div data-component="text-block"><p>Images sent to news outlets showed the night sky visible through the gap in the fuselage, with insulation material and other debris also seen.</p></div><div data-component="text-block"><p>There were no immediate indications of the cause of the apparent structural failure, nor any reports of injuries.</p></div><div data-component="text-block"><p>Evan Smith, one of the 171 passengers on board, said: "There was a really loud bang towards the left rear of the plane and a woosh noise - and all the air masks dropped.</p></div><div data-component="text-block"><p>"They said there was a kid in that row who had his shirt sucked off him and out of the plane and his mother was holding onto him to make sure he didn't go with it."</p></div><div data-component="video-block"><figure><figcaption><span>Media caption, </span><p>Listen: Alaska flight's distress call to air traffic control</p></figcaption></figure></div><div data-component="text-block"><p>In an audio clip, the pilot can be heard talking to air traffic control requesting a diversion.</p></div><div data-component="text-block"><p>"We are an emergency," she said. "We are depressurised, we do need to return back."</p></div><div data-component="text-block"><p>According to photographs, the affected area was in the back third of the plane, behind the wing and engines.</p></div><div data-component="text-block"><p>The section of fuselage involved appears to be an area that can be used as an additional emergency exit door by some operators, but not by Alaska.</p></div><div data-component="text-block"><p>Announcing the initial grounding of 65 planes, Alaska Airlines' CEO Ben Minicucci said: "Each aircraft will be returned to service only after completion of full maintenance and safety inspections."</p></div><div data-component="text-block"><p>A later statement said that more than a quarter of those planes had been inspected and would return to service as there were no issues found.</p></div><div data-component="text-block"><p>In a statement, Boeing said it supported the FAA's decision. and was co-operating with the National Transportation Safety Board's investigation of the Alaska Airlines incident.</p></div><div data-component="text-block"><p>"Safety is our top priority and we deeply regret the impact this event has had on our customers and their passengers," Boeing said.</p></div><div data-component="text-block"><p>It is the latest problem involving Boeing's best-selling model, which was grounded for almost two years following crashes in 2018 and 2019. </p></div><section data-component="links-block"><p><h2>More on this story</h2></p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Westworld: The first film with CGI (and its source code) (125 pts)]]></title>
            <link>https://behind-the-screens.tv/#ww1</link>
            <guid>38893684</guid>
            <pubDate>Sat, 06 Jan 2024 18:13:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://behind-the-screens.tv/#ww1">https://behind-the-screens.tv/#ww1</a>, See on <a href="https://news.ycombinator.com/item?id=38893684">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img src="https://behind-the-screens.tv/header.png">
</p>

<div><p>
Back in 2017, I started a YouTube channel based upon my Tumblr <a href="https://moviecode.tumblr.com/">Movie Code</a>. Movie Code shows real source code appearing in films and television and the YouTube channel went deeper into some of that source code to show how it works and where it came from.
</p><p>
I removed my channel from YouTube in 2022 and uploaded all the videos here.
</p></div>
<hr>
<p>
<h2 id="airwolf">Fixing a bug in "Airwolf" code from 1984</h2>
</p>
<div><p>
Back in 1984 the Apple ][ was an exciting computer. So exciting that it played a key role in the Airwolf episode "Moffett's Ghost". Unfortunately, the code, like Airwolf itself, is buggy. Luckily we can fix the bug... 30 years on.
</p><p>
<a href="http://linapple.sourceforge.net/">LinApple Apple ][ emulator</a> /
<a href="https://gist.github.com/jgrahamc/926a7854c2668f20edb58fe62ac3f99f">Source code</a> used in Airwolf /
Tumblr <a href="https://moviecode.tumblr.com/post/72844286142/in-episode-3-of-series-2-of-airwolf-computer">entry</a>.
</p></div>

<hr>
<p>
<h2 id="sv">The Silicon Valley S03E01 Easter Egg code explained</h2>
</p>
<div><p>
Episode 1 of Season 3 of HBO's Silicon Valley contained a cool Easter Egg that was supposedly the Pied Piper compression library. This episode explains the Easter Egg and how it works.
</p><p>
Tumblr <a href="https://moviecode.tumblr.com/post/143722848307/silicon-valley-s3e1-looks-like-custom-c-code">entry</a> /
Reddit <a href="https://www.reddit.com/r/SiliconValleyHBO/comments/4gbra7/i_got_the_silicon_valley_s03e01_code_to_compile/">discussion</a> /
The <a href="https://gist.github.com/jgrahamc/4174885c1e443dc2cf89a7fc1200f8b5">code</a>.
</p></div>

<hr>
<p>
<h2 id="jurassic">The code Dennis Nedry used to shut down Jurassic Park's security system</h2>
</p>
<div><p>
In Jurassic Park, Dennis Nedry writes some code to cover his tracks. We see a glimpse of his code before it disappears from his screen when he sets his plan in action. But what was it? In this video, I track the code down.
</p><p>
Tumblr entries: <a href="https://moviecode.tumblr.com/post/75454781347/jurassic-park-nedrys-screen-sorry-for-the-poor">1</a>, <a href="https://moviecode.tumblr.com/post/94130118095/jurassic-park-1993-nedrys-workstation-as-he">2</a> /
<a href="https://basilisk.cebix.net/">Basilisk II</a> /
Previous blogs on this code: <a href="http://www.joecullin.com/site/2012/01/jurassic-park-source-code/">1</a>, <a href="https://movies.stackexchange.com/questions/28030/source-code-on-nedrys-workstation-real-programming-language-s">2</a> /
Reddit <a href="https://www.reddit.com/r/JurassicPark/comments/10u3wn/nedryland_dennis_you_sly_fox/">discussion</a> /
<a href="http://dkallen.org/DKABio.htm">Dan Allen</a>.
</p></div>

<hr>
<p>
<h2 id="terminator">The origin of all The Terminator's source code revealed</h2>
</p>
<div><p>
James Cameron's classic 1984 film The Terminator contained glimpses of code for the Apple ][. In this episode the real origin of that code and what it really does is revealed for the first time.
</p><p>
Tumblr <a href="https://moviecode.tumblr.com/post/72085828946/in-the-film-terminator-the-hud-shows-a-listing-of">entry</a> /
Mentions of the code being from Nibble Magazine: <a href="http://www.computerworld.com/article/2477933/mac-os-x/do-androids-dream-of-electric-apples-.html">1</a>, <a href="https://www.marksimonson.com/notebook/view/TheAppleTerminatorII">2</a>.
</p></div>

<hr>
<p>
<h2 id="ww1">Westworld: the first film with CGI (and its source code)</h2>
</p>
<div><p>
The 1973 film Westworld was the first film to feature computer graphics. These were used to show how the robots saw the world. But Westworld also had something else... computer code. And it's computer code with a surprising source.
</p><p>
Tumblr entries: <a href="https://moviecode.tumblr.com/post/153763691128/screenshot-from-the-movie-westworld-1973">1</a>, <a href="https://moviecode.tumblr.com/post/153807271773/screenshot-2-taken-from-the-movie-westworld-1973">2</a> and <a href="https://moviecode.tumblr.com/post/153850533676/screenshot-3-taken-from-the-movie-westworld-1973">3</a> /
FR-80 <a href="http://www.chilton-computing.org.uk/acl/literature/manuals/fr80/contents.htm">documentation</a> /
FR-80 Systems Software <a href="https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19760011736.pdf">documentation</a> /
PDP-15 assembly <a href="http://bitsavers.informatik.uni-stuttgart.de/pdf/dec/pdp15/DEC-15-AMZA-D_MACRO15.pdf">manual</a> /
“Creating the Special Effects for ‘Westworld’”. American Cinematographer, November 1973: <a href="https://www.questia.com/read/1P3-1323938781/creating-the-special-effects-for-westworld">here</a>.
</p></div>

<hr>
<p>
<h2 id="kitt">Unmasking the authors of KITT's code</h2>
</p>
<div><p>
In this video I run some code that appears in one episode of Knight Rider and produces an effect in a different episode and I dig into the code to reveal who wrote it.
</p><p>
Spotted by: <a href="http://www.aldenbates.com/archives/2007/01/28/kitt_was_programmed_in_basic.html">1</a>, <a href="https://ao2.it/en/blog/2014/08/25/kitt-showing-applesoft-basic-program">2</a>, and <a href="https://www.youtube.com/watch?v=4ArRw8c7tIo">3</a> /
Tumblr <a href="https://moviecode.tumblr.com/post/98122062303/knight-rider-s03e01-e02-kitt-showing-an-applesoft">entry</a> /
<a href="https://web-beta.archive.org/web/20040906153345/http://accad.osu.edu:80/~waynec/history/PDFs/universal.pdf">Universal Studios Computer Graphics</a> /
The <a href="https://gist.github.com/jgrahamc/fe64fac1fc9cb46d6fbc82a7db440af2">code</a>.
</p></div>

<hr>
<p>
<h2 id="exmachina">The hidden meaning of "Blue Book" in Ex Machina</h2>
</p>
<div><p>
Blue Book isn't just the name of Nathan Bateman's company, it's also a double Easter Egg in Ex Machina. The hidden meaning is locked inside a program shown briefly on screen. In this video I'll show the true meaning of Blue Book and how it's clever Easter Egg by the director.
</p><p>
Reddit <a href="https://www.reddit.com/r/movies/comments/365f9b/secret_code_in_ex_machina/">discussion</a> /
Tumblr <a href="https://moviecode.tumblr.com/post/119171520870/in-the-movie-ex-machina-which-is-really-great">entry</a> /
The <a href="https://www.amazon.co.uk/Embodiment-inner-life-Cognition-Consciousness/dp/0199226555">book</a> /
Android <a href="http://elinux.org/Android_Power_Management">code</a> /
<a href="https://www.scientificamerican.com/article/the-inner-lives-of-robots-an-interview-with-filmmaker-alex-garland/">Interview</a> with the director.
</p></div>

<hr>
<p>
<h2 id="d83">My descent into 8-bit insanity running code from Deutschland 83</h2>
</p>
<div><p>
In the TV series Deutschland 83 some IBM BASIC code was shown. In this episode I run it and then try to determine precisely which version of IBM BASIC from the early 1980s was used. I do that by looking at the output of the random number generator.
</p><p>
Tumblr <a href="https://moviecode.tumblr.com/post/137602974251/from-deutschland-83-series-1-episode-3-the">entry</a> /
<a href="https://gist.github.com/jgrahamc/901b26c6fe8f5af6a9622396b991203e">Source code</a> /
<a href="http://www.pcjs.org/">PCjs</a>.
</p></div>

<hr>
<p>
<h2 id="swordfish">Is Swordfish the worst hacking movie ever? Not quite!</h2>
</p>
<div><p>
Just how bad was Swordfish? Not as bad a you might think, at least when it comes to source code that's relevant to the plot. This video takes a look at about the only thing that makes sense in this 2001 hacking caper.
</p><p>
Tumblr <a href="https://moviecode.tumblr.com/post/72205913880/in-swordfish-lines-of-code-from-a-des-cracking">entry</a> /
Credits: <a href="https://twitter.com/tiborsaas/status/419471661113434112">1</a> and <a href="http://www.modestolan.com/~xwred1/swordfish-hack1.png">2</a> /
<a href="https://en.wikipedia.org/wiki/Data_Encryption_Standard">DES</a> /
DES cracking: <a href="http://www.ic.unicamp.br/~lucchesi/cracking-des/main.html">1</a> and <a href="http://www.ic.unicamp.br/~lucchesi/cracking-des/CH5/SEARCH.C">2</a> /
EFF <a href="https://w2.eff.org/Privacy/Crypto/Crypto_misc/DESCracker/HTML/19980716_eff_des_faq.html">DES Cracker</a> /
Image of EFF <a href="https://en.wikipedia.org/wiki/EFF_DES_cracker#/media/File:Board300.jpg">DES Cracker</a> /
Cracking <a href="http://www.ic.unicamp.br/~lucchesi/cracking-des/cracking-des.htm">book</a>.
</p></div>

<hr>
<p>
<h2 id="ironman">What is the Iron Man suit made of? Could it be Lego?</h2>
</p>
<div><p>
The original Iron Man suit made in a cave in Afghanistan needed some code to make it run. But where did that code come from?
</p><p>
Original Code: <a href="http://www.mralligator.com/rcx/firmdl3.c">here</a> /
RCX Tools: <a href="http://www.mralligator.com/rcx/tools.html">here</a> /
Oliver Jones' <a href="https://deeperdesign.wordpress.com/2010/02/26/is-iron-man-made-of-lego/">blog</a> /
Lego Mindstorms <a href="https://www.flickr.com/photos/gocarts/2451203237/in/photolist-4JB4jM-dTx2v-75tvXH-dTx7T-agENcA-5Jcdo">image</a>.
</p></div>

<hr>
<p>
<h2 id="martian">The NASA code hidden in The Martian and an unsolved mystery (Improved Audio)</h2>
</p>
<div><p>
The 2015 film "The Martian" contains lots of real NASA code and some code that hasn't been understood... yet. Perhaps with your help we can understand what Mark Watney was really looking at.
</p><p>
<a href="https://github.com/nasa/pvslib">PVSLib</a> /
<a href="https://shemesh.larc.nasa.gov/fm/">NASA Langley Formal Methods</a> /
NASA's <a href="https://shemesh.larc.nasa.gov/people/cam/TheMartian">page</a> on The Martian.
</p></div>

<hr>
<p>
<h2 id="rogue">The Mission: Impossible - Rogue Nation "data transfer" code explained</h2>
</p>
<div><p>
While Simon Pegg is stealing a list of Syndicate agents from a data center in Morocco we see some 'data transfer' code on screen. But what is it really? Could it be code for convective heat flow and to draw a mathematical equation? Yes! So, let's run it.
</p><p>
Tumblr <a href="https://moviecode.tumblr.com/post/132920059484/from-mission-impossible-rogue-nation-agent">entry</a> /
<a href="https://processing.org/">Processing</a> /
The Processing <a href="https://processing.org/examples/graphing2dequation.htmlTEXSTAN:%20http://www.texstan.com/">code</a>.
</p></div>

<hr>
<p>
<h2 id="spidey">A source code mystery in the Spider-Man: Homecoming trailer</h2>
</p>
<div><p>
The third trailer for Spider-Man: Homecoming contains a flash of source code and it's source code that appeared two years ago in an unrelated TV series. Why?
</p><p>
<a href="https://www.flashback.org/p54684291">Discussion</a> in Swedish about the code that appeared in Extant /
Spider-Man: Homecoming <a href="https://www.youtube.com/watch?v=U0D3AOldjMU">Trailer 3</a> /
Reddit <a href="https://www.reddit.com/r/itsaunixsystem/comments/6d0t8e/spiderman_homecoming_might_be_the_wrong_sub_but/">discussion</a>.
</p></div>

<hr>
<p>
<h2 id="ww2">The real GPS and ArduPilot code in HBO's Westworld S01E05: Contrapasso</h2>
</p>
<div><p>
The creators of HBO's Westworld have used real code and data in this episode. The GPS data comes from a real orbiting GPS satellite, and the code to control a bird from a real open source auto-pilot project called ArduPilot.
</p><p>
Tumblr <a href="https://moviecode.tumblr.com/post/153160245449/this-is-screenshot-from-hbos-westworld-season-1">entry</a> /
<a href="https://ngdc.noaa.gov/stp/space-weather/satellite-data/satellite-systems/gps//documentation/readme.pdf">NOAA file format</a> /
Example <a href="https://ngdc.noaa.gov/stp/space-weather/satellite-data/satellite-systems/gps/data/ns70/ns70_161204_v1.03.ascii">code</a> /
<a href="https://en.wikipedia.org/wiki/USA-154">SVN41</a> /
<a href="https://digital.library.unt.edu/ark:/67531/metadc711710/">BDD-IIR</a> /
Reddit <a href="https://www.reddit.com/r/itsaunixsystem/comments/5agcbi/west_world_bird_programming/">discussion</a> /
The ArduPilot <a href="https://github.com/ArduPilot/ardupilot/commit/92b0c302f2701ed2b88254e5f6593c40e7c8824b">change</a>.
</p></div>

<hr>
<p>
<h2 id="return">Resurrecting Apple ][ code from a 1980 B movie</h2>
</p>
<div><p>
"The Return" starring Jan-Michael Vincent and Cybill Shepherd is not a good movie. But it does have some 1980 code in it. Specifically, it's code for an Apple ][ and with a little work we can get it to run in an emulator.
</p><p>
Tumblr <a href="https://moviecode.tumblr.com/post/139772447102/from-1980-ufo-movie-the-return-part-of-the/embed">entry</a> /
Code <a href="https://gist.github.com/jgrahamc/c996de594b4faad8d0321c56ef674004">snippet</a> /
LinApple <a href="http://linapple.sourceforge.net/">emulator</a>.
</p></div>

<hr>
<p>
<h2 id="ww3">The five real robots in the teaser for Westworld</h2>
</p>


<hr>
<p>
<h2 id="stargate">The Stargate Replicators used code from a Canadian bank!</h2>
</p>
<div><p>
In Stargate: The Ark of Truth there are two shots of computer code. One of them is hilarious as it's not only irrelevant to the action but is real computer code from a web site that was probably being surfed by the visual effects crew.
</p><p>
Tumblr <a href="https://moviecode.tumblr.com/post/72208356646/in-stargate-sg1-the-ark-of-truth-there-is">entry</a> /
Twitter <a href="https://twitter.com/DandumontP/status/419448485243797504">link</a> /
Archive of Royal Bank of Canada web site from 2008: <a href="https://web.archive.org/web/20080101140200/http://www.rbcroyalbank.com/">here</a> /
Ostrosoft Winsock: <a href="http://ostrosoft.com/oswinsck.aspx">here</a> and the <a href="http://ostrosoft.com/oswinsck/oswinsck_vb.zip">sample seen on screen</a>.
</p></div>

<hr>
<p>
<h2 id="robocop">Robocop: The Librarian</h2>
</p>
<div><p>
In the 2014 film Robocop, the part machine, part human's real mission is revealed. He's not a cop, he's a librarian out to get you to return those overdue books. At least that's what his source code reveals!
</p><p>
Tumblr entries: <a href="https://moviecode.tumblr.com/post/86760434547/in-robocop-2014-we-can-see-that-the-main">1</a> and <a href="https://moviecode.tumblr.com/post/86475995241/source-code-on-robocop-2014-its-from-a-java">2</a> /
The library system: <a href="https://www.daniweb.com/programming/software-development/threads/183056/help-longest-error-message">here</a>.
</p></div>

<hr>
<p>
<h2 id="doctorwho">Secrets of a Doctor Who opening sequence revealed</h2>
</p>
<div><p>
The opening sequence of the Doctor Who episode "The Bells of Saint John" reveals far more about the visual designer who worked on the special effects than they probably ever realized.
</p></div>

<hr>
<p>
<h2 id="scorpion">A virus that's actually real code to control traffic lights</h2>
</p>
<div><p>
In the 2014 episode "Single Point of Failure" of the CBS series Scorpion, the daughter of the Governor of California is infected with a virus and so is her computer. But the code that flashes by on screen is really for controlling a set of traffic lights.
</p><p>
Links and References:
<a href="https://twitter.com/irrelevant_com/status/870059204656103424">Tweet</a> /
<a href="https://en.wikipedia.org/wiki/Structured_text">Structured Text</a> /
PDF containing <a href="http://aiut.tugab.bg/Library/ATP/DOC/StructuredText.pdf">the code seen on screen</a>.
</p></div>

<hr>
<p>
<h2>About</h2>
</p>
<div><p>
This site and all the videos were made by <a href="https://jgc.org/">John Graham-Cumming</a>.
</p></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NIST identifies types of cyberattacks that manipulate behavior of AI systems (103 pts)]]></title>
            <link>https://www.nist.gov/news-events/news/2024/01/nist-identifies-types-cyberattacks-manipulate-behavior-ai-systems</link>
            <guid>38893614</guid>
            <pubDate>Sat, 06 Jan 2024 18:06:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nist.gov/news-events/news/2024/01/nist-identifies-types-cyberattacks-manipulate-behavior-ai-systems">https://www.nist.gov/news-events/news/2024/01/nist-identifies-types-cyberattacks-manipulate-behavior-ai-systems</a>, See on <a href="https://news.ycombinator.com/item?id=38893614">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
         
  <figure data-lightbox="https://www.nist.gov/sites/default/files/styles/2800_x_2800_limit/public/images/2024/01/03/AI-evasion-rev-3.png?itok=msOI5gEd" data-media-id="696936">
        
    <img src="https://www.nist.gov/sites/default/files/styles/960_x_960_limit/public/images/2024/01/03/AI-evasion-rev-3.png?itok=H1xDCN22" width="960" height="566" alt="Overhead view of intersection shows how deceptive markings on the road could cause an AI-directed car to veer into oncoming traffic. " loading="lazy" typeof="foaf:Image">




            <figcaption>
      <p>An AI system can malfunction if an adversary finds a way to confuse its decision making. In this example, errant markings on the road mislead a driverless car, potentially making it veer into oncoming traffic. This “evasion” attack is one of numerous adversarial tactics described in a new NIST publication intended to help outline the types of attacks we might expect along with approaches to mitigate them.</p>
                    <p><span>Credit:</span>
          
  N. Hanacek/NIST

        </p>
          </figcaption>
  </figure>

  
  
  
      <p>Adversaries can deliberately confuse or even “poison” artificial intelligence (AI) systems to make them malfunction — and there’s no foolproof defense that their developers can employ. Computer scientists from the National Institute of Standards and Technology (NIST) and their collaborators identify these and other vulnerabilities of AI and machine learning (ML) in a new publication.</p>

<p>Their work, titled <a href="https://csrc.nist.gov/pubs/ai/100/2/e2023/final"><em>Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations</em> (NIST.AI.100-2)</a>, is part of NIST’s broader effort to support the development of <a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="7ec397c9-eabb-4ae3-b281-0a355cdffc32" href="https://www.nist.gov/artificial-intelligence/executive-order-safe-secure-and-trustworthy-artificial-intelligence" title="Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence">trustworthy AI</a>, and it can help put NIST’s AI Risk Management Framework into practice. The publication, a collaboration among government, academia and industry, is intended to help AI developers and users get a handle on the types of attacks they might expect along with approaches to mitigate them — with the understanding that there is no silver bullet.</p>

<p>“We are providing an overview of attack techniques and methodologies that consider all types of AI systems,” said NIST computer scientist Apostol Vassilev, one of the publication’s authors. “We also describe current mitigation strategies reported in the literature, but these available defenses currently lack robust assurances that they fully mitigate the risks. We are encouraging the community to come up with better defenses.” &nbsp;&nbsp;</p>

<p>AI systems have permeated modern society, working in capacities ranging from driving vehicles to helping doctors diagnose illnesses to interacting with customers as online chatbots. To learn to perform these tasks, they are trained on vast quantities of data: An autonomous vehicle might be shown images of highways and streets with road signs, for example, while a chatbot based on a large language model (LLM) might be exposed to records of online conversations. This data helps the AI predict how to respond in a given situation.&nbsp;</p>

<p>One major issue is that the data itself may not be trustworthy. Its sources may be websites and interactions with the public. There are many opportunities for bad actors to corrupt this data — both during an AI system’s training period and afterward, while the AI continues to refine its behaviors by interacting with the physical world. This can cause the AI to perform in an undesirable manner. Chatbots, for example, might learn to respond with abusive or racist language when their guardrails get circumvented by carefully crafted malicious prompts.&nbsp;</p>

<p>“For the most part, software developers need more people to use their product so it can get better with exposure,” Vassilev said. “But there is no guarantee the exposure will be good. A chatbot can spew out bad or toxic information when prompted with carefully designed language.”</p>

<p>In part because the datasets used to train an AI are far too large for people to successfully monitor and filter, there is no foolproof way as yet to protect AI from misdirection. To assist the developer community, the new report offers an overview of the sorts of attacks its AI products might suffer and corresponding approaches to reduce the damage.&nbsp;</p>

<p>The report considers the four major types of attacks: evasion, poisoning, privacy and abuse attacks. It also classifies them according to multiple criteria such as the attacker’s goals and objectives, capabilities, and knowledge.</p>

<p><strong>Evasion</strong> attacks, which occur after an AI system is deployed, attempt to alter an input to change how the system responds to it. Examples would include adding markings to stop signs to make an autonomous vehicle misinterpret them as speed limit signs or creating confusing lane markings to make the vehicle veer off the road.&nbsp;</p>

<p><strong>Poisoning</strong> attacks occur in the training phase by introducing corrupted data. An example would be slipping numerous instances of inappropriate language into conversation records, so that a chatbot interprets these instances as common enough parlance to use in its own customer interactions.&nbsp;</p>

<p><strong>Privacy</strong> attacks, which occur during deployment, are attempts to learn sensitive information about the AI or the data it was trained on in order to misuse it. An adversary can ask a chatbot numerous legitimate questions, and then use the answers to reverse engineer the model so as to find its weak spots — or guess at its sources. Adding undesired examples to those online sources could make the AI behave inappropriately, and making the AI unlearn those specific undesired examples after the fact can be difficult.</p>

<p><strong>Abuse</strong> attacks involve the insertion of incorrect information into a source, such as a webpage or online document, that an AI then absorbs. Unlike the aforementioned poisoning attacks, abuse attacks attempt to give the AI incorrect pieces of information from a legitimate but compromised source to repurpose the AI system’s intended use.&nbsp;</p>

<p>“Most of these attacks are fairly easy to mount and require minimum knowledge of the AI system and limited adversarial capabilities,” said co-author Alina Oprea, a professor at Northeastern University. “Poisoning attacks, for example, can be mounted by controlling a few dozen training samples, which would be a very small percentage of the entire training set.”&nbsp;</p>

<p>The authors — who also included Robust Intelligence Inc. researchers Alie Fordyce and Hyrum Anderson — break down each of these classes of attacks into subcategories and add approaches for mitigating them, though the publication acknowledges that the defenses AI experts have devised for adversarial attacks thus far are incomplete at best. Awareness of these limitations is important for developers and organizations looking to deploy and use AI technology, Vassilev said.&nbsp;</p>

<p>“Despite the significant progress AI and machine learning have made, these technologies are vulnerable to attacks that can cause spectacular failures with dire consequences,” he said. “There are theoretical problems with securing AI algorithms that simply haven’t been solved yet. If anyone says differently, they are selling snake oil.”&nbsp;</p>
  
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[BYD's YangWang U8 launched, can float on water for 30 minutes and sail 3km/h (153 pts)]]></title>
            <link>https://carnewschina.com/2023/09/20/byds-yangwang-u8-launched-can-float-on-water-for-30-minutes-and-sail-3km-h/</link>
            <guid>38893534</guid>
            <pubDate>Sat, 06 Jan 2024 17:58:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://carnewschina.com/2023/09/20/byds-yangwang-u8-launched-can-float-on-water-for-30-minutes-and-sail-3km-h/">https://carnewschina.com/2023/09/20/byds-yangwang-u8-launched-can-float-on-water-for-30-minutes-and-sail-3km-h/</a>, See on <a href="https://news.ycombinator.com/item?id=38893534">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-td-block-uid="tdi_157">
<p>On September 20, the <a href="https://carnewschina.com/tag/yangwang-u8/">YangWang U8</a> EREV SUV was launched in China. It is AWD with four electric motors and a combined output of 880 kW (1,197 horsepower). U8 can perform <a href="https://youtu.be/dkROmU9Bxbw?si=9at1doXOW79gZ-R5">360° tank turn</a>, wade, and float on water in emergencies.</p><p><span>- Advertisement -</span>
<ins data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-7299825303701007" data-ad-slot="5123236728"></ins>
</p>
<figure><img fetchpriority="high" decoding="async" width="800" height="533" src="https://carnewschina.com/wp-content/uploads/2023/09/Obru8-800x533.jpg" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/Obru8-800x533.jpg 800w, https://carnewschina.com/wp-content/uploads/2023/09/Obru8-300x200.jpg 300w, https://carnewschina.com/wp-content/uploads/2023/09/Obru8-120x80.jpg 120w, https://carnewschina.com/wp-content/uploads/2023/09/Obru8-768x512.jpg 768w, https://carnewschina.com/wp-content/uploads/2023/09/Obru8-696x464.jpg 696w, https://carnewschina.com/wp-content/uploads/2023/09/Obru8-1068x712.jpg 1068w, https://carnewschina.com/wp-content/uploads/2023/09/Obru8.jpg 1158w" sizes="(max-width: 800px) 100vw, 800px"><figcaption>The launch ceremony of Yangwagn U8 Premium Edition</figcaption></figure>
<p><a href="https://carnewschina.com/tag/yangwang/">YangWang </a>is a premium brand under <a href="https://carnewschina.com/tag/byd/">BYD</a>. The U8 launched in two versions – Premium Edition and Off-road Master Edition. The version that started presales today was the Premium Edition, and it will begin deliveries in October. </p>
<figure><img decoding="async" width="800" height="451" src="https://carnewschina.com/wp-content/uploads/2023/09/03-YangwangU8-Premium-Edition-off-raod-scene-800x451.jpg" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/03-YangwangU8-Premium-Edition-off-raod-scene-800x451.jpg 800w, https://carnewschina.com/wp-content/uploads/2023/09/03-YangwangU8-Premium-Edition-off-raod-scene-300x169.jpg 300w, https://carnewschina.com/wp-content/uploads/2023/09/03-YangwangU8-Premium-Edition-off-raod-scene-120x68.jpg 120w, https://carnewschina.com/wp-content/uploads/2023/09/03-YangwangU8-Premium-Edition-off-raod-scene-768x433.jpg 768w, https://carnewschina.com/wp-content/uploads/2023/09/03-YangwangU8-Premium-Edition-off-raod-scene-696x393.jpg 696w, https://carnewschina.com/wp-content/uploads/2023/09/03-YangwangU8-Premium-Edition-off-raod-scene-1068x603.jpg 1068w, https://carnewschina.com/wp-content/uploads/2023/09/03-YangwangU8-Premium-Edition-off-raod-scene.jpg 1161w" sizes="(max-width: 800px) 100vw, 800px"><figcaption>U8 Premium Edition</figcaption></figure>
<p>The Off-road Master will hit the market later and has some extra off-road add-ons, like a tougher-looking front bumper, that extends towards the bottom of the car to protect against collision in the wild. It also features a snorkel and detachable roof rack with a small ladder on the side.</p><p><span>- Advertisement -</span>
<ins data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-7299825303701007" data-ad-slot="3633264926"></ins>
</p>
<figure><img decoding="async" width="800" height="554" src="https://carnewschina.com/wp-content/uploads/2023/09/u8-12-800x554.jpg" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/u8-12-800x554.jpg 800w, https://carnewschina.com/wp-content/uploads/2023/09/u8-12-300x208.jpg 300w, https://carnewschina.com/wp-content/uploads/2023/09/u8-12-120x83.jpg 120w, https://carnewschina.com/wp-content/uploads/2023/09/u8-12-768x532.jpg 768w, https://carnewschina.com/wp-content/uploads/2023/09/u8-12-218x150.jpg 218w, https://carnewschina.com/wp-content/uploads/2023/09/u8-12-696x482.jpg 696w, https://carnewschina.com/wp-content/uploads/2023/09/u8-12-1068x740.jpg 1068w, https://carnewschina.com/wp-content/uploads/2023/09/u8-12.jpg 1393w" sizes="(max-width: 800px) 100vw, 800px"><figcaption>U8 Off-Road Master Edition. </figcaption></figure>
<p>U8 Premium Edition costs <strong>1,089,000 yuan (150k USD)</strong>, making U8 the most expensive China’s mass-produced electric car. Master Off-road Edition pre-sale price wasn’t announced yet but is expected to be the same. </p>
<h2>Specs</h2>
<p>YangWang U8 is a giant SUV with dimensions (L/W/H) 5319/2050/1930 mm and a wheelbase of 3050 mm. It is 502 mm longer, 137 mm narrower, and 39 mm lower than the Mercedes-Benz G-Class. As for the wheelbase, it is 160 mm longer.</p>
<p>The car sits on an e4 platform and is equipped with a Disus-P hydraulic body control system, which allows the vehicle to rise and lower up to 150 mm.</p><p><span>- Advertisement -</span>
<ins data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-7299825303701007" data-ad-slot="4893557466"></ins>
</p>
<p>Despite such enormous dimensions, it is only a 5-seater, not offering a third-row option. The gross weight is 3,985 kg, and the curb weight is 3,460 kg, based on <a href="https://carnewschina.com/2023/05/10/byds-yangwang-u8-gets-green-light-for-production-in-china-with-massive-curb-weight/">MIIT-released data</a>. </p>
<figure><img loading="lazy" decoding="async" width="555" height="335" src="https://carnewschina.com/wp-content/uploads/2023/09/5seater.jpg" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/5seater.jpg 555w, https://carnewschina.com/wp-content/uploads/2023/09/5seater-300x181.jpg 300w, https://carnewschina.com/wp-content/uploads/2023/09/5seater-120x72.jpg 120w" sizes="(max-width: 555px) 100vw, 555px"></figure>
<p>U8 is an electric range extended vehicle (EREV), like Li Auto. It means it has an ICE that works as a power generator for the battery but isn’t connected to the wheels. In this case, a 2.0L turbo powers a 49.05 kWh Blade battery with LFP chemistry inside. It is supplemented with Cell-to-Chassis (CTC) technology, which means the battery is integrated into the chassis. The range is 180 km under CLTC terms, and the comprehensive range with a full battery and 75-liter fuel tank is 1000 km. </p>
<p>ICE is mated with quad electric motors, each with a power of 220 kW. Total power is 880 kW with 1,280 Nm peak torque. Top speed is limited to 200 km/h, and <strong>0-100 acceleration is 3.6 seconds. </strong></p>
<p>According to CarNewsChina information from sources familiar with the matter, BYD is also considering the all-electric version next year. That would compete with<a href="https://carnewschina.com/2023/08/25/m-hero-917-from-dongfeng-launched-in-china-with-1088-hp-starts-at-87500-usd/"> Dongfeng’s all-electric off-road beast M-Hero 917</a>.</p>
<figure><img loading="lazy" decoding="async" width="800" height="450" src="https://carnewschina.com/wp-content/uploads/2023/09/04-YangwangU8-Premium-Edition-off-raod-scene-800x450.jpg" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/04-YangwangU8-Premium-Edition-off-raod-scene-800x450.jpg 800w, https://carnewschina.com/wp-content/uploads/2023/09/04-YangwangU8-Premium-Edition-off-raod-scene-300x169.jpg 300w, https://carnewschina.com/wp-content/uploads/2023/09/04-YangwangU8-Premium-Edition-off-raod-scene-120x68.jpg 120w, https://carnewschina.com/wp-content/uploads/2023/09/04-YangwangU8-Premium-Edition-off-raod-scene-768x432.jpg 768w, https://carnewschina.com/wp-content/uploads/2023/09/04-YangwangU8-Premium-Edition-off-raod-scene-1536x864.jpg 1536w, https://carnewschina.com/wp-content/uploads/2023/09/04-YangwangU8-Premium-Edition-off-raod-scene-696x392.jpg 696w, https://carnewschina.com/wp-content/uploads/2023/09/04-YangwangU8-Premium-Edition-off-raod-scene-1068x601.jpg 1068w, https://carnewschina.com/wp-content/uploads/2023/09/04-YangwangU8-Premium-Edition-off-raod-scene-1920x1080.jpg 1920w, https://carnewschina.com/wp-content/uploads/2023/09/04-YangwangU8-Premium-Edition-off-raod-scene.jpg 2000w" sizes="(max-width: 800px) 100vw, 800px"></figure>
<p>U8 supports DC fast charging up to 110 kW, making 30% – 80% possible in 18 minutes. Its 6 kW vehicle-to-load (VTL) discharge capability can power electronic devices for up to 25 hours.</p>
<p>The silicon heart of Yangwang’s SUV is the Nvidia Drive Orin SoC, with a computational power of 508 TOPS. 38 sensors feed it with input, including 3 lidars, 13 cameras, 12 ultrasonic radars, and 5 millimeter wave radars. It supports L2 ADAS. </p>
<figure>
<figure><img loading="lazy" decoding="async" width="800" height="533" data-id="162623" src="https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131757135756458009840.jpg-800x533.webp" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131757135756458009840.jpg-800x533.webp 800w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131757135756458009840.jpg-300x200.webp 300w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131757135756458009840.jpg-120x80.webp 120w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131757135756458009840.jpg-768x512.webp 768w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131757135756458009840.jpg-696x464.webp 696w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131757135756458009840.jpg-1068x712.webp 1068w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131757135756458009840.jpg.webp 1200w" sizes="(max-width: 800px) 100vw, 800px"></figure>
<figure><img loading="lazy" decoding="async" width="800" height="533" data-id="162624" src="https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131762135757149282909.jpg-800x533.webp" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131762135757149282909.jpg-800x533.webp 800w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131762135757149282909.jpg-300x200.webp 300w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131762135757149282909.jpg-120x80.webp 120w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131762135757149282909.jpg-768x512.webp 768w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131762135757149282909.jpg-696x464.webp 696w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131762135757149282909.jpg-1068x712.webp 1068w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131762135757149282909.jpg.webp 1200w" sizes="(max-width: 800px) 100vw, 800px"></figure>
<figure><img loading="lazy" decoding="async" width="800" height="533" data-id="162625" src="https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131761135757116963464.jpg-800x533.webp" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131761135757116963464.jpg-800x533.webp 800w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131761135757116963464.jpg-300x200.webp 300w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131761135757116963464.jpg-120x80.webp 120w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131761135757116963464.jpg-768x512.webp 768w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131761135757116963464.jpg-696x464.webp 696w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131761135757116963464.jpg-1068x712.webp 1068w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131761135757116963464.jpg.webp 1200w" sizes="(max-width: 800px) 100vw, 800px"></figure>
<figure><img loading="lazy" decoding="async" width="800" height="533" data-id="162627" src="https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131758135756619062720.jpg-800x533.webp" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131758135756619062720.jpg-800x533.webp 800w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131758135756619062720.jpg-300x200.webp 300w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131758135756619062720.jpg-120x80.webp 120w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131758135756619062720.jpg-768x512.webp 768w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131758135756619062720.jpg-696x464.webp 696w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131758135756619062720.jpg-1068x712.webp 1068w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131758135756619062720.jpg.webp 1200w" sizes="(max-width: 800px) 100vw, 800px"></figure>
<figure><img loading="lazy" decoding="async" width="800" height="533" data-id="162626" src="https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131759135756635114021.jpg-800x533.webp" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131759135756635114021.jpg-800x533.webp 800w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131759135756635114021.jpg-300x200.webp 300w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131759135756635114021.jpg-120x80.webp 120w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131759135756635114021.jpg-768x512.webp 768w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131759135756635114021.jpg-696x464.webp 696w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131759135756635114021.jpg-1068x712.webp 1068w, https://carnewschina.com/wp-content/uploads/2023/09/w1200_yichecar_202304131759135756635114021.jpg.webp 1200w" sizes="(max-width: 800px) 100vw, 800px"></figure>
</figure>
<p>Inside the car, you find lots of Nappa leather and 6 massive screens: a 23.6″ instrument panel for the driver, a 12.8″ central control screen, and another 23.6″ infotainment screen for the co-pilot. Two more screens are for rear passengers mounted to the back of the front seats, and the last LCD is on the console between the rear seats. There is also a 70″ augmented reality head-up display and 22 speakers from Dynaudio.</p>
<figure>
<figure><img loading="lazy" decoding="async" width="800" height="450" data-id="162618" src="https://carnewschina.com/wp-content/uploads/2023/09/interioru84-800x450.jpg" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/interioru84-800x450.jpg 800w, https://carnewschina.com/wp-content/uploads/2023/09/interioru84-300x169.jpg 300w, https://carnewschina.com/wp-content/uploads/2023/09/interioru84-120x67.jpg 120w, https://carnewschina.com/wp-content/uploads/2023/09/interioru84-768x432.jpg 768w, https://carnewschina.com/wp-content/uploads/2023/09/interioru84-696x391.jpg 696w, https://carnewschina.com/wp-content/uploads/2023/09/interioru84-1068x600.jpg 1068w, https://carnewschina.com/wp-content/uploads/2023/09/interioru84.jpg 1258w" sizes="(max-width: 800px) 100vw, 800px"></figure>
<figure><img loading="lazy" decoding="async" width="800" height="450" data-id="162616" src="https://carnewschina.com/wp-content/uploads/2023/09/interioru83-800x450.jpg" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/interioru83-800x450.jpg 800w, https://carnewschina.com/wp-content/uploads/2023/09/interioru83-300x169.jpg 300w, https://carnewschina.com/wp-content/uploads/2023/09/interioru83-120x68.jpg 120w, https://carnewschina.com/wp-content/uploads/2023/09/interioru83-768x432.jpg 768w, https://carnewschina.com/wp-content/uploads/2023/09/interioru83-696x392.jpg 696w, https://carnewschina.com/wp-content/uploads/2023/09/interioru83-1068x601.jpg 1068w, https://carnewschina.com/wp-content/uploads/2023/09/interioru83.jpg 1264w" sizes="(max-width: 800px) 100vw, 800px"></figure>
<figure><img loading="lazy" decoding="async" width="800" height="450" data-id="162617" src="https://carnewschina.com/wp-content/uploads/2023/09/interioru8-800x450.jpg" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/interioru8-800x450.jpg 800w, https://carnewschina.com/wp-content/uploads/2023/09/interioru8-300x169.jpg 300w, https://carnewschina.com/wp-content/uploads/2023/09/interioru8-120x68.jpg 120w, https://carnewschina.com/wp-content/uploads/2023/09/interioru8-768x432.jpg 768w, https://carnewschina.com/wp-content/uploads/2023/09/interioru8-696x392.jpg 696w, https://carnewschina.com/wp-content/uploads/2023/09/interioru8-1068x601.jpg 1068w, https://carnewschina.com/wp-content/uploads/2023/09/interioru8.jpg 1283w" sizes="(max-width: 800px) 100vw, 800px"></figure>
</figure>
<p>The U8 also features three smartphone wireless chargers and some not-so-common in-car tech, such as a thermographic camera and integrated satellite phone. Both versions of U8 have spare tires. </p>
<h2>Wading and floating mode</h2>
<p>Like every good off-roader, U8 can wade. The Premium Edition can wade in a maximum depth of 1000 mm, while the Off-road Master Edition can wade up to 1,400mm thanks to the snorkel. </p>
<figure><img loading="lazy" decoding="async" width="600" height="337" src="https://carnewschina.com/wp-content/uploads/2023/09/88d5a23a-67af-4e5e-8ecd-170159b40215.png" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/88d5a23a-67af-4e5e-8ecd-170159b40215.png 600w, https://carnewschina.com/wp-content/uploads/2023/09/88d5a23a-67af-4e5e-8ecd-170159b40215-300x169.png 300w, https://carnewschina.com/wp-content/uploads/2023/09/88d5a23a-67af-4e5e-8ecd-170159b40215-120x67.png 120w" sizes="(max-width: 600px) 100vw, 600px"></figure>
<p>When the wading mode is activated, the vehicle will use its sensors to monitor the environment, water depth, wheel slip status, etc. The emergency floating mode is automatically activated when the car gets into depth over the limit. </p>
<figure><img loading="lazy" decoding="async" width="800" height="478" src="https://carnewschina.com/wp-content/uploads/2023/09/F6KTqtGXsAAgwU9-800x478.jpeg" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/F6KTqtGXsAAgwU9-800x478.jpeg 800w, https://carnewschina.com/wp-content/uploads/2023/09/F6KTqtGXsAAgwU9-300x179.jpeg 300w, https://carnewschina.com/wp-content/uploads/2023/09/F6KTqtGXsAAgwU9-120x72.jpeg 120w, https://carnewschina.com/wp-content/uploads/2023/09/F6KTqtGXsAAgwU9-768x459.jpeg 768w, https://carnewschina.com/wp-content/uploads/2023/09/F6KTqtGXsAAgwU9-696x416.jpeg 696w, https://carnewschina.com/wp-content/uploads/2023/09/F6KTqtGXsAAgwU9-1068x638.jpeg 1068w, https://carnewschina.com/wp-content/uploads/2023/09/F6KTqtGXsAAgwU9.jpeg 1278w" sizes="(max-width: 800px) 100vw, 800px"></figure>
<p>When activated, the combustion engine will immediately shut down, <strong>Disus-P will raise the suspension to the maximum level</strong>, windows will be closed automatically, A/C will be switched to internal circulation, and<strong> the sunroof will open, providing an emergency exit</strong>. Moreover, the vehicle will display the water depth and attitude on the central control screen.</p>
<figure><img loading="lazy" decoding="async" width="800" height="493" src="https://carnewschina.com/wp-content/uploads/2023/09/F6KTuTFXQAAt_a--800x493.jpeg" alt="" srcset="https://carnewschina.com/wp-content/uploads/2023/09/F6KTuTFXQAAt_a--800x493.jpeg 800w, https://carnewschina.com/wp-content/uploads/2023/09/F6KTuTFXQAAt_a--300x185.jpeg 300w, https://carnewschina.com/wp-content/uploads/2023/09/F6KTuTFXQAAt_a--120x74.jpeg 120w, https://carnewschina.com/wp-content/uploads/2023/09/F6KTuTFXQAAt_a--768x473.jpeg 768w, https://carnewschina.com/wp-content/uploads/2023/09/F6KTuTFXQAAt_a--696x429.jpeg 696w, https://carnewschina.com/wp-content/uploads/2023/09/F6KTuTFXQAAt_a--1068x658.jpeg 1068w, https://carnewschina.com/wp-content/uploads/2023/09/F6KTuTFXQAAt_a-.jpeg 1454w" sizes="(max-width: 800px) 100vw, 800px"></figure>
<p>U8 can stay afloat for 30 minutes and move forward at 3 km/h by accelerating and turning wheels. It can even perform tank turns while floating. BYD highlights that it is only for emergencies such as floods and not for fun crossing lakes and rivers. Interestingly, their marketing materials showcase U8 floating on the water exactly in those situations. After the emergency floating mode activation, the car needs to be brought to a service center for inspection. BYD claims U8 is IP68 level waterproof.</p>
<figure><p>
<iframe loading="lazy" title="BYD's YangWang showcase emergency floating mode" width="696" height="392" src="https://www.youtube.com/embed/ssxVNTa6IGE?feature=oembed&amp;enablejsapi=1&amp;origin=https://carnewschina.com" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>
<h2>Direct sales model</h2>
<p>During the launch conference, Yangwang announced they would focus on a direct sales model. They have 60 stores under construction in 40 cities and plan to open 90 stores by the year-end. That is a similar strategy BYD has with its second brand –<a href="https://carnewschina.com/tag/fang-cheng-bao/"> Fang Cheng Bao </a>– which recently started to <a href="https://carnewschina.com/2023/09/13/byd-takes-over-bmw-mercedes-aston-martin-stores-in-china/">take over stores and showrooms of premium brands like Mercedes-Benz</a>, Aston Martin or Maserati. Unlike YangWang and Fang Cheng Bao, BYD uses a standard dealership model for its BYD-badged cars. </p>
<h2>Editor’s comment</h2>
<p>BYD was never building premium EVs, and YangWang U8 is their most expensive car ever and one of the most costly cars in China. The goal is obvious: BYD needs to increase its margin, and budged EVs such as best-selling Seagull and Dolphin can’t help. Thus, new sub-brands come to the rescue: YangWang, Fang Cheng Bao, and Denza.</p>
<p>“We must develop high-end technology to take on high-end brands,” said Wang Chuanfu, founder and CEO of BYD, and continued, “Technological innovation is our only opportunity to take the lead in change.”</p>
<p>BYD has always had a dream to build a high-end luxury car. However, there are also doubts. Will customers buy a premium car from a brand historically focused on mid-class and affordable vehicles? BYD will undergo the market test, and we will soon see the result. Based on the excitement U8 caused in Chinese media and the tech that looks great on paper, the starting position of BYD is more than favorable. </p>
<figure><p>
<iframe loading="lazy" title="BYD's YangWang U8 doing full 360° tank turn in front of hotel in China" width="696" height="392" src="https://www.youtube.com/embed/xEflRdyIbJc?feature=oembed&amp;enablejsapi=1&amp;origin=https://carnewschina.com" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sounds of Cologne – The WDR Studio for Electronic Music (161 pts)]]></title>
            <link>https://artsandculture.google.com/story/sounds-of-cologne-wdr/gQWRzsVclcCYPA?hl=en</link>
            <guid>38892685</guid>
            <pubDate>Sat, 06 Jan 2024 16:22:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://artsandculture.google.com/story/sounds-of-cologne-wdr/gQWRzsVclcCYPA?hl=en">https://artsandculture.google.com/story/sounds-of-cologne-wdr/gQWRzsVclcCYPA?hl=en</a>, See on <a href="https://news.ycombinator.com/item?id=38892685">Hacker News</a></p>
<div id="readability-page-1" class="page"><div jsname="JYFd0b"><p>The Hungarian composer György Ligeti called the relationship between New Music, Westdeutscher Rundfunk and the city of Cologne a "triumvirate". It was an inseparable connection that made WDR a pioneer in the development and promotion of an entirely new musical production process. For months, composers were able to experiment here for the creation of never-before-heard sound constructions, to fiddle with tape snippets and noise generators, to play with ring modulators and octave filters.&nbsp;<br></p><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-is-first="true" data-prid="sssa90f62f4-6373-48f3-a570-cfad520e43de"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>WDR-Studio Impulsgeber<strong>Original Source: WDR/Alfred Jansen</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>For the Fonologica Musicale at Milan Radio and the Institute for Sonology in Utrecht which followed, and for the leading studios in Tokyo and New York: for all of them, the WDR Studio for Electronic Music, founded in 1951, was the ultimate role model, the mother of all studios, and the first place in the world to work entirely in the field of electronic synthesis, i.e. without any instruments or conventional sounds.</p></div></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-prid="sss21b359dc-a94c-452a-a05c-cd09cf42024c"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>WDR-Studio Frequenzmesser<strong>Original Source: WDR/Alfred Jansen</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>The Studio in Cologne was, rather, the main focal point in the development of electronic music. What Wimbledon is for tennis players and Wembley Stadium for footballers, the WDR Studio was for the new composers of that time. Receiving an invitation there felt like being knighted. If you were allowed to experiment there, you had made it. And it was with correspondingly immense respect and awe that most sound artists approached the place.</p></div></div><p>The composer and subsequent Artistic Director York Höller (born in 1944), looking back to 1972, when he created his work Horizonte here, said that he had been determined "to work in a really concentrated way and not just play around with things, as might have happened at other studios, because he wanted to live up to the tradition."<br></p><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-is-first="true" data-prid="sss633e96aa-2188-4102-a02f-81ae825f61a9"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>WDR-Studio Schaltknöpfe<strong>Original Source: WDR/Alfred Jansen</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>So how did it all begin? However did this revolutionary institution come to be founded, six years after the end of the Second World War, six years after the end of the Third Reich, under which some art was classified as degenerate and electronic music was frowned upon?</p></div></div><div><p>Of course, people had been interested in designing and producing new sound structures, especially in connection with the new medium of radio, long before the Nazis came to power in 1933.</p><p> Even in the 1920s, radio stations and university institutes were open to the idea of trying out and developing new instruments and coming up with new technical and musical ideas. One example of this was the Radio Research Institute that was set up by the Prussian Ministry of Culture in Berlin in May 1928 and abolished by the Nazis in 1935. The Research Institute was seen as a laboratory for new sounds, but also carried out research into the sound film technology that was becoming popular in 1929 and into early ways of storing music on discs. Among the pioneers of electric music who came and went at the Berlin Research Institute were Oskar Sala, Friedrich Trautwein, and Paul Hindemith.</p></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-is-first="true" data-prid="ssscfc955c4-a88b-435b-bb70-5a6da4b97b8e"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>WDR-Studio Stoppuhr<strong>Original Source: WDR/Alfred Jansen</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><div jscontroller="DuQ7xb" jsaction="mouseup:Jav5X; mouseleave:Jav5X; touchend:Jav5X;rcuQ6b:rcuQ6b;JIbuQc:KgzOP(NziyQe),gxeK8b(HvfI2b);qAQfF:NziyQe;B6U6Md:HvfI2b;KVVEYe:S9gUrf;vT2oud:VdHBib;cdsA9e:CQj15d;xbUZBb:mdvgpd;JZCswc:HvfI2b;B5zXod:HvfI2b;" jsname="lqC4kf"><p>Vocoder - Demonstration 1949.12.28</p></div><div jsname="Vm7gUc"><p>WDR was considered a pioneer in the field of electronic music at an early stage, explaining to listeners how a vocoder worked as early as 1949.The WDR Studio for Electronic Music is regarded as having been born on October 18, 1951, when, at a meeting at the then North-West Germany radio station, the decision was made to set up a studio for electronic music: This was 10 days after the contemporary music concert series Musik der Zeit celebrated its premiere with a performance by Igor Stravinsky.&nbsp;</p></div></div></div><p>Participants in the October 18 meeting, who were key protagonists in getting the studio off the ground, included:&nbsp;<b>Robert Beyer (1901–89)</b>, a radical innovator in the art of sound, had a vision of tone color music as early as the 1920s. The physicist and phoneticist&nbsp;<b>Werner Meyer-Eppler (1913–60)</b>, who did a lot of work on sound synthesis and was the first to coin the term electronic music in 1949. <b>Fritz Enkel</b>, the technician who designed the first set-up for the studio. And, of course, <b>Herbert Eimert (1897–1972)</b>, the musicologist who was thrown out of the Cologne University of Music for writing a paper on Atonal Music Teaching, but later became the first Director of the WDR Studio for Electronic Music</p><p>Eimert's first words when the first program The Sound World of Electronic Music was broadcast in the Musikalisches Nachtprogramm (Musical Evening Program) series on October 18, 1951 sounded like a warning: "Please don't be alarmed, the few bars that you have just heard are simply setting the tone for our evening program." That first broadcast was about the 12-tone theory in Thomas Mann's Doctor Faustus novels: explained on the piano. It was an indication that not everything that came out of the Studio for Electronic Music was simple fare and was not intended to be.</p><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-is-first="true" data-prid="sssc5525ab0-2113-41de-8211-9ec6c65be9a6"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>Impulsgeber H83A<strong>Western Broadcasting Corporation (North Rhine-Westphalia)</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><div jscontroller="DuQ7xb" jsaction="mouseup:Jav5X; mouseleave:Jav5X; touchend:Jav5X;rcuQ6b:rcuQ6b;JIbuQc:KgzOP(NziyQe),gxeK8b(HvfI2b);qAQfF:NziyQe;B6U6Md:HvfI2b;KVVEYe:S9gUrf;vT2oud:VdHBib;cdsA9e:CQj15d;xbUZBb:mdvgpd;JZCswc:HvfI2b;B5zXod:HvfI2b;" jsname="lqC4kf"><p>Herbert Eimert 1951 Vocoder, Filter &amp; Co</p></div><p>Still on the first evening Herbert Eimert gave an introduction to the sound production of electronic music.<br></p></div></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-prid="sss2d24fb58-4a7c-4e1a-bfb0-fd069ccc4f4d"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>Frequenzmessgerät<strong>Western Broadcasting Corporation (North Rhine-Westphalia)</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>People consciously wanted to create something new in the post-war years. They wanted to counter the weaponization of music by the Nazi dictatorship, a time when Bruckner, Beethoven, and, of course, Wagner and their works were appropriated for the purposes of pathos and propaganda.<br></p></div></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-prid="sss58f8aaf0-7af2-488c-9fb3-118a39a68ffc"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>WDR-Studio Frequenzmesser<strong>Original Source: WDR/Alfred Jansen</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>It was a question of stopping the misuse of music. What's more: they wanted to take the emotion out of music, so that it could never again be made vulnerable, open to attack, and subject to use for the wrong purposes.&nbsp;<br></p></div></div><p>Serial music was a further development of Arnold Schönberg's 12-tone technique, the unwieldy soundscape of composers like Karlheinz Stockhausen, Pierre Boulez, and Luigi Nono: of no use to autocrats. Advances in technology also opened up new ways of playing around with entirely new kinds of sound creation.</p><div jsname="cHYyed" jsaction="qPyyM:WAmPje;B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="u3081d" jsmodel="D5Uy5c" data-lazy-load="true" data-transition-type="1" data-is-first="true" data-prid="vs:9c9142af-bcef-41b9-93be-74d29bd7dee2"><p>Among the first works produced at the WDR Studio were Herbert Eimert's Klangstudien I and II. They were a foretaste of what was to come in the decades that followed.</p></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-prid="sssdc418371-466c-40d8-8ad8-135e751fca2e"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>Schaltwand seitlich<strong>Western Broadcasting Corporation (North Rhine-Westphalia)</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><div jscontroller="DuQ7xb" jsaction="mouseup:Jav5X; mouseleave:Jav5X; touchend:Jav5X;rcuQ6b:rcuQ6b;JIbuQc:KgzOP(NziyQe),gxeK8b(HvfI2b);qAQfF:NziyQe;B6U6Md:HvfI2b;KVVEYe:S9gUrf;vT2oud:VdHBib;cdsA9e:CQj15d;xbUZBb:mdvgpd;JZCswc:HvfI2b;B5zXod:HvfI2b;" jsname="lqC4kf"><p>Herbert Eimert 1957 Anmoderation</p></div><p>Herbert Eimert brought the secrets of electronic music to interested listeners in the WDR radio broadcasts. In December 1957, Eimert spoke about the first live concerts of compositions from the studio as well as about the challenges and difficulties in general. &nbsp;<br></p></div></div><p>Herbert Eimert's musical interests were a good example of the expansion of 12-tone music with elements of serialism, as referred to previously. Eimert himself was in charge of the Studio for 12 years, until 1963. At the same time, until 1957, he also lectured on the Darmstadt Summer Course for New Music. As a professor at the University of Music, he was also in charge of the Studio for Electronic Music there, from 1965 to 1971. He worked with Hans-Ulrich Humpert, his successor at the Electronic Studio at the University of Music, on a lexicon of electronic music. Shortly before the manuscript was finished, on December 15th, 1972, Eimert died in Düsseldorf.</p><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-is-first="true" data-prid="sss7e7771f2-e591-4dfb-85b8-3c9faa539dcc"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>WDR-Studio Magnetophon<strong>Original Source: WDR/Alfred Jansen</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>But let's get back to the WDR Studio for Electronic Music. Of course, in the early days, many musicians and composers were overwhelmed by the new equipment and first had to learn how to use it. Composer Gottfried Michael Koenig, the permanent studio engineer, helped numerous colleagues who were still clueless about new developments in electronic music to produce their works.&nbsp;<br></p></div></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="BEuohd" jsmodel="LXMIqe" data-lazy-load="true" data-transition-type="1" data-prid="ipt42eb7c16-fb5b-4ac5-8bad-54e3a0be015b"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>Mischpult und Impulsgeber<strong>Western Broadcasting Corporation (North Rhine-Westphalia)</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div><div jsname="cHYyed"><p>For example, Koenig produced Henri Pousseur's Seismogramme and Bo Nilsson's Audiogramme by himself in this way. The music theoretician and composer Herbert Brün later recounted how Koenig had initially sent him to stand in a corner and watch the work in progress.<br></p></div><div jsname="cHYyed"><p>The composers who were invited were always, as Herbert Eimert put it, "suited to using this equipment, specially commissioned by the radio channel," and the studio was then entirely at their disposal for up to three months, for them to make their ideas a reality.<br></p></div></div></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-prid="sssd7f4784c-631f-4c67-8558-48616b247fac"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>WDR-Studio Tieftongenerator<strong>Original Source: WDR/Alfred Jansen</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>The WDR Studio soon became a meeting place for members of the innovative musical avant-garde. Karlheinz Stockhausen, who succeeded Herbert Eimert as Director of the Studio in 1963, was a frequent visitor here. This was a place not just for composing but also for discussion.&nbsp;<br></p></div></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="BEuohd" jsmodel="LXMIqe" data-lazy-load="true" data-transition-type="1" data-prid="ipt31cca6e4-145a-4626-949e-0804c8c35d8d"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>Digital Meter Unit<strong>Western Broadcasting Corporation (North Rhine-Westphalia)</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div><div jsname="cHYyed"><p>The composer Konrad Boehmer (1941–2014) once described it like this: "The Studio was the place where these discussions took place. It was in the basement and you had to go down in a musty little lift, into a really small studio, which, if there were five or six people in it …<br></p></div><div jsname="cHYyed"><p>… was already full. It looked more like a student room. There were never enough chairs, so people sat on the filters and generators …<br></p></div></div></div><p><br>… at the time they were still really sturdy, iron cans, and people smoked so much you couldn't see your hand in front of your face. It was a cross between the Florentine Camerata and a bohemian bar in Paris."</p><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-is-first="true" data-prid="sss70ebefda-7a84-4c16-8953-285bf70fcee7"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>WDR-Studio Verstärker<strong>Original Source: WDR/Alfred Jansen</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>There was a big discrepancy between, on the one hand, people's ideas, visions, and expectations, which were unbounded, and, on the other, the limited technical capabilities of the studio. Or, as Konrad Boehmer put it, the technicians and composers found themselves stuck "in a dialectic between the woeful technology and their fertile imaginations."&nbsp;<br></p></div></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-prid="sss5f3e37b2-239c-4f9f-8381-e1e6bb34172e"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>WDR-Studio Teilaufnahme<strong>Original Source: WDR/Thomas Brill</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>Yards of tape looped in all directions around the studio.&nbsp;Whereas, at the start, instruments like the Trautonium and the Melochord were still part of the Studio, those instruments soon disappeared, leaving gray metal boxes such as pulse generators, a pure tone generator, noise generator, filter, ring modulator, and much more besides.&nbsp;</p></div></div><div><p>Gottfried Michael Koenig summed it up in a lecture in January 2000, when he said that it was no longer a question of writing down a note as a C, an F flat, or a G sharp, like it always used to be in music. Now a composer would say: "I need a sound at 483 Hertz," and the technician would set 483 Hertz on the generator.</p><p>But from the 1970s, the studio lost its monopoly position as a technological pioneer for the creation of electronic music. Other studios around the world were more modernly equipped, especially in view of the increasing importance of digital sound synthesis from then on. Mauricio Kagel (1931–2008), the Argentinian composer who came to Cologne in 1957, was later appointed Director of the Institute for New Music at the <a href="https://de.wikipedia.org/wiki/Rheinische_Musikschule" target="_blank">Rheinische Musikschule </a>in Cologne and then succeeded <a href="https://de.wikipedia.org/wiki/Karlheinz_Stockhausen" target="_blank">Karlheinz Stockhausen</a> as Head of the Cologne Courses for New Music (until 1975). He said once, looking back: "I was impressed by the fact that composing electronic music was regarded as an activity that deserved a permanent place in the program."</p></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-is-first="true" data-prid="sss59d18c21-0084-4f1d-aa91-dcbbdcb6357a"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>WDR-Studio Regler<strong>Original Source: WDR/Alfred Jansen</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>He also admitted that: "The Studio gradually became like a waxworks museum—worthy of Madame Tussaud's, because nothing was updated, technologically or aesthetically and it all gathered dust, both literally and metaphorically. By 1967, I sensed that there was unlikely to be any change of direction at the Electronic Studio."</p></div></div><p>Even though other studios around the world may to some extent have been better equipped, with more modern technology, the Studio in Cologne never lost its huge importance as the pioneering center of the genre of electronic music. <br> In 1987, the Studio moved out of the main WDR building and into a building owned by the broadcaster in the Annostrasse, a few kilometers away. It remained in use there, by Karlheinz Stockhausen and the Artistic Director York Höller, until 2001. <br></p><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-is-first="true" data-prid="sss4516bfcc-c1c5-46c1-8a02-18365800edc5"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>WDR-Studio in Köln-Ossendorf<strong>Western Broadcasting Corporation (North Rhine-Westphalia)</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>Then, after exactly half a century, no-one had any use for the studio any more. The equipment was removed and stored in a cellar in Cologne-Ossendorf.</p></div></div><div jsname="dHC5Hf" jsaction="qPyyM:WAmPje;B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="u3081d" jsmodel="D5Uy5c" data-lazy-load="true" data-transition-type="1" data-prid="vs:a3c5bce4-6e31-4fa2-8461-b80190601051"><a href="https://www.youtube.com/watch?v=5UmByH1x1_k" jsmodel="nDkrdf" target="_blank" jsname="PEMpI" data-bgsrc="//i2.ytimg.com/vi/5UmByH1x1_k/hqdefault.jpg" jsshadow=""><span jsslot=""></span></a><div jsname="sRTgqf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>WDR Studio für Elektronische Musik: 360-Grad-Tour<strong>Western Broadcasting Corporation (North Rhine-Westphalia)</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-prid="sssddb8a580-5283-4487-8fb2-49ccff33f072"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>WDR Archiv des Studios für Elektronische Musik<strong>Western Broadcasting Corporation (North Rhine-Westphalia)</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>As part of the move, thousands of tape recordings of composers working in the studio were systematically recorded in a database, completely digitized and set up as a closed collection in a magazine in the WDR broadcasting center. <br>&nbsp;</p></div></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-prid="sss623cf21b-27f4-4748-8759-917b2be99b8c"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>WDR Tonbandarchiv des Studios für Elektronische Musik<strong>Western Broadcasting Corporation (North Rhine-Westphalia)</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>… such as Kontakte, the famous composition by Karlheinz Stockhausen, which was created here in the WDR Studio.</p></div></div><p>In 2017, the search for a permanent home seemed to be over, and an agreement on a move to Haus Mödrath, Stockhausen's birthplace, seemed within reach, but the plans fell apart in early 2020. In an exclusive interview with "Google Arts &amp; Culture," the former head of programming at cultural radio station WDR 3, Professor Karl Karst spoke about the studio's place in the history of music, its importance to WDR, and also about his hopes for keeping the studio in the long term.&nbsp;<br></p><div jsname="dHC5Hf" jscontroller="u3081d" jsaction="qPyyM:WAmPje;B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" data-no-scroll="true" jsmodel="D5Uy5c" data-lazy-load="true" data-transition-type="1" data-is-first="true" data-prid="vs:d19c79d6-a53c-46ea-b2de-4cc7d950937f"><p>Interview Karl Karst 2020<strong>Western Broadcasting Corporation (North Rhine-Westphalia)</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-prid="sss8cfe37b2-dc3a-4d7d-8301-a77f65fa4a81"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>Volker Müller (9)<strong>Western Broadcasting Corporation (North Rhine-Westphalia)</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>Until 2021 Volker Müller took care of the legacy.&nbsp;<br></p></div></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-prid="sss1857d86c-f120-46a6-be31-f678abaed565"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>Volker Müller (6)<strong>Western Broadcasting Corporation (North Rhine-Westphalia)</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>Volker Müller started work as a sound engineer here in 1971 because he wanted to "do something new and exciting." He worked here for 30 years, with Stockhausen and many other important musicians.</p></div></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-prid="sss74f282ca-ebec-4206-9b3a-efc060070333"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>Volker Müller (5)<strong>Western Broadcasting Corporation (North Rhine-Westphalia)</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>For many years, Volker Müller has kept coming back to Cologne-Ossendorf and visiting the studio in exile in the cellar there—in the hope that one day a more worthy location will be found where a museum can be created that is open to the public, to commemorate such a unique place in the history of German music.</p></div></div><p>In the summer of 2020, Volker Müller gave an exclusive interview for "Google Arts &amp; Culture" in which he talked about the special features of the studio, about his work and about important artists who worked here.<br></p><div jsname="dHC5Hf" jscontroller="u3081d" jsaction="qPyyM:WAmPje;B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" data-no-scroll="true" jsmodel="D5Uy5c" data-lazy-load="true" data-transition-type="1" data-is-first="true" data-prid="vs:dbdf4690-718d-4389-96a1-883645ed57e3"><p>Interview Volker Müller 2020<strong>Western Broadcasting Corporation (North Rhine-Westphalia)</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsaction="B0tf1d:q680Y;BilGBc:VysLmc;rcuQ6b:WYd;IBB03b:rXxpye;MFzXR:OuzsSd;UdR7Zd:G3bbwe;GvneHb:GfDKYb;B3pdtc:yCd0Z; click:TKu1tc(BFDyo),aExa5e(sRTgqf);aLV0Fb:hcd3ve;glgWqd:NNsoCe;zcPKj:GfDKYb;" jscontroller="ofbBhc" jsmodel="o792Ff" data-lazy-load="true" data-transition-type="1" data-prid="sss5e280738-8b37-495f-9411-50a62fdf3d46"><div jsname="dHC5Hf" jscontroller="qRvY" jsaction="rcuQ6b:WYd;at19dd:nJCZlc;Vds8De:ZYIfFd;OqFQ0:nzJ4Kc;Yv4Nyd:AmA4ee; mouseover:VtwqKb(ejhxMd); mouseout:cN9A8(ejhxMd);" data-no-scroll="true"><p>Volker Müller (1942 - 2021)<strong>Original Source: WDR/Alfred Jansen</strong></p><svg width="24" height="24" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"></path><path d="M11 17h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zM11 9h2V7h-2v2z"></path></svg></div><div jsname="cHYyed"><p>In February 2021, exactly half a century after he took up his position at the WDR studio in 1971, Volker Müller died suddenly and unexpectedly - and with him one of the last contemporary witnesses and most profound experts of this unique technology. His name will forever have a permanent place in the history of the WDR Studio for Electronic Music. We bow our heads in great gratitude. RIP Volker Müller (1942 - 2021).<br></p></div></div><div><p>Credits: All media</p><p>The story featured may in some cases have been created by an independent third party and may not always represent the views of the institutions, listed below, who have supplied the content.</p></div><div><p>Stories from Western Broadcasting Corporation (North Rhine-Westphalia)</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tex-Oberon: Make Project Oberon Pretty Again (127 pts)]]></title>
            <link>https://github.com/guidoism/tex-oberon</link>
            <guid>38892164</guid>
            <pubDate>Sat, 06 Jan 2024 15:25:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/guidoism/tex-oberon">https://github.com/guidoism/tex-oberon</a>, See on <a href="https://news.ycombinator.com/item?id=38892164">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Project Oberon TeX Typesetting Project</h2>
<p dir="auto">Project Oberon is an amazing piece of computer science and amazing
tool for teaching. The book and the code demonstrate, without any tiny
amount of doubt, that it is possible to build a usable computing
system small enough to fit in the head of a normal programmer. That,
in my not-so-humble opinion, is a truly great achievement and I am in
awe of Niklaus Wirth and Jürg Gutknecht for it.</p>
<p dir="auto">The last edition -- from 2013 -- could use some tender loving care.
It should look beautiful.</p>
<p dir="auto">This project is an attempt to:</p>
<ol start="0" dir="auto">
<li>Liberate the text from the un-editable PDF sources.</li>
<li>Typeset the book using (plain) TeX and Knuth's own <code>taocpmac.tex</code> macros.</li>
<li>Add the full (typeset) source code to the book for those of us who like reading code in bed.</li>
<li>Convert it into a Literate Programming project where code is more liberally scattered amongst the prose and "tangled" into the final product.</li>
</ol>
<p dir="auto">Being a document with TeX sources, instead of a dead PDF, future
Project Oberon engineers will be able to modify the text to keep it
up-to-date with the running source code.</p>
<p dir="auto">I have a dream that we -- as the Oberon community -- can edit and publish this
book. I would love for this to be printed as a nice hardcover that can sit
right next to my Art of Computer Programming books.</p>
<p dir="auto">PDF and code taken from <a href="http://www.projectoberon.com/" rel="nofollow">http://www.projectoberon.com</a></p>
<h2 tabindex="-1" dir="auto">Progress</h2>
<ul>
<li> Copy text / basic typesetting</li>
<li> First proofreading pass</li>
<li> Second proofreading pass</li>
</ul>
<h2 tabindex="-1" dir="auto">Building</h2>

<p dir="auto">A recent in-progress PDF can be found in the Releases</p>
<h2 tabindex="-1" dir="auto">Quotes</h2>
<p dir="auto">The requirement of many megabytes of store for an operating system is, albeit commonly tolerated, absurd and another hallmark of user-unfriendliness, or perhaps manufacturer friendliness.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IRS to Begin Trial of Its Own Free Tax-Filing System (614 pts)]]></title>
            <link>https://www.nytimes.com/2024/01/05/your-money/irs-tax-filing-free-online.html</link>
            <guid>38892044</guid>
            <pubDate>Sat, 06 Jan 2024 15:12:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/01/05/your-money/irs-tax-filing-free-online.html">https://www.nytimes.com/2024/01/05/your-money/irs-tax-filing-free-online.html</a>, See on <a href="https://news.ycombinator.com/item?id=38892044">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/01/05/your-money/irs-tax-filing-free-online.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Live Map of Swiss Trains (145 pts)]]></title>
            <link>https://maps.vasile.ch/transit-sbb/</link>
            <guid>38891712</guid>
            <pubDate>Sat, 06 Jan 2024 14:32:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://maps.vasile.ch/transit-sbb/">https://maps.vasile.ch/transit-sbb/</a>, See on <a href="https://news.ycombinator.com/item?id=38891712">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>This map is an animation based on the official timetables of the <a href="http://www.sbb.ch/en/home.html" target="_blank">Swiss Federal Railways(SBB)</a> network.</p>

                    <p><img src="https://maps.vasile.ch/transit-sbb/static/images/qr-code-sbb-expo.png" width="200" height="200">
                    </p>    
                    
                    <p>Timetables update: <strong>December 2023.</strong></p>

                    <p>© 2007-2022 - <a href="https://www.vasile.ch/" target="_blank">Vasile Coțovanu</a> • <a href="https://twitter.com/vasile23">@vasile23</a></p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In-Browser Code Playgrounds (192 pts)]]></title>
            <link>https://antonz.org/in-browser-code-playgrounds/</link>
            <guid>38891177</guid>
            <pubDate>Sat, 06 Jan 2024 13:15:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antonz.org/in-browser-code-playgrounds/">https://antonz.org/in-browser-code-playgrounds/</a>, See on <a href="https://news.ycombinator.com/item?id=38891177">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><header></header><p>I'm a big fan of interactive code snippets in all kinds of technical writing, from product docs to online courses to blog posts. Like this one:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>greet</span>(<span>name</span>):
</span></span><span><span>    <span>print</span>(<span>f</span><span>"Hello, </span><span>{</span><span>name</span><span>}</span><span>!"</span>)
</span></span><span><span>
</span></span><span><span><span>greet</span>(<span>"World"</span>)
</span></span></code></pre></div><codapi-snippet sandbox="python" editor="basic"></codapi-snippet><p>In fact, I even built an open source tool called Codapi<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> for embedding such snippets.</p><p>Typically, a code playground consists of a client-side widget and a server-side part that executes the code and returns the result:</p><pre tabindex="0"><code>  browser
┌───────────────────────────────┐
│ def greet(name):              │
│   print(f"Hello, {name}!")    │
│                               │
│ greet("World")                │
└───────────────────────────────┘
  Run ►
    ↓
  server
┌───────────────────────────────┐
│ docker run codapi/python      │
│ python main.py                │
└───────────────────────────────┘
    ↓
  browser
┌───────────────────────────────┐
│ Hello, World!                 │
└───────────────────────────────┘
</code></pre><p>Personally, I'm quite happy with this setup. But often people prefer not to depend on a server and run the code entirely in the browser. So I decided to look into it and implemented embeddable in-browser code playgrounds for JavaScript, Python, PHP, Ruby, Lua, and SQLite.</p><h2 id="running-language-runtimes-in-the-browser">Running language runtimes in the browser</h2><p>The modern way to run arbitrary programs in the browser seems to be WebAssembly System Interface (WASI<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>) — an executable binary format based on WebAssembly. With WASI, you compile a program (originally written in C, Rust, Go, or some other language) into a WASI binary and then run it with a WASI runtime (there are a number of these runtimes from different vendors).</p><p>Just as we can compile an arbitrary program into WASI binary, we can take a language interpreter like Lua or CPython, compile it into WASI, and run it with the WASI runtime to execute Lua or Python code. In practice, however, it's not that easy, because WASI compilers do not (yet) implement all the features of traditional compilers like GCC.</p><p>Fortunately, VMWare Labs has already done the hard part and compiled PHP<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>, Python<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> and Ruby<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> into WASI. So all I had to do was publish the WASI binaries as NPM packages to make them available on the CDN. I've also compiled Lua<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> and SQLite<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup> to WASI.</p><blockquote><p>There is also Kohei Tokunaga's container2wasm initiative<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup>, which converts arbitrary Docker images into WASI binaries. It looks promising, but it generates 100+ MB binaries for even the smallest Alpine-based images. And since downloading hundreds of megabytes just to read an interactive article is probably not the best idea, this approach is not very practical (yet).</p></blockquote><p>Language runtimes compiled into WASI are one part of the equation. The other one is the WASI runtime (the thing that runs the binaries) capable of working in the browser. I chose the Runno<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup> runtime by Ben Taylor because it's simple and lightweight (27 KB).</p><p>The last step was to modify the JavaScript widget<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup> to support pluggable engines (WASI is one of them).</p><p>And that was it!</p><h2 id="showcase">Showcase</h2><p>Here are some interactive code snippets implemented as described above. Note that the language runtime is downloaded when you click the Run button, so the first run may take some time. Subsequent runs are almost instantaneous.</p><h3 id="python">Python</h3><p>Executes the code using the Python 3.12 WASI runtime (26.3 MB).</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>greet</span>(<span>name</span>):
</span></span><span><span>    <span>print</span>(<span>f</span><span>"Hello, </span><span>{</span><span>name</span><span>}</span><span>!"</span>)
</span></span><span><span>
</span></span><span><span><span>greet</span>(<span>"World"</span>)
</span></span></code></pre></div><codapi-snippet engine="wasi" sandbox="python" editor="basic"></codapi-snippet><h3 id="php">PHP</h3><p>Executes the code using the PHP 8.2 WASI runtime (13.2 MB).</p><div><pre tabindex="0"><code data-lang="php"><span><span><span>function</span> <span>greet</span>(<span>$name</span>) {
</span></span><span><span>    <span>echo</span> <span>"Hello, </span><span>$name</span><span>!"</span>;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>greet</span>(<span>"World"</span>);
</span></span></code></pre></div><codapi-snippet engine="wasi" sandbox="php" editor="basic" template="main.php"></codapi-snippet><h3 id="ruby">Ruby</h3><p>Executes the code using the Ruby 3.2 WASI runtime (24.5 MB).</p><div><pre tabindex="0"><code data-lang="ruby"><span><span><span>def</span> <span>greet</span>(<span>name</span>)
</span></span><span><span>  <span>puts</span> <span>"Hello, </span><span>#{</span><span>name</span><span>}</span><span>!"</span>
</span></span><span><span><span>end</span>
</span></span><span><span>
</span></span><span><span><span>greet</span>(<span>"World"</span>)
</span></span></code></pre></div><codapi-snippet engine="wasi" sandbox="ruby" editor="basic"></codapi-snippet><h3 id="lua">Lua</h3><p>Executes the code using the Lua 5.4 WASI runtime (330 KB).</p><div><pre tabindex="0"><code data-lang="lua"><span><span><span>function</span> <span>greet</span>(<span>name</span>)
</span></span><span><span>  <span>print</span>(<span>"Hello, "</span> <span>..</span> <span>name</span> <span>..</span> <span>"!"</span>)
</span></span><span><span><span>end</span>
</span></span><span><span>
</span></span><span><span><span>greet</span>(<span>"World"</span>)
</span></span></code></pre></div><codapi-snippet engine="wasi" sandbox="lua" editor="basic"></codapi-snippet><h3 id="javascript">JavaScript</h3><p>Executes the code using the AsyncFunction<sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup>.</p><div><pre tabindex="0"><code data-lang="js"><span><span><span>const</span> <span>greet</span> <span>=</span> (<span>name</span>) =&gt; {
</span></span><span><span>    <span>console</span>.<span>log</span>(<span>`Hello, </span><span>${</span><span>name</span><span>}</span><span>!`</span>);
</span></span><span><span>};
</span></span><span><span>
</span></span><span><span><span>greet</span>(<span>"World"</span>);
</span></span></code></pre></div><codapi-snippet engine="browser" sandbox="javascript" editor="basic"></codapi-snippet><h3 id="fetch">Fetch</h3><p>Executes the code using the Fetch API<sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup>.</p><div><pre tabindex="0"><code data-lang="js"><span><span><span>POST</span> <span>https</span><span>:</span><span>//httpbingo.org/dump/request
</span></span></span><span><span><span></span><span>content</span><span>-</span><span>type</span><span>:</span> <span>application</span><span>/</span><span>json</span>
</span></span><span><span>
</span></span><span><span>{ <span>"message"</span><span>:</span> <span>"hello"</span> }
</span></span></code></pre></div><codapi-snippet engine="browser" sandbox="fetch" editor="basic"></codapi-snippet><h3 id="sqlite">SQLite</h3><p>Executes the code using the SQLite 3.44 WASI runtime (2.1 MB).</p><div><pre tabindex="0"><code data-lang="sql"><span><span><span>select</span> <span>id</span>, <span>name</span>, <span>department</span>
</span></span><span><span><span>from</span> <span>employees</span>
</span></span><span><span><span>order</span> <span>by</span> <span>id</span> <span>limit</span> <span>3</span>;
</span></span></code></pre></div><codapi-snippet engine="wasi" sandbox="sqlite" editor="basic" template="employees.sql"></codapi-snippet><h2 id="advanced-features">Advanced features</h2><p>Because the WASI runtime plugs into the existing architecture, WASI-powered code snippets support advanced Codapi features such as templates or code cells.</p><p>Templates<sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup> allow you to hide some code behind the scenes and show only the relevant part. For example, in the SQLite example above, the <code>employees</code> table is created as part of the template, so the snippet can take it for granted:</p><div><pre tabindex="0"><code data-lang="sql"><span><span><span>select</span> <span>id</span>, <span>name</span>, <span>department</span>
</span></span><span><span><span>from</span> <span>employees</span>
</span></span><span><span><span>order</span> <span>by</span> <span>id</span> <span>limit</span> <span>3</span>;
</span></span></code></pre></div><codapi-snippet engine="wasi" sandbox="sqlite" editor="basic" template="employees.sql"></codapi-snippet><p>Code cells<sup id="fnref:14"><a href="#fn:14" role="doc-noteref">14</a></sup> allow you to make code snippets depend on each other. For example, the first snippet defines the <code>wrap</code> function, while the second snippet uses it:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> <span>textwrap</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>wrap</span>(<span>text</span>, <span>width</span><span>=</span><span>20</span>):
</span></span><span><span>    <span>"""Wraps the text so every line is at most width characters long."""</span>
</span></span><span><span>    <span>return</span> <span>textwrap</span><span>.</span><span>fill</span>(<span>text</span>, <span>width</span>)
</span></span></code></pre></div><codapi-snippet id="cell-1" engine="wasi" sandbox="python" editor="basic"></codapi-snippet><div><pre tabindex="0"><code data-lang="python"><span><span><span>text</span> <span>=</span> (
</span></span><span><span>    <span>"Python is a programming language that lets you work quickly "</span>
</span></span><span><span>    <span>"and integrate systems more effectively."</span>
</span></span><span><span>)
</span></span><span><span><span>print</span>(<span>wrap</span>(<span>text</span>))
</span></span></code></pre></div><codapi-snippet id="cell-2" engine="wasi" sandbox="python" editor="basic" depends-on="cell-1"></codapi-snippet><h2 id="usage">Usage</h2><p>To use native browser playgrounds (e.g. JavaScript or Fetch), include the <code>snippet.js</code> script and add the <code>codapi-snippet</code> element next to the static code example. Use the <code>browser</code> engine:</p><div><pre tabindex="0"><code data-lang="html"><span><span>&lt;<span>pre</span>&gt;console.log("hello")&lt;/<span>pre</span>&gt;
</span></span><span><span>
</span></span><span><span>&lt;<span>codapi-snippet</span> <span>engine</span><span>=</span><span>"browser"</span> <span>sandbox</span><span>=</span><span>"javascript"</span> <span>editor</span><span>=</span><span>"basic"</span>&gt;
</span></span><span><span>&lt;/<span>codapi-snippet</span>&gt;
</span></span><span><span>
</span></span><span><span>&lt;<span>script</span> <span>src</span><span>=</span><span>"https://unpkg.com/@antonz/codapi@0.12.0/dist/snippet.js"</span>&gt;&lt;/<span>script</span>&gt;
</span></span></code></pre></div><p>To use WASI-powered playgrounds (e.g. Python or SQLite), include two additional scripts and use the <code>wasi</code> engine:</p><div><pre tabindex="0"><code data-lang="html"><span><span>&lt;<span>pre</span>&gt;print("hello")&lt;/<span>pre</span>&gt;
</span></span><span><span>
</span></span><span><span>&lt;<span>codapi-snippet</span> <span>engine</span><span>=</span><span>"wasi"</span> <span>sandbox</span><span>=</span><span>"python"</span> <span>editor</span><span>=</span><span>"basic"</span>&gt;&lt;/<span>codapi-snippet</span>&gt;
</span></span><span><span>
</span></span><span><span>&lt;<span>script</span> <span>src</span><span>=</span><span>"https://unpkg.com/@antonz/runno@0.6.1/dist/runno.js"</span>&gt;&lt;/<span>script</span>&gt;
</span></span><span><span>&lt;<span>script</span> <span>src</span><span>=</span><span>"https://unpkg.com/@antonz/codapi@0.12.0/dist/engine/wasi.js"</span>&gt;&lt;/<span>script</span>&gt;
</span></span><span><span>&lt;<span>script</span> <span>src</span><span>=</span><span>"https://unpkg.com/@antonz/codapi@0.12.0/dist/snippet.js"</span>&gt;&lt;/<span>script</span>&gt;
</span></span></code></pre></div><p>To switch from in-browser to server-side playgrounds (which can run virtually any software), remove the <code>engine</code> attribute:</p><div><pre tabindex="0"><code data-lang="html"><span><span>&lt;<span>pre</span>&gt;
</span></span><span><span>fn main() {
</span></span><span><span>    println!("Hello, World!");
</span></span><span><span>}
</span></span><span><span>&lt;/<span>pre</span>&gt;
</span></span><span><span>
</span></span><span><span>&lt;<span>codapi-snippet</span> <span>sandbox</span><span>=</span><span>"rust"</span> <span>editor</span><span>=</span><span>"basic"</span>&gt;&lt;/<span>codapi-snippet</span>&gt;
</span></span></code></pre></div><p>See the documentation<sup id="fnref:15"><a href="#fn:15" role="doc-noteref">15</a></sup> for details.</p><h2 id="summary">Summary</h2><p>WASI-powered sandboxes allow code snippets to run completely in-browser, with no server involved. They may take some time and traffic to initialize the runtime, but after that they run almost instantly.</p><p>As implemented in Codapi, they fit nicely into the overall architecture, providing access to features like templates and code cells. You can also easily switch from a browser-side to a server-side execution model.</p><p>Give them a try!</p><p><a href="https://github.com/nalgeon/codapi-js/blob/main/docs/browser-only.md">Playgrounds</a> •
<a href="https://github.com/nalgeon/codapi-js">Snippet widget</a> •
<a href="https://codapi.org/">About Codapi</a></p>

<p><em><a href="https://antonz.org/subscribe/"><i></i>&nbsp;<strong>Subscribe</strong></a>
to keep up with new posts.</em></p></div></article></div></div>]]></description>
        </item>
    </channel>
</rss>