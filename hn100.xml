<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 02 Oct 2024 22:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[OpenAI completes deal that values company at $157B (113 pts)]]></title>
            <link>https://www.nytimes.com/2024/10/02/technology/openai-valuation-150-billion.html</link>
            <guid>41722742</guid>
            <pubDate>Wed, 02 Oct 2024 17:04:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/10/02/technology/openai-valuation-150-billion.html">https://www.nytimes.com/2024/10/02/technology/openai-valuation-150-billion.html</a>, See on <a href="https://news.ycombinator.com/item?id=41722742">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/10/02/technology/openai-valuation-150-billion.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[An adult fruit fly brain has been mapped (209 pts)]]></title>
            <link>https://www.economist.com/science-and-technology/2024/10/02/an-adult-fruit-fly-brain-has-been-mapped-human-brains-could-follow</link>
            <guid>41722159</guid>
            <pubDate>Wed, 02 Oct 2024 16:16:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/science-and-technology/2024/10/02/an-adult-fruit-fly-brain-has-been-mapped-human-brains-could-follow">https://www.economist.com/science-and-technology/2024/10/02/an-adult-fruit-fly-brain-has-been-mapped-human-brains-could-follow</a>, See on <a href="https://news.ycombinator.com/item?id=41722159">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><section><div><p><span><a href="https://www.economist.com/science-and-technology" data-analytics="sidebar:section"><span>Science &amp; technology</span></a></span><span> | <!-- -->On the fly</span></p></div><h2>For now, it is the most sophisticated connectome ever made</h2></section><section><figure><img alt="50 largest neurons of the fly brain connectome." fetchpriority="high" width="1280" height="720" decoding="async" data-nimg="1" sizes="(min-width: 960px) 700px, 95vw" srcset="https://www.economist.com/cdn-cgi/image/width=360,quality=80,format=auto/content-assets/images/20241005_STP001.jpg 360w, https://www.economist.com/cdn-cgi/image/width=384,quality=80,format=auto/content-assets/images/20241005_STP001.jpg 384w, https://www.economist.com/cdn-cgi/image/width=480,quality=80,format=auto/content-assets/images/20241005_STP001.jpg 480w, https://www.economist.com/cdn-cgi/image/width=600,quality=80,format=auto/content-assets/images/20241005_STP001.jpg 600w, https://www.economist.com/cdn-cgi/image/width=834,quality=80,format=auto/content-assets/images/20241005_STP001.jpg 834w, https://www.economist.com/cdn-cgi/image/width=960,quality=80,format=auto/content-assets/images/20241005_STP001.jpg 960w, https://www.economist.com/cdn-cgi/image/width=1096,quality=80,format=auto/content-assets/images/20241005_STP001.jpg 1096w, https://www.economist.com/cdn-cgi/image/width=1280,quality=80,format=auto/content-assets/images/20241005_STP001.jpg 1280w, https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20241005_STP001.jpg 1424w" src="https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20241005_STP001.jpg"><figcaption><span>Photograph: Tyler Sloan &amp; Amy Sterling/ FlyW</span></figcaption></figure></section><div><div><p><time datetime="2024-10-02T15:02:44.481Z"> <!-- -->Oct 2nd 2024</time></p></div><section data-body-id="cp2"><p data-component="paragraph"><span data-caps="initial">F</span><small>RUIT FLIES</small> are smart. For a start—the clue is in the name—they can fly. They can also flirt; fight; form complex, long-term memories of their surroundings; and even warn one another about the presence of unseen dangers, such as parasitic wasps. </p></section><div><p><a href="https://s100.copyright.com/AppDispatchServlet?publisherName=economist&amp;publication=economist&amp;title=An%20adult%20fruit%20fly%20brain%20has%20been%20mapped%E2%80%94human%20brains%20could%20follow&amp;publicationDate=2024-10-02&amp;contentID=%2Fcontent%2Fc2v47l1ps9nddstit39seocd2ecb2k1m&amp;type=A&amp;orderBeanReset=TRUE" target="_blank" rel="noreferrer" data-analytics="end_of_article:reuse_this_content"><span>Reuse this content</span></a></p></div></div><div data-test-id="right-hand-rail-ads"></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Fastest Mutexes (449 pts)]]></title>
            <link>https://justine.lol/mutex/</link>
            <guid>41721668</guid>
            <pubDate>Wed, 02 Oct 2024 15:31:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://justine.lol/mutex/">https://justine.lol/mutex/</a>, See on <a href="https://news.ycombinator.com/item?id=41721668">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p>
Oct 2<sup>nd</sup>, 2024 @ <a href="https://justine.lol/index.html">justine's web page</a>
</p>

<p><img src="https://justine.lol/mutex/honeybadger.png" alt="[cosmpolitan honeybadger logo]" width="334" height="200"><br>
Cosmopolitan Honeybadger
</p>

<p>
<a href="https://justine.lol/cosmo3/">Cosmopolitan Libc</a> is well-known for
its <a href="https://justine.lol/ape.html">polyglot fat binary</a> hack
that lets your executables run on six OSes for AMD64 / ARM64. What may
surprise you is that it could also be the best C library for your
production workloads too. To demonstrate this point, let's compare
Cosmo's mutex library with other platforms.

</p><p>
We'll do this by writing a simple test that spawns <strong>30
threads</strong> which increment the same integer <strong>100,000
times</strong>. This tests how well a mutex implementation performs in
the heavily contended use case. In essence, that means the following
(see the segment at the bottom of the page for the full source code).

</p><pre><span>int</span> g_chores;
<span>pthread_mutex_t</span> g_locker = PTHREAD_MUTEX_INITIALIZER;

<span>void</span> *<span>worker</span>(<span>void</span> *arg) {
  <span>for</span> (<span>int</span> i = 0; i &lt; ITERATIONS; ++i) {
    pthread_mutex_lock(&amp;g_locker);
    ++g_chores;
    pthread_mutex_unlock(&amp;g_locker);
  }
  <span>return</span> 0;
}
</pre>

<p>
Now let's start with the exciting part, which are my benchmark results.

</p><h3>Benchmarks</h3>

<p>
Times will be measured in microseconds. Wall time is how long the test
program takes to run. That includes the overhead of spawning and joining
threads. User time is how much CPU time was spent in userspace, and
system time is how much CPU time was spent in the kernel. System and
user time can exceed the actual wall time because multiple threads are
running in parallel.

</p><p>
The first results I'll show are for Windows because Mark Waterman did an
excellent <a href="https://github.com/markwaterman/MutexShootout">mutex
shootout</a> three months ago, where he said, "in the highly contended
scenario, Windows wins the day with its SRWLOCK". Contention is where
mutex implementations show their inequality. Mark was so impressed by
Microsoft's SRWLOCK that he went on to recommend Linux and FreeBSD users
consider targeting Windows if mutex contention is an issue.

</p><div>
<table>
<tbody><tr>
  <th colspan="4"><p>
      Windows Mutex Implementations<br>
      24-core Threadripper 29070WX

</p></th></tr><tr>
  <th>wall<br>time (µs)
  </th><th>user<br>time (µs)
  </th><th>system<br>time (µs)
  </th><th>implementation

</th></tr><tr>
  <td>148,940
  </td><td>328,125
  </td><td>62,500
  </td><td>Cosmopolitan pthread_mutex_t

</td></tr><tr>
  <td>410,416
  </td><td>5,515,625
  </td><td>1,640,625
  </td><td>Microsoft SRWLOCK

</td></tr><tr>
  <td>949,187
  </td><td>7,937,500
  </td><td>5,078,125
  </td><td>Microsoft CRITICAL_SECTION

</td></tr><tr>
  <td>991,750
  </td><td>12,156,250
  </td><td>4,031,250
  </td><td>MSVC 2022 std::mutex

</td></tr><tr>
  <td>1,165,435
  </td><td>24,515,000
  </td><td>15,000
  </td><td>spin lock

</td></tr><tr>
  <td>9,780,803
  </td><td>1,937,000
  </td><td>6,156,000
  </td><td>Cygwin pthread_mutex_t

</td></tr></tbody></table>
</div>

<p>
As we can see, Cosmopolitan Libc mutexes go 2.75x faster than
Microsoft's SRWLOCK (which was previously believed to be the best of the
best) while consuming 18x fewer CPU resources. Cosmopolitan mutexes were
also 65x faster than Cygwin, which like Cosmopolitan provides a POSIX
implementation on Windows. Cygwin's mutexes are so bad that they would
have been better off for this use case just using a spin lock.

</p><p>
Now onto Linux, the lord of all operating systems.

</p><div>
<table>
<tbody><tr>
  <th colspan="4"><p>
      Linux Mutex Implementations<br>
      96-core Threadripper Pro 7995WX

</p></th></tr><tr>
  <th>wall<br>time (µs)
  </th><th>user<br>time (µs)
  </th><th>system<br>time (µs)
  </th><th>implementation

</th></tr><tr>
  <td>36,905
  </td><td>44,511
  </td><td>23,492
  </td><td>Cosmopolitan pthread_mutex_t

</td></tr><tr>
  <td>101,353
  </td><td>150,706
  </td><td>2,724,851
  </td><td>glibc pthread_mutex_t

</td></tr><tr>
  <td>202,423
  </td><td>4,694,749
  </td><td>2,000
  </td><td>spin lock

</td></tr><tr>
  <td>411,013
  </td><td>2,167,898
  </td><td>9,926,850
  </td><td>Musl libc pthread_mutex_t

</td></tr></tbody></table>
</div>

<p>
  Here we see Cosmopolitan mutexes are:

</p><ul>
<li>3x faster than glibc
</li><li>11x faster than glibc
</li><li>42x less CPU time than glibc
</li><li>178x less CPU time than Musl Libc
</li></ul>

<p>
Here's how things actually work in practice. Imagine you have a workload
where all your threads need to do a serialized operation. With Cosmo, if
you're looking at htop, then it's going to appear like only one core is
active, whereas glibc and musl libc will fill up your entire CPU meter.
That's bad news if you're running a lot of jobs on the same server. If
just one of your servers has a mutex bloodbath, then all your resources
are gone, unless you're using cosmo. It's still a new C library and it's
a little rough around the edges. But it's getting so good, so fast, that
I'm starting to view <em>not</em> using it in production as an
abandonment of professional responsibility. The C library is so deeply
embedded in the software supply chain, and so depended upon, that you
really don't want it to be a planet killer. If essential unquestioned
tools are this wasteful then it's no wonder Amazon Cloud makes such a
fortune.

</p><p>
Last but not least, we have MacOS.

</p><div>
<table>
<tbody><tr>
  <th colspan="4"><p>
      MacOS Mutex Implementations<br>
      M2 Ultra

</p></th></tr><tr>
  <th>wall<br>time (µs)
  </th><th>user<br>time (µs)
  </th><th>system<br>time (µs)
  </th><th>implementation

</th></tr><tr>
  <td>52,263
  </td><td>43,202
  </td><td>911,009
  </td><td>Apple Libc

</td></tr><tr>
  <td>54,700
  </td><td>63,055
  </td><td>1,003,674
  </td><td>Cosmopolitan pthread_mutex_t

</td></tr></tbody></table>
</div>

<p>
On MacOS with an M2 ARM64 microprocessor, Apple's Libc slightly
outperforms Cosmopolitan's mutexes. For reasons I do not yet fully
understand, Cosmopolitan's normal mutex implementation doesn't work well
on this platform. It's possibly because the M2 and XNU are in league. So
on MacOS ARM, Cosmopolitan uses a simpler algorithm based on Ulrich
Drepper's
"<a href="https://dept-info.labri.fr/~denis/Enseignement/2008-IR/Articles/01-futex.pdf">Futexes
Are Tricky</a>" paper that basically just farms out all the heavy
lifting to XNU's ulock system calls. That's why performance is nearly
identical to what Apple does.

</p><p>
So in summary, these benchmark results indicate that Cosmopolitan
mutexes, in the best case, will be overwhelmingly better than
alternatives at the contended + tiny critical section use case, and in
the worst case, Cosmopolitan will be roughly as good.

</p><h3>How I Did It</h3>

<p>
The reason why Cosmopolitan Mutexes are so good is because I used a
library called <a href="https://github.com/google/nsync">nsync</a>. It
only has 371 stars on GitHub, but it was written by a distinguished
engineer at Google named Mike Burrows. If you don't know who he is, he's
the guy who coded Google's fiercest competitor, which was Altavista. If
you're not old enough to remember Altavista, it was the first search
engine that was good, and it ran on a single computer.

</p><p>
I've had a lot of fun integrating nsync into Cosmopolitan. I've even
had the opportunity to make upstream contributions. For example, I found
and fixed a bug in his mutex unlock function that had gone undiscovered
for years. I also managed to make contended nsync mutexes go 30% faster
than nsync upstream on AARCH64, by porting it to use C11 atomics. I
wrote new system integration for things like futexes that enable it do
portability at runtime. Lastly I made it work seamlessly with POSIX
thread cancelations.

</p><p>
So how does nsync do it? What are the tricks that it uses? Here's some
of my takes and analysis:

</p><ul>

<li>
nsync uses an optimistic CAS (compare and swap) immediately, so that
locking happens quickly when there's no contention.

</li><li>
When a lock can't be acquired, nsync adds the calling thread to a
doubly linked list of waiters. Each waiter gets its own semaphore on a
separate independent cacheline. This serves an important purpose. Once a
thread enters the wait state, it's no longer touching the main lock. To
understand why that's important, read Ulrich Drepper's paper
"<a href="https://people.freebsd.org/~lstewart/articles/cpumemory.pdf">What
Every Programmer Should Know About Memory</a>". He goes into great depth
on the coherency protocols used by modern microprocessors, where cores
basically talk to each other under the hood about which cachelines
they're using. When multiple cores touch the same ones, that creates a
lot of communication overhead within the processor.

</li><li>
nsync enlists the help of the operating system by using futexes. This is
a great abstraction invented by Linux some years ago, that quickly found
its way into other OSes. On MacOS, futexes are called ulock. On Windows,
futexes are called <code>WaitOnAddress()</code>. The only OS Cosmo
supports that doesn't have futexes is NetBSD, which implements POSIX
semaphores in kernelspace, and each semaphore sadly requires creating a
new file descriptor. But the important thing about futexes and
semaphores is they allow the OS to put a thread to sleep. That's what
lets nsync avoid consuming CPU time when there's nothing to do.

</li><li>
nsync avoids starvation with this concept of a "long wait". If a waiter
gets woken 30 times and fails to acquire the lock internally every time,
then nsync adds a bit to the lock that prevents threads that haven't
waited yet from acquiring. This means that initial CAS at the beginning
will fail for everyone else until the queue has had some time to clear.

</li><li>
nsync makes the use case we benchmarked go fast (contended lock with a
small critical section) using this concept of a "designated waker". This
bit on the main lock is set when a thread is awake and trying to acquire
the lock. In nsync, the unlock function is what's responsible for waking
the next thread in line waiting for the lock. Having this bit allows the
unlocking thread to know it needn't bother waking a second locker since
one is already awake.

</li></ul>

<p>
To learn more of nsync's secrets, you can read the source code here:
<a href="https://github.com/jart/cosmopolitan/tree/master/third_party/nsync/mu.c"><code>cosmopolitan/third_party/nsync/mu.c</code></a>.
See also
<a href="https://github.com/jart/cosmopolitan/tree/master/libc/intrin/pthread_mutex_lock.c"><code>cosmopolitan/libc/intrin/pthread_mutex_lock.c</code></a>.

</p><h3>Online Proof</h3>

<p>
If you want to see a live demo of a piece of software built with Cosmo
mutexes, then do your worst DDOS to the
<a href="http://ipv4.games/">http://ipv4.games/</a> web server. Now this
is truly a game for hackers, competing to dominate the Internet. You're
already playing this game because your IP was just claimed for jart. The
service runs on a GCE VM with 2 cores and so far it's managed to survive
being DDOS'd by botnets as large as 49,131,669 IPs. A lot of that is
thanks to nsync which allowed me to move SQL queries to background
threads which send messages to each other. There are still improvements
to be made, but overall it's held up well. You can even monitor
its <a href="http://ipv4.games/statusz">/statusz</a> health metrics.

</p><h3>Source Code</h3>

<pre><span>#define ITERATIONS 100000
#define THREADS    30</span>

<span>int</span> g_chores;
<span>pthread_mutex_t</span> g_locker = PTHREAD_MUTEX_INITIALIZER;

<span>void</span> *<span>worker</span>(<span>void</span> *arg) {
  <span>for</span> (<span>int</span> i = 0; i &lt; ITERATIONS; ++i) {
    pthread_mutex_lock(&amp;g_locker);
    ++g_chores;
    pthread_mutex_unlock(&amp;g_locker);
  }
  <span>return</span> 0;
}

<span>struct</span> <span>timeval</span> <span>tub</span>(<span>struct</span> <span>timeval</span> a, <span>struct</span> <span>timeval</span> b) {
  a.tv_sec -= b.tv_sec;
  <span>if</span> (a.tv_usec &lt; b.tv_usec) {
    a.tv_usec += 1000000;
    a.tv_sec--;
  }
  a.tv_usec -= b.tv_usec;
  <span>return</span> a;
}

<span>long</span> <span>tomicros</span>(<span>struct</span> <span>timeval</span> x) {
  <span>return</span> x.tv_sec * 1000000ul + x.tv_usec;
}

<span>int</span> <span>main</span>() {
  <span>struct</span> <span>timeval</span> start;
  gettimeofday(&amp;start, 0);

  <span>pthread_t</span> th[THREADS];
  for (int i = 0; i &lt; THREADS; ++i)
    pthread_create(&amp;th[i], 0, worker, 0);
  for (int i = 0; i &lt; THREADS; ++i)
    pthread_join(th[i], 0);
  assert(g_chores == THREADS * ITERATIONS);

  <span>struct</span> <span>rusage</span> ru;
  <span>struct</span> <span>timeval</span> end;
  gettimeofday(&amp;end, 0);
  getrusage(RUSAGE_SELF, &amp;ru);
  printf(<span>"%16ld us real\n"</span>
         <span>"%16ld us user\n"</span>
         <span>"%16ld us sys\n"</span>,
         tomicros(tub(end, start)),
         tomicros(ru.ru_utime),
         tomicros(ru.ru_stime));
}

</pre>

<p>
The reason we care about the contended case, because that's where mutex
implementations show their inequality. Uncontended mutexes usually
perform the same across implementations, and even then you might be
better off with a spin lock, which only takes a few lines:

</p><pre><span>void</span> <span>spin_lock</span>(<span>atomic_int</span> *lock) {
  if (atomic_exchange_explicit(lock, 1, memory_order_acquire)) {
    <span>for</span> (;;) {
      <span>for</span> (;;)
        <span>if</span> (!atomic_load_explicit(lock, memory_order_relaxed))
          <span>break</span>;
      <span>if</span> (!atomic_exchange_explicit(lock, 1, memory_order_acquire))
        <span>break</span>;
    }
  }
}

<span>void</span> <span>spin_unlock</span>(<span>atomic_int</span> *lock) {
  atomic_store_explicit(lock, 0, memory_order_release);
}
</pre>

<p>
Please note that spin locks should really only be used when you have no
other choice. They're useful in kernels, where extreme low level
constraints disallow anything fancy. Spin locks are also a useful
implementation detail in nsync locks. But overall they're bad. I imagine
many developers believe they're good. If so, it's probably because
they've only benchmarked wall time. With locks, it's important to take
CPU time into consideration too. That's why we use
<code>getrusage()</code>.

</p><h2 class="page" id="funding"><a href="#funding">Funding</a></h2>

<p>
  <a href="https://justine.lol/lemuria.png">
    <picture>
      <source srcset="https://worker.jart.workers.dev/sectorlisp2/lemuria.webp" type="image/webp">
      <img src="https://worker.jart.workers.dev/sectorlisp2/lemuria.png" width="850" height="360" alt="[United States of Lemuria - two dollar bill - all debts public and primate]">
    </picture>
  </a>

</p><p>
Funding for The Fastest Mutex was crowdsourced from Justine
Tunney's <a href="https://github.com/sponsors/jart">GitHub sponsors</a>
and <a href="https://www.patreon.com/jart">Patreon subscribers</a>, the
backing of
<a href="https://future.mozilla.org/mieco/">Mozilla's MIECO program</a>,
and the generous contributions of our
<a href="https://discord.gg/FwAVVu7eJ4">developer community</a> on
Discord. Your support is what makes projects like Cosmopolitan possible.
Thank you!

</p>
<img src="https://ipv4.games/claim?name=jart">
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[American WWII bomb explodes at Japanese airport, causing large crater in taxiway (218 pts)]]></title>
            <link>https://www.cnn.com/2024/10/02/travel/wwii-bomb-miyazaki-airport-japan-scli-intl/index.html</link>
            <guid>41721567</guid>
            <pubDate>Wed, 02 Oct 2024 15:22:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2024/10/02/travel/wwii-bomb-miyazaki-airport-japan-scli-intl/index.html">https://www.cnn.com/2024/10/02/travel/wwii-bomb-miyazaki-airport-japan-scli-intl/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=41721567">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-editable="main" data-track-zone="main" data-reorderable="main">  <article data-uri="cms.cnn.com/_components/article/instances/cm1rwxx2i000whzqq8c9whnqb@published" role="main" data-drag-disable="true" data-unselectable="true" data-regwall-disabled="false" data-subscription-only="false" data-paywall-disabled="false">
      
  <section data-tabcontent="Content">
    <main>
                <div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}" data-uri="cms.cnn.com/_components/image/instances/cm1rx7zq6000x356k3dt6urfs@published" data-name="GettyImages-2175357542.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.667" data-original-height="1334" data-original-width="2000" data-url="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2175357542.jpg?c=original" data-editable="lede" data-freewheel-lede="true">
       <picture><source height="383" width="680" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2175357542.jpg?c=16x9&amp;q=h_383,w_680,c_fill/f_webp" type="image/webp"><source height="653" width="1160" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2175357542.jpg?c=16x9&amp;q=h_653,w_1160,c_fill/f_webp" type="image/webp"><source height="605" width="1075" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2175357542.jpg?c=16x9&amp;q=h_605,w_1075,c_fill/f_webp" type="image/webp"><source height="833" width="1480" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2175357542.jpg?c=16x9&amp;q=h_833,w_1480,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2175357542.jpg?c=16x9&amp;q=h_833,w_1480,c_fill" alt="A crater from the explosion at Miyazaki Airport" onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="1334" width="2000"></picture>
    </div>
        
        
        
          <div data-editable="content" itemprop="articleBody" data-reorderable="content">
                  <p><cite>
      <span data-editable="location">Tokyo</span>
      <span data-editable="source">AP</span>
        &nbsp;—&nbsp;
    </cite>
</p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm1rwxx2i000vhzqqcpal5ftf@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            An unexploded American bomb from World War II that had been buried at a Japanese airport exploded Wednesday, causing a large crater in a taxiway and the cancellation of more than 80 flights but no injuries, Japanese officials said.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm1rx1ed80007356k3bbqlcj3@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Land and Transport Ministry officials said there were no aircraft nearby when the bomb exploded at Miyazaki Airport in southwestern Japan.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm1rx1k3z0009356kvyrq0f7n@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Officials said an investigation by the Self-Defense Forces and police confirmed that the explosion was caused by a 500-pound US bomb and there was no further danger. They were determining what caused its sudden detonation.
    </p>

  


    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm1rx1lze000b356ktnn3fkos@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            A video recorded by a nearby aviation school showed the blast spewing pieces of asphalt into the air like a fountain.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm1rx7pbb000v356k5dicgg2t@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Videos broadcast on Japanese television showed a crater in the taxiway reportedly about 7 meters (23 feet) in diameter and 1 meter (3 feet) deep.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm1rx1odc000d356kjghq1wup@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Chief Cabinet Secretary Yoshimasa Hayashi said more than 80 flights had been canceled at the airport, which hopes to resume operations on Thursday morning.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm1rx1vg4000f356ktk4157xu@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Miyazaki Airport was built in 1943 as a former Imperial Japanese Navy flight training field from which some kamikaze pilots took off on suicide attack missions.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm1rx1x2q000h356kw7pc14ig@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            A number of unexploded bombs dropped by the US military during World War II have been unearthed in the area, Defense Ministry officials said.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm1rx6s1p000s356kaecsj4ec@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Hundreds of tons of unexploded bombs from the war remain buried around Japan and are sometimes dug up at construction sites.
    </p>

              </div>
    </main>
  </section>
</article>

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VC Fund gives money back, says the market for mature startups is too weak (140 pts)]]></title>
            <link>https://www.nytimes.com/2024/10/02/technology/crv-vc-fund-returning-money.html</link>
            <guid>41721368</guid>
            <pubDate>Wed, 02 Oct 2024 15:04:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/10/02/technology/crv-vc-fund-returning-money.html">https://www.nytimes.com/2024/10/02/technology/crv-vc-fund-returning-money.html</a>, See on <a href="https://news.ycombinator.com/item?id=41721368">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/10/02/technology/crv-vc-fund-returning-money.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The bunkbed conjecture is false (114 pts)]]></title>
            <link>https://igorpak.wordpress.com/2024/10/01/the-bunkbed-conjecture-is-false/</link>
            <guid>41721318</guid>
            <pubDate>Wed, 02 Oct 2024 15:01:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://igorpak.wordpress.com/2024/10/01/the-bunkbed-conjecture-is-false/">https://igorpak.wordpress.com/2024/10/01/the-bunkbed-conjecture-is-false/</a>, See on <a href="https://news.ycombinator.com/item?id=41721318">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>What follows is an unusual story of perseverance. We start with a conjecture and after some plot twists  end up discussing the meaning of truth. While the title is a spoiler, you might not be able to guess how we got there…</p>



<h4>The conjecture</h4>



<p>The <strong><em><a href="https://en.wikipedia.org/wiki/Bunkbed_conjecture">bunkbed conjecture</a></em></strong> (BBC) is a basic claim about random subgraphs. Start with a finite graph <em><strong>G</strong></em><strong>=(<em>V</em>,<em>E</em>)</strong> and consider a product graph <strong><em>G</em> <em>x</em> <em>K</em><sub>2</sub></strong> obtained by connecting the corresponding vertices on levels <strong><em>V</em><sup>(1)</sup></strong> and <strong><em>V</em><sup>(2)</sup></strong>.  Sort of like a <a href="https://i.etsystatic.com/22453533/r/il/81fe4f/3189746867/il_1588xN.3189746867_dodu.jpg">bunkbed</a>.  Now consider random subgraphs of a this product graph.  </p>



<blockquote>
<p><strong><mark>Bunkbed Conjecture:</mark></strong>  The probability that vertices <em><strong>u</strong></em><sup><strong>(1)</strong></sup> and <em><strong>v</strong></em><sup><strong>(1)</strong></sup> are connected is greater or equal than the probability that vertices <em><strong>u</strong></em><sup><strong>(1)</strong></sup> and <strong><em>v</em></strong><sup><strong>(2)</strong></sup> are connected.</p>
</blockquote>



<p>In other words, the probability of connecting two vertices on the same level cannot be smaller than when connect vertices on different levels. This is completely obvious, of course! And yet the conjecture this problem defeated several generations of probabilists and remained open until now. For a good reason, of course. It was <em><a href="https://as1.ftcdn.net/v2/jpg/04/87/13/32/1000_F_487133220_dPDd98wC3pFZcHxs4wfEnZYDAO4wDqLd.jpg">false</a></em>!</p>



<p>The origins of the conjecture are murky, but according to <a href="https://www.jstor.org/stable/2652916">van den Berg and Kahn</a> it was conjectured by <a href="https://en.wikipedia.org/wiki/Pieter_Kasteleyn">Kasteleyn </a>in the early 1980s. There are many versions of this conjecture; notably one can condition on the subset of vertical edges and ask the same question. Many partial results are known, as well as results for other probabilistic models. The conjecture is <em><strong><mark>false</mark></strong></em> nonetheless!</p>



<h4>The direction</h4>



<p>Why look for a counterexample if the conjecture is so obviously true?  Well, because you always should.  For any conjecture.  Especially if everyone else is so <em><strong>sure</strong></em>, as in <em>completely absolutely sure without a doubt</em>, that the conjecture is true.  <strong><em><mark>What if they are all wrong?</mark></em></strong>  I discuss this at length in <a href="https://wp.me/p211iQ-uT">this blog post</a>, so there is no need to rehash this point.  </p>



<h4>The counterexample</h4>



<p>We disprove the conjecture in a<strong> <a href="https://www.math.ucla.edu/~pak/papers/BunkBed4.pdf">joint paper</a></strong> with <a href="https://www.math.ucla.edu/~gladkovna/">Nikita Gladkov</a> (UCLA) and <a href="https://web.mit.edu/azimin/www/">Alexandr Zimin</a> (MIT), both graduate students.  Roughly speaking we take the following 3-hypergraph from a <a href="https://arxiv.org/abs/2406.01790">recent paper</a> by <a href="https://scholar.google.com/citations?user=PabtgaQAAAAJ&amp;hl=en">Hollom</a>.  </p>



<figure><a href="https://igorpak.wordpress.com/wp-content/uploads/2024/10/hollom-graph.png"><img data-attachment-id="3238" data-permalink="https://igorpak.wordpress.com/2024/10/01/the-bunkbed-conjecture-is-false/hollom-graph/" data-orig-file="https://igorpak.wordpress.com/wp-content/uploads/2024/10/hollom-graph.png" data-orig-size="1043,1326" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Hollom-graph" data-image-description="" data-image-caption="" data-medium-file="https://igorpak.wordpress.com/wp-content/uploads/2024/10/hollom-graph.png?w=236" data-large-file="https://igorpak.wordpress.com/wp-content/uploads/2024/10/hollom-graph.png?w=595" tabindex="0" role="button" width="805" height="1023" src="https://igorpak.wordpress.com/wp-content/uploads/2024/10/hollom-graph.png?w=805" alt=""></a></figure>



<p>We then replace each yellow triangle with the following gadget using <em>n</em>=1204, placing <em>a</em> in the shaded vertex, while <em><strong>v</strong></em><sub><strong>1</strong></sub> and <em><strong>v</strong></em><sub><strong>n</strong></sub> are placed in the other vertices of the triangle (so the red path goes into the red path).  For a stronger version of the conjecture that’s all there is.  For a weaker version, some additional tweaks needed to be made (they are not so important).  And we are done!  </p>



<figure><a href="https://igorpak.wordpress.com/wp-content/uploads/2024/10/image.png"><img data-attachment-id="3242" data-permalink="https://igorpak.wordpress.com/2024/10/01/the-bunkbed-conjecture-is-false/image-14/" data-orig-file="https://igorpak.wordpress.com/wp-content/uploads/2024/10/image.png" data-orig-size="799,436" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://igorpak.wordpress.com/wp-content/uploads/2024/10/image.png?w=300" data-large-file="https://igorpak.wordpress.com/wp-content/uploads/2024/10/image.png?w=595" tabindex="0" role="button" width="799" height="436" src="https://igorpak.wordpress.com/wp-content/uploads/2024/10/image.png?w=799" alt=""></a></figure>



<p>The resulting graph is has 7523 vertices and 15654 edges.   The difference between probabilities for paths between <strong><em>u</em><sub>1</sub></strong> and <strong><em>u</em><sub>10</sub></strong> at the same and different levels as in the conjecture is astronomically small, on the order of -10<sup>-6500</sup>.  But it’s negative, which is all we need.  Very very roughly speaking, the red path is the only path which avoids shaded vertices and creates a certain bias which give this probability gap.  Formalizing this is a bit technical.  </p>



<h4>The experiments</h4>



<p>Of course, the obvious way to verify our counterexample computationally would fail miserably — the graph is much too large.  Instead, we give a relatively elementary completely combinatorial disproof of the BBC that is accessible to a wide audience.  I would rather not rehash technical details and ideas in the proof — it’s all in <strong><a href="https://www.math.ucla.edu/~pak/papers/BunkBed4.pdf">our paper</a></strong>, which is only 12 pages!   See also the <a href="https://github.com/Kroneckera/bunkbed-counterexample">GitHub code</a> and some <a href="https://web.mit.edu/azimin/www/notebooks/bunkbed_counterexample.html">explanation</a>. </p>



<p>I do want to mention that giving formal disproof was not our first choice.  It’s what we ended up doing after many failures.  There is always a bit of a stigma people have about publicly discussing their failures.  I know very few examples, only <a href="https://www.scribd.com/document/549797884/How-Not-to-Prove-Poincare-Conjecture">this one</a> famous enough to be mentioned.   So let me mention briefly how we failed.  </p>



<p>Since I was sure the bunkbed conjecture is false (for reasons somewhat different from my contrarian philosophy), we started with a myriad of computer experiments trying all small graphs.  When those failed, we tried to use AI and other computer assisted tools.  We burned many hours on a giant <em><a href="https://www.hoffman2.idre.ucla.edu/">UCLA Hoffman2 Cluster</a></em> getting closer for a while.  In hindsight, we didn’t look in the right place, obviously.  After several months of computer experiments and no clear counterexample, it felt we are wasting time.  We then thought a bit more about philosophy of what we are doing and stopped.  </p>



<p>Before I tell you why we stopped, let me make a <strong>general recommendation</strong>.  Please do try computer experiments for whatever you are working on.  Make an effort to think it through and design a good experiment.  Work hard to test as much as your computer technology allows.  If you need some computing power, ask around.  Your university might just have the resources.  Occasionally, you can even <a href="https://epubs.siam.org/doi/abs/10.1137/120867366">ask a private company</a> to donate theirs.  </p>



<p>If you succeed, write a paper and publish it.  Make your code and work notes publicly available.  If you fail, do exactly the same.  If the journals refuse to publish your paper, just keep it on the arXiv.  Other people in your area would want to know.  And as far as the NSF is concerned, all of this is “work product”.  You can’t change the nature of the problem and the results you are getting, but you deserve the credit regardless.  </p>



<p><strong>Let me repeat:</strong> Do not fear telling other you have not succeeded in your computer testing.  Fear others making the same mistakes or repeating the same work that you did.</p>



<h4>The curse</h4>



<p>One reason we stopped is because in our initial rush to testing we failed to contemplate the implications of Monte Carlo testing of even moderately large graphs.  Here is a quote from the paper:</p>



<blockquote>
<p>Suppose we did find a potential counterexample graph with only <em>m</em>=100 edges and the probability gap was large enough to be statistically detectable. Since analyzing all of 2<sup>m</sup> ≈ 10<sup>30</sup> subgraphs is not feasible, our Monte Carlo simulations could only confirm the desired inequality with high probability. While this probability could be amplified by repeated testing, one could never formally disprove the bunkbed conjecture this way, of course. </p>



<p><br>This raises somewhat uncomfortable questions whether the mathematical community is ready to live with an uncertainty over validity of formal claims that are only known with high probability. It is also unclear whether in this imaginary world the granting agencies would be willing to support costly computational projects to further increase such probabilities (cf. [<a href="https://arxiv.org/abs/1609.03543">Garrabrant+’16</a>], [<a href="https://arxiv.org/abs/math/9301202">Zeilberger’93</a>]). Fortunately, our failed computational effort avoided this dystopian reality, and we were able to disprove the bunkbed conjecture by a formal argument.</p>
</blockquote>



<p>Societal implications aside, it is an interesting question whether a reputable math journal <em>should </em>accept a counterexample that is tested with 99.99% confidence, and the results can be replicated and rechecked by others.  Five sigma may be a <a href="https://home.cern/resources/faqs/five-sigma">gold standard</a> in nuclear physics, but math journals tend to prefer 100% correctness (even though <a href="https://retractionwatch.com/2017/02/13/journal-retracts-paper-state-senator-former-mathematician/">some papers</a> they publish are 100% incorrect).  </p>



<p>What I <em>do know</em>, is that most journals would refuse to even <em>consider</em> a “five sigma counterexample”.  While details of the situations differ quite a bit, I knew <a href="https://sites.math.rutgers.edu/~zeilberg/Opinion121.html">what happened</a> to the (rather interesting) Sills–Zeilberger paper, which was <a href="https://www.tandfonline.com/doi/full/10.1080/10236198.2012.678837">eventually published</a>, but not after several desk rejections.  But PhD students need jobs in reality, not in theory.  That is really why we stopped.  Why persevere and create controversy when you can just try doing something else?  </p>



<p><strong>P.S.</strong> There is yet another layer to all of this. Back in 1999, I asked <a href="https://en.wikipedia.org/wiki/Avi_Wigderson">Avi Wigderson</a> if <strong>P=BPP</strong>? He said “<em>Yes</em>“. Last week I asked him again. This is 25 years later, almost to the day. He said “<em>Yes, I am absolutely sure of that</em>.” It’s one of his <a href="https://www.zdnet.com/article/for-turing-award-winner-everything-is-computation-and-some-problems-are-unsolvable/">favorite conjectures</a>, of course. If he is right, every probabilistic counterexample can be turned into deterministic. In other words, there would be a fully rigorous way to estimate both probabilities and prove on a computer that the conjecture is false. But you must have guessed what I was thinking when I heard what he said — now he used “<em>absolutely sure</em>“…</p>
						
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a game you can play without anyone knowing (No Visuals/Sound) (343 pts)]]></title>
            <link>https://apps.apple.com/us/app/tik/id6720712299</link>
            <guid>41719531</guid>
            <pubDate>Wed, 02 Oct 2024 11:48:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apps.apple.com/us/app/tik/id6720712299">https://apps.apple.com/us/app/tik/id6720712299</a>, See on <a href="https://news.ycombinator.com/item?id=41719531">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
<!---->
<!---->
<!---->
    

<!---->
    <section>
      <h2>iPhone Screenshots</h2>
  
</section>


  <div dir="" data-test-bidi=""><p>There are times in life, when we have to be present. We can't be seen looking at our phones, no matter how boring the activity in front of us may be. So we just sit there, losing brain cells by the second. At least, thats how things were BEFORE TIK!</p><p>LIFE AFTER TIK:<br>Boredom is a thing of the past! Tik has no sounds or visuals, meaning you can play it anytime, anywhere! Just hold your phone in your hand, whether it be under a desk or in your pocket, and tap away! Because Tik uses haptic feedback (the thing your phone does when you make a purchase, or hold the send button in iMessages for a couple seconds) in order to provide interactive, immersive gameplay, there's no need to ever look at the screen or hear anything!</p><p>OK. SO HOW DOES TIK WORK?<br>You will feel a series of 5 to 8 “Tiks” (haptic vibrations). Then, try to copy the rhythm of the sequence by tapping it anywhere on the screen. If the timing of your taps is close enough, you win! If not, Tik will replay the sequence so you can try again.</p><p>THIS SEEMS TOO GOOD TO BE TRUE!<br>It Is! Well, almost, because it is true! Download Tik today and say goodbye to mundane meetings and boring presentations forever!</p><p>HAVE A QUESTION?<br>We would love to hear it! Please reach out to us at freestyle.feedback@gmail.com</p><p>(One more thing. We realize that Tik may be accessible to those with certain disabilities since the game relies solely on haptic feedback. We have added accessibility support within the app, but if there is anything else we can add to make the app more accessible please let us know by reaching out at freestyle.feedback@gmail.com or by telling us in a review. Thank you!)</p></div>

<!---->
  <div>
      <h2>What’s New</h2>
        

    </div>

<!---->
<!---->
<!---->
<!---->
  <section>
  <div>
    <h2>
      App Privacy
    </h2>

    


  </div>

  <p>
    The developer, <span>Rohan Sachdeva</span>, indicated that the app’s privacy practices may include handling of data as described below. For more information, see the <a href="https://sites.google.com/view/tik-game/home">developer’s privacy policy</a>.
  </p>

  <div>
        
        <h3>Data Not Collected</h3>
        <p>The developer does not collect any data from this app.</p>
<!---->      </div>

    <p>Privacy practices may vary, for example, based on the features you use or your age. <a href="https://apps.apple.com/story/id1538632801">Learn&nbsp;More</a></p>
</section>


<section>
  <div>
    <h2>Information</h2>
    <dl>
        <p>
          <dt>Seller</dt>
          <dd>
              Rohan Sachdeva
          </dd>
        </p>
        <p>
          <dt>Size</dt>
          <dd aria-label="367.6 kilobytes">367.6 KB</dd>
        </p>
        <p>
          <dt>Category</dt>
          <dd>
              <a href="https://itunes.apple.com/us/genre/id6014" data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;actionUrl&quot;:&quot;https://itunes.apple.com/us/genre/id6014&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;GenrePage&quot;}">
                Games
              </a>
          </dd>
        </p>
      <div>
        <dt>Compatibility</dt>
          <dd>
              <dl>
                <dt>
                  iPhone
                </dt>
                <dd>Requires iOS 14.0 or later.
                </dd>
              </dl>
              <dl>
                <dt>
                  iPod&nbsp;touch
                </dt>
                <dd>Requires iOS 14.0 or later.
                </dd>
              </dl>
          </dd>
      </div>
<!---->      
      
<!---->      <p>
        <dt>Copyright</dt>
        <dd>© 2024 Rohan Sachdeva</dd>
      </p>
        <p>
          <dt>Price</dt>
          <dd>$0.95</dd>
        </p>
<!---->
    </dl>
  </div>
  <div>
    <ul>
<!---->        <li>
          <a data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;LinkToAppSupport&quot;}" href="https://sites.google.com/view/tik-game/home">
            App Support
          </a>
        </li>
        <li>
          <a data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;LinkToPrivacyPolicy&quot;}" href="https://sites.google.com/view/tik-game/home">
            Privacy Policy
          </a>
        </li>
<!----><!---->    </ul>
  </div>
</section>

<section>
  <ul>
<!---->      <li>
        <a data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;LinkToAppSupport&quot;}" href="https://sites.google.com/view/tik-game/home">
          App Support
        </a>
      </li>
<!---->      <li>
        <a data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;LinkToPrivacyPolicy&quot;}" href="https://sites.google.com/view/tik-game/home">
          Privacy Policy
        </a>
      </li>
  </ul>
</section>

  <section>
    <p>
      <h2>Supports</h2>
    </p>
    <ul>
        <li>
          <img src="https://apps.apple.com/assets/images/supports/supports-FamilySharing@2x-f58f31bc78fe9fe7be3565abccbecb34.png" alt="" role="presentation">
          <div>
              <h3 dir="ltr">
    Family Sharing
</h3>


              <h4 dir="">
        

                    <p data-test-bidi="">Up to six family members can use this app with Family&nbsp;Sharing enabled.</p>

    


<!----></h4>


          </div>
        </li>
    </ul>
  </section>

<!---->
    <section>
      <p>
        <h2>
          More By This Developer
        </h2>
        <!---->
      </p>

      
    </section>

<!---->

<!---->

<!----></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You Too Could Have Made Curl (130 pts)]]></title>
            <link>https://daniel.haxx.se/blog/2024/02/06/fosdem-2024-you-too-could-have-made-curl/</link>
            <guid>41719031</guid>
            <pubDate>Wed, 02 Oct 2024 10:24:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daniel.haxx.se/blog/2024/02/06/fosdem-2024-you-too-could-have-made-curl/">https://daniel.haxx.se/blog/2024/02/06/fosdem-2024-you-too-could-have-made-curl/</a>, See on <a href="https://news.ycombinator.com/item?id=41719031">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">

	<div id="primary" role="main">
			
<article id="post-24163">
	
		<p><img width="672" height="345" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/02/http3-for-everyone-audience.jpg" alt="" decoding="async">		</p>

		
	<!-- .entry-header -->

		<div>
		
<figure><p>
<iframe title="you too could have made curl - Daniel Stenberg at FOSDEM 2024" width="474" height="267" src="https://www.youtube.com/embed/kCJmAyUr1j4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>



<p>This is the video recording of my talk with this title, done at February 4, 2024 10:00 in the K1.105 room at FOSDEM 2024. The room can hold some 800 people but there were a few hundred seats still unoccupied. Several people I met up with later have insisted that 10 am on a Sunday is way too early for attending talks…</p>



<p>When I was about to start my talk, the slides would not show on the projector. Yeah, sigh. Nothing surprising maybe, but you always hope you can avoid these problems – in particular in the last moment with a huge audience waiting.</p>



<p>There was this separate video monitor laptop that clearly showed that my laptop would output the correct thing – in a proper resolution (1280 x 720 as per auto-negotiation), but the projector refused to play ball. The live stream could also see my output, so the problem was somehow from the video box to the projector.</p>



<p>Several people eventually got involved, things were rebooted multiple times, cables were yanked and replugged in again. First after I installed <a href="https://christian.amsuess.com/tools/arandr/">arandr</a> and forced-updated the resolution of my HDMI output to 1920×1080  the projector would suddenly show my presentation. (Later on I was told that people had <em>the same</em> problem in this room the day before…)</p>



<p>That was about nine minutes of technical difficulties that is cut out from the recording. Nine minutes to test my nerves and presentation finesse as I had to adapt.</p>
	</div><!-- .entry-content -->
	
	</article><!-- #post-24163 -->
		<nav>
		<h2>
			Post navigation		</h2>
		<!-- .nav-links -->
		</nav><!-- .navigation -->
		
<!-- #comments -->
		</div><!-- #primary -->

<!-- #content-sidebar -->
<div id="secondary">
		<h2>curl, open source and networking</h2>
	
	
		<!-- #primary-sidebar -->
	</div><!-- #secondary -->

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Automattic–WP Engine Term Sheet (109 pts)]]></title>
            <link>https://automattic.com/2024/10/01/wpe-terms/</link>
            <guid>41718485</guid>
            <pubDate>Wed, 02 Oct 2024 08:41:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://automattic.com/2024/10/01/wpe-terms/">https://automattic.com/2024/10/01/wpe-terms/</a>, See on <a href="https://news.ycombinator.com/item?id=41718485">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">
	<main id="content">
				
		
						
<p>One of the many lies in <a href="https://wpengine.com/wp-content/uploads/2024/09/Cease-and-Desist-Letter-to-Automattic-and-Request-to-Preserve-Documents-Sent.pdf">Silver Lake and WP Engine’s C&amp;D</a> was their claim that Automattic demanded money from them moments before our <a href="https://www.youtube.com/watch?v=fnI-QcVSwMU&amp;t=149s">CEO Matt Mullenweg gave his keynote at WordCamp US</a>. </p>



<p>That is not true. Automattic asked for a verbal agreement that WP Engine would give some percentage of their revenue back into WordPress, either in the form of a trademark agreement or employee hours spent on core WordPress.</p>



<p>For transparency, <a href="https://automattic.com/wp-content/uploads/2024/09/term-sheet-wp-engine-inc.-automattic-trademark-license_09.19.2024-1.pdf">Automattic is publishing the full term sheet</a> WP Engine was offered on September 20<sup>th</sup>. Given WP Engine’s behavior, deception, and incompetence since September 20<sup>th</sup> these terms are no longer sufficient.</p>



<p>Lee Wittlinger and Heather Brunner had ample time to resolve this conflict. In an effort to continue our transparency, here are some of the meetings we’ve had with WP Engine over the past 20 months:</p>



<p><strong>February 2023</strong></p>



<ul>
<li>February 23: Matt Mullenweg (Automattic CEO) and Heather Brunner (WP Engine CEO) meet for lunch at Eunice Restaurant in Houston.</li>
</ul>



<p><strong>March 2023</strong></p>



<ul>
<li>March 21: <a href="https://www.youtube.com/watch?v=kKHY6QPnclQ">Matt and Matías Ventura (WP’s Chief Architect) speak at WP Engine’s DE{CODE} conference</a>.</li>
</ul>



<p><strong>November 2023</strong></p>



<ul>
<li>November 28: Matt and Heather Brunner speak via phone.</li>
</ul>



<p><strong>January 2024</strong></p>



<ul>
<li>January 26: Matt and Heather Brunner meet via Zoom.</li>



<li>January 28: Matt, Toni Schneider, and Heather Brunner meet via Zoom. Matt was <a href="https://ma.tt/2024/02/samattical/">heading out on sabbatical</a> and Toni would be continuing discussions as Automattic’s interim CEO.</li>
</ul>



<p><strong>February 2024</strong></p>



<ul>
<li>February 5: Toni and Heather Brunner discuss trademarks and commercial agreement.</li>
</ul>



<p><strong>March 2024</strong></p>



<ul>
<li>March 22: Toni and Paul Maiorana from Automattic meet with Heather, Ezinne Udezue, and Ramadass Prabhakar from WP Engine.</li>



<li>March 29: Toni and Heather Brunner discuss agreement terms via email.</li>
</ul>



<p><strong>May 2024</strong></p>



<ul>
<li>May 30: Automattic shares first term sheet with WP Engine via email.</li>
</ul>



<p><strong>June 2024</strong></p>



<ul>
<li>June 12: Steve Deckert from Automattic meets with Victor Yuan, Carl Hargraves, Ezinne Udezue, and Ramadass Prabhakar from WP Engine.</li>



<li>June 26: Steve follows up with Victor Yuan. Victor requests a call.</li>
</ul>



<p><strong>July 2024</strong></p>



<ul>
<li>July 3:&nbsp;Steve and Jesse Friedman from Automattic meet via Zoom with Victor Yuan from WP Engine to review the terms a second time.</li>



<li>July 16: Steve follows up with Victor Yuan, expressing concern about the timeline.</li>
</ul>



<p><strong>September 2024</strong> </p>



<ul>
<li>September 17: Mark Davies (Automattic CFO) and Lee Wittlinger (Silver Lake managing director and WP Engine board member) meet via Zoom.</li>



<li>September 19: Mark and Lee Wittlinger meet via Zoom.</li>



<li>September 20: Mark and Lee Wittlinger meet by Zoom.</li>
</ul>



<p>On September 19 and 20, Lee Wittlinger and Heather Brunner did not answer any of Matt’s calls.</p>
					</main><!-- #content  -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD Inference (138 pts)]]></title>
            <link>https://github.com/slashml/amd_inference</link>
            <guid>41718030</guid>
            <pubDate>Wed, 02 Oct 2024 07:16:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/slashml/amd_inference">https://github.com/slashml/amd_inference</a>, See on <a href="https://news.ycombinator.com/item?id=41718030">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">⚙️ AMD GPU Inference</h2><a id="user-content-️-amd-gpu-inference" aria-label="Permalink: ⚙️ AMD GPU Inference" href="#️-amd-gpu-inference"></a></p>
<p dir="auto"><a href="https://github.com/bentoml/OpenLLM/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/57be334b2d2a3757c225808c0d6c45a33eae0710590ef1b662efcde77a1da63e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322d677265656e2e737667" alt="License: Apache-2.0" data-canonical-src="https://img.shields.io/badge/License-Apache%202-green.svg"></a>
<a href="https://twitter.com/slash_ml" rel="nofollow"><img src="https://camo.githubusercontent.com/2c432bee872891c8591000ef640a44e5f231d233d56cfbbe8a226a5c4c228f2c/68747470733a2f2f62616467656e2e6e65742f62616467652f69636f6e2f40736c6173685f6d6c2f3030303030303f69636f6e3d74776974746572266c6162656c3d466f6c6c6f77" alt="X" data-canonical-src="https://badgen.net/badge/icon/@slash_ml/000000?icon=twitter&amp;label=Follow"></a>
<a href="https://discord.gg/KyUYq8uX" rel="nofollow"><img src="https://camo.githubusercontent.com/664c15d4fa2ab074a49f42c215d53b98ac607c63fd9b69c1f3d78f9fb2fc8c3a/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3132333435363738393031323334353637383f6c6f676f3d646973636f7264266c6162656c3d4a6f696e253230446973636f7264" alt="Community" data-canonical-src="https://img.shields.io/discord/123456789012345678?logo=discord&amp;label=Join%20Discord"></a></p>
<p dir="auto">This project provides a Docker-based inference engine for running Large Language Models (LLMs) on AMD GPUs. It's designed to work with models from Hugging Face, with a focus on the LLaMA model family.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prerequisites</h2><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li>AMD GPU with ROCm support</li>
<li>Docker installed on your system</li>
<li>ROCm drivers installed on your host system (version 5.4.2 or compatible)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Project Structure</h2><a id="user-content-project-structure" aria-label="Permalink: Project Structure" href="#project-structure"></a></p>
<div data-snippet-clipboard-copy-content="amd-gpu-inference/
├── src/
│   ├── __init__.py
│   ├── engine.py
│   ├── model.py
│   ├── utils.py
│   └── amd_setup.py
├── Dockerfile
├── requirements.txt
├── run_inference.py
├── run-docker-amd.sh
└── README.md"><pre><code>amd-gpu-inference/
├── src/
│   ├── __init__.py
│   ├── engine.py
│   ├── model.py
│   ├── utils.py
│   └── amd_setup.py
├── Dockerfile
├── requirements.txt
├── run_inference.py
├── run-docker-amd.sh
└── README.md
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Clone this repository:</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/yourusername/amd-gpu-inference.git
cd amd-gpu-inference"><pre><code>git clone https://github.com/yourusername/amd-gpu-inference.git
cd amd-gpu-inference
</code></pre></div>
</li>
<li>
<p dir="auto">Make the run script executable:</p>
<div data-snippet-clipboard-copy-content="chmod +x run-docker-amd.sh"><pre><code>chmod +x run-docker-amd.sh
</code></pre></div>
</li>
<li>
<p dir="auto">Run the inference engine with a specified model and prompt:</p>
<div data-snippet-clipboard-copy-content="./run-docker-amd.sh &quot;meta-llama/Llama-2-7b-chat-hf&quot; &quot;Translate the following English text to French: 'Hello, how are you?'&quot;"><pre><code>./run-docker-amd.sh "meta-llama/Llama-2-7b-chat-hf" "Translate the following English text to French: 'Hello, how are you?'"
</code></pre></div>
<p dir="auto">Replace <code>"meta-llama/Llama-2-7b-chat-hf"</code> with the Hugging Face model you want to use, and provide your own prompt.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Detailed Usage</h2><a id="user-content-detailed-usage" aria-label="Permalink: Detailed Usage" href="#detailed-usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Aptfile</h3><a id="user-content-aptfile" aria-label="Permalink: Aptfile" href="#aptfile"></a></p>
<p dir="auto">The project includes an <code>Aptfile</code> that lists the necessary ROCm packages to be installed in the Docker container. This ensures that all required ROCm drivers and libraries are available for the inference engine to utilize the AMD GPU effectively.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building the Docker Image</h3><a id="user-content-building-the-docker-image" aria-label="Permalink: Building the Docker Image" href="#building-the-docker-image"></a></p>
<p dir="auto">The <code>run-docker-amd.sh</code> script builds the Docker image automatically. If you want to build it manually, use:</p>
<div data-snippet-clipboard-copy-content="docker build -t amd-gpu-inference ."><pre><code>docker build -t amd-gpu-inference .
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running the Container</h3><a id="user-content-running-the-container" aria-label="Permalink: Running the Container" href="#running-the-container"></a></p>
<p dir="auto">The <code>run-docker-amd.sh</code> script handles running the container with the necessary AMD GPU flags. If you want to run it manually:</p>
<div data-snippet-clipboard-copy-content="docker run --rm -it \
    --device=/dev/kfd \
    --device=/dev/dri \
    --group-add=video \
    --cap-add=SYS_PTRACE \
    --security-opt seccomp=unconfined \
    amd-gpu-inference &quot;model_name&quot; &quot;your prompt here&quot;"><pre><code>docker run --rm -it \
    --device=/dev/kfd \
    --device=/dev/dri \
    --group-add=video \
    --cap-add=SYS_PTRACE \
    --security-opt seccomp=unconfined \
    amd-gpu-inference "model_name" "your prompt here"
</code></pre></div>
<p dir="auto">Replace <code>"model_name"</code> with the Hugging Face model you want to use, and <code>"your prompt here"</code> with your input text.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Customization</h2><a id="user-content-customization" aria-label="Permalink: Customization" href="#customization"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Changing the Model</h3><a id="user-content-changing-the-model" aria-label="Permalink: Changing the Model" href="#changing-the-model"></a></p>
<p dir="auto">You can use any model available on Hugging Face by specifying its repository name when running the container. For example:</p>
<div data-snippet-clipboard-copy-content="./run-docker-amd.sh &quot;facebook/opt-1.3b&quot; &quot;Your prompt here&quot;"><pre><code>./run-docker-amd.sh "facebook/opt-1.3b" "Your prompt here"
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Modifying the Inference Logic</h3><a id="user-content-modifying-the-inference-logic" aria-label="Permalink: Modifying the Inference Logic" href="#modifying-the-inference-logic"></a></p>
<p dir="auto">If you need to change how the inference is performed, modify the <code>run_inference.py</code> file. Remember to rebuild the Docker image after making changes.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<ul dir="auto">
<li>Ensure that your AMD GPU drivers and ROCm are correctly installed and configured on your host system.</li>
<li>If you encounter "out of memory" errors, try using a smaller model or reducing the input/output length.</li>
<li>For model-specific issues, refer to the model's documentation on Hugging Face.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Please feel free to submit a Pull Request.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<ul dir="auto">
<li>This project uses the Hugging Face Transformers library.</li>
<li>ROCm is developed by AMD. Licensed under MIT License
See <a href="https://rocm.docs.amd.com/en/latest/about/license.html" rel="nofollow">https://rocm.docs.amd.com/en/latest/about/license.html</a> for details.</li>
</ul>
<p dir="auto">For any questions or issues, please open an issue in the GitHub repository.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NixOS is a good server OS, except when it isn't (168 pts)]]></title>
            <link>https://sidhion.com/blog/posts/nixos_server_issues/</link>
            <guid>41717050</guid>
            <pubDate>Wed, 02 Oct 2024 04:06:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sidhion.com/blog/posts/nixos_server_issues/">https://sidhion.com/blog/posts/nixos_server_issues/</a>, See on <a href="https://news.ycombinator.com/item?id=41717050">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Ever since I built my first NixOS system (I started by building a custom image to upload on DigitalOcean), I’ve been bothered by one thing:
the default installation size is large.
To give you an idea, this simple system (using flakes):</p>
<pre tabindex="0"><code>nixpkgs.lib.nixosSystem {
    system = "x86_64-linux";
    modules = [
        (nixpkgs.outPath + "/nixos/modules/profiles/minimal.nix")
        (nixpkgs.outPath + "/nixos/modules/profiles/headless.nix")
        {
            fileSystems."/".device = "/dev/sda1";
            boot.loader.systemd-boot.enable = true;
        }
    ];
}
</code></pre><p>ends up taking ~900MB of disk size on my system<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>!
Minimal and headless!</p>
<p>When I started working on improving this, I expected the eventual blog post to be very different than what it became, but you can’t win everything in life.
There’s a bit of pain ahead.</p>
<h2 id="the-context">The context</h2>
<p>I really like Nix and NixOS (I wouldn’t be spending time <a href="https://sidhion.com/blog/posts/joining-nix-documentation/">helping with their documentation</a> otherwise).
After spending some time managing NixOS servers, I really can’t see myself going back to other systems unless required by some external factor.
I’m also working on a system that has worker machines which will spin up a bunch of microVMs.</p>
<p>Naturally, I wanted to use NixOS both for the worker machines and the microVMs themselves.
Currently, the system on the microVMs is taking ~210MB (including kernel) of disk space, but it’s based on <a href="https://alpinelinux.org/">Alpine</a>.
The worker machines are already using NixOS, but I’d like them to be as lean as possible.</p>
<p>NixOS makes it very simple to manage a server from the outside.
You can push an entirely new system configuration without the server changing its behaviour, and then almost atomically switch the server to the new configuration.
You can easily configure the whole thing deterministically, deploy the same configuration to multiple servers, and even deploy the same configuration under a VM so you can locally test things if you wish to.</p>
<p>I envisioned a world where all my worker machines ran the bare minimum software required for things to work, which would be an amazing help to lock the system down, prevent any escalations in case some piece of software was broken into, and would also make deployments and tests faster.</p>
<p>And if I could achieve something like that on those machines, why not extend this to the OS running on the microVMs and keep things really lean?
This would be super helpful to cut boot times as much as possible, short of using a unikernel.</p>
<p>I knew from my previous experience with NixOS that it didn’t generate lean images by default, so a couple days ago I started looking into this to see if I could fix things, or at least significantly improve them.</p>
<p><strong>Important:</strong> what follows is an analysis of NixOS specifically for the purposes stated here.
I don’t really care about having some runtime-flexible server OS which lets me install packages and configure things ad-hoc, I want a thin, locked-down server with the single purpose of running the software I declare, and not a single extra tool in it.
(yes, I know I essentially want containers without containers.
I comment on this at the end.)</p>
<h2 id="figuring-out-package-dependencies-and-their-sizes">Figuring out package dependencies and their sizes</h2>
<p>A curious thing about the Nix ecosystem is that it has some pretty powerful tools, but they’re severely underdocumented, sometimes functionality is hidden by their naming, and/or some tools have some really specific assumptions, which makes it harder to use them more generally.</p>
<p>One such tool is <a href="https://nixos.org/manual/nix/stable/command-ref/nix-store/query"><code>nix-store --query</code></a> (to be honest, it is way more known than some other more “obscure” tools).
More specifically, <code>nix-store --query --tree</code> will give you a tree of packages<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>, starting from a package you specify, and show you the dependencies of that package, and their dependencies, and so on…
Running it will give you some output like this:</p>
<pre tabindex="0"><code>/nix/store/g4ppw7x76dyykj33x99xzf30zq5ym29z-nixos-system-nixos-24.05.20240323.44d0940
├───/nix/store/09fpwkb108ckhljahy7p84if7m8qh1wh-firmware
├───/nix/store/0v0wrr6ngh9d487lhwicwr5z61kz40zw-kmod-31
│   ├───/nix/store/1rm6sr6ixxzipv5358x0cmaw8rs84g2j-glibc-2.38-44
│   │   ├───/nix/store/3sxwxqzkkrgpgaibkm27ggb9kjbzdy31-xgcc-13.2.0-libgcc
│   │   ├───/nix/store/n9sq1bvghs9z0qg6cmwg27y4jmszwgqi-libidn2-2.3.7
│   │   │   ├───/nix/store/77yhmwrwism02371kzyda4d127kdwdnf-libunistring-1.1
│   │   │   │   └───/nix/store/77yhmwrwism02371kzyda4d127kdwdnf-libunistring-1.1 [...]
│   │   │   └───/nix/store/n9sq1bvghs9z0qg6cmwg27y4jmszwgqi-libidn2-2.3.7 [...]
...
</code></pre><p>To complement that, <code>nix-store --query --size</code> gives you (roughly) the size that a specific package takes on disk.
It’s slightly more complicated than this, but for our purposes it will be enough to understand how much disk is used.</p>
<p>There are some tools which help visualise all this information in cool ways.
Two of my favorites are <a href="https://github.com/utdemir/nix-tree">nix-tree</a> and <a href="https://github.com/craigmbooth/nix-visualize">nix-visualize</a>.
However, ideally I wanted an interactive graph so I could see each node in the graph by their size on disk, and inspect their dependencies, search things, and so on.
nix-visualize was the closest of the tools to give me a graph, but it wasn’t interactive and the node sizes weren’t based on disk usage, so I decided to write my own.</p>
<p>It took me some hours to come up with code that generated a <a href="https://graphviz.org/">graphviz</a> file, with node sizes based on disk usage.
Coupled with <a href="https://github.com/tintinweb/vscode-interactive-graphviz">vscode-interactive-graphviz</a>, I felt like I had a good approach to interactively working with the graph, but the visualisations turned out to be too crowded.
I tried to add some more space into things, but it was kind of a hack because graphviz likes to be the one to position elements.
In the end, I gave up on that idea and decided to just generate a CSV, which worked way better than I expected.
No wonder we still use spreadsheets for a lot of things!</p>
<p>The repository with the code and the final config of the NixOS system from this post is <a href="https://github.com/DanielSidhion/nixos-investigation">here</a>.</p>
<h2 id="an-investigation-of-a-minimal-headless-nixos-system">An investigation of a minimal, headless NixOS system</h2>
<p>With a way to see each package, its disk usage, and all its dependencies, let’s look at the minimal, headless system I mentioned in the beginning of the post.
The one that takes ~900MB.</p>
<figure><img src="https://sidhion.com/nixos_bare_starting.png" alt="A list of the heaviest packages by disk usage, viewed with the Edit csv extension for VSCode"><figcaption>
<p>A list of the heaviest packages by disk usage, viewed with the <a href="https://marketplace.visualstudio.com/items?itemName=janisdd.vscode-edit-csv">Edit csv</a> extension for VSCode</p>
</figcaption>
</figure>
<p>Each subsection below will be a small report of me investigating some items in this CSV<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.
It starts “easy” and gets progressively more complicated.
Feel free to skim and skip any part if you don’t feel like it.</p>
<h2 id="getting-rid-of-nix-179mb-reduction">Getting rid of Nix (~179MB reduction)</h2>
<p>The heaviest item in that list is a mysterious “source” package.
A quick look into what the heck could be taking 170MB of disk space shows it’s actually a complete copy of Nixpkgs!</p>
<pre tabindex="0"><code>❯ ls /nix/store/amxd2p02wx78nyaa4bkb0hjvgwhz1dq7-source
CONTRIBUTING.md  README.md    doc        lib          nixos
COPYING          default.nix  flake.nix  maintainers  pkgs
</code></pre><p>Searching for that package’s <code>pos</code> (just an identifier I used in the code that generates the CSV and the graphviz files) shows that it’s only used by this other package:</p>
<p><img src="https://sidhion.com/etc_nix_registry.png" alt="etc-nix-registry.json using that package"></p>
<p>That package is a single file which doesn’t have a lot in it other than a link to the “source” package.
A search through Nixpkgs shows the file coming from <a href="https://github.com/NixOS/nixpkgs/blob/05677341df3102d5387629b7855afeff6069a6f3/nixos/modules/config/nix-flakes.nix#L90">here</a>, the actual content of <code>registry</code> coming from <a href="https://github.com/NixOS/nixpkgs/blob/6f414604e2a627ae3d8bc8f58573d8b50fb16829/nixos/modules/misc/nixpkgs-flake.nix#L91-L94">here</a>, and the <code>source</code> attribute being set <a href="https://github.com/NixOS/nixpkgs/blob/6f414604e2a627ae3d8bc8f58573d8b50fb16829/flake.nix#L39">here</a>.</p>
<p>I’m building this system with flakes and I’m using that <code>nixosSystem</code> function from Nixpkgs’s <code>flake.nix</code>, which means by default I get this extra 170MB in the system.
I <em>think</em> it would’ve been easy to just undo what Nixpkgs’s <code>flake.nix</code> is doing, but if you look at the list of the heaviest packages again, you’ll see that Nix itself is the 10th heaviest package in the system.</p>
<p>Nix also pulls a lot of dependencies, each one taking quite some space as well (for example, <code>aws-sdk-cpp-1.11.207</code> eats another 5.7MB by itself, and is <strong>only</strong> used by Nix).</p>
<p>After some thinking, I realised that I don’t need Nix in any of these systems.
I definitely don’t need it in a microVM, but I also don’t need it in my servers, because I’m building their configurations in an external machine and deploying the built bits directly.
So let’s add this to the system configuration:</p>
<pre tabindex="0"><code>nix.enable = false;
</code></pre><p>After rebuilding the system, we’re at ~733MB.</p>
<h2 id="getting-rid-of-perl-python-242mb-reduction">Getting rid of Perl, Python (~242MB reduction)</h2>
<p>After removing Nix, the 2nd heaviest package is Python3, and 3rd is Perl.</p>
<p>Python only comes in because of <code>install-systemd-boot.sh</code> (truly a shame, why waste so much disk space like this!), and Perl comes in through a bunch of perl-envs (search for <code>perl-5.38.2-env</code> in the CSV and you’ll see them).
Those perl-envs are all used in the top-level package, so let’s figure out where they’re being used there:</p>
<pre tabindex="0"><code>❯ grep -nr 'perl-5.38.2-env' /nix/store/7z0y5sscnpx4hczzkjh3jvjgn2mq3106-nixos-system-nixos-24.05.20240323.44d0940
/nix/store/7z0y5sscnpx4hczzkjh3jvjgn2mq3106-nixos-system-nixos-24.05.20240323.44d0940/dry-activate:23:/nix/store/d3qxgm4ffhi2ixx3n9clwqlr6z21dd8i-perl-5.38.2-env/bin/perl \
/nix/store/7z0y5sscnpx4hczzkjh3jvjgn2mq3106-nixos-system-nixos-24.05.20240323.44d0940/activate:43:/nix/store/d3qxgm4ffhi2ixx3n9clwqlr6z21dd8i-perl-5.38.2-env/bin/perl \
/nix/store/7z0y5sscnpx4hczzkjh3jvjgn2mq3106-nixos-system-nixos-24.05.20240323.44d0940/activate:63:/nix/store/zkmm5iha0rsm4ypwfc67byq52gz0jb8b-perl-5.38.2-env/bin/perl /nix/store/rg5rf512szdxmnj9qal3wfdnpfsx38qi-setup-etc.pl /nix/store/jq5a0yw04ichvggf7dx80xc438z2v1gv-etc/etc
/nix/store/7z0y5sscnpx4hczzkjh3jvjgn2mq3106-nixos-system-nixos-24.05.20240323.44d0940/bin/switch-to-configuration:1:#! /nix/store/8mlvyl3sab5hxpxz2naz5g2sfd42a40q-perl-5.38.2-env/bin/perl
</code></pre><p>To make it easier to parse this bunch of text:
Perl is used in the <code>dry-activate</code>, <code>activate</code>, and <code>bin/switch-to-configuration</code> scripts.
<code>dry-activate</code> only needs Perl to run the <code>update-users-groups.pl</code> script, while the <code>activate</code> script runs the same script and also <code>setup-etc.pl</code>, and <code>bin/switch-to-configuration</code> is a Perl script from the beginning.</p>
<p><strong>Fun fact:</strong> the minimal profile disables man pages and most other documentation bits, but the perl man pages are the only thing that still get included in the system because of the perl-envs!</p>
<p>I thought Perl was going to be hard to remove, but I was determined to at least take a look.
After all, judging only by the naming, <code>update-users-groups.pl</code> doesn’t seem like the kind of thing I need - I don’t expect my servers to create any extra users or groups dynamically, so there’s nothing to update.</p>
<p>(note that I have no idea what <code>update-users-groups.pl</code> actually does, this was just my thinking from reading its name)</p>
<p>I decided to search Nixpkgs for that script name to get an idea of how it was being added to the system.
It was through this search that I stumbled upon a Nixpkgs tracking issue called <a href="https://github.com/NixOS/nixpkgs/issues/267982">Perlless Activation - Tracking Issue</a>.</p>
<p>Someone decided it wasn’t a good idea to have Perl in the base NixOS system for slightly different reasons, and they did a lot of work to get rid of it!
Luckily for me, I could piggyback off their work and include the following module in my system configuration:</p>
<pre tabindex="0"><code>modules = [
    ...
    (nixpkgs.outPath + "/nixos/modules/profiles/perlless.nix")
];
</code></pre><p>After rebuilding the system, we’re at ~491MB.
As a bonus, Python is now gone as well!</p>
<h2 id="deduplicating-systemd-14mb-reduction">Deduplicating systemd (~14MB reduction)</h2>
<p>systemd is now the 2nd heaviest package.
It has some stuff inside that I think could be removed, but since it’s an integral part of the system, let’s overlook it for now.
Going through the list of packages, what’s this in 5th place?</p>
<p><img src="https://sidhion.com/systemd_minimal.png" alt="systemd-minimal is also in the list of packages!"></p>
<p>For some reason, our NixOS system has both systemd <strong>and</strong> systemd-minimal!
A look through which packages use systemd-minimal show that only dbus uses it.
It comes from <a href="https://github.com/NixOS/nixpkgs/blob/71c7097220cf92a2b005d82904afc590e05b3a5a/pkgs/development/libraries/dbus/default.nix#L7">here</a>.</p>
<p>Nixpkgs has a lot of packages, and sometimes due to circular dependencies or to keep the size of dependencies smaller, it introduces variants of packages/functions that have reduced functionality.
If you contribute to Nixpkgs, chances are that at some point, someone will give some feedback on ways you can use variants that have a smaller dependency chain, or smaller size (a common example is using <code>stdenvNoCC</code> instead of <code>stdenv</code>).
systemd-minimal probably exists to avoid certain circular dependencies, but I’m not sure.
It’s defined <a href="https://github.com/NixOS/nixpkgs/blob/2726f127c15a4cc9810843b96cad73c7eb39e443/pkgs/top-level/all-packages.nix#L28207-L28251">here</a>.</p>
<p>In any case, I’d like to get rid of systemd-minimal, since we already have the full systemd in our system anyway.
There is no easy way to override the package used by the NixOS module that brings in dbus, so we’ll have to add a <a href="https://nixos.org/manual/nixpkgs/unstable/#chap-overlays">Nixpkgs overlay</a> to change the dbus package directly:</p>
<pre tabindex="0"><code>nixpkgs.overlays = [
(
    self: super:
    {
        dbus = super.dbus.override {
            systemdMinimal = self.systemd;
        };
    }
)
];
</code></pre><p>After rebuilding the system, we’re now at ~477MB.</p>
<h2 id="removing-udev-lvm-sudo-and-security-wrappers-30mb-reduction">Removing udev, lvm, sudo and security wrappers (~30MB reduction)</h2>
<p>This is where things start to get very messy.
While looking through the list of heaviest packages, I saw an “hwdb.bin” package which seems linked to udev.
I don’t know about udev too much, but it feels like it’s only needed for scenarios that won’t happen on the kind of servers I want to manage.</p>
<p>In case it is actually used for something important and this breaks the system, I have a feeling that a workaround could be hard-coded and wouldn’t require udev anyway.
I’d gladly go into that rabbit hole, but (spoiler alert) you’ll see that I gave up well before that.</p>
<p>There’s an option to disable it:</p>
<pre tabindex="0"><code>services.udev.enable = false;
</code></pre><p>While looking through the stuff adjacent to udev, I noticed that lvm is also enabled by default.
Similar reasoning to udev, I don’t think I’d need lvm for these servers, so I disabled it.</p>
<pre tabindex="0"><code>services.lvm.enable = false;
</code></pre><p>Before proceeding with more of this, I took a break and looked at some more packages in the list.
At that point, it became clear that I’d have to butcher a LOT of NixOS config to remove many packages in there.
I made a decision to continue with this exercise, but make it less about keeping a working system throughout and more about understanding the efforts to just get rid of some of these packages.
To get to a barebones system, I’d need to remove a lot of them.</p>
<p>While looking through the lvm stuff, I noticed fuse2 and fuse3 are hard-coded by default (and changing those gets complicated quickly).
I saw they’re used by some security wrappers, which also set other security wrappers for mount, umount, sudo, and a bunch of other binaries.
This is needed because Nix doesn’t support sid/gid binaries by design, so NixOS has a binary that dynamically sets some capabilities and permissions, and then executes any other binary with the elevated bits.</p>
<p>I don’t like having this functionality.
Instead of a single wrapper binary which receives an argument with the binary to execute with elevated permissions, I’d rather have X wrapper binaries with hardcoded paths and no parametrisation of any kind (one for each of the X things I want to execute), and that’s only IF I actually need this functionality.</p>
<p>For anything I want to run in these servers, I think I can configure the proper permissions through systemd unit configs.</p>
<p>The security wrappers module doesn’t have an <code>enable</code> option to toggle it off, so one way to get rid of it completely is to add it to the <a href="https://nixos.org/manual/nixos/unstable/#sec-replace-modules"><code>disabledModules</code> attribute</a>.
This requires me to provide dummy options that were provided by the security wrappers module earlier, because when building a NixOS system, by default <strong>every module</strong> gets evaluated (most of them just won’t do anything because they’re not enabled).
Some of these modules set additional wrappers, so the dummy options are needed to make the module system happy.</p>
<pre tabindex="0"><code>({ lib, pkgs, ... }: {
    disabledModules = [ "security/wrappers/default.nix" ];

    options.security = {
        wrappers = lib.mkOption {
            type = lib.types.attrs;
            default = { };
        };
        wrapperDir = lib.mkOption {
            type = lib.types.path;
            default = "/run/wrappers/bin";
        };
    };

    config = {
        # ...
    };
})
</code></pre><p>I <em>think</em> doing this could break some script that calls mount or umount or fuse (because those are hardcoded in the security wrappers module), but I also think that most scripts that use those are being run directly as root, so I’m not sure.</p>
<p>To finish this section, let’s also disable sudo completely because it’s useless without its security wrapper.</p>
<pre tabindex="0"><code>security.sudo.enable = false;
</code></pre><p>We’re at ~447MB now.</p>
<h2 id="some-other-minimal-shenanigans">Some other minimal shenanigans</h2>
<p>At this point, the 10th and 11st heaviest packages are util-linux and util-linux-minimal, respectively.
Well, this seems similar to that systemd-minimal thing from a while ago!</p>
<p>Let’s look at where these are being used:</p>
<ul>
<li>util-linux
<ul>
<li>system-path, a bunch of systemd services and a “mount-pstore” shell script.</li>
</ul>
</li>
<li>util-linux-minimal
<ul>
<li>fuse2 and fuse3, and etc-systemd-system.conf</li>
</ul>
</li>
</ul>
<p>Removing fuse is very annoying (although to be honest, with all the mess in the config so far, it wouldn’t even look that bad anymore).
But we can at least try to make them use util-linux instead of util-linux-minimal, right?</p>
<p>To get there, let’s look at how these packages <a href="https://github.com/NixOS/nixpkgs/blob/6504cbb171517dc371c40b502a55fe4a32aad6d1/pkgs/top-level/all-packages.nix#L27345-L27349">are declared in <code>all-packages.nix</code></a>.
We’ll need an overlay, but trying to change <code>fusePackages</code> to use the normal util-linux will hit an infinite recursion error, so I’ll start by overlaying <code>fuse3</code>:</p>
<pre tabindex="0"><code>nixpkgs.overlays = [(
    self: super: {
        fuse3 = (self.lib.dontRecurseIntoAttrs (self.callPackage (nixpkgs.outPath + "/pkgs/os-specific/linux/fuse") { })).fuse_3;
    }
)];
</code></pre><p>This builds nicely, but the moment I try to do this with fuse2, the infinite recursion error is back.
<em>Sigh</em>.
Whatever.</p>
<p>Browsing through the list of packages, I also see systemd-minimal-libs sneaking in there.
It’s being used by a bunch of other packages, and it’s equally difficult to add more overlays to get rid of it.
Infinite recursions all around.</p>
<p>This is where I look at the current system config, look at all the notes I made of things to look into that I haven’t yet (the list is right there in the next section), think about how much worse it’ll get by trying to fix all of this, and give up.</p>
<h2 id="things-i-noted-but-didnt-look-at">Things I noted, but didn’t look at</h2>
<ul>
<li>
<p>With Nix gone, the heaviest package was the linux kernel at ~136MB.
I know I can get it down to ~50MB easily (the kernel used by default on NixOS has a lot of modules and extra things that a server doesn’t need), so I left that for later because it was easy.</p>
</li>
<li>
<p>One disadvantage of a perlless system is that it can’t switch to a different configuration at runtime, because the script that does this is written in Perl.
This isn’t an issue for most of the servers in my scenario.
MicroVMs don’t need that, and I’d be perfectly ok just killing a bunch of other servers and starting new ones with the updated configuration.</p>
<p>However, I made a note to look into that perl script and figure out how much work it would be to build a replacement.
This is something that needs to happen anyway at some point, and would benefit the NixOS community at large.</p>
</li>
<li>
<p>A bunch of packages build with all locales and some internationalisation content.
Those end up taking some good space, so I made a note to look at how to simplify this and get rid of most of the locales and files I wouldn’t need.</p>
</li>
<li>
<p>The system has both libressl and openssl packages.
libressl is only used for netcat, but I don’t really need it in the servers.
In fact, NixOS includes a lot of utilities by default (and marks them as required, making it super annoying to remove them) which aren’t really needed on the servers.</p>
</li>
<li>
<p>A bunch of extra default config files that aren’t needed (such as bashrc) could be removed.
This would also remove some packages that they use, such as bash-completion, which won’t ever be needed in the servers.</p>
</li>
<li>
<p>coreutils and util-linux are both kind of heavy, but it’s very likely that the scripts and things that use them only really need a few binaries from each one.
Perhaps an overlay that filters the binaries only to the list that are used would help free up quite a bunch of disk space.</p>
</li>
<li>
<p><code>nix-store</code> has a command to <a href="https://nixos.org/manual/nix/stable/command-ref/nix-store/optimise">optimise</a> disk space by finding identical files and hard-linking them.
This could be helpful in some cases, but might not be possible in others, depending on how a server is imaged, or how new configuration gets pushed to it.
It could be useful to decrease the disk space used by some of those “-minimal” packages (as long as they share exactly the same file).</p>
</li>
</ul>
<h2 id="leaving-this-to-the-future">Leaving this to the future</h2>
<p>There is a huge audience that uses NixOS as a personal OS, and a lot of the defaults and modules present in NixOS reflect that.
NixOS can still be used as a server OS, but it requires a very different set of configurations, and it still ends up not being adequate in every situation.</p>
<p>I can (and will) still apply many of the configs I used in this post to my existing servers and make them leaner, which will already be useful, because it cuts ~300MB of stuff I don’t need.
I got some experience and figured out some tools to help me investigate these issues more whenever I feel the need to.</p>
<p>But over the 2 days I spent looking at this, I concluded that trying to mold NixOS into the shape I envisioned just isn’t the way to go, but I also don’t like the other option if I want to stick with it, which is creating a “fork” of NixOS that is <strong>very</strong> opinionated and completely focused on server scenarios.</p>
<p>I was trying to bring NixOS to a bare minimum, which is an exercise similar to building containers with the bare minimum required for the software in the container to run.
I think this is a worthy endeavour.
I think we have all the tools in regular non-docker, non-kubernetes linux to get to a similar outcome, except we won’t need docker or kubernetes or whatever in this new land, thus removing quite a bunch of complexity from the systems we build.</p>
<p>But doing it on top of NixOS currently feels like a bad path to take.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia releases NVLM 1.0 72B open weight model (147 pts)]]></title>
            <link>https://huggingface.co/nvidia/NVLM-D-72B</link>
            <guid>41716975</guid>
            <pubDate>Wed, 02 Oct 2024 03:48:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://huggingface.co/nvidia/NVLM-D-72B">https://huggingface.co/nvidia/NVLM-D-72B</a>, See on <a href="https://news.ycombinator.com/item?id=41716975">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<!-- HTML_TAG_START --><p>
  <img width="300" alt="Image Description" src="https://huggingface.co/nvidia/NVLM-D-72B/resolve/main/nvlm-logo-light.png">
</p>



<h2>
	<a rel="nofollow" href="#model-details" id="model-details">
		
	</a>
	<span>
		Model Details
	</span>
</h2>
<p>Today (September 17th, 2024), we introduce <a rel="nofollow" href="https://arxiv.org/abs/2409.11402">NVLM 1.0</a>, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. </p>
<p>In this repo, we are open-sourcing NVLM-1.0-D-72B (decoder-only architecture), the decoder-only model weights and code for the community.</p>
<h2>
	<a rel="nofollow" href="#other-resources" id="other-resources">
		
	</a>
	<span>
		Other Resources
	</span>
</h2>
<p><a rel="nofollow" href="https://huggingface.co/nvidia/NVLM-D-72B/tree/main">Inference Code (HF)</a>   <a rel="nofollow" href="">Training Code (Coming soon)</a>   <a rel="nofollow" href="https://research.nvidia.com/labs/adlr/NVLM-1/">Website</a>   <a rel="nofollow" href="https://arxiv.org/abs/2409.11402">Paper</a></p>
<h2>
	<a rel="nofollow" href="#benchmark-results" id="benchmark-results">
		
	</a>
	<span>
		Benchmark Results
	</span>
</h2>
<p>We train our model with legacy <a rel="nofollow" href="https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/legacy">Megatron-LM</a> and adapt the codebase to Huggingface for model hosting, reproducibility, and inference.
We observe numerical differences between the Megatron and Huggingface codebases, which are within the expected range of variation. 
We provide the results from both the Huggingface codebase and the Megatron codebase for reproducibility and comparison with other models.</p>
<p>Results (as of September 17th, 2024) in the multimodal benchmarks are as follows:</p>
<div>
	<table>
		<thead><tr>
<th>Benchmark</th>
<th>MMMU (val / test)</th>
<th>MathVista</th>
<th>OCRBench</th>
<th>AI2D</th>
<th>ChartQA</th>
<th>DocVQA</th>
<th>TextVQA</th>
<th>RealWorldQA</th>
<th>VQAv2</th>
</tr>

		</thead><tbody><tr>
<td>NVLM-D 1.0 72B (Huggingface)</td>
<td>58.7 / 54.9</td>
<td>65.2</td>
<td>852</td>
<td>94.2</td>
<td>86.0</td>
<td>92.6</td>
<td>82.6</td>
<td>69.5</td>
<td>85.4</td>
</tr>
<tr>
<td>NVLM-D 1.0 72B (Megatron)</td>
<td>59.7 / 54.6</td>
<td>65.2</td>
<td>853</td>
<td>94.2</td>
<td>86.0</td>
<td>92.6</td>
<td>82.1</td>
<td>69.7</td>
<td>85.4</td>
</tr>
<tr>
<td>Llama 3.2 90B</td>
<td>60.3 / -</td>
<td>57.3</td>
<td>-</td>
<td>92.3</td>
<td>85.5</td>
<td>90.1</td>
<td>-</td>
<td>-</td>
<td>78.1</td>
</tr>
<tr>
<td>Llama 3-V 70B</td>
<td>60.6 / -</td>
<td>-</td>
<td>-</td>
<td>93.0</td>
<td>83.2</td>
<td>92.2</td>
<td>83.4</td>
<td>-</td>
<td>79.1</td>
</tr>
<tr>
<td>Llama 3-V 405B</td>
<td>64.5 / -</td>
<td>-</td>
<td>-</td>
<td>94.1</td>
<td>85.8</td>
<td>92.6</td>
<td>84.8</td>
<td>-</td>
<td>80.2</td>
</tr>
<tr>
<td>InternVL2-Llama3-76B</td>
<td>55.2 / -</td>
<td>65.5</td>
<td>839</td>
<td>94.8</td>
<td>88.4</td>
<td>94.1</td>
<td>84.4</td>
<td>72.2</td>
<td>-</td>
</tr>
<tr>
<td>GPT-4V</td>
<td>56.8 / 55.7</td>
<td>49.9</td>
<td>645</td>
<td>78.2</td>
<td>78.5</td>
<td>88.4</td>
<td>78.0</td>
<td>61.4</td>
<td>77.2</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>69.1 / -</td>
<td>63.8</td>
<td>736</td>
<td>94.2</td>
<td>85.7</td>
<td>92.8</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Claude 3.5 Sonnet</td>
<td>68.3 / -</td>
<td>67.7</td>
<td>788</td>
<td>94.7</td>
<td>90.8</td>
<td>95.2</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Gemini 1.5 Pro (Aug 2024)</td>
<td>62.2 / -</td>
<td>63.9</td>
<td>754</td>
<td>94.4</td>
<td>87.2</td>
<td>93.1</td>
<td>78.7</td>
<td>70.4</td>
<td>80.2</td>
</tr>
</tbody>
	</table>
</div>
<h2>
	<a rel="nofollow" href="#how-to-use" id="how-to-use">
		
	</a>
	<span>
		How to use
	</span>
</h2>
<p>When converting Megatron checkpoint to Huggingface, we adapt <a rel="nofollow" href="https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B">InternVL codebase</a> to support model loading and multi-GPU inference in HF. For training, please refer to <a rel="nofollow" href="">Megatron-LM (Coming soon)</a>.</p>
<h3>
	<a rel="nofollow" href="#prepare-the-environment" id="prepare-the-environment">
		
	</a>
	<span>
		Prepare the environment
	</span>
</h3>
<p>We provide a docker build file in the <a rel="nofollow" href="https://huggingface.co/nvidia/NVLM-D-72B/blob/main/Dockerfile">Dockerfile</a> for reproduction. </p>
<p>The docker image is based on <code>nvcr.io/nvidia/pytorch:23.09-py3</code>. </p>
<p><em>Note: We observe that different transformer versions / CUDA versions / docker versions can lead to slight benchmark number differences. We recommend using the Dockerfile above for precise reproduction.</em></p>
<h3>
	<a rel="nofollow" href="#model-loading" id="model-loading">
		
	</a>
	<span>
		Model loading
	</span>
</h3>
<pre><code><span>import</span> torch
<span>from</span> transformers <span>import</span> AutoModel

path = <span>"nvidia/NVLM-D-72B"</span>
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=<span>True</span>,
    use_flash_attn=<span>False</span>,
    trust_remote_code=<span>True</span>).<span>eval</span>()
</code></pre>
<h3>
	<a rel="nofollow" href="#multiple-gpus" id="multiple-gpus">
		
	</a>
	<span>
		Multiple GPUs
	</span>
</h3>
<p>The model can be loaded on multiple GPUs as follows:</p>
<pre><code><span>import</span> torch
<span>import</span> math
<span>from</span> transformers <span>import</span> AutoModel

<span>def</span> <span>split_model</span>():
    device_map = {}
    world_size = torch.cuda.device_count()
    num_layers = <span>80</span>
    <span># Since the first GPU will be used for ViT, treat it as half a GPU.</span>
    num_layers_per_gpu = math.ceil(num_layers / (world_size - <span>0.5</span>))
    num_layers_per_gpu = [num_layers_per_gpu] * world_size
    num_layers_per_gpu[<span>0</span>] = math.ceil(num_layers_per_gpu[<span>0</span>] * <span>0.5</span>)
    layer_cnt = <span>0</span>
    <span>for</span> i, num_layer <span>in</span> <span>enumerate</span>(num_layers_per_gpu):
        <span>for</span> j <span>in</span> <span>range</span>(num_layer):
            device_map[<span>f'language_model.model.layers.<span>{layer_cnt}</span>'</span>] = i
            layer_cnt += <span>1</span>
    device_map[<span>'vision_model'</span>] = <span>0</span>
    device_map[<span>'mlp1'</span>] = <span>0</span>
    device_map[<span>'language_model.model.tok_embeddings'</span>] = <span>0</span>
    device_map[<span>'language_model.model.embed_tokens'</span>] = <span>0</span>
    device_map[<span>'language_model.output'</span>] = <span>0</span>
    device_map[<span>'language_model.model.norm'</span>] = <span>0</span>
    device_map[<span>'language_model.lm_head'</span>] = <span>0</span>
    device_map[<span>f'language_model.model.layers.<span>{num_layers - <span>1</span>}</span>'</span>] = <span>0</span>

    <span>return</span> device_map

path = <span>"nvidia/NVLM-D-72B"</span>
device_map = split_model()
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=<span>True</span>,
    use_flash_attn=<span>False</span>,
    trust_remote_code=<span>True</span>,
    device_map=device_map).<span>eval</span>()
</code></pre>
<h3>
	<a rel="nofollow" href="#inference" id="inference">
		
	</a>
	<span>
		Inference
	</span>
</h3>
<pre><code><span>import</span> torch
<span>from</span> transformers <span>import</span> AutoTokenizer, AutoModel
<span>import</span> math
<span>from</span> PIL <span>import</span> Image
<span>import</span> torchvision.transforms <span>as</span> T
<span>from</span> torchvision.transforms.functional <span>import</span> InterpolationMode


<span>def</span> <span>split_model</span>():
    device_map = {}
    world_size = torch.cuda.device_count()
    num_layers = <span>80</span>
    <span># Since the first GPU will be used for ViT, treat it as half a GPU.</span>
    num_layers_per_gpu = math.ceil(num_layers / (world_size - <span>0.5</span>))
    num_layers_per_gpu = [num_layers_per_gpu] * world_size
    num_layers_per_gpu[<span>0</span>] = math.ceil(num_layers_per_gpu[<span>0</span>] * <span>0.5</span>)
    layer_cnt = <span>0</span>
    <span>for</span> i, num_layer <span>in</span> <span>enumerate</span>(num_layers_per_gpu):
        <span>for</span> j <span>in</span> <span>range</span>(num_layer):
            device_map[<span>f'language_model.model.layers.<span>{layer_cnt}</span>'</span>] = i
            layer_cnt += <span>1</span>
    device_map[<span>'vision_model'</span>] = <span>0</span>
    device_map[<span>'mlp1'</span>] = <span>0</span>
    device_map[<span>'language_model.model.tok_embeddings'</span>] = <span>0</span>
    device_map[<span>'language_model.model.embed_tokens'</span>] = <span>0</span>
    device_map[<span>'language_model.output'</span>] = <span>0</span>
    device_map[<span>'language_model.model.norm'</span>] = <span>0</span>
    device_map[<span>'language_model.lm_head'</span>] = <span>0</span>
    device_map[<span>f'language_model.model.layers.<span>{num_layers - <span>1</span>}</span>'</span>] = <span>0</span>

    <span>return</span> device_map


IMAGENET_MEAN = (<span>0.485</span>, <span>0.456</span>, <span>0.406</span>)
IMAGENET_STD = (<span>0.229</span>, <span>0.224</span>, <span>0.225</span>)


<span>def</span> <span>build_transform</span>(<span>input_size</span>):
    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
    transform = T.Compose([
        T.Lambda(<span>lambda</span> img: img.convert(<span>'RGB'</span>) <span>if</span> img.mode != <span>'RGB'</span> <span>else</span> img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=MEAN, std=STD)
    ])
    <span>return</span> transform


<span>def</span> <span>find_closest_aspect_ratio</span>(<span>aspect_ratio, target_ratios, width, height, image_size</span>):
    best_ratio_diff = <span>float</span>(<span>'inf'</span>)
    best_ratio = (<span>1</span>, <span>1</span>)
    area = width * height
    <span>for</span> ratio <span>in</span> target_ratios:
        target_aspect_ratio = ratio[<span>0</span>] / ratio[<span>1</span>]
        ratio_diff = <span>abs</span>(aspect_ratio - target_aspect_ratio)
        <span>if</span> ratio_diff &lt; best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        <span>elif</span> ratio_diff == best_ratio_diff:
            <span>if</span> area &gt; <span>0.5</span> * image_size * image_size * ratio[<span>0</span>] * ratio[<span>1</span>]:
                best_ratio = ratio
    <span>return</span> best_ratio


<span>def</span> <span>dynamic_preprocess</span>(<span>image, min_num=<span>1</span>, max_num=<span>12</span>, image_size=<span>448</span>, use_thumbnail=<span>False</span></span>):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height

    <span># calculate the existing image aspect ratio</span>
    target_ratios = <span>set</span>(
        (i, j) <span>for</span> n <span>in</span> <span>range</span>(min_num, max_num + <span>1</span>) <span>for</span> i <span>in</span> <span>range</span>(<span>1</span>, n + <span>1</span>) <span>for</span> j <span>in</span> <span>range</span>(<span>1</span>, n + <span>1</span>) <span>if</span>
        i * j &lt;= max_num <span>and</span> i * j &gt;= min_num)
    target_ratios = <span>sorted</span>(target_ratios, key=<span>lambda</span> x: x[<span>0</span>] * x[<span>1</span>])

    <span># find the closest aspect ratio to the target</span>
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)

    <span># calculate the target width and height</span>
    target_width = image_size * target_aspect_ratio[<span>0</span>]
    target_height = image_size * target_aspect_ratio[<span>1</span>]
    blocks = target_aspect_ratio[<span>0</span>] * target_aspect_ratio[<span>1</span>]

    <span># resize the image</span>
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    <span>for</span> i <span>in</span> <span>range</span>(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + <span>1</span>) * image_size,
            ((i // (target_width // image_size)) + <span>1</span>) * image_size
        )
        <span># split the image</span>
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    <span>assert</span> <span>len</span>(processed_images) == blocks
    <span>if</span> use_thumbnail <span>and</span> <span>len</span>(processed_images) != <span>1</span>:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    <span>return</span> processed_images


<span>def</span> <span>load_image</span>(<span>image_file, input_size=<span>448</span>, max_num=<span>12</span></span>):
    image = Image.<span>open</span>(image_file).convert(<span>'RGB'</span>)
    transform = build_transform(input_size=input_size)
    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=<span>True</span>, max_num=max_num)
    pixel_values = [transform(image) <span>for</span> image <span>in</span> images]
    pixel_values = torch.stack(pixel_values)
    <span>return</span> pixel_values

path = <span>"nvidia/NVLM-D-72B"</span>
device_map = split_model()
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=<span>True</span>,
    use_flash_attn=<span>False</span>,
    trust_remote_code=<span>True</span>,
    device_map=device_map).<span>eval</span>()

<span>print</span>(model)

tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=<span>True</span>, use_fast=<span>False</span>)
generation_config = <span>dict</span>(max_new_tokens=<span>1024</span>, do_sample=<span>False</span>)

<span># pure-text conversation</span>
question = <span>'Hello, who are you?'</span>
response, history = model.chat(tokenizer, <span>None</span>, question, generation_config, history=<span>None</span>, return_history=<span>True</span>)
<span>print</span>(<span>f'User: <span>{question}</span>\nAssistant: <span>{response}</span>'</span>)

<span># single-image single-round conversation</span>
pixel_values = load_image(<span>'path/to/your/example/image.jpg'</span>, max_num=<span>6</span>).to(
    torch.bfloat16)
question = <span>'&lt;image&gt;\nPlease describe the image shortly.'</span>
response = model.chat(tokenizer, pixel_values, question, generation_config)
<span>print</span>(<span>f'User: <span>{question}</span>\nAssistant: <span>{response}</span>'</span>)
</code></pre>
<h2>
	<a rel="nofollow" href="#correspondence-to" id="correspondence-to">
		
	</a>
	<span>
		Correspondence to
	</span>
</h2>
<p>Wenliang Dai* (<a rel="nofollow" href="mailto:wdai@nvidia.com">wdai@nvidia.com</a>), Nayeon Lee* (<a rel="nofollow" href="mailto:nayeonl@nvidia.com">nayeonl@nvidia.com</a>), Boxin Wang* (<a rel="nofollow" href="mailto:boxinw@nvidia.com">boxinw@nvidia.com</a>), Zhuolin Yang* (<a rel="nofollow" href="mailto:zhuoliny@nvidia.com">zhuoliny@nvidia.com</a>), Wei Ping* (<a rel="nofollow" href="mailto:wping@nvidia.com">wping@nvidia.com</a>)</p>
<p>*Equal contribution</p>
<h2>
	<a rel="nofollow" href="#citation" id="citation">
		
	</a>
	<span>
		Citation
	</span>
</h2>
<pre>@article{nvlm2024,
  title={NVLM: Open Frontier-Class Multimodal LLMs},
  author={Dai, Wenliang and Lee, Nayeon and Wang, Boxin and Yang, Zhuolin and Liu, Zihan and Barker, Jon and Rintamaki, Tuomas and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint},
  year={2024}}
</pre>


<h2>
	<a rel="nofollow" href="#license" id="license">
		
	</a>
	<span>
		License
	</span>
</h2>
<p>The use of this model is governed by the <a rel="nofollow" href="https://spdx.org/licenses/CC-BY-NC-4.0">cc-by-nc-4.0</a></p>
<!-- HTML_TAG_END --></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Make Pottery at Home Without a Kiln (Or Anything Else) [video] (109 pts)]]></title>
            <link>https://www.youtube.com/watch?v=VaX1iOyKsB0</link>
            <guid>41716075</guid>
            <pubDate>Wed, 02 Oct 2024 00:45:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=VaX1iOyKsB0">https://www.youtube.com/watch?v=VaX1iOyKsB0</a>, See on <a href="https://news.ycombinator.com/item?id=41716075">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Math from Three to Seven (315 pts)]]></title>
            <link>https://www.thepsmiths.com/p/review-math-from-three-to-seven-by</link>
            <guid>41715762</guid>
            <pubDate>Tue, 01 Oct 2024 23:58:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thepsmiths.com/p/review-math-from-three-to-seven-by">https://www.thepsmiths.com/p/review-math-from-three-to-seven-by</a>, See on <a href="https://news.ycombinator.com/item?id=41715762">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><em><a href="https://www.amazon.com/Math-Three-Seven-Mathematical-Preschoolers/dp/082186873X" rel="">Math from Three to Seven: The Story of a Mathematical Circle for Preschoolers</a></em><span>, Alexander Zvonkin (Moscow Center for Continuing Mathematical Education, 2007).</span></p><p><span>To me, one of the greatest historical puzzles is why the Cold War was even a contest. Consider it a mirror image of the </span><a href="https://en.wikipedia.org/wiki/Joseph_Needham#The_Needham_Question" rel="">Needham Question</a><span>: Joseph Needham famously wondered why it was that, despite having a vastly larger population and GDP, Imperial China nevertheless lost out scientifically to the West. (I examined this question at some length in </span><a href="https://www.thepsmiths.com/p/review-science-in-traditional-china" rel="">this review</a><span>.) Well, with the Soviets it all went in the opposite direction: they had a smaller population, a worse starting industrial base, a lower GDP, and a vastly less efficient economic system. How, then, did they maintain military and technological parity</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-149130375" href="https://www.thepsmiths.com/p/review-math-from-three-to-seven-by#footnote-1-149130375" target="_self" rel="">1</a></span><span> with the United States for so long?</span></p><p>The puzzle was partly solved for me, but partly deepened, when those of us who grew up in the ‘90s and ‘00s encountered the vast wave of former Soviet émigrés that washed up in the United States after the fall of communism. Anybody who played competitive chess back then, or who participated in math competitions, knows what I’m talking about: the sinking feeling you got upon seeing that your opponent had a Russian name. These days, the same scenes are dominated by Chinese and Indian kids. But China and India have large populations — the Russians were punching way about their weight, demographically speaking. Today, those same Russians are all over Wall Street and Silicon Valley and Ivy League math departments, still overrepresented in technical fields. What explains it? Are Russians just naturally better at math and physics?</p><p><span>When I related these questions to an Ashkenazi-supremacist friend of mine, he immediately suggested that “maybe it’s because they’re all Jewish.” (I’ve noticed that the most philosemitic people and the most antisemitic people sometimes have curiously similar models of the world, they just disagree on whether it’s a good thing.) My friend’s question wasn’t crazy, since there are definitely times when asking “were they all Jewish?” </span><a href="https://slatestarcodex.com/2017/05/26/the-atomic-bomb-considered-as-hungarian-high-school-science-fair-project/" rel="">yields an affirmative answer</a><span>. But in this case I had to disappoint him with the knowledge that many of these Russian math and chess superstars were gentiles.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-149130375" href="https://www.thepsmiths.com/p/review-math-from-three-to-seven-by#footnote-2-149130375" target="_self" rel="">2</a></span><span> What’s more, by the ‘60s and ‘70s the Soviets had </span><a href="https://arxiv.org/pdf/1110.1556" rel="">an entire discriminatory apparatus</a><span> dedicated to keeping Jews out of the scientific establishment, so it would be impressive indeed if they were the foundation of its success.</span></p><p><span>Another possible explanation actually hinges on the relative poverty of the Soviet Union. Assume there are a lot of people out there with natural mathematical talent, but who given their druthers would major in underwater basket-weaving instead. The United States, because it’s so wealthy, can afford to “waste” a huge proportion of our talented population on humanities, arts, and other stuff that doesn’t involve you sitting in the school library until 3am. In other words, </span><em>not</em><span> going into a technical field is a form of luxury, which America can afford to consume. The Soviets, rather like the Chinese today, were forced by their underdog status to allocate human capital more efficiently (and had the authoritarian means to do so by force if necessary). This theory is related to the curious fact that, on average, </span><a href="https://www.theatlantic.com/science/archive/2018/02/the-more-gender-equality-the-fewer-women-in-stem/553592/" rel="">the more feminist your society, the fewer women there are in math and science</a><span> — which makes total sense if you assume that on average women are good at math but uninterested in it.</span></p><p><span>The thing is, the émigré superstars I encountered didn’t seem at all grudging or resentful about their studies. If anything it was the opposite. I’ve </span><a href="https://www.thepsmiths.com/p/joint-review-who-we-are-and-how-we" rel="">previously complained</a><span> about how much I hate Russian mathematician Edward Frenkel’s</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-149130375" href="https://www.thepsmiths.com/p/review-math-from-three-to-seven-by#footnote-3-149130375" target="_self" rel="">3</a></span><span> book, but one thing it gets across well is just how important </span><em>passion</em><span> is to being a great mathematician, and passion was the thing the émigrés seemed to have a surfeit of. In college, the joke was that seminars by American professors would last an hour, whereas seminars by Russian professors would turn into boisterous debates lasting all night. People have been writing for centuries about Russians having a tendency towards “maximalism” — whether aesthetic or ideological or anything else. Maybe a culture-wide commitment to not doing anything by half-measures explains it?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca929aa2-a7d2-4837-8156-cd02da28136f_1380x776.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca929aa2-a7d2-4837-8156-cd02da28136f_1380x776.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca929aa2-a7d2-4837-8156-cd02da28136f_1380x776.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca929aa2-a7d2-4837-8156-cd02da28136f_1380x776.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca929aa2-a7d2-4837-8156-cd02da28136f_1380x776.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca929aa2-a7d2-4837-8156-cd02da28136f_1380x776.jpeg" width="1380" height="776" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ca929aa2-a7d2-4837-8156-cd02da28136f_1380x776.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:776,&quot;width&quot;:1380,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Math chalkboard - Clark Butler&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Math chalkboard - Clark Butler" title="Math chalkboard - Clark Butler" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca929aa2-a7d2-4837-8156-cd02da28136f_1380x776.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca929aa2-a7d2-4837-8156-cd02da28136f_1380x776.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca929aa2-a7d2-4837-8156-cd02da28136f_1380x776.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca929aa2-a7d2-4837-8156-cd02da28136f_1380x776.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>These are all fun theories, but in the interviews I’ve read with Soviet mathematicians and scientists, the things that comes up over and over again are “</span><a href="https://www.amazon.com/Mathematical-Circles-Russian-Experience-World/dp/0821804308" rel="">mathematical circles</a><span>,” a practice that originated in the pre-revolutionary Russian Empire and then spread far and wide through the Soviet Union. A mathematical circle is an informal group of teenagers and adults who really enjoy math and want to spend a lot of time thinking and talking about it. They’re a little bit like sports teams, in that they develop their own high-intensity internal culture and camaraderie, and often have a “coach” who is especially talented or famous. But they’re also very unlike sports teams, because they don’t compete with each other or play in leagues or anything like that, and usually any given circle will contain members of widely varying skill levels. Maybe a better analogy is a neighborhood musical ensemble that gets together and jams on a regular basis, but for math.</span></p><p><span>The most important thing to understand about mathematical circles is that the math they jam on is completely unlike the math you study in school, and also completely unlike the “competition” math that bright kids in the United States sometimes do. Both school math and competition math are primarily comprised of </span><em>exercises</em><span>. An </span><em>exercise</em><span> is a question concocted by a human being for a didactic purpose. Any bright kid with any amount of genre-savviness can immediately make a few assumptions upon being assigned an </span><em>exercise</em><span>. He or she can guess that the </span><em>exercise</em><span> is solvable in fewer than five minutes with the appropriate techniques, and that it is related to the material in the current chapter of the book. A clever student can often use psychological techniques to reverse-engineer what the teacher or the designer of the standardized test was trying to get at with the </span><em>exercise</em><span>, and answer it through a process of elimination or savvy guessing or pattern matching.</span></p><p><span>Solving an </span><em>exercise</em><span> is like hunting a neutered zoo animal. It may be a low-stress environment for polishing particular aspects of your technique, but it will not help you to survive in the wilderness. For that, you need to see people solving </span><em>problems</em><span>. A </span><em>problem</em><span> is a question of interest that comes up when somebody is trying to do something real. A </span><em>problem</em><span> may not be solvable by you, or by your coach, or by any human being. Even if the </span><em>problem</em><span> is solvable, it may require weeks or months of dedicated, painful pursuit. It may not be obvious what techniques are required to solve a </span><em>problem</em><span>, they may not be techniques that you know, or it may require a surprising combination of techniques. The </span><em>problem</em><span> is mathematical nature red in tooth and claw. There are no guardrails. There are no hints or answers at the back of the book. There is no book. It may eat you.</span></p><p><span>(For more on exercises vs. problems, read my review of </span><em>The Education of Cyrus</em><span>.)</span></p><p><span>The bread and butter of the mathematical circle is solving </span><em>problems</em><span> together, as a team. There is no time here for </span><em>exercises</em><span>; you can do that lame stuff at school. Sometimes the coach picks a </span><em>problem</em><span> for you, something just beyond your ability, just the thing you need to hone your edge. But sometimes the whole circle works together on a </span><em>problem</em><span> that nobody has the answer to and that challenges the very best members. These </span><em>problems</em><span> are the most important, because with them you see great minds, men older and more talented than you, stretched to the breaking point and occasionally beaten. You see them grind and grind and try every possible attack on a </span><em>problem </em><span>and sometimes lose anyway. And you see them not run from being defeated, but cheerfully charge in again, because losing is </span><em>good for you</em><span>, losing is how you know you’ve picked an opponent worthy of a man. You learn to love things that are hard. And occasionally you win, and when you win it feels like you all win, like humanity wins, because you’re all in it together, all doing something beautiful and </span><a href="https://www.thepsmiths.com/p/review-when-we-cease-to-understand" rel="">dangerous</a><span> and exemplary of the best qualities that human beings have.</span></p><p><span>There are also times when everybody is too tired to work on a </span><em>problem</em><span>, and in those moments of recuperation, it’s the coach’s job to tell stories of legendary </span><em>problems</em><span> of the past and of the mathematicians who slew them. These stories often contain lessons, inspiration, or perspective on how mathematics evolved and got to be the way it is. Human history would look very different, after all, without the </span><em><a href="https://www.thepsmiths.com/p/review-the-variational-principles" rel="">brachistochrone problem</a><span> </span></em><span>or the </span><em><a href="https://www.thepsmiths.com/p/review-galois-theory-by-david-cox" rel="">roots of a quintic polynomial problem</a><span> </span></em><span>or the </span><em><a href="https://www.thepsmiths.com/p/review-lectures-on-the-icosahedron" rel="">icosahedron problem</a><span> </span></em><span>or the </span><em><a href="https://www.thepsmiths.com/p/review-einsteins-unification-by-jeroen" rel="">precession of Mercury’s perihelion problem</a></em><span>. But other times there’s no hidden lesson, no grand perspective on the human story. They’re just ripping good yarns, and hearing them is a process of initiation into mathematical </span><em>folklore</em><span>, because every culture (and mathematics is surely a culture) has shared stories and references and inside jokes, even when they’re purely for fun.</span></p><p>This book is the story of one such mathematical circle. But it’s an unusual one because…it’s for preschoolers.</p><p><span>The “coach” of this circle is Alexander Zvonkin, a professional mathematician frustrated that his kids are having all the wonder and life and joy crushed out of them by the grey functionaries at their school. So he starts a circle for his son Dmitry and a few of the neighbors’ kids, most of whom are around three or four years old. That’s young enough that </span><a href="https://www.simplypsychology.org/piaget.html" rel="">according to Piaget’s experiments</a><span> there are cognitive modules related to number and volume that simply haven’t come online yet. Fortunately, Zvonkin is familiar with the latest research on developmental psychology, and turns lemons into lemonade by using the kids’ lack of numerical intuition to introduce them to some pretty deep ideas about when two sets have equal cardinality. (If you’re curious, he talks more about these experiments in </span><a href="https://eric.ed.gov/?id=EJ453566" rel="">this journal article</a><span>.)</span></p><p><span>At this point I expect you are rolling your eyes, especially if you have experience with three-year-olds. It can be difficult enough to get them to sit still, never mind ponder deep questions about the cardinalities of sets. And what exactly does it look like to pit somebody against a </span><em>problem</em><span> who is barely potty-trained? This is where the genius of Zvonkin’s format kicks in — it’s not really a book, it’s a journal, and one that is barely edited. So it’s full of failure after failure, entries like, “today I had a cool idea for a puzzle but everybody just screamed instead and then one of the kids vomited.” And yet, slowly, wondrously, over the four years of the circle’s existence, his patience pays off and the kids start doing really incredible things.</span></p><p><span>His initial goal, when the kids are at their youngest and are least prepared to match wits against a </span><em>problem</em><span>, is just to counter-program their schoolwork a little bit. Zvonkin hates the fact that their school presents math as a set of formulas to be obeyed, and DESPISES the fact that the kids’ midwit teacher deducts marks for failing to write answers in a designated format.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-149130375" href="https://www.thepsmiths.com/p/review-math-from-three-to-seven-by#footnote-4-149130375" target="_self" rel="">4</a></span><span> Worst of all, the teacher docks points when the kids use techniques that they “aren’t supposed to know yet.” So Zvonkin throws himself full-bore into discomfiting and unsettling the pat answers they’re bringing home from school, emulating Socrates both in his methods and in his singular focus on corrupting the youth.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-149130375" href="https://www.thepsmiths.com/p/review-math-from-three-to-seven-by#footnote-5-149130375" target="_self" rel="">5</a></span></p><p><span>About a year into this, Zvonkin decides that the kids are ready for their first real </span><em>problem</em><span>, and asks them the following simple question: how many ways are there to select two items from a collection of five items, if the order in which they’re chosen doesn’t matter? This is a question from the field of combinatorics, which Zvonkin notes with a sneer used to be taught to advanced high-schoolers, until the authorities decided it was too hard. And here he’s going to teach it to preschoolers, or, even harder, he’s going to make them figure it out for themselves with very few hints.</span></p><p><span>He does this, maddeningly, by showing them the </span><em>problem</em><span> over and over again, in many different guises and disguises, over a period of months. He does it inductively, because four-year-olds are tactile creatures who have not yet assembled the cognitive tools required to reason formally and symbolically (more on that in </span><a href="https://www.thepsmiths.com/p/review-mindstorms-by-seymour-papert" rel="">my review of </a><em><a href="https://www.thepsmiths.com/p/review-mindstorms-by-seymour-papert" rel="">Mindstorms</a></em><span>). First he gets beads and asks them to make chains of two red and three blue and asks them how many possible such chains there are. A boisterous debate follows: if you take a chain and rotate it 180 degrees, does it count as the same chain or a different one? This is a good mathematical question, and Zvonkin gives the mathematician’s answer: “We can make up the rules, we can decide whether it counts, so now we have two </span><em>problems</em><span> instead of one, but it turns out that one of them is both easier to solve and more interesting.”</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00deefd5-8a57-461f-a784-9f5bc1c74470_756x508.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00deefd5-8a57-461f-a784-9f5bc1c74470_756x508.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00deefd5-8a57-461f-a784-9f5bc1c74470_756x508.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00deefd5-8a57-461f-a784-9f5bc1c74470_756x508.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00deefd5-8a57-461f-a784-9f5bc1c74470_756x508.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00deefd5-8a57-461f-a784-9f5bc1c74470_756x508.png" width="756" height="508" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/00deefd5-8a57-461f-a784-9f5bc1c74470_756x508.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:508,&quot;width&quot;:756,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:95988,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00deefd5-8a57-461f-a784-9f5bc1c74470_756x508.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00deefd5-8a57-461f-a784-9f5bc1c74470_756x508.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00deefd5-8a57-461f-a784-9f5bc1c74470_756x508.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00deefd5-8a57-461f-a784-9f5bc1c74470_756x508.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The kids have fun with their beads, but they are still nowhere near ready for the theory of how to count ways of choosing </span><em>k</em><span> items from an </span><em>n</em><span> item set. That’s okay, he’s just getting started. He lets a few months go by, he lets them forget it, then he gives them sheets of paper with rows of five circles, and challenges them to find as many different ways as possible of coloring just two of them. As the kids are coloring, he asks if this reminds them of anything… they pause. There’s something in the backs of their heads, but they aren’t sure. It’s getting closer though. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F827eb271-8b04-48db-ad2d-6c0d63863693_1518x432.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F827eb271-8b04-48db-ad2d-6c0d63863693_1518x432.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F827eb271-8b04-48db-ad2d-6c0d63863693_1518x432.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F827eb271-8b04-48db-ad2d-6c0d63863693_1518x432.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F827eb271-8b04-48db-ad2d-6c0d63863693_1518x432.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F827eb271-8b04-48db-ad2d-6c0d63863693_1518x432.png" width="1456" height="414" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/827eb271-8b04-48db-ad2d-6c0d63863693_1518x432.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:414,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:163812,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F827eb271-8b04-48db-ad2d-6c0d63863693_1518x432.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F827eb271-8b04-48db-ad2d-6c0d63863693_1518x432.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F827eb271-8b04-48db-ad2d-6c0d63863693_1518x432.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F827eb271-8b04-48db-ad2d-6c0d63863693_1518x432.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>More months go by, and it’s time for the </span><em>problem</em><span> to visit again. This time he brings a 4x3 grid, and explains that it’s a city map, and that a taxi is starting in the lower-left corner, and wants to go to the top-right corner, and asks how many possible paths it can take with no backtracking. Once again they leap into action. Once again he asks if this reminds them of anything. The kids are confused, they’ve never played the taxi game before. This time he drops a hint, takes one of their paths, and each time the taxi makes a decision to go right, he draws a little red circle, and each time it goes up he draws a little blue circle. Pandemonium. The kids are going nuts. They have discovered the beauty of isomorphism, a secret passage leading from one part of the world, up into the Platonic realm, and back into another superficially very different place. But they still haven’t answered the </span><em>problem</em><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd06c3e29-eb71-41a0-a03e-6a4050b5565c_720x526.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd06c3e29-eb71-41a0-a03e-6a4050b5565c_720x526.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd06c3e29-eb71-41a0-a03e-6a4050b5565c_720x526.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd06c3e29-eb71-41a0-a03e-6a4050b5565c_720x526.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd06c3e29-eb71-41a0-a03e-6a4050b5565c_720x526.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd06c3e29-eb71-41a0-a03e-6a4050b5565c_720x526.png" width="720" height="526" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d06c3e29-eb71-41a0-a03e-6a4050b5565c_720x526.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:526,&quot;width&quot;:720,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:108099,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd06c3e29-eb71-41a0-a03e-6a4050b5565c_720x526.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd06c3e29-eb71-41a0-a03e-6a4050b5565c_720x526.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd06c3e29-eb71-41a0-a03e-6a4050b5565c_720x526.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd06c3e29-eb71-41a0-a03e-6a4050b5565c_720x526.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>He lets a couple more months go by. Now he places five empty matchboxes and two balls on the table. How many different ways are there to put the balls in the boxes? This time the kids figure it out quickly: it’s the same as the puzzle with the beads! And that means they’re finally ready to stare the </span><em>problem</em><span> in the face, ready to </span><em>begin</em><span> their ascent of this mountain. The answers to all of these puzzles, which are really all the same puzzle in different clothing, is that there are ten ways. But if we enumerate all ten of them, how can we prove that there aren’t any more? And what if our original set had four items or six items instead of five, how would the answer change?  And these little kids who don’t yet know their times tables will totally figure it out.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4985941-faca-4a0a-a7ba-96edc69cf7ec_1518x224.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4985941-faca-4a0a-a7ba-96edc69cf7ec_1518x224.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4985941-faca-4a0a-a7ba-96edc69cf7ec_1518x224.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4985941-faca-4a0a-a7ba-96edc69cf7ec_1518x224.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4985941-faca-4a0a-a7ba-96edc69cf7ec_1518x224.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4985941-faca-4a0a-a7ba-96edc69cf7ec_1518x224.png" width="1456" height="215" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d4985941-faca-4a0a-a7ba-96edc69cf7ec_1518x224.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:215,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:97147,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4985941-faca-4a0a-a7ba-96edc69cf7ec_1518x224.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4985941-faca-4a0a-a7ba-96edc69cf7ec_1518x224.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4985941-faca-4a0a-a7ba-96edc69cf7ec_1518x224.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4985941-faca-4a0a-a7ba-96edc69cf7ec_1518x224.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9889f108-78cc-4004-a674-46cd168c0924_794x224.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9889f108-78cc-4004-a674-46cd168c0924_794x224.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9889f108-78cc-4004-a674-46cd168c0924_794x224.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9889f108-78cc-4004-a674-46cd168c0924_794x224.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9889f108-78cc-4004-a674-46cd168c0924_794x224.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9889f108-78cc-4004-a674-46cd168c0924_794x224.png" width="794" height="224" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9889f108-78cc-4004-a674-46cd168c0924_794x224.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:224,&quot;width&quot;:794,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:40420,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9889f108-78cc-4004-a674-46cd168c0924_794x224.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9889f108-78cc-4004-a674-46cd168c0924_794x224.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9889f108-78cc-4004-a674-46cd168c0924_794x224.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9889f108-78cc-4004-a674-46cd168c0924_794x224.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>What should we expect from children intellectually? Are they, as Piaget says, neurologically incapable of certain kinds of thought? Or are they, as Zvonkin believes, capable of solving challenging problems that many adults would struggle with? I think the answer is “both.” There’s a persistently wrong belief that many people, even many teachers, implicitly or explicitly hold, which says that children are basically like adults, only they’re dumber and know less stuff. Call this the “homunculus theory” of childhood cognition. But the truth is that kids are more like artificial neural networks — they’re at a subtly different point in mind-space, they’re good and bad at different things than adults are good and bad at. </p><p><span>For example: young children are much more concrete thinkers than adults, whether that’s for neurological reasons or because they haven’t built the cognitive tools for abstraction yet.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-6-149130375" href="https://www.thepsmiths.com/p/review-math-from-three-to-seven-by#footnote-6-149130375" target="_self" rel="">6</a></span><span> When you meet an adult who can’t reason symbolically or propositionally, they’re often (though not always!) pretty dumb and bad at thinking in general. What a mistake it is to apply the homunculus theory and assume the same of a child. You’ll underestimate and condescend to them and cheat them of opportunities to figure things out for themselves, while simultaneously getting frustrated at their inability to grasp incredibly simple ideas. They aren’t homunculi, they aren’t tiny adults, they’re an intelligence that is differently shaped from yours. This is an unintuitive and uncomfortable idea for us, and we don’t have the mirror neurons to really grok it, but consider it good practice for all the very strangely shaped intelligences we will be encountering in the coming decades.</span></p><p><span>One thing Zvonkin does that works about equally well on kids and adults is repeating things over and over again, and letting a long time go by between the repetitions. The reason this works is the same reason that when I’m trying to figure something out for myself, I’ll think about it really hard for a while and then go for a walk or take a shower. You’re hijacking the interplay between the focused-mode and the diffuse-mode of cognition (which I discussed in a </span><a href="https://www.thepsmiths.com/p/review-the-cruise-of-the-nona-by" rel="">review of a book by Hillaire Belloc</a><span>, of all people). It works even better if on each repetition you come at the problem from a different angle, so your subconscious has new material to chew on. Using this technique I was once able to get a five- or six-year old to figure out why summing the numbers in any row of </span><a href="https://en.wikipedia.org/wiki/Pascal%27s_triangle" rel="">Pascal’s triangle</a><span> must give a power of two.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-7-149130375" href="https://www.thepsmiths.com/p/review-math-from-three-to-seven-by#footnote-7-149130375" target="_self" rel="">7</a></span></p><p><span>The most important lesson of Zvonkin’s book, though, and also the realest and the rawest, comes at the end. He’s been running his little circle for four years, has racked up triumphs and failures, formed all sorts of theories about how best to teach math to little kids, and recorded it all in gory detail. That last is important, because his daughter Evgenia is now a little older than his son Dmitry was when he started the original circle, so naturally he decides to do it all over again with Evgenia and with the neighborhood kids who are </span><em>her</em><span> age. But this time, he’s armed! He already knows what works and doesn’t, he has handwritten journals full of hard-won lessons, he’ll be able to save a ton of time and run everything much more smoothly.</span></p><p><span>As if. In the second iteration of the circle, all of his notes are completely useless, and all of his initial attempts to teach anything fail, because these are different kids with different aptitudes and different interests. Zvonkin, raised in a communist society and a believer in the absolute malleability of human nature, is fairly bowled over by this, especially by how young all these differences are manifesting. Reading between the lines, it sounds like he got quite lucky with his first set of children, and that the second group were much more challenging to teach.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-8-149130375" href="https://www.thepsmiths.com/p/review-math-from-three-to-seven-by#footnote-8-149130375" target="_self" rel="">8</a></span><span> The most eloquent testimony to this is that after about a year he gives up, and the journal ends abruptly.</span></p><p><span>This is just an extreme version of the universal experience of being the parent of more than one child. The moment your second kid is born, or sometimes even before they’re born, they begin teaching you how little impact you actually had over the life trajectory of your first kid. The differences between them,</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-9-149130375" href="https://www.thepsmiths.com/p/review-math-from-three-to-seven-by#footnote-9-149130375" target="_self" rel="">9</a></span><span> despite the fact that you do almost everything the same, testify to just how much of parenting is actually a powerless process of watching a new being discover and disclose to the world what it is going to be. In one way, this makes what’s going on even more existentially dramatic — you’ve produced not a blank slate that you can program, but an alien intelligence that </span><a href="https://www.thepsmiths.com/p/review-the-children-of-men-by-pd" rel="">will produce a whole new universe</a><span>. </span></p><p><span>The other thing is that despite being a tiny bit demoralizing, this is also tremendously liberating: your actions can only change who they are on the margin, so you can relax and do things that are fun for both of you. Sometimes that means a favorite book or movie, sometimes a love of sport or hiking. And sometimes, the shared activity that brings both of you joy is a love of thinking, even the strenuous sort of thinking that comes from wrestling with a </span><em>problem</em><span>.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Northern Ontario man solves local legend, finds vintage liquor at bottom of lake (198 pts)]]></title>
            <link>https://www.cbc.ca/news/canada/sudbury/larder-lake-local-legend-sunken-taxi-vintage-liquor-1.7332124</link>
            <guid>41715747</guid>
            <pubDate>Tue, 01 Oct 2024 23:56:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cbc.ca/news/canada/sudbury/larder-lake-local-legend-sunken-taxi-vintage-liquor-1.7332124">https://www.cbc.ca/news/canada/sudbury/larder-lake-local-legend-sunken-taxi-vintage-liquor-1.7332124</a>, See on <a href="https://news.ycombinator.com/item?id=41715747">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="detailContent"><!--$--><p><span><a href="https://www.cbc.ca/news/canada/sudbury"><span>Sudbury</span></a></span></p><!--/$--><p>After 35 years of searching, a man from the small northern Ontario town of Larder Lake has uncovered the truth about a local legend dating back to 1937.</p><h2 lang="en">Bottle of liquor recovered from the bottom of Larder Lake will be auctioned off at local charity event</h2><!--$--><!--/$--><!--$--><div data-cy="storyWrapper"><!--$--><figure><p><img alt="A man in a wet suit holds up a bottle of booze ." src="https://i.cbc.ca/1.7332335.1727209101!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/jason-and-the-bottle.jpg" data-cy="leadmedia-story-img" fetchpriority="high"></p><figcaption>Jason Ploeger holds up a bottle of whisky that he recovered from a sunken taxi in Larder Lake following a 35-year search. <!-- --> <!-- -->(Submitted by Jason Ploeger)</figcaption></figure><!--/$--><div><p>As a kid, Jason Ploeger remembers people searching in the waters of Larder Lake for a taxi cab that&nbsp;sank beneath the surface decades before.</p><p>And of course, like everyone else in the small northern Ontario town near&nbsp;the Québec border&nbsp;that was once a major gold mining centre, he heard the stories.&nbsp;&nbsp;</p><p>"Growing up in the town, we all heard the legend," said Ploeger.</p><p>"Everybody you talked to had a different story."</p><div><ul><li><a href="https://www.cbc.ca/news/canada/sudbury/northern-ontario-bottle-hunters-1.6242208" text="Tag along with two northern Ontario bottle hunters as they dig for treasure in old garbage dumps" flag="Audio" data-contentid=""><p><span>Audio</span></p><span>Tag along with two northern Ontario bottle hunters as they dig for treasure in old garbage dumps</span></a></li></ul><ul><li><a href="https://www.cbc.ca/news/canada/sudbury/unclaimed-1million-lottery-ticket-larder-lake-1.6851883" text="Small northern Ontario town gripped with lotto fever as $1M ticket set to expire" flag="" data-contentid=""><span>Small northern Ontario town gripped with lotto fever as $1M ticket set to expire</span></a></li></ul></div><p>The version of the story Ploeger heard the most was that the taxi went through the ice of Larder Lake on the way to a Christmas party, carrying liquor and holiday bonuses for local gold&nbsp;miners.</p><p>But there were other stories that the cab was hauling gold bars or gold ore, or that is was a bootlegger delivering alcohol to mining camps in the Larder Lake area.&nbsp;</p><p>Ploeger spent 35 years searching for the taxi and then while fishing in a local derby last year, he saw something come up on the screen of his side imager that he uses to look deep into the lake for fish. Or sunken treasure.&nbsp;&nbsp;</p><div><figure><p><img loading="lazy" alt="An underwater photo of a hub cap." srcset="https://i.cbc.ca/1.7332337.1727270911!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/sunken-hub-cap.jpg 300w,https://i.cbc.ca/1.7332337.1727270911!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/sunken-hub-cap.jpg 460w,https://i.cbc.ca/1.7332337.1727270911!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/sunken-hub-cap.jpg 620w,https://i.cbc.ca/1.7332337.1727270911!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/sunken-hub-cap.jpg 780w,https://i.cbc.ca/1.7332337.1727270911!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/sunken-hub-cap.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.7332337.1727270911!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/sunken-hub-cap.jpg" data-cy="image-img"></p><figcaption>The hub cap showing the REO logo of the 1929 REO Flying Cloud that sank to the bottom of Larder Lake in 1937. <!-- --> <!-- -->(Submitted by Jason Ploeger)</figcaption></figure></div><p>"Is that what I think it is?" he said to himself, seeing the outline of the back window and tires, knowing it was "obviously a car."</p><p>"I was excited. I almost gave up fishing for the day."</p><p>But instead, he marked the spot and went back later with some fellow divers, going down 15 metres into the pitch black waters of Larder Lake.</p><p>"I had one of the other divers right beside me and I'd have to shine my light right on him to see him," Ploeger said.</p><p>"It was a spooky dive."</p><div><figure><p><img loading="lazy" alt="Two divers on a boat getting ready to go down " srcset="https://i.cbc.ca/1.7332323.1727209147!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/divers-on-larder-lake.jpg 300w,https://i.cbc.ca/1.7332323.1727209147!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/divers-on-larder-lake.jpg 460w,https://i.cbc.ca/1.7332323.1727209147!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/divers-on-larder-lake.jpg 620w,https://i.cbc.ca/1.7332323.1727209147!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/divers-on-larder-lake.jpg 780w,https://i.cbc.ca/1.7332323.1727209147!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/divers-on-larder-lake.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.7332323.1727209147!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/divers-on-larder-lake.jpg" data-cy="image-img"></p><figcaption>Plans are to keep the taxi at the bottom of the lake, so that other divers can visit the sunken relic.  <!-- --> <!-- -->(Submitted by Jason Ploeger)</figcaption></figure></div><p>There was no gold, but they did spot six bottles in the back of what they now know is a 1929 REO Flying Cloud.</p><p>He says&nbsp;he later spoke with the daughter of one of the five men who rode in the taxi that day and learned they were heading to a Christmas party at the&nbsp;Martin-Bird Mine when the car started to go through the ice and everyone got out&nbsp;safely.&nbsp;</p><div><ul><li><a href="https://www.cbc.ca/news/canada/sudbury/jack-munroe-documentary-1.7214041" text="New documentary tells the story of 'lost legend' Jack Munroe" flag="" data-contentid=""><span>New documentary tells the story of 'lost legend' Jack Munroe</span></a></li></ul><ul><li><a href="https://www.cbc.ca/news/canada/sudbury/black-history-northeastern-ontario-1.5924113" text="Black settlers part of the much more 'complex history' of northern Ontario" flag="Black History" data-contentid=""><p><span>Black History</span></p><span>Black settlers part of the much more 'complex history' of northern Ontario</span></a></li></ul></div><p>Ploeger says they brought one bottle to the surface and it "promptly exploded on my boat from the pressure," but he dipped a finger in it and said it tasted like&nbsp;"very, very smooth" rye whisky.</p><p>This summer, they went back down and brought up five more bottles, two of which also blew their cork on the way to the surface.&nbsp;</p><p>"I do not believe it was a good whisky when it was bought," Ploeger said of drinking from one of the bottles.</p><p>"I'm not blind yet so that's a good sign."</p><div><figure><p><img loading="lazy" alt="A hand holding a bottle of whisky on a boat" srcset="https://i.cbc.ca/1.7332320.1727271016!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/bottle-of-whisky.jpg 300w,https://i.cbc.ca/1.7332320.1727271016!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/bottle-of-whisky.jpg 460w,https://i.cbc.ca/1.7332320.1727271016!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/bottle-of-whisky.jpg 620w,https://i.cbc.ca/1.7332320.1727271016!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/bottle-of-whisky.jpg 780w,https://i.cbc.ca/1.7332320.1727271016!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/bottle-of-whisky.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.7332320.1727271016!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/bottle-of-whisky.jpg" data-cy="image-img"></p><figcaption>One of the bottles of alcohol that Jason Ploeger recovered from a taxi cab that sunk into Larder Lake back in 1937. <!-- --> <!-- -->(Submitted by Jason Ploeger)</figcaption></figure></div><p>One of the intact bottles recovered from the bottom will be auctioned off on Sept. 28 at a steak dinner and charity auction to benefit a community group&nbsp;called the Friends of Larder.</p><p>"I've been thinking for a while, if I ever found it, it's not my story, it's the town's story. So I figured I would donate it back to the town," Ploeger said.&nbsp;</p><p>He says the car is "too fragile to remove," so it will stay at the bottom of Larder Lake for other divers to enjoy.</p><div><figure><p><img loading="lazy" alt="A man holding a shot glass winces." srcset="https://i.cbc.ca/1.7332327.1727209195!/fileImage/httpImage/image.jpeg_gen/derivatives/original_300/trying-vintage-booze.jpeg 300w,https://i.cbc.ca/1.7332327.1727209195!/fileImage/httpImage/image.jpeg_gen/derivatives/original_460/trying-vintage-booze.jpeg 460w,https://i.cbc.ca/1.7332327.1727209195!/fileImage/httpImage/image.jpeg_gen/derivatives/original_620/trying-vintage-booze.jpeg 620w,https://i.cbc.ca/1.7332327.1727209195!/fileImage/httpImage/image.jpeg_gen/derivatives/original_780/trying-vintage-booze.jpeg 780w,https://i.cbc.ca/1.7332327.1727209195!/fileImage/httpImage/image.jpeg_gen/derivatives/original_1180/trying-vintage-booze.jpeg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.7332327.1727209195!/fileImage/httpImage/image.jpeg_gen/derivatives/original_780/trying-vintage-booze.jpeg" data-cy="image-img"></p><figcaption>Ploeger says he tried some of the recovered alcohol and it was 'super smooth' but another bottle was very 'rough' whisky. <!-- --> <!-- -->(Submitted by Jason Ploeger)</figcaption></figure></div><p>"It's a weird feeling, because I've been looking, looking and looking," said Ploeger.</p><p>"And now I've seen it. I've been there. And it's over."</p><p>But he says he is still chasing down other local mysteries, including some boats sunk in the lake and a military plane that crashed in the bush nearby during the Second World War.&nbsp;</p></div></div><!--/$--><!--$--><!--/$--><div><h2>ABOUT THE AUTHOR</h2><div><figure><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.4887087.1664195273!/fileImage/httpImage/image.jpg_gen/derivatives/square_1180/erik-white.jpg?im=Resize%3D620 300w,https://i.cbc.ca/1.4887087.1664195273!/fileImage/httpImage/image.jpg_gen/derivatives/square_1180/erik-white.jpg?im=Resize%3D620 460w,https://i.cbc.ca/1.4887087.1664195273!/fileImage/httpImage/image.jpg_gen/derivatives/square_1180/erik-white.jpg?im=Resize%3D620 620w" sizes="(max-width: 258pxpx) 258pxpx" src="https://i.cbc.ca/1.4887087.1664195273!/fileImage/httpImage/image.jpg_gen/derivatives/square_1180/erik-white.jpg?im=Resize%3D620" data-cy="author-image-img"></p></figure></div><p>Erik White is a CBC journalist based in Sudbury. He covers a wide range of stories about northern Ontario. Send story ideas to erik.white@cbc.ca</p><ul></ul></div><!--$--><!--/$--><!--$--><!--/$--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How CERN serves 1EB of data via FUSE [video] (226 pts)]]></title>
            <link>https://kernel-recipes.org/en/2024/schedule/how-cern-serves-1eb-of-data-via-fuse/</link>
            <guid>41715277</guid>
            <pubDate>Tue, 01 Oct 2024 23:05:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kernel-recipes.org/en/2024/schedule/how-cern-serves-1eb-of-data-via-fuse/">https://kernel-recipes.org/en/2024/schedule/how-cern-serves-1eb-of-data-via-fuse/</a>, See on <a href="https://news.ycombinator.com/item?id=41715277">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>CERN, the European Organization for Nuclear Research, generates vast amounts of data from experiments at the Large Hadron Collider (LHC). CERN’s Storage and Data Management Group at the IT department is responsible for managing this data, including its long-term archival on tape, distribution across the Worldwide LHC Computing Grid (WLCG), as well as providing secure and convenient forms of data access to more than 30000 users that need it.</p>
<p>In this talk we will give an overview of the open source projects leveraged by CERN to satisfy the storage needs of the&nbsp; organization, such as CERNBox and EOS, and discuss some of the unique challenges faced by the high-energy physics community in data storage and management. Additionally, we discuss how FUSE is used to allow users to access data securely from anywhere in the world.</p>
<p><strong>Guilherme AMADIO</strong></p></div></div>]]></description>
        </item>
    </channel>
</rss>