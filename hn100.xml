<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 19 Oct 2024 05:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[US probes Tesla's Full Self-Driving software in 2.4M cars after fatal crash (123 pts)]]></title>
            <link>https://www.reuters.com/business/autos-transportation/nhtsa-opens-probe-into-24-mln-tesla-vehicles-over-full-self-driving-collisions-2024-10-18/</link>
            <guid>41884740</guid>
            <pubDate>Sat, 19 Oct 2024 00:46:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/business/autos-transportation/nhtsa-opens-probe-into-24-mln-tesla-vehicles-over-full-self-driving-collisions-2024-10-18/">https://www.reuters.com/business/autos-transportation/nhtsa-opens-probe-into-24-mln-tesla-vehicles-over-full-self-driving-collisions-2024-10-18/</a>, See on <a href="https://news.ycombinator.com/item?id=41884740">Hacker News</a></p>
Couldn't get https://www.reuters.com/business/autos-transportation/nhtsa-opens-probe-into-24-mln-tesla-vehicles-over-full-self-driving-collisions-2024-10-18/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Express v5 (123 pts)]]></title>
            <link>https://expressjs.com/2024/10/15/v5-release.html</link>
            <guid>41882955</guid>
            <pubDate>Fri, 18 Oct 2024 20:02:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://expressjs.com/2024/10/15/v5-release.html">https://expressjs.com/2024/10/15/v5-release.html</a>, See on <a href="https://news.ycombinator.com/item?id=41882955">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="blog-doc" markdown="1">


<p>Ten years ago (July 2014) the <a href="https://github.com/expressjs/express/pull/2237">Express v5 release pull request</a> was opened, and now at long last it’s been merged and published!</p>
<p>We want to recognize the work of all our contributors, especially <a href="https://github.com/dougwilson">Doug Wilson</a>, who spent the last ten years ensuring Express was the most stable project around. Without his contributions and those of many others, this release could not have happened.</p>
<p>Eight months ago we went public with a plan to move <a href="https://github.com/expressjs/discussions/issues/160">Express forward</a>. This plan included re-committing to the governance outlined years ago and adding more contributors to help kickstart progress. Many people may not realize that robust project governance is critical to the health of a large open-source project. We want to thank the <a href="https://github.com/openjs-foundation/cross-project-council/">OpenJS Foundation Cross Project
Council</a> and its members for helping us put together this plan.</p>
<h2 id="so-what-about-v5">So what about v5?</h2>
<p>This release is designed to be boring!
That may sound odd, but we’ve intentionally kept it simple to unblock the ecosystem and enable more impactful changes in future releases. This is also about signaling to the Node.js ecosystem that Express is moving again.
The focus of this release is on dropping old Node.js version support, addressing security concerns, and simplifying maintenance.</p>
<p>Before going into the changes in this release, let’s address why it was released v5 on the <code>next</code> dist-tag. As part of reviving the project, we started a <a href="https://github.com/expressjs/security-wg">Security working group</a> and <a href="https://github.com/expressjs/security-wg?tab=readme-ov-file#security-triage-team">security triage team</a> to address the growing needs around open source supply chain security. We undertook a security audit (more details to come on that) and uncovered some problems that needed to be addressed. Thus, in addition to the “normal” work done in public issues, we also did a lot of security work in private forks.
This security work required orchestration when releasing, to ensure the code and CVE reports went out together. You can find a summary of the most recent vulnerabilities patched in <a href="https://expressjs.com/2024/09/29/security-releases.html">our security release notes</a>.</p>
<p>While we weren’t able to simultaneously release v5, this blog post, the changelog, and documentation, we felt it was most important to have a secure and stable release.</p>
<p>As soon as possible, we’ll provide more details on our long-term support (LTS) plans, including when the release will move from <code>next</code> to <code>latest</code>. For now, if you are uncomfortable being on the bleeding edge (even if it is a rather dull edge) then you should wait to upgrade until the release is tagged <code>latest</code>. That said, we look forward to working with you to address any bugs you encounter as you upgrade.</p>
<h2 id="breaking-changes">Breaking changes</h2>
<p>The v5 release has the minimum possible number of breaking changes, listed here in order of impact to applications.</p>
<ul>
<li><a href="#goodbye-nodejs-010-hello-node-18">Ending support for old Node.js versions</a></li>
<li><a href="#changes-to-path-matching-and-regular-expressions">Changes to path matching and regular expressions</a></li>
<li><a href="#promise-support">Promise support</a></li>
<li><a href="#body-parser-changes">Body parser changes</a></li>
<li><a href="#removing-deprecated-method-signatures">Removing deprecated method signatures</a></li>
</ul>
<p>There are also a number of subtle changes: for details, see <a href="http://expressjs.com/en/guide/migrating-5">Migrating to Express 5</a>.</p>
<h3 id="ending-support-for-old-nodejs-versions">Ending support for old Node.js versions</h3>
<p>Goodbye Node.js 0.10, hello Node 18 and up!</p>
<p>This release drops support for Node.js versions before v18. This is an important change because supporting old Node.js versions has been holding back many critical performance and maintainability changes. This change also enables more stable and maintainable continuous integration (CI), adopting new language and runtime features, and dropping dependencies that are no longer required.</p>
<p>We recognize that this might cause difficulty for some enterprises with older or “parked” applications, and because of this we are working on a <a href="https://expressjs.com/2024/10/01/HeroDevs-partnership-announcement.html">partnership with HeroDevs</a> to offer “never-ending support” that will include critical security patches even after v4 enters end-of-life (more on these plans soon). That said, we strongly suggest that you update to modern Node.js versions as soon as possible.</p>
<h3 id="changes-to-path-matching-and-regular-expressions">Changes to path matching and regular expressions</h3>
<p>The v5 releases updates to <code>path-to-regexp@8.x</code> from <code>path-to-regexp@0.x</code>, which incorporates many years of changes. If you were using any of the 5.0.0-beta releases, a last-minute update which greatly changed the path semantics to <a href="https://blakeembrey.com/posts/2024-09-web-redos/">remove the possibility of any ReDoS attacks</a>. For more detailed changes, <a href="https://github.com/pillarjs/path-to-regexp?tab=readme-ov-file#express--4x">see the <code>path-to-regexp</code> readme</a>.</p>
<h4 id="no-more-regex">No more regex</h4>
<p>This release no longer supports “sub-expression” regular expressions, for example <code>/:foo(\\d+)</code>.
This is a commonly-used pattern, but we removed it for security reasons. Unfortunately, it’s easy to write a regular expression that has exponential time behavior when parsing input: The dreaded regular expression denial of service (ReDoS) attack. It’s very difficult to prevent this, but as a library that converts strings to regular expressions, we are on the hook for such security aspects.</p>
<p><em>How to migrate:</em> The best approach to prevent ReDoS attacks is to use a robust input validation library. <a href="https://www.npmjs.com/search?q=validate%20express">There are many on <code>npm</code></a> depending on your needs. TC member Wes Todd maintains <a href="https://www.npmjs.com/package/@wesleytodd/openapi">a middleware-based “code first” OpenAPI library</a> for this kind of thing.</p>
<h4 id="splats-optional-and-captures-oh-my">Splats, optional, and captures oh my</h4>
<p>This release includes simplified patterns for common route patterns. With the removal of regular expression semantics comes other small but impactful changes to how you write your routes.</p>
<ol>
<li><code>:name?</code> becomes <code>{:name}</code>. Usage of <code>{}</code> for optional parts of your route means you can now do things like <code>/base{/:optional}/:required</code> and what parts are actually optional is much more explicit.</li>
<li><code>*</code> becomes <code>*name</code>.</li>
<li>New reserved characters: <code>(</code>, <code>)</code>, <code>[</code>, <code>]</code>, <code>?</code>, <code>+</code>, &amp; <code>!</code>. These have been reserved to leave room for future improvements and to prevent mistakes when migrating where those characters mean specific things in previous versions.</li>
</ol>
<h4 id="name-everything">Name everything</h4>
<p>This release no longer supports ordered numerical parameters.</p>
<p>In Express v4, you could get numerical parameters using regex capture groups (for example, <code>/user(s?)</code> =&gt; <code>req.params[0] === 's'</code>). Now all parameters must be named. Along with requiring a name, Express now supports all valid JavaScript identifiers or quoted (for example, <code>/:"this"</code>).</p>
<h3 id="promise-support">Promise support</h3>
<p>This one may be a bit contentious, but we “promise” we’re moving in the right direction. We added support for returned <em>rejected</em> promises from errors raised in middleware. This <em>does not include</em> calling <code>next</code> from returned <em>resolved</em> promises. There are a lot of edge cases in old Express apps that have expectations of <code>Promise</code> behavior, and before we can run we need to walk. For most folks, this means you can now write middleware like the following:</p>
<pre><code>app.use(async (req, res, next) =&gt; {
  req.locals.user = await getUser(req);
  next();
});
</code></pre>
<p>Notice that this example uses <code>async/await</code> and the <code>getUser</code> call may throw an error (if, for example, the user doesn’t exist, the user database is down, and so on), but we still call <code>next</code> if it is successful. We don’t need to catch the error in line anymore if we want to rely on error-handling middleware instead because the router will now catch the rejected promise and treat that as calling <code>next(err)</code>.</p>
<p>NOTE: Best practice is to handle errors as close to the site as possible. So while this is now handled in the router, it’s best to catch the error in the middleware and handle it without relying on separate error-handling middleware.</p>
<h3 id="body-parser-changes">Body parser changes</h3>
<p>There are a number of <code>body-parser</code> changes:</p>
<ul>
<li>Add option to customize the urlencoded body depth with a default value of 32 as mitigation for <a href="https://nvd.nist.gov/vuln/detail/CVE-2024-45590">CVE-2024-45590</a> (<a href="https://github.com/expressjs/body-parser/commit/b2695c4450f06ba3b0ccf48d872a229bb41c9bce">technical details</a>)</li>
<li>Remove deprecated <code>bodyParser()</code> combination middleware</li>
<li><code>req.body</code> is no longer always initialized to <code>{}</code></li>
<li><code>urlencoded</code> parser now defaults <code>extended</code> to false</li>
<li>Added support for Brotli lossless data compression</li>
</ul>
<h3 id="removing-deprecated-method-signatures">Removing deprecated method signatures</h3>
<p>Express v5 removes a number of deprecated method signatures, many of which were carried over from v3. Below are the changes you need to make:</p>
<ul>
<li><code>res.redirect('back')</code> and <code>res.location('back')</code>: The magic string <code>'back'</code> is no longer supported. Use <code>req.get('Referrer') || '/'</code> explicitly instead.</li>
<li><code>res.send(status, body)</code> and <code>res.send(body, status)</code> signatures: Use <code>res.status(status).send(body)</code>.</li>
<li><code>res.send(status)</code> signature: Use <code>res.sendStatus(status)</code> for simple status responses, or <code>res.status(status).send()</code> for sending a status code with an optional body.</li>
<li><code>res.redirect(url, status)</code> signature: Use <code>res.redirect(status, url)</code>.</li>
<li><code>res.json(status, obj)</code> and <code>res.json(obj, status)</code> signatures: Use <code>res.status(status).json(obj)</code>.</li>
<li><code>res.jsonp(status, obj)</code> and <code>res.jsonp(obj, status)</code> signatures: Use <code>res.status(status).jsonp(obj)</code>.</li>
<li><code>app.param(fn)</code>: This method has been deprecated. Instead, access parameters directly via <code>req.params</code>, or use <code>req.body</code> or <code>req.query</code> as needed.</li>
<li><code>app.del('/', () =&gt; {})</code> method: Use <code>app.delete('/', () =&gt; {})</code> instead.</li>
<li><code>req.acceptsCharset</code>: Use <code>req.acceptsCharsets</code> (plural).</li>
<li><code>req.acceptsEncoding</code>: Use <code>req.acceptsEncodings</code> (plural).</li>
<li><code>req.acceptsLanguage</code>: Use <code>req.acceptsLanguages</code> (plural).</li>
<li><code>res.sendfile</code> method: Use <code>res.sendFile</code> instead.</li>
</ul>
<p>As a framework, we aim to ensure that the API is as consistent as possible. We’ve removed these deprecated signatures to make the API more predictable and easier to use. By streamlining each method to use a single, consistent signature, we simplify the developer experience and reduce confusion.</p>
<h2 id="migration-and-security-guidance">Migration and security guidance</h2>
<p>For developers looking to migrate from v4 to v5, there’s a <a href="http://expressjs.com/en/guide/migrating-5">detailed migration guide</a> to help you navigate through the changes and ensure a smooth upgrade process.</p>
<p>Additionally, we’ve been working hard on a comprehensive <a href="https://github.com/expressjs/security-wg/blob/main/docs/ThreatModel.md">Threat Model</a> that helps illustrate our philosophy of a “Fast, unopinionated, minimalist web framework for Node.js.” It provides critical insights into areas like user input validation and security practices that are essential for safe and secure usage of Express in your applications.</p>
<h2 id="our-work-is-just-starting">Our work is just starting</h2>
<p>We see the v5 release as a milestone toward an Express ecosystem that’s a stable and reliable tool for companies, governments, educators, and hobby projects. It is our commitment as the new stewards of the Express project to move the ecosystem forward with this goal in mind. If you want to support this work, which we do on a volunteer basis, please consider supporting the project and its maintainers via <a href="https://opencollective.com/express">our sponsorship opportunities</a>.</p>
<p>We have an <a href="https://github.com/expressjs/discussions/issues/266">extensive working backlog</a> of tasks, PRs, and issues for Express and dependencies. Naturally, we expect developers will continue to report issues to add to this backlog and open PRs moving forward, and we’ll continue to collaborate with the community to triage and resolve them. We look forward to continuing to improve Express and making it useful for its users across the world.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Focus on decisions, not tasks (123 pts)]]></title>
            <link>https://technicalwriting.dev/strategy/decisions.html</link>
            <guid>41881872</guid>
            <pubDate>Fri, 18 Oct 2024 17:58:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://technicalwriting.dev/strategy/decisions.html">https://technicalwriting.dev/strategy/decisions.html</a>, See on <a href="https://news.ycombinator.com/item?id=41881872">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="main">
<span id="decisions"></span>
<p>A quote from <a href="https://xmlpress.net/publications/eppo/">Every Page Is Page One</a>
that has deeply changed how I approach technical writing:</p>
<blockquote>
<p>In technical communication, we don’t talk much about decision support; we talk
about task support… In many cases, the information people need to complete their
tasks is not information on how to operate machines, but information to support their
decision making… simply documenting the procedures is never enough… What I am talking
about is documenting the context, letting users know what decisions they must make,
making them aware of the consequences, and, as far as possible, leading them to
resources and references that will assist them in deciding what to do.</p></blockquote>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Feds Are Coming for John Deere over the Right to Repair (388 pts)]]></title>
            <link>https://gizmodo.com/the-feds-are-coming-for-john-deere-over-the-right-to-repair-2000513521</link>
            <guid>41880981</guid>
            <pubDate>Fri, 18 Oct 2024 16:32:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/the-feds-are-coming-for-john-deere-over-the-right-to-repair-2000513521">https://gizmodo.com/the-feds-are-coming-for-john-deere-over-the-right-to-repair-2000513521</a>, See on <a href="https://news.ycombinator.com/item?id=41880981">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              
              
              <p>The Federal Trade Commission is investigating tractor manufacturer John Deere over long standing allegations that Deere makes its farm equipment hard to repair. The investigation has been ongoing since 2021, and we know more about it now thanks to a <a href="https://www.reuters.com/business/us-ftc-probing-deere-antitrust-consumer-protection-inquiry-filing-shows-2024-10-17/">court filing</a> made public on Thursday.</p>

 <p>A data analytics company called Hargrove &amp; Associates Inc (HAI) who works for the Association of Equipment Manufacturers (AEM), of which John Deere is a prominent member, filed a court brief in an attempt to quash the FTC’s investigation. The FTC wants HAI to turn over data submitted from AEM about sales, but HAI is saying that the FTC’s request is too broad and could hurt its business.</p> <p>Court drama aside, HAI spelled out exactly what the FTC is looking for. “The stated purpose of the FTC’s [investigation] is ‘[t]o determine whether Deere &amp; Company, or any other person, has engaged in or is engaging in unfair, deceptive, anticompetitive, collusive, coercive, predatory, exploitative, or exclusionary acts or practices in or affecting commerce related to the repair of agricultural equipment in violation of Section 5 of the Federal Trade Commission Act,’” HAI said in the court records.</p> <p>John Deere has been notorious for years for making its farm equipment hard to repair. Much like today’s cars, John Deere’s farm equipment comes with a lot of computers. When something simple in one of its tractors or threshers breaks, a farmer can’t just fix it themselves. Even if the farmer has the technical and mechanical know-how to make a simple repair, they often have to return to the manufacturer at great expense. Why? The on-board computers brick the machines until a certified Deere technician flips a switch.</p> <p>Farmers have been complaining about this for years and Deere has repeatedly promised to make its tractors easier to repair. <a href="https://www.vice.com/en/article/john-deere-promised-farmers-it-would-make-tractors-easy-to-repair-it-lied/">It lied</a>. John Deere equipment was so hard to repair that it led to an explosion in the <a href="https://www.vice.com/en/article/the-used-tractor-market-is-far-wilder-than-the-used-car-market/">used tractor market</a>. Old farm equipment made before the advent of onboard computing sold for a pretty penny because it was easier to repair.</p>

 <p>In 2022, a group of farmers filed a class action lawsuit against John Deere and <a href="https://www.vice.com/en/article/john-deere-hit-with-class-action-lawsuit-for-alleged-tractor-repair-monopoly/">accused it</a> of running a repair monopoly. Deere, of course, attempted to get the case dismissed <a href="https://www.reuters.com/legal/litigation/deere-must-face-us-farmers-right-to-repair-lawsuits-judge-rules-2023-11-27/">but failed</a>. Last year, the company issued a “<a href="https://www.vice.com/en/article/john-deere-agrees-to-let-farmers-repair-tractors-as-long-as-states-dont-pass-any-laws/">memorandum of understanding</a>.” The document was a promise to farmers that it would finally let them repair their own equipment, so long as states didn’t pass any laws around the right to repair.</p> <p>Chief among Deere’s promises was that it would provide farmers and independent repair shops with the equipment and documentation they needed to repair their equipment. The promises of the memorandum have not come to pass. Senator Elizabeth Warren called Deere out in a letter about all of this on <a href="https://www.warren.senate.gov/imo/media/doc/final_-_warren_letter_to_john_deere_rerighttorepair.pdf">October 2</a>. “Rather than uphold their end of the bargain, John Deere has provided impaired tools and inadequate disclosures,” Warren said in the letter.</p>

 <p>Now we know, thanks to HAI’s court filing, that the FTC has been investigating John Deere for at least three years. That’s good news for farmers and anyone who buys groceries.</p> <p>“We are grateful that the FTC has taken our complaint seriously and is investigating Deere’s conduct. We should be able to fix our own stuff. When farmers can’t access the proprietary software tools which are required to diagnose or complete repairs, that means they have to wait for an authorized technician before they can finish their work,” Nathan Proctor, U.S. PIRG’s senior right to repair campaign director, said in a statement. “The weather doesn’t wait on a dealership’s schedule—a delay could mean the loss of your harvest.”</p>
                          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Subvert – Collectively owned music marketplace (171 pts)]]></title>
            <link>https://subvert.fm/</link>
            <guid>41880829</guid>
            <pubDate>Fri, 18 Oct 2024 16:17:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://subvert.fm/">https://subvert.fm/</a>, See on <a href="https://news.ycombinator.com/item?id=41880829">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" tabindex="-1">
                
<section>
  <p>Own and control the next Bandcamp.</p>
  <p>Join 989 artists, 156 labels, and 911 supporters in collectively owning and
    shaping Subvert.</p>
  <p><img src="https://subvert.fm/assets/images/book-closed.png?v=fea39ec7d0">
    <img src="https://subvert.fm/assets/images/book-open.png?v=fea39ec7d0">
  </p>
  
</section>
<section>
  <div><p>Introducing: Plan for the Artist-Owned Internet</p>
    <div>
      <p>
      Bandcamp's corporate acquisitions threaten independent music. It's time
      for a new model - one we collectively own and control.
      </p><p>
      Our zine outlines how we'll turn our vision of a collectively owned
      Bandcamp successor into a reality.
      </p><p>
      Get your copy today to join us as a Founding Member.
    </p></div>
  </div>
  <div>
    <p>JOIN AND
      RECEIVE:</p>
    <div>
      <p>Limited Edition Physical Zine</p>
      <p>Receive a limited physical copy of this publication
        and a membership certificate</p>
    </div>
    <div>
      <p>Subvert Co-op Membership</p>
      <p>Subvert is a cooperative, collectively owned by its
        members</p>
    </div>
    <div>
      <p>Founding Member Status</p>
      <p>Achieve a special status within the Subvert Co-op as
        a Founding Member</p>
    </div>
    
  </div>
</section>
<section>
  <video autoplay="" loop="" playsinline="" muted="" src="https://subvert.fm/assets/images/lp_video.mp4?v=fea39ec7d0" poster="https://subvert.fm/assets/images/zine-inside.png?v=fea39ec7d0">
</video></section>
<section id="foundingMember">
  <p>BECOME A FOUNDING MEMBER:</p>
  <div>
      <div>
        <p>Musicians &amp; Labels</p>
        <p>&nbsp;FREE</p>
        <p>Founding Artist or Label Membership in the
          Subvert Co-op
        </p>
      </div>
      <p><a href="https://subvert.typeform.com/joinus" target="_blank">JOIN AND CLAIM ZINE</a>
    </p></div>
</section>
<section>
  <p>BENEFITS:</p>
  <div>
    
    <p>
        Physical zine<br>
        Free worldwide shipping [Supporters]<br>
        Founding Membership in the Subvert Co-op<br>
        Access to our members only forum<br>
        Co-ownership of Subvert<br>
        Unique member number<br>
        Membership certificate<br>
        Ability to influence platform policies and features
      </p>
    
  </div>
</section>
<div>
        <p>“Subvert goes beyond
          hopetimism with a meticulously built, actionable framework. Very
          exciting!”
        </p>
        <div><p>
            Alex Durlak<br>
            Label Member<br>
            Idée Fixe Records</p></div>
        <p>“Subvert's new model of
          collective ownership could shatter the music industry's outdated
          hierarchy.”</p>
        <div><p>
            Lindsey Mills<br>
            Artist Member<br>
            Surfer Blood</p></div>
        <p>“Ownership is the
          fundamental tension in platforms and marketplaces. It's exciting to
          see Subvert address this head-on with a thoughtfully designed
          structure.”</p>
        <div><p>
            Charles Broskowski<br>
            Supporter Member<br>
            Founder, Are.na</p></div>
        <p>“Artists and labels
          have succumbed to a hopelessness in the clenches of tech’s strong-arm.
          Subvert is a step forward to fight this default.”</p>
        <div><p>
            Matt Werth<br>
            Label Member<br>
            RVNG Intl.</p></div>
      </div>
<section>
  <p>LEARN:</p>
  
  <div>
      <div><p>Bandcamp's
        back-to-back acquisitions—first by Epic Games in 2022, then by Songtradr
        in 2023—have its community feeling betrayed and concerned about the
        platform they once relied on. Bandcamp's trajectory illustrates a
        depressing reality of the contemporary internet: platforms position
        themselves as artist-friendly alternatives, only to seemingly abandon
        their core values and community.</p><p>It's time for a new model -
        one we collectively own and control.
      </p></div>
      <p>A cooperative, or
        co-op, is a business owned and democratically controlled by its members.
        Subvert is owned by its membership base of artists, labels, supporters,
        and workers.</p>
      <p>We're aiming to
        launch Subvert in 2025. Development is underway, and we're incorporating
        feedback from founding members to shape the platform's features. We'll
        provide regular updates through our newsletter and community forums.
      </p>
      <div><p>Yes. Enter your email to receive a digital version of the zine and
          agree to join our newsletter list.
          </p></div>
      <p>No, there is no
        personal financial liability associated with being a co-op member of
        Subvert. Your liability is limited to your membership contribution.</p>
      <p>We require minimal
        information: your name, a valid email address, and your mailing address.
        This is necessary for maintaining membership records and communications.</p>
      <p>Not at all. Joining
        Subvert doesn't limit your ability to use other platforms.
      </p>
      <p>No.</p>
    </div>
</section>
            </div><div data-footer="">
          <p><a data-brand="" href="https://subvert.fm/" aria-label="Subvert — The Collectively Owned Music Marketplace">
                <img src="https://subvert.fm/content/images/2024/10/fulllogo.png" alt="Subvert — The Collectively Owned Music Marketplace" loading="lazy">
          </a></p><div><p>
            Sign up for email updates:</p>
          </div>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Go Plan9 Memo, Speeding Up Calculations 450% (250 pts)]]></title>
            <link>https://pehringer.info/go_plan9_memo.html</link>
            <guid>41879854</guid>
            <pubDate>Fri, 18 Oct 2024 14:36:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pehringer.info/go_plan9_memo.html">https://pehringer.info/go_plan9_memo.html</a>, See on <a href="https://news.ycombinator.com/item?id=41879854">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <h2><a href="https://pehringer.info/">Jacob_Ray_Pehringer</a></h2>
      

      
<p>I want to take advantage of Go’s concurrency and parallelism for some of my upcoming projects, allowing for some serious number crunching capabilities. But what if I wanted EVEN MORE POWER?!? Enter SIMD, <strong>S</strong>ame <strong>I</strong>nstruction <strong>M</strong>uliple <strong>D</strong>ata [“sim”-“dee”]. Simd instructions allow for parallel number crunching capabilities right down at the hardware level. Many programming languages either have compiler optimizations that use simd or libraries that offer simd support. However, (as far as I can tell) Go’s compiler does not utilizes simd, and I cound not find a general propose simd package that I liked. <strong><em>I just want a package that offers a thin abstraction layer over arithmetic and bitwise simd operations</em></strong>. So like any good programmer I decided to slightly reinvent the wheel and write my very own simd package. How hard could it be?</p>

<p>After doing some preliminary research I discovered that Go uses its own internal assembly language called Plan9. I consider it more of an assembly format than its own language. Plan9 uses target platforms instructions and registers with slight modifications to their names and usage. This means that x86 Plan9 is different then say arm Plan9. Overall, pretty weird stuff. I am not sure why the Go team went down this route. Maybe it simplifies the compiler by having this bespoke assembly format?</p>
<h2 id="plan9-crash-course">Plan9 Crash Course</h2>
<p>I always find learning by example to be the most informative.
So lets Go (haha) over a simple example.</p>
<div><pre><code>example
 ┣━ AddInts_amd64.s
 ┗━ main.go
</code></pre></div>
<p><strong>example/AddInts_amd64.s</strong></p>
<div><pre><code>1  // +build amd64
2
3  TEXT ·AddInts(SB), 4, $0
4      MOVL    left+0(FP), AX
5      MOVL    right+8(FP), BX
6      ADDL    BX, AX
7      MOVL    AX, int+16(FP)
8      RET
</code></pre></div>
<p><strong>LINE 1</strong>: The file contains <code>amd64</code> specific instructions, so we need to include a Go build tag to make sure Go does not try to compile this file for non x86 machines.</p>

<p><strong>LINE 3</strong>: You can think of this line as the functions declaration. <code>TEXT</code> declares that this is a function or text section. <code>·AddInts(SB)</code> specifies our functions name. <code>4</code> represents “NOSPLIT” which we need for some reason. And <code>$0</code> is the size of the function’s stack frame (used for local variables). It’s zero in this case because we can easily fit everything into the registers.</p>

<p><strong>LINE 4 &amp; 5</strong>: Go’s calling convention is to put the function arguments onto the stack. So we <code>MOV</code>e both <code>L</code>ong 32-bit values into the <code>AX</code> and <code>BX</code> registers by dereferencing the frame pointer (<code>FP</code>) with the appropriate offsets. The first argument is stored at offset <code>0</code>. The second argument is stored at offset <code>8</code> (int’s only need 4 bytes but I think Go offsets all arguments by 8 to maintain memory alignment).</p>

<p><strong>LINE 6</strong>: <code>Add</code> the <code>L</code>ong 32-bit value in <code>AX</code> (left) with the <code>L</code>ong 32-bit value in <code>BX</code>. And store the resulting <code>L</code>ong 32-bit value in <code>AX</code>.</p>

<p><strong>LINE 7 &amp; 8</strong>: Go’s calling convention (as far as I can tell) is to put the function return values after its arguments on the stack. So we <code>MOV</code>e the <code>L</code>ong 32-bit values in the <code>AX</code> register onto the stack by dereferencing the frame pointer (<code>FP</code>) with the appropriate offset. Which is 16 in this case.</p>

<p><strong>example/main.go</strong></p>
<div><pre><code>1  package main
2
3  import "fmt"
4
5  func AddInts(left, right) int
6
7  func main() {
8      fmt.Println("1 + 2 = ", AddInts(1, 2))
9  }
</code></pre></div>

<p><strong>LINE 5</strong>: This is the forward functions declaration for our Plan9 function. Since they both share the same name (<code>AddInts</code>) Go will link them together during compilation.</p>

<p><strong>LINE 8</strong>: We can now use our Plan9 function just like any other function.</p>

<h2 id="my-design-plan-9">My Design Plan…. 9</h2>
<p>Now that we are Go assembly experts, let’s get into the details of how I structured the package. <strong><em>My main goal for the package was to offer a thin abstraction layer over arithmetic and bitwise simd operations</em></strong>. Basically, I wanted a set of functions that would allow me to perform simd operations on slices.</p>

<p>Here’s a look at a simplified example of my project structure.</p>
<div><pre><code>example
 ┣━ internal
 ┃   ┗━ addition
 ┃       ┣━ AddInts_amd64.s
 ┃       ┗━ addition_amd64.go
 ┣━ init_amd64.go
 ┗━ example.go
</code></pre></div>
<p>First, we will create a private function pointer with a corresponding public function that wraps around it. By default we will point the private pointer to a software implementation of the function.</p>

<p><strong>example/example.go</strong>:</p>
<div><pre><code> 1  package example
 2
 3  func fallbackAddInts(left, right int) int {
 4     return left + right
 5  }
 6
 7  var addInts func(left, right int) int = fallbackAddInts
 8
 9  func AddInts(left, right int) int {
10      return addInts(left, right)  
11  }
</code></pre></div>
<p>Next, we create an internal package that contains an architecture specific Plan9 implementation of our function.</p>

<p><strong>example/internal/addition/AddInts_amd64.s</strong></p>
<div><pre><code>1  // +build amd64
2
3  TEXT ·AddInts(SB), 4, $0
4      MOVL    left+0(FP), AX
5      MOVL    right+8(FP), BX
6      ADDL    BX, AX
7      MOVL    AX, int+16(FP)
8      RET
</code></pre></div>
<p><strong>example/internal/addition/addition_amd64.go</strong></p>
<div><pre><code>1  // +build amd64
2
3  package addition
4 
5  func AddInts(left, right int) int
</code></pre></div>
<p>Lastly, we will create an init function to configure the private function pointer with our internal packages corresponding Plan9 function.</p>

<p><strong>example/init_amd64.go</strong></p>
<div><pre><code>1  // +build amd64
2
3  package example
4
5  import "example/internal/addition"
6 
7  func init() {
8      addInts = addition.AddInts
9  }
</code></pre></div>
<p><strong>TLDR</strong> The use of a private function pointer combined with architecture specific init functions and packages (using Go build tags) allows our example package to support multiple architectures easily!</p>
<h2 id="some-juicy-simd">Some Juicy Simd</h2>
<p>Now with all that gunk loaded into your mind I will let you decipher some of my x86 simd plan9 functions.</p>

<p><strong><a href="https://github.com/pehringer/simd/blob/main/internal/sse/Supported_amd64.s">simd/internal/sse/Supported_amd64.s</a></strong></p>
<div><pre><code> 1  // +build amd64
 2
 3  // func Supported() bool
 4  TEXT ·Supported(SB), 4, $0
 5    //Check SSE supported.
 6    MOVQ    $1, AX
 7    CPUID
 8    TESTQ   $(1&lt;&lt;25), DX
 9    JZ      sseFalse
10    //sseTrue:
11    MOVB    $1, bool+0(FP)
12    RET
13  sseFalse:
14    MOVB    $0, bool+0(FP)
15    RET
</code></pre></div>

<p><strong><a href="https://github.com/pehringer/simd/blob/main/internal/sse/AddFloat32_amd64.s">simd/internal/sse/AddFloat32_amd64.s</a></strong></p>
<div><pre><code> 1  // +build amd64
 2
 3  // func AddFloat32(left, right, result []float32) int
 4  TEXT ·AddFloat32(SB), 4, $0
 5      //Load slices lengths.
 6      MOVQ    leftLen+8(FP), AX
 7      MOVQ    rightLen+32(FP), BX
 8      MOVQ    resultLen+56(FP), CX
 9      //Get minimum length.
10      CMPQ    AX, CX
11      CMOVQLT AX, CX
12      CMPQ    BX, CX
13      CMOVQLT BX, CX
14      //Load slices data pointers.
15      MOVQ    leftData+0(FP), SI
16      MOVQ    rightData+24(FP), DX
17      MOVQ    resultData+48(FP), DI
18      //Initialize loop index.
19      MOVQ    $0, AX
20  multipleDataLoop:
21      MOVQ    CX, BX
22      SUBQ    AX, BX
23      CMPQ    BX, $4
24      JL      singleDataLoop
25      //Add four float32 values.
26      MOVUPS  (SI)(AX*4), X0
27      MOVUPS  (DX)(AX*4), X1
28      ADDPS   X1, X0
29      MOVUPS  X0, (DI)(AX*4)
30      ADDQ    $4, AX
31      JMP     multipleDataLoop
32  singleDataLoop:
33      CMPQ    AX, CX
34      JGE     returnLength
35      //Add one float32 value.
36      MOVSS   (SI)(AX*4), X0
37      MOVSS   (DX)(AX*4), X1
38      ADDSS   X1, X0
39      MOVSS   X0, (DI)(AX*4)
40      INCQ    AX
41      JMP     singleDataLoop
42  returnLength:
43      MOVQ    CX, int+72(FP)
44      RET
</code></pre></div>
<h2 id="performace-and-the-future">Performace And The Future</h2>
<p>I promise all this gunk is worth it. I made a few charts so you can see the performance difference between a Go software implementation and a Plan9 simd implementation. There is roughly a 200-450% speed up depending on the number of elements. I hope this memo inspires others to use Plan9 and simd in their future projects!</p>

<ul>
  <li><strong>Simd Repo:</strong> <a href="https://github.com/pehringer/simd">github.com/pehringer/simd</a></li>
  <li><strong>Simd Docs:</strong> <a href="https://pkg.go.dev/github.com/pehringer/simd">pkg.go.dev/github.com/pehringer/simd</a></li>
</ul>

<p>Currently, my package only supports 64-bit x86 machines. If there is enough interest, I will throw in some 64-bit ARM support as well!</p>

<p><img src="https://pehringer.info/go_plan9_memo/LargeVectorsFloat32Addition.png" alt="Large Vectors">
<img src="https://pehringer.info/go_plan9_memo/MediumVectorsFloat32Addition.png" alt="Medium Vectors">
<img src="https://pehringer.info/go_plan9_memo/SmallVectorsFloat32Addition.png" alt="Large Vectors"></p>


      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Running an open source app: Usage, costs and community donations (180 pts)]]></title>
            <link>https://spliit.app/blog/spliit-by-the-stats-usage-costs-donations</link>
            <guid>41879845</guid>
            <pubDate>Fri, 18 Oct 2024 14:35:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spliit.app/blog/spliit-by-the-stats-usage-costs-donations">https://spliit.app/blog/spliit-by-the-stats-usage-costs-donations</a>, See on <a href="https://news.ycombinator.com/item?id=41879845">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I created Spliit a few years ago, but the version you can use today is only one year old. As a user <a target="_blank" href="https://github.com/spliit-app/spliit/issues/242">suggested on GitHub</a>, it’s a great opportunity to release some information about the project as a transparency exercise.</p><h2 id="how-many-people-use-it">How many people use it?</h2><p>In the last 12 months, Spliit received 152k visits, starting from ~200/week, and now regularly 5-6k/week. What is more interesting: the bounce rate is 33%, meaning that most people don’t just visit the home page; they <em>act</em> on the website, either by switching groups, creating expenses, or reading a blog post.</p><figure><img alt="Spliit's visitors in the last 12 months (stats by Plausible)" loading="lazy" width="2212" height="1008" decoding="async" data-nimg="1" srcset="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2F1033ec0cdf78b62e0ac95134e78540bd%2Fscreenshot-2024-10-13-at-18.58.36.png&amp;w=3840&amp;q=75 1x" src="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2F1033ec0cdf78b62e0ac95134e78540bd%2Fscreenshot-2024-10-13-at-18.58.36.png&amp;w=3840&amp;q=75"><figcaption>Spliit's visitors in the last 12 months (stats by Plausible)</figcaption></figure><p>Among these 152k visitors, at least 29k used a shared link. Which confirms that the normal use case scenario is for someone to create a group, then share it with the participants. But many visitors also come from Reddit, thanks to the many posts where someone asks for a viable alternative to Splitwise.</p><figure><img alt="Top sources and countries visitors came from in the last 12 months (stats by Plausible)" loading="lazy" width="2220" height="914" decoding="async" data-nimg="1" srcset="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2F2786f808d5817352a196fac35ef5a1cb%2Fscreenshot-2024-10-13-at-19.03.45.png&amp;w=3840&amp;q=75 1x" src="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2F2786f808d5817352a196fac35ef5a1cb%2Fscreenshot-2024-10-13-at-19.03.45.png&amp;w=3840&amp;q=75"><figcaption>Top sources and countries visitors came from in the last 12 months (stats by Plausible)</figcaption></figure><p>When looking where the visitors geographically come from, we can observe that countries where Spliit is the most popular are Germany, United States, and India. (To be honest, I don’t know what makes Spliit more successful in these countries specifically.)</p><h2 id="what-do-people-do-on-spliit">What do people do on Spliit?</h2><p>When using Spliit, there are basically two things you can do: creating groups or adding expenses. As displayed on the home page, users created almost 15k groups at the time I’m writing this post, and a total of 162k expenses.</p><p>Since last January (I didn’t use to track it before), about 300 groups are created each week, and 2000 expenses.</p><figure><img alt="Number of group created in 2024, by week (stats by Plausible)" loading="lazy" width="2168" height="778" decoding="async" data-nimg="1" srcset="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2F850141583a29c85d150da06bdecc3ac9%2Fscreenshot-2024-10-13-at-17.37.22.png&amp;w=3840&amp;q=75 1x" src="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2F850141583a29c85d150da06bdecc3ac9%2Fscreenshot-2024-10-13-at-17.37.22.png&amp;w=3840&amp;q=75"><figcaption>Number of group created in 2024, by week (stats by Plausible)</figcaption></figure><figure><img alt="Number of expenses created in 2024, by week (stats by Plausible)" loading="lazy" width="2168" height="778" decoding="async" data-nimg="1" srcset="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2F22a7b5533b3cf312747eb0e42cc8054c%2Fscreenshot-2024-10-13-at-17.37.09.png&amp;w=3840&amp;q=75 1x" src="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2F22a7b5533b3cf312747eb0e42cc8054c%2Fscreenshot-2024-10-13-at-17.37.09.png&amp;w=3840&amp;q=75"><figcaption>Number of expenses created in 2024, by week (stats by Plausible)</figcaption></figure><p>Here is the repartition of groups by number of expenses. An obvious information we can see here is that at least 4,600 groups were created only to test the application, as no expenses were created then. Then, on the 10k other groups, half have more than five expenses.</p><figure><img alt="Number of groups by how many expenses they contain" loading="lazy" width="1700" height="852" decoding="async" data-nimg="1" srcset="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2F35d0e851507e1f3c5c027900b4283932%2Fscreenshot-2024-10-13-at-21.10.01.png&amp;w=1920&amp;q=75 1x, https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2F35d0e851507e1f3c5c027900b4283932%2Fscreenshot-2024-10-13-at-21.10.01.png&amp;w=3840&amp;q=75 2x" src="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2F35d0e851507e1f3c5c027900b4283932%2Fscreenshot-2024-10-13-at-21.10.01.png&amp;w=3840&amp;q=75"><figcaption>Number of groups by how many expenses they contain</figcaption></figure><h2 id="how-much-does-it-cost">How much does it cost?</h2><p>Let’s talk money! I started many side projects in the past, and for most of them, I didn’t have to spend any cent. But when a project starts having hundreds of users, it’s hard to avoid some costs.</p><p>As you can see in the chart below, Spliit costs me about $115 each month, most of them being for the database hosting. <em>(Note: the amounts in this post are all in US dollars.)</em></p><figure><img alt="Spliit's costs in 2024 (in USD)" loading="lazy" width="1314" height="1074" decoding="async" data-nimg="1" srcset="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2Fb1d0eaf85e63a305d0035d497853319d%2Fscreenshot-2024-10-13-at-18.36.19.png&amp;w=1920&amp;q=75 1x, https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2Fb1d0eaf85e63a305d0035d497853319d%2Fscreenshot-2024-10-13-at-18.36.19.png&amp;w=3840&amp;q=75 2x" src="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2Fb1d0eaf85e63a305d0035d497853319d%2Fscreenshot-2024-10-13-at-18.36.19.png&amp;w=3840&amp;q=75"><figcaption>Spliit's costs in 2024 (in USD)</figcaption></figure><p>The database is hosted on Vercel, and the price is calculated from the total time the database is read in the month (the writings cost almost nothing). As Spliit is used worldwide, it is used almost the full month…</p><p>It is the source of cost I’d really like to work on. There must be database providers that charge less for Spliit’s use case. If you have any good opportunity to tell me about, feel free to <a target="_blank" href="https://scastiel.dev/contact">contact me</a> 😉.</p><h2 id="how-much-does-spliit-earn">How much does Spliit earn?</h2><p>Since the beginning, Spliit’s “business model” is clear to me: it is <a target="_blank" href="https://spliit.app/blog/we-need-an-open-source-alternative-to-splitwise">an open source project</a>, that will stay free to use forever. There might be some premium features in the future, but they will be only for advanced use cases, never to limit the normal usage of the application.</p><p>This makes it hard to find a way to finance the app, which is why currently the only way Spliit makes money is via donations, either by <a target="_blank" href="https://github.com/sponsors/scastiel">GitHub sponsoring</a>, or with direct donations (via a <a target="_blank" href="https://donate.stripe.com/28o3eh96G7hH8k89Ba">Stripe link</a>).</p><figure><img alt="Spliit's donations in 2024 (in USD)" loading="lazy" width="1314" height="1020" decoding="async" data-nimg="1" srcset="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2Fd88429adfb5125b83ff1b7193f435a40%2Fscreenshot-2024-10-13-at-18.36.29.png&amp;w=1920&amp;q=75 1x, https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2Fd88429adfb5125b83ff1b7193f435a40%2Fscreenshot-2024-10-13-at-18.36.29.png&amp;w=3840&amp;q=75 2x" src="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2Fd88429adfb5125b83ff1b7193f435a40%2Fscreenshot-2024-10-13-at-18.36.29.png&amp;w=3840&amp;q=75"><figcaption>Spliit's donations in 2024 (in USD)</figcaption></figure><p>And I’ll be honest: I didn’t expect to receive that much money for Spliit! On the first month, someone donated $70! Since then, some people donate $5, others $20… I am amazed at people’s generosity.</p><p>A short disclaimer: I don’t <em>need</em> donations to make Spliit work. I am lucky enough to have a full-time job that pays me enough to live comfortably and I am happy to give some of the money I earn to the community. (Side note: as a developer, Spliit is also a pretty cool project to show in interview when I look for a job 😉)</p><p>I added the ability to donate for two reasons:</p><ol><li><p>Several users like the project so much they <a target="_blank" href="https://github.com/spliit-app/spliit/issues/40">asked for a way to give back</a>.</p></li><li><p>If I decide someday to leave the project, I would love if the community could keep it alive without financial issues.</p></li></ol><p>You can see that the donations aren’t enough yet to cover all the project costs, but it’s close! 😊</p><h2 id="how-much-time-do-i-spend-working-on-spliit">How much time do I spend working on Spliit?</h2><p>Not as much as I would like! Something like 5 to 10 hours a month. Most of (if not all) the new features you’ve noticed in the app for a few months aren’t made by me, but the community!</p><figure><img alt="The many contributors who make Spliit" loading="lazy" width="1434" height="696" decoding="async" data-nimg="1" srcset="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2Faac295436bd9df2c77a17d26be4555b1%2Fscreenshot-2024-10-14-at-17.35.06.png&amp;w=1920&amp;q=75 1x, https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2Faac295436bd9df2c77a17d26be4555b1%2Fscreenshot-2024-10-14-at-17.35.06.png&amp;w=3840&amp;q=75 2x" src="https://spliit.app/_next/image?url=https%3A%2F%2Fbasehub.earth%2F2277c7b3%2Faac295436bd9df2c77a17d26be4555b1%2Fscreenshot-2024-10-14-at-17.35.06.png&amp;w=3840&amp;q=75"><figcaption>The many contributors who make Spliit</figcaption></figure><p>The time I spend on Spliit, I use it to manage the <a target="_blank" href="https://github.com/spliit-app/spliit/issues">issues</a> (feature suggestions, bug reports, questions), review and test the <a target="_blank" href="https://github.com/spliit-app/spliit/pulls">pull requests</a> (feature implemented by contributors), and deploy the new features so that they’re accessible to everyone.</p><p>So a lot of time is spent on Spliit, but more by <a target="_blank" href="https://github.com/spliit-app/spliit/graphs/contributors">other contributors</a> than by me. 😊</p><hr><p>Spliit is a project I’m really passionate about, and it’s clear that others appreciate it as well! Generous donations help cover most of the hosting costs, though I still contribute some each month—which I’m happy to do 😊.</p><p>I’ll aim to keep these transparency updates coming, giving a regular look at the project's progress and funding.</p><p><b>Do you have any follow-up question about Spliit?</b></p><p>Two places you can ask them:</p><ul><li><p><a target="_blank" href="https://www.reddit.com/r/spliit/comments/1g3spf3/spliit_by_the_stats_usage_costs_donations/">In this Reddit post</a></p></li><li><p><a target="_blank" href="https://github.com/spliit-app/spliit/issues/242">In this GitHub issue</a></p></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You Can Now See the Code That Helped End Apartheid (234 pts)]]></title>
            <link>https://www.wired.com/story/plaintext-you-can-now-see-the-code-that-ended-apartheid/</link>
            <guid>41879072</guid>
            <pubDate>Fri, 18 Oct 2024 13:05:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/plaintext-you-can-now-see-the-code-that-ended-apartheid/">https://www.wired.com/story/plaintext-you-can-now-see-the-code-that-ended-apartheid/</a>, See on <a href="https://news.ycombinator.com/item?id=41879072">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>John Graham-Cumming doesn’t ping me often, but when he does I pay attention. His day job is the CTO of the security giant Cloudflare, but he is also a lay historian of technology, guided by a righteous compass. He might be best known for successfully leading a campaign to force the UK government to <a href="https://www.wired.com/2014/11/the-man-who-made-the-uk-say-im-sorry-for-what-we-did-to-turing/">apologize to the legendary computer scientist</a> Alan Turing for prosecuting him for homosexuality and essentially harassing him to death. So when he DM’d me to say that he had “a hell of a story”—promising “one-time pads! 8-bit computers! Flight attendants smuggling floppies full of random numbers into South Africa!”—I responded.</p><p>The story he shared centers around Tim Jenkin, a former anti-apartheid activist. Jenkin grew up “as a regular racist white South African,” as he described it when I contacted him. But when Jenkin traveled abroad—beyond the filters of the police-state government—he learned about the brutal oppression in his home country, and in 1974 he offered his help to the African National Congress, the banned organization trying to overthrow the white regime. He returned to South Africa and engaged as an activist, distributing pamphlets. He had always had a penchant for gadgetry and was skilled in creating “<a href="https://www.youtube.com/watch?v=wV9TTU5XCt8">leaflet bombs</a>”—devices placed on the street that, when triggered, shot anti-government flyers into the air to be spread by the wind. Unfortunately, he says, in 1978 “we got nicked.” Jenkin was sentenced to 12 years in prison.</p><p>Jenkin has a hacker mind—even as a kid he was fiddling with gadgets, and as a teen he took apart and reassembled his motorcycle. Those skills proved his salvation. Working in the woodshop, he crafted mockups of the large keys that could unlock the prison doors. After months of surreptitious carpentry and testing, he and two colleagues walked out of the prison and eventually got to London.</p><p>It was the early 1980s, and the ANC’s efforts were flagging. The problem was communications. Activists, especially ANC leaders, were under constant surveillance by South African officials. “The decision was taken to get leadership figures back into the country to be closer to the activists, but to do that they still had to be in touch with the outside,” says Jenkin, who was given a mandate to solve the problem. Rudimentary methods—like invisible ink and sending codes by touch-tone dials—weren’t terribly effective. They wanted a communication system that was computerized and unbreakable. The plan was dubbed Operation Vula.</p><p>Working in his small council flat in the Islington neighborhood in London—nicknamed GCHQ, after the top-secret British intelligence agency—Jenkins set about learning to code. It was the early days of PCs, and the equipment by today’s standards was laughably weak. Breakthroughs in public key cryptography had come out a few years earlier, but there was no easily available implementation. And Jenkin was suspicious of prepackaged cryptosystems, fearing they might harbor back doors that would provide governments access.</p><p>Using a <a data-offer-url="https://www.reddit.com/r/retrobattlestations/comments/tzr11x/1987_toshiba_t1000_laptop_exchanged_for_a_500g_of/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.reddit.com/r/retrobattlestations/comments/tzr11x/1987_toshiba_t1000_laptop_exchanged_for_a_500g_of/&quot;}" href="https://www.reddit.com/r/retrobattlestations/comments/tzr11x/1987_toshiba_t1000_laptop_exchanged_for_a_500g_of/" rel="nofollow noopener" target="_blank">Toshiba T1000 PC</a> running an early version of MS-DOS, Jenkin wrote a system using the most secure form of crypto, a one-time pad, which scrambles messages character by character using a shared key that’s as long as the message itself. Using the program, an activist could type a message on a computer and encrypt it with a floppy disk containing the one-time pad of random numbers. The activist could then convert the encrypted text into audio signals and play them to a tape recorder, which would store them. Then, using a public phone, the activist could call, say, ANC leaders in London or Lusaka, Zambia, and play the tape. The recipient would use a modem with an acoustic coupler to capture the sounds, translate them back into digital signals, and decrypt the message with Jenkin’s program.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>One potential problem was getting the materials—the disks and computers—to Africa. The solution, as Graham-Cumming noted, was accomplished by enlisting a sympathetic Dutch flight attendant who routinely flew to Pretoria. “She didn't know what she was taking in, because everything was packaged up; we didn't talk about it at all,” says Jenkin. “She just volunteered to take the stuff, and she took in the laptops and acoustic modems and those sorts of things.”</p><p>Operation Vula gave the ANC the confidence to sneak some leaders back into the country to supervise anti-government actions, coordinating efforts with the top leaders abroad. The Vula coding system even made it possible for the ANC brain trust to establish contact with the incarcerated Nelson Mandela. He received local visitors who came in carrying books that hid the decrypted dispatches—another product of Jenkin’s MacGyver-esque powers. “We smuggled these specially doctored books—innocuous looking books, maybe about flowers or travel—with a secret hidden compartment in the cover,” says Jenkin. “If you knew how to do it, you could extract the message and put another one back in there.”</p><p>Jenkin’s system allowed countless messages to be sent securely, as the ANC reached closer to its goal of defeating apartheid. He is unaware of any instance where the authorities decoded a single communication. When the ANC was ultimately unbanned in 1991, it credited Operation Vula as a key factor in its victory. In April 1994, Nelson Mandela became the president of South Africa.</p><p>You might be thinking that Jenkin’s story is so amazing that someone should make a movie out of it. Someone already has—focusing on the prison break. It’s called <a data-offer-url="https://www.imdb.com/title/tt5797184/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.imdb.com/title/tt5797184/&quot;}" href="https://www.imdb.com/title/tt5797184/" rel="nofollow noopener" target="_blank"><em>Escape From Pretoria</em></a> and stars Daniel Radcliffe as Jenkin. There’s also a <a href="https://www.youtube.com/watch?v=zSOTVfNe54A">short documentary</a> about Jenkin and Operation Vula. But until this year one thing had not been documented—Jenkin’s artisanal cryptosystem.</p><p>That’s where Graham-Cumming enters the picture. Years ago, he’d heard about Operation Vula and found the story fascinating. Earlier this year, he came across a mention of it and wondered—what happened to the code? He felt it should be open-sourced and uploaded to GitHub for all to see and play with. So he contacted Jenkin—and heard a sad story.</p><p>When Jenkin returned to South Africa in 1992, he had been worried about taking his tools with him, as some elements of the operation were still ongoing. “I didn't want to just walk in with all this communication equipment and have this coding wind up in their hands, so I compressed everything into single files, zipped it with passwords, and brought in the disks like that.” He had no problem at the border. Eventually, people felt safe meeting face-to-face and no longer needed Jenkin’s system. “Then life caught up with me,” he says. “I got married, had kids and all that. And one day, I thought, 'Let me have a look at this thing again.’ And I couldn't remember the password.” Over the years, Jenkin and others tried to break the encryption, and failed.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Rather than being disappointed, Graham-Cumming was thrilled. “I’ve got to have a go at this,” he told himself, and asked for the files.</p><p>When Graham-Cumming received them on May 20, he was encouraged that they were compressed and encrypted in the old encrypted PKZIP format. It had a known flaw you could exploit if you knew some part of the original unencrypted message. But you’d have to know where in the zipped file that text is represented. He asked if Jenkin had any unencrypted versions of the code files, and indeed there were a few. But they turned out to be different from what was in the zip file, so they weren’t immediately helpful.</p><p>Graham-Cumming took a few days to think out his next attack. He realized the zip file contained another zip file, and that since all he needed was the right original text for a specific part of the scrambled text, his best chance was using the first file name mentioned in the zip within the zip. “You could predict the very first bit of that zip file using that name,” he says. “And I knew the names he was using. I was like, ‘Oh, I'm gonna try out a name,’ and I wrote a little program to try it.” (This is a much simplified explanation—Graham-Cumming provides more details <a data-offer-url="https://blog.jgc.org/2024/09/cracking-old-zip-file-to-help-open.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://blog.jgc.org/2024/09/cracking-old-zip-file-to-help-open.html&quot;}" href="https://blog.jgc.org/2024/09/cracking-old-zip-file-to-help-open.html" rel="nofollow noopener" target="_blank">in a blog post</a>.)</p><p>On May 29, Graham-Cumming ran the program and stepped away to eat a breakfast of scrambled eggs. Twenty-three minutes later, the program finished. He’d broken the encryption and unzipped the file. The workings of Jenkin’s cryptosystem were exposed. It had been nine days since he first exchanged emails with Jenkin.</p><p>The next step was to actually run the code, which Graham-Cumming did using an emulator of the ancient version of MS-DOS used in the Toshiba T1000. It worked perfectly. Jenkin had feared that a professional coder like Graham-Cumming might find his work hopelessly amateurish, but his reaction was quite the opposite. “I’m pretty amazed, given the limitations he had in terms of knowledge, in terms of hardware, that they built something that was pretty credible, especially for the time,” says Graham-Cumming. Even more impressive: It did a job in the wild.</p><p>Jenkin, who has spent the past few decades in South Africa as a computer programmer and web designer, has now <a data-offer-url="https://github.com/Vulacode" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://github.com/Vulacode&quot;}" href="https://github.com/Vulacode" rel="nofollow noopener" target="_blank">uploaded the code</a> to GitHub and open-sourced it. He plans to unzip and upload some of the messages exchanged in the ’80s that helped bring down apartheid.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>“The code itself is a historical document,” says Graham-Cumming. “It wasn't like, ‘Oh, I'm going to create some theoretical crypto system.’ It was like, ‘I’ve got real activists, real people in danger. I need real communications, and I need to be practical.’” It’s also, as he promised me, a hell of a story.</p><figure></figure><p id="timetravel" tabindex="-1"><h2>Time Travel</h2></p><p>In November 2014, I <a href="https://www.wired.com/2014/11/the-man-who-made-the-uk-say-im-sorry-for-what-we-did-to-turing/">wrote for Backchannel</a> about Graham-Cumming’s campaign to evoke an apology from the UK for its shameful actions against Alan Turing.</p><p><em>On September 10, Graham-Cumming was sick with the flu. He stayed in bed most of the day. Late in the afternoon, he dragged himself to his computer to check his email. Sitting there, in rumpled gym garb, he found the following message from one Kirsty McNeill, a person he did not know. The email signature, as well as the email domain, indicated an association with 10 Downing Street.</em></p><p><em>Graham-Cumming, even in his flu-addled state, knew that this might just be some prank. It wasn’t hard to spoof an address, even from the Prime Minister’s office. He Googled the telephone number in the signature. It was the switchboard to 10 Downing Street. He dialed, asked for Ms. McNeill, and was quickly connected. “We are doing the apology tonight,” she told him. Was it all right if she read him the text? Somewhat stunned, he listened and approved.</em></p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><em>Ten minutes later, his iPhone rang. “Hello, John, this is [Prime Minister] Gordon Brown,” came a familiar voice. “I think you know why I’m calling you.” Over the next few minutes the two chatted. Prime Minister Brown was not a politician of the oozing Tony Blair/Bill Clinton “feel your pain” school. Graham-Cumming admits to some of the same social awkwardness. So the two of them stumbled through a conversation in which Brown confessed that until the petition he had not realized the government’s role in persecuting and prosecuting one of its greatest war heroes. Within a half an hour, 10 Downing released the apology.</em></p><figure></figure><p id="askme" tabindex="-1"><h2>Ask Me One Thing</h2></p><p>Jean-Daniel asks, “Can we train AI to spot and flag AI-generated content automatically? If so can we incorporate that as a default in search engines, phones, and PCs?”</p><p>Thanks for the question, Jean-Daniel. You clearly understand that the messages, videos, and documents that come before us may or may not be generated by algorithms and not humans. There is a natural preference to know if you are on the receiving end of something that came from a living breathing person or a soulless robot. The state-of-the-art large language models do have specific tells. (For one thing, they don’t express themselves creatively as a really smart human can.) It’s reasonable to think that an excellent AI-powered sniffer might be able to root out the fakes. But as AI gets better, identifying its output gets harder. Also, once your AI detector figures out the giveaways, those building the models would probably then share those secrets with their products, and an arms race would ensue.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Even if you did have a great way to tell what was algorithm and what was human, it would probably be a bad idea to block the AI stuff. All the companies making productivity apps are providing tools for people to use AI for communications, writing, illustrating, and even video production. You might not like AI, but if you block emails and documents that use it you’ll probably miss a lot of meetings and important information.</p><p>Instead of labeling which things are made by AI, I think it’s more practical to adopt techniques that affirm that something came from actual people. For instance, the Authors Guild (disclosure: I’m on the the council) has recently <a data-offer-url="https://authorsguild.org/news/ag-partners-with-created-by-humans-to-empower-authors-in-ai-era/#:~:text=NEW%20YORK,%20October%209,%202024,authors'%20rights%20in%20the%20age" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://authorsguild.org/news/ag-partners-with-created-by-humans-to-empower-authors-in-ai-era/#:~:text=NEW%20YORK,%20October%209,%202024,authors'%20rights%20in%20the%20age&quot;}" href="https://authorsguild.org/news/ag-partners-with-created-by-humans-to-empower-authors-in-ai-era/#:~:text=NEW%20YORK,%20October%209,%202024,authors'%20rights%20in%20the%20age" rel="nofollow noopener" target="_blank">started a program</a> where books can earn a sticker that says “Created by Humans.” Systems like this might help AI-haters like you to limit your consumption to the dwindling percentage of content that’s not output from an LLM.</p><p><em>You can submit questions to</em> <em><a href="mailto:mail@wired.com?subject=ASK%20LEVY">mail@wired.com</a>. Write <strong>ASK LEVY</strong> in the subject line.</em></p><figure></figure><h2>End Times Chronicle</h2><p>When the Northern Lights are <a href="https://www.youtube.com/watch?v=oBhPW9Gr77c">seen in the night skies of New Mexico,</a> can we still call them northern?</p><figure></figure><h2>Last but Not Least</h2><p><a href="https://www.wired.com/story/big-interview-marissa-mayer-yahoo-sunshine-ai/">Marissa Mayer explains</a> how she found sunshine after leaving Yahoo.</p><p><a href="https://www.wired.com/story/jd-vance-adviser-posted-drug-use-reddit-cocaine-gas-station-heroin/">A key JD Vance adviser touted his addiction to “gas station heroin</a>” and called his boss “a Trump boot licker.” Even dumber: He didn’t erase his social media posts when he took the job.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>National Security Adviser Jack Sullivan is <a href="https://www.wired.com/story/jake-sullivan-china-tech-profile/">waging a quiet war</a> with China.</p><p>Oh no! It’s the <a href="https://www.wired.com/story/gadget-lab-podcast-662/">last episode of WIRED’S Gadget Lab podcast</a>! But don’t worry, and for heaven’s sake don’t unsubscribe—a new one is coming soon.</p><figure></figure><p><em>Don't miss future subscriber-only editions of this column.</em> <a href="https://subscribe.wired.com/subscribe/splits/wired/WIR_STEVEN_LEVY?source=EDT_WIR_ARTICLE_SUBSCRIBE_LINK_0_STEVEN_LEVY_ZZ"><em><strong>Subscribe to WIRED (50% off for Plaintext readers)</strong></em></a> <em>today.</em></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Passwords’ Generated Strong Password Format (359 pts)]]></title>
            <link>https://rmondello.com/2024/10/07/apple-passwords-generated-strong-password-format/</link>
            <guid>41878290</guid>
            <pubDate>Fri, 18 Oct 2024 11:12:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rmondello.com/2024/10/07/apple-passwords-generated-strong-password-format/">https://rmondello.com/2024/10/07/apple-passwords-generated-strong-password-format/</a>, See on <a href="https://news.ycombinator.com/item?id=41878290">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="entry-2699">
										<blockquote><p>This post briefly summarizes part of a talk I gave in 2018. All information in this post has been accessible on YouTube since then. There is no new information or news in this post.</p></blockquote>
<p>On Mastodon recently, jsveningsson@mastodon.social <a href="https://mastodon.social/@jsveningsson/113238174790723066">asked me</a>:</p>
<blockquote>
<p>Having an annoying argument on Threads about Apple generated passwords. Every iOS Password (like hupvEw-fodne1-qabjyg) seems to be constructed from gibberish two-syllable “words”. Hup-vew, fod-ne and qab-jyg above. Is this all in my head? Am I going crazy? Is the two-syllable thing by design or random?</p>
</blockquote>
<p>This is not in their head, they are not “going crazy”, and the two-syllable thing is by design. Let me explain!</p>
<p>I gave a talk in 2018 called, “<a href="https://www.youtube.com/watch?v=-0dwX2kf6Oc" target="_blank" rel="noopener">How iOS Encourages Healthy Password Practices</a>”, that told the story of this generated password format. Although the talk is a bit dated now, it also covers other topics related to password management that, given that you’re reading this post, you might be interested in.</p>
<p>I explain the thinking behind the generated strong password format <a href="https://www.youtube.com/watch?v=-0dwX2kf6Oc&amp;t=1110s" target="_blank" rel="noopener">at 18 minutes and 30 seconds into the video</a>:</p>
<p><img decoding="async" src="https://rmondello.com/blog/wp-content/uploads/2024/10/generated-password.png"></p>
<blockquote>
<p>To make these passwords easier to type on suboptimal keyboard layouts like my colleague’s game controller, where the mode switching might be difficult, these new passwords are actually dominated by lowercase characters. And to make it easier to short-term have in your head little chunks of it to bring over to the other device, the passwords are based on syllables. That’s consonant, vowel, consonant patterns. With these considerations put together, in our experience, these passwords are actually a lot easier to type on a foreign, weird keyboard, in the rare instances where that might be needed for some of our users.</p>
<p>And we weren’t going to make any changes to our password format unless we can guarantee that it was as strong or stronger than our old format. So if you want to talk in terms of Shannon entropy once again, these new passwords have 71 bits of entropy, up from the 69 from the previous format. And a little tidbit for folks who are trying to match our math — [note that] we actually have a dictionary of offensive terms on device that we filter these generated passwords against and we’ll skip over passwords that we generate that contain those offensive substrings.</p>
<p>…</p>
<p>So these new passwords are 20 characters long. They contain the standard stuff, an uppercase character. They’re dominated by lowercase. We chose a symbol to use, which is hyphen. We put two of them in there, and a single [digit]. We picked this length and the mix of characters to be compatible with a good mix of existing websites.</p>
<p>And a few more details: These aren’t real syllables as defined by any language. We have a certain number of characters we consider to be consonants, which is 19. Another set we consider to be vowels, which is six. And we pick them at random. There are five positions for where the digit can go, which is on either side of the hyphen or at the end of the password.</p>
</blockquote>
<p>So yes, the passwords that Apple Passwords generates do contain gibberish two-syllable “words”. The syllables help them to be memorable briefly, but still not memorizable. I hope this helps!</p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft and OpenAI's close partnership shows signs of fraying (287 pts)]]></title>
            <link>https://www.nytimes.com/2024/10/17/technology/microsoft-openai-partnership-deal.html</link>
            <guid>41878281</guid>
            <pubDate>Fri, 18 Oct 2024 11:11:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/10/17/technology/microsoft-openai-partnership-deal.html">https://www.nytimes.com/2024/10/17/technology/microsoft-openai-partnership-deal.html</a>, See on <a href="https://news.ycombinator.com/item?id=41878281">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/10/17/technology/microsoft-openai-partnership-deal.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Impact of early life adversity on reward processing in young adults (2014) (121 pts)]]></title>
            <link>https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0104185</link>
            <guid>41878167</guid>
            <pubDate>Fri, 18 Oct 2024 10:51:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0104185">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0104185</a>, See on <a href="https://news.ycombinator.com/item?id=41878167">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">

<header>





<ul id="almSignposts">
  <li id="loadingMetrics">
    <p>Loading metrics</p>
  </li>
</ul>







    <div>
  <p id="licenseShort">Open Access</p>
  <p id="peerReviewed">Peer-reviewed</p>

<p id="artType">Research Article</p>


</div>
    <div>



<div>
  

<ul data-js-tooltip="tooltip_container" id="author-list">



<li data-js-tooltip="tooltip_trigger">
       
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="1">
Nathalie E. Holz,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="2">
Arlette F. Buchmann,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="3">
Dorothea Blomeyer,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="4">
Michael M. Plichta,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="5">
Isabella Wolf,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="6">
Sarah Baumeister,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="7">
Andreas Meyer-Lindenberg,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="8">
Tobias Banaschewski,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="9">
Daniel Brandeis <span> </span>,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="10">
Manfred Laucht <span> </span> <span>  </span></a>    
</li>

</ul>


</div>


<div id="floatTitleTop" data-js-floater="title_author" role="presentation">
    <div>
      <h2><!--?xml version="1.0" encoding="UTF-8"?-->Impact of Early Life Adversity on Reward Processing in Young Adults: EEG-fMRI Results from a Prospective Study over 25 Years</h2>

<ul id="floatAuthorList" data-js-floater="floated_authors">

  <li data-float-index="1">Regina Boecker,&nbsp;

  </li>
  <li data-float-index="2">Nathalie E. Holz,&nbsp;

  </li>
  <li data-float-index="3">Arlette F. Buchmann,&nbsp;

  </li>
  <li data-float-index="4">Dorothea Blomeyer,&nbsp;

  </li>
  <li data-float-index="5">Michael M. Plichta,&nbsp;

  </li>
  <li data-float-index="6">Isabella Wolf,&nbsp;

  </li>
  <li data-float-index="7">Sarah Baumeister,&nbsp;

  </li>
  <li data-float-index="8">Andreas Meyer-Lindenberg,&nbsp;

  </li>
  <li data-float-index="9">Tobias Banaschewski,&nbsp;

  </li>
  <li data-float-index="10">Daniel Brandeis

</li></ul>



    </div>
    <div id="titleTopCloser">
      <p><img src="https://journals.plos.org/resource/img/logo-plos.png" alt="PLOS"></p><p>x</p>
    </div>
  </div>

      <ul>
        <li id="artPubDate">Published: August 13, 2014</li>
        <li id="artDoi">
<a href="https://doi.org/10.1371/journal.pone.0104185">https://doi.org/10.1371/journal.pone.0104185</a>
        </li>
        <li></li>
      </ul>

    </div>
  
</header>
  <div>




  


<div id="figure-carousel-section">
  <h2>Figures</h2>

  
</div>





        <div id="artText">
          



<div xmlns:plos="http://plos.org"><h2>Abstract</h2><div><p>Several lines of evidence have implicated the mesolimbic dopamine reward pathway in altered brain function resulting from exposure to early adversity. The present study examined the impact of early life adversity on different stages of neuronal reward processing later in life and their association with a related behavioral phenotype, i.e. attention deficit/hyperactivity disorder (ADHD). 162 healthy young adults (mean age = 24.4 years; 58% female) from an epidemiological cohort study followed since birth participated in a simultaneous EEG-fMRI study using a monetary incentive delay task. Early life adversity according to an early family adversity index (EFA) and lifetime ADHD symptoms were assessed using standardized parent interviews conducted at the offspring's age of 3 months and between 2 and 15 years, respectively. fMRI region-of-interest analysis revealed a significant effect of EFA during reward anticipation in reward-related areas (i.e. ventral striatum, putamen, thalamus), indicating decreased activation when EFA increased. EEG analysis demonstrated a similar effect for the contingent negative variation (CNV), with the CNV decreasing with the level of EFA. In contrast, during reward delivery, activation of the bilateral insula, right pallidum and bilateral putamen increased with EFA. There was a significant association of lifetime ADHD symptoms with lower activation in the left ventral striatum during reward anticipation and higher activation in the right insula during reward delivery. The present findings indicate a differential long-term impact of early life adversity on reward processing, implicating hyporesponsiveness during reward anticipation and hyperresponsiveness when receiving a reward. Moreover, a similar activation pattern related to lifetime ADHD suggests that the impact of early life stress on ADHD may possibly be mediated by a dysfunctional reward pathway.</p>
</div></div>


<div xmlns:plos="http://plos.org"><p><strong>Citation: </strong>Boecker R, Holz NE, Buchmann AF, Blomeyer D, Plichta MM, Wolf I, et al.  (2014) Impact of Early Life Adversity on Reward Processing in Young Adults: EEG-fMRI Results from a Prospective Study over 25 Years. PLoS ONE 9(8):
           e104185.
        
        https://doi.org/10.1371/journal.pone.0104185</p><p><strong>Editor: </strong>Andrea Antal, University Medical Center Goettingen, Germany</p><p><strong>Received: </strong>March 19, 2014; <strong>Accepted: </strong>July 7, 2014; <strong>Published: </strong> August 13, 2014</p><p><strong>Copyright: </strong> © 2014 Boecker et al. This is an open-access article distributed under the terms of the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Data Availability: </strong>The authors confirm that, for approved reasons, some access restrictions apply to the data underlying the findings. Data are available on request from the ethics committee of the University of Heidelberg for researchers who meet the criteria for access to confidential data.</p><p><strong>Funding: </strong>This work was supported by grants from the German Research Foundation (DFG LA 733/1–2) to ML, DB, TB, and AML. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p><p><strong>Competing interests: </strong> Dr. Banaschewski served in an advisory or consultancy role for Hexal Pharma, Lilly, Medice, Novartis, Otsuka, Oxford outcomes, PCM scientific, Shire, and Viforpharma. He received conference attendance support and conference support or received speaker's fee by Lilly, Medice, Novartis, and Shire. He is/has been involved in clinical trials conducted by Lilly, Shire, and Viforpharma. The present work is unrelated to the above grants and relationships. Dr. Meyer-Lindenberg received consultant fees and travel expenses from AstraZeneca, Hoffmann-La Roche, Lundbeck Foundation, speaker's fees from Pfizer Pharma, Lilly Deutschland, Glaxo SmithKline, Janssen Cilag, Bristol-Myers Squibb, Lundbeck, and AstraZeneca. All other authors report no financial relationships with commercial interests. This does not alter the authors' adherence to PLOS ONE policies on sharing data and materials.</p></div>





<div xmlns:plos="http://plos.org" id="section1"><h2>Introduction</h2><p>Accumulating evidence suggests that adversity in early childhood may impair human brain development and mental health later in life <a href="#pone.0104185-Lupien1">[1]</a>–<a href="#pone.0104185-McLaughlin1">[3]</a>. Moreover, clinical studies have highlighted striking effects of early life adversity on the development and persistence of mental disorders such as attention deficit/hyperactivity disorder (ADHD) <a href="#pone.0104185-Pheula1">[4]</a>–<a href="#pone.0104185-Pressman1">[7]</a>. Among the mechanisms mediating the detrimental impact of early adversity on psychopathology and brain development, alterations of the mesolimbic reward pathway have been suggested to play a major role <a href="#pone.0104185-Stark1">[8]</a>–<a href="#pone.0104185-Dillon1">[10]</a>. Several functionally related brain regions have been implicated in the processing of rewards by a large body of functional magnetic resonance imaging (fMRI) findings and have been related to different stages of reward processing <a href="#pone.0104185-Diekhof1">[11]</a>, <a href="#pone.0104185-Haber1">[12]</a>. These findings emphasize a functional dissection of reward processing. While anticipation or “wanting” of a reward addresses the motivational aspect to receive a reward, reward delivery or “liking” has been interpreted as the hedonic impact of a reward producing the feeling of pleasure <a href="#pone.0104185-Berridge1">[13]</a>.</p>
<p>Common regions that are preferentially activated during the anticipation of rewards encompass the ventral striatum (VS), including the nucleus accumbens, ventral caudate nucleus and ventral putamen. Another region suggested to be involved in the delivery of rewards covers the medial orbitofrontal cortex, adjacent parts of the ventromedial prefrontal cortex, medial and dorsal caudate as well as putamen. With regard to ADHD, most studies have demonstrated a reduced activation of the VS during reward anticipation in patients compared to healthy controls <a href="#pone.0104185-Stark1">[8]</a>, <a href="#pone.0104185-Scheres1">[9]</a>, <a href="#pone.0104185-Plichta1">[14]</a>–<a href="#pone.0104185-Furukawa1">[16]</a>, while for the delivery phase, an increased activation of the caudate nucleus was observed <a href="#pone.0104185-Strohle1">[15]</a>–<a href="#pone.0104185-Paloyelis1">[17]</a>. These effects are in line with the dopamine transfer deficit model, which postulates a diminished dopaminergic response shift from the actual reward to the anticipatory stimulus, but a remaining strong response during reward delivery <a href="#pone.0104185-Tripp1">[18]</a>, <a href="#pone.0104185-Tripp2">[19]</a>.</p>
<p>In contrast to neuroimaging research, fewer studies have examined electrophysiological correlates of anticipatory reward processing. One anticipatory event-related potential (ERP) which has been investigated more systematically measuring an electroencephalogram (EEG) is the contingent negative variation (CNV) type activity, a slow negative potential shift before target or feedback stimuli, with a maximum over central sites, elicited by preparation and anticipation paradigms <a href="#pone.0104185-Walter1">[20]</a>, <a href="#pone.0104185-Irwin1">[21]</a>. If feedback immediately follows the response, the CNV reflects reward anticipation along with motor and cognitive preparation or time estimation. If feedback is delayed, reward anticipation is also postponed and is mainly reflected by the feedback or stimulus-preceding negativity (SPN) following the CNV and the motor response <a href="#pone.0104185-Brunia1">[22]</a>. So far, some findings have indicated higher CNV-like activity during reward anticipation <a href="#pone.0104185-Capa1">[23]</a>–<a href="#pone.0104185-Pierson1">[25]</a>, although other studies did not find an effect of reward anticipation on the target-preceding CNV in tests in which feedback was postponed or predictable <a href="#pone.0104185-Broyd1">[26]</a>, <a href="#pone.0104185-Goldstein1">[27]</a>. In turn, several studies have shown a reduced CNV for children with ADHD or adults with a childhood diagnosis of ADHD, acting on a cued continuous performance test (CPT), investigating developmental effects of impaired cognitive brain functions <a href="#pone.0104185-Banaschewski1">[28]</a>–<a href="#pone.0104185-vanLeeuwen1">[31]</a>.</p>
<p>Increasing evidence has implicated the neural circuitry of reward in altered brain function resulting from exposure to early life adversity. At the behavioral level, impaired responding to rewarding stimuli in maltreated individuals was reported <a href="#pone.0104185-Guyer1">[32]</a>. These individuals exhibited faster reactions for risky options in a decision-making task than controls, but lacked the typical increase in response speed with the chance of winning. Further evidence for a reduced sensitivity to reward was provided in an fMRI study <a href="#pone.0104185-Dillon1">[10]</a>. Young adults maltreated during childhood showed a blunted basal ganglia response (left putamen, left globus pallidus) and less positive ratings of reward cues during reward anticipation. Another study underscored these results by demonstrating a decreased activation in the VS to reward-predicting cues in Romanian adoptees who had experienced global early deprivation <a href="#pone.0104185-Mehta1">[33]</a>. In these studies, no effect of early adversity on reward delivery was observed, suggesting that adversity might specifically affect responses to reward-predicting cues. However, a recent study by Kumar et al. <a href="#pone.0104185-Kumar1">[34]</a> investigating the impact of acute stress found differential effects on phases of reward processing, with increased neuronal activation in the caudate and the amygdala during reward anticipation and decreased activation in the caudate and the putamen while receiving a reward. Hence, acute and early chronic stress seem to impact on the anticipatory and delivery stage of reward processing in specific ways, most likely mediated by alterations of the hypothalamus-pituitary-adrenal (HPA) axis <a href="#pone.0104185-UlrichLai1">[35]</a>.</p>
<p>In the present study, the impact of early adversity on reward processing was examined in a large sample of young adults from an epidemiological cohort study followed since birth. Using a monetary incentive delay (MID) task offering either money or verbal feedback, simultaneous EEG-fMRI was recorded in order to detect alterations at different stages of reward processing. Given the fact that the verbal feedback (control condition) of the MID represents a special reward characteristic, such as if receiving a social reward <a href="#pone.0104185-Rademacher1">[36]</a>, <a href="#pone.0104185-Delmonte1">[37]</a>, modality-specific differences in rewarding quality will be examined. The use of EEG and fMRI provides both high spatial and temporal resolution of neuronal alterations during reward processing. Especially, the EEG enables a cue related analysis of time-resolved neurophysiological signatures within the anticipation phase as recently demonstrated by Plichta et al. <a href="#pone.0104185-Plichta2">[38]</a>. First, we hypothesized that activation of reward-related areas induced by the anticipation of a monetary reward, especially the VS, would decrease with the level of early adversity. Second, we expected the same effect for the EEG, i.e. that the CNV, reflecting the motivational signature of reward anticipation, would decrease with increasing adversity. Third, in line with previous research, no adversity-specific alterations of the neuronal response to monetary reward outcome were predicted <a href="#pone.0104185-Dillon1">[10]</a>, <a href="#pone.0104185-Mehta1">[33]</a>. Fourth, we hypothesized that reward-related neuronal activation was related to lifetime ADHD symptoms, showing decreasing neuronal activity during reward anticipation and increasing activation during reward delivery with the level of ADHD <a href="#pone.0104185-Plichta1">[14]</a>, <a href="#pone.0104185-Strohle1">[15]</a>, <a href="#pone.0104185-Paloyelis1">[17]</a>.</p>
</div>

<div xmlns:plos="http://plos.org" id="section2"><h2>Materials and Methods</h2>
<div id="section1"><h3>Ethics statement</h3>
<p>The current assessment was approved by the ethics committee of the University of Heidelberg. After complete description of the study to the participants, written informed consent was obtained. For assessments during childhood (age three months to 15 years) written informed consent was obtained from parents on behalf of the children.</p>
</div>

<div id="section2"><h3>Sample</h3>
<p>This investigation was conducted in the framework of an epidemiological cohort study examining the long-term outcome of early risk factors from birth into adulthood. Detailed information about this study has been published elsewhere <a href="#pone.0104185-Laucht2">[39]</a>, <a href="#pone.0104185-Laucht3">[40]</a>. The initial sample consisted of 384 infants born between 1986 and 1988 of predominantly (&gt;99%) European descent, who were consecutively recruited from two obstetric and six children's hospitals of the Rhine-Neckar Region of Germany. Only firstborn children with singleton births and German-speaking parents were enrolled in the study. Assessments were first conducted at the age of three months and subsequently at regular intervals throughout development, most recently in young adulthood. From the initial sample, 18 (4.7%) were excluded due to severe handicaps and 57 (14.8%) were dropouts, leaving a final sample of 309 for the current assessment. From these, 122 individuals had to be excluded due to usual contraindications for MRI and EEG, current psychopathology or psychotropic medication. This sample of N = 187 individuals participated in a simultaneous EEG-fMRI measurement. Another twenty-five participants were discarded due to movement artifacts (&gt;3 mm) or insufficient EEG quality, leaving a final sample of N = 162 participants (mean age = 24.4 years; 58% female).</p>
</div>

<div id="section3"><h3>Psychological assessment</h3>
<p>Early adversity was assessed using a standardized parent interview according to an ‘enriched’ family adversity index as proposed by Rutter and Quinton <a href="#pone.0104185-Rutter1">[41]</a>. The interview comprised 11 items covering characteristics of the family environment (e.g., Overcrowding: More than 1.0 person per room or size of housing ≤50 m<sup>2</sup>), the parents (e.g., Parental psychiatric disorder: Moderate to severe disorder according to DSM-III-R criteria) and their partnership (e.g., Unwanted pregnancy: An abortion was seriously considered) during a period of one year prior to the assessment (see <a href="#pone.0104185.s004">Table S1</a>). A total early family adversity (EFA) score was formed by counting the number of items present at the 3-month assessment [mean = 1.71±1.87; range: 0–7; Cronbach's alpha = .72]. The EFA index is a prospective and comprehensive measure of family adversity and does not exclusively focus on emotional and sexual abuse or neglect. Empirical evidence has largely confirmed the cumulative risk hypothesis that the likelihood of unfavorable child outcomes increases with the number of adversity factors <a href="#pone.0104185-Evans1">[42]</a>. Furthermore, a series of studies conducted in the context of the Mannheim Study of Children at Risk have provided evidence of the current validity of the family adversity measure <a href="#pone.0104185-Laucht2">[39]</a>, <a href="#pone.0104185-Laucht4">[43]</a>, <a href="#pone.0104185-Laucht5">[44]</a>. The Structured Clinical Interview for DSM-IV (SCID-I German version) <a href="#pone.0104185-Wittchen1">[45]</a> was administered to measure young adults' psychiatric disorders. To examine current drug use, participants completed a substance use inventory <a href="#pone.0104185-Mller1">[46]</a>. Lifetime ADHD symptoms were assessed with the Mannheim Parent Interview (MEI) <a href="#pone.0104185-Esser1">[47]</a> at age 2, 4, 8 and 11 years and with the Schedule for Affective Disorders and Schizophrenia for School- Aged Children (K-SADS-PL) <a href="#pone.0104185-Kaufman1">[48]</a>, German version <a href="#pone.0104185-Delmo1">[49]</a> at age 15 years and sum scores were formed, indexing the severity and persistence of ADHD. The MEI is a highly structured interview adapted from Rutter's parent interviews to include all symptoms related to major DSM-IV diagnoses, and has been shown to be a sensitive measure of child disturbance <a href="#pone.0104185-Laucht3">[40]</a>, <a href="#pone.0104185-Laucht6">[50]</a>, <a href="#pone.0104185-Holz1">[51]</a>. With regard to ADHD, agreement with an independent child psychiatric examination was seen in 100% of cases. The K-SADS is a widely used structured diagnostic interview completed independently by parents and adolescents with established reliability and validity <a href="#pone.0104185-Kaufman2">[52]</a>. Information from different sources was combined by the logical operator OR.</p>
</div>

<div id="section4"><h3>Experimental paradigm</h3>
<p>The fMRI paradigm (<a href="#pone-0104185-g001">Figure 1</a>) was a modified version of the MID task <a href="#pone.0104185-Kirsch1">[53]</a>, <a href="#pone.0104185-Knutson1">[54]</a>, probing reward anticipation and delivery which was adapted to simultaneous EEG-fMRI measurements. Previous results using this paradigm have shown reliable and robust activation of the VS <a href="#pone.0104185-Plichta3">[55]</a>. The task requires a fast button press directly after a flash target following a reward anticipation cue to win a potential reward. Targets followed cues which consistently signaled different types of reward anticipation (unlike in reversal-learning paradigm): either a happy smiley signaling that responding fast enough would yield a monetary feedback (0.50 Euro), or a scrambled smiley indicating only verbal feedback (“Fast reaction!”; usually treated as the control condition). Smileys were used to further minimize learning effects. After every trial, the participants were informed about the current account balance. Boost trials with a monetary reward of 2 Euro instead of 0.50 Euro were given approximately every eighth win trial in order to improve the participants' motivational level. In total, 50 monetary and 50 verbal trials were presented in a pseudo-randomized order. The cue duration (and consequently trial length) was jittered (3-5sec) to cover the whole hemodynamic response function (HRF). The reaction time window (common for both reward conditions) was adaptively tailored to account for inter-individual differences and to yield comparable winnings across participants.</p>
</div>

<div id="section5"><h3>Data acquisition</h3>


<ol>

<li><strong>1. EEG.</strong> The EEG was recorded inside the scanner using an MRI-compatible EEG system with 5 kHz sampling rate, 32 mV input range and 0.1–250 Hz band-pass filters. The signal was measured by equidistantly spaced silver/silver chloride (Ag/AgCl) scalp electrodes using EEG caps with twisted and fixed electrode cables (Easycap, Munich, Germany). The 60-channel EEG montage included most 10–10 system positions (for further information see Plichta et al. <a href="#pone.0104185-Plichta3">[55]</a>). F1 served as recording reference, and F2 as the ground electrode. Four additional electrodes were placed to record the electrooculogram (EOG) and the electrocardiogram (ECG). Via optic fibers, the signal was transmitted from two MRI-compatible amplifiers (Brain Products, Gilching, Germany) outside the scanner room. Electrode impedances were kept below 20 kΩ, except for ECG and EOG electrodes (&lt;30 kΩ) as well as reference and ground (&lt;10 kΩ). The EEG was monitored while scanning using online correction software (RecView Brain Products, Gilching, Germany).</li>

<li><strong>2. fMRI.</strong> The fMRI data was recorded on a 3 Tesla whole-body scanner (Magnetom Trio, Siemens Erlangen, Germany). fMRI data were measured using a T2*-weighted EPI sequence with the following parameters: 400 volumes, 36 slices in ascending order and oriented 20° steeper than AC-PC-plane, 3mm slice thickness, TR/TE = 2210/28 ms, FOV = 220×220 mm, 64×64 matrix, Flip angle = 90°. A T1-weighted anatomical 3D sequence (MPRAGE) was acquired for each subject. The paradigm was created with Presentation software (Neurobehavioural Systems Inc., Albany, USA) and presented via video goggles (Resonance Technology Inc., Northridge, USA). Performance of participants was recorded using response pads (Current Designs, Philadelphia, USA &amp; Presentation software).</li>

</ol></div>

<div id="section6"><h3>Data analysis</h3>


<ol>

<li><strong>1. Behavior.</strong> Reaction times (RT) were averaged across trials per condition (monetary, verbal) and the amount of win trials per condition was summed up. Condition differences were examined by means of paired t-tests, effects of EFA with linear regression analysis. The interaction between EFA and condition with regard to behavioral measures was obtained using repeated measures ANOVAs in SPSS Software package (Version 20, IBM Corp., Armonk, USA).</li>

<li><strong>2. EEG.</strong> EEG data were corrected for MRI gradient <a href="#pone.0104185-Allen1">[56]</a> and cardioballistic artifacts <a href="#pone.0104185-Allen2">[57]</a> using standard template subtraction procedures as implemented in the Brain Vision Analyzer software 2.0 (Brain Products, Gilching, Germany). EEG data were digitally low-pass filtered (70 Hz) and down-sampled to 500 Hz. After exclusion of physical artifacts via raw data inspection, infomax independent component analysis (ICA) <a href="#pone.0104185-Bates1">[6]</a>, <a href="#pone.0104185-Makeig1">[58]</a> was used to remove ocular (blinks, movements) and residual cardioballistic artifacts <a href="#pone.0104185-Debener1">[59]</a> related to gradient modulation. EEG data were re-referenced to the average reference, baseline-corrected to a 500 ms pre-stimulus interval and low-pass filtered with a cut-off of 30 Hz. Segmentation into ERP epochs of 3.5 seconds began 500 ms prior to cue onset. ERP averages for both conditions (monetary and verbal feedback) were calculated for each participant. The CNV at electrode Cz, commonly showing the highest amplitude and therefore the best signal-to-noise <a href="#pone.0104185-Albrecht1">[29]</a>, <a href="#pone.0104185-Plichta2">[38]</a>, was measured as the mean amplitude for the 2–3-second time window following cue onset.</li>

<li><strong>3. fMRI.</strong> The fMRI data was analyzed using statistical parametric mapping (SPM8; Wellcome Trust Centre for Neuroimaging, London, UK). Preprocessing included slice-time correction, realignment (motion correction), spatial normalization into Montreal Neurological Institute space, resampling to 2×2×2 mm and spatial smoothing with an 8-mm full-width at half maximum (FWHM) Gaussian kernel. Spatial normalization was performed by coregistering the realigned mean image to the anatomical image, normalizing the anatomical image to the T1 template and applying these transformation parameters to the time series.</li>

</ol><p>Individual first-level analysis was performed by linear regression analysis. A general linear model with eight regressors of interest (laughing and scrambled smiley, flash, response, monetary and verbal, win and no-win trials, respectively) was designed and convolved with the SPM hemodynamic response function (HRF). Six motion parameters were included in the design matrix and modeled as regressors of no interest. A high-pass filter with a cut-off frequency of 1/128 Hz was used to attenuate low-frequency components. All analyses were corrected for serially correlated errors by fitting a first-order autoregressive process (AR<a href="#pone.0104185-Lupien1">[1]</a>) to the error term.</p>
<p>First-level contrasts were implemented in the second-level group analysis (monetary&gt;verbal cue; win&gt;no-win; monetary win&gt;monetary no-win; verbal win&gt;verbal no-win) with EFA embedded as a covariate of interest and controlling for gender. In a subsequent analysis, results were controlled for subclinical psychopathology as measured using the Young Adult Self-Report (YASR) <a href="#pone.0104185-Achenbach1">[60]</a> at the current assessment. A statistical threshold of p&lt;.001 (uncorrected) was applied in a whole-brain analysis and, a region of interest (ROI) analysis was performed. ROIs were anatomically labeled with WFU PickAtlas <a href="#pone.0104185-Maldjian1">[61]</a>, determining putamen, pallidum, ACC, thalamus, insula, hippocampus and substantia nigra as ROIs of interest <a href="#pone.0104185-Diekhof1">[11]</a>, <a href="#pone.0104185-Haber1">[12]</a>. Results were thresholded at p&lt;.05; family-wise error (FWE) corrected. The VS mask was defined according to Plichta et al. <a href="#pone.0104185-Plichta2">[38]</a> as a fusion of the “caudate head” mask taken from the WFU-PickAtlas (human-atlas TD Brodmann areas+) and the “accumbens” mask from the Harvard–Oxford Subcortical Structural Atlas (implemented in FSLView 3.1.8; see <a href="http://www.cma.mgh.harvard.edu/fsl_atlas.html">http://www.cma.mgh.harvard.edu/fsl_atlas.html</a>; probability threshold was set to 50%). The left and right VS were treated as separate ROIs. The hippocampus mask was divided along the y-axis in an anterior and a posterior part with MARINA <a href="#pone.0104185-Walter2">[62]</a> according to Poppenk et al. <a href="#pone.0104185-Poppenk1">[63]</a>. Mean beta values (across all voxels within ROIs) were imported into SPSS 20 for linear regression analysis. Post-hoc repeated measures ANOVAs, with <em>Phase</em> (Anticipation/Delivery) as the repeated factor and EFA as a covariate of interest were calculated for the left VS, right insula and left putamen ROIs. The same type of ANOVA was also conducted using the input of two different ROIs (left VS for reward anticipation/right insula for reward delivery), in order to directly examine the effects of phase in those regions which showed the highest activation in the separate ROI-analysis from each phase. In addition, a post-hoc factorial analysis provided a direct test of the interaction between condition (monetary vs. verbal) and outcome (win vs. no-win) during the delivery phase.</p>


<ol>

<li><strong>4. Association between early life stress, ADHD symptoms and neuronal activation.</strong> Pearson correlations were computed to establish the relationship of the significant cluster of neuronal activation with EFA and ADHD symptoms. A mediation analysis was conducted to assess whether a possible effect of EFA on ADHD symptoms is mediated by neuronal activation. Mediation was tested following previous work of our group <a href="#pone.0104185-Plichta2">[38]</a> by means of the Sobel test <a href="#pone.0104185-Baron1">[64]</a> accompanied by a bootstrapping method with N = 5000 samples <a href="#pone.0104185-Preacher1">[65]</a> using SPSS 20.</li>

</ol></div>
</div>

<div xmlns:plos="http://plos.org" id="section3"><h2>Results</h2>
<div id="section1"><h3>Behavior</h3>
<p>Behavioral data analysis showed a performance advantage for RT and number of win trials when contrasting the monetary with the verbal condition. Participants responded faster after the presentation of a monetary relative to a verbal cue (monetary: 195.81±26.78 ms; verbal: 225.52±41.96 ms; t<sub>(161)</sub> = −10.56; p&lt;.001) and won a monetary trial more often than a verbal trial (monetary: 28.55±2.97; verbal: 21.52±3.19; t<sub>(161)</sub> = 15.01; p&lt;.001). A significant effect of EFA on RT of monetary trials emerged (F<sub>(1,160)</sub> = 9.22, p = .003), with RT increasing with the level of adversity. A similar effect was observed for the verbal reward condition (F<sub>(1,160)</sub> = 6.45, p = .012), after exclusion of one participant who exceeded the 3-fold interquartile range. No effect of EFA on the number of win trials (monetary: F<sub>(1,160)</sub> = .03; p = .863; verbal: F<sub>(1,160)</sub> = .395; p = .531) and no interaction between EFA and condition on RT (F<sub>(7,154)</sub> = .609, p = .748) or number of win trials (F<sub>(7,153)</sub> = .076, p = .999) was found. All participants gained money (mean: 21.91 €±2.02; range: 16.00–26.50 €). No effect of EFA on payoff emerged (F<sub>(1,160)</sub> = .040; p = .843).</p>
</div>

<div id="section2"><h3>EEG</h3>
<p>A task effect on the contingent negative variation (CNV) revealed that the anticipation of a monetary reward induced a higher CNV than the anticipation of a verbal reward [t<sub>(161)</sub> = −7.18; p&lt;.001; see <a href="#pone.0104185.s003">Figure S3</a>]. Furthermore, an effect of EFA indicated that the CNV (contrasting monetary to verbal reward) decreased when EFA increased (see <a href="#pone-0104185-g002">Figure 2d</a>).</p>
<div data-doi="10.1371/journal.pone.0104185.g002"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosone/article/figure/image?size=medium&amp;id=10.1371/journal.pone.0104185.g002" data-doi="10.1371/journal.pone.0104185" data-uri="10.1371/journal.pone.0104185.g002"><img src="https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0104185.g002" alt="thumbnail" loading="lazy"></a></p></div><p><span>Figure 2. </span></p><p>Left: neuronal activity (p<sub>FWE</sub>&lt;.05; ROI corr.) for the contrast monetary &gt; verbal reward during reward anticipation by early family adversity (EFA) in <strong>a</strong>) left VS, <strong>b</strong>) left putamen and <strong>c</strong>) left thalamus; right: scatterplots of the correlations between the mean BOLD response of the respective regions and EFA; <strong>d</strong>) left: Scalp distribution of CNV difference (monetary &gt; verbal condition; mean difference: 2–3 sec after stimulus presentation) dependent on EFA; right: scatterplot of the correlation between CNV difference at Cz (marked with an asterisk) and EFA [F<sub>(1,160)</sub> = 9.14, p = .003].</p>
<p><a href="https://doi.org/10.1371/journal.pone.0104185.g002">
              https://doi.org/10.1371/journal.pone.0104185.g002</a></p></div></div>

<div id="section3"><h3>fMRI</h3>


<ol>

<li><strong>1. Effects on reward anticipation.</strong> Whole-brain analysis contrasting monetary to verbal reward anticipation revealed higher activation of reward-related regions (VS, supplementary motor area and anterior cingulate cortex, all p<sub>FWE</sub>&lt;.0001; see <a href="#pone.0104185.s001">Figure S1a</a>). Furthermore, a ROI-analysis revealed a significant effect of EFA on the contrast (monetary&gt;verbal) in the VS, putamen, pallidum, left thalamus, left insula, left ACC and right anterior hippocampus (p<sub>FWE</sub>&lt;.05; ROI-corrected; see <a href="#pone-0104185-t001">Table 1</a>), indicating that activation in these regions decreased with the level of EFA. <a href="#pone-0104185-g002">Figure 2a–c</a> shows the respective activation maps together with corresponding extracted mean betas for the predefined ROIs, estimated by linear regression. Results remained constant or even improved (VS left: t = 4.65, p&lt;.001; putamen left: t = 4.36, p = .001; pallidum left: t = 4.37, p&lt;.001), when controlling for subclinical psychopathology at the current assessment.</li>

<li><strong>2. Effects on reward delivery.</strong> Whole-brain analysis of the reward delivery phase induced similarly robust activation of reward-related areas (for the win&gt;no-win contrast; task effect, pooling of monetary and verbal feedback), specifically the putamen, caudate, left inferior frontal gyrus, and right dorsolateral prefrontal cortex (p<sub>FWE</sub>&lt;.0001; see <a href="#pone.0104185.s001">Figure S1b</a>). In addition, a ROI-analysis indicated a significant effect of EFA on the former mentioned contrast, with increasing activation in the bilateral insula, right pallidum and bilateral putamen with the level of EFA (p<sub>FWE</sub>&lt;.05; ROI-corrected; see <a href="#pone-0104185-t002">Table 2</a>). Activation maps and extracted mean betas are displayed in <a href="#pone-0104185-g003">Figure 3</a>. Separate analysis for verbal outcome revealed that with increasing EFA, participants showed higher activation in the bilateral insula, pallidum, substantia nigra and right posterior hippocampus (contrasting verbal win&gt;no-win; p<sub>FWE</sub>&lt;.05; ROI-corrected; see <a href="#pone-0104185-t003">Table 3</a>). Activation maps and extracted mean betas are displayed in <a href="#pone-0104185-g004">Figure 4</a>. In contrast, there was no significant EFA effect on activation in reward-related areas for monetary outcomes (regression analysis for left insula: F<sub>(1,160)</sub> = 1.41, p = .24; right insula: F<sub>(1,160)</sub> = 1.47, p = .23; left putamen: F<sub>(1,160)</sub> = 1.42, p = .24; right putamen: F<sub>(1,160)</sub> = 2.7, p = .10; right pallidum: F<sub>(1,160)</sub> = 2.73, p = .10). Results for both contrasts (win&gt;no-win, verbal win&gt;no-win) were attenuated, when controlling for subclinical psychopathology at the current assessment, but still remained significant (win&gt;no-win: putamen right: t = 3.56, p = .021, pallidum right: t = 3.19, p = .021, insula right: t = 4.17, p = .005; verbal win&gt;no-win: pallidum right: t = 3.43, p = .011, posterior hippocampus right: t = 3.97, p = .003, insula right: t = 4.15, p = .006).The factorial interaction (directly testing increased EFA-related modulation of verbal&gt;monetary wins) revealed overlapping activation with the verbal outcome in a hippocampal area, indicating that the difference between the EFA-related activation for win&gt;no-win trials was higher for verbal than for monetary outcome.</li>

</ol><div data-doi="10.1371/journal.pone.0104185.g003"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosone/article/figure/image?size=medium&amp;id=10.1371/journal.pone.0104185.g003" data-doi="10.1371/journal.pone.0104185" data-uri="10.1371/journal.pone.0104185.g003"><img src="https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0104185.g003" alt="thumbnail" loading="lazy"></a></p></div><p><span>Figure 3. </span></p><p>Left: neuronal activity (p<sub>FWE</sub>&lt;.05; ROI corr.) for the contrast win &gt; no-win trials during reward delivery by EFA in <strong>a</strong>) right insula, <strong>b</strong>) right pallidum and <strong>c</strong>) left putamen; right: scatterplots of the correlations between the mean BOLD response of the respective regions and EFA.</p>
<p><a href="https://doi.org/10.1371/journal.pone.0104185.g003">
              https://doi.org/10.1371/journal.pone.0104185.g003</a></p></div><div data-doi="10.1371/journal.pone.0104185.g004"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosone/article/figure/image?size=medium&amp;id=10.1371/journal.pone.0104185.g004" data-doi="10.1371/journal.pone.0104185" data-uri="10.1371/journal.pone.0104185.g004"><img src="https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0104185.g004" alt="thumbnail" loading="lazy"></a></p></div><p><span>Figure 4. </span></p><p>Left: neuronal activity (p<sub>FWE</sub>&lt;.05; ROI corr.) for the contrast <span>verbal</span> win &gt; no-win trials during reward delivery by EFA in <strong>a</strong>) right pallidum, <strong>b</strong>) right insula, <strong>c</strong>) left substantia nigra and <strong>d</strong>) right posterior hippocampus; right: scatterplots of the correlations between the mean BOLD response of the respective regions and EFA.</p>
<p><a href="https://doi.org/10.1371/journal.pone.0104185.g004">
              https://doi.org/10.1371/journal.pone.0104185.g004</a></p></div><p>Furthermore, a significant negative correlation between neuronal activation during reward anticipation and delivery emerged, indicating that activation of the left VS decreased during anticipation when activation of the right insula [r = −.189; p = .016] and right pallidum [r = −.225; p = .004] increased during delivery (see <a href="#pone.0104185.s002">Figure S2</a>).</p>


<ol>

<li><strong>3. Interaction between <em>Phase</em> (Anticipation/Delivery) and EFA.</strong> A significant interaction between Phase and EFA was obtained in all three ROIs (left VS: F<sub>(1,160)</sub> = 7.36, p = .007; right insula: F<sub>(1,160)</sub> = 10.32, p = .002; left putamen: F<sub>(1,160)</sub> = 10.75, p = .001), when considering the same ROIs for both stages of reward processing. Post-hoc regression analysis revealed that the interaction effect in the left VS was driven by anticipation alone (anticipation: F<sub>(1,160)</sub> = 14.33, p&lt;.001; delivery: F<sub>(1,160)</sub> = 1.31, p = .255), while, in the other ROIs, it was driven by both anticipation and delivery (right insula: anticipation: F<sub>(1,160)</sub> = 4.33, p = .039; delivery: F<sub>(1,160)</sub> = 7.68, p = .006; left putamen: anticipation: F<sub>(1,160)</sub> = 11.58, p = .001; delivery: F<sub>(1,160)</sub> = 4.22, p = .042). Additionally, a significant interaction between Phase and EFA emerged, when considering different ROIs for both reward processing stages (anticipation: left VS; delivery: right insula; F<sub>(1,160)</sub> = 17.72, p&lt;.001).</li>

</ol></div>

<div id="section4"><h3>Correlation of fMRI activation with behavioral measures and CNV</h3>
<p>Negative correlations between RT to monetary trials and neuronal activation (VS, putamen, thalamus) during reward anticipation, contrasting monetary over verbal reward, occurred in several regions (VS: r = −.347; p&lt;.01; putamen: r = −.367; p&lt;.01; thalamus: r = −.345; p = &lt;.01). The analogous correlation emerged between RT and CNV contrasting monetary over verbal reward (r = −.215; p&lt;.01). No significant correlation for RT of verbal trials was found. Furthermore, the number of monetary win trials was positively related to neuronal activation during reward anticipation contrasting monetary over verbal reward (VS: r = −.321; p&lt;.01; putamen: r = −.360; p&lt;.01; thalamus: r = −.328; p = &lt;.01). A similar finding was obtained for the number of verbal win trials (VS: r = −.198; p&lt;.05; putamen: r = −.220; p&lt;.01; thalamus: r = −.187; p = &lt;.05). There was no significant correlation between the number of win trials and CNV. A significant negative correlation of fMRI activation with the CNV during reward anticipation (contrasting monetary to verbal cues for both measures) emerged (VS: r = −.215; p = .006; putamen: r = −.222; p = .004; thalamus: r = −.250; p = .001), showing that the CNV decreased when fMRI activity increased.</p>
</div>

<div id="section5"><h3>Correlation of neuronal activity (fMRI/CNV) and EFA with lifetime ADHD</h3>
<p>There was a significant correlation of fMRI activation contrasting monetary to verbal cues during reward anticipation in the left VS with ADHD symptoms, revealing decreasing activity with the number of ADHD symptoms (r = −.160; p = .042). Moreover, a significant correlation of fMRI activation contrasting win to no-win trials during reward delivery of the right insula with ADHD symptoms was obtained, showing increasing activity with the number of ADHD symptoms (r = .203; p = .01). In contrast, the CNV was found to be unrelated to ADHD symptoms (r = .10; p = .207). Furthermore, EFA correlated significantly with ADHD symptoms (r = .285; p&lt;.001). The mediation analysis of the association between EFA and ADHD symptoms revealed no significant mediation by neuronal activation (left VS: Z = 1.01, p = .31, 95% CI: −.019 □ .084; right insula: Z = 1.52, p = .13, 95% CI: 0 □ .105).</p>
</div>
</div>

<div xmlns:plos="http://plos.org" id="section4"><h2>Discussion</h2><p>The current simultaneous EEG-fMRI study investigated the long-term impact of early life adversity on neuronal alterations of the reward system into adulthood. Using data of an epidemiological cohort study from birth onwards, the results presented above provided evidence of altered reward processing later in life following exposure to early adversity. Specifically, our findings demonstrated a differential impact of adversity on neural responding to distinct phases of reward processing, indicating that the activation of specific reward-related brain areas (VS, putamen, thalamus) decreased with the level of adversity during reward anticipation, while there was an increase in activity of other reward-related areas (pallidum, insula, substantia nigra, right posterior hippocampus) with the level of adversity during reward delivery. The fMRI finding during reward anticipation converged with EEG results showing a negative association between the CNV and adversity, matching the negative correlation of CNV with fMRI activation. Further analysis of the single reward conditions revealed striking effects of early adversity on the processing of verbal reward, which accounted for major parts of the total reward-related activity during the delivery phase.</p>

<div id="section1"><h3>Reward anticipation &amp; early life stress</h3>
<p>The results of the present study replicate recent findings with regard to reward anticipation <a href="#pone.0104185-Dillon1">[10]</a>, <a href="#pone.0104185-Mehta1">[33]</a>, highlighting deficits in the reward processing circuitry associated with exposure to early adversity. While in these studies, small samples of individuals exposed to severe childhood adversity (maltreatment, deprivation) were investigated, the present study extends these findings to a substantially larger number of individuals from an epidemiological cohort study who experienced low to moderate levels of adversity. Moreover, in contrast to these studies, which included maltreated individuals with a current psychiatric disorder, the present analysis focused on currently healthy individuals only. The observed activation of the VS, the putamen and the thalamus is in accordance with previous research, supporting the assumption of a specific reward circuitry affected by stress in early life <a href="#pone.0104185-Diekhof1">[11]</a>, <a href="#pone.0104185-Haber1">[12]</a>. Interestingly, while Dillon et al. <a href="#pone.0104185-Dillon1">[10]</a> reported less activation for maltreated individuals in the left pallidum and putamen, we replicated this effect for the putamen and, additionally, for the thalamus and the VS, the latter serving as the core region of reward processing. The prominent role of the thalamus in the reward circuit has recently been established by the demonstration of a strong direct link to the nucleus accumbens in studies measuring effective connectivity using dynamic causal modeling <a href="#pone.0104185-Plichta2">[38]</a>, <a href="#pone.0104185-Cho1">[66]</a>.</p>
<p>The finding of a negative association between the CNV and early adversity, which to our knowledge is new to the field, provides additional evidence to substantiate the hypothesis of an adversity-driven dysfunctional neuronal reward circuit, suggesting that reward processing is already impaired less than three seconds after cue onset. This result supports the notion of a reward-driven variability of the CNV-like activity preceding uncertain feedback, and is in accordance with previous findings of a relationship between CNV and reward anticipation <a href="#pone.0104185-Capa1">[23]</a>–<a href="#pone.0104185-Pierson1">[25]</a>. Along the same lines, slower RTs with the level of EFA during reward anticipation were found to be linked with blunted neuronal activity and a reduced CNV.</p>
</div>

<div id="section2"><h3>Reward delivery &amp; early life stress</h3>
<p>The finding that neural activity in reward-related areas increased with the level of early adversity during reward delivery is in contrast to previous studies <a href="#pone.0104185-Dillon1">[10]</a>, <a href="#pone.0104185-Mehta1">[33]</a>, which were unable to establish an effect of adversity on the processing of reward outcomes. Several reasons may account for this inconsistency: First, given our substantially larger sample, the present study had a considerably higher power to uncover effects of adversity. Second, a continuous, prospective measure of adversity such as applied in this study may enable the detection of subtle adversity-modulated reward activation in contrast to a case-control design. Third, differences in the MID tasks used to assess reward processing may contribute to the discrepant findings. While in our study, monetary reward was contrasted with verbal reward as a control condition, others included a loss condition or used different intensities of monetary reward as contrasts <a href="#pone.0104185-Dillon1">[10]</a>, <a href="#pone.0104185-Mehta1">[33]</a>. This reduces the number of trials per condition and, in combination with small sample sizes, may lead to reduced effect sizes and less sensitivity to reward outcome.</p>
<p>The activation of the pallidum, insula, hippocampus and substantia nigra demonstrated here is in accordance with the assumption of the phasic transmission of reward information via dopaminergic projections from the midbrain to the VS <a href="#pone.0104185-Schultz1">[67]</a>. The hippocampal area plays a prominent role in regulating the reward circuit <a href="#pone.0104185-Treadway1">[68]</a>, <a href="#pone.0104185-Knutson2">[69]</a> by showing afferent and efferent projections to the VS <a href="#pone.0104185-Friedman1">[70]</a>, regulating emotional, motivational and learning processes <a href="#pone.0104185-Murty1">[71]</a>, <a href="#pone.0104185-Wise1">[72]</a>. The observation of more pronounced EFA-related modulation following verbal reward may indicate a specific sensitivity for social reward appreciation in individuals exposed to high adversity in early childhood, which might be specifically represented by activation of the hippocampus. The VS directly projects to the pallidum, integrating reward information and driving action output <a href="#pone.0104185-Diekhof1">[11]</a>, <a href="#pone.0104185-Frank1">[73]</a>. Moreover, pallidum activation affected by early adversity, as previously found for reward anticipation <a href="#pone.0104185-Dillon1">[10]</a>, suggests a high involvement of the basal ganglia in reward processing, including both the anticipation and outcome phase. A specific activation of the medial orbitofrontal and ventromedial prefrontal cortex during reward delivery as proposed by Diekhof et al. <a href="#pone.0104185-Diekhof1">[11]</a> was not supported by the current study. This might be due to the absence of different magnitudes in monetary rewards in the present MID paradigm, which have been suggested to be processed by frontal activation.</p>
<p>Our finding that the impact of early adversity on reward outcome was only marked in the verbal reward control condition highlights the special reward quality of this condition, and may suggest that individuals exposed to early adversity are particularly prone to social rewards, such as verbal praise. This higher responsiveness to social rewards may result from the experience of poor parenting during childhood in individuals exposed to early family adversity <a href="#pone.0104185-Schmid1">[74]</a>–<a href="#pone.0104185-Burchinal1">[76]</a>, which may have increased the rewarding effect of social stimuli later in life. A retrospective cohort study by Baker and Hoerger <a href="#pone.0104185-Baker1">[77]</a> has implicated dysfunctional parenting, including low parental warmth or high rejection and control, in the development of difficulty delaying gratification. Such findings may support the hypothesis that poor parenting may lead to a reward deficiency syndrome <a href="#pone.0104185-Blum1">[78]</a>, resulting in increased social reward retrieval.</p>
</div>

<div id="section3"><h3>Reward processing &amp; acute vs. early life stress</h3>
<p>In contrast to our results, Kumar et al. <a href="#pone.0104185-Kumar1">[34]</a>, when investigating the impact of acute stress on reward processing, reported an opposite activation pattern in regions (caudate [VS], putamen) partly overlapping with ours, indicating increased neuronal activation during reward anticipation and decreased activation while receiving a reward. These findings underpin the functional differences between the impact of acute stress vs. early life stress on reward processing. Specifically, it has been demonstrated that the stress (HPA axis) and the reward system show considerable overlap on both the structural and the functional level <a href="#pone.0104185-UlrichLai1">[35]</a>. Acute stress leads to an up regulation of HPA axis activity, thereby increasing motivation and approach behaviors, but blunting reward responsiveness <a href="#pone.0104185-Berghorst1">[79]</a>–<a href="#pone.0104185-Bogdan1">[82]</a>. In contrast, early life stress may result in an adaptation of HPA axis activity with first increased cortisol release during stress exposure followed by later hypocortisolism <a href="#pone.0104185-Fries1">[83]</a>, <a href="#pone.0104185-Heim2">[84]</a> which might be linked to decreased dopamine transmission during reward anticipation but increased when receiving a reward.</p>
</div>

<div id="section4"><h3>Reward processing, early life stress &amp; ADHD</h3>
<p>Our findings provide additional insights into the relationship between altered reward processing in individuals exposed to early adversity and mental disorders related to dysfunction of the dopamine reward circuit, such as ADHD. The differential effect of early life stress on both stages of reward processing, characterized by hyporesponsiveness in individuals exposed to high early adversity when anticipating a monetary reward and hyperresponsiveness when receiving a reward, is in line with the literature on dysfunctional reward processing in ADHD <a href="#pone.0104185-Stark1">[8]</a>, <a href="#pone.0104185-Scheres1">[9]</a>, <a href="#pone.0104185-Plichta1">[14]</a>–<a href="#pone.0104185-Paloyelis1">[17]</a>. Accordingly, there was a significant association between fMRI activation in reward-related regions and lifetime ADHD symptoms, showing the same differential effect on anticipatory and delivery phases as observed for early life stress. Given that the latter represents a major risk factor of ADHD <a href="#pone.0104185-Laucht4">[43]</a>, this might suggest an impact of EFA on ADHD via a dysfunctional reward pathway. Here, this pathway could not be confirmed by the mediation analysis. In contrast to fMRI, the CNV proved to be unrelated to ADHD. As the CNV has been identified as a stable ADHD marker for preparation deficits measured by cognitive paradigms such as the CPT <a href="#pone.0104185-Albrecht1">[29]</a>, <a href="#pone.0104185-Doehnert1">[30]</a>, this result might be an effect of paradigm. Although there was a significant impact of EFA on the CNV, reward anticipation as part of an emotional paradigm might be less sensitive to ADHD effects than cognitive paradigms. However, given the significant correlation between CNV and fMRI activation, the different measures alone could not explain this differential effect.</p>
</div>

<div id="section5"><h3>Limitations</h3>
<p>Several limitations have to be considered in the interpretation of our results. First, due to reduced data quality following the button press, it was not possible to analyze EEG feedback components such as the feedback-related negativity <a href="#pone.0104185-Broyd1">[26]</a>. However, it would be most interesting to examine whether EEG feedback components would display a similar pattern of outcome-related EFA effects to that found for fMRI. Second, given the small effect size of EFA and the fact that several characteristics of EFA would not change during the individual's life course, the results cannot be attributed to early life stress alone but probably also reflects stress during later development <a href="#pone.0104185-Heim1">[2]</a>. Third, the present results do not provide evidence of the mechanisms mediating between exposure to EFA and altered reward processing in adulthood. Several mechanisms have been discussed as determining the transduction of environmental influences into changes in brain physiology and morphology. Among these, a major role has been attributed to epigenetic regulation <a href="#pone.0104185-Meaney1">[85]</a>, <a href="#pone.0104185-Roth1">[86]</a>. Hence, the investigation of epigenetic signatures induced by exposure to EFA that persist into adulthood appears to be a promising research perspective. Fourth, current research has highlighted the differential susceptibility of individuals to EFA. Greater insight into the interplay between environmental and genetic factors that affect reward processing may further contribute to a better understanding of the underlying mechanisms. Genes that have been shown to exert remarkable effects on the reward circuit include, among others, the dopamine transporter gene (DAT) <a href="#pone.0104185-Hahn1">[87]</a>, <a href="#pone.0104185-Dreher1">[88]</a> and the catechol-O-methyltransferase gene (COMT) <a href="#pone.0104185-Bates1">[6]</a>, <a href="#pone.0104185-Camara1">[89]</a>, <a href="#pone.0104185-Yacubian1">[90]</a>.</p>
</div>
</div>

<div xmlns:plos="http://plos.org" id="section5"><h2>Conclusion</h2><p>In sum, the present findings provide evidence of a differential long-term impact of early life adversity on two distinct phases of reward processing in adulthood, characterized by hyporesponsiveness during reward anticipation followed by hyperresponsiveness when receiving a reward. Moreover, a similar activation pattern related to lifetime ADHD may suggest that the impact of early life stress on ADHD may possibly be mediated by a dysfunctional reward pathway.</p>
</div>

<div xmlns:plos="http://plos.org" id="section6"><h2>Supporting Information</h2><div><h3><a href="https://journals.plos.org/plosone/article/file?type=supplementary&amp;id=10.1371/journal.pone.0104185.s001">Figure S1. </a></h3><p><strong>Whole-brain task effects a) during the anticipation of monetary vs. verbal rewards, indicating significantly higher activation in the ventral striatum (VS), thalamus, anterior cingulate cortex, supplementary motor area, primary motor area and occipital cortex and b) during reward delivery (win vs. no-win), yielding significantly higher activation in the putamen, caudate, left inferior frontal gyrus, right dorsolateral prefrontal cortex, primary motor area, right medial frontal gyrus and occipital cortex (all p<sub>FWE</sub>&lt;.0001; k≥20).</strong></p>
<p><a href="https://doi.org/10.1371/journal.pone.0104185.s001">https://doi.org/10.1371/journal.pone.0104185.s001</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/plosone/article/file?type=supplementary&amp;id=10.1371/journal.pone.0104185.s002">Figure S2. </a></h3><p><strong>Significant negative correlation of activation in the left VS during reward anticipation with a) right insula activation (pooled reward) [r = −.189; p = .016] and b) right pallidum activation (verbal reward) [r = −.225; p = .004] during reward delivery.</strong></p>
<p><a href="https://doi.org/10.1371/journal.pone.0104185.s002">https://doi.org/10.1371/journal.pone.0104185.s002</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/plosone/article/file?type=supplementary&amp;id=10.1371/journal.pone.0104185.s003">Figure S3. </a></h3><p><strong>Grand average ERPs showing the stronger contingent negative variation (CNV) developing at electrode Cz (marked with an asterisk) after the presentation of monetary (happy smiley, black curve) compared to verbal (scrambled smiley, red curve) reward cues; p&lt;.001 in the analysis time window (blue, 2–3 sec following cue onset and preceding target onset on all trials).</strong></p>
<p><a href="https://doi.org/10.1371/journal.pone.0104185.s003">https://doi.org/10.1371/journal.pone.0104185.s003</a></p><p>(TIF)</p>
</div></div>



<div xmlns:plos="http://plos.org"><h2>Author Contributions</h2><p>Conceived and designed the experiments: ML D. Brandeis TB AML. Performed the experiments: RB NEH SB. Analyzed the data: RB. Contributed reagents/materials/analysis tools: AFB D. Blomeyer MMP IW. Contributed to the writing of the manuscript: RB ML D. Brandeis.</p></div><div xmlns:plos="http://plos.org"><h2>References</h2><ol><li id="ref1"><span>1.
            </span><a name="pone.0104185-Lupien1" id="pone.0104185-Lupien1"></a>Lupien SJ, McEwen BS, Gunnar MR, Heim C (2009) Effects of stress throughout the lifespan on the brain, behaviour and cognition. Nat Rev Neurosci 10: 434–445. <ul><li><a href="#" data-author="Lupien" data-cit="LupienSJ%2C%20McEwenBS%2C%20GunnarMR%2C%20HeimC%20%282009%29%20Effects%20of%20stress%20throughout%20the%20lifespan%20on%20the%20brain%2C%20behaviour%20and%20cognition.%20Nat%20Rev%20Neurosci%2010%3A%20434%E2%80%93445." data-title="Effects%20of%20stress%20throughout%20the%20lifespan%20on%20the%20brain%2C%20behaviour%20and%20cognition" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Effects+of+stress+throughout+the+lifespan+on+the+brain%2C+behaviour+and+cognition+Lupien+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref2"><span>2.
            </span><a name="pone.0104185-Heim1" id="pone.0104185-Heim1"></a>Heim C, Binder EB (2012) Current research trends in early life stress and depression: review of human studies on sensitive periods, gene-environment interactions, and epigenetics. Exp Neurol 233: 102–111. <ul><li><a href="#" data-author="Heim" data-cit="HeimC%2C%20BinderEB%20%282012%29%20Current%20research%20trends%20in%20early%20life%20stress%20and%20depression%3A%20review%20of%20human%20studies%20on%20sensitive%20periods%2C%20gene-environment%20interactions%2C%20and%20epigenetics.%20Exp%20Neurol%20233%3A%20102%E2%80%93111." data-title="Current%20research%20trends%20in%20early%20life%20stress%20and%20depression%3A%20review%20of%20human%20studies%20on%20sensitive%20periods%2C%20gene-environment%20interactions%2C%20and%20epigenetics" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Current+research+trends+in+early+life+stress+and+depression%3A+review+of+human+studies+on+sensitive+periods%2C+gene-environment+interactions%2C+and+epigenetics+Heim+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref3"><span>3.
            </span><a name="pone.0104185-McLaughlin1" id="pone.0104185-McLaughlin1"></a>McLaughlin KA, Greif GJ, Gruber MJ, Sampson NA, Zaslavsky AM, et al. (2012) Childhood adversities and first onset of psychiatric disorders in a national sample of US adolescents. Arch Gen Psychiatry 69: 1151–1160. <ul><li><a href="#" data-author="McLaughlin" data-cit="McLaughlinKA%2C%20GreifGJ%2C%20GruberMJ%2C%20SampsonNA%2C%20ZaslavskyAM%2C%20et%20al.%20%282012%29%20Childhood%20adversities%20and%20first%20onset%20of%20psychiatric%20disorders%20in%20a%20national%20sample%20of%20US%20adolescents.%20Arch%20Gen%20Psychiatry%2069%3A%201151%E2%80%931160." data-title="Childhood%20adversities%20and%20first%20onset%20of%20psychiatric%20disorders%20in%20a%20national%20sample%20of%20US%20adolescents" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Childhood+adversities+and+first+onset+of+psychiatric+disorders+in+a+national+sample+of+US+adolescents+McLaughlin+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref4"><span>4.
            </span><a name="pone.0104185-Pheula1" id="pone.0104185-Pheula1"></a>Pheula GF, Rohde LA, Schmitz M (2011) Are family variables associated with ADHD, inattentive type? A case-control study in schools. Eur Child Adolesc Psychiatry 20: 137–145. <ul><li><a href="#" data-author="Pheula" data-cit="PheulaGF%2C%20RohdeLA%2C%20SchmitzM%20%282011%29%20Are%20family%20variables%20associated%20with%20ADHD%2C%20inattentive%20type%3F%20A%20case-control%20study%20in%20schools.%20Eur%20Child%20Adolesc%20Psychiatry%2020%3A%20137%E2%80%93145." data-title="Are%20family%20variables%20associated%20with%20ADHD%2C%20inattentive%20type%3F%20A%20case-control%20study%20in%20schools" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Are+family+variables+associated+with+ADHD%2C+inattentive+type%3F+A+case-control+study+in+schools+Pheula+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref5"><span>5.
            </span><a name="pone.0104185-Laucht1" id="pone.0104185-Laucht1"></a>Laucht M, Hohm E, Esser G, Schmidt MH, Becker K (2007) Association between ADHD and smoking in adolescence: shared genetic, environmental and psychopathological factors. J Neural Transm 114: 1097–1104. <ul><li><a href="#" data-author="Laucht" data-cit="LauchtM%2C%20HohmE%2C%20EsserG%2C%20SchmidtMH%2C%20BeckerK%20%282007%29%20Association%20between%20ADHD%20and%20smoking%20in%20adolescence%3A%20shared%20genetic%2C%20environmental%20and%20psychopathological%20factors.%20J%20Neural%20Transm%20114%3A%201097%E2%80%931104." data-title="Association%20between%20ADHD%20and%20smoking%20in%20adolescence%3A%20shared%20genetic%2C%20environmental%20and%20psychopathological%20factors" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Association+between+ADHD+and+smoking+in+adolescence%3A+shared+genetic%2C+environmental+and+psychopathological+factors+Laucht+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref6"><span>6.
            </span><a name="pone.0104185-Bates1" id="pone.0104185-Bates1"></a>Bates J, Bayles K, Bennet D, Ridge B, Brown M (1991) Origins of externalizing behavior problems at eight years of age. In: Pepler D, Rubin K, editors. The Development and Treatment of Childhood Aggression. Hillsdale, NJ: Lawrence Erlbaum. <ul></ul></li><li id="ref7"><span>7.
            </span><a name="pone.0104185-Pressman1" id="pone.0104185-Pressman1"></a>Pressman LJ, Loo SK, Carpenter EM, Asarnow JR, Lynn D, et al. (2006) Relationship of family environment and parental psychiatric diagnosis to impairment in ADHD. J Am Acad Child Adolesc Psychiatry 45: 346–354. <ul><li><a href="#" data-author="Pressman" data-cit="PressmanLJ%2C%20LooSK%2C%20CarpenterEM%2C%20AsarnowJR%2C%20LynnD%2C%20et%20al.%20%282006%29%20Relationship%20of%20family%20environment%20and%20parental%20psychiatric%20diagnosis%20to%20impairment%20in%20ADHD.%20J%20Am%20Acad%20Child%20Adolesc%20Psychiatry%2045%3A%20346%E2%80%93354." data-title="Relationship%20of%20family%20environment%20and%20parental%20psychiatric%20diagnosis%20to%20impairment%20in%20ADHD" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Relationship+of+family+environment+and+parental+psychiatric+diagnosis+to+impairment+in+ADHD+Pressman+2006" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref8"><span>8.
            </span><a name="pone.0104185-Stark1" id="pone.0104185-Stark1"></a>Stark R, Bauer E, Merz CJ, Zimmermann M, Reuter M, et al. (2011) ADHD related behaviors are associated with brain activation in the reward system. Neuropsychologia 49: 426–434. <ul><li><a href="#" data-author="Stark" data-cit="StarkR%2C%20BauerE%2C%20MerzCJ%2C%20ZimmermannM%2C%20ReuterM%2C%20et%20al.%20%282011%29%20ADHD%20related%20behaviors%20are%20associated%20with%20brain%20activation%20in%20the%20reward%20system.%20Neuropsychologia%2049%3A%20426%E2%80%93434." data-title="ADHD%20related%20behaviors%20are%20associated%20with%20brain%20activation%20in%20the%20reward%20system" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=ADHD+related+behaviors+are+associated+with+brain+activation+in+the+reward+system+Stark+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref9"><span>9.
            </span><a name="pone.0104185-Scheres1" id="pone.0104185-Scheres1"></a>Scheres A, Lee A, Sumiya M (2008) Temporal reward discounting and ADHD: task and symptom specific effects. J Neural Transm 115: 221–226. <ul><li><a href="#" data-author="Scheres" data-cit="ScheresA%2C%20LeeA%2C%20SumiyaM%20%282008%29%20Temporal%20reward%20discounting%20and%20ADHD%3A%20task%20and%20symptom%20specific%20effects.%20J%20Neural%20Transm%20115%3A%20221%E2%80%93226." data-title="Temporal%20reward%20discounting%20and%20ADHD%3A%20task%20and%20symptom%20specific%20effects" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Temporal+reward+discounting+and+ADHD%3A+task+and+symptom+specific+effects+Scheres+2008" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref10"><span>10.
            </span><a name="pone.0104185-Dillon1" id="pone.0104185-Dillon1"></a>Dillon DG, Holmes AJ, Birk JL, Brooks N, Lyons-Ruth K, et al. (2009) Childhood adversity is associated with left basal ganglia dysfunction during reward anticipation in adulthood. Biol Psychiatry 66: 206–213. <ul><li><a href="#" data-author="Dillon" data-cit="DillonDG%2C%20HolmesAJ%2C%20BirkJL%2C%20BrooksN%2C%20Lyons-RuthK%2C%20et%20al.%20%282009%29%20Childhood%20adversity%20is%20associated%20with%20left%20basal%20ganglia%20dysfunction%20during%20reward%20anticipation%20in%20adulthood.%20Biol%20Psychiatry%2066%3A%20206%E2%80%93213." data-title="Childhood%20adversity%20is%20associated%20with%20left%20basal%20ganglia%20dysfunction%20during%20reward%20anticipation%20in%20adulthood" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Childhood+adversity+is+associated+with+left+basal+ganglia+dysfunction+during+reward+anticipation+in+adulthood+Dillon+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref11"><span>11.
            </span><a name="pone.0104185-Diekhof1" id="pone.0104185-Diekhof1"></a>Diekhof EK, Kaps L, Falkai P, Gruber O (2012) The role of the human ventral striatum and the medial orbitofrontal cortex in the representation of reward magnitude - an activation likelihood estimation meta-analysis of neuroimaging studies of passive reward expectancy and outcome processing. Neuropsychologia 50: 1252–1266. <ul><li><a href="#" data-author="Diekhof" data-cit="DiekhofEK%2C%20KapsL%2C%20FalkaiP%2C%20GruberO%20%282012%29%20The%20role%20of%20the%20human%20ventral%20striatum%20and%20the%20medial%20orbitofrontal%20cortex%20in%20the%20representation%20of%20reward%20magnitude%20-%20an%20activation%20likelihood%20estimation%20meta-analysis%20of%20neuroimaging%20studies%20of%20passive%20reward%20expectancy%20and%20outcome%20processing.%20Neuropsychologia%2050%3A%201252%E2%80%931266." data-title="The%20role%20of%20the%20human%20ventral%20striatum%20and%20the%20medial%20orbitofrontal%20cortex%20in%20the%20representation%20of%20reward%20magnitude%20-%20an%20activation%20likelihood%20estimation%20meta-analysis%20of%20neuroimaging%20studies%20of%20passive%20reward%20expectancy%20and%20outcome%20processing" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+role+of+the+human+ventral+striatum+and+the+medial+orbitofrontal+cortex+in+the+representation+of+reward+magnitude+-+an+activation+likelihood+estimation+meta-analysis+of+neuroimaging+studies+of+passive+reward+expectancy+and+outcome+processing+Diekhof+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref12"><span>12.
            </span><a name="pone.0104185-Haber1" id="pone.0104185-Haber1"></a>Haber SN, Knutson B (2010) The reward circuit: linking primate anatomy and human imaging. Neuropsychopharmacology 35: 4–26. <ul><li><a href="#" data-author="Haber" data-cit="HaberSN%2C%20KnutsonB%20%282010%29%20The%20reward%20circuit%3A%20linking%20primate%20anatomy%20and%20human%20imaging.%20Neuropsychopharmacology%2035%3A%204%E2%80%9326." data-title="The%20reward%20circuit%3A%20linking%20primate%20anatomy%20and%20human%20imaging" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+reward+circuit%3A+linking+primate+anatomy+and+human+imaging+Haber+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref13"><span>13.
            </span><a name="pone.0104185-Berridge1" id="pone.0104185-Berridge1"></a>Berridge KC, Robinson TE, Aldridge JW (2009) Dissecting components of reward: 'liking', 'wanting', and learning. Curr Opin Pharmacol 9: 65–73. <ul><li><a href="#" data-author="Berridge" data-cit="BerridgeKC%2C%20RobinsonTE%2C%20AldridgeJW%20%282009%29%20Dissecting%20components%20of%20reward%3A%20%27liking%27%2C%20%27wanting%27%2C%20and%20learning.%20Curr%20Opin%20Pharmacol%209%3A%2065%E2%80%9373." data-title="Dissecting%20components%20of%20reward%3A%20%26apos%3Bliking%26apos%3B%2C%20%26apos%3Bwanting%26apos%3B%2C%20and%20learning" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Dissecting+components+of+reward%3A+%26apos%3Bliking%26apos%3B%2C+%26apos%3Bwanting%26apos%3B%2C+and+learning+Berridge+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref14"><span>14.
            </span><a name="pone.0104185-Plichta1" id="pone.0104185-Plichta1"></a>Plichta MM, Vasic N, Wolf RC, Lesch KP, Brummer D, et al. (2009) Neural hyporesponsiveness and hyperresponsiveness during immediate and delayed reward processing in adult attention-deficit/hyperactivity disorder. Biol Psychiatry 65: 7–14. <ul><li><a href="#" data-author="Plichta" data-cit="PlichtaMM%2C%20VasicN%2C%20WolfRC%2C%20LeschKP%2C%20BrummerD%2C%20et%20al.%20%282009%29%20Neural%20hyporesponsiveness%20and%20hyperresponsiveness%20during%20immediate%20and%20delayed%20reward%20processing%20in%20adult%20attention-deficit%2Fhyperactivity%20disorder.%20Biol%20Psychiatry%2065%3A%207%E2%80%9314." data-title="Neural%20hyporesponsiveness%20and%20hyperresponsiveness%20during%20immediate%20and%20delayed%20reward%20processing%20in%20adult%20attention-deficit%2Fhyperactivity%20disorder" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Neural+hyporesponsiveness+and+hyperresponsiveness+during+immediate+and+delayed+reward+processing+in+adult+attention-deficit%2Fhyperactivity+disorder+Plichta+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref15"><span>15.
            </span><a name="pone.0104185-Strohle1" id="pone.0104185-Strohle1"></a>Strohle A, Stoy M, Wrase J, Schwarzer S, Schlagenhauf F, et al. (2008) Reward anticipation and outcomes in adult males with attention-deficit/hyperactivity disorder. Neuroimage 39: 966–972. <ul><li><a href="#" data-author="Strohle" data-cit="StrohleA%2C%20StoyM%2C%20WraseJ%2C%20SchwarzerS%2C%20SchlagenhaufF%2C%20et%20al.%20%282008%29%20Reward%20anticipation%20and%20outcomes%20in%20adult%20males%20with%20attention-deficit%2Fhyperactivity%20disorder.%20Neuroimage%2039%3A%20966%E2%80%93972." data-title="Reward%20anticipation%20and%20outcomes%20in%20adult%20males%20with%20attention-deficit%2Fhyperactivity%20disorder" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Reward+anticipation+and+outcomes+in+adult+males+with+attention-deficit%2Fhyperactivity+disorder+Strohle+2008" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref16"><span>16.
            </span><a name="pone.0104185-Furukawa1" id="pone.0104185-Furukawa1"></a>Furukawa E, Bado P, Tripp G, Mattos P, Wickens JR, et al. (2014) Abnormal Striatal BOLD Responses to Reward Anticipation and Reward Delivery in ADHD. PloS one 9: e89129. <ul><li><a href="#" data-author="Furukawa" data-cit="FurukawaE%2C%20BadoP%2C%20TrippG%2C%20MattosP%2C%20WickensJR%2C%20et%20al.%20%282014%29%20Abnormal%20Striatal%20BOLD%20Responses%20to%20Reward%20Anticipation%20and%20Reward%20Delivery%20in%20ADHD.%20PloS%20one%209%3A%20e89129." data-title="Abnormal%20Striatal%20BOLD%20Responses%20to%20Reward%20Anticipation%20and%20Reward%20Delivery%20in%20ADHD" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Abnormal+Striatal+BOLD+Responses+to+Reward+Anticipation+and+Reward+Delivery+in+ADHD+Furukawa+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref17"><span>17.
            </span><a name="pone.0104185-Paloyelis1" id="pone.0104185-Paloyelis1"></a>Paloyelis Y, Mehta MA, Faraone SV, Asherson P, Kuntsi J (2012) Striatal sensitivity during reward processing in attention-deficit/hyperactivity disorder. J Am Acad Child Adolesc Psychiatry 51: 722–732. <ul><li><a href="#" data-author="Paloyelis" data-cit="PaloyelisY%2C%20MehtaMA%2C%20FaraoneSV%2C%20AshersonP%2C%20KuntsiJ%20%282012%29%20Striatal%20sensitivity%20during%20reward%20processing%20in%20attention-deficit%2Fhyperactivity%20disorder.%20J%20Am%20Acad%20Child%20Adolesc%20Psychiatry%2051%3A%20722%E2%80%93732." data-title="Striatal%20sensitivity%20during%20reward%20processing%20in%20attention-deficit%2Fhyperactivity%20disorder" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Striatal+sensitivity+during+reward+processing+in+attention-deficit%2Fhyperactivity+disorder+Paloyelis+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref18"><span>18.
            </span><a name="pone.0104185-Tripp1" id="pone.0104185-Tripp1"></a>Tripp G, Wickens JR (2008) Research review: dopamine transfer deficit: a neurobiological theory of altered reinforcement mechanisms in ADHD. J Child Psychol Psychiatry 49: 691–704. <ul><li><a href="#" data-author="Tripp" data-cit="TrippG%2C%20WickensJR%20%282008%29%20Research%20review%3A%20dopamine%20transfer%20deficit%3A%20a%20neurobiological%20theory%20of%20altered%20reinforcement%20mechanisms%20in%20ADHD.%20J%20Child%20Psychol%20Psychiatry%2049%3A%20691%E2%80%93704." data-title="Research%20review%3A%20dopamine%20transfer%20deficit%3A%20a%20neurobiological%20theory%20of%20altered%20reinforcement%20mechanisms%20in%20ADHD" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Research+review%3A+dopamine+transfer+deficit%3A+a+neurobiological+theory+of+altered+reinforcement+mechanisms+in+ADHD+Tripp+2008" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref19"><span>19.
            </span><a name="pone.0104185-Tripp2" id="pone.0104185-Tripp2"></a>Tripp G, Wickens JR (2009) Neurobiology of ADHD. Neuropharmacology 57: 579–589. <ul><li><a href="#" data-author="Tripp" data-cit="TrippG%2C%20WickensJR%20%282009%29%20Neurobiology%20of%20ADHD.%20Neuropharmacology%2057%3A%20579%E2%80%93589." data-title="Neurobiology%20of%20ADHD" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Neurobiology+of+ADHD+Tripp+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref20"><span>20.
            </span><a name="pone.0104185-Walter1" id="pone.0104185-Walter1"></a>Walter WG, Cooper R, Aldridge VJ, McCallum WC, Winter AL (1964) Contingent negative variation: an electric sign of sensomotor association and expectancy in the human brain. Nature 203: 380–384. <ul><li><a href="#" data-author="Walter" data-cit="WalterWG%2C%20CooperR%2C%20AldridgeVJ%2C%20McCallumWC%2C%20WinterAL%20%281964%29%20Contingent%20negative%20variation%3A%20an%20electric%20sign%20of%20sensomotor%20association%20and%20expectancy%20in%20the%20human%20brain.%20Nature%20203%3A%20380%E2%80%93384." data-title="Contingent%20negative%20variation%3A%20an%20electric%20sign%20of%20sensomotor%20association%20and%20expectancy%20in%20the%20human%20brain" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Contingent+negative+variation%3A+an+electric+sign+of+sensomotor+association+and+expectancy+in+the+human+brain+Walter+1964" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref21"><span>21.
            </span><a name="pone.0104185-Irwin1" id="pone.0104185-Irwin1"></a>Irwin DA, Knott JR, McAdam DW, Rebert CS (1966) Motivational determinants of the "contingent negative variation". Electroencephalogr Clin Neurophysiol 21: 538–543. <ul><li><a href="#" data-author="Irwin" data-cit="IrwinDA%2C%20KnottJR%2C%20McAdamDW%2C%20RebertCS%20%281966%29%20Motivational%20determinants%20of%20the%20%22contingent%20negative%20variation%22.%20Electroencephalogr%20Clin%20Neurophysiol%2021%3A%20538%E2%80%93543." data-title="Motivational%20determinants%20of%20the%20%26quot%3Bcontingent%20negative%20variation%26quot%3B" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Motivational+determinants+of+the+%26quot%3Bcontingent+negative+variation%26quot%3B+Irwin+1966" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref22"><span>22.
            </span><a name="pone.0104185-Brunia1" id="pone.0104185-Brunia1"></a>Brunia CH, Hackley SA, van Boxtel GJ, Kotani Y, Ohgami Y (2011) Waiting to perceive: reward or punishment? Clin Neurophysiol 122: 858–868. <ul><li><a href="#" data-author="Brunia" data-cit="BruniaCH%2C%20HackleySA%2C%20van%20BoxtelGJ%2C%20KotaniY%2C%20OhgamiY%20%282011%29%20Waiting%20to%20perceive%3A%20reward%20or%20punishment%3F%20Clin%20Neurophysiol%20122%3A%20858%E2%80%93868." data-title="Waiting%20to%20perceive%3A%20reward%20or%20punishment%3F" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Waiting+to+perceive%3A+reward+or+punishment%3F+Brunia+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref23"><span>23.
            </span><a name="pone.0104185-Capa1" id="pone.0104185-Capa1"></a>Capa RL, Bouquet CA, Dreher JC, Dufour A (2013) Long-lasting effects of performance-contingent unconscious and conscious reward incentives during cued task-switching. Cortex 49: 1943–1954. <ul><li><a href="#" data-author="Capa" data-cit="CapaRL%2C%20BouquetCA%2C%20DreherJC%2C%20DufourA%20%282013%29%20Long-lasting%20effects%20of%20performance-contingent%20unconscious%20and%20conscious%20reward%20incentives%20during%20cued%20task-switching.%20Cortex%2049%3A%201943%E2%80%931954." data-title="Long-lasting%20effects%20of%20performance-contingent%20unconscious%20and%20conscious%20reward%20incentives%20during%20cued%20task-switching" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Long-lasting+effects+of+performance-contingent+unconscious+and+conscious+reward+incentives+during+cued+task-switching+Capa+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref24"><span>24.
            </span><a name="pone.0104185-Kotani1" id="pone.0104185-Kotani1"></a>Kotani Y, Hiraku S, Suda K, Aihara Y (2001) Effect of positive and negative emotion on stimulus-preceding negativity prior to feedback stimuli. Psychophysiology 38: 873–878. <ul><li><a href="#" data-author="Kotani" data-cit="KotaniY%2C%20HirakuS%2C%20SudaK%2C%20AiharaY%20%282001%29%20Effect%20of%20positive%20and%20negative%20emotion%20on%20stimulus-preceding%20negativity%20prior%20to%20feedback%20stimuli.%20Psychophysiology%2038%3A%20873%E2%80%93878." data-title="Effect%20of%20positive%20and%20negative%20emotion%20on%20stimulus-preceding%20negativity%20prior%20to%20feedback%20stimuli" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Effect+of+positive+and+negative+emotion+on+stimulus-preceding+negativity+prior+to+feedback+stimuli+Kotani+2001" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref25"><span>25.
            </span><a name="pone.0104185-Pierson1" id="pone.0104185-Pierson1"></a>Pierson A, Ragot R, Ripoche A, Lesevre N (1987) Electrophysiological changes elicited by auditory stimuli given a positive or negative value: a study comparing anhedonic with hedonic subjects. Int J Psychophysiol 5: 107–123. <ul><li><a href="#" data-author="Pierson" data-cit="PiersonA%2C%20RagotR%2C%20RipocheA%2C%20LesevreN%20%281987%29%20Electrophysiological%20changes%20elicited%20by%20auditory%20stimuli%20given%20a%20positive%20or%20negative%20value%3A%20a%20study%20comparing%20anhedonic%20with%20hedonic%20subjects.%20Int%20J%20Psychophysiol%205%3A%20107%E2%80%93123." data-title="Electrophysiological%20changes%20elicited%20by%20auditory%20stimuli%20given%20a%20positive%20or%20negative%20value%3A%20a%20study%20comparing%20anhedonic%20with%20hedonic%20subjects" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Electrophysiological+changes+elicited+by+auditory+stimuli+given+a+positive+or+negative+value%3A+a+study+comparing+anhedonic+with+hedonic+subjects+Pierson+1987" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref26"><span>26.
            </span><a name="pone.0104185-Broyd1" id="pone.0104185-Broyd1"></a>Broyd SJ, Richards HJ, Helps SK, Chronaki G, Bamford S, et al. (2012) An electrophysiological monetary incentive delay (e-MID) task: a way to decompose the different components of neural response to positive and negative monetary reinforcement. J Neurosci Methods 209: 40–49. <ul><li><a href="#" data-author="Broyd" data-cit="BroydSJ%2C%20RichardsHJ%2C%20HelpsSK%2C%20ChronakiG%2C%20BamfordS%2C%20et%20al.%20%282012%29%20An%20electrophysiological%20monetary%20incentive%20delay%20%28e-MID%29%20task%3A%20a%20way%20to%20decompose%20the%20different%20components%20of%20neural%20response%20to%20positive%20and%20negative%20monetary%20reinforcement.%20J%20Neurosci%20Methods%20209%3A%2040%E2%80%9349." data-title="An%20electrophysiological%20monetary%20incentive%20delay%20%28e-MID%29%20task%3A%20a%20way%20to%20decompose%20the%20different%20components%20of%20neural%20response%20to%20positive%20and%20negative%20monetary%20reinforcement" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=An+electrophysiological+monetary+incentive+delay+%28e-MID%29+task%3A+a+way+to+decompose+the+different+components+of+neural+response+to+positive+and+negative+monetary+reinforcement+Broyd+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref27"><span>27.
            </span><a name="pone.0104185-Goldstein1" id="pone.0104185-Goldstein1"></a>Goldstein RZ, Cottone LA, Jia Z, Maloney T, Volkow ND, et al. (2006) The effect of graded monetary reward on cognitive event-related potentials and behavior in young healthy adults. Int J Psychophysiol 62: 272–279. <ul><li><a href="#" data-author="Goldstein" data-cit="GoldsteinRZ%2C%20CottoneLA%2C%20JiaZ%2C%20MaloneyT%2C%20VolkowND%2C%20et%20al.%20%282006%29%20The%20effect%20of%20graded%20monetary%20reward%20on%20cognitive%20event-related%20potentials%20and%20behavior%20in%20young%20healthy%20adults.%20Int%20J%20Psychophysiol%2062%3A%20272%E2%80%93279." data-title="The%20effect%20of%20graded%20monetary%20reward%20on%20cognitive%20event-related%20potentials%20and%20behavior%20in%20young%20healthy%20adults" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+effect+of+graded+monetary+reward+on+cognitive+event-related+potentials+and+behavior+in+young+healthy+adults+Goldstein+2006" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref28"><span>28.
            </span><a name="pone.0104185-Banaschewski1" id="pone.0104185-Banaschewski1"></a>Banaschewski T, Brandeis D, Heinrich H, Albrecht B, Brunner E, et al. (2003) Association of ADHD and conduct disorder–brain electrical evidence for the existence of a distinct subtype. J Child Psychol Psychiatry 44: 356–376. <ul><li><a href="#" data-author="Banaschewski" data-cit="BanaschewskiT%2C%20BrandeisD%2C%20HeinrichH%2C%20AlbrechtB%2C%20BrunnerE%2C%20et%20al.%20%282003%29%20Association%20of%20ADHD%20and%20conduct%20disorder%E2%80%93brain%20electrical%20evidence%20for%20the%20existence%20of%20a%20distinct%20subtype.%20J%20Child%20Psychol%20Psychiatry%2044%3A%20356%E2%80%93376." data-title="Association%20of%20ADHD%20and%20conduct%20disorder%E2%80%93brain%20electrical%20evidence%20for%20the%20existence%20of%20a%20distinct%20subtype" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Association+of+ADHD+and+conduct+disorder%E2%80%93brain+electrical+evidence+for+the+existence+of+a+distinct+subtype+Banaschewski+2003" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref29"><span>29.
            </span><a name="pone.0104185-Albrecht1" id="pone.0104185-Albrecht1"></a>Albrecht B, Brandeis D, Uebel H, Valko L, Heinrich H, et al. (2013) Familiality of neural preparation and response control in childhood attention deficit-hyperactivity disorder. Psychol Med 43: 1997–2011. <ul><li><a href="#" data-author="Albrecht" data-cit="AlbrechtB%2C%20BrandeisD%2C%20UebelH%2C%20ValkoL%2C%20HeinrichH%2C%20et%20al.%20%282013%29%20Familiality%20of%20neural%20preparation%20and%20response%20control%20in%20childhood%20attention%20deficit-hyperactivity%20disorder.%20Psychol%20Med%2043%3A%201997%E2%80%932011." data-title="Familiality%20of%20neural%20preparation%20and%20response%20control%20in%20childhood%20attention%20deficit-hyperactivity%20disorder" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Familiality+of+neural+preparation+and+response+control+in+childhood+attention+deficit-hyperactivity+disorder+Albrecht+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref30"><span>30.
            </span><a name="pone.0104185-Doehnert1" id="pone.0104185-Doehnert1"></a>Doehnert M, Brandeis D, Schneider G, Drechsler R, Steinhausen HC (2013) A neurophysiological marker of impaired preparation in an 11-year follow-up study of attention-deficit/hyperactivity disorder (ADHD). J Child Psychol Psychiatry 54: 260–270. <ul><li><a href="#" data-author="Doehnert" data-cit="DoehnertM%2C%20BrandeisD%2C%20SchneiderG%2C%20DrechslerR%2C%20SteinhausenHC%20%282013%29%20A%20neurophysiological%20marker%20of%20impaired%20preparation%20in%20an%2011-year%20follow-up%20study%20of%20attention-deficit%2Fhyperactivity%20disorder%20%28ADHD%29.%20J%20Child%20Psychol%20Psychiatry%2054%3A%20260%E2%80%93270." data-title="A%20neurophysiological%20marker%20of%20impaired%20preparation%20in%20an%2011-year%20follow-up%20study%20of%20attention-deficit%2Fhyperactivity%20disorder%20%28ADHD%29" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=A+neurophysiological+marker+of+impaired+preparation+in+an+11-year+follow-up+study+of+attention-deficit%2Fhyperactivity+disorder+%28ADHD%29+Doehnert+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref31"><span>31.
            </span><a name="pone.0104185-vanLeeuwen1" id="pone.0104185-vanLeeuwen1"></a>van Leeuwen TH, Steinhausen HC, Overtoom CC, Pascual-Marqui RD, van't Klooster B, et al. (1998) The continuous performance test revisited with neuroelectric mapping: impaired orienting in children with attention deficits. Behav Brain Res 94: 97–110. <ul><li><a href="#" data-author="van%20Leeuwen" data-cit="van%20LeeuwenTH%2C%20SteinhausenHC%2C%20OvertoomCC%2C%20Pascual-MarquiRD%2C%20van%27t%20KloosterB%2C%20et%20al.%20%281998%29%20The%20continuous%20performance%20test%20revisited%20with%20neuroelectric%20mapping%3A%20impaired%20orienting%20in%20children%20with%20attention%20deficits.%20Behav%20Brain%20Res%2094%3A%2097%E2%80%93110." data-title="The%20continuous%20performance%20test%20revisited%20with%20neuroelectric%20mapping%3A%20impaired%20orienting%20in%20children%20with%20attention%20deficits" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+continuous+performance+test+revisited+with+neuroelectric+mapping%3A+impaired+orienting+in+children+with+attention+deficits+van+Leeuwen+1998" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref32"><span>32.
            </span><a name="pone.0104185-Guyer1" id="pone.0104185-Guyer1"></a>Guyer AE, Kaufman J, Hodgdon HB, Masten CL, Jazbec S, et al. (2006) Behavioral alterations in reward system function: the role of childhood maltreatment and psychopathology. J Am Acad Child Adolesc Psychiatry 45: 1059–1067. <ul><li><a href="#" data-author="Guyer" data-cit="GuyerAE%2C%20KaufmanJ%2C%20HodgdonHB%2C%20MastenCL%2C%20JazbecS%2C%20et%20al.%20%282006%29%20Behavioral%20alterations%20in%20reward%20system%20function%3A%20the%20role%20of%20childhood%20maltreatment%20and%20psychopathology.%20J%20Am%20Acad%20Child%20Adolesc%20Psychiatry%2045%3A%201059%E2%80%931067." data-title="Behavioral%20alterations%20in%20reward%20system%20function%3A%20the%20role%20of%20childhood%20maltreatment%20and%20psychopathology" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Behavioral+alterations+in+reward+system+function%3A+the+role+of+childhood+maltreatment+and+psychopathology+Guyer+2006" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref33"><span>33.
            </span><a name="pone.0104185-Mehta1" id="pone.0104185-Mehta1"></a>Mehta MA, Gore-Langton E, Golembo N, Colvert E, Williams SC, et al. (2010) Hyporesponsive reward anticipation in the basal ganglia following severe institutional deprivation early in life. J Cogn Neurosci 22: 2316–2325. <ul><li><a href="#" data-author="Mehta" data-cit="MehtaMA%2C%20Gore-LangtonE%2C%20GolemboN%2C%20ColvertE%2C%20WilliamsSC%2C%20et%20al.%20%282010%29%20Hyporesponsive%20reward%20anticipation%20in%20the%20basal%20ganglia%20following%20severe%20institutional%20deprivation%20early%20in%20life.%20J%20Cogn%20Neurosci%2022%3A%202316%E2%80%932325." data-title="Hyporesponsive%20reward%20anticipation%20in%20the%20basal%20ganglia%20following%20severe%20institutional%20deprivation%20early%20in%20life" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Hyporesponsive+reward+anticipation+in+the+basal+ganglia+following+severe+institutional+deprivation+early+in+life+Mehta+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref34"><span>34.
            </span><a name="pone.0104185-Kumar1" id="pone.0104185-Kumar1"></a>Kumar P, Berghorst LH, Nickerson LD, Dutra SJ, Goer FK, et al. (2014) Differential effects of acute stress on anticipatory and consummatory phases of reward processing. Neuroscience 266C: 1–12. <ul><li><a href="#" data-author="Kumar" data-cit="KumarP%2C%20BerghorstLH%2C%20NickersonLD%2C%20DutraSJ%2C%20GoerFK%2C%20et%20al.%20%282014%29%20Differential%20effects%20of%20acute%20stress%20on%20anticipatory%20and%20consummatory%20phases%20of%20reward%20processing.%20Neuroscience%20266C%3A%201%E2%80%9312." data-title="Differential%20effects%20of%20acute%20stress%20on%20anticipatory%20and%20consummatory%20phases%20of%20reward%20processing" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Differential+effects+of+acute+stress+on+anticipatory+and+consummatory+phases+of+reward+processing+Kumar+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref35"><span>35.
            </span><a name="pone.0104185-UlrichLai1" id="pone.0104185-UlrichLai1"></a>Ulrich-Lai YM, Herman JP (2009) Neural regulation of endocrine and autonomic stress responses. Nat Rev Neurosci 10: 397–409. <ul><li><a href="#" data-author="Ulrich-Lai" data-cit="Ulrich-LaiYM%2C%20HermanJP%20%282009%29%20Neural%20regulation%20of%20endocrine%20and%20autonomic%20stress%20responses.%20Nat%20Rev%20Neurosci%2010%3A%20397%E2%80%93409." data-title="Neural%20regulation%20of%20endocrine%20and%20autonomic%20stress%20responses" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Neural+regulation+of+endocrine+and+autonomic+stress+responses+Ulrich-Lai+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref36"><span>36.
            </span><a name="pone.0104185-Rademacher1" id="pone.0104185-Rademacher1"></a>Rademacher L, Krach S, Kohls G, Irmak A, Grunder G, et al. (2010) Dissociation of neural networks for anticipation and consumption of monetary and social rewards. Neuroimage 49: 3276–3285. <ul><li><a href="#" data-author="Rademacher" data-cit="RademacherL%2C%20KrachS%2C%20KohlsG%2C%20IrmakA%2C%20GrunderG%2C%20et%20al.%20%282010%29%20Dissociation%20of%20neural%20networks%20for%20anticipation%20and%20consumption%20of%20monetary%20and%20social%20rewards.%20Neuroimage%2049%3A%203276%E2%80%933285." data-title="Dissociation%20of%20neural%20networks%20for%20anticipation%20and%20consumption%20of%20monetary%20and%20social%20rewards" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Dissociation+of+neural+networks+for+anticipation+and+consumption+of+monetary+and+social+rewards+Rademacher+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref37"><span>37.
            </span><a name="pone.0104185-Delmonte1" id="pone.0104185-Delmonte1"></a>Delmonte S, Balsters JH, McGrath J, Fitzgerald J, Brennan S, et al. (2012) Social and monetary reward processing in autism spectrum disorders. Mol Autism 3: 7. <ul><li><a href="#" data-author="Delmonte" data-cit="DelmonteS%2C%20BalstersJH%2C%20McGrathJ%2C%20FitzgeraldJ%2C%20BrennanS%2C%20et%20al.%20%282012%29%20Social%20and%20monetary%20reward%20processing%20in%20autism%20spectrum%20disorders.%20Mol%20Autism%203%3A%207." data-title="Social%20and%20monetary%20reward%20processing%20in%20autism%20spectrum%20disorders" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Social+and+monetary+reward+processing+in+autism+spectrum+disorders+Delmonte+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref38"><span>38.
            </span><a name="pone.0104185-Plichta2" id="pone.0104185-Plichta2"></a>Plichta MM, Wolf I, Hohmann S, Baumeister S, Boecker R, et al. (2013) Simultaneous EEG and fMRI Reveals a Causally Connected Subcortical-Cortical Network during Reward Anticipation. J Neurosci 33: 14526–14533. <ul><li><a href="#" data-author="Plichta" data-cit="PlichtaMM%2C%20WolfI%2C%20HohmannS%2C%20BaumeisterS%2C%20BoeckerR%2C%20et%20al.%20%282013%29%20Simultaneous%20EEG%20and%20fMRI%20Reveals%20a%20Causally%20Connected%20Subcortical-Cortical%20Network%20during%20Reward%20Anticipation.%20J%20Neurosci%2033%3A%2014526%E2%80%9314533." data-title="Simultaneous%20EEG%20and%20fMRI%20Reveals%20a%20Causally%20Connected%20Subcortical-Cortical%20Network%20during%20Reward%20Anticipation" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Simultaneous+EEG+and+fMRI+Reveals+a+Causally+Connected+Subcortical-Cortical+Network+during+Reward+Anticipation+Plichta+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref39"><span>39.
            </span><a name="pone.0104185-Laucht2" id="pone.0104185-Laucht2"></a>Laucht M, Esser G, Baving L, Gerhold M, Hoesch I, et al. (2000) Behavioral sequelae of perinatal insults and early family adversity at 8 years of age. J Am Acad Child and Adolesc Psychiatry 39: 1229–1237. <ul><li><a href="#" data-author="Laucht" data-cit="LauchtM%2C%20EsserG%2C%20BavingL%2C%20GerholdM%2C%20HoeschI%2C%20et%20al.%20%282000%29%20Behavioral%20sequelae%20of%20perinatal%20insults%20and%20early%20family%20adversity%20at%208%20years%20of%20age.%20J%20Am%20Acad%20Child%20and%20Adolesc%20Psychiatry%2039%3A%201229%E2%80%931237." data-title="Behavioral%20sequelae%20of%20perinatal%20insults%20and%20early%20family%20adversity%20at%208%20years%20of%20age" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Behavioral+sequelae+of+perinatal+insults+and+early+family+adversity+at+8+years+of+age+Laucht+2000" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref40"><span>40.
            </span><a name="pone.0104185-Laucht3" id="pone.0104185-Laucht3"></a>Laucht M, Esser G, Schmidt MH (1997) Developmental outcome of infants born with biological and psychosocial risks. J Child Psychol Psychiatry 38: 843–853. <ul><li><a href="#" data-author="Laucht" data-cit="LauchtM%2C%20EsserG%2C%20SchmidtMH%20%281997%29%20Developmental%20outcome%20of%20infants%20born%20with%20biological%20and%20psychosocial%20risks.%20J%20Child%20Psychol%20Psychiatry%2038%3A%20843%E2%80%93853." data-title="Developmental%20outcome%20of%20infants%20born%20with%20biological%20and%20psychosocial%20risks" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Developmental+outcome+of+infants+born+with+biological+and+psychosocial+risks+Laucht+1997" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref41"><span>41.
            </span><a name="pone.0104185-Rutter1" id="pone.0104185-Rutter1"></a>Rutter M, Quinton D (1977) Psychiatric disorder - ecological factors and concepts of causation. In: McGurk M, editor. Ecological factors in human development. Amsterdam: North Holland. pp.173–187. <ul></ul></li><li id="ref42"><span>42.
            </span><a name="pone.0104185-Evans1" id="pone.0104185-Evans1"></a>Evans GW, Li D, Whipple SS (2013) Cumulative risk and child development. Psychol Bull 139: 1342–1396. <ul><li><a href="#" data-author="Evans" data-cit="EvansGW%2C%20LiD%2C%20WhippleSS%20%282013%29%20Cumulative%20risk%20and%20child%20development.%20Psychol%20Bull%20139%3A%201342%E2%80%931396." data-title="Cumulative%20risk%20and%20child%20development" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Cumulative+risk+and+child+development+Evans+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref43"><span>43.
            </span><a name="pone.0104185-Laucht4" id="pone.0104185-Laucht4"></a>Laucht M, Skowronek MH, Becker K, Schmidt MH, Esser G, et al. (2007) Interacting effects of the dopamine transporter gene and psychosocial adversity on attention-deficit/hyperactivity disorder symptoms among 15-year-olds from a high-risk community sample. Arch Gen Psychiatry 64: 585–590. <ul><li><a href="#" data-author="Laucht" data-cit="LauchtM%2C%20SkowronekMH%2C%20BeckerK%2C%20SchmidtMH%2C%20EsserG%2C%20et%20al.%20%282007%29%20Interacting%20effects%20of%20the%20dopamine%20transporter%20gene%20and%20psychosocial%20adversity%20on%20attention-deficit%2Fhyperactivity%20disorder%20symptoms%20among%2015-year-olds%20from%20a%20high-risk%20community%20sample.%20Arch%20Gen%20Psychiatry%2064%3A%20585%E2%80%93590." data-title="Interacting%20effects%20of%20the%20dopamine%20transporter%20gene%20and%20psychosocial%20adversity%20on%20attention-deficit%2Fhyperactivity%20disorder%20symptoms%20among%2015-year-olds%20from%20a%20high-risk%20community%20sample" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Interacting+effects+of+the+dopamine+transporter+gene+and+psychosocial+adversity+on+attention-deficit%2Fhyperactivity+disorder+symptoms+among+15-year-olds+from+a+high-risk+community+sample+Laucht+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref44"><span>44.
            </span><a name="pone.0104185-Laucht5" id="pone.0104185-Laucht5"></a>Laucht M, Treutlein J, Blomeyer D, Buchmann AF, Schmid B, et al. (2009) Interaction between the 5-HTTLPR serotonin transporter polymorphism and environmental adversity for mood and anxiety psychopathology: evidence from a high-risk community sample of young adults. Int J Neuropsychopharmacol 12: 737–747. <ul><li><a href="#" data-author="Laucht" data-cit="LauchtM%2C%20TreutleinJ%2C%20BlomeyerD%2C%20BuchmannAF%2C%20SchmidB%2C%20et%20al.%20%282009%29%20Interaction%20between%20the%205-HTTLPR%20serotonin%20transporter%20polymorphism%20and%20environmental%20adversity%20for%20mood%20and%20anxiety%20psychopathology%3A%20evidence%20from%20a%20high-risk%20community%20sample%20of%20young%20adults.%20Int%20J%20Neuropsychopharmacol%2012%3A%20737%E2%80%93747." data-title="Interaction%20between%20the%205-HTTLPR%20serotonin%20transporter%20polymorphism%20and%20environmental%20adversity%20for%20mood%20and%20anxiety%20psychopathology%3A%20evidence%20from%20a%20high-risk%20community%20sample%20of%20young%20adults" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Interaction+between+the+5-HTTLPR+serotonin+transporter+polymorphism+and+environmental+adversity+for+mood+and+anxiety+psychopathology%3A+evidence+from+a+high-risk+community+sample+of+young+adults+Laucht+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref45"><span>45.
            </span><a name="pone.0104185-Wittchen1" id="pone.0104185-Wittchen1"></a>Wittchen HU, Zaudig M, Fydrich T (1997) [Structured clinical interview for DSM-IV Axis I and II - SCID]. Göttingen: Hogrefe. <ul></ul></li><li id="ref46"><span>46.
            </span><a name="pone.0104185-Mller1" id="pone.0104185-Mller1"></a>Müller R, Abbet JP (1991) Changing trends in the consumption of legal and illegal drugs by 11-16-year-old adolescent pupils. Findings from a study conducted under the auspices of the WHO Europe. Lausanne: Swiss Professional Service for Alcohol Problems. <ul></ul></li><li id="ref47"><span>47.
            </span><a name="pone.0104185-Esser1" id="pone.0104185-Esser1"></a>Esser G, Blanz B, Geisel B, Laucht M (1989) Mannheim Parent Interview - Structured interview for child psychiatric disorders. Weinheim: Beltz. <ul></ul></li><li id="ref48"><span>48.
            </span><a name="pone.0104185-Kaufman1" id="pone.0104185-Kaufman1"></a>Kaufman J, Birmaher B, Brent D, Rao U, Ryan N (1996) Kiddie-Sads-Present and Lifetime Version (K-SADS_PL) [Internet]. Available: <a href="http://www.psychiatry.pitt.edu/node/8233">http://www.psychiatry.pitt.edu/node/8233</a>. <ul></ul></li><li id="ref49"><span>49.
            </span><a name="pone.0104185-Delmo1" id="pone.0104185-Delmo1"></a>Delmo C, Weiffenbach O, Gabriel M, Poustka F (2000) Kiddie-SADS-Present and Lifetime version (K-SADS-PL) 3. Auflage der deutschen Forschungsversion. Frankfurt. Available: <a href="http://www.adhs-essen.com/PDF/K-SADS_Fragebogen.pdf">http://www.adhs-essen.com/PDF/K-SADS_Fragebogen.pdf</a>. <ul></ul></li><li id="ref50"><span>50.
            </span><a name="pone.0104185-Laucht6" id="pone.0104185-Laucht6"></a>Laucht M, Esser G, Schmidt MH (2001) Differential development of infants at risk for psychopathology: the moderating role of early maternal responsivity. Dev Med Child Neurol 43: 292–300. <ul><li><a href="#" data-author="Laucht" data-cit="LauchtM%2C%20EsserG%2C%20SchmidtMH%20%282001%29%20Differential%20development%20of%20infants%20at%20risk%20for%20psychopathology%3A%20the%20moderating%20role%20of%20early%20maternal%20responsivity.%20Dev%20Med%20Child%20Neurol%2043%3A%20292%E2%80%93300." data-title="Differential%20development%20of%20infants%20at%20risk%20for%20psychopathology%3A%20the%20moderating%20role%20of%20early%20maternal%20responsivity" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Differential+development+of+infants+at+risk+for+psychopathology%3A+the+moderating+role+of+early+maternal+responsivity+Laucht+2001" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref51"><span>51.
            </span><a name="pone.0104185-Holz1" id="pone.0104185-Holz1"></a>Holz NE, Boecker R, Baumeister S, Hohm E, Zohsel K, et al.. (2014) Effect of Prenatal Exposure to Tobacco Smoke on Inhibitory Control: Neuroimaging Results From a 25-Year Prospective Study. JAMA Psychiatry. <ul></ul></li><li id="ref52"><span>52.
            </span><a name="pone.0104185-Kaufman2" id="pone.0104185-Kaufman2"></a>Kaufman J, Birmaher B, Brent D, Rao U, Flynn C, et al. (1997) Schedule for Affective Disorders and Schizophrenia for School-Age Children-Present and Lifetime Version (K-SADS-PL): initial reliability and validity data. J Am Acad Child Adolesc Psychiatry 36: 980–988. <ul><li><a href="#" data-author="Kaufman" data-cit="KaufmanJ%2C%20BirmaherB%2C%20BrentD%2C%20RaoU%2C%20FlynnC%2C%20et%20al.%20%281997%29%20Schedule%20for%20Affective%20Disorders%20and%20Schizophrenia%20for%20School-Age%20Children-Present%20and%20Lifetime%20Version%20%28K-SADS-PL%29%3A%20initial%20reliability%20and%20validity%20data.%20J%20Am%20Acad%20Child%20Adolesc%20Psychiatry%2036%3A%20980%E2%80%93988." data-title="Schedule%20for%20Affective%20Disorders%20and%20Schizophrenia%20for%20School-Age%20Children-Present%20and%20Lifetime%20Version%20%28K-SADS-PL%29%3A%20initial%20reliability%20and%20validity%20data" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Schedule+for+Affective+Disorders+and+Schizophrenia+for+School-Age+Children-Present+and+Lifetime+Version+%28K-SADS-PL%29%3A+initial+reliability+and+validity+data+Kaufman+1997" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref53"><span>53.
            </span><a name="pone.0104185-Kirsch1" id="pone.0104185-Kirsch1"></a>Kirsch P, Schienle A, Stark R, Sammer G, Blecker C, et al. (2003) Anticipation of reward in a nonaversive differential conditioning paradigm and the brain reward system: an event-related fMRI study. Neuroimage 20: 1086–1095. <ul><li><a href="#" data-author="Kirsch" data-cit="KirschP%2C%20SchienleA%2C%20StarkR%2C%20SammerG%2C%20BleckerC%2C%20et%20al.%20%282003%29%20Anticipation%20of%20reward%20in%20a%20nonaversive%20differential%20conditioning%20paradigm%20and%20the%20brain%20reward%20system%3A%20an%20event-related%20fMRI%20study.%20Neuroimage%2020%3A%201086%E2%80%931095." data-title="Anticipation%20of%20reward%20in%20a%20nonaversive%20differential%20conditioning%20paradigm%20and%20the%20brain%20reward%20system%3A%20an%20event-related%20fMRI%20study" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Anticipation+of+reward+in+a+nonaversive+differential+conditioning+paradigm+and+the+brain+reward+system%3A+an+event-related+fMRI+study+Kirsch+2003" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref54"><span>54.
            </span><a name="pone.0104185-Knutson1" id="pone.0104185-Knutson1"></a>Knutson B, Adams CM, Fong GW, Hommer D (2001) Anticipation of increasing monetary reward selectively recruits nucleus accumbens. J Neurosci 21: RC159. <ul><li><a href="#" data-author="Knutson" data-cit="KnutsonB%2C%20AdamsCM%2C%20FongGW%2C%20HommerD%20%282001%29%20Anticipation%20of%20increasing%20monetary%20reward%20selectively%20recruits%20nucleus%20accumbens.%20J%20Neurosci%2021%3A%20RC159." data-title="Anticipation%20of%20increasing%20monetary%20reward%20selectively%20recruits%20nucleus%20accumbens" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Anticipation+of+increasing+monetary+reward+selectively+recruits+nucleus+accumbens+Knutson+2001" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref55"><span>55.
            </span><a name="pone.0104185-Plichta3" id="pone.0104185-Plichta3"></a>Plichta MM, Wolf I, Hohmann S, Baumeister S, Boecker R, et al. (2013) Simultaneous EEG and fMRI reveals a causally connected subcortical-cortical network during reward anticipation. J Neurosci 33: 14526–14533. <ul><li><a href="#" data-author="Plichta" data-cit="PlichtaMM%2C%20WolfI%2C%20HohmannS%2C%20BaumeisterS%2C%20BoeckerR%2C%20et%20al.%20%282013%29%20Simultaneous%20EEG%20and%20fMRI%20reveals%20a%20causally%20connected%20subcortical-cortical%20network%20during%20reward%20anticipation.%20J%20Neurosci%2033%3A%2014526%E2%80%9314533." data-title="Simultaneous%20EEG%20and%20fMRI%20reveals%20a%20causally%20connected%20subcortical-cortical%20network%20during%20reward%20anticipation" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Simultaneous+EEG+and+fMRI+reveals+a+causally+connected+subcortical-cortical+network+during+reward+anticipation+Plichta+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref56"><span>56.
            </span><a name="pone.0104185-Allen1" id="pone.0104185-Allen1"></a>Allen PJ, Josephs O, Turner R (2000) A method for removing imaging artifact from continuous EEG recorded during functional MRI. Neuroimage 12: 230–239. <ul><li><a href="#" data-author="Allen" data-cit="AllenPJ%2C%20JosephsO%2C%20TurnerR%20%282000%29%20A%20method%20for%20removing%20imaging%20artifact%20from%20continuous%20EEG%20recorded%20during%20functional%20MRI.%20Neuroimage%2012%3A%20230%E2%80%93239." data-title="A%20method%20for%20removing%20imaging%20artifact%20from%20continuous%20EEG%20recorded%20during%20functional%20MRI" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=A+method+for+removing+imaging+artifact+from+continuous+EEG+recorded+during+functional+MRI+Allen+2000" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref57"><span>57.
            </span><a name="pone.0104185-Allen2" id="pone.0104185-Allen2"></a>Allen PJ, Polizzi G, Krakow K, Fish DR, Lemieux L (1998) Identification of EEG events in the MR scanner: the problem of pulse artifact and a method for its subtraction. Neuroimage 8: 229–239. <ul><li><a href="#" data-author="Allen" data-cit="AllenPJ%2C%20PolizziG%2C%20KrakowK%2C%20FishDR%2C%20LemieuxL%20%281998%29%20Identification%20of%20EEG%20events%20in%20the%20MR%20scanner%3A%20the%20problem%20of%20pulse%20artifact%20and%20a%20method%20for%20its%20subtraction.%20Neuroimage%208%3A%20229%E2%80%93239." data-title="Identification%20of%20EEG%20events%20in%20the%20MR%20scanner%3A%20the%20problem%20of%20pulse%20artifact%20and%20a%20method%20for%20its%20subtraction" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Identification+of+EEG+events+in+the+MR+scanner%3A+the+problem+of+pulse+artifact+and+a+method+for+its+subtraction+Allen+1998" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref58"><span>58.
            </span><a name="pone.0104185-Makeig1" id="pone.0104185-Makeig1"></a>Makeig S, Bell AJ, Jung TP, Sejnowski TJ (1996) Independent component analysis of electroencephalographic data. Adv Neural Inf Process Syst 8: 145–151. <ul><li><a href="#" data-author="Makeig" data-cit="MakeigS%2C%20BellAJ%2C%20JungTP%2C%20SejnowskiTJ%20%281996%29%20Independent%20component%20analysis%20of%20electroencephalographic%20data.%20Adv%20Neural%20Inf%20Process%20Syst%208%3A%20145%E2%80%93151." data-title="Independent%20component%20analysis%20of%20electroencephalographic%20data" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Independent+component+analysis+of+electroencephalographic+data+Makeig+1996" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref59"><span>59.
            </span><a name="pone.0104185-Debener1" id="pone.0104185-Debener1"></a>Debener S, Mullinger KJ, Niazy RK, Bowtell RW (2008) Properties of the ballistocardiogram artefact as revealed by EEG recordings at 1.5, 3 and 7 T static magnetic field strength. Int J Psychophysiol 67: 189–199. <ul><li><a href="#" data-author="Debener" data-cit="DebenerS%2C%20MullingerKJ%2C%20NiazyRK%2C%20BowtellRW%20%282008%29%20Properties%20of%20the%20ballistocardiogram%20artefact%20as%20revealed%20by%20EEG%20recordings%20at%201.5%2C%203%20and%207%20T%20static%20magnetic%20field%20strength.%20Int%20J%20Psychophysiol%2067%3A%20189%E2%80%93199." data-title="Properties%20of%20the%20ballistocardiogram%20artefact%20as%20revealed%20by%20EEG%20recordings%20at%201.5%2C%203%20and%207%20T%20static%20magnetic%20field%20strength" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Properties+of+the+ballistocardiogram+artefact+as+revealed+by+EEG+recordings+at+1.5%2C+3+and+7+T+static+magnetic+field+strength+Debener+2008" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref60"><span>60.
            </span><a name="pone.0104185-Achenbach1" id="pone.0104185-Achenbach1"></a>Achenbach T (1997) Manual for the Young Adult Self-Report and Young Adult Behavior Checklist. Burlington, VT: Department of Psychiatry, University of Vermont <ul></ul></li><li id="ref61"><span>61.
            </span><a name="pone.0104185-Maldjian1" id="pone.0104185-Maldjian1"></a>Maldjian JA, Laurienti PJ, Kraft RA, Burdette JH (2003) An automated method for neuroanatomic and cytoarchitectonic atlas-based interrogation of fMRI data sets. Neuroimage 19: 1233–1239. <ul><li><a href="#" data-author="Maldjian" data-cit="MaldjianJA%2C%20LaurientiPJ%2C%20KraftRA%2C%20BurdetteJH%20%282003%29%20An%20automated%20method%20for%20neuroanatomic%20and%20cytoarchitectonic%20atlas-based%20interrogation%20of%20fMRI%20data%20sets.%20Neuroimage%2019%3A%201233%E2%80%931239." data-title="An%20automated%20method%20for%20neuroanatomic%20and%20cytoarchitectonic%20atlas-based%20interrogation%20of%20fMRI%20data%20sets" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=An+automated+method+for+neuroanatomic+and+cytoarchitectonic+atlas-based+interrogation+of+fMRI+data+sets+Maldjian+2003" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref62"><span>62.
            </span><a name="pone.0104185-Walter2" id="pone.0104185-Walter2"></a>Walter B, Blecker C, Kirsch P, Sammer G, Schienle A, et al.. (2003) MARINA: An easy to use tool for the creation of MAsks for Region of INterest Analyses [abstract]. Presented at the 9th International Conference on Functional Mapping of the Human Brain, June 19–22, New York, NY. Neuroimage 19 : Available on CD-Rom. <ul></ul></li><li id="ref63"><span>63.
            </span><a name="pone.0104185-Poppenk1" id="pone.0104185-Poppenk1"></a>Poppenk J, Evensmoen HR, Moscovitch M, Nadel L (2013) Long-axis specialization of the human hippocampus. Trends Cogn Sci 17: 230–240. <ul><li><a href="#" data-author="Poppenk" data-cit="PoppenkJ%2C%20EvensmoenHR%2C%20MoscovitchM%2C%20NadelL%20%282013%29%20Long-axis%20specialization%20of%20the%20human%20hippocampus.%20Trends%20Cogn%20Sci%2017%3A%20230%E2%80%93240." data-title="Long-axis%20specialization%20of%20the%20human%20hippocampus" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Long-axis+specialization+of+the+human+hippocampus+Poppenk+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref64"><span>64.
            </span><a name="pone.0104185-Baron1" id="pone.0104185-Baron1"></a>Baron RM, Kenny DA (1986) The moderator-mediator variable distinction in social psychological research: conceptual, strategic, and statistical considerations. J Pers Soc Psychol 51: 1173–1182. <ul><li><a href="#" data-author="Baron" data-cit="BaronRM%2C%20KennyDA%20%281986%29%20The%20moderator-mediator%20variable%20distinction%20in%20social%20psychological%20research%3A%20conceptual%2C%20strategic%2C%20and%20statistical%20considerations.%20J%20Pers%20Soc%20Psychol%2051%3A%201173%E2%80%931182." data-title="The%20moderator-mediator%20variable%20distinction%20in%20social%20psychological%20research%3A%20conceptual%2C%20strategic%2C%20and%20statistical%20considerations" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+moderator-mediator+variable+distinction+in+social+psychological+research%3A+conceptual%2C+strategic%2C+and+statistical+considerations+Baron+1986" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref65"><span>65.
            </span><a name="pone.0104185-Preacher1" id="pone.0104185-Preacher1"></a>Preacher KJ, Hayes AF (2004) SPSS and SAS procedures for estimating indirect effects in simple mediation models. Behav Res Methods Instrum Comput 36: 717–731. <ul><li><a href="#" data-author="Preacher" data-cit="PreacherKJ%2C%20HayesAF%20%282004%29%20SPSS%20and%20SAS%20procedures%20for%20estimating%20indirect%20effects%20in%20simple%20mediation%20models.%20Behav%20Res%20Methods%20Instrum%20Comput%2036%3A%20717%E2%80%93731." data-title="SPSS%20and%20SAS%20procedures%20for%20estimating%20indirect%20effects%20in%20simple%20mediation%20models" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=SPSS+and+SAS+procedures+for+estimating+indirect+effects+in+simple+mediation+models+Preacher+2004" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref66"><span>66.
            </span><a name="pone.0104185-Cho1" id="pone.0104185-Cho1"></a>Cho YT, Fromm S, Guyer AE, Detloff A, Pine DS, et al. (2012) Nucleus accumbens, thalamus and insula connectivity during incentive anticipation in typical adults and adolescents. Neuroimage 66C: 508–521. <ul><li><a href="#" data-author="Cho" data-cit="ChoYT%2C%20FrommS%2C%20GuyerAE%2C%20DetloffA%2C%20PineDS%2C%20et%20al.%20%282012%29%20Nucleus%20accumbens%2C%20thalamus%20and%20insula%20connectivity%20during%20incentive%20anticipation%20in%20typical%20adults%20and%20adolescents.%20Neuroimage%2066C%3A%20508%E2%80%93521." data-title="Nucleus%20accumbens%2C%20thalamus%20and%20insula%20connectivity%20during%20incentive%20anticipation%20in%20typical%20adults%20and%20adolescents" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Nucleus+accumbens%2C+thalamus+and+insula+connectivity+during+incentive+anticipation+in+typical+adults+and+adolescents+Cho+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref67"><span>67.
            </span><a name="pone.0104185-Schultz1" id="pone.0104185-Schultz1"></a>Schultz W (1998) Predictive reward signal of dopamine neurons. J Neurophysiol 80: 1–27. <ul><li><a href="#" data-author="Schultz" data-cit="SchultzW%20%281998%29%20Predictive%20reward%20signal%20of%20dopamine%20neurons.%20J%20Neurophysiol%2080%3A%201%E2%80%9327." data-title="Predictive%20reward%20signal%20of%20dopamine%20neurons" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Predictive+reward+signal+of+dopamine+neurons+Schultz+1998" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref68"><span>68.
            </span><a name="pone.0104185-Treadway1" id="pone.0104185-Treadway1"></a>Treadway MT, Buckholtz JW, Zald DH (2013) Perceived stress predicts altered reward and loss feedback processing in medial prefrontal cortex. Front Hum Neurosci 7: 180. <ul><li><a href="#" data-author="Treadway" data-cit="TreadwayMT%2C%20BuckholtzJW%2C%20ZaldDH%20%282013%29%20Perceived%20stress%20predicts%20altered%20reward%20and%20loss%20feedback%20processing%20in%20medial%20prefrontal%20cortex.%20Front%20Hum%20Neurosci%207%3A%20180." data-title="Perceived%20stress%20predicts%20altered%20reward%20and%20loss%20feedback%20processing%20in%20medial%20prefrontal%20cortex" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Perceived+stress+predicts+altered+reward+and+loss+feedback+processing+in+medial+prefrontal+cortex+Treadway+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref69"><span>69.
            </span><a name="pone.0104185-Knutson2" id="pone.0104185-Knutson2"></a>Knutson B, Bhanji JP, Cooney RE, Atlas LY, Gotlib IH (2008) Neural responses to monetary incentives in major depression. Biol Psychiatry 63: 686–692. <ul><li><a href="#" data-author="Knutson" data-cit="KnutsonB%2C%20BhanjiJP%2C%20CooneyRE%2C%20AtlasLY%2C%20GotlibIH%20%282008%29%20Neural%20responses%20to%20monetary%20incentives%20in%20major%20depression.%20Biol%20Psychiatry%2063%3A%20686%E2%80%93692." data-title="Neural%20responses%20to%20monetary%20incentives%20in%20major%20depression" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Neural+responses+to+monetary+incentives+in+major+depression+Knutson+2008" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref70"><span>70.
            </span><a name="pone.0104185-Friedman1" id="pone.0104185-Friedman1"></a>Friedman DP, Aggleton JP, Saunders RC (2002) Comparison of hippocampal, amygdala, and perirhinal projections to the nucleus accumbens: combined anterograde and retrograde tracing study in the Macaque brain. J Comp Neurol 450: 345–365. <ul><li><a href="#" data-author="Friedman" data-cit="FriedmanDP%2C%20AggletonJP%2C%20SaundersRC%20%282002%29%20Comparison%20of%20hippocampal%2C%20amygdala%2C%20and%20perirhinal%20projections%20to%20the%20nucleus%20accumbens%3A%20combined%20anterograde%20and%20retrograde%20tracing%20study%20in%20the%20Macaque%20brain.%20J%20Comp%20Neurol%20450%3A%20345%E2%80%93365." data-title="Comparison%20of%20hippocampal%2C%20amygdala%2C%20and%20perirhinal%20projections%20to%20the%20nucleus%20accumbens%3A%20combined%20anterograde%20and%20retrograde%20tracing%20study%20in%20the%20Macaque%20brain" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Comparison+of+hippocampal%2C+amygdala%2C+and+perirhinal+projections+to+the+nucleus+accumbens%3A+combined+anterograde+and+retrograde+tracing+study+in+the+Macaque+brain+Friedman+2002" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref71"><span>71.
            </span><a name="pone.0104185-Murty1" id="pone.0104185-Murty1"></a>Murty VP, Adcock RA (2013) Enriched Encoding: Reward Motivation Organizes Cortical Networks for Hippocampal Detection of Unexpected Events. Cereb Cortex. <ul></ul></li><li id="ref72"><span>72.
            </span><a name="pone.0104185-Wise1" id="pone.0104185-Wise1"></a>Wise RA (2004) Dopamine, learning and motivation. Nat Rev Neurosci 5: 483–494. <ul><li><a href="#" data-author="Wise" data-cit="WiseRA%20%282004%29%20Dopamine%2C%20learning%20and%20motivation.%20Nat%20Rev%20Neurosci%205%3A%20483%E2%80%93494." data-title="Dopamine%2C%20learning%20and%20motivation" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Dopamine%2C+learning+and+motivation+Wise+2004" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref73"><span>73.
            </span><a name="pone.0104185-Frank1" id="pone.0104185-Frank1"></a>Frank MJ, Claus ED (2006) Anatomy of a decision: striato-orbitofrontal interactions in reinforcement learning, decision making, and reversal. Psychol Rev 113: 300–326. <ul><li><a href="#" data-author="Frank" data-cit="FrankMJ%2C%20ClausED%20%282006%29%20Anatomy%20of%20a%20decision%3A%20striato-orbitofrontal%20interactions%20in%20reinforcement%20learning%2C%20decision%20making%2C%20and%20reversal.%20Psychol%20Rev%20113%3A%20300%E2%80%93326." data-title="Anatomy%20of%20a%20decision%3A%20striato-orbitofrontal%20interactions%20in%20reinforcement%20learning%2C%20decision%20making%2C%20and%20reversal" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Anatomy+of+a+decision%3A+striato-orbitofrontal+interactions+in+reinforcement+learning%2C+decision+making%2C+and+reversal+Frank+2006" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref74"><span>74.
            </span><a name="pone.0104185-Schmid1" id="pone.0104185-Schmid1"></a>Schmid B, Blomeyer D, Buchmann AF, Trautmann-Villalba P, Zimmermann US, et al. (2011) Quality of early mother-child interaction associated with depressive psychopathology in the offspring: A prospective study from infancy to adulthood. J Psychiatr Res 45: 1387–1394. <ul><li><a href="#" data-author="Schmid" data-cit="SchmidB%2C%20BlomeyerD%2C%20BuchmannAF%2C%20Trautmann-VillalbaP%2C%20ZimmermannUS%2C%20et%20al.%20%282011%29%20Quality%20of%20early%20mother-child%20interaction%20associated%20with%20depressive%20psychopathology%20in%20the%20offspring%3A%20A%20prospective%20study%20from%20infancy%20to%20adulthood.%20J%20Psychiatr%20Res%2045%3A%201387%E2%80%931394." data-title="Quality%20of%20early%20mother-child%20interaction%20associated%20with%20depressive%20psychopathology%20in%20the%20offspring%3A%20A%20prospective%20study%20from%20infancy%20to%20adulthood" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Quality+of+early+mother-child+interaction+associated+with+depressive+psychopathology+in+the+offspring%3A+A+prospective+study+from+infancy+to+adulthood+Schmid+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref75"><span>75.
            </span><a name="pone.0104185-Glascoe1" id="pone.0104185-Glascoe1"></a>Glascoe FP, Leew S (2010) Parenting behaviors, perceptions, and psychosocial risk: impacts on young children's development. Pediatrics 125: 313–319. <ul><li><a href="#" data-author="Glascoe" data-cit="GlascoeFP%2C%20LeewS%20%282010%29%20Parenting%20behaviors%2C%20perceptions%2C%20and%20psychosocial%20risk%3A%20impacts%20on%20young%20children%27s%20development.%20Pediatrics%20125%3A%20313%E2%80%93319." data-title="Parenting%20behaviors%2C%20perceptions%2C%20and%20psychosocial%20risk%3A%20impacts%20on%20young%20children%26apos%3Bs%20development" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Parenting+behaviors%2C+perceptions%2C+and+psychosocial+risk%3A+impacts+on+young+children%26apos%3Bs+development+Glascoe+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref76"><span>76.
            </span><a name="pone.0104185-Burchinal1" id="pone.0104185-Burchinal1"></a>Burchinal M, Vernon-Feagans L, Cox M (2008) Key Family Life Project I (2008) Cumulative Social Risk, Parenting, and Infant Development in Rural Low-Income Communities. Parenting, science and practice 8: 41–69. <ul><li><a href="#" data-author="Burchinal" data-cit="BurchinalM%2C%20Vernon-FeagansL%2C%20CoxM%20%282008%29%20Key%20Family%20Life%20Project%20I%20%282008%29%20Cumulative%20Social%20Risk%2C%20Parenting%2C%20and%20Infant%20Development%20in%20Rural%20Low-Income%20Communities.%20Parenting%2C%20science%20and%20practice%208%3A%2041%E2%80%9369." data-title="Cumulative%20Social%20Risk%2C%20Parenting%2C%20and%20Infant%20Development%20in%20Rural%20Low-Income%20Communities" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Cumulative+Social+Risk%2C+Parenting%2C+and+Infant+Development+in+Rural+Low-Income+Communities+Burchinal+2008" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref77"><span>77.
            </span><a name="pone.0104185-Baker1" id="pone.0104185-Baker1"></a>Baker CN, Hoerger M (2012) Parental Child-Rearing Strategies Influence Self-Regulation, Socio-Emotional Adjustment, and Psychopathology in Early Adulthood: Evidence from a Retrospective Cohort Study. Pers Individ Dif 52: 800–805. <ul><li><a href="#" data-author="Baker" data-cit="BakerCN%2C%20HoergerM%20%282012%29%20Parental%20Child-Rearing%20Strategies%20Influence%20Self-Regulation%2C%20Socio-Emotional%20Adjustment%2C%20and%20Psychopathology%20in%20Early%20Adulthood%3A%20Evidence%20from%20a%20Retrospective%20Cohort%20Study.%20Pers%20Individ%20Dif%2052%3A%20800%E2%80%93805." data-title="Parental%20Child-Rearing%20Strategies%20Influence%20Self-Regulation%2C%20Socio-Emotional%20Adjustment%2C%20and%20Psychopathology%20in%20Early%20Adulthood%3A%20Evidence%20from%20a%20Retrospective%20Cohort%20Study" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Parental+Child-Rearing+Strategies+Influence+Self-Regulation%2C+Socio-Emotional+Adjustment%2C+and+Psychopathology+in+Early+Adulthood%3A+Evidence+from+a+Retrospective+Cohort+Study+Baker+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref78"><span>78.
            </span><a name="pone.0104185-Blum1" id="pone.0104185-Blum1"></a>Blum K, Braverman ER, Holder JM, Lubar JF, Monastra VJ, et al.. (2000) Reward deficiency syndrome: a biogenetic model for the diagnosis and treatment of impulsive, addictive, and compulsive behaviors. J Psychoactive Drugs 32 : Suppl: i–iv, 1–112. <ul></ul></li><li id="ref79"><span>79.
            </span><a name="pone.0104185-Berghorst1" id="pone.0104185-Berghorst1"></a>Berghorst LH, Bogdan R, Frank MJ, Pizzagalli DA (2013) Acute stress selectively reduces reward sensitivity. Front Hum Neurosci 7: 133. <ul><li><a href="#" data-author="Berghorst" data-cit="BerghorstLH%2C%20BogdanR%2C%20FrankMJ%2C%20PizzagalliDA%20%282013%29%20Acute%20stress%20selectively%20reduces%20reward%20sensitivity.%20Front%20Hum%20Neurosci%207%3A%20133." data-title="Acute%20stress%20selectively%20reduces%20reward%20sensitivity" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Acute+stress+selectively+reduces+reward+sensitivity+Berghorst+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref80"><span>80.
            </span><a name="pone.0104185-Lupien2" id="pone.0104185-Lupien2"></a>Lupien SJ, Maheu F, Tu M, Fiocco A, Schramek TE (2007) The effects of stress and stress hormones on human cognition: Implications for the field of brain and cognition. Brain Cogn 65: 209–237. <ul><li><a href="#" data-author="Lupien" data-cit="LupienSJ%2C%20MaheuF%2C%20TuM%2C%20FioccoA%2C%20SchramekTE%20%282007%29%20The%20effects%20of%20stress%20and%20stress%20hormones%20on%20human%20cognition%3A%20Implications%20for%20the%20field%20of%20brain%20and%20cognition.%20Brain%20Cogn%2065%3A%20209%E2%80%93237." data-title="The%20effects%20of%20stress%20and%20stress%20hormones%20on%20human%20cognition%3A%20Implications%20for%20the%20field%20of%20brain%20and%20cognition" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+effects+of+stress+and+stress+hormones+on+human+cognition%3A+Implications+for+the+field+of+brain+and+cognition+Lupien+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref81"><span>81.
            </span><a name="pone.0104185-Cabib1" id="pone.0104185-Cabib1"></a>Cabib S, Puglisi-Allegra S (1996) Stress, depression and the mesolimbic dopamine system. Psychopharmacology (Berl) 128: 331–342. <ul><li><a href="#" data-author="Cabib" data-cit="CabibS%2C%20Puglisi-AllegraS%20%281996%29%20Stress%2C%20depression%20and%20the%20mesolimbic%20dopamine%20system.%20Psychopharmacology%20%28Berl%29%20128%3A%20331%E2%80%93342." data-title="Stress%2C%20depression%20and%20the%20mesolimbic%20dopamine%20system" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Stress%2C+depression+and+the+mesolimbic+dopamine+system+Cabib+1996" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref82"><span>82.
            </span><a name="pone.0104185-Bogdan1" id="pone.0104185-Bogdan1"></a>Bogdan R, Pizzagalli DA (2006) Acute stress reduces reward responsiveness: implications for depression. Biol Psychiatry 60: 1147–1154. <ul><li><a href="#" data-author="Bogdan" data-cit="BogdanR%2C%20PizzagalliDA%20%282006%29%20Acute%20stress%20reduces%20reward%20responsiveness%3A%20implications%20for%20depression.%20Biol%20Psychiatry%2060%3A%201147%E2%80%931154." data-title="Acute%20stress%20reduces%20reward%20responsiveness%3A%20implications%20for%20depression" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Acute+stress+reduces+reward+responsiveness%3A+implications+for+depression+Bogdan+2006" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref83"><span>83.
            </span><a name="pone.0104185-Fries1" id="pone.0104185-Fries1"></a>Fries E, Hesse J, Hellhammer J, Hellhammer DH (2005) A new view on hypocortisolism. Psychoneuroendocrinology 30: 1010–1016. <ul><li><a href="#" data-author="Fries" data-cit="FriesE%2C%20HesseJ%2C%20HellhammerJ%2C%20HellhammerDH%20%282005%29%20A%20new%20view%20on%20hypocortisolism.%20Psychoneuroendocrinology%2030%3A%201010%E2%80%931016." data-title="A%20new%20view%20on%20hypocortisolism" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=A+new+view+on+hypocortisolism+Fries+2005" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref84"><span>84.
            </span><a name="pone.0104185-Heim2" id="pone.0104185-Heim2"></a>Heim C, Ehlert U, Hellhammer DH (2000) The potential role of hypocortisolism in the pathophysiology of stress-related bodily disorders. Psychoneuroendocrinology 25: 1–35. <ul><li><a href="#" data-author="Heim" data-cit="HeimC%2C%20EhlertU%2C%20HellhammerDH%20%282000%29%20The%20potential%20role%20of%20hypocortisolism%20in%20the%20pathophysiology%20of%20stress-related%20bodily%20disorders.%20Psychoneuroendocrinology%2025%3A%201%E2%80%9335." data-title="The%20potential%20role%20of%20hypocortisolism%20in%20the%20pathophysiology%20of%20stress-related%20bodily%20disorders" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+potential+role+of+hypocortisolism+in+the+pathophysiology+of+stress-related+bodily+disorders+Heim+2000" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref85"><span>85.
            </span><a name="pone.0104185-Meaney1" id="pone.0104185-Meaney1"></a>Meaney MJ (2010) Epigenetics and the biological definition of gene x environment interactions. Child Dev 81: 41–79. <ul><li><a href="#" data-author="Meaney" data-cit="MeaneyMJ%20%282010%29%20Epigenetics%20and%20the%20biological%20definition%20of%20gene%20x%20environment%20interactions.%20Child%20Dev%2081%3A%2041%E2%80%9379." data-title="Epigenetics%20and%20the%20biological%20definition%20of%20gene%20x%20environment%20interactions" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Epigenetics+and+the+biological+definition+of+gene+x+environment+interactions+Meaney+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref86"><span>86.
            </span><a name="pone.0104185-Roth1" id="pone.0104185-Roth1"></a>Roth TL, Sweatt JD (2011) Epigenetic marking of the BDNF gene by early-life adverse experiences. Horm Behav 59: 315–320. <ul><li><a href="#" data-author="Roth" data-cit="RothTL%2C%20SweattJD%20%282011%29%20Epigenetic%20marking%20of%20the%20BDNF%20gene%20by%20early-life%20adverse%20experiences.%20Horm%20Behav%2059%3A%20315%E2%80%93320." data-title="Epigenetic%20marking%20of%20the%20BDNF%20gene%20by%20early-life%20adverse%20experiences" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Epigenetic+marking+of+the+BDNF+gene+by+early-life+adverse+experiences+Roth+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref87"><span>87.
            </span><a name="pone.0104185-Hahn1" id="pone.0104185-Hahn1"></a>Hahn T, Heinzel S, Dresler T, Plichta MM, Renner TJ, et al. (2011) Association between reward-related activation in the ventral striatum and trait reward sensitivity is moderated by dopamine transporter genotype. Hum Brain Mapp 32: 1557–1565. <ul><li><a href="#" data-author="Hahn" data-cit="HahnT%2C%20HeinzelS%2C%20DreslerT%2C%20PlichtaMM%2C%20RennerTJ%2C%20et%20al.%20%282011%29%20Association%20between%20reward-related%20activation%20in%20the%20ventral%20striatum%20and%20trait%20reward%20sensitivity%20is%20moderated%20by%20dopamine%20transporter%20genotype.%20Hum%20Brain%20Mapp%2032%3A%201557%E2%80%931565." data-title="Association%20between%20reward-related%20activation%20in%20the%20ventral%20striatum%20and%20trait%20reward%20sensitivity%20is%20moderated%20by%20dopamine%20transporter%20genotype" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Association+between+reward-related+activation+in+the+ventral+striatum+and+trait+reward+sensitivity+is+moderated+by+dopamine+transporter+genotype+Hahn+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref88"><span>88.
            </span><a name="pone.0104185-Dreher1" id="pone.0104185-Dreher1"></a>Dreher JC, Kohn P, Kolachana B, Weinberger DR, Berman KF (2009) Variation in dopamine genes influences responsivity of the human reward system. Proc Natl Acad Sci U S A 106: 617–622. <ul><li><a href="#" data-author="Dreher" data-cit="DreherJC%2C%20KohnP%2C%20KolachanaB%2C%20WeinbergerDR%2C%20BermanKF%20%282009%29%20Variation%20in%20dopamine%20genes%20influences%20responsivity%20of%20the%20human%20reward%20system.%20Proc%20Natl%20Acad%20Sci%20U%20S%20A%20106%3A%20617%E2%80%93622." data-title="Variation%20in%20dopamine%20genes%20influences%20responsivity%20of%20the%20human%20reward%20system" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Variation+in+dopamine+genes+influences+responsivity+of+the+human+reward+system+Dreher+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref89"><span>89.
            </span><a name="pone.0104185-Camara1" id="pone.0104185-Camara1"></a>Camara E, Kramer UM, Cunillera T, Marco-Pallares J, Cucurell D, et al. (2010) The effects of COMT (Val108/158Met) and DRD4 (SNP -521) dopamine genotypes on brain activations related to valence and magnitude of rewards. Cereb Cortex 20: 1985–1996. <ul><li><a href="#" data-author="Camara" data-cit="CamaraE%2C%20KramerUM%2C%20CunilleraT%2C%20Marco-PallaresJ%2C%20CucurellD%2C%20et%20al.%20%282010%29%20The%20effects%20of%20COMT%20%28Val108%2F158Met%29%20and%20DRD4%20%28SNP%20-521%29%20dopamine%20genotypes%20on%20brain%20activations%20related%20to%20valence%20and%20magnitude%20of%20rewards.%20Cereb%20Cortex%2020%3A%201985%E2%80%931996." data-title="The%20effects%20of%20COMT%20%28Val108%2F158Met%29%20and%20DRD4%20%28SNP%20-521%29%20dopamine%20genotypes%20on%20brain%20activations%20related%20to%20valence%20and%20magnitude%20of%20rewards" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+effects+of+COMT+%28Val108%2F158Met%29+and+DRD4+%28SNP+-521%29+dopamine+genotypes+on+brain+activations+related+to+valence+and+magnitude+of+rewards+Camara+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref90"><span>90.
            </span><a name="pone.0104185-Yacubian1" id="pone.0104185-Yacubian1"></a>Yacubian J, Sommer T, Schroeder K, Glascher J, Kalisch R, et al. (2007) Gene-gene interaction associated with neural reward sensitivity. Proc Natl Acad Sci U S A 104: 8125–8130. <ul><li><a href="#" data-author="Yacubian" data-cit="YacubianJ%2C%20SommerT%2C%20SchroederK%2C%20GlascherJ%2C%20KalischR%2C%20et%20al.%20%282007%29%20Gene-gene%20interaction%20associated%20with%20neural%20reward%20sensitivity.%20Proc%20Natl%20Acad%20Sci%20U%20S%20A%20104%3A%208125%E2%80%938130." data-title="Gene-gene%20interaction%20associated%20with%20neural%20reward%20sensitivity" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Gene-gene+interaction+associated+with+neural+reward+sensitivity+Yacubian+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li></ol></div>



          

        </div>
      </div>
  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Net 9.0 LINQ Performance Improvements (190 pts)]]></title>
            <link>https://blog.ndepend.com/net-9-0-linq-performance-improvements/</link>
            <guid>41878095</guid>
            <pubDate>Fri, 18 Oct 2024 10:39:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.ndepend.com/net-9-0-linq-performance-improvements/">https://blog.ndepend.com/net-9-0-linq-performance-improvements/</a>, See on <a href="https://news.ycombinator.com/item?id=41878095">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-6272">
	
	<!-- .entry-header -->

	<div>
		<p><span>October 17, 2024</span> <span> </span> <span>5 minutes read </span></p><p><a href="https://blog.ndepend.com/wp-content/uploads/Alternate-Lookup-for-Dictionary-and-HashSet-in-.NET-9-1.png"><img fetchpriority="high" decoding="async" src="https://blog.ndepend.com/wp-content/uploads/Alternate-Lookup-for-Dictionary-and-HashSet-in-.NET-9-1.png" alt=".NET 9.0 LINQ Performance Improvements" width="1230" height="722" srcset="https://blog.ndepend.com/wp-content/uploads/Alternate-Lookup-for-Dictionary-and-HashSet-in-.NET-9-1.png 1230w, https://blog.ndepend.com/wp-content/uploads/Alternate-Lookup-for-Dictionary-and-HashSet-in-.NET-9-1-300x176.png 300w, https://blog.ndepend.com/wp-content/uploads/Alternate-Lookup-for-Dictionary-and-HashSet-in-.NET-9-1-1024x601.png 1024w, https://blog.ndepend.com/wp-content/uploads/Alternate-Lookup-for-Dictionary-and-HashSet-in-.NET-9-1-768x451.png 768w" sizes="(max-width: 1230px) 100vw, 1230px"></a> NET 9.0 brings significant improvements to LINQ performance, with some scenarios showing remarkable gains. Let’s take a closer look at what’s driving these enhancements. The lessons learned will be relevant to your code.</p>
<div id="ez-toc-container">

<nav><ul><li><a href="#Iterating_with_Span_when_Possible" title="Iterating with Span<T> when Possible">Iterating with Span&lt;T&gt; when Possible</a><ul><li><a href="#The_TryGetSpan_Method" title="The TryGetSpan() Method">The TryGetSpan() Method</a></li><li><a href="#TryGetSpan_Callers" title="TryGetSpan() Callers">TryGetSpan() Callers</a></li></ul></li><li><a href="#Specialized_Iterators" title="Specialized Iterators">Specialized Iterators</a><ul><li><a href="#The_Astute" title="The Astute">The Astute</a></li><li><a href="#The_implementation_Iterator_and_its_Derived_Class" title="The implementation: Iterator<T> and its Derived Class">The implementation: Iterator&lt;T&gt; and its Derived Class</a></li><li><a href="#Case_Study_ListWhereSelectIterator" title="Case Study: ListWhereSelectIterator<TSource, TResult>">Case Study: ListWhereSelectIterator&lt;TSource, TResult&gt;</a></li><li><a href="#Case_Study_IListSkipTakeIterator" title="Case Study: IListSkipTakeIterator<TSource>">Case Study: IListSkipTakeIterator&lt;TSource&gt;</a></li></ul></li><li><a href="#Conclusion" title="Conclusion">Conclusion</a></li></ul></nav></div>
<h2><span id="Iterating_with_Span_when_Possible"></span>Iterating with Span&lt;T&gt; when Possible<span></span></h2>
<p>Let’s start by running this benchmark on .NET 8 versus .NET 9.</p><!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-67124f6522f22762917268" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					<div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p></div>
				</td>
						<td><div><p><span>using </span><span>BenchmarkDotNet</span><span>.</span><span>Configs</span><span>;</span></p><p><span>using </span><span>BenchmarkDotNet</span><span>.</span><span>Running</span><span>;</span></p><p><span>using </span><span>BenchmarkDotNet</span><span>.</span><span>Attributes</span><span>;</span></p><p><span>using </span><span>BenchmarkDotNet</span><span>.</span><span>Jobs</span><span>;</span></p><p><span>BenchmarkRunner</span><span>.</span><span>Run</span><span>&lt;</span><span>Benchmarks</span><span>&gt;</span><span>(</span><span>)</span><span>;</span></p><p><span>[</span><span>MemoryDiagnoser</span><span>]</span></p><p><span>[</span><span>HideColumns</span><span>(</span><span>"StdDev"</span><span>,</span><span> </span><span>"Median"</span><span>,</span><span> </span><span>"Job"</span><span>,</span><span> </span><span>"RatioSD"</span><span>,</span><span> </span><span>"Error"</span><span>,</span><span> </span><span>"Gen0"</span><span>,</span><span> </span><span>"Alloc Ratio"</span><span>)</span><span>]</span></p><p><span>[</span><span>SimpleJob</span><span>(</span><span>RuntimeMoniker</span><span>.</span><span>Net80</span><span>,</span><span> </span><span>baseline</span><span>:</span><span> </span><span>true</span><span>)</span><span>]</span></p><p><span>[</span><span>SimpleJob</span><span>(</span><span>RuntimeMoniker</span><span>.</span><span>Net90</span><span>)</span><span>]</span></p><p><span>public</span><span> </span><span>class</span><span> </span><span>Benchmarks</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp; </span><span>private</span><span> </span><span>IEnumerable</span><span>&lt;</span><span>int</span><span>&gt;</span><span> </span><span>_array</span><span> </span>=<span> </span><span>Enumerable</span><span>.</span><span>Range</span><span>(</span><span>1</span><span>,</span><span> </span><span>10_000</span><span>)</span><span>.</span><span>ToArray</span><span>(</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>[</span><span>Benchmark</span><span>]</span><span> </span><span>public</span><span> </span><span>int</span><span> </span><span>Count</span><span>(</span><span>)</span><span> </span>=<span>&gt;</span><span> </span><span>_array</span><span>.</span><span>Count</span><span>(</span><span>i</span><span> </span>=<span>&gt;</span><span> </span><span>i</span><span> </span><span>&gt;</span><span> </span><span>0</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>[</span><span>Benchmark</span><span>]</span><span> </span><span>public</span><span> </span><span>bool</span><span> </span><span>All</span><span>(</span><span>)</span><span> </span>=<span>&gt;</span><span> </span><span>_array</span><span>.</span><span>All</span><span>(</span><span>i</span><span> </span>=<span>&gt;</span><span> </span><span>i</span><span> </span><span>&gt;</span><span> </span><span>500</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>[</span><span>Benchmark</span><span>]</span><span> </span><span>public</span><span> </span><span>bool</span><span> </span><span>Any</span><span>(</span><span>)</span><span> </span>=<span>&gt;</span><span> </span><span>_array</span><span>.</span><span>Any</span><span>(</span><span>i</span><span> </span>=<span>&gt;</span><span> </span><span>i</span><span> </span>==<span> </span><span>9_999</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>[</span><span>Benchmark</span><span>]</span><span> </span><span>public</span><span> </span><span>int</span><span> </span><span>First</span><span>(</span><span>)</span><span> </span>=<span>&gt;</span><span> </span><span>_array</span><span>.</span><span>First</span><span>(</span><span>i</span><span> </span>=<span>&gt;</span><span> </span><span>i</span><span> </span><span>&gt;</span><span> </span><span>9_000</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>[</span><span>Benchmark</span><span>]</span><span> </span><span>public</span><span> </span><span>int</span><span> </span><span>Single</span><span>(</span><span>)</span><span> </span>=<span>&gt;</span><span> </span><span>_array</span><span>.</span><span>Single</span><span>(</span><span>i</span><span> </span>=<span>&gt;</span><span> </span><span>i</span><span> </span>==<span> </span><span>9_999</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>[</span><span>Benchmark</span><span>]</span><span> </span><span>public</span><span> </span><span>int</span><span> </span><span>Last</span><span>(</span><span>)</span><span> </span>=<span>&gt;</span><span> </span><span>_array</span><span>.</span><span>Last</span><span>(</span><span>i</span><span> </span>=<span>&gt;</span><span> </span><span>i</span><span> </span><span>&gt;</span><span> </span><span>0</span><span>)</span><span>;</span></p><p><span>}</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
<!-- [Format Time: 0.0017 seconds] -->


<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-67124f6522f27054400802" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&lt;</span><span>Project </span><span>Sdk</span>=<span>"Microsoft.NET.Sdk"</span><span>&gt;</span></p><p><span>&nbsp;&nbsp;</span><span>&lt;</span><span>PropertyGroup</span><span>&gt;</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>&lt;</span><span>OutputType</span><span>&gt;</span><span>Exe</span><span>&lt;</span>/<span>OutputType</span><span>&gt;</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>&lt;</span><span>TargetFrameworks</span><span>&gt;</span><span>net8</span><span>.</span><span>0</span><span>;</span><span>net9</span><span>.</span><span>0</span><span>&lt;</span>/<span>TargetFrameworks</span><span>&gt;</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>&lt;</span><span>ImplicitUsings</span><span>&gt;</span><span>enable</span><span>&lt;</span>/<span>ImplicitUsings</span><span>&gt;</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>&lt;</span><span>Nullable</span><span>&gt;</span><span>enable</span><span>&lt;</span>/<span>Nullable</span><span>&gt;</span></p><p><span>&nbsp;&nbsp;</span><span>&lt;</span>/<span>PropertyGroup</span><span>&gt;</span></p><p><span>&nbsp;&nbsp;</span><span>&lt;</span><span>ItemGroup</span><span>&gt;</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>&lt;</span><span>PackageReference </span><span>Include</span>=<span>"BenchmarkDotNet"</span><span> </span><span>Version</span>=<span>"0.14.0"</span><span> </span>/<span>&gt;</span></p><p><span>&nbsp;&nbsp;</span><span>&lt;</span>/<span>ItemGroup</span><span>&gt;</span></p><p><span>&lt;</span>/<span>Project</span><span>&gt;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
<!-- [Format Time: 0.0003 seconds] -->
<p>Here are the results, which clearly speak for themselves.</p><!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-67124f6522f29360496698" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					<div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p></div>
				</td>
						<td><div><p><span>|</span><span> </span><span>Method</span><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span> </span><span>Runtime</span><span>&nbsp;&nbsp;</span><span>|</span><span> </span><span>Mean</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span> </span><span>Ratio</span><span> </span><span>|</span><span> </span><span>Allocated</span><span> </span><span>|</span></p><p><span>|</span>-----------<span> </span><span>|</span>---------<span> </span><span>|</span>--------------<span>:</span><span>|</span>------<span>:</span><span>|</span>----------<span>:</span><span>|</span></p><p><span>|</span><span> </span><span>LinqCount</span><span>&nbsp;&nbsp;</span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>8.0</span><span> </span><span>|</span><span> </span><span>16</span><span>,</span><span>198.490</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>1.00</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>32</span><span> </span><span>B</span><span> </span><span>|</span></p><p><span>|</span><span> </span><span>LinqCount</span><span>&nbsp;&nbsp;</span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>9.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>3</span><span>,</span><span>043.563</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>0.19</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-<span> </span><span>|</span></p><p><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span></p><p><span>|</span><span> </span><span>LinqAll</span><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>8.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>10.588</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>1.00</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>32</span><span> </span><span>B</span><span> </span><span>|</span></p><p><span>|</span><span> </span><span>LinqAll</span><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>9.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>2.562</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>0.24</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-<span> </span><span>|</span></p><p><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span></p><p><span>|</span><span> </span><span>LinqAny</span><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>8.0</span><span> </span><span>|</span><span> </span><span>17</span><span>,</span><span>096.735</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>1.00</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>32</span><span> </span><span>B</span><span> </span><span>|</span></p><p><span>|</span><span> </span><span>LinqAny</span><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>9.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>2</span><span>,</span><span>483.927</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>0.15</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-<span> </span><span>|</span></p><p><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span></p><p><span>|</span><span> </span><span>LinqFirst</span><span>&nbsp;&nbsp;</span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>8.0</span><span> </span><span>|</span><span> </span><span>15</span><span>,</span><span>289.747</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>1.00</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>32</span><span> </span><span>B</span><span> </span><span>|</span></p><p><span>|</span><span> </span><span>LinqFirst</span><span>&nbsp;&nbsp;</span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>9.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>2</span><span>,</span><span>243.341</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>0.15</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-<span> </span><span>|</span></p><p><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span></p><p><span>|</span><span> </span><span>LinqSingle</span><span> </span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>8.0</span><span> </span><span>|</span><span> </span><span>21</span><span>,</span><span>684.114</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>1.00</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>32</span><span> </span><span>B</span><span> </span><span>|</span></p><p><span>|</span><span> </span><span>LinqSingle</span><span> </span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>9.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>4</span><span>,</span><span>884.329</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>0.23</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-<span> </span><span>|</span></p><p><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span></p><p><span>|</span><span> </span><span>LinqLast</span><span>&nbsp;&nbsp; </span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>8.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>15.967</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>1.00</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-<span> </span><span>|</span></p><p><span>|</span><span> </span><span>LinqLast</span><span>&nbsp;&nbsp; </span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>9.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>6.918</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>0.43</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-<span> </span><span>|</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
<!-- [Format Time: 0.0013 seconds] -->

<h3><span id="The_TryGetSpan_Method"></span>The TryGetSpan() Method<span></span></h3>
<p>In the post <a href="https://blog.ndepend.com/c-array-and-list-fastest-loop" target="_blank" rel="noopener">C# Array and List Fastest Loop</a>, we demonstrated that using a <code>Span&lt;T&gt;</code> for iterating over an array is faster than regular <code>for</code> and <code>foreach</code> loops. In the benchmark above, the performance enhancement is primarily due to the use of the method <code><a href="https://github.com/dotnet/runtime/blob/main/src/libraries/System.Linq/src/System/Linq/Enumerable.cs#L40" target="_blank" rel="noopener">TryGetSpan()</a></code>. If the enumerable being iterated is an array or list, the method <code>TryGetSpan()</code> returns a <code>ReadOnlySpan&lt;T&gt;</code> for faster iteration. Here is the code extracted from <code>TryGetSpan()</code> to test if the <code>source</code> to enumerate is an array or a list, and then to obtain the span from the array or the list.</p><!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-67124f6522f2a289099695" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>&nbsp;&nbsp; </span><span>if</span><span> </span><span>(</span><span>source</span><span>.</span><span>GetType</span><span>(</span><span>)</span><span> </span>==<span> </span><span>typeof</span><span>(</span><span>TSource</span><span>[</span><span>]</span><span>)</span><span>)</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>span</span><span> </span>=<span> </span><span>Unsafe</span><span>.</span><span>As</span><span>&lt;</span><span>TSource</span><span>[</span><span>]</span><span>&gt;</span><span>(</span><span>source</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>}</span><span> </span><span>else</span><span> </span><span>if</span><span> </span><span>(</span><span>source</span><span>.</span><span>GetType</span><span>(</span><span>)</span><span> </span>==<span> </span><span>typeof</span><span>(</span><span>List</span><span>&lt;</span><span>TSource</span><span>&gt;</span><span>)</span><span>)</span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>span</span><span> </span>=<span> </span><span>CollectionsMarshal</span><span>.</span><span>AsSpan</span><span>(</span><span>Unsafe</span><span>.</span><span>As</span><span>&lt;</span><span>List</span><span>&lt;</span><span>TSource</span><span>&gt;</span><span>&gt;</span><span>(</span><span>source</span><span>)</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>}</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
<!-- [Format Time: 0.0003 seconds] -->
<p>To me, this code does not look optimized.</p>
<ul>
<li><code>source.GetType()</code> is called twice!</li>
<li>Why not try to cast <code>source</code> to <code>TSource[]</code> or <code>List&lt;TSource&gt;</code> only once and then test the nullity of the obtained reference and use it?</li>
</ul>
<p>This code was written by Stephen Toub and his team, who are THE .NET performance experts. They have a deep understanding of the C# compiler and JIT compiler optimizations, so it’s clear that this approach is the optimal one. The good news is that you can reuse this code in your own performance-critical code. And there is a lesson: In today’s highly optimized .NET stack, micro-optimizations in code are not obvious at all. Therefore, the advice to avoid premature optimization has never been more relevant.</p>
<p>One final note: <code>List&lt;TSource&gt;</code> internally references an array. When the list’s capacity needs to grow or shrink, a new array is created and then referenced. The call to <code>CollectionsMarshal.AsSpan(Unsafe.As&lt;List&lt;TSource&gt;&gt;(source))</code>&nbsp;retrieves a <code>Span&lt;TSource&gt;</code> from this internal array. Do you see the risk? If the list’s capacity changes somehow, the array obtained through this method might become invalid.</p>
<p>Definitely, the class <code>System.Runtime.CompilerServices.Unsafe</code>&nbsp;is well-named.</p>
<h3><span id="TryGetSpan_Callers"></span>TryGetSpan() Callers<span></span></h3>
<p>Now, let’s examine which methods call <code>TryGetSpan()</code>. Using NDepend, we scanned the assembly located at <code>C:\Program Files\dotnet\shared\Microsoft.NETCore.App\9.0.0-rc.1.24431.7\System.Linq.dll</code>. From the <code>TryGetSpan()</code> method, <a href="https://www.ndepend.com/docs/cqlinq-features#Querying-the-Code-Architecture" target="_blank" rel="noopener">we generated a code query</a> to identify both direct and indirect callers. We then exported the 56 matched methods to the <a href="https://www.ndepend.com/docs/visual-studio-dependency-graph" target="_blank" rel="noopener">dependency graph</a>. This analysis reveals that many standard <code>Enumerable</code> methods attempt to iterate over a span when the collection is an array or a list.</p>
<p>However, since holding the internal array of a list obtained via <code>CollectionsMarshal.AsSpan()</code> is not a safe option (as mentioned earlier), certain <code>Enumerable</code> operations that defer iteration (like when using the <code>yield</code> C# keyword) cannot rely on this optimization.</p>
<p><a href="https://blog.ndepend.com/wp-content/uploads/Call-Graph-To-TryGetSpan.png"><img decoding="async" src="https://blog.ndepend.com/wp-content/uploads/Call-Graph-To-TryGetSpan.png" alt="Call Graph To TryGetSpan" width="1143" height="899" srcset="https://blog.ndepend.com/wp-content/uploads/Call-Graph-To-TryGetSpan.png 1143w, https://blog.ndepend.com/wp-content/uploads/Call-Graph-To-TryGetSpan-300x236.png 300w, https://blog.ndepend.com/wp-content/uploads/Call-Graph-To-TryGetSpan-1024x805.png 1024w, https://blog.ndepend.com/wp-content/uploads/Call-Graph-To-TryGetSpan-768x604.png 768w" sizes="(max-width: 1143px) 100vw, 1143px"></a></p>
<h2><span id="Specialized_Iterators"></span>Specialized Iterators<span></span></h2>
<p>Now let’s run the following benchmark found into this PR: <a href="https://github.com/dotnet/runtime/pull/98969" target="_blank" rel="noopener">Consolidate LINQ’s internal IIListProvider/IPartition into base Iterator class</a></p><!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-67124f6522f32199072912" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					<div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p></div>
				</td>
						<td><div><p><span>using </span><span>Perfolizer</span><span>.</span><span>Horology</span><span>;</span></p><p><span>using </span><span>BenchmarkDotNet</span><span>.</span><span>Configs</span><span>;</span></p><p><span>using </span><span>BenchmarkDotNet</span><span>.</span><span>Running</span><span>;</span></p><p><span>using </span><span>BenchmarkDotNet</span><span>.</span><span>Attributes</span><span>;</span></p><p><span>using </span><span>BenchmarkDotNet</span><span>.</span><span>Jobs</span><span>;</span></p><p><span>BenchmarkRunner</span><span>.</span><span>Run</span><span>&lt;</span><span>Benchmarks</span><span>&gt;</span><span>(</span><span>)</span><span>;</span></p><p><span>[</span><span>MemoryDiagnoser</span><span>]</span></p><p><span>[</span><span>HideColumns</span><span>(</span><span>"StdDev"</span><span>,</span><span> </span><span>"Median"</span><span>,</span><span> </span><span>"Job"</span><span>,</span><span> </span><span>"RatioSD"</span><span>,</span><span> </span><span>"Error"</span><span>,</span><span> </span><span>"Gen0"</span><span>,</span><span> </span><span>"Alloc Ratio"</span><span>)</span><span>]</span></p><p><span>[</span><span>SimpleJob</span><span>(</span><span>RuntimeMoniker</span><span>.</span><span>Net80</span><span>,</span><span> </span><span>baseline</span><span>:</span><span> </span><span>true</span><span>)</span><span>]</span></p><p><span>[</span><span>SimpleJob</span><span>(</span><span>RuntimeMoniker</span><span>.</span><span>Net90</span><span>)</span><span>]</span></p><p><span>public</span><span> </span><span>class</span><span> </span><span>Benchmarks</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp; </span><span>private</span><span> </span><span>IEnumerable</span><span>&lt;</span><span>int</span><span>&gt;</span><span> </span><span>_arrayDistinct</span><span> </span>=<span> </span><span>Enumerable</span><span>.</span><span>Range</span><span>(</span><span>0</span><span>,</span><span> </span><span>1000</span><span>)</span><span>.</span><span>ToArray</span><span>(</span><span>)</span><span>.</span><span>Distinct</span><span>(</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>private</span><span> </span><span>IEnumerable</span><span>&lt;</span><span>int</span><span>&gt;</span><span> </span><span>_appendSelect</span><span> </span>=<span> </span><span>Enumerable</span><span>.</span><span>Range</span><span>(</span><span>0</span><span>,</span><span> </span><span>1000</span><span>)</span><span>.</span><span>ToArray</span><span>(</span><span>)</span><span>.</span><span>Append</span><span>(</span><span>42</span><span>)</span><span>.</span><span>Select</span><span>(</span><span>i</span><span> </span>=<span>&gt;</span><span> </span><span>i</span><span> </span>*<span> </span><span>2</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>private</span><span> </span><span>IEnumerable</span><span>&lt;</span><span>int</span><span>&gt;</span><span> </span><span>_rangeReverse</span><span> </span>=<span> </span><span>Enumerable</span><span>.</span><span>Range</span><span>(</span><span>0</span><span>,</span><span> </span><span>1000</span><span>)</span><span>.</span><span>Reverse</span><span>(</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>private</span><span> </span><span>IEnumerable</span><span>&lt;</span><span>int</span><span>&gt;</span><span> </span><span>_listDefaultIfEmptySelect</span><span> </span>=<span> </span><span>Enumerable</span><span>.</span><span>Range</span><span>(</span><span>0</span><span>,</span><span> </span><span>1000</span><span>)</span><span>.</span><span>ToList</span><span>(</span><span>)</span><span>.</span><span>DefaultIfEmpty</span><span>(</span><span>)</span><span>.</span><span>Select</span><span>(</span><span>i</span><span> </span>=<span>&gt;</span><span> </span><span>i</span><span> </span>*<span> </span><span>2</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>private</span><span> </span><span>IEnumerable</span><span>&lt;</span><span>int</span><span>&gt;</span><span> </span><span>_listSkipTake</span><span> </span>=<span> </span><span>Enumerable</span><span>.</span><span>Range</span><span>(</span><span>0</span><span>,</span><span> </span><span>1000</span><span>)</span><span>.</span><span>ToList</span><span>(</span><span>)</span><span>.</span><span>Skip</span><span>(</span><span>500</span><span>)</span><span>.</span><span>Take</span><span>(</span><span>100</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>private</span><span> </span><span>IEnumerable</span><span>&lt;</span><span>int</span><span>&gt;</span><span> </span><span>_rangeUnion</span><span> </span>=<span> </span><span>Enumerable</span><span>.</span><span>Range</span><span>(</span><span>0</span><span>,</span><span> </span><span>1000</span><span>)</span><span>.</span><span>Union</span><span>(</span><span>Enumerable</span><span>.</span><span>Range</span><span>(</span><span>500</span><span>,</span><span> </span><span>1000</span><span>)</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>private</span><span> </span><span>IEnumerable</span><span>&lt;</span><span>int</span><span>&gt;</span><span> </span><span>_selectWhereSelect</span><span> </span>=<span> </span><span>Enumerable</span><span>.</span><span>Range</span><span>(</span><span>0</span><span>,</span><span> </span><span>1000</span><span>)</span><span>.</span><span>Select</span><span>(</span><span>i</span><span> </span>=<span>&gt;</span><span> </span><span>i</span><span> </span>*<span> </span><span>2</span><span>)</span><span>.</span><span>Where</span><span>(</span><span>i</span><span> </span>=<span>&gt;</span><span> </span><span>i</span><span> </span><span>%</span><span> </span><span>2</span><span> </span>==<span> </span><span>0</span><span>)</span><span>.</span><span>Select</span><span>(</span><span>i</span><span> </span>=<span>&gt;</span><span> </span><span>i</span><span> </span>*<span> </span><span>2</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>[</span><span>Benchmark</span><span>]</span><span> </span><span>public</span><span> </span><span>int</span><span> </span><span>DistinctFirst</span><span>(</span><span>)</span><span> </span>=<span>&gt;</span><span> </span><span>_arrayDistinct</span><span>.</span><span>First</span><span>(</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>[</span><span>Benchmark</span><span>]</span><span> </span><span>public</span><span> </span><span>int</span><span> </span><span>AppendSelectLast</span><span>(</span><span>)</span><span> </span>=<span>&gt;</span><span> </span><span>_appendSelect</span><span>.</span><span>Last</span><span>(</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>[</span><span>Benchmark</span><span>]</span><span> </span><span>public</span><span> </span><span>int</span><span> </span><span>RangeReverseCount</span><span>(</span><span>)</span><span> </span>=<span>&gt;</span><span> </span><span>_rangeReverse</span><span>.</span><span>Count</span><span>(</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>[</span><span>Benchmark</span><span>]</span><span> </span><span>public</span><span> </span><span>int</span><span> </span><span>DefaultIfEmptySelectElementAt</span><span>(</span><span>)</span><span> </span>=<span>&gt;</span><span> </span><span>_listDefaultIfEmptySelect</span><span>.</span><span>ElementAt</span><span>(</span><span>999</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>[</span><span>Benchmark</span><span>]</span><span> </span><span>public</span><span> </span><span>int</span><span> </span><span>ListSkipTakeElementAt</span><span>(</span><span>)</span><span> </span>=<span>&gt;</span><span> </span><span>_listSkipTake</span><span>.</span><span>ElementAt</span><span>(</span><span>99</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>[</span><span>Benchmark</span><span>]</span><span> </span><span>public</span><span> </span><span>int</span><span> </span><span>RangeUnionFirst</span><span>(</span><span>)</span><span> </span>=<span>&gt;</span><span> </span><span>_rangeUnion</span><span>.</span><span>First</span><span>(</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>[</span><span>Benchmark</span><span>]</span><span> </span><span>public</span><span> </span><span>int</span><span> </span><span>SelectWhereSelectSum</span><span>(</span><span>)</span><span> </span>=<span>&gt;</span><span> </span><span>_selectWhereSelect</span><span>.</span><span>Sum</span><span>(</span><span>)</span><span>;</span></p><p><span>}</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
<!-- [Format Time: 0.0013 seconds] -->
<p>The performance improvements are even more remarkable! What caused this?</p><!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-67124f6522f37358715221" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					<div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p></div>
				</td>
						<td><div><p><span>|</span><span> </span><span>Method</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span> </span><span>Runtime</span><span>&nbsp;&nbsp;</span><span>|</span><span> </span><span>Mean</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span> </span><span>Ratio</span><span> </span><span>|</span><span> </span><span>Allocated</span><span> </span><span>|</span></p><p><span>|</span>------------------------------<span> </span><span>|</span>---------<span> </span><span>|</span>-------------<span>:</span><span>|</span>------<span>:</span><span>|</span>----------<span>:</span><span>|</span></p><p><span>|</span><span> </span><span>DistinctFirst</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>8.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>65.318</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>1.00</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>328</span><span> </span><span>B</span><span> </span><span>|</span></p><p><span>|</span><span> </span><span>DistinctFirst</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>9.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>11.192</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>0.17</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-<span> </span><span>|</span></p><p><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span></p><p><span>|</span><span> </span><span>AppendSelectLast</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>8.0</span><span> </span><span>|</span><span> </span><span>4</span><span>,</span><span>122.007</span><span> </span><span>ns</span><span> </span><span>|</span><span> </span><span>1.000</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>144</span><span> </span><span>B</span><span> </span><span>|</span></p><p><span>|</span><span> </span><span>AppendSelectLast</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>9.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>2.661</span><span> </span><span>ns</span><span> </span><span>|</span><span> </span><span>0.001</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-<span> </span><span>|</span></p><p><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span></p><p><span>|</span><span> </span><span>RangeReverseCount</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>8.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>11.024</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>1.00</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-<span> </span><span>|</span></p><p><span>|</span><span> </span><span>RangeReverseCount</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>9.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>6.134</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>0.56</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-<span> </span><span>|</span></p><p><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span></p><p><span>|</span><span> </span><span>DefaultIfEmptySelectElementAt</span><span> </span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>8.0</span><span> </span><span>|</span><span> </span><span>4</span><span>,</span><span>090.818</span><span> </span><span>ns</span><span> </span><span>|</span><span> </span><span>1.000</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>144</span><span> </span><span>B</span><span> </span><span>|</span></p><p><span>|</span><span> </span><span>DefaultIfEmptySelectElementAt</span><span> </span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>9.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>5.724</span><span> </span><span>ns</span><span> </span><span>|</span><span> </span><span>0.001</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-<span> </span><span>|</span></p><p><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span></p><p><span>|</span><span> </span><span>ListSkipTakeElementAt</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>8.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>6.268</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>1.00</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-<span> </span><span>|</span></p><p><span>|</span><span> </span><span>ListSkipTakeElementAt</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>9.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>2.916</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>0.47</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-<span> </span><span>|</span></p><p><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span></p><p><span>|</span><span> </span><span>RangeUnionFirst</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>8.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>66.309</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>1.00</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>344</span><span> </span><span>B</span><span> </span><span>|</span></p><p><span>|</span><span> </span><span>RangeUnionFirst</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>9.0</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>6.193</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>0.09</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-<span> </span><span>|</span></p><p><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>|</span></p><p><span>|</span><span> </span><span>SelectWhereSelectSum</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>8.0</span><span> </span><span>|</span><span> </span><span>3</span><span>,</span><span>959.622</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>1.00</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>112</span><span> </span><span>B</span><span> </span><span>|</span></p><p><span>|</span><span> </span><span>SelectWhereSelectSum</span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>|</span><span> </span><span>.</span><span>NET</span><span> </span><span>9.0</span><span> </span><span>|</span><span> </span><span>4</span><span>,</span><span>460.008</span><span> </span><span>ns</span><span> </span><span>|</span><span>&nbsp;&nbsp;</span><span>1.13</span><span> </span><span>|</span><span>&nbsp;&nbsp;&nbsp;&nbsp; </span><span>112</span><span> </span><span>B</span><span> </span><span>|</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
<!-- [Format Time: 0.0012 seconds] -->

<h3><span id="The_Astute"></span>The Astute<span></span></h3>
<p>In summary, the .NET performance team designed the code to recognize common LINQ call chains. When such a chain is detected, some special iterators are created to handle the workflow more efficiently. Some more optimizations can happen when the chain ends up with methods like <code>Count()</code>, <code>First()</code>, <code>Last()</code>, <code>ElementAt()</code> or <code>Sum()</code>. For instance, <code>OrderBy(criteria).First()</code> can be optimized to execute as <code>Min(criteria)</code>.</p>
<h3><span id="The_implementation_Iterator_and_its_Derived_Class"></span>The implementation: Iterator&lt;T&gt; and its Derived Class<span></span></h3>
<p>Let’s have a look at the abstract base class <a href="https://github.com/dotnet/runtime/blob/main/src/libraries/System.Linq/src/System/Linq/Iterator.SpeedOpt.cs" target="_blank" rel="noopener"><code>Iterator&lt;T&gt;</code></a> and its 40 derivatives. They are all nested in the class <code>Enumerable</code>.</p>
<p><code>Iterator&lt;T&gt;</code> is an abstract class but its methods are virtual. Hence its derivatives only override the required methods.</p>
<p><a href="https://blog.ndepend.com/wp-content/uploads/Iterators-Methods.png"><img decoding="async" src="https://blog.ndepend.com/wp-content/uploads/Iterators-Methods.png" alt="Iterators Methods" width="485" height="582" srcset="https://blog.ndepend.com/wp-content/uploads/Iterators-Methods.png 485w, https://blog.ndepend.com/wp-content/uploads/Iterators-Methods-250x300.png 250w" sizes="(max-width: 485px) 100vw, 485px"></a></p>
<p>Here are the derivatives classes listed and exported to the graph:</p>
<p><a href="https://blog.ndepend.com/wp-content/uploads/Iterators-Derived-2.png"><img loading="lazy" decoding="async" src="https://blog.ndepend.com/wp-content/uploads/Iterators-Derived-2.png" alt="Iterators Derived" width="1486" height="894" srcset="https://blog.ndepend.com/wp-content/uploads/Iterators-Derived-2.png 1486w, https://blog.ndepend.com/wp-content/uploads/Iterators-Derived-2-300x180.png 300w, https://blog.ndepend.com/wp-content/uploads/Iterators-Derived-2-1024x616.png 1024w, https://blog.ndepend.com/wp-content/uploads/Iterators-Derived-2-768x462.png 768w" sizes="(max-width: 1486px) 100vw, 1486px"></a></p>
<h3><span id="Case_Study_ListWhereSelectIterator"></span>Case Study: ListWhereSelectIterator&lt;TSource, TResult&gt;<span></span></h3>
<p>Let’s focus on the iterator <a href="https://github.com/dotnet/runtime/blob/main/src/libraries/System.Linq/src/System/Linq/Where.cs#L303" target="_blank" rel="noopener"><code>ListWhereSelectIterator&lt;TSource, TResult&gt;</code></a>.</p>

<!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-67124f6522f39159396614" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>public</span><span> </span><span>override </span><span>IEnumerable</span><span>&lt;</span><span>TResult</span><span>&gt;</span><span> </span><span>Select</span><span>&lt;</span><span>TResult</span><span>&gt;</span><span>(</span><span>Func</span><span>&lt;</span><span>TSource</span><span>,</span><span> </span><span>TResult</span><span>&gt;</span><span> </span><span>selector</span><span>)</span><span> </span>=<span>&gt;</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>new</span><span> </span><span>ListWhereSelectIterator</span><span>&lt;</span><span>TSource</span><span>,</span><span> </span><span>TResult</span><span>&gt;</span><span>(</span><span>_source</span><span>,</span><span> </span><span>_predicate</span><span>,</span><span> </span><span>selector</span><span>)</span><span>;</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
<!-- [Format Time: 0.0002 seconds] -->
<p><code>ListWhereIterator&lt;TSource, TResult&gt;</code> <a href="https://github.com/dotnet/runtime/blob/main/src/libraries/System.Linq/src/System/Linq/Where.cs#L41" target="_blank" rel="noopener">is instantiated within the <code>Enumerable.Where()</code></a> method using the following code:</p><!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-67124f6522f3c242467875" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>if</span><span> </span><span>(</span><span>source </span><span>is</span><span> </span><span>List</span><span>&lt;</span><span>TSource</span><span>&gt;</span><span> </span><span>list</span><span>)</span><span>{</span></p><p><span>&nbsp;&nbsp;</span><span>return</span><span> </span><span>new</span><span> </span><span>ListWhereIterator</span><span>&lt;</span><span>TSource</span><span>&gt;</span><span>(</span><span>list</span><span>,</span><span> </span><span>predicate</span><span>)</span><span>;</span></p><p><span>}</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
<!-- [Format Time: 0.0001 seconds] -->
<p>The <code>ListWhereSelectIterator&lt;TSource, TResult&gt;</code> doesn’t override methods like <code>TryGetFirst()</code> or <code>TryGetLast()</code>, so how does it improve performance? The key optimization is that it acts as a single iterator for the supercommon <code>Where(...).Select(...)</code> chain on a list, which would typically require two separate iterators. By consolidating both operations into one, it inherently improves efficiency. You can see it in <a href="https://github.com/dotnet/runtime/blob/main/src/libraries/System.Linq/src/System/Linq/Where.cs#L323" target="_blank" rel="noopener">its implementation of </a><code>MoveNext()</code>&nbsp;where both delegates <code>_predicate</code> and <code>_selector</code> are invoked:</p><!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-67124f6522f3d877907597" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>while</span><span> </span><span>(</span><span>_enumerator</span><span>.</span><span>MoveNext</span><span>(</span><span>)</span><span>)</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp; </span><span>TSource </span><span>item</span><span> </span>=<span> </span><span>_enumerator</span><span>.</span><span>Current</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>if</span><span> </span><span>(</span><span>_predicate</span><span>(</span><span>item</span><span>)</span><span>)</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>_current</span><span> </span>=<span> </span><span>_selector</span><span>(</span><span>item</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>return</span><span> </span><span>true</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>}</span></p><p><span>}</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
<!-- [Format Time: 0.0002 seconds] -->

<h3><span id="Case_Study_IListSkipTakeIterator"></span>Case Study: IListSkipTakeIterator&lt;TSource&gt;<span></span></h3>
<p>Here is the implementation of <code>MoveNext()</code> in the <code>IListSkipTakeIterator&lt;TSource&gt;</code> class:</p><!-- Crayon Syntax Highlighter v_2.7.2_beta -->

		<div id="crayon-67124f6522f40135409288" data-settings=" minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="show">
					
				</td>
						<td><div><p><span>public</span><span> </span><span>override </span><span>bool</span><span> </span><span>MoveNext</span><span>(</span><span>)</span><span> </span><span>{</span></p><p><span>&nbsp;&nbsp; </span><span>// _state - 1 represents the zero-based index into the list.</span></p><p><span>&nbsp;&nbsp; </span><span>// Having a separate field for the index would be more readable. However, we save it</span></p><p><span>&nbsp;&nbsp; </span><span>// into _state with a bias to minimize field size of the iterator.</span></p><p><span>&nbsp;&nbsp; </span><span>int</span><span> </span><span>index</span><span> </span>=<span> </span><span>_state</span><span> </span>-<span> </span><span>1</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>if</span><span> </span><span>(</span><span>(</span><span>uint</span><span>)</span><span>index</span><span> </span><span>&lt;</span>=<span> </span><span>(</span><span>uint</span><span>)</span><span>(</span><span>_maxIndexInclusive</span><span> </span>-<span> </span><span>_minIndexInclusive</span><span>)</span><span> </span><span>&amp;&amp; index &lt; _source.Count - _minIndexInclusive) {</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_current = _source[_minIndexInclusive + index];</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>++<span>_state</span><span>;</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>return</span><span> </span><span>true</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>}</span></p><p><span>&nbsp;&nbsp; </span><span>Dispose</span><span>(</span><span>)</span><span>;</span></p><p><span>&nbsp;&nbsp; </span><span>return</span><span> </span><span>false</span><span>;</span></p><p><span>}</span></p></div></td>
					</tr>
				</tbody></table>
			</div>
<!-- [Format Time: 0.0002 seconds] -->


<h2><span id="Conclusion"></span>Conclusion<span></span></h2>
<p>With .NET 9, LINQ becomes faster in several common scenarios. As with every new version of .NET, you simply need to migrate and recompile to take advantage of these improvements. Additionally, LINQ has been optimized in other ways: SIMD is utilized whenever possible, such as when summing a sequence of integers. Moreover, enumerating empty sequences incurs lower costs due to early detection.</p>

<p>One final note: the web is increasingly inundated with AI-generated crap content. Search engines struggles to differentiate between valuable, handcrafted content and inferior material. If you appreciate this article and others like it that are thoughtfully created, please consider sharing it.</p>


	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft BitNet: inference framework for 1-bit LLMs (123 pts)]]></title>
            <link>https://github.com/microsoft/BitNet</link>
            <guid>41877609</guid>
            <pubDate>Fri, 18 Oct 2024 09:10:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/BitNet">https://github.com/microsoft/BitNet</a>, See on <a href="https://news.ycombinator.com/item?id=41877609">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">bitnet.cpp</h2><a id="user-content-bitnetcpp" aria-label="Permalink: bitnet.cpp" href="#bitnetcpp"></a></p>
<p dir="auto"><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6581c31c16c1b13ddc2efb92e2ad69a93ddc4a92fd871ff15d401c4c6c9155a4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/license-MIT-blue.svg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/93172b08c863df96ed7d00e798caaddd15891f35d34596605dad69bf0ebf5d04/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f76657273696f6e2d312e302d626c7565"><img src="https://camo.githubusercontent.com/93172b08c863df96ed7d00e798caaddd15891f35d34596605dad69bf0ebf5d04/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f76657273696f6e2d312e302d626c7565" alt="version" data-canonical-src="https://img.shields.io/badge/version-1.0-blue"></a></p>
<p dir="auto">bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support <strong>fast</strong> and <strong>lossless</strong> inference of 1.58-bit models on CPU (with NPU and GPU support coming next).</p>
<p dir="auto">The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of <strong>1.37x</strong> to <strong>5.07x</strong> on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by <strong>55.4%</strong> to <strong>70.0%</strong>, further boosting overall efficiency. On x86 CPUs, speedups range from <strong>2.37x</strong> to <strong>6.17x</strong> with energy reductions between <strong>71.9%</strong> to <strong>82.2%</strong>. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. More details will be provided soon.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/BitNet/blob/main/assets/m2_performance.jpg"><img src="https://github.com/microsoft/BitNet/raw/main/assets/m2_performance.jpg" alt="m2_performance" width="800"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/BitNet/blob/main/assets/intel_performance.jpg"><img src="https://github.com/microsoft/BitNet/raw/main/assets/intel_performance.jpg" alt="m2_performance" width="800"></a></p>
<blockquote>
<p dir="auto">The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<p dir="auto">A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:</p>
<details open="">
  <summary>
    
    <span aria-label="Video description demo.mp4">demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/54800242/377447164-7f46b736-edec-4828-b809-4be780a3e5b1.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjkyODczMDMsIm5iZiI6MTcyOTI4NzAwMywicGF0aCI6Ii81NDgwMDI0Mi8zNzc0NDcxNjQtN2Y0NmI3MzYtZWRlYy00ODI4LWI4MDktNGJlNzgwYTNlNWIxLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMTglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDE4VDIxMzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk4MmI0Mjk3NzVjODAwYzk3MzE5NDMxMTFjZDQ5ZjA4YzMxYTMxZjlhZjUwNGQ2NDM4ODgxNjZkNWUyNTQ3OWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.KxQAhjc6QrNI2Lr5tOXrslTJ9fl_x8bjFWUsXSj8WFk" data-canonical-src="https://private-user-images.githubusercontent.com/54800242/377447164-7f46b736-edec-4828-b809-4be780a3e5b1.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjkyODczMDMsIm5iZiI6MTcyOTI4NzAwMywicGF0aCI6Ii81NDgwMDI0Mi8zNzc0NDcxNjQtN2Y0NmI3MzYtZWRlYy00ODI4LWI4MDktNGJlNzgwYTNlNWIxLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMTglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDE4VDIxMzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk4MmI0Mjk3NzVjODAwYzk3MzE5NDMxMTFjZDQ5ZjA4YzMxYTMxZjlhZjUwNGQ2NDM4ODgxNjZkNWUyNTQ3OWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.KxQAhjc6QrNI2Lr5tOXrslTJ9fl_x8bjFWUsXSj8WFk" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Timeline</h2><a id="user-content-timeline" aria-label="Permalink: Timeline" href="#timeline"></a></p>
<ul dir="auto">
<li>10/17/2024 bitnet.cpp 1.0 released.</li>
<li>02/27/2024 <a href="https://arxiv.org/abs/2402.17764" rel="nofollow">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a></li>
<li>10/17/2023 <a href="https://arxiv.org/abs/2310.11453" rel="nofollow">BitNet: Scaling 1-bit Transformers for Large Language Models</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Models</h2><a id="user-content-supported-models" aria-label="Permalink: Supported Models" href="#supported-models"></a></p>
<p dir="auto">bitnet.cpp supports a list of 1-bit models available on <a href="https://huggingface.co/" rel="nofollow">Hugging Face</a>, which are trained with research settings. We hope the release of bitnet.cpp can inspire more 1-bit LLMs trained in large-scale settings.</p>
<markdown-accessiblity-table><table>
    
    <tbody><tr>
        <th rowspan="2">Model</th>
        <th rowspan="2">Parameters</th>
        <th rowspan="2">CPU</th>
        <th colspan="3">Kernel</th>
    </tr>
    <tr>
        <th>I2_S</th>
        <th>TL1</th>
        <th>TL2</th>
    </tr>
    <tr>
        <td rowspan="2"><a href="https://huggingface.co/1bitLLM/bitnet_b1_58-large" rel="nofollow">bitnet_b1_58-large</a></td>
        <td rowspan="2">0.7B</td>
        <td>x86</td>
        <td>✔</td>
        <td>✘</td>
        <td>✔</td>
    </tr>
    <tr>
        <td>ARM</td>
        <td>✔</td>
        <td>✔</td>
        <td>✘</td>
    </tr>
    <tr>
        <td rowspan="2"><a href="https://huggingface.co/1bitLLM/bitnet_b1_58-3B" rel="nofollow">bitnet_b1_58-3B</a></td>
        <td rowspan="2">3.3B</td>
        <td>x86</td>
        <td>✘</td>
        <td>✘</td>
        <td>✔</td>
    </tr>
    <tr>
        <td>ARM</td>
        <td>✘</td>
        <td>✔</td>
        <td>✘</td>
    </tr>
    <tr>
        <td rowspan="2"><a href="https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens" rel="nofollow">Llama3-8B-1.58-100B-tokens</a></td>
        <td rowspan="2">8.0B</td>
        <td>x86</td>
        <td>✔</td>
        <td>✘</td>
        <td>✔</td>
    </tr>
    <tr>
        <td>ARM</td>
        <td>✔</td>
        <td>✔</td>
        <td>✘</td>
    </tr>
</tbody></table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Requirements</h3><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>python&gt;=3.9</li>
<li>cmake&gt;=3.22</li>
<li>clang&gt;=18
<ul dir="auto">
<li>
<p dir="auto">For Windows users, install <a href="https://visualstudio.microsoft.com/downloads/" rel="nofollow">Visual Studio 2022</a>. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):</p>
<ul dir="auto">
<li>Desktop-development with C++</li>
<li>C++-CMake Tools for Windows</li>
<li>Git for Windows</li>
<li>C++-Clang Compiler for Windows</li>
<li>MS-Build Support for LLVM-Toolset (clang)</li>
</ul>
</li>
<li>
<p dir="auto">For Debian/Ubuntu users, you can download with <a href="https://apt.llvm.org/" rel="nofollow">Automatic installation script</a></p>
<p dir="auto"><code> bash -c "$(wget -O - https://apt.llvm.org/llvm.sh)"</code></p>
</li>
</ul>
</li>
<li>conda (highly recommend)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build from source</h3><a id="user-content-build-from-source" aria-label="Permalink: Build from source" href="#build-from-source"></a></p>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands</p>
</div>
<ol dir="auto">
<li>Clone the repo</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet"><pre>git clone --recursive https://github.com/microsoft/BitNet.git
<span>cd</span> BitNet</pre></div>
<ol start="2" dir="auto">
<li>Install the dependencies</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt"><pre><span><span>#</span> (Recommended) Create a new conda environment</span>
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt</pre></div>
<ol start="3" dir="auto">
<li>Build the project</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Download the model from Hugging Face, convert it to quantized gguf format, and build the project
python setup_env.py --hf-repo HF1BitLLM/Llama3-8B-1.58-100B-tokens -q i2_s

# Or you can manually download the model and run with local path
huggingface-cli download HF1BitLLM/Llama3-8B-1.58-100B-tokens --local-dir models/Llama3-8B-1.58-100B-tokens
python setup_env.py -md models/Llama3-8B-1.58-100B-tokens -q i2_s"><pre><span><span>#</span> Download the model from Hugging Face, convert it to quantized gguf format, and build the project</span>
python setup_env.py --hf-repo HF1BitLLM/Llama3-8B-1.58-100B-tokens -q i2_s

<span><span>#</span> Or you can manually download the model and run with local path</span>
huggingface-cli download HF1BitLLM/Llama3-8B-1.58-100B-tokens --local-dir models/Llama3-8B-1.58-100B-tokens
python setup_env.py -md models/Llama3-8B-1.58-100B-tokens -q i2_s</pre></div>
<pre>usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
</pre>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic usage</h3><a id="user-content-basic-usage" aria-label="Permalink: Basic usage" href="#basic-usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run inference with the quantized model
python run_inference.py -m models/Llama3-8B-1.58-100B-tokens/ggml-model-i2_s.gguf -p &quot;Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?\nAnswer:&quot; -n 6 -temp 0

# Output:
# Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?
# Answer: Mary is in the garden.
"><pre><span><span>#</span> Run inference with the quantized model</span>
python run_inference.py -m models/Llama3-8B-1.58-100B-tokens/ggml-model-i2_s.gguf -p <span><span>"</span>Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?\nAnswer:<span>"</span></span> -n 6 -temp 0

<span><span>#</span> Output:</span>
<span><span>#</span> Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?</span>
<span><span>#</span> Answer: Mary is in the garden.</span>
</pre></div>
<pre>usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
</pre>
<p dir="auto"><h3 tabindex="-1" dir="auto">Benchmark</h3><a id="user-content-benchmark" aria-label="Permalink: Benchmark" href="#benchmark"></a></p>
<p dir="auto">We provide scripts to run the inference benchmark providing a model.</p>
<div data-snippet-clipboard-copy-content="usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. "><pre><code>usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
</code></pre></div>
<p dir="auto">Here's a brief explanation of each argument:</p>
<ul dir="auto">
<li><code>-m</code>, <code>--model</code>: The path to the model file. This is a required argument that must be provided when running the script.</li>
<li><code>-n</code>, <code>--n-token</code>: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.</li>
<li><code>-p</code>, <code>--n-prompt</code>: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.</li>
<li><code>-t</code>, <code>--threads</code>: The number of threads to use for running the inference. It is an optional argument with a default value of 2.</li>
<li><code>-h</code>, <code>--help</code>: Show the help message and exit. Use this argument to display usage information.</li>
</ul>
<p dir="auto">For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  "><pre>python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  </pre></div>
<p dir="auto">This command would run the inference benchmark using the model located at <code>/path/to/model</code>, generating 200 tokens from a 256 token prompt, utilizing 4 threads.</p>
<p dir="auto">For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128"><pre>python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

<span><span>#</span> Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate</span>
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">This project is based on the <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> framework. We would like to thank all the authors for their contributions to the open-source community. We also thank <a href="https://github.com/microsoft/T-MAC/">T-MAC</a> team for the helpful discussion on the LUT method for low-bit LLM inference.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Secret 3D Scans in the French Supreme Court (631 pts)]]></title>
            <link>https://cosmowenman.substack.com/p/secret-3d-scans-in-the-french-supreme</link>
            <guid>41877513</guid>
            <pubDate>Fri, 18 Oct 2024 08:50:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cosmowenman.substack.com/p/secret-3d-scans-in-the-french-supreme">https://cosmowenman.substack.com/p/secret-3d-scans-in-the-french-supreme</a>, See on <a href="https://news.ycombinator.com/item?id=41877513">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>For the last seven years, I have been campaigning to establish and defend the public’s right to access all French national museums’ 3D scans of their collections, starting with the prestigious and influential Rodin Museum in Paris. My efforts have reached the Conseil d’État, France’s supreme court for administrative justice, and a great deal is at stake. The museum and the French Ministry of Culture appear determined to lie, break the law, defy court orders, and set fire to French freedom of information law in order to prevent the public from accessing 3D scans of cultural heritage.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe7cb9ed-6d15-46b9-8a12-0b046143e5a2_660x418.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe7cb9ed-6d15-46b9-8a12-0b046143e5a2_660x418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe7cb9ed-6d15-46b9-8a12-0b046143e5a2_660x418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe7cb9ed-6d15-46b9-8a12-0b046143e5a2_660x418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe7cb9ed-6d15-46b9-8a12-0b046143e5a2_660x418.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe7cb9ed-6d15-46b9-8a12-0b046143e5a2_660x418.png" width="726" height="459.8" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/be7cb9ed-6d15-46b9-8a12-0b046143e5a2_660x418.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;normal&quot;,&quot;height&quot;:418,&quot;width&quot;:660,&quot;resizeWidth&quot;:726,&quot;bytes&quot;:723521,&quot;alt&quot;:&quot;Hands touching the face of a bronze statue&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Hands touching the face of a bronze statue" title="Hands touching the face of a bronze statue" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe7cb9ed-6d15-46b9-8a12-0b046143e5a2_660x418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe7cb9ed-6d15-46b9-8a12-0b046143e5a2_660x418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe7cb9ed-6d15-46b9-8a12-0b046143e5a2_660x418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe7cb9ed-6d15-46b9-8a12-0b046143e5a2_660x418.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Many museums around the world make high-quality 3D scans of important artwork and ancient artifacts in their collections. Several forward-thinking organizations freely share their 3D scans, allowing the public to view, copy, adapt, and experiment with the underlying works in ways that have never before been possible.</p><p><span>Anyone in the world with an internet connection can view, interact with, and download the </span><a href="https://sketchfab.com/3d-models/the-rosetta-stone-1e03509704a3490e99a173e53b93e282" rel="nofollow ugc noopener">British Museum’s</a><span> 3D scan of the Rosetta Stone, for example. The public can freely access hundreds of scans of classical sculpture from the </span><a href="https://open.smk.dk/en/art?q=*&amp;page=2&amp;filters=has_3d_file%3Atrue" rel="nofollow ugc noopener">National Gallery of Denmark</a><span>, and visitors to the </span><a href="https://3d.si.edu/explore" rel="nofollow ugc noopener">Smithsonian’</a><span>s website can view, navigate, and freely download thousands of high-quality scans of artifacts ranging from dinosaur fossils to the Apollo 11 space capsule.</span></p><p>With access to digitizations like these, artists can rework and incorporate our common cultural heritage into new works, such as films, video games, virtual reality, clothing, architecture, sculpture, and more. Researchers and educators can use 3D scans to further our understanding of the arts, and the art-loving public can use them to appreciate, study, and even replicate beloved works in new ways that are not possible within the confines of a museum or with the original works.</p><p><span>For example, I </span><a href="https://www.conceptrealizations.com/" rel="nofollow ugc noopener">design and fabricate</a><span> universal access wayfinding tools, interactive replicas and exhibits for the blind. I know that there is need for unfettered access to 3D scans of important sculptures and artifacts for use in making replicas the public can explore through touch.</span></p><p>If set loose to run free in our digital, visual, and tactile landscape, the creative potential for cultural heritage 3D scans is limitless, and the value of what the general public can do with them vastly exceeds what museums could ever create if they kept their digitizations for themselves.</p><p><span>Unfortunately, some ostensibly public-spirited organizations do keep their 3D scans hidden. I’ve been trying to help them see the light. Beginning in 2017 I spent three years using German freedom of information law to </span><a href="https://reason.com/2019/11/13/a-german-museum-tried-to-hide-this-stunning-3d-scan-of-an-iconic-egyptian-artifact-today-you-can-see-it-for-the-first-time/" rel="nofollow ugc noopener">successfully pressure</a><span> the Egyptian Museum of Berlin to release its 3D scan of its most prized possession and national treasure, the 3,000 year-old </span><em>Bust of Nefertiti.</em><span> Since then I’ve turned my attention to the digital treasures being hoarded by taxpayer funded institutions in France.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b137ce5-bfd4-4f47-bef9-8e62fca364d3_1015x524.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b137ce5-bfd4-4f47-bef9-8e62fca364d3_1015x524.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b137ce5-bfd4-4f47-bef9-8e62fca364d3_1015x524.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b137ce5-bfd4-4f47-bef9-8e62fca364d3_1015x524.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b137ce5-bfd4-4f47-bef9-8e62fca364d3_1015x524.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b137ce5-bfd4-4f47-bef9-8e62fca364d3_1015x524.png" width="1015" height="524" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3b137ce5-bfd4-4f47-bef9-8e62fca364d3_1015x524.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:524,&quot;width&quot;:1015,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:814486,&quot;alt&quot;:&quot;Winged Victory and Venus de Milo being 3D scanned at the Louvre.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Winged Victory and Venus de Milo being 3D scanned at the Louvre." title="Winged Victory and Venus de Milo being 3D scanned at the Louvre." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b137ce5-bfd4-4f47-bef9-8e62fca364d3_1015x524.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b137ce5-bfd4-4f47-bef9-8e62fca364d3_1015x524.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b137ce5-bfd4-4f47-bef9-8e62fca364d3_1015x524.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b137ce5-bfd4-4f47-bef9-8e62fca364d3_1015x524.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Winged Victory</em><span> and </span><em>Venus de Milo</em><span> being scanned at the Louvre. Photos via Art Graphique &amp; Patrimoine and RMN.</span></figcaption></figure></div><p><span>The Louvre, for example, will not allow the public to access its ultra-high quality 3D scan of </span><em><a href="https://focus.louvre.fr/fr/la-victoire-de-samothrace/la-victoire-de-samothrace/le-monument-de-la-victoire-de-samothrace-0" rel="nofollow ugc noopener">Winged Victory, the Nike of Samothrace</a><span>,</span></em><span> despite its aggressive public and corporate fundraising campaign to digitize the iconic Greek sculpture. Nor its scan of </span><em>Venus de Milo.</em></p><p><span>The French Ministry of Culture’s </span><em>Réunion des musées nationaux</em><span> (RMN) receives tens of millions of dollars anually in public subsidies to provide services to French national museums. In 2013 RMN received from the </span><em>Fonds national pour la société numérique</em><span> (FSN) a €1.1M subsidy and an additional loan of €1.1M to digitize objects in French museum collections and create a web platform for the publication and economic exploitation of its 3D scans. Since then RMN has 3D scanned thousands of artworks and ancient artifacts all over France. RMN </span><a href="https://sketchfab.com/francecollections/models" rel="nofollow ugc noopener">advertises</a><span> its scans’ availability to the public, which makes for great PR, but its ads </span><a href="https://cosmowenman.substack.com/p/reunion-des-musees-nationaux-lies" rel="nofollow ugc noopener">are false</a><span>. In fact, RMN has a strict look-but-don’t-touch policy for its 3D scans and absolutely refuses to allow the public to access them directly. My own investigation has revealed that, in private, RMN </span><a href="https://cosmowenman.files.wordpress.com/2024/04/20221122-correspondance-rmn-cada-si604822112216300-occlus-par-la-cada_fr_en_text.pdf" rel="nofollow ugc noopener">admits</a><span> it won’t release its scans because it wants to protect its gift shops’ sales revenue from competition from the public making their own replicas. For practical applications and creative potential, and direct value to the public, it is as though these scans simply do not exist.</span></p><p><span>And then there is the Rodin Museum. Founded in 1917 shortly after the death of famed sculptor Auguste Rodin, </span><em>le musée Rodin</em><span> is a state-run administrative agency and an arm of the Ministry of Culture. It has a legally mandated mission to preserve, study, enhance and disseminate Rodin’s works, all of which have been in the public domain since their copyrights expired decades ago. Even though musée Rodin never passes up an opportunity to remind the public that it is France’s sole “</span><a href="https://www.musee-rodin.fr/en/museum/institution/self-funding-museum" rel="nofollow ugc noopener">self-funded</a><span>” national museum, it sought and obtained direct public funding from the Ministry of Culture’s national digitization program, and in 2010 as part of its public service mission began 3D scanning its collection with the stated purpose of publishing the results.</span></p><p>Fourteen years later, musée Rodin’s scans have not been shared with the public.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5bc5eee-0321-436d-92ee-399bcd1dc752_723x1080.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5bc5eee-0321-436d-92ee-399bcd1dc752_723x1080.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5bc5eee-0321-436d-92ee-399bcd1dc752_723x1080.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5bc5eee-0321-436d-92ee-399bcd1dc752_723x1080.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5bc5eee-0321-436d-92ee-399bcd1dc752_723x1080.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5bc5eee-0321-436d-92ee-399bcd1dc752_723x1080.png" width="723" height="1080" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f5bc5eee-0321-436d-92ee-399bcd1dc752_723x1080.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1080,&quot;width&quot;:723,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1321798,&quot;alt&quot;:&quot;The Gates of Hell being scanned at musée Rodin. Photo by Jean-Pierre Dalbéra.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="The Gates of Hell being scanned at musée Rodin. Photo by Jean-Pierre Dalbéra." title="The Gates of Hell being scanned at musée Rodin. Photo by Jean-Pierre Dalbéra." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5bc5eee-0321-436d-92ee-399bcd1dc752_723x1080.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5bc5eee-0321-436d-92ee-399bcd1dc752_723x1080.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5bc5eee-0321-436d-92ee-399bcd1dc752_723x1080.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5bc5eee-0321-436d-92ee-399bcd1dc752_723x1080.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>The Gates of Hell</em><span> being 3D scanned at musée Rodin. Photo by Jean-Pierre Dalbéra.</span></figcaption></figure></div><p>When I approach cultural heritage organizations about their 3D scans of public domain works, my ultimate goal is to secure unfettered public access&nbsp;to them. I’m much less interested in half measures, whereby the public might be granted access to scans only for, say, educational purposes, but be improperly prohibited from using them commercially. Those kinds of compromises undermine the public’s right to reuse public domain works and can lead to people hesitating to use them in any way because they are unsure of their rights and fear unknown punishments.</p><p>In 2017, I approached musée Rodin with a strategy to illicit a full airing of every possible argument that a prestigious, well-positioned public institution might bring to bear against a member of the public interested in accessing its 3D scans. I wanted to discover and deal with all their concerns at once. So, from the start I made it perfectly clear to musée Rodin that I had a commercial interest in scans of Rodin’s works, and I made absolutely no apologies for it. From there I asked its administrators straightforward questions about the museum’s policies and let them treat me as they might treat any other inquiring member of the public. It did not go well.</p><p>I expected resistance, but I did not anticipate the extent to which the museum would abuse its authority.</p><p><span>After more than a year of musée Rodin’s administrators ignoring my requests for information about its 3D scans and public access-related policies, I decided to escalate. I asked for help from Paris-based civil rights attorney and strategic litigation expert </span><a href="https://afocavocat.eu/" rel="nofollow ugc noopener">Alexis Fitzjean Ó Cobhthaigh</a><span>, who made a formal demand on my behalf under French freedom of information law, which requires government agencies to communicate their administrative documents to the public. We requested copies of the museum’s policy-related documents, a list of the 3D scan files it held, </span><em>and the scan files themselves.</em></p><p>When musée Rodin refused to provide records of any kind we referred its refusal to the Commission on Access to Administrative Documents (CADA), the independent French government agency that administrations and courts rely on for its legal analysis of questions relating to public access and reuse of government documents. The CADA had never before considered any dispute about 3D scans. It affirmed my request that musée Rodin communicate copies of its scans to me, determining for the first time that public agencies’ 3D scans are in fact administrative documents and by law must be made available to the public.</p><p><span>In light of the government’s own </span><a href="https://cosmowenman.com/wp-content/uploads/2024/04/20190606-cada-avis-n20192300-supports-wenman_fr_en.pdf" rel="nofollow ugc noopener">legal analysis</a><span>, musée Rodin confided to the ministry — </span><a href="https://cosmowenman.com/wp-content/uploads/2024/04/20190426-musee-rodin-warns-requests-guidance-from-ministry-of-culture_fr_en.pdf" rel="nofollow ugc noopener">in writing</a><span> — its plan to disobey the law and its fear that I would bring it to court and make its position known to the public.</span></p><p><span>In 2019 we filed suit against the museum in the Administrative Tribunal of Paris, asking the court to anul the museum’s refusals and order it to make its scans public. Open culture and digital rights advocacy organizations </span><a href="https://communia-association.org/" rel="nofollow ugc noopener">Communia</a><span>, </span><a href="https://www.laquadrature.net/" rel="nofollow ugc noopener">La Quadrature du Net</a><span> and </span><a href="https://www.wikimedia.fr/" rel="nofollow ugc noopener">Wikimédia France</a><span> joined me as co-plaintiffs in support of our case. We were all represented by Fitzjean Ó Cobhthaigh.</span></p><p><span>After more than three years of litigation and musée Rodin’s desperate efforts to evade the law, in April 2023 the Administrative Tribunal of Paris issued a historic, precedent-setting decision, ordering the prestigious museum to make several of its 3D scans of some of the world’s most famous sculptures accessible to the public including </span><em>The Thinker, The Kiss, </em><span>and</span><em> The Gates of Hell. </em><span>The Administrative Tribunal’s April 21, 2023 decision is available </span><a href="https://cosmowenman.files.wordpress.com/2024/03/20230421-cosmo-wenman-vs-musee-rodin-decision-administrative-tribunal-of-paris_fr_en.pdf" rel="nofollow ugc noopener">here</a><span>.</span></p><p>Our victory has broad implications for public access to and reuse of digitizations of important cultural heritage works throughout France and the world.</p><p>Naturally, we wanted to publicize the court’s 2023 decision at that time, but for strategic reasons remained silent because our victory was not complete. Despite the important rulings affirming the public’s right to access 3D scans, the tribunal also ruled against us on several related issues with reasoning that, if left unchallenged, threatens to broadly undermine French freedom of information law as well as permit the government to conceal a specific type of important document from the public — more on that in a moment.</p><p><span>When the tribunal issued its decision, Fitzjean Ó Cobhthaigh and I remained silent because we did not want to give the government advance notice that we intended to contest the flawed portions of the decision. We also wanted to quietly observe as musée Rodin’s deadline passed without it initiating an appeal of its own. In the months following the tribunal’s April 2023 decision, Fitzjean and I spent countless hours quietly researching, gathering expert testimony, and preparing an analysis and presentation of an appeal challenging the lower court’s legal and procedural errors. We engaged Fitzjean’s colleagues at the law firm </span><a href="https://www.scp-mdlb.fr/" rel="nofollow ugc noopener">SCP Marlange - de La Burgade</a><span> — specialists in appeals to the Conseil d’État, France’s supreme court for administrative justice — for assistance and to present our case to the court. On December 1, 2023 we submitted our brief to the Conseil d’État for its preliminary review and its decision on the admissability of our appeal and the need for a hearing.</span></p><p><span>From the beginning of this seven-year effort, through today, musée Rodin has refused to give any comment to the press about its secret access policies or legal defense. In effect, the museum’s administrators have quietly transferred their press communications, policy decisions, and legal responsibilities</span><em> to me</em><span>, my attorney and co-plaintiffs, and the court, while making every effort to evade the law. But behind the scenes, in court, and when communicating with other arms of the government, however, musée Rodin has been relentlessly mendacious, lying to the Ministry of Culture, the government, the court, and the public it is meant to serve.</span></p><p>Now that we have challenged the lower court’s errors and await a response from the Ministry of Culture and its proxy, musée Rodin, I can finally give a full account of our important achievements in the tribunal and expose the museum’s deceptive tactics, all of which should be of interest to policymakers and professionals in the cultural heritage sector and the art market, open access advocates, as well as archivists, educators, and the art-loving public.</p><p>I can also now tell the public about the tribunal’s errors and lack of technical understanding and the danger they pose. What is currently at stake in my appeal to the Conseil d’État should be of particular interest to scientists, researchers, citizen activists, investigative journalists, and government watchdog organizations.</p><p>We had to fight the museum’s constant evasions, delays, deceptions, and lawless special pleading for each and every legal question we won. I recount below the three judges’ findings and their effects, some of musée Rodin’s failed legal and extralegal maneuvers, and how we countered them. I hope this summary will be helpful to others’ efforts to access documents of any kind from similarly uncooperative and hostile administrative agencies.</p><ul><li><p><em><strong>Moral rights: rejected</strong></em></p></li></ul><p><span>We definitively destroyed </span><em>droit moral</em><span> (moral rights) as a legal barrier to public access and reuse of 3D scans of public domain cultural heritage works. This is an incredible achievement in itself; no one in the world has a stronger claim to the legal protections and privileges of </span><em>droit moral</em><span> than musée Rodin, one of the most active, powerful, recognizable, and prestigious public domain artist’s estates in the world. The museum constantly reminded the court that it is the legal, state-appointed beneficiary of Auguste Rodin’s estate and perpetual moral rights, which include the right to attribution and the right to protect the integrity of an author’s works. For dramatic effect, musée Rodin submitted into evidence Rodin’s handwritten 1916 donation of his estate to the French state. Yet the court correctly considered </span><em>droit moral</em><span> to be utterly irrelevant to the public’s right to access and reuse works whose copyrights have expired and which have entered the public domain. This element of the court’s decision undercuts countless organizations’ hesitancy and dithering on public access to public domain works, sometimes vaguely justified as deference to artists’ estates and poorly understood, almost mystical claims of </span><em>droit moral. </em><span>Institutions like the Baltimore Museum of Art, for example, which has been too timid to publish </span><a href="https://web.archive.org/web/20140806033239/http:/blog.artbma.org/2014/07/3d-scanning-auguste-rodins-the-thinker/" rel="nofollow ugc noopener">its own 3D scan</a><span> of </span><em>The Thinker</em><span> due to unfounded fear of somehow violating musée Rodin’s moral rights, should take note.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d6db5d-d9bc-4d5c-8e28-df33d2ce343a_638x576.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d6db5d-d9bc-4d5c-8e28-df33d2ce343a_638x576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d6db5d-d9bc-4d5c-8e28-df33d2ce343a_638x576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d6db5d-d9bc-4d5c-8e28-df33d2ce343a_638x576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d6db5d-d9bc-4d5c-8e28-df33d2ce343a_638x576.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d6db5d-d9bc-4d5c-8e28-df33d2ce343a_638x576.png" width="638" height="576" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/19d6db5d-d9bc-4d5c-8e28-df33d2ce343a_638x576.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:576,&quot;width&quot;:638,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:337670,&quot;alt&quot;:&quot;The Baltimore Museum of Art’s unpublished 3D scan of The Thinker. Image by Direct Dimensions.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="The Baltimore Museum of Art’s unpublished 3D scan of The Thinker. Image by Direct Dimensions." title="The Baltimore Museum of Art’s unpublished 3D scan of The Thinker. Image by Direct Dimensions." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d6db5d-d9bc-4d5c-8e28-df33d2ce343a_638x576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d6db5d-d9bc-4d5c-8e28-df33d2ce343a_638x576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d6db5d-d9bc-4d5c-8e28-df33d2ce343a_638x576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d6db5d-d9bc-4d5c-8e28-df33d2ce343a_638x576.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>The Baltimore Museum of Art’s unpublished 3D scan of </span><em>The Thinker.</em><span> Image by Direct Dimensions.</span></figcaption></figure></div><ul><li><p><em><strong>Fears over competition, counterfeiting, and sales revenue: rejected</strong></em></p></li></ul><p><span>Musée Rodin argued that public access to its 3D scans would have "conséquences désastreuses" for all French national museums by subjecting their gift shop and artwork sales revenue to unfair competition from the public, and that their scans would facilitate criminal counterfeiting. The court ruled that the museum’s revenue, business model, and supposed threats from competition and counterfeiting are irrelevant to the public’s right to access its scans, a dramatic rejection of the museum’s position that makes the public’s right to access and reuse public domain works crystal clear. This element of the court’s ruling simultaneously neutralizes administrative agencies’ perpetual, self-serving pleas of poverty and denies their ability to withhold documents and cultural heritage digitizations based on prospective misuse by the public — including prospective </span><em>criminal</em><span> misuse by the person requesting access. On this point the court echoed the exact objection I made to musée Rodin’s then-director in my very first requests to the museum: </span><em>courts</em><span>, not administrative agencies nor museums, adjudicate crimes and only after a crime has been alleged, not in advance.</span></p><ul><li><p><em><strong>Trade secrecy: defeated with secret evidence</strong></em></p></li></ul><p>The court noted that musée Rodin had obviously created its 3D scans in the context of its public service mission to disseminate Rodin’s works and ruled the museum could not withhold them on the grounds that they would expose trade secrets related to their commercial operations. In an ironic development, the judges specifically reasoned against musée Rodin’s trade secrecy claim by citing its 3D digitization funding applications to the Ministry of Culture, in which the museum stipulated its commitment to publishing its scans. The museum had attempted to hide these funding applications from us and the court, telling the court they did not exist. However, in the course of the trial we obtained those applications by forcing a parallel documents request directly to the Ministry of Culture — which the museum complained to the court was a "crude maneuver" — exposing the museum’s deception and badly wounding the defense on this critical issue. Defeating the museum’s trade secrecy defense significantly narrows the scope of this legal exception to the public’s right to access 3D scans and other administrative documents, which other powerful cultural heritage organizations would likely attempt to abuse in similar cases.</p><ul><li><p><em><strong>Hiding public documents in closed formats: rejected</strong></em></p></li></ul><p>The court rejected musée Rodin’s argument that it could not publish several 3D scan documents because they were in a closed format that required third-party software to open. By advancing this absurd defense, the museum revealed its utter hostility to public transparency, recklessly proposing a broad exception to the public’s right to access administrative documents that would render it completely useless. Imagine the damage to public oversight if the government could legally hide embarrassing documents merely by keeping them in file formats that required proprietary software to open. The court rejected the museum’s argument and specifically ordered it to make those same proprietary 3D files accessible to the public.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F334cddb2-51a1-441b-b8a0-4d211084f20d_1488x720.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F334cddb2-51a1-441b-b8a0-4d211084f20d_1488x720.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F334cddb2-51a1-441b-b8a0-4d211084f20d_1488x720.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F334cddb2-51a1-441b-b8a0-4d211084f20d_1488x720.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F334cddb2-51a1-441b-b8a0-4d211084f20d_1488x720.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F334cddb2-51a1-441b-b8a0-4d211084f20d_1488x720.png" width="1200" height="581.0439560439561" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/334cddb2-51a1-441b-b8a0-4d211084f20d_1488x720.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:705,&quot;width&quot;:1456,&quot;resizeWidth&quot;:1200,&quot;bytes&quot;:497285,&quot;alt&quot;:&quot;Digital renders by Cosmo Wenman of 3D scans of The Kiss and Sleep, which musée Rodin was forced to publish.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Digital renders by Cosmo Wenman of 3D scans of The Kiss and Sleep, which musée Rodin was forced to publish." title="Digital renders by Cosmo Wenman of 3D scans of The Kiss and Sleep, which musée Rodin was forced to publish." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F334cddb2-51a1-441b-b8a0-4d211084f20d_1488x720.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F334cddb2-51a1-441b-b8a0-4d211084f20d_1488x720.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F334cddb2-51a1-441b-b8a0-4d211084f20d_1488x720.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F334cddb2-51a1-441b-b8a0-4d211084f20d_1488x720.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Digital renders by Cosmo Wenman of 3D scans of </span><em>The Kiss</em><span> and </span><em>Sleep</em><span>, which musée Rodin was forced to publish.</span></figcaption></figure></div><ul><li><p><em><strong>Secrecy of original scans and 3D printing formats: defeated</strong></em></p></li></ul><p><span>In the course of the trial, musée Rodin attempted to neutralize an element of our case by quietly publishing a small set of 3D scans </span><a href="https://www.musee-rodin.fr/recherche/fonds-patrimoniaux-et-documentaires-du-musee/fonds-historiques-et-archives-institutionnelles/archives-institutionnelles/fichiers-3d" rel="nofollow ugc noopener">on its website</a><span>, including </span><em>The Kiss</em><span> and </span><em>Sleep.</em><span> It hoped that by publishing them it would remove them from the dispute and avoid the court ruling specifically on the communicability of 3D files in .STL format, commonly used in 3D printing. But in a strategic blunder, the museum could not resist editing these few files prior to publishing them. We alerted the court to these modifications, and it ruled not only that .STL files are communicable administrative documents, but also that the law requires the museum to communicate its original, unaltered scan documents.</span></p><ul><li><p><em><strong>Counterfeiting countermeasures and political manipulation: rejected</strong></em></p></li></ul><p><span>Musée Rodin told the court that </span><em>all</em><span> of its other digital 3D scan files were “unfinished” — and therefore could lawfully be withheld from the public — because they had not been edited to include visible, indelible inscriptions of “Reproduction” on the sculptures’ scanned, digital surfaces. It claimed that the law against counterfeiting required such marks to prevent replicas from being misrepresented as originals and that without them, a digital, 3D scan document itself would constitute a criminal counterfeit. Those claims were nonsensical, of course, not only since it’s obvious that no one could mistake a digital file for a physical artifact, but also because vandalizing a scan by digitally carving the word “Reproduction” on it would be completely at odds with scanning practices in research, conservation, and cultural heritage contexts where fidelity to the original subject is paramount.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bf83d72-b6b1-4964-9758-5c8e5543d57c_1200x600.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bf83d72-b6b1-4964-9758-5c8e5543d57c_1200x600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bf83d72-b6b1-4964-9758-5c8e5543d57c_1200x600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bf83d72-b6b1-4964-9758-5c8e5543d57c_1200x600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bf83d72-b6b1-4964-9758-5c8e5543d57c_1200x600.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bf83d72-b6b1-4964-9758-5c8e5543d57c_1200x600.png" width="1200" height="600" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2bf83d72-b6b1-4964-9758-5c8e5543d57c_1200x600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:600,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:735346,&quot;alt&quot;:&quot;Digital render by Wenman of the edited 3D scan of The Kiss published by musée Rodin.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Digital render by Wenman of the edited 3D scan of The Kiss published by musée Rodin." title="Digital render by Wenman of the edited 3D scan of The Kiss published by musée Rodin." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bf83d72-b6b1-4964-9758-5c8e5543d57c_1200x600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bf83d72-b6b1-4964-9758-5c8e5543d57c_1200x600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bf83d72-b6b1-4964-9758-5c8e5543d57c_1200x600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bf83d72-b6b1-4964-9758-5c8e5543d57c_1200x600.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Digital render by Wenman of edited 3D scan of </span><em>The Kiss</em><span> published by musée Rodin.</span></figcaption></figure></div><p>On the supposed legal requirement for “Reproduction” marks on 3D scans, the museum could only cite policy guidance from the Ministry of Culture, in the convenient form of a “Charter of best practices in 3D printing” produced by the ministry’s High Council of Literary and Artistic Property (CSPLA). Years earlier, when I had learned that the museum itself was contributing to the CSPLA’s policy recommendations, I alerted the CSPLA and objected to the museum’s obvious conflict of interest, warning that the museum would attempt to propose policies crafted specifically to retroactively oppose my requests for access. Later, in trial, as predicted, musée Rodin cited the CSPLA’s “charter” and submitted it as evidence in support of the idea that “Reproduction” marks are needed on 3D scans. The museum submitted this evidence without disclosing to the court that it had itself co-authored that self-serving — and, frankly, absurd — guidance.</p><p><em>The court completely rejected musée Rodin’s claims about marks of “Reproduction” on scans</em><span>, echoing my earlier warnings to the CSPLA, specifically noting that the ministry’s policy recommendations were irrelevant because they had been issued years after the scans’ completion and could not be applied retroactively.</span></p><ul><li><p><em><strong>Imperfection of scans and technological incompetence: defeated with evidence from London</strong></em></p></li></ul><p>Musée Rodin also argued that many of its 3D scan documents were incomplete and therefore non-communicable because they are in text format, have gaps in their surface measurements where the sculptures were not scanned in their entirety, and are generally mediocre in quality.</p><p><span>Incredibly, the museum also argued its scan documents were non-communicable because they are supposedly unusable since the museum’s own anonymous technical consultants claimed they did not know how to use them. An astonishing set of pleadings; imagine the thinking involved in arguing that documents may be concealed from the public because they are in a universally accessible </span><em>text</em><span> format, or because the administrative agency that holds them is too incompetent to use them.</span></p><p><span>In response to the museum’s nonsensical technological claims, we submitted expert testimony from </span><a href="https://scholar.google.com/citations?user=soUq_eQAAAAJ" rel="nofollow ugc noopener">Professor Michael Kazhdan</a><span>, full professor of computer graphics in the department of computer science at Johns Hopkins University and co-developer of the </span><em>Poisson Surface Reconstruction</em><span> algorithm, which is used worldwide in 3D scan data analysis. Professor Kazhdan explained to the court that documents in plaintext format are fundamentally well suited for preserving scan data, and that such documents are easily exploitable by experts and amateurs alike. He also explained that gaps in scanned surfaces are a normal, expected, and ubiquitous phenomenon in 3D scanning and are not considered to render a scan document “incomplete” or unusable.</span></p><p><span>We also submitted testimony from artist and 3D scanning expert </span><a href="https://www.galerie-guilin.fr/" rel="nofollow ugc noopener">Ghislain Moret de Rocheprise</a><span>, who explained that plaintext format is the exact format the Ministry of Culture itself recommends for 3D scanned point-cloud documents and that it is commonly used by the general public as well as industry and government precisely because it is open, non-proprietary, and easy to preserve and access.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e5b6863-15e8-4388-9c2a-726fdb0fedc3_1073x1600.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e5b6863-15e8-4388-9c2a-726fdb0fedc3_1073x1600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e5b6863-15e8-4388-9c2a-726fdb0fedc3_1073x1600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e5b6863-15e8-4388-9c2a-726fdb0fedc3_1073x1600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e5b6863-15e8-4388-9c2a-726fdb0fedc3_1073x1600.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e5b6863-15e8-4388-9c2a-726fdb0fedc3_1073x1600.png" width="1073" height="1600" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3e5b6863-15e8-4388-9c2a-726fdb0fedc3_1073x1600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1600,&quot;width&quot;:1073,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:698675,&quot;alt&quot;:&quot;“M41LL0L_1”, from “P01NT_CL0UD” series of impressionist works made directly from point-clouds by Ghislain Moret de Rocheprise&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="“M41LL0L_1”, from “P01NT_CL0UD” series of impressionist works made directly from point-clouds by Ghislain Moret de Rocheprise" title="“M41LL0L_1”, from “P01NT_CL0UD” series of impressionist works made directly from point-clouds by Ghislain Moret de Rocheprise" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e5b6863-15e8-4388-9c2a-726fdb0fedc3_1073x1600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e5b6863-15e8-4388-9c2a-726fdb0fedc3_1073x1600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e5b6863-15e8-4388-9c2a-726fdb0fedc3_1073x1600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e5b6863-15e8-4388-9c2a-726fdb0fedc3_1073x1600.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>“M41LL0L_1”</em><span>, from “P01NT_CL0UD” series of impressionist works made directly from point-clouds by Ghislain Moret de Rocheprise.</span></figcaption></figure></div><p><span>On the question of the supposed uselessness of so-called incomplete 3D scans, we also submitted as evidence correspondence from 2020 between musée Rodin and the Tate London museum, which I obtained through a separate UK </span><em>Freedom of Information Act</em><span> request to the Tate. In that correspondence, Tate requested permission to 3D scan several sculptures on loan from musée Rodin, so that Tate could create digital renders of the works for use in a promotional video. Musée Rodin granted Tate that permission on the express condition that the sculptures </span><em>not</em><span> be scanned in their entirety. This directly contradicted musée Rodin’s position in court, demonstrating that it fully recognizes the utility of so-called “incomplete” scans. Musée Rodin’s preferential treatment of Tate London also revealed that it well understands there is no legal requirement to append marks of “Reproduction” to 3D scans of Rodin’s works.</span></p><div id="youtube2-EyvYLiIeBfw" data-attrs="{&quot;videoId&quot;:&quot;EyvYLiIeBfw&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/EyvYLiIeBfw?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>Evaluating our evidence and the museum’s strange claims, the court determined that musée Rodin cannot evade its obligation to communicate 3D scans by claiming they exclude portions of the scanned subjects or are of low quality, nor by admitting its technological incompetence to use them.</p><ul><li><p><em><strong>Trademarks: irrelevant</strong></em></p></li></ul><p>The court viewed musée Rodin’s trademarks in the "Rodin" name to be irrelevant, despite the museum’s repeated pleadings. This decision will help prevent other organizations from blocking public access to artwork by attaching their trademarks to digital records and physical replicas of public domain works.</p><ul><li><p><em><strong>Secrecy of attorney-client privilege: rejected</strong></em></p></li></ul><p>Musée Rodin had originally assured the CADA that it would comply with my request for the museum’s internal correspondence discussing my first requests to the museum. Once in trial, though, the museum reversed its position and attempted to conceal these documents from me with an improper, retroactive claim of attorney-client privilege which the judges rejected. The court ordered musée Rodin to communicate to me its early, internal deliberations on my initial inquiries.</p><ul><li><p><em><strong>Intimidation tactics: rejected</strong></em></p></li></ul><p><span>Before I filed my suit, in correspondence between musée Rodin and the Ministry of Culture — which the museum accidentally revealed during trial — the museum’s then-director asked the Ministry for advice on how the museum should respond to my requests and defend its refusals. The Ministry declined to give the museum any guidance, noting the Ministry’s own need for the clarifications from the court that would result from my likely litigation. Despite this, and even though musée Rodin reluctantly made several concessions to me only in the course of the trial and acknowledged it was doing so as a direct result of my suit, the museum nonetheless petitioned the court to condemn </span><em>me</em><span> to pay for its legal defense, and requested a figure more than ten times what is typical in this kind of procedure. This maneuver was clearly intended to penalize and intimidate me and discourage future requesters from standing up for the public’s rights.</span></p><p>Not only did the court reject that abusive request, the judges condemned musée Rodin to pay me €&nbsp;1,500 towards the expenses I incurred in bringing this action against the museum. Even though this a very small, symbolic sum compared to the time and effort my attorney and I have devoted to this effort, we are happy to have this recognition from the court of the legitimacy and need for our efforts.</p><p>As of this writing, more than a year past the court’s deadline to communicate its scans and related documents to the public, musée Rodin has still not complied; it has not published nor sent me a single document, nor has it paid me the token compensation. The museum has apparently decided to compound its lawlessness by defying the court's orders, likely with the intent to exhaust our very limited resources by forcing us to bring it back to the tribunal to get another order forcing it to actually comply with the ruling.</p><p><span>Nevertheless, altogether the court’s decision amounts to a complete repudiation of all arguments against public access that relied on the museum’s </span><em>droit moral</em><span>, economic model, cultural and patrimony sensitivities, institutional prestige and romantic mystique. We accomplished this despite the museum’s false alarms of an existential threat to all French national museums, and we did this with a foreign plaintiff (me), whom musée Rodin portrayed as a direct commercial competitor, and whom it repeatedly insinuated had </span><em>criminal</em><span> intent, a notion we did not even credit with a denial. Our entire approach and framing of the case, and the resulting decision, clear away the government’s strongest, most sensational — and most ludicrous — objections to public access to digitizations of public domain artwork.</span></p><p>We have established the strongest legal foundation for public access and reuse of digitizations of public domain works ever, in the most challenging possible circumstances, against an extremely well-positioned adversary. This ruling is a significant accomplishment that will help with every other fight for public access to cultural heritage works in France and beyond.</p><p>It’s a victory for open access.</p><p>Along with the important precedents described above, however, the tribunal made several serious legal and procedural errors that Fitzjean Ó Cobhthaigh and I feel obligated to correct. If those errors stand, they would weaken the public’s right to access administrative documents in general, and specifically permit the government to conceal an extraordinarily important class of documents. The tribunal’s errors pose a danger to the public well beyond the question of accessing simple 3D scans from museum collections.</p><ul><li><p><em><strong>An imaginary list of files</strong></em></p></li></ul><p>From the very start, I had requested that musée Rodin communicate to me a list of the 3D scan files in its possession, including scans of works from other organizations’ collections. The CADA’s analysis affirmed my request, noting that the law requires the museum to communicate such a list to the public even if one does not exist and must be generated on demand. In response, the museum not only denied that it held scans from other organizations, but it flatly told the CADA — in writing but without any justification — that it simply would not give me a list of any scan files it held. In court it never offered any defense of this lawless position.</p><p><span>Instead, during trial musée Rodin provided two incomplete, imprecise, and contradictory inventories, not of files, but of the few </span><em>sculptures</em><span> it admits having scanned and partial, vague descriptions of their file formats. At the same time, the museum complained to the court that our request for documents lacked sufficient precision since we could not guess at the exact files it held.</span></p><p>We repeatedly urged the court to recognize that the museum had in fact never supplied a simple list of files and had never identified even a single scan file by its actual file name.</p><p>The court nonetheless ruled that my request for a list of files had been rendered moot because the museum had provided one during the trial, which, again, it quite simply never did.</p><p>To this day, the court itself has no idea what scan files the museum possesses nor what, specifically, it has ordered the museum to publish, and the public can only guess. It is a very strange situation.</p><ul><li><p><em><strong>The Three Shades vanish</strong></em></p></li></ul><p><span>In its first defense brief, musée Rodin confirmed that it held 3D scans of several sculptures, including </span><em>The Kiss</em><span>, </span><em>Sleep, </em><span>and </span><em>The Three Shades.</em><span> It specifically told the court it would publish these three scans in order to neutralize elements of my suit. While it eventually, improperly, published edited versions of </span><em>The Kiss </em><span>and </span><em>Sleep,</em><span> it published nothing for </span><em>The Three Shades.</em></p><p><span>We not only alerted the court to musée Rodin’s failure to publish the </span><em>The Three Shades</em><span> scan, but we were also able to locate and present the court with photographic evidence that the museum had indeed scanned the sculpture.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c3fa4e2-d4dc-4eb9-a58d-60b5bda7b524_435x281.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c3fa4e2-d4dc-4eb9-a58d-60b5bda7b524_435x281.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c3fa4e2-d4dc-4eb9-a58d-60b5bda7b524_435x281.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c3fa4e2-d4dc-4eb9-a58d-60b5bda7b524_435x281.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c3fa4e2-d4dc-4eb9-a58d-60b5bda7b524_435x281.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c3fa4e2-d4dc-4eb9-a58d-60b5bda7b524_435x281.png" width="728" height="470.2712643678161" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9c3fa4e2-d4dc-4eb9-a58d-60b5bda7b524_435x281.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;normal&quot;,&quot;height&quot;:281,&quot;width&quot;:435,&quot;resizeWidth&quot;:728,&quot;bytes&quot;:340210,&quot;alt&quot;:&quot;FNAC’s The Three Shades being scanned at C2RMF. Photo via 7DWorks.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="FNAC’s The Three Shades being scanned at C2RMF. Photo via 7DWorks." title="FNAC’s The Three Shades being scanned at C2RMF. Photo via 7DWorks." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c3fa4e2-d4dc-4eb9-a58d-60b5bda7b524_435x281.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c3fa4e2-d4dc-4eb9-a58d-60b5bda7b524_435x281.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c3fa4e2-d4dc-4eb9-a58d-60b5bda7b524_435x281.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c3fa4e2-d4dc-4eb9-a58d-60b5bda7b524_435x281.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>FNAC’s </span><em>The Three Shades</em><span> being scanned at C2RMF. Photo via 7DWorks.</span></figcaption></figure></div><p><span>The museum responded in its second defense brief by reluctantly elaborating that in 2008 it had cooperated with several organizations to 3D scan a plaster cast of </span><em>The Three Shades</em><span> that belongs to the National Foundation for Contemporary Art (FNAC) and has been deposited in the musée des Beaux-Arts de Quimper since 1914, before Auguste Rodin died and before musée Rodin was even established. Musée Rodin explained to the court that it had mistakenly promised to publish a scan of </span><em>The Three Shades</em><span> only to realize later that it did not have a scan of </span><em>its own</em><span> cast of the sculpture. Obviously worried that it had accidentally confirmed that it holds scans from other organizations’ collections after specifically denying this, the museum further explained… nothing.</span></p><p><span>Musée Rodin never suggested there had been any technical problems when it scanned FNAC’s </span><em>Three Shades</em><span>. It never suggested it could not locate the scan. It never claimed the scan did not exist, and it never denied possessing it. </span><em>The Three Shades</em><span> simply disappeared from musée Rodin’s defense arguments, and for some reason the court took it upon itself to imagine an explanation for its vanishing.</span></p><p><span>Incredibly, in its written decision, the tribunal ruled that my request for the scan of </span><em>The Three Shades </em><span>was moot because, in the judges’ own words, “musée Rodin ultimately maintains that its research shows that it did not proceed with the digitalization of </span><em>The Three Shades</em><span> due to technical difficulties. Under these conditions, no files can be communicated.”</span></p><p><span>It’s fitting that </span><em>The Three Shades</em><span> stands atop the </span><em>Gates of Hell</em><span>. The court’s mishandling of a simple list of files and </span><em>The Three Shades</em><span> together would damn the public’s right to access administrative documents to a hopeless fate. If on one hand the court may credit an administrative agency with identifying files it never identified, while on the other hand erroneously conclude that other files never existed — without the agency even making that claim and despite the agency’s own statements and independent proof to the contrary — the government could conceal any documents it pleased simply by being strategically sloppy with its statements to the court, rendering freedom of information law completely dysfunctional.</span></p><p><span>The potential for harm from these errors is difficult to overstate, since it is multiplied by the court’s further confusion over a simple type of document commonly used to record and preserve 3D measurements: </span><em>point-clouds.</em><span> At a </span><em>minimum</em><span>, the documents and data at risk relate to the entirety of the French national territory.</span></p><p><span>As we carefully explained to the tribunal, when a subject is 3D scanned the scanning equipment takes measurements of many millions — sometimes billions — of points on the subject’s surface. A document known as a </span><em>point-cloud</em><span> is saved, which records the X, Y, and Z positions of each measured point relative to the scanner. Sometimes they also record each point’s color data and other properties. Point-cloud documents are the primary, fundamental metrological records that archive the results of 3D scan surveys. They are the essential documents from which all analyses and understanding flow and are typically among the most important deliverables scanning service providers produce for their clients, be they private or public.</span></p><p>Because they are so important, point-clouds are frequently saved in plaintext formats like .ASCII, .OBJ, and .TXT to ensure that they remain open source, easily accessible, and well-preserved for future generations. Anyone can open a plaintext formatted point-cloud document in text editing software and view the numeric values it records. Point-cloud documents are of course directly exploitable with 3D software and simple web-browser visualization tools which can easily and immediately read them and display visualizations, 3D digital environments and reconstructions, and precise analyses of the scanned subject. The Ministry of Culture itself recommends preserving and publishing scan data in open text format point-clouds for these very reasons, and this is also why musée Rodin originally agreed to create and publish text-formatted point-clouds in its digitization funding applications to the ministry so many years ago.</p><p>So what is really at risk here?</p><p>Our case deals with scans of sculptures held by musée Rodin — perhaps a few dozen, perhaps hundreds; the number is unclear — but the lower court did not seem to realize that 3D scanning isn’t limited to sculpture or the cultural heritage sector, of course. Point-cloud documents are used to record and preserve information about almost every type of subject worth scanning, across countless important sectors of intense public interest.</p><div id="youtube2-oxo7paE7MeM" data-attrs="{&quot;videoId&quot;:&quot;oxo7paE7MeM&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/oxo7paE7MeM?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>France’s National Institute of Geographic and Forestry Information (IGN), for example, is undertaking an ambitious aerial 3D scanning survey of “the entire soil and subsoil of France.” Forests, farmland, coastlines, roads, civil infrastructure and entire cities — the entire built and natural environment — are being 3D scanned and recorded in point-cloud documents.</p><p>According to IGN, trillions of points of 3D data are being gathered “to respond to major societal challenges” and all the data being acquired and produced within the framework of the program is being distributed under open data licenses. IGN’s point-cloud documents are intended to be used in environmental monitoring, risk management, forestry and agriculture, biodiversity monitoring, municipal and infrastructure planning, archaeology, and whatever else the public wishes to do with them, including uses we have not yet invented.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b490cb0-5496-4123-b7e5-bdb78d143f25_940x530.webp" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b490cb0-5496-4123-b7e5-bdb78d143f25_940x530.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b490cb0-5496-4123-b7e5-bdb78d143f25_940x530.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b490cb0-5496-4123-b7e5-bdb78d143f25_940x530.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b490cb0-5496-4123-b7e5-bdb78d143f25_940x530.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b490cb0-5496-4123-b7e5-bdb78d143f25_940x530.webp" width="940" height="530" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1b490cb0-5496-4123-b7e5-bdb78d143f25_940x530.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:530,&quot;width&quot;:940,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:137468,&quot;alt&quot;:&quot;Point-cloud visualization from 3D scans of Notre Dame by Dr. Andrew Tallon, prior to the 2019 fire. Image via National Geographic.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Point-cloud visualization from 3D scans of Notre Dame by Dr. Andrew Tallon, prior to the 2019 fire. Image via National Geographic." title="Point-cloud visualization from 3D scans of Notre Dame by Dr. Andrew Tallon, prior to the 2019 fire. Image via National Geographic." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b490cb0-5496-4123-b7e5-bdb78d143f25_940x530.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b490cb0-5496-4123-b7e5-bdb78d143f25_940x530.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b490cb0-5496-4123-b7e5-bdb78d143f25_940x530.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b490cb0-5496-4123-b7e5-bdb78d143f25_940x530.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Point-cloud visualization from 3D scans of Notre Dame by Dr. Andrew Tallon, prior to the 2019 fire. Image via National Geographic.</figcaption></figure></div><p>Point-cloud documents are also used in specific cases we cannot anticipate, including assessing disaster sites, crime scenes, and industrial accidents. For example, point-clouds of Notre Dame de Paris resulting from 3D surveys made prior to the 2019 fire are now essential to its current restoration.</p><p>In our case, the administrative tribunal of Paris appears to have disregarded these incontrovertible facts and our experts’ advisories on the role, usefulness, universal accessibility, direct exploitability, and importance of point-cloud documents. It also ignored our request that it undertake an investigation of its own to better inform itself and understand the technology and issues at stake.</p><p>Instead, the court appears to have allowed itself to be confused by the unsubstantiated and contradictory statements of musée Rodin’s anonymous and confessed incompetent technical consultants.</p><p>The court articulated its profound misunderstanding of the technology and nature of point-cloud documents by explaining that they may be withheld from the public because they are not directly useful in themselves, “but only constitute raw material needed for the production of subsequent documents.”</p><p><span>The court not only deferred to the museum’s misleading technical characterizations, it did something far more strange and ominous. On its own initiative the tribunal ruled that since it contains unedited raw data, a document containing a point-cloud — </span><em>an entire class of documents</em><span> — may be withheld from the public because, as the court explained, it “does not reveal any intention on the part of the administration”.</span></p><p>Worse, during trial, musée Rodin itself did not even suggest either of these strange technical and legal arguments, which only appeared for the first time in the judges’ own final written decision. My attorney and I were never afforded an opportunity to point out to the court that its reasoning had no basis in the law, which holds that an administration’s “intentions” are irrelevant to the communicability of its documents. Nor could we gently remind the court that its factual assertions were untrue and wildly at odds with 3D scanning and 3D data handling practices and consensus in the cultural heritage field, in industry, by amateurs, and with the government’s own recommendations for 3D data preservation and dissemination.</p><p><span>The tribunal’s legal and technological errors on the point-cloud issue alone pose a threat to public access to </span><em>millions</em><span> of documents containing </span><em>petabytes</em><span> of data from government-funded 3D surveys of the environment, oceans, the atmosphere, forests, floodplains, farmland, architecture, towns and cities, civil infrastructure, and archaeological sites, as well as the primary, archival, highest-resolution 3D scans of cultural heritage works.</span></p><p>Under the tribunal’s ruling, publicly funded point-cloud documents such as those being produced by IGN and others could legally be withheld entirely from the public. Or, the government could elect, at its own discretion, to grant the public access to such documents only after they had been processed, refined, or edited to accommodate private contractors’ and special interests’ commercial considerations, or perhaps political sensitivities, with no legal recourse for the public to examine the original, unaltered point-cloud documents directly.</p><p>Concerning point-clouds alone, the court’s errors immediately threaten to stall important technological developments, hinder scientific research and openness, and obscure and impede infrastructure and environmental monitoring by scientists, the public, journalists, and the general public.</p><p>I can only imagine and hope that the tribunal was simply unaware of the scope and degree of damage these highly irregular elements of its decision would cause. They are dangerous.</p><p><span>France’s supreme court formally</span><strong> </strong><span>accepted our appeal on April 17, 2024 and transmitted our brief to musée Rodin and the Ministry of Culture.  You can read our brief </span><a href="https://cosmowenman.files.wordpress.com/2024/03/20231201-wenman-brief-1-and-exhibits-conseil-detat_fr_en.zip" rel="nofollow ugc noopener">here</a><span>. The court gave the ministry and museum two months to present their defense, a deadline they have, as of this writing, missed by over three months. It is unlikely they will publicize their defense nor solicit input from the public while drafting it. </span></p><p>Our brief explains point-by-point the Paris tribunal’s grave legal and procedural errors and unambiguously demonstrates the lower court’s obvious and profound technological misunderstanding of point-cloud documents.</p><p><span>So much more is now at stake than public access to 3D scans of a few popular sculptures, or preserving the 19</span><sup>th</sup><span>-century business model of one intransigent museum. </span><em>One</em><span> museum could have lawless and reactionary administrators opposed to public oversight and public access, with no vision or planning for the future or innovation in the arts, and the damage might be reasonably contained to the loss of scans of its own collection. But with its deceptive arguments, reckless tactics, and the approval of the Ministry of Culture, musée Rodin managed to distract and confuse the tribunal with dangerous implications on a much larger scale.</span></p><p>The public and the Conseil d’État need to understand that the tribunal’s reasoning would fundamentally undermine the public’s right to access administrative documents and hinder its ability to monitor the government’s activities in countless sectors. The lower court’s errors would subject the public’s right to access administrative documents to the arbitrary whims of administrations that could simply hide, obfuscate, or misrepresent their holdings and their “intentions” for documents as a means of blocking public inquiry. The tribunal’s decision would also set a precedent allowing regional courts to simply invent their own legal standards on a case-by-case basis, based on total unfamiliarity with both well-established and emerging technologies, without giving litigants or the public a chance to respond.</p><p>All this public damage because musée Rodin wants to operate like a private atelier with a monopoly to exclusively service one long-dead artist’s estate and his wealthy collectors, forever. Rather than embrace the future and permit the public to access their 3D digitizations of the public’s cultural heritage, musée Rodin and the Ministry of Culture have created a smokescreen of legal and technological nonsense and started a fire that threatens our common resources across the entire digital landscape.</p><p>The productive, lawful elements of the Paris tribunal’s decision will be essential to securing access to countless important 3D scans from French national museums. The decision will help others’ efforts to modernize policies and establish public access to digitizations in public institutions throughout Europe. We can all look forward to what the public will create by fully exercising our newly clarified rights, and we will all be enriched by the full use of 3D scans of our shared cultural heritage.</p><p>But first the Conseil d’État must extinguish the fire musée Rodin and the Ministry of Culture have started.</p><p><em><span>Cosmo Wenman is an open access activist and a design and fabrication consultant. He lives in San Diego. He can be reached at </span><a href="http://www.cosmowenman.com/" rel="nofollow ugc noopener">cosmowenman.com</a><span> and </span><a href="https://twitter.com/CosmoWenman" rel="nofollow ugc noopener">twitter.com/CosmoWenman</a></em></p><p><em>Copyright 2024 Cosmo Wenman</em></p><p>If you enjoyed this story, please share it and subscribe. I’ll be writing more about this case as it progresses.</p><p data-attrs="{&quot;url&quot;:&quot;https://cosmowenman.substack.com/p/secret-3d-scans-in-the-french-supreme?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://cosmowenman.substack.com/p/secret-3d-scans-in-the-french-supreme?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel="nofollow ugc noopener"><span>Share</span></a></p><p data-attrs="{&quot;url&quot;:&quot;https://cosmowenman.substack.com/p/secret-3d-scans-in-the-french-supreme/comments&quot;,&quot;text&quot;:&quot;Leave a comment&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://cosmowenman.substack.com/p/secret-3d-scans-in-the-french-supreme/comments" rel="nofollow ugc noopener"><span>Leave a comment</span></a></p><p><strong><span>Cosmo Wenman</span><br></strong><a href="http://www.cosmowenman.com/" rel="nofollow ugc noopener">cosmowenman.com</a><br><a href="mailto:cosmo.wenman@gmail.com" rel="nofollow ugc noopener">cosmo.wenman@gmail.com</a></p><p><strong>Alexis Fitzjean Ó Cobhthaigh</strong><br><span>Attorney at the Paris Bar</span><br><a href="https://afocavocat.eu/" rel="nofollow ugc noopener">afocavocat.eu</a><br><a href="mailto:afoc@afocavocat.eu" rel="nofollow ugc noopener">afoc@afocavocat.eu</a><br><a href="mailto:afoc.avocat@protonmail.com" rel="nofollow ugc noopener">afoc.avocat@protonmail.com</a></p><p><strong>Hugues Herpin</strong><br><span>Head of Service, Musée Rodin</span><br><a href="mailto:herpin@musee-rodin.fr" rel="nofollow ugc noopener">herpin@musee-rodin.fr</a><br><span>+33 (0)1 44 18 61 10</span></p><p><strong>Hélène Pilidjian</strong><br><span>Head of the litigation office,</span><br><span>Ministry of Culture</span><br><a href="mailto:helene.pilidjian@culture.gouv.fr" rel="nofollow ugc noopener">helene.pilidjian@culture.gouv.fr</a><br><span>+33 (0)1 40 15 80 00</span></p><p><strong>Caroline-Sarah Ellenberg</strong><br><span>Deputy Director, in charge of legal affairs, Réunion des musées nationaux</span><br><a href="mailto:caroline-sarah.ellenberg@rmngp.fr" rel="nofollow ugc noopener">caroline-sarah.ellenberg@rmngp.fr</a><br><span>+33 (0)1 40 13 48 00</span></p><p><strong>Pierre Vigneron</strong><br><span>Head of Grand Palais Rmn Photo</span><br><a href="mailto:Pierre.Vigneron@rmngp.fr" rel="nofollow ugc noopener">Pierre.Vigneron@rmngp.fr</a><br><a href="mailto:agence.photo@grandpalaisrmn.fr" rel="nofollow ugc noopener">agence.photo@grandpalaisrmn.fr</a><br><span>+33 (0)1 40 13 48 00</span></p><p><span>CADA conseil 20190026:</span><br><a href="https://cada.data.gouv.fr/20190026/" rel="nofollow ugc noopener">https://cada.data.gouv.fr/20190026/</a></p><p><span>CADA avis 20192300:</span><br><a href="https://cada.data.gouv.fr/20192300/" rel="nofollow ugc noopener">https://cada.data.gouv.fr/20192300/</a></p><p><span>Decision of the Administrative Tribunal of Paris, April 21 2023:</span><br><a href="https://cosmowenman.files.wordpress.com/2024/03/20230421-cosmo-wenman-vs-musee-rodin-decision-administrative-tribunal-of-paris_fr_en.pdf" rel="nofollow ugc noopener">https://cosmowenman.files.wordpress.com/2024/03/20230421-cosmo-wenman-vs-musee-rodin-decision-administrative-tribunal-of-paris_fr_en.pdf</a></p><p><span>Wenman’s appeal to the Conseil d'État, December 1, 2023:</span><br><a href="https://cosmowenman.files.wordpress.com/2024/03/20231201-wenman-brief-1-and-exhibits-conseil-detat_fr_en.zip" rel="nofollow ugc noopener">https://cosmowenman.files.wordpress.com/2024/03/20231201-wenman-brief-1-and-exhibits-conseil-detat_fr_en.zip</a></p><p><span>Report on 3D scans advertised by RMN:</span><br><a href="https://cosmowenman.com/wp-content/uploads/2024/10/20220613-rmngp-sketchfab-3d-scan-ads-en.pdf" rel="nofollow ugc noopener">https://cosmowenman.com/wp-content/uploads/2024/10/20220613-rmngp-sketchfab-3d-scan-ads-en.pdf</a></p><p><span>RMN’s secret legal arguments to CADA:</span><br><a href="https://cosmowenman.files.wordpress.com/2024/04/20221122-correspondance-rmn-cada-si604822112216300-occlus-par-la-cada_fr_en_text.pdf" rel="nofollow ugc noopener">https://cosmowenman.files.wordpress.com/2024/04/20221122-correspondance-rmn-cada-si604822112216300-occlus-par-la-cada_fr_en_text.pdf</a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Life is not a story (110 pts)]]></title>
            <link>https://psyche.co/ideas/your-life-is-not-a-story-why-narrative-thinking-holds-you-back</link>
            <guid>41876979</guid>
            <pubDate>Fri, 18 Oct 2024 06:56:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://psyche.co/ideas/your-life-is-not-a-story-why-narrative-thinking-holds-you-back">https://psyche.co/ideas/your-life-is-not-a-story-why-narrative-thinking-holds-you-back</a>, See on <a href="https://news.ycombinator.com/item?id=41876979">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Our stories help us make sense of a chaotic world, but they can be harmful and restrictive. There’s a liberating alternative</h2><p>Narratives are everywhere, and the need to construct and share them is almost inescapable. ‘A man is always a teller of tales,’ wrote Jean-Paul Sartre in his novel <em>Nausea</em> (1938), ‘he lives surrounded by his stories and the stories of others, he sees everything that happens to him through them; and he tries to live his own life as if he were telling a story.’</p>
<p>We rely on narratives because they help us understand the world. They make life more meaningful. According to Sartre, to turn the most banal series of events into an adventure, you simply ‘begin to recount it’. However, telling a story is not just a powerful creative act. Some philosophers think that narratives are fundamental to our experiences. Alasdair MacIntyre believes we can understand our actions and those of others only as part of a narrative life. And Peter Goldie <a href="https://global.oup.com/academic/product/the-mess-inside-9780199230730" target="_blank" rel="noreferrer noopener">argues</a> that our very lives ‘have narrative structure’ – it is only by grappling with this structure that we can understand our emotions and those of others. This suggests that narratives play central, possibly fundamental, roles in our lives. But as Sartre warns in <em>Nausea</em>: ‘everything changes when you tell about life.’</p>
<p>In some cases, narratives can hold us back by limiting our thinking. In other cases, they may diminish our ability to live freely. They also give us the illusion that the world is ordered, logical, and difficult to change, reducing the real complexity of life. They can even become dangerous when they persuade us of a false and harmful world view. Perhaps we shouldn’t be too eager to live our lives as if we were ‘telling a story’. The question is: what other options do we have?</p>
<p><strong>Narratives work by organising our experiences by</strong> connecting them into sequences, which give our lives meaning. The ability to form these sequences is something we learn very young. As the educator Carol Fox found during research in the 1990s, stories begin shaping us from childhood. Fox found that reading to children at home gives them tacit knowledge of linguistic and narrative structures, which they incorporate into their own spoken stories. Her research showed that children as young as three used stories to experiment with language as they made sense of the world. The older we get, the more we keep playing – and the more we keep relying on narratives.</p>
<p>Random events can be reframed as being part of some grand plan</p>
<p>As adults, we adopt different roles, including friend, lover, employee, parent, carer and more. The way we understand these roles is often framed in terms of expected behaviour. For example, we have a narrative grasp of what a ‘friend’ is, and we judge ourselves and others by how well they fit that narrative – sometimes favourably, sometimes <span>less so.</span></p>
<p>So, why is this a problem? One issue is complexity. Seeing yourself as the <a href="https://aeon.co/essays/why-main-character-syndrome-is-philosophically-dangerous" target="_blank" rel="noopener">main character</a> in a story can overly simplify the fullness of life. Think of the way in which people talk about their ‘journey’ through life. Through this narrative, certain events become more significant while others are overlooked, and random events can be reframed as being part of some grand plan. Yet viewing our lives in such a narrow way hinders our ability to understand the complex behaviour of others and ourselves. For example, a child that accepts the narrative of being ‘naughty’ may incorrectly frame their behaviour as bad, rather than as an expression of their unmet needs. Stories can change us by locking us into ways of acting, thinking, and feeling.</p>
<p>In the 1970s, a recognition of this limitation gave rise to narrative therapy. Rather than seeing people as illogical or overly emotional, this new form of psychotherapy focused on the role of narratives in a person’s life. As the therapist Martin Payne explains in his <a href="https://sk.sagepub.com/books/narrative-therapy-2e" target="_blank" rel="noreferrer noopener">book</a> <em>Narrative Therapy </em>(2000), the approach allows ‘richer, combined narratives to emerge from disparate descriptions of experience’. A new narrative can be <a href="https://aeon.co/essays/once-upon-a-time-how-stories-change-hearts-and-brains" target="_blank" rel="noopener">incredibly powerful</a> for someone who is unaware of how their established stories are obscuring other ways of understanding their life.</p>
<p>The stories that might need changing are not only grand, but also minor, such as the ‘scripts’ that we rely on throughout our lives. These scripts can become habitual patterns of thinking, influencing our interpretations of <a href="https://aeon.co/ideas/your-love-story-is-a-narrative-that-gets-written-in-tandem" target="_blank" rel="noopener">family members</a>, friends or colleagues. As narrative therapy shows, we can also get these scripts wrong, and may need help altering them.</p>
<p>Though narrative therapy can be effective, it is unable to help people understand what creates and <a href="https://aeon.co/videos/are-certain-familiar-narrative-arcs-inherently-appealing" target="_blank" rel="noopener">shapes</a> their narratives. It merely helps them to choose between different narratives or construct new stories about themselves and the world. Swapping one ‘script’ for another doesn’t help someone see the full range of possibilities that lie in front of them, including what it might mean to <em>reject</em> a narrative altogether.</p>
<p>The narrative he follows gives him a limited understanding of himself</p>
<p><strong>The possibility of rejecting a narrative can be</strong> found in Sartre’s <em>Being and Nothingness </em>(1943) where he describes a café waiter. According to Sartre, the waiter has adopted a particular narrative that shapes his identity and governs how he ought to behave. Being wedded to a narrative view of the self can lead to living in what Sartre calls ‘bad faith’ – that is, living without being aware of one’s responsibility or in control of one’s own destiny:</p>
<blockquote>All his behaviour seems to us a game. He applies himself to chaining his movements as if they were mechanisms, the one regulating the other; his gestures and even his voice seem to be mechanisms; he gives himself the quickness and pitiless rapidity of things. He is playing, he is amusing himself. But what is he playing? We need not watch long before we can explain it: he is playing at being a waiter in a café. There is nothing there to <span>surprise us.</span> </blockquote>
<p>In other words, he is playing the role of a waiter in a similar manner to an actor on stage who follows a script. As a result of embodying the waiter-narrative, he lives inauthentically because he can only act in a way that fits with the role. The narrative he follows gives him a limited understanding of himself, determining his actions and preventing him from taking ownership of his life. But what would happen if the waiter rejected that narrative identity? For Sartre, this would be a step towards true selfhood, or an authentic existence – what he called ‘being’ – rather than merely playing a role.</p>
<p>So, what does it mean to reject a narrative? Living in a <a href="https://aeon.co/essays/let-s-ditch-the-dangerous-idea-that-life-is-a-story" target="_blank" rel="noopener">non-narrative</a> way means rejecting a particular identity, and instead seeing life and meaning as a set of open choices. For the waiter, rejecting his narrative identity would mean acting in a way that reflects his choices and sense of self, not just the story he tells about himself.</p>
<p>To understand what is involved in rejecting a narrative, it is important to remember that narratives do not exist outside of people’s minds. The stories we tell ourselves are not out there in the world. They are tools that mediate our relationships with the world. Though they relate to facts, and real events, they are not factual. In fact, they are neither true nor false. Instead, stories help us make sense of things. So, if we rejected the power of narratives to sequence events in our lives, how else would we organise our thoughts about the world?</p>
<p>Think of the ways that perspectives organise experiences differently. By ‘perspective’ I mean something more complex than ‘point of view’. I’m referring to the way we engage with the world from a particular position or orientation that draws our attention to aspects of experience, like how our visual ‘perspective’ allows bright colours to show up more easily than dull ones. Perspectives are shaped by our place in the world, our beliefs, values and what we think matters. As the philosopher Elisabeth Camp <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1475-4975.2009.00186.x" target="_blank" rel="noreferrer noopener">explains</a>, a perspective ‘helps us to <em>do</em> things with the thoughts we have: to make quick judgments based on what’s most important, to grasp intuitive connections, and to respond emotionally, among other things.’ Through perspective some features of our experiences ‘stick out in our minds while others fade into the background.’</p>
<p>Poetry captures <em>a way </em>of seeing and feeling, not just a sequence of events</p>
<p>Perspectives, then, determine the narratives we adopt. In other words, our core beliefs and values shape the way we see things and what we take to be important in our experiences. It is our perspectives that generate our narratives. Perspective also explains why our narratives can differ so radically from those of other people, even when we experience the same events. But once we understand these perspectives, we can see how flexible our narratives can truly become. Perspectives, it turns out, don’t have a linear, ordered structure. We can’t think of them in terms of sequences of events, like stories. In some ways, perspectives are better represented by the non-linearity of poetry.</p>
<p>Poems, particularly lyric poems, are inherently perspectival; they unify words, images, thoughts and feelings to express value. Poetry captures <em>a way </em>of seeing and feeling, not just a sequence of events.</p>
<p>Think of ‘Thirteen Ways of Looking at a Blackbird ’ (1917) by the American poet Wallace Stevens. Each stanza focuses on a different way of looking at a blackbird and its relationship to the self:</p>
<blockquote>Icicles filled the long window<br>With barbaric glass.<br>The shadow of the blackbird<br>Crossed it, to and fro.<br>The mood<br>Traced in the shadow<br>An indecipherable cause.</blockquote>
<p>In Stevens’s poem, he brings experiences together without explaining how they are related – they are connected only by his perspective. Likewise, understanding ourselves in a non-linear way means seeing how we relate to a complex and chaotic world in the present moment. Within that moment, we find significance without needing an ordered pattern.</p>
<p>And so, instead of just changing our narratives, we should learn to understand the perspectives that shape them. When we focus on our own stories, we live life as we already know it, but by loosening the grip that stories hold over our lives – by focusing on the perspectives of ourselves and others – we can begin opening ourselves up to other possibilities. We can adopt new orientations, find significance in new places, and even move toward the exciting unpredictability of shared<em> </em>perspectives.</p>
<p>As Sartre warned, everything changes when you tell a story. Narratives limit our potential. Though we are complex beings, living in a chaotic universe, our stories create the illusion that our lives are ordered, logical and complete.</p>
<p>We might never fully escape the narratives that surround us, but we can learn to change the perspectives behind them. And so, we are never bound by stories, only by our ability to understand how our beliefs and values shape the way we perceive and engage with the world. We don’t need better narratives; we need to expand and refine our perspectives.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Factorio – Visualizing construction material dependencies (177 pts)]]></title>
            <link>https://community.wolfram.com/groups/-/m/t/1793319</link>
            <guid>41876821</guid>
            <pubDate>Fri, 18 Oct 2024 06:25:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://community.wolfram.com/groups/-/m/t/1793319">https://community.wolfram.com/groups/-/m/t/1793319</a>, See on <a href="https://news.ycombinator.com/item?id=41876821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<td>
			<div>

				
				<p><img src="https://community.wolfram.com//c/portal/getImageAttachment?filename=fd1f632099404d13bb4a9c2ce3f1284e92058e00078b28baf5e6858887e5c505_product_card_v2_mobile_slider_639.jpg&amp;userId=73716" alt="enter image description here"></p>
<p>Factorio is a game where you crashed on a planet with your space-craft. You have to built a new rocket and leave this planet again. In order to do so you will need to mine metals and built yourself a factory, make labs to do research, and built machine that make other machines, and finally combine all of this to make rockets, satellites, rocket fuel, trains, flying robots, oil products, steam engines, plastics, electronic chips, iron, copper, uranium centrifuging, solar panels etc etc. An incredibly complicated game where the key idea is that you automate this entire process using machines in your factory.</p>
<p>See <a href="https://www.youtube.com/watch?v=KVvXv1Z6EY8">https://www.youtube.com/watch?v=KVvXv1Z6EY8</a> .</p>
<p>To do research you need to manufacture research-packs which are created from other resources, which might also be created from other resources etc. etc. </p>
<p>Here is some code that interprets the Factorio wiki:</p> 
<pre>baseurl = "https://wiki.factorio.com";
ClearAll[GetImage]
ClearAll[FindDependencies]
GetImage[url_] := GetImage[url] = Import[url]
FindDependencies[url_String] := 
 FindDependencies[url] = Module[{xml, c, end, other, sel = 1},
   xml = Import[url, "XMLObject"];
   c = Cases[xml, 
     XMLElement["table", {}, {contents_}] :&gt; contents, \[Infinity]];
   c = Select[c, 
     MemberQ[#, XMLElement["p", {}, {"Recipe\n"}], \[Infinity]] &amp;];
   c = FirstCase[#, 
       XMLElement[
         "tr", {}, {XMLElement[
           "td", {___, 
            "class" -&gt; "infobox-vrow-value"}, {x__}]}] :&gt; {x}, 
       Missing[], \[Infinity]] &amp; /@ c;
   If[Length[c] &gt; 0,
    c = c[[sel]];
    c = Cases[c, 
      XMLElement[
        "div", {"class" -&gt; "factorio-icon", 
         "style" -&gt; "background-color:#999;"}, {XMLElement[
          "a", {"shape" -&gt; "rect", "href" -&gt; hrefurl_, 
           "title" -&gt; name_}, {XMLElement[
            "img", {"alt" -&gt; _, "src" -&gt; imgurl_, "width" -&gt; "32", 
             "height" -&gt; "32",___}, {}]}], 
         XMLElement[
          "div", {"class" -&gt; "factorio-icon-text"}, {num_}]}] :&gt; 
       FactorioObject[baseurl &lt;&gt; hrefurl, name, 
        GetImage[baseurl &lt;&gt; imgurl], 
        ToExpression@StringTrim[StringReplace[num, "k" -&gt; "000"]]], \[Infinity]];

    c = DeleteCases[c, FactorioObject[_, "Time", _, _]];
    {{end}, other} = TakeDrop[c, -1];
    other -&gt; end,
    {}
    ]
   ]
ClearAll[FindDependencyTree]
FindDependencyTree[url_String, iterations_: 6] := 
 Module[{a, known, unknown, new, vlbls, vertices},
  a = FindDependencies[url];
  known = {a};
  Do[
   unknown = Join @@ known[[All, 1]];
   unknown = DeleteDuplicates[Complement[unknown, known[[All, 2]]]];
   new = DeleteDuplicates[FindDependencies@*First /@ unknown];
   new = DeleteCases[new, {}];
   known = DeleteDuplicates[Join[known, new]];
   ,
   {iterations}
   ];
  vlbls = 
   Cases[known, 
    FactorioObject[_, name_, 
      icon_, _] :&gt; (name -&gt; 
       Image[icon, ImageSize -&gt; 32]), \[Infinity]];
  vertices = 
   DeleteDuplicates[
    Join @@ Table[(# -&gt; k[[2, 2]]) &amp; /@ k[[1, All, 2]], {k, known}]];
  &lt;|"LabelRules" -&gt; vlbls, "Vertices" -&gt; vertices, 
   "Dependencies" -&gt; known|&gt;
  ]
</pre>
<p>Let's ask the dependency tree for the first science pack:</p> 
<pre>out1 = FindDependencyTree["https://wiki.factorio.com/Science_pack_1"];
Graph[out1["Vertices"], VertexShape -&gt; out1["LabelRules"], 
 VertexSize -&gt; {"Scaled", 0.05}]
</pre>
<p><img src="https://community.wolfram.com//c/portal/getImageAttachment?filename=2381Factorio_1.png&amp;userId=73716" alt="enter image description here"></p>
<p>To make Science pack 1, we need gears and copper plates. And to make gears we need iron plates. The iron and copper plates are made from iron and copper ore. </p>
<p>This is still relatively simple, let's look at the other science packs:</p> 
<pre>out2 = FindDependencyTree[
   "https://wiki.factorio.com/Science_pack_2"];
Graph[out2["Vertices"], VertexShape -&gt; out2["LabelRules"], 
 VertexSize -&gt; {"Scaled", 0.05}]
out3 = FindDependencyTree[
   "https://wiki.factorio.com/Science_pack_3"];
Graph[out3["Vertices"], VertexShape -&gt; out3["LabelRules"], 
 VertexSize -&gt; {"Scaled", 0.05}]
out4 = FindDependencyTree[
   "https://wiki.factorio.com/Military_science_pack"];
Graph[out4["Vertices"], VertexShape -&gt; out4["LabelRules"], 
 VertexSize -&gt; {"Scaled", 0.05}]
out5 = FindDependencyTree[
   "https://wiki.factorio.com/Production_science_pack"];
Graph[out5["Vertices"], VertexShape -&gt; out5["LabelRules"], 
 VertexSize -&gt; {"Scaled", 0.05}]
out6 = FindDependencyTree[
   "https://wiki.factorio.com/High_tech_science_pack"];
Graph[out6["Vertices"], VertexShape -&gt; out6["LabelRules"], 
 VertexSize -&gt; {"Scaled", 0.05}]
</pre>
<p>Resulting in:</p>
<p><img src="https://community.wolfram.com//c/portal/getImageAttachment?filename=Factorio_2.png&amp;userId=73716" alt="enter image description here"></p>
<p><img src="https://community.wolfram.com//c/portal/getImageAttachment?filename=Factorio_3.png&amp;userId=73716" alt="enter image description here"> <img src="https://community.wolfram.com//c/portal/getImageAttachment?filename=Factorio_4.png&amp;userId=73716" alt="enter image description here"> <img src="https://community.wolfram.com//c/portal/getImageAttachment?filename=Factorio_5.png&amp;userId=73716" alt="enter image description here"> <img src="https://community.wolfram.com//c/portal/getImageAttachment?filename=Factorio_6.png&amp;userId=73716" alt="enter image description here"></p>
<p>To summarize, let's combine all the graphs:</p> 
<pre>o = {out1, out2, out3, out4, out5, out6};
Graph[Union @@ o[[All, "Vertices"]], 
 VertexShape -&gt; Union @@ o[[All, "LabelRules"]], 
 VertexSize -&gt; {"Scaled", 0.02}, ImageSize -&gt; 1000, 
 AspectRatio -&gt; 1/GoldenRatio]
</pre>
<p><img src="https://community.wolfram.com//c/portal/getImageAttachment?filename=Factorio_All.png&amp;userId=73716" alt="enter image description here"></p>
<p>As you can see the dependencies are very complex to get all the research packs. Of course there are many things you need to create with your machines, think of transport belts, trains, mining, steam generation, and energy production, water and other chemicals etc etc. </p>
<p>One of the most expensive parts is a satellite (to guide your rocket):</p> 
<pre>out = FindDependencyTree["https://wiki.factorio.com/Satellite"];
Graph[out["Vertices"], VertexShape -&gt; out["LabelRules"], 
 VertexSize -&gt; {"Scaled", 0.05}]
</pre>
<p><img src="https://community.wolfram.com//c/portal/getImageAttachment?filename=Factorio_Satellite.png&amp;userId=73716" alt="enter image description here"></p>
<p>I hope you like this little exploration on Graphs and Factorio. We could also use Mathematica's Graph technology to design balanced belt splitter designs: <a href="http://i.imgur.com/tz2Jc3p.png">http://i.imgur.com/tz2Jc3p.png</a> ! I will leave that for some other time. For now, explore the different buildings or parts, for example have a look at the rocket silo:</p> 
<pre>out = FindDependencyTree["https://wiki.factorio.com/Rocket_silo"];
Graph[out["Vertices"], VertexShape -&gt; out["LabelRules"], 
 VertexSize -&gt; {"Scaled", 0.05}]
</pre>
<p>If you haven't played already be careful it is an incredibly addicting game!</p>

				
					
				
				
			</div>
		</td>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using static websites for tiny archives (316 pts)]]></title>
            <link>https://alexwlchan.net/2024/static-websites/</link>
            <guid>41876750</guid>
            <pubDate>Fri, 18 Oct 2024 06:12:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alexwlchan.net/2024/static-websites/">https://alexwlchan.net/2024/static-websites/</a>, See on <a href="https://news.ycombinator.com/item?id=41876750">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" tabindex="-1"> <article>   <p>In <a href="https://alexwlchan.net/2024/digital-decluttering/">my previous post</a>, I talked about how I’m trying to be more intentional and deliberate with my digital data. I don’t just want to keep everything – I want to keep stuff that I’m actually going to look at again. As part of that process, I’m trying to be better about organising my files. Keeping something is pointless if I can’t find it later.</p> <p>Over the last year or so, I’ve been creating static websites to browse my local archives. I’ve done this for a variety of collections, including:</p> <ul> <li>paperwork I’ve scanned</li> <li>documents I’ve created</li> <li>screenshots I’ve taken</li> <li>web pages I’ve bookmarked</li> <li>video and audio files I’ve saved</li> </ul> <p>I create one website per collection, each with a different design, suited to the files it describes. For example, my collection of screenshots is shown as a grid of images, my bookmarks are a series of text links, and my videos are a list with a mixture of thumbnails and text.</p> <figure> <a href="https://alexwlchan.net/images/2024/static-screenshots.png"><picture> <source srcset="https://alexwlchan.net/images/2024/static-screenshots_1x.avif 400w, https://alexwlchan.net/images/2024/static-screenshots_2x.avif 800w, https://alexwlchan.net/images/2024/static-screenshots_3x.avif 1200w" sizes="(max-width: 400px) 100vw, 400px" type="image/avif"> <source srcset="https://alexwlchan.net/images/2024/static-screenshots_1x.webp 400w, https://alexwlchan.net/images/2024/static-screenshots_2x.webp 800w, https://alexwlchan.net/images/2024/static-screenshots_3x.webp 1200w" sizes="(max-width: 400px) 100vw, 400px" type="image/webp"> <source srcset="https://alexwlchan.net/images/2024/static-screenshots_1x.png 400w, https://alexwlchan.net/images/2024/static-screenshots_2x.png 800w, https://alexwlchan.net/images/2024/static-screenshots_3x.png 1200w" sizes="(max-width: 400px) 100vw, 400px" type="image/png"> <img src="https://alexwlchan.net/images/2024/static-screenshots_1x.png" alt="My screenshots collection. It's a two-column grid of images, with a line of small text below each image (usually tags or a description)." loading="lazy" width="400"> </picture></a> <a href="https://alexwlchan.net/images/2024/static-bookmarks.png"><picture> <source srcset="https://alexwlchan.net/images/2024/static-bookmarks_1x.avif 400w, https://alexwlchan.net/images/2024/static-bookmarks_2x.avif 800w, https://alexwlchan.net/images/2024/static-bookmarks_3x.avif 1200w" sizes="(max-width: 400px) 100vw, 400px" type="image/avif"> <source srcset="https://alexwlchan.net/images/2024/static-bookmarks_1x.webp 400w, https://alexwlchan.net/images/2024/static-bookmarks_2x.webp 800w, https://alexwlchan.net/images/2024/static-bookmarks_3x.webp 1200w" sizes="(max-width: 400px) 100vw, 400px" type="image/webp"> <source srcset="https://alexwlchan.net/images/2024/static-bookmarks_1x.png 400w, https://alexwlchan.net/images/2024/static-bookmarks_2x.png 800w, https://alexwlchan.net/images/2024/static-bookmarks_3x.png 1200w" sizes="(max-width: 400px) 100vw, 400px" type="image/png"> <img src="https://alexwlchan.net/images/2024/static-bookmarks_1x.png" alt="My bookmarks collection. The screenshot shows three blue links, and below each link is some descriptive text – a quote from the link, or some text I've written myself." loading="lazy" width="400"> </picture></a> <a href="https://alexwlchan.net/images/2024/static-videos.png"><picture> <source srcset="https://alexwlchan.net/images/2024/static-videos_1x.avif 400w, https://alexwlchan.net/images/2024/static-videos_2x.avif 800w, https://alexwlchan.net/images/2024/static-videos_3x.avif 1200w" sizes="(max-width: 400px) 100vw, 400px" type="image/avif"> <source srcset="https://alexwlchan.net/images/2024/static-videos_1x.webp 400w, https://alexwlchan.net/images/2024/static-videos_2x.webp 800w, https://alexwlchan.net/images/2024/static-videos_3x.webp 1200w" sizes="(max-width: 400px) 100vw, 400px" type="image/webp"> <source srcset="https://alexwlchan.net/images/2024/static-videos_1x.png 400w, https://alexwlchan.net/images/2024/static-videos_2x.png 800w, https://alexwlchan.net/images/2024/static-videos_3x.png 1200w" sizes="(max-width: 400px) 100vw, 400px" type="image/png"> <img src="https://alexwlchan.net/images/2024/static-videos_1x.png" alt="My video archive. The list of videos has a large thumbnail on the left-hand side, and on the right-hand side is the title of the video, a link to the original, and a description I wrote." loading="lazy" width="400"> </picture></a> </figure> <p>These websites aren’t complicated – they’re just meant to be a slightly nicer way of browsing files than I get in the macOS Finder. I can put more metadata on the page, and build my own ways to search and organise the files.</p> <p>Each collection is a folder on my local disk, and the website is one or more HTML files in the root of that folder. To use the website, I open the HTML files in my web browser.</p> <figure> <a href="https://alexwlchan.net/images/2024/finder_website.png"><picture> <source srcset="https://alexwlchan.net/images/2024/finder_website.dark_1x.avif 450w, https://alexwlchan.net/images/2024/finder_website.dark_2x.avif 900w, https://alexwlchan.net/images/2024/finder_website.dark_3x.avif 1350w" sizes="(max-width: 450px) 100vw, 450px" type="image/avif" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/finder_website.dark_1x.webp 450w, https://alexwlchan.net/images/2024/finder_website.dark_2x.webp 900w, https://alexwlchan.net/images/2024/finder_website.dark_3x.webp 1350w" sizes="(max-width: 450px) 100vw, 450px" type="image/webp" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/finder_website.dark_1x.png 450w, https://alexwlchan.net/images/2024/finder_website.dark_2x.png 900w, https://alexwlchan.net/images/2024/finder_website.dark_3x.png 1350w" sizes="(max-width: 450px) 100vw, 450px" type="image/png" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/finder_website_1x.avif 450w, https://alexwlchan.net/images/2024/finder_website_2x.avif 900w, https://alexwlchan.net/images/2024/finder_website_3x.avif 1350w" sizes="(max-width: 450px) 100vw, 450px" type="image/avif"> <source srcset="https://alexwlchan.net/images/2024/finder_website_1x.webp 450w, https://alexwlchan.net/images/2024/finder_website_2x.webp 900w, https://alexwlchan.net/images/2024/finder_website_3x.webp 1350w" sizes="(max-width: 450px) 100vw, 450px" type="image/webp"> <source srcset="https://alexwlchan.net/images/2024/finder_website_1x.png 450w, https://alexwlchan.net/images/2024/finder_website_2x.png 900w, https://alexwlchan.net/images/2024/finder_website_3x.png 1350w" sizes="(max-width: 450px) 100vw, 450px" type="image/png"> <img src="https://alexwlchan.net/images/2024/finder_website_1x.png" loading="lazy" alt="My 'screenshots' folder in the macOS Finder. There's a series of per-year folders from 2010 to 2024, a JavaScript file 'metadata.js' and an HTML file 'index.html'." width="450"> </picture></a> <figcaption> This is what my screenshots website looks like. The individual images are stored in per-year folders, there's some machine-readable metadata in <code>metadata.js</code>, and I can double-click <code>index.html</code> to open the file in my browser and use the website. The HTML file uses the metadata to render the grid of images. </figcaption> </figure> <p>I’m deliberately going low-scale, low-tech. There’s no web server, no build system, no dependencies, and no JavaScript frameworks. I’m writing everything by hand, which is very manageable for small projects. Each website is a few hundred lines of code at most.</p> <p>Because this system has no moving parts, and it’s just files on a disk, I hope it will last a long time. I’ve already migrated a lot of my files to this approach, and I’m pleased with how it’s going. I get all the simplicity and portability of a file full of folders, with just a bit of extra functionality sprinkled on top.</p> <hr> <h2 id="how-did-i-get-to-static-websites">How did I get to static websites?</h2> <p>Before static websites, I tried other approaches for organising my files, but they never stuck.</p> <p><strong>I’ve made several attempts to use files and folders, the plain filesystem.</strong> Where I always struggled is that folders require you to use hierarchical organisation, and everything has to be stored in exactly one place. That works well for some data – all my code, for example – but I find it more difficult for media. I could never design a hierarchy that I was happy with. I’d stall on organising files because I was unsure of which folder to put them in, and I ended up with a disorganised mess of a desktop.</p> <p>I much prefer the flexibility of keyword tagging. Rather than put a file in a single category, I can apply multiple labels and use any of them to find the file later. The macOS Finder does support tagging, but I’ve always found its implementation to be a bit lacklustre, and I don’t want to use it for anything serious.</p> <p><strong>When I was younger, I tried “everything buckets” like <a href="https://www.devontechnologies.com/apps/devonthink">DEVONThink</a>, <a href="https://evernote.com/">Evernote</a>, and <a href="https://www.barebones.com/products/yojimbo/">Yojimbo</a>.</strong> I&nbsp;know lots of people like this sort of app, but I could never get into them. I always felt like I had to wrap my brain around the app’s way of thinking – changing myself to fit the app’s approach, not the other way round.</p> <p><strong>Once I had some programming experience, I tried writing my own tools to organise my files.</strong> I made at least a dozen attempts at this, the last of which was <a href="https://alexwlchan.net/2019/my-scanning-setup/#how-did-i-create-an-app-to-tag-my-pdfs">docstore</a>. Building my own tool meant I got something that was a closer match to my mental model, but now I was on the hook for maintenance. Every time I upgraded Python or updated macOS, something would break and I’d have to dive into the the code to fix it. These tools never required a lot of ongoing work, but it was enough to be annoying.</p> <p>Every time I gave up on an app, I had another go at using plain files and folders. They’re the default way to organise files on my Mac. They’re lightweight, portable, easy to back up, and I expect to be able to read them for many years to come. But the limited support for custom metadata and keyword tags was always a deal breaker.</p> <p><strong>At some point I realised I could solve these problems by turning folders into mini-websites.</strong> I could create an HTML file in the top-level folder, which could be an index – a list of all the files, displayed with all the custom metadata and tags I wanted.</p> <p>This allowed me to radically simplify the folder structure, and stop chasing the perfect hierarchy. In these mini-websites, I use very basic folders – files are either grouped by year or by first letter of their filename. I only look at the folders when I’m adding new files, and never for browsing. When I’m looking for files, I always use the website. The website can use keyword tags to let me find files in multiple ways, and abstract away the details of the underlying folders.</p> <p>HTML is low maintenance, it’s flexible, and it’s not going anywhere. It’s the foundation of the entire web, and pretty much every modern computer has a web browser that can render HTML pages. These files will be usable for a very long time – probably decades, if not more.</p> <p>(I still have the first website I made, for a school class in 2006. It renders flawlessly in a modern browser. I feel safe betting on HTML.)</p> <h2 id="emphasis-on-tiny">Emphasis on “tiny”</h2> <p>I’m doing a lot of this by hand – organising the files, writing the metadata, building the viewers. This doesn’t scale to a large collection. Even storing a few hundred items this way takes a non-trivial amount of time – but I actually like that.</p> <p>Introducing a bit of friction is helping me to decide what I really care about saving. What’s worth taking the time to organise properly, and what can’t I be bothered with? If I don’t want to take even a minute to save it, am I going to look at it again? But if I do save something, I’ve become more willing to take time to write proper metadata, in a way that will make it easier to find later.</p> <p>I used to have large, amorphous folders where I collected en masse. I had thousands of poorly organised files and I couldn’t find anything, so I never looked at what I’d saved. Now I have tiny websites with a few hundred items which are carefully selected and usefully described.</p> <p>Even though I usually love automation, I’m enjoying some of the constraints imposed by a more manual process.</p> <h2 id="prior-art">Prior art</h2> <p>Using a static website like this isn’t new – my inspiration was <a href="https://alexwlchan.net/images/2024/twitter_account_export.png">Twitter’s account export</a>, which gives you a mini-website you can browse locally. I’ve seen several other social media platforms that give you a website as a human-friendly way to browse your data.</p> <p>I think this could be a powerful idea for digital preservation, as a way to describe <a href="https://en.wikipedia.org/wiki/Born-digital">born-digital</a> archives. All the benefits of simplicity, longevity, and low maintenance are even more valuable in a memory institution where you want to preserve something for decades or centuries. (And HTML is so low-tech, you can create a basic HTML website on any computer with just the built-in notepad or text editor. No IT support required!)</p> <p>It’s been exciting to explore this idea at work, where we’re building larger static websites as part of our <a href="https://www.flickr.org/programs/content-mobility/data-lifeboat/">Data Lifeboat project</a>. This is a way to package up an archival sliver from Flickr. Where my local archives are typically just a list view, the website inside a Data Lifeboat has more pages and functionality. And while I was finishing this article, I saw a <a href="https://social.coop/@edsu/113306537369602233">post from Ed Summers</a> about creating static sites as a way to preserve Historypin.</p> <p>I’d love to this static websites get more use as a preservation tool.</p> <hr> <p>I already have a lot of files, which are sprawled across my disk. I’d love to consolidate them all in this new approach, but that would be a tremendous amount of work. My colleague Jessamyn wrote about this <a href="https://www.librarian.net/stax/5585/be-organized-from-the-very-beginning/">in a follow-up to my digital decluttering article</a>: <em>“no one is ever starting at the beginning, not in 2024”</em>.</p> <p>Rather than moving everything at once, I’m doing a little at a time. As I create new files, I’m saving them into static websites. As I look for older files, I’m pulling them out of their existing storage and moving them into the appropriate static site folder.</p> <p>I’m enjoying this approach, so I’m going to keep using it. What I particularly like is that the maintenance burden has been essentially zero – once I set up the initial site structure, I haven’t had to do anything to keep it working.</p> <p>If you’ve never written a website and it’s something you want to try, have a look at Blake Watson’s new book <a href="https://htmlforpeople.com/">HTML for People</a>. <em>“I feel strongly that anyone should be able to make a website with HTML if they want. This book will teach you how to do just that.”</em>. I love that philosophy. I’m only a third of the way through, but already I can tell this is a great resource.</p> <p>For a long time, I thought of HTML as a tool for publishing on the web, a way to create websites that other people can look at. But all these websites I’m creating are my local, personal archives – just for me. I’m surprised it took me this long to realise HTML isn’t just for sharing on the web.</p> </article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What is theoretical computer science? (134 pts)]]></title>
            <link>https://cacm.acm.org/opinion/what-is-theoretical-computer-science/</link>
            <guid>41876723</guid>
            <pubDate>Fri, 18 Oct 2024 06:04:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cacm.acm.org/opinion/what-is-theoretical-computer-science/">https://cacm.acm.org/opinion/what-is-theoretical-computer-science/</a>, See on <a href="https://news.ycombinator.com/item?id=41876723">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p>Thinking of theoretical computer science as a branch of mathematics is harmful to the discipline.</p>

<figure>
<p><img width="1024" height="576" src="https://cacm.acm.org/wp-content/uploads/2024/08/080224.OP_.Moshe-Vardi-2400x1350-1.jpg" alt="Moshe Y. Vardi" loading="eager" decoding="async" srcset="https://cacm.acm.org/wp-content/uploads/2024/08/080224.OP_.Moshe-Vardi-2400x1350-1.jpg 2400w, https://cacm.acm.org/wp-content/uploads/2024/08/080224.OP_.Moshe-Vardi-2400x1350-1.jpg?resize=300,169 300w, https://cacm.acm.org/wp-content/uploads/2024/08/080224.OP_.Moshe-Vardi-2400x1350-1.jpg?resize=768,432 768w, https://cacm.acm.org/wp-content/uploads/2024/08/080224.OP_.Moshe-Vardi-2400x1350-1.jpg?resize=1024,576 1024w, https://cacm.acm.org/wp-content/uploads/2024/08/080224.OP_.Moshe-Vardi-2400x1350-1.jpg?resize=1536,864 1536w, https://cacm.acm.org/wp-content/uploads/2024/08/080224.OP_.Moshe-Vardi-2400x1350-1.jpg?resize=2048,1152 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></p> </figure>

</div><div>
<article><section id="sec1" lang="en"><p id="p-1">I consider myself a computer science (CS) theoretician, but Wikipedia describes me as a “mathematician and computer scientist.”<a href="#fn1" data-jats-ref-type="fn" data-jats-rid="fn1"><sup>a</sup></a> So, what am I? To answer that question, we must consider theoretical computer science (TCS), which Wikipedia defines as “a subfield of computer science and mathematics that focuses on the abstract mathematical foundations of computation.”<a href="#fn2" data-jats-ref-type="fn" data-jats-rid="fn2"><sup>b</sup></a> I’d like to take issue with this definition.</p><p id="p-2">In his lovely 2019 book, <i><a href="https://www.math.ias.edu/avi/book" data-jats-ext-link-type="uri">Mathematics and Computation</a></i>,<a href="#fn3" data-jats-ref-type="fn" data-jats-rid="fn3"><sup>c</sup></a> 2023 ACM A.M. Turing Award recipient Avi Wigderson defines the theory of computation as “the study of the formal foundations of computer science and technology.” This is a very broad definition, but the scope of the book does not match that definition. It offers a very U.S.-centric view of TCS. As I have written <a href="https://cacm.acm.org/opinion/why-doesnt-acm-have-a-sig-for-theoretical-computer-science/" data-jats-ext-link-type="uri">elsewhere</a>,<a href="#fn4" data-jats-ref-type="fn" data-jats-rid="fn4"><sup>d</sup></a> U.S. TCS has a quite narrower scope than European TCS, which I find unfortunate.</p><p id="p-3">I believe that Avi has the right and broad definition for theoretical computer science; it is “the study of the formal foundations of computer science and technology.” In fact, if one browses 1970s proceedings of the ACM Symposium on the Theory of Computing (STOC) and the IEEE Symposium on Foundations of Computer Science (FOCS), one sees indeed a very broad conception of TCS. Only in the 1980s, with the proliferation of the so-called “satellite conferences,” dedicated to topics such as databases, program verification, and the like, did the scope of STOC and FOCS narrow, which led to a narrowing of how TCS is viewed in the U.S.</p><p id="p-4">Regardless of the breadth of TCS, the question remained as to whether it is a subfield of mathematics. Undoubtedly, TCS is abstract and mathematical, but is it mathematics? For that matter, what is mathematics? Mathematics is notoriously hard to define, so I prefer the sociological definition: <i>Mathematics is what mathematicians do</i>. In 1993, as a young computer science theoretician, I was offered a faculty position in the CS department at Rice University. I doubt I would have received such an offer from the math department at Rice. Avi is one of a handful of computer science theoreticians worldwide with a primary position in a department of mathematics. I must conclude that TCS is not a branch of mathematics, at least sociologically.</p><p id="p-5">But my objection to “TCS is a branch of mathematics” is deeper than the sociological argument. I believe that thinking of TCS as a branch of mathematics is harmful to the discipline. The centrality of computing stems from the fact that it is a technology that has been changing the world for the past 80 years, ever since the British used early computing to change the tide of war in World War II. As computer scientists, we should look for inspiration from physics rather than from mathematics. Theoretical physics is highly mathematical, but it aims to explain and predict the real world. Theories that fail at this “explain/predict” task would ultimately be discarded. Analogously, I’d argue that the role of TCS is to explain/predict real-life computing. I am not saying that every TCS paper should be held to this standard, but the standard should be applied to branches of TCS. We should remember the <a href="https://mathshistory.st-andrews.ac.uk/Extras/Von_Neumann_Part_1/" data-jats-ext-link-type="uri">warning</a> of John von Neuman,<a href="#fn5" data-jats-ref-type="fn" data-jats-rid="fn5"><sup>e</sup></a> one of the greatest mathematicians and computer scientists of the 20<sup>th</sup> century, regarding the danger of mathematics driven solely by internal esthetics: “There is a grave danger that the subject will develop along the line of least resistance.”</p><p id="p-6">Consider, for example, computational-complexity theory—the main focus of Avi’s book—which I find to be the most elegant theory in CS. The theory focuses on classifying computational problems according to their resource usage, usually time and space. One of the crown jewels of that theory is the concept of NP-completeness, which crystalizes the difference between checking solutions and finding solutions. The paradigmatic NP-complete problem is the Boolean Satisfiability Problem (SAT), which asks whether a given Boolean formula, with Boolean gates such as AND and NOT, has some assignment of 0s and 1s to its input variables such that the formula yields the value 1. When Cook proved in 1971 that SAT is NP-complete, the problem was considered computationally hard.</p><p id="p-7">Over the past 30 years, however, we <a href="https://cacm.acm.org/opinion/boolean-satisfiability/" data-jats-ext-link-type="uri">have made</a><a href="#fn6" data-jats-ref-type="fn" data-jats-rid="fn6"><sup>f</sup></a> tremendous progress in SAT solving, which is today an industrial reality. NP-completeness theory, however, does not explain or predict the unreasonable effectiveness of SAT solvers. In spite of recent <a href="https://www.cambridge.org/core/books/beyond-the-worstcase-analysis-of-algorithms/8A8128BBF7FC2857471E9CA52E69AC21" data-jats-ext-link-type="uri">efforts</a><a href="#fn7" data-jats-ref-type="fn" data-jats-rid="fn7"><sup>g</sup></a> to go beyond worst-case complexity, this approach is still the prevalent approach to computational-complexity analysis, but it shed little light on “real-word complexity.”</p><p id="p-8">So, I do not consider myself a mathematician. I am squarely in the computer science camp.</p></section></article>
</div><div>
<div>
<p>
Submit an Article to CACM </p>
<p>
CACM welcomes unsolicited <a href="https://cacm.acm.org/author-guidelines/#CACMsubmission">submissions</a> on topics of relevance and value to the computing community. </p>
</div>
<section>
<div>
<p>
You Just Read </p>
<h4>
What Is Theoretical Computer Science? </h4>
<a href="https://dl.acm.org/doi/10.1145/3698060">
 View in the ACM Digital Library  </a>
</div>
<p>©ACM 0001-0782/24/09</p>
</section>
</div><div data-component="ctaMembership">
<div>
<h3>
Shape the Future of Computing </h3>
<p>
ACM encourages its members to take a direct hand in shaping the future of the association. There are more ways than ever to get involved. </p>
<p><a href="https://www.acm.org/about-acm/get-involved">
Get Involved </a>
</p></div>
<div>
<h3>
Communications of the ACM (CACM) is now a fully Open Access publication. </h3>
<p>
By opening CACM to the world, we hope to increase engagement among the broader computer science community and encourage non-members to discover the rich resources ACM has to offer. </p>
<p><a href="https://cacm.acm.org/news/cacm-is-becoming-open-access">
Learn More </a>
</p></div>
</div></div>]]></description>
        </item>
    </channel>
</rss>