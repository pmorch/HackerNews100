<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 29 May 2025 03:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[US Trade Court Finds Trump Tariffs Illegal (379 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2025-05-28/trump-s-global-tariffs-blocked-by-us-trade-court</link>
            <guid>44121732</guid>
            <pubDate>Thu, 29 May 2025 00:06:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2025-05-28/trump-s-global-tariffs-blocked-by-us-trade-court">https://www.bloomberg.com/news/articles/2025-05-28/trump-s-global-tariffs-blocked-by-us-trade-court</a>, See on <a href="https://news.ycombinator.com/item?id=44121732">Hacker News</a></p>
Couldn't get https://www.bloomberg.com/news/articles/2025-05-28/trump-s-global-tariffs-blocked-by-us-trade-court: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Long live American Science and Surplus (which needs your help) (197 pts)]]></title>
            <link>https://milwaukeerecord.com/city-life/long-live-american-science-surplus-which-needs-your-help/</link>
            <guid>44120507</guid>
            <pubDate>Wed, 28 May 2025 20:47:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://milwaukeerecord.com/city-life/long-live-american-science-surplus-which-needs-your-help/">https://milwaukeerecord.com/city-life/long-live-american-science-surplus-which-needs-your-help/</a>, See on <a href="https://news.ycombinator.com/item?id=44120507">Hacker News</a></p>
Couldn't get https://milwaukeerecord.com/city-life/long-live-american-science-surplus-which-needs-your-help/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[A toy RTOS inside Super Mario Bros. using emulator save states (158 pts)]]></title>
            <link>https://prettygoodblog.com/p/what-threads-are-part-2</link>
            <guid>44120241</guid>
            <pubDate>Wed, 28 May 2025 20:15:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://prettygoodblog.com/p/what-threads-are-part-2">https://prettygoodblog.com/p/what-threads-are-part-2</a>, See on <a href="https://news.ycombinator.com/item?id=44120241">Hacker News</a></p>
Couldn't get https://prettygoodblog.com/p/what-threads-are-part-2: Error: getaddrinfo ENOTFOUND prettygoodblog.com]]></description>
        </item>
        <item>
            <title><![CDATA[Deepseek R1-0528 (300 pts)]]></title>
            <link>https://huggingface.co/deepseek-ai/DeepSeek-R1-0528</link>
            <guid>44118818</guid>
            <pubDate>Wed, 28 May 2025 17:59:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528">https://huggingface.co/deepseek-ai/DeepSeek-R1-0528</a>, See on <a href="https://news.ycombinator.com/item?id=44118818">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><section>
				
				
				
				
				<p>No model card</p></section>
			<section><div><dl><dt>Downloads last month</dt><dd>0
							</dd></dl>
						</div>
					

				<div data-target="ModelTensorsParams" data-props="{&quot;modelId&quot;:&quot;deepseek-ai/DeepSeek-R1-0528&quot;,&quot;safetensors&quot;:{&quot;parameters&quot;:{&quot;BF16&quot;:3918786560,&quot;F8_E4M3&quot;:680571043840,&quot;F32&quot;:41555600},&quot;total&quot;:684531386000,&quot;sharded&quot;:true},&quot;isGated&quot;:false,&quot;query&quot;:{},&quot;model&quot;:{&quot;author&quot;:&quot;deepseek-ai&quot;,&quot;config&quot;:{&quot;architectures&quot;:[&quot;DeepseekV3ForCausalLM&quot;],&quot;auto_map&quot;:{&quot;AutoConfig&quot;:&quot;configuration_deepseek.DeepseekV3Config&quot;,&quot;AutoModel&quot;:&quot;modeling_deepseek.DeepseekV3Model&quot;,&quot;AutoModelForCausalLM&quot;:&quot;modeling_deepseek.DeepseekV3ForCausalLM&quot;},&quot;model_type&quot;:&quot;deepseek_v3&quot;,&quot;quantization_config&quot;:{&quot;quant_method&quot;:&quot;fp8&quot;},&quot;tokenizer_config&quot;:{&quot;bos_token&quot;:{&quot;__type&quot;:&quot;AddedToken&quot;,&quot;content&quot;:&quot;<｜begin▁of▁sentence｜>&quot;,&quot;lstrip&quot;:false,&quot;normalized&quot;:true,&quot;rstrip&quot;:false,&quot;single_word&quot;:false},&quot;eos_token&quot;:{&quot;__type&quot;:&quot;AddedToken&quot;,&quot;content&quot;:&quot;<｜end▁of▁sentence｜>&quot;,&quot;lstrip&quot;:false,&quot;normalized&quot;:true,&quot;rstrip&quot;:false,&quot;single_word&quot;:false},&quot;pad_token&quot;:{&quot;__type&quot;:&quot;AddedToken&quot;,&quot;content&quot;:&quot;<｜end▁of▁sentence｜>&quot;,&quot;lstrip&quot;:false,&quot;normalized&quot;:true,&quot;rstrip&quot;:false,&quot;single_word&quot;:false},&quot;unk_token&quot;:null,&quot;chat_template&quot;:&quot;{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' in message %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- else %}{{'<｜Assistant｜>' + message['content'] + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- endfor %}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' not in message %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}&quot;}},&quot;createdAt&quot;:&quot;2025-05-28T09:46:42.000Z&quot;,&quot;discussionsDisabled&quot;:false,&quot;downloads&quot;:0,&quot;downloadsAllTime&quot;:0,&quot;id&quot;:&quot;deepseek-ai/DeepSeek-R1-0528&quot;,&quot;isLikedByUser&quot;:false,&quot;availableInferenceProviders&quot;:[],&quot;inference&quot;:&quot;&quot;,&quot;lastModified&quot;:&quot;2025-05-28T18:01:18.000Z&quot;,&quot;likes&quot;:251,&quot;librariesOther&quot;:[],&quot;trackDownloads&quot;:true,&quot;private&quot;:false,&quot;repoType&quot;:&quot;model&quot;,&quot;gated&quot;:false,&quot;pwcLink&quot;:{&quot;error&quot;:&quot;Unknown error, can't generate link to Papers With Code.&quot;},&quot;tags&quot;:[&quot;safetensors&quot;,&quot;deepseek_v3&quot;,&quot;custom_code&quot;,&quot;fp8&quot;,&quot;region:us&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;safetensors&quot;,&quot;label&quot;:&quot;Safetensors&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;deepseek_v3&quot;,&quot;label&quot;:&quot;deepseek_v3&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;custom_code&quot;,&quot;label&quot;:&quot;custom_code&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;fp8&quot;,&quot;label&quot;:&quot;fp8&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;type&quot;:&quot;region&quot;,&quot;label&quot;:&quot;🇺🇸 Region: US&quot;,&quot;id&quot;:&quot;region:us&quot;}],&quot;safetensors&quot;:{&quot;parameters&quot;:{&quot;BF16&quot;:3918786560,&quot;F8_E4M3&quot;:680571043840,&quot;F32&quot;:41555600},&quot;total&quot;:684531386000,&quot;sharded&quot;:true},&quot;hasBlockedOids&quot;:false,&quot;region&quot;:&quot;us&quot;,&quot;isQuantized&quot;:false,&quot;xetEnabled&quot;:false},&quot;canWrite&quot;:false}"><div><p>Safetensors</p><a target="_blank" href="https://huggingface.co/docs/safetensors"></a></div>
		<div><div><p>Model size</p>
				<p>685B params</p></div>
			<div><p>Tensor type</p>
				<div><p>BF16
						</p><p>·</p><p>F8_E4M3
						</p><p>·</p><p>F32
						</p><p>·</p></div></div>
			</div></div>
					

				<div data-target="InferenceWidget" data-props="{&quot;model&quot;:{&quot;author&quot;:&quot;deepseek-ai&quot;,&quot;config&quot;:{&quot;architectures&quot;:[&quot;DeepseekV3ForCausalLM&quot;],&quot;auto_map&quot;:{&quot;AutoConfig&quot;:&quot;configuration_deepseek.DeepseekV3Config&quot;,&quot;AutoModel&quot;:&quot;modeling_deepseek.DeepseekV3Model&quot;,&quot;AutoModelForCausalLM&quot;:&quot;modeling_deepseek.DeepseekV3ForCausalLM&quot;},&quot;model_type&quot;:&quot;deepseek_v3&quot;,&quot;quantization_config&quot;:{&quot;quant_method&quot;:&quot;fp8&quot;},&quot;tokenizer_config&quot;:{&quot;bos_token&quot;:{&quot;__type&quot;:&quot;AddedToken&quot;,&quot;content&quot;:&quot;<｜begin▁of▁sentence｜>&quot;,&quot;lstrip&quot;:false,&quot;normalized&quot;:true,&quot;rstrip&quot;:false,&quot;single_word&quot;:false},&quot;eos_token&quot;:{&quot;__type&quot;:&quot;AddedToken&quot;,&quot;content&quot;:&quot;<｜end▁of▁sentence｜>&quot;,&quot;lstrip&quot;:false,&quot;normalized&quot;:true,&quot;rstrip&quot;:false,&quot;single_word&quot;:false},&quot;pad_token&quot;:{&quot;__type&quot;:&quot;AddedToken&quot;,&quot;content&quot;:&quot;<｜end▁of▁sentence｜>&quot;,&quot;lstrip&quot;:false,&quot;normalized&quot;:true,&quot;rstrip&quot;:false,&quot;single_word&quot;:false},&quot;unk_token&quot;:null,&quot;chat_template&quot;:&quot;{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' in message %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- else %}{{'<｜Assistant｜>' + message['content'] + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- endfor %}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' not in message %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}&quot;}},&quot;createdAt&quot;:&quot;2025-05-28T09:46:42.000Z&quot;,&quot;discussionsDisabled&quot;:false,&quot;downloads&quot;:0,&quot;downloadsAllTime&quot;:0,&quot;id&quot;:&quot;deepseek-ai/DeepSeek-R1-0528&quot;,&quot;isLikedByUser&quot;:false,&quot;availableInferenceProviders&quot;:[],&quot;inference&quot;:&quot;&quot;,&quot;lastModified&quot;:&quot;2025-05-28T18:01:18.000Z&quot;,&quot;likes&quot;:251,&quot;librariesOther&quot;:[],&quot;trackDownloads&quot;:true,&quot;private&quot;:false,&quot;repoType&quot;:&quot;model&quot;,&quot;gated&quot;:false,&quot;pwcLink&quot;:{&quot;error&quot;:&quot;Unknown error, can't generate link to Papers With Code.&quot;},&quot;tags&quot;:[&quot;safetensors&quot;,&quot;deepseek_v3&quot;,&quot;custom_code&quot;,&quot;fp8&quot;,&quot;region:us&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;safetensors&quot;,&quot;label&quot;:&quot;Safetensors&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;deepseek_v3&quot;,&quot;label&quot;:&quot;deepseek_v3&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;custom_code&quot;,&quot;label&quot;:&quot;custom_code&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;fp8&quot;,&quot;label&quot;:&quot;fp8&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;type&quot;:&quot;region&quot;,&quot;label&quot;:&quot;🇺🇸 Region: US&quot;,&quot;id&quot;:&quot;region:us&quot;}],&quot;safetensors&quot;:{&quot;parameters&quot;:{&quot;BF16&quot;:3918786560,&quot;F8_E4M3&quot;:680571043840,&quot;F32&quot;:41555600},&quot;total&quot;:684531386000,&quot;sharded&quot;:true},&quot;hasBlockedOids&quot;:false,&quot;region&quot;:&quot;us&quot;,&quot;isQuantized&quot;:false,&quot;xetEnabled&quot;:false},&quot;canWrite&quot;:false,&quot;shouldUpdateUrl&quot;:true,&quot;inferenceContextData&quot;:{&quot;billableEntities&quot;:[],&quot;entityName2Providers&quot;:{}},&quot;linkToInferenceDiscussion&quot;:{&quot;url&quot;:&quot;/spaces/huggingface/InferenceSupport/discussions/2216&quot;,&quot;numReactions&quot;:10}}">

<div>
					<p><span>Inference Providers</span>
					<a target="_blank" href="https://huggingface.co/docs/inference-providers">NEW
					</a></p></div>


		<div><p><span>This model isn't deployed by any Inference Provider.</span>

					<a href="https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/2216"><span><span>🙋</span>
							<span>10</span></span>
						<span>Ask for provider support</span></a></p></div></div>

				
	<h2>
		Model tree for <span>deepseek-ai/DeepSeek-R1-0528</span>
		<a href="https://huggingface.co/docs/hub/model-cards#specifying-a-base-model" target="_blank"></a></h2>
	<div><div>
				<svg width="19" height="28" viewBox="0 0 19 28" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M1 0C1 7.42391 7.4588 13.5 15.5 13.5V14.5C6.9726 14.5 0 8.04006 0 0H1Z" fill="currentColor"></path></svg></div>
			<p>Finetunes</p>
			
			<p><a href="https://huggingface.co/models?other=base_model:finetune:deepseek-ai/DeepSeek-R1-0528">1 model</a>
		</p></div>

				
				
				
					<h2>
						Collection including
						<span>deepseek-ai/DeepSeek-R1-0528</span></h2>
					<div><article><a href="https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d"><header title="DeepSeek-R1"><h4>DeepSeek-R1</h4>
				<div><p>
					Collection
				</p></div></header>

			<div>
				<p><span>9 items</span>
				<span>• </span>
				<span>Updated
					<time datetime="2025-05-28T17:43:04" title="2025-05-28T17:43:04.961Z">about 2 hours ago</time></span>
				<span>•</span></p><p>

					645</p></div></a></article>
	</div>

				
				</section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Compiling a Neural Net to C for a 1,744× speedup (197 pts)]]></title>
            <link>https://slightknack.dev/blog/difflogic/</link>
            <guid>44118373</guid>
            <pubDate>Wed, 28 May 2025 17:22:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://slightknack.dev/blog/difflogic/">https://slightknack.dev/blog/difflogic/</a>, See on <a href="https://news.ycombinator.com/item?id=44118373">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            

            <!-- <div class="text-input-container">
                <input
                    id="search"
                    type="search"
                    placeholder="Jump to..."
                    autocomplete="off"
                />
                <div class="search-results" style="display: none">
                    <div class="search-results__items"></div>
                </div>
            </div> -->

            
<!-- <div> -->

<p>2025-05-27 · About 25 minutes long</p>
<p><strong>tl;dr:</strong> I trained a neural network (NN), with logic gates in the place of activation functions, to learn a 3×3 kernel function for Conway’s Game of Life. I wanted to see if I could speed up inference by extracting the learned logic circuit from the NN. So, I wrote some code to extract and compile the extracted logic circuit to bit-parallel C (with some optimizations to remove gates that don’t contribute to the output). I benchmarked the original NN against the extracted 300-line single-threaded C program.; compiling the NN to C resulted in a 1,744× speedup! Crazy, right? <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic">Here’s the repo</a>: <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/blob/6f01c83a0e6d02dcec59ab91c64eaf91ee4a3776/main.py">~354 lines of Python/JAX</a>, <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/blob/master/gate.c">~331 lines of C</a>, if you want to <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/tree/89e2ed2f2c018122132598ea610a960900079bea?tab=readme-ov-file#to-reproduce">reproduce it</a> and/or mess around.</p>
<!-- <iframe src="/conway" width="100%" frameborder="0"></iframe> -->
<p><img alt="ghostty with 512 by 512 board of conway's game of life on it" src="https://slightknack.dev/content/conway-tui.png"></p><h2 id="the-longer-story">The longer story</h2>
<p>While plumbing the intertubes (as one does), I came across <a rel="noopener nofollow" target="_blank" href="https://google-research.github.io/self-organising-systems/difflogic-ca/">this fun publication</a> by the <a rel="noopener nofollow" target="_blank" href="https://google-research.github.io/self-organising-systems/"><em>Self Organising Systems</em></a> group at Google, about <em>Differentiable Logic Cellular Automata</em>. This research caught my attention (I mean, who doesn’t love a pretty picture), and as I read it, I realized the whole idea wouldn’t be <em>too</em> hard to replicate. (Which is crazy, because <em>this</em> is crazy. I mean, the authors cite <em>creating <a rel="noopener nofollow" target="_blank" href="https://en.wikipedia.org/wiki/Computronium">computronium</a></em> as a source of inspiration. Awesome!)</p>
<p>To break things down a little bit: <em><a rel="noopener nofollow" target="_blank" href="https://arxiv.org/abs/2210.08277">Differentiable Logic</a> <a rel="noopener nofollow" target="_blank" href="https://distill.pub/2020/growing-ca/">Cellular Automata</a></em> is a quite a mouthful, I know, but the idea is the straightforward composition of two existing ideas:</p>
<ol>
<li>
<p><strong>Cellular Automata (CA)</strong> are grids of cells, where each cell changes over time according to some <em>local</em> rule. The most famous cellular automata are <a rel="noopener nofollow" target="_blank" href="https://blog.oimo.io/2023/04/10/life-universe-en/"><em>Conway’s Game of Life</em></a> and perhaps <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/machine-110"><em>Rule 110</em></a>. We call this local update rule a <em>kernel</em>, a function that looks at the local neighborhood around a cell to calculate the cell’s next state. By applying the kernel to each cell in our grid, we step the cellular automata forward through time. Simple rules can give rise to <a rel="noopener nofollow" target="_blank" href="https://www.youtube.com/watch?v=C2vgICfQawE">strikingly complex behaviour</a>. <a rel="noopener nofollow" target="_blank" href="https://distill.pub/2020/growing-ca/"><strong>Neural Cellular Automata (NCA)</strong></a> are a variant of CA that replace the kernel function with a neural network. NCA can be trained to learn known kernels (what I do), or more generally, to learn kernels that give rise to a specified target behavior.</p>
</li>
<li>
<p><strong>Deep Differentiable Logic Gate Networks (DLGNs)</strong> are like neural networks, with two key differences. First, the <strong>weights are fixed</strong>, and set to 0 or 1; each neuron has exactly two inputs (one left, one right) We call these weights <em>wires</em>. Since wires are fixed, we <strong>learn the activation function</strong>. In this case, our activation function is a weighted linear combination of a set of 16 logic gates, applied to the two inputs. Essentially, we learn which logic gate should be used for each node in a fixed circuit. (If you’re a little lost, don’t worry, later, I’ll go into more detail to describe what this looks like in practice.)</p>
</li>
</ol>
<p>After reading the <a rel="noopener nofollow" target="_blank" href="https://google-research.github.io/self-organising-systems/difflogic-ca/">original publication</a>, I wanted to play with a few different experiments. To start, I wanted to replicate the research without looking at it too closely, to give room for creative misinterpretation, to understand what decisions were important. I also wanted to mess around with whatever the result of that was, until something <em>interesting</em> fell out. Consider this to be something of a pilot episode: there is a lot I still would like to experiment with. (Like fluid simulation! We could find a circuit for the kernel rule for <em><a rel="noopener nofollow" target="_blank" href="https://michaelmoroz.github.io/Reintegration-Tracking/">Reintegration Tracking</a></em>. Wouldn’t that be neat?)</p>
<p>I tried something new for the first time, which was to keep a <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic?tab=readme-ov-file#journal">journal</a> during development. I have no idea why I’ve never done this before, because it made recording training runs, figuring out what to debug, tracking next steps, and scoping changes <em>so</em> much easier. (Not to mention writing this write-up!) This project took three days (well, <del>four</del> five if you count this write-up), and it’s cool to see how much progress I made each day.</p>
<h2 id="conway-s-what-a-quick-recap">Conway’s what? A quick recap</h2>
<p><em>Conway’s Game of Life</em> is a Cellular Automata that takes place on a 2D grid of square cells. Each cell has 8 neighbors. At each step, looking at this 3×3 neighborhood,  we decide whether each cell is alive or dead, according to two rules:</p>
<ol>
<li>If the cell has exactly 3 live neighbors, it is alive.</li>
<li>If the cell has exactly 2 live neighbors, and is alive, it remains alive.</li>
</ol>
<p>I guess there’s a harsh third rule which is, “if the cell is dead, it stays dead”. You may already begin to see how this could be written as a logic circuit. Here’s one way: given an array, <code>inputs</code>, with 9 cells where <code>0</code> is dead, <code>1</code> is alive, we can write:</p>
<pre data-lang="python"><code data-lang="python"><span>n = </span><span>sum</span><span>(inputs) - alive </span><span># neighbors excluding center
</span><span>alive_next = (n == </span><span>3</span><span>) or ((n == </span><span>2</span><span>) and alive)
</span></code></pre>
<p>The hard part, you might glean, would be coming up with a compact circuit for counting <code>n</code>, the number of neighbors. That’s what we’re up against: given a 9-bit input, we’re going to try to learn a circuit of logic gates whose output matches the 1-bit <code>alive_next</code> output, end-to-end.</p>
<blockquote>
<p><em>N.B.</em> You should try coming up with a circuit yourself, by hand! I did, it’s not too crazy. Here’s a hint: A cell has 8 neighbors. You can count how many cells are alive for any pair of inputs: <code>xor</code> for 1, <code>and</code> for 2. Can you use two pairs to count 4? What about 8? Once you can determine whether the neighborhood count is 2 or 3, the rest is fairly straightforward. How many logic gates did you use? How deep is your circuit?</p>
</blockquote>
<h2 id="in-the-beginning-was-jax">In the beginning was JAX</h2>
<p>Now, I suppose I’ll show my age by saying that my knowledge of ML frameworks is stuck around 2017, with <em>old</em> Keras (never change, <code>Sequential</code>) and TensorFlow (<em>before</em> 2.0). I used to mess around with GANs (<em>StyleGAN</em> &lt;3) and I even implemented <a rel="noopener nofollow" target="_blank" href="https://openai.com/index/reinforcement-learning-with-prediction-based-rewards/">RND</a> on top of <a rel="noopener nofollow" target="_blank" href="https://arxiv.org/abs/1707.06347">PPO</a> at some point! So I have a good intuition of the basics. Much has been lost to the sands of time, aside from a <a rel="noopener nofollow" target="_blank" href="https://github.com/Tloru/CommaSpeedChallenge">handful of random projects</a> shotgunned across GitHub. Sigh.</p>
<p>Well, as Oogway would say, “Yesterday is <em>history</em>, tomorrow is a <em>mystery</em>, but today… is a gift. That is why it is called the <em>present</em>.” There is no better day than today to get back on your A-game.</p>
<p><strong>JAX</strong> is a <a rel="noopener nofollow" target="_blank" href="https://docs.jax.dev/en/latest/quickstart.html">machine learning framework</a> for Python. I think of it as numpy on steroids. JAX has an API-compatible implementation of numpy living at <code>jax.jnp</code>: you can do all the fun matrix stuff (like multiplication!) and you get a couple things for free:</p>
<ol>
<li><strong>grad</strong> will <a rel="noopener nofollow" target="_blank" href="https://docs.jax.dev/en/latest/automatic-differentiation.html">compute the gradient</a> of any non-sadistic function. It does this using automatic <a rel="noopener nofollow" target="_blank" href="https://docs.jax.dev/en/latest/advanced-autodiff.html#jacobian-vector-products-jvps-a-k-a-forward-mode-autodiff">reverse-mode differentiation</a>, but you can get as fancy as you’d like.</li>
<li><strong>vmap</strong> will <a rel="noopener nofollow" target="_blank" href="https://docs.jax.dev/en/latest/autodidax.html">automatically parallelize</a> and vectorize computations that can be run in parallel. For example, I use vmap to batch training in my implementation.</li>
<li><strong>jit</strong> will <a rel="noopener nofollow" target="_blank" href="https://docs.jax.dev/en/latest/jit-compilation.html">just-in-time compile</a> all of the above, and can produce code that runs on the GPU. Compiling Python at the library level using decorators is crazy!</li>
</ol>
<p>JAX has an ecosystem of libraries that mix and match these operations to build more powerful primitives. <strong>Optax</strong>, for example, implements common <a rel="noopener nofollow" target="_blank" href="https://optax.readthedocs.io/en/latest/api/optimizers.html">optimization strategies</a>, like <code>adamw</code>, on top of <code>jax.grad</code>. <strong>Flax</strong>, is a NN library built on top of JAX. (Tbh, flax is a little confusing: there’s <code>nn</code> (<a rel="noopener nofollow" target="_blank" href="https://docs.google.com/document/d/1hYavTVPaKVVe9Be8pCB7yW7r6dDv3RALVNit8NZca4c/edit?tab=t.0">deprecated</a>), <a rel="noopener nofollow" target="_blank" href="https://flax-linen.readthedocs.io/en/latest/"><code>linen</code></a>, <a rel="noopener nofollow" target="_blank" href="https://flax.readthedocs.io/en/latest/index.html"><code>nnx</code></a>. Everyone uses linen but the flax devs want people to use nnx it seems).</p>
<blockquote>
<p><em>N.B.</em> One other thing I like about JAX is that randomness is reproducible. All functions that generate non-deterministic output take a RNG <em>key</em>, and keys can be split into subkeys. The root key is generated when you set the seed; everything else derives from the root key. This means if you run a program twice, you’ll have the same initialization, same training samples, same convergence. It makes debugging a lot easier, because you can reproduce e.g. a blow-up during training with some patience.</p>
</blockquote>
<h2 id="continuous-relaxation">Continuous relaxation</h2>
<p>To learn a circuit, we need some way to translate the hard, discrete language of logic into the smooth, continuous language of <em>gradients</em>. (Gradients, now those are something an optimizer can get a handle on!)</p>
<p>A logic gate, like <code>and(a, b)</code>, is a function of two inputs; we can arrange them into a table like so:</p>

<p>Here, inputs are defined to be exactly <code>0</code> or <code>1</code>. What if the inputs <em>vary</em> between <code>0.0</code> and <code>1.0</code>; is there a continuous function of two inputs <code>f(a, b)</code> that behaves like <code>and(a, b)</code> at the boundary? Well, yes, <code>f(a, b) = a * b</code> fits the bill. We can say that <code>a * b</code> a <em>continuous relaxation</em> of <code>and(a, b)</code>. There are 16 fundamental logical functions of two inputs; here they all are, along with a continuous relaxation for each:</p>
<table><thead><tr><th>gate</th><th>relaxation</th><th>gate</th><th>relaxation</th></tr></thead><tbody>
<tr><td><code>false</code></td><td><code>0.</code></td><td><code>true</code></td><td><code>1.</code></td></tr>
<tr><td><code>and</code></td><td><code>a*b</code></td><td><code>nand</code></td><td><code>1. - a*b</code></td></tr>
<tr><td><code>a&amp;~b</code></td><td><code>a - a*b</code></td><td><code>b/~a</code></td><td><code>1. - a + a*b</code></td></tr>
<tr><td><code>a</code></td><td><code>a</code></td><td><code>not a</code></td><td><code>1. - a</code></td></tr>
<tr><td><code>b&amp;~a</code></td><td><code>b - a*b</code></td><td><code>a/~b</code></td><td><code>1. - b + a*b</code></td></tr>
<tr><td><code>b</code></td><td><code>b</code></td><td><code>not b</code></td><td><code>1. - b</code></td></tr>
<tr><td><code>xor</code></td><td><code>a+b - 2.*a*b</code></td><td><code>xnor</code></td><td><code>1. - (a+b - 2.*a*b)</code></td></tr>
<tr><td><code>or</code></td><td><code>a+b - a*b</code></td><td><code>nor</code></td><td><code>1. - (a+b - a*b)</code></td></tr>
</tbody></table>
<p><img alt="graphs of and, or, and xor" src="https://slightknack.dev/content/cont-relax.svg"></p><p>This is great! (It’s fun to try to derive continuous relaxations yourself.) With this groundwork in place, we could create a “learnable logic gate” of sorts by taking a weighted sum of each relaxation. In JAX, using <code>jnp</code>:</p>
<pre data-lang="python"><code data-lang="python"><span>jnp.</span><span>sum</span><span>(gate_weight * </span><span>gate_all</span><span>(left, right)) </span><span># axis=0
</span></code></pre>
<p>Here, <code>gate_all</code> returns a vector where each entry is the result of one of the functions above. If we want to make sure that the gate weights stay in a reasonable range, we can apply a <em>softmax</em> to the learned vector <code>w</code>, which squashes each gate weight to be between <code>0.0</code> and <code>1.0</code>:</p>
<pre data-lang="python"><code data-lang="python"><span>gate_weight = jnp.</span><span>exp</span><span>(w) / jnp.</span><span>sum</span><span>(jnp.</span><span>exp</span><span>(w)) </span><span># axis=0, keepdims=True
</span></code></pre>
<p>We can train a network to learn these gate weights. Once we have a trained network, can replace the softmax with an <em>argmax</em> (taking the gate with the highest weight). This gives us a circuit with a hard <code>0</code> or <code>1</code> as an output; a discrete logic gate is also much cheaper to compute. (It’s almost as if computers are full of them!)</p>
<h2 id="hardness-zero">Hardness zero</h2>
<p>Well, we have our continuous relaxations and we have a NN. Let’s just put them together, replace <code>relu</code> with <code>gate</code>, and call it a day? Not so fast.</p>
<p>Machine learning papers almost make research look <em>effortless</em>, as though NNs magically converge when enough data is forced through their weights. This could not be further from the truth: there are so many failure modes; so many experiments that have to be run to guess the right hyperparameters; training a NN requires a weird combination of patience (giving the model enough time to converge) and urgency (stopping runs early when something is wrong). It’s fun, but it can also be frustrating, yet somehow addicting.</p>
<p>I could skip over the two days of elbow grease it took to get this working. However, differentiable logic gate networks train a little differently than your standard dense relu network, and there were a couple things, like how you initialize DLGNs, that surprised me.</p>
<h2 id="wiring">Wiring</h2>
<p>At the start of this project, I wanted to see if I could learn the wires in addition to the gates. I still think it’s possible, but it’s something I had to abandon to get the model to converge.</p>
<p>I started this project by writing a simple dense NN with relu activation and standard SGD, just to see if things were working. They were, and my small model converged very quickly!</p>
<p>In traditional NNs, it’s commonly-accepted wisdom that you should initialize the wire weight matrices according to a tight normal distribution centered around zero. This is what I did for the relu network above, and it worked like a charm!</p>
<p>I switched from <code>relu</code> to <code>gate</code> by adding two weight matrices per layer, one for the right gate input, the other for the left. After this switch, however, try as I might, the model <em>would not</em> converge. I also started to worry about whether having <em>negative</em> wire weights would make it hard to extract the logical circuit after training. So, I thought some more, and decided to initialize wire weights <em>uniformly</em> between 0 and 1. This performed even worse!</p>
<p>Thinking some more, I had an epiphany: “well, since the goal is to learn a wiring, we should softmax the wires in the same way we softmax the gates!” In desperation, I implemented a row-wise softmax over wire weights initialized uniformly… this also went about as well as you would expect. (Poorly.)</p>
<p>At this point I realized: maybe a fixed <code>1</code> or <code>0</code> wiring is not just a random choice, but <em>highly</em> essential! Wouldn’t a fixed wiring let the gradients propagate all the way to the gates at the <em>input</em> end of the network, so the gates at the <em>output</em> end of the network could begin to converge? I began to look at how wiring was implemented in the paper; I decided to abandon my learned-wiring dreams for the time being.</p>
<p>In hindsight, it’s obvious that how you wire a network determines how information flows through it, so it’s important that the wiring is <em>good</em>, especially if the wiring is fixed. I don’t know why it took me so long to realize this.</p>
<p>After I refactored everything to use fixed wiring, I first I tried completely random wiring. This did a lot better than any of the previous approaches, but was still nowhere <em>near</em> the publication. After careful inspection, I realized that with this approach, you risk not wiring gates, or wiring two gates the same way, losing information as it flows through the network.</p>
<p>My next thought was to wire the network like a tree. We had descending power-of-two layers: 64, 32, 16, 8, 4, 2, 1; what if each layer was connected to the corresponding two cells in the layer before it? This way there is total information flow. Tree wiring worked like a charm, and it was at this point that I started to have hope.</p>
<p>I read the publication more closely, and at this point I looked at the <a rel="noopener nofollow" target="_blank" href="https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/diffLogic_CA.ipynb">colab notebook</a>. The wiring technique used is interesting: it guarantees we get unique pairs, like tree wiring, but we also shuffle the branches between layers to allow for some cross-pollination. The algorithm looks something like this:</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>wire_rand_unique</span><span>(</span><span>key</span><span>, </span><span>m</span><span>, </span><span>n</span><span>):
</span><span>    key_rand, key_perm = random.</span><span>split</span><span>(key)
</span><span>    evens   = [(</span><span>0</span><span>, </span><span>1</span><span>), (</span><span>2</span><span>, </span><span>3</span><span>), ...]
</span><span>    odds    = [(</span><span>1</span><span>, </span><span>2</span><span>), (</span><span>3</span><span>, </span><span>4</span><span>), ...]
</span><span>    padding = </span><span>rand_pairs_pad</span><span>(key_rand, m, n)
</span><span>    pairs   = jnp.</span><span>array</span><span>([*evens, *odds, *padding])
</span><span>    perm    = random.</span><span>permutation</span><span>(key_perm, pairs)
</span><span>    </span><span>return </span><span>wire_from_pairs</span><span>(perm.T, m, n)
</span></code></pre>
<p>(Note that as long as n ≤ m ≤ 2×n, there will be total network connection with no random padding.)</p>
<p>With the wiring from the publication implemented, the model was within spitting distance of fully converging! That’s when one last revelation <em>shook me to my bones</em>.</p>
<h2 id="initialize-gates-to-pass-through">Initialize gates to pass through</h2>
<p>The biggest surprise was the way you’re supposed to <em>initialize</em> the gate weights of a differentiable logic gate network; of course, it now makes <em>100% sense</em> in hindsight.</p>
<p>Here’s the big idea: we want the gradients to reach the input of the network, but if we initialize gate weights uniformly, or even randomly, we’ll get a flat activation function. Flat activation functions <em>kill</em> all gradients, period. I realized this was happening when I wrote code to visualize each layer in the network, and watched it change over time:</p>
<pre><code><span># [...]
</span><span>layer 3 (16, 8)
</span><span>▄ ▄ ▄ ▄ ▄ ▄ ▄ ▁ ▄ ▄ ▄ ▁ ▄ ▁ ▁ ▁
</span><span>▄ ▄ ▄ ▄ ▄ ▄ ▄ ▁ ▄ ▄ ▄ ▁ ▄ ▁ ▁ ▁
</span><span>▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ █
</span><span>▁ ▁ ▁ ▄ ▁ ▄ ▄ ▄ ▁ ▄ ▄ ▄ ▄ ▄ ▄ ▄
</span><span>▁ ▁ ▁ ▁ ▁ ▁ ▄ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁
</span><span>▄ ▄ ▄ ▁ ▄ ▁ ▁ ▁ ▄ ▁ ▁ ▁ ▁ ▁ ▁ ▁
</span><span>▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ █
</span><span>▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ █ ▁ ▁ ▁ ▁ ▁ ▁ ▁
</span><span>layer  4 (16, 4)
</span><span>▁ ▁ ▁ ▁ ▁ █ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁
</span><span>▄ ▄ ▄ ▁ ▄ ▁ ▁ ▁ ▄ ▁ ▁ ▁ ▁ ▁ ▁ ▁
</span><span>▁ ▁ ▁ ▁ ▁ ▁ ▁ █ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▄
</span><span>▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ █ ▁ ▁ ▁ ▁ ▁
</span><span>layer 5 (16, 2)
</span><span>▁ ▁ ▁ ▁ ▄ ▁ █ ▁ ▁ ▁ ▁ ▁ ▄ ▁ ▄ ▁
</span><span>▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ █ ▁ ▁ ▁ ▁ ▁
</span><span>layer 6 (16, 1)
</span><span>▁ ▁ █ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁
</span><span>F &amp; . A . B X / / X B . A . &amp; F
</span><span>                ~~~~~~~~~~~~~~~
</span><span># gate order
</span><span>| FALSE | AND  | A&amp;!B  | A    |
</span><span>| B&amp;!A  | B    | XOR   | OR   |
</span><span>| NOR   | XNOR | NOT B | A/!B |
</span><span>| NOT A | B/!A | NAND  | TRUE |
</span></code></pre>
<p>With dead gradients, I’d see the output gate converge, then the one before it, and so on until the gradients reached the input, at which point the later gates were so fixed in their ways that they were impossible to change, even as better earlier gate weights were discovered. Obviously, the performance of the network plateaus shortly thereafter.</p>
<p>I didn’t see an obvious way around this problem, so I started to wonder if there was some weird trick to get around the problem… and there was. So I read the code.</p>
<p>Of all the gates, these two stand out:</p>
<table><thead><tr><th>gate</th><th>relaxation</th><th>explanation</th></tr></thead><tbody>
<tr><td><code>a</code></td><td><code>a</code></td><td>Forward a, drop b.</td></tr>
<tr><td><code>b</code></td><td><code>b</code></td><td>Forward b, drop a.</td></tr>
</tbody></table>
<p>These pass-through gates do no mixing, and will propagate gradients straight along their wires, all the way to the input! This is critical! And in retrospect, I had missed a seemingly-innocuous line:</p>
<blockquote>
<p>To facilitate training stability, the initial distribution of gates is biased toward the pass-through gate.</p>
</blockquote>
<p><em>Of course</em> it is biased! There’s no way to train the network otherwise! So I made a simple one-line change, and, like that, <code>test_loss_hard: 0</code>. <strong>Perfect convergence:</strong></p>
<pre><code><span>Epoch (3001/3001) in 0.0031 s/epoch
</span><span>[ 0.999 0.999 0.00173 0.000287 0.000537 ] [1. 1. 0. 0. 0.] [1. 1. 0. 0. 0.]
</span><span>train_loss: 0.000738; test_loss: 9.91e-05; test_loss_hard: 0
</span></code></pre>
<p>At long last, it all fits together.</p>
<h2 id="hyperfrustration">Hyperfrustration</h2>
<p>What frustrates me is that, after correctly implementing the architecture from the publication, the main issues preventing convergence were seemingly arbitrary hyperparameter choices. In my journal I wrote:</p>
<div>
<p>The model training code was good, as was the gate implementation. So what <em>was</em> wrong?</p>
<ul>
<li>The model was the wrong size (way too small).</li>
<li>The model was initialized the wrong way (randomly, instead of with gate passthrough).</li>
<li>The model did not have clipping in the optimizer, which seems like a rather arbitrary, hacky hyperparameter choice.</li>
</ul>
<p>Hyperparameters suck! I hate that I was pulling my hair out over why I wasn’t getting the same results when all that was different were the hyperparameters at play.</p>
<p>I wonder how many research breakthroughs are possible but just sitting in hyperparameters beyond reach?</p>
</div>
<p>Rather existential, I suppose. There are lots of other things I tried that didn’t work. Earlier, I tried to learn the <em>wiring</em> in addition to gates, but the network wouldn’t converge. I thought it was because the network was too complex, but now I’m convinced it’s because it was too small and initialized incorrectly. There is so much more I would like to try.</p>
<blockquote>
<p><em>N.B.</em> All this wiring got me thinking about gaussian splatting of all things. You see, I was recently reading a paper about <a rel="noopener nofollow" target="_blank" href="https://arxiv.org/abs/2505.05587">when to split gaussian splats</a>; the rough idea is to split whenever the optimizer encounters a saddle with respect to any one splat. While watching underparameterized gate networks train, I would see them get stuck between a handful of gates on a given layer, and not converge further. I wonder if it would be possible to build a circuit network that grows over time, splitting gate saddles when loss stops going down? I suppose that’s what inspired the splat paper, so we’ve come full circle.</p>
</blockquote>

<p>At this point we have learned a <em>perfect</em> binary circuit that looks at a 3×3 cell neighborhood and computes the game of life kernel. Only one problem: This circuit is stuck as a pile of floating point numbers, tangled up in the vast sea of gate weights and wires. How will we ever get it out?</p>
<p>By compiling it to C.</p>
<p>This isn’t as hard as it sounds. All we have to do is <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/blob/89e2ed2f2c018122132598ea610a960900079bea/main.py#L269-L279">traverse the network</a>, layer by layer. We use our old friend <em>argmax</em> to figure out <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/blob/89e2ed2f2c018122132598ea610a960900079bea/main.py#L258-L267">which wires and gate to use</a> for each gate in the circuit.</p>
<p>At this point, we have a massive network. However, only a tiny fraction of the gates are used! (Most gates are still pass-through.) So I implemented two optimizations to clean up the circuit:</p>
<p><strong>Dead code elimination</strong> gets rid of all gates that do not contribute to the output. We’re not doing anything fancy here, like testing for truisms; just doing the simple thing and taking the transitive closure of all dependencies, starting from the root output, going backwards:</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>ext_elim</span><span>(</span><span>instrs</span><span>, </span><span>root</span><span>):
</span><span>    out = []
</span><span>    req = </span><span>set</span><span>([root])
</span><span>    </span><span>for </span><span>instr </span><span>in </span><span>instrs[::-</span><span>1</span><span>]:
</span><span>        (o, idx, l, r) = instr
</span><span>        </span><span>if </span><span>o in req:
</span><span>            req = </span><span>ext_add_deps</span><span>(req, idx, l, r)
</span><span>            out.</span><span>append</span><span>(instr)
</span><span>    </span><span>return </span><span>list</span><span>(out[::-</span><span>1</span><span>])
</span></code></pre>
<p><strong>Copy propagation</strong> lets us get rid of all those pesky useless pass-through gates by forwarding their output to all downstream gates. This time we go forward, and keep track of what to rename pass-through gates.</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>ext_copy_prop</span><span>(</span><span>instrs</span><span>, </span><span>root</span><span>):
</span><span>    out = []
</span><span>    rename = </span><span>dict</span><span>()
</span><span>    </span><span>for </span><span>instr </span><span>in </span><span>instrs:
</span><span>        (o, idx, l, r) = instr
</span><span>        </span><span>if </span><span>l in rename: l = rename[l]
</span><span>        </span><span>if </span><span>r in rename: r = rename[r]
</span><span>        </span><span>if </span><span>o == root: out.</span><span>append</span><span>((o, idx, l, r))
</span><span>        </span><span>elif </span><span>idx == </span><span>3</span><span>: rename[o] = l
</span><span>        </span><span>elif </span><span>idx == </span><span>5</span><span>: rename[o] = r
</span><span>        </span><span>else</span><span>: out.</span><span>append</span><span>((o, idx, l, r))
</span><span>    </span><span>return </span><span>out
</span></code></pre>
<p>(Index <code>3</code> and <code>5</code> are the indices of the pass-through gates.)</p>
<p>After this, we do a simple rename for all gates (since we’ve decimated the number of gates in use), and emit C. This is just a big switch-case:</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>ext_gate_name</span><span>(</span><span>idx</span><span>, </span><span>l</span><span>, </span><span>r</span><span>):
</span><span>    names = [
</span><span>        </span><span>lambda </span><span>a</span><span>, </span><span>b</span><span>: </span><span>"</span><span>0</span><span>"</span><span>,
</span><span>        </span><span>lambda </span><span>a</span><span>, </span><span>b</span><span>: </span><span>f</span><span>"</span><span>{a}</span><span> &amp; </span><span>{b}</span><span>"</span><span>,
</span><span>        </span><span>lambda </span><span>a</span><span>, </span><span>b</span><span>: </span><span>f</span><span>"</span><span>{a}</span><span> &amp; ~</span><span>{b}</span><span>"</span><span>,
</span><span>        </span><span># ...
</span><span>        </span><span>lambda </span><span>a</span><span>, </span><span>b</span><span>: </span><span>f</span><span>"</span><span>~(</span><span>{a}</span><span> &amp; </span><span>{b}</span><span>)</span><span>"</span><span>,
</span><span>        </span><span>lambda </span><span>a</span><span>, </span><span>b</span><span>: </span><span>"</span><span>~0</span><span>"</span><span>,
</span><span>    ]
</span><span>    </span><span>return </span><span>names[idx](l, r)
</span></code></pre>
<p>After formatting, we get a <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/blob/89e2ed2f2c018122132598ea610a960900079bea/gate.c#L7-L172">long list of instructions</a> that looks like this:</p>
<pre data-lang="c"><code data-lang="c"><span>cell </span><span>conway</span><span>(cell </span><span>in</span><span>[</span><span>9</span><span>]) {
</span><span>    </span><span>// ... 157 lines hidden
</span><span>    cell hh = hc ^ gs;
</span><span>    cell hi = hb &amp; ~gy;
</span><span>    cell hj = hh &amp; hi;
</span><span>    cell hk = hg &amp; cy;
</span><span>    cell out = hj | hk;
</span><span>    </span><span>return</span><span> out;
</span><span>}
</span></code></pre>
<p>And like that, all your floats are belong to us. Here’s the final tally for each basic logical operation used in the circuit. (Note that some gates, e.g. nand, are composed of multiple operations):</p>
<table><thead><tr><th>Gate</th><th>Count</th></tr></thead><tbody>
<tr><td><code>and</code></td><td>69</td></tr>
<tr><td><code>or</code></td><td>68</td></tr>
<tr><td><code>xor</code></td><td>17</td></tr>
<tr><td><code>not</code></td><td>16</td></tr>
<tr><td><strong>total</strong></td><td><strong>170</strong></td></tr>
</tbody></table>
<p>Nice. I don’t know why, but it’s satisfying to see a pretty even split between <code>and</code> and <code>or</code>, as with <code>xor</code> and <code>not</code>. I wonder what the distribution looks like in real-world code. (I thought I never used <code>xor</code>, but then I realized that <code>!=</code> is really just <code>xor</code> in disguise.)</p>
<h2 id="bit-crunching-c">Bit-crunching C</h2>
<p>I pulled a sneaky little trick. I don’t know if you noticed. Here’s a hint:</p>
<pre data-lang="c"><code data-lang="c"><span>typedef </span><span>uint64_t </span><span>cell</span><span>;
</span></code></pre>
<p>That’s right: a <code>cell</code> is not one grid cell, but 64! When we calculate <code>cell conway(cell in[9]);</code>, we are, through <em>bit-parallelism</em>, computing the rule on 64 cells at once!</p>
<p>We compile to C and produce a function containing the circuit, <code>conway</code>, but we need a runtime to saturate it. I took a compilers class this semester (6.1100), so I have been writing a lot of <em>Unnamed Subset of C</em> this semester. With C on the mind, I wrote a little runtime. Here’s how it works.</p>
<p>First, I define a board as a collection of cells:</p>
<pre data-lang="c"><code data-lang="c"><span>typedef struct </span><span>{
</span><span>    cell* cells;
</span><span>    size_t cells_len;
</span><span>    size_t width;
</span><span>    size_t height;
</span><span>} </span><span>board_t</span><span>;
</span></code></pre>
<p>Each <code>cell</code> is just a horizontal slab, 64 bits wide. I’m lazy here, so I require <code>width</code> be a multiple of 64. We initialize this board with random state using an <code>xorshift</code> prng I am currently lending from Wikipedia:</p>
<pre data-lang="c"><code data-lang="c"><span>// https://en.wikipedia.org/wiki/Xorshift
</span><span>// constant is frac(golden_ratio) * 2**64
</span><span>// global state bad cry me a river
</span><span>uint64_t rand_state = </span><span>0x9e3779b97f4a7c55</span><span>;
</span><span>
</span><span>uint64_t </span><span>rand_uint64_t</span><span>() {
</span><span>    uint64_t x = rand_state;
</span><span>    x ^= x &lt;&lt; </span><span>13</span><span>;
</span><span>    x ^= x &gt;&gt; </span><span>7</span><span>;
</span><span>    x ^= x &lt;&lt; </span><span>17</span><span>;
</span><span>    rand_state = x;
</span><span>    </span><span>return</span><span> x;
</span><span>}
</span></code></pre>
<p>On to the meat and potatoes. The function <code>conway</code> requires a list of <code>9</code> cells. The neighbors directly above and below are easy: we just index forward or back a row in <code>cells</code>. The side-by-side neighbors are a bit harder. Luckily, we can just bitshift to the left and right to create two new boards. This works as long as we’re careful to <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/blob/89e2ed2f2c018122132598ea610a960900079bea/gate.c#L243-L271">catch all bits</a> that might fall into the bitbucket when we shift:</p>
<pre data-lang="c"><code data-lang="c"><span>void </span><span>board_step_scratch_mut</span><span>(
</span><span>    board_t *</span><span>board</span><span>,
</span><span>    board_t *</span><span>scratch_left</span><span>,
</span><span>    board_t *</span><span>scratch_right</span><span>, </span><span>// syntax highlighting breaks without a comma here, sigh
</span><span>) {
</span><span>    </span><span>// 5: dense loops and bitshifts,
</span><span>    </span><span>// 7: no beauty I can present.
</span><span>    </span><span>// 5: click the link above.
</span><span>}
</span></code></pre>
<p>I guess a better illustration would be, to analogize:</p>
<table><thead><tr><th>ceos hate these seats</th><th>8-bit analogy</th></tr></thead><tbody>
<tr><td><code>scratch_left</code></td><td><code>10011010 10011010 10011010 ...</code></td></tr>
<tr><td><code>board</code></td><td><code>01001101 01001101 01001101 ...</code></td></tr>
<tr><td><code>scratch_right</code></td><td><code>10100110 10100110 10100110 ...</code></td></tr>
</tbody></table>
<p>With all this machinery in place, we can step the board! I’m proud of this code:</p>
<div>
<pre data-lang="c"><code data-lang="c"><span>void </span><span>board_step_mut</span><span>(
</span><span>    board_t *</span><span>board</span><span>,
</span><span>    board_t *</span><span>s_left</span><span>, </span><span>// scratch
</span><span>    board_t *</span><span>s_right</span><span>,
</span><span>    board_t *</span><span>s_out</span><span>,  </span><span>// sigh
</span><span>) {
</span><span>    </span><span>board_step_scratch_mut</span><span>(board, s_left, s_right);
</span><span>
</span><span>    size_t step = board-&gt;width / </span><span>64</span><span>;
</span><span>    size_t wrap = board-&gt;cells_len;
</span><span>
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; board-&gt;cells_len; i++) {
</span><span>        cell in[</span><span>9</span><span>];
</span><span>        size_t i_top = (i + wrap - step) % wrap;
</span><span>        size_t i_bottom = (i + step) % wrap;
</span><span>        </span><span>// top row
</span><span>        in[</span><span>0</span><span>] = s_left-&gt;cells[i_top];
</span><span>        in[</span><span>1</span><span>] = board-&gt;cells[i_top];
</span><span>        in[</span><span>2</span><span>] = s_right-&gt;cells[i_top];
</span><span>        </span><span>// middle row
</span><span>        in[</span><span>3</span><span>] = s_left-&gt;cells[i];
</span><span>        in[</span><span>4</span><span>] = board-&gt;cells[i];
</span><span>        in[</span><span>5</span><span>] = s_right-&gt;cells[i];
</span><span>        </span><span>// bottom row
</span><span>        in[</span><span>6</span><span>] = s_left-&gt;cells[i_bottom];
</span><span>        in[</span><span>7</span><span>] = board-&gt;cells[i_bottom];
</span><span>        in[</span><span>8</span><span>] = s_right-&gt;cells[i_bottom];
</span><span>        </span><span>// update output
</span><span>        s_out-&gt;cells[i] = </span><span>conway</span><span>(in);
</span><span>    }
</span><span>
</span><span>    </span><span>// double-buffering
</span><span>    cell* tmp_cells = board-&gt;cells;
</span><span>    board-&gt;cells = s_out-&gt;cells;
</span><span>    s_out-&gt;cells = tmp_cells;
</span><span>}
</span></code></pre>
<p>*waves* welcome to the club. mention bananas in any comment about this project for immediate +10 respect.</p>
</div>
<p>And there you have it. There’s some more code for printing that I haven’t included here. One call to <code>main</code>, and you’re off to the races! If you’d like to absorb the monstrosity in full, <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/blob/master/gate.c">here’s a link</a>.</p>
<p>(I mean, 331 lines of C, half generated. That can’t hurt anyone.)</p>
<h2 id="benchmarks">Benchmarks</h2>
<p>Benchmarking is always fraught with peril, especially for bold claims like <span>"GUYS! I found a 1,744× speedup!!"</span> so I'd like to qualify exactly what I'm measuring:</p><p>I am comparing the <em>inference speed</em> of my Python JAX (with JIT) implementation against that of my bit-parallel C implementation.</p>
<p><strong>Disclaimer,</strong> because I’m certain someone won’t read the above. You can definitely simulate <em>Conway’s Game of Life</em> with JAX a lot faster by not using a DLGN, if that’s your goal. (Indeed, I have a faster pure-JAX kernel to prepare the boards used for training!) Here, I want to compare floating-point GPU inference to bit-parallel CPU inference. And for fun, if you just want to simulate Conway’s Game of Life, you can totally shred with a faster algorithm like <em>Hashlife</em>, which I’ve <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/hashlife">half-implemented before</a>. These benchmarks, while semi-rigorous, are just for fun!</p>
<p>If you want the exact setup, there are <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic/tree/89e2ed2f2c018122132598ea610a960900079bea?tab=readme-ov-file#to-reproduce">reproduction steps</a> in the repository, with more details in the <a rel="noopener nofollow" target="_blank" href="https://github.com/slightknack/difflogic?tab=readme-ov-file#journal">journal</a>. Here’s my approach:</p>
<p>I use a board of size 512×512 cells. I find the average time per <em>step</em> in the C implementation, by running <em>step</em> on the board 100k times. I find the average time per pass in the Python JAX+JIT implementation, by predicting 512 batches of 512. Here are the results:</p>
<table><thead><tr><th>method</th><th>μs/pass (512×512)</th><th>μs/step (512×512)</th><th>fps</th><th>speedup</th></tr></thead><tbody>
<tr><td>Python</td><td>71200</td><td>—</td><td>14</td><td>1×</td></tr>
<tr><td>C</td><td>—</td><td>40.9</td><td>24,400</td><td>1,744×</td></tr>
</tbody></table>
<p>And there it is: <strong>1,744×</strong>.</p>
<blockquote>
<p><em>N.B.</em> Computing 512 batches of 512 was faster than a single batch of size 512×512 = 262,144, which would have been a more direct comparison. Take 1,744× with a grain of salt, if anything.</p>
</blockquote>
<p>I did some back-of-the-napkin math, and this seems to check out. On the JAX side, the network I’m evaluating is of size [9, 128×17, 64, 32, 16, 8, 4, 2, 1]. Each 128 matrix-vector product requires ~16k floating-point multiplications. We have 17 of them, so we’re looking at at least ~270k flop for a single cell; we have 512×512 cells to evaluate, so lower bound is 70.7 gflop. All things considered, JAX is doing a <em>very</em> good job optimizing the workload. My machine can apparently do about 4.97 tflop/s: dividing that by the estimated 70.7 gflop workload, I get 70.3 fps, and as a lower bound, is ~within an order of magnitude of the 14 fps from the benchmark.</p>
<p>The bit-parallel C implementation, on the other hand, is about <a rel="noopener nofollow" target="_blank" href="https://godbolt.org/z/e3xsYsaYE">~349 instructions long</a> (Godbolt). Each instruction processes 64 bits in parallel, which works out to about 5.45 instructions per bit. There’s quite a bit of register spilling going on, and it takes time to write to memory. Given we have 512×512 cells, it should take around 1.43 million instrs per step. A core on my machine runs at about 3.70 gcycles/s. If we assume instruction latency is 1 cycle, we should expect 2,590 fps. But we measure a number nearly 10× higher! What gives? I expect something along the lines of “insane instruction-level parallelism”, but this is something I’ll have to come back to. Regardless, this is also within an order of magnitude of the measured figure. (Now I’m really curious! I’ll have to dig into it…)</p>
<p>Well, there you have it: <strong>doing less work is indeed faster!</strong> News at 11.</p>
<h2 id="next-steps">Next steps</h2>
<p>I have lots of ideas about what to do next. Some ideas:</p>
<ul>
<li>
<p>Try learning a bigger circuit, like one for fluid simulation, using <a rel="noopener nofollow" target="_blank" href="https://michaelmoroz.github.io/Reintegration-Tracking/">reintegration tracking</a>.</p>
</li>
<li>
<p>Try optimizing further, by vectorizing with SIMD, or outputting a bit-parallel compute shader that runs on the GPU.</p>
</li>
<li>
<p>Try letting <em>you</em> mess around with the project in-browser, by exporting various circuits at different points in training, so you can get a feel for how the network learns.</p>
</li>
</ul>
<p>Well, if you made it this far, you’re one of the real ones. I hope you enjoyed the read and learned something new. I certainly did in writing this! This post isn’t finished; I’d like to add a little in-browser demo, or visualization. Perfect is the enemy of the good. I’m saying adios to this project for now as I have a week off between finishing my first year at MIT and starting an internship writing Rust in SF this summer. I’m sure there will be plenty of time to stress the heck out about optimizing things later in life! Peace out homie.</p>
<p>(Please don’t kill me for writing thousands of words about Conway’s Game of Life without a <em>single</em> picture or animation; I know, I’m working on it. Update: added a 600kb picture LOL. Animation coming soon.)</p>
<p>Thank you to Shaw, Anthony, Mike, Clara and friends for taking a look, fixing typos, and providing feedback/moral support while I worked on <em>difflogic</em>.</p>

<!-- </div> -->

        </div><p>
                Padded so you can keep scrolling. I know. I love you.
                How about we take you <a href="#top">back up to the top of this page</a>?
            </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I rewrote my Mac Electron app in Rust (429 pts)]]></title>
            <link>https://desktopdocs.com/?v=2025</link>
            <guid>44118023</guid>
            <pubDate>Wed, 28 May 2025 16:53:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://desktopdocs.com/?v=2025">https://desktopdocs.com/?v=2025</a>, See on <a href="https://news.ycombinator.com/item?id=44118023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-j7pv25f6=""> <!-- Hero Section --> <div data-astro-cid-j7pv25f6=""> <!-- Header Content --> <div data-astro-cid-j7pv25f6=""> <!-- Product badge --> <p><img src="https://cosmos-media-content.s3.us-east-1.amazonaws.com/public-content/desktop-docs/mr-desktop-docs_240.png" alt="Desktop Docs mascot" data-astro-cid-j7pv25f6=""> <span data-astro-cid-j7pv25f6="">Meet Desktop Docs: Your AI File Explorer</span> </p> <h2 data-astro-cid-j7pv25f6="">
We make it easy <br data-astro-cid-j7pv25f6=""> to
<span data-astro-cid-j7pv25f6="">work with your files</span> </h2> <p data-astro-cid-j7pv25f6="">
Desktop Docs is the all-in-one platform for advanced image and video search.
</p> <!-- CTA -->  </div> <!-- Hero Product Image --> <div data-astro-cid-j7pv25f6=""> <!-- Window chrome -->  <!-- App screenshot with adjusted height and object position --> <p><img src="https://cosmos-media-content.s3.us-east-1.amazonaws.com/public-content/desktop-docs/dd_search_street_art_cropped.webp" alt="Desktop Docs main interface showing AI-powered file organization and search capabilities" loading="eager" data-astro-cid-j7pv25f6=""> </p> </div> <!-- Benefits --> <div data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <p><span data-astro-cid-j7pv25f6="">🧠</span> </p> <h3 data-astro-cid-j7pv25f6="">AI Understanding</h3> <p data-astro-cid-j7pv25f6="">Analyzes content, not just filenames</p> </div> <div data-astro-cid-j7pv25f6=""> <p><span data-astro-cid-j7pv25f6="">🔒</span> </p> <h3 data-astro-cid-j7pv25f6="">100% Private</h3> <p data-astro-cid-j7pv25f6="">All processing happens locally</p> </div> <div data-astro-cid-j7pv25f6=""> <p><span data-astro-cid-j7pv25f6="">⚡</span> </p> <h3 data-astro-cid-j7pv25f6="">Lightning Fast</h3> <p data-astro-cid-j7pv25f6="">Results in under 0.3 seconds</p> </div> <div data-astro-cid-j7pv25f6=""> <p><span data-astro-cid-j7pv25f6="">💰</span> </p> <h3 data-astro-cid-j7pv25f6="">One-Time Payment</h3> <p data-astro-cid-j7pv25f6="">No subscription required</p> </div> </div> </div> <!-- Social Proof Section --> <div data-astro-cid-j7pv25f6=""> <!-- Header with modern styling --> <div data-astro-cid-j7pv25f6=""> <p data-astro-cid-j7pv25f6="">TRUSTED WORLDWIDE</p> <h2 data-astro-cid-j7pv25f6="">
Trusted by Professionals Worldwide
</h2> <p data-astro-cid-j7pv25f6="">
Join thousands of creators and studios who rely on Desktop Docs daily
</p> </div> <div data-astro-cid-j7pv25f6=""> <!-- Studio Card --> <div data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-j7pv25f6="">Production Studios</h3> <p data-astro-cid-j7pv25f6="">Trusted by leading production houses for efficient media management</p> </div> <!-- Creators Card --> <div data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-j7pv25f6="">Professional Creators</h3> <p data-astro-cid-j7pv25f6="">Empowering content creators with smart file organization</p> </div> <!-- Award Card --> <div data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-j7pv25f6="">Featured App</h3> <p data-astro-cid-j7pv25f6="">Recognized in AI Tech Suite for innovation in file management</p> </div> </div> </div> <!-- Trust Badges Section -->  <!-- Social Testimonials Section --> <div data-astro-cid-j7pv25f6=""> <p> Testimonials </p> <p> <h2 data-astro-cid-j7pv25f6="">
What Our Users Say
</h2>  </p> <div> <p data-astro-cid-j7pv25f6="">
Hear from professionals who use Desktop Docs every day
</p>  </div> </div> <!-- Product Showcase Section --> <div data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <p> Browsing Benefits </p> <p> <h2 data-astro-cid-j7pv25f6="">
Do less admin work
</h2>  </p> <div> <p data-astro-cid-j7pv25f6="">
Let AI do the boring work of finding files for you
</p>  </div> </div> <!-- Main Product Demo --> <div data-astro-cid-j7pv25f6=""> <!-- Window chrome -->  <!-- App screenshot container --> <p><img src="https://cosmos-media-content.s3.us-east-1.amazonaws.com/public-content/desktop-docs/dd_image_search_couple_273.webp" alt="Desktop Docs color search interface showing AI-powered palette matching" loading="eager" data-astro-cid-j7pv25f6=""> </p> </div> <!-- Feature Grid with Product Focus --> <div data-astro-cid-j7pv25f6=""> <!-- AI Search Feature --> <div data-astro-cid-j7pv25f6="">  <h3 data-astro-cid-j7pv25f6="">Find Files 10x Faster</h3> <p data-astro-cid-j7pv25f6="">
Desktop Docs analyzes your images and videos' actual content, not just filenames. Search for "sunset photos" or "videos with dogs" and get instant results.
</p> <div data-astro-cid-j7pv25f6=""> <p><span data-astro-cid-j7pv25f6=""></span> <span data-astro-cid-j7pv25f6="">Natural language queries</span> </p> <p><span data-astro-cid-j7pv25f6=""></span> <span data-astro-cid-j7pv25f6="">Content-based matching</span> </p> <p><span data-astro-cid-j7pv25f6=""></span> <span data-astro-cid-j7pv25f6="">Instant results</span> </p> </div> </div> <!-- Visual Search Feature --> <div data-astro-cid-j7pv25f6="">  <h3 data-astro-cid-j7pv25f6="">Image Similarity Search</h3> <p data-astro-cid-j7pv25f6="">
Upload a reference image and instantly find matching content across your entire library. Perfect for finding variations, duplicates, or related images.
</p> <div data-astro-cid-j7pv25f6=""> <p><span data-astro-cid-j7pv25f6=""></span> <span data-astro-cid-j7pv25f6="">Advanced visual matching</span> </p> <p><span data-astro-cid-j7pv25f6=""></span> <span data-astro-cid-j7pv25f6="">Duplicate detection</span> </p> <p><span data-astro-cid-j7pv25f6=""></span> <span data-astro-cid-j7pv25f6="">Cross-format support</span> </p> </div> </div> <!-- Privacy Feature --> <div data-astro-cid-j7pv25f6="">  <h3 data-astro-cid-j7pv25f6="">Your Files Stay on Your Mac</h3> <p data-astro-cid-j7pv25f6="">
No cloud uploads needed. Search right where your files already are, with AI sprinkled on top.
</p> <div data-astro-cid-j7pv25f6=""> <p><span data-astro-cid-j7pv25f6=""></span> <span data-astro-cid-j7pv25f6="">100% local processing</span> </p> <p><span data-astro-cid-j7pv25f6=""></span> <span data-astro-cid-j7pv25f6="">No cloud uploads</span> </p> <p><span data-astro-cid-j7pv25f6=""></span> <span data-astro-cid-j7pv25f6="">Your data stays yours</span> </p> </div> </div> </div> </div> <!-- Value Proposition Section --> <section data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <p><span data-astro-cid-j7pv25f6="">⚡</span> <span data-astro-cid-j7pv25f6="">Stop wasting time. Start getting organized.</span> </p> </div> <div data-astro-cid-j7pv25f6=""> <!-- Key Features Highlight --> <div data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-j7pv25f6="">
Find What You Need, When You Need It
</h3> <p data-astro-cid-j7pv25f6="">
Smart search that understands your files, not just their names
</p> </div> <!-- Color Search Product Demo -->  <div data-astro-cid-j7pv25f6=""> <!-- Feature 1 --> <div data-astro-cid-j7pv25f6="">  <h4 data-astro-cid-j7pv25f6="">Index Everything</h4> <p data-astro-cid-j7pv25f6="">Index unlimited files and never lose track of important documents again.</p> </div> <!-- Feature 2 --> <div data-astro-cid-j7pv25f6="">  <h4 data-astro-cid-j7pv25f6="">Smart Search</h4> <p data-astro-cid-j7pv25f6="">Find media files by their content, not just names.</p> </div> <!-- Feature 3 --> <div data-astro-cid-j7pv25f6="">  <h4 data-astro-cid-j7pv25f6="">Quick Edits</h4> <p data-astro-cid-j7pv25f6="">Make changes on the fly without opening heavy applications.</p> </div> <!-- Feature 4 --> <div data-astro-cid-j7pv25f6="">  <h4 data-astro-cid-j7pv25f6="">Get Organized</h4> <p data-astro-cid-j7pv25f6="">Transform chaos into clarity with a structured knowledge base.</p> </div> </div> </div> </section> <!-- CTA (2) Section --> <div data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">Feel Inspired</h2> <p data-astro-cid-j7pv25f6="">
Join thousands of professionals who've transformed their digital chaos into organized creativity with Desktop Docs.
</p> <!-- Product Showcase -->   </div> <!-- Support Formats Section --> <div data-astro-cid-j7pv25f6=""> <p> File Compatibility </p> <p> <h2 data-astro-cid-j7pv25f6="">
Works with All Your Files
</h2>  </p> <div> <p data-astro-cid-j7pv25f6="">
Desktop Docs supports all major image and video formats, so you can organize everything in one place.
</p>  </div> </div> <!-- FAQ Section --> <div data-astro-cid-j7pv25f6=""> <p> FAQ </p> <p> <h2 data-astro-cid-j7pv25f6="">
Frequently Asked Questions
</h2>  </p> <div> <p data-astro-cid-j7pv25f6="">
Everything you need to know about Desktop Docs
</p>  </div> </div> <!-- Newsletter Section --> <div data-astro-cid-j7pv25f6=""> <p> Stay Connected </p> <p> <h2 data-astro-cid-j7pv25f6="">
Get the Latest Updates
</h2>  </p> <div> <p data-astro-cid-j7pv25f6="">
Subscribe for tips, updates, and insights to maximize your productivity with Desktop Docs.
</p>  </div> </div> <!-- CTA (3) Section --> <div data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">Ready to get organized?</h2> <p data-astro-cid-j7pv25f6="">
Turn creative chaos into beautiful organization - join the Desktop Docs community.
</p>  </div> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Japan Post launches 'digital address' system (212 pts)]]></title>
            <link>https://www.japantimes.co.jp/business/2025/05/27/companies/japan-post-digital-address/</link>
            <guid>44117779</guid>
            <pubDate>Wed, 28 May 2025 16:33:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.japantimes.co.jp/business/2025/05/27/companies/japan-post-digital-address/">https://www.japantimes.co.jp/business/2025/05/27/companies/japan-post-digital-address/</a>, See on <a href="https://news.ycombinator.com/item?id=44117779">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                      <p>Japan Post said Monday that it has launched a "digital address" system that links seven-digit combinations of numbers and letters to physical addresses.</p>

<p>Under the system, users can input these seven-digit codes on online shopping websites, and their addresses will automatically appear on the sites.</p>

<p>People can obtain digital addresses by registering with Japan Post's Yu ID membership service. Their digital addresses will not change even if their physical addresses change. Their new addresses will be linked to the codes if they submit notices of address changes.</p>

<p>With Japan Post opening up a system for obtaining address information using the codes, e-commerce giant Rakuten and others are considering adopting it.</p>

<p>Japan Post plans to spend about a decade to promote broad adoption of the new system.</p>
                    

                                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Compiler Explorer and the promise of URLs that last forever (229 pts)]]></title>
            <link>https://xania.org/202505/compiler-explorer-urls-forever</link>
            <guid>44117722</guid>
            <pubDate>Wed, 28 May 2025 16:28:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xania.org/202505/compiler-explorer-urls-forever">https://xania.org/202505/compiler-explorer-urls-forever</a>, See on <a href="https://news.ycombinator.com/item?id=44117722">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        

        <p>The history is this: back in the old days (2012), we used to store the entire <a href="https://godbolt.org/">Compiler Explorer</a> state in the URL. That got unwieldy (who would have thought encoding an entire compiler state in a URL might get a bit long?), so we added support for Google’s link shortener <a href="https://goo.gl/">goo.gl</a> in March 2014. That meant short links were of the form <code>goo.gl/abc123</code>. Clicking a goo.gl link would eventually redirect you to the full URL link on our site, and we’d decode the state from the URL.</p>
<p>In 2016, <a href="https://stackoverflow.com/">Stack Overflow</a> banned link shorteners because of how they cloak the actual destination of links. Abusers could post innocent goo.gl links that directed folks unwittingly to bad content. However, that meant our Compiler Explorer links were also affected. At the time, we had no intention of storing any user data, so we came up with a hack: we still used goo.gl, but we then rewrote the link we handed out to be <code>godbolt.org/g/abc123</code> (where the abc123 is the goo.gl unique ID). We then redirected any hits to <code>/g/abc123</code> to <code>goo.gl/abc123</code>, which then (finally) redirected back to godbolt.org with the appropriate state in the URL. If you’re keeping track, that’s three redirects to show you some assembly code. We were really committed to making things complicated. Later, we used Google’s API to avoid the redirection dance.</p>
<p>By 2018, the limitations of storing state in the URL started to bite. There’s a limit to how long a URL can be (and we’d already started compressing the data in the URL), so we needed a better solution. We finally implemented our own storage solution: we hash the input, save the state as a JSON document on <a href="https://aws.amazon.com/s3/">S3</a> under the hash, and then give out a shortened form of the hash as a <code>godbolt.org/z/hashbit</code> URL. We use <a href="https://aws.amazon.com/dynamodb/">DynamoDB</a> to store the mapping of shortened hashes to the full paths (accounting for partial collisions, etc.). And, amusingly, we check the short link’s hash for rude words and add deliberate extra information into the document until we no longer get a rude word. Yes, we literally check if your shortened URL contains profanity. Because apparently even random hashes can’t be trusted to keep it clean. This led to <a href="https://github.com/compiler-explorer/compiler-explorer/issues/1297">bug #1297</a>, which remains one of my favourite issues we’ve ever had to fix.</p>
<p>We still support the <code>godbolt.org/g/abc123</code> links, but… despite Google <a href="https://developers.googleblog.com/en/transitioning-google-url-shortener-to-firebase-dynamic-links/">solemnly promising</a> that “all existing links will continue to redirect to the intended destination,” it went read-only a few years back, and now they’re <a href="https://developers.googleblog.com/en/google-url-shortener-links-will-no-longer-be-available/">finally sunsetting it</a> in August 2025. Here I was in 2014, thinking I was so clever using Google’s shortener. “It’ll be around forever!” I said. “Google never discontinues products!” I said. Er…</p>
<p>That means we’ll no longer be able to resolve goo.gl-based links! Which is, to use technical terminology, a bit pants. One of my founding principles is that Compiler Explorer links should last forever. I can’t do anything about the <em>really</em> legacy actual <code>goo.gl</code> links, but I can do something about the <code>godbolt.org/g/abc123</code> links!</p>
<p>Over the last few days, I’ve been scraping everywhere I can think of, collating the links I can find out in the wild, and compiling my own database of links<sup id="fnref:2"><a href="#fn:2">1</a></sup> – and importantly, the URLs they redirect to. So far, I’ve found 12,000 links from scraping:</p>
<ul>
<li><a href="https://developers.google.com/custom-search">Google</a> (using their web search API)</li>
<li><a href="https://docs.github.com/en/rest">GitHub</a> (using their API)</li>
<li>Our own (somewhat limited) web logs</li>
<li>The <a href="https://archive.org/">archive.org</a> Stack Overflow data dumps</li>
<li>Archive.org’s own list of archived webpages</li>
</ul>
<p>
<img src="https://xania.org/202505/sqlite.png" width="600" height="128" alt="SQLite terminal showing query result: SELECT COUNT(*) from google_links; returning 12298 rescued links">
<br>12,298 rescued links and counting - not bad for a few days of digital archaeology
</p>

<p>We’re now <a href="https://github.com/compiler-explorer/compiler-explorer/pull/7724">using the database in preference</a> to <code>goo.gl</code> internally, so I’m also keeping an eye on new “g” links that we don’t yet have.</p>
<p>Thanks to <a href="https://stackoverflow.com/users/224132/peter-cordes">Peter Cordes</a> for reminding us about this issue and <a href="https://github.com/compiler-explorer/compiler-explorer/discussions/7719">bringing it to our attention</a><sup id="fnref:1"><a href="#fn:1">2</a></sup>.</p>
<p>If you have a secret cache of godbolt.org/g/abc123 links you have lying around, now’s the time to visit each of them! That will ensure they’re in my web logs and I’ll add them to the database. Otherwise, sadly, in August 2025 those links will stop working, joining the great digital graveyard alongside Flash games and GeoCities pages.</p>
<h2>The Bigger Picture</h2>
<p>This whole saga reinforces why I’m skeptical of relying on third-party services for critical infrastructure. Google’s URL shortener was supposed to be permanent. The redirect chains we built were clever workarounds that bought us time, but ultimately, the only way to truly keep a promise of “URLs that last forever” is to own the entire stack.</p>
<p>It’s been a fascinating archaeological dig through the internet, hunting down these legacy links like some sort of digital Indiana Jones, except instead of ancient artifacts I’m rescuing compiler flags and optimization examples. Each one represents someone’s attempt to share knowledge, ask a question, or demonstrate a concept. Preserving them feels like preserving a small piece of programming history.</p>
<p>So if you’ve got old Compiler Explorer links bookmarked somewhere, dust them off and give them a click. You’ll be helping preserve a little corner of the internet’s shared knowledge – and keeping a promise I made back in 2012. And hey, at least this time I’m in control of the infrastructure. What could possibly go wrong?</p>
<hr>
<h3>Disclaimer</h3>
<p>This article was written by a human, but links were suggested by and grammar checked by an LLM.</p>

    </div><div>
                
                
                <p>Posted at 10:12:00 CDT on 28<sup>th</sup> May 2025.</p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting a Cease and Desist from Waffle House (270 pts)]]></title>
            <link>https://www.jack.bio/blog/wafflehouse</link>
            <guid>44117302</guid>
            <pubDate>Wed, 28 May 2025 15:48:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jack.bio/blog/wafflehouse">https://www.jack.bio/blog/wafflehouse</a>, See on <a href="https://news.ycombinator.com/item?id=44117302">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span>Editor's Note:</span> <!-- -->This post is written in good faith and with a spirit of humor and admiration. It recounts events between myself, Waffle House, its legal and marketing teams, and affiliated parties. Nothing here is meant with malice, and it was a pleasure corresponding with a breakfast chain I'm quite fond of.</p><article><p>In late September of 2024, Hurricane Helene was spiraling towards Florida, my home state. My university had cancelled classes for the week, and while people were barricading their homes, I was spending time reverse-engineering Waffle House’s website.</p>
<p>Why, you may ask? If you’ve never heard of the Waffle House Index, you’re in for a bit of weirdly fascinating disaster response lore. The Waffle House Index is an (incredibly) unofficial tool used by FEMA to gauge the severity of natural disasters. Why Waffle House? Because they’re infamous for not closing even during the worst of storms. If the House is closed, that means things are getting real.</p>
<p>The problem with the Waffle House Index is that there’s not really an actual “index” you can check. No live feed, no map, and certainly no counter of closed restaurants — just a few wisps of a mention on Wikipedia pages and articles throughout the web (<a href="https://www.wafflehouse.com/how-to-measure-a-storms-fury-one-breakfast-at-a-time/">including one blog post on their actual website!</a>).</p>
<p>...so, naturally, I built one.</p>
<p>I already dove into the technical side of things in a YouTube video I published, which I’ll link below, but here’s the technical gist of how the site worked.</p>
<h2>The Technicals</h2>
<p>Waffle House uses Next.js for their site (or at least, their location information site). Incredibly based choice, by the way. This meant that they also utilized React Server Components, which made it hard to scrape or find any single source of truth for data about the websites.</p>
<p>React Server Components run on the server, and unlike client-side components, they don’t return raw HTML you can easily inspect in Dev Tools. That means you’ll need to get a bit creative to see how data is being fetched.</p>
<p>After spending more time than I’d like to admit digging through their source code, I ended up finding a Next.js file that had a JSON body of the data injected into the client after being executed on the server. This file had information about every single open location, their status (if they were busy or not), and more importantly, <strong>if they were closed</strong>.</p>
<p>Using some light data scraping and processing with Python, along with a Next.js frontend and Redis implementation for caching, I was able to make a live map that tracked which Waffle Houses were closed, and by extension, which parts of the country might be going south.</p>
<p>If you're curious about how I ended up finding the file and the technical implications of the site, watch the YouTube video below:</p>
<p><iframe src="https://www.youtube.com/embed/TBrR3AEutsI?si=k9J_72704VCVnwRq" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<h2>The Beginning of the End</h2>
<p>After I created the site, I snapped up wafflehouseindex[.]org, threw it up on Vercel, and sent out a tweet about it.</p>
<p>At the time, my account had fewer than 200 followers. I genuinely didn’t expect anyone at the company to see the tweet, let alone for it to blow up.</p>
<p>But then they <em>did</em> see it. And suddenly, the corporate account was replying, saying that the information was incorrect and reminding everyone that information about closures would come from their official communication channels.</p>
<p><img src="https://www.jack.bio/images/whstress.png" alt="F"></p>
<p>Which — first of all — I can understand. But I was quite literally USING their data for this, so it wasn’t really incorrect at all. Either way, seeing as the entire situation was making me laugh, I quote-tweeted their response and jokingly said, “haha, now you do!”, not really thinking much of it. Who would think the mega-chain breakfast place would be spending time replying to a teenager online talking about if their stores were closed?</p>
<p>That was the case, until Frank Luntz (American political commentator commonly seen on CNBC, Bloomberg, who also has a 400,000+ follower count on Twitter) somehow saw my tweet and took interest in the site.</p>
<h3>Mom, get the cameras!</h3>
<p>At that point, I started sweating a little. Obviously being recognized for your work online is very cool, but it’s another thing entirely to be casually chatting with a major media figure about a random half-joke, half-engineering marvel I’d made.</p>
<p><img src="https://www.jack.bio/images/whfrank.png" alt="F"></p>
<p>Frank decided to share the site with a tweet directly linking to it. Within minutes, I had a few hundred people browsing the site, poking and prodding at the index and checking out the map.</p>
<p>Unfortunately, the devil works hard, but Waffle House’s marketing (and at this point, probably legal) team works harder. They swiftly replied to Frank’s tweet, echoing what they’d said to me: - the site was unofficial and incorrect, and that any closure info would come from an official Waffle House account. Frank, wanting to maintain credibility, quickly apologized and deleted the tweet.</p>
<p>I figured that would be the end of it until I went to look back at what they said.</p>
<p>Their tweet was gone.</p>
<p>What?</p>
<p>And then I clicked their profile.</p>
<p><img src="https://www.jack.bio/images/waffleblocked.png" alt="Waffle House emailing me"></p>
<h3>Kicked (and scolded) out the House</h3>
<p>In a million years did I not expect that one coming. Two responses to their tweets about an “unofficial” website and I get blocked?</p>
<p>In hindsight, I can imagine that my actions probably stressed some poor marketing person out and they needed to do damage control somehow. Regardless, it’s incredibly funny that I got <em>blocked</em> by the breakfast chain.</p>
<p>Shortly after getting blocked (and after the hurricane had passed), I woke up to an email from someone with a Very High Up Position at Waffle House informing me that I needed to immediately “cease and desist all unauthorized use of Trademarks owned by WH Intellectual, LLC and any confusingly similar marks in connection with your website.”</p>
<p><img src="https://www.jack.bio/images/whemail.png" alt="Waffle House emailing me"></p>
<p>Honestly, I was more surprised that the silly logo I made (a very great representation, if I do say so myself) was what got me in trouble, and less so the scraping or reverse-engineering part.</p>
<p>You may be asking yourself: <em>How did I reply?</em></p>
<p>Great question.</p>
<h3>With Respect and Syrup</h3>
<p>I gave my response the same treatment as I did the rest of this entire process: complete silliness.</p>
<p><img src="https://www.jack.bio/images/whemailreply.png" alt="Me replying to Waffle House"></p>
<ul>
<li>“huge fan of the House”</li>
<li>"Waffle House has become much like the American Flag in the Star Spangled Banner"</li>
<li>“honor and respect Waffle House with this data”</li>
<li>and finally, “with respect and syrup”.</li>
</ul>
<p>The Very High Up Person actually did reply to this with much less legal jargon than the first email and was very down to earth with me, even thanking me for wanting to help out with the recovery efforts. But at the end of the day, I was still violating their trademarks and needed to take it down.</p>
<p>I did take it down and even emailed back asking if there was anything I could do to keep the site running officially under the same branding I had created, but unfortunately got ghosted after that.</p>
<h2>The End of the End</h2>
<p>One of my favorite parts of programming is the ability to simply build things just for the fun of it, and this project was nothing short of exactly that. While I do wish I could have kept it up for longer than a few weeks, being able to use data that seems meaningless on the surface to build something bigger is always such an adventure.</p>
<p>Thanks again to Waffle House for being good sports in the end, even after I may have violated their trademarks…and stole their data along the way :)</p>
<i><small><p>Thanks to Moo, Kai, and the Babgel GC for proofreading &amp; editing. &lt;3</p></small></i></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Tesseral – Open-Source Auth (144 pts)]]></title>
            <link>https://github.com/tesseral-labs/tesseral</link>
            <guid>44117059</guid>
            <pubDate>Wed, 28 May 2025 15:27:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tesseral-labs/tesseral">https://github.com/tesseral-labs/tesseral</a>, See on <a href="https://news.ycombinator.com/item?id=44117059">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/splash.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/splash.png" alt=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tesseral</h2><a id="user-content-tesseral" aria-label="Permalink: Tesseral" href="#tesseral"></a></p>
<p dir="auto">Tesseral is <strong>open source auth infrastructure for business software</strong> (i.e., B2B
SaaS).</p>
<p dir="auto">Tesseral is a multi-tenant, API-first service designed to run on the cloud. It
is not an authentication library tied to a particular language or framework;
Tesseral works with any tech stack.</p>
<p dir="auto">Most developers should start by using Tesseral's managed service, available at
<a href="https://console.tesseral.com/" rel="nofollow">console.tesseral.com</a>. You can also <a href="https://tesseral.com/docs/features/self-hosting-tesseral" rel="nofollow">self-host
Tesseral</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key Features</h2><a id="user-content-key-features" aria-label="Permalink: Key Features" href="#key-features"></a></p>
<p dir="auto">Tesseral bundles everything that a developer needs to manage users in business software.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><a target="_blank" rel="noopener noreferrer" href=""></a></th>
<th><a target="_blank" rel="noopener noreferrer" href=""></a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://tesseral.com/docs/features/customizing-your-login-experience" rel="nofollow"><strong>Hosted, customizable login pages</strong></a><p>Prebuilt UIs, customizable to your brand. Add and remove login methods with just a few clicks in the Tesseral Console.</p></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/hosted_custom_login_pages.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/hosted_custom_login_pages.png"></a></td></tr><tr></tr>

<tr>
<td><a href="https://tesseral.com/docs/features/b2b-multitenancy" rel="nofollow"><strong>B2B multitenancy</strong></a><p>Tesseral is built for B2B SaaS. Your customer's admins control how their users log in to their tenant, and can add or remove users at will.</p></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/b2b_multitenancy.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/b2b_multitenancy.png"></a></td></tr><tr></tr>

<tr>
<td><a href="https://tesseral.com/docs/features/user-impersonation" rel="nofollow"><strong>User impersonation</strong></a><p>See exactly what your users see. Debug and support faster by logging in as your users.</p></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/impersonate.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/impersonate.png"></a></td></tr><tr></tr>

<tr>
<td><a href="https://tesseral.com/docs/features/self-serve-organization-settings" rel="nofollow"><strong>Self-service config for your customers</strong></a><p>Pre-built settings pages where your customers can invite coworkers, edit their login settings, and everything else they need.</p></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/self_serve.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/self_serve.png"></a></td></tr><tr></tr>

<tr>
<td><a href="https://tesseral.com/docs/login-methods/primary-factors/log-in-with-email-magic-links" rel="nofollow"><strong>Magic Links</strong></a><p>Add "Log in with Email" support using magic links, without writing any code.</p></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/magic_links.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/magic_links.png"></a></td></tr><tr></tr>

<tr>
<td><strong>Social Login</strong><p>Add <a href="https://tesseral.com/docs/login-methods/primary-factors/log-in-with-google" rel="nofollow">Log in with Google</a>, <a href="https://tesseral.com/docs/login-methods/primary-factors/log-in-with-github" rel="nofollow">Log in with GitHub</a>, and <a href="https://tesseral.com/docs/login-methods/primary-factors/log-in-with-microsoft" rel="nofollow">Log in with Microsoft</a> support without writing any code.</p></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/social_login.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/social_login.png"></a></td></tr><tr></tr>

<tr>
<td><a href="https://tesseral.com/docs/features/saml-sso" rel="nofollow"><strong>SAML (Enterprise Single Sign-On)</strong></a><p>Add SAML support to your product without writing any code.</p></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/saml.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/saml.png"></a></td></tr><tr></tr>

<tr>
<td><a href="https://tesseral.com/docs/features/scim-provisioning" rel="nofollow"><strong>SCIM (Enterprise Directory Sync)</strong></a><p>Add SCIM support to your product without writing any code.</p></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/scim.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/scim.png"></a></td></tr><tr></tr>

<tr>
<td><a href="https://tesseral.com/docs/features/role-based-access-control" rel="nofollow"><strong>Role-based access control (RBAC)</strong></a><p>Add fine-grained permissions to your product. The UI's done for you, just plug in <code>hasPermission</code> calls wherever you need them.</p></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/rbac.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/rbac.png"></a></td></tr><tr></tr>

<tr>
<td><a href="https://tesseral.com/docs/features/multifactor-authentication-mfa" rel="nofollow"><strong>Multi-factor authentication (MFA)</strong></a><p>Add 2FA to your product without writing any code. Your customers can choose to require MFA for their users if they wish.</p></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/mfa.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/mfa.png"></a></td></tr><tr></tr>

<tr>
<td><a href="https://tesseral.com/docs/login-methods/secondary-factors/login-with-passkey" rel="nofollow"><strong>Passkeys / WebAuthn</strong></a><p>Add "Log in with Passkey" support to your product without writing any code. Supports all passkey platforms, including Touch ID, Yubikeys, and more.</p></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/passkeys.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/passkeys.png"></a></td></tr><tr></tr>

<tr>
<td><a href="https://tesseral.com/docs/login-methods/secondary-factors/login-with-authenticator-app" rel="nofollow"><strong>Authenticator apps (TOTPs)</strong></a><p>Add time-based one-time-password (TOTP) support to your product without writing any code.</p></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/totp.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/totp.png"></a></td></tr><tr></tr>

<tr>
<td><a href="https://tesseral.com/docs/features/managed-api-keys" rel="nofollow"><strong>API key management</strong></a><p>Not just user authentication. If you want your customers to call your endpoints automatically, give them API keys. UIs, permissions, and authentication checks all come pre-built.</p></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/api_keys.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/api_keys.png"></a></td></tr><tr></tr>

<tr>
<td><a href="https://tesseral.com/docs/concepts/user-invites" rel="nofollow"><strong>User invitations</strong></a><p>Your users can invite their coworkers, or you can invite them yourself from the Tesseral Console.</p></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/invites.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/invites.png"></a></td></tr><tr></tr>

<tr>
<td><a href="https://tesseral.com/docs/features/webhooks" rel="nofollow"><strong>Webhooks</strong></a><p>Live-sync data from Tesseral into your database with realtime webhook delivery.</p></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/tesseral-labs/tesseral/blob/main/.github/img/webhooks.png"><img src="https://github.com/tesseral-labs/tesseral/raw/main/.github/img/webhooks.png"></a></td></tr><tr></tr>

</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get Started</h2><a id="user-content-get-started" aria-label="Permalink: Get Started" href="#get-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Read the documentation</h3><a id="user-content-read-the-documentation" aria-label="Permalink: Read the documentation" href="#read-the-documentation"></a></p>
<p dir="auto">We encourage all developers to read the full documentation first, which is
available at tesseral.com/docs. This README provides only a very brief subset of
the docs to illustrate some basic ideas.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">SDKs</h3><a id="user-content-sdks" aria-label="Permalink: SDKs" href="#sdks"></a></p>
<p dir="auto">Tesseral currently offers several SDKs for common web development frameworks.</p>
<ul dir="auto">
<li>Clientside SDKs
<ul dir="auto">
<li><a href="https://tesseral.com/docs/sdks/clientside-sdks/tesseral-sdk-react" rel="nofollow">React</a></li>
</ul>
</li>
<li>Serverside SDKs
<ul dir="auto">
<li><a href="https://tesseral.com/docs/sdks/serverside-sdks/tesseral-sdk-express" rel="nofollow">Express</a></li>
<li><a href="https://tesseral.com/docs/sdks/serverside-sdks/tesseral-sdk-flask" rel="nofollow">Flask</a></li>
<li><a href="https://tesseral.com/docs/sdks/serverside-sdks/tesseral-sdk-go" rel="nofollow">Golang</a></li>
</ul>
</li>
</ul>
<p dir="auto">More SDKs, in particular Next.js, are in active development. If you do not see
your preferred framework listed here, please get in touch with
<a href="mailto:support@tesseral.com">support@tesseral.com</a>; we may be able to give you early access.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Sign up</h3><a id="user-content-sign-up" aria-label="Permalink: Sign up" href="#sign-up"></a></p>
<p dir="auto">For Tesseral’s managed service, you will first need to create an account at
<a href="https://console.tesseral.com/" rel="nofollow">https://console.tesseral.com</a>.</p>
<p dir="auto">You will need to create a Project and generate a Publishable Key. Publishable
Keys always look like this: <code>publishable_key_...</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integrate your frontend</h3><a id="user-content-integrate-your-frontend" aria-label="Permalink: Integrate your frontend" href="#integrate-your-frontend"></a></p>
<p dir="auto">To integrate Tesseral into your app, you'll first need to integrate your
frontend. This example uses the <a href="https://tesseral.com/docs/sdks/clientside-sdks/tesseral-sdk-react" rel="nofollow">Tesseral React
SDK</a>.</p>
<p dir="auto">Install the SDK like this:</p>
<div data-snippet-clipboard-copy-content="npm install @tesseral/tesseral-react"><pre><code>npm install @tesseral/tesseral-react
</code></pre></div>
<p dir="auto">Then, using your Publishable Key (starts with <code>publishable_key_...</code>), wrap your
React app in the <code>&lt;TesseralProvider&gt;</code> component:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import { createRoot } from &quot;react-dom/client&quot;
import { TesseralProvider } from &quot;@tesseral/tesseral-react&quot;;
import App from &quot;./App.tsx&quot;

const root = createRoot(document.getElementById(&quot;root&quot;)) 
root.render(
  // use your Project's Publishable Key here
  <TesseralProvider publishableKey=&quot;publishable_key_...&quot;>
    <App />
  </TesseralProvider>
)"><pre><span>import</span> <span>{</span> <span>createRoot</span> <span>}</span> <span>from</span> <span>"react-dom/client"</span>
<span>import</span> <span>{</span> <span>TesseralProvider</span> <span>}</span> <span>from</span> <span>"@tesseral/tesseral-react"</span><span>;</span>
<span>import</span> <span>App</span> <span>from</span> <span>"./App.tsx"</span>

<span>const</span> <span>root</span> <span>=</span> <span>createRoot</span><span>(</span><span>document</span><span>.</span><span>getElementById</span><span>(</span><span>"root"</span><span>)</span><span>)</span> 
<span>root</span><span>.</span><span>render</span><span>(</span>
  <span>// use your Project's Publishable Key here</span>
  <span>&lt;</span><span>TesseralProvider</span> <span>publishableKey</span><span>=</span><span>"publishable_key_..."</span><span>&gt;</span>
    <span>&lt;</span><span>App</span> <span>/&gt;</span>
  <span>&lt;/</span><span>TesseralProvider</span><span>&gt;</span>
<span>)</span></pre></div>
<p dir="auto">The <code>&lt;TesseralProvider&gt;</code> will handle a variety of auth-related tasks for you,
including:</p>
<ul dir="auto">
<li>Redirecting unauthenticated users to the login page ("login gating")</li>
<li>Refreshing users' access tokens in the background when they're close to
expiring</li>
<li>Automatically including access tokens in requests from your frontend
to your backend</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integrate your backend</h3><a id="user-content-integrate-your-backend" aria-label="Permalink: Integrate your backend" href="#integrate-your-backend"></a></p>
<p dir="auto">Once you have your frontend integrated with Tesseral, you'll then need to
integrate your backend.</p>
<p dir="auto">Tesseral works with any backend or framework. SDKs are available for the
following:</p>
<ul dir="auto">
<li><a href="https://tesseral.com/docs/sdks/serverside-sdks/tesseral-sdk-express" rel="nofollow">Express</a></li>
<li><a href="https://tesseral.com/docs/sdks/serverside-sdks/tesseral-sdk-flask" rel="nofollow">Flask</a></li>
<li><a href="https://tesseral.com/docs/sdks/serverside-sdks/tesseral-sdk-go" rel="nofollow">Golang</a></li>
</ul>
<p dir="auto">Your app might look something like this example, using the <a href="https://tesseral.com/docs/sdks/serverside-sdks/tesseral-sdk-flask" rel="nofollow">Flask
SDK</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from flask import Flask
from tesseral_flask import access_token_claims, require_auth


app = Flask(__name__)

# use the same Publishable Key you used for your frontend
app.before_request(require_auth(publishable_key=&quot;publishable_key_...&quot;))


@app.route(&quot;/api/hello&quot;, methods=[&quot;GET&quot;])
def hello():
    # get the user's email from the current request
    # Tesseral ensures that user emails are always verified
    email = access_token_claims().user.email
    return (&quot;hello, &quot; + email)


if __name__ == &quot;__main__&quot;:
    app.run(debug=True, port=5050)"><pre><span>from</span> <span>flask</span> <span>import</span> <span>Flask</span>
<span>from</span> <span>tesseral_flask</span> <span>import</span> <span>access_token_claims</span>, <span>require_auth</span>


<span>app</span> <span>=</span> <span>Flask</span>(<span>__name__</span>)

<span># use the same Publishable Key you used for your frontend</span>
<span>app</span>.<span>before_request</span>(<span>require_auth</span>(<span>publishable_key</span><span>=</span><span>"publishable_key_..."</span>))


<span>@<span>app</span>.<span>route</span>(<span>"/api/hello"</span>, <span>methods</span><span>=</span>[<span>"GET"</span>])</span>
<span>def</span> <span>hello</span>():
    <span># get the user's email from the current request</span>
    <span># Tesseral ensures that user emails are always verified</span>
    <span>email</span> <span>=</span> <span>access_token_claims</span>().<span>user</span>.<span>email</span>
    <span>return</span> (<span>"hello, "</span> <span>+</span> <span>email</span>)


<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>app</span>.<span>run</span>(<span>debug</span><span>=</span><span>True</span>, <span>port</span><span>=</span><span>5050</span>)</pre></div>
<p dir="auto">Tesseral's
<a href="https://tesseral.com/docs/sdks/serverside-sdks/tesseral-sdk-flask#getting-started" rel="nofollow"><code>require_auth()</code></a>
middleware (or its equivalent in your framework's SDK) validates access tokens
for you, and only authenticated requests will go through to your endpoint
handlers. A client can successfully <code>GET /api/hello</code> if and only if it has a
valid Tesseral access token.</p>
<p dir="auto">You can extract out details about the requester using:</p>
<ul dir="auto">
<li><a href="https://tesseral.com/docs/sdks/serverside-sdks/tesseral-sdk-flask#getting-the-current-organization" rel="nofollow"><code>organization_id()</code></a></li>
<li><a href="https://tesseral.com/docs/sdks/serverside-sdks/tesseral-sdk-flask#getting-the-requests-authenticated-credentials" rel="nofollow"><code>credentials()</code></a></li>
<li><a href="https://tesseral.com/docs/sdks/serverside-sdks/tesseral-sdk-flask#getting-details-about-the-current-user" rel="nofollow"><code>access_token_claims()</code></a></li>
<li><a href="https://tesseral.com/docs/features/role-based-access-control#permission-checks" rel="nofollow"><code>has_permission()</code></a></li>
</ul>
<p dir="auto">Or their equivalent in your framework's SDK.</p>
<p dir="auto">Once you have your backend integrated, you have implemented Tesseral!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto"><a href="https://github.com/tesseral-labs/tesseral/blob/main/LICENSE">MIT</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We welcome outside contributions!</p>
<p dir="auto">Please be aware, however, that auth software is complex and extremely delicate.
We are very cautious with the changes that we merge. We recommend you first open
a GitHub issue outlining any proposed changes.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security</h2><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">Please immediately report any potential vulnerabilities to
<a href="mailto:security@tesseral.com">security@tesseral.com</a>. We will get back to you over email.</p>
<p dir="auto">Please do not open GitHub issues for any security-related concerns.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Community</h2><a id="user-content-community" aria-label="Permalink: Community" href="#community"></a></p>
<p dir="auto">We love enterprise software and the people building it.</p>
<p dir="auto">Please join our community and stay up to date on new releases, events, and other
Tesseral news by following us on
<a href="https://www.linkedin.com/company/tesseral" rel="nofollow">LinkedIn</a> and on <a href="https://x.com/tesseralhq" rel="nofollow">X
(Twitter)</a>. You can also check out our
<a href="https://newsletter.tesseral.com/" rel="nofollow">newsletter</a> and our
<a href="https://tesseral.com/blog" rel="nofollow">blog</a>.</p>
<p dir="auto">You should also feel welcome to get in touch at <a href="mailto:founders@tesseral.com">founders@tesseral.com</a> with
questions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Who we are</h2><a id="user-content-who-we-are" aria-label="Permalink: Who we are" href="#who-we-are"></a></p>
<p dir="auto">This is commercial open source software managed by Tesseral, a startup based in
San Francisco. We previously built
<a href="https://github.com/ssoready/ssoready">SSOReady</a>, an open source middleware for
SAML SSO and SCIM provisioning.</p>
<p dir="auto">Primary technical responsibility for Tesseral belongs to <a href="https://ucarion.com/" rel="nofollow">Ulysse
Carion</a>, cofounder and CTO at Tesseral, and to Tesseral's
technical staff: <a href="https://github.com/blakeofwilliam">Blake Williams</a> and <a href="https://github.com/dnys1">Dillon
Nys</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLM codegen go brrr – Parallelization with Git worktrees and tmux (101 pts)]]></title>
            <link>https://www.skeptrune.com/posts/git-worktrees-agents-and-tmux/</link>
            <guid>44116872</guid>
            <pubDate>Wed, 28 May 2025 15:13:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.skeptrune.com/posts/git-worktrees-agents-and-tmux/">https://www.skeptrune.com/posts/git-worktrees-agents-and-tmux/</a>, See on <a href="https://news.ycombinator.com/item?id=44116872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-g44mhg6z="">  <article> <p> May 26, 2025 , Nicholas Khami </p>   <p> <h6> If you're underwhelmed with AI coding agents or simply want to get more out of them, give parallelization a try. After seeing the results firsthand over the past month, I'm ready to call myself an evangelist. The throughput improvements are incredible, and I don't feel like I'm losing control of the codebase. </h6> </p>  <p>This realization isn’t unique to me; the effectiveness of using Git worktrees for simultaneous execution is gaining broader recognition, as evidenced by mentions in <a href="https://docs.anthropic.com/en/docs/claude-code/tutorials#run-parallel-claude-code-sessions-with-git-worktrees">Claude Code’s docs</a>, <a href="https://news.ycombinator.com/item?id=44043717">discussion on Hacker News</a>, projects like <a href="https://github.com/smtg-ai/claude-squad">Claude Squad</a>, and conversation on <a href="https://x.com/search?q=git%20worktree&amp;src=typed_query&amp;f=live">X</a>.</p>
<br>
<div>  <p>
I had to vibe code <i>something</i> for this post haha. Use mouse or touch to
    rotate and zoom. Click "Reset View" to return to the initial angle.
</p> </div> 
<h3 id="example-use-case-adding-a-ui-component">Example use-case: adding a UI component</h3>
<p>I’m building a component library called <a href="https://astrobits.dev/">astrobits</a> and wanted to add a <code>Toggle</code>. To tackle the task, I deployed two <a href="https://www.anthropic.com/claude-code">Claude Code</a> agents and two <a href="https://openai.com/index/introducing-codex/">Codex</a> agents, all with the same prompt, running in parallel within their own <a href="https://git-scm.com/docs/git-worktrees">git worktrees</a>. Worktrees are essential because they provide each agent with an isolated directory, allowing them to execute simultaneously without overwriting each other’s changes.</p>
<p>The number of agents I choose to rollout depends on the complexity of the task. Over time, you’ll develop an intuition for estimating the right number based on the situation. Here, I felt 4 was appropriate.</p>
<br>
<div data-astro-cid-xaorfurn=""> <div data-astro-cid-xaorfurn=""> <p><img src="https://www.skeptrune.com/_astro/4320-attempt.s98T-aCF_ZLWpD1.webp" alt="Worktree example run toggle image" data-astro-cid-xaorfurn="true" width="365" height="365" loading="lazy" decoding="async"></p><p data-astro-cid-xaorfurn=""><code>claude-1</code>. Mostly <b>correct</b>, workable, but pixelated border-image and shadow needs fixing.</p> </div><div data-astro-cid-xaorfurn=""> <p><img src="https://www.skeptrune.com/_astro/4321-attempt.BHAntrrQ_Z25wuP0.webp" alt="Worktree example run toggle image" data-astro-cid-xaorfurn="true" width="365" height="365" loading="lazy" decoding="async"></p><p data-astro-cid-xaorfurn=""><code>claude-2</code>. Completely <b>broken</b>, sliding circle too small, wrong color.</p> </div><div data-astro-cid-xaorfurn=""> <p><img src="https://www.skeptrune.com/_astro/4323-attempt.A25xk5py_Z7SM6S.webp" alt="Worktree example run toggle image" data-astro-cid-xaorfurn="true" width="365" height="365" loading="lazy" decoding="async"></p><p data-astro-cid-xaorfurn=""><code>codex-1</code>. Very <b>wrong</b> Shadow on top, active state wrong side.</p> </div><div data-astro-cid-xaorfurn=""> <p><img src="https://www.skeptrune.com/_astro/4324-attempt.DnwzufAQ_28akmr.webp" alt="Worktree example run toggle image" data-astro-cid-xaorfurn="true" width="365" height="365" loading="lazy" decoding="async"></p><p data-astro-cid-xaorfurn=""><code>codex-2</code>. Is <b>unusable</b>, circle color wrong, active side incorrect.</p> </div> </div> 

<p>Voila, results! Only one of the four LLMs produced a solution that actually saved me time. This validates the necessity of rolling multiple agents: if each has a <code>~25%</code> chance of producing something useful, then running four gives a <code>68%</code> chance that at least one will succeed <code>(1 - 0.75^4 ≈ 0.68)</code>. Four agents was essentially the bare minimum to have reasonable confidence in getting a workable solution.</p>
<p>With LLMs being so affordable, there’s virtually no downside to running multiple agents. The cost difference between using one agent ($0.10) versus four ($0.40) is negligible compared to the 20 minutes of development time saved. Since the financial risk is minimal, you can afford to be aggressive with parallelization. If anything, I could have run even more agents to further increase the odds of getting a perfect solution on the first try.</p>
<p>And yet, the process of running them is still cumbersome and manual, it’s more effort to setup 8 than 4, so I’m often lazy and opt to run the minimum number of agents I think will get the job done. This is where the problem comes in, and why I’m excited to share my proposed solution.</p>
<h3 id="current-workflow-pain-points">Current workflow pain points</h3>
<p>Right now, I manually create git worktrees using <code>git worktree add -b newbranch ../path</code>, start a <code>tmux</code> session for each one, run Claude Code in the first pane, paste a prompt, <code>leader+c</code> into a new pane, run <code>yarn dev</code> to get a preview, switch to my browser to review, repeat if no agents succeed, then finally commit, push, and create a PR once I’m satisfied with an output.</p>
<p>Here are the top frustrations:</p>
<ul>
<li>I can’t tell which branch a worktree was most recently rebased onto. For example, if <code>agent-1</code> was rebased onto <code>feature-x</code> but <code>agent-2</code> onto <code>main</code>, it’s easy to lose track without manual notes.</li>
<li>There is no easy way to send the same prompt to multiple agents at once. For instance, if all agents are stuck on the same misunderstanding of the requirements, I have to copy-paste the clarification into each session.</li>
<li>I really wish I had a shortcut to open my IDE for a given worktree without having to <code>tmux a</code>, <code>leader + c</code>, and <code>code .</code> manually. I could use a long one-liner with <code>tmux send-keys and xargs</code> to automate this, but that still feels clunky.</li>
<li>Web previewing is a pain. I have to run <code>yarn dev</code> in each worktree, and then hold the mental model of which port each worktree is on. Automating a reverse proxy to handle this with a decent naming scheme would be a game-changer.</li>
<li>Committing and creating pull requests (PRs) is also more cumbersome than it should be. For example, after finding a solution in <code>agent-3</code>, I have to manually attach to that tmux session then <code>commit</code>, <code>push</code>, and <code>gh pr</code>.</li>
</ul>
<p>I feel like I’ve been through the wringer enough times with this process that I can see a solution shape which would create a smoother experience.</p>
<h3 id="proposing-a-solution-uzi">Proposing a solution: <em>uzi</em></h3>
<p>To address these challenges head-on, the ideal developer experience (DX) would involve a lightweight CLI that wraps tmux, automating this complex orchestration. My co-founder Denzell and I felt these pain points acutely enough that we’ve begun developing such a tool, which we’re calling <a href="https://github.com/devflowinc/uzi"><em>uzi</em></a>. The core idea behind <em>uzi</em> is to abstract away the manual, repetitive tasks involved in managing multiple AI agent worktrees.</p>
<p>See some of the <code>uzi</code> commands we are thinking to implement below. Our goal is to make the workflow more seamless while sticking closely to the existing mechanics of worktres and tmux. We want to make sure that we feel at home using <code>uzi</code> alongside standard unix tools like <code>xargs</code>, <code>grep</code>, and <code>awk</code>.</p>
<ul>
<li><code>uzi start --agents claude:3,codex:2 --prompt "Implement feature X"</code> could initialize and prompt three Claude instances and two Codex instances, each in its own worktree.</li>
<li><code>uzi ls</code> would display all active agents, their target branches, and current statuses.</li>
<li><code>uzi exec --all -- yarn dev</code> could run a command like <code>yarn dev</code> across all agent worktrees.</li>
<li><code>uzi broadcast -- "Refine the previous response by focusing on Y"</code> would send a follow-up prompt to all active agents.</li>
<li><code>uzi checkpoint --agent claude-1 --message "Implemented initial draft"</code> could rebase the specified agent’s worktree and commit the changes.</li>
<li><code>uzi kill --agent codex-2</code> would clean up a specific agent’s tmux session and optionally its worktree.</li>
</ul>
<p>These commands would primarily operate via <code>tmux send-keys</code> instructions to the appropriate sessions. We don’t want to reinvent the wheel; we just want to polish the existing process and make it more efficient.</p>
<h3 id="the-future-is-parallel-beyond-code">The Future is Parallel: Beyond Code</h3>
<p>While <code>uzi</code> focuses on software developers, its methodology isn’t limited to tech; the principle of leveraging multiple agents running in parallel to increase the odds of finding an optimal solution applies universally.</p>
<p>Consider a company like <a href="https://www.versionstory.com/">versionstory</a>, which is pioneering version control for transactional lawyers. An attorney could leverage their software to run multiple instances of an agent to redline a contract. After reviewing the outputs, they could select and merge the best components to finalize the document. This approach would provide additional confidence in the quality of the final review as it would be based on multiple independent analyses rather than a single agent’s output.</p>
<p>Similarly, a marketing team could employ this parallel strategy to perform data analysis on ad performance. By prompting multiple AI instances, they could quickly gather a range of analyses, review them, and select the most insightful ones to inform their strategy. More coverage of the solution space leads to better decision-making and more effective campaigns.</p>
<p>This parallel paradigm isn’t just a new technique for developers; it’s a glimpse into a more efficient, robust, and powerful future for AI-assisted productivity across various fields. I expect to see existing software products start to gain more powerful version control and parallel execution capabilities which emulate the workflow enabled by git worktrees for software development.</p>
<p>My DMs are open if you want to chat about this topic or have any questions. I’m happy to discuss.</p> </article>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[xAI to pay telegram $300M to integrate Grok into the chat app (267 pts)]]></title>
            <link>https://techcrunch.com/2025/05/28/xai-to-invest-300m-in-telegram-integrate-grok-into-app/</link>
            <guid>44116862</guid>
            <pubDate>Wed, 28 May 2025 15:12:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/05/28/xai-to-invest-300m-in-telegram-integrate-grok-into-app/">https://techcrunch.com/2025/05/28/xai-to-invest-300m-in-telegram-integrate-grok-into-app/</a>, See on <a href="https://news.ycombinator.com/item?id=44116862">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<div>
		<figure><img width="1024" height="683" src="https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?w=1024" alt="The logo for Telegram Signal messenger application arranged on a smartphone." decoding="async" fetchpriority="high" srcset="https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg 6427w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=150,100 150w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=300,200 300w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=768,512 768w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=680,453 680w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=1200,800 1200w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=1280,853 1280w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=430,287 430w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=720,480 720w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=900,600 900w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=800,533 800w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=1536,1024 1536w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=2048,1365 2048w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=668,445 668w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=562,375 562w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=925,617 925w, https://techcrunch.com/wp-content/uploads/2024/06/telegram-app-cellphone.jpg?resize=708,472 708w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><strong>Image Credits:</strong>Lam Yik/Bloomberg / Getty Images</figcaption></figure>	</div>
	<div>
						<p><time datetime="2025-05-28T06:25:29-07:00">6:25 AM PDT · May 28, 2025</time></p>											</div>
</div><div>
		
		<div>
			<div>
<p id="speakable-summary">Telegram has struck a partnership with Elon Musk’s AI company, xAI, to distribute the latter’s chatbot, Grok, via Telegram and integrate it into apps available on the chat app’s platform for one year.</p>

<p>xAI will pay $300 million in cash and equity to the chat app as part of the deal, Telegram’s CEO Pavel Durov <a href="https://x.com/durov/status/1927697402095378432" target="_blank" rel="noreferrer noopener nofollow">said</a> on Tuesday.</p>







<p>Durov said Telegram will also earn 50% of the revenue from xAI subscriptions purchased through the app.</p>

<p>Earlier this year, xAI made the Grok chatbot available to <a href="https://x.com/grok/status/1904840863622066629" target="_blank" rel="noreferrer noopener nofollow">Telegram’s premium users</a>. It seems Grok might now be made available to all users.</p>

<p>A video posted by Durov on X suggested that Grok can be pinned on top of chats within the app, and users can also ask questions to Grok from the search bar. Notably, Meta has also integrated Meta AI into <a href="https://techcrunch.com/2024/04/12/meta-is-testing-an-ai-powered-search-bar-in-instagram/">the search bar on Instagram and WhatsApp</a>.</p>

<p>The video also shows that you will be able to use Grok for writing suggestions, summarizing chats, links, and documents, and creating stickers. Grok will supposedly also help answer questions for businesses and assist with moderation.</p>

<p><em>Note: This story was corrected to clarify that xAI is paying Telegram $300 million for distributing Grok.</em></p>

</div>

			

			


			
			
			

			




			
			
			

			



			
<div>
	
	
	
	

	
<div>
	<p>
		Ivan covers global consumer tech developments at TechCrunch. He is based out of India and has previously worked at publications including Huffington Post and The Next Web. You can reach out to him at im[at]ivanmehta[dot]com	</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/ivan-mehta/" data-event="button" href="https://techcrunch.com/author/ivan-mehta/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div>


		</div>
		

		
		<div id="wp-block-techcrunch-most-popular-posts__heading">
<h2 id="h-most-popular">Most Popular</h2>

</div>
		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mullvad Leta (315 pts)]]></title>
            <link>https://leta.mullvad.net</link>
            <guid>44116503</guid>
            <pubDate>Wed, 28 May 2025 14:38:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://leta.mullvad.net">https://leta.mullvad.net</a>, See on <a href="https://news.ycombinator.com/item?id=44116503">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-cy="locationSelect"><p><label for="country">Country</label></p><!--[!--><!--]--><!----></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[De-anonymization attacks against the privacy coin XMR (157 pts)]]></title>
            <link>https://monero.forex/is-monero-totally-private-a-comprehensive-analysis-of-de-anonymization-attacks-against-the-privacy-coin/</link>
            <guid>44116236</guid>
            <pubDate>Wed, 28 May 2025 14:11:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://monero.forex/is-monero-totally-private-a-comprehensive-analysis-of-de-anonymization-attacks-against-the-privacy-coin/">https://monero.forex/is-monero-totally-private-a-comprehensive-analysis-of-de-anonymization-attacks-against-the-privacy-coin/</a>, See on <a href="https://news.ycombinator.com/item?id=44116236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h3>Analyzing Attempts to Deanonymize and Track Monero Transactions: A Technical and Strategic Overview.</h3>


<p><strong>December 23, 2024<br>By: Trevor Baaddi</strong></p>


<p>Monero (XMR), a cryptocurrency renowned for its privacy-centric design, has drawn the attention of governments, cybersecurity experts, and analytics firms seeking to deanonymize its transactions. Unlike Bitcoin, where transactions are publicly visible on the blockchain, Monero employs sophisticated privacy features such as ring signatures, stealth addresses, and confidential transactions. This article analyzes notable attempts to compromise Monero’s privacy, detailing the methodologies used, their efficacy, and their limitations.</p>



<h4>Monero’s Privacy Features: A Brief Overview</h4>



<p>To appreciate the challenges of deanonymizing Monero, it is essential to understand its privacy architecture:</p>



<ul>
<li><strong>Ring Signatures</strong>: Combine a sender’s output with decoy outputs, obscuring the true origin of a transaction.</li>



<li><strong>Stealth Addresses</strong>: Generate unique one-time addresses for recipients, ensuring transactions cannot be linked back to a public address.</li>



<li><strong>Ring Confidential Transactions (RingCT)</strong>: Conceal transaction amounts.</li>



<li><strong>Dandelion++ Protocol</strong>: Helps obscure the origin of transactions during network propagation.</li>
</ul>



<p>These features collectively create a blockchain that is opaque by design, frustrating conventional blockchain analysis techniques.</p>



<hr>



<h4>1. Chainalysis and Monero Tracking Services</h4>



<p><strong>Attempt</strong>: Chainalysis, a prominent blockchain analytics company, has been reported to develop tools capable of providing some insights into Monero transactions. While specifics are scarce due to the proprietary nature of their methods, Chainalysis has publicly acknowledged limited success with Monero tracking.</p>



<p><strong>Methodology</strong>: Their approach likely involves exploiting known vulnerabilities, such as timing analysis of transactions or correlating off-chain data (e.g., exchange deposits and withdrawals).</p>



<p><strong>Efficacy</strong>: Limited. Chainalysis’s methods have reportedly provided probabilistic results rather than deterministic deanonymization. For example, law enforcement contracts suggest they may only achieve partial success in scenarios where Monero interacts with non-private cryptocurrencies or centralized exchanges.</p>



<hr>



<h4>2. CipherTrace’s Monero Analysis Tool</h4>



<p><strong>Attempt</strong>: In 2020, CipherTrace claimed to develop tools to track Monero transactions, purportedly offering these capabilities to the U.S. Department of Homeland Security (DHS).</p>



<p><strong>Methodology</strong>:</p>



<ul>
<li><strong>Transaction Heuristics</strong>: CipherTrace suggested it could identify patterns or anomalies in transaction data.</li>



<li><strong>Network Analysis</strong>: Monitoring IP addresses and network-level metadata during transaction broadcasts.</li>
</ul>



<p><strong>Efficacy</strong>: Controversial. Critics argue CipherTrace has not demonstrated comprehensive deanonymization and question the validity of their claims. Monero’s development team and community members have scrutinized CipherTrace’s assertions, pointing out the lack of independent verification.</p>



<hr>



<h4>3. Academic Research: Breaking Monero’s Privacy with Statistical Analysis</h4>



<p><strong>Attempt</strong>: Various academic papers have explored vulnerabilities in Monero’s privacy model. A notable example is the 2017 research by Möser, Kappos, and Böhme, which analyzed Monero’s ring signature scheme.</p>



<p><strong>Methodology</strong>:</p>



<ul>
<li><strong>Statistical Analysis of Ring Signatures</strong>: The researchers identified weaknesses in older versions of Monero’s ring signature implementation, where decoy selection was not truly random.</li>



<li><strong>Temporal Analysis</strong>: Exploiting the time at which outputs were used as inputs in subsequent transactions to infer relationships.</li>
</ul>



<p><strong>Efficacy</strong>: Significant but outdated. Monero developers promptly addressed these vulnerabilities by introducing mandatory RingCT and improving decoy selection algorithms.</p>



<hr>



<h4>4. Blockchain Analysis Firms and Metadata Correlation</h4>



<p><strong>Attempt</strong>: Firms like Elliptic and others have explored off-chain metadata correlation to deanonymize Monero users indirectly.</p>



<p><strong>Methodology</strong>:</p>



<ul>
<li><strong>Exchange Data</strong>: Linking Monero transactions to known addresses at centralized exchanges.</li>



<li><strong>IP and Geolocation Data</strong>: Using network-level surveillance to track transaction broadcasts.</li>
</ul>



<p><strong>Efficacy</strong>: Partial. Success depends on external factors, such as users’ operational security practices and reliance on exchanges with Know Your Customer (KYC) protocols.</p>



<hr>



<h4>5. Government Initiatives: The IRS Bounty Program</h4>



<p><strong>Attempt</strong>: In 2020, the Internal Revenue Service (IRS) offered a $625,000 bounty for anyone capable of cracking Monero’s privacy.</p>



<p><strong>Methodology</strong>:</p>



<ul>
<li><strong>Collaborations with Analytics Firms</strong>: Contracts with Chainalysis and Integra FEC aimed to develop Monero-tracking tools.</li>



<li><strong>Network Surveillance</strong>: Leveraging external data sources and possible timing attacks.</li>
</ul>



<p><strong>Efficacy</strong>: Unknown/Zero. While the bounty led to partnerships, there is no public evidence that these efforts resulted in comprehensive deanonymization.</p>



<hr>



<h4>6. Community-Driven Efforts: “Breaking Monero” Series</h4>



<p><strong>Attempt</strong>: The Monero Research Lab and community members created the “Breaking Monero” series, which proactively identifies and mitigates potential vulnerabilities.</p>



<p><strong>Methodology</strong>:</p>



<ul>
<li><strong>White-Hat Analysis</strong>: Evaluating transaction linkability, decoy selection, and network-level attacks.</li>



<li><strong>Ongoing Hard Forks</strong>: Implementing upgrades to enhance Monero’s privacy features.</li>
</ul>



<p><strong>Efficacy</strong>: Proactive and robust. These efforts have strengthened Monero’s resistance to tracking.</p>



<hr>



<h3>Conclusion: Monero’s Privacy Remains Resilient</h3>



<p>Despite various attempts by governments, firms, and researchers to deanonymize Monero, the cryptocurrency’s privacy features have proven remarkably resilient. While certain methodologies have exploited weaknesses or probabilistically reduced anonymity, none have achieved reliable, widespread deanonymization. Monero’s active development community continues to fortify its defenses, ensuring it remains a leading choice for privacy-focused users.</p>



<h4>References:</h4>



<ol start="1">
<li><a href="https://www.chainalysis.com/blog/2024-crypto-crime-report-introduction/" target="_blank">Chainalysis Insights</a></li>



<li><a href="https://ciphertrace.com/" target="_blank">CipherTrace Monero Tracking Announcement</a></li>



<li>Möser, Kappos, and Böhme (2017). “Empirical Analysis of Privacy in Monero.”</li>



<li><a href="https://www.irs.gov/compliance/criminal-investigation/former-security-engineer-for-international-technology-company-pleads-guilty-to-hacking-two-decentralized-cryptocurrency-exchanges" target="_blank">IRS Cryptocurrency Bounty</a></li>



<li><a href="https://www.youtube.com/playlist?list=PLsSYUeVwrHBnAUre2G_LYDsdo-tD0ov-y" target="_blank">Breaking Monero Series</a></li>
</ol>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Blowtorch Theory: A new model for structure formation in the universe (133 pts)]]></title>
            <link>https://theeggandtherock.com/p/the-blowtorch-theory-a-new-model</link>
            <guid>44115973</guid>
            <pubDate>Wed, 28 May 2025 13:43:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theeggandtherock.com/p/the-blowtorch-theory-a-new-model">https://theeggandtherock.com/p/the-blowtorch-theory-a-new-model</a>, See on <a href="https://news.ycombinator.com/item?id=44115973">Hacker News</a></p>
Couldn't get https://theeggandtherock.com/p/the-blowtorch-theory-a-new-model: Error: getaddrinfo ENOTFOUND theeggandtherock.com]]></description>
        </item>
        <item>
            <title><![CDATA[The Who Cares Era (589 pts)]]></title>
            <link>https://dansinker.com/posts/2025-05-23-who-cares/</link>
            <guid>44115620</guid>
            <pubDate>Wed, 28 May 2025 13:07:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dansinker.com/posts/2025-05-23-who-cares/">https://dansinker.com/posts/2025-05-23-who-cares/</a>, See on <a href="https://news.ycombinator.com/item?id=44115620">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>Earlier this week, <a href="https://bsky.app/profile/rachaelking70.bsky.social/post/3lplwve5ar22h">it was discovered</a> that the <em>Chicago Sun-Times</em> and the <em>Philadelphia Inquirer</em> <a href="https://www.npr.org/2025/05/20/nx-s1-5405022/fake-summer-reading-list-ai">had both published</a> an externally-produced "special supplement" that contained facts, experts, and book titles entirely made up by an AI chatbot. There's been <a href="https://www.theatlantic.com/technology/archive/2025/05/ai-written-newspaper-chicago-sun-times/682861/">a lot</a> written about this (<a href="https://marthabayne.substack.com/p/journalism-dreams">former <em>Chicago Reader</em> editor Martha Bayne's is the best</a>), and I don't need to rehash it all. But the thing that is most disheartening to me is how at every step along the way, nobody cared.</p>
<p>The writer didn't care. The supplement's editors didn't care. The biz people on both sides of the sale of the supplement didn't care. The production people didn't care. And, the fact that it took <em>two days</em> for anyone to discover this epic fuckup in print means that, ultimately, the reader didn't care either.</p>
<p>It's so emblematic of the moment we're in, the Who Cares Era, where completely disposable things are shoddily produced for people to mostly ignore.</p>
<p>AI is, of course, at the center of this moment. It's a mediocrity machine by default, attempting to bend everything it touches toward a mathematical average. Using <a href="https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/">extraordinary amounts of resources</a>, it has the ability to create something <em>good enough</em>, a squint-and-it-looks-right simulacrum of normality. If you don't care, it's miraculous. If you do, <a href="https://dansinker.com/posts/illusions/">the illusion</a> falls apart pretty quickly. The fact that the userbase for AI chatbots <a href="https://www.theverge.com/2024/12/4/24313097/chatgpt-300-million-weekly-users">has exploded exponentially</a> demonstrates that <em>good enough</em> is, in fact, good enough for most people. Because most people don't care.</p>
<p>(It's worth pointing out that I'm not a full-throated hater and know people—coders, mostly—who work with AI that <em>do</em> care and have used it to make real, meaningful things. Most people, however, use it quickly and thoughtlessly to make more mediocrity.)</p>
<p>It's easy to blame this all on AI, but it's not just that. Last year I was deep in negotiations with a big-budget podcast production company. We started talking about making a deeply reported, limited-run show about the concept of living in a multiverse that I was (and still am) very excited about. But over time, our discussion kept getting dumbed down and dumbed down until finally the show wasn't about the multiverse at all but instead had transformed into a daily chat show about the Internet, which everyone was trying to make back then. Discussions fell apart.</p>
<p>Looking back, it feels like a little microcosm of everything right now: Over the course of two months, we went from something smart that would demand a listener's attention in a way that was challenging and new to something that sounded like every other thing: some dude talking to some other dude about apps that some third dude would half-listen-to at 2x speed while texting a fourth dude about plans for later.</p>
<p>Hanif Abdurraqib, in one of his excellent <a href="https://www.instagram.com/nifmuhammad/?hl=en">Instagram</a> mini-essays the other week, wrote about the rise of content that's designed to be <a href="https://www.theguardian.com/tv-and-radio/2025/jan/17/not-second-screen-enough-is-netflix-deliberately-dumbing-down-tv-so-people-can-watch-while-scrolling">consumed while doing something else</a>. In Hanif's case, he was writing about <a href="https://podcasts.apple.com/us/podcast/time-machine-the-score-side-a/id1566642706?i=1000535020855"><em>Time Machine</em></a>, his incredible 90 minute deep dive into The Fugees' seminal album <em>The Score</em>. Released in 2021, Hanif marveled at the budget, time, and effort that went into crafting the two-part 90 minute podcast and how, today, there's no way it would have happened.</p>
<p>He's right. Nobody's funding that kind of work right now, because nobody cares.</p>
<p>(It's worth pointing out that Hanif wrote this using Stories, a system that erased it 24 hours later. Another victim of the Who Cares Era.)</p>
<p>Of course we're all victims of the biggest perpetrators of this uncaring era, as the Trump administration declares "Who Cares?" to vast swaths of the federal government, to public health, to immigrant families, to college students, to you, to me. As Elon Musk's DOGE rats gnaw their way through federal agencies, not caring is their guiding light. They cut indiscriminately, a smug grin on their faces. That they believe they can replace government workers—people who care an <em>extraordinary</em> amount about their arcane corner of the bureaucracy—with <a href="https://www.wired.com/story/doge-is-in-its-ai-era/">hastily-written AI code</a> is another defining characteristic of right now.</p>
<p>I keep coming back to the word "disheartening," because it all really is.</p>
<p>Without getting into too many specifics, I recently was involved in reviewing hundreds of applications for something. Over the course of reviewing, I was struck by the nearly-identical phrasing that threaded through dozens of the applications. It was eerie at first, like seeing a shadow in the distance, then frustrating, and ultimately completely disheartening: It was AI. For whatever their reasons, a bunch of people had used a chatbot to help write their answers to questions that asked them to draw from their own, unique, personal experience. They had fed their resumes or their personal websites or their actual stories and experiences into the machine, and it had filled in the blanks, Mad Libs-style. I felt crushed.</p>
<p>Until.</p>
<p>Until I read an application written entirely by a person. And then another. And another. They <em>glowed</em> with delight and joy and sadness and with the unexpected at every turn.</p>
<p>They were human.</p>
<p>They were written by people that cared.</p>
<p>In the Who Cares Era, the most radical thing you can do is care.</p>
<p>In a moment where machines churn out mediocrity, make something yourself. Make it imperfect. Make it rough. Just make it.</p>
<p>At a time where the government's uncaring boot is pressing down on all of our necks, the best way to fight back is to care. Care loudly. Tell others. Get going.</p>
<p>As the culture of the Who Cares Era grinds towards the lowest common denominator, support those that are making real things. Listen to something with your full attention. Watch something with your phone in the other room. Read an actual paper magazine or a book.</p>
<p>Be yourself.</p>
<p>Be imperfect.</p>
<p>Be human.</p>
<p>Care.</p>

    </div><p>Published May 23, 2025. | </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI: Accelerated Incompetence (282 pts)]]></title>
            <link>https://www.slater.dev/accelerated-incompetence/</link>
            <guid>44114631</guid>
            <pubDate>Wed, 28 May 2025 10:50:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.slater.dev/accelerated-incompetence/">https://www.slater.dev/accelerated-incompetence/</a>, See on <a href="https://news.ycombinator.com/item?id=44114631">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
<article>
  <!-- Page Start inject -->
  

  <header>
    
    <div>
  
  <p><img src="https://www.slater.dev/images/doug.jpg" alt="Doug Slater">
  </p>

  
  <p><span>Doug</span>
    
    
    <span>·</span>
    <time>2025-05-19</time>
    
    
  </p>
</div>

  </header>

  

  <!-- TOC -->
  <!---->


<!---->

  <!-- Content -->
  <section><p><em>In software engineering, over-reliance on LLMs accelerates incompetence. LLMs can't replace human critical thinking.</em></p>
<p><em>The text in this essay was written without any use of AI.</em></p>
<img src="https://www.slater.dev/llm_dependence.jpg" alt="A chart showing a speculative inverse correlation between LLM dependence and IQ">
<p>
    A speculative inverse correlation between LLM dependence and IQ
  </p>
<p>By now much ink has dried on the wave of AI and LLMs which crashed upon the public consciousness in late 2022. As an experienced software engineer, I'd like to speak to two troubling engineering perspectives I've observed on LLMs.</p>
<h2 id="llms-are-my-friend">"LLMs are my friend"</h2>
<p>I don't think anyone believes that a computer program is literally their companion, so let's address the euphemistic intent of the above phrase: namely that an LLM conveys magnificent benefits upon its user.</p>
<p>Engineers who view LLMs as an ally invariably prioritize or feel pressured to prioritize velocity; for them, production trumps perspicacity. While it's true that LLMs can deliver a lot of code quickly, their use carries a long tail of <em>risks</em>.</p>
<h2 id="risks-of-using-llm">Risks of using LLM</h2>
<ul>
<li><strong>Output Risk</strong>. An LLM can give output that is blatantly incorrect, for example code that won't compile. More likely and dangerously, it can give output that is subtly and undetectably wrong, like logic bugs. The risk is elevated if the prompter is not qualified to evaluate the output, for example project managers prompting for source code.</li>
<li><strong>Input Risk</strong>. An LLM does not challenge a prompt which is leading<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">1</a></sup> or whose assumptions are flawed or context is incomplete. Example: An engineer prompts, "Provide a thread-safe list implementation in C#" and receives 200 lines of flawless, correct code. It's still the wrong answer, because the question should have been, "How can I make this code thread-safe?" and whose answer is "Use <code>System.Collections.Concurrent</code>" and 1 line of code. The LLM is not able to recognize an instance of the XY problem<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">2</a></sup> because it was not asked to.</li>
<li><strong>Future Velocity</strong>. This is your typical "tech debt" argument, but more urgent. AI can degrade the quality of your codebase <em>so fast</em>. Have you ever seen the fruits of hoarding disorder? From the outside, a house or apartment may look fine. But the inside is unsanitary, reprehensible, and nonfunctional. Developers are discovering that without strong guardrails, code produced by an LLM is like such a space.</li>
<li><strong>User Infantilization</strong>. An extinction of talent will occur within individuals and organizations that outsource thinking and problem solving to LLMs:
<ul>
<li>As senior engineers are deprived of the opportunity to learn through productive struggle, their existing problem solving and critical thinking skills atrophy:
<ul>
<li>"Microsoft research on knowledge workers found that AI-driven confidence often comes at the expense of critical thinking"<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">3</a></sup></li>
<li>"In a world pushing for “reflexive AI usage,” I’m advocating for something different: thoughtful, intentional collaboration with AI that preserves the essence of coding as a craft"<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">4</a></sup></li>
<li>"LLMs give me finished thoughts, polished and convincing, but none of the intellectual growth that comes from developing them myself" <sup><a href="https://www.slater.dev/accelerated-incompetence/#references">5</a></sup></li>
</ul>
</li>
<li>Junior engineers never develop such skills to begin with and so can never in turn mentor future junior engineers.</li>
</ul>
</li>
<li><strong>Loss of Joy</strong>. Many developers are reporting that using AI robs them of flow state and the joy of creation.<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">6</a></sup> AI-generated code is miserable to read and change.</li>
</ul>
<p>In a future post, I plan to write about mitigations for each of these risks. Be sure to subscribe below if that sounds interesting.</p>
<h2 id="i-ll-become-redundant">"I'll become redundant"</h2>
<p>Source<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">7</a></sup></p>
<p>No, you won't. That said, there are certainly things you can do to further differentiate yourself from an LLM. To stay on topic, I'll defer that to a future post.</p>
<p>There are two programming competences that LLMs cannot furnish: <em>program theory</em> and <em>program entropy</em>.</p>
<h2 id="program-theory">Program Theory</h2>
<blockquote>
<p>...programming properly should be regarded as an activity by which the programmers form or achieve a certain kind of insight, a theory, of the matters at hand</p>
</blockquote>
<p>-- Peter Naur, <em>Programming as Theory Building</em>, 1985<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">8</a></sup></p>
<p>Naur was one of the greats in computing. He argued, against popular belief at the time, that a program is not its source code. Rather, the program is a shared mental construct: a <em>theory</em> or <em>design</em>. From that, the engineer derives code, but the work product of value is the design, not code.</p>
<p>To help you think about the difference between program theory and program text, consider this thought experiment: Imagine that two engineering teams of equivalent talent, A and B, are locked in separate rooms. Each team is told not to communicate with the other. Team A is tasked to write a program, for example a simple terminal-based Chess game. Team B just waits, plays real Chess, or whatever. When Team A is finished, their source code is handed to Team B. Now each team is asked in parallel to add a feature to the program, for example a virtual chess player so the game can be played solo. (We'll let Team A take a coffee break before they get started).</p>
<p><em>Question</em>: Which team will deliver a better solution?</p>
<p><em>Answer</em>: Team A, because those engineers have a fresh mental model of the program they just created, while Team B has none.</p>
<p>According to Naur, the theory matters because inevitably a program needs to be <em>maintained</em>, i.e. modified after its initial creation. If all you have is the source code and not an internalized understanding of its design, the cost for those modifications will be higher. I think we can each remember a time we were introduced to a big existing codebase. At first our productivity was near zero. As we loaded the program theory into our mind, productivity rose.</p>
<h3 id="llms-and-program-theory">LLMs and Program Theory</h3>
<p>LLMs as they currently exist cannot master a theory, design, or mental construct because they don't remember beyond their context window. Only humans can can gain and retain program theory.</p>
<h2 id="program-entropy">Program Entropy</h2>
<p>Complexity is a fundamental opposing force of programming<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">9</a></sup>, and it correlates with entropy.</p>
<blockquote>
<p>...program building is an entropy-decreasing process...program maintenance is an entropy-increasing process, and even its most skillful execution only delays the subsidence of the system into unfixable obsolescence</p>
</blockquote>
<p>-- Fred Brooks, <em>The Mythical Man-Month</em>, 1975</p>
<p>Brooks, another prominent historical figure in computing, asserted that after initial construction, the changes made to a program can only make the source code more complex. However, changes made in harmony with the design will do so at a slower rate.</p>
<h3 id="llms-and-program-entropy">LLMs and Program Entropy</h3>
<p>An LLM is a token predictor. It works only at the level of text. It is not capable of working at a conceptual level: it doesn't reason about ideas, diagrams, or requirements specifications. Everyone who has prompted an LLM with a large chunk of code has beheld that the LLM tends to apply unnecessary and bizarre changes, and the longer the conversation drags on, the more it diverges. How often have you witnessed an LLM <em>reduce</em> the complexity of a piece of code?</p>
<p>Only humans can decrease or resist complexity.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We found wisdom for the LLM age by remembering what two forerunners of our discipline had to say about software design and complexity.</p>
<p>If you had hoped that AI would launch your engineering career to the next level, be warned that it could do the opposite. <em>LLMs can accelerate incompetence.</em></p>
<p>If you're a skilled, experienced engineer and you fear that AI will make you unemployable, adopt a more nuanced view. <em>LLMs can't replace human engineering.</em></p>
<p>The business allure of AI is reduced costs through commoditized engineering, but just like offshore engineering talent brings forth mixed fruit, LLMs fall short and open risks.</p>
<p>The AI hype cycle will eventually peak<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">10</a></sup>. Companies which overuse AI now will inherit a long tail of costs, and they'll either pivot or go extinct. As such, the long-term value proposition for humans in engineering remains unchanged. The world still needs and will pay for technical skills and deep thinking in engineering.</p>
<p>AI will stick around, though. Use it as a tool, not a crutch, and continue to invest in the same fundamental engineering skills that were deemed valuable in 2019.</p>
<h2 id="next">Next...</h2>
<p>Subscribe to my email list below. I plan to write more.</p>
<h2 id="references">References</h2>
<ol>
<li><a href="https://en.wikipedia.org/wiki/Leading_question">Leading Question</a></li>
<li><a href="https://en.wikipedia.org/wiki/XY_problem">The XY Problem</a></li>
<li><a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/radar/2025/04/tr_technology_radar_vol_32_en.pdf">ThoughtWorks Technology Radar Volume 32</a></li>
<li><a href="https://cekrem.github.io/posts/coding-as-craft-going-back-to-the-old-gym/">Coding as Craft: Going Back to the Old Gym</a></li>
<li><a href="https://dcurt.is/thinking">Thoughts on Thinking</a></li>
<li><a href="https://terriblesoftware.org/2025/04/23/the-hidden-cost-of-ai-coding/">The Hidden Cost of AI Coding</a></li>
<li><a href="https://www.reddit.com/r/ExperiencedDevs/comments/1h3xpke/dont_know_if_the_right_place_how_to_work_on/">"I wonder if I'll become redundant"</a></li>
<li><a href="https://pablo.rauzy.name/dev/naur1985programming.pdf">Programming as Theory Building</a></li>
<li><a href="https://grugbrain.dev/#grug-on-complexity">Grug on Complexity</a></li>
<li><a href="https://en.wikipedia.org/wiki/Gartner_hype_cycle">Gartner Hype Cycle</a></li>
</ol>
</section>

  <hr>

              <div>
                <h3>Subscribe for More</h3>
                <h5>I'll tell you about new posts. I take your privacy seriously.</h5>
                
            </div>
 

  <!-- Post Taxonomies -->
  


<!---->

  <!-- Post Nav -->
  
<nav>
  
  <a href="https://www.slater.dev/tech-risk-is-business-risk/"><span>←</span><span>Tech Risk is Business Risk</span></a>
  <!---->
  
</nav>

<!---->

  <!-- Comment -->
  <!---->
  


<!---->
<!---->
  

  <!-- Page End inject -->
  
</article>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft is starting to open Windows Update up to any third-party app (105 pts)]]></title>
            <link>https://www.theverge.com/news/675446/microsoft-windows-update-all-apps-orchestration-platform</link>
            <guid>44114621</guid>
            <pubDate>Wed, 28 May 2025 10:48:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/news/675446/microsoft-windows-update-all-apps-orchestration-platform">https://www.theverge.com/news/675446/microsoft-windows-update-all-apps-orchestration-platform</a>, See on <a href="https://news.ycombinator.com/item?id=44114621">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zephr-anchor"><p>Microsoft is starting to open Windows Update up to any third-party app that needs to be updated. The software giant is now allowing developers to sign up for a private preview of what it calls the Windows Update orchestration platform, that will enable Windows Update to support any update for apps or drivers in the future. It’s focused largely on business apps, but it will be open to any apps or management tools.</p><p>Windows Update is largely used to update the core parts of Windows right now, alongside key drivers for devices and even install some third-party management apps for peripherals. “We’re building a vision for a unified, intelligent update orchestration platform capable of supporting any update (apps, drivers, etc.) to be orchestrated alongside Windows updates,” <a href="https://techcommunity.microsoft.com/blog/windows-itpro-blog/introducing-a-unified-future-for-app-updates-on-windows/4416354">explains Angie Chen</a>, a product manager at Microsoft.</p><p>Most apps on Windows are updated independently, using update mechanisms that developers have created themselves. Microsoft’s new Windows Update orchestration platform will let app developers take advantage of scheduled updates based on user activity, battery status, and even sustainable energy timing.</p><p>Developers will also be able to hook directly into the native Windows Update notifications, and be listed in the app update history part of Windows Update. Microsoft will support MSIX / APPX packaged apps, and even some custom Win32 apps. Any apps that are part of the Windows Update orchestrator will automatically get future improvements to the underlying Windows Update platform, too.</p><p>Microsoft has tried in the past to convince developers to list their apps in the Microsoft Store, where the store can handle updates or developers can continue to use their own update mechanisms. While the store on Windows has greatly improved in recent years, there are still some missing apps and businesses prefer to update their own line of business apps independently.</p><p>Microsoft’s <a href="https://www.theverge.com/2020/5/28/21272964/microsoft-winget-windows-package-manager-appget-copied">Windows Package Manager</a> has also tried to solve some of the problems with installing and updating apps on Windows, but it’s not a widely used way to install and manage apps outside of power users and developers.</p><p>Integrating more app updates into Windows Update certainly makes sense for a variety of apps, and it will be interesting to see whether this will be used primarily by businesses or if big developers like Adobe might move over to the Windows Update system instead of a separate installer that runs in the background.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CheerpJ 4.1: Java in the browser, now supporting Java 17 (preview) (114 pts)]]></title>
            <link>https://labs.leaningtech.com/blog/cheerpj-4.1</link>
            <guid>44114483</guid>
            <pubDate>Wed, 28 May 2025 10:23:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://labs.leaningtech.com/blog/cheerpj-4.1">https://labs.leaningtech.com/blog/cheerpj-4.1</a>, See on <a href="https://news.ycombinator.com/item?id=44114483">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pagefind-body="" data-pagefind-filter="productId:blog">  <p>Around a month ago we announced <a href="https://labs.leaningtech.com/blog/cheerpj-4.0" rel="nofollow" target="_blank">CheerpJ 4.0<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a>, the first release of our WebAssembly-based JVM with support for multiple versions of Java.</p>
<p><a href="https://cheerpj.com/docs/getting-started" target="_blank">  Get started <svg viewBox="0 0 24 24" astro-icon="mi:arrow-right"><path fill="currentColor" d="M12.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L16.586 13H5a1 1 0 1 1 0-2h11.586l-4.293-4.293a1 1 0 0 1 0-1.414z"></path></svg> </a><a href="https://discord.leaningtech.com/" target="_blank"> <svg viewBox="0 0 640 512" astro-icon="fa-brands:discord"><path fill="currentColor" d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485.065 485.065 0 0 0 404.081 32.03a1.816 1.816 0 0 0-1.923.91 337.461 337.461 0 0 0-14.9 30.6 447.848 447.848 0 0 0-134.426 0 309.541 309.541 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.689 483.689 0 0 0-119.688 37.107 1.712 1.712 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.016 2.016 0 0 0 .765 1.375 487.666 487.666 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348.2 348.2 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321.173 321.173 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251.047 251.047 0 0 0 9.109-7.137 1.819 1.819 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.812 1.812 0 0 1 1.924.233 234.533 234.533 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.407 301.407 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391.055 391.055 0 0 0 30.014 48.815 1.864 1.864 0 0 0 2.063.7A486.048 486.048 0 0 0 610.7 405.729a1.882 1.882 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541zM222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241zm195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241z"></path></svg> Join us on Discord  </a></p>
<p>The release of CheerpJ 4.0 was focused on getting the infrastructure right, maintaining our long standing Java 8 support while introducing Java 11, and allowing further versions of Java to be supported.</p>
<p>As stated in our <a href="https://cheerpj.com/our-roadmap-for-modern-java-in-the-browser/" rel="nofollow" target="_blank">roadmap<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a>, our plan was to introduce Java 17 support only later in the year. Thanks to faster than expected progress, we are now releasing this feature today, much earlier than we expected, as part of <strong>CheerpJ 4.1</strong>. This is intended to be a preview of the stable support that is still scheduled for a future CheerpJ 5.0 release, expected before the end of 2025.</p>
<p>Moreover, the initial support for Java 11 in the previous release received extensive testing by the developer community on a big variety of applications and libraries. Thanks to the help of our users, several critical bugs in the original release were found and fixed. This new 4.1 release also serves as an opportunity to bring much more stability to Java 11 support.</p>
<h2 id="what-is-new">What is new?</h2>
<figure><p><img src="https://labs.leaningtech.com/_astro/Runelite.C0PpYRV-_ZtkT15.webp" alt="Runeline screenshot" width="1596" height="1050" loading="lazy" decoding="async"></p><figcaption><p>RuneLite running in the browser via CheerpJ in Java 11 mode</p></figcaption></figure>
<p>Although not long has passed since the previous release, there are already many improvements in CheerpJ, across different subsystems.</p>
<p>CheerpJ 4.1 introduces:</p>
<ul>
<li>Support for SSL and Audio in Java 11</li>
<li>Performance optimizations</li>
<li>Improved networking stack</li>
<li>Improvements to mobile usability, for both Swing and AWT</li>
<li>Preview of Java 17 support</li>
</ul>
<p>Feedback and testing from the community have been invaluable to help us improve the stability of CheerpJ. We expect that Java 17 will attract even more attention, resulting in a positive feedback loop that will bring CheerpJ ever closer to our vision of running modern Java in the browser.</p>
<h2 id="what-can-cheerpj-do">What can CheerpJ do?</h2>
<p>CheerpJ is a full WebAssembly-based JVM for the browser, and comes with a complete OpenJDK runtime, as well as a powerful emulation layer to provide file system access, general networking support and other OS-level features. It works fully client-side, via WebAssembly, JavaScript and HTML5 technologies. It is, in essence, a JavaScript library, with no server-side or cloud-based component of any sort.</p>
<p>CheerpJ is a complete, flexible Java platform for modern browsers. It is an extremely powerful tool, designed and tested to work at the scale of real-world, large enterprise applications. Here is an overview of what CheerpJ can be used for.</p>
<h3 id="running-large-scale-swing--awt-applications">Running large-scale Swing / AWT applications</h3>
<p>CheerpJ can run existing, full Java applications from unmodified JARs, with no recompilation or pre-processing, straight from bytecode. Obfuscated or encrypted JARs are supported irrespective of the obfuscator being used.</p>
<figure><figcaption><p>A complex Swing application running live. Use the bottom-right control
button to try it fullscreen.</p></figcaption></figure>

<p>Both AWT- and Swing-based applications are supported, including third-party Swing Look&amp;Feels. Multiple applications, each with multiple windows, can run at the same time on the same page.</p>
<p>CheerpJ 4.1 introduces an unprecedented level of support for mobile devices, enabling for the first time to make complex Java applications available to users across phones and, especially, tablets.</p>
<p>Running a Java application is straightforward, requiring just three calls to the CheerpJ APIs (see our <a href="https://cheerpj.com/docs/getting-started/Java-app" rel="nofollow" target="_blank">Getting Started<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a> guide for a fully worked example).</p>
<div><figure><pre><code><p><span>await</span><span> </span><span>cheerpjInit</span><span>()</span><span>;</span></p><p><span>cheerpjCreateDisplay</span><span>(</span><span>800</span><span>,</span><span> </span><span>600</span><span>)</span><span>;</span></p><p><span>await</span><span> </span><span>cheerpjRunJar</span><span>(</span><span>"</span><span>/app/my_application_archive.jar</span><span>"</span><span>)</span><span>;</span></p></code></pre></figure></div>
<p>CheerpJ is built to run Java bytecode at scale, and is robust to very large applications. As a point of reference, our internal stress test is IntelliJ IDEA, an application comprising around 400MB of JAR files.</p>
<video controls="" autoplay="" loop="" muted="true" playsinline=""><source src="https://labs.leaningtech.com/blog/CJ_idea19_2025.mp4" type="video/mp4"></video>
<h3 id="using-java-libraries-as-part-of-web-applications">Using Java libraries as part of Web Applications</h3>
<p>CheerpJ makes it possible to use Java libraries from JavaScript using a natural and expressive <code>async</code>/<code>await</code> based approach, we call this feature <em>Library Mode</em>.</p>
<p>The following snippet of code should give an idea about this capability, by using the popular <code>iText</code> library to generate a PDF completely client-side in the browser.</p>
<div><figure><pre><code><p><span>async</span><span> </span><span>function</span><span> </span><span>iTextExample</span><span>()</span><span> </span><span>{</span></p><p><span>  </span><span>await</span><span> </span><span>cheerpjInit</span><span>()</span><span>;</span></p><p><span>  </span><span>const</span><span> </span><span>lib</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>cheerpjRunLibrary</span><span>(</span><span>"</span><span>/app/itextpdf-5.5.13.3.jar</span><span>"</span><span>)</span><span>;</span></p><p><span>  </span><span>try</span><span> </span><span>{</span></p><p><span>    </span><span>const</span><span> </span><span>Document</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>lib</span><span>.</span><span>com</span><span>.</span><span>itextpdf</span><span>.</span><span>text</span><span>.</span><span>Document</span><span>;</span></p><p><span>    </span><span>const</span><span> </span><span>Paragraph</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>lib</span><span>.</span><span>com</span><span>.</span><span>itextpdf</span><span>.</span><span>text</span><span>.</span><span>Paragraph</span><span>;</span></p><p><span>    </span><span>const</span><span> </span><span>PdfWriter</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>lib</span><span>.</span><span>com</span><span>.</span><span>itextpdf</span><span>.</span><span>text</span><span>.</span><span>pdf</span><span>.</span><span>PdfWriter</span><span>;</span></p><p><span>    </span><span>const</span><span> </span><span>FileOutputStream</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>lib</span><span>.</span><span>java</span><span>.</span><span>io</span><span>.</span><span>FileOutputStream</span><span>;</span></p><p><span>    </span><span>const</span><span> </span><span>document</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>new</span><span> </span><span>Document</span><span>()</span><span>;</span></p><p><span>    </span><span>const</span><span> </span><span>writer</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>PdfWriter</span><span>.</span><span>getInstance</span><span>(</span></p><p><span>      </span><span>document</span><span>,</span></p><p><span>      </span><span>await</span><span> </span><span>new</span><span> </span><span>FileOutputStream</span><span>(</span><span>"</span><span>/files/HelloIText.pdf</span><span>"</span><span>)</span></p><p><span>    )</span><span>;</span></p><p><span>    </span><span>await</span><span> </span><span>document</span><span>.</span><span>open</span><span>()</span><span>;</span></p><p><span>    </span><span>await</span><span> </span><span>document</span><span>.</span><span>add</span><span>(</span><span>await</span><span> </span><span>new</span><span> </span><span>Paragraph</span><span>(</span><span>"</span><span>Hello World!</span><span>"</span><span>))</span><span>;</span></p><p><span>    </span><span>await</span><span> </span><span>document</span><span>.</span><span>close</span><span>()</span><span>;</span></p><p><span>    </span><span>await</span><span> </span><span>writer</span><span>.</span><span>close</span><span>()</span><span>;</span></p><p><span>    </span><span>const</span><span> </span><span>blob</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>cjFileBlob</span><span>(</span><span>"</span><span>/files/HelloIText.pdf</span><span>"</span><span>)</span><span>;</span></p><p><span>    </span><span>const</span><span> </span><span>url</span><span> </span><span>=</span><span> </span><span>URL</span><span>.</span><span>createObjectURL</span><span>(</span><span>blob</span><span>)</span><span>;</span></p><p><span>    </span><span>pdfDisplay</span><span>.</span><span>data</span><span> </span><span>=</span><span> </span><span>url</span><span>;</span></p><p><span>  </span><span>}</span><span> </span><span>catch</span><span> (</span><span>e</span><span>) </span><span>{</span></p><p><span>    </span><span>const</span><span> </span><span>IOException</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>lib</span><span>.</span><span>java</span><span>.</span><span>io</span><span>.</span><span>IOException</span><span>;</span></p><p><span>    </span><span>if</span><span> (</span><span>e</span><span> </span><span>instanceof</span><span> </span><span>IOException</span><span>) </span><span>console</span><span>.</span><span>log</span><span>(</span><span>"</span><span>I/O error</span><span>"</span><span>)</span><span>;</span></p><p><span>    </span><span>else</span><span> </span><span>console</span><span>.</span><span>log</span><span>(</span><span>"</span><span>Unknown error: </span><span>"</span><span> </span><span>+</span><span> (</span><span>await</span><span> </span><span>e</span><span>.</span><span>getMessage</span><span>()))</span><span>;</span></p><p><span>  </span><span>}</span></p><p><span>}</span></p></code></pre></figure></div>
<p><em>Library Mode</em> provides extensive access to Java, with these main features:</p>
<ul>
<li>Creating new objects</li>
<li>Calling static and instance methods. Overloading is supported and the correct method is resolved taking into account the argument types.</li>
<li>Accessing static and instance fields, both for reading and for writing.</li>
<li>Handling Java exceptions from JavaScript (via regular <code>try</code>/<code>catch</code> blocks)</li>
</ul>
<p><em>Library Mode</em> is a unique feature, which makes it possible to build a new generation of fully client-side Web applications that combine Web-native components with Java libraries to implement complex functionalities.</p>
<p>In more enterprise scenarios, this approach can be used to progressively migrate large-scale Java applications to native Web apps, by rewriting the UI while keeping all or part of the original business logic in Java. This can provide significant reduction of risk, costs, and timeline to large modernisation projects.</p>
<p>Check out our dedicated documentation for more information on <a href="https://cheerpj.com/docs/getting-started/Java-library" rel="nofollow" target="_blank">Library Mode<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a></p>
<h2 id="demo-unmodified-minecraft-in-the-browser">Demo: Unmodified Minecraft in the browser</h2>
<video controls="" autoplay="" loop="" muted="true" playsinline=""><source src="https://labs.leaningtech.com/blog/browsercraft4.mp4" type="video/mp4"></video>
<p>To showcase the capabilities of CheerpJ, we created a side project named <a href="https://browsercraft.cheerpj.com/" rel="nofollow" target="_blank">Browsercraft<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a>, a web-based “embedding” of a historical Java version of Minecraft.</p>
<p>Contrary to other approaches you might have seen, Browsercraft is not based on decompilation or reverse engineering attempts. The original <code>client.jar</code> is fetched directly from Mojang servers on the end-user browser and runs unmodified. The LWJGL dependency, available from Maven, is also unmodified.</p>
<p>How LWJGL works in CheerpJ is particularly interesting, since it is only <em>superficially</em> Java. Most of its value comes from JNI methods which provide direct access to each and every method exposed by OpenGL. These methods are written in C and automatically generated by the LWJGL build system from a declarative representation of the OpenGL API.</p>
<p>CheerpJ 4.1 introduces support for these scenarios via JNI WebAssembly modules, which are loaded and executed dynamically, similarly to what happens on native platforms via shared libraries. Browsercraft takes advantage of this capability for LWJGL native code and also for the <a href="https://github.com/ptitSeb/gl4es/" rel="nofollow" target="_blank">gl4es<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a> library. This latter component provides a compatibility layer between OpenGL, used by Minecraft, and GLES as provided by WebGL.</p>
<p>By combining these WebAssembly modules and the unmodified JARs, CheerpJ can now correctly render Minecraft in the browser. It should be noted that Minecraft is a notoriously inefficient and resource intensive application, so we consider it to be a <em>stress test</em> for CheerpJ. Nevertheless, thanks to recent improvements in our JIT compiler, the demo can now run with satisfactory performance on most mid range machines. The situation will further improve in the future thanks to more advanced optimizations currently planned, stay tuned.</p>
<h2 id="whats-next">What’s next?</h2>
<p>Development of CheerpJ is moving fast. The CheerpJ 5.0 release is still scheduled for later in the year as originally announced in our <a href="https://cheerpj.com/our-roadmap-for-modern-java-in-the-browser/" rel="nofollow" target="_blank">roadmap<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a>.</p>
<p>There is a long list of improvements that we plan to ship, here are as few highlights:</p>
<ul>
<li><strong>Stable Java 17 support</strong>: With CheerpJ 4.1 we have released a preview of what Java 17 support will look like in the not so distant future, we plan to gather feedback from users to achieve a high level of stability by the time CheerpJ 5.0 is released.</li>
<li><strong>NPM support</strong>: CheerpJ has been historically used primarily to run legacy Java applications, most usually in environments that pre-date the npm ecosystem. Now that CheerpJ can run modern Java we want to provide a npm-native CheerpJ version, to streamline integration into modern projects.</li>
<li><strong>Extend native JNI modules support</strong>: This will allow us to support JavaFX, SWT and eventually allow users to build their own Wasm JNI module for any Java library.</li>
</ul>
<h2 id="licensing">Licensing</h2>
<p>CheerpJ is commercial software, but it’s free to use for FOSS projects, personal projects and one-person companies. Affordable and transparent licensing apply to small businesses.</p>
<p>Enterprise licensing and support are available, with significant discounts for non-profit and educational institutions. For more information see <a href="https://cheerpj.com/cheerpj-core/#compare-plans" rel="nofollow" target="_blank">Licensing<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a>.</p>
<h2 id="try-it-out-and-join-the-community">Try it out and join the community</h2>
<p>You can find a lot of beginner-friendly resources in our developer documentation, as well as tutorials and a full API reference.</p>
<p><a href="https://cheerpj.com/docs/getting-started" target="_blank">  Get started <svg viewBox="0 0 24 24" astro-icon="mi:arrow-right"><path fill="currentColor" d="M12.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L16.586 13H5a1 1 0 1 1 0-2h11.586l-4.293-4.293a1 1 0 0 1 0-1.414z"></path></svg> </a></p>
<p>For questions, discussion, and support, join our <a href="https://discord.leaningtech.com/" rel="nofollow" target="_blank">Discord<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a>. It’s an active community where both Leaning Technologies developers and experienced users can provide help.</p>
<p>Following the success of the second edition of the <a href="https://cheerpx.io/hackathon" rel="nofollow" target="_blank">WebVM Hackathon<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a> earlier this year, we have decided to host the first <a href="https://cheerpj-the-hackathon.devpost.com/" rel="nofollow" target="_blank">CheerpJ Hackathon<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a>. The event theme and precise dates are still being determined, but it will be a week-long competition to be held between September and October 2025, with a £500 prize awaiting the winning team. <a href="https://cheerpj-the-hackathon.devpost.com/" rel="nofollow" target="_blank">Sign up now<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a> to stay updated.</p>
<hr>
<p>CheerpJ is a product built with passion and a lot of coffee by Leaning Technologies, an international team of WebAssembly hackers based in Amsterdam (NL) and Leeds (UK). We hope you’ll love it as much as we do.</p>
<a href="https://github.com/leaningtech/cheerpj-meta" target="_blank"> <svg viewBox="0 0 1664 1600" astro-icon="fa:star"><path fill="currentColor" d="M1664 615q0 22-26 48l-363 354 86 500q1 7 1 20 0 21-10.5 35.5T1321 1587q-19 0-40-12l-449-236-449 236q-22 12-40 12-21 0-31.5-14.5T301 1537q0-6 2-20l86-500L25 663Q0 636 0 615q0-37 56-46l502-73L783 41q19-41 49-41t49 41l225 455 502 73q56 9 56 46z"></path></svg> Star CheerpJ on GitHub  </a>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cory Doctorow on how we lost the internet (129 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/1021871/4bec46993258f6b7/</link>
            <guid>44113735</guid>
            <pubDate>Wed, 28 May 2025 08:07:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/1021871/4bec46993258f6b7/">https://lwn.net/SubscriberLink/1021871/4bec46993258f6b7/</a>, See on <a href="https://news.ycombinator.com/item?id=44113735">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<div>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider <a href="https://lwn.net/subscribe/">subscribing to LWN</a>.  Thank you
for visiting LWN.net!
</p></div>
</blockquote>

<p>
Cory Doctorow <a href="https://craphound.com/bio/">wears many hats</a>:
digital activist, science-fiction author, journalist, and more.  He has
also written many books, both fiction and non-fiction, runs the <a href="https://pluralistic.net/">Pluralistic blog</a>, is a visiting
professor, and is an advisor to the <a href="https://www.eff.org/">Electronic
Frontier Foundation</a> (EFF); his <a href="https://chokepointcapitalism.com/"><i>Chokepoint Capitalism</i></a>
co-author, Rebecca Giblin, gave a <a href="https://lwn.net/Articles/927278/">2023 keynote
in Australia</a> that we covered.  Doctorow gave a rousing keynote on
the state of the "enshitternet"—today's internet—to kick
off the recently held <a href="https://us.pycon.org/2025/">PyCon US
2025</a> in Pittsburgh, Pennsylvania.
</p>

<p>
He began by noting that he is known for coining the term
"<a href="https://en.wikipedia.org/wiki/Enshittification">enshittification</a>" about the decay of tech platforms, so attendees were
probably expecting to hear about that; instead, he wanted to start by
talking about nursing.  A <a href="https://rooseveltinstitute.org/publications/uber-for-nursing/">recent
study</a> described how nurses are increasingly getting work through one of
three main apps that "<q>bill themselves out as 'Uber for nursing'</q>".
The nurses never know what they will be paid per hour prior to accepting a
shift and the three companies act as a cartel in order to "<q>play all
kinds of games with the way that labor is priced</q>".
</p>

<p>
In particular, the
companies purchase financial information from a data broker before offering
a nurse a shift; if the nurse is carrying a lot of credit-card debt,
especially if some of that is delinquent, the amount offered is
reduced. "<q>Because, the more desperate you are, the less you'll accept to
come into work and do that grunt work of caring for the sick, the elderly,
and the dying.</q>"  That is horrific on many levels, he said, but "<q>it
is emblematic of 'enshittification'</q>", which is one of the reasons he
highlighted it.
</p>

<h4>Platform decay</h4>

<p>
Enshittification is a three-stage process; he used Google to
illustrate the idea.  At first, Google minimized ads and maximized spending
on engineering to produce a great search engine; while it was doing that,
however, it was buying its way to dominance. "<q>They bribed every service,
every product
that had a search box to make sure that that was a Google search box.</q>"
No matter which browser, phone carrier, or operating system you were using,
Google ensured that you were using its search by default; by the early
2020s, it was spending the equivalent of buying a Twitter every 18 months
to do so, he said.  That is the first stage of the process: when the
provider is being good to its users, but is finding ways to lock them in.
</p>

<p><a href="https://lwn.net/Articles/1022663/">
<img src="https://static.lwn.net/images/2025/pycon-doctorow-sm.png" alt="[Cory Doctorow]" title="Cory Doctorow" width="206" height="300">
</a></p><p>
The second phase occurs once the company recognizes that it has users
locked in, so it will be difficult for them to switch away, and it shifts
to making things worse for its users in order to enrich its business
customers.  For Google, those are the publishers and advertisers.  A
growing portion of the search results page is shifted over to ads
"<q>marked off with ever-subtler, ever-smaller, ever-grayer labels
distinguishing them from the organic search results</q>".  While the
platform is getting better for business customers—at the expense of the
users—those customers are also getting locked in.
</p>

<p>
Phase three of enshittification is when the value of the platform is
clawed back until all that is left is kind of a "<q>homeopathic residue—the
least value needed to keep both business customers and end users locked to
the platform</q>".  We have gained a view into this process from the three
monopoly cases that Google has lost over the last 18 months. In 2019, the
company had 90% of the world's search traffic and its users were loyal;
"<q>everyone who searched on Google, searched everything on Google</q>".
</p>

<p>
But that meant that Google's search growth had plateaued, so how was the
company going to be able to grow?  It could "<q>raise a billion humans to
adulthood and make them Google customers, which is <a href="https://classroom.google.com/">Google Classroom</a>, but that's a
slow process</q>".  From the internal memos that came to light from the
court cases, we can see what the company chose to do, he said: "<q>they
made search worse</q>".  
</p>

<p>
The accuracy of the search results was reduced, which meant that users
needed to do two or three queries to the get the results they would have
seen on the first page.  That increased the number of ads that could be
shown, which is obviously bad for searchers, but the company was also
attacking its business customers at the same time.  For example, "<q>Google entered into
an illegal, collusive arrangement with Meta, called <a href="https://en.wikipedia.org/wiki/Jedi_Blue">Jedi Blue</a></q>" that
"<q>gamed the advertising market</q>" so that publishers got paid less and
advertisers had to pay more, he said.
</p>

<p>
So that's how we have ended up at the Google of today, where the top of the
search results page is "<q>a mountain of AI slop</q>", followed by five
paid results "<q>marked with the word 'Ad' in eight point, 90%
gray-on-white type</q>", ending with "<q>ten spammy SEO [search-engine
optimization] links from someone else who's figured out how to game
Google</q>".  The amazing thing is "<q>that we are still using Google
because we're locked into it</q>".  It is a perfect example of the result
of the "<q>tragedy in three acts</q>" that is enshittification.
</p>

<h4>Twiddling</h4>

<p>
The underlying technical means that allows this enshittification is
something he calls "twiddling".  Because the companies run their apps on
computers, they can change a nearly infinite number of knobs to potentially
alter "<q>the prices, the cost, the search rankings, the
recommendations</q>" each time the platform is visited.  Going back to the
nursing example, "<q>that's just twiddling, it's something you can only do
with computers</q>".
</p>

<p>
Legal scholar Veena Dubal coined the term "<a href="https://en.wikipedia.org/wiki/Algorithmic_wage_discrimination">algorithmic
wage discrimination</a>" to describe this kind of twiddling for the "gig
economy", which is "<q>a major locus for enshittification</q>"; the nursing
apps, Uber, and others are examples of that economy. "<q>Gig work is that
place where your shitty boss is a shitty app and you're not allowed to call
yourself an employee.</q>"
</p>

<p>
Uber invented a particular form of algorithmic wage discrimination; if its
drivers are picky about which rides they accept, Uber will slowly raise the
rates to entice those drivers—until they start accepting rides.  Once a
driver does accept a ride, "<q>the wage starts to push down and down at
random intervals in increments that are too small for human beings to
readily notice</q>".  It is not really "<q>boiling the frog</q>", Doctorow
said, so much as it is "<q>slowly poaching it</q>".
</p>

<p>
As anyone with a technical background knows, "<q>any task that is simple,
but time-consuming is a prime candidate for automation</q>".  This
kind of "<q>wage theft</q>" would be tedious and expensive to do by hand,
but it is trivial to play these games using computers.  This kind of thing
is not just bad for nurses, he said, its bad for those who are using their
services.
</p><blockquote>
Do you really think that paying nurses based on how desperate
they are, at a rate calculated to increase their desperation so that
they'll accept ever-lower wages,
is going to result in us getting the best care when we see a nurse?  Do you
really want your catheter inserted by a nurse on food stamps who drove an
Uber until midnight the night before and skipped breakfast this morning so
that they could pay the rent?
</blockquote>


<h4>Paying and products</h4>

<p>
It is misguided to say "<q>if you're not paying for the product, you're the
product</q>", because it makes it seem like we are complicit in sustaining
<a href="https://en.wikipedia.org/wiki/Surveillance_capitalism">surveillance
capitalism</a>—and we are not. The thinking goes that if we were only
willing to start paying for things, "<q>we could restore capitalism to its
functional non-surveillance state and companies would treat us better
because we'd be customers and not products</q>".  That thinking elevates
companies like Apple as "<q>virtuous alternatives</q>" because the company
charges money and not attention, so it can focus on improving the
experience for its customers.
</p>

<p>
There is a small sliver of truth there, he said; Apple rolled out a feature
on its phones that allowed users to opt-out of third-party
surveillance—notably Facebook tracking.  96% of users opted out, he said;
the other 4% "<q>were either drunk or Facebook employees or drunk Facebook
employees</q>".  
</p>

<p>
So that makes it seem like Apple will not treat its customers as products,
but at the same time it added the opt-out, the company secretly started gathering
exactly the same information for its "<q>own surveillance
advertising network</q>".  There was no notice given to users and no way to
opt out of that surveillance; when journalists discovered it and published
their findings, Apple "<q>lied about it</q>".  The "<q>$1000 Apple
distraction rectangle in your pocket is something you paid for</q>", but
that does not stop Apple from "<q>treating you like the product</q>".
</p>

<p>
It is not just end users that Apple treats like products; the app vendors
are also treated that way with 30% fees for payment processing in the App
Store. That's what is happening with gig-app nurses: "<q>the nurses are the
product, the patients are the product, the hospitals are the product—in
enshittification, the product is anyone you can productize</q>".
</p>

<p>
While it is tempting to blame tech, Doctorow said, these companies did not
start out enshittified.  He recounted the "<q>magic</q>"  when Google debuted;
"<q>you could <a href="https://en.wikipedia.org/wiki/Ask.com">ask
Jeeves</a> questions for a thousand years and still not get an answer as
crisp, as useful, as helpful as the answer you would get by typing a few
vague keywords</q>" into Google.  Those companies spent decades producing
great products, which is why people switched to Google, bought iPhones, and
joined their friends on Facebook.  They were all born digital, thus could
have enshittified at any time, "<q>but they didn't, until they did, and
then they did it all at once</q>". 
</p>

<p>
He believes that changes to the policy environment is what has led to
enshittification, not changes in technology.  These changes to the rules of
the game were "<q>undertaken in living memory by named parties who were
warned at the time of the likely outcomes</q>"—and did it anyway.
Those people are now extremely rich and respected; they have "<q>faced no
consequences, no accountability for their role in ushering in the
Enshittocene</q>".  We have created a perfect breeding ground for the worst
practices in our society, which allowed them to thrive and dominate
decision-making for companies and governments "<q>leading to a vast
enshittening of everything</q>".
</p>

<p>
That is a dismal outlook, he said, but there is a bit of good news hidden
in there.  This change did not come about because of a new kind of evil
person or the weight of history, but rather because of specific policy
choices that were made—and can be unmade.  We can consign the enshitternet
to the scrap heap as
simply "<q>a transitional state from the old good internet that we used to
have and the new good internet that we could have</q>".
</p>

<p>
All companies want to maximize profits and the equation to do so is simple:
charge as much as you can, pay suppliers and workers as little as you can,
and spend the smallest amount possible on quality and safety.  The
theoretically "perfect" company that charges infinity and spends nothing
fails because no one wants to work for it—or buy anything from it.  That
shows that there are external constraints that tend to tamp down the
"<q>impulse to charge infinity and deliver nothing</q>".
</p>

<h4>Four constraints</h4>

<p>
In technology, there are four constraints that help make companies
better; they help push back against the impulse to enshittify.  The first
is markets; businesses that charge more and deliver less lose customers,
all else being equal.  This is the bedrock idea behind capitalism and it is
also the basis of antitrust law, but the
rules on antitrust have changed since the <a href="https://en.wikipedia.org/wiki/Sherman_Antitrust_Act">Sherman
Antitrust Act</a> was enacted in 1890.  More than forty years ago, during the Reagan
administration in the US, the interpretation of what it means to be a
monopoly was changed, not just in US, but also with its major trading
partners in the UK, EU, and Asia.
</p>

<p>
Under this interpretation, monopolies are assumed to be efficient; if
Google has 90% of the market, it means that it deserves to be there because
no one can possibly do search any better.  No competitor has arisen because
there is no room to improve on what Google is doing. This pro-monopoly
stance did exactly what might be expected, he said, it gave us more
monopolies: "<q>in pharma, in beer, in glass bottles, vitamin C, athletic
shoes, microchips, cars, mattresses, eyeglasses, and, of course,
professional wrestling</q>", he said to laughter.
</p>

<p>
Markets do not constrain technology firms because those firms do not compete
with their rivals—they simply buy their rivals instead. That is confirmed
by a memo from Mark Zuckerberg—"<q>a man who puts all of his dumbest ideas
in writing</q>"—who wrote: "<q>It is better to buy than to compete</q>".
Even though that anti-competitive behavior came to light before Facebook
was allowed to buy Instagram in order to ensure that users switching would
still be part of Facebook the platform, the Obama administration
permitted the sale.  Every government over the past 40 years, of all political stripes, has treated monopolies as efficient,
Doctorow said.
</p>

<p>
Regulation is also a constraint, unless the regulators have already been
captured by the industry they are supposed to oversee.  There are several
examples of <a href="https://en.wikipedia.org/wiki/Regulatory_capture">regulatory
capture</a> in the nursing saga, but the most egregious is that anyone in
the US can obtain financial information on anyone else in the country,
simply by contacting a data broker.  "<q>This is because the US congress
has not passed a new consumer privacy law since 1988.</q>"  The <a href="https://en.wikipedia.org/wiki/Video_Privacy_Protection_Act">Video
Privacy Protection Act</a> was aimed at stopping video-store clerks from
telling newspapers what VHS video titles were purchased or rented, but no
protections have been added since then.
</p>

<p>
The reason congress has not addressed privacy legislation "<q>since <a href="https://en.wikipedia.org/wiki/Die_Hard"><i>Die
Hard</i></a> was in its first run in theaters</q>" is neither a coincidence
nor an oversight, he said.  It is "<q>expensively purchased inaction</q>"
by an industry that has "<q>monetized the abuse of human rights at
unimaginable scale</q>".  The coalition in favor of freezing privacy law
keeps growing because there are so many ways to "<q>transmute the
systematic invasion of our privacy into cash</q>". 
</p>

<p>
Tech companies are not being constrained by either markets or governments,
but there are two other factors that could serve to tamp down "<q>the
reproduction of sociopathic, enshittifying monsters</q>" within these
companies.  The first is interoperability; in the non-digital world, it is
a lot of work to, say, ensure that any light bulb can be used with any
light socket.
In the digital world, all of our programs run on the same
"<q>Turing-complete, universal <a href="https://en.wikipedia.org/wiki/Von_Neumann_architecture">Von Neumann machine</a></q>", so a program that
breaks interoperability can be undone with a program that restores it.
Every ten-foot fence can be surmounted with an 11-foot ladder; if HP writes
a program to ensure that third-party ink cannot be used with its printers, someone
can write a program to undo that restriction.
</p>

<p>
DoorDash workers generally make their money on tips, but the app hides the
amount of the tip until the driver commits to taking the delivery.  A
company called Para wrote a program that looked inside the JSON that was
exchanged to find the tip, which it then displayed <i>before</i> the driver
had to commit.  DoorDash shut down the Para app, "<q>because in America,
apps like Para are illegal</q>".  The 1998 <a href="https://en.wikipedia.org/wiki/Digital_Millennium_Copyright_Act">Digital
Millennium Copyright Act</a> (DMCA) signed by Bill Clinton "<q>makes it a
felony to 'bypass an access control for a copyrighted work'</q>".  So even
just reverse-engineering the DoorDash app is a potential felony, which is
why companies are so desperate to move their users to apps instead of web
sites.  "<q>An app is just a web site that we have wrapped in a correct
DRM [<a href="https://en.wikipedia.org/wiki/Digital_rights_management">digital
rights management</a>] to make it a felony to protect your privacy while
you use it</q>", he said to widespread applause.
</p>

<p>
At the behest of the US trade representative, Europe and Canada have also
enacted DMCA-like laws.  This happened despite experts warning the leaders
of those countries that "<q>laws that banned tampering with digital locks
would let American tech giants corner digital markets in their
countries</q>".  The laws were a gift to monopolists and allowed companies
like HP to continually raise the price of ink until it "<q>has become the
most expensive substance you, as a civilian, can buy without a permit</q>";
printing a shopping list uses "<q>colored water that costs more than the
semen of a <a href="https://en.wikipedia.org/wiki/Kentucky_Derby">Kentucky-Derby</a>-winning
stallion</q>".
</p>

<p>
The final constraint, which did hold back platform decay for quite some
time, is labor. Tech workers have historically been respected and
well-paid, without unions.  The power of tech workers did not come from
solidarity, but from scarcity, Doctorow said.  The minute bosses ordered
tech workers to enshittify the product they were loyally working on,
perhaps missing various important social and family events  to
ship it on time, those workers could say no—perhaps in a much more coarse
way.  Tech workers could simply walk across the street "<q>and have a new
job by the end of the day</q>" if the boss persisted.
</p>

<p>
So labor held off enshittification after competition, regulation, and
interoperability were all systematically undermined and did so for quite
some time—until the mass tech layoffs.  There have been half a million
tech workers laid off since 2023, more are announced regularly, sometimes
in conjunction with raises for executive salaries and bonuses.  Now,
workers cannot turn their bosses down because there are ten others out
there just waiting to take their job.
</p>

<h4>Reversing course</h4>

<p>
Until we fix the environment we find ourselves in, the contagion will
spread to other companies, he said.  The good news is that after 40 years
of antitrust decline, there has been a lot of worldwide antitrust activity
and it is coming from all over the political spectrum.  The EU, UK,
Australia, Germany, France, Japan, South Korea, "<q>and China, yes,
China</q>" have passed new antitrust laws and launched enforcement actions.
The countries often collaborate, so a UK study on Apple's 30%
payment-processing fee was used by the EU to fine the company for billions
of euros and ban Apple's payment monopoly; those cases then found their way
to Japan and South Korea where Apple was further punished.
</p>

<p>
"<q>There are no billionaires funding the project to make billionaires
obsolete</q>", Doctorow said, so the antitrust work has come from and been
funded by
grassroots efforts.
</p>

<p>
Europe and Canada have passed strong right-to-repair legislation, but those
efforts "<q>have been hamstrung by the anti-circumvention laws</q>" (like
the DMCA).  Those laws can only be used if there are no locks to get
around, but the manufacturers ensure that every car, tractor, appliance,
medical implant, and hospital medical device has locks to prevent repair.
That raises the question of why these countries don't repeal their versions
of the DMCA.
</p>

<p>
The answer is tariffs, it seems.  The US trade representative has long
threatened countries with tariffs if they did not have such a law on their
books. "<q>Happy 'Liberation Day' everyone</q>", he said with a smile,
which resulted in laughter, cheering, and applause.  The response of most
countries when faced with the US tariffs (or threats thereof) has been to
impose retaliatory tariffs, making US products more expensive for their
citizens, which is a weird way to punish Americans.  "<q>It's like punching
yourself in the face really hard and hoping someone else says 'ouch'.</q>"
</p>

<p>
What would be better is for the countries to break the monopolies of the US
tech giants by making it legal to reverse-engineer, jailbreak, and modify
American products and services.  Let companies jailbreak Teslas and deliver
all of the features that ship in the cars, but are disabled by software,
for one price; that is a much better way to hurt Elon Musk, rather than by
expressing outrage at his Nazi salutes, since he loves the
attention. "<q>Kick him in the dongle.</q>"
</p>

<p>
Or, let
a Canadian company set up an App Store that only charges 3% for payment
processing, which will give any content producer an immediate 25% raise, so
publishers will flock to it.   The same could be done for car and tractor
diagnostic devices and more.
"<q>Any country in the world has it right now in their power to become a
tech-export powerhouse.</q>"
Doing so would directly attack the tech giants in their most profitable
lines of business: "<q>it takes the revenues
from those rip-off scams globally from hundreds of billions of dollars to
zero overnight</q>".  And "<q>that is how you win a trade war</q>", he said
to more applause.
</p>

<p>
He finished with a veritable laundry list of all of the ills facing the
world today (the "<q>omni-shambolic poly-crisis</q>"), both on and off the
internet, and noted that the tech giants
would willingly "<q>trade a habitable planet and human rights for a 3% tax
cut</q>".  But it did not have to be this way, "<q>the enshitternet was not
inevitable</q>" and was, in fact, the product of policy choices made by
known people in the last few decades.  "<q>They chose enshittification; we
warned them what would come of it and we don't have to be eternal prisoners
of the catastrophic policy blunders of clueless lawmakers of old.</q>"
</p>

<p>
There once was an "<q>old good internet</q>", Doctorow said, but it was
too difficult for non-technical people to connect up to; web 2.0 changed
that, making it easy for everyone to get online, but that led directly into
hard-to-escape walled gardens.  A new good internet is possible and needed; "<q>we can
build it with all of the technological self-determination of the old good
internet and the ease of web 2.0</q>".  It can be a place to come together
and organize in order to "<q>resist and survive climate collapse, fascism,
genocide, and authoritarianism</q>".  He concluded: "<q>we can build it and
we must</q>".
</p>

<p>
His speech was well-received and was met with a standing ovation.  Some of
his harshest rhetoric (much of which was toned down here) may not have been
popular with everyone, perhaps especially the PyCon sponsors who were named and
shamed in the keynote, but it did seem to resonate within the crowd of
attendees.   Doctorow's perspective is always interesting—and he certainly
pulls no punches.
</p>

<p>
A <a href="https://www.youtube.com/watch?v=ydVmzg_SJLw">YouTube video</a>
of the talk is available.
</p>

<p>
[I would like to thank LWN's travel sponsor, the Linux Foundation, for
supporting my travel to Pittsburgh for PyCon.]
</p><br clear="all"><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Archives/ConferenceIndex/">Conference</a></td><td><a href="https://lwn.net/Archives/ConferenceIndex/#PyCon-2025">PyCon/2025</a></td></tr>
            </tbody></table><br clear="all">

               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why are 2025/05/28 and 2025-05-28 different days in JavaScript? (135 pts)]]></title>
            <link>https://brandondong.github.io/blog/javascript_dates/</link>
            <guid>44113397</guid>
            <pubDate>Wed, 28 May 2025 07:09:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brandondong.github.io/blog/javascript_dates/">https://brandondong.github.io/blog/javascript_dates/</a>, See on <a href="https://news.ycombinator.com/item?id=44113397">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <article><header>  <time datetime="2025-05-28">2025-05-28</time> </header><p>While setting up this site itself, I ran into the following oddity:</p>
<pre><code><span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(<span>'2025/05/28'</span>).<span>toDateString</span>()); <span>// Wed May 28 2025</span>
<span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(<span>'2025-05-28'</span>).<span>toDateString</span>()); <span>// Tue May 27 2025</span>
<span>// Bonus: (omit leading 0)</span>
<span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(<span>'2025-5-28'</span>).<span>toDateString</span>()); <span>// Wed May 28 2025</span></code></pre>
<p>You may get different results on your machine!</p>
<section id="What's_going_on?"> <h2><a href="#What's_going_on?">What's going on?</a></h2> <p>A <code>Date</code> in JavaScript always represents a point in time (i.e. milliseconds since epoch). This is more apparent when printing out the full date string:</p><pre><code><span>const</span> date = <span>new</span> <span>Date</span>(<span>'2025/05/28'</span>);
<span>console</span>.<span>log</span>(date); <span>// Wed May 28 2025 00:00:00 GMT-0700 (Pacific Daylight Time)</span>
<span>console</span>.<span>log</span>(date.<span>toDateString</span>()); <span>// Wed May 28 2025</span></code></pre><p>In this case, the passed-in date string is being interpreted as a timestamp in my local time zone. <code>toDateString()</code> also operates relative to the local time and so we get the same day-of-the-month back out.</p><p>The difference with <code>'2025-05-28'</code> is in parsing behavior; the string is interpreted as UTC and so ends up at a different point in time:</p><pre><code><span>const</span> date = <span>new</span> <span>Date</span>(<span>'2025-05-28'</span>);
<span>console</span>.<span>log</span>(date); <span>// Tue May 27 2025 17:00:00 GMT-0700 (Pacific Daylight Time)</span>
<span>console</span>.<span>log</span>(date.<span>toDateString</span>()); <span>// Tue May 27 2025</span></code></pre><p>Why the discrepancy?</p> </section>
<section id="The_misadventures_of_browser_date-parsing"> <h2><a href="#The_misadventures_of_browser_date-parsing">The misadventures of browser date-parsing</a></h2> <p>After digging through the code and commit histories of Chrome/Firefox/Safari, I’ve reconstructed a timeline:</p><ol>
<li>In 2009, these browsers supported parsing a mishmash of date-time formats. When time zone offsets are not explicitly specified in the string, they all fall back to using local time, including for a date string like <code>'2025/05/28'</code>.</li>
<li><a href="https://ecma-international.org/wp-content/uploads/ECMA-262_5th_edition_december_2009.pdf" target="_blank">ES5</a>, to be released at the end of the year, includes a requirement for supporting a new standardized date-time format based heavily off of <a href="https://en.wikipedia.org/wiki/ISO_8601" target="_blank">ISO 8601</a>. This format is broken up into date-<em>only</em> forms like <code>'2025-05-28'</code> and date-<em>time</em> forms like <code>'2025-05-27T17:00-07:00'</code> where the ending UTC offset is optional.
<ul>
<li>What does the spec say about time zone interpretation for date-only forms (which never have an offset) or date-time forms missing an offset? Only that <q>The String may be interpreted as a local time, a UTC time, or a time in some other time zone, depending on the contents of the String.</q> (Gee, thanks…)</li>
</ul>
</li>
<li>Firefox is the first to <a href="https://github.com/mozilla-firefox/firefox/commit/b866df4f3680502a8e78e67bd495a96ea3d9c59e" target="_blank">implement this requirement</a>. They choose to interpret date-only forms as UTC and date-time forms missing an offset as local time. Not only is there now a discrepancy between <code>'2025/05/28'</code> and <code>'2025-05-28'</code>, but also surprising behavior like: <pre><code><span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(<span>'2025-05-28'</span>)); <span>// Tue May 27 2025 17:00:00 GMT-0700 (Pacific Daylight Time)</span>
<span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(<span>'2025-05-28T00:00'</span>)); <span>// Wed May 28 2025 00:00:00 GMT-0700 (Pacific Daylight Time)</span></code></pre></li>
<li>Chrome is <a href="https://chromium.googlesource.com/v8/v8.git/+/6ceb02e6eb791f837ed84b7ed41332058cd3f1dc" target="_blank">next</a>, choosing to use local time for both.</li>
<li>Safari is <a href="https://github.com/WebKit/WebKit/commit/d9bdbae4126006e130914e5ebe57a761d3ea19bb" target="_blank">next</a>, but its parsing logic incorrectly requires that all date, time, and offset fields be present.</li>
<li><a href="https://262.ecma-international.org/5.1/index.html#sec-15.9.4.2" target="_blank">ES5.1</a> releases in mid-2011 and now additionally mentions that <q>The value of an absent time zone offset is Z.</q></li>
<li>Chrome <a href="https://chromium.googlesource.com/v8/v8.git/+/ff9ce1abd4add01bbb3f1917bd72207e5ddd70b5" target="_blank">updates its implementation</a> to use UTC for both cases.</li>
<li>Safari <a href="https://github.com/WebKit/WebKit/commit/a841b97de44dbff4ecb30f62e984b1fc72493ac6" target="_blank">fixes the earlier bug</a> and uses UTC for both cases.</li>
<li>A <a href="https://web.archive.org/web/20141214115940/https://bugs.ecmascript.org/show_bug.cgi?id=112" target="_blank">bug</a> is filed against the spec itself, pointing out that ISO 8601 represents date-times without offsets as local time. <a href="https://262.ecma-international.org/6.0/index.html#sec-date-time-string-format" target="_blank">ES6</a> in 2015 replaces the ES5.1 addition with <q>If the time zone offset is absent, the date-time is interpreted as a local time.</q></li>
<li>Chrome <a href="https://chromium.googlesource.com/v8/v8.git/+/f06754a8e1d305a43560705f6c167d85d40e602d" target="_blank">switches back</a> to using local time for both cases.</li>
<li>A <a href="https://issues.chromium.org/issues/40440226" target="_blank">bug</a> is filed against Chrome for breaking backwards compatibility when parsing date-only forms. They <a href="https://chromium.googlesource.com/v8/v8.git/+/dd3f1ecf719afd21b4c695c776b4da2fb494ef92" target="_blank">revert the previous change</a>.</li>
<li>Chrome files an <a href="https://github.com/tc39/ecma262/issues/87" target="_blank">issue</a> against the spec and after discussion, it’s decided to switch date-only forms back to UTC but leave date-time forms without offset as local (i.e. Firefox’s behavior).</li>
<li><a href="https://262.ecma-international.org/7.0/index.html#sec-date.parse" target="_blank">ES7</a> releases with the updated requirement. Chrome <a href="https://chromium.googlesource.com/v8/v8.git/+/d31c5410c4fdfc5eb66582892d5e3ecd3706bd58" target="_blank">makes the change</a> and then eventually, <a href="https://github.com/WebKit/WebKit/commit/2148a43f377e67c60b167f5730c7b5c5c21b202d" target="_blank">Safari</a>.</li>
</ol><p>This behavior has been maintained to the present day where every possible string accepted by the <code>Date</code> constructor falls back to local time <em>except</em> valid ISO date-strings like <code>'2025-05-28'</code>.</p><p>What’s interesting looking at the timeline is that despite being designed as a standardized format, from its release in 2009 up until early 2020, there would never exist a point where the major browsers behaved consistently for missing offsets. Meanwhile, Chrome has flipped hilariously from <a href="https://chromium.googlesource.com/v8/v8.git/+/6ceb02e6eb791f837ed84b7ed41332058cd3f1dc" target="_blank">local</a> → <a href="https://chromium.googlesource.com/v8/v8.git/+/ff9ce1abd4add01bbb3f1917bd72207e5ddd70b5" target="_blank">UTC</a> → <a href="https://chromium.googlesource.com/v8/v8.git/+/f06754a8e1d305a43560705f6c167d85d40e602d" target="_blank">local</a> → <a href="https://chromium.googlesource.com/v8/v8.git/+/dd3f1ecf719afd21b4c695c776b4da2fb494ef92" target="_blank">UTC</a> → <a href="https://chromium.googlesource.com/v8/v8.git/+/d31c5410c4fdfc5eb66582892d5e3ecd3706bd58" target="_blank">local</a> when parsing <code>'2025-05-28T00:00'</code>. And all this just to settle at Firefox’s 2009 behavior which, in my opinion, is the most unintuitive of them all.</p> </section>
<section id="What_about_Temporal?"> <h2><a href="#What_about_Temporal?">What about Temporal?</a></h2> <p>For the unaware, <a href="https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/" target="_blank">JavaScript Temporal is coming</a>: a new set of date and time APIs intended to replace the <code>Date</code> object.</p><p>Our whole original date parsing issue stemmed from time zone ambiguity but in many cases, the desire is to treat date-only strings as exactly that — dates only. For example, when I say that Christmas this year is <code>2025-12-25</code>, I don’t mean the universal instant in time that is <code>2025-12-25T00:00:00.000Z</code>.</p><p>While <code>Date</code> can only ever represent the latter, Temporal offers the option of <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal/PlainDate" target="_blank">plain dates</a> (i.e. a date without a time zone). <code>'2025-12-25'</code> is just <code>2025-12-25</code>, side-stepping the parsing ambiguity issue entirely.</p><p>But what if one really wants to parse a date-only string into an instant in time? What time zone will Temporal choose when absent in the string itself?</p><p>Answer: It’s a hard error; an <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal/Instant#z%C2%B1hhmm" target="_blank">offset</a> or <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal/ZonedDateTime#time_zone_id" target="_blank">time zone identifier</a> must be provided. No repeat mistakes here.</p> </section>
<section id="Bonus:_enter_the_cursed_zone"> <h2><a href="#Bonus:_enter_the_cursed_zone">Bonus: enter the cursed zone</a></h2> <p>One thing I never realized until reading browser date-parsing source code is just how lenient it can be.</p><p>Here’s a fun example for Chrome/Firefox: can you spot why this (valid!) date string is being parsed as the month of May?</p><pre><code><span>const</span> date = <span>'it is wednesday, my dudes. 2025, April, maybe...28(?)'</span>;
<span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(date)); <span>// Wed May 28 2025 00:00:00 GMT-0700 (Pacific Daylight Time)</span></code></pre> </section></article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[As a developer, my most important tools are a pen and a notebook (348 pts)]]></title>
            <link>https://hamatti.org/posts/as-a-developer-my-most-important-tools-are-a-pen-and-a-notebook/</link>
            <guid>44113210</guid>
            <pubDate>Wed, 28 May 2025 06:27:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hamatti.org/posts/as-a-developer-my-most-important-tools-are-a-pen-and-a-notebook/">https://hamatti.org/posts/as-a-developer-my-most-important-tools-are-a-pen-and-a-notebook/</a>, See on <a href="https://news.ycombinator.com/item?id=44113210">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>
  After I signed my contract to join my new job a month ago, I was so excited.
  Not only that I would join the company but also because I got to buy a new
  notebook. The weekend before my first day I headed to the local bookstore and
  spent a good amount of time browsing through the notebooks available and ended
  up with this happy orange one.
</p>
<p><img src="https://hamatti.org/assets/img/posts/as-a-developer-my-most-important-tools-are-a-pen-and-a-notebook/1.png" alt=" "></p><p>Why am I so excited about a notebook then?</p>

<p>
  <b>Because it’s the most important tool I have as a software developer.</b>
</p>

<p>
  When it comes to building software or solving problems, writing code is the
  necessary bit at the end where we tell the computer what to do but way more
  important than writing that code is figuring out what code to write.
</p>

<p>
  I learned very early on in my career that I’m not very good at thinking when
  I’m at a computer. When I have my code editor open, I’m in a “function mode”
  where I write stuff that does something. When my brain hits that mode, there’s
  not much creative energies flowing around.
</p>

<p>
  So I often step away from the computer. Sometimes it’s for walks (I tend to
  work at companies near water features so I’ve used to go for strolls near San
  Francisco Bay, Spree or the best of them all, Aurajoki) but often I take my
  notebook and sit on a couch or outside at the patio and ponder.
</p>

<p>
  I might be thinking of initial solutions to new problems (ie. designing how to
  approach it, drawing UI sketches or flowcharts) or I’m helping myself
  understand the flow of data and interactions in the current code base to
  figure out how to fix a bug or add new functionality.
</p>

<p>
  Writing (and sketching) is such a powerful tool for thinking in that. It helps
  me turn my vague abstract ideas into tangible artefacts through words and
  drawings. It helps me expose the gaps in my knowledge or understanding because
  I can’t skip them as easily when writing as I can when just thinking about
  them.
</p>

<p>
  When I’ve written code, I like to write about it as if I would be explaining
  it to someone else. Whenever possible, I like to publish them as blog posts as
  well but even when it’s not possible, writing it that way helps me find
  inconsistencies, bad designs and even mistakes in my code. I’ve written about
  <a href="https://hamatti.org/posts/blogging-is-my-new-favorite-refactoring-tool/">how writing is my favourite refactoring tool</a>.
</p>

<p>
  A lovely side benefit of thinking through writing is that it leaves behind a
  copy of my thoughts and the process of how I reached them. I don’t have to
  separately go and write notes because my thinking process already created most
  of them and I usually just reorganise and polish them a bit to make them more
  useful in the future as well.
</p>

<p>
  That way, if someone asks what I was thinking about when I did X two weeks,
  six months or two years ago, I can go back to my notes and tell them exactly
  that. (Spoilers: quite often that someone is me in the future.)
</p>

<p>
  I also have a longer post about
  <a href="https://hamatti.org/posts/how-i-take-work-notes-as-a-developer/">how I take work notes as a developer</a>
  that focuses more on the content of my notes.
</p>





            <hr>
          <p>If something above resonated with you, let's start a discussion about it! <strong>Email me at juhamattisantala at gmail dot com and share your thoughts</strong>. In 2025, I want to have more deeper discussions with people from around the world and I'd love if you'd be part of that.</p>

        </div></div>]]></description>
        </item>
    </channel>
</rss>