<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 29 Jun 2024 12:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[How to waste bandwidth, battery power, and annoy sysadmins (204 pts)]]></title>
            <link>https://rachelbythebay.com/w/2024/06/28/fxios/</link>
            <guid>40828203</guid>
            <pubDate>Sat, 29 Jun 2024 06:07:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rachelbythebay.com/w/2024/06/28/fxios/">https://rachelbythebay.com/w/2024/06/28/fxios/</a>, See on <a href="https://news.ycombinator.com/item?id=40828203">Hacker News</a></p>
Couldn't get https://rachelbythebay.com/w/2024/06/28/fxios/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The XAES-256-GCM extended-nonce AEAD (135 pts)]]></title>
            <link>https://words.filippo.io/dispatches/xaes-256-gcm/</link>
            <guid>40826683</guid>
            <pubDate>Sat, 29 Jun 2024 00:01:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://words.filippo.io/dispatches/xaes-256-gcm/">https://words.filippo.io/dispatches/xaes-256-gcm/</a>, See on <a href="https://news.ycombinator.com/item?id=40826683">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <p>About a year ago <a href="https://words.filippo.io/dispatches/xaes-256-gcm-11/">I wrote</a> that “I want to use XAES-256-GCM/11, which has a number of nice properties and only the annoying defect of not existing.” Well, there is now <a href="https://c2sp.org/XAES-256-GCM?ref=words.filippo.io">an XAES-256-GCM specification</a>. (Had to give up on the /11 part, but that was just a performance optimization.)</p>
<p>XAES-256-GCM is an <em>authenticated encryption with additional data</em> (AEAD) algorithm with 256-bit keys and <strong>192-bit nonces</strong>. It was designed with the following goals:</p>
<ol>
<li>supporting a nonce large enough to be safe to generate randomly for a virtually unlimited number of messages (2⁸⁰ messages with collision risk 2⁻³²);</li>
<li>full, straightforward FIPS 140 compliance; and</li>
<li>trivial implementation on top of common cryptographic libraries.</li>
</ol>
<p>The large nonce enables safer and more friendly APIs that automatically read a fresh nonce from the operating system’s CSPRNG for every message, without burdening the user with any <a href="https://en.wikipedia.org/wiki/Birthday_attack?ref=words.filippo.io">birthday bound</a> calculations. Compliance and compatibility make it available anywhere an AEAD might be needed, including in settings where alternative large-nonce AEADs are not an option.</p>
<p>Like XChaCha20Poly1305, XAES-256-GCM is an extended-nonce construction on top of AES-256-GCM. That is, it uses the key and the large nonce to compute a derived key for the underlying AEAD.</p>
<p>It’s simple enough to fit inline in this newsletter. Here we go. <em>K</em> and <em>N</em> are the input key and nonce, <em>Kₓ</em> and <em>Nₓ</em> are the derived AES-256-GCM key and nonce.</p>
<ol>
<li><em>L</em> = AES-256ₖ(0¹²⁸)</li>
<li>If MSB₁(<em>L</em>) = 0, then <em>K1</em> = <em>L</em> &lt;&lt; 1;<br>
Else <em>K1</em> = (<em>L</em> &lt;&lt; 1) ⊕ 0¹²⁰10000111</li>
<li><em>M1</em> = 0x00 || 0x01 || <code>X</code> || 0x00 || <em>N</em>[:12]</li>
<li><em>M2</em> = 0x00 || 0x02 || <code>X</code> || 0x00 || <em>N</em>[:12]</li>
<li><em>Kₓ</em> = AES-256ₖ(<em>M1</em> ⊕ <em>K1</em>) || AES-256ₖ(<em>M2</em> ⊕ <em>K1</em>)</li>
<li><em>Nₓ</em> = <em>N</em>[12:]</li>
</ol>
<p>As you can see, it costs three AES-256ₖ calls per message, although one can be precomputed for a given key, and the other two can reuse its key schedule.</p>
<p>The <a href="https://github.com/C2SP/C2SP/blob/main/XAES-256-GCM/go/XAES-256-GCM.go?ref=words.filippo.io">Go reference implementation</a> fits in less than 100 lines of mostly boilerplate, including the precomputation optimization, and only uses the standard library’s crypto/cipher and crypto/aes.</p>
<p>Importantly, you could also describe XAES-256-GCM entirely in terms of a standard <a href="https://csrc.nist.gov/publications/detail/sp/800-108/rev-1/final?ref=words.filippo.io">NIST SP 800-108r1</a> KDF and the standard NIST AES-256-GCM AEAD (<a href="https://csrc.nist.gov/pubs/sp/800/38/d/final?ref=words.filippo.io">NIST SP 800-38D</a>, <a href="https://csrc.nist.gov/pubs/fips/197/final?ref=words.filippo.io">FIPS 197</a>).</p>
<blockquote>
<p>Instantiate a counter-based KDF (<a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-108r1.pdf?ref=words.filippo.io#%5B%7B%22num%22%3A79%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C70%2C300%2C0%5D">NIST SP 800-108r1, Section 4.1</a>) with CMAC-AES256 (<a href="https://csrc.nist.gov/publications/detail/sp/800-38b/final?ref=words.filippo.io">NIST SP 800-38B</a>) and the input key as <em>Kin</em>, the ASCII letter <code>X</code> (0x58) as <em>Label</em>, the first 96 bits of the input nonce as <em>Context</em> (as recommended by <a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-108r1.pdf?ref=words.filippo.io#%5B%7B%22num%22%3A71%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C70%2C720%2C0%5D">NIST SP 800-108r1, Section 4</a>, point 4), a counter (<em>i</em>) size of 16 bits, and omitting the optional <em>L</em> field, and produce a 256-bit derived key. Use that derived key and the last 96 bits of the input nonce with AES-256-GCM.</p>
</blockquote>
<p>Thanks to the choice of parameters, if we peel off the KDF and CMAC abstractions, the result is barely slower and more complex than straightforwardly invoking AES-256 on a counter. In exchange, we get a vetted and compliant solution. The parameters <a href="https://github.com/C2SP/C2SP/blob/main/XAES-256-GCM/openssl/openssl.c?ref=words.filippo.io">are supported by the high-level OpenSSL API</a>, too.</p>
<p>Why no more “/11”? Well, half the point of using AES-GCM is FIPS 140 compliance. (The other half being hardware acceleration.) If we mucked with the rounds number the design wouldn’t be compliant.</p>
<p>Indeed, if compliance is not a goal there are a number of alternatives, from AES-GCM-SIV to modern AEAD constructions based on the AES core. The specification has <a href="https://c2sp.org/XAES-256-GCM?ref=words.filippo.io#alternatives">an extensive Alternatives section</a> that compares each of them to XAES-256-GCM.</p>
<p>Also included in the specification are <a href="https://c2sp.org/XAES-256-GCM?ref=words.filippo.io#test-vectors">test vectors</a> for the two main code paths (MSB₁(<em>L</em>) = 0 and 1), and <a href="https://c2sp.org/XAES-256-GCM?ref=words.filippo.io#accumulated-randomized-tests">accumulated test vectors</a> that compress 10 000 or 1 000 000 random iterations.</p>
<p>To sum up, XAES-256-GCM is designed to be a safe, boring, compliant, and interoperable AEAD that can fit high-level APIs, the kind we’d like to add to Go. It’s designed to complement XChaCha20Poly1305 and AES-GCM-SIV as implementations of a hypothetical <a href="https://github.com/golang/go/issues/54364?ref=words.filippo.io#issuecomment-1642676993">nonce-less AEAD API</a>. If other cryptography library maintainers like it (or don’t), I would love to hear about it, because we are not big fans of adding Go-specific constructions to the standard library.</p>
<p>By the way, I have an exciting update about my professional open source maintainer effort coming in less than two weeks! Make sure to subscribe to <a href="https://filippo.io/newsletter?ref=words.filippo.io">Maintainer Dispatches</a> or to follow me on Bluesky at <a href="https://bsky.app/profile/filippo.abyssdomain.expert?ref=words.filippo.io">@filippo.abyssdomain.expert</a> or on Mastodon at <a href="https://abyssdomain.expert/@filippo?ref=words.filippo.io">@filippo@abyssdomain.expert</a>. (Or, see you at <a href="https://www.gophercon.com/?ref=words.filippo.io">GopherCon</a> in Chicago!)</p>
<h2 id="the-picture">The picture</h2>
<p>Earlier this year I ran in the <a href="https://www.centopassi.net/?ref=words.filippo.io">Centopassi</a> motorcycle competition. It involves driving more than 1600km on mountain roads, through one hundred GPS coordinates you select in advance from a long list, in three days and a half. It’s been fantastic. It took me to corners of Italy I would have never seen, and I had a lot of fun. This picture is taken at our 100th location, after a couple kilometers of unpaved hairpins on the side of the hill. The finish line was at the lake you can see in the distance. I was ecstatic.</p>
<p>That’s my 2014 KTM Duke 690, a single-cylinder “naked” from before KTM knew how to make larger street bikes. It’s weird and I love it.</p>
<p><img src="https://words.filippo.io/content/images/2024/06/IMG_1921.jpeg" alt="A black motorcycle with saddlebags and a race plate, parked on a dirt road overlooking a vast, scenic valley with green hills, a lake in the distance, and mountains under a bright blue sky with scattered white clouds." loading="lazy"></p>
<p>My awesome clients—<a href="https://www.sigsum.org/?ref=words.filippo.io">Sigsum</a>, <a href="https://www.latacora.com/?ref=words.filippo.io">Latacora</a>, <a href="https://interchain.io/?ref=words.filippo.io">Interchain</a>, <a href="https://smallstep.com/?ref=words.filippo.io">Smallstep</a>, <a href="https://www.avalabs.org/?ref=words.filippo.io">Ava Labs</a>, <a href="https://goteleport.com/?ref=words.filippo.io">Teleport</a>, <a href="https://www.sandboxaq.com/?ref=words.filippo.io">SandboxAQ</a>, <a href="https://charm.sh/?ref=words.filippo.io">Charm</a>, and <a href="https://tailscale.com/?ref=words.filippo.io">Tailscale</a>—are funding all my work for the community and through our retainer contracts they get face time and unlimited access to advice on Go and cryptography.</p>
<p>Here are a few words from some of them!</p>
<p>Latacora — <a href="https://www.latacora.com/?ref=words.filippo.io">Latacora</a> bootstraps security practices for startups. Instead of wasting your time trying to hire a security person who is good at everything from Android security to AWS IAM strategies to SOC2 and apparently has the time to answer all your security questionnaires plus never gets sick or takes a day off, you hire us. We provide a crack team of professionals prepped with processes and power tools, coupling individual security capabilities with strategic program management and tactical project management.</p>
<p>Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. <a href="https://goteleport.com/identity-governance-security/?utm=filippo&amp;ref=words.filippo.io">Teleport Identity Governance &amp; Security</a> is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.</p>
<p>Ava Labs — We at <a href="https://www.avalabs.org/?ref=words.filippo.io">Ava Labs</a>, maintainer of <a href="https://github.com/ava-labs/avalanchego?ref=words.filippo.io">AvalancheGo</a> (the most widely used client for interacting with the <a href="https://www.avax.network/?ref=words.filippo.io">Avalanche Network</a>), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.</p>
<p>SandboxAQ — <a href="https://www.sandboxaq.com/?ref=words.filippo.io">SandboxAQ</a>’s <a href="https://www.sandboxaq.com/solutions/aqtive-guard?ref=words.filippo.io">AQtive Guard</a> is a unified cryptographic management software platform that helps protect sensitive data and ensures compliance with authorities and customers. It provides a full range of capabilities to achieve cryptographic agility, acting as an essential cryptography inventory and data aggregation platform that applies current and future standardization organizations mandates. AQtive Guard automatically analyzes and reports on your cryptographic security posture and policy management, enabling your team to deploy and enforce new protocols, including quantum-resistant cryptography, without re-writing code or modifying your IT infrastructure.</p>

        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Eulogy for DevOps (194 pts)]]></title>
            <link>https://matduggan.com/a-eulogy-for-devops/</link>
            <guid>40826236</guid>
            <pubDate>Fri, 28 Jun 2024 22:59:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matduggan.com/a-eulogy-for-devops/">https://matduggan.com/a-eulogy-for-devops/</a>, See on <a href="https://news.ycombinator.com/item?id=40826236">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
<p>We hardly knew ye. </p><p>DevOps, like many trendy technology terms, has gone from the peak of optimism to the depths of exhaustion. While many of the fundamental ideas behind the concept have become second-nature for organizations, proving it did in fact have a measurable outcome, the difference between the initial intent and where we ended up is vast. For most organizations this didn't result in a wave of safer, easier to use software but instead encouraged new patterns of work that centralized risk and introduced delays and irritations that didn't exist before. We can move faster than before, but that didn't magically fix all our problems. </p><p>The cause of its death was a critical misunderstanding over what was causing software to be hard to write. The belief was by removing barriers to deployment, more software would get deployed and things would be easier and better. Effectively that the issue was that developers and operations teams were being held back by ridiculous process and coordination. In reality these "soft problems" of communication and coordination are much more difficult to solve than the technical problems around pushing more code out into the world more often. </p><h3 id="what-is-devops">What is DevOps?</h3><p>DevOps, when it was introduced around 2007, was a pretty radical concept of removing the divisions between people who ran the hardware and people who wrote the software. Organizations still had giant silos between teams, with myself experiencing a lot of that workflow. </p><p>Since all computer nerds also love space, it was basically us cosplaying as NASA. Copying a lot of the procedures and ideas from NASA to try and increase the safety around pushing code out into the world. Different organizations would copy and paste different parts, but the basic premise was every release was as close to bug free as time allowed. You were typically shooting for zero exceptions.</p><p>When I worked for a legacy company around that time, the flow for releasing software looked as follows. </p><ul><li>Development team would cut a release of the server software with a release number in conjunction with the frontend team typically packaged together as a full entity. They would test this locally on their machines, then it would go to dev for QA to test, then finally out to customers once the QA checks were cleared. </li><li>Operations teams would receive a playbook of effectively what the software was changing and what to do if it broke. This would include how it was supposed to be installed, if it did anything to the database, it was a whole living document. The idea was the people managing the servers, networking equipment and SANs had no idea what the software did or how to fix it so they needed what were effectively step by step instructions. Sometimes you would even get this as a paper document. </li><li>Since these happened often inside of your datacenter, you didn't have unlimited elasticity for growth. So, if possible, you would slowly roll out the update and stop to monitor at intervals. But you couldn't do what people see now as a blue/green deployment because rarely did you have enough excess server capacity to run two versions at the same time for all users. Some orgs did do different datacenters at different times and cut between them (which was considered to be sort of the highest tier of safety). </li><li>You'd pick a deployment day, typically middle of the week around 10 AM local time and then would monitor whatever metrics you had to see if the release was successful or not. These were often pretty basic metrics of success, including some real eyebrow raising stuff like "is support getting more tickets" and "are we getting more hits to our uptime website". Effectively "is the load balancer happy" and "are customers actively screaming at us". </li><li>You'd finish the deployment and then the on-call team would monitor the progress as you went. </li></ul><h3 id="why-didnt-this-work">Why Didn't This Work</h3><p>Part of the issue was this design was very labor-intensive. You needed enough developers coordinating together to put together a release. Then you needed a staffed QA team to actually take that software and ensure, on top of automated testing which was jusssttttt starting to become a thing, that the software actually worked. Finally you needed a technical writer working with the development team to walk through what does a release playbook look like and then finally have the Operations team receive, review the book and then implement the plan. </p><p>It was also slow. Features would often be pushed for months even when they were done just because a more important feature had to go out first. Or this update was making major changes to the database and we didn't want to bundle in six things with the one possibly catastrophic change. It's effectively the Agile vs Waterfall design broken out to practical steps. </p><figure><img src="https://kruschecompany.com/wp-content/uploads/2021/09/Waterfall-vs-Agile-in-software-development-infographic.jpg" alt="Waterfall vs Agile in software development infographic" loading="lazy" width="1000" height="2000"></figure><p>A lot of the lip service around this time that was given as to why organizations were changing was, frankly, bullshit. The real reason companies were so desperate to change was the following:</p><ul><li>Having lots of mandatory technical employees they couldn't easily replace was a bummer</li><li>Recruitment was hard and expensive. </li><li>Sales couldn't easily inject whatever last-minute deal requirement they had into the release cycle since that was often set it stone. </li><li>It provided an amazing opportunity for SaaS vendors to inject themselves into the process by offloading complexity into their stack so they pushed it hard.</li><li>The change also emphasized the strengths of cloud platforms at the time when they were starting to gobble market share. You didn't need lots of discipline, just allocate more servers. </li><li>Money was (effectively) free so it was better to increase speed regardless of monthly bills. </li><li>Developers were understandably frustrated that minor changes could take weeks to get out the door while they were being blamed for customer complaints.</li></ul><p>So executives went to a few conferences and someone asked them if they were "doing DevOps" and so we all changed our entire lives so they didn't feel like they weren't part of the cool club. </p><h3 id="what-was-devops">What Was DevOps?</h3><p>Often this image is used to sum it up:</p><figure><img src="https://wac-cdn.atlassian.com/dam/jcr:ef9fe684-c6dc-4ba0-a636-4ef7bcfa11f1/New%20DevOps%20Loop%20image.png?cdnVersion=1833" alt="DevOps Infinity Wheel" loading="lazy" width="2240" height="1090"></figure><p>In a nutshell, the basic premise was that development teams and operations teams were now one team. QA was fired and replaced with this idea that because you could very quickly deploy new releases and get feedback on those releases, you didn't need a lengthy internal test period where every piece of functionality was retested and determined to still be relevant. </p><p>Often this is <em>conflated</em> with the concept of SRE from Google, which I will argue until I die is a giant mistake. SRE is in the same genre but a very different tune, with a much more disciplined and structured approach to this problem. DevOps instead is about the simplification of the stack such that any developer on your team can deploy to production as many times in a day as they wish with only the minimal amounts of control on that deployment to ensure it had a reasonably high chance of working. </p><p>In reality DevOps as a practice looks much more like how Facebook operated, with employees committing to production on their first day and relying extensively on real-world signals to determine success or failure vs QA and tightly controlled releases. </p><p>In practice it looks like this:</p><ul><li>Development makes a branch in git and adds a feature, fix, change, etc. </li><li>They open up a PR and then someone else on that team looks at it, sees it passes their internal tests, approves it and then it gets merged into main. This is effectively the only safety step, relying on the reviewer to have perfect knowledge of all systems. </li><li>This triggers a webhook to the CI/CD system which starts the build (often of an entire container with this code inside) and then once the container is built, it's pushed to a container registry. </li><li>The CD system tells the servers that the new release exists, often through a Kubernetes deployment or pushing a new version of an internal package or using the internal CLI of the cloud providers specific "run a container as a service" platform. It then monitors and tells you about the success or failure of that deployment. </li><li>Finally there are release-aware metrics which allow that same team, who is on-call for their application, to see if something has changed since they released it. Is latency up, error count up, etc. This is often just a line in a graph saying this was old and this is new. </li><li>Depending on the system, this can either be something where every time the container is deployed it is on brand-new VMs or it is using some system like Kubernetes to deploy "the right number" of containers. </li></ul><p>The sales pitch was simple. Everyone can do everything so teams no longer need as many specialized people. Frameworks like Rails made database operations less dangerous, so we don't need a team of DBAs. Hell, use something like Mongo and you never need a DBA! </p><p>DevOps combined with Agile ended up with a very different philosophy of programming which had the following conceits:</p><ul><li>The User is the Tester</li><li>Every System Is Your Specialization </li><li>Speed Of Shipping Above All</li><li>Catch It In Metrics</li><li>Uptime Is Free, SSO Costs Money (free features were premium, expensive availability wasn't charged for)</li><li>Logs Are Business Intelligence</li></ul><h3 id="what-didnt-work">What Didn't Work</h3><p>The first cracks in this model emerged pretty early on. Developers were testing on their local Mac and Windows machines and then deploying code to Linux servers configured from Ansible playbooks and left running for months, sometimes years. Inevitably small differences in the running fleet of production servers emerged, either from package upgrades for security reasons or just from random configuration events. This could be mitigated by frequently rotating the running servers by destroying and rebuilding them as fresh VMs, but in practice this wasn't done as often as it should have been. </p><p>Soon you would see things like "it's running fine on box 1,2, 4, 5, but 3 seems to be having problems". It wasn't clear in the DevOps model <em>who</em> exactly was supposed to go figure out what was happening or how. In the previous design someone who worked with Linux for years and with these specific servers would be monitoring the release, but now those team members often wouldn't even know a deployment was happening. Telling someone who is amazing at writing great Javascript to go "find the problem with a Linux box" turned out to be easier said than done. </p><p>Quickly feedback from developers started to pile up. They didn't want to have to spend all this time figuring out what Debian package they wanted for this or that dependency. It wasn't what they were interested in doing and also they weren't being rewarded for that work, since they were almost exclusively being measured for promotions by the software they shipped. This left the Operations folks in charge of "smoothing out" this process, which in practice often meant really wasteful practices. </p><p>You'd see really strange workflows around this time of doubling the number of production servers you were paying for by the hour during a deployment and then slowly scaling them down, all relying on the same AMI (server image) to ensure some baseline level of consistency. However since any update to the AMI required a full dev-stage-prod check, things like security upgrades took <em>a very long time</em>. </p><p>Soon you had just a pile of issues that became difficult to assign. Who "owned" platform errors that didn't result in problems for users? When a build worked locally but failed inside of Jenkins, what team needed to check that? The idea of we're all working on the same team broke down when it came to assigning ownership of annoying issues because <em>someone</em> had to own them or they'd just sit there forever untouched. </p><h3 id="enter-containers">Enter Containers</h3><p>DevOps got a real shot in the arm with the popularization of containers, which allowed the movement to progress past its awkward teenage years. Not only did this (mostly) solve the "it worked on my machine" thing but it also allowed for a massive simplification of the Linux server component part. Now servers were effectively dumb boxes running containers, either on their own with Docker compose or as part of a fleet with Kubernetes/ECS/App Engine/Nomad/whatever new thing that has been invented in the last two weeks. </p><p>Combined with you could move almost everything that might previous be a networking team problem or a SAN problem to configuration inside of the cloud provider through tools like Terraform and you saw a real flattening of the skill curve. This greatly reduced the expertise required to operate these platforms and allowed for more automation. Soon you started to see what we now recognize as the current standard for development which is "I push out a bajillion changes a day to production". </p><h3 id="what-containers-didnt-fix">What Containers Didn't Fix</h3><p>So there's a lot of other shit in that DevOps model we haven't talked about. </p><figure><img src="https://matduggan.com/content/images/2024/06/New-DevOps-Loop-image.png" alt="" loading="lazy" width="2000" height="973" srcset="https://matduggan.com/content/images/size/w600/2024/06/New-DevOps-Loop-image.png 600w, https://matduggan.com/content/images/size/w1000/2024/06/New-DevOps-Loop-image.png 1000w, https://matduggan.com/content/images/size/w1600/2024/06/New-DevOps-Loop-image.png 1600w, https://matduggan.com/content/images/2024/06/New-DevOps-Loop-image.png 2240w" sizes="(min-width: 720px) 720px"></figure><p>So far teams had improved the "build, test and deploy" parts. However operating the crap was still very hard. Observability was <em>really really</em> hard and expensive. Discoverability was actually harder than ever because stuff was constantly changing beneath your feet and finally the Planning part immediately collapsed into the ocean because now teams could do whatever they wanted all the time. </p><p><strong>Operate</strong></p><p>This meant someone going through and doing all the boring stuff. Upgrading Kubernetes, upgrading the host operating system, making firewall rules, setting up service meshes, enforcing network policies, running the bastion host, configuring the SSH keys, etc. What organizations quickly discovered was that this stuff was very time consuming to do and often required <em>more</em> specialization than the roles they had previously gotten rid of. </p><p>Before you needed a DBA, a sysadmin, a network engineer and some general Operations folks. Now you needed someone who not only understood databases but understood <em>your specific cloud providers</em> version of that database. You still needed someone with the sysadmin skills, but in addition they needed to be experts in your cloud platform in order to ensure you weren't exposing your database to the internet. Networking was still critical but now it all existed at a level outside of your control, meaning weird issues would sometimes have to get explained as "well that sometimes happens". </p><p>Often teams would delay maintenance tasks out of a fear of breaking something like k8s or their hosted database, but that resulted in delaying the pain and making their lives more difficult. This was the era where every startup I interviewed with basically just wanted someone to update all the stuff in their stack "safely". Every system was well past EOL and nobody knew how to Jenga it all together. </p><p><strong>Observe</strong></p><p>As applications shipped more often, knowing they worked became more important so you could roll back if it blew up in your face. However replacing simple uptime checks with detailed traces, metrics and logs was hard. These technologies are specialized and require detailed understanding of what they do and how they work. A syslog centralized box lasts <em>to a point</em> and then it doesn't. Prometheus scales to x amount of metrics and then no longer works on a single box. You needed someone who had a detailed understanding of how metrics, logs and traces worked and how to work with development teams in getting them sending the correct signal to the right places at the right amount of fidelity. </p><p>Or you could pay a SaaS a <em>shocking amount</em> to do it for you. The rise of companies like Datadog and the eye-watering bills that followed was proof that they understood how important what they were providing was. You quickly saw Observability bills exceed CPU and networking costs for organizations as one team would misconfigure their application logs and suddenly you have blown through your monthly quota in a week. </p><p>Developers were being expected to monitor with detailed precision what was happening with their applications without a full understanding of what they were seeing. How many metrics and logs were being dropped on the floor or sampled away, how did the platform work in displaying these logs to them, how do you write an query for terabytes of logs so that you can surface what you need quickly, all of this was being passed around in Confluence pages being written by desperate developers who were learning as they were getting paged at 2AM how all this shit works together. </p><p><strong>Continuous Feedback</strong></p><p>This to me is the same problem as Observe. It's about whether your deployment worked or not and whether you had signal from internal tests if it was likely to work. It's also about feedback from the team on what in this process worked and what didn't, but because nobody ever did anything with that internal feedback we can just throw that one directly in the trash. </p><p>I guess in theory this would be retros where we all complain about the same six things every sprint and then continue with our lives. I'm not an Agile Karate Master so you'll need to talk to the experts. </p><p><strong>Discover</strong></p><p>A big pitch of combining these teams was the idea of more knowledge sharing. Development teams and Operation teams would be able to cross-share more about what things did and how they worked. Again it's an interesting idea and there was some improvement to discoverability, but in practice that isn't how the incentives were aligned. </p><p>Developers weren't rewarded for discovering more about how the platform operated and Operations didn't have any incentive to sit down and figure out how the frontend was built. It's not a lack of intellectual curiosity by either party, just the finite amount of time we all have before we die and what we get rewarded for doing. Being surprised that this didn't work is like being surprised a mouse didn't go down the tunnel with no cheese just for the experience. </p><p>In practice I "discovered" that if NPM was down nothing worked and the frontend team "discovered" that troubleshooting Kubernetes was a bit like Warhammer 40k Adeptus Mechanicus waving incense in front of machines they didn't understand in the hopes that it would make the problem go away. </p><figure><img src="https://warhammeruniverse.com/wp-content/uploads/2023/12/00007-2552221792-1024x569.png" alt="The Adeptus Mechanicus - Warhammer Universe (2024)" loading="lazy" width="1024" height="569"><figcaption><span>Try restarting the Holy Deployment</span></figcaption></figure><p><strong>Plan</strong></p><p>Maybe more than anything else, this lack of centralization impacted planning. Since teams weren't syncing on a regular basis anymore, things could continue in crazy directions unchecked. In theory PMs were syncing with each other to try and ensure there were railroad tracks in front of the train before it plowed into the ground at 100 MPH, but that was a lot to put on a small cadre of people. </p><p>We see this especially in large orgs with microservices where it is easier to write a new microservice to do something rather than figure out which existing microservice does the thing you are trying to do. This model was sustainable when money was free and cloud budgets were unlimited, but once that gravy train crashed into the mountain of "businesses need to be profitable and pay taxes" that stopped making sense. </p><h3 id="the-part-where-we-all-gave-up">The Part Where We All Gave Up</h3><p>A lot of orgs solved the problems above by simply throwing bodies into the mix. More developers meant it was possible for teams to have someone (anyone) learn more about the systems and how to fix them. Adding more levels of PMs and overall planning staff meant even with the frantic pace of change it was...more possible to keep an eye on what was happening. While cloud bills continued to go unbounded, for the most part these services worked and allowed people to do the things they wanted to do. </p><p>Then layoffs started and budget cuts. Suddenly it wasn't acceptable to spend unlimited money with your logging platform and your cloud provider as well as having a full team. Almost instantly I saw the shift as organizations started talking about "going back to basics". Among this was a hard turn in the narrative around Kubernetes where it went from an amazing technology that lets you grow to Google-scale to a weight around an organizations neck nobody understood. </p><p><strong>Platform Engineering</strong></p><p>Since there are no new ideas, just new terms, a successor to the throne has emerged. No longer are development teams expected to understand and troubleshoot the platforms that run their software, instead the idea is that the entire process is completely abstracted away from them. They provide the container and that is the end of the relationship. </p><p>From a certain perspective this makes more sense since it places the ownership for the operation of the platform with the people who should have owned it from the beginning. It also removes some of the ambiguity over what is whose problem. The development teams are still on-call for their specific application errors, but platform teams are allowed to enforce more global rules. </p><p>Well at least in theory. In practice this is another expansion of roles. You went from needing to be a Linux sysadmin to being a cloud-certified Linux sysadmin to being a Kubernetes-certified multicloud Linux sysadmin to finally being an application developer who can create a useful webUI for deploying applications on top of a multicloud stack that runs on Kubernetes in multiple regions with perfect uptime and observability that doesn't blow the budget. I guess at some point between learning the difference between AWS and GCP we were all supposed to go out and learn how to make useful websites. </p><figure><img src="https://cdn.prod.website-files.com/622b2fcc29fc56492b771cb8/637d1841fc5e9a6a942d5a02_1.png" alt="" loading="lazy" width="1600" height="683"></figure><p>This division of labor makes no sense but at least it's something I guess. Feels like somehow Developers got stuck with a lot more work and Operation teams now need to learn 600 technologies a week. Surprisingly tech executives didn't get any additional work with this system. I'm sure the next reorg they'll chip in more. </p><h3 id="conclusion">Conclusion</h3><p>We are now seeing a massive contraction of the Infrastructure space. Teams are increasingly looking for simple, less platform specific tooling. In my own personal circles it feels like a real return to basics, as small and medium organizations abandon technology like Kubernetes and adopt much more simple and easy-to-troubleshoot workflows like "a bash script that pulls a new container". </p><p>In some respects it's a positive change, as organizations stop pretending they needed a "global scale" and can focus on actually servicing the users and developers they have. In reality a lot of this technology was adopted by organizations who weren't ready for it and didn't have a great plan for how to use it. </p><p>However Platform Engineering is not a magical solution to the problem. It is instead another fabrication of an industry desperate to show monthly growth in cloud providers who know teams lack the expertise to create the kinds of tooling described by such practices. In reality organizations need to be more brutally honest about what they <em>actually need</em> vs what bullshit they've been led to believe they need. </p><p>My hope is that we keep the gains from the DevOps approach and focus on simplification and stability over rapid transformation in the Infrastructure space. I think we desperately need a return to basics ideology that encourages teams to stop designing with the expectation that endless growth is the only possible outcome of every product launch. </p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open source 'Eclipse Theia IDE' exits beta to challenge Visual Studio Code (166 pts)]]></title>
            <link>https://visualstudiomagazine.com/Articles/2024/06/27/eclipse-theia-ide.aspx</link>
            <guid>40825146</guid>
            <pubDate>Fri, 28 Jun 2024 20:49:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://visualstudiomagazine.com/Articles/2024/06/27/eclipse-theia-ide.aspx">https://visualstudiomagazine.com/Articles/2024/06/27/eclipse-theia-ide.aspx</a>, See on <a href="https://news.ycombinator.com/item?id=40825146">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="level0"> 
        
        <p id="ph_pcontent2_0_KickerText"><a href="https://visualstudiomagazine.com/Articles/List/News.aspx">News</a></p>
        
        <h3 id="ph_pcontent2_0_MainHeading">Open Source 'Eclipse Theia IDE' Exits Beta to Challenge Visual Studio Code</h3>
        
        
        

        <p>
  Some seven years in the making, the Eclipse Foundation's Theia IDE project is now generally available, emerging from beta to challenge Microsoft's similar Visual Studio Code editor, with which it shares much tech.
  
</p>



<p>
   The <a href="https://theia-ide.org/#theiaide" target="_blank">Eclipse Theia IDE</a>, part of the <a href="https://ecdtools.eclipse.org/" target="_blank">Eclipse Cloud DevTools ecosystem</a>, primarily differs from VS Code in licensing and governance. Open-source champion Eclipse Foundation calls it a "true open-source alternative" to VS Code, which Microsoft has <a href="https://code.visualstudio.com/docs/supporting/faq" target="_blank">described</a> as being "built" on open source but with proprietary elements like default telemetry with which usage data is collected.
 
</p>



<div><figure> <a href="https://visualstudiomagazine.com/Articles/2024/06/27/~/media/ECG/visualstudiomagazine/Images/2024/06/theia_desktop.ashx" target="_blank">
<img alt="Eclipse Theia IDE in on Windows" src="https://visualstudiomagazine.com/Articles/2024/06/27/~/media/ECG/visualstudiomagazine/Images/2024/06/theia_desktop_s.ashx" height="160" width="300"> </a>
<figcaption> <b>[Click on image for larger view.]</b> Eclipse Theia IDE in on Windows <em>(source: Screenshot).</em></figcaption>
</figure></div>




<p>
  Note that Eclipse Theia IDE is a separate component from the overall Theia project's related <a href="https://projects.eclipse.org/projects/ecd.theia" target="_blank">Eclipse Theia Platform</a>, used to build IDEs and tools based on modern web technologies.
  
</p>






<p>
  As far as the similarities with VS Code, Theia is built on the same <a href="https://microsoft.github.io/monaco-editor/" target="_blank">Monaco editor</a> that powers VS Code, and it supports the same Language Server Protocol (LSP) and Debug Adapter Protocol (DAP) that provide IntelliSense code completions, error checking and other features.
  
</p>



<p>
  Eclipse Theia IDE also supports the same extensions as VS Code (via the <a href="https://open-vsx.org/" target="_blank">Open VSX Registry</a> instead of Microsoft's Visual Studio Code Marketplace), which are typically written in TypeScript and JavaScript. There are many, many more extensions available for VS Code in Microsoft's marketplace, while "Extensions for VS Code Compatible Editors" in the Open VSX Registry number 3,784 at the time of this writing. 
</p>

    

<br>



<div><figure> <a href="https://visualstudiomagazine.com/Articles/2024/06/27/~/media/ECG/visualstudiomagazine/Images/2024/06/open_vsx.ashx" target="_blank">
<img alt="Open VSX Registry" src="https://visualstudiomagazine.com/Articles/2024/06/27/~/media/ECG/visualstudiomagazine/Images/2024/06/open_vsx_s.ashx" height="159" width="300"> </a>
<figcaption> <b>[Click on image for larger view.]</b> Open VSX Registry <em>(source: Open VSX Registry).</em></figcaption>
</figure></div>




<p>
Eclipse Foundation <a href="https://eclipsesource.com/blogs/2019/12/06/the-eclipse-theia-ide-vs-vs-code/" target="_blank">compared the two tools</a> in 2019, when it said to make a good decision between using VS Code or Eclipse Theia as a platform for a tool, an organization will need to evaluate custom project requirements, noting that as a general direction:
</p>




<ul>
<li>If you want to provide some tooling, which is focussed on code and want as many developers as possible to use it in their existing IDE, providing an extension for VS Code seems like a valid choice.
</li>
<li>If you want to provide a white-labeled product for customers or your own developers, which is tailored to a specific use case and possibly contains more features than code editing, you might be better served with Eclipse Theia.</li>
</ul>




<p>
  A somewhat more recent <a href="https://blogs.eclipse.org/post/mike-milinkovich/eclipse-theia-and-vs-code-differences-explained" target="_blank">post</a> from 2020 exploring the differences between the Eclipse Theia Platform (not IDE) and VS Code noted two primary ways in which the projects' architectures differ:
  
</p>



<ul>
<li>Eclipse Theia allows developers to create desktop and cloud IDEs using a single, open source technology stack. Microsoft now offers VS Online for cloud development environments, but like VS Code, it cannot be used in open source initiatives such as Gitpod.
</li>
<li>Eclipse Theia allows developers to customize every aspect of the IDE without forking or patching the code. This means they can easily use Theia as a base to develop desktop and cloud IDEs that are fully tailored for the needs of internal company projects or for commercial resale as a branded product. VS Code is a developer IDE only. It was never intended to be used as the base for other IDEs, extended, or further distributed.</li>
</ul>




<p>
  For developers just wanting to pick a tool to write apps with, an Eclipse Foundation blog <a href="https://eclipsesource.com/blogs/2024/06/27/introducing-the-theia-ide/" target="_blank">post</a> today said: "For developers in search of an IDE that combines flexibility, openness, and cutting-edge technology, the Theia IDE is a compelling choice. Distinctive features like an adaptable toolbar, detachable views, remote development support, and the forthcoming live collaboration mode set Theia apart from other open-source IDEs. Moreover, its commitment to privacy and its stance against incorporating telemetry by default reflect its respect for user preferences."
</p>






<p>
  Eclipse Foundation today emphasized another difference between its Theia IDE and VS Code: the surrounding ecosystem/community.
</p>




<div><figure> <a href="https://visualstudiomagazine.com/Articles/2024/06/27/~/media/ECG/visualstudiomagazine/Images/2024/06/theia_community.ashx" target="_blank">
<img alt="Eclipse Theia Community" src="https://visualstudiomagazine.com/Articles/2024/06/27/~/media/ECG/visualstudiomagazine/Images/2024/06/theia_community_s.ashx" height="220" width="300"> </a>
<figcaption> <b>[Click on image for larger view.]</b> Eclipse Theia Community <em>(source: Eclipse).</em></figcaption>
</figure></div>




<p>
  "At the core of Theia IDE is its vibrant open source community hosted by the Eclipse Foundation," the organization <a href="https://newsroom.eclipse.org/news/announcements/eclipse-foundation-introduces-theia-ide-elevate-modern-developer-experience" target="_blank">said</a> in a news release. "This ensures freedom for commercial use without proprietary constraints and fosters innovation and reliability through contributions from companies like Ericsson, EclipseSource, STMicroelectronics, TypeFox, and more. The community-driven model encourages participation and adaptation according to user needs and feedback."
</p>




<p>
  Indeed, the list of contributors to and adopters of the platform is extensive, also featuring Broadcom, Arm, IBM, Red Hat, SAP, Samsung, Google, Gitpod, Huawei and many others.
</p>




<p>
  "The Theia IDE's open-source foundation, supported by a vibrant community and underpinned by a license that champions commercial use, sets the stage for a development environment that is not only powerful and flexible but also inclusive and forward-looking," Eclipse Foundation concluded in its announcement today. "By choosing the Theia IDE, developers and organizations are not just adopting an IDE; they are joining a movement that values collaboration, freedom, and the collective pursuit of excellence in software development."
</p>
<br>
        
        
        
        
        
        
        
        <!-- pager start -->
        
        <!-- pager end -->
        
        
            
        

        
                <div>
                    <p id="ph_pcontent2_0_AuthorInfo_AboutAuthor">About the Author</p>
                    
                <p>
                    <strong></strong>
                    David Ramel is an editor and writer for Converge360.
                    <br>
                    
                    <a id="ph_pcontent2_0_AuthorInfo_AuthorEmail_0"></a>
                </p>
            
                </div>
            
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The story, as best I can remember, of the origin of Mosaic and Netscape [video] (259 pts)]]></title>
            <link>https://pmarca.substack.com/p/the-true-story-as-best-i-can-remember</link>
            <guid>40825033</guid>
            <pubDate>Fri, 28 Jun 2024 20:39:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pmarca.substack.com/p/the-true-story-as-best-i-can-remember">https://pmarca.substack.com/p/the-true-story-as-best-i-can-remember</a>, See on <a href="https://news.ycombinator.com/item?id=40825033">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><div data-testid="navbar"><p><a href="https://pmarca.substack.com/" native=""><img src="https://substackcdn.com/image/fetch/w_96,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg"></a></p><h2><a href="https://pmarca.substack.com/" native="">Marc Andreessen Substack</a></h2></div><div><div><article><div><div><div><div><div><div><div><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="white" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="m12 14 4-4"></path><path d="M3.34 19a10 10 0 1 1 17.32 0"></path></svg><p>Playback speed</p></div><div><p>×</p></div></div><div><div><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="white" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg><p>Share post</p></div></div><div><div><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="white" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg><p>Share post at current time</p></div></div></div><div><div><div><div id="trigger27149" aria-expanded="false" aria-haspopup="dialog" aria-controls="dialog27150" arialabel="View more"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="white" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg><p>Share from 0:00</p></div></div></div><div><div><p>0:00</p><p>/</p><p>0:00</p></div></div></div></div><div><div><p>Transcript</p></div></div></div></div><div><div><div><div><p>Enjoy!</p></div><div><div><a href="https://substack.com/profile/22353-marc-andreessen" target="_blank" rel="noopener"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_80,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg"><img src="https://substackcdn.com/image/fetch/w_80,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg" sizes="100vw" alt="" width="80"></picture></a></div><div><div><p><a href="https://substack.com/@pmarca">Marc Andreessen</a></p></div><div><p>Jun 28, 2024</p></div></div></div><div><span><p>Share</p></span><a role="button"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="var(--color-secondary-themed)" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="21" x2="3" y1="6" y2="6"></line><line x1="15" x2="3" y1="12" y2="12"></line><line x1="17" x2="3" y1="18" y2="18"></line></svg><p>Transcript</p></a></div></div></div><div><div><div><div><a href="https://pmarca.substack.com/" native=""><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_48,h_48,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg 48w, https://substackcdn.com/image/fetch/w_96,h_96,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg 96w, https://substackcdn.com/image/fetch/w_144,h_144,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg 144w" sizes="48px"><img src="https://substackcdn.com/image/fetch/w_48,h_48,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg" sizes="48px" alt="Marc Andreessen Substack" srcset="https://substackcdn.com/image/fetch/w_48,h_48,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg 48w, https://substackcdn.com/image/fetch/w_96,h_96,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg 96w, https://substackcdn.com/image/fetch/w_144,h_144,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg 144w" width="48" height="48"></picture></a></div><p>Marc Andreessen Substack</p></div><div data-component-name="SubscribeWidget"><form action="/api/v1/free?nojs=true" method="post" novalidate=""></form></div></div><div><div><p>Authors</p><div><div><a href="https://substack.com/profile/22353-marc-andreessen?utm_source=author-byline-face-podcast" target="_blank" rel="noopener"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_64,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg"><img src="https://substackcdn.com/image/fetch/w_64,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8ef02fe-d089-466f-9b4a-ea19df828473_400x400.jpeg" sizes="100vw" alt="" width="64"></picture></a></div><div><p>Marc Andreessen</p></div></div></div><div><p>Recent Posts</p><div><div><p><img type="image/gif" src="https://pmarca.substack.com/api/v1/video/upload/b33e4bc3-2428-4b56-8367-cd1d85a680cc/preview.gif?height=480"></p><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_150,h_150,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F143254349%2Fb33e4bc3-2428-4b56-8367-cd1d85a680cc%2Ftranscoded-00001.png"><img src="https://substackcdn.com/image/fetch/w_150,h_150,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F143254349%2Fb33e4bc3-2428-4b56-8367-cd1d85a680cc%2Ftranscoded-00001.png" sizes="(min-width:768px) 50vw, 100vw" alt="" width="150" height="150"></picture></div><div><div><p><a href="https://pmarca.substack.com/p/on-tech-politicspolicy-2-hour-video" data-testid="post-preview-title">On Tech Politics/Policy -- 2 hour video discussion</a></p></div><div><p><time datetime="2024-04-04T05:37:52.095Z">Apr 4</time>&nbsp;<span>•</span>&nbsp;<span><a href="https://substack.com/@pmarca">Marc Andreessen</a></span></p></div></div></div></div></div></div></div></div></article></div><div><p>Ready for more?</p><div><form action="/api/v1/free?nojs=true" method="post" novalidate=""></form></div></div></div><div><div><p>© 2024 Marc Andreessen</p><div><p><a href="https://substack.com/privacy" target="_blank" rel="noopener noreferrer">Privacy</a><span> ∙ </span><a href="https://substack.com/tos" target="_blank" rel="noopener noreferrer">Terms</a><span> ∙ </span><a href="https://substack.com/ccpa#personal-data-collected" target="_blank" rel="noopener noreferrer">Collection notice</a></p></div></div><div><a native="" href="https://substack.com/signup?utm_source=substack&amp;utm_medium=web&amp;utm_content=footer"><svg role="img" width="1000" height="1000" viewBox="0 0 1000 1000" fill="#FF6719" stroke-width="1.8" stroke="none" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M764.166 348.371H236.319V419.402H764.166V348.371Z"></path><path d="M236.319 483.752V813.999L500.231 666.512L764.19 813.999V483.752H236.319Z"></path><path d="M764.166 213H236.319V284.019H764.166V213Z"></path></g></svg> Start Writing</a><p><a native="" href="https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&amp;utm_content=web-footer-button">Get the app</a></p></div><p><a href="https://substack.com/" native="">Substack</a> is the home for great culture</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Supreme Court allows cities to ban homeless camps (111 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cj774nxrpy7o</link>
            <guid>40823850</guid>
            <pubDate>Fri, 28 Jun 2024 18:54:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cj774nxrpy7o">https://www.bbc.com/news/articles/cj774nxrpy7o</a>, See on <a href="https://news.ycombinator.com/item?id=40823850">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="byline" data-component="byline-block"><p><time>10 hours ago</time></p><div><p><span data-testid="byline-name">By&nbsp;<!-- -->Samantha Granville<!-- -->,&nbsp;<!-- --></span><span>BBC News, Los Angeles</span></p></div></div><div data-component="text-block"><p>The US Supreme Court has ruled in a 6-3 vote along ideological lines that cities can ban homeless people from sleeping rough.<!-- --></p><p>It is the court's most significant decision on homelessness since at least the 1980s, when many experts say the modern US homeless crisis began.<!-- --></p><p>The ruling says that local governments can enforce laws against people sleeping in public places without being in violation of the US constitution's limits on cruel and unusual punishment.<!-- --></p><p>The case started in the small city of Grants Pass, Oregon, where three homeless people sued after receiving citations for sleeping and camping outside.<!-- --></p><p>At a Supreme Court hearing in April, the city argued that criminal penalties were necessary to enforce local laws banning homeless people from public spaces for "reasons of cleanliness and safety".<!-- --></p><p>The homeless residents said those penalties violated the Eighth Amendment of the US Constitution because the city did not have any public shelters.<!-- --></p><p>Writing for the conservative majority in an opinion issued on Friday, Justice Neil Gorsuch wrote that the city's regulations on camping do not inflict “terror, pain or disgrace”.<!-- --></p><p>He added that the law does not criminalise the “mere status” of being homeless, and that the ban focuses more on the actions taken by individuals rather than their status alone.<!-- --></p><p>“Under the city’s laws, it makes no difference whether the charged defendant is homeless, a backpacker on vacation passing through town, or a student who abandons his dorm room to camp out in protest on the lawn of a municipal building,” Justice Gorsuch wrote.<!-- --></p><p>Justice Sonia Sotomayor, writing on behalf of the three dissenting liberal justices, wrote: “Sleep is a biological necessity, not a crime. Homelessness is a reality for so many Americans.”<!-- --></p><p>Several cities issued statements welcoming the ruling. San Francisco said it would help cities "manage our public spaces more effectively and efficiently," and the city of Grants Pass, the centre of the legal dispute, said that city leaders would meet with their lawyers to discuss next steps. <!-- --></p><p>Homelessness is on the rise in the US, fuelled in part by chronic shortages of affordable housing. Around 653,000 people did not have homes in 2023, the largest number since tracking began in 2007, according to US government figures. <!-- --></p><p>There were also an estimated 256,000 people living without shelter on a given night across the country last year, according to the <!-- --><a target="_blank" href="https://www.huduser.gov/portal/sites/default/files/pdf/2023-AHAR-Part-1.pdf">Department of Housing and Urban Development.<!-- --></a></p><p>Reacting to the ruling, the National Alliance to End Homelessness said it "sets a dangerous precedent that will cause undue harm to people experiencing homelessness and give free reign to local officials who prefer pointless and expensive arrests and imprisonment, rather than real solutions".<!-- --></p><p>Grants Pass's population has doubled to 40,000 in the last 20 years, but its supply of affordable or public housing has not.<!-- --></p><p>Soaring housing costs led to a sizeable number of people losing their homes.<!-- --></p><p>Town officials responded by passing laws that fined people for sleeping or camping in public. Over time, those fines stacked up, reaching thousands of dollars for some.<!-- --></p><p>Unable to pay for multiple citations, three homeless people sued the city.<!-- --></p><p>Their lawsuit reached the 9th Circuit Court of Appeals, which decided in 2022 that the restrictions in Grants Pass were so tight that they amounted to an effective ban on being homeless within city limits.<!-- --></p><p>The court had determined four years earlier in a similar case in Idaho that the constitution “bars a city from prosecuting people criminally for sleeping outside on public property when those people have no home or other shelter to go to".<!-- --></p></div><div data-component="text-block"><p>Meanwhile, the homeless crisis has continued to worsen.<!-- --></p><p>Jennifer Friedenbach, of the Coalition on Homelessness in San Francisco, said that money and resources should "go towards getting folks off the streets".<!-- --></p><p>“What we know is that arresting and fining people for being homeless doesn't work," she said. "It doesn't get anybody off the streets. It wastes municipal resources, and it exacerbates homelessness."<!-- --></p><p>The Supreme Court's Grants Pass decision will now allow cities to take more severe measures without the fear of legal recourse.<!-- --></p><p>The first problem with putting homeless people in jail is that it is extremely expensive, and when they get out, the person is still homeless and now even less apt to finding employment with a criminal record, says Elizabeth Funk, founder of DignityMoves, a nonprofit dedicated to ending unsheltered homelessness.<!-- --></p><p>“We need to be thinking about how to get this problem solved," she says. "It's not going to be fining people for doing something they can't avoid. It's helping them.”<!-- --></p><p>Some of the highest concentrations of homeless are on the West Coast. <!-- --></p><p>California, with its moderate temperatures, accounts for nearly half of all homeless people who live outside and has a total of 123,423 homeless, according to data from the US Department of Housing and Urban Development.<!-- --></p><p>Cities across the country have been wrestling with how to combat the growing crisis. <!-- --></p><p>The issue has been at the heart of recent election cycles in West Coast cities, including Los Angeles, where officials have poured record amounts of money into creating shelters and affordable housing while homelessness has still increased.<!-- --></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Llama-agents: an async-first framework for building production ready agents (103 pts)]]></title>
            <link>https://github.com/run-llama/llama-agents</link>
            <guid>40822512</guid>
            <pubDate>Fri, 28 Jun 2024 16:54:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/run-llama/llama-agents">https://github.com/run-llama/llama-agents</a>, See on <a href="https://news.ycombinator.com/item?id=40822512">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">🦙 <code>llama-agents</code> 🤖</h2><a id="user-content--llama-agents-" aria-label="Permalink: 🦙 llama-agents 🤖" href="#-llama-agents-"></a></p>
<p dir="auto"><code>llama-agents</code> is an async-first framework for building, iterating, and productionizing multi-agent systems, including multi-agent communication, distributed tool execution, human-in-the-loop, and more!</p>
<p dir="auto">In <code>llama-agents</code>, each agent is seen as a <code>service</code>, endlessly processing incoming tasks. Each agent pulls and publishes messages from a <code>message queue</code>.</p>
<p dir="auto">At the top of a <code>llama-agents</code> system is the <code>control plane</code>. The control plane keeps track of ongoing tasks, which services are in the network, and also decides which service should handle the next step of a task using an <code>orchestrator</code>.</p>
<p dir="auto">The overall system layout is pictured below.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/run-llama/llama-agents/blob/main/system_diagram.png"><img src="https://github.com/run-llama/llama-agents/raw/main/system_diagram.png" alt="A basic system in llama-agents"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><code>llama-agents</code> can be installed with pip, and relies mainly on <code>llama-index-core</code>:</p>

<p dir="auto">If you don't already have llama-index installed, to follow these examples, you'll also need</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install llama-index-agent-openai"><pre>pip install llama-index-agent-openai</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">The quickest way to get started is with an existing agent (or agents) and wrapping into launcher.</p>
<p dir="auto">The example below shows a trivial example with two agents from <code>llama-index</code>.</p>
<p dir="auto">First, lets setup some agents and initial components for our <code>llama-agents</code> system:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from llama_agents import (
    AgentService,
    AgentOrchestrator,
    ControlPlaneServer,
    SimpleMessageQueue,
)

from llama_index.core.agent import ReActAgent
from llama_index.core.tools import FunctionTool
from llama_index.llms.openai import OpenAI


# create an agent
def get_the_secret_fact() -> str:
    &quot;&quot;&quot;Returns the secret fact.&quot;&quot;&quot;
    return &quot;The secret fact is: A baby llama is called a 'Cria'.&quot;


tool = FunctionTool.from_defaults(fn=get_the_secret_fact)

agent1 = ReActAgent.from_tools([tool], llm=OpenAI())
agent2 = ReActAgent.from_tools([], llm=OpenAI())

# create our multi-agent framework components
message_queue = SimpleMessageQueue(port=8000)
control_plane = ControlPlaneServer(
    message_queue=message_queue,
    orchestrator=AgentOrchestrator(llm=OpenAI(model=&quot;gpt-4-turbo&quot;)),
    port=8001,
)
agent_server_1 = AgentService(
    agent=agent1,
    message_queue=message_queue,
    description=&quot;Useful for getting the secret fact.&quot;,
    service_name=&quot;secret_fact_agent&quot;,
    port=8002,
)
agent_server_2 = AgentService(
    agent=agent2,
    message_queue=message_queue,
    description=&quot;Useful for getting random dumb facts.&quot;,
    service_name=&quot;dumb_fact_agent&quot;,
    port=8003,
)"><pre><span>from</span> <span>llama_agents</span> <span>import</span> (
    <span>AgentService</span>,
    <span>AgentOrchestrator</span>,
    <span>ControlPlaneServer</span>,
    <span>SimpleMessageQueue</span>,
)

<span>from</span> <span>llama_index</span>.<span>core</span>.<span>agent</span> <span>import</span> <span>ReActAgent</span>
<span>from</span> <span>llama_index</span>.<span>core</span>.<span>tools</span> <span>import</span> <span>FunctionTool</span>
<span>from</span> <span>llama_index</span>.<span>llms</span>.<span>openai</span> <span>import</span> <span>OpenAI</span>


<span># create an agent</span>
<span>def</span> <span>get_the_secret_fact</span>() <span>-&gt;</span> <span>str</span>:
    <span>"""Returns the secret fact."""</span>
    <span>return</span> <span>"The secret fact is: A baby llama is called a 'Cria'."</span>


<span>tool</span> <span>=</span> <span>FunctionTool</span>.<span>from_defaults</span>(<span>fn</span><span>=</span><span>get_the_secret_fact</span>)

<span>agent1</span> <span>=</span> <span>ReActAgent</span>.<span>from_tools</span>([<span>tool</span>], <span>llm</span><span>=</span><span>OpenAI</span>())
<span>agent2</span> <span>=</span> <span>ReActAgent</span>.<span>from_tools</span>([], <span>llm</span><span>=</span><span>OpenAI</span>())

<span># create our multi-agent framework components</span>
<span>message_queue</span> <span>=</span> <span>SimpleMessageQueue</span>(<span>port</span><span>=</span><span>8000</span>)
<span>control_plane</span> <span>=</span> <span>ControlPlaneServer</span>(
    <span>message_queue</span><span>=</span><span>message_queue</span>,
    <span>orchestrator</span><span>=</span><span>AgentOrchestrator</span>(<span>llm</span><span>=</span><span>OpenAI</span>(<span>model</span><span>=</span><span>"gpt-4-turbo"</span>)),
    <span>port</span><span>=</span><span>8001</span>,
)
<span>agent_server_1</span> <span>=</span> <span>AgentService</span>(
    <span>agent</span><span>=</span><span>agent1</span>,
    <span>message_queue</span><span>=</span><span>message_queue</span>,
    <span>description</span><span>=</span><span>"Useful for getting the secret fact."</span>,
    <span>service_name</span><span>=</span><span>"secret_fact_agent"</span>,
    <span>port</span><span>=</span><span>8002</span>,
)
<span>agent_server_2</span> <span>=</span> <span>AgentService</span>(
    <span>agent</span><span>=</span><span>agent2</span>,
    <span>message_queue</span><span>=</span><span>message_queue</span>,
    <span>description</span><span>=</span><span>"Useful for getting random dumb facts."</span>,
    <span>service_name</span><span>=</span><span>"dumb_fact_agent"</span>,
    <span>port</span><span>=</span><span>8003</span>,
)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Local / Notebook Flow</h3><a id="user-content-local--notebook-flow" aria-label="Permalink: Local / Notebook Flow" href="#local--notebook-flow"></a></p>
<p dir="auto">Next, when working in a notebook or for faster iteration, we can launch our <code>llama-agents</code> system in a single-run setting, where one message is propagated through the network and returned.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from llama_agents import LocalLauncher

# launch it
launcher = LocalLauncher(
    [agent_server_1, agent_server_2],
    control_plane,
    message_queue,
)
result = launcher.launch_single(&quot;What is the secret fact?&quot;)

print(f&quot;Result: {result}&quot;)"><pre><span>from</span> <span>llama_agents</span> <span>import</span> <span>LocalLauncher</span>

<span># launch it</span>
<span>launcher</span> <span>=</span> <span>LocalLauncher</span>(
    [<span>agent_server_1</span>, <span>agent_server_2</span>],
    <span>control_plane</span>,
    <span>message_queue</span>,
)
<span>result</span> <span>=</span> <span>launcher</span>.<span>launch_single</span>(<span>"What is the secret fact?"</span>)

<span>print</span>(<span>f"Result: <span><span>{</span><span>result</span><span>}</span></span>"</span>)</pre></div>
<p dir="auto">As with any agentic system, its important to consider how reliable the LLM is that you are using. In general, APIs that support function calling (OpenAI, Anthropic, Mistral, etc.) are the most reliable.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Server Flow</h3><a id="user-content-server-flow" aria-label="Permalink: Server Flow" href="#server-flow"></a></p>
<p dir="auto">Once you are happy with your system, we can launch all our services as independent processes, allowing for higher throughput and scalability.</p>
<p dir="auto">By default, all task results are published to a specific "human" queue, so we also define a consumer to handle this result as it comes in. (In the future, this final queue will be configurable!)</p>
<p dir="auto">To test this, you can use the server launcher in a script:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from llama_agents import ServerLauncher, CallableMessageConsumer


# Additional human consumer
def handle_result(message) -> None:
    print(f&quot;Got result:&quot;, message.data)


human_consumer = CallableMessageConsumer(
    handler=handle_result, message_type=&quot;human&quot;
)

# Define Launcher
launcher = ServerLauncher(
    [agent_server_1, agent_server_2],
    control_plane,
    message_queue,
    additional_consumers=[human_consumer],
)

# Launch it!
launcher.launch_servers()"><pre><span>from</span> <span>llama_agents</span> <span>import</span> <span>ServerLauncher</span>, <span>CallableMessageConsumer</span>


<span># Additional human consumer</span>
<span>def</span> <span>handle_result</span>(<span>message</span>) <span>-&gt;</span> <span>None</span>:
    <span>print</span>(<span>f"Got result:"</span>, <span>message</span>.<span>data</span>)


<span>human_consumer</span> <span>=</span> <span>CallableMessageConsumer</span>(
    <span>handler</span><span>=</span><span>handle_result</span>, <span>message_type</span><span>=</span><span>"human"</span>
)

<span># Define Launcher</span>
<span>launcher</span> <span>=</span> <span>ServerLauncher</span>(
    [<span>agent_server_1</span>, <span>agent_server_2</span>],
    <span>control_plane</span>,
    <span>message_queue</span>,
    <span>additional_consumers</span><span>=</span>[<span>human_consumer</span>],
)

<span># Launch it!</span>
<span>launcher</span>.<span>launch_servers</span>()</pre></div>
<p dir="auto">Now, since everything is a server, you need API requests to interact with it. The easiest way is to use our client and the control plane URL:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from llama_agents import LlamaAgentsClient, AsyncLlamaAgentsClient

client = LlamaAgentsClient(&quot;<control plane URL>&quot;)  # i.e. http://127.0.0.1:8001
task_id = client.create_task(&quot;What is the secret fact?&quot;)
# <Wait a few seconds>
# returns TaskResult or None if not finished
result = client.get_task_result(task_id)"><pre><span>from</span> <span>llama_agents</span> <span>import</span> <span>LlamaAgentsClient</span>, <span>AsyncLlamaAgentsClient</span>

<span>client</span> <span>=</span> <span>LlamaAgentsClient</span>(<span>"&lt;control plane URL&gt;"</span>)  <span># i.e. http://127.0.0.1:8001</span>
<span>task_id</span> <span>=</span> <span>client</span>.<span>create_task</span>(<span>"What is the secret fact?"</span>)
<span># &lt;Wait a few seconds&gt;</span>
<span># returns TaskResult or None if not finished</span>
<span>result</span> <span>=</span> <span>client</span>.<span>get_task_result</span>(<span>task_id</span>)</pre></div>
<p dir="auto">Rather than using a client or raw <code>curl</code> requests, you can also use a built-in CLI tool to monitor and interact with your services.</p>
<p dir="auto">In another terminal, you can run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="llama-agents monitor --control-plane-url http://127.0.0.1:8000"><pre>llama-agents monitor --control-plane-url http://127.0.0.1:8000</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/run-llama/llama-agents/blob/main/llama_agents_monitor.png"><img src="https://github.com/run-llama/llama-agents/raw/main/llama_agents_monitor.png" alt="The llama-agents monitor app"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">You can find a host of examples in our examples folder:</p>
<ul dir="auto">
<li><a href="https://github.com/run-llama/llama-agents/blob/main/examples/agentic_rag_toolservice.ipynb">Agentic RAG + Tool Service</a></li>
<li><a href="https://github.com/run-llama/llama-agents/blob/main/examples/agentic_local_single.py">Agentic Orchestrator w/ Local Launcher</a></li>
<li><a href="https://github.com/run-llama/llama-agents/blob/main/examples/agentic_server.py">Agentic Orchestrator w/ Server Launcher</a></li>
<li><a href="https://github.com/run-llama/llama-agents/blob/main/examples/agentic_human_local_single.py">Agentic Orchestrator w/ Human in the Loop</a></li>
<li><a href="https://github.com/run-llama/llama-agents/blob/main/examples/agentic_toolservice_local_single.py">Agentic Orchestrator w/ Tool Service</a></li>
<li><a href="https://github.com/run-llama/llama-agents/blob/main/examples/pipeline_local_single.py">Pipeline Orchestrator w/ Local Launcher</a></li>
<li><a href="https://github.com/run-llama/llama-agents/blob/main/examples/pipeline_human_local_single.py">Pipeline Orchestrator w/ Human in the Loop</a></li>
<li><a href="https://github.com/run-llama/llama-agents/blob/main/examples/pipeline_agent_service_tool_local_single.py">Pipeline Orchestrator w/ Agent Server As Tool</a></li>
<li><a href="https://github.com/run-llama/llama-agents/blob/main/examples/query_rewrite_rag.ipynb">Pipeline Orchestrator w/ Query Rewrite RAG</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Components of a <code>llama-agents</code> System</h2><a id="user-content-components-of-a-llama-agents-system" aria-label="Permalink: Components of a llama-agents System" href="#components-of-a-llama-agents-system"></a></p>
<p dir="auto">In <code>llama-agents</code>, there are several key components that make up the overall system</p>
<ul dir="auto">
<li><code>message queue</code> -- the message queue acts as a queue for all services and the <code>control plane</code>. It has methods for publishing methods to named queues, and delegates messages to consumers.</li>
<li><code>control plane</code> -- the control plane is a the central gateway to the <code>llama-agents</code> system. It keeps track of current tasks, as well as the services that are registered to the system. It also holds the <code>orchestrator</code>.</li>
<li><code>orchestrator</code> -- The module handles incoming tasks and decides what service to send it to, as well as how to handle results from services. An orchestrator can be agentic (with an LLM making decisions), explicit (with a query pipeline defining a flow), a mix of both, or something completely custom.</li>
<li><code>services</code> -- Services are where the actual work happens. A services accepts some incoming task and context, processes it, and publishes a result
<ul dir="auto">
<li>A <code>tool service</code> is a special service used to off-load the compution of agent tools. Agents can instead be equipped with a meta-tool that calls the tool service.</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Low-Level API in <code>llama-agents</code></h2><a id="user-content-low-level-api-in-llama-agents" aria-label="Permalink: Low-Level API in llama-agents" href="#low-level-api-in-llama-agents"></a></p>
<p dir="auto">So far, you've seen how to define components and how to launch them. However in most production use-cases, you will need to launch services manually, as well as define your own consumers!</p>
<p dir="auto">So, here is a quick guide on exactly that!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Launching</h3><a id="user-content-launching" aria-label="Permalink: Launching" href="#launching"></a></p>
<p dir="auto">First, you will want to launch everything. This can be done in a single script, or you can launch things with multiple scripts per service, or on different machines, or even in docker images.</p>
<p dir="auto">In this example, we will assume launching from a single script.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio

# launch the message queue
queue_task = asyncio.create_task(message_queue.launch_server())

# wait for the message queue to be ready
await asyncio.sleep(1)

# launch the control plane
control_plane_task = asyncio.create_task(self.control_plane.launch_server())

# wait for the control plane to be ready
await asyncio.sleep(1)

# register the control plane as a consumer
await self.message_queue.client.register_consumer(
    self.control_plane.as_consumer(remote=True)
)

# register the services
control_plane_url = (
    f&quot;http://{self.control_plane.host}:{self.control_plane.port}&quot;
)
service_tasks = []
for service in self.services:
    # first launch the service
    service_tasks.append(asyncio.create_task(service.launch_server()))

    # register the service to the message queue
    await service.register_to_message_queue()

    # register the service to the control plane
    await service.register_to_control_plane(control_plane_url)"><pre><span>import</span> <span>asyncio</span>

<span># launch the message queue</span>
<span>queue_task</span> <span>=</span> <span>asyncio</span>.<span>create_task</span>(<span>message_queue</span>.<span>launch_server</span>())

<span># wait for the message queue to be ready</span>
<span>await</span> <span>asyncio</span>.<span>sleep</span>(<span>1</span>)

<span># launch the control plane</span>
<span>control_plane_task</span> <span>=</span> <span>asyncio</span>.<span>create_task</span>(<span>self</span>.<span>control_plane</span>.<span>launch_server</span>())

<span># wait for the control plane to be ready</span>
<span>await</span> <span>asyncio</span>.<span>sleep</span>(<span>1</span>)

<span># register the control plane as a consumer</span>
<span>await</span> <span>self</span>.<span>message_queue</span>.<span>client</span>.<span>register_consumer</span>(
    <span>self</span>.<span>control_plane</span>.<span>as_consumer</span>(<span>remote</span><span>=</span><span>True</span>)
)

<span># register the services</span>
<span>control_plane_url</span> <span>=</span> (
    <span>f"http://<span><span>{</span><span>self</span>.<span>control_plane</span>.<span>host</span><span>}</span></span>:<span><span>{</span><span>self</span>.<span>control_plane</span>.<span>port</span><span>}</span></span>"</span>
)
<span>service_tasks</span> <span>=</span> []
<span>for</span> <span>service</span> <span>in</span> <span>self</span>.<span>services</span>:
    <span># first launch the service</span>
    <span>service_tasks</span>.<span>append</span>(<span>asyncio</span>.<span>create_task</span>(<span>service</span>.<span>launch_server</span>()))

    <span># register the service to the message queue</span>
    <span>await</span> <span>service</span>.<span>register_to_message_queue</span>()

    <span># register the service to the control plane</span>
    <span>await</span> <span>service</span>.<span>register_to_control_plane</span>(<span>control_plane_url</span>)</pre></div>
<p dir="auto">With that done, you may want to define a consumer for the results of tasks.</p>
<p dir="auto">By default, the results of tasks get published to a <code>human</code> message queue.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from llama_agents import (
    CallableMessageConsumer,
    RemoteMessageConsumer,
    QueueMessage,
)


def handle_result(message: QueueMessage) -> None:
    print(message.data)


human_consumer = CallableMessageConsumer(
    handler=handle_result, message_type=&quot;human&quot;
)

message_queue.register_consumer(human_consumer)

# or, you can send the message to any URL
# human_consumer = RemoteMessageConsumer(url=&quot;some destination url&quot;)
# message_queue.register_consumer(human_consumer)"><pre><span>from</span> <span>llama_agents</span> <span>import</span> (
    <span>CallableMessageConsumer</span>,
    <span>RemoteMessageConsumer</span>,
    <span>QueueMessage</span>,
)


<span>def</span> <span>handle_result</span>(<span>message</span>: <span>QueueMessage</span>) <span>-&gt;</span> <span>None</span>:
    <span>print</span>(<span>message</span>.<span>data</span>)


<span>human_consumer</span> <span>=</span> <span>CallableMessageConsumer</span>(
    <span>handler</span><span>=</span><span>handle_result</span>, <span>message_type</span><span>=</span><span>"human"</span>
)

<span>message_queue</span>.<span>register_consumer</span>(<span>human_consumer</span>)

<span># or, you can send the message to any URL</span>
<span># human_consumer = RemoteMessageConsumer(url="some destination url")</span>
<span># message_queue.register_consumer(human_consumer)</span></pre></div>
<p dir="auto">Or, if you don't want to define a consumer, you can just use the <code>monitor</code> to observe your system results</p>
<div dir="auto" data-snippet-clipboard-copy-content="llama-agents monitor --control-plane-url http://127.0.0.1:8000"><pre>llama-agents monitor --control-plane-url http://127.0.0.1:8000</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft informs customers that Russian hackers spied on emails (107 pts)]]></title>
            <link>https://www.reuters.com/technology/cybersecurity/microsoft-tells-clients-russian-hackers-viewed-emails-bloomberg-news-reports-2024-06-27/</link>
            <guid>40821994</guid>
            <pubDate>Fri, 28 Jun 2024 16:01:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/cybersecurity/microsoft-tells-clients-russian-hackers-viewed-emails-bloomberg-news-reports-2024-06-27/">https://www.reuters.com/technology/cybersecurity/microsoft-tells-clients-russian-hackers-viewed-emails-bloomberg-news-reports-2024-06-27/</a>, See on <a href="https://news.ycombinator.com/item?id=40821994">Hacker News</a></p>
Couldn't get https://www.reuters.com/technology/cybersecurity/microsoft-tells-clients-russian-hackers-viewed-emails-bloomberg-news-reports-2024-06-27/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Supreme Court overrules Chevron deference [pdf] (111 pts)]]></title>
            <link>https://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf</link>
            <guid>40821007</guid>
            <pubDate>Fri, 28 Jun 2024 14:36:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf">https://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=40821007">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Supreme Court overturns 40-year-old "Chevron deference" doctrine (664 pts)]]></title>
            <link>https://www.axios.com/2024/06/28/supreme-court-chevron-doctrine-ruling</link>
            <guid>40820949</guid>
            <pubDate>Fri, 28 Jun 2024 14:31:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.axios.com/2024/06/28/supreme-court-chevron-doctrine-ruling">https://www.axios.com/2024/06/28/supreme-court-chevron-doctrine-ruling</a>, See on <a href="https://news.ycombinator.com/item?id=40820949">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-cy="au-image" data-chromatic="ignore"><img data-cy="StoryImage" alt="The U.S. Supreme Court in April 2024." fetchpriority="high" width="1920" height="1080" decoding="async" data-nimg="1" sizes="100vw" srcset="https://images.axios.com/0HWtMwcrXgCb_5lFNMsNTUU6Yy8=/0x900:8640x5760/320x180/2024/05/16/1715879188297.jpg?w=320 320w, https://images.axios.com/0HWtMwcrXgCb_5lFNMsNTUU6Yy8=/0x900:8640x5760/320x180/2024/05/16/1715879188297.jpg?w=320 320w, https://images.axios.com/7ZxL6bKpkNxWL6ubjcOmQOyctKo=/0x900:8640x5760/640x360/2024/05/16/1715879188297.jpg?w=640 640w, https://images.axios.com/7ZxL6bKpkNxWL6ubjcOmQOyctKo=/0x900:8640x5760/640x360/2024/05/16/1715879188297.jpg?w=640 640w, https://images.axios.com/lSDvbXXvNLIXeF2xPC0G-oouy-c=/0x900:8640x5760/768x432/2024/05/16/1715879188297.jpg?w=768 768w, https://images.axios.com/lSDvbXXvNLIXeF2xPC0G-oouy-c=/0x900:8640x5760/768x432/2024/05/16/1715879188297.jpg?w=768 768w, https://images.axios.com/hXC9UdkVp4dGz5KhHQvsDxPQfXw=/0x900:8640x5760/1024x576/2024/05/16/1715879188297.jpg?w=1024 1024w, https://images.axios.com/hXC9UdkVp4dGz5KhHQvsDxPQfXw=/0x900:8640x5760/1024x576/2024/05/16/1715879188297.jpg?w=1024 1024w, https://images.axios.com/Fqe49_mapPRZ-bwhEVixaLwVxSg=/0x900:8640x5760/1366x768/2024/05/16/1715879188297.jpg?w=1366 1366w, https://images.axios.com/Fqe49_mapPRZ-bwhEVixaLwVxSg=/0x900:8640x5760/1366x768/2024/05/16/1715879188297.jpg?w=1366 1366w, https://images.axios.com/99g12vnypLK_H6s5a3GwwpWuoss=/0x900:8640x5760/1600x900/2024/05/16/1715879188297.jpg?w=1600 1600w, https://images.axios.com/99g12vnypLK_H6s5a3GwwpWuoss=/0x900:8640x5760/1600x900/2024/05/16/1715879188297.jpg?w=1600 1600w, https://images.axios.com/9RMqkLL-qeuwtkEPKrMjkj_KObk=/0x900:8640x5760/1920x1080/2024/05/16/1715879188297.jpg?w=1920 1920w, https://images.axios.com/9RMqkLL-qeuwtkEPKrMjkj_KObk=/0x900:8640x5760/1920x1080/2024/05/16/1715879188297.jpg?w=1920 1920w" src="https://images.axios.com/9RMqkLL-qeuwtkEPKrMjkj_KObk=/0x900:8640x5760/1920x1080/2024/05/16/1715879188297.jpg?w=1920"><figcaption><p>The U.S. Supreme Court in April 2024. Photo; Bill Clark/CQ-Roll Call, Inc via Getty Images</p></figcaption></div><div data-chromatic="ignore"><p><span data-schema="smart-brevity"><p>The <a data-vars-link-text="Supreme Court" data-vars-click-url="https://www.axios.com/2024/06/20/supreme-court-rulings-trump-abortion" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/06/20/supreme-court-rulings-trump-abortion" target="_self">Supreme Court</a> on Friday curtailed the executive branch's ability to interpret laws it's charged with implementing, giving the judiciary more say in what federal agencies can do.</p><p><strong>Why it matters:</strong> The<strong> </strong><a data-vars-link-text="landmark 6-3 ruling" data-vars-click-url="https://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf" target="_blank">landmark 6-3 ruling</a> along ideological lines overturns<strong> </strong>the court's 40-year-old "Chevron deference" doctrine. It could make it harder for executive agencies to tackle<strong> </strong>a wide array of policy areas, including <a data-vars-link-text="environmental" data-vars-click-url="https://www.axios.com/2024/01/17/supreme-court-chevron-environment" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/01/17/supreme-court-chevron-environment" target="_self">environmental</a> and <a data-vars-link-text="health" data-vars-click-url="https://www.axios.com/2024/01/17/supreme-court-chevron-deference-medicare-medicaid" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/01/17/supreme-court-chevron-deference-medicare-medicaid" target="_self">health</a> regulations and <a data-vars-link-text="labor and employment laws" data-vars-click-url="https://www.bloomberglaw.com/external/document/X1N1TE8K000000/labor-relations-professional-perspective-apres-moi-le-deluge-big" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.bloomberglaw.com/external/document/X1N1TE8K000000/labor-relations-professional-perspective-apres-moi-le-deluge-big" target="_blank">labor and employment laws</a>.</p></span></p><p><strong>Driving the news:</strong> Chief Justice John Roberts, writing the opinion of the court, argued Chevron "defies the command of" the Administrative Procedure Act, which governs federal administrative agencies.</p><ul><li>He said it "requires a court to ignore, not follow, 'the reading the court would have reached had it exercised its independent judgment as required by the APA.'"</li><li>Further, he said it "is misguided" because "agencies have no special competence in resolving statutory ambiguities. Courts do."</li></ul><p><strong>Roberts noted </strong>the court's decision did not call into question prior cases that relied on Chevron, including holdings pertaining to the Clean Air Act, because they "are still subject to statutory stare decisis despite our change in interpretive methodology."</p><ul><li>"Mere reliance on Chevron cannot constitute a 'special justification' for overruling such a holding," he said.</li></ul><p><strong>Justice Elena Kagan,</strong> in a dissenting opinion, wrote that the ruling Friday was "yet another example of the Court's resolve to roll back agency authority, despite congressional direction to the contrary."</p><ul><li>"Congress knows that it does not — in fact cannot — write perfectly complete regulatory statutes," she wrote. "It knows that those statutes will inevitably contain ambiguities that some other actor will have to resolve, and gaps that some other actor will have to fill. And it would usually prefer that actor to be the responsible agency, not a court.<strong>"</strong></li><li>She warned the decision "is likely to produce large-scale disruption." </li><li>"In one fell swoop, the majority today gives itself exclusive power over every open issue — no matter how expertise-driven or policy-laden — involving the meaning of regulatory law."</li><li>"The majority disdains restraint, and grasps for power."</li></ul><p><strong>Context: </strong>The ruling <a data-vars-link-text="marks another major victory" data-vars-click-url="https://www.axios.com/2023/05/25/supreme-court-epa-wetlands-clean-water-act" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2023/05/25/supreme-court-epa-wetlands-clean-water-act" target="_self">marks another major victory</a> for conservatives, who for decades have sought to limit the federal government's ability to regulate businesses.</p><ul><li>In the wake of the court's ruling, it's expected that more federal rules will be challenged in the courts and judges will have greater discretion to invalidate agency actions.</li><li>The decision comes one day after the Supreme Court <a data-vars-link-text="curtailed federal agencies' use of administrative law judges" data-vars-click-url="https://www.axios.com/2024/06/27/scotus-sec-jarkesy-decision" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/06/27/scotus-sec-jarkesy-decision" target="_self">curtailed federal agencies' use of administrative law judges</a> in another blow to the administrative state.</li></ul><p><strong>How it works:</strong> <a data-vars-link-text="The doctrine" data-vars-click-url="https://sgp.fas.org/crs/misc/R44954.pdf" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://sgp.fas.org/crs/misc/R44954.pdf" target="_blank">The doctrine</a> was created by the Reagan-era Supreme Court in Chevron U.S.A. v. Natural Resources Defense Council in 1984 and has since become <a data-vars-link-text="the most cited" data-vars-click-url="https://www.yalejreg.com/nc/most-cited-supreme-court-administrative-law-decisions-by-chris-walker/" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.yalejreg.com/nc/most-cited-supreme-court-administrative-law-decisions-by-chris-walker/" target="_blank">the most cited</a> Supreme Court decision in administrative law.</p><ul><li>Under Chevron deference, courts would defer to how to expert federal agencies interpret the laws they are charged with implementing provided their reading is reasonable — even if it's not the only way the law can be interpreted.</li><li>It allowed Congress to rely on the expertise within the federal government when implementing everything from health and safety regulations to environmental and financial laws.</li></ul><p><strong>Zoom in: </strong>However, Chevron was challenged in two separate cases over a National Marine Fisheries Service regulation meant to prevent overfishing on commercial fishing vessels.</p><ul><li>Fishing companies challenging the regulation claimed the doctrine violated Article III of the Constitution by shifting the authority to interpret federal law from the courts to the executive branch.</li><li> They also claimed it violated Article I by allowing agencies to formulate policy when only Congress should have lawmaking power.</li></ul><p><strong>The other side:</strong> The government argued that the doctrine had  safeguards within it that prevented agencies from usurping Congress's lawmaking authority.</p><ul><li>It noted that Chevron only applied to ambiguous text in laws passed by Congress and instances in which lawmakers had given interpretive authority to an agency.</li><li>The doctrine was also necessary to limit federal judges' abilities to make public policy when they may not have the expertise to do so and aren't subject to democratic accountability, the government said.</li></ul><p><strong>Between the lines: </strong>Lawyers who worked pro bono to represent fishing companies involved in the cases are also staff attorneys for Americans for Prosperity, a libertarian political advocacy group funded by billionaire Charles Koch, the <a data-vars-link-text="New York Times" data-vars-click-url="https://www.nytimes.com/2024/01/16/climate/koch-chevron-deference-supreme-court.html" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.nytimes.com/2024/01/16/climate/koch-chevron-deference-supreme-court.html" target="_blank">New York Times</a> reported earlier this year.</p><ul><li>The political network associated with Charles Koch and his late brother, David Koch, have long championed efforts to get cases before the Supreme Court that, if decided in their favor, would roll back the federal government's regulatory powers.</li><li>The Koch network also successfully attracted Supreme Court Justice Clarence Thomas, who voted against the doctrine, to speak at at least one of its <a data-vars-link-text="donor events" data-vars-click-url="https://www.axios.com/2023/09/22/clarence-thomas-koch-brothers-donor-events" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2023/09/22/clarence-thomas-koch-brothers-donor-events" target="_self">donor events</a> in 2018, <a data-vars-link-text="ProPublica" data-vars-click-url="https://www.propublica.org/article/clarence-thomas-secretly-attended-koch-brothers-donor-events-scotus" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.propublica.org/article/clarence-thomas-secretly-attended-koch-brothers-donor-events-scotus" target="_blank">ProPublica</a> reported last year.</li><li>It's unclear who purchased Thomas' flight to the 2018 event, as he never  reported it in his annual financial disclosure form. Thomas has attended at least two of such events in past years.</li></ul><p><strong>The big picture: </strong>In recent years, Chevron had fallen out of favor of the <a data-vars-link-text="conservative-majority Supreme Court" data-vars-click-url="https://crsreports.congress.gov/product/pdf/LSB/LSB11061" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://crsreports.congress.gov/product/pdf/LSB/LSB11061" target="_blank">conservative-majority Supreme Court</a>, which had declined to apply it or cite it in cases which it may once have applied.</p><ul><li>The ruling comes as some federal judges have taken a more active role in overruling agency expertise.</li><li>For example, Texas District Judge Matthew Kacsmaryk last April <a data-vars-link-text="paused" data-vars-click-url="https://www.axios.com/2024/03/23/abortion-pill-access-supreme-court-mifepristone-fda" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/03/23/abortion-pill-access-supreme-court-mifepristone-fda" target="_self">paused</a> the FDA's original 23-year-old approval of the abortion pill mifepristone in a case that's now to be decided by the Supreme Court.</li></ul><p><strong>Go deeper: </strong><a data-vars-link-text="Supreme Court brushes off payday lenders' challenge to consumer watchdog's funding" data-vars-click-url="https://www.axios.com/2024/05/16/supreme-court-cfpb-funding" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/05/16/supreme-court-cfpb-funding" target="_self">Supreme Court brushes off payday lenders' challenge to consumer watchdog's funding</a></p><p><em>Editor's note: This story was updated with details from the court's ruling.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple II graphics: More than you wanted to know (156 pts)]]></title>
            <link>https://nicole.express/2024/phasing-in-and-out-of-existence.html</link>
            <guid>40820311</guid>
            <pubDate>Fri, 28 Jun 2024 13:14:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nicole.express/2024/phasing-in-and-out-of-existence.html">https://nicole.express/2024/phasing-in-and-out-of-existence.html</a>, See on <a href="https://news.ycombinator.com/item?id=40820311">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>The Apple ][ is one of the most iconic vintage computers of all time. But since Wozniak’s monster lasted all the way until 1993 (1995 if you could the IIe card, which I won’t count until I get one), it can be easy to forget that in 1977, it was a video <em>extravaganza</em>. The competitors– even much bigger and established companies like Commodore and Tandy– generally only had text modes, let alone pixel-addressable graphics, and they certainly didn’t have sixteen colors. (Gray and grey are different colors, right?)</p>

<h2 id="preliminary">Preliminary</h2>

<p>My main source here is going to be <em>Understanding the Apple II</em> by Jim Sather. You might say, why should I read this post then, when I can go to the source? And honestly <a href="https://archive.org/details/utaii">yeah go do that</a>. What I’ll do here is try to digest it for myself by writing it in a form I find understandable, focused on details I find interesting. I’ll also throw together some looks at my own personal Apple II, maybe an oscilloscope, that sort of thing.</p>

<p><img src="https://nicole.express/assets/a2p-2.JPG" title="This photo is from one of the earliest posts on this blog" alt="The Apple II, plugged into a greenscreen monitor"></p>

<p>Now, my personal Apple ][<sub><i>plus</i></sub> and the book have something in common: they predate the Apple IIe. So this blog post will focus on the original Apple ][ designed machines. So when I talk about graphics mode, you won’t see the 80 column or double-width graphics modes. Those were IIe features; there were no provisions for such things on the original models. If this post proves interesting maybe I’ll dig into them later; I have an Apple IIgs, so certainly I <em>can</em> explore IIe exclusives. (And Jim Sather even wrote a follow-up book, <em>Understanding the Apple IIe</em>)</p>



<p><img src="https://nicole.express/assets/img/2apple2furious/lang.jpeg" title="taking out the language card is a pain so I left it in" alt="Apple II motherboard"></p>

<p>Here is the motherboard of my Apple II plus. It’s serial number 820-0044-01, which despite the 1979 copyright date, implies it’s definitely one of the later Apple IIs of its type– in 1981, the 820-0044-XX motherboard series was created by Apple in order to try to reduce radio-frequency interference (RFI), so this is known as the “RFI” motherboard. Go dig into <a href="https://archive.org/details/apple-ii-circuit-description/page/n157/mode/2up"><em>The Apple II Circuit Description</em></a> for all the nitty-gritty on motherboard variants.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/serial.jpeg" title="I want a Rev 0 so bad" alt="Apple II motherboard serial number"></p>

<p>It’s worth noting for those not familiar with the Apple II that the “Apple II” and the “Apple II plus” are the same system, whose major difference is just the ROM. After the introduction of the new ROMs in 1979, there was a period where the same motherboard, when sold with the original Integer BASIC ROM set, it was an Apple II; when Applesoft (Microsoft BASIC for the Apple) was baked in instead, the badge was changed to II plus. Eventually all Apples shipped with Applesoft and the original II badges stopped being used. The internal capabilities are identical, including all the graphics modes I’ll talk about.</p>

<h2 id="everything-but-the-kitchen-sync">Everything but the kitchen sync</h2>

<p>Television video systems predate computers wanting to use them. Therefore, they are greedy– a video signal must produce the expected signals at the expected times, or your television will lose synchronization with the signal. Regaining synchronization will likely result in a delay, and definitely a loss of visual signals.</p>

<p>So when the video signal is being drawn, everything else has to bow to the video system’s will. The <a href="https://nicole.express/2022/the-nes-as-an-artifact.html">Nintendo Entertainment System</a> creates a separate world, the PPU bus, for the video system to inhabit, and the developer should avoid touching it unless it’s convenient. Other machine did things differently– the Atari 130XE I recently <a href="https://nicole.express/2024/have-you-typed-atari-today.html">upgraded the keyboard</a> on uses a variant of the 6502 processor with an extra pin whose sole purpose is to halt the CPU whenever the video chip needs extra time to access RAM. Both the 130XE and the NES have a 1.7MHz CPU, but the NES can run its just a little faster. (The 130XE has literally 64 times the CPU-accessible RAM, it’ll be fine)</p>



<p>The Apple II is a little bit different than that. The CPU can access RAM whenever it wants. The graphics system, known as the video scanner (it’s not one chip), can also access the RAM it needs whenever it wants. How did Woz do this? A clue is hidden in the <a href="http://www.6502.org/documents/datasheets/synertek/">6502 datasheets</a>– specifically, this diagram is from the August 1976 “SY6500/MCS6500 Microcomputer Family Hardware Manual”.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/book.png" title="This diagram is vintage" alt="Timing diagram for the 6502, showing that the address and data only both need be valid when the clock is down"></p>

<p>The important thing to note is to look at φ<sub>2</sub>, the input clock. The data output of external memory only needs to be valid at the very end of the period in which φ<sub>2</sub> is low. The <em>entire</em> period when φ<sub>2</sub> is high, the 6502 is doing internal stuff, and you don’t actually want its signals to show up on the data bus. So if you have sufficiently fast memory, you can have your memory off doing something else while φ<sub>2</sub> is low, and the 6502 will never know the difference.</p>



<p>Now, my understanding is that the Commodore VIC-20 interlaces its memory accesses the same way. But there are three major differences between the Apple II and the Commodore VIC-20. Well, okay, there’s more than that. But there are a few particularly relevant ones:</p>

<ol>
  <li>The VIC-20 uses static RAM. The Apple II’s video scanner also handles DRAM refresh, while the Commodore doesn’t need to worry about that.</li>
  <li>The VIC-20 has no directly-accessible-pixel screen modes.</li>
  <li>The VIC-20’s Video Interface Chip is, well, a chip. A highly integrated circuit that is opaque to exterior analysis, and with room for extra logic to simplify the external interface. The Apple II video scanner is constructed out of easily-analyzable discrete logic, and wears its implementation details on its sleeve.</li>
</ol>

<p>It’s that last one that I think is really important here. A lot of the fiddly details of the Apple II’s video that a programmer has to put up with could have been papered over with a few extra logic gates, internal registers, and buffers. On an integrated circuit this wouldn’t be a big deal as long as everything still fit within the planned mask size. But Steve Wozniak was building the Apple II out of discrete logic, and Apple paid for each one of those chips. So it was in the interest of cost-effectiveness that Apple offloaded some complexity to the programmer.</p>

<h3 id="what-frequency-is-it-anyways">What frequency is it anyways?</h3>

<p>Let’s talk about pixels. The Apple II has a core oscillator at 14.318180MHz (“14M”), which is divided by two to create a 7.15909MHz (“7M”) signal, and then divided again to create a 3.579545MHz signal. This latter suspiciously-specific frequency is the NTSC “colorburst” frequency. 7M is our pixel clock; during active display, a pixel is output every (1 / 7.15909MHz). A division of 7M by 7 gives you 1.0227MHz, which sounds like the 1MHz CPU clock. <em>But is it?</em></p>

<p>The horizontal scanning rate of NTSC television is 15.734kHz. PAL is 15.625kHz, but we’ll ignore that which challenges us. That means we have 63.56μs to finish a line, or a quick trip to <a href="https://www.wolframalpha.com/input?i=%281+%2F+15.734kHz%29+%2F+%281+%2F+14.318180MHz%29">Wolfram Alpha</a> says that’s 910 clicks of our 14M clock. 65 clicks of 1.0227MHz, 455 pixels (including in blanking periods; 280 pixels the screen actually draws), and therefore 227.5 ticks of our color reference. Which isn’t evenly divided.</p>

<p>That’s actually correct and how the spec expects it, however, we need to keep all the accesses perfectly synchronized, and we want the color reference to also be constant relative to the CPU. (Ever wonder why systems like PC clones don’t always have consistent artifact colors?) So the Apple II lengthens that last 65th clock– it’s the “long cycle”, taking an extra tick. Now the scanline frequency is dropped to 15.700kHz (fine for most TVs), but also now the Apple II CPU clock is <strong>not constant</strong>, it varies based off of where the screen is drawing. It’s 1.0205MHz on average, but only on average.</p>

<p>Combine that with the knowledge of the Apple II’s <a href="https://nicole.express/2021/stop-mocking-me.html">audio system</a> and the stock Apple II’s lack of any hardware timers, and this is actually worth knowing. Unfortunately, the Apple II doesn’t give the programmer any ability to know where in its cycle the video scanner is at any given time. (<i>Understanding the Apple II</i> has a few possible mods you can do to your computer to let it know, though!)</p>

<h2 id="text-mode">TEXT mode</h2>

<p>Many vintage computers are defined by their fonts. The PET’s PETSCII is iconic, of course (though probably moreso for its use on the Commodore 64). The TRS-80 had its “pseudographics” characters allowing for very blocky pixel graphics despite only having text mode. And I’ve always enjoyed the thick letters of the Atari 8-bit font, which not only look nice, but also help readability on a system whose text mode is single-color (varying only in luminance) and would be viewed by most users over a noisy RF modulator.</p>

<p><img src="https://nicole.express/assets/img/dasatari/name-intro.jpg" title="Please exclusively refer to me as (5-note tune) from now on" alt="The point in the software described above"></p>

<p>And then we get to the Apple II, with its stark simple character forms, and absolutely no special characters to speak of. (Though at least it has built in inverse and flashing text modes, always useful?) One can easily imagine Steve Jobs in the mindset that would lead to the lack of arrow keys on the first Macintosh keyboard, insisting that since the Apple II’s standout feature was its graphics modes, there would be no special incentives to create pseudographics with text mode.</p>

<p><video src="https://nicole.express/assets/img/2apple2furious/txt.mp4" width="640px" autoplay="" loop="" muted=""> You don't have a video tag support or something? So you can't see this footage of The Apple II, showing its narrow font? Ah too bad.</video></p>

<p>You could think that, but you’d be wrong. Early revisions of the Apple II used the Signetics 2513 character generator ROM. This was a commercial, off the shelf part. You can go find its datasheet online.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/sign.png" title="Understanding the Apple II says it was a GI version but everything was second and third sourced those days" alt="Signetics logo advertising a 2513 HIGH SPEED 64x7x5 CHARACTER GENERATOR"></p>

<p>This was a popular part that Woz had used earlier in the Apple 1, and was a popular use for hobbyist projects like the famous 1973 <a href="https://en.wikipedia.org/wiki/TV_Typewriter">TV Typewriter</a>. So the message the Apple II font actually sends is “hey tinkerers, this is for you”. Now later models of the Apple II, like my II plus, use a more standard mask ROM instead of this weird 5-bit character-specific ROM; you can even mod it to put your own EEPROM in. On my RFI board, it’s “ROM SPCL” deep under the keyboard.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/spcl.jpeg" title="a special rom for a special computer <3" alt="ROM SPCL is underneath the keyboard PCB. It is only visible by the edge of its socket"></p>

<p>Despite the Signetics ROM being only 5 pixels wide, text characters on the Apple II are 7 pixels wide. But why 7? Well, the reason is all that screen math again. The memory access clock is 7M divided by 7 to get the memory timing; so you have one memory cycle to get 7 pixels. We have 40 columns in the visible area, and 24 rows, fine for low resolution text mode. Pretty basic, right?</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/steve.jpg" title="I swear this won't become a habit but I couldn't resist" alt="Steve Jobs saying 'One More Thing'"></p>

<h3 id="memory-layout">Memory layout</h3>

<p>The official Apple <em>Apple II Reference Manual</em>, signed by Woz on the cover, provides some detail on the memory layout that starts to be a bit concerning.</p>

<blockquote>
  <p>The area of memory which is used for the primary text page starts at location number 1024 and extends to location number 2047. The secondary screen begins at location number 2048 and extends to location 3071. In machine language, the primary page is from hexadecimal address $400 to address $7FF; the secondary page is from $800 to $BFF. Each of these pages is 1,024 bytes long. Those of you intrepid enough to do the multiplication will realize that there are only 960 characters displayed on the screen. The remaining 64 bytes in each page which are not displayed on the screen are used as temporary storage locations by programs stored in PROM on Apple Intelligent Interface (r) peripheral boards (see page 82).</p>
</blockquote>

<p>You might wonder why they’re giving memory addresses in decimal– well, that was pretty normal for 70’s and 80’s computer manuals. You might also wonder why they’re so desperate for RAM that such a small amount of extra RAM would be in demand for peripheral cards– well, the original Apple II was sold with a base RAM configuration of 4kiB, so no addresses above <code>0x1000</code> would exist.</p>

<p>But the real question is how that memory is laid out. The Reference Manual just gives a screen map, but it doesn’t tell you <em>why</em> it is the way it is. My scan of this isn’t great, but you can see that the rows are very much not sequential.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/text.png" title="and you thought microsoft was bad at counting to ten" alt="Screen layout diagram"></p>

<p>I’ve called the Apple II screen memory layout bizarre before, and from a programmer’s perspective, it really is. But I was also criticized for that– because Steve Wozniak is really doing something quite impressive here. You have to understand the constraints he was under.</p>

<ol>
  <li><strong>Use as few chips as possible</strong>. Each piece of discrete logic costs money. So wherever possible work with the signals you have– the binary counters that are used for all that counting logic, for example.</li>
  <li><strong>But don’t waste RAM</strong>. The constraints of the television standard give us 40 columns wide. That’s not a simple binary number; you could have a gap after every text line, but that’d give 24 small areas of wasted RAM.</li>
  <li><strong>Refresh DRAM</strong>. DRAM addressing is pretty complicated, relying on “row” and “column” signals. But long story short, when going through the screen to display video, you also need to access every “row” address every 2ms. A 60Hz frame is 16.67ms. (Note that each chip is either 4kiB or 16kiB, so if you refresh the right range in one chip you can simultaneously refresh the rest)</li>
</ol>

<p>I’m not going to go into the full detail of the design because I think I’d just be repeating Jim Sather’s book in full here. But more-or-less, the screen is divided into three areas: top, middle, and bottom, each of eight rows. Then, the memory page, let’s use the primary text page <code>0x400</code>, is divided into eight subsections of 128 bytes each– 128 bytes gives us something our binary counters can easily catch. Each of these 128 byte sections is as follows:</p>

<ul>
  <li>One row of 40 characters for the top area</li>
  <li>One row of 40 characters for the middle area</li>
  <li>One row of 40 characters for the bottom area</li>
  <li>One 8 byte “screen hole” given to the Apple Intelligent Interface (r) peripheral boards</li>
</ul>

<p>In order to refresh a larger part of the screen during TEXT and LORES modes, the video scanner actually accesses different addresses during the horizontal blanking period, which allows it to refresh a wider range. These are wrong for video, but there’s no video during the blanking period. It doesn’t need to do this in HIRES mode, so it doesn’t.</p>



<p>From the programmer’s perspective, this usually just is papered over with a lookup table, and isn’t a big deal in the end.</p>

<h2 id="hires-mode">HIRES mode</h2>

<p>The Apple II offers no ability to customize the blocks in text mode; that Signetics ROM could not be replaced with RAM. This was the case for all three machines of the 1977 “Trinity”, but later Commodore machines would allow it. Unlike the other two “Trinity” machines, though, Apple lets you address the screen pixels directly.</p>

<p>With 40 bytes per row, and 24 * 8 = 192 rows in the visible area, you’d need at least 7.5kiB for just one screen. So we’ve abandoned the 4kiB Apple II users here– the 4kiB Apple II was not on the market very long though, and the upgraded 16kiB was the low-end model for most of the late 70’s. By the time of later models like my plus, 48kiB was more or less assumed anyways. With 16kiB you get one HIRES page at <code>0x2000</code>, with 48kiB you can get a second one at <code>0x4000</code>. (Double-buffering in 1977?) Of course, that’s a lot of RAM, so if you don’t need it a program will probably use it for something else.</p>

<p>Now, there’s an interesting problem that HIRES memory has to handle– the addresses and layout for text mode are very carefully chosen and set up to allow DRAM refresh. But now we need to get eight times as many addresses, with as few changes as possible. How do we do it? HIRES mode uses <em>higher</em> address bits, which are mapped to the DRAM “columns”, mostly not impacting the careful dance of refresh. But this creates a pretty wild memory layout.</p>

<p>
<svg width="640" height="860" viewBox="-10 0 320 420" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Memory map">
    
    <defs>
        <pattern id="striped" viewBox="0,0,7,8" width="4.16%" height="4.16%">
            <rect fill="#0000ff" x="-2" y="0" width="40" height="1"></rect>
            <rect fill="#00a0a0" x="-2" y="1" width="40" height="1"></rect>
            <rect fill="#00ff00" x="-2" y="2" width="40" height="1"></rect>
            <rect fill="#a0a000" x="-2" y="3" width="40" height="1"></rect>
            <rect fill="#ff0000" x="-2" y="4" width="40" height="1"></rect>
            <rect fill="#a0a0ff" x="-2" y="5" width="40" height="1"></rect>
            <rect fill="#ffa0a0" x="-2" y="6" width="40" height="1"></rect>
            <rect fill="#a000a0" x="-2" y="7" width="40" height="1"></rect>
        </pattern>
    </defs>
    <rect fill="#fff" x="-20" y="-20" width="320" height="450"></rect>
    <text x="0" y="12">Screen</text>
    <rect fill="url(#striped)" height="192" width="240" x="0" y="20"></rect>
    <text x="0" y="227">Memory</text>
    <rect fill="#0000ff" x="0" y="230" width="240" height="24"></rect>
    <rect fill="#00a0a0" x="0" y="254" width="240" height="24"></rect>
    <rect fill="#00ff00" x="0" y="278" width="240" height="24"></rect>
    <rect fill="#a0a000" x="0" y="302" width="240" height="24"></rect>
    <rect fill="#ff0000" x="0" y="326" width="240" height="24"></rect>
    <rect fill="#a0a0ff" x="0" y="350" width="240" height="24"></rect>
    <rect fill="#ffa0a0" x="0" y="374" width="240" height="24"></rect>
    <rect fill="#a000a0" x="0" y="398" width="240" height="24"></rect>
</svg>
</p>

<p>Apologies to the colorblind for the graph above! In fact, maybe I should just apologize to everyone with eyes. The SVGs are an experiment, we’ll see how they go.</p>

<p>Essentially, the HIRES memory space is divided into eight sections. The first section is the top row of pixels for each 7x8 text mode tile, the second section the second row, etc. etc. Each section (the large colored blocks in memory above) is itself laid out the same way as TEXT mode, complete with some screen holes. Confusing? Sure, but again, most programmers made a lookup table or two and called it a day. Each byte has seven pixels, the first bit being ignored. (The bits are also pushed to the screen in <em>opposite</em> order to how they’re usually written, but this is all just convention anyway)</p>

<h3 id="color">Color</h3>

<p><img src="https://nicole.express/assets/img/softcard/monitor3.jpg" title="Probably because it looks awesome" alt="The Monitor III sitting on the Apple II plus"></p>

<p>Everything I just described is good enough for business software and users of the monochrome Monitor ///. But this is an Apple II, the ultimate gaming PC of 1977. We want <em>color</em>. You probably know about “NTSC artifacts”, but what does that mean? And that’s where all our timing synchronization comes into play.</p>

<p>Imagine the Apple II drawing an alternating pixel pattern, <code>0101</code>. It draws those pixels at the rate of its pixel clock, <code>7M</code>. An important thing to remember about square waves is that the frequency is the frequency of a <em>complete cycle</em>, both the “up” and “down” of the wave.</p>

<p>
<svg width="500" height="325" viewBox="0 0 100 65" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <text x="5" y="7">7M (pixel clock)</text>
    <polyline points="10,10 10,20 20,20 20,10 30,10 30,20 40,20 40,10 50,10 50,20 60,20 60,10 70,10 70,20 80,20 80,10 90,10 90,20" stroke="red" stroke-width="2" fill="none"></polyline>
    <text x="5" y="27">3.5M (color signal)</text>
    <polyline points="10,30 10,40 30,40 30,30 50,30 50,40 70,40 70,30 90,30" stroke="blue" stroke-width="2" fill="none"></polyline>
    <text x="5" y="47">Pixels</text>
    <rect fill="#000" x="10" y="50" width="20" height="10" rx="4"></rect>
    <rect fill="#000" x="50" y="50" width="20" height="10" rx="4"></rect>
</svg>
</p>

<p>That is to say, if you alternate pixels, you’re creating a signal that repeats at the colorburst frequency! This is a real color signal, just like you’d generate if you had one of those fancy TMS9918As or something, but it’s being generated using the same mechanism that generates the pixels. (Sure, it’s a square wave here, but that’s what signal filters and such are for) Also, as the programmer, you get to control it directly.</p>

<p><img src="https://nicole.express/assets/img/pang-sparts/rf2av.jpg" title="Yep it's a rainbow" alt="Pong with rainbow backgrounds and art"></p>

<p>This is a screenshot from <a href="https://nicole.express/2024/super-duper-rainbow-pong.html">Atari’s <em>Pong Sports IV</em></a>, which like Atari’s other <em>Home Pong</em> series of consoles, uses a slightly out of phase crystal to create a cool rainbow effect. Obviously that isn’t possible here– with such strict pixel timing, we can only create two color phase shifts. And now you know why the “long cycle” that keeps the memory accesses in sync with the colorburst frequency is so important, or the phase shifts would also be different on each scanline.</p>

<p>
<svg width="500" height="400" viewBox="0 0 100 80" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <text x="5" y="7">7M (pixel clock)</text>
    <polyline points="10,10 10,20 20,20 20,10 30,10 30,20 40,20 40,10 50,10 50,20 60,20 60,10 70,10 70,20 80,20 80,10 90,10 90,20" stroke="red" stroke-width="2" fill="none"></polyline>
    <text x="5" y="27">3.5M (color signal)</text>
    <polyline points="10,30 10,40 30,40 30,30 50,30 50,40 70,40 70,30 90,30" stroke="blue" stroke-width="2" fill="none"></polyline>
    <text x="5" y="47">Patterns</text>
    <polyline points="10,50 10,60 30,60 30,50 50,50 50,60 70,60 70,50 90,50" stroke="orange" stroke-width="2" fill="none"></polyline>
    <polyline points="10,75 10,65 30,65 30,75 50,75 50,65 70,65 70,75 90,75" stroke="orange" stroke-width="2" fill="none"></polyline>
    <rect fill="rgba(0, 0, 0, 0.5)" x="10" y="50" width="20" height="10" rx="4"></rect>
    <rect fill="rgba(0, 0, 0, 0.5)" x="50" y="50" width="20" height="10" rx="4"></rect>
    <rect fill="rgba(0, 0, 0, 0.5)" x="30" y="65" width="20" height="10" rx="4"></rect>
    <rect fill="rgba(0, 0, 0, 0.5)" x="70" y="65" width="20" height="10" rx="4"></rect>
</svg>
</p>

<p>I deliberately used the chosen colors above because they are <em>not</em> the colors generated, because I don’t necessarily have the phase relationships perfectly correct. Take the diagrams above as basic conceptual scribbles, not necessarily oscilloscope traces– the point is, there are two signals: one in phase with the colorburst, one 180° out of phase with the colorburst. But the colorburst is itself defined as 180° out of phase with the color carrier, so this is a bit complex.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/snek.png" title="no step on snek" alt="Ultima II showing a green dorky-looking snake in a dungeon"></p>

<p>Anyway long story short, as <em>Ultima II</em> shows us above, it’s pink and green, the colors are pink and green. Well, pink is looking awfully purplish today, but that’s the wonder of NTSC. (And the horizontal lines visible on the green snake are the wonder of using square waves and this particular upscaler-capture combo) Notice those horizontal lines also picked up some color– if you want to guarantee white, you’ll need to create a signal with a frequency that <em>isn’t</em> a color carrier. The line above is just one pixel, but two pixels next to each other will do it.</p>

<p>
<svg width="500" height="325" viewBox="0 0 100 65" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <text x="5" y="7">7M (pixel clock)</text>
    <polyline points="10,10 10,20 20,20 20,10 30,10 30,20 40,20 40,10 50,10 50,20 60,20 60,10 70,10 70,20 80,20 80,10 90,10 90,20" stroke="red" stroke-width="2" fill="none"></polyline>
    <text x="5" y="27">3.5M (color signal)</text>
    <polyline points="10,30 10,40 30,40 30,30 50,30 50,40 70,40 70,30 90,30" stroke="blue" stroke-width="2" fill="none"></polyline>
    <text x="5" y="47">Pixels</text>
    <polyline points="10,50 10,60 50,60 50,50 90,50 90,60" stroke="orange" stroke-width="2" fill="none"></polyline>
    <rect fill="rgba(0, 0, 0, 0.5)" x="10" y="50" width="20" height="10" rx="4"></rect>
    <rect fill="rgba(0, 0, 0, 0.5)" x="30" y="50" width="20" height="10" rx="4"></rect>
</svg>
</p>

<p>Even if you alternate groups of two pixels, that signal isn’t at 3.579545MHz, so your television won’t be able to pull out any color information from it– it’ll just be treated as monochrome white. Modulo some higher-frequency fringes, after all, this <em>is</em> still good old-fashioned <a href="https://nicole.express/2021/shouldve-had-field-sequential.html">composite video</a> and no filter is perfect. Apple II users got used to some color fringing, or used a monochrome monitor.</p>

<p>Now, unfortunately, there is another catch here. Our addressable pixel areas are 7 pixels wide, which is not evenly divisible by two.<sup><i>citation needed</i></sup> This means that if you have a pattern, <code>x0101010</code>, whether it’s pink or green will depend on its position relative to the beginning of the line. Odd addresses will have one color, even addresses the other. This is why many Apple II games, like <em>Ultima III: Exodus</em>, create a grid of 14-pixel wide tiles that things move about on.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/exy.png" title="let my people go?" alt="Exodus, showing a tiled area"></p>

<p>This is still HIRES graphics mode, not a tile-based mode– the developer is just implementing tiles to make their lives easier. And on this title screen they only do so where things will move around in the bottom half.</p>

<h3 id="whats-the-big-deal">What’s the big deal?</h3>

<p>Now you might have noticed something about the screenshot above. It’s got colors that aren’t pink and green– it’s got blue water, and red lava. That’s true, but it’s only true because my Apple II isn’t a Revision 0. Those early adopters only have a three-color HIRES mode. The rest of us have something better.</p>

<p>Remember that first bit? It’d be pretty wasteful to just leave that unused. This is especially true from the perspective of the video scanner– this bit is fully decoded and just sitting there, waiting to have a use applied for it. What Woz did was have it delay the output of pixels by one cycle of the <em>14M</em> master clock, breaking the 7M pixel clock, but creating new phase shifts.</p>

<p>
<svg width="500" height="400" viewBox="0 0 100 80" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <text x="5" y="7">7M (pixel-ish clock)</text>
    <polyline points="10,10 10,20 20,20 20,10 30,10 30,20 40,20 40,10 50,10 50,20 60,20 60,10 70,10 70,20 80,20 80,10 90,10 90,20" stroke="red" stroke-width="2" fill="none"></polyline>
    <text x="5" y="27">3.5M (color signal)</text>
    <polyline points="10,30 10,40 30,40 30,30 50,30 50,40 70,40 70,30 90,30" stroke="blue" stroke-width="2" fill="none"></polyline>
    <text x="5" y="47">Patterns (offset)</text>
    <polyline points="9,50 20,50 20,60 40,60 40,50 60,50 60,60 80,60 80,50 90,50" stroke="#0d0" stroke-width="2" fill="none"></polyline>
    <polyline points="9,75 20,75 20,65 40,65 40,75 60,75 60,65 80,65 80,75 90,75" stroke="#0d0" stroke-width="2" fill="none"></polyline>
    <polyline points="10,50 10,60 30,60 30,50 50,50 50,60 70,60 70,50 90,50" stroke="rgba(0,0,0,0.2)" stroke-width="2" fill="none"></polyline>
    <polyline points="10,75 10,75 10,65 30,65 30,75 50,75 50,65 70,65 70,75 90,75" stroke="rgba(0,0,0,0.2)" stroke-width="2" fill="none"></polyline>
</svg>
</p>

<p>This gives us two more phases to work with, 90° out of phase with the pink and green colors we had before– blue and red-orange. But remember, we’re limited to using them within groups of 7 pixels. You can almost think of this as like being able to choose a palette for a 1-pixel high and 7-pixel wide area, except for…</p>

<h3 id="the-boundaries">The boundaries</h3>

<p>An interesting thing can be seen in this screenshot from Sega’s <em>Frogger</em>. You should now understand why Frogger is white, and why the colored areas are laid out the way they are. But take a look at where the coast meets the water. The green areas and the blue areas don’t quite line up.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/f1.png" title="Frogger on the TRS-80 has horizontal scrolling" alt="Frogger port"></p>

<p><img src="https://nicole.express/assets/img/2apple2furious/zoom.png" title="plus turtles arent as big as trucks, inaccurate" alt="Zoomed in to show misaligned blocks"></p>

<p>Why don’t they line up? It’s not the developer’s fault, it’s because they can’t. Take a look at what happens if I plug the Apple II’s output into the component luma input on the OSSC. (This is actually how I got most of the text mode captures too, to avoid unnecessary color noise)</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/f2.jpeg" title="the wormhole aliens HATE the apple ii because it is linear" alt="Frogger port all made out of lines"></p>

<p>The lines just don’t line up. The Apple II can get you in a mindset trap; of <em>course</em> it can’t line up, you might start to think. But on a system where the luminance and chrominance are set separately, and where it can output analog values, not just 0 or 1, of course it can. On the Apple II, all sorts of weirdness will happen where non-delayed pixels interact with delayed pixels. Say, at the edge of the screen, when we enter the screen border, the seven pixels will be abruptly cut off, leaving a half-pixel at the edge.</p>

<p>
<svg width="500" height="250" viewBox="0 40 100 50" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <text x="30" y="45">Screen boundary</text>
    <line stroke-width="2" x1="70" y1="0" x2="70" y2="100" stroke="#f00"></line>
    <polyline points="9,50 20,50 20,60 40,60 40,50 60,50 60,60 90,60" stroke="#0d0" stroke-width="2" fill="none"></polyline>
    <polyline points="9,75 20,75 20,65 40,65 40,75 60,75 60,65 70,65 70,75 90,75" stroke="#0d0" stroke-width="2" fill="none"></polyline>
</svg>
</p>

<p>Now, do these effects matter? On a CRT, probably not, but you might see some fun color fringes here and there. But if you’re wondering why your Apple II image capture is looking so much worse than your other systems, even over composite? Well, stuff like this doesn’t help.</p>

<p>HIRES is by far the most important graphics mode on the Apple II; more games used it than any of the other options, and even business software used it to do things like implement 80-column text in software. A bit awkward and weird? People got over it. I’ll end this discussion with one of my more nostalgic vintage HIRES intro sequences, from <em>Ultima II</em>. Sorry about the lack of audio; I kept the internal speaker wired up separately when I installed the Mockingboard, and this game only uses the internal beeper.</p>

<p><video src="https://nicole.express/assets/img/2apple2furious/u2.mp4" width="640px" controls="">You can't see this due to lacking video tag support. It's pretty cool, a dragon shows up and breathes fire.</video></p>

<p>I especially like the color animation on the “II” from toggling that seventh bit.</p>

<h2 id="kill-it-with-fire-or-a-transistor-will-do">Kill it with fire, or a transistor will do</h2>

<p>With all the above in mind, text mode <em>should</em> have the same color fringing everywhere that you see in HIRES graphics mode. The pixels are the same size, and Apple didn’t even design the font, so it’d be pretty impressive if it had been optimized to not fringe. But on most displays you’ll see nice pure white in Apple II text mode. How come?</p>

<p>Well, take a look at a screenshot of <em>Mission Asteroid</em> by Sierra. This uses a mixed mode, which I won’t really go into detail on how it works, but basically has four lines worth of text mode at the bottom of the screen underneath the graphics mode. And in this mode, the text fringes quite a bit. All those single-pixel horizontal lines suffer the same problem as the horizontal lines the snake was hanging out in in <em>Ultima II</em>.</p>

<p><img src="https://nicole.express/assets/img/yellow/mission.jpg" title="this is art hang it in the louvre" alt="A secretary sits at a desk. The game is a text adventure with a prompt at the bottom of the screen."></p>

<p>If I had a Revision 0 Apple II, I’d have the same experience with the pure text mode. But I don’t– that’s because later revisions of the Apple II like mine added a circuit called the “color killer”, which removes the color burst when in text mode. In theory, a signal without a color burst should always be interpreted by the TV as a monochrome signal, because NTSC is backwards-compatible. The problem is, the color killer isn’t great– it merely reduces the color burst.</p>

<p>Here’s the signal with the color burst present, which I obtained by booting BASIC and typing <code>HGR</code>.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/color1.png" title="doctor please my color's burst" alt="Oscilloscope trace showing color burst"></p>

<p>And here it is with the color burst on, which I obtained by typing <code>TEXT</code> with the same trace showing. I’m kind of surprised this ever doesn’t work, honestly– I guess that little bit of a cycle must be enough to confuse a sufficiently sensitive detector. Interesting, <em>Understanding the Apple II</em> suggests you mod your TV to detect the color burst, only suggesting modding the computer as a last resort, despite the many other computer mods recommended.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/color2.png" title="dont like all that noise" alt="Oscilloscope trace showing color burst gone, except for a cycle"></p>

<p>This impact for me has always shown up on higher-quality scalers, which are desperate to try to extract a color signal. The color killer works fine on a cheap AV2HD box, but the Micromsoft Framemeister ends up with a fringy mess. Check out my <a href="https://nicole.express/2021/composite-conflict-completed.html#test-10-the-apple-">composite scaler competition</a> post for more details on that.</p>

<p><img src="https://nicole.express/assets/img/composhoot/apii-meister.gif" title="I talk about the color killer way too much" alt="Poorly color killed signal"></p>

<h2 id="lores-mode">LORES mode</h2>

<p>The Apple II’s LORES graphics mode is very impressive. It can display any pixel on screen in any of 16 colors. Well, 15. More or less. There’s two greys that are usually the same. But still, far more colors than HIRES, and with pure pixel-level color selection. So what’s the catch? The resolution is a whopping 40x48. When even <a href="https://nicole.express/2024/radio-keith-orpheum.html">RCA Studio II</a> fans think you could use a few more pixels, you’re in trouble.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/duke1.png" title="Duke Nukem accepts all babes, regardless of how few pixels" alt="Duke Nukem port in LORES mode"></p>

<p>That’s not to say LORES mode is useless. <a href="http://deater.net/weave/vmwprod/duke/">Deater</a> has done quite a few demakes into LORES mode; such a low resolution makes it fast to update the whole screen even doing things like parallax, and the mixed text/graphics mode means you can use text mode for things that absolutely have to be readable, like scores and such.</p>

<p><img src="https://nicole.express/assets/img/mock-me/little-brick-out.png" title="cuz she's a brick... OUT" alt="Game Over screen in Little Brick Out. Only a few bricks are broken and the game is telling me my score is not too good."></p>

<p>And honestly LORES mode is part of the heart of the Apple II. It’s the mode that Wozniak created so that, having done <em>Breakout</em> in hardware for Atari, he could now do it in software with <em>Little Brick Out</em>. It’s the mode classic business simulation <em>Lemonade Stand</em> used. It justified the rainbow Apple logo. So how does it work?</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/lores.png" title="I took a new version of this picture, because I love you" alt="LORES color palette from the Diagonstics II plus disk"></p>

<p>The Koryuu I’m using here has a filter option that kind of blurs everything horizontally; I generally keep it disabled, but it does at least blur the lines together and gets rid of the high frequency noise. Of course, as we’ll see, that noise is the point. By the way, these color bars are from the Apple Diagnostics II+ disk.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/blurry.png" title="fuzzy" alt="LORES color palette from the Diagonstics II plus disk, blurred a bit"></p>

<p>The LORES mode is twice the height of TEXT mode, and that’s no coincidence. The same screen data as text mode is used, with the same layout– the difference is, the two “nybbles” of each byte each become one of 16 colors, stacked on top of each other. A LORES pixel is 7 HIRES pixels wide, and four HIRES pixels tall.</p>

<p>But how does LORES get so many colors?</p>

<p>
<svg width="500" height="325" viewBox="0 0 100 65" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <line stroke-width="2" x1="50" y1="0" x2="50" y2="100" stroke="#ccc"></line>
    <text x="5" y="07">14M (oscillator)</text>
    <polyline points="10,10 10,20 15,20 15,10 20,10 20,20 25,20 25,10 30,10 30,20 35,20 35,10 40,10 40,20 45,20 45,10 50,10 50,20 55,20 55,10 60,10 60,20 65,20 65,10 70,10 70,20 75,20 75,10 80,10 80,20 85,20 85,10 90,10 90,20" stroke="green" stroke-width="2" fill="none"></polyline>
    <text x="5" y="27">7M (TEXT pixel clock)</text>
    <polyline points="10,30 10,40 20,40 20,30 30,30 30,40 40,40 40,30 50,30 50,40 60,40 60,30 70,30 70,40 80,40 80,30 90,30 90,40" stroke="red" stroke-width="2" fill="none"></polyline>
    <text x="5" y="47">3.5M (color signal)</text>
    <polyline points="10,50 10,60 30,60 30,50 50,50 50,60 70,60 70,50 90,50" stroke="blue" stroke-width="2" fill="none"></polyline>
</svg>
</p>

<p>Notice that for every four cycles of the <code>14M</code> master oscillator, there’s one cycle of the 3.5MHz color burst signal. So if you repeat a four-bit pattern at the rate of the 14MHz clock, you’ll create a signal with a period of 3.5MHz. That’s all <code>LORES</code> mode is doing to create its colors. Sure, there will be components of the signal at other frequencies– you can see the OSSC trying to show the high-frequency lines, while other devices like an RF modulator might blur the high-frequency signal out into a flat luminance.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/lores2.png" title="i am the lores, i speak for the pixels" alt="LORES color palette in monochrome, showing lines"></p>

<p>And the repeating patterns? That’s the genius part– they <em>are</em> the nybble in question. Take a look at the chart again. Repeating <code>0000</code> over and over again? Of course that’s a pure black. Repeating <code>1111</code> again and again? That’s pure white. What are the grey patterns? <code>0101</code> (5) and <code>1010</code> (10), which alternate fast enough that they don’t really have a low-frequency component, so no color to pick up on. There are two alternating patterns, so two greys. (For homework, consider what happens when those two greys are next to each other)</p>

<p>There is a bit more to it; for example, the phase inversion caused by having 7-pixel-wide slots is compensated for in LORES, but in general this is really a very clever graphics mode. Double HIRES mode on the 80-column Apple IIe uses the same pixel patterns, but that’s a story for another time.</p>

<h2 id="apple-ii-forever">Apple II Forever</h2>

<p><img src="https://nicole.express/assets/img/2apple2furious/oregon.png" title="not going to lie I have more nostalgia for the monochrome Mac oregon trail" alt="Oregon Trail: NICOLE has cholera"></p>

<p>The Apple II is one of the oldest computers I recall using; my kindergarten in 1995 had an old machine they let the kids bang on to avoid them breaking anything new. And yet that computer still fascinates me to this day. I think it’s because it’s not only a useful machine, with a lot of history, but also that despite its rougher edges, it’s something that beckons to be understood. And is well documented to boot. Definitely beats the arcade boards for that.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/timezone.png" title="this is the opening to TIME ZONE, the biggest Apple II game of all time, and the most expensive at release. if you can afford this game, you can afford this big house" alt="YOU ARE IN FRONT OF YOUR OWN HOUSE. Underneath an image of a house, from the Apple II game TIME ZONE."></p>

<p>Since Jim Sather’s book was crucial to the completion of this blog post, I think it’s only fair that we end with a quote from <em>Understanding the Apple 2</em>.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/suzy.png" title="what are you talking about" alt="...bus system. This means that a peripheral card can control all hardware features of the Apple. It is as if you could plug a Suzy brain into Johnny and have the Suzy brain control Johnny's body, a concept much in vogue in some circles."></p>

<p>Jim Sather is in way cooler circles than me.</p>

  </div>

  
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New ways to catch gravitational waves (218 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-02003-6</link>
            <guid>40820063</guid>
            <pubDate>Fri, 28 Jun 2024 12:41:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-02003-6">https://www.nature.com/articles/d41586-024-02003-6</a>, See on <a href="https://news.ycombinator.com/item?id=40820063">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>In September 2015, a vibration lasting just one-fifth of a second changed the history of physics. It was the <a href="https://www.nature.com/articles/530261a" data-track="click" data-label="https://www.nature.com/articles/530261a" data-track-category="body text link">first direct detection of gravitational waves</a> — perturbations in the geometry of space-time that move across the Universe at the speed of light.</p><p>Astronomers say it was like gaining a new sense — as if, until 2015, they had only been able to ‘see’ cosmic events, and now could ‘hear’ them, too. Since then, it has become almost a matter of daily routine to record the passage of gravitational waves at the two massive facilities of the Laser Interferometer Gravitational-wave Observatory (LIGO) in Louisiana and Washington state, along with their sibling Virgo observatory near Pisa, Italy.</p><p>The detection of gravitational waves has provided <a href="https://www.nature.com/articles/nature.2016.19337" data-track="click" data-label="https://www.nature.com/articles/nature.2016.19337" data-track-category="body text link">new ways to explore the laws of nature and the history of the Universe</a>, including clues about the life story of black holes and the large stars they originated from. For many physicists, the birth of gravitational-wave science was a rare bright spot in the past decade, says Chiara Caprini, a theoretical physicist at the University of Geneva in Switzerland. Other promising fields of exploration have disappointed: <a href="https://www.nature.com/articles/d41586-020-02741-3" data-track="click" data-label="https://www.nature.com/articles/d41586-020-02741-3" data-track-category="body text link">dark-matter searches</a> have kept coming up empty handed; the Large Hadron Collider near Geneva has found nothing beyond the Higgs boson; and even some promising hints of <a href="https://www.nature.com/articles/d41586-023-02532-6" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02532-6" data-track-category="body text link">new physics</a> seem to be fading. “In this rather flat landscape, the arrival of gravitational waves was a breath of fresh air,” says Caprini.</p><p>That rare bright spot looks set to become brighter.</p><p>All of the more than 100 gravitational-wave events spotted so far have been just a tiny sample of what physicists think is out there. The window opened by LIGO and Virgo was rather narrow, limited mostly to frequencies in the range 100–1,000 hertz. As pairs of heavy stars or black holes slowly spiral towards each other, over millions of years, they produce gravitational waves of slowly increasing frequency, until, in the final moments before the objects collide, the waves ripple into this detectable range. But this is only one of <a href="https://www.nature.com/articles/nature.2016.19337" data-track="click" data-label="https://www.nature.com/articles/nature.2016.19337" data-track-category="body text link">many kinds of phenomenon</a> that are expected to produce gravitational waves.</p><p>LIGO and Virgo are laser interferometers: they work by detecting small differences in travel time for lasers fired along perpendicular arms, each a few kilometres long. The arms expand and contract by minuscule amounts as gravitational waves wash over them. Researchers are now working on several next-generation LIGO-type observatories, both on Earth and, in space, the Laser Interferometer Space Antenna; some have even proposed building one on the Moon<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>. Some of these could be sensitive to gravitational waves at frequencies as low as 1 Hz.</p><p>But physicists are also exploring entirely different techniques to detect gravitational waves. These strategies, which range from watching pulsars to measuring quantum fluctuations, hope to catch a much wider variety of gravitational waves, with frequencies in the megahertz to nanohertz range (see ‘Opening the window on gravitational waves’).</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257006.png?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257006.png?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Opening the window on gravitational waves: graphic that shows a range of new detectors, and the range of frequencies from different sources that they will be able to detect." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257006.png">
  <figcaption>
   
  </figcaption>
 </picture>
</figure><p>By broadening their observational window, astronomers should be able to watch black holes circling each other for days, weeks or even years, rather than just catching the last few seconds before collision. And they’ll be able to spot waves made by totally different cosmic phenomena — including mega black holes and even the start of the Universe itself. All this, they say, will crack open many remaining secrets of the cosmos.</p><h2>Pulsar timing array: catching waves that last a decade</h2><p>Last year, one viable alternative to interferometers entered the game.</p><p>Since the early 2000s, radio astronomers have been attempting to use the entire Galaxy as a gravitational-wave detector. The trick is to monitor dozens of neutron stars called pulsars. These spin on their axis hundreds of times per second while emitting a radio-frequency beam, producing what looks like a pulse of light on each turn.</p><p>Gravitational waves sweeping the Galaxy would change the distance between Earth and each pulsar, creating anomalies in detected pulsar frequencies from one year to the next. Observations of a collection or array of pulsars — called a pulsar timing array (PTA) — should be able to detect changes induced by gravitational waves with frequencies of just nanohertz, as might be produced by pairs of supermassive black holes, for example. It takes tens of years for successive crests of such waves to pass a given vantage point, meaning that tens of years of observations are needed to spot them.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-02203-6" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_25559284.jpg"><p>Giant gravitational waves: why scientists are so excited</p></a>
 </article><p>In 2023, the PTA technique <a href="https://www.nature.com/articles/d41586-023-02203-6" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02203-6" data-track-category="body text link">began to bear fruit</a>. Four separate collaborations, in North America, Europe, Australia and China, <a href="https://www.nature.com/articles/d41586-023-02167-7" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02167-7" data-track-category="body text link">unveiled tantalizing hints</a> of a pattern expected from a random ‘stochastic background’ of gravitational waves that make Earth slosh around, probably caused by a cacophony of supermassive black-hole binaries, says astrophysicist Chiara Mingarelli at Yale University in New Haven, Connecticut.</p><p>The teams have not yet used the word ‘discovery’, because the evidence that each collaboration unveiled is not yet rock solid. But three of the groups — all but the Chinese one — are now pooling their data and conducting a joint analysis in the hope of getting to the ‘D’ word. This requires painstaking work, because each group processed its raw data in slightly different ways, and so it could take at least another year to get to publication, says Scott Ransom, an astrophysicist at the US National Radio Astronomy Observatory in Charlottesville, Virginia, and a senior member of the North American collaboration.</p><p>“In our current data, we almost certainly have the hints of individual supermassive black-hole binaries out there,” says Ransom. With each extra year of observation, they should get closer to resolving single black-hole pairs out of the cacophony, he adds. “Things are just going to get better and better.”</p><h2>Microwave telescopes: spotting waves from the Big Bang</h2><p>A year before LIGO’s 2015 detection, a team of cosmologists using a South Pole telescope called BICEP2 claimed to have spotted gravitational waves — not directly, but in the pattern of light called the cosmic microwave background (CMB), sometimes described as the afterglow of the Big Bang.</p><p>The <a href="https://www.nature.com/articles/nature.2014.15440" data-track="click" data-label="https://www.nature.com/articles/nature.2014.15440" data-track-category="body text link">BICEP2 claim turned out to be premature</a>, but cosmologists are now doubling down on this idea. An array of telescopes much more powerful than BICEP2, called the <a href="https://www.nature.com/articles/d41586-024-00333-z" data-track="click" data-label="https://www.nature.com/articles/d41586-024-00333-z" data-track-category="body text link">Simons Observatory</a>, is being set up on a mountaintop in northern Chile’s Atacama Desert. Some researchers are holding out hope for an even more powerful array called CMB-S4 (originally proposed to include 12 telescopes in Chile and at the South Pole) — although in May, plans for that project were put on hold because of the disrepair of the US South Pole base.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/nature.2016.19337" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257160.jpg"><p>Gravitational waves: 6 cosmic questions they can tackle</p></a>
 </article><p>What cosmologists are looking for in the CMB is a specific ‘B mode’ pattern in the swirls of its polarization — the preferential directions in which the microwaves wiggle — that would have been imprinted by the passage of gravitational waves. The theory is that such waves should have been produced by inflation, a quick burst of exponential cosmic expansion thought to have happened around the time of the Big Bang<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup>. Inflation would explain many of the Universe’s most striking properties, such as its flatness and how mass is distributed. The gravitational waves that inflation produced would have started at high frequencies, but would by now be at incredibly low frequencies of around 10<sup>−14</sup> Hz.</p><p>Although inflation is a cornerstone of accepted cosmological theory, there’s no proof of it yet. The B-mode pattern would be the smoking gun and, moreover, would reveal the energy scales involved, which would be a first step towards understanding what powered inflation.</p><p>The problem is, no one knows whether that energy scale was large enough to have left a noticeable mark. “Inflation predicts the B modes, but we don’t know if it’s big enough to be detected,” says Marc Kamionkowski, a theoretical astrophysicist at Johns Hopkins University in Baltimore, Maryland. But if the leading models are right, either the Simons Observatory or CMB-S4 should eventually find it, he says.</p><h2>Atom interferometry: closing the gap</h2><p>Although many of these projects push gravitational-wave science towards lower frequencies, they leave a crucial gap just below 1 Hz.</p><p>Detecting such frequencies could reveal mergers of black holes much more massive than those seen by LIGO (which spots waves from collapsing stars that weigh at most a few tens of solar masses). “This is an unexplored region, but it could be populated with lots of black holes,” says Caprini.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27252636.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27252636.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Physicists Jason Hogan and Mark Kasevich pictured next to equipment they are developing for measuring gravitational waves." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27252636.jpg">
  <figcaption>
   <p><span>Jason Hogan (left) and Mark Kasevich work on an atom interferometer — a device that could reveal mergers of black holes much more massive than those seen by current laser interferometers.</span><span>Credit: L.A. Cicero and Stanford University</span></p>
  </figcaption>
 </picture>
</figure><p>A nascent technique could come to the rescue, according to physicist Oliver Buchmüller at Imperial College London. “Atom interferometry sits in that gap which we currently cannot explore with any other technology,” he says. An atom interferometer is a vertical high-vacuum pipe in which atoms can be released and allowed to fall under gravity. As they do so, physicists tickle the atoms with laser light to toggle them between an excited and a relaxed state — the same principle used by atomic clocks. “We’re trying to push this atomic-clock technique to what’s ultimately possible,” says Jason Hogan, a physicist at Stanford University in California.</p><p>To detect gravitational waves, physicists plan to drop two or more sets of atoms at different heights inside the same vertical pipe, and to measure the time it takes for a laser pulse to travel from one set of atoms to the next, says Hogan. The passage of gravitational waves would result in light spending either slightly less or slightly more time travelling between them — a variation smaller than one part in 100 billion billion.</p><p>Pioneering experiments at Stanford University have developed atom interferometers with 10-metre drops, but detecting gravitational waves would require devices at least 1 kilometre in height, which could be installed in a mine shaft, say, or even in space. As a first step, several groups around the world are planning to build 100-m atom interferometers as test beds. One such facility, called MAGIS-100, is already under construction in an existing shaft at the Fermi National Accelerator Laboratory outside Chicago, Illinois, and is scheduled for completion in 2027.</p><h2>Desktop detectors: pushing the frequency up</h2><p>Other researchers are exploring ways of detecting gravitational waves with much, much smaller (and cheaper) detectors — including some that could fit on a desktop. These are designed to watch for extremely high-frequency gravity waves. Known phenomena probably don’t produce such waves, but some speculative theories do predict them.</p><p>The Levitated Sensor Detector (LSD) at Northwestern University in Evanston, Illinois looks like a toy LIGO: it bounces lasers between pairs of mirrors just 1 metre apart. The LSD is a prototype for a new type of instrument designed to sense gravitational waves using resonance: the same principle by which even little pushes can make a child on a swing go higher and higher if they are timed just right<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d43978-024-00069-4" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257164.jpg"><p>Will the Einstein Telescope be split in two?</p></a>
 </article><p>In a vacuum inside each of the LSD’s arms, laser light suspends a particle just micrometres wide. As with an interferometer, the passage of gravitational waves will alternately elongate and compress the length of each arm. If the frequency of the gravitational waves resonates with that of the device, the lasers will then give many tiny kicks to the particle. The LSD can track the particle’s motion with a precision of femtometres, says Northwestern physicist Andrew Geraci, who is leading the project.</p><p>The LSD is designed to be sensitive to gravitational waves with frequencies of around 100 kHz. This prototype might already have a shot at detecting some, if the team can keep experimental noise in check — and provided that such waves exist. “Depending how optimistic you are, we may be able to measure a real signal in that band even with a 1-m instrument,” Geraci says. Future versions could be scaled up to 100-m-long arms, he adds, which would increase their sensitivity.</p><p>Theoretical physicist Ivette Fuentes at the University of Southampton, UK, has an idea for making an even smaller resonant detector. She aims to exploit sound waves in an exotic state of matter called a Bose–Einstein condensate (BEC) — a cloud of atoms kept at temperatures as low as a few millionths of a degree above absolute zero. If a gravitational wave passes through at a frequency that resonates with the sound wave, it can be detected. Because the act of looking for this signal destroys the BEC, a new flood of atoms needs to be released every second. The process might need to be repeated for months for a successful detection, Fuentes says.</p><p>In principle, a BEC-based detector could expand the search for gravitational waves to extremely high frequencies of 1 MHz or more — again, provided they exist. Fuentes says her scheme would require pushing BEC techniques just a little beyond the current state of the art. “I think the idea is very bold,” she says. Physicists have posited that high-frequency gravitational waves could reveal exotic physics that went on in the first second or so after the Big Bang. “We could use it to study the state of the Universe at very high energies,” says Caprini.</p><h2>Quantum crystal: only takes a second</h2><p>A final, more radical proposal for detecting gravitational waves involves putting objects in two places at once.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-024-00333-z" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257162.jpg"><p>‘Best view ever’: observatory will map Big Bang’s afterglow in new detail</p></a>
 </article><p>Sougato Bose, a physicist at University College London, has proposed a device in which a micrometre-sized diamond crystal is put in a superposition of two quantum states. In his scheme, the crystal’s two ‘personas’ would be pushed apart by as much as 1 metre and then brought together again — an extremely delicate procedure that has been compared to putting the nursery-rhyme character Humpty Dumpty back together after a fall. The passage of gravitational waves would make one persona travel further than the other when apart, putting them out of sync — in a measurable way — when reunited. The whole process would take around one second to complete, which would make the device sensitive to gravitational waves of around 1 Hz.</p><p>This idea is extremely ambitious: such quantum tricks have so far been shown to work only for objects the size of molecules, and no one has ever tested whether quantum weirdness can be pushed to such extremes. “Putting Humpty Dumpty back together has never been demonstrated for crystals,” says Bose.</p><p>But if the technique can be perfected, then table-top experiments such as this one could take gravitational-wave detection out of the hands of just a few large-scale labs. Together, these techniques could blow open the window on what can be seen. “The outlook is very positive,” says Caprini.</p>
                </div></div>]]></description>
        </item>
    </channel>
</rss>