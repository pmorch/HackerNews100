<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 23 Nov 2024 13:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Quake 3 Source Code Review: Network Model (2012) (139 pts)]]></title>
            <link>https://fabiensanglard.net/quake3/network.php</link>
            <guid>42218532</guid>
            <pubDate>Sat, 23 Nov 2024 00:55:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fabiensanglard.net/quake3/network.php">https://fabiensanglard.net/quake3/network.php</a>, See on <a href="https://news.ycombinator.com/item?id=42218532">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content" role="main">



		


<p>
       June 30, 2012</p>
   <a href="https://fabiensanglard.net/quake3/qvm.php">
   </a><div id="paperbox"><p><a href="https://fabiensanglard.net/quake3/qvm.php">
	   	
          </a><a href="https://fabiensanglard.net/quake3/index.php">
          <img src="https://fabiensanglard.net/quake3/quake3_icon.jpg">   
          </a> 
         

The network model of Quake3 is with no doubt the most elegant part of the engine. At the lower level Quake III still abstract communications
with the <a href="https://fabiensanglard.net/quakeSource/quakeSourceNetWork.php">NetChannel module that first appeared in Quake World</a>.
The most important thing to understand is:</p><p>
In a fast paced environment any information that is not received on first transmission is not worth sending again because it will be too old anyway.</p><p>

As a result the engine relies essentially on UDP/IP: There is no trace of TCP/IP anywhere since the "Reliable transmission" 
aspect introduced intolerable latency. The network stack has been augmented with two mutually exclusive layers:
</p></div><ul>
    <li>Encryption using preshared key.</li>
    <li>Compression with pre-computed huffman key.</li>
</ul>
<div>

<p><img src="https://fabiensanglard.net/lazy_load/grey.gif" data-original="/fd_proxy/quake3/q3_netstack.png"></p><p>

But where the design really shine is on the server side where an elegant system minimize the size of each UDP datagram while 
compensating for the unreliablity of UDP:
An history of snapshots generate deltas packets via memory introspection.

</p></div>



<h3>Architecture</h3>
<div id="paperbox"><p>
The Client side of the network model is fairly simple: Client sends commands to the Server each frame and receive update for the gamestate. The Server side is as bit more complex since it has to propagate the Master gamestate to each Client while accounting for lost UDP packets. This mechanism features three key elements:<br>
<img src="https://fabiensanglard.net/lazy_load/grey.gif" data-original="/fd_proxy/quake3/q3_network_archi.png"></p></div><ul>
  <li>A <b>Master Gamestate</b> that is the universal true state of things. Clients send theirs commands on the Netchannel. They are transformed in event_t which will modifiy the state of the game when they arrive on the Server.</li>
  <li>For each Client the server keeps the <b>32 last gamestate</b> sent over the network in a cycling array: They are called snapshots. The array cycle with the famous binary mask trick I mentioned in Quake World Network (<a href="http://fabiensanglard.net/quakeSource/quakeSourceNetWork.php">Some elegant things</a>).</li>
  <li>The server also features <b>a "dummy" gamestate</b> with every single field set to zero. This is used to delta snapshots when there is no "previous state" available.</li>
 </ul>
 <div><p>
When the server decides to send an update to a client it uses each three elements in order to generate a message that is then carried over the NetChannel.</p><u><b>Trivia :</b></u><p> To keep so many gamestate for each players consumes a lot of memory: 8 MB for 4 players according to my measurements.  
</p></div>


<h3>Snapshot systems</h3>
<p id="paperbox">
In order to understand the snapshop system, here is an example with the following conditions:
</p><ul>
    <li>The server is sending update to a Client1.</li>
    <li>The server is attempting to propagate the state of Client2 which has four 4  fields (3 ints position[X], position[Y], position[Z] and one int health).</li>
    <li>Communication are done over UDP/IP: Those messages gets lost quite often on the internet.</li>
</ul>
<div>
<u>Server Frame 1:</u><p>

The Server has received a few updates from every client. They have impacted the Master gamestate (in green). It is now time to propagate the state to Client1:
<img src="https://fabiensanglard.net/lazy_load/grey.gif" data-original="/fd_proxy/quake3/q3_network_t1.png"></p><p>

In order to generate a message the network module will ALWAYS do the following :
</p></div><ol>
   <li>Copy the Master gamestate in the next Client history slot.</li>
   <li>Compare it with an other snapshot.</li>
</ol>
<p>
This is what we can see in the next drawing:
</p><ol>
   <li> Master gamestate is copied at index 0 in Client1 history: It is now called"Snapshot1".</li>
   <li> Since this is the first udpate, there are no valid snapshot in Client1 history so the engine is going to use the "Dummy snapshot" where all fields are always ZEROed. This results in a FULL update since every single field is sent to the NetChannel.</li>
 </ol>
 <div>
<p><img src="https://fabiensanglard.net/lazy_load/grey.gif" data-original="/fd_proxy/quake3/q3_network_t2.png"><br>
The key point to understand here is that if no valid snapshots are available in the client history the engine will pick "dummy snapshot" to
 generate a delta message. This will result in a full udpate sent to the Client using 132 bits (each field is <a href="https://github.com/id-Software/Quake-III-Arena/blob/master/code/qcommon/msg.c#L1200">
preceeded by a bit marker</a>): <code>[1 A_on32bits 1 B_on32bits 1 B_on32bits 1 C_on32bits]</code>.
</p><u>Server Frame 2:</u><p>

Now let's move forward in time: this is the Server second frame. As we can see each client have sent commands and they have impacted the Master gamestate: Client2 has moved on the Y axis so pos[1] is now equal to E (in blue). Client1 has also sent commands but more important it has also acknowledged receiving the previous udpate so Snapshot1 has been marked as "ACK":
<img src="https://fabiensanglard.net/lazy_load/grey.gif" data-original="/fd_proxy/quake3/q3_network_t3.png"><br>

The process is the same:</p></div><ol>
   <li>Copy the Master gamestate in the next Client history slot: (index 1): This is Snapshot2</li>
   <li>This time we have a valid snapshot in the client history (snapshot1). Compare those two snapshots</li>
</ol>
<div><p>
As result only a partial update (   pos[1] = E ) is sent over the network. This is the beauty of the design: The process is always the same.
<img src="https://fabiensanglard.net/lazy_load/grey.gif" data-original="/fd_proxy/quake3/q3_network_t4.png"></p><u>Note :</u><p> Since each field is <a href="https://github.com/id-Software/Quake-III-Arena/blob/master/code/qcommon/msg.c#L1200">
preceeded by a bit marker</a> (1=changed, 0=not changed) the partial update above would uses 36 bits: <code>[0 1 32bitsNewValue 0 0]</code>. 
</p><u>Server Frame 3:</u><p>

Let's move forward one more step in order to see how the system deals with lost packets. This is now Frame 3. Clients have kept on sending commands to the server.
Client2 has lost life and health is now equal to H. But Client1 has not acknowledged the last update. Maybe the Server's UDP got lost, maybe the ACK from the Client
got lost but bottom line is that it cannot be used.

<img src="https://fabiensanglard.net/lazy_load/grey.gif" data-original="/fd_proxy/quake3/q3_network_t5.png"></p><p>


Regardless the process remains the same:
</p></div><ol>
   <li>Copy the Master gamestate in the next Client history slot: (index 2): This is Snapshot3</li>
   <li>Compare with the last valid acknowledged snapshot (snapshot1).</li>
</ol>
<div>
<p><img src="https://fabiensanglard.net/lazy_load/grey.gif" data-original="/fd_proxy/quake3/q3_network_t6.png"></p><p>

As a result the message sent it partial and contains a combination of old changes and new changes: (pos[1]=E and health=H). Note that snapshot1 could have been too old to be used. In this case the engine would have used the "dummy snapshot" again, resulting in a full update.</p><p>

The beauty and elegance of the system resides in its simplicity. The same algorithm automatically:
</p></div><ul>
   <li>Generate partial or full update.</li>
   <li>Resend OLD information that were not received and NEW information in a single message.</li>
</ul>



<h3>Memory introspection with C</h3>
<div id="paperbox"><p>
You may wonder how Quake3 is comparing snapshots with introspection...since C does not have introspection.<br>
The answer is that each field locations for a <code>netField_t</code> is preconstructed via an array and some clever 
preprocessing directives:</p></div><pre>

    typedef struct {
        <span>char</span>    *name;
        <span>int</span>     offset;
        <span>int</span>     bits;
    } netField_t; 

    // using the stringizing operator to save typing...
    #define	NETF(x) #x,(<span>int</span>)&amp;((entityState_t*)0)-&gt;x

    netField_t	entityStateFields[] = 
    {
    { NETF(pos.trTime), <span>32</span> },
    { NETF(pos.trBase[0]), <span>0</span> },
    { NETF(pos.trBase[1]), <span>0</span> },
    ...
    }

</pre>
<div><p>
The full code of this part can be found in <a href="https://github.com/id-Software/Quake-III-Arena/blob/master/code/qcommon/msg.c">snapshot.c</a>'s 
<code>MSG_WriteDeltaEntity</code>. Quake3 does not even know what it is comparing: It just blindly follow <code>entityStateFields</code>'s index,offset and size...and
sends the difference over the network.

</p></div>





<h3>Pre-fragmentation</h3>
<p id="paperbox">
Digging into the code we see that the NetChannel module slices messages in chunks of 1400 bytes (<a href="https://github.com/id-Software/Quake-III-Arena/blob/master/code/qcommon/net_chan.c">
<code>Netchan_Transmit</code></a>), even though the maximum size of an UDP datagram is 65507 bytes. Doing so the engine avoid having its packets
fragmented by routers while traveling over the internet since most network MTU are 1500 bytes. Avoiding router fragmentation is very important since:<br>
</p><ul>
   <li>Upon entering the network the router must block the packet while it is fragmenting it.</li>
   <li>Upon leaving the network problems are even worse since every part of the datagram have to be waited on and then time-costly re-assembled.</li>
</ul>










<h3>Reliable and Unreliable messages</h3>
<div id="paperbox"><p>
If the snapshot system compensate for UDP datagrams lost over the network,  some messages and commands must be GUARANTEED to be delivered (when
a player quits or when the Server needs the Client to load a new level).</p><p>

	This guarantee is abstracted by the NetChannel: I wrote about it <a href="https://fabiensanglard.net/quakeSource/quakeSourceNetWork.php">a few years ago</a> (wow my drawings have come a long way !!).
</p></div>  






<h3>Recommended readings</h3>








<h3>Next part</h3>
<p id="paperbox">

<a href="https://fabiensanglard.net/quake3/qvm.php">The Virtual Machine system</a>
</p>

			
	<!-- <h2>Comments</h2>
<p> -->


     <!-- <div id="disqus_thread"></div> -->
    <!-- <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'fabiensanglardswebsite'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || 
                document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript> -->
    <!--<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->
<!--     




</p> -->

 
<p>@</p>

		</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MaXX Interactive Desktop -- the little brother of the great SGI Desktop on IRIX (107 pts)]]></title>
            <link>https://docs.maxxinteractive.com/</link>
            <guid>42218184</guid>
            <pubDate>Fri, 22 Nov 2024 23:21:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.maxxinteractive.com/">https://docs.maxxinteractive.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42218184">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">
        <main>
            <div dir="auto" page-display="34">

    <h2 id="bkmrk-page-title">Welcome to MaXX Interactive Desktop</h2>

    

            <blockquote id="bkmrk-new-version-of-maxx-">
<h3 id="bkmrk-%C2%A0-1"><strong><em>New Release Available Today<br>November&nbsp; 21st&nbsp; 9PM EST</em></strong></h3>
<h5 id="bkmrk-%3E%3E-link-to-release-n"><em><strong><a title="November 21st 2024 - Octane Alpha Release" href="https://docs.maxxinteractive.com/books/whats-new/page/november-21st-2024-octane-v220-alpha">&gt;&gt; Release information &lt;&lt;</a></strong></em></h5>
</blockquote>
<hr id="bkmrk-"><h5 id="bkmrk-yes-the-maxx-interac"><em><strong>MaXX Interactive Desktop is the little brother of the great SGI Desktop on IRIX<br></strong></em></h5>

<p id="bkmrk-maxx-desktop-v2.1.1-"><strong>MaXXdesktop v2.2.0 (alpha) running in Modern Look &amp; Feel with Indigo Magic SGI Scheme (4K @120Hz)</strong></p>

<h3 id="bkmrk-introduction">Introduction</h3>
<p id="bkmrk-the-maxx-interactive">The MaXX Interactive Desktop a.k.a. MaXX<strong>desktop</strong> is the continuation of the 5Dwm project released many years back. So don't be mistaken, there is only one real&nbsp; re-implementation of the IRIX Interactive Desktop found on SGI systems, and&nbsp; it's MaXX<strong>desktop</strong>!</p>
<p id="bkmrk-maxx-desktop-is-a-tr">MaXX<strong>desktop</strong> is a true re-implementation of the "SGI Desktop" with the added benefits of using a modern software stack of highly tuned loosely-coupled components delivering maximum performance and stability while using as little resources as possible. We believe that high performance&nbsp; computing and energy friendly are not mutually exclusive. What if you could run applications in a smarter, greener and sustainable way? MaXX<strong>desktop</strong> aims at improving just that, do more with less.</p>
<p id="bkmrk-while-respecting-the">While respecting the original retro CLASSIC SGI look and feel, which is very important for us to get it right,&nbsp; we created a newer and more modern user experience called the MODERN look that feels like a natural evolution of the original SGI look, as if SGI did it themselves throughout the years perfecting an already pretty awesome recipe. The MODERN look still supports SGI Colour Schemes but introduce Unicode and UTF-8 support, anti-aliased font rendering, more hardware acceleration and a new virtual-desktop manager. The user can switch between looks with one mouse-click, now that's cool.</p>
<h3 id="bkmrk-goals-and-features-l">Goals and Features List</h3>
<p id="bkmrk-so-there-is-no-surpr">We believe in the need of&nbsp; a highly tuned workstation environment where performance, robustness, low resources consumption are at the core of everything.</p>
<p id="bkmrk-here-are-the-goals-w">Here are the goals we want to achieve and features that makes a great modern desktop:</p>
<ul id="bkmrk-it%27s-learn%2C-very-fas"><li>Lean, very fast and robust. Basically MaXX<strong>desktop</strong> gets out of your way.</li>
<li>Smart and efficient multi-cores workload management with CPU and Core partitioning which help reduce process bouncing of&nbsp; CPU cores and computation resources allocation/partitioning translate in better throughput (<a title="MaXX Scope" href="https://docs.maxxinteractive.com/books/software-architecture/page/maxx-scope" target="_blank" rel="noopener">MaXXscope</a>).</li>
<li>Built on solid and time proven foundations.</li>
<li>Desktop Applications are distributed as self contained AppImages.</li>
<li>Provides a robust modern and high performance asynchronous multi-threaded messaging sub-system (with Shared Memory support and Zero copy principle for local communications) for fast and efficient inter-processes communications (<a title="MaXX Links" href="https://docs.maxxinteractive.com/books/software-architecture/page/maxx-links" target="_blank" rel="noopener">MaXXlinks</a>).</li>
<li>Based on a modular micro-services like architecture that allow decentralization of core desktop services as we rapidly moving toward containerization for better security.</li>
<li>Modern Configuration Management sub-system with both a command line interface (CLI) or C/C++/Java API for easy application integration (<a title="MaXX Settings" href="https://docs.maxxinteractive.com/books/software-architecture/page/maxx-settings" target="_blank" rel="noopener">MaXXsettings</a>).</li>
<li>Highly focused on facilitating quick and easy access to your content with fast content previewers right from the file-manager.</li>
<li>Leverage hardware acceleration and optimization for CPU&nbsp; and GPU.</li>
<li>Centralized system monitoring sub-system (<a title="MaXX Monitor" href="https://docs.maxxinteractive.com/books/software-architecture/page/maxx-monitor">MaXXmonitor</a>).</li>
<li>HiDPI supports for X/Motif applications and easy presets control for applications.</li>
<li>Much needed Motif<sup>TM</sup> face-lift and modernization of the ViewKit framework with new modern components that are fully integrated with all MaXX Desktop sub-systems.</li>
<li>Developer friendly software development environment with integration to <strong>CLion</strong>,<strong> Intelli-J</strong> and PyCharm IDEs from Jetbrains which translate into building better and faster applications.</li>
<li>To support multiple CPU architectures (x86, Arm64 and RISC-V)</li>
<li>To run on multiple OS: Linux, FreeBSD and Windows11 WSL2.</li>
<li>To provide a source code compatibility for IRIX visual applications.</li>
</ul><p id="bkmrk-more-info-on-the-ori"><a title="Not just a theme..." href="https://docs.maxxinteractive.com/books/misc/page/not-just-a-theme"><strong>&gt;&gt; More details on what MaXXdesktop is made of</strong></a></p>
<h3 id="bkmrk-our-mission">Our Mission</h3>
<p id="bkmrk-more-for-your-creati"><em>More for your creativity.</em></p>
<p id="bkmrk-our-goal-is-to-bring">Our mission is to bring back this great user experience which focused on performance, stability and productivity while sporting a smooth-clean-minimalism look and feel with low-resources consumption. A <strong>smart</strong> and <strong>green</strong> Desktop that puts the user's application in the forefront.&nbsp;&nbsp;</p>
<p id="bkmrk-we-believe-in-a-high">We believe in a High Performance Desktop Environment that provides the right set of tools to maximize creativity and productivity without sacrificing&nbsp; your system resources to some eye candy nonsense.&nbsp;<em> <strong>Again, less is more... And it keeps you focused.<br></strong></em></p>
<h3 id="bkmrk-from-the-ground-up">From the Ground Up</h3>
<p id="bkmrk-our-design-philosoph"><em>Our design philosophy is simple, do more with less...</em></p>
<p id="bkmrk-maxx-desktop-is-desi">The MaXX<strong>desktop</strong> is designed from the ground up for speed,&nbsp; fast/responsive, lightweight/simplicity over eye-candidness, but more importantly, to get the heck out of your way... The name <em>MaXX Interactive</em> doesn't mean maximum visual interaction, which are distractions or so what we call, UI noises. It's means maximum creativity with interactive assistance from the Desktop. In many ways, it's made for you and your brain so that it can relax, focus on let the creative juice flowing with far fewer distractions. We see desktop notifications in a very different way, but this is for another discussion. In short, the MaXX<strong>desktop</strong> let you focus on the creative&nbsp; tasks ahead without interference or visual distractions.<em><strong>&nbsp; <br></strong></em></p>
<h3 id="bkmrk-experience-matter">Experience Matters</h3>
<p id="bkmrk-finally%2C-our-team-me">Our team is sharing the same vision of making use of the right set of technologies with industry proven best practices and guidelines to build the right software, the right way.&nbsp; We aim at providing a consistent and pleasant user experience built on top of modern and stable foundations.&nbsp; Every good and useful piece of tech deserves to be future-proof and this is where several decades of experience in building battle-proven mission critical systems for high performance Enterprise class applications comes in. If it's architect-ed properly,&nbsp; it can evolve without breaking apart!</p>
<h3 id="bkmrk-is-maxx-desktop-for-">Is MaXX Desktop for you?</h3>
<p id="bkmrk-maxx-desktop%27s-typic">MaXX<strong>desktop</strong>'s typical users are old IRIX users/sysadmins, Computer Graphics Artists, Motion Pictures and Special Effects Studios, Software/Game Developers, Visualization/Simulation, Virtual Reality power-users or Oil and Gas research to name a few. MaXX<strong>desktop</strong> is also for anyone who wants get a break from all the surrounding noises and create stuff while sporting a very unique/cool daily driver.<strong> If it is the case, then MaXX is for you.</strong></p>
<hr id="bkmrk--1"><h3 id="bkmrk-navigation">Navigation</h3>
<p id="bkmrk-this-site-is-powered"><em>This site is powered by <a href="https://www.bookstackapp.com/" target="_blank" rel="noopener"><strong>BookStack</strong></a> (a type of Wiki engine) and you can navigate it by using the upper-right links 'Shelves and Books'.&nbsp; You may use the search bar on top or the convenient links below.</em></p>
<p id="bkmrk-the-maxx-desktop-tea">The MaXX<strong>desktop</strong> team</p>
    </div>
        </main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RGFW: Single-header C99 window abstraction library (125 pts)]]></title>
            <link>https://github.com/ColleagueRiley/RGFW</link>
            <guid>42217535</guid>
            <pubDate>Fri, 22 Nov 2024 21:31:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ColleagueRiley/RGFW">https://github.com/ColleagueRiley/RGFW</a>, See on <a href="https://news.ycombinator.com/item?id=42217535">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Riley's Graphics library FrameWork</h2><a id="user-content-rileys-graphics-library-framework" aria-label="Permalink: Riley's Graphics library FrameWork" href="#rileys-graphics-library-framework"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ColleagueRiley/RGFW/blob/main/logo.png?raw=true"><img src="https://github.com/ColleagueRiley/RGFW/raw/main/logo.png?raw=true" alt="THE RGFW Logo"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build statuses</h2><a id="user-content-build-statuses" aria-label="Permalink: Build statuses" href="#build-statuses"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ColleagueRiley/RGFW/actions/workflows/linux.yml/badge.svg"><img src="https://github.com/ColleagueRiley/RGFW/actions/workflows/linux.yml/badge.svg" alt="workflow"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/ColleagueRiley/RGFW/actions/workflows/windows.yml/badge.svg"><img src="https://github.com/ColleagueRiley/RGFW/actions/workflows/windows.yml/badge.svg" alt="workflow windows"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/ColleagueRiley/RGFW/actions/workflows/macos.yml/badge.svg"><img src="https://github.com/ColleagueRiley/RGFW/actions/workflows/macos.yml/badge.svg" alt="workflow macOS"></a></p>
<p dir="auto">A cross-platform lightweight single-header very simple-to-use window abstraction library for creating graphics Libraries or simple graphical programs. Written in pure C99.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">About</h2><a id="user-content-about" aria-label="Permalink: About" href="#about"></a></p>
<p dir="auto">RGFW is a free multi-platform single-header very simple-to-use window abstraction framework for creating graphics Libraries or simple graphical programs. it is meant to be used as a very small and flexible alternative library to GLFW.</p>
<p dir="auto">The window backend supports XLib (UNIX), Cocoas (MacOS), webASM (emscripten) and WinAPI (tested on windows <em>XP</em>, 10 and 11, and reactOS)<br>
Windows 95 &amp; 98 have also been tested with RGFW, although results are iffy</p>
<p dir="auto">Wayland: to compile wayland add (RGFW_WAYLAND=1). Wayland support is very experimental and broken.</p>
<p dir="auto">The graphics backend supports OpenGL (EGL, software, OSMesa, GLES), Vulkan, DirectX, <a href="https://github.com/RSGL/RGFW-Metal">Metal</a> and software rendering buffers.</p>
<p dir="auto">RGFW was designed as a backend for RSGL, but it can be used standalone or for other libraries, such as Raylib which uses it as an optional alternative backend.</p>
<p dir="auto">RGFW is multi-paradigm,<br>
By default RGFW uses a flexible event system, similar to that of SDL, however you can use callbacks if you prefer that method.</p>
<p dir="auto">This library</p>
<ol dir="auto">
<li>is single header and portable (written in C99 in mind)</li>
<li>is very small compared to other libraries</li>
<li>only depends on system API libraries, Winapi, X11, Cocoa</li>
<li>lets you create a window with a graphics context (OpenGL, Vulkan or DirectX) and manage the window and its events only with a few function calls</li>
</ol>
<p dir="auto">This library does not</p>
<ol dir="auto">
<li>Handle any rendering for you (other than creating your graphics context)</li>
<li>do anything above the bare minimum in terms of functionality</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Officially tested Platforms</h2><a id="user-content-officially-tested-platforms" aria-label="Permalink: Officially tested Platforms" href="#officially-tested-platforms"></a></p>
<ul dir="auto">
<li>Linux</li>
<li>Raspberry PI OS</li>
<li>Windows, (XP, Windows 10, 11, ReactOS)</li>
<li>MacOS, (10.13, 10.14, 14.5) (x86_64)</li>
<li>HTML5 (webasm / Emscripten)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported GUI libraries</h2><a id="user-content-supported-gui-libraries" aria-label="Permalink: Supported GUI libraries" href="#supported-gui-libraries"></a></p>
<p dir="auto">A list of GUI libraries that can be used with RGFW can be found on the RGFW wiki <a href="https://github.com/ColleagueRiley/RGFW/wiki/GUI-libraries-that-can-be-used-with-RGFW">here</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">examples</h2><a id="user-content-examples" aria-label="Permalink: examples" href="#examples"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ColleagueRiley/RGFW/blob/main/screenshot.PNG?raw=true"><img src="https://github.com/ColleagueRiley/RGFW/raw/main/screenshot.PNG?raw=true" alt="examples"></a></p>
<p dir="auto">The examples can also <a href="https://colleagueriley.github.io/RGFW/" rel="nofollow">run in your browser</a> with emscripten</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">compiling</h2><a id="user-content-compiling" aria-label="Permalink: compiling" href="#compiling"></a></p>
<p dir="auto">The examples can be compiled by using <code>make debug</code>, which compiles them in debug mode and then runs them<br>
or <code>make</code> which simply compiles them.</p>
<p dir="auto">The dx11 example has its own Makefile functions because it is Windows only, those include
<code>make DX11</code> and <code>make debugDX11</code></p>
<p dir="auto">You can do CC=<code>compiler</code> to specify a specific compiler<br>
Tested and supported compilers include, <code>gcc</code>, <code>clang</code>, <code>[x86_64 / i686-w64]-w64-mingw32-gcc</code>, <code>cl</code> (linux AND windows)</p>
<p dir="auto"><code>tcc</code> has also been tested but work on linux only</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">basic</h2><a id="user-content-basic" aria-label="Permalink: basic" href="#basic"></a></p>
<p dir="auto">A basic example can be found in <code>examples/basic</code>, it includes a basic OpenGL example of just about all of RGFW's functionalities.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">events</h2><a id="user-content-events" aria-label="Permalink: events" href="#events"></a></p>
<p dir="auto">The event example can be found in <code>examples/events</code>, it shows all the events and the data they send.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">callbacks</h2><a id="user-content-callbacks" aria-label="Permalink: callbacks" href="#callbacks"></a></p>
<p dir="auto">The callback example can be found in <code>examples/callbacks</code>, it shows all the events and the data they send, but processed with callbacks instead.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">dx11</h2><a id="user-content-dx11" aria-label="Permalink: dx11" href="#dx11"></a></p>
<p dir="auto"><code>examples/dx11</code> is a minimalistic example of the use of DirectX with RGFW</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">gl33</h2><a id="user-content-gl33" aria-label="Permalink: gl33" href="#gl33"></a></p>
<p dir="auto"><code>examples/gl33</code> is a minimalistic example of the use of OpenGL 3.3 with RGFW, this example was made by <a href="https://github.com/THISISAGOODNAME">AICDG</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">gles2</h2><a id="user-content-gles2" aria-label="Permalink: gles2" href="#gles2"></a></p>
<p dir="auto"><code>examples/gles2</code> is a minimalistic example of the use of OpenGL ES 2 with RGFW</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">vk10</h2><a id="user-content-vk10" aria-label="Permalink: vk10" href="#vk10"></a></p>
<p dir="auto"><code>examples/vk10</code> is a minimalistic example of the use of Vulkan with RGFW, this example was made by <a href="https://github.com/THISISAGOODNAME">AICDG</a></p>
<p dir="auto">It also includes <code>examples/vk10/RGFW_vulkan.h</code> which can be used to create a basic vulkan context for RGFW.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">basic</h2><a id="user-content-basic-1" aria-label="Permalink: basic" href="#basic-1"></a></p>
<p dir="auto">A basic example can be found in <code>examples/basic</code>, it includes a basic OpenGL example of just about all of RGFW's functionalities.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">buff</h2><a id="user-content-buff" aria-label="Permalink: buff" href="#buff"></a></p>
<p dir="auto"><code>examples/buffer</code> is an example that shows how you can use software rendering with RGFW using RGFW_BUFFER mode which allows you to render directly to the window's draw buffer.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">PortableGL</h2><a id="user-content-portablegl" aria-label="Permalink: PortableGL" href="#portablegl"></a></p>
<p dir="auto"><code>examples/PortableGL</code> is an example that shows how you'd use RGFW with <code>portablegl.h</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">first person camera</h2><a id="user-content-first-person-camera" aria-label="Permalink: first person camera" href="#first-person-camera"></a></p>
<p dir="auto"><code>examples/first-person-camera</code> is an example that shows how you'd make a game with a first person camera with RGFW</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">a very simple example</h2><a id="user-content-a-very-simple-example" aria-label="Permalink: a very simple example" href="#a-very-simple-example"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="#define RGFW_IMPLEMENTATION
#include &quot;RGFW.h&quot;

u8 icon[4 * 3 * 3] = {0xFF, 0x00, 0x00, 0xFF,    0xFF, 0x00, 0x00, 0xFF,     0xFF, 0x00, 0x00, 0xFF,   0xFF, 0x00, 0x00, 0x00, 0xFF, 0xFF, 0x00, 0xFF, 0xFF, 0xFF, 0x00, 0xFF,     0xFF, 0x00, 0x00, 0xFF, 0xFF, 0x00, 0x00, 0xFF, 0xFF, 0x00, 0x00, 0xFF};

void keyfunc(RGFW_window* win, u32 keycode, char keyName[16], u8 lockState, u8 pressed) {
    printf(&quot;this is probably early\n&quot;);
}

int main() {
    RGFW_window* win = RGFW_createWindow(&quot;name&quot;, RGFW_RECT(500, 500, 500, 500), (u64)RGFW_CENTER);

    RGFW_window_setIcon(win, icon, RGFW_AREA(3, 3), 4);
    
    RGFW_setKeyCallback(keyfunc); // you can use callbacks like this if you want 

    i32 running = 1;

    while (running) {
        while (RGFW_window_checkEvent(win)) { // or RGFW_window_checkEvents(); if you only want callbacks
            if (win->event.type == RGFW_quit || RGFW_isPressed(win, RGFW_Escape)) {
                running = 0;
                break;
            }

            if (win->event.type == RGFW_keyPressed) // this is the 'normal' way of handling an event
                printf(&quot;This is probably late\n&quot;);
        }
        
        glClearColor(0xFF / 255.0f, 0XFF / 255.0f, 0xFF / 255.0f, 0xFF / 255.0f);
        glClear(GL_COLOR_BUFFER_BIT);

        RGFW_window_swapBuffers(win);
    }

    RGFW_window_close(win);
}"><pre><span>#define</span> <span>RGFW_IMPLEMENTATION</span>
<span>#include</span> <span>"RGFW.h"</span>

<span>u8</span> <span>icon</span>[<span>4</span> <span>*</span> <span>3</span> <span>*</span> <span>3</span>] <span>=</span> {<span>0xFF</span>, <span>0x00</span>, <span>0x00</span>, <span>0xFF</span>,    <span>0xFF</span>, <span>0x00</span>, <span>0x00</span>, <span>0xFF</span>,     <span>0xFF</span>, <span>0x00</span>, <span>0x00</span>, <span>0xFF</span>,   <span>0xFF</span>, <span>0x00</span>, <span>0x00</span>, <span>0x00</span>, <span>0xFF</span>, <span>0xFF</span>, <span>0x00</span>, <span>0xFF</span>, <span>0xFF</span>, <span>0xFF</span>, <span>0x00</span>, <span>0xFF</span>,     <span>0xFF</span>, <span>0x00</span>, <span>0x00</span>, <span>0xFF</span>, <span>0xFF</span>, <span>0x00</span>, <span>0x00</span>, <span>0xFF</span>, <span>0xFF</span>, <span>0x00</span>, <span>0x00</span>, <span>0xFF</span>};

<span>void</span> <span>keyfunc</span>(<span>RGFW_window</span><span>*</span> <span>win</span>, <span>u32</span> <span>keycode</span>, <span>char</span> <span>keyName</span>[<span>16</span>], <span>u8</span> <span>lockState</span>, <span>u8</span> <span>pressed</span>) {
    <span>printf</span>(<span>"this is probably early\n"</span>);
}

<span>int</span> <span>main</span>() {
    <span>RGFW_window</span><span>*</span> <span>win</span> <span>=</span> <span>RGFW_createWindow</span>(<span>"name"</span>, <span>RGFW_RECT</span>(<span>500</span>, <span>500</span>, <span>500</span>, <span>500</span>), (<span>u64</span>)<span>RGFW_CENTER</span>);

    <span>RGFW_window_setIcon</span>(<span>win</span>, <span>icon</span>, <span>RGFW_AREA</span>(<span>3</span>, <span>3</span>), <span>4</span>);
    
    <span>RGFW_setKeyCallback</span>(<span>keyfunc</span>); <span>// you can use callbacks like this if you want </span>

    <span>i32</span> <span>running</span> <span>=</span> <span>1</span>;

    <span>while</span> (<span>running</span>) {
        <span>while</span> (<span>RGFW_window_checkEvent</span>(<span>win</span>)) { <span>// or RGFW_window_checkEvents(); if you only want callbacks</span>
            <span>if</span> (<span>win</span><span>-&gt;</span><span>event</span>.<span>type</span> <span>==</span> <span>RGFW_quit</span> <span>||</span> <span>RGFW_isPressed</span>(<span>win</span>, <span>RGFW_Escape</span>)) {
                <span>running</span> <span>=</span> <span>0</span>;
                <span>break</span>;
            }

            <span>if</span> (<span>win</span><span>-&gt;</span><span>event</span>.<span>type</span> <span>==</span> <span>RGFW_keyPressed</span>) <span>// this is the 'normal' way of handling an event</span>
                <span>printf</span>(<span>"This is probably late\n"</span>);
        }
        
        <span>glClearColor</span>(<span>0xFF</span> / <span>255.0f</span>, <span>0</span><span>XFF</span> / <span>255.0f</span>, <span>0xFF</span> / <span>255.0f</span>, <span>0xFF</span> / <span>255.0f</span>);
        <span>glClear</span>(<span>GL_COLOR_BUFFER_BIT</span>);

        <span>RGFW_window_swapBuffers</span>(<span>win</span>);
    }

    <span>RGFW_window_close</span>(<span>win</span>);
}</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="linux : gcc main.c -lX11 -lXcursor -lGL
windows : gcc main.c -lopengl32 -lshell32 -lgdi32
macos : gcc main.c -framework Foundation -framework AppKit -framework OpenGL -framework CoreVideo"><pre>linux <span>:</span> gcc main.c -lX11 -lXcursor -lGL
windows <span>:</span> gcc main.c -lopengl32 -lshell32 -lgdi32
macos <span>:</span> gcc main.c -framework Foundation -framework AppKit -framework OpenGL -framework CoreVideo</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">There is a lot of in-header-documentation, but more documentation can be found at <a href="https://colleagueriley.github.io/RGFW/docs/index.html" rel="nofollow">https://colleagueriley.github.io/RGFW/docs/index.html</a>
If you wish to build the documentation yourself, there is also a Doxygen file attached.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Bindings</h2><a id="user-content-bindings" aria-label="Permalink: Bindings" href="#bindings"></a></p>
<p dir="auto">A list of bindings can be found on the RGFW wiki <a href="https://github.com/ColleagueRiley/RGFW/wiki/Bindings">here</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">projects</h2><a id="user-content-projects" aria-label="Permalink: projects" href="#projects"></a></p>
<p dir="auto">A list of projects that use RGFW can be found on the RGFW wiki <a href="https://github.com/ColleagueRiley/RGFW/wiki/Projects-that-use-RGFW">here</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contacts</h2><a id="user-content-contacts" aria-label="Permalink: Contacts" href="#contacts"></a></p>
<ul dir="auto">
<li>email : <a href="mailto:ColleagueRiley@gmail.com">ColleagueRiley@gmail.com</a></li>
<li>discord : ColleagueRiley</li>
<li>discord server : <a href="https://discord.gg/pXVNgVVbvh" rel="nofollow">https://discord.gg/pXVNgVVbvh</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supporting RGFW</h2><a id="user-content-supporting-rgfw" aria-label="Permalink: Supporting RGFW" href="#supporting-rgfw"></a></p>
<p dir="auto">There is a RGFW wiki page about things you can do if you want to support the development of RGFW <a href="https://github.com/ColleagueRiley/RGFW/wiki/Supporting-RGFW">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">RGFW vs GLFW</h2><a id="user-content-rgfw-vs-glfw" aria-label="Permalink: RGFW vs GLFW" href="#rgfw-vs-glfw"></a></p>
<p dir="auto">A comparison of RGFW and GLFW can be found at <a href="https://github.com/ColleagueRiley/RGFW/wiki/RGFW-vs-GLFW">on the wiki</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">RGFW uses the Zlib/libPNG license, this means you can use RGFW freely as long as you do not claim you wrote this software, mark altered versions as such and keep the license included with the header.</p>
<div data-snippet-clipboard-copy-content="Permission is granted to anyone to use this software for any purpose,
including commercial applications, and to alter it and redistribute it
freely, subject to the following restrictions:
  
1. The origin of this software must not be misrepresented; you must not
   claim that you wrote the original software. If you use this software
   in a product, an acknowledgment in the product documentation would be
   appreciated but is not required. 
2. Altered source versions must be plainly marked as such, and must not be
   misrepresented as being the original software.
3. This notice may not be removed or altered from any source distribution."><pre><code>Permission is granted to anyone to use this software for any purpose,
including commercial applications, and to alter it and redistribute it
freely, subject to the following restrictions:
  
1. The origin of this software must not be misrepresented; you must not
   claim that you wrote the original software. If you use this software
   in a product, an acknowledgment in the product documentation would be
   appreciated but is not required. 
2. Altered source versions must be plainly marked as such, and must not be
   misrepresented as being the original software.
3. This notice may not be removed or altered from any source distribution.
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Understanding Google's Quantum Error Correction Breakthrough (151 pts)]]></title>
            <link>https://www.quantum-machines.co/blog/understanding-googles-quantum-error-correction-breakthrough/</link>
            <guid>42215910</guid>
            <pubDate>Fri, 22 Nov 2024 17:53:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantum-machines.co/blog/understanding-googles-quantum-error-correction-breakthrough/">https://www.quantum-machines.co/blog/understanding-googles-quantum-error-correction-breakthrough/</a>, See on <a href="https://news.ycombinator.com/item?id=42215910">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Imagine trying to balance thousands of spinning tops at the same time—each top representing a qubit, the fundamental building block of a quantum computer. Now imagine these tops are so sensitive that even a slight breeze, a tiny vibration, or a quick peek to see if they’re still spinning could make them wobble or fall. That’s the challenge of quantum computing: Qubits are incredibly fragile, and even the process of controlling or measuring them introduces errors.</p>
<p>This is where Quantum Error Correction (QEC) comes in. By combining multiple fragile physical qubits into a more robust logical qubit, QEC allows us to correct errors faster than they accumulate. The goal is to operate below a critical threshold—the point where adding more qubits reduces, rather than increases, errors. That’s precisely what <a href="https://arxiv.org/abs/2408.13687">Google Quantum AI has achieved with their recent breakthrough [1]</a>.</p>

<h2 aria-level="1"><strong>Google’s Breakthrough Achievement&nbsp;</strong></h2>
<p><span data-contrast="auto">To grasp the significance of Google’s result, let’s first understand what success in error correction looks like. In classical computers, error-resistant memory is achieved by duplicating bits to detect and correct errors. A method called majority voting is often used, where multiple copies of a bit are compared, and the majority value is taken as the correct bit. In quantum systems, physical qubits are combined to create logical qubits, where errors are corrected by monitoring correlations among qubits instead of directly observing the qubits themselves. It involves redundancy like majority voting, but does not rely on observation but rather entanglement. This indirect approach is crucial because directly measuring a qubit’s state would disrupt its quantum properties. Effective quantum error correction maintains the integrity of logical qubits, even when some physical qubits experience errors, making it essential for scalable quantum computing.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">However, this only works if the physical error rate is below a critical threshold. In fact, intuition says that increasing the number of physical qubits that make a logical qubit should allow for better error correction. In truth if each physical qubit is very error-prone, adding qubits makes errors accumulate faster than we can detect and correct them. In other words, quantum error correction works only if each qubit can operate below an error threshold even before any error correction. Having more physical qubits allows to increase the QEC code distance, which is a measure of a quantum code’s ability to detect and correct errors.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">By showing logical error decreased by a factor of 2.14 when increasing code distance from five to seven, Google has now demonstrated below-threshold operation using surface codes—a specific type of quantum error correction code.&nbsp; This reduction in errors (which is exponential with increasing code distance) is the smoking gun proving that their QEC strategy works. With this, Google could show that their logical qubit lasted more than twice as long as their best physical qubit, as shown in Figure 1, demonstrating that logical qubits didn’t just survive—they outperformed physical ones.</span><span data-ccp-props="{}">&nbsp;</span></p>

<div id="attachment_16103"><p><img decoding="async" aria-describedby="caption-attachment-16103" src="https://www.quantum-machines.co/wp-content/uploads/2024/11/Screenshot-2024-11-17-091724-1024x388.png" alt="An adapted plot showing logical qubit error rates versus code distance, highlighting exponential suppression of logical errors as the code distance increases." width="800" height="303" srcset="https://www.quantum-machines.co/wp-content/uploads/2024/11/Screenshot-2024-11-17-091724-1024x388.png 1024w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Screenshot-2024-11-17-091724-300x114.png 300w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Screenshot-2024-11-17-091724-768x291.png 768w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Screenshot-2024-11-17-091724.png 1470w" sizes="(max-width: 800px) 100vw, 800px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20303'%3E%3C/svg%3E" data-lazy-srcset="https://www.quantum-machines.co/wp-content/uploads/2024/11/Screenshot-2024-11-17-091724-1024x388.png 1024w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Screenshot-2024-11-17-091724-300x114.png 300w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Screenshot-2024-11-17-091724-768x291.png 768w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Screenshot-2024-11-17-091724.png 1470w" data-lazy-src="https://www.quantum-machines.co/wp-content/uploads/2024/11/Screenshot-2024-11-17-091724-1024x388.png"></p><p id="caption-attachment-16103">Fig. 1 – An adapted plot showing logical qubit error rates versus code distance, highlighting exponential suppression of logical errors as the code distance increases. The figure illustrates the transition to below-threshold performance and the “beyond break-even” behavior achieved with distance-7 codes. (Adapted from [1] by Google Quantum AI, CC BY 4.0)</p></div>
<p><span data-contrast="auto" xml:lang="EN-US" lang="EN-US"><span>A distance-7 surface code on 101 qubits effectively doubled the logical qubit’s lifetime</span><span> (blue line in Figure 1c)</span><span> compared to uncorrected physical qubits</span><span> (green line in Figure 1c)</span><span>. This accomplishment </span><span>demonstrates</span><span> that error-corrected qubits can preserve coherence </span><span>for longer periods, which is crucial for running extended quantum algorithms and computations. </span></span><span data-ccp-props="{}">&nbsp;</span></p>

<h2 aria-level="1"><span data-contrast="none">A Control Engineering Perspective: How Google Made It Work. </span><span data-ccp-props="{&quot;134245418&quot;:true,&quot;134245529&quot;:true,&quot;335559738&quot;:360,&quot;335559739&quot;:80}">&nbsp;</span></h2>
<p><span data-contrast="auto">The experiment wasn’t just a test of surface codes—it was a carefully orchestrated feat of engineering and control. The control system had to deliver flawless precision on multiple fronts—synchronization, frequency control, measurement fidelity, real-time decoding, and stability—over many hours of operation. Let’s stop for a second to talk about some of these interesting challenges.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">At the heart of the system was </span><b><span data-contrast="auto">real-time synchronization</span></b><span data-contrast="auto">. Every correction cycle had to complete within 1.1 µs—a narrow window in which the qubits were measured. The precision of this synchronization was critical to preventing errors from accumulating and destabilizing the computation. Achieving this required precise coordination of control signals across the qubit array, ensuring that every gate operation, measurement, was perfectly aligned.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">One of the most important components was </span><b><span data-contrast="auto">real-time decoding</span></b><span data-contrast="auto">. Decoding refers to the process of analyzing measurement data to determine where and how errors have occurred. To use logical qubits to perform universal quantum computation, certain gates called non-Clifford gates must be applied. Applying these gates, required correcting errors in real-time based on the real-time decoding. In Google’s system, the real-time decoder maintained a constant latency of about 63 µs while operating over one million correction cycles. Namely, the real-time error correction pipeline could process the measurements fast enough to avoid congestion. This rapid decoding process was essential, as any delay could allow errors to propagate and accumulate, potentially destabilizing the logical qubits. </span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">The experiment also demanded </span><b><span data-contrast="auto">high-fidelity gate operations</span></b><span data-contrast="auto">. Errors in qubit gates could easily propagate through the system, jeopardizing the stability of the logical qubit. Google achieved single-qubit gate errors below 0.1% and two-qubit CZ gate errors around 0.3%—thresholds essential to keeping logical qubits stable over time. For this goal, high performance of the control electronics is paramount, as fidelity can directly be impaired by errors of control pulses. These fidelities are especially critical when scaling surface codes, where even minor gate errors could degrade the effectiveness of error correction.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">As quantum computers scale to more qubits and longer computations, these and more control requirements will only grow more demanding, making the development of advanced control hardware essential for the future of fault-tolerant quantum computing.&nbsp;</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">Out of the requirements above, real-time decoding, in particular, is fundamental for any scalable quantum computing system, as it provides the rapid response required to keep quantum information stable.</span><span data-ccp-props="{}">&nbsp;</span></p>

<h2 aria-level="1"><span data-contrast="none">A deeper dive into real-time decoding&nbsp;</span><span data-ccp-props="{&quot;134245418&quot;:true,&quot;134245529&quot;:true,&quot;335559738&quot;:360,&quot;335559739&quot;:80}">&nbsp;</span></h2>
<p><span data-contrast="auto">Google’s work highlights that the feasibility of the decoding depends on the decoder latency and throughput, as one of the most important pieces for running QEC below threshold.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">Decoding is a classical compute task, and it can be done effectively on various classical architectures, such as FPGAs or GPUs. However, there is usually a trade-off between computational resources. FPGAs for example, are limited in computing power, but operate deterministically and in strict timing, making them suitable to manage the qubit control and measurement tasks as well as perform dedicated classical computations with low latency. On the other hand, CPUs or GPUs might have increased latency but enable far more advanced and larger computation. At Quantum Machines, </span><a href="https://www.quantum-machines.co/blog/quantum-machines-announces-deep-quantum-classical-integration-to-power-quantum-accelerated-supercomputers-with-nvidia/"><span data-contrast="none">we partnered with NVIDIA</span></a><span data-contrast="auto"> to deliver a unique platform, called DGX Quantum, that provides a unique combination of ultra-low controller-decoder latency, high-performance computational power, and flexible SW programmability. Our platform, which includes a less than 4 µs communication between our controller, OPX1000 and the CPU/GPU, allows to easily program and execute QEC workflows, including real-time decoding such as Google’s decoding. The SW programmability allows iterating over the decoding algorithm and scheme very quickly. A feature we believe is key for faster progress towards scalable and effective QEC. The truth is that a lot more experimentation and benchmarking is needed to learn what decoders to use, which classical resources optimize performance and meet requirements and how to design systems that can eventually run QEC on a much larger scale. What we know so far is that the latency of decoders should be less than 10 µs for QEC schemes to converge. </span><a href="https://qm.quantum-machines.co/factoring21"><span data-contrast="none">Watch our CEO Itamar Sivan explaining this further</span></a><span data-contrast="auto"> with the example of Shor’s algorithm for factorizing the number 21. </span><span><br>
</span><span><br>
</span><span data-contrast="auto">DGX-quantum is already live, showcasing less than 4 µs round-trip latency between controller and GPU. To learn more, <a href="https://www.quantum-machines.co/resources/tutorials/tightly-integrating-gpus-and-qpus-for-quantum-error-correction-and-optimal-control-part-1/">watch the IEEE QCE 2024 tutorial below</a>, on DGX-quantum, co-authored by QM and NVIDIA.</span><span data-ccp-props="{}">&nbsp;</span></p>

<div id="attachment_16105"><p><a href="https://www.quantum-machines.co/resources/tutorials/tightly-integrating-gpus-and-qpus-for-quantum-error-correction-and-optimal-control-part-1/"><img decoding="async" aria-describedby="caption-attachment-16105" src="https://www.quantum-machines.co/wp-content/uploads/2024/11/Link-video-icon-1024x563.png" alt="Video tutorial: Tightly integrating GPUs and QPUs for Quantum Error Correction and Optimal Control." width="800" height="440" srcset="https://www.quantum-machines.co/wp-content/uploads/2024/11/Link-video-icon-1024x563.png 1024w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Link-video-icon-300x165.png 300w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Link-video-icon-768x422.png 768w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Link-video-icon-1536x844.png 1536w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Link-video-icon-2048x1126.png 2048w" sizes="(max-width: 800px) 100vw, 800px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20440'%3E%3C/svg%3E" data-lazy-srcset="https://www.quantum-machines.co/wp-content/uploads/2024/11/Link-video-icon-1024x563.png 1024w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Link-video-icon-300x165.png 300w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Link-video-icon-768x422.png 768w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Link-video-icon-1536x844.png 1536w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Link-video-icon-2048x1126.png 2048w" data-lazy-src="https://www.quantum-machines.co/wp-content/uploads/2024/11/Link-video-icon-1024x563.png"></a></p><p id="caption-attachment-16105">Video tutorial: Tightly integrating GPUs and QPUs for Quantum Error Correction and Optimal Control.</p></div>

<h2 aria-level="1"><span data-contrast="none">So, what’s next?&nbsp;</span><span data-ccp-props="{&quot;134245418&quot;:true,&quot;134245529&quot;:true,&quot;335559738&quot;:360,&quot;335559739&quot;:80}">&nbsp;</span></h2>
<p><span data-contrast="auto">Google’s demonstration of below-threshold quantum error correction marks a milestone towards fault-tolerant quantum computing. By demonstrating that logical qubits can outperform physical qubits and showing that errors can be corrected faster than they accumulate, they’ve paved the way for scalable quantum processors.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">However, this is just the beginning. In the future, to perform universal quantum computation with error corrected logical qubits, the full feedback loop must be closed, meaning that the control system needs to make decisions in real-time based on the decoder computation. Future developments will require faster decoders, better error mitigation strategies, automated calibrations embedded within&nbsp;quantum programs to stabilize parameters, and control hardware that tightly integrates and manages classical and quantum workflows.&nbsp;</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">Google’s achievement signifies a substantial step toward fault-tolerant quantum computing. By demonstrating that logical error rates can be exponentially suppressed through the use of surface codes, the work provides a scalable and practical pathway to reliable quantum computing. As code distance increases, errors decrease at a rapid rate, setting the stage for quantum processors capable of handling complex operations with higher fidelity. Furthermore, this implementation of&nbsp;fast&nbsp;decoding represents a fundamental advancement in QEC. This technique allows for correction of errors faster than their propagation, minimizing the chance for errors to propagate through the quantum system.</span><span data-ccp-props="{&quot;134233117&quot;:false,&quot;134233118&quot;:false,&quot;201341983&quot;:0,&quot;335551550&quot;:1,&quot;335551620&quot;:1,&quot;335559685&quot;:0,&quot;335559737&quot;:0,&quot;335559738&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p>

<h2 aria-level="1"><span data-contrast="none">Quantum Error Correction and the Vision for Fault Tolerance</span><span data-ccp-props="{&quot;134245418&quot;:true,&quot;134245529&quot;:true,&quot;335559738&quot;:360,&quot;335559739&quot;:80}">&nbsp;</span></h2>
<p><span data-contrast="auto">Real-time, low-latency feedback loops are going to be an essential element of future fault tolerant quantum devices, to ensure that errors are corrected faster than they accumulate. This principle resonates across the broader quantum computing community, where rapid and robust control mechanisms are viewed as the key to achieving large-scale, reliable quantum operations.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">By focusing on low-latency, high-fidelity feedback and decoding, the broader quantum technology field is advancing toward the shared goal of fault-tolerant quantum computing, just as Google’s milestone achievement shows. The evolution of quantum control systems that support agile error correction and real-time adaptability will continue to play a central role in the pursuit of stable, scalable quantum computing systems that can be deployed in practical applications. And with DGX-quantum, we are just starting this exciting journey, so stay tuned for what’s to come! </span><span data-contrast="auto">​</span><span data-ccp-props="{}">&nbsp;</span></p>

<div id="attachment_16104"><p><img decoding="async" aria-describedby="caption-attachment-16104" src="https://www.quantum-machines.co/wp-content/uploads/2024/11/Picture1.png" alt="The DGX-Quantum solution, co-developed by NVIDIA and Quantum Machines, enables quantum error correction (QEC), calibration, and fast retuning for large-scale quantum computers. It leverages classical resources (GPUs and CPUs) for quantum computing, with ultra-fast data round-trip delays of under 4 microseconds." width="800" height="306" srcset="https://www.quantum-machines.co/wp-content/uploads/2024/11/Picture1.png 602w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Picture1-300x115.png 300w" sizes="(max-width: 800px) 100vw, 800px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20306'%3E%3C/svg%3E" data-lazy-srcset="https://www.quantum-machines.co/wp-content/uploads/2024/11/Picture1.png 602w, https://www.quantum-machines.co/wp-content/uploads/2024/11/Picture1-300x115.png 300w" data-lazy-src="https://www.quantum-machines.co/wp-content/uploads/2024/11/Picture1.png"></p><p id="caption-attachment-16104">The DGX Quantum solution, co-developed by NVIDIA and Quantum Machines, enables quantum error correction, calibration, and fast retuning for large-scale quantum computers. It allows the use of robust classical resources (GPUs and CPUs) for quantum computer operation, with ultra-fast data round-trip delays of under 4 µs.</p></div>

<h2><strong>Reference</strong></h2>
<p>[1] Acharya, Rajeev, et al. <a href="https://arxiv.org/abs/2408.13687">“Quantum error correction below the surface code threshold.” arXiv preprint arXiv:2408.13687</a> (2024).</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Phased Array Microphone (2023) (501 pts)]]></title>
            <link>https://benwang.dev/2023/02/26/Phased-Array-Microphone.html</link>
            <guid>42215552</guid>
            <pubDate>Fri, 22 Nov 2024 17:10:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://benwang.dev/2023/02/26/Phased-Array-Microphone.html">https://benwang.dev/2023/02/26/Phased-Array-Microphone.html</a>, See on <a href="https://news.ycombinator.com/item?id=42215552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article>

  

  <div>
    <p>A 192-channel phased array microphone, with FPGA data acquisition and beamforming/visualization on the GPU.
Phased arrays allow for applications not possible with traditional directional microphones, as the directionality
can be changed instantly, after the recording is made, or even be focused at hundreds of thousands of points 
simultaneously in real time.</p>

<p>All designs are open source:</p>

<ul>
  <li><a href="https://github.com/kingoflolz/mic_host">Host software</a></li>
  <li><a href="https://github.com/kingoflolz/mic_gateware">FPGA gateware</a></li>
  <li><a href="https://github.com/kingoflolz/mic_hardware">PCB layout and schematics, mechanical components</a></li>
</ul>

<p><img src="https://benwang.dev/assets/mic%20block%20diagram.png" alt=""></p>
<p> Block diagram </p>

<p><img src="https://benwang.dev/assets/mic%20overall.jpg" alt=""></p>
<p> Glamor shot </p>

<h2 id="hardware">Hardware</h2>

<p>To create a phased array microphone, a large number of microphones needs to be placed in an arrangement with
 a wide distribution of spacing. For a linear array, exponential spacing between microphones is found to be optimal for
 broadband signals. To create a 2d array, these symmetrical linear arrays (“arms”) are be placed radially, which allows
the central (“hub”) board to be compact. The total cost for the array is approximately $700.</p>

<h3 id="arms">Arms</h3>

<p>The length of each arm is dictated by the limits of PCB manufacturing and assembly. These boards were made at JLCPCB,
where the maximum length for manufacturing and assembly of 4 layer PCBs was 570mm.</p>

<p>The microphones chosen were the <a href="https://www.lcsc.com/product-detail/MEMS-Microphones_MEMS-MSM261D4030H1CPM_C966942.html">cheapest digital output MEMS microphone</a>
(because there are a lot of them!), which were about $0.5. At this bottom of the barrel price
range, there is little differentiation in the performance characteristics between different microphones. Most have 
decent performance up to 10khz and unspecified matching of phase delay and volume.</p>

<p>These microphones output data using pulse density modulation (PDM), which provides a one bit output at a frequency
significantly higher than the audible range (up to 4 MHz), with the high sampling rate compensating for quantization noise. 
These microphones also support latching the data either on the rising or falling edge of the clock (DDR), which allows
two microphones to be multiplexed on a single wire, reducing the amount of connections required.</p>

<p>Each arm contains 8 microphones sharing 4 output lines, as well as an output buffer on the clock input line.
This ensures the rise times are reasonable, even with hundreds of microphones sharing the same clock signal.</p>

<p>For some reason (likely the low rigidity of the panel and some suboptimal solder paste stencil patterns combined with 
the LGA microphone footprints), the yields on the arm PCBs are not very good, with only 50% of them working out of the 
box. The most common fault was the clock line being shorted to either 3V3 or ground, which unfortunately requires trial
and error of removing microphones from the PCB until the short is resolved. Next time some series resistors on the
clock line would speed this process up a lot, and improving the panelization and paste stencil would likely 
improve yields so extensive rework isn’t required.</p>

<p>Even with rework, there are still some microphones which produce bogus data. These are just masked out in the code, as
there are enough working ones to make up for it (and it’s too much work to remove a bunch of arms to do more rework…)</p>

<p><img src="https://benwang.dev/assets/mic%20arm%20panel.png" alt=""></p>

<h3 id="hub">Hub</h3>

<p>An FPGA is used to collect all the data, due to the large number of low latency IOs available combined with the ability 
to communicate using high speed interfaces (e.g. Gigabit Ethernet). Specifically, the <a href="https://www.colorlight-led.com/product/colorlight-i5-led-display-receiver-card.html">Colorlight i5</a>
card is used, as it has enough IOs, is cheap and readily available, and has two integrated ethernet PHYs 
(only one is used for this project). The card is originally designed as an ethernet interface for LED panels, but has 
been <a href="https://github.com/wuxx/Colorlight-FPGA-Projects">fully reverse engineered</a>. About 100 GPIOs are broken out over 
the DDR2 connector, which is much easier to fan out than the BGA of the original FPGA.</p>

<p><img src="https://benwang.dev/assets/mic%20hub%20board.png" alt=""></p>

<p>Other than the FPGA, the hub contains some simple power management circuitry, and connectors for the arm boards as well
as an Ethernet connector with integrated magnetics.</p>

<h3 id="mechanical-design">Mechanical Design</h3>

<p>The arms are attached with M3 screws to the hub using <a href="https://www.lcsc.com/product-detail/Nuts_Sinhoo-SMTSO3080CTJ_C2916369.html">PCB mounted standoffs/nuts</a> 
, which conveniently can be assembled with SMD processes. The connections from each arm to the hub is made with 8 pin,
2mm pitch connectors.</p>

<p><img src="https://benwang.dev/assets/mic%20hub%20attachment.png" alt=""></p>

<p>The original mechanical design consists of slots on the arm PCBs which interlock with circumferential structural PCBs,
however the low torsional rigidity of the arms means the whole structure deformed too easily.</p>

<p><img src="https://benwang.dev/assets/mic%20structural%20pcb.png" alt=""></p>

<p>The final mechanical design consists of pieces of laser cut 1/4th inch MDF around the outer edge of the array, with each
arm attached to the MDF with some zip ties.</p>

<p><img src="https://benwang.dev/assets/mic%20arm%20attachment.jpg" alt=""></p>

<p>As the microphone array is mounted on the wall (which is very susceptible to reflections), a layer of acoustic foam is
used to attenuate the reflections to make calibration easier.</p>

<h2 id="gateware">Gateware</h2>

<p>The main objective for the gateware is to reliably transmit the raw acquired data losslessly to the computer for 
further processing, while keeping it as simple as possible. Performing decimation and filtering on the
FPGA would reduce the data rate, but sending the raw PDM data is achievable with Gigabit Ethernet. This
reduces the complexity of the FPGA code and allowing faster iteration. Compiling is much faster than place and route,
and it’s much easier to use a debugger in code than in gateware!</p>

<p>There are three major components to the gateware, a module for interfacing with the PDM interfaces, a module for 
creating fixed size packets from those readings, and a UDP streamer to write the packets to the Ethernet interface.</p>

<h3 id="pdm-interface">PDM Interface</h3>

<p>The PDM input module is a relatively simple piece of logic, which divides the 50 MHz system clock by a factor of 16 to 
output a 3.125MHz PDM clock, latches all 96 of the input pins after each clock edge, and then shifts out 32 bits of the 
data on each clock cycle. Each chunk of 192 bits is has a header added which is a 32 bit incrementing integer.</p>

<p>The PDM interface receives data at a rate of 3.125Mhz * 96 (input pins) * 2 (DDR), which is 600Mbps. With the header,
the data rate output from this module is 700Mbps, or approximately 40% utilization of the 32 bit output data path.</p>

<h3 id="packetizer">Packetizer</h3>

<p>The packetizer is essentially a FIFO buffer with a special interface on the input. A standard FIFO marks the output as available
whenever there is at least one item in the queue, but this would lead to smaller packets than requested as the ethernet
interface operates faster than the PDM output (leading to buffer underruns).
Thus, the packetizer waits until there is at least a full packet worth of 
data in the queue before starting a packet, which ensures constant sized packets.</p>

<p>48 PDM output blocks at 224 bits (192 bits of data with a 32 bit header) are placed into each packet, which totals
1344 bytes of data per packet, plus a 20 byte IPv4 header and an 8 byte UDP header, at a rate of approximately 65k pps.</p>

<p>This leads to a wire rate of 715 Mbps, or about 70% utilization of Gigabit Ethernet.</p>

<h3 id="udp-streamer">UDP Streamer</h3>

<p>The LiteEth project made this module very easy, as it abstracts out the lower level complexities of UDP and IP 
encapsulation, ARP tables and the like, and provides a convenient interface for simply hooking up a FIFO to a UDP 
stream. Occasionally there is some latency, but there is enough slack in the bus and buffer in the
packetizer FIFO to absorb any hiccups.</p>

<h3 id="utilization-and-timing">Utilization and Timing</h3>

<p>The FPGA on the Colorlight i5 is a <code>LFE5U-25F-6BG381C</code>, which has 25k LUTs. The design is 
placed and routed with the open source Project Trellis toolchain. By keeping the gateware very simple, 
the utilization on the device is quite low, and there is lots of room for additional functionality.</p>

<div><pre><code>Info: Device utilisation:
Info:                 DP16KD:    16/   56    28%
Info:                EHXPLLL:     1/    2    50%
Info:             TRELLIS_FF:  1950/24288     8%
Info:           TRELLIS_COMB:  3701/24288    15%
Info:           TRELLIS_RAMW:    49/ 3036     1%

Info: Max frequency for clock                   '$glbnet$crg_clkout': 73.17 MHz (PASS at 50.00 MHz)
Warning: Max frequency for clock '$glbnet$eth_clocks1_rx$TRELLIS_IO_IN': 124.07 MHz (FAIL at 125.00 MHz)
</code></pre></div>

<p>(Timing violations on eth rx clock is due to <a href="https://github.com/litex-hub/litex-boards/issues/40#issuecomment-1108817182">false positive from gray counter in liteeth</a>)</p>

<h2 id="software">Software</h2>

<h3 id="cic-filter">CIC Filter</h3>

<p>Each microphone produces a 1 bit signal at 3.125Mhz, and needs to be reduced to a more reasonable sample rate
and bit depth for further
processing. This is done very efficiently with a CIC filter, which only requires a few arithmetic operations to 
process each sample. For understanding more about CIC filters, <a href="https://tomverbeure.github.io/2020/09/30/Moving-Average-and-CIC-Filters.html">this series</a>
of blog posts from Tom Verbeure provides an excellent introduction. Following the nice graphs from there, 
I decided on a 4 stage, 16x decimation CIC filter which reduced the sample rate to a much reasonable 195kHz, at 32 bits.</p>

<p>To ingest the data at 3.125Mhz, the filter must be able to process each set of samples in 320ns. A naive implementation
in Rust wasn’t fast enough on a single core, but an implementation with some less abstraction (and a hence some more 
autovectorization) got there, and is what was used at the end. I also experimented with a version using SIMD 
intrinsics which was much faster, but ended up running into alignment issues when using it in together with other code.</p>

<p>Even with close to a billion bits per second of data to process, a single CPU core can do quite a few operations on 
each individual bit!</p>

<div><pre><code>test cic::bench_cic       ... bench: 574 ns/iter (+/- 79) = 41 MB/s
test cic::bench_fast_cic  ... bench: 181 ns/iter (+/- 24) = 132 MB/s
test cic::bench_simd_cic  ... bench:  36 ns/iter (+/- 0)  = 666 MB/s
</code></pre></div>

<h3 id="calibration">Calibration</h3>

<p>To perform array calibration, a speaker playing white noise is moved around the room in front of the array. An FFT based
cross correlation is performed between all pairs of microphones to compute relative delays.</p>

<p>A cross correlation can be
performed by computing the FFT of both signals (cached and computed once for each signal), and then computing the 
inverse FFT of the complex multiplication of the two. This is quite compute intensive, as there are over 18 thousand
pairs! For the window sizes used of 16-64k, the FFTs are memory bound, and thus the IFFT and peak finding is fused to
avoid writing the results to memory, which results in a 15x speedup. On a 7950X, this process runs in realtime.</p>

<p>Then the positions of the source at each timestep 
and the positions of each microphone is optimized using gradient descent (when you know PyTorch, all optimization 
problems look like gradient descent…). The loss function tries to minimize the difference between the measured
correlations and the ideal correlations, while trying to minimize the deviation of the microphone positions from
the initial positions as well as the jerk of the source trajectory.</p>

<p>As part of the calibration, the speed of sound is also a parameter which is optimized to obtain the best model of the
system, which allows this whole procedure to act as a ridiculously overengineered thermometer.</p>

<p>After a few hundred iterations, it converges to a reasonable solution for both
the source positions and the microphone positions, as well as constants such as the speed of sound. Fortunately this
problem vectorizes well for GPU, and converges in a few seconds.</p>

<p>The final mean position error is on the order of 1mm, and is able to correct for large scale systematic distortions
such as concavity from the lack of structural rigidity. The largest position error between the calibrated positions
and the designed positions is on the order of 5mm, which is enough to introduce significant phase errors to high 
frequency sound if uncorrected, although perhaps not strictly necessary (10khz sound has a wavelength of ~3.4cm).</p>

<p><img src="https://benwang.dev/assets/mic%20calibration.png" alt=""></p>

<h3 id="beamforming">Beamforming</h3>

<p>Beamforming is how the raw microphone inputs are processed to produce directional responses. The simplest method of
beamforming is delay-and-sum (DAS), where each signal is delayed according to its distance from the source. This is 
the type of beamforming implemented for this process, with the beamforming happening in the frequency domain.</p>

<p>In the frequency domain, a delay can be implemented by the complex multiplication of the signal with a linear phase term
proportional to the delay required, which also nicely handles delays which are not integer multiples of the sampling 
period.</p>

<p>Multiple nested subarrays of the original array are used for different frequency ranges. This reduces the processing
required for beamforming, as each frequency does not need to be beamformed with all the microphone. This also ensures
that the beamforming gains of all the frequencies are matched.</p>

<p><img src="https://benwang.dev/assets/mic%20subarray.png" alt=""></p>

<p>Two different types of beamforming visualizations are implemented, a 3d near field beamformer and a 2d far field
beamformer. When the audio source is far away, the wavefront is essentially a flat plane, and how far away the
source is does not meaningfully change the signals at the array. On the other hand, if the source is close to the 
array, the wavefront will have a significant curvature which allows the 3d location of the source to be determined.</p>

<p>The beamformer is implemented as a <a href="https://github.com/openai/triton/">Triton kernel</a>, a Python DSL which compiles to 
run on Nvidia GPUs. When beamforming to hundreds of thousands of points, the massive parallelism provided
by GPUs allows for results to be produced in real time. Some <a href="https://github.com/openai/triton/issues/974">current limitations</a>
with the Triton language around 
support to indexing with shared memory arrays lead to slightly suboptimal performance, but writing CUDA C++ doesn’t 
seem very fun…</p>

<h4 id="near-field-3d-beamforming">Near Field 3D Beamforming</h4>

<p>Near field 3D beamforming is performed a 5cm voxel grid with size 64x64x64. An update rate of 12hz is achieved on a
RTX 4090 with higher update rates limited by overhead of suboptimal CPU-GPU synchronization with the smaller work units. 
The voxel grid is then visualized using <a href="https://vispy.org/">VisPy</a>,
a high performance data visualization library which uses OpenGL. Modern games have millions of polygons, so rendering
a quarter million semi-transparent voxels at interactive framerates is no issue.</p>

<p>A quick demo of the voxel visualization below, note the reflection off the roof!</p>

<video controls="">
<source src="https://benwang.dev/assets/mic%203d%20demo.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>

<h4 id="far-field-2d-beamforming">Far Field 2D Beamforming</h4>

<p>Far field beamforming works similarly, but can be performed in higher resolution as there is no depth dimension
required. A 512x512 pixel grid is used, and the same 12hz update rate achieved. (The far field beamforming uses an 
approximation of just putting the points far away instead of actually assuming planar wavefront due to laziness…)</p>

<p>A demo of the 2d visualization here, but it’s not very exciting due to the poor acoustic environment of my room around 
the array, with lots of reflections and multipath.</p>

<video controls="">
<source src="https://benwang.dev/assets/mic%202d%20demo.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>

<h4 id="directional-audio">Directional Audio</h4>

<p>The previous two beamforming implementations compute the energy of sound from each location, but never materializes
the beamformed audio in memory. A time domain delay and sum beamformer is implemented to allow for directional audio 
recording. It takes a 3D coordinate relative from array center and outputs audio samples. 
An interesting aspect about this beamformer is that it is differentiable with regard to the location from the output. 
This means the location of the audio sources can be optimized
based on some differentiable loss function (like neural network), which might allow for some interesting applications 
such as using a forced alignment model of a multi-party transcript to determine the physical location of each speaker.</p>

<p>A speaker playing some audio is placed in front of the array, with another speaker placed approximately 45 degrees away
at the same distance from array center, playing white noise. The effectiveness of the beamforming can be demonstrated
by comparing the raw audio from a single microphone with the output from the beamforming.</p>

<p>Raw audio from a single microphone:</p>

<p><audio controls="">
<source src="https://benwang.dev/assets/mic%20raw.wav" type="audio/wav">
Your browser does not support the video tag.
</audio></p><p>Beamformed audio:</p>

<p><audio controls="">
<source src="https://benwang.dev/assets/mic%20beamformed.wav" type="audio/wav">
Your browser does not support the video tag.
</audio></p><h3 id="recording">Recording</h3>

<p>As the data from the microphone array is just UDP packets, it can be recorded with tools like <code>tcpdump</code>, and the 
packet capture file can be read to reinject the packets back into the listener. All the programs in the
previous sections are designed to work at real time, but can also work on recorded data using this process.</p>

<p>The tradeoff with this recording implementation is that the output data rate is quite high (due to faithfully recording
everything, even the quantization noise). At 87.5 MBps, a 1-hour recording would be 315 GB! A more optimized
implementation would do some compression, and do the recording after the CIC filter at a lower sample rate.</p>

<h2 id="next-steps">Next Steps</h2>

<p>I consider this project essentially complete, and don’t plan to work on it any further for the foreseeable future, but
there are still lots of possible cool extensions if you’d like to build one!</p>
<ul>
  <li>Using more advanced beamforming algorithms (<a href="https://ntrs.nasa.gov/api/citations/20080015889/downloads/20080015889.pdf">DAMAS</a> etc.)</li>
  <li>Better GUI to combine all existing functions (e.g. See where sound is coming from, and record audio from there)</li>
  <li>Combine differentiable beamforming and neural models (e.g. forced alignment example mentioned above)</li>
</ul>

  </div>

</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rendering "modern" Winamp skins in the browser (104 pts)]]></title>
            <link>https://jordaneldredge.com/notes/webamp-modern/</link>
            <guid>42215438</guid>
            <pubDate>Fri, 22 Nov 2024 16:58:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jordaneldredge.com/notes/webamp-modern/">https://jordaneldredge.com/notes/webamp-modern/</a>, See on <a href="https://news.ycombinator.com/item?id=42215438">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p><time datetime="2024-11-20T00:34:00.000Z">Nov 19, 2024</time></p><p><img src="https://jordaneldredge.com/notion-mirror/84ebb48c-616a-4f51-ae9a-991a4e0a7e9b/8b644494-12b4-4aa3-8e6f-fd02f95341bd/Screenshot_2024-11-19_at_4.39.04_PM.png" alt=""></p><p><em>Discussed on</em> <a href="https://news.ycombinator.com/item?id=42215438"><em>Hacker News</em></a></p><hr><p><em>TL;DR several years ago I got a proof of concept working where I was able to render highly interactive “modern” Winamp skins in the browser by reverse engineering Maki byte code and implementing an interpreter for it in JavaScript. You can try the</em> <a href="https://webamp.org/modern/"><em>proof of concept</em></a> <em>in your browser.</em></p><hr><p>One of the most rewarding projects I’ve worked on was <a href="https://webamp.org/">Webamp</a>. Seeing classic Winamp skins come to life in my browser via code I wrote was intoxicating and eventually inspired me to create the <a href="https://jordaneldredge.com/blog/winamp-skin-musuem/">Winamp Skin Museum</a>. But Webamp “just” implements “classic” Winamp skins, which were basically a glorified set of sprite sheets. They could change the appearance of the player but not the layout, and they could not add any custom interactions.</p><p>However, after <a href="https://jordaneldredge.com/notes/winamp-4-skin/">skipping Winamp 4</a>, Winamp 5 introduced a new, dramatically more powerful, skinning engine. The new engine was powered by XML files describing the UI, which was made interactive via skinner defined scripts written in a bespoke language called MAKI (”Make a killer interface”). Together, XML and Maki worked much like HTML and JavaScript. They enabled “skinners” to create highly dynamic UIs. This included interfaces with custom animations, interactive elements, and more.</p><p>After tackling classic Winamp skins it was only natural that I should be curious about modern skins, and I was! Could I get modern skins to run in the browser?</p><p>After reading up on how these skins were implemented, I learned that the modern skins were distributed as <code>.zip</code> files with the extension <code>.wal</code> that consisted of <code>.xml</code> and <code>.maki</code> files along with images. The <code>.maki</code> scripting files contained a custom compiled bytecode. Some skins also included the source <code>.m</code> files, but not all. The skin authors had been required to compile their skins before uploading them. If I was going to render these skins in the browser, I was going to need to understand the bytecode.</p><h2>Reverse engineering Maki</h2><p>At this point I was new to the concepts of byte code, interpreters <em>and</em> reverse engineering, so I needed some help. Luckily, I found an ancient <a href="https://web.archive.org/web/20180627114343/http://www.rengels.de/maki_decompiler/doc.html">Maki dissembler written by Ralf Engels</a>. This Perl script would take a Maki byte code file and try to construct a source file from it. The intended audience was skinners who wanted to learn from an existing skin’s script which was not distributed with its source code. Aside: The tool’s page has an interesting meditation on the ethics of enabling people to see code that skinners had potentially intentionally tried to hide from others.</p><p>Since the Perl code had to understand the semantics of the byte code in order to produce the equivalent source code, I was able to read the Perl code and (slowly!) build my own JavaScript parser capable of converting Maki byte code files into a structured representation. By leveraging a large collection of real <code>.maki</code> files extracted from downloaded skins, I was able to fuzz my implementation and rattle out many bugs.</p><p>As a resource to any fellow traveler who decides to go down this same path, I attempted to document all my findings in this file: <a href="http://maki-bytecode.mdhttps//github.com/captbaritone/webamp/blob/master/packages/webamp-modern/src/maki/maki-bytecode.md"><code>maki-bytecode.md</code></a></p><h2>Crafting an interpreter</h2><p>With a structured version of the byte code in hand, I was able to start work on an interpreter. As a newbie to this type of programming I leaned into a “<a href="https://jordaneldredge.com/notes/lazy-learning/">learn by doing</a>” approach. To any reader interested in reading this type of work, I highly recommend Bob Nystrom’s <a href="https://jordaneldredge.com/notes/crafting-interpreters/">Crafting Interpreters</a>.</p><p>Partially because the language is a bit quirky, and mostly because I had no idea what I was doing, I spent a fair bit of time getting hung up on things like:</p><ul><li><p>How return pointers worked (do they go on the stack? Is there some other return stack?)</p></li><li><p>Some mysterious byte codes which the decompiler implied had to do with “stack protection”</p></li><li><p>How to model both scalars and complex objects on the stack</p></li></ul><p>Each of these was an interesting puzzle to solve! At one point I even tried <a href="https://jordaneldredge.com/notes/winamp-ghidra/">disassembling Winamp itself using Ghidra</a>, and while I was able to locate the main interpreter loop, my C++/disassembly chops were not sufficient for this to provide much insight.</p><p>But, with enough iteration, and enough test cases (again mined from my collection of real skins) I was able to get it basically working!</p><p>You can see the current version of the interpreter <a href="https://github.com/captbaritone/webamp/blob/master/packages/webamp-modern/src/maki/interpreter.ts">here on GitHub</a>.</p><h2>The standard library and the DOM (equivalent)</h2><p>Having an interpreter was actually just the beginning. Just like having a JavaScript engine is not sufficient to build a browser, I needed to figure out how to parse the accompanying XML files, bind them to the scripts in the <code>.maki</code> files and also implement all the “standard library” of Maki. This included things from basic utility functions, all the way up to the various classes that modeled all the different types of UI objects. On the order of 65 classes with many methods each (you can find a <a href="https://github.com/captbaritone/webamp/blob/master/packages/webamp-modern/resources/maki_compiler/v1.2.0%20(Winamp%205.66)/lib/std.mi">full list here</a>).</p><p>Basically each of these classes needed to be implemented and define some mapping/binding from its properties and methods to an equivilent DOM representation. I took a pragmatic approach. I picked the simplest skin I could find and started implementing the classes and methods needed just to render that one skin. Slowly but surely I was able to get the one skin rendering! After that first skin, I pick another small skin and over time I had a small handful working and then dozens!</p><p><img src="https://jordaneldredge.com/notion-mirror/84ebb48c-616a-4f51-ae9a-991a4e0a7e9b/68b66ce9-6089-4fe1-8b71-7f85cdb2772c/Screenshot_2024-11-19_at_6.57.08_PM.png" alt=""></p><p><a href="https://webamp.org/modern/?skin=assets%2Fskins%2FCornerAmp_Redux.wal">CornerAmp_Redux.wal, the first skin I got working in Webamp Modern</a></p><p>But this is eventually where I lost steam. The API surface was just to large for me to complete with my available time, and even figuring out what the expected behavior of any class/method/property was required hours of manual trial and error in Winamp. But more importantly, I never found a satisfactory way to connect these nested objects to the DOM that was scalable to implement reliably, performant, didn’t leak, and preserved the subtle difference to how the DOM and Maki worked. I suspect a way exists, I just wasn’t able to find it.</p><h2>A hero comes along</h2><p>Despite the project basically sitting on ice, <a href="https://github.com/x2nie">x2nie</a> appeared in our Discord one day and wanted to drive the project forward. However, his style was dramatically different than my own. Sprawling ambitious PRs focusing on getting things to “work” instead of carefully considered incremental improvements, focusing on detailed parity with Winamp and elegant architecture on the JavaScript side.</p><p>I was stuck in an awkward position of holding up PRs because they were time consuming to review and I often didn’t find the approaches taken to be satisfactory. At the same time, I didn’t have the time or brain space to come up with satisfactory solutions to help unblock him. After all, it was my inability to come up with satisfactory solutions to these hard problems that had lead me to stall out on the project in the first place!</p><p>In the end, I had to admit that my approach was stalled out and his approach, while different than my own, had forward momentum. I opted to just try to get out of his way and “let him cook”.</p><p>I revived the <a href="https://webamp.org/modern/progress.html">progress dashboard</a> I had made which introspected the implementation to derive an always-up-to-date progress report, and just stamped his diffs as they poured in.</p><p><img src="https://jordaneldredge.com/notion-mirror/84ebb48c-616a-4f51-ae9a-991a4e0a7e9b/fb623f47-d527-46b9-8635-69a5082c18e3/Screenshot_2024-11-19_at_6.55.47_PM.png" alt=""></p><p>In the end, he made considerable progress, getting many additional features working, but the project still struggled to feel robust or complete. Eventually his attention moved on to other things, but I’m still grateful for the energy he brought to the project!</p><h2>Conclusion, for now?</h2><p>I still don’t have a clear idea how to structure the JS code to make it scalable to fill in all the blanks needed while being largely “correct”. And, while more features work now than when I last actively worked on it, the code is likely harder for me to get into the shape I envision. Mostly because it’s no-longer code I wrote.</p><p>That said, several things have changed in the intervening years. LLMs have made highly repetitive/derivative programming tasks easier to scale, and the source code for Winamp has been released as “open”, so in theory I could go look at the actual source code and get more authoritative answers to what the expected Winamp behavior <em>should</em> be, and maybe have a higher likelihood of being able to cover all the ground needed to get a fully working version.</p><p>Unfortunately, the license of the released Winamp code is not actually permissive and they’ve actually <a href="https://www.theregister.com/2024/10/16/opensourcing_of_winamp_goes_badly/">pulled the code from GitHub</a>. At this point, it actually feels legally more risky to build derivative works than it did before the source was “opened up”.</p><p>I still hold out hope that I’ll be motivated at some point in the future to come back to the project and have some epiphany. But in the mean time, I’m very happy to have seen the project come this far!</p><p><strong>Give it a try!</strong> <a href="https://webamp.org/modern/"><strong>https://webamp.org/modern/</strong></a></p><h2>Gallery</h2><p>I’ll end with a collection of screenshots showing some of the interesting skins that Webamp Modern is capable of rendering. I’d encourage you to click into the skins to try them in your browser and interact with them to see the animations and explore all the little drawers and tabs.</p><p><img src="https://jordaneldredge.com/notion-mirror/84ebb48c-616a-4f51-ae9a-991a4e0a7e9b/404c13f1-053b-4d3b-8212-f7732d039260/Screenshot_2024-11-19_at_7.03.26_PM.png" alt=""></p><p><a href="https://webamp.org/modern/?skin=">https://webamp.org/modern/</a></p><hr><p><img src="https://jordaneldredge.com/notion-mirror/84ebb48c-616a-4f51-ae9a-991a4e0a7e9b/514cbb99-2b2d-4d18-a621-d6f826a9cf05/Screenshot_2024-11-22_at_9.49.46_PM.png" alt=""></p><p><a href="https://webamp.org/modern/?skin=https%3A%2F%2Fr2.webampskins.org%2Fskins%2Fc2273648295a986350f0e2007b705e85.wal">https://webamp.org/modern/?skin=https://r2.webampskins.org/skins/c2273648295a986350f0e2007b705e85.wal</a></p><hr><p><img src="https://jordaneldredge.com/notion-mirror/84ebb48c-616a-4f51-ae9a-991a4e0a7e9b/ac26dbf8-edb7-4fc9-96fe-d590489de37b/Screenshot_2024-11-22_at_9.52.46_PM.png" alt=""></p><p><a href="https://webamp.org/modern/?skin=https%3A%2F%2Fr2.webampskins.org%2Fskins%2F84be4029fa8dd4305b3eee70c648749b.wal">https://webamp.org/modern/?skin=https://r2.webampskins.org/skins/84be4029fa8dd4305b3eee70c648749b.wal</a></p><hr><p><img src="https://jordaneldredge.com/notion-mirror/84ebb48c-616a-4f51-ae9a-991a4e0a7e9b/322618d1-d08c-4ada-bcd6-ba97e837abf5/Screenshot_2024-11-22_at_9.54.11_PM.png" alt=""></p><p><a href="https://webamp.org/modern/?skin=https%3A%2F%2Fr2.webampskins.org%2Fskins%2F00bf47f38660c04f89c3abe06eacd5af.wal">https://webamp.org/modern/?skin=https://r2.webampskins.org/skins/00bf47f38660c04f89c3abe06eacd5af.wal</a></p><hr><p><img src="https://jordaneldredge.com/notion-mirror/84ebb48c-616a-4f51-ae9a-991a4e0a7e9b/d0ef561f-1616-418b-9044-c2121865e4cf/Screenshot_2024-11-22_at_9.56.53_PM.png" alt=""></p><p><a href="https://webamp.org/modern/?skin=https%3A%2F%2Fr2.webampskins.org%2Fskins%2F97a759e2f0261eb0b7c65452d70318d0.wal">https://webamp.org/modern/?skin=https://r2.webampskins.org/skins/97a759e2f0261eb0b7c65452d70318d0.wal</a></p><hr><p><img src="https://jordaneldredge.com/notion-mirror/84ebb48c-616a-4f51-ae9a-991a4e0a7e9b/dad3ca85-9543-4d3a-b129-7e83839762c9/Screenshot_2024-11-22_at_9.58.03_PM.png" alt=""></p><p><a href="https://webamp.org/modern/?skin=https%3A%2F%2Fr2.webampskins.org%2Fskins%2F026d840ca4bebf678704f460f740790b.wal">https://webamp.org/modern/?skin=https://r2.webampskins.org/skins/026d840ca4bebf678704f460f740790b.wal</a></p><hr><p><img src="https://jordaneldredge.com/notion-mirror/84ebb48c-616a-4f51-ae9a-991a4e0a7e9b/61e55b5e-c750-4013-8a45-2d3400f1c670/Screenshot_2024-11-22_at_9.59.09_PM.png" alt=""></p><p><a href="https://webamp.org/modern/?skin=https%3A%2F%2Fr2.webampskins.org%2Fskins%2F2f2d4a3b9aff93ed9d1a240597c298c6.wal">https://webamp.org/modern/?skin=https://r2.webampskins.org/skins/2f2d4a3b9aff93ed9d1a240597c298c6.wal</a></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon to invest another $4B in Anthropic, OpenAI's biggest rival (594 pts)]]></title>
            <link>https://www.cnbc.com/2024/11/22/amazon-to-invest-another-4-billion-in-anthropic-openais-biggest-rival.html</link>
            <guid>42215126</guid>
            <pubDate>Fri, 22 Nov 2024 16:25:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/11/22/amazon-to-invest-another-4-billion-in-anthropic-openais-biggest-rival.html">https://www.cnbc.com/2024/11/22/amazon-to-invest-another-4-billion-in-anthropic-openais-biggest-rival.html</a>, See on <a href="https://news.ycombinator.com/item?id=42215126">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ArticleBody-InlineImage-107408832" data-test="InlineImage"><p>Anadolu | Anadolu | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/AMZN/">Amazon</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> on Friday announced it would invest an additional $4 billion in Anthropic, the artificial intelligence startup founded by ex-OpenAI research executives.</p><p>The new funding brings the tech giant's total investment to $8 billion, though Amazon will retain its position as a minority investor, according to Anthropic, the San Francisco-based company behind the Claude chatbot and AI model.</p><p>Amazon Web Services will also become Anthropic's "primary cloud and training partner," according to a blog post. From now on, Anthropic will use AWS Trainium and Inferentia chips&nbsp;to train and deploy its largest AI models.</p><p>Anthropic is the company behind Claude — one of the chatbots that, like OpenAI's ChatGPT and&nbsp;Google's Gemini, has exploded in popularity. Startups like Anthropic and OpenAI, alongside tech giants such as&nbsp;<a href="https://www.cnbc.com/quotes/GOOG/">Google</a>,&nbsp;<a href="https://www.cnbc.com/quotes/AMZN/">Amazon</a>,&nbsp;<a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a>&nbsp;and&nbsp;<a href="https://www.cnbc.com/quotes/META/">Meta</a>, are all part of a generative AI arms race to ensure they don't fall behind in a market&nbsp;<a href="https://www.bloomberg.com/professional/insights/data/generative-ai-races-toward-1-3-trillion-in-revenue-by-2032/#:~:text=Generative%20AI%20is%20poised%20to,our%20proprietary%20market%2Dsizing%20model." target="_blank">predicted to top $1 trillion</a>&nbsp;in revenue within a decade. </p><p>Some, like Microsoft and Amazon, are <a href="https://www.cnbc.com/2024/03/30/fomo-drives-tech-heavyweights-to-invest-billions-in-generative-ai-.html">backing generative AI startups with hefty investments</a> as well as working on in-house generative AI.</p><p>The partnership announced Friday will also allow AWS customers "early access" to an Anthropic feature: the ability for an AWS customer to do fine-tuning with their own data on Anthropic's Claude. It's a unique benefit for AWS customers, according to a company blog post.</p><p>In March, Amazon's $2.75 billion investment in Anthropic was the company's largest outside investment in its three-decade history. The companies announced an&nbsp;<a href="https://www.cnbc.com/2023/09/25/amazon-to-invest-up-to-4-billion-in-anthropic-a-rival-to-chatgpt-developer-openai.html">initial $1.25 billion investment</a>&nbsp;in September 2023.</p><p>Amazon does not have a seat on Anthropic's board.</p><p>News of Amazon's additional investment comes one month after <a href="https://www.cnbc.com/2024/10/22/anthropic-announces-ai-agents-for-complex-tasks-racing-openai.html">Anthropic announced</a> a significant milestone for the company: AI agents that can use a computer to complete complex tasks like a human would.</p><p>Anthropic's new Computer Use capability, part of its two newest AI models, allows its tech to interpret what's on a computer screen, select buttons, enter text, navigate websites, and execute tasks through any software and real-time internet browsing.</p><p>The tool can "use computers in basically the same way that we do," Jared Kaplan, Anthropic's chief science officer, told CNBC in an interview last month, adding it can do tasks with "tens or even hundreds of steps."</p><p>Amazon had early access to the tool, Anthropic told CNBC at the time, and early customers and beta testers included Asana, Canva and Notion. The company had been working on the tool since early this year, according to Kaplan.</p><p>In September, Anthropic&nbsp;<a href="https://www.cnbc.com/2024/09/04/amazon-backed-anthropic-rolls-out-claude-enterprise-ai-for-big-business.html">rolled out Claude Enterprise</a>, its biggest new product since its chatbot's debut, designed for businesses looking to integrate Anthropic's AI. In&nbsp;<a href="https://www.cnbc.com/2024/06/20/anthropic-claude-3point5-sonnet-ai-announced.html">June</a>, the company debuted its more powerful AI model, Claude 3.5 Sonnet, and in May, it rolled out its&nbsp;<a href="https://www.cnbc.com/2024/05/01/anthropic-iphone-ai-app-business-plan-to-compete-with-openai-announced.html">"Team" plan for smaller businesses</a>.</p><p>Last year, Google&nbsp;<a href="https://www.cnbc.com/2023/10/27/google-commits-to-invest-2-billion-in-openai-competitor-anthropic.html">committed</a>&nbsp;to invest $2 billion in Anthropic, after previously confirming it had taken a 10% stake in the startup alongside a large cloud contract between the two companies.</p></div><div id="SpecialReportArticle-RelatedContent-1"><h2>Don’t miss these insights from CNBC PRO</h2><div><ul><li><a href="https://www.cnbc.com/2024/11/14/warren-buffetts-berkshire-hathaway-takes-a-stake-in-dominos-pizza.html">Warren Buffett's Berkshire Hathaway takes a stake in Domino's Pizza</a></li><li><a href="https://www.cnbc.com/2024/11/17/wall-street-gears-up-for-ma-boom-these-names-could-be-attractive-targets.html">Wall Street is gearing up for an M&amp;A boom under Trump. These companies could be targets</a></li><li><a href="https://www.cnbc.com/2024/11/13/inflation-report-shows-market-could-have-a-recipe-for-disaster-heading-into-new-year-says-economist.html">Inflation report shows market could have a 'recipe for disaster' heading into new year, says economist</a></li><li><a href="https://www.cnbc.com/2024/11/18/morningstar-strategist-picks-2-stocks-from-a-sector-he-is-betting-on.html">Morningstar names cheap stocks in a sector that ‘deserves a place in everybody’s portfolio’</a></li><li><a href="https://www.cnbc.com/2024/11/18/these-2-active-etfs-have-outperformed-the-sp-500-this-year-last-year-and-over-5-years.html">These 2 active ETFs have outperformed the S&amp;P 500 this year, last year and over 5 years</a><br></li></ul></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Deceptively Asymmetric Unit Sphere (129 pts)]]></title>
            <link>https://www.tangramvision.com/blog/the-deceptively-asymmetric-unit-sphere</link>
            <guid>42214880</guid>
            <pubDate>Fri, 22 Nov 2024 16:00:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tangramvision.com/blog/the-deceptively-asymmetric-unit-sphere">https://www.tangramvision.com/blog/the-deceptively-asymmetric-unit-sphere</a>, See on <a href="https://news.ycombinator.com/item?id=42214880">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Previously, we talked about [Pinhole Obsession](https://www.tangramvision.com/blog/camera-modeling-pinhole-obsession) and the downsides of assuming a default pinhole model. We presented an alternative formulation where rays are modeled on the unit sphere \\(\mathcal{S}^2\\) instead of the normalized image plane \\(\mathbb{T}^2\\).</p><p>We’ve also previously written posts about how many problems in robotics can be formulated as [an optimization](https://www.tangramvision.com/blog/introduction-to-optimization-theory). At a high level, many continuous optimization algorithms perform the following steps</p><p>- Compute a quantity to minimize — error, misalignment, risk, loss, etc.<br>- Compute a direction along which the quantity is locally reduced<br>- Move the parameters in the quantity-reducing direction<br>- Repeat until the problem converges</p><p>Many continuous optimization problems are modeled on [vector spaces](https://en.wikipedia.org/wiki/Vector_space) and “moving” the parameters is simply vector addition. However, our prescription of using the unit sphere to model rays is obviously **not** a linear space! Therefore, we must start our discussion by describing how to travel on the sphere.</p><p>## Traveling on the Sphere</p><p>Generally, when we *travel* we want to do so in the most efficient manner. Undoubtedly, you’ve heard the expression “get from point a to b,” perhaps also with the implication “as fast as possible.” Therefore, we’ll want to minimize the distance that we travel or equivalently travel along the shortest path. In Differential Geometry, we call these “shortest paths” *geodesics.*</p><p>An alternate way to view a *geodesic* is by starting at a point \\(p\\) on the manifold and traveling in the direction of some vector \\(v\\), carefully minimizing the distance at each increment. However, minimizing the distance at each step may mean we have to *change direction* along our shortest path.</p><p>“Changing direction” and “traveling in the shortest path” may seem to counter one another. After all, we all know that the shortest path between two points is a line. However, imagine we are flying in a plane starting at the equator and a heading of 45°. If our goal is to travel *as far as possible* from our starting point, how should we chart our course?</p><p>&gt;&nbsp;As usual, ignore air resistance, the jet stream and assume a spherical earth. This is a thought exercise, not pilot’s school.</p><p>A simple suggestion would be to maintain a heading of 45° during our travel, like we would locally: travel in a constant direction. However, if we maintain a fixed heading indefinitely, we will end up close to the north pole! The line traced by a path of constant heading is known as a [Rhumb Line](https://en.wikipedia.org/wiki/Rhumb_line) and, as pictured, is certainly not the shortest distance between two points on a sphere.</p><p>![Loxodrome.png](https://cdn.prod.website-files.com/5fff85e7f613e35edb5806ed/673f6319ebc7cd01b693200a_Loxodrome.png)<br>*PC: [https://en.wikipedia.org/wiki/Rhumb_line#/media/File:Loxodrome.png](https://en.wikipedia.org/wiki/Rhumb_line#/media/File:Loxodrome.png)*</p><p>Alternatively, to truly travel the *farthest distance,* we travel along the [great circle](https://en.wikipedia.org/wiki/Great-circle_distance) until the aircraft runs out of fuel. Here it’s more obvious that the geodesic’s heading is continuously changing. One implication of this is that if we start at one point along the geodesic with a vector \\(v\\), we may not end up in the same location as if we start at a different point along the geodesic with the same vector \\(v\\).</p><p>![Illustration_of_great-circle_distance.svg](https://cdn.prod.website-files.com/5fff85e7f613e35edb5806ed/673f8b9f2d9bfdd0a7e52425_Illustration_of_great-circle_distance.png)<br>*PC: [https://en.wikipedia.org/wiki/Great-circle_distance#/media/File:Illustration_of_great-circle_distance.svg](https://en.wikipedia.org/wiki/Great-circle_distance#/media/File:Illustration_of_great-circle_distance.svg)*</p><p>In Differential Geometry, the operator that traces the geodesic is known as “the exponential map.” For the reasons listed above, it’s defined as a local operator originating at a point \\(p\\) and traveling in a direction \\(v\\) i.e.</p><p>$$<br>\text{Exp}_p(v)<br>$$</p><p>Similarly, the operator that computes the direction and distance between two points on the manifold (the manifold’s logarithm) is defined as a local operator \\(\text{Log}_p(q)\\).</p><p>So to perform optimization on the unit sphere, we</p><p>- Compute the quantity to minimize — error, misalignment, risk, loss, etc.<br>- Compute a direction \\(v\\) along which the quantity can be locally reduced<br>- Move the parameters along the great circle using the exponential map \\(\text{Exp}_p(v)\\)<br>- Repeat until the problem converges</p><p>This begs the question: how do we compute this direction \\(v\\)? What does it really mean?</p><p>---</p><p>## The Briefest Introduction to Differential Geometry</p><p>&gt;&nbsp;⚠️ Many maths below! But they’re cool maths, we promise.</p><p>To explain what a direction \\(v\\) means on the sphere and to explain why working with \\(\mathcal{S}^2\\) is difficult, we have to briefly touch on one of my favorite subjects: [Differential Geometry](https://en.wikipedia.org/wiki/Differential_geometry). This post will dive deep into the motivations behind why we use differential geometry in computer vision, and what advantages it brings. Much of this may seem like a tangent (no pun intended), but don’t worry: we’ll bring it all back to our Pinhole Obsession and optimization at the end!</p><p>&gt;&nbsp;💡 Obviously, we cannot cover the entirety of Differential Geometry in one blog post. There are many great resources to learn Differential Geometry. Some of our favorites are:<br>&gt;&nbsp;- [Introduction to Smooth Manifolds](https://link.springer.com/book/10.1007/978-1-4419-9982-5)<br>&gt;&nbsp;- [A Micro Lie Theory for State Estimation in Robotics](https://arxiv.org/pdf/1812.01537)<br>&gt;&nbsp;- [Differential Geometry and Lie Groups](https://link.springer.com/book/10.1007/978-3-030-46040-2)</p><p>Historically, differential geometry arose from the study of “curves and surfaces.” As traditionally taught in multi-variable calculus, a curve is a mapping \\(\gamma: \mathbb{R} \to \mathbb{R}^3\\) and likewise, a surface is a mapping \\(\sigma: \mathbb{R}^2 \to \mathbb{R}^3\\).</p><p>Differential geometry generalizes these “curves and surfaces” to arbitrary dimensions. These generalized “curves and surfaces” are known as *smooth manifolds*. In this post, we’ll skip the rigorous definition of a smooth manifold and simply define it as</p><p>&gt; *Smooth Manifold:* An N-dimensional space without corners, edges or self-intersections.</p><p>This smoothness (called continuity) allows us to perform optimization “on the manifold” using our standard calculus techniques. We can compute errors and also compute the directions along which we can reduce those errors.</p><p>## Intrinsic vs Extrinsic Manifolds</p><p>To assist intuition, we’ve leaned on prior knowledge of multi-variable calculus of curves and surfaces. At the undergraduate level, curves and surfaces are presented as [embedded entities](https://en.wikipedia.org/wiki/Nash_embedding_theorems) living in a higher-dimensional ambient space e.g. 3D Euclidean Space. However, this ambient space isn’t *strictly* needed for the development of differential geometry. In their search for a minimal set of axioms, differential geometers have taken great care to remove this dependence on an ambient space; they describe *smooth manifolds* as objects existing in their own right. Thus, there are two ways of thinking about differential geometry:</p><p>- *Extrinsic*: manifolds as embedded objects in space<br>- *Intrinsic*: manifolds are entities in their own right</p><p>In this manner, the error-minimizing direction we are searching for can either be viewed as *extrinsic* (embedded in ambient space) or it can be viewed as *intrinsic* (living alongside the manifold).</p><p>As engineers, the decision to use the intrinsic view of a manifold vs. the extrinsic view mostly impacts representation and computation. Thus, for a specific manifold, we choose the representation that’s computationally easiest to work with. Although the remainder of this post only briefly touches on the representation of manifolds, it’s **always** necessary to determine what representation is being used. The decision of an intrinsic or extrinsic representation will change how computation is performed on the manifold. Regardless of the choice of *intrinsic* or *extrinsic* representation, we often “bundle” the representation of the manifold with the representation directions on that manifold.</p><p>&gt;&nbsp;⚠️ The concepts of *intrinsic* and *extrinsic* used in differential geometry are unrelated to intrinsics (interior orientation) and extrinsics (exterior orientation) of cameras. This is simply an unfortunate naming collision at the intersection of two fields.</p><p>## A Brief Tangent for Tangents</p><p>…which brings us to **the** key concept in differential geometry: the [Tangent Bundle](https://en.wikipedia.org/wiki/Tangent_bundle). The Tangent Bundle is the underlying manifold stitched together with the vector spaces that are tangent to the manifold at each point.</p><p>To illustrate this, consider the unit circle \\(\mathcal{S}^1\\) below. The blue circle represents the underlying manifold and the red lines represent the one dimensional tangent space at each point on the circle. Together, the points and tangent spaces form the tangent bundle. It is important to note that that vectors in one tangent space should be regarded as distinct and separate from vectors in another tangent space.</p><p>![Tangent_bundle.svg](https://cdn.prod.website-files.com/5fff85e7f613e35edb5806ed/673f63501b983d8e9953910b_image.png)<br>*PC: [https://commons.wikimedia.org/wiki/File:Tangent_bundle.svg](https://commons.wikimedia.org/wiki/File:Tangent_bundle.svg)*</p><p>Now that we have the correct picture in our heads, let’s visualize tangent vectors and the tangent bundle in more detail.</p><p>Consider an arbitrary manifold \\(M\\), and furthermore consider a curve on that manifold \\(\gamma: \mathbb{R} \to M\\). This curve \\(\gamma(t)\\) can be thought of “the location on the manifold at time \\(t\\).” Furthermore, let \\(\gamma(0)\\) be some point of interest \\(p \in M\\) on the manifold.</p><p>For visualization, consider this wavy circle manifold:</p><p>![GenericManifold_ManimCE_v0.18.1.png](https://cdn.prod.website-files.com/5fff85e7f613e35edb5806ed/673f636093b3a75b96cec282_GenericManifold_ManimCE_v0.18.1.png)</p><p>Now, consider the curve \\(\gamma(t)\\) living on this manifold.</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GenericManifoldWithCurve.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GenericManifoldWithCurve.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>If we take the derivative of this curve, we obtain a pair \\((\gamma(t), \gamma'(t))\\), where the first entry describes the point along the curve and the second entry describes how the curve is changing in time. This can be visualized as a vector “riding along” the curve.</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GenericManifoldWithCurveAndTangent.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GenericManifoldWithCurveAndTangent.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>The magnitude and sign of this vector can be changed by re-parametrizing the curve as</p><p>$$\gamma_{\alpha}(t) = \gamma(\alpha t)$$</p><p>Specifically, note that</p><p>$$\gamma'_\alpha(0) = \alpha \gamma'(0)$$</p><p>In this manner, we can “scale” curves in the same way that we would scale vectors. Similarly, we can derive addition, inverse and zero curves and form a “vector space of curves.”</p><p>Thus, we define the tangent space \\(T_pM\\) as the set of all curves that pass through a point of interest \\(p \in M\\). The tangent bundle of a manifold \\(TM\\) is the set of all tangent spaces combined under a *disjoint* union. In this context, a disjoint union simply means we cannot combine vectors in distinct tangent spaces. This is perhaps the most important takeaway: tangent vectors living in distinct tangent spaces cannot be added, combined or related without using some additional manifold structure.</p><p>&gt;&nbsp;⚠️ Here, we have *de facto* assumed that we can take the derivative of a curve on the manifold. This is made more rigorous in most differential geometry books by either using charts on the manifold or an extrinsic view of a manifold facilitated by an embedding. Furthermore, we didn’t really show how to “add curves.” We haven’t done anything *improper* with this explanation, but it certainly is lacking detail! You can refer to the previously mentioned resources to learn more about the mathematically precise definitions of the Tangent Bundle.</p><p>If this is your first time studying Differential Geometry, you may be very uncomfortable with the notion of a vector being *attached* in any way to a specific point. After all, vectors are commonly taught as being geometric entities that can be moved anywhere in space, e.g. vector addition is taught using the “head to tail” method. It’s unclear when presenting these geometric explanations if the vectors are “sitting on top of” the underlying manifold or if they are in their own “vector space.”</p><p>To see why this “head to tail” method breaks down on manifolds, imagine traveling 1000 miles north and then 1000 miles west. This will take you to a very different location than if you had traveled 1000 miles west and then 1000 miles north. In other words, vectors may point in different directions at different points on the manifold, so we can only really discuss what a vector means *locally*.</p><p>The representation we have presented here of \\((\gamma(0), \gamma'(0))\\) , is useful in describing the construction tangent vectors on a manifold and also useful in derivations of certain operators, but it is typically not used in computation. In “the real world,” we typically choose some canonical coordinates for the manifold and a basis for the associated tangent space and think of tangent vectors as \\((p, v) \in M \times \mathbb{R}^n\\).</p><p>With this understanding of tangent spaces, the exponential map can be understood more formally as an operator mapping vectors in a tangent space onto manifold.</p><p>$$<br>\text{Exp}_p(v): T_pM \to M<br>$$</p><p>Likewise, the logarithmic map can be understood as an operator mapping points on the manifold back into a tangent space</p><p>$$<br>\text{Log}_p(q): M \times M \to T_pM<br>$$</p><p>&gt;&nbsp;🚧 Ok, *technically*, more structure is needed for the existence of an exponential map. Since the exponential map minimizes distance along a path, we must have some notion of *length.* The subfield of Differential Geometry with *lengths* and *angles* is called [Riemannian Geometry](https://en.wikipedia.org/wiki/Riemannian_geometry) and is a fascinating subject in its own right. The definition of *lengths* and *angles* for a specific manifold is called the Riemannian Metric. We don’t have the space in this post to develop these ideas fully but we encourage the motivated reader to learn more!</p><p>With this understanding, we have the following prescription for optimization on a generic manifold \\(M\\)</p><p>- Compute the quantity to minimize — perhaps using our logarithmic map \\(\text{Log}_p(q)\\)<br>- Compute a direction \\(v\\) in \\(T_pM\\) &nbsp;along which the quantity is locally reduced<br>- Move the parameters using the exponential map \\(\text{Exp}_p(v)\\)<br>- Repeat until the problem converges</p><p>At this point, you are probably saying</p><p>&gt; “Why isn’t multi-variable optimization taught like this?”<br></p><p>&gt; “I haven’t had to worry about *tangent spaces* and *exponential maps* before.”<br></p><p>&gt; “Adding vectors has always *just worked* for me in the past.”<br></p><p>Yes, BUT. The secret here is that you’ve been exploiting a very special property of euclidean space: *continuous symmetry* of its tangent bundle.</p><p>## Continuous Symmetry</p><p>Some smooth manifolds exhibit a natural symmetry. By symmetry, we mean a concept that implies more than reflections or rotations of polygons taught in grade school. For instance, reflections and rotations of polygons are examples of *discrete symmetry.* However, in the study of smooth manifolds, we are interested in *continuous* *symmetry* of the Tangent Bundle. This is what makes optimization so simple in euclidean space.</p><p>To illustrate, consider the smooth manifold of real numbers of dimension \\(n\\): \\(\mathbb{R}^n\\) or Euclidean Space. We will visualize the plane \\(\mathbb{R}^2\\), but realize that these properties apply in higher dimensions, too.</p><p>Let’s place a uniform vector field on \\(\mathbb{R}^n\\), where “the same” vector \\(v\\) is attached to each point in the space \\(p\\). Imagine those points *flowing* along the vectors, as if the vectors describe the current of a river and the points are rafts in that river.</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GridShiftPoints.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GridShiftPoints.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>You’ll notice that the points before and after the flow are effectively the same! Since this flow doesn’t *fundamentally* change the total set of points, it’s called an *invariant flow.*</p><p>Now let’s flip it: instead of moving each point along the flow of individual vectors, let’s recenter the points on a new origin.</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GridShiftOrigin.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GridShiftOrigin.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>Notice that this recentering effectively produces the same transformation as the vector field above. This is one important property of continuous symmetry of the manifold: the equivalence of local transformations under *constant* vector fields and global transformations of the entire space.</p><p>We can also recenter both the vector field and its associated points. Doing so, we find that the points and vector field are fundamentally *unchanged.*</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GridShiftOriginPointsVectors.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GridShiftOriginPointsVectors.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>Furthermore, the points and vector field don’t have to be recentered in the same direction as the vector field. Under any arbitrary translation, the points and the *constant* vector field remain unchanged*.* In this way, both the manifold and the Tangent Bundle exhibit continuous symmetry.</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GridShiftOriginPointsVectors2.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GridShiftOriginPointsVectors2.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>So to recap, when we say *continuous symmetry* of the tangent bundle of \\(\mathbb{R}^n\\) we mean three things:</p><p>- Local flows along an invariant vector field are the same as a global translation of all points<br>- Arbitrary translations of points yields the same set of points<br>- Arbitrary translations of *constant* vector fields yields the same *constant* vector field</p><p>One consequence of this continuous symmetry is that we can effectively treat *any* point in \\(\mathbb{R}^n\\) as the origin. To illustrate this, we will “subtract” two points \\(p\\) and \\(q\\) &nbsp;and get a vector pointing from \\(p\\) to \\(q\\), &nbsp;and then flow \\(p\\) along the vector to recover \\(q\\).</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GridSubtractPoints.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GridSubtractPoints.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>If we follow this animation *carefully,* we have to remember that this red vector lives in \\(T_p\mathbb{R}^n\\) and then *flow* the point starting at \\(p\\) until it reaches \\(q\\). Now, let’s first recenter the origin at \\(p\\) and then perform the subtraction.</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GridSubtractPointsAtOrigin.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/GridSubtractPointsAtOrigin.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>We see that this produces the *same* result, and this time the vector was “attached to the origin.” In effect, we can do the subtraction *as if* the space was centered at \\(p\\) and then only worry about vectors living in \\(T_0 \mathbb{R}^n\\).</p><p>You probably didn’t realize it, but we’ve set ourselves up for easy computation! If we use the standard basis of \\(\mathbb{R}^n\\), we simply subtract the coordinates of each point element-wise to get the vector. If we want to translate a point by a vector, we again can simply perform element-wise addition of the coordinates. The annoying book-keeping that comes with this vector distribution is effectively non-existent, because we just treat all vectors *as if* they lived in the tangent space at the origin.</p><p>In fact, since \\(T_0\mathbb{R}^n\\) is essentially just a copy of \\(\mathbb{R}^n\\), we can confuse points and vectors and likely *get away with it*. The distinction between a point and the vector that points from the origin to a point is so subtle that in practice it’s often not worth mentioning.</p><p>Using the continuous symmetry of \\(\mathbb{R}^n\\) we formulate our optimization steps as</p><p>- Compute the quantity to minimize *as if* our parameters represent the origin<br>- Compute a direction along which the quantity is locally reduced<br>- Add the minimizing direction to our parameters *as if* our parameters represent the origin<br>- Repeat until the problem converges</p><p>With continuous symmetry in euclidean space on our side, we can use the \\(\mathbb{R}^n\\) hammer on *everything*. Recalling our [previous post](https://www.tangramvision.com/blog/camera-modeling-pinhole-obsession): consider \\(\mathbb{T}^2\\). You’ll notice now that it is essentially \\(\mathbb{R}^2\\) with a couple of “extra points at infinity.” This makes computer vision with camera rays and the pinhole camera model feel easy! We just treat \\(\mathbb{T}^2\\) &nbsp;as \\(\mathbb{R}^2\\) and compute reprojection error by subtracting points and minimizing that error.</p><p>## Lie Groups: *Continuing* the Symmetry Party</p><p>But, I hear you mutter to yourself,</p><p>&gt; “Are there manifolds that have continuous symmetry of their Tangent Bundle, but are not flat like \\(\mathbb{R}^n\\)?”<br></p><p>Great question! The answer is yes! An example of manifolds that fit these criteria are [Lie Groups](https://en.wikipedia.org/wiki/Lie_group). A Lie Group (we’ll call it \\(\mathcal{G})\\) is a [mathematical group](https://en.wikipedia.org/wiki/Group_(mathematics)) that is also a smooth manifold.</p><p>Many commonly-used manifolds in robotics are Lie Groups:</p><p>- Special Orthogonal Group \\(SO(3)\\) - used for describing rigid-body rotations<br>- Special Euclidean Group \\(SE(3)\\) - used for describing rigid-body motion<br>- Special Linear Group \\(SL(3)\\) - used for describing continuous-time homographies<br>- Similarity Group \\(Sim(3)\\) - used for describing similarity transforms (rigid-body + scale)</p><p>Given their wide usage, it’s worthwhile to explore what makes them so useful.</p><p>A Lie Group is a mathematical group; it’s in the name. This means it has a way to compose elements, typically denoted as \\(g \circ h\\) for group elements \\(g,h \in \mathcal{G}\\). Additionally, the group has an identity element (typically called \\(e\\)). In some fashion, this identity can be considered the “origin”. Since a Lie group is *also* a smooth manifold, it has a tangent space at this “origin.” The tangent space at the identity element is called the [Lie Algebra](https://en.wikipedia.org/wiki/Lie_algebra) and is denoted by \\(T_e \mathcal{G} = \mathfrak{g}.\\)</p><p>To help us develop some intuition of Lie Groups, we will consider the simplest non-euclidean Lie Group, the unit circle \\(\mathcal{S}^1\\) and its associated Lie Algebra. Note that unlike our visualization of \\(\mathbb{R}^n,\\) we have chosen to explicitly display a tangent space of unit circle, since it takes a different shape than the underlying manifold.</p><p>![CircleLieAlgebra_ManimCE_v0.18.1.png](https://cdn.prod.website-files.com/5fff85e7f613e35edb5806ed/673f8903a9b45c2d4aa6779b_CircleLieAlgebra_ManimCE_v0.18.1.png)</p><p> Now, we proceed to choose some vector in this tangent space at the identity.</p><p>![CircleVector_ManimCE_v0.18.1.png](https://cdn.prod.website-files.com/5fff85e7f613e35edb5806ed/673f89037c2ce14a94eb854b_CircleVector_ManimCE_v0.18.1.png)</p><p>Now, let’s try to move a point along our chosen vector.</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleVectorFlowWrong.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleVectorFlowWrong.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>If we simply move the point along the vector, the point leaves the manifold! This is not a valid flow, because the points must *by definition* live on the underlying manifold. To fix this, we will define the *flow* as the curve \\(\varphi(t)\\) such that &nbsp;\\(\varphi'(t)\\) matches the tangent vector. For a vector field \\(X\\) defined on the entire manifold, we denote this flow as \\(\varphi_X(t)\\).</p><p>However, this definition makes solving for \\(\varphi_X(t)\\) a bit tough since we have to solve an ordinary differential equation on a manifold. If we instead think of Euler integration, we can *approximate* a solution by “moving along” the manifold in the direction of the vector field.</p><p>In \\(\mathbb{R}^n\\), for a given vector field \\(X: \mathbb{R}^n \to T\mathbb{R}^n\\) euler integration gives us the recurrence of</p><p>$$<br>p_k = p_{k-1} + X(p_{k-1}) \Delta t<br>$$</p><p>On the manifold we can use the exponential map to similar effect. For a given vector field \\(X: M \to TM\\) “euler integration” takes the form of</p><p>&lt;p&gt;$$p_k = \text{Exp}_{p_{k-1}}(X(p_{k-1})\Delta t)$$&lt;/p&gt;&lt;!-- manual html to prevent showdown changing underscores into emphasis tags before mathjax gets to it --&gt;</p><p>We can imagine this exponential map as a “pushed down” vector on the manifold.</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleVectorFlowRight.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleVectorFlowRight.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>&gt; ⚠️ In actuality, there is no such thing as a “pushed down vector,” however it’s a useful concept for visualization. If we use the definition of the exponential map, we can imagine this “pushed down vector” as the line drawn by the equation \\(\text{Exp}(\alpha v)\\) for a fixed vector \\(v\\) and a scalar \\(\alpha \in [0,1]\\).</p><p>Now following our example with \\(\mathbb{R}^2\\), let’s try to create a *constant* vector field by copying a vector into all points on the manifold. If we naively copy this vector in the same way that we did in the euclidean case (remember that big grid of arrows?), we get the following:</p><p>![CircleTangentSectionWrong_ManimCE_v0.18.1.png](https://cdn.prod.website-files.com/5fff85e7f613e35edb5806ed/673f89065b7a81cc793276c6_CircleTangentSectionWrong_ManimCE_v0.18.1.png)</p><p>This cannot be correct; our vectors wouldn’t live in their own respective tangent spaces. These “tangent vectors” have some component pointing normal to the manifold, which implies they are not tangent vectors at all! How do we address this mathematically?</p><p>&gt; ⚠️ Here we are claiming that this visualization is incorrect because we are considering the unit circle and its tangent bundle as entities *embedded* in ambient euclidean space \\(\mathbb{R}^2\\). In reality, we only care about the geometric and algebraic structure of the unit circle and our visualization is *somewhat independent* of that structure. It it not uncommon to visualize a manifold and an associated fibre bundle (e.g. the tangent bundle) as living in orthogonal spaces but connected at a single point. This explains why you sometimes may see the tangent bundle visualized as a manifold with non-overlapping tangent spaces.</p><p>&gt; ![Tangent_bundle.png](https://cdn.prod.website-files.com/5fff85e7f613e35edb5806ed/673f8903a9b1afcd2cb9b285_Tangent_bundle.png)<br>*PC: [https://commons.wikimedia.org/wiki/File:Tangent_bundle.svg](https://commons.wikimedia.org/wiki/File:Tangent_bundle.svg)*</p><p>&gt; In this post, we’ll continue with the embedded and overlapping view to aid intuition, but don’t be surprised if you see the tangent bundle drawn differently in other resources.</p><p>Instead of naively copying the vector at identity to every point on the lie group, let’s consider an arbitrary point \\(g \in \mathcal{G}\\). We can consider this point on the manifold as simply a point or, using the group structure, we can consider it as an operator since \\(g \circ e = g\\). In other words, \\(g\\) can be regarded as an operator that takes the identity into \\(g\\). This “operatorness” of \\(g\\) is typically denoted as the map</p><p>$$<br>\begin{aligned}<br>L_g: \mathcal{G} &amp;\to \mathcal{G} \\\\<br>h &amp;\mapsto g \circ h<br>\end{aligned}<br>$$</p><p>where the \\(L\\) denotes that we are composing with \\(g\\) on the left. You can form a similar map by composing on the right; we’ll call it \\(R_g\\).</p><p>We can use this operator concept to “push” vectors around on the manifold. Instead of \\(L_g\\) operating on a fixed value \\(h\\), we can imagine it operating on a curve \\(\gamma(t)\\) with \\(\gamma(0) = h\\).</p><p>Now we have a new *shifted* curve</p><p>$$<br>L_g(\gamma(t)) = g \circ \gamma(t)<br>$$</p><p>We can now consider the derivative of this new *shifted vector* as a vector living in the tangent space \\(T_{g \circ h} \mathcal{G}\\):</p><p>$$<br>\frac{d}{dt}L_g(\gamma(t))\vert_{t=0}<br>$$</p><p>…just like we did in our original vector field in \\(\mathbb{R}^n\\). For convenience of notation, we’ll drop the explicit parametrization of the curves and denote this vector shift as \\(dL_g(h,v)\\) for some \\(v \in T_h \mathcal{G}.\\)</p><p>&lt;blockquote&gt;<br>💡</p><p>The argument made here to derive this &lt;em&gt;vector shift&lt;/em&gt; function actually extends to arbitrary manifolds. If we have an map between manifolds</p><p>$$<br>\begin{aligned}<br>f: M \to N<br>\end{aligned}<br>$$</p><p>that obeys the continuity properties of the manifold, we can create an induced map on the tangent bundle</p><p>$$<br>df: TM \to TN<br>$$</p><p>This induced map is often referred to as the differential or “push forward” map because it “pushes” vectors in tangent spaces of one manifold onto tangent spaces of another manifold.</p><p>&lt;/blockquote&gt;</p><p>Now, we will state a few facts without proof about \\(dL_g(h,v)\\)</p><p>1. \\(dL_g(h,v)\\) &nbsp;is linear in \\(v\\)<br>2. \\(dL_g(h, v)\\) actually does not depend on \\(h\\), so we can write it as \\(dL_g(v)\\)<br>3. \\(dL_g(dL_h(v)) = dL_{g \circ h}(v)\\) or in other words group composition is compatible with vector shifting.</p><p>Got all that? Good! Now that we have this structure in place, let’s return to our tangent vector at the origin.</p><p>![CircleVector_ManimCE_v0.18.1.png](https://cdn.prod.website-files.com/5fff85e7f613e35edb5806ed/673f89037c2ce14a94eb854b_CircleVector_ManimCE_v0.18.1.png)</p><p>This time we will copy the vector to the other points on the manifold by *pushing* the vector with \\(dL_g\\) to each \\(g\\) on the manifold.</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleTangentSectionRight.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleTangentSectionRight.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>If we flow all the points on the manifold along these vectors we’ll notice that all the points change in a consistent way! In the case for \\(\mathcal{S}^1\\) they are all rotating around the circle.</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleTangentSectionFlow.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleTangentSectionFlow.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>Here, we’ve found another invariant flow field. Let’s determine if the vector field is *unchanged* under the combined transformation</p><p>$$<br>(h, v) \mapsto (L_g(h), dL_g(v))<br>$$</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleShiftVectors.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleShiftVectors.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>Just like the case for \\(\mathbb{R}^n\\), the points and vector field remain *unchanged*. Furthermore, like \\(\mathbb{R}^n\\) the transformation does not have to be *in the same direction* as the vector field.</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleShiftVectors2.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleShiftVectors2.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>Since these vectors don’t all point in the same direction (at least from an extrinsic point of view), we cannot call this a *constant* vector field. However, the vector field is invariant to group transformations, so we will call it an *invariant* vector field.</p><p>With these observations, we can state that all Lie Groups have a *continuously symmetric* Tangent Bundle*.* The difference from the euclidean case is that points and (certain) vector fields are invariant under group *transformations* instead of *translations*.</p><p>&gt; 💡Although this difference may seem significant at first, it turns that \\(\mathbb{R}^n\\) is actually Lie Group with translation as the group operator! Would you look at that.</p><p>As an added bonus, for these *invariant* vector fields, the Euler integration using the exponential map is **exact.** Specifically, for an *invariant* vector field the following closed-form integration holds:</p><p>$$<br>\varphi_X(t) = \text{Exp}_{p}(X(p)t)<br>$$</p><p>&gt; ⚠️ Here in this section, we’ve implied a parallel between *geodesics* on manifolds with a Riemannian Metric and the exponential map on Lie Groups. Technically, we have not defined a notion of a Riemannian Metric for Lie Groups. In this manner, it is possible to define a Riemannian Metric on Lie Groups such that the *geodesic exponential* and the *group exponential* are distinct. Regardless, broadly speaking, in both cases the exponential map helps us move along the manifold in a coherent manner.</p><p>As with Euclidean space, we can “recenter” the manifold by using the group operator to move \\(g\\) to the identity \\(e\\) while maintaining the distance to all other points. To do this, we simply apply the operator \\(L_{g^{-1}}\\) to all points on the manifold.</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleRecenter.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleRecenter.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>Likewise, we can “subtract” any two points \\(h\\) and \\(g\\) on the manifold by shifting the point we are subtracting to the origin and then using our aforementioned logarithm \\(\text{Log}_e(g^{-1} \circ h)\\).</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleRecenterSubtract.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/CircleRecenterSubtract.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>Leveraging this *continuous symmetry* of Lie Groups, our optimization steps become</p><p>- Compute the quantity to minimize *as if* our parameters represent the origin i.e. using \\(\text{Log}_e(g^{-1} \circ h)\\)<br>- Compute a direction along which the quantity is locally reduced<br>- Add the minimizing direction to our parameters *as if* our parameters represent the origin i.e. using \\(g \circ \text{Exp}_e(v)\\)<br>- Repeat until the problem converges</p><p>Which makes for a pretty simple algorithm! We really only have to worry about two extra operators \\(\text{Log}_e\\) and \\(\text{Exp}_e\\). Maybe optimizing “on the manifold” isn’t so difficult after all? Let’s apply this *continuous symmetry* concept to the unit sphere to finally cure our Pinhole Obsession.</p><p>## Symmetry of the Sphere</p><p>Back to our unit sphere! The whole reason we did this in the first place.</p><p>Ideally, we’d want the unit sphere to exhibit *continuous symmetry* of the Tangent Bundle so we can take advantage of its benefits:</p><p>- Local flow along *invariant* vector fields is the same as a global transformation<br>- Some arbitrary concept of “origin” or “identity”<br>- Invariance of points and certain vector fields under global transformations</p><p>At first glance, the sphere *looks* fairly symmetric, so constructing an *invariant* vector field should be straightforward. Let’s try to find such a vector field. To start, let’s choose a vector field where all the vectors point at the north pole.</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/longitude_sphere.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/longitude_sphere.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>Here we notice that the points on the manifold are generally not all moving in the “same direction.” The points close to the north pole are converging and the points close to the south pole are diverging. This is not an invariant flow field.</p><p>Let’s again try to find an invariant flow field but this time choose the vector field where all the vectors are pointing east.</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/east_sphere.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/east_sphere.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>Following the flow, the total set of points remains unchanged*.* This flow field yields a rotation of the sphere. So, the manifold itself exhibits some *continuous symmetry* because rotation leaves the set of points on the manifold unchanged.</p><p>&gt; ⭐ If you are wondering why these points flowing east are not moving along great circles, then you are a very astute reader! Remember, that in general using the exponential map \\(\text{Exp}_p(v)\\) to solve the flow \\(\varphi_X(t)\\) of a vector field \\(X\\) is an *approximation.* The true definition is that derivative of the flow \\(\varphi_X'(t)\\) must match the vector field \\(X\\) at every point along the curve.</p><p>Does this symmetry extend to the tangent bundle \\(T\mathcal{S}^2\\)? Well, if we swap the north pole and south pole, the vector field is now pointing west! So, the vector field that produces rotations is *not* *invariant* under rotations.</p><p>&lt;video width="100%" preload="metadata" loop muted controls&gt;&lt;source src="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/flip_sphere.mp4" type="video/mp4"&gt; If this video doesn't work in browser, &lt;a href="https://tangram-vision-blog-resources.s3.us-west-1.amazonaws.com/video/flip_sphere.mp4"&gt;download the video to view it&lt;/a&gt;&lt;/video&gt;</p><p>These visualizations, and the aircraft example from beginning of this post point to \\(\mathcal{S}^2\\) not having *continuous symmetry* of its Tangent Bundle. This lack of Tangent Bundle symmetry can be proven rigorously by using the [Hairy Ball theorem](https://en.wikipedia.org/wiki/Hairy_ball_theorem). Unfortunately, this means that we cannot take advantage of **all** the benefits of symmetry when optimizing over \\(\mathcal{S}^2\\). Specifically:</p><p>- There is no invariant vector field under a set of global transformations<br>- We can’t *recenter* a point of interest to do tangent-space computations in a more convenient location<br>- Each tangent space must be regarded as distinct and not simply *shifted copies* of an origin tangent space</p><p>Ultimately, optimizing over a generic asymmetric manifold is a large increase in complexity as opposed optimizing over Euclidean Space or Lie Groups. For each computation, we must perform careful book-keeping to ensure the coherence between:</p><p>- Points on the manifold: \\(p \in \mathcal{S}^2\\)<br>- Vectors in the tangent spaces at each point: \\(v \in T\mathcal{S}^2\\)<br>- Exponential maps to “move along the manifold”: \\(\text{Exp}_p(v)\\)<br>- Logarithmic maps to compute the difference between points: \\(\text{Log}_p(q)\\)</p><p>If this coherence is not maintained and properly modeled, the optimization is likely to fail. After really digging into the full complexity of optimization over a generic asymmetric manifold, Lie Groups and Euclidean Space seem even more pleasant to work with.</p><p>## “Grouping” it All Up</p><p>As we can see, optimization over Lie Groups (including Euclidean space) is actually a special case of optimization over a manifold. With *continuous symmetry,* we can actually ignore the effects of the starting point on the exponential operator*. Continuous symmetry* allows us to recenter the manifold on our starting point \\(g\\) (the Lie algebra) and *pretend* that we started at the origin. This is *incredibly handy* and in robotics we use this property a lot!</p><p>You’ll notice that in the optimization steps For Lie Groups, we only used the exponential and logarithmic maps at one point: the identity \\(e\\). From a mathematical perspective, this is **the** defining characteristic of Lie Groups. This defining characteristic can be described as an isomorphism between</p><p>- The group itself \\(\mathcal{G}\\)<br>- The Lie Algebra \\(\mathfrak{g} = T_e\mathcal{G}\\)<br>- The invariant vector fields on \\(\mathcal{G}\\)</p><p>There is a number of deep mathematical connections that can be made here, and we encourage the interested reader to learn more. However, for our use-case, it means we can rewrite our exponential map as</p><p>$$<br>\text{Exp}_g(v) = g \circ \text{Exp}_e(v)<br>$$</p><p>If you’ve read the previously mentioned “Micro Lie Theory” paper, you’ll recognize this as the “circle plus” operator i.e. \\(g \oplus v\\). With Lie Groups, there **really** is only one exponential map of concern — the one defined at the identity. This does not hold true for general manifolds. This is why when talking about Lie Groups **the** exponential \\(\text{Exp}\\) map will be described instead of mentioning exponential maps at each point.</p><p>So, when using Lie Groups for optimization, we can ignore the specifics about “which tangent space” we’re in. We operate either on the group \\(\mathcal{G}\\), or in the [Lie Algebra](https://en.wikipedia.org/wiki/Lie_algebra) \\(\mathfrak{g}\\). To recall our tools analogy, In the land of Lie Groups, we have two tools to master: a hammer \\(\mathcal{G}\\) &nbsp;and… screwdriver \\(\mathfrak{g}\\).</p><p>---</p><p>Did you have fun? We sure did. Thanks for going on this deep dive into Differential Geometry and its implications for optimization problems in robotics. Everyone loves Lie groups, no doubt about it.</p><p>Of course, if you’re experiencing these concerns in your day-to-day development, it’s probably time you considered working with the Tangram team. We’ve solved these problems so many different ways, on twice as many platforms. It’s our specialty! All this cool math in an easy-to-use package that can shorten your time to market; what’s not to love?</p><p>‍</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What made Dostoevsky's work immortal (143 pts)]]></title>
            <link>https://thoughts.wyounas.com/p/what-made-dostoevsky-immortal</link>
            <guid>42214331</guid>
            <pubDate>Fri, 22 Nov 2024 14:59:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thoughts.wyounas.com/p/what-made-dostoevsky-immortal">https://thoughts.wyounas.com/p/what-made-dostoevsky-immortal</a>, See on <a href="https://news.ycombinator.com/item?id=42214331">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em>Along with air, earth, water, and fire, money is the fifth natural force a human being has to reckon with most often. This is one, if not the main, reason why today, one hundred years after Dostoevsky’s death, his novels preserve their relevance. – Joseph Brodsky.&nbsp;</em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F363b2e5d-d3f7-479c-b1e8-3bfdc37e547c_6240x4160.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F363b2e5d-d3f7-479c-b1e8-3bfdc37e547c_6240x4160.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F363b2e5d-d3f7-479c-b1e8-3bfdc37e547c_6240x4160.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F363b2e5d-d3f7-479c-b1e8-3bfdc37e547c_6240x4160.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F363b2e5d-d3f7-479c-b1e8-3bfdc37e547c_6240x4160.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F363b2e5d-d3f7-479c-b1e8-3bfdc37e547c_6240x4160.jpeg" width="1456" height="971" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/363b2e5d-d3f7-479c-b1e8-3bfdc37e547c_6240x4160.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1503923,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F363b2e5d-d3f7-479c-b1e8-3bfdc37e547c_6240x4160.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F363b2e5d-d3f7-479c-b1e8-3bfdc37e547c_6240x4160.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F363b2e5d-d3f7-479c-b1e8-3bfdc37e547c_6240x4160.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F363b2e5d-d3f7-479c-b1e8-3bfdc37e547c_6240x4160.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Image sourced from pexels.com</figcaption></figure></div><p><span>Joseph Brodsky is a name I wasn’t familiar with until a few months ago. I read about him for the first time in one of the pieces in </span><em>The New Yorker</em><span> magazine. I enjoyed reading that piece. Whenever I like some writing that makes a good impression of another author, then often I do more research about the recommended author and if the appeal remains I go ahead and buy their books and either put them in the library to be read later or, if the urge is strong, I try to read as soon as I get my hands on them. So I went ahead and bought a book of essays, titled “Less than one”, by Joseph Brodsky.&nbsp;</span></p><p>I’m glad I did.&nbsp;</p><p>He didn’t write ordinary essays, some of his essays are profound. Just as a beautiful sunrise captures your gaze, some of his insightful prose holds your attention. Joseph Brodsky’s mastery of language isn’t surprising given his background as a poet, the mastery shines through in his prose. If you enjoy reading good prose, you’ll miss out on a real treat if you don’t read prose written by a skilled poet like him.</p><p>As an aside, I just discovered that he died at the age of 57 and, judging by the amount and quality of his wiring, it seems he lived quite a life (not to forget he also won a Nobel prize for literature).</p><p>Anyway, back to his essays. One of his essays is about Dostoevsky, one of the great Russian novelists, and it’s a profound one not just because it’s about a profound writer but because it offers some subtle insights about Dostoevsky’s work.</p><p><span>Immortality of Dostoevsky’s art is unquestionable; his art will likely continue to live on. One of the first questions that Brodsky tackles in his essay is why do Dostoevsky's works preserve their relevance? Brodsky notes (though the following is quoted at the top, but repeating here for convenience):</span><br><span>“Along with air, earth, water, and fire, money is the fifth natural force a human being has to reckon with most often. This is one, if not the main, reason why today, one hundred years after Dostoevsky’s death, his novels preserve their relevance.”</span></p><p>Money never followed Dostoevsky; he had to follow it. Just as sharks pursue their prey, debtors and deadlines pursued him. There is a story about him that gives chills. He once signed a contract with a publisher on perilous terms: if he missed the deadline, the publisher would gain the rights to all his current and future works. He had one year to complete the project, but he did nothing for eleven months. In the final month, he hired a stenographer and dictated the entire book to her and, remarkably, he finished the book just in time. (And a few days later, he married the stenographer, a fitting celebration perhaps.) It's somewhat comforting to know that even great minds procrastinated heavily! But yes, they also produced timeless art.&nbsp;</p><p>Near the start of the essay, Brodsky notes: “For the best way to avoid mistakes in dealing with the future is to perceive it through the prism of poverty or guilt.”</p><p>There is something to ponder here. The road from poverty to prosperity is never guaranteed, but poverty can drive people to prosperity if people develop the required discipline and relentless drive for excellence. The lives of many great men and women stand as a testament to this.</p><p>Brodsky then comes back to the part where he connects the dots about why money is the reason Dostoevsky's work preserves relevance. Brodsky shares an excerpt about Dostoevsky from the diary of Russian socialite Elizaveta Stackenschneider:</p><p>“. . .&nbsp; but he is a petit bourgeois, yes, a petit bourgeois. Not of the gentry, nor of the clergy, not a merchant, nor an odd ball, like an artist or scholar, but precisely a petit bourgeois. And yet this petit bourgeois is the most profound thinker and a writer of genius . . . Now he frequents the house of the aristocracy and even those of the high nobility, and of course he bears himself with dignity, and yet the petit bourgeois in him trickles through. It can be spotted in certain traits, surfacing in private conversation, but most of all, in his works . . . in his depiction of big capital he will always regard 6,000 rubles as a vast amount of money.”</p><p>And Brodsky comments:</p><p>“What Mme Stackenschneider, a product of her epoch’s social stratification, calls petit bourgeois is known today as middle class, as defined in terms of annual income and not social affiliation. In other words, the said amount means neither great riches nor screaming poverty, but a tolerable human condition: a condition that makes one human.”</p><p>And he continues:</p><p>“A writer who regards six thousand rubles as a vast amount of money operates, therefore, on the same physical and psychological plan as the majority of people; i.e., he deals with life on its own general terms, since, like every natural process, human life gravitates toward moderation. Conversely, a writer who belongs to the upper echelon of society or to its lower depths will invariably produce a somewhat distorted picture of existence, for, in either case, he would regard it at too sharp an angle. Criticism of society (which is a nickname for life) from either above or below may produce a great read; but it’s only an inside job that can supply you with moral imperatives.</p><p>Furthermore, a middle-class writer’s own position is precarious enough to make him view what goes on below with considerable keenness. Alternatively, the situation above, due to its physical proximity, lacks in celestial appeal. Numerically, to say the least, a middle-class writer deals with a greater variety of plights, increasing, by the same token, the size of his audience. In any case, this is one way to account for the wide readership enjoyed by Dostoevsky, as well as by Melville, Balzac, Hardy, Kafka, Joyce, Faulkner. It looks as if the equivalent of six thousand rubles ensures great literature.”</p><p>This is an incisive observation. It’s not to say that the rich can’t write but, generally speaking, and as Brodsky hints above, a precarious financial condition is often a precondition for great literature, especially one with moral imperatives. There is something profound about the human condition which is not too rich and not too poor to have sensibilities required to produce great literature. Just enough suffering that fuels great literature.&nbsp;</p><p>Perhaps there is another reason, beyond the precarious position of the middle-class writer: a middle-class person must navigate every twist and turn of life on their own. And when you confront life at every turn, you can’t escape its realities. You inevitably notice nuances of human psychology and morality that a wealthy person might miss, as luxuries can keep them at a distance.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45cc327c-19df-492e-8be9-b450600f5715_1024x1024.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45cc327c-19df-492e-8be9-b450600f5715_1024x1024.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45cc327c-19df-492e-8be9-b450600f5715_1024x1024.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45cc327c-19df-492e-8be9-b450600f5715_1024x1024.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45cc327c-19df-492e-8be9-b450600f5715_1024x1024.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45cc327c-19df-492e-8be9-b450600f5715_1024x1024.webp" width="510" height="510" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/45cc327c-19df-492e-8be9-b450600f5715_1024x1024.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:510,&quot;bytes&quot;:366984,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45cc327c-19df-492e-8be9-b450600f5715_1024x1024.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45cc327c-19df-492e-8be9-b450600f5715_1024x1024.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45cc327c-19df-492e-8be9-b450600f5715_1024x1024.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45cc327c-19df-492e-8be9-b450600f5715_1024x1024.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Life wasn’t easy for many great writers, and honestly, there would have been nothing remarkable about their lives if they had been easy. Many struggled financially. I randomly looked up the profiles of a few Nobel prize winners in literature. Though my data sample is small, none of them had a great financial situation when they were starting as a writer. Gabriel García Márquez, a Nobel Prize winner, while writing his magnum opus </span><em>One Hundred Years of Solitude</em><span>, “sold his car so his family would have money to live on while he wrote. Writing the novel took far longer than he expected; he wrote every day for 18 months. His wife had to ask for food on credit from their butcher and baker as well as nine months of rent on credit from their landlord.” [1] The road to greatness for him wasn’t built with riches. Those 18 months seemed far from a “tolerable human condition,” and you’ve got to give him credit for mustering all his talent and courage to not only produce, under such difficult circumstances, another book, but to create a remarkable work of fiction that brought South American literature into the spotlight. Most people lose their wits when they don't know how they'll put food on the table next month.</span></p><p><span>Another Nobel prize winner, William Faulkner, was born in poverty. He first attempted to join the army but later dropped out. “After dropping out, he took a series of odd jobs: at a New York City bookstore, as a carpenter in Oxford, and as the Ole Miss postmaster. He resigned from the post office with the declaration: ‘I will be damned if I propose to be at the beck and call of every itinerant scoundrel who has two cents to invest in a postage stamp.’“ [2] Definitely not great riches nor screaming poverty. He also had the courage of a genius because not many have the courage to put out the beck and call of their masters. “By 1932, Faulkner was in need of money. He asked Wasson to sell the serialization rights for his newly completed novel, </span><em>Light in August</em><span>, to a magazine for $5,000, but none accepted the offer.” [2] Life must have not been easy for Faulkner.&nbsp;</span></p><p>For another Nobel prize winner, Gabriela Mistral, “Poverty was a constant presence in her early life.” [3] Poverty was a stimulus for another winner, Albert Camus: “ His identity and poor background had a substantial effect on his later life.” [4]&nbsp;</p><p>One needs to further this analysis of evaluating financial well-being of great writers with a larger data pool, but none of the above writers had great riches. It seems that a certain kind of financial struggle often serves as a catalyst for writers to produce great works. That may not be true for all writers from all kinds of social and financial backgrounds, but some seem to thrive only under such circumstances.&nbsp;</p><p>Another question that often captures my attention, as a writer and a reader, is what makes Dostoevsky a great writer? His narratives? His writing style? Brodsky believes it’s neither. He observes:</p><p>“Almost without exception, all his novels are about people in narrow circumstances. This kind of material itself guarantees absorbing reading. However, what turned Dostoevsky into a great writer was neither the inevitable intricacy of his subject matter nor even the unique profundity of his mind and his capacity for compassion; it was the tool or, rather, the texture of the material he was using, i.e., the Russian language.”</p><p>He adds:</p><p>“Its polysyllabic nature (the average length of a Russian word is three to four syllables) reveals the elemental, primeval force of the phenomena covered by a word a lot better than any rationalization possibly could, and a writer sometimes, instead of developing his thought, stumbles and simply revels in the word’s euphonic contents, thereby sidetracking his issue in an unforeseen direction. And in Dostoevsky’s writing we witness an extraordinary friction, nearly sadistic in its intensity, between the metaphysics of the subject matter and that of the language.</p><p>He made the most of Russian’s irregular grammar. His sentences have a feverish, hysterical, idiosyncratic pace and their lexical content is an all but maddening fusion of belles lettres, colloquialisms, and bureaucratese. True, he never wrote at leisure. Much like his characters, he worked to make ends meet: there was always either creditors or a deadline. Still, for a man beset with deadlines, he was extraordinarily digressive, and those digressions, I venture to say, were prompted more by the language than by the requirements of a plot. Reading him simply makes one realize that stream of consciousness springs not from consciousness but from a word which alters or redirects one’s consciousness.”</p><p>I must admit, it takes some mastery of language to recognize the subtle role of language, as Brodsky did. After all, only a star can fully appreciate the beauty of another star.</p><p>I do wonder though if Dostoevsky’s digressions were only due to the nuances of Russian language? Did not his own suffering, of being a victim to epilepsy and then of Siberian exile, play any part in his narrative explorations? Was it not his intent to dig deeper into moral questions that play some part in his narrative explorations?&nbsp; I believe all this suffering must have played some role in shaping his thoughts and imaginations. A person who hasn’t endured hardship may struggle to capture the subtle complexities of human behavior when faced with adversity.</p><p>There is another insight about Dostoevsky that struck a chord with me. Brodsky notes near the end of the essay:</p><p>“From classicism, he took the principle that before you come forth with your argument, however right or righteous you may feel, you have to list all the arguments of the opposite side. And it is not that in the process of listing them one is being swayed by the opposite side; it is simply that the listing itself is a mightily absorbing process. One may not in the end drift away from one’s original stance, but after having exhausted all the arguments on behalf of evil, one utters the creed’s dictums with nostalgia rather with fervor. This, in its own way, also fosters the case of verisimilitude.”</p><p>This is beautifully expressed. Few can argue their opponent's case better than the opponents themselves. Rare, very rare is this skill, probably because it’s not easy to develop. But when someone possesses it and uses it masterfully, we must listen. Dostoevsky had this gift. Perhaps that’s why we still pay attention to his works. Life would be a little calmer if all were trained to adopt this approach whenever we found ourselves in disagreement.</p><p>Overall, the essay sparked some interesting thoughts on the life of a great writer. It’s enlightening to read a critique of Dostoevsky’s work by Joseph Brodsky—not just because his prose is above average, but also because his unique polyglot skills and extensive reading make his perspective stand out.</p><p>And it’s hard to refute Brodsky about the fact that being able to argue better can enrich fictional arcs; it can open up many pathways for the writer to follow in his narration that otherwise wouldn’t have been possible. Such an author can offer readers not just a richer reading experience but also a richer emotional experience. If fiction broadens our general awareness, then the writing of an author skilled in analyzing different viewpoints and adept at using language to probe the depths of the human experience also offers us a more profound emotional journey. A journey that stirs our souls.&nbsp;</p><p>Is there some correlation between an author's financial situation and the quality of their fiction writing? It's possible that the authors' financial conditions influence how they shape their characters, how their characters talk and act, or even how their characters transform over time. As Brodsky noted, you have to be in a certain kind of a financial situation to gather subtle insights about human psychology and morality. Although this poses an interesting dilemma for those that are financially well off and still want to write with incredible depth: how do they gather such subtle insights? Because though you can adapt the lifestyle of a financially struggling person, it's hard to replicate the state of mind of an impoverished individual. A wealthy person aiming to write great fiction might move into a middle-class neighborhood to experience the "tolerable human condition," but as long as their pockets remain full, they cannot truly capture the mental anguish of someone facing real financial hardship.This line of thought deserves an essay of its own though.&nbsp;</p><p>In the end, Brodky’s insightful insights about a great author makes his essay a special one.&nbsp;</p><p>References:</p><ol><li><p><a href="https://en.wikipedia.org/wiki/Gabriel_Garc%C3%ADa_M%C3%A1rquez" rel="">https://en.wikipedia.org/wiki/Gabriel_Garc%C3%ADa_M%C3%A1rquez</a><span>&nbsp;</span></p></li><li><p><a href="https://en.wikipedia.org/wiki/William_Faulkner" rel="">https://en.wikipedia.org/wiki/William_Faulkner</a><span>&nbsp;</span></p></li><li><p><a href="https://en.wikipedia.org/wiki/Gabriela_Mistral" rel="">https://en.wikipedia.org/wiki/Gabriela_Mistral</a><span>&nbsp;</span></p></li><li><p><a href="https://en.wikipedia.org/wiki/Albert_Camus" rel="">https://en.wikipedia.org/wiki/Albert_Camus</a><span>&nbsp;</span></p></li></ol></div></article></div></div>]]></description>
        </item>
    </channel>
</rss>