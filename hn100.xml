(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 22 Feb 2025 16:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Do US tech firms realize the backlash growing in Europe? (125 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43139172</link>
            <guid>43139172</guid>
            <pubDate>Sat, 22 Feb 2025 14:24:21 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43139172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="43139424"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139424" href="https://news.ycombinator.com/vote?id=43139424&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>For what it counts, I deleted my Amazon account today (and lost all my Kindle books in the process), I'll never buy anything from them again. 
I'll close my S3 storage account with a US company in the coming months : I'll switch to a German service.
In the coming weeks, I'll also delete my GMail account (my devices are already deGoogled since many years).
There a lot of European alternatives that respect our privacy, our values and principles.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139488"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139488" href="https://news.ycombinator.com/vote?id=43139488&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>I'm in the U.S. and doing the same.  While EU laws don't protect me as well as they would someone on the correct side of the pond, I do still benefit from better integrity and transparency.</p><p>Would you be up for making recommendations?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139703"><td></td></tr>
                  <tr id="43139495"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139495" href="https://news.ycombinator.com/vote?id=43139495&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>Beware if you want to use Hetzner's s3, they had a massive repeated outage over the past few days and don't offer any reliability guarantee, not even something like 99.9%. Maybe you're thinking of something else, or don't care if you can't upload/download for a few hours here and there - just wanted to warn as I was surprised by this as a user.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139574"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139574" href="https://news.ycombinator.com/vote?id=43139574&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Yes, I was considering Hetzner or Scaleway, I have a few months to decide before my current prepaid plan expires.</p><p>Thank you for the heads-up !</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139697"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43139697" href="https://news.ycombinator.com/vote?id=43139697&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>I've had 0 issues with Hetzner's servers, they're great at that. I hoped this would translate to cloud services as well, but it turns out (not surprisingly in hindsight) that offering a fully managed service is something completely different than cheap and easily administered server.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43139505"><td></td></tr>
                <tr id="43139787"><td></td></tr>
                  <tr id="43139510"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139510" href="https://news.ycombinator.com/vote?id=43139510&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>From a software developer viewpoint, my biggest worry is about GitHub, owned by Microsoft.
I self host my open source code with Forgejo.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139722"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139722" href="https://news.ycombinator.com/vote?id=43139722&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>Can you explain what exactly your worry is with GitHub? If you have your repos locally, you can always go somewhere else if/when something happens with GitHub. Except if you rely on things like Github actions or other features that are harder to move.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139796"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43139796" href="https://news.ycombinator.com/vote?id=43139796&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>GitHub Actions, the issues system, pull request history, discussions, wiki.</p><p>Truth is, GitHub has a whole suite of really great features that's hard to move. Especially for open sourced projects/communities</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43139536"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139536" href="https://news.ycombinator.com/vote?id=43139536&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>I don't know how to degoogle at this point. 90% of my registered accounts across the internet are with my Gmail account because I didn't want them tied to my private email.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139760"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139760" href="https://news.ycombinator.com/vote?id=43139760&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>One at a time.</p><p>If you have the time and are interacting with the account, take the second to change it away from gMail. Insisting on doing them all immediately is setting up a Herculean task that'll almost certainly leave you demoralized. This gets your most-used (and presumably most important) accounts first,and feels much smaller and more manageable.</p><p>After 6 (or 18, whenever) months of this you can summon some motivation to change over the last 20 accounts and be done with it.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139768"><td></td></tr>
            <tr id="43139720"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139720" href="https://news.ycombinator.com/vote?id=43139720&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>You should probably spend some time on changing that. The only thing keeping me inside the Google ecosystem is my university and I hate that since they are paying for it anyway, so it makes absolutely no sense if not for it being the lazy way of doing things. Why choose to be lazy when it's clearly doing more harm than good?
I managed to install android with microg and it has worked flawlessly for 4 years now. There are many options to do this of course, but they all require a lot of time to set up. Still a better alternative than being locked up under Google.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139737"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139737" href="https://news.ycombinator.com/vote?id=43139737&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>Years ago, when I saw the invitation "Sign in with your Google account" popping up on every site, I suspected it was a trap to hold users hostage.
Take a private email address and move away from Gmail one service at time.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43139518"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139518" href="https://news.ycombinator.com/vote?id=43139518&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>I've sold all my holdings in US companies to instead invest in STOXX Europe 600 alternatives. I know others here in Sweden that have said they have done the same.</p><p>In Sweden, Russia has always been the enemy.</p><p>My prediction is that France and Germany will soon join the Joint Expeditionary Force (JEF) and it will be the end of NATO. UK will probably join the Maximator intelligence alliance.</p><p>The death of Five Eyes and NATO in 2025 was not on my bingo card. Good luck with your new alliance with the Russians, Americans!</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139537"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139537" href="https://news.ycombinator.com/vote?id=43139537&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>I think it depends a lot on what happens in the elections 2 and 4 years from now. The world may forgive or may be very well justified in their turn. I’m in the enterprise space and there’s not a lot of open acknowledgement of changes yet. I don’t think my particular company thinks there’s a way this is good for them.</p><p>To some degree I think it’s being treated as yet another line item in the long list of things that need to be managed in global companies of a certain size. I presume that the threat of retaliation (see story about X advertisers) is keeping voices quiet too.</p><p>It’s different this time… it’s going to get weird. And possibly dangerous. Talk of third term, crashing the economy as a pretense for parting out the assets of the federal government, finding a reason to cancel the next election, and being the war machine for our enemies may not feel like the purview of technology companies even as they angle for federal contracts to “support” some of these things.</p><p>We need more bodies at protests. Not everything announced has gone into effect, but they’re testing all the things they want to make happen. And they’re persistent.</p><p>The public haven’t yet seen the spark that may light the next resistance fire. It may not feel like we have moves available to us, but public boycotts, protests/ marches, calls to our politicians, honking for peace, supporting our neighbors — these are all reps in the resistance. They are recruitment and rallying measures. They are all little tiny sparks, from which something may alight. Look for something happening today and show up.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139766"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139766" href="https://news.ycombinator.com/vote?id=43139766&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>People keep mentioning elections in two or four years, I guess assuming that the MAGA mafia could lose and be ousted. I think it’s 100% wishful thinking. There is no way they’ll let go of power peacefully. But my bet is they won’t even technically lose, thanks to an arsenal of massive disinformation, voter suppression, gerrymandering, intimidation, up to jailing opponents and so on.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43139383"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139383" href="https://news.ycombinator.com/vote?id=43139383&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Tesla FSD wont be authorized in Europe, this will be one direct consequence of all of that.</p><p>In addition, tariffs will be put on cars coming from the US.</p><p>Plus, US is now recognized as a very unreliable partner in terms of defense, that Europe regrets buying their systems.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139553"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139553" href="https://news.ycombinator.com/vote?id=43139553&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>I have a Tesla with so-called FSD in Europe, and it still feels like it’s ten years away from being road-safe. It can’t even recognize speed limit signs correctly.</p><p>I bought this car six years ago paying for the FSD feature upfront, and since then Tesla and their CEO have been constantly lying about it. I’ll never buy another car from this company just for this reason.</p><p>I think Musk probably sees the writing on the wall. Tesla’s brand is destroyed in Europe and they can’t ship these promised advanced features in this market. But he doesn’t mind giving up the European market because he’s got a sweet deal at home where his businesses are now part of a 1930s-style union between corporations and authoritarian government. The gains from that arrangement far outweigh the headaches of trying to sell cars globally.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139525"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139525" href="https://news.ycombinator.com/vote?id=43139525&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>There's already 10% tariff on cars from US imported into Europe. 
But somehow I have feeling that even if it would be reduced to 0%, it wouldn't make a big difference Cars of US brands are generally terrible, not sure how they fulfill EU emission standards, have poor MPG which with EU gas prices are very expensive in maintenance.
Maybe Japanese, Korean or EU brands manufactured US could do better.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139586"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139586" href="https://news.ycombinator.com/vote?id=43139586&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>Tesla FSD should never have been authorized in the US tbh. The safety record is horrifying. The whole reason Google didn't buy Tesla out early on is because they found out how much of their autonomous driving tech claims were all just marketing. Mercedes-Benz and many other vehicles are way ahead of Tesla on this front but Tesla still goes the most in on marketing this feature</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139476"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139476" href="https://news.ycombinator.com/vote?id=43139476&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>Given the constant promises and slow rate of improvement, there's a high chance Tesla's FSD was never going to be authorised in Europe even absent this.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139522"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139522" href="https://news.ycombinator.com/vote?id=43139522&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>I think this is true. Given the regulatory environment, it's hard to imagine FSD <i>ever</i> getting approved -- even if it actually does get truly safer than the average driver! Europe does not want innovation. And it will not take the (admittedly non trivial) risks required to have it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43139406"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139406" href="https://news.ycombinator.com/vote?id=43139406&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>Will this really happen? Or will the US prevent Europe from doing that by saying “then defend from Russia all by yourself?”</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139475"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139475" href="https://news.ycombinator.com/vote?id=43139475&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>The US has basically said that. It doesn't matter though, because at the end of the day, dramatic foreign policy shifts and realignments only make the U.S. even less attractive as an ally and a partner.
Decades of relationships are being strained in the attempt of mending ties with a state that never really did much to advance America's interests in the modern era. And it could all be completely undone in four years. The whip lash of back and forth, radical shifts in policy, are as damaging as the acts that directly alienate America's global partners.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139449"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139449" href="https://news.ycombinator.com/vote?id=43139449&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>That's already expected. Right now, preparations are made for nuclear (re-)armament of europe. Germany in particular is working towards that at this moment [EDIT: politically. not in the "actively preparing weapons"-phase.].</p><p>Also, france has a first-strike policy.</p><p>So even without US troops, europe will be fine.</p><p>A sizable part of the population has wanted the US out of especially germany for a looong time now, so those movements have become pretty popular again. I haven't heard "ami go home" in a long time, but right now it's common.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139620"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43139620" href="https://news.ycombinator.com/vote?id=43139620&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>&gt; Right now, preparations are made for nuclear (re-)armament of europe. Germany in particular is working towards that at this moment.</p><p>Can you provide a source for this? It would be big if true, in particular the Germany claim. I've not heard it and couldn't find things with a quick google search.</p><p>Germany is currently already doing nuclear weapons sharing with the US, i.e. they have access to US nuclear weapons though of course with some restrictions.</p><p>What you wrote sounds like Germany would be working towards having new nuclear weapons to be produced in Europe without US involvement and shared without US restrictions.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139674"><td></td></tr>
                <tr id="43139753"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_43139753" href="https://news.ycombinator.com/vote?id=43139753&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Thanks, that's helpful! To break it down for others: Germany is considering making a similar deal for sharing with France as it currently has with the US. So it wouldn't really be a (re)armament, more an increase in resilience by not just relying on US for sharing but adding France.</p><p>I think that's quite an important difference from your original wording.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43139512"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43139512" href="https://news.ycombinator.com/vote?id=43139512&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>&gt;&gt;france has a first-strike policy.</p><p>It's served us well having a member of NATO saying they'll go full-on nuts. One of the best deterrents ever. Thanks France, with love, USA.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139738"><td></td></tr>
                  <tr id="43139627"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139627" href="https://news.ycombinator.com/vote?id=43139627&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>&gt; Will this really happen? Or will the US prevent Europe from doing that by saying “then defend from Russia all by yourself?”</p><p>If push came to shove, the only ally Ukraine <i>needs</i> to equal Russia (as it is today) is Poland.</p><p>They'd rather not have to as it would be expensive, but Poland could match 100% of current outside assistance for Ukraine[0] for a smaller fraction of their GDP than Ukraine itself is currently spending on the war[1].</p><p>The USA and Russia agreeing with each other to carve up Ukraine would be a much harder battle.</p><p>Well, it would be harder unless DOGE actually does make good on the claim of $2T cuts, because the only way of reaching $2T without touching the "mandatory" budget (mandatory = social security etc.) is to delete the entire armed forces and the CIA (and basically everything else) and just under half the interest payments on the loans, which in turn means they'd have no power <i>to</i> carve anything up.</p><p>[0] $380bn over the first 2 years according to Wikipedia, so lets say $190bn/year</p><p>[1] Ukraine plans to spend ~$53.7 billion in 2025, about 26% GDP</p><p>Poland's GDP is $915, 26% of that would be $237.9 billion / year.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139538"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139538" href="https://news.ycombinator.com/vote?id=43139538&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>This is already the expectation from our leaders. It is the belief of a good % of EU leaders that, if Russia were to push into Poland and the Baltic states, and Article 5 was invoked, the US would not respond. Or at least not without some Versailles Treaty-level extortion, as they're trying to do with Ukraine.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139441"><td></td></tr>
                <tr id="43139517"><td></td></tr>
                <tr id="43139690"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43139690" href="https://news.ycombinator.com/vote?id=43139690&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>Which ironically means the US also loses it's leverage. They're already threatening to functionally (if not actually) withdraw from NATO and economically split off via tariffs. The next step is actual invasion, which while sorta-kinda has been talked about with Greenland, probably won't happen.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43139503"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139503" href="https://news.ycombinator.com/vote?id=43139503&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>That's not a problem, US is doing that, effectively marginalizing the importance they will have in the future.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139479"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139479" href="https://news.ycombinator.com/vote?id=43139479&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Bidens aid undoubtedly moved the needle, but that’s over and from what I’ve heard weapon deliveries have already stopped.</p><p>So I guess my question is aren’t Europe already by themselves?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139662"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43139662" href="https://news.ycombinator.com/vote?id=43139662&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>Trump can still make things worse, e.g. explicitly saying "If Putin invaded Estonia, US would stay out of it" or things like this. He hasn't said that (I hope) so there's still some strategic ambiguity - i.e. it's not clear for Putin if Trump is just uttering empty words re letting Europe go or whether he would in the end do defend.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43139560"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139560" href="https://news.ycombinator.com/vote?id=43139560&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>I'm a US Millennial and my impression is that the US has spent my adult lifetime destroying its soft power, and this week is really just a culmination of that. US tech firms do not and have never (at least in my time working for them) cared about anything other than a pathological pursuit of profit that borders on nihilistic paperclip-optimization.</p><p>I believe that US firms see European tech companies as slow/lazy and think that good engineers only seek high compensation which is available to them in the US. Certainly there are some who are motivated by that, but I have known some great engineers in Europe which was one of my reasons for cautioning my American peers about pushing too hard for WFH; if they can do the job from home, then so can others in countries with lower costs of living.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139751"><td></td></tr>
            <tr id="43139519"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139519" href="https://news.ycombinator.com/vote?id=43139519&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>I said this else where, but, dramatic foreign policy shifts and realignments only make the U.S. even less attractive as an ally and a partner.
Decades of relationships are being strained in the attempt of mending ties with a state that never really did much to advance America's interests in the modern era. And it could all be completely undone in four years. The whip lash of back and forth, radical shifts in policy, are as damaging as the acts that directly alienate America's global partners.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139490"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139490" href="https://news.ycombinator.com/vote?id=43139490&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>I think you are hugely overestimating the impact in Europe. Some politically aware are switching to non-US big tech, if possible. But the vast majority either doesn't care about politics, think it's too much effort, or even agrees with what is happening in the US right now.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139704"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139704" href="https://news.ycombinator.com/vote?id=43139704&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>As an European I can confirm this is partly true. I wouldn't say it's completely destroyed it's soft power, but politically we do feel betrayed so whenever possible people would be looking to switch to non-US product. However, in business we're so intertwined with the US that it's hard to tell right now if there ever will be a justified switch.</p><p>In terms of pensions - I think 99.9% of us have a huge chunk the S&amp;P, so even though we're upset I doubt there will be any movement on that front at all. Money will win in this battle.</p><p>Personally I do feel betrayed as well and for the first time in my life I've started looking for where the product I consume come from. About a third is from the US. Stopped buying them, found a European alternative.</p><p>To all the Europeans out here - let them live how they want over the pond, let's use this opportunity to promote our industries and products as well as become as independent as we possibly can given our current financial and political constraints.</p><p>No hard feelings at all, live and let live.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139454"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139454" href="https://news.ycombinator.com/vote?id=43139454&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Switching search engines to what? AI-powered search also created by American companies?</p><p>America's image is toxic to everyone right now (including its own citizens). But I don't really see that many valid alternatives. It's not like Europe has done much to cultivate any kind of relevant software industry.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139640"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139640" href="https://news.ycombinator.com/vote?id=43139640&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>LLMs are something everybody can reproduce with enough money, as shown by Musk itself recently: in one year they have Grok3 which is a SOTA model, without having particular talents or innovations, just following the recipe everybody else is using (and this is why you see new companies competing with established companies: all are training transformers with a lot of tokens, basically). So Europe can have their LLMs as well: the important thing is to relax this stupid AI act, that the biggest countries in Europe didn't want (Germany, France, Italy) but that somewhat passed (and it is not impossible that there are external influences and corruptions). About the search engine: Google at this point is almost a joke, to the point it is simpler to redo it right than trying to fix it from the internal. Remember that a lot of key technologies are created by europeans: from Python to Linux, to MySQL and so on. Europe has all the potential needed <i>if the right choices are made</i> to do everything needed.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139732"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139732" href="https://news.ycombinator.com/vote?id=43139732&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>&gt; So Europe can have their LLMs as well: the important thing is to relax this stupid AI act</p><p>But you're kind of showing here <i>why</i> Europe doesn't have the software innovation America does.</p><p>Americans are not inherently smarter or more capable of doing innovative things than Europeans. It's just that whenever a European wants to do something interesting, there's a handy European regulation making it harder or less profitable to do it.</p><p>If this American administration is the impetus for that to change, that would be good! But I doubt it.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139770"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43139770" href="https://news.ycombinator.com/vote?id=43139770&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>I believe that certain drives for certain regulations that are clearly shooting themselves in the feet were approved with less clear processes than needed (remember corruption cases of certain European politicians recently for the Olympics? If it happens for minor stuff like that... go figure). This was acceptable even if many countries were unhappy as long as the US were believed to be a strong partner. Now this changed, and it is likely that certain regulations will be relaxed.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43139595"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139595" href="https://news.ycombinator.com/vote?id=43139595&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>I don’t understand this notion suggesting Americans are the only people on earth capable of software development. This comes up in various threads about China, too.</p><p>For independent indexes, Brave’s search came from a German company and Yandex certainly isn’t American.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139744"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139744" href="https://news.ycombinator.com/vote?id=43139744&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>There's a certain irony in switching to a Russian search engine... because America aligned itself more closely with Russia.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43139664"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139664" href="https://news.ycombinator.com/vote?id=43139664&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>I'm a USian and I use Yandex frequently. I suspect this puts me on some sort of a government watch list, if they bother to sort it out from my VPN endpoint which I generally keep out of the US <i>and</i> EU.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139600"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139600" href="https://news.ycombinator.com/vote?id=43139600&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>As a European, completely eliminating corporate US tech doesn't have to be an all or nothing goal. Switching from Google to DDG or Kagi is an improvement even if they're just a frontend for Bing.</p><p>It's a massive opportunity for European-based companies to compete. I expect a lot more funding to start materialising here as a result.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139721"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139721" href="https://news.ycombinator.com/vote?id=43139721&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>You're right, but cooperation and centralization emerge under external pressures. It's almost like a natural law of social systems. (I could say more but don't want to digress, let me know if you'd like some pointers.)</p><p>In other words, Europe's weakness and dividedness is a <i>consequence</i> of the soft power the US used to have. The EU has the capital (human and economic) to be a world power in principle, but never needed to.</p><p>This has all changed very suddenly. Whether Europe can rise to the challenge I don't know, but it's the first time in a very long time that there is a possibility of it happening.</p><p>I find the game theoretic view quite interesting here. Even if we interpret Trump's action in the best possible light, that he knows what he's doing and playing the madman strategy to get others to comply, the destruction of trust that this strategy causes makes it impossible in the long term to build mutually beneficial cooperative structures.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139520"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139520" href="https://news.ycombinator.com/vote?id=43139520&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Maybe this will galvanize EU into actually creating something instead of just importing and regulating American tech.</p><p>Doubt it though</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43139431"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139431" href="https://news.ycombinator.com/vote?id=43139431&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>I was just thinking today how long will it be until Windows is turned into a weapon. I know I know, but its not really paranoid to think that anymore is it? Imagine the chaos around the world caused by Windows machines simultaneously reformatting hard disks except in America.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139464"><td></td></tr>
            <tr id="43139472"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139472" href="https://news.ycombinator.com/vote?id=43139472&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>This is a strange statement given it's the UK, not the US, that currently poses the greatest threat to OS weaponization via what they're trying to make Apple do with backdoor access.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139569"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139569" href="https://news.ycombinator.com/vote?id=43139569&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>I guess Microsoft would like to maintain trust by their customers. The attack vector is the other direction, if Trump missteps and tries to use force on EU member states, EU will use its economic “bazooka” which for instance would especially target firms like Microsoft.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139501"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139501" href="https://news.ycombinator.com/vote?id=43139501&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>It's really funny to me that people are so partisan. You only worry about backdoors under Trump? As if the CIA has not be orchestrating coups around the world for half a century?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43139701"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139701" href="https://news.ycombinator.com/vote?id=43139701&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>As a US citizen I can only hope that this kickstarts alternatives and paid for options in the spaces big (ad)tech dominates here. These bloated monopolies with short lived products or soon to be acquired competitors are getting old. (And these constant layoffs and shareholder/promo driven development to a smaller degree)</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139439"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139439" href="https://news.ycombinator.com/vote?id=43139439&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>From a European Saas firm. I am reviewing technology partners for my firm, and due to the possible trade wars and polticial climate we have started to treat US-centric partner as a negative.</p><p>That is just the business part of things, the removal of DEI at all cost in conjunction with the Russian dealings and the statements from JD Vance are also a bit hard to be positive about.</p><p>We like the companies but the noise increases risk as opposed to European partners.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139657"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139657" href="https://news.ycombinator.com/vote?id=43139657&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>IMO, this is a great wake-up call for every country out there that over-relies on US tech and considers them a messiah. On the short term, this is going to be painful if US continues to bully and holds tech hostage but in the long term, trying to be as independent as possible will only prove to be advantageous (if done seriously and done right). China is a good example.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139649"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139649" href="https://news.ycombinator.com/vote?id=43139649&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>&gt; the US completely destroyed its soft power this week</p><p>Soft power sounds like manipulation to me. So if we destroyed our manipulation, great. The United States has put its fingers on too many scales rather than respecting the rights of individuals in other countries to manage their affairs the way they see fit.</p><p>I welcome tech competition from Europe. Let the best products and services win.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139726"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139726" href="https://news.ycombinator.com/vote?id=43139726&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Yes, and.</p><p>Something like 40% of the humanitarian contribution in the world came from USAID, and Thanos just fingersnapped it out of existence last Tuesday.</p><p>Until that gap is filled, there will be tons of unnecessary suffering. A "more humane" way would have been to slowly wind down over a period of months/years, giving the market time to react. Even better would have been to work with foreign powers to spin off USAID-backed initiatives and transfer control+funding responsibility to them, but I'm just a girl with a unrealistic dream at that point</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43139514"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139514" href="https://news.ycombinator.com/vote?id=43139514&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Wishing and doing are two different things. Integration of US products and services is still incredibly deep in Europe, as is political cooperation on all layers. I don't think this can be abandoned on a whim, even if people wanted to.</p><p>(Same goes for the other side of the geopolitical aisle: European politicians would have liked a much faster and much more comprehensive economic decoupling from China since at least the start of the Ukraine war. But feedback from the private sector was pretty clear that this would have been economic suicide. Hence the official stance is now "de-risking" instead of "decoupling")</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139552"><td></td></tr>
                  <tr id="43139404"><td></td></tr>
                <tr id="43139548"><td></td></tr>
            <tr id="43139451"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139451" href="https://news.ycombinator.com/vote?id=43139451&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>Check out Andreas Klinger on Twitter, and people he's retweeting and talking about. That should give you a good overview.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139555"><td></td></tr>
                <tr id="43139660"><td></td></tr>
                        <tr id="43139513"><td></td></tr>
                <tr id="43139728"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139728" href="https://news.ycombinator.com/vote?id=43139728&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>I suggest you expand your information sources to include contrary perspectives. These read like the typical hysteriaporn to freak normies out. It would be equivalent to my sharing supporting links from the NYTimes or WaPo.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43139528"><td></td></tr>
            <tr id="43139692"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139692" href="https://news.ycombinator.com/vote?id=43139692&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Look at <a href="https://www.reddit.com/r/worldnews/top/" rel="nofollow">https://www.reddit.com/r/worldnews/top/</a> with timeline=last_month.</p><p>It  has many links to articles with an European viewpoint.</p><p>But you have to read between the lines as EU media does not like to write "this is too crazy to be happening, what are they thinking?".</p><p>E.g. a title like "Macron calls emergency European summit on Trump".  How bad do you think it has to create a EU summit solely for handling the new US relationship?</p><p>tldr: large increase in EU military and which has to be EU made.  US is seen as ending "rule of law" (<a href="https://en.wikipedia.org/wiki/Rule_of_law" rel="nofollow">https://en.wikipedia.org/wiki/Rule_of_law</a>) and has become an unreliable partner, great loss of soft power and prestige.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43139535"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139535" href="https://news.ycombinator.com/vote?id=43139535&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>They don't realize anything and they don't care, all the leadership in these companies have no vision beyond goosing the next quarter's numbers. They have been promoted to the top because of their ability to meet targets, nothing else. If they see weakness in Europe they will compensate by pushing more ads, lay off some people or increase prices to keep the number go up.</p><p>They are all sitting ducks for black swan events, but I don't think getting roasted in Europe will be enough to trouble them.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139558"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139558" href="https://news.ycombinator.com/vote?id=43139558&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>They probably know it very well but there's not much they can do about it since the Democratic party lost so thoroughly in the last election, leaving them little ability to influence the course of events.  Even something as utterly disastrous for US tech companies as the proposed tariff on semiconductors is, as far as I can tell, largely being met by silence.  That's how bad the situation is.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139515"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139515" href="https://news.ycombinator.com/vote?id=43139515&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>My experience is in SaaS marketing.</p><p>EMEA has always been a soft market for tech. And it's always been a cost center for regionalization and compliance.</p><p>A move away from US tech is not necessarily new or surprising. But it's also not as damning to the bottom line as one would expect.</p><p>I will say that the tech sector in Europe is not nearly as robust. While alternatives exist for some products, they are not likely going to find international markets of their own. (Overgeneralization, obv).</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139561"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139561" href="https://news.ycombinator.com/vote?id=43139561&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>It really depends on if Europe hates America more than they hate China (or even Russia). Once Europe embraces Chinese tech over American tech (eg alibaba over Amazon, WeChat over Facebook, baidu over Google, BYD over Tesla) it’s all over for the USA as a world power. If they still hate China more than the USA, then it’s somehow salvageable.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139577"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139577" href="https://news.ycombinator.com/vote?id=43139577&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>I’m not making this up. I was thinking about posting the exact same question an hour ago.
I am questioning my entire tech and communication stack. The first thing I did was download all my emails locally with offineimap. I can’t deny that I’m worried to get rug pulled by US companies any moment now.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139462"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139462" href="https://news.ycombinator.com/vote?id=43139462&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>The majority of Americans do not care and do not really want US tech to be global. The majority of Americans are ashamed of US soft power and the way we meddle with other countries constantly. While loss of dollar hegemony will have economic implicatiosn for the US, American's just don't really care about having dollar hegemony. We don't care about any of these things or what Europe is doing.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139663"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139663" href="https://news.ycombinator.com/vote?id=43139663&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>&gt; "<i>The majority of Americans do not care and do not really want US tech to be global.</i>"</p><p>They will care very, very much when it bites them in terms of higher prices and loss of jobs because of decreased tech exports.  Dollar hegemony has benefitted the US economy tremendously over the past few decades; money rushed _into_ US dollar denominated holdings in the 2008 crash as one big example.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139579"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139579" href="https://news.ycombinator.com/vote?id=43139579&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Correction, ignorant and misinformed Americans don’t want these things and do not care.</p><p>Let’s stop pretending that the individuals who voted for this administration actually wanted most of what it is doing. The majority voted off of feelings and will not like the end result. There is a lot of explaining away the stupidity and I find it really annoying.</p><p>It’s not a difference of opinion that Tariffs will cause a rise in prices, no matter what the current administration says.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139508"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139508" href="https://news.ycombinator.com/vote?id=43139508&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>Tbh it's the same sentiment in Europe as well. We really want to stop caring who is your new/next president what side of the bed did he wake up on.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43139412"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139412" href="https://news.ycombinator.com/vote?id=43139412&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Good headlines for sure, but i wonder:</p><p>Suppose the war ends. Will the Europeans keep boycotting the cheap russian gas?</p><p>Tarriffs are a nuisance to the US - the US doesn't export much.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139493"><td></td></tr>
            <tr id="43139641"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139641" href="https://news.ycombinator.com/vote?id=43139641&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>Exports are not consumer goods only. Imagine a car is assembled in Mexico, but seats and wheels are made in the US: the pieces count as exports. If you see a table with export/import data for the US, you are going to notice a lot of Mexico and Canada, because that kind of capital goods move a lot between the three countries, well before the consumer good is finally created.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139498"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139498" href="https://news.ycombinator.com/vote?id=43139498&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>&gt; Suppose the war ends. Will the Europeans keep boycotting the cheap russian gas?</p><p>I think they absolutely will, yes. It will cost some money, just like spending more money on defense will, but from what I can see, no one in the European politic elite imagines a world where we can go back to buying gas from Russia.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139532"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139532" href="https://news.ycombinator.com/vote?id=43139532&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>The US imports way too much and our economy has been slowly dying since the 70s. Tarriffs are only a nuisance for the upperclass, they're a boon for middle class Americans because they allow more industry to come back to the United States.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139793"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139793" href="https://news.ycombinator.com/vote?id=43139793&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Tariffs suck for everyone except for the few that suddenly see their sector protected by them.</p><p>For starters, they will shock well greased supply chains, that will cause shortages and thus price raises. Who suffer price raising the most? Then, protected industries will have quasi-monopoly power to raise prices (this is the populism behind tariffs). But other industries will suffer because they can't import their resources and have to switch to more expensive and/or lower quality locals. As an example: suppose that car makers are happy with tariffs protecting them from japanese cars, but now they have to buy expensive US steel, and have to move their assembly lines back to US where today is hard to find experienced and cheap workers. Because everyone always claim to want industry back, but then nobody wants a blue collar job if they can get a white collar one.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139619"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139619" href="https://news.ycombinator.com/vote?id=43139619&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Over what timeline will industry come back? And are $5,000 or $10,000 in 2025 dollars washer and dryers acceptable to Americans?</p><p>It’s possible all economists are wrong. It is more likely they aren’t. Tariffs are going to hurt middle and lower classes.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43139458"><td></td></tr>
                <tr id="43139636"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139636" href="https://news.ycombinator.com/vote?id=43139636&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>The global supply chain is enormously complex. US companies depend on European companies as well, which makes the whole thing much more stupid.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43139500"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139500" href="https://news.ycombinator.com/vote?id=43139500&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>I believe the stark realization over here in the EU is, that strategic dependencies we created either towards the East (Russia, e.g. gas) or the West (the US, e.g. defence), are making us too vulnerable.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139580"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43139580" href="https://news.ycombinator.com/vote?id=43139580&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>Is the EU prepared for shale gas/oil? Willing to exploit the dutch oil fields or cyprus' giant gas fields. So far it has said a decisive no to those.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43139467"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139467" href="https://news.ycombinator.com/vote?id=43139467&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Did you mean the opposite?</p><p>US to EU gas trade is insane right now due to pricing and a 20% tariff won't change anything.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43139455"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139455" href="https://news.ycombinator.com/vote?id=43139455&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>If Europe somewhat deregulated startup stage businesses an countries issued more visas to engineers it could probably turn a lot of things around. So far European founders have been moving to US. But these days I imagine I'm not the only one who doesn't want to work in the shadow of US technofeudalist oligarchy and receive money from people like Marc Andreessen.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                      <tr id="43139565"><td></td></tr>
            <tr id="43139445"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139445" href="https://news.ycombinator.com/vote?id=43139445&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Never thought I'd live to see the end of Pax Americana and yet here we are. To give up this amount of control and soft power just to cozy up to the corpse of the Soviet union is not the universe branch I expected to find myself in.</p><p>You yanks might think this is some display of your upper hand. Just wait until the civilized world turns its back on the greenback as the reserve currency. You will be so fucked.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139602"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139602" href="https://news.ycombinator.com/vote?id=43139602&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>It'd take a lot more than that.</p><p>EU has lagged US due to systemic issues like over regulation and limited investments. See Draghi's report.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139563"><td></td></tr>
                  <tr id="43139478"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139478" href="https://news.ycombinator.com/vote?id=43139478&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>Is the backlash worse than the Bush invasion of Iraq in 2003? Is it worse than the first Trump administration? Genuinely curious. The reality is the 50% of voters who DID not vote for the current president are just as shell shocked as the Europeans. Only we KNEW it was coming.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139597"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139597" href="https://news.ycombinator.com/vote?id=43139597&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Not a European, but yes.  The US is actively siding with Russia against Europe during the largest war in Europe since WWII, and using the threat of revoking access to American tech (Starlink) and weapons to force them into a terrible and exploitative agreement whose only beneficiaries are the US and Russia. Europeans now need to realistically consider that working with US technology companies is like relying on Russian gas.</p><p>Meanwhile our President also keeps threatening to invade sovereign territory (Greenland) of a NATO ally (Denmark) and using language w/r/t Canada that is almost indistinguishable from how Russia talked about Ukraine a decade ago.  And Canadians are fucking pissed about it.</p><p>It's very, very different than his first term or even Iraq.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139616"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139616" href="https://news.ycombinator.com/vote?id=43139616&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>It's totally different. There was negative public sentiment, but the diplomatic relations still were stable and good. Germany wanted to be convinced, but could not be. Tony Blair went with Bush.</p><p>This time it is diplomats that are raising the flags. It actually has not yet arrived in the public sphere as drastically as it has in the policy side.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139603"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139603" href="https://news.ycombinator.com/vote?id=43139603&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>It's much, much worse than both of those. And we view it more like, only 1/3 of Americans have voted against this while 1/3 voted for it and 1/3 was ok with that. So yeah, the resentment and anger towards the US and it's citizens is growing.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139659"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139659" href="https://news.ycombinator.com/vote?id=43139659&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Yes, it's much worse than 2003. Back then, multiple NATO countries took part in the invasion of Iraq, as US invoked Article 5. 
Now, 22 years later US is saying "f... o..".</p><p>In the first term Trump was unprepared, now they had 4 years to prepare and it seems they are very methodical in dismantling US global influence, and but also dismantling structures internally.</p><p>As far as I'm worried about security of Europe or Far East allies (Japan, SK, Taiwan, Philippines), I'm worried about future of US itself.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43139736"><td></td></tr>
            <tr id="43139718"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139718" href="https://news.ycombinator.com/vote?id=43139718&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>I use links2 and lynx web browsers.</p><p>The issue in EU is Big Tech has many "minions" in public administrations and local critical online services (many dominant online services are theirs anyway). They manage to make hard dependent the "web" on their cartel of whatng web rendering engines (and their SDK).</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139521"><td></td></tr>
                <tr id="43139596"><td></td></tr>
                  <tr id="43139442"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139442" href="https://news.ycombinator.com/vote?id=43139442&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>The big companies do, thus the pressure from Vance earlier this week on the EU dropping any regulations the US doesn’t agree with. Obviously that plan is questionable in terms of efficacy and morality, but it’s definitely in progress</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139527"><td></td></tr>
            <tr id="43139471"><td></td></tr>
                <tr id="43139734"><td></td></tr>
            <tr id="43139776"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139776" href="https://news.ycombinator.com/vote?id=43139776&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Vances speech in Munich to Europe, and Trumps apparent selling out of Ukraine.</p><p>I can tell you that the media cycle and conversation here (The Netherlands) has been very much about how we can move forward without the United States. People around me have been pulling out of American social media, dropping WhatsApp (which has been the biggest chat app here for a long time), and stopped using American exports.</p><p>It's even overshadowing the German elections, which would otherwise be the biggest news topic, as the election in Europe's largest economy.</p><p>A lot of people here have lost a huge amount of trust in the US, and I don't know if it can ever really come back.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139504"><td></td></tr>
                  <tr id="43139483"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139483" href="https://news.ycombinator.com/vote?id=43139483&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Good information, and I see no sign it has been noticed in the US (which is why your report is good info).</p><p>The first question is how broad is this shift; is it really everywhere on the street or just in the "early adopters"?</p><p>The next question is how durable is the trend?</p><p>The things I've noticed here are reports of AfD rapidly losing 5% in the polls after Vance spoke in Munich, and a French right-wing leader cancelling his planned speech at CPAC after Bannon did a Nazi salute in his talk.  When the US wannabe-Nazis are too toxic for European pro-fascists, that is saying something...</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139556"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139556" href="https://news.ycombinator.com/vote?id=43139556&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>Trust build slowly and erode quickly.</p><p>I don’t know about people usage of tech.</p><p>But it’s now really clear that the US is not a reliable ally. ( not even getting into politics, just stability wise : it’s clear that a change of regime can have the country do a 180 in term of foreign policies. And it’s not a good look )</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139567"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139567" href="https://news.ycombinator.com/vote?id=43139567&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>I suspect it is far too early to tell. In 6 months the data may be an indication of something. Right now, it’s just not sufficient amount of time to conclude.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43139547"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43139547" href="https://news.ycombinator.com/vote?id=43139547&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>The shift is not that broad at all. True, in some politically and technology aware circles, a lot of people are shifting.
But the vast majority of people isn't aware of what's happening, doesn't feel like putting effort to change, or even agrees with Trump &amp; Musk.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43139544"><td></td></tr>
            <tr id="43139511"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139511" href="https://news.ycombinator.com/vote?id=43139511&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div><p>&gt; do they take it seriously or believe there is no alternative to US tech</p><p>There isn't, broadly speaking. Oh yeah, gonna use OpenEuroLLM? Have y'all made the ePhone yet or is that still stuck in committee?</p><p>Other countries will moan for a bit, no one likes having their free money and handouts taken away, then realize that US technology is still broadly the only option, and its actually quite good and even other US companies cant compete with US big tech with all of the free money here, let alone the tech-backward eurozone. Right now its the UK forcing Apple to remove Advanced Data Protection for UK citizens (not the eurozone, to be clear, but adjacent and culturally aligned)</p><p>The "boycotts" are great for headlines though (don't worry, Apple's revenue this year will be larger than ever, as always). If y'all don't want to buy Teslas though, I get that; I feel the same way.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139585"><td></td></tr>
                  <tr id="43139502"><td></td></tr>
            <tr id="43139385"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43139385" href="https://news.ycombinator.com/vote?id=43139385&amp;how=up&amp;goto=item%3Fid%3D43139172"></a></center>    </td><td><br><div>
                  <p>That's honestly happening here, too. So many of my acquaintances are dropping big tech reliances. Some of us are building our own tools as work around where we can.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43139432"><td></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DOGE's Only Public Ledger Is Riddled with Mistakes (133 pts)]]></title>
            <link>https://www.nytimes.com/2025/02/21/upshot/doge-musk-trump-errors.html</link>
            <guid>43138238</guid>
            <pubDate>Sat, 22 Feb 2025 12:03:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/02/21/upshot/doge-musk-trump-errors.html">https://www.nytimes.com/2025/02/21/upshot/doge-musk-trump-errors.html</a>, See on <a href="https://news.ycombinator.com/item?id=43138238">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/02/21/upshot/doge-musk-trump-errors.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[After layoffs, Meta rewards top executives with a substantial bonus increase (171 pts)]]></title>
            <link>https://www.theregister.com/2025/02/22/meta_pumps_executive_bonuses/</link>
            <guid>43138191</guid>
            <pubDate>Sat, 22 Feb 2025 11:55:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/02/22/meta_pumps_executive_bonuses/">https://www.theregister.com/2025/02/22/meta_pumps_executive_bonuses/</a>, See on <a href="https://news.ycombinator.com/item?id=43138191">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>After another round of mass layoffs and reports of slashed stock options for remaining employees, Meta has like clockwork opted to reward its top executives with a substantial bonus increase.</p>
<p>The Facebook giant revealed in a government <a href="https://www.sec.gov/ix?doc=/Archives/edgar/data/1326801/000132680125000021/meta-20250213.htm" rel="nofollow">filing</a> that its Compensation, Nominating and Governance Committee (CNGC) approved a target annual bonus increase for its top executive officers bar CEO Mark Zuckerberg. The bonus was raised from 75 percent of base salary to a whopping 200 percent, effective with the 2025 annual performance period.</p>
<p>"Following this increase, the target total cash compensation for the named executive officers (other than the CEO) falls at approximately the 50th percentile of the Peer Group Target Cash Compensation," the filing notes. It added that prior to the adjustment, those executives were only at or below the 15th percentile in total cash compensation compared to their peers.&nbsp;</p>

    

<p>According to Meta's April 2024 proxy <a href="https://d18rn0p25nwr6d.cloudfront.net/CIK-0001326801/8c7f110d-9cc4-4034-b356-ece89d4cb1c5.pdf" rel="nofollow">statement</a> [PDF], CTO Andrew Bosworth's base salary was $945,000. His actual eligible earnings were slightly lower due to the timing of his raise. However, factoring in a 75 percent target bonus and Meta's 150 percent company performance multiplier for 2023, his total bonus payout amounted to about $1.05 million.</p>

        


        

<p>Assuming Bosworth's salary remains the same, and Meta's company performance percentage stays at 150 percent in 2025, the new 200 percent target bonus would push his bonus to nearly $3 million. That's before any stock-based compensation and other add-ons. And he's not even the highest-paid member of Meta's named executive team.</p>
<p>For balance's sake, and some might find this hard to swallow but, $3 million annual cash compensation for a CTO in Bosworth's position is about right for Silicon Valley; it's nothing outrageous, relatively speaking. The vast majority of his pay package is in shares; in 2023 for instance, he was awarded more than $20 million in stock. The salary, like for many in his role, is the cherry on top of an enormous cake.</p>

        

<p>Some of that bonus cash, though, might be coming from Meta's latest round of layoffs, which saw <a href="https://www.theregister.com/2025/02/10/meta_to_toss_5_of/">around 3,700 people</a> - about five percent of its workforce - axed this month. The cut reportedly targeted low performers, and followed a year in which the biz reported a net income of $62.36 billion, a 59 percent year-over-year increase.&nbsp;</p>
<p>This comes <a href="https://www.ft.com/content/67a4c030-a7f6-47af-bab0-a998f0a09506" rel="nofollow">reports</a> surfaced this week that Meta has cut back on its yearly distribution of stock options by 10 percent to most staff, though we do note that the corp's share price has climbed 10 percent in the past month, and 46 percent for the past year.</p>
<p>We've reached out to Meta to confirm reports of the stock option cut.&nbsp;</p>
<ul>

<li><a href="https://www.theregister.com/2025/01/07/meta_eliminate_fact_check/">Zuck takes a page from Musk: Meta dumps fact-checkers, loosens speech restrictions</a></li>

<li><a href="https://www.theregister.com/2024/11/06/meta_weaponizing_llama_us/">Meta gives nod to weaponizing Llama – but only for the good guys</a></li>

<li><a href="https://www.theregister.com/2024/11/07/top_10_billionaires/">Top 10 billionaires make nearly $64B in post-Trump election stock surge</a></li>

<li><a href="https://www.theregister.com/2024/07/22/meta_layoff_severance_agreements/">Meta's mass layoff severance agreements illegal, says judge</a></li>
</ul>
<p>Zuckerberg started 2025 by describing Meta's plans for the year as "intense," with massive AI investments lined up as the Facebook maker shifts focus beyond its struggling <a href="https://www.theregister.com/2023/05/10/meta_metaverse_report/">metaverse</a> ambitions.&nbsp;</p>
<p>Following OpenAI's announcement that it and its partners plan to invest up to <a href="https://www.theregister.com/2025/01/22/openai_stargate_ai_datacenter_company/">$500 billion</a> in an AI infrastructure project dubbed Stargate, Zuckerberg announced his own AI spending plans - Meta would pour $60 billion or more to expand its AI infrastructure in 2025.&nbsp;</p>

        

<p>"This is a massive effort, and over the coming years it will drive our core products and business, unlock historic innovation, and extend American technology leadership," Zuckerberg <a href="https://www.facebook.com/zuck/posts/pfbid0219ude255AKkmk4JAueXZeZ9zpjNYio2tBkd7bNmCaRbJ6iJaVVjypUgDg78CNdq5l" rel="nofollow">said</a> of the effort. Hopefully it will prove more successful than that Metaverse shift for the sake of those executives' bonuses.&nbsp;®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['The tyranny of apps': those without smartphones are unfairly penalised (181 pts)]]></title>
            <link>https://www.theguardian.com/money/2025/feb/22/the-tyranny-of-apps-those-without-smartphones-are-unfairly-penalised-say-campaigners</link>
            <guid>43137488</guid>
            <pubDate>Sat, 22 Feb 2025 09:18:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/money/2025/feb/22/the-tyranny-of-apps-those-without-smartphones-are-unfairly-penalised-say-campaigners">https://www.theguardian.com/money/2025/feb/22/the-tyranny-of-apps-those-without-smartphones-are-unfairly-penalised-say-campaigners</a>, See on <a href="https://news.ycombinator.com/item?id=43137488">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Michael is in his late 50s and is among the millions of people in the UK who cannot or do not want to use mobile apps, and feels he is being penalised for his choice.</p><p>He does own a smartphone – an Apple iPhone he bought secondhand about three years ago – but says: “I don’t use apps at all. I don’t download them for security reasons.”</p><p>Apps have burrowed their way into seemingly every aspect of our lives and there are lots of reasons why companies are pushing us to use them. With an app, it is often “one click and you’re in”, rather than having to faff around online finding the website and remembering passwords. It is also for the “push notifications” that mobile apps send to grab our attention and get us to buy stuff. Many tech experts also argue that apps are generally more secure than websites and allow banks and others to carry out sophisticated ID verification using face, voice and fingerprint biometrics.</p><p>But millions of people who cannot afford a smartphone or have an older device that does not support some services are increasingly being locked out of deals, discounts and even some vital services, say digital exclusion and pro-cash campaigners.</p><p>They are missing out on everything from savings on their weekly shop, to some of the best interest rates for their cash. And not signing up to the app revolution is making activities including paying for parking and going to concerts increasingly challenging.</p><p>“It’s the tyranny of the apps,” says Ron Delnevo, the chair of the campaign committee at lobby group the <a href="https://www.paymentchoicealliance.org/" data-link-name="in body link">Payment Choice Alliance</a>. “In this country we’re being treated like sheep,” he says. “We’re always being told there’s no alternative.” But when a new smartphone can set you back hundreds of pounds, it is “an expensive passport to participate”, Delnevo says.</p><p>According to the latest data from the telecoms regulator Ofcom, 8% of people aged 16 or over do not have a smartphone, which for the UK translates into just under 4.5 million people. Among those aged 75-plus, the proportion is said to be 28%. Add in all those who don’t or can’t use apps and the total number of people affected is a lot bigger.</p><p>Lots of retailers offer enhanced or exclusive deals to their app users, and some run loyalty schemes through them. This is potentially a big deal as those schemes often give you access to discounts that are not otherwise available.</p><p>“Some people are missing out on lower prices offered by loyalty schemes as they do not have access to phones or smart devices to download apps,” Reena Sewraz, the retail editor at Which? says.</p><p>The consumer group is among those to have highlighted Lidl’s loyalty scheme, Lidl Plus, as one that is only accessible via an app, with an email address also required. That means the “big savings” available via the scheme’s weekly offers – at the time of writing these included 25% off tinned tuna and 20% off microwave rice and grain pouches – plus various coupons and rewards, are off-limits to those who cannot or do not wish to go digital.</p><p>Delnevo says Lidl is “disenfranchising” many of its customers, adding: “The people who probably need the discounts most are the people who can’t afford or don’t have a smartphone.”</p><p>In response, the supermarket chain says the Lidl Plus scheme “forms part of our commitment to providing customers with the best value. Nonetheless, we remain mindful of those who don’t have access to a smartphone or tablet and continue to offer in-store promotions through our ‘pick of the&nbsp;week’ offers.”</p><p>Rival supermarket Asda also runs a scheme, Asda Rewards, where you have to download an app and there is no physical card available. This scheme does not offer price promotions – instead, you earn Asda “pounds” when you shop, which you convert into vouchers to spend in-store and online.</p><p>Some retailers have gone further down the app path than others. The Boots Advantage card scheme, one of the UK’s most popular, still lets you use a physical plastic card, although there are “tailored offers” available via the app.</p><figure id="71b82c96-b4ae-45cb-905e-4e30a50a1b58" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/23d3e6d6a058e57eb942e300c1f0d54f9cecc3a4/0_233_6720_4032/master/6720.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/23d3e6d6a058e57eb942e300c1f0d54f9cecc3a4/0_233_6720_4032/master/6720.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/23d3e6d6a058e57eb942e300c1f0d54f9cecc3a4/0_233_6720_4032/master/6720.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/23d3e6d6a058e57eb942e300c1f0d54f9cecc3a4/0_233_6720_4032/master/6720.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/23d3e6d6a058e57eb942e300c1f0d54f9cecc3a4/0_233_6720_4032/master/6720.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/23d3e6d6a058e57eb942e300c1f0d54f9cecc3a4/0_233_6720_4032/master/6720.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="Customers with pastries and coffee in a Greggs bakery" src="https://i.guim.co.uk/img/media/23d3e6d6a058e57eb942e300c1f0d54f9cecc3a4/0_233_6720_4032/master/6720.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="267" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>At Greggs you get a free hot drink just for downloading the app.</span> Photograph: Christopher Thomond/The Guardian</figcaption></figure><p>And at the bakery chain Greggs, you can collect loyalty “stamps” for free food and drink and&nbsp;get “exclusive app-only gifts”.&nbsp;You currently get a free hot drink just for downloading the app.</p><p>Meanwhile, the online clothes retailer Asos and the tools and hardware specialist Screwfix are just two of those that are or have recently been running “app exclusive” campaigns. Asos was this week offering 20% off “1,000s of styles,” while Screwfix’s Stacks of Rewards promotion let you collect coins and earn rewards by spending via the app.</p><h2 id="parking"><strong>Parking</strong></h2><p>If there is one area of life where many people probably feel the app revolution has got out of hand, it is parking. In the UK there are thought to be at least 30 different parking apps, and it is not unusual for an individual to have eight to 10 different ones on their phone.</p><p>Many older people have told the charity Age UK that they are “<a href="https://www.ageuk.org.uk/siteassets/documents/reports-and-publications/reports-and-briefings/offline-and-overlooked-report.pdf" data-link-name="in body link">at the end of their tether</a>” when it comes to paying for parking because they cannot park if an app or mobile is required. “In some cases this has meant missing important appointments like seeing their GP,”&nbsp;it adds.</p><figure id="cb7e8122-e65c-4a62-ad25-0b07a7645bbf" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-3"><picture><source srcset="https://i.guim.co.uk/img/media/aef4d0b2c9163735a910a40c85d7bebcee1a334e/0_276_8192_4918/master/8192.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/aef4d0b2c9163735a910a40c85d7bebcee1a334e/0_276_8192_4918/master/8192.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/aef4d0b2c9163735a910a40c85d7bebcee1a334e/0_276_8192_4918/master/8192.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/aef4d0b2c9163735a910a40c85d7bebcee1a334e/0_276_8192_4918/master/8192.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/aef4d0b2c9163735a910a40c85d7bebcee1a334e/0_276_8192_4918/master/8192.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/aef4d0b2c9163735a910a40c85d7bebcee1a334e/0_276_8192_4918/master/8192.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="PaybyPhone parking meters on a street next to some parked cars" src="https://i.guim.co.uk/img/media/aef4d0b2c9163735a910a40c85d7bebcee1a334e/0_276_8192_4918/master/8192.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="267.152099609375" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Many older people have told Age UK they cannot park if an app or mobile is required.</span> Photograph: Christopher Thomond/The Guardian</figcaption></figure><p>The RAC’s head of policy, Simon Williams, says many people are overwhelmed by the multitude of apps they have to use, “when in reality you want one that you like and you’re happy using and that you can use everywhere”.</p><p>Six years ago the Department for Transport started developing a “national parking platform”&nbsp;(NPP) designed to enable drivers to use one app of their choice to pay for all their parking. It has been trialled by a number of councils, but a&nbsp;big question mark hangs over its future as public funding for the project looks likely to be withdrawn.</p><p>Some councils are now permanently getting rid of parking ticket machines that accept debit and credit cards in order to save money (many stopped accepting cash a long time ago).</p><p>The London borough of Barnet is one area where all the council pay and display machines at car parks have just been taken out of service. As of last month, motorists have been greeted with signs telling them to use the PayByPhone app.</p><p>“Withdrawal of the ability to pay by card, following the removal of the payment by cash facility, will disappoint some drivers, especially the elderly, who find it difficult to use the PayByPhone app,” according to <a href="https://www.barnetsociety.org.uk/barnet-council-planning-22-million-in-cuts-and-another-significant-hike-in-council-tax-to-tackle-spending-crisis" data-link-name="in body link">an article on the Barnet Society website</a>.</p><p>A Barnet council spokesperson says: “Pay and display machines were declining in use and in 2023-24 accounted for less than 7% of overall transactions. There are easy alternatives … including the PayByPhone app, phone or text, and cash payments at over 100 local&nbsp;PayPoint retailers.”</p><h2 id="entertainment"><strong>Entertainment</strong></h2><p>Increasingly with gigs and other events, you need to download an app to access your tickets.</p><p>For example, tickets for some events at the O2 arena in London are delivered as a “mobile ID” via the venue’s own app, and you get a barcode which is scanned to let you in.</p><p>This type of app-based mobile ID system is also billed as the only admission method at venues including Ovo Arena Wembley in London and University of Wolverhampton at The Halls.</p><figure id="b3a9c6e9-19de-4ba4-bd47-d3d6258cf385" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-4"><picture><source srcset="https://i.guim.co.uk/img/media/0d01462a76cfed6e254cb9ccd4844cc4a9ec8241/0_342_5318_3192/master/5318.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/0d01462a76cfed6e254cb9ccd4844cc4a9ec8241/0_342_5318_3192/master/5318.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/0d01462a76cfed6e254cb9ccd4844cc4a9ec8241/0_342_5318_3192/master/5318.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/0d01462a76cfed6e254cb9ccd4844cc4a9ec8241/0_342_5318_3192/master/5318.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/0d01462a76cfed6e254cb9ccd4844cc4a9ec8241/0_342_5318_3192/master/5318.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/0d01462a76cfed6e254cb9ccd4844cc4a9ec8241/0_342_5318_3192/master/5318.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="James Bay sings and plays guitar on stage at the OVO Arena in London" src="https://i.guim.co.uk/img/media/0d01462a76cfed6e254cb9ccd4844cc4a9ec8241/0_342_5318_3192/master/5318.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="267.1004136893569" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>James Bay at the OVO Arena in London, where an app-based system is billed as the only admission method.</span> Photograph: Gus Stewart/Redferns</figcaption></figure><p>That said, the venue websites do usually provide details of how people who do not have a smartphone or cannot download the app can gain entry to their event – but it might involve bringing photo ID and your confirmation email to the box office perhaps 90 minutes before the doors open.</p><p>When it comes to theatre, popular services such as TodayTix offer savings on tickets and can be accessed online, although to take advantage of some of the benefits and offers, such as discounted same-day “Rush” tickets, you need to have the app.</p><h2 id="eating-and-drinking"><strong>Eating and drinking</strong></h2><p>Those who cannot or do not use apps are missing out on some of the best deals for meals, takeaways and pub grub.</p><p>McDonald’s is running a high-profile promotion called Deal Drop, where it offers items at “bargain” prices, such as a classic Big Mac for £1.49 (normally £4.99) and a children’s Happy Meal for £1.99 (normally £3.59) – but all of the discounts are available exclusively with the<strong> </strong>company’s app.</p><p>The fastfood chain Subway runs a loyalty scheme where you earn points that you can convert into “Subway Cash” that can be spent on anything on the menu – but again it is app-only. If you were a member of Subway’s previous rewards scheme, which closed last May, your plastic membership card “is no longer usable”.</p><p>Similarly, many pubs have&nbsp;apps that offer big discounts or let you earn rewards. The more than 200-strong Sizzling Pubs chain has recently been promoting eye-catching app-only deals such as 40% off main meals at some locations.</p><p>Some coffee shop chains, such as Harris + Hoole, have also gone app-only with their loyalty schemes. With the Harris + Hoole one, you get a free drink for every six coffees you buy.</p><figure id="2b433abe-899d-4914-80b9-eb66593ff91a" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-5"><picture><source srcset="https://i.guim.co.uk/img/media/3a1eebb3b8ae41f5c8fea5671560d1df3182a6ec/0_340_4941_2966/master/4941.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/3a1eebb3b8ae41f5c8fea5671560d1df3182a6ec/0_340_4941_2966/master/4941.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/3a1eebb3b8ae41f5c8fea5671560d1df3182a6ec/0_340_4941_2966/master/4941.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/3a1eebb3b8ae41f5c8fea5671560d1df3182a6ec/0_340_4941_2966/master/4941.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/3a1eebb3b8ae41f5c8fea5671560d1df3182a6ec/0_340_4941_2966/master/4941.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/3a1eebb3b8ae41f5c8fea5671560d1df3182a6ec/0_340_4941_2966/master/4941.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="Customers sitting at a table in a Harris + Hoole coffee shop in London" src="https://i.guim.co.uk/img/media/3a1eebb3b8ae41f5c8fea5671560d1df3182a6ec/0_340_4941_2966/master/4941.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="267.12608783647033" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>With the Harris + Hoole app, you get a free drink for every six coffees you buy.</span> Photograph: Sarah Lee/The Guardian</figcaption></figure><h2 id="banking"><strong>Banking</strong></h2><p>Some of the best savings rates are offered by app-only providers – made up of banks and “electronic money institutions” (EMIs), which do not have their own banking licence, but put your money in a bank that does.</p><p>At the time of writing, the Moneyfacts list of top-paying easy access accounts included products from the app-only providers Atom Bank, Chip and Plum paying 4.6%, 4.58% and 4.38% respectively.</p><p>“App-only accounts are the new ‘online-only’ accounts,” says Anna Bowes, savings expert at the financial advisory firm The Private Office. “While not for everyone, apps are becoming far more popular with savers, especially as they can often be found paying some of the best rates.”</p><p>To give you an idea of how apps have taken over savings, she says the current top five easy-access cash Isas are all provided by EMIs. “The next five top paying cash Isas are all with regulated banks or building societies but two must be opened via a mobile banking app,” she says.</p><p>Some of the providers aimed at smartphone users are not entirely app-based. For example, saving and investing firm Moneybox primarily provides its services via its app, but says you can access them through other means, including its website.</p><p>Meanwhile, the high street banks sometimes have products or services that are app-only – for example, HSBC’s Global Money service, which lets you convert, spend and send multiple currencies with no bank fees, is only available via the HSBC app.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Who needs a sneaker bot when AI can hallucinate a win for you? (159 pts)]]></title>
            <link>https://www.eql.com/media/sneaker-bot-ai-error</link>
            <guid>43135382</guid>
            <pubDate>Sat, 22 Feb 2025 02:08:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eql.com/media/sneaker-bot-ai-error">https://www.eql.com/media/sneaker-bot-ai-error</a>, See on <a href="https://news.ycombinator.com/item?id=43135382">Hacker News</a></p>
<div id="readability-page-1" class="page"><div fs-toc-element="contents" fs-richtext-element="rich-text"><p>Every February we see a big spike in retailers running sneaker launches on EQL to coincide with the 2025 NBA All-Star Weekend. This year, the festivities kicked off a little earlier than normal, courtesy of Jordan Brand, who have been getting serious about getting back their mojo and dropping some serious heat in the process. This year marks 40 years since a talented rookie by the name of Michael Jordan wore his signature shoes at the 1985 All-Star Dunk Contest in a colorway that is now firmly ingrained in sneaker culture lore. To mark the occasion, Jordan Brand recreated the shoe in what they say is the closest to OG spec ever. Sneakerheads have been anticipating the drop for months and the demand was predictably crazy.</p><p>At first, everything appeared to be going smoothly. Thousands of entries were rolling in, bots were being neutralized, winners were being picked and notified, but as we started notifying non-winners that they’d missed out, things got weird.</p><p>We started seeing reports of people being told that they had <em>won and lost in the same email…</em> the Schrödinger’s cat of bug reports. What on earth was going on??</p><figure><p><img src="https://cdn.prod.website-files.com/63f4b950d4ba499d292a5807/67b6501b3cd8c968f609f0d6_Screenshot1.webp" loading="lazy" alt=""></p></figure><figure><p><img src="https://cdn.prod.website-files.com/63f4b950d4ba499d292a5807/67b650268e295d42fbd80cf9_Screenshot2.webp" loading="lazy" alt=""></p></figure><p>Now, we’re accustomed to a baseline level of crazy – it comes with the territory of running some of the hottest product launches on the internet. Having a 24/7 support team who can bring calm to the chaos is big reason why brands work with EQL. But even for us, this one felt odd.</p><p>Fans were reporting that their email app was listing ”You’ve been selected” but when they clicked on the email it displayed the heartbreaking “SORRY” non-winner message. Sneaker fans are a passionate bunch, and online launches often trigger the full range of human emotions - to put it mildly! People were understandably mad about the confusing messaging.</p><p>When things heat up, it’s helpful to have a community you can turn to. We operate in a space where people sometimes troll for fun – or profit, like the <a href="https://x.com/EQLofficial/status/1632861398425559041">time</a> someone tried to pretend they’d won 750 pairs of the <a href="https://tiffany.runfair.com/en-GB/us/niketiffany-air-force-1-1837">Nike/Tiffany Air Force 1 1837</a> to pump a cookgroup.</p><p>We reached out to our <a href="https://discord.com/servers/eql-1136853080487514182">Discord community</a>, who confirmed that the issue was real and shared some more screenshots:</p><figure><p><img src="https://cdn.prod.website-files.com/63f4b950d4ba499d292a5807/67b6505e233e0e61481beccc_Screenshots3.webp" loading="lazy" alt=""></p></figure><p>The bizarre thing is that our winner emails don’t actually say “You’ve been selected”. As an on-call engineer, this is the point when you start questioning your life choices. You know that the issue is affecting thousands of users, but the offending phrase doesn’t appear anywhere in EQL’s codebase, aside from some very old launches several years ago.</p><p>As more screenshots poured in, another odd thing jumped out – the winning phrase seemed to change from email to email. Some said “selected <em>to purchase </em>JORDAN AJ1”, others said “selected <em>for the </em>JORDAN AJ1”:</p><figure><p><img src="https://cdn.prod.website-files.com/63f4b950d4ba499d292a5807/67b6507dc983919b3c85a7df_Screenshot4.webp" loading="lazy" alt=""></p></figure><p>By this point, we’d narrowed down the affected users to a single email client - <strong>Yahoo Mail</strong>, which is where we got suspicious. Had Yahoo Mail introduced any features lately that might be causing this…?</p><p>As it turns out, yes, yes they had. A quick Google search revealed that a few months ago Yahoo jumped on the AI craze with the launch of ”<a href="https://www.yahooinc.com/press/yahoo-mail-launches-a-new-app-experience-with-mobile-first-ai-features"><strong>AI-generated, one-line email summaries”</strong></a><strong>.</strong></p><p>At this point, the penny dropped. Just like Apple AI <a href="https://edition.cnn.com/2025/01/16/media/apple-ai-news-fake-headlines/index.html">generating fake news summaries</a>, Yahoo AI was hallucinating the fake winner messages, presumably as a result of training their model on our old emails. Worse, they were putting an untrustworthy AI summary in the exact place that users expect to see an email subject, with no mention of it being AI-generated 🤯</p><p>Some people use Yahoo Mail to read emails from other providers like Gmail, so this issue hit a broad range of sneaker fans. And it’s worth mentioning that as of the time of writing, <strong>this AI feature is still live in Yahoo Mail</strong>, so more confusion is expected in future – not just for sneaker fans receiving launch results, but for anyone reading important emails in Yahoo Mail.</p><p>For EQL users, if you’re ever in doubt, you can always double-check your launch results in the <a href="https://app.eql.com/">fans app</a>, contact our support team (support@eql.com) or join our helpful <a href="https://discord.com/servers/eql-1136853080487514182">Discord community</a>. Until then, we’ll continue fighting the good fight to put products into the hands of real fans and trying to prevent bad AI from ruining your day! 🤖</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Start a computer club in the place that you live (2023) (158 pts)]]></title>
            <link>https://startacomputer.club/</link>
            <guid>43135176</guid>
            <pubDate>Sat, 22 Feb 2025 01:30:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://startacomputer.club/">https://startacomputer.club/</a>, See on <a href="https://news.ycombinator.com/item?id=43135176">Hacker News</a></p>
<div id="readability-page-1" class="page">

<!-- make translations available -->
<hr>







<hr>
<!-- translation section is done -->

<h2>YOU SHOULD START A COMPUTER CLUB IN THE PLACE THAT YOU LIVE</h2>
<pre dir="ltr" lang="eo">              ,---------------------------,
              |  /---------------------\  |
              | | LASTA NOVAĴO:         | |
              | | VI DEVUS ESTABLI      | |
              | | KOMPUTILAN KLUBON     | |
              | | EN LA LOKO,           | |
              | | KIE VI LOĜAS          | |
              |  \_____________________/  |
              |___________________________|
            ,---\_____     []     _______/------,
          /         /______________\           /|
        /___________________________________ /  | ___
        |                                   |   |    )
        |  _ _ _                 [-------]  |   |   (
        |  o o o                 [-------]  |  /    _)_
        |__________________________________ |/     /  /
    /-------------------------------------/|      ( )/
  /-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/ /
/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/ /
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
</pre>
<p><em>i'm glad someone finally said it</em></p>

<h2>WHAT IS A COMPUTER CLUB?</h2>
<p>a computer club is where a group of people hang out and do computer together</p>

<h2>WHAT IS "DOING COMPUTER"?</h2>
<p>doing computer is whatever you want...on computers, together. our bias is towards programming and <span dir="ltr" lang="en"> diy </span> shared computing infrastructure. but there's also art and music and open data science and circuit-bending and a million other things we don't know about</p>

<h2>WHY SHOULD I START A COMPUTER CLUB?</h2>
<p>the political economy of computing is awful. have you read <a dir="ltr" lang="en" href="https://bookwyrm.social/book/525851/s/palo-alto"> palo alto</a>? me neither. we should read it. we deserve better than the <span dir="ltr" lang="en"> darpa-funded </span> visions of <span dir="ltr" lang="en"> xerox parc </span> technologists</p>
<p>make the political economy of computing less awful and bring it home to you and yours by starting a computer club in the place that you live</p>

<h2>RULES</h2>
<p>we can't make you do anything, so do whatever you want. but these are the things we think are really important</p>
<ul>
    <li><strong>hang out in real life:</strong> online has rude vibes, real life has kinder vibes. hang out in real life for trust and strength and to ground your computer club in your actual local context</li>
    <li><strong>reject corporate sponsorship:</strong> corporate sponsorship constrains behavior and undermines collective ownership. computer club isn't yours if a corporate sponsor might get upset by something you do</li>
    <li><strong>computer club is a collective project:</strong> you don't need a mission statement, but you might have an ethos that defines your computer club. computer club is beholden only to that ethos and the people who show up</li>
</ul>

<h2>GUIDELINES</h2>
<p>we still can't make you do anything, so continue doing whatever you want. but these are things we think are important to think about</p>
<ul>
    <li>computing is political, so let computer club be political too</li>
    <li>the <a href="https://www.recurse.com/social-rules"><span dir="ltr" lang="en"> recurse center </span> social rules</a> foster collaboration and psychological safety, consider using them</li>
    <li>be inspired by permaculture</li>
    <li>be inspired by small web</li>
    <li>be inspired by <span dir="ltr" lang="en"> diy </span> culture</li>
    <li>be inspired by computing as a medium through which better things are possible</li>
    <li>try to host computer club's stuff on your own computers in the place that you live</li>
    <li>be open to interdisciplinary computing</li>
    <li>be open to different histories and "skill levels" with computing</li>
</ul>

<h2>HOW SHOULD I START THE COMPUTER CLUB</h2>
<p>there are lots of ways to start a computer club; how you start will be unique to yours. but you could try</p>
<ul>
    <li>talk about what used to excite you about doing computer. talk about how the political economy of computing could be better. talk about these things publicly. the computer club might already be breathing in the communities you're in</li>
    <li>attend preexisting computing meetups and find like-minded people. meetups about "how to <span dir="ltr" lang="en"> node.js </span> apolitically" are sidelining people who want "how to <span dir="ltr" lang="en"> node.js </span> pro-socially." don't try to be a recruiter -- just be excited at events and find people who are also excited</li>
    <li>does your city have a food coop? food coops and computer clubs have a similar ethos; and food coops are a great place to get your food for the same reasons that a computer club is a great place to do computer. make friends at your food coop and find the computer-doers</li>
    <li>join a project or start a project and talk to people there about starting a computer club. are you or someone you know doing<ul>
        <li>web design for a <span dir="ltr" lang="en"> diy </span> venue? work together!</li>
        <li>technical support for a cool local project? work together!</li>
        <li>communications for your neighborhood's nascent mesh network? work together!</li>
        <li>analysis for local open data projects? work together!</li>
    </ul></li>
    <li>talk to existing computer clubs!</li>
</ul>

<h2>COMPUTER CLUBS AND SIMILAR THINGS THAT WE KNOW OF</h2>
<ul>
    <li dir="ltr" lang="en"><a href="https://cyberia.club/">cyberia computer club</a></li>
    <li dir="ltr" lang="en"><a href="https://www.ccc.de/">chaos computer club</a></li>
    <li dir="ltr" lang="en"><a href="https://bunk.computer/">bunk computer club</a></li>
    <li dir="ltr" lang="fr"><a href="https://deuxfleurs.fr/">deuxfleurs</a></li>
    <li dir="ltr" lang="en"><a href="https://lurk.org/">LURK</a></li>
    <li dir="ltr" lang="en"><a href="https://plover.digital/">plover digital</a></li>
    <li>computer clubs are inspired by <a dir="ltr" lang="en" href="https://en.m.wikipedia.org/wiki/Category:Hackerspaces"> hackerspaces</a></li>
    <li>[coming soon] your computer club!</li>
</ul>

<h2>TOOLS THAT ARE USEFUL FOR A COMPUTER CLUB</h2>
<ul>
    <li>computers (personal computers)</li>
    <li>computers (servers)</li>
    <li>computers (people doing computer)</li>
    <li><strong>chat:</strong><span dir="ltr" lang="en"> delta chat, matrix, zulip, mattermost, signal, discord</span></li>
    <li>love and trust!</li>
    <li>the <a href="https://www.recurse.com/social-rules"><span dir="ltr" lang="en"> recurse center </span> social rules</a></li>
    <li>a git forge (we like<span dir="ltr" lang="eo"> forgejo</span>)</li>
    <li>a physical <span> tilde </span> server (it can motivate everyone to create something together)</li>
    <li><strong>locations:</strong> living rooms, libraries, a cool local bookstore</li>
    <li><strong>mob programming:</strong> a big monitor, tv, or a projector. computing together is fun and contributes to your computer club's culture!</li>
    <li><a href="https://awesome-selfhosted.net/">a very thorough resource for collective computing infrastructure</a></li>
    <li>connections to other local projects</li>
    <li><em>connections to other local projects</em></li>
    <li><em><strong>connections to other local projects</strong></em></li>
</ul>

<h2>WHAT NOW?</h2>
<p>go! find the computer club! make the computer club! tell us about it!</p>

<hr>

<hr>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[We the Builders (479 pts)]]></title>
            <link>https://www.wethebuilders.org/</link>
            <guid>43133648</guid>
            <pubDate>Fri, 21 Feb 2025 22:07:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wethebuilders.org/">https://www.wethebuilders.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43133648">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="gridContainer"><h2>Blog</h2><p>Real stories from federal employees.</p><ul><li><a href="https://www.wethebuilders.org/posts/a-tale-of-two-effiencies">A Tale of Two Efficiencies: U.S. Digital Service vs. DOGE</a></li><li><a href="https://www.wethebuilders.org/posts/what-is-us-digital-service">What is the US Digital Service and Why Does it Matter?</a></li></ul><h2>Who We Are</h2><p>For decades, we've done our jobs in the background. We made it easier to file taxes, get veterans' benefits, and apply for financial aid. During times of crisis, we helped refugees navigate immigration processes, helped everyone find vaccines, and helped parents find baby formula.</p><p>Along the way, we made government websites easier to use while protecting the integrity of your personal information.</p><p>If they really wanted to know how to use technology to build a more efficient country, they would ask us.</p><p>But they haven't. They are destroyers.</p><p>We are the builders.</p><h2>Our mission</h2><p>We don't work for DOGE. We have always worked for you.</p><p>Here, you'll find stories from real government employees: How we save you time and money, how we protect your personal information, and how DOGE's dangerous dismantling of government technology puts you at risk.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[20 years working on the same software product (437 pts)]]></title>
            <link>https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/</link>
            <guid>43133174</guid>
            <pubDate>Fri, 21 Feb 2025 21:22:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/">https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/</a>, See on <a href="https://news.ycombinator.com/item?id=43133174">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>I released version 1 of my <a href="https://www.perfecttableplan.com/">table seating planning software</a>, PerfectTablePlan, in February 2005. 20 years ago this month. It was a different world. A world of Windows, shareware and CDs. A lot has changed since then, but PerfectTablePlan is now at version 7 and still going strong.</p>



<figure><a href="https://successfulsoftware.net/wp-content/uploads/2025/02/v1-screenshot-20th.png"><img data-attachment-id="12446" data-permalink="https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/v1-screenshot-20th/" data-orig-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v1-screenshot-20th.png" data-orig-size="700,438" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="v1-screenshot-20th" data-image-description="" data-image-caption="" data-medium-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v1-screenshot-20th.png?w=300" data-large-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v1-screenshot-20th.png?w=625" width="700" height="438" src="https://successfulsoftware.net/wp-content/uploads/2025/02/v1-screenshot-20th.png?w=700" alt=""></a></figure>



<p>PerfectTablePlan v1</p>



<figure><a href="https://successfulsoftware.net/wp-content/uploads/2025/02/v7-screenshot-20th.png"><img data-attachment-id="12447" data-permalink="https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/v7-screenshot-20th/" data-orig-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v7-screenshot-20th.png" data-orig-size="700,444" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="v7-screenshot-20th" data-image-description="" data-image-caption="" data-medium-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v7-screenshot-20th.png?w=300" data-large-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v7-screenshot-20th.png?w=625" width="700" height="444" src="https://successfulsoftware.net/wp-content/uploads/2025/02/v7-screenshot-20th.png?w=700" alt=""></a></figure>



<p>PerfectTablePlan v7</p>



<p>I have released several other products since then, and done some training and consulting, but PerfectTablePlan remains my most successful product. It’s success is due to a lot of hard work, and a certain amount of dumb luck.</p>



<p>I was getting married and I volunteered to do the seating plan for our wedding reception. It sounded like a relatively straightforward optimization problem, as we only had 60 guests and no family feuds to worry about. But it was surprisingly difficult to get right. I looked around for some software to help me. There were a couple of software packages, but I wasn’t impressed. I could do better myself! So I wrote a (very rough) first version, which I used for our wedding.</p>



<p>Things weren’t going great at my day job, at a small software startup. Maybe I could commercialize my table planner? I was a bit wary, as my potential competitors all seemed rather moribund and I didn’t think I would be able to make a living off it. But I thought I could do everything worth doing in 6-12 months and then start on the next product. Wrong on both counts!</p>



<p>Web-based software was still in its infancy in 2005. So I decided to write it as desktop software using C++ and cross-platform framework Qt, which I had plenty of experience in. Initially, I just released a Windows version. But I later added a Mac version as well. Qt has had its commercial ups and downs in the last 20 years, but it has grown with me and is now very robust, comprehensive and well documented. I think I made a good choice.</p>



<p>I financed PerfectTablePlan out of my own savings and it has been profitable every year since version 1 was launched. I could have taken on employees and grown the business, but I preferred to keep it as a <a href="https://successfulsoftware.net/2013/11/06/lifestyle-programming/">lifestyle business</a>. My wife does the accounts and proof reading and I do nearly everything else, with a bit of help from my accountant, web designers and a few other contractors. I don’t regret that decision. 20 years without meetings, ties or alarm clocks. My son was born 18 months after PerfectTablePlan was launched and it has been great to have the flexibility to be fully present as a Dad.</p>



<p>CDs, remember them? I sent out around 5,000 CDs (with some help from my father), before I stopped shipping CDs in 2016.</p>


<div>
<figure><a href="https://successfulsoftware.net/wp-content/uploads/2025/02/l-shadow-only-hq.png"><img data-attachment-id="12477" data-permalink="https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/l-shadow-only-hq-2/" data-orig-file="https://successfulsoftware.net/wp-content/uploads/2025/02/l-shadow-only-hq.png" data-orig-size="500,500" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="L-Shadow-Only-HQ" data-image-description="" data-image-caption="" data-medium-file="https://successfulsoftware.net/wp-content/uploads/2025/02/l-shadow-only-hq.png?w=300" data-large-file="https://successfulsoftware.net/wp-content/uploads/2025/02/l-shadow-only-hq.png?w=500" width="500" height="500" src="https://successfulsoftware.net/wp-content/uploads/2025/02/l-shadow-only-hq.png?w=500" alt=""></a></figure></div>


<p>During the lifetime of PerfectTablePlan it became clear that things were increasingly moving to the web. But I couldn’t face rewriting PerfectTablePlan from scratch for the web. Javascript. Ugh. Also PerfectTablePlan is quite compute intensive, using a genetic algorithm to generate an automated seating plan and I felt it was better running this on the customer’s local computers than my server. And some of my customers consider their seating plans to be confidential and don’t want to store them on third party servers. So I decided to stick with desktop. But, if I was starting PerfectTablePlan from scratch now, I might make a different decision.</p>



<p>Plenty of strange and wonderful things have happened over the last 20 years, including:</p>



<ul>
<li>PerfectTablePlan has been used by some very famous organizations for some very famous events (which we mostly don’t have permission to mention). It has seated royalty, celebrities and heads of state.</li>



<li>PerfectTablePlan was used as part of a <a href="https://www.perfecttableplan.com/newsletters/newsletter10_web.html">demonstration of the (controversial) first commercial quantum computer by D-Wave</a>.</li>



<li>A mock-up of PerfectTablePlan, including icons I did myself, was <a href="https://www.perfecttableplan.com/newsletters/newsletter_8.html">used without our permission</a> by Sony in their ‘Big day’ TV comedy series. I threated them with legal action. Years later, I am still awaiting a reply.</li>



<li>I got to grapple with some interesting problems, including the mathematics of <a href="https://www.perfecttableplan.com/html/genetic_algorithm.html">large combinatorial problems</a> and <a href="https://successfulsoftware.net/2008/07/18/a-mathematical-digression/">elliptical tables</a>. Some customers have seated 4,000 guests and 4000! (4000x3999x3998 .. x 1) is a mind-bogglingly huge number.</li>



<li>A well known wedding magazine ran a promotion with a valid licence key clearly visible in a photograph of a PerfectTablePlan CD. I worked through the night to release a new version of PerfectTablePlan that didn’t work with this key.</li>



<li>I found out that <a href="https://www.perfecttableplan.com/html/the_dog_ate_my_cd.html">CDs are edible</a>.</li>



<li>I sponsored the <a href="https://thejunipertrust.org/jt_projects/bampti-kindergaten-school/">building of a kindergarten in Nepal</a>.</li>



<li>I once had to stay up late, in a state of some inebriation, to fix an issue so that a world famous event wasn’t a disaster (no I can’t tell you the event).</li>
</ul>



<p>The lowest point was the pandemic, when sales pretty much dropped to zero.</p>



<p>Competitors and operating systems have come and gone and the ecosystem for software has changed a lot, but PerfectTablePlan is still here and still paying the bills. It is about 145,000 lines of C++. Some of the code is a bit ugly and not how I would write it now. But the product is very solid, with very few bugs. The website and user documentation are also substantial pieces of work. The PDF version of the documentation is nearly 500 pages.</p>



<p>I now divide my time between PerfectTablePlan and my 2 other products: <a href="https://www.easydatatransform.com/">data wrangling software</a> Easy Data Transform and <a href="https://www.hyperplan.com/">visual planner</a> Hyper Plan. Having multiple products keeps things varied and avoids having all my eggs in one basket. In May 2024 I released PerfectTablePlan v7 with a load of improvements and new features. And I have plenty of ideas for future improvements. I fully expect to keep working on PerfectTablePlan until I retire (I’m 59 now).</p>




					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sparse Voxels Rasterization: Real-Time High-Fidelity Radiance Field Rendering (102 pts)]]></title>
            <link>https://svraster.github.io/</link>
            <guid>43132964</guid>
            <pubDate>Fri, 21 Feb 2025 21:06:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://svraster.github.io/">https://svraster.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=43132964">Hacker News</a></p>
<div id="readability-page-1" class="page">

  <div>
            
            

          <!-- affiliations -->
          <p><sup>1</sup> Nvidia &nbsp;
            <sup>2</sup> Cornell University &nbsp;
            <sup>3</sup> National Taiwan University
          </p>

          <!-- TODO: update paper and video link once upload -->
          
        </div>


  <div>
        
        <p>We optimize adaptive sparse voxels radiance field from multi-view images without SfM points. The fly-through videos are rendered by our SVRaster in &gt;100 FPS.</p>
      </div>


  <div>
      <!-- Abstract. -->
      <div>
          <h2>Overview</h2>
          <p>
              We propose an efficient radiance field rendering algorithm that incorporates a rasterization process on adaptive sparse voxels without neural networks or 3D Gaussians. There are two key contributions coupled with the proposed system. The first is to adaptively and explicitly allocate sparse voxels to different levels of detail within scenes, faithfully reproducing scene details with 65536<sup>3</sup> grid resolution while achieving high rendering frame rates. Second, we customize a rasterizer for efficient adaptive sparse voxels rendering. We render voxels in the correct depth order by using ray direction-dependent Morton ordering, which avoids the well-known popping artifact found in Gaussian splatting. Our method improves the previous neural-free voxel model by over 4db PSNR and more than 10x FPS speedup, achieving state-of-the-art comparable novel-view synthesis results. Additionally, our voxel representation is seamlessly compatible with grid-based 3D processing techniques such as Volume Fusion, Voxel Pooling, and Marching Cubes, enabling a wide range of future extensions and applications.
            </p>
        </div>
      <!--/ Abstract. -->
      
      <p><img src="https://svraster.github.io/images/teaser.jpg">
      </p>
    </div>


  <div>
          <h2>Adaptive Sparse Voxel Representation and Rendering</h2>
          <div>
            <p>
              Our scene representation is a hybrid of primitive and volumetric model.
              <strong>(a) Primitive component.</strong> We explicitly allocate voxels primitives to cover different scene level-of-details under an Octree layout. Note that we do not replicate a traditional Octree data structure with parent-child pointers or linear Octree. We only keep voxels at the Octree leaf nodes without any ancestor nodes.
              <strong>(b) Volumetric component.</strong> Inside a voxel is a volumetric (trilinear) density field and a (constant) spherical harmonic field. We sample K points on the ray-voxel intersection segment to compute the intensity contribution from the voxel to the pixel with numerical integration.
            </p>
            <p><img src="https://svraster.github.io/images/representations.jpg">
            </p>
            <p>
              Adaptive level-of-details is crucial to scalability and quality. Sparse voxels with uniform voxel size can not scale up.
            </p>
            <p><img src="https://svraster.github.io/images/main_ablation.jpg">
            </p>
            <p>
              We sort voxels by ray direction-dependent Morton order, which ensures correct primitive blending order with mathematical proof. Sorting by primtive centers like 3DGS can produce inaccurate rendering.
              The inaccurate sorting causes 3DGS popping artifact (see left video below) while we don't have this issue (right video).
            </p>
            <p><img src="https://svraster.github.io/images/morton_order.jpg">
            </p>
            <video poster="" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
              <source src="https://svraster.github.io/videos/garden_sidebyside.mp4" type="video/mp4">
            </video>
          </div>
        </div>


  <div>
          <h2>Novel-view Synthesis Results</h2>
          <div>
            <p><img src="https://svraster.github.io/images/quantitative_nvs.jpg">
            </p>
          </div>
        </div>


  <div>
          <h2>Adaptive Sparse Voxel Fusion</h2>
          <div>
            <p>
              Fusing 2D modalities into the trained sparse voxels is efficient. The grid points simply take the weighted from the 2D views following classical volume fusing method. We show several examples in the following.
            </p>
            <p>
              <strong>Rendered depths → Sparse-voxel SDF → Mesh</strong>
            </p>
            <p><img src="https://svraster.github.io/images/meshing.jpg">
            </p>
            <p>
              <strong>Image segmentation by Segformer → Sparse-voxel semantic field</strong>
              <br>
              Check it in jupyter notebook <a href="https://github.com/NVlabs/svraster/blob/main/notebooks/demo_segformer.ipynb">here</a>.
            </p>
            <div>
              <p>Render view</p>
              <p>3D fused semantic field</p>
              <p>2D Segformer predction</p>
            </div>
            <video controls="" muted="" loop="" playsinline="" height="100%">
              <source src="https://svraster.github.io/videos/fusion_segformer_bicycle.mp4" type="video/mp4">
            </video>
            <video controls="" muted="" loop="" playsinline="" height="100%">
              <source src="https://svraster.github.io/videos/fusion_segformer_room.mp4" type="video/mp4">
            </video>
            <p>
              <strong>Vision foundation model feature by RADIOv2.5 → Voxel pooling → Sparse-voxel foudation feature field</strong>
              <br>
              Check it in jupyter notebook <a href="https://github.com/NVlabs/svraster/blob/main/notebooks/demo_vfm_radio.ipynb">here</a>.
            </p>
            <div>
              <p>Render view</p>
              <p>3D fused RADIO feature field</p>
              <p>2D RADIO predction</p>
            </div>
            <video controls="" muted="" loop="" playsinline="" height="100%">
              <source src="https://svraster.github.io/videos/fusion_radio_bonsai.mp4" type="video/mp4">
            </video>
            <video controls="" muted="" loop="" playsinline="" height="100%">
              <source src="https://svraster.github.io/videos/fusion_radio_garden.mp4" type="video/mp4">
            </video>
            <p>
              <strong>Dense CLIP feature by LangSplat → Voxel pooling → Sparse-voxel language field</strong>
            </p>
            <video controls="" muted="" loop="" playsinline="" height="100%">
              <source src="https://svraster.github.io/videos/fusion_langfeat.mp4" type="video/mp4">
            </video>
          </div>
        </div>

  

  <div id="BibTeX">
      <h2>BibTeX</h2>
      <pre><code>@article{Sun2024SVR,
  title={Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering},
  author={Cheng Sun and Jaesung Choe and Charles Loop and Wei-Chiu Ma and Yu-Chiang Frank Wang},
  journal={ArXiv},
  year={2024},
  volume={abs/2412.04459},
}</code>
      </pre>
    </div>


  



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Slime OS – An open-source app launcher for RP2040 based devices (124 pts)]]></title>
            <link>https://github.com/abeisgoat/slime_os</link>
            <guid>43132482</guid>
            <pubDate>Fri, 21 Feb 2025 20:22:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/abeisgoat/slime_os">https://github.com/abeisgoat/slime_os</a>, See on <a href="https://news.ycombinator.com/item?id=43132482">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">slime_os</h2><a id="user-content-slime_os" aria-label="Permalink: slime_os" href="#slime_os"></a></p>
<p dir="auto">Slime OS is an app launcher for the <a href="https://collabs.shop/fca3j3" rel="nofollow">PicoVision</a> (and soon other RP2040 and RP2350 devices). It was originally designed for the <a href="https://youtu.be/rnwPmoWMGqk" rel="nofollow">Slimedeck Zero</a>, a mini cyberdeck project. However I hope to expand it to other devices and form factors.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/abeisgoat/slime_os/blob/main/screenshot.gif"><img src="https://github.com/abeisgoat/slime_os/raw/main/screenshot.gif" alt="Slime OS launcher and i2c Scan app" data-animated-image=""></a></p>
<p dir="auto"><em>This README contains affiliate links which help support this project!</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Software</h2><a id="user-content-software" aria-label="Permalink: Software" href="#software"></a></p>
<p dir="auto">Slime OS runs in a limited 32-color mode with a 400x240 internal resolution which is interlaced up to 800x480. This resolution should scale well on most HDMI displays.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Setup</h3><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<ol dir="auto">
<li>Flash the <a href="https://github.com/pimoroni/picovision/releases">widescreen build</a> of the PicoVision firmware to your PicoVision CPU.</li>
<li>Use <a href="https://thonny.org/" rel="nofollow">Thonny</a> to replace the contents of your PicoVision Micropython filesystem with the files in <code>src</code>.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Making apps</h3><a id="user-content-making-apps" aria-label="Permalink: Making apps" href="#making-apps"></a></p>
<p dir="auto">Please refer to an <a href="https://github.com/abeisgoat/slime_os/blob/main/src/flashlight_app.py">example app</a> for boiler plate.</p>
<p dir="auto">Slime OS includes various libraries which are used internally but may also be helpful when making apps.</p>
<p dir="auto">Begin by importing slime_os...</p>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Slime OS Library</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/abeisgoat/slime_os/blob/main/src/slime_os/graphics.py">sos.gfx</a></td>
<td>Drawing methods including shapes, text, and other utilities.</td>
</tr>
<tr>
<td><a href="https://github.com/abeisgoat/slime_os/blob/main/src/slime_os/intents.py">sos.intents</a></td>
<td>Intents are used to send signals from an app to the OS, including quitting the app, swapping apps, or flipping the frame buffer.</td>
</tr>
<tr>
<td><a href="https://github.com/abeisgoat/slime_os/blob/main/src/slime_os/expansion.py">sos.ctrl</a></td>
<td>Controller for identifying expansions.</td>
</tr>
<tr>
<td><a href="https://github.com/abeisgoat/slime_os/blob/main/src/slime_os/keyboard_i2c.py">sos.kbd</a></td>
<td>Keyboard instance for reading buttons.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Issues</h3><a id="user-content-issues" aria-label="Permalink: Issues" href="#issues"></a></p>
<p dir="auto">This software is experimental and does not work completely, specifically issues include...</p>
<ul dir="auto">
<li>Input is only supported via an i2c keyboard, which is not documented (hoping to add USB keyboard soon)</li>
<li>Some apps are upside down due to the Slimedeck having the screen rotated 180 degrees. Newer apps use the <code>sos.graphics.*</code> methods which support the <code>display/flipped</code> value in <code>config.py</code>. Older apps use <code>self.display.*</code> and have wild math to rotate everything manually.</li>
<li>Everything is generally incomplete</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hardware</h2><a id="user-content-hardware" aria-label="Permalink: Hardware" href="#hardware"></a></p>
<p dir="auto">Currently this project uses a very specific set of hardware, however I'd like to expand it to support other RP2040 and RP2350 boards in the future. Feel free to do PRs to add more general hardware support.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mainboard</h3><a id="user-content-mainboard" aria-label="Permalink: Mainboard" href="#mainboard"></a></p>
<p dir="auto">This project is currently only tested on the <a href="https://collabs.shop/fca3j3" rel="nofollow">PicoVision</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Keyboard</h3><a id="user-content-keyboard" aria-label="Permalink: Keyboard" href="#keyboard"></a></p>
<p dir="auto">The (currently) only support keyboard is based off this <a href="https://www.amazon.com/dp/B01IOZBNBC/?tag=boosteroven-20" rel="nofollow">XRT500 remote</a>. There are a few variants of this same "version" of remote, but I've only used this exact one.</p>
<p dir="auto">Along with the remote, you will need an <a href="https://www.adafruit.com/product/732" rel="nofollow">MCP23017</a> to convert the key matrix to I2C. I have a few extra of the <a href="https://abe.today/products/mcp23017-port-expander-for-xrt500-tv-remote" rel="nofollow">keyboard PCBs available for sale</a>. You could also pick up an <a href="https://www.adafruit.com/product/5346" rel="nofollow">Adafruit MCP23017 board</a> and wire it to the keyboard PCB by hand.</p>
<p dir="auto">I would like to add more input types in the future.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Expansion Port</h3><a id="user-content-expansion-port" aria-label="Permalink: Expansion Port" href="#expansion-port"></a></p>
<p dir="auto">The expansion port used on the Slimedeck is the <a href="https://cdn.shopify.com/s/files/1/0174/1800/files/DK925A-10M.pdf?v=1643016288" rel="nofollow">5-pin Dk925A-10M</a> which, as far as I can tell, are only <a href="https://collabs.shop/knlijz" rel="nofollow">sold via Pimoroni</a>.</p>
<p dir="auto">The pinout used for an expansion read left to right, with the edge connector facing you is...</p>
<blockquote>
<p dir="auto">5V - SDA/TX/CPU:GP0 - SCL/RX/CPU:GP1 - CTRL/GPU:GP29 - GND</p>
</blockquote>
<p dir="auto">A 4.7k resistor should be placed on the PicoVision connecting the GPU's GP29 to GND while the expansion side should place a 4.7k resistor between CTRL (GP29) and 5v.</p>
<p dir="auto">Although the plan is to eventually support various resistor values for different protocols / speeds, due to a design mistake this is not currently reliable. The PicoVision only has one ADC pin exposed (GPU:GP29) which is used as CTRL, however to reliably determine the value of the expansion resistor we also need a second ADC to act as a VREF (voltage reference) so we can determine the true voltage of our 5v line. As it stands if the 5v line is 5.2v or 4.9v due to load it may shift the value of our expansion voltage divider out of the expected range and the expansion may be incorrectly recognized. This is easily remidied with an external i2c ADC, but it is not currently implemented.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This software is licensed as MIT.</p>
<p dir="auto">App icons are from <a href="https://piiixl.itch.io/mega-1-bit-icons-bundle" rel="nofollow">PiiiXL on Itch.io</a> and are licensed <a href="https://creativecommons.org/licenses/by/4.0/deed.en" rel="nofollow">CC BY 4.0</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Ren'Py Visual Novel Engine (196 pts)]]></title>
            <link>https://www.renpy.org/</link>
            <guid>43132336</guid>
            <pubDate>Fri, 21 Feb 2025 20:09:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.renpy.org/">https://www.renpy.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43132336">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <p><img src="https://www.renpy.org/static/index-logo.png" alt=""></p><h2>What is Ren'Py?</h2>
    <p>Ren'Py is a visual novel engine – used by thousands of creators from around the world –
      that helps you use words, images, and sounds to tell interactive stories that run on computers and mobile devices.
      These can be both visual novels and life simulation games. The easy to learn script language allows
      anyone to efficiently write large visual novels, while its Python scripting is enough for complex
      simulation games.</p>
    <p>Ren'Py is open source and free for commercial use.</p>
    

    <h2>Where does it run?</h2>

    <div>

      <div>
        <p><img src="https://www.renpy.org/static/android-small.png" alt="">
        </p><p>
        Android 5.0+
      </p></div>

      <div>
        <p><img src="https://www.renpy.org/static/html5-small.png" alt="">
        </p><p>
        HTML5/Web Assembly (Beta)
      </p></div>

      <div>
        <p><img src="https://www.renpy.org/static/linux-small.png" alt="">
        </p><p>
        Linux x86_64/Arm
      </p></div>

      <div>
        <p><img src="https://www.renpy.org/static/windows-small.png" alt="">
        </p><p>
        Windows 7+
      </p></div>

      <div>
        <p><img src="https://www.renpy.org/static/osx-small.png" alt="">
        </p><p>
        Mac OS X 10.10+
      </p></div>

      <div>
        <p><img src="https://www.renpy.org/static/ios-small.png" alt="">
        </p><p>
        iOS 11+<br>
      </p></div>

    </div>

    <h2>Where do I get it?</h2>

    <div>

      <p>
          The latest official release of Ren'Py 8 is 8.3.4 "Second Star to the Right", released on
          December 8, 2024. Ren'Py 8 is recommended for all projects.
        </p>

      <p>
          The nightly fix version of Ren'Py is built every night, and contains fixes to the latest stable version. It isn't
          tested as well as the official release, but often has fixes that haven't made it through the release process.
        </p>




    </div>

    





    <div>
      <p>
        Ren'Py 7 is the legacy version of Ren'Py, to support ongoing projects that will be released in 2024.
        The latest version of Ren'Py 7 is 7.8.4 "Straight on Till Morning", released on
        December 8, 2024.
      </p>
      
    </div>

    <div>

        <h2>How do I keep in touch?</h2>

        <p>
          The best places to ask questions about Ren'Py are the <a href="http://lemmasoft.renai.us/forums/">Lemma Soft
            Forums</a>,
          the <a href="https://discord.gg/6ckxWYm">Ren'Py Discord</a>,
          and the <a href="http://webchat.freenode.net/?channels=renpy">#renpy IRC channel</a>.
        </p>

        <p>We make news about Ren'Py available on a number of social platforms:</p>

        <p>
          <b>Twitter:</b> You can follow Ren'Py's lead developer <a href="http://twitter.com/renpytom">@renpytom</a>
          for release announcements, development news, and general commentary on life.
        </p>

        <p>
          <b>Facebook:</b>
          We announce new releases on <a href="https://www.facebook.com/renpy">our Facebook page</a>.
        </p>


        <!--
    <p>
    <b>Deviantart:</b>
    Artists interested in Ren'Py and visual novels might want to visit our <a href="http://renpy.deviantart.com/">DeviantArt</a> group.
    </p>
     -->

      </div>

    <h2>Who is it sponsored by?</h2>

    



    <div>
      
<p>

Ariane Barnes


</p>

<p>

EriksBlue


</p>

<p>

Steven Shearer


</p>

<p>

Eris Discordia


</p>

<p>

Adam Wright


</p>

<p>

KEXBOY


</p>

<p>

Rachel


</p>

<p>

Felix Schmid


</p>

    </div>

    


    <p>
      To ask questions that aren't appropriate for a public forum, or to find a
      speaker for your visual novel-related conference or con, please <a href="https://www.renpy.org/email">contact us via email</a>.
    </p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yocto, RockPi and SBOMs: Building modern embedded Linux images (139 pts)]]></title>
            <link>https://vpetersson.com/2025/02/21/yocto-rockpi-and-sboms.html</link>
            <guid>43131902</guid>
            <pubDate>Fri, 21 Feb 2025 19:34:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vpetersson.com/2025/02/21/yocto-rockpi-and-sboms.html">https://vpetersson.com/2025/02/21/yocto-rockpi-and-sboms.html</a>, See on <a href="https://news.ycombinator.com/item?id=43131902">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <p><strong>TLDR</strong>: <em>I wanted to generate an up-to-date disk image for a Rock Pi 4 using Yocto that included CUPS and Docker to both get a better understanding of Yocto and test the new SBOM generation feature.</em></p>

<p>As with many single-board computers (SBCs) from China, the issue often isn’t the board itself but rather the software. RockPi from Radxa is no exception. If you go and download the <a href="https://wiki.radxa.com/Rock4/downloads">latest disk images</a> for this board, you will notice that they are all end-of-life (EoL). However, these boards are still great and work very well for many applications. This should be top of mind if you are building a product that uses any of these devices.</p>

<p>I wanted to use one of the RockPi 4 boards I had for a simple print server. It’s not a customer product, of course, but let’s assume it was. Since it has the option to add eMMC storage, I find it more reliable than Raspberry Pi (I know the Raspberry Pi 5 allows for proper storage). However, given that I neither trust the Radxa disk images nor did I want to set things up on an already EoL Linux distribution, I started doing some digging. As it turns out, the RockPi is supported in Yocto.</p>

<p>Say what you want about Raspberry Pi, but you can still download an up-to-date OS that runs on the Pi 1.
In this article, I will show you not only how to build a disk image with Yocto (in this case for the Rock Pi 4, but it can easily be adjusted for other boards), but we will also talk a bit about how Yocto generates SBOMs (hint: it’s really clever) and where to find your SBOMs.</p>
<h2 id="what-is-yocto-anyways">What is Yocto anyways?</h2>

<p>The Yocto Project is an open-source framework for building custom Linux distributions tailored to embedded systems. It provides a flexible, modular build system based on BitBake and OpenEmbedded, enabling developers to create highly optimized and reproducible Linux images for specific hardware. Yocto is widely used in industries like automotive, IoT, and networking due to its ability to support diverse architectures and long-term maintenance needs. With its layered architecture, extensive BSP support, and strong focus on customization, Yocto is a powerful tool for developers looking to build and maintain embedded Linux systems efficiently.</p>

<p>I’ve toyed with it a few times over the years to build images for Raspberry Pis, but never really used it seriously. However, I recently crossed paths with some of the Yocto people in a CISA working group I’m co-chairing on <a href="https://github.com/CISA-SBOM-Community/SBOM-Generation">SBOM generation</a>. As it turns out, Yocto is very sophisticated when it comes to generating SBOMs, so I wanted to get some more up-to-date exposure to Yocto. Color me impressed. Not only did Yocto produce a Software Bill of Materials (SBOM) for me – it did so without even asking me.</p>

<p>Since Yocto builds everything from source and is essentially a package manager, it is able to capture all the dependencies into an SBOM. Moreover, since Yocto maintains detailed information about every dependency, it is able to generate very high-quality SBOMs.</p>

<h2 id="key-yocto-terminology">Key Yocto Terminology</h2>

<p>Before we dive in, here are some key terms in Yocto that you probably want to understand:</p>

<ul>
  <li><strong>Poky</strong> – The reference distribution of the Yocto Project, containing the OpenEmbedded build system, BitBake, and a set of metadata</li>
  <li><strong>Scarthgap</strong> – The codename for the Yocto Project 5.0 release</li>
  <li><strong>Mickledore</strong> – The codename for Yocto 4.2</li>
  <li><strong>Kirkstone</strong> – The codename for Yocto 4.0, a long-term support (LTS) release</li>
  <li><strong>Dunfell</strong> – The codename for Yocto 3.1, another LTS release</li>
  <li><strong>Layers</strong> – Modular additions to the base Yocto version that provide extra functionality</li>
  <li><strong>BitBake</strong> – The build tool used by Yocto to process recipes and generate images</li>
  <li><strong>OpenEmbedded (OE)</strong> – The build framework Yocto is based on</li>
  <li><strong>Recipes</strong> (.bb files) – Build instructions for individual packages or applications</li>
  <li><strong>BSP</strong> (Board Support Package) – A set of metadata and configurations for specific hardware platforms</li>
</ul>

<h2 id="building-a-disk-image-with-yocto">Building a disk image with Yocto</h2>

<p>Before we build, you will need a pretty beefy server to build this image (or a lot of time). I’m using my <a href="https://vpetersson.com/2024/05/04/home-server-journey.html">home server</a>, and I think it took about an hour or two to build the initial version. Subsequent builds will be a lot faster due to cache.</p>

<p>I’ve used an Ubuntu 24.04 VM to build my disk images, and you can find the base dependencies you need to install <a href="https://docs.yoctoproject.org/ref-manual/system-requirements.html">here</a>.</p>

<h3 id="lets-get-our-hands-dirty">Let’s get our hands dirty</h3>

<p>First, clone the repositories and set up the layers:</p>

<div><pre><code><span>$ </span>git clone <span>-b</span> scarthgap https://git.yoctoproject.org/poky
<span>$ </span><span>cd </span>poky
</code></pre></div>

<div><pre><code><span># Add layers</span>
<span>$ </span>git clone <span>-b</span> scarthgap git://git.yoctoproject.org/meta-arm
<span>$ </span>git clone <span>-b</span> scarthgap git://git.yoctoproject.org/meta-rockchip
<span>$ </span>git clone <span>-b</span> scarthgap git://git.openembedded.org/meta-openembedded
<span>$ </span>git clone <span>-b</span> scarthgap git://git.yoctoproject.org/meta-virtualization
</code></pre></div>

<div><pre><code><span>$ </span><span>source </span>oe-init-build-env
</code></pre></div>

<div><pre><code><span>$ </span>bitbake-layers add-layer ../meta-arm/meta-arm-toolchain
<span>$ </span>bitbake-layers add-layer ../meta-arm/meta-arm
<span>$ </span>bitbake-layers add-layer ../meta-rockchip
<span>$ </span>bitbake-layers add-layer ../meta-openembedded/meta-oe

<span># Add docker support</span>
<span>$ </span>bitbake-layers add-layer ../meta-openembedded/meta-python
<span>$ </span>bitbake-layers add-layer ../meta-openembedded/meta-networking
<span>$ </span>bitbake-layers add-layer ../meta-openembedded/meta-filesystems
<span>$ </span>bitbake-layers add-layer ../meta-virtualization
</code></pre></div>

<p>Next, adjust your <code>conf/local.conf</code> by appending these configurations:</p>

<div><pre><code>MACHINE <span>=</span> <span>"rock-pi-4b"</span>

INIT_MANAGER <span>=</span> <span>"systemd"</span>
DISTRO_FEATURES:append <span>=</span> <span>" virtualization wifi"</span>
DISTRO_FEATURES:remove <span>=</span> <span>" x11 wayland"</span>
CORE_IMAGE_EXTRA_INSTALL +<span>=</span> <span>"openssh cups cups-filters ghostscript qpdf vim docker e2fsprogs-resize2fs"</span>
</code></pre></div>

<p>Finally, build the image:</p>

<div><pre><code><span>$ </span>bitbake core-image-base
</code></pre></div>

<p>Note, if you’re building on Ubuntu 24.04, you might need to run:</p>

<div><pre><code><span>$ </span><span>sudo </span>apparmor_parser <span>-R</span> /etc/apparmor.d/unprivileged_userns
</code></pre></div>

<p>After the build completes, you can find your image here:</p>

<div><pre><code><span>$ </span><span>ls</span> <span>-lah</span> tmp/deploy/images/rock-pi-4b/core-image-base-rock-pi-4b.rootfs-<span>*</span>.wic
</code></pre></div>

<p>Flash this disk image and you should be good to go. Once it’s up and running, you should be able to SSH into the device using <code>root</code> and a blank password.</p>

<h2 id="on-updating">On updating</h2>

<p>It’s important to note that Yocto generates a disk image. By default, you cannot update this disk image by any other means than reflashing it (e.g., you can’t run “apt update”). There are over-the-air (OTA) platforms that can be integrated into Yocto, such as <a href="https://mender.io/">Mender</a> and <a href="https://rauc.io/">RAUC</a>, but by default, you need to rebuild the image from scratch to update dependencies and patch vulnerabilities.</p>

<h2 id="finding-your-sboms">Finding Your SBOMs</h2>

<p>One of the cool features of Yocto is that it automatically generates SBOMs. You can find them in the deploy directory:</p>

<div><pre><code><span>$ </span><span>ls</span> <span>-lah</span> tmp/deploy/images/rock-pi-4b/<span>*</span>spdx<span>*</span>
<span>[</span>..]
</code></pre></div>

<p>You can extract the SPDX file with:</p>

<div><pre><code><span>$ </span><span>tar</span> <span>--zstd</span> <span>-xvf</span> <span>\</span>
    path/to/tmp/deploy/images/rock-pi-4b/core-image-base-rock-pi-4b.rootfs-<span>*</span>.spdx.tar.zst
</code></pre></div>

<p>Do note that this will generate a lot of files. You will find a file called <code>index.json</code> in there, which links to all other SBOMs using document linking.</p>

<p>(Check out my article <a href="https://sbomify.com/2025/02/21/mastering-sbom-generation-with-yocto/">Mastering SBOM Generation with Yocto</a> for more details on the SBOMs.)</p>

<h2 id="on-running-in-production">On running in production</h2>

<p>If you are intending to run this in production, please do not just copy the above. These images are configured for lab or test mode. Yocto is very well suited for production images, but you need to harden them and also have an OTA strategy in place. Alternatively, I can recommend <a href="https://www.balena.io/">Balena</a>, which uses Yocto under the hood and also supports the Rock Pi.</p>

<h2 id="future-improvements">Future improvements</h2>

<p>One limitation of the current disk image for Rock Pi is that you don’t have a functional TTY. You can SSH in, or you could use a serial console, but the regular TTY doesn’t work and I haven’t spent much time trying to figure out why. Also, the disk system doesn’t automatically expand to use all available space on the eMMC/SD.</p>

<p>Some things I’m planning to add in the future:</p>
<ul>
  <li>Add support for Tailscale (there’s a <a href="https://github.com/ChristophHandschuh/meta-tailscale">meta-tailscale</a> layer)</li>
  <li>Add support for auto disk expansion</li>
  <li>Add WiFi support</li>
</ul>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://docs.yoctoproject.org/">Yocto Project Documentation</a></li>
  <li><a href="https://kacperstapor.com/blog/24-11-2024/adding-docker-to-yocto-project">Adding Docker to Yocto Project</a></li>
  <li><a href="https://www.konsulko.com/rauc-on-rockchip">RAUC on Rockchip</a></li>
</ul>


      <div>
        <h4>Enjoyed this post? Check out my podcast!</h4>
        <p>If you found this interesting, you might enjoy <a href="https://vpetersson.com/podcast" onclick="if (!window.__cfRLUnblockHandlers) return false; trackEvent('Blog CTA', 'Click', 'Podcast Link', 'Yocto, RockPi and SBOMs: Building Modern Embedded Linux Images')" data-cf-modified-7e8e2357781c951b71c2fa6a-="">"Nerding Out with Viktor"</a> - my podcast where I dive deep into tech, entrepreneurship, and security with industry experts.</p>
        <div>
          <h5>Listen on:</h5>
          





        </div>
      </div>

      <i>Found an error or typo? File PR against <a href="https://github.com/vpetersson/vpetersson.com/tree/master/_posts/2025-02-21-yocto-rockpi-and-sboms.md" rel="nofollow">this file</a>.</i>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Suckless.org: software that sucks less (313 pts)]]></title>
            <link>https://suckless.org/</link>
            <guid>43131059</guid>
            <pubDate>Fri, 21 Feb 2025 18:27:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://suckless.org/">https://suckless.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43131059">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">

<p>Home of <a href="https://dwm.suckless.org/">dwm</a>, <a href="https://tools.suckless.org/dmenu">dmenu</a> and
other quality software with a focus on simplicity, clarity, and frugality.</p>
<p>Read more about our <a href="https://suckless.org/philosophy">philosophy</a> and join us on the <a href="https://suckless.org/community">mailing
list</a>.</p>
<h2>News</h2>
<p><a href="https://suckless.org/atom.xml">Atom feed</a></p>
<h2>2024-11-26</h2>
<ul>
<li><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.4.2</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.4.2.tar.gz">download</a></li>
</ul>
<h2>2024-04-05</h2>
<ul>
<li><a href="https://st.suckless.org/">st 0.9.2</a> released: <a href="https://dl.suckless.org/st/st-0.9.2.tar.gz">download</a></li>
</ul>
<p>This reverts a commit and a regression with cursor move with wide glyphs, for
example with GNU readline.</p>
<h2>2024-03-20</h2>
<p>Below are some highlights of the changes for the recent releases of dmenu, dwm,
st and tabbed, see the git logs for all details:</p>
<p>General small Makefile improvements, rationale being: just be verbose and show
what is done: do not abstract/hide details from the user/developer.
Respect (more) the package manager and build system flags (CFLAGS, LDFLAGS, etc).</p>
<p><a href="https://git.suckless.org/dwm/log.html">dwm</a>:
</p><ul>
<li>Improvements to signal handling.</li>
<li>Fix: Avoid missing events when a keysym maps to multiple keycodes.</li>
</ul>

<p><a href="https://git.suckless.org/dmenu/log.html">dmenu</a>:
</p><ul>
<li>Reduce memory usage for reading the lines.</li>
<li>Fix: X11 BadMatch error when embedding on some windows.</li>
</ul>

<p><a href="https://git.suckless.org/st/log.html">st</a>:
</p><ul>
<li>Fix: bounds checks of dc.col.</li>
<li>Fix: buffer overflow when handling long composed input.</li>
<li>Ignore C1 control characters in UTF-8 mode.</li>
<li>Improvements to cell handling and wide characters.</li>
<li>Default config: decrease the default minlatency.</li>
<li><a href="https://git.suckless.org/st/log.html">Various other terminal fixes and compatibility improvements.</a></li>
</ul>

<p><a href="https://git.suckless.org/tabbed/log.html">tabbed</a>:
</p><ul>
<li>Fix: faulty zombie process reaping.</li>
<li>Improvements to signal handling.</li>
<li>Improve compatibility with compiling on older systems such as Slackware 11.</li>
</ul>

<p>Thanks to all contributors who submitted patches.</p>
<h2>2024-03-19</h2>
<ul>
<li><a href="https://tools.suckless.org/dmenu/">dmenu 5.3</a> released: <a href="https://dl.suckless.org/tools/dmenu-5.3.tar.gz">download</a></li>
<li><a href="https://dwm.suckless.org/">dwm 6.5</a> released: <a href="https://dl.suckless.org/dwm/dwm-6.5.tar.gz">download</a></li>
<li><a href="https://st.suckless.org/">st 0.9.1</a> released: <a href="https://dl.suckless.org/st/st-0.9.1.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/tabbed">tabbed 0.8</a> released: <a href="https://dl.suckless.org/tools/tabbed-0.8.tar.gz">download</a></li>
</ul>
<h2>2023-07-04</h2>
<p><a href="https://tools.suckless.org/slstatus">slstatus 1.0</a> released: <a href="https://dl.suckless.org/tools/slstatus-1.0.tar.gz">download</a></p>
<h2>2022-12-28</h2>
<p><a href="https://tools.suckless.org/lchat">lchat 1.0</a> released: <a href="https://dl.suckless.org/tools/lchat-1.0.tar.gz">download</a></p>
<h2>2022-11-02</h2>
<p><a href="https://libs.suckless.org/libgrapheme">libgrapheme 2.0.2</a> released: <a href="https://dl.suckless.org/libgrapheme/libgrapheme-2.0.2.tar.gz">download</a></p>
<h2>2022-10-08</h2>
<p><a href="https://libs.suckless.org/libgrapheme">libgrapheme 2.0.1</a> released: <a href="https://dl.suckless.org/libgrapheme/libgrapheme-2.0.1.tar.gz">download</a></p>
<h2>2022-10-06</h2>
<p><a href="https://libs.suckless.org/libgrapheme">libgrapheme 2.0.0</a> released: <a href="https://dl.suckless.org/libgrapheme/libgrapheme-2.0.0.tar.gz">download</a></p>
<h2>2022-10-04</h2>
<ul>
<li><a href="https://tools.suckless.org/dmenu/">dmenu 5.2</a> released: <a href="https://dl.suckless.org/tools/dmenu-5.2.tar.gz">download</a></li>
<li><a href="https://dwm.suckless.org/">dwm 6.4</a> released: <a href="https://dl.suckless.org/dwm/dwm-6.4.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/ii">ii 2.0</a> released: <a href="https://dl.suckless.org/tools/ii-2.0.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/sic">sic 1.3</a> released: <a href="https://dl.suckless.org/tools/sic-1.3.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/slock">slock 1.5</a> released: <a href="https://dl.suckless.org/tools/slock-1.5.tar.gz">download</a></li>
<li><a href="https://st.suckless.org/">st 0.9</a> released: <a href="https://dl.suckless.org/st/st-0.9.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/tabbed">tabbed 0.7</a> released: <a href="https://dl.suckless.org/tools/tabbed-0.7.tar.gz">download</a></li>
</ul>
<h2>2022-04-19</h2>
<p>Suckless now has a dark mode CSS style for its pages.
Surf also now has support for <a href="https://git.suckless.org/surf/commit/1f5b8f3bd1f37d4d3dc45d21285f34ef4752dbaa.html">dark mode</a>.</p>
<h2>2022-02-11</h2>
<p><a href="https://tools.suckless.org/dmenu/">dmenu 5.1</a> released: <a href="https://dl.suckless.org/tools/dmenu-5.1.tar.gz">download</a></p>
<h2>2022-01-07</h2>
<ul>
<li><a href="https://dwm.suckless.org/">dwm 6.3</a> released: <a href="https://dl.suckless.org/dwm/dwm-6.3.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/ii">ii 1.9</a> released: <a href="https://dl.suckless.org/tools/ii-1.9.tar.gz">download</a></li>
<li><a href="https://st.suckless.org/">st 0.8.5</a> released: <a href="https://dl.suckless.org/st/st-0.8.5.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.4.1</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.4.1.tar.gz">download</a></li>
</ul>
<h2>2021-12-22</h2>
<p><a href="https://libs.suckless.org/libgrapheme">libgrapheme 1.0.0</a> released: <a href="https://dl.suckless.org/libgrapheme/libgrapheme-1.0.0.tar.gz">download</a></p>
<h2>2021-07-30</h2>
<p><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.4</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.4.tar.gz">download</a></p>
<h2>2021-05-09</h2>
<p>On Tuesday, 2021-05-11 there will be scheduled maintenance of the suckless
servers. It's estimated this will take about 1 hour from about 21:00 to
22:00 UTC+02:00.</p>
<p>The mailinglist, website and source-code repositories will have some downtime.</p>
<p><strong>Update:</strong> the maintenance was finished at 2021-05-12 23:33 UTC+02:00.
P.S.: It didn't actually take 26h30, I just had forgotten to do it.</p>
<h2>2021-05-08</h2>
<p><a href="https://surf.suckless.org/">surf 2.1</a> released: <a href="https://dl.suckless.org/surf/surf-2.1.tar.gz">download</a></p>
<h2>2021-03-28</h2>
<p><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.3</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.3.tar.gz">download</a></p>
<h2>2021-03-28</h2>
<p>On Wednesday, 2021-03-31 there will be scheduled maintenance of the suckless
servers. It's estimated this will take about 2-3 hours from about 19:00 to
21:00 - 22:00 UTC+02:00.</p>
<p>The mailinglist, website and source-code repositories will have some downtime.</p>
<p><strong>Update:</strong> the maintenance was finished at 2021-03-31 19:10 UTC+02:00.</p>
<h2>2021-01-19</h2>
<p><a href="https://tools.suckless.org/scroll/">scroll 0.1</a> released: <a href="https://dl.suckless.org/tools/scroll-0.1.tar.gz">download</a></p>
<h2>2020-12-11</h2>
<p><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.2.2</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.2.2.tar.gz">download</a></p>
<h2>2020-09-18</h2>
<p><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.2.1</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.2.1.tar.gz">download</a></p>
<h2>2020-09-13</h2>
<p><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.2</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.2.tar.gz">download</a></p>
<h2>2020-09-02</h2>
<p><a href="https://tools.suckless.org/dmenu/">dmenu 5.0</a> released: <a href="https://dl.suckless.org/tools/dmenu-5.0.tar.gz">download</a></p>
<h2>2020-06-19</h2>
<p><a href="https://st.suckless.org/">st 0.8.4</a> released: <a href="https://dl.suckless.org/st/st-0.8.4.tar.gz">download</a></p>
<h2>2020-05-27</h2>
<p>The <a href="https://suckless.org/conferences/2020">slcon7</a> has been cancelled due to the 2019-nCoV
pandemic.</p>
<h2>2020-04-27</h2>
<p><a href="https://st.suckless.org/">st 0.8.3</a> released: <a href="https://dl.suckless.org/st/st-0.8.3.tar.gz">download</a></p>
<h2>2019-12-01</h2>
<p>On Wednesday, 2019-12-04 there will be scheduled maintenance of the suckless
servers. It's estimated this will take about 2-3 hours from about 19:00 to
21:00 - 22:00 UTC+01:00.</p>
<p>The mailinglist, website and source-code repositories will have some downtime.</p>
<p><strong>Update:</strong> the maintenance was finished at 2019-12-04 20:00 UTC+01:00.</p>
<h2>2019-04-04</h2>
<p>Registrations are now open for <a href="https://suckless.org/conferences/2019">slcon6</a> that will be held in
Bad Liebenzell, Germany on 2019-10-(04-06).</p>
<p>The CfP for interested participants will end on 2019-06-30.</p>
<h2>2019-03-30</h2>
<p>There is now a <a href="https://gunther.suckless.org/patches/">patch overview</a> tool to have a
quick overview of the patch status list. This list is generated each day from
the <a href="https://git.suckless.org/sites/">sites</a> repository. It checks if patches apply
cleanly in a normal patching manner. Of course it does not check patch
combinations.</p>
<ul>
<li><a href="https://suckless.org/hacking/">Hacking patches guidelines</a></li>
<li><a href="https://git.suckless.org/sites/file/testpatches.sh.html">Tool source-code</a></li>
</ul>
<p>Please keep the patches tidy and maintain or remove them.</p>
<h2>2019-02-09</h2>
<p><a href="https://st.suckless.org/">st 0.8.2</a> released: <a href="https://dl.suckless.org/st/st-0.8.2.tar.gz">download</a></p>
<p>This release has mostly bugfixes.</p>
<h2>2019-02-03</h2>
<ul>
<li><a href="https://dwm.suckless.org/">dwm 6.2</a> released: <a href="https://dl.suckless.org/dwm/dwm-6.2.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/dmenu/">dmenu 4.9</a> released: <a href="https://dl.suckless.org/tools/dmenu-4.9.tar.gz">download</a></li>
</ul>
<h2>2018-06-01</h2>
<p>The maintainance is completed. Let me know of any important things that are broken.
Internally we will keep tweaking the server configuration over the course of
time.</p>
<h2>2018-05-27</h2>
<p>There will be a scheduled server maintenance next Friday and Saturday, 2018-06-(01-02).
The migration to the new server will happen on these days and the git
repositories and mailing list will be frozen on the old (now current)
server.</p>
<h2>2018-04-11</h2>
<p><a href="https://tools.suckless.org/farbfeld/">farbfeld 4</a> released: <a href="https://dl.suckless.org/farbfeld/farbfeld-4.tar.gz">download</a></p>
<h2>2018-03-20</h2>
<p><a href="https://st.suckless.org/">st 0.8.1</a> released: <a href="https://dl.suckless.org/st/st-0.8.1.tar.gz">download</a></p>
<p>This release fixes some regressions introduced in the 0.8 release.</p>
<h2>2018-03-19</h2>
<p>Registrations for <a href="https://suckless.org/conferences/2018/">slcon5</a> are now open.</p>
<h2>2018-03-14</h2>
<ul>
<li><a href="https://tools.suckless.org/dmenu/">dmenu 4.8</a> released: <a href="https://dl.suckless.org/tools/dmenu-4.8.tar.gz">download</a></li>
<li><a href="https://st.suckless.org/">st 0.8</a> released: <a href="https://dl.suckless.org/st/st-0.8.tar.gz">download</a></li>
</ul>
<h2>2018-02-04</h2>
<p><a href="https://tools.suckless.org/ii">ii 1.8</a> released: <a href="https://dl.suckless.org/tools/ii-1.8.tar.gz">download</a></p>
<h2>2017-09-04</h2>
<p><a href="https://suckless.org/conferences/2017">suckless hackathon</a>: we met on Sep 1-3 2017 in Würzburg, Germany.</p>
<h2>2017-09-04</h2>
<p><a href="https://tools.suckless.org/sent">sent 1</a> released: <a href="https://dl.suckless.org/tools/sent-1.tar.gz">download</a></p>
<h2>2017-08-30</h2>
<p>suckless.org now supports TLS using <a href="https://letsencrypt.org/">Let's Encrypt</a>.
Cloning git repos over HTTPS now works. Some links on the page have been
changed to allow both HTTP and HTTPS.</p>
<p>HSTS is not fully working yet. This will be fixed.</p>
<p>The IPv6 AAAA record was added and IPv6 is fully working now.</p>
<p>suckless has many subdomains, these should hopefully all work via TLS. If you
see a subdomain without a signed certificate please report it. If you find any
broken links on the wiki pages, these can be fixed by anyone.</p>
<h2>2017-07-03</h2>
<p>The suckless.org project is now hosted on a new server. All inactive accounts
have been removed during the relocation.</p>
<p>Please note that the new ECDSA key fingerprint is
SHA256:7DBXcYScmsxbv7rMJUJoJsY5peOrngD4QagiXX6MiQU.</p>
<h2>2017-05-06</h2>
<p><a href="https://tools.suckless.org/blind">blind 1.1</a> released:
<a href="https://dl.suckless.org/tools/blind-1.1.tar.gz">download</a></p>
<h2>2017-05-02</h2>
<p><a href="https://tools.suckless.org/dmenu">dmenu 4.7</a> released:
<a href="https://dl.suckless.org/tools/dmenu-4.7.tar.gz">download</a></p>
<h2>2017-04-14</h2>
<p><a href="https://tools.suckless.org/farbfeld/">farbfeld 3</a> released:
<a href="https://dl.suckless.org/farbfeld/farbfeld-3.tar.gz">download</a></p>
<h2>2017-03-28</h2>
<p><a href="https://surf.suckless.org/">surf</a> now uses webkit2 by default. The webkit1 version
is kept in the <a href="https://git.suckless.org/surf/log/?h=surf-webkit1">surf-webkit1</a>
branch. The “master” branch doesn't exist anymore, HEAD is now
<a href="https://git.suckless.org/surf/log/">surf-webkit2</a>, so be sure to rebase your local
master commits onto surf-webkit1.</p>
<h2>2016-11-20</h2>
<p><a href="https://tools.suckless.org/slock">slock 1.4</a> released:
<a href="https://dl.suckless.org/tools/slock-1.4.tar.gz">download</a></p>
<h2>2016-09-26</h2>
<p>Videos of the <a href="https://suckless.org/conferences/2016">slcon 2016 talks</a> are now available.</p>
<h2>2016-08-24</h2>
<p><a href="https://suckless.org/conferences/2016">slcon3</a> preliminary schedule now published. If you want to
attend please register before: <strong>2016-09-01</strong>.</p>
<h2>2015-12-19</h2>
<p><a href="https://surf.suckless.org/">surf 0.7</a> released:
<a href="https://dl.suckless.org/surf/surf-0.7.tar.gz">download</a></p>
<h2>2015-11-25</h2>
<p><a href="https://tools.suckless.org/sent">sent 0.2</a> released:
<a href="https://dl.suckless.org/tools/sent-0.2.tar.gz">download</a></p>
<h2>2015-11-13</h2>
<p>Videos of the <a href="https://suckless.org/conferences/2015">slcon2 talks</a> are now available.</p>
<h2>2015-11-09</h2>
<p><a href="https://dwm.suckless.org/">dwm 6.1</a> released:
<a href="https://dl.suckless.org/dwm/dwm-6.1.tar.gz">download</a></p>
<h2>2015-09-23</h2>
<p>Kai and Anselm gave an interview about suckless.org on Randal Schwartz's <a href="https://twit.tv/shows/floss-weekly/episodes/355?autostart=false">FLOSS
Weekly show</a></p>
<h2>2015-07-07</h2>
<p><a href="https://st.suckless.org/">st 0.6</a> released:
<a href="https://dl.suckless.org/st/st-0.6.tar.gz">download</a></p>
<h2>2015-02-14</h2>
<p><a href="https://suckless.org/conferences/2015">slcon2</a> will be held in Budapest on 2015-10-(30-31).</p>
<p>The CfP for interested participants is now open and will end on 2015-04-30.</p>
<h2>2014-11-29</h2>
<p><a href="https://tools.suckless.org/x/lsw">lsw 0.3</a> released:
<a href="https://dl.suckless.org/tools/lsw-0.3.tar.gz">download</a></p>
<h2>2014-11-24</h2>
<p>There will be a
<a href="https://events.ccc.de/congress/2014/wiki/Assembly%3ASuckless">suckless assembly</a>
at the <a href="https://events.ccc.de/congress/2014">31C3</a>. The whole suckless
community is invited to come, meet and hack!</p>
<h2>2014-08-05</h2>
<p><a href="https://core.suckless.org/sinit">sinit 0.9.1</a> released:
<a href="https://dl.suckless.org/sinit/sinit-0.9.1.tar.gz">download</a></p>
<h2>2014-05-01</h2>
<p><a href="https://core.suckless.org/ubase">ubase 0.1</a> released:
<a href="https://dl.suckless.org/ubase/ubase-0.1.tar.gz">download</a></p>
<h2>2014-01-21</h2>
<p><a href="https://tools.suckless.org/tabbed">tabbed 0.6</a> released:
<a href="https://dl.suckless.org/tools/tabbed-0.6.tar.gz">download</a></p>
<h2>2013-06-16</h2>
<p><a href="https://tools.suckless.org/sic">sic 1.2</a> released:
<a href="https://dl.suckless.org/tools/sic-1.2.tar.gz">download</a></p>
<h2>2013-05-07</h2>
<p><a href="https://tools.suckless.org/x/xssstate">xssstate 1.1</a> released:
<a href="https://dl.suckless.org/tools/xssstate-1.1.tar.gz">download</a></p>
<h2>2013-05-06</h2>
<p><a href="https://tools.suckless.org/tabbed">tabbed 0.5</a> released:
<a href="https://dl.suckless.org/tools/tabbed-0.5.tar.gz">download</a></p>
<h2>2013-04-21</h2>
<p>We are glad to announce the <a href="https://suckless.org/conferences/2013">slcon 2013</a> programme.</p>
<h2>2012-11-29</h2>
<p>We are glad to announce the switch to git from mercurial in all of our
repositories. You can find them at <a href="https://git.suckless.org/">git.suckless.org</a> Many
thanks to 20h for his contribution!</p>
<h2>2012-10-28</h2>
<p><a href="https://tools.suckless.org/x/sprop">sprop 0.1</a> released:
<a href="https://dl.suckless.org/tools/sprop-0.1.tar.gz">download</a></p>
<h2>2012-10-14</h2>
<p>Today we heard a very sad news that our friend, contributor and philosophical
advisor Uriel has passed away peacefully. We will miss him a lot.</p>
<p><img src="https://suckless.org/uriel.png" alt="uriel"></p>
<p>RIP</p>
<h2>2011-05-14</h2>
<p>Anselm gave a talk about <strong>The 'suckless.org' universe</strong> at the <a href="http://www.linuxtag.org/">LinuxTag
2011</a> conference in Berlin.</p>
<h2>2011-01-31</h2>
<p><a href="https://tools.suckless.org/ii">ii 1.6</a> released (regression fix):
<a href="https://dl.suckless.org/tools/ii-1.6.tar.gz">download</a></p>
<h2>2010-06-04</h2>
<p><a href="https://tools.suckless.org/9base">9base-6</a> released:
<a href="https://dl.suckless.org/tools/9base-6.tar.gz">download</a></p>
<h2>2010-03-28</h2>
<p>We learned today that the previous wmii maintainer, who wasn't actively
involved since 2007, Denis Grelich,
<a href="https://web.archive.org/web/20140208043925/http://www.lmt.uni-saarland.de/de/aktuelles/grelich.html">died on 2010-03-12</a>.
We thank him for his work. Rest in peace.</p>
<h2>2010-03-07</h2>
<p>We applied as a mentoring organisation for GSoC 2010. See our <a href="https://suckless.org/project_ideas">project ideas
for GSoC 2010</a> page for further details.</p>
<h2>2010-02-13</h2>
<p>Some of us will visit <a href="http://chemnitzer.linux-tage.de/2010/">CLT2010</a>. Anselm
will give a
<a href="http://chemnitzer.linux-tage.de/2010/vortraege/detail.html?idx=308">talk</a>
about stali on the second day of CLT2010 at 17:00.</p>
<h2>2009-12-28</h2>
<p>There was a small community meeting in Berlin! Thanks to all attendees.</p>
<h2>2008-08-02</h2>
<p><a href="https://tools.suckless.org/x/wmname">wmname 0.1</a> released:
<a href="https://dl.suckless.org/tools/wmname-0.1.tar.gz">download</a></p>
<h2>2008-07-29</h2>
<p><a href="https://tools.suckless.org/x/sselp">sselp 0.2</a> released:
<a href="https://dl.suckless.org/tools/sselp-0.2.tar.gz">download</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA's James Webb Space Telescope faces potential 20% budget cut (149 pts)]]></title>
            <link>https://www.space.com/space-exploration/james-webb-space-telescope/nasa-james-webb-space-telescope-faces-20-percent-budget-cuts</link>
            <guid>43131045</guid>
            <pubDate>Fri, 21 Feb 2025 18:25:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.space.com/space-exploration/james-webb-space-telescope/nasa-james-webb-space-telescope-faces-20-percent-budget-cuts">https://www.space.com/space-exploration/james-webb-space-telescope/nasa-james-webb-space-telescope-faces-20-percent-budget-cuts</a>, See on <a href="https://news.ycombinator.com/item?id=43131045">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1200-80.jpg" alt="a spacecraft with a large gold hexagon on top in deep space" srcset="https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: dima_zel/iStock/Getty Images)</span>
</figcaption>
</div>

<div id="article-body">
<p>The scientists behind NASA's largest and most powerful space telescope ever built are bracing for potentially crippling budget cuts, and the observatory is only halfway through its primary mission.</p><p>The team overseeing NASA's <a data-analytics-id="inline-link" href="https://www.space.com/21925-james-webb-space-telescope-jwst.html" data-before-rewrite-localise="https://www.space.com/21925-james-webb-space-telescope-jwst.html"><u>James Webb Space Telescope</u></a> (JWST) has been directed to prepare for up to 20% in budget cuts that would touch on every aspect of the flagship observatory's operations, which are managed by the Space Telescope Science Institute (STScI) in Maryland. The potential cut comes even as the space observatory is more in demand than ever before, with astronomers requesting the equivalent of nine years' worth of Webb observing time in one operational year.</p><p>"NASA is having budget constraints across the entire board, so the institute is being asked to consider a significant — about 20% — cut to our operational budget for the mission starting later this year," Tom Brown, who leads the Webb mission office at STScI, told a crowd of scientists last month at the 245th American Astronomical Society (AAS) meeting in National Harbor, Maryland. "So the impacts of that, if it comes to pass, pretty much cut across the entire mission."</p><p>NASA's <a data-analytics-id="inline-link" href="https://www.space.com/nasa-white-house-2025-budget-request" data-before-rewrite-localise="https://www.space.com/nasa-white-house-2025-budget-request"><u>$25.4 billion budget request for 2025</u></a> set aside $317 million to fund the Webb space telescope, as well as the <a data-analytics-id="inline-link" href="https://www.space.com/15892-hubble-space-telescope.html" data-before-rewrite-localise="https://www.space.com/15892-hubble-space-telescope.html"><u>Hubble Space Telescope</u></a> and <a data-analytics-id="inline-link" href="https://www.space.com/18669-chandra-x-ray-observatory.html" data-before-rewrite-localise="https://www.space.com/18669-chandra-x-ray-observatory.html"><u>Chandra X-ray Observatory</u></a> that together comprise NASA's currently operational "Great Observatories." The Hubble Telescope program is facing a potential 20% budget cut of its own, <a data-analytics-id="inline-link" href="https://spacenews.com/hubble-budget-cuts-could-impact-science-and-mission-operations/"><u>according to SpaceNews</u></a>. And Chandra <a data-analytics-id="inline-link" href="https://www.space.com/chandra-x-ray-observatory-nasa-fy2025-budget" data-before-rewrite-localise="https://www.space.com/chandra-x-ray-observatory-nasa-fy2025-budget"><u>is facing the end of its mission</u></a>, with NASA's 2025 budget request including plans to wind down operations, with its budget dropping from $41.1 million this year to just $5.2 million in 2029.</p><p>But unlike Hubble, which turns 35 this spring, and Chandra, which launched in 1999, Webb is in its prime, approaching the midpoint of a primary 10-year mission. It could last at least 20 years or more, NASA officials have said. The mission is an international partnership between NASA, the European Space Agency and the Canadian Space Agency.</p><p>"Frankly, this mission works far better than, really, most folks expected it to, you know," Brown said during the Webb town hall event on Jan. 15 at the AAS conference. "It's extremely worrisome that, while we're in the middle of the prime mission, we're also maybe looking at significant budget cuts."</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-320-80.png.webp 320w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-1200-80.png.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-320-80.png" alt="The galaxy GN-z11 as seen by Hubble (inset) an illustration of a feeding black hole" srcset="https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-320-80.png 320w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-1200-80.png 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg.png"></picture></p></div><figcaption itemprop="caption description"><span>The James Webb Space Telescope has made mind-boggling discoveriesin its first four years, like this one of the galaxy GN-z11 with the oldest and farthest black hole ever seen. </span><span itemprop="copyrightHolder">(Image credit: NMASA, ESA, P. Oesch (Yale University), G. Brammer (STScI), P. van Dokkum (Yale University), and G. Illingworth (University of California, Santa Cruz) (Inset) Robert Lea)</span></figcaption></figure><p>The $10 billion Webb space telescope survived a tumultuous development process, one that included cost overruns and technical delays that nearly killed the observatory before it ever flew. Lawmakers with the House Appropriations Committee <a data-analytics-id="inline-link" href="https://www.space.com/12187-nasa-budget-bill-cancels-space-telescope-house.html" data-before-rewrite-localise="https://www.space.com/12187-nasa-budget-bill-cancels-space-telescope-house.html"><u>proposed cancelling the mission</u></a> in 2011, a decade before <a data-analytics-id="inline-link" href="https://www.space.com/nasa-james-webb-space-telescope-launch-success" data-before-rewrite-localise="https://www.space.com/nasa-james-webb-space-telescope-launch-success"><u>Webb's Christmas Day launch in 2021</u></a>, only to back down after backlash from scientists and influential politicians defending the observatory.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-rTLnXKeQtgwCKwvYQVHpC6"><section><p>Breaking space news, the latest updates on rocket launches, skywatching events and more!</p></section></div><p>Since its 2021 launch, the Webb space telescope <a data-analytics-id="inline-link" href="https://www.space.com/james-webb-space-telescope-first-year-astronomers-in-tears" data-before-rewrite-localise="https://www.space.com/james-webb-space-telescope-first-year-astronomers-in-tears"><u>has outmatched even the most optimistic predictions</u></a> for its performance. Its infrared optics have looked deep into the universe's past, observed distant galaxies and exoplanets, and even peered at our own local solar system planets closer to home.</p><p>"In a nutshell, it is truly fulfilling its promise," Macarena Garcia Marin, STScI's Webb project scientist, said during the same town hall event. "Across every field, JWST is truly delivering cutting-edge science."</p><p>Some of Webb's budget challenges stem from its operational costs, which were set "idealistically low" in 2011 when the observatory was saved from cancellation. Those costs, coupled with inflation rates that were much higher than expected and less flexibility in NASA's budget, have also contributed, Brown said.</p><p>According to a <a data-analytics-id="inline-link" href="https://www.stsci.edu/files/live/sites/www/files/home/jwst/news-events/events/2025/_documents/0125-jwst-townhall-mission-status-brown.pdf"><u>presentation by Brown</u></a>, a 20% cut to Webb's operational budget would definitely affect how much science the telescope could perform. The impacts would be felt across teams that review proposals for observing targets, data analysis, observatory efficiencies, and anomaly resolution when something goes wrong, not to mention the need to engage with the scientific community and public on Webb's science results.</p><p>"It's a huge cut. That's not like kind of trying to nibble away at the edges," Brown told Space.com. "That impacts everything across the board, all the way up to how many modes we're offering to the observers."</p><p>Those impacts, Brown said, would likely be felt for the first time in October, when the next fiscal year begins.</p><p>Brown's comments at the Webb observatory town hall at AAS came just before the <a data-analytics-id="inline-link" href="https://www.space.com/space-exploration/we-will-pursue-our-manifest-destiny-into-the-stars-president-trump-wants-astronauts-to-raise-the-american-flag-on-mars" data-before-rewrite-localise="https://www.space.com/space-exploration/we-will-pursue-our-manifest-destiny-into-the-stars-president-trump-wants-astronauts-to-raise-the-american-flag-on-mars"><u>inauguration of President Donald Trump</u></a>, who in subsequent weeks created the Department of Government Efficiency headed by SpaceX CEO <a data-analytics-id="inline-link" href="https://www.space.com/18849-elon-musk.html" data-before-rewrite-localise="https://www.space.com/18849-elon-musk.html"><u>Elon Musk</u></a> to reduce government spending. DOGE, as it's known, has worked to dismantle some entire agencies, like the U.S. Agency for International Development, which provides aid to other countries during disasters and other emergencies, while also overseeing massive cuts to the federal workforce. Nearly <a data-analytics-id="inline-link" href="https://www.space.com/the-universe/earth/over-1-000-nasa-employees-saved-from-dismissal-as-trump-downsizes-federal-workforce" data-before-rewrite-localise="https://www.space.com/the-universe/earth/over-1-000-nasa-employees-saved-from-dismissal-as-trump-downsizes-federal-workforce"><u>1,000 NASA jobs could be eliminated</u></a>, though they appear to have been saved from layoffs earlier this week.</p><p>Trump has nominated American billionaire entrepreneur <a data-analytics-id="inline-link" href="https://www.space.com/space-exploration/private-spaceflight/who-is-jared-isaacman-trumps-pick-for-nasa-chief" data-before-rewrite-localise="https://www.space.com/space-exploration/private-spaceflight/who-is-jared-isaacman-trumps-pick-for-nasa-chief"><u>Jared Isaacman</u></a>, who has flown in orbit twice on private SpaceX missions he financed himself, to serve as the next NASA administrator, though Isaacman has yet to be confirmed. The agency is currently being led by Acting Administrator <a data-analytics-id="inline-link" href="https://www.space.com/space-exploration/missions/who-is-janet-petro-trumps-pick-for-acting-nasa-administrator" data-before-rewrite-localise="https://www.space.com/space-exploration/missions/who-is-janet-petro-trumps-pick-for-acting-nasa-administrator"><u>Janet Petro</u></a>, former director of the agency's Kennedy Space Center in Florida.</p>
</div>
<p><em><a href="https://forums.space.com/">Join our Space Forums</a> to keep talking space on the latest missions, night sky and more! And if you have a news tip, correction or comment, let us know at: <a href="mailto:community@space.com">community@space.com.</a></em></p>

<div id="slice-container-authorBio-rTLnXKeQtgwCKwvYQVHpC6"><p>Tariq is the Editor-in-Chief of <a href="https://www.space.com/" data-before-rewrite-localise="https://www.space.com/">Space.com</a> and joined the team in 2001, first as an intern and staff writer, and later as an editor. He covers human spaceflight, exploration and space science, as well as skywatching and entertainment. He became Space.com's Managing Editor in 2009 and Editor-in-Chief in 2019. Before joining Space.com, Tariq was a staff reporter for The Los Angeles Times covering education and city beats in La Habra, Fullerton and Huntington Beach. In October 2022, <a href="https://www.nscfl.org/kolcum-award/" target="_blank">Tariq received the Harry Kolcum Award</a> for excellence in space reporting from the National Space Club Florida Committee. He is also an Eagle Scout (yes, he has the Space Exploration merit badge) and went to Space Camp four times as a kid and a fifth time as an adult. He has journalism degrees from the University of Southern California and New York University. You can find Tariq at Space.com and as the co-host to the <a href="https://twit.tv/shows/this-week-in-space" target="_blank">This Week In Space podcast</a> with space historian Rod Pyle on the <a href="https://twit.tv/" target="_blank">TWiT network</a>. To see his latest project, you can follow Tariq on&nbsp;Twitter <a href="https://twitter.com/tariqjmalik" target="_blank">@tariqjmalik</a>.</p></div>

</section>



<div data-test-id="more-about">
<p>More about science astronomy</p>


</div>

<div id="slice-container-relatedArticles"><p><h3>Most Popular</h3></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I think Yann Lecun was right about LLMs (but perhaps only by accident) (116 pts)]]></title>
            <link>https://substack.com/home/post/p-157633768</link>
            <guid>43131022</guid>
            <pubDate>Fri, 21 Feb 2025 18:23:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://substack.com/home/post/p-157633768">https://substack.com/home/post/p-157633768</a>, See on <a href="https://news.ycombinator.com/item?id=43131022">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tab="[object Object]"><p><h3 translated="">The app for independent voices</h3></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Richard Feynman's blackboard at the time of his death (1988) (423 pts)]]></title>
            <link>https://digital.archives.caltech.edu/collections/Images/1.10-29/</link>
            <guid>43131017</guid>
            <pubDate>Fri, 21 Feb 2025 18:22:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://digital.archives.caltech.edu/collections/Images/1.10-29/">https://digital.archives.caltech.edu/collections/Images/1.10-29/</a>, See on <a href="https://news.ycombinator.com/item?id=43131017">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article><p>These digitized collections are accessible for purposes of education and research. Due to the nature of archival collections, archivists at the Caltech Archives and Special Collections are not always able to identify copyright and rights of privacy, publicity, or trademark. We are eager to <a href="mailto:archives@caltech.edu">hear from any rights holders</a>, so that we may obtain accurate information. Upon request, we’ll remove material from public view while we address a rights issue.</p></article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lithium-sulfur battery retains 80% charge capacity after 25,000 cycles (112 pts)]]></title>
            <link>https://techxplore.com/news/2025-01-lithium-sulfur-battery-retains-capacity.html</link>
            <guid>43130923</guid>
            <pubDate>Fri, 21 Feb 2025 18:15:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techxplore.com/news/2025-01-lithium-sulfur-battery-retains-capacity.html">https://techxplore.com/news/2025-01-lithium-sulfur-battery-retains-capacity.html</a>, See on <a href="https://news.ycombinator.com/item?id=43130923">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2025/lithium-sulfur-battery.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2025/lithium-sulfur-battery.jpg" data-sub-html="The experimental evidence of diffusion of I<sub>2</sub> along the SE particle surface and the occurrence of the reaction between I<sub>2</sub> as redox mediator and Li<sub>2</sub>S. Credit: <i>Nature</i> (2025). DOI: 10.1038/s41586-024-08298-9">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2025/lithium-sulfur-battery.jpg" alt="Lithium-sulfur battery retains 80% charge capacity after 25,000 cycles" title="The experimental evidence of diffusion of I2 along the SE particle surface and the occurrence of the reaction between I2 as redox mediator and Li2S. Credit: Nature (2025). DOI: 10.1038/s41586-024-08298-9" width="800" height="530">
             <figcaption>
                The experimental evidence of diffusion of I<sub>2</sub> along the SE particle surface and the occurrence of the reaction between I<sub>2</sub> as redox mediator and Li<sub>2</sub>S. Credit: <i>Nature</i> (2025). DOI: 10.1038/s41586-024-08298-9
            </figcaption>        </figure>
    </div><p>An international team of engineers and materials scientists has developed a lithium-sulfur battery capable of retaining 80% of its charge capacity after 25,000 cycles. Their paper is <a href="https://www.nature.com/articles/s41586-024-08298-9" target="_blank">published</a> in the journal <i>Nature</i>.</p>

                                        
                                              
                                        
                                                                                    <p>To make batteries smaller and lighter, engineers continually look for new materials. Such efforts tend to focus on the electrodes where lithium is held by other materials. Finding a better material to hold the lithium could result in an overall lighter and more compact battery.</p>
<p>One of the more promising materials is sulfur, due to its quality, abundance and low cost. Unfortunately, some of sulfur's reactions with lithium lead to ion loss, and worse, it tends to expand, leading to degradation and a short battery life.</p>
<p>In this new study, the research team working in China found a way around such problems and built a battery that can hold up longer than other batteries over thousands of recharge cycles.</p>
<p>The approach uses sulfur to create a solid electrode—its porous atomic structure allows for ion diffusion without movement of intermediaries. To create the electrode, the team created a glass-like mixture made from <a href="https://techxplore.com/tags/sulfur/" rel="tag">sulfur</a>, boron, lithium, phosphorus and iodine. The latter proved to be the key; it helped speed the movement of electrons through the <a href="https://techxplore.com/tags/redox+reactions/" rel="tag">redox reactions</a>, which led to faster reaction speeds.</p>

<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2025/lithium-sulfur-battery-1.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2025/lithium-sulfur-battery-1.jpg" data-sub-html="Fundamental concept and characterization of the lithium thioborophosphate iodide glass-phase solid electrolytes. Credit: <i>Nature</i> (2025). DOI: 10.1038/s41586-024-08298-9">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2025/lithium-sulfur-battery-1.jpg" alt="Lithium-sulfur battery retains 80% charge capacity after 25,000 cycles" title="Fundamental concept and characterization of the lithium thioborophosphate iodide glass-phase solid electrolytes. Credit: Nature (2025). DOI: 10.1038/s41586-024-08298-9">
             <figcaption>
                Fundamental concept and characterization of the lithium thioborophosphate iodide glass-phase solid electrolytes. Credit: <i>Nature</i> (2025). DOI: 10.1038/s41586-024-08298-9
            </figcaption>        </figure>
    </div>
<p>The result was a battery that could be charged quickly, even when exposed to <a href="https://techxplore.com/tags/high+temperatures/" rel="tag">high temperatures</a>. But more importantly, the battery was capable of retaining an 80% charge capacity after undergoing 25,000 charge/recharge cycles—a noticeable improvement over typical lithium-ion batteries, which tend to degrade after just 1,000 cycles.</p>
<p>The researchers suggest more work is required to improve the <a href="https://techxplore.com/tags/energy+density/" rel="tag">energy density</a> and perhaps to find other materials to use for the mix to ensure a low-weight battery.</p>

                                                                                
                                        											<div>
																								<p><strong>More information:</strong>
												Huimin Song et al, All-solid-state Li–S batteries with fast solid–solid sulfur reaction, <i>Nature</i> (2025). <a data-doi="1" href="https://dx.doi.org/10.1038/s41586-024-08298-9" target="_blank">DOI: 10.1038/s41586-024-08298-9</a>
																								
																								</p>
																							</div>
                                        											
										                                                                                    <p>
                                                © 2025 Science X Network
                                            </p>
                                                                                
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                Lithium-sulfur battery retains 80% charge capacity after 25,000 cycles (2025, January 20)
                                                retrieved 22 February 2025
                                                from https://techxplore.com/news/2025-01-lithium-sulfur-battery-retains-capacity.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SWE-Bench tainted by answer leakage; real pass rates significantly lower (336 pts)]]></title>
            <link>https://arxiv.org/abs/2410.06992</link>
            <guid>43130732</guid>
            <pubDate>Fri, 21 Feb 2025 17:59:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2410.06992">https://arxiv.org/abs/2410.06992</a>, See on <a href="https://news.ycombinator.com/item?id=43130732">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2410.06992">View PDF</a>
    <a href="https://arxiv.org/html/2410.06992v2">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Large Language Models (LLMs) in Software Engineering (SE) can offer assistance for coding. To facilitate a rigorous evaluation of LLMs in practical coding contexts, Carlos et al. introduced the SWE-bench dataset, which comprises 2,294 real-world GitHub issues and their corresponding pull requests, collected from 12 widely used Python repositories. Several impressive LLM-based toolkits recently are developed and evaluated on this dataset. However, a systematic evaluation of the quality of SWE-bench remains missing. In this paper, we addressed this gap by presenting an empirical analysis of the SWE-bench dataset. We conducted a manual screening of instances where SWEAgent + GPT-4 successfully resolved issues by comparing the model-generated patches with the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench leaderboard during the time of our study. Our analysis reveals some critical issues with the SWE-bench dataset: 1) 32.67% of the successful patches involve cheating as the solutions were directly provided in the issue report or the comments. We refer to as solution leakage problem. 2) 31.08% of the passed patches are suspicious patches due to weak test cases, i.e., the tests were not adequate to verify the correctness of a patch. When we filtered out these problematic issues, the resolution rate of SWE-Agent+GPT-4 dropped from 12.47% to 3.97%. We also observed that the same data quality issues also exist in the two variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In addition, over 94% of the issues were created before LLM's knowledge cutoff dates, posing potential data leakage issues.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Song Wang [<a href="https://arxiv.org/show-email/8151c019/2410.06992" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2410.06992v1" rel="nofollow">[v1]</a></strong>
        Wed, 9 Oct 2024 15:38:53 UTC (3,863 KB)<br>
    <strong>[v2]</strong>
        Thu, 10 Oct 2024 13:13:09 UTC (1,714 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Ruby on Rails still matters (417 pts)]]></title>
            <link>https://www.contraption.co/rails-versus-nextjs/</link>
            <guid>43130546</guid>
            <pubDate>Fri, 21 Feb 2025 17:46:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.contraption.co/rails-versus-nextjs/">https://www.contraption.co/rails-versus-nextjs/</a>, See on <a href="https://news.ycombinator.com/item?id=43130546">Hacker News</a></p>
Couldn't get https://www.contraption.co/rails-versus-nextjs/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The Profitable Startup (141 pts)]]></title>
            <link>https://linear.app/blog/the-profitable-startup</link>
            <guid>43130480</guid>
            <pubDate>Fri, 21 Feb 2025 17:41:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://linear.app/blog/the-profitable-startup">https://linear.app/blog/the-profitable-startup</a>, See on <a href="https://news.ycombinator.com/item?id=43130480">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>For years, startups have been taught to prioritize growth over everything else. Profitability was seen as unambitious or even wrong – something to worry about when you hit scale. Why focus on profits when money and valuations were easy to come by?</p><p>But that thinking was always flawed.</p><p>Profitability isn't unambitious; it's controlling your own destiny. It means you don't have to rely on investors for survival. It means you can focus on your unaltered vision and mission. And it means you as a founder decide the pace of growth. And once you experience it, it's hard to imagine doing things any other way.</p><p><a href="https://paulgraham.com/ramenprofitable.html">Paul Graham famously wrote about "ramen profitability"</a> – the point where a founding team could survive without external funding. He argued this made startups more attractive to investors, showing they could get customers to pay, were serious about building valuable products, and were disciplined with expenses.</p><p>Graham wrote his essay in 2009. I’d argue that we now live in a world where it’s not just easier to get ramen profitable, but traditionally profitable – while also growing fast.</p><p>At Linear we didn't set out to be profitable but kind of stumbled into it. We believed that to win this market we really needed to build a superior tool. The best way we knew how to do that was to keep the team small and focused. And when we launched after a year in private beta, almost all of our 100 beta users converted to paid customers. To our surprise, we realized it wouldn't take that long to become profitable if we kept the costs in check. Twelve months after launch, we hit profitability, and we've stayed profitable ever since.</p><p>I don't know why hiring massive teams ever became the norm. In my own experience, small teams always delivered better quality, and faster. Maybe it's fear of missing out if you don't grow the team fast. Maybe it's investors whispering that your team is "understaffed compared to benchmarks." Being understaffed compared to benchmarks almost always should be a source of pride, not a problem. People should be surprised how small your team is, not how big it is.</p><p>What holds you back is rarely team size – it's the clarity of your focus, skill and ability to execute. Larger teams mean slower progress, more management overhead, more meetings, more opinions, and usually dilution of vision and standards. Yet growing the team has somehow become a symbol of success.</p><p>At Linear, we hired our first employee after six months and roughly doubled the team each year. With each hire, we make sure they truly elevate the team. We don't set out to hire ten engineers – we hire the next <em>great</em> engineer. This intentional approach has allowed us to maintain both quality and culture.</p><p>The most underrated thing about profitability is how much peace of mind it gives you. Once you're profitable, you stop worrying about survival and focus on what really matters: building something great. Building the way you want. Instead of optimizing for the next fundraising round, you optimize for value creation.</p><p>While profitability might not come quickly for every startup, I believe it's achievable sooner than most think. If you're creating a new market, or truly require massive scale like a social network, or significant upfront investment like a hardware company, it might take longer. But if you're in a category where there isn't hard upfront investment, and you get some level of product-market fit with customers willing to pay, you can probably be profitable. You can decide to become profitable. And usually, it's a decision about how much and how fast you hire.</p><p><strong>Measure What Matters</strong></p><p>Revenue per employee is one of the clearest ways to see you’re hiring appropriately. While some of the best public companies benchmark at $1-2M per employee, for startups it's not unreasonable to target the range of $500k-$1M per employee.</p><p><strong>Understand Your Risk Profile</strong></p><p>Are you building something highly speculative where you're not sure if there's a market for it, or are you building something that already has a market but with a different take on it? In the former case profitability takes longer, but in the latter it could happen right away. Most software today, especially in the B2B space, is about building a modern version of something existing.</p><p><strong>Hire Intentionally and Slower</strong></p><p>For most software startups, ten people before product-market fit should be your ceiling, not your target. After PMF, every hire should address a specific, pressing need – not just fill out an org chart. At Linear, our deliberately slow headcount growth forced us to be selective, which meant making better hires. It also protected our culture, since rapid hiring often dilutes the very things that made your startup special in the first place. When you hire less, you naturally hire better.</p><p><strong>Raise on Your Own Terms </strong></p><p>Being profitable doesn't mean you have to be anti-investors. It means you have that choice, and investors are quite interested in profitable companies that also grow fast. You can raise more, less, or nothing. You can wait for the right timing, the right partner, or fund. For most ambitious startups, it can still be a good idea to raise something even if you could get by bootstrapping. Investors can still be helpful, and the additional cash balance can help you to make larger investments, or acquisitions.</p><p>The point is that you can be and are allowed to be profitable as a startup. It's not a bad thing, it's not an oxymoron or as hard as people make it out to be. The secret is that a lot of successful companies actually were quite profitable early on, they just didn't talk about it. When you're profitable, you make decisions based on what's best for your customers and your product, not what's best for impressing investors.</p><p>I didn't set out to build a profitable startup. But once I got there, I realized I wouldn't want to build a company any other way.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bybit CEO Confirms Exchange Was Hacked for $1.46B (253 pts)]]></title>
            <link>https://www.tradingview.com/news/coindesk:cda1c390e094b:0-bybit-ceo-confirms-exchange-was-hacked-for-1-46b-says-his-firm-can-cover-the-loss/</link>
            <guid>43130143</guid>
            <pubDate>Fri, 21 Feb 2025 17:15:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tradingview.com/news/coindesk:cda1c390e094b:0-bybit-ceo-confirms-exchange-was-hacked-for-1-46b-says-his-firm-can-cover-the-loss/">https://www.tradingview.com/news/coindesk:cda1c390e094b:0-bybit-ceo-confirms-exchange-was-hacked-for-1-46b-says-his-firm-can-cover-the-loss/</a>, See on <a href="https://news.ycombinator.com/item?id=43130143">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-name="news-story-content"><article><p><span><time datetime="Fri, 21 Feb 2025 15:34:03 GMT"><span>Feb 21, 2025</span><span>15:34<!-- --> <!-- -->UTC</span></time></span></p><p><span><p>Cryptocurrency exchange Bybit has experienced $1.46 billion worth of "suspicious outflows," according to blockchain sleuth ZachXBT.</p><p>The wallet in question appears to have sent 401,346 ETH ($1.1 billion) as well as several other iterations of staked ether (stETH) to a fresh wallet, which is now liquidating mETH and stETH on decentralized exchanges, etherscan shows. The wallet has sold around $200 million worth of stETH so far.</p><p>Bybit CEO Ben Zhou wrote on X that a hacker "took control of the specific ETH cold wallet and transferred all the ETH in the cold wallet to this unidentified address."</p><p>"Please rest assured that all other cold wallets are secure. All withdrawals are normal," he added.</p><p>"My sources confirm it's a security incident," ZachXBT added on Telegram.</p><p>$1.46 billion would equate to the largest cryptocurrency hack of all time in dollar terms, with $470 million being lost in the Mt Gox Hack, $530 million in the 2018 hack of CoinCheck, and $650 million in the Ronin Bridge exploit.</p><p>BTC and ETH dropped more than 1.5% and 2%, respectively, following the transfers.</p><p>UPDATE (15:44 UTC, Feb. 21): Adds quote from Bybit CEO and details of historical crypto hacks.</p></span></p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepDive in everything of Llama3: revealing detailed insights and implementation (189 pts)]]></title>
            <link>https://github.com/therealoliver/Deepdive-llama3-from-scratch</link>
            <guid>43129887</guid>
            <pubDate>Fri, 21 Feb 2025 16:57:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/therealoliver/Deepdive-llama3-from-scratch">https://github.com/therealoliver/Deepdive-llama3-from-scratch</a>, See on <a href="https://news.ycombinator.com/item?id=43129887">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/logo.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/logo.png" width="600px"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deepdive-llama3-from-scratch</h2><a id="user-content-deepdive-llama3-from-scratch" aria-label="Permalink: Deepdive-llama3-from-scratch" href="#deepdive-llama3-from-scratch"></a></p>
<p dir="auto">
    <a href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/8714a45b348376ef75d52e9adcc597d84fc855efd11b2e5eca5b2b5cb52a99fe/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7468657265616c6f6c697665722f44656570646976652d6c6c616d61332d66726f6d2d73637261746368" alt="License" data-canonical-src="https://img.shields.io/github/license/therealoliver/Deepdive-llama3-from-scratch"></a>
    <a href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/stargazers"><img src="https://camo.githubusercontent.com/12e1dc75bd78c858f8d5f6986ed939b9c9a8403e4f4d528da7d0759d3a8ae25d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7468657265616c6f6c697665722f44656570646976652d6c6c616d61332d66726f6d2d73637261746368" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/therealoliver/Deepdive-llama3-from-scratch"></a>
    <a href="#from_me"><img src="https://camo.githubusercontent.com/32783fcfbbf44f49658a770f3403e9a13329d7c170898e02ba0e5d4baf85aaac/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe298952532304275792532306d6525323061253230636f666665652d666636396234" alt="Buy me a coffee" data-canonical-src="https://img.shields.io/badge/☕%20Buy%20me%20a%20coffee-ff69b4"></a>
</p>

<hr>
<div dir="auto"><p>This project is an enhanced version based on <a href="https://github.com/naklecha/llama3-from-scratch">naklecha/llama3-from-scratch</a>. It has been comprehensively improved and optimized on the basis of the original project, aiming to help everyone more easily understand and master the implementation principle and the detailed reasoning process of the Llama3 model. Thanks to the contributions of the original author :)
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">
The following are the core improvements of this project:
</h3><a id="user-content-the-following-are-the-core-improvements-of-this-project" aria-label="Permalink: 
The following are the core improvements of this project:
" href="#the-following-are-the-core-improvements-of-this-project"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Structural Optimization</strong><br>
The presentation sequence of the content has been rearranged, and the directory structure has been adjusted to make the learning process clearer and more reasonable, facilitating everyone to understand the code step by step.</p>
</li>
<li>
<p dir="auto"><strong>Code Annotations</strong><br>
A large number of detailed code annotations have been added to teach you how to understand the function of each piece of code. Even beginners can get started easily.</p>
</li>
<li>
<p dir="auto"><strong>Dimension Tracking</strong><br>
The changes in the matrix dimensions in each step of the calculation are fully annotated, making it easier for you to understand the entire process.</p>
</li>
<li>
<p dir="auto"><strong>Principle Explanation</strong><br>
Abundant principle-related explanations and a large number of detailed derivations have been added. It not only tells you "what to do" but also deeply explains "why to do it", helping you fundamentally master the design concept of the model.</p>
</li>
<li>
<p dir="auto"><strong>KV-Cache Insights</strong><br>
An additional derivation chapter on KV-Cache has been added, covering detailed core concepts, principle derivations, and the application process in the attention mechanism, allowing you to understand every detail and philosophy of KV-Cache from its roots.</p>
</li>
<li>
<div dir="auto"><p><strong>Bilingual Documents</strong><br>
Code files in both Chinese and English are provided. The native Chinese translation avoids the problem of inaccurate expressions caused by machine translation.
</p></div>
</li>
</ol>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#loading-the-model">Loading the model</a>
<ul dir="auto">
<li><a href="#loading-the-tokenizer">Loading the tokenizer</a></li>
<li><a href="#reading-model-files-and-configuration-files">Reading model files and configuration files</a>
<ul dir="auto">
<li><a href="#inferring-model-details-using-the-configuration-file">Inferring model details using the configuration file</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#convert-the-input-text-into-embeddings">Convert the input text into embeddings</a>
<ul dir="auto">
<li><a href="#convert-the-text-into-a-sequence-of-token-ids">Convert the text into a sequence of token ids</a></li>
<li><a href="#convert-the-sequence-of-token-ids-into-embeddings">Convert the sequence of token ids into embeddings</a></li>
</ul>
</li>
<li><a href="#build-the-first-transformer-block">Build the first Transformer block</a>
<ul dir="auto">
<li><a href="#normalization">Normalization</a>
<ul dir="auto">
<li><a href="#using-rms-normalization-for-embeddings">Using RMS normalization for embeddings</a></li>
</ul>
</li>
<li><a href="#implementing-the-single-head-attention-mechanism-from-scratch">Implementing the single-head attention mechanism from scratch</a>
<ul dir="auto">
<li><a href="#obtain-the-qkv-vectors-corresponding-to-the-input-tokens">Obtain the QKV vectors corresponding to the input tokens</a>
<ul dir="auto">
<li><a href="#obtain-the-query-vector">Obtain the query vector</a>
<ul dir="auto">
<li><a href="#unfold-the-query-weight-matrix">Unfold the query weight matrix</a></li>
<li><a href="#obtain-the-first-head">Obtain the first head</a></li>
<li><a href="#multiply-the-token-embeddings-by-the-query-weights-to-obtain-the-query-vectors-corresponding-to-the-tokens">Multiply the token embeddings by the query weights to obtain the query vectors corresponding to the tokens</a></li>
</ul>
</li>
<li><a href="#obtain-the-key-vector-almost-the-same-as-the-query-vector">Obtain the key vector (almost the same as the query vector)</a></li>
<li><a href="#obtain-the-value-vector-almost-the-same-as-the-key-vector">Obtain the value vector (almost the same as the key vector)</a></li>
</ul>
</li>
<li><a href="#add-positional-information-to-the-query-and-key-vectors">Add positional information to the query and key vectors</a>
<ul dir="auto">
<li><a href="#rotary-position-encoding-rope">Rotary Position Encoding (RoPE)</a></li>
<li><a href="#add-positional-information-to-the-query-vectors">Add positional information to the query vectors</a></li>
<li><a href="#add-positional-information-to-the-key-vectors-same-as-the-query">Add positional information to the key vectors (same as the query)</a></li>
</ul>
</li>
<li><a href="#everythings-ready-lets-start-calculating-the-attention-weights-between-tokens">Everything's ready. Let's start calculating the attention weights between tokens.</a>
<ul dir="auto">
<li><a href="#multiply-the-query-and-key-vectors-to-obtain-the-attention-scores">Multiply the query and key vectors to obtain the attention scores.</a></li>
<li><a href="#now-we-must-mask-the-future-query-key-scores">Now we must mask the future query-key scores.</a></li>
<li><a href="#calculate-the-final-attention-weights-that-is-softmaxscore">Calculate the final attention weights, that is, softmax(score).</a></li>
</ul>
</li>
<li><a href="#finally-calculate-the-final-result-of-the-single-head-attention-mechanism">Finally! Calculate the final result of the single-head attention mechanism!</a></li>
</ul>
</li>
<li><a href="#calculate-the-multi-head-attention-mechanism-a-simple-loop-to-repeat-the-above-process">Calculate the multi-head attention mechanism (a simple loop to repeat the above process)</a>
<ul dir="auto">
<li><a href="#calculate-the-result-for-each-head">Calculate the result for each head</a></li>
<li><a href="#merge-the-results-of-each-head-into-a-large-matrix">Merge the results of each head into a large matrix</a></li>
<li><a href="#head-to-head-information-interaction-linear-mapping-the-final-step-of-the-self-attention-layer">Head-to-head information interaction (linear mapping), the final step of the self-attention layer!</a></li>
</ul>
</li>
<li><a href="#perform-the-residual-operation-add">Perform the residual operation (add)</a></li>
<li><a href="#perform-the-second-normalization-operation">Perform the second normalization operation</a></li>
<li><a href="#perform-the-calculation-of-the-ffn-feed-forward-neural-network-layer">Perform the calculation of the FFN (Feed-Forward Neural Network) layer</a></li>
<li><a href="#perform-the-residual-operation-again-finally-we-get-the-final-output-of-the-transformer-block">Perform the residual operation again (Finally, we get the final output of the Transformer block!)</a></li>
</ul>
</li>
<li><a href="#everything-is-here-lets-complete-the-calculation-of-all-32-transformer-blocks-happy-reading-">Everything is here. Let's complete the calculation of all 32 Transformer blocks. Happy reading :)</a></li>
<li><a href="#lets-complete-the-last-step-and-predict-the-next-token">Let's complete the last step and predict the next token</a>
<ul dir="auto">
<li><a href="#first-perform-one-last-normalization-on-the-output-of-the-last-transformer-layer">First, perform one last normalization on the output of the last Transformer layer</a></li>
<li><a href="#then-make-the-prediction-based-on-the-embedding-corresponding-to-the-last-token-perform-a-linear-mapping-to-the-vocabulary-dimension">Then, make the prediction based on the embedding corresponding to the last token (perform a linear mapping to the vocabulary dimension)</a></li>
<li><a href="#heres-the-prediction-result">Here's the prediction result!</a></li>
</ul>
</li>
<li><a href="#lets-dive-deeper-and-see-how-different-embeddings-or-token-masking-strategies-might-affect-the-prediction-results-">Let's dive deeper and see how different embeddings or token masking strategies might affect the prediction results :)</a></li>
<li><a href="#need-to-predict-multiple-tokens-just-using-kv-cache-it-really-took-me-a-lot-of-effort-to-sort-this-out-orz">Need to predict multiple tokens? Just using KV-Cache! (It really took me a lot of effort to sort this out. Orz)</a></li>
<li><a href="#thank-you-all-thanks-for-your-continuous-learning-love-you-all-">Thank you all. Thanks for your continuous learning. Love you all :)</a>
<ul dir="auto">
<li><a href="#from-me">From Me</a></li>
<li><a href="#from-the-author-of-predecessor-project">From the author of predecessor project</a></li>
</ul>
</li>
<li><a href="#license">LICENSE</a></li>
</ul>
<hr>
<p dir="auto"><h3 tabindex="-1" dir="auto">
Now, let's start the formal learning process!
</h3><a id="user-content-now-lets-start-the-formal-learning-process" aria-label="Permalink: 
Now, let's start the formal learning process!
" href="#now-lets-start-the-formal-learning-process"></a></p>
<br>
<div dir="auto"><p>In this file, I implemented Llama3 from scratch, one tensor and matrix multiplication at a time.
<br>
Also, I'm going to load tensors directly from the model file that meta provided for Llama3 (Meta-Llama-3-8B), you need to download the weights before running this file. Here is the offical link to download the weights: <a href="https://llama.meta.com/llama-downloads/" rel="nofollow">https://llama.meta.com/llama-downloads/</a>
<br>
Note: This project uses the original model files, that is, the models in the "original" folder of the downloaded model files.
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">
    Please Note! There is a small mistake in the figure:<br>
    </h3><a id="user-content-----please-note-there-is-a-small-mistake-in-the-figure----" aria-label="Permalink: 
    Please Note! There is a small mistake in the figure:" href="#----please-note-there-is-a-small-mistake-in-the-figure----"></a></p><p dir="auto"><h4 tabindex="-1" dir="auto">
        In each Transformer block, the input of the second "add" operation should be the output of the feed-forward layer and the output of the first "add" operation, instead of the result after normalization.
        <br>
        If we consider multi-head self-attention and feed-forward as the same type of operations (both for feature transformation), then the forms and processes of the two "normalization - feature transformation - residual connection (add)" are exactly the same.
    </h4><a id="user-content---------in-each-transformer-block-the-input-of-the-second-add-operation-should-be-the-output-of-the-feed-forward-layer-and-the-output-of-the-first-add-operation-instead-of-the-result-after-normalization----------------if-we-consider-multi-head-self-attention-and-feed-forward-as-the-same-type-of-operations-both-for-feature-transformation-then-the-forms-and-processes-of-the-two-normalization---feature-transformation---residual-connection-add-are-exactly-the-same----" aria-label="Permalink: 
        In each Transformer block, the input of the second &quot;add&quot; operation should be the output of the feed-forward layer and the output of the first &quot;add&quot; operation, instead of the result after normalization.
        
        If we consider multi-head self-attention and feed-forward as the same type of operations (both for feature transformation), then the forms and processes of the two &quot;normalization - feature transformation - residual connection (add)&quot; are exactly the same.
    " href="#--------in-each-transformer-block-the-input-of-the-second-add-operation-should-be-the-output-of-the-feed-forward-layer-and-the-output-of-the-first-add-operation-instead-of-the-result-after-normalization----------------if-we-consider-multi-head-self-attention-and-feed-forward-as-the-same-type-of-operations-both-for-feature-transformation-then-the-forms-and-processes-of-the-two-normalization---feature-transformation---residual-connection-add-are-exactly-the-same----"></a></p>

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/archi.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/archi.png"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Loading the model</h2><a id="user-content-loading-the-model" aria-label="Permalink: Loading the model" href="#loading-the-model"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Loading the tokenizer</h2><a id="user-content-loading-the-tokenizer" aria-label="Permalink: Loading the tokenizer" href="#loading-the-tokenizer"></a></p>
<p dir="auto">The tokenizer is used to split the input text string into a sequence of sub-words, making it easier to input to the model.
<br>
I'm not going to implement a bpe tokenizer (but andrej karpathy has a really clean implementation),
link to his implementation: <a href="https://github.com/karpathy/minbpe">https://github.com/karpathy/minbpe</a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/karpathyminbpe.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/karpathyminbpe.png" width="600"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Summary of the steps to load the BPE-based tokenizer:</h3><a id="user-content-summary-of-the-steps-to-load-the-bpe-based-tokenizer" aria-label="Permalink: Summary of the steps to load the BPE-based tokenizer:" href="#summary-of-the-steps-to-load-the-bpe-based-tokenizer"></a></p>
<ol dir="auto">
<li>Loading regular words: Load the local tokenizer model dictionary (which only contains regular subwords and no special tokens).</li>
<li>Definition of the special words: Manually define special tokens (using ready-made ones or modifying based on the ready-made ones).</li>
<li>Definition of the text rough-splitting rule: Define the regular expression for text rough-splitting (just using a ready-made one). The input will go through two steps of rough-splitting (based on the regular expression) and fine-splitting (based on BPE) to obtain the final tokenization result.</li>
<li>Create tokenizer: Create a text encoder-decoder object based on the open-sourced tiktoken library by OpenAI (which can further split the rough-splitting result based on the BPE algorithm).</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Loading the BPE-based Tokenizer

# Import related libraries
from pathlib import Path  # Used to obtain the file name/model name from the file path
import tiktoken  # An open-source library developed by OpenAI for text encoding and decoding (mutual conversion between text and token IDs)
from tiktoken.load import load_tiktoken_bpe  # Load the BPE model
import torch  # Used for building models and matrix calculations
import json  # Used for loading configuration files
import matplotlib.pyplot as plt  # Used for plotting graphs


tokenizer_path = &quot;Meta-Llama-3-8B/original/tokenizer.model&quot;  # Path to the tokenizer model

# Special tokens outside the regular dictionary.
# These special tokens are present in the 'added_tokens' field of both 'tokenizer.json' and 'tokenizer_config.json' in the &quot;Meta-Llama-3-8B/&quot; path
special_tokens = [
            &quot;<|begin_of_text|>&quot;,
            &quot;<|end_of_text|>&quot;,
            &quot;<|reserved_special_token_0|>&quot;,  # Reserved special tokens from 0 to 250
            &quot;<|reserved_special_token_1|>&quot;,
            &quot;<|reserved_special_token_2|>&quot;,
            &quot;<|reserved_special_token_3|>&quot;,
            &quot;<|start_header_id|>&quot;,  # Start of header information, used to mark the header information that wraps structured data, such as metadata
            &quot;<|end_header_id|>&quot;,  # End of header information
            &quot;<|reserved_special_token_4|>&quot;,
            &quot;<|eot_id|>&quot;,  # end of turn, used to mark the end of the current turn in multi-turn conversations
        ] + [f&quot;<|reserved_special_token_{i}|>&quot; for i in range(5, 256 - 5)]


# Load the BPE model (actually a dictionary)
# A dictionary of subword(bytes type, decoded with utf-8)-rank(id) pairs, with 128000 words, not including the 256 special tokens above,
# so the total size of the model's dictionary will be 128256 in after operation (but not here)
# The rank values are an increasing sequence starting from 0, used to determine the priority order of subword unit merging,
# the higher the priority, the earlier the merging. Therefore, the variable name here is &quot;mergeable_ranks&quot; instead of something like BPE or word dictionary
# The special tokens are not added to the dictionary probably for flexibility,
# making it easy to add specific tokens when facing different model architectures or tasks with different special tokens, and keeping the dictionary size unchanged
mergeable_ranks = load_tiktoken_bpe(tokenizer_path)


# Create a text encoder-decoder object
# The pat_str is roughly divided into three types: words with abbreviations &amp; words, Chinese segments, 1-3-digit numbers &amp; other special characters
tokenizer = tiktoken.Encoding(
    name=Path(tokenizer_path).name,  # Name of the encoder, which is convenient when debugging and logging to use different encoders
    pat_str=r&quot;(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&quot;,  # Regular expression for initially roughly splitting the text into a token sequence
    mergeable_ranks=mergeable_ranks,  # Pass in the loaded BPE model
    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},  # Dictionary for adding special token-id pairs
)


# Test whether the creation is successful, that is, whether the encoder-decoder can run correctly
print(tokenizer.decode(tokenizer.encode(&quot;create tokenizer successed!&quot;)))


# The following is a case test to test the effects and differences between the rough splitting of pat_str and the fine splitting of the tokenizer
# The regular expression of pat_str only provides a preliminary splitting,
# some long sentences or Chinese text will not be split and will be further refined based on the BPE algorithm in the tokenizer
import regex  # Since some Unicode syntax such as \p{L} is used in pat_str, the re library cannot be used

## Create a regular expression
pat_str=r&quot;(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&quot;
pattern = regex.compile(pat_str)

## Text segmentation
text = &quot;Hello world! It's a test. 这是一个测试. alongwords. a long words. 123 456 789.&quot;  # testing string
re_tokens = pattern.findall(text)  # Split the string using the regular expression
merge_tokens_id = tokenizer.encode(text)  # Split the string using the tokenizer
merge_tokens = [tokenizer.decode([i]) for i in merge_tokens_id]  # Convert the id sequence of the tokenizer's splitting result into an actual subword sequence

## Output result
print(&quot;Original string: &quot;, text)
print(&quot;Regular expression splitting result: &quot;, re_tokens)
print(&quot;Tokenizer splitting result: &quot;, merge_tokens)
print(&quot;Tokenizer splitting result ids: &quot;, list(zip(merge_tokens, merge_tokens_id)))

## From the results, it can be seen that the leading spaces of all words are retained, rather than being a single space token or being deleted.
## This is beneficial for the model to correctly understand the boundary information between words, such as 'alongwords' in the example."><pre><span># Loading the BPE-based Tokenizer</span>

<span># Import related libraries</span>
<span>from</span> <span>pathlib</span> <span>import</span> <span>Path</span>  <span># Used to obtain the file name/model name from the file path</span>
<span>import</span> <span>tiktoken</span>  <span># An open-source library developed by OpenAI for text encoding and decoding (mutual conversion between text and token IDs)</span>
<span>from</span> <span>tiktoken</span>.<span>load</span> <span>import</span> <span>load_tiktoken_bpe</span>  <span># Load the BPE model</span>
<span>import</span> <span>torch</span>  <span># Used for building models and matrix calculations</span>
<span>import</span> <span>json</span>  <span># Used for loading configuration files</span>
<span>import</span> <span>matplotlib</span>.<span>pyplot</span> <span>as</span> <span>plt</span>  <span># Used for plotting graphs</span>


<span>tokenizer_path</span> <span>=</span> <span>"Meta-Llama-3-8B/original/tokenizer.model"</span>  <span># Path to the tokenizer model</span>

<span># Special tokens outside the regular dictionary.</span>
<span># These special tokens are present in the 'added_tokens' field of both 'tokenizer.json' and 'tokenizer_config.json' in the "Meta-Llama-3-8B/" path</span>
<span>special_tokens</span> <span>=</span> [
            <span>"&lt;|begin_of_text|&gt;"</span>,
            <span>"&lt;|end_of_text|&gt;"</span>,
            <span>"&lt;|reserved_special_token_0|&gt;"</span>,  <span># Reserved special tokens from 0 to 250</span>
            <span>"&lt;|reserved_special_token_1|&gt;"</span>,
            <span>"&lt;|reserved_special_token_2|&gt;"</span>,
            <span>"&lt;|reserved_special_token_3|&gt;"</span>,
            <span>"&lt;|start_header_id|&gt;"</span>,  <span># Start of header information, used to mark the header information that wraps structured data, such as metadata</span>
            <span>"&lt;|end_header_id|&gt;"</span>,  <span># End of header information</span>
            <span>"&lt;|reserved_special_token_4|&gt;"</span>,
            <span>"&lt;|eot_id|&gt;"</span>,  <span># end of turn, used to mark the end of the current turn in multi-turn conversations</span>
        ] <span>+</span> [<span>f"&lt;|reserved_special_token_<span><span>{</span><span>i</span><span>}</span></span>|&gt;"</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>5</span>, <span>256</span> <span>-</span> <span>5</span>)]


<span># Load the BPE model (actually a dictionary)</span>
<span># A dictionary of subword(bytes type, decoded with utf-8)-rank(id) pairs, with 128000 words, not including the 256 special tokens above,</span>
<span># so the total size of the model's dictionary will be 128256 in after operation (but not here)</span>
<span># The rank values are an increasing sequence starting from 0, used to determine the priority order of subword unit merging,</span>
<span># the higher the priority, the earlier the merging. Therefore, the variable name here is "mergeable_ranks" instead of something like BPE or word dictionary</span>
<span># The special tokens are not added to the dictionary probably for flexibility,</span>
<span># making it easy to add specific tokens when facing different model architectures or tasks with different special tokens, and keeping the dictionary size unchanged</span>
<span>mergeable_ranks</span> <span>=</span> <span>load_tiktoken_bpe</span>(<span>tokenizer_path</span>)


<span># Create a text encoder-decoder object</span>
<span># The pat_str is roughly divided into three types: words with abbreviations &amp; words, Chinese segments, 1-3-digit numbers &amp; other special characters</span>
<span>tokenizer</span> <span>=</span> <span>tiktoken</span>.<span>Encoding</span>(
    <span>name</span><span>=</span><span>Path</span>(<span>tokenizer_path</span>).<span>name</span>,  <span># Name of the encoder, which is convenient when debugging and logging to use different encoders</span>
    <span>pat_str</span><span>=</span><span>r"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+"</span>,  <span># Regular expression for initially roughly splitting the text into a token sequence</span>
    <span>mergeable_ranks</span><span>=</span><span>mergeable_ranks</span>,  <span># Pass in the loaded BPE model</span>
    <span>special_tokens</span><span>=</span>{<span>token</span>: <span>len</span>(<span>mergeable_ranks</span>) <span>+</span> <span>i</span> <span>for</span> <span>i</span>, <span>token</span> <span>in</span> <span>enumerate</span>(<span>special_tokens</span>)},  <span># Dictionary for adding special token-id pairs</span>
)


<span># Test whether the creation is successful, that is, whether the encoder-decoder can run correctly</span>
<span>print</span>(<span>tokenizer</span>.<span>decode</span>(<span>tokenizer</span>.<span>encode</span>(<span>"create tokenizer successed!"</span>)))


<span># The following is a case test to test the effects and differences between the rough splitting of pat_str and the fine splitting of the tokenizer</span>
<span># The regular expression of pat_str only provides a preliminary splitting,</span>
<span># some long sentences or Chinese text will not be split and will be further refined based on the BPE algorithm in the tokenizer</span>
<span>import</span> <span>regex</span>  <span># Since some Unicode syntax such as \p{L} is used in pat_str, the re library cannot be used</span>

<span>## Create a regular expression</span>
<span>pat_str</span><span>=</span><span>r"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+"</span>
<span>pattern</span> <span>=</span> <span>regex</span>.<span>compile</span>(<span>pat_str</span>)

<span>## Text segmentation</span>
<span>text</span> <span>=</span> <span>"Hello world! It's a test. 这是一个测试. alongwords. a long words. 123 456 789."</span>  <span># testing string</span>
<span>re_tokens</span> <span>=</span> <span>pattern</span>.<span>findall</span>(<span>text</span>)  <span># Split the string using the regular expression</span>
<span>merge_tokens_id</span> <span>=</span> <span>tokenizer</span>.<span>encode</span>(<span>text</span>)  <span># Split the string using the tokenizer</span>
<span>merge_tokens</span> <span>=</span> [<span>tokenizer</span>.<span>decode</span>([<span>i</span>]) <span>for</span> <span>i</span> <span>in</span> <span>merge_tokens_id</span>]  <span># Convert the id sequence of the tokenizer's splitting result into an actual subword sequence</span>

<span>## Output result</span>
<span>print</span>(<span>"Original string: "</span>, <span>text</span>)
<span>print</span>(<span>"Regular expression splitting result: "</span>, <span>re_tokens</span>)
<span>print</span>(<span>"Tokenizer splitting result: "</span>, <span>merge_tokens</span>)
<span>print</span>(<span>"Tokenizer splitting result ids: "</span>, <span>list</span>(<span>zip</span>(<span>merge_tokens</span>, <span>merge_tokens_id</span>)))

<span>## From the results, it can be seen that the leading spaces of all words are retained, rather than being a single space token or being deleted.</span>
<span>## This is beneficial for the model to correctly understand the boundary information between words, such as 'alongwords' in the example.</span></pre></div>
<div data-snippet-clipboard-copy-content="create tokenizer successed!
Original string:  Hello world! It's a test. 这是一个测试. alongwords. a long words. 123 456 789.
Regular expression splitting result:  ['Hello', ' world', '!', ' It', &quot;'s&quot;, ' a', ' test', '.', ' 这是一个测试', '.', ' alongwords', '.', ' a', ' long', ' words', '.', ' ', '123', ' ', '456', ' ', '789', '.']
Tokenizer splitting result:  ['Hello', ' world', '!', ' It', &quot;'s&quot;, ' a', ' test', '.', ' 这', '是一个', '测试', '.', ' along', 'words', '.', ' a', ' long', ' words', '.', ' ', '123', ' ', '456', ' ', '789', '.']
Tokenizer splitting result ids:  [('Hello', 9906), (' world', 1917), ('!', 0), (' It', 1102), (&quot;'s&quot;, 596), (' a', 264), (' test', 1296), ('.', 13), (' 这', 122255), ('是一个', 122503), ('测试', 82805), ('.', 13), (' along', 3235), ('words', 5880), ('.', 13), (' a', 264), (' long', 1317), (' words', 4339), ('.', 13), (' ', 220), ('123', 4513), (' ', 220), ('456', 10961), (' ', 220), ('789', 16474), ('.', 13)]"><pre><code>create tokenizer successed!
Original string:  Hello world! It's a test. 这是一个测试. alongwords. a long words. 123 456 789.
Regular expression splitting result:  ['Hello', ' world', '!', ' It', "'s", ' a', ' test', '.', ' 这是一个测试', '.', ' alongwords', '.', ' a', ' long', ' words', '.', ' ', '123', ' ', '456', ' ', '789', '.']
Tokenizer splitting result:  ['Hello', ' world', '!', ' It', "'s", ' a', ' test', '.', ' 这', '是一个', '测试', '.', ' along', 'words', '.', ' a', ' long', ' words', '.', ' ', '123', ' ', '456', ' ', '789', '.']
Tokenizer splitting result ids:  [('Hello', 9906), (' world', 1917), ('!', 0), (' It', 1102), ("'s", 596), (' a', 264), (' test', 1296), ('.', 13), (' 这', 122255), ('是一个', 122503), ('测试', 82805), ('.', 13), (' along', 3235), ('words', 5880), ('.', 13), (' a', 264), (' long', 1317), (' words', 4339), ('.', 13), (' ', 220), ('123', 4513), (' ', 220), ('456', 10961), (' ', 220), ('789', 16474), ('.', 13)]
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Reading model files and configuration files</h2><a id="user-content-reading-model-files-and-configuration-files" aria-label="Permalink: Reading model files and configuration files" href="#reading-model-files-and-configuration-files"></a></p>
<p dir="auto">Generally, reading a model file depends on how its model class is written and the variable names within it.
<br>
However, since we are implementing Llama3 from scratch, we will read one tensor file at a time.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/model.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/model.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Load the model, a dictionary such as {&quot;network-layer-name&quot;: tensor-type parameters}
model = torch.load(&quot;Meta-Llama-3-8B/original/consolidated.00.pth&quot;)

# Print the names of the first 20 network layers to verify if the model is loaded correctly.
print(json.dumps(list(model.keys())[:20], indent=4))"><pre><span># Load the model, a dictionary such as {"network-layer-name": tensor-type parameters}</span>
<span>model</span> <span>=</span> <span>torch</span>.<span>load</span>(<span>"Meta-Llama-3-8B/original/consolidated.00.pth"</span>)

<span># Print the names of the first 20 network layers to verify if the model is loaded correctly.</span>
<span>print</span>(<span>json</span>.<span>dumps</span>(<span>list</span>(<span>model</span>.<span>keys</span>())[:<span>20</span>], <span>indent</span><span>=</span><span>4</span>))</pre></div>
<div data-snippet-clipboard-copy-content="[
    &quot;tok_embeddings.weight&quot;,
    &quot;layers.0.attention.wq.weight&quot;,
    &quot;layers.0.attention.wk.weight&quot;,
    &quot;layers.0.attention.wv.weight&quot;,
    &quot;layers.0.attention.wo.weight&quot;,
    &quot;layers.0.feed_forward.w1.weight&quot;,
    &quot;layers.0.feed_forward.w3.weight&quot;,
    &quot;layers.0.feed_forward.w2.weight&quot;,
    &quot;layers.0.attention_norm.weight&quot;,
    &quot;layers.0.ffn_norm.weight&quot;,
    &quot;layers.1.attention.wq.weight&quot;,
    &quot;layers.1.attention.wk.weight&quot;,
    &quot;layers.1.attention.wv.weight&quot;,
    &quot;layers.1.attention.wo.weight&quot;,
    &quot;layers.1.feed_forward.w1.weight&quot;,
    &quot;layers.1.feed_forward.w3.weight&quot;,
    &quot;layers.1.feed_forward.w2.weight&quot;,
    &quot;layers.1.attention_norm.weight&quot;,
    &quot;layers.1.ffn_norm.weight&quot;,
    &quot;layers.2.attention.wq.weight&quot;
]"><pre><code>[
    "tok_embeddings.weight",
    "layers.0.attention.wq.weight",
    "layers.0.attention.wk.weight",
    "layers.0.attention.wv.weight",
    "layers.0.attention.wo.weight",
    "layers.0.feed_forward.w1.weight",
    "layers.0.feed_forward.w3.weight",
    "layers.0.feed_forward.w2.weight",
    "layers.0.attention_norm.weight",
    "layers.0.ffn_norm.weight",
    "layers.1.attention.wq.weight",
    "layers.1.attention.wk.weight",
    "layers.1.attention.wv.weight",
    "layers.1.attention.wo.weight",
    "layers.1.feed_forward.w1.weight",
    "layers.1.feed_forward.w3.weight",
    "layers.1.feed_forward.w2.weight",
    "layers.1.attention_norm.weight",
    "layers.1.ffn_norm.weight",
    "layers.2.attention.wq.weight"
]
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Load the configuration file.
# The specific meaning of each configuration is described in the next section.
with open(&quot;Meta-Llama-3-8B/original/params.json&quot;, &quot;r&quot;) as f:
    config = json.load(f)
config"><pre><span># Load the configuration file.</span>
<span># The specific meaning of each configuration is described in the next section.</span>
<span>with</span> <span>open</span>(<span>"Meta-Llama-3-8B/original/params.json"</span>, <span>"r"</span>) <span>as</span> <span>f</span>:
    <span>config</span> <span>=</span> <span>json</span>.<span>load</span>(<span>f</span>)
<span>config</span></pre></div>
<div data-snippet-clipboard-copy-content="{'dim': 4096,
 'n_layers': 32,
 'n_heads': 32,
 'n_kv_heads': 8,
 'vocab_size': 128256,
 'multiple_of': 1024,
 'ffn_dim_multiplier': 1.3,
 'norm_eps': 1e-05,
 'rope_theta': 500000.0}"><pre><code>{'dim': 4096,
 'n_layers': 32,
 'n_heads': 32,
 'n_kv_heads': 8,
 'vocab_size': 128256,
 'multiple_of': 1024,
 'ffn_dim_multiplier': 1.3,
 'norm_eps': 1e-05,
 'rope_theta': 500000.0}
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Inferring model details using the configuration file</h3><a id="user-content-inferring-model-details-using-the-configuration-file" aria-label="Permalink: Inferring model details using the configuration file" href="#inferring-model-details-using-the-configuration-file"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Configuration Item</th>
<th>Configuration Value</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>dim</td>
<td>4096</td>
<td>Dimension of the hidden layer, i.e., the vector representation of each token has a dimension of 4096.</td>
</tr>
<tr>
<td>n_layers</td>
<td>32</td>
<td>Number of model layers, i.e., the model has 32 Transformer layers or say Transformer blocks.</td>
</tr>
<tr>
<td>n_heads</td>
<td>32</td>
<td>Number of heads in multi-head attention, i.e., each multi-head attention block has 32 heads. The so-called multi-head means that multiple independent attention mechanisms are used simultaneously to capture different features or information of the input data.</td>
</tr>
<tr>
<td>n_kv_heads</td>
<td>8</td>
<td>Number of heads in key-value attention, used for Grouped Query Attention (GQA). That is, the key-value attention has 8 heads, while the query has n_heads=32 heads. Every 4 query heads will share a set of key-value pairs.</td>
</tr>
<tr>
<td>vocab_size</td>
<td>128256</td>
<td>Size of the vocabulary, including 128000 ordinary tokens and 256 special tokens.</td>
</tr>
<tr>
<td>multiple_of</td>
<td>1024</td>
<td>Multiple constraint on the dimension of the hidden layer. That is, the dimension of the model's hidden layer should be a multiple of 1024 to optimize computational efficiency.</td>
</tr>
<tr>
<td>ffn_dim_multiplier</td>
<td>1.3</td>
<td>Multiplier for the hidden layer dimension of the feed-forward network layer, used to calculate the hidden layer dimension of the FFN. The calculation process can be seen in the corresponding section.</td>
</tr>
<tr>
<td>norm_eps</td>
<td>1e-05</td>
<td>Constant added to the denominator in layer normalization calculation to prevent division by zero and ensure numerical stability.</td>
</tr>
<tr>
<td>rope_theta</td>
<td>500000.0</td>
<td>Basic frequency scaling factor in Rotary Position Encoding (RoPE), which controls the periodicity and resolution of position encoding, thus affecting the model's ability to capture sequences of different lengths and positional relationships.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto"><h3 tabindex="-1" dir="auto">
Based on the configuration details, the internal calculation process of attention given an input can be inferred as follows:
</h3><a id="user-content-based-on-the-configuration-details-the-internal-calculation-process-of-attention-given-an-input-can-be-inferred-as-follows" aria-label="Permalink: 
Based on the configuration details, the internal calculation process of attention given an input can be inferred as follows:
" href="#based-on-the-configuration-details-the-internal-calculation-process-of-attention-given-an-input-can-be-inferred-as-follows"></a></p>
<pre>input(L, 4096) -&gt; query_proj(L, 128, 32)
               -&gt; key_proj(L, 128, 8)
               -&gt; value_proj(L, 128, 8)
                                           -&gt; group_query_attention(L, 128, 32)
                                           -&gt; output_proj(L, 4096)
                                                                                   -&gt; output(L, 4096)
</pre>
<div dir="auto" data-snippet-clipboard-copy-content="# Record these configurations, which will be gradually used later.
dim = config[&quot;dim&quot;]
n_layers = config[&quot;n_layers&quot;]
n_heads = config[&quot;n_heads&quot;]
n_kv_heads = config[&quot;n_kv_heads&quot;]
vocab_size = config[&quot;vocab_size&quot;]
multiple_of = config[&quot;multiple_of&quot;]
ffn_dim_multiplier = config[&quot;ffn_dim_multiplier&quot;]
norm_eps = config[&quot;norm_eps&quot;]
rope_theta = torch.tensor(config[&quot;rope_theta&quot;])"><pre><span># Record these configurations, which will be gradually used later.</span>
<span>dim</span> <span>=</span> <span>config</span>[<span>"dim"</span>]
<span>n_layers</span> <span>=</span> <span>config</span>[<span>"n_layers"</span>]
<span>n_heads</span> <span>=</span> <span>config</span>[<span>"n_heads"</span>]
<span>n_kv_heads</span> <span>=</span> <span>config</span>[<span>"n_kv_heads"</span>]
<span>vocab_size</span> <span>=</span> <span>config</span>[<span>"vocab_size"</span>]
<span>multiple_of</span> <span>=</span> <span>config</span>[<span>"multiple_of"</span>]
<span>ffn_dim_multiplier</span> <span>=</span> <span>config</span>[<span>"ffn_dim_multiplier"</span>]
<span>norm_eps</span> <span>=</span> <span>config</span>[<span>"norm_eps"</span>]
<span>rope_theta</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>config</span>[<span>"rope_theta"</span>])</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Convert the input text into embeddings</h2><a id="user-content-convert-the-input-text-into-embeddings" aria-label="Permalink: Convert the input text into embeddings" href="#convert-the-input-text-into-embeddings"></a></p>
<p dir="auto">Before inputting the text in string form to the network layer, it needs to be converted into vector form for mathematical calculations.
<br>
The required process is: use the tokenizer to split the input text into a subword sequence -&gt; convert the subwords into vector representations.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Convert the text into a sequence of token ids</h2><a id="user-content-convert-the-text-into-a-sequence-of-token-ids" aria-label="Permalink: Convert the text into a sequence of token ids" href="#convert-the-text-into-a-sequence-of-token-ids"></a></p>
<p dir="auto">Here, we use tiktoken (a library from OpenAI) as the tokenizer.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/tokens.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/tokens.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Convert the input prompt into a sequence of token ids
prompt = &quot;the answer to the ultimate question of life, the universe, and everything is &quot;  # Input text
tokens = [128000] + tokenizer.encode(prompt)  # Perform subword segmentation and add a special token <|begin_of_text|> indicating the start of the text at the beginning of the text. Dimension: [17]
print(tokens)  # Check the segmentation result
tokens = torch.tensor(tokens)  # Convert to tensor type for subsequent matrix calculations. [17]

# Convert the token ids into a specific sequence of token subwords, which is only for display purposes and not actually needed
prompt_split_as_tokens = [tokenizer.decode([token]) for token in tokens]
print(prompt_split_as_tokens)"><pre><span># Convert the input prompt into a sequence of token ids</span>
<span>prompt</span> <span>=</span> <span>"the answer to the ultimate question of life, the universe, and everything is "</span>  <span># Input text</span>
<span>tokens</span> <span>=</span> [<span>128000</span>] <span>+</span> <span>tokenizer</span>.<span>encode</span>(<span>prompt</span>)  <span># Perform subword segmentation and add a special token &lt;|begin_of_text|&gt; indicating the start of the text at the beginning of the text. Dimension: [17]</span>
<span>print</span>(<span>tokens</span>)  <span># Check the segmentation result</span>
<span>tokens</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>tokens</span>)  <span># Convert to tensor type for subsequent matrix calculations. [17]</span>

<span># Convert the token ids into a specific sequence of token subwords, which is only for display purposes and not actually needed</span>
<span>prompt_split_as_tokens</span> <span>=</span> [<span>tokenizer</span>.<span>decode</span>([<span>token</span>]) <span>for</span> <span>token</span> <span>in</span> <span>tokens</span>]
<span>print</span>(<span>prompt_split_as_tokens</span>)</pre></div>
<div data-snippet-clipboard-copy-content="[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]
['<|begin_of_text|>', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']"><pre><code>[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]
['&lt;|begin_of_text|&gt;', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Convert the sequence of token ids into embeddings</h2><a id="user-content-convert-the-sequence-of-token-ids-into-embeddings" aria-label="Permalink: Convert the sequence of token ids into embeddings" href="#convert-the-sequence-of-token-ids-into-embeddings"></a></p>
<div dir="auto"><p>Sorry, this is the only part in this codebase where I use built-in neural network modules.
<br>
In short, our original [17x1] token sequence is now [17x4096], that is, 17 embeddings of length 4096 (one for each token).
</p><p>
Note: Pay attention to the change in the shape of this tensor, which will make it easier for you to understand the entire process (And i will annotate the shape changes in all steps).</p></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/embeddings.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/embeddings.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Create an embedding network layer to map discrete token ids to a continuous vector space
embedding_layer = torch.nn.Embedding(vocab_size, dim)

# Update the parameters of the embedding network with the pre-trained parameter values in Llama3
embedding_layer.weight.data.copy_(model[&quot;tok_embeddings.weight&quot;])

# Use the embedding network to convert the input sequence of token ids into vector representations
# The embedding network only looks up the corresponding vectors based on the ids in a dictionary and does not involve interactions between tokens.
# [17] -> [17x4096]
token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)  # By default, it is in full-precision float32. Here, we change it to the half-precision format to reduce memory usage.

token_embeddings_unnormalized.shape"><pre><span># Create an embedding network layer to map discrete token ids to a continuous vector space</span>
<span>embedding_layer</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>Embedding</span>(<span>vocab_size</span>, <span>dim</span>)

<span># Update the parameters of the embedding network with the pre-trained parameter values in Llama3</span>
<span>embedding_layer</span>.<span>weight</span>.<span>data</span>.<span>copy_</span>(<span>model</span>[<span>"tok_embeddings.weight"</span>])

<span># Use the embedding network to convert the input sequence of token ids into vector representations</span>
<span># The embedding network only looks up the corresponding vectors based on the ids in a dictionary and does not involve interactions between tokens.</span>
<span># [17] -&gt; [17x4096]</span>
<span>token_embeddings_unnormalized</span> <span>=</span> <span>embedding_layer</span>(<span>tokens</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)  <span># By default, it is in full-precision float32. Here, we change it to the half-precision format to reduce memory usage.</span>

<span>token_embeddings_unnormalized</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Build the first Transformer block</h2><a id="user-content-build-the-first-transformer-block" aria-label="Permalink: Build the first Transformer block" href="#build-the-first-transformer-block"></a></p>
<p dir="auto">From the pre-trained parameters involved in the first Transformer block shown below, it includes:</p>
<ol dir="auto">
<li>Two normalizations (attention_norm and ffn_norm)</li>
<li>Implementation of the attention mechanism (4 attention.w)</li>
<li>Implementation of the feed-forward network layer (3 feed_forward.w)</li>
<li>(Of course, it also includes two residual connection operations that do not require pre-trained parameters)</li>
</ol>
<p dir="auto">In general, the operation process in a Transformer block is as follows:
<br>
Normalization -&gt; Multi-head self-attention -&gt; Residual connection -&gt; Normalization -&gt; Feed-forward neural network -&gt; Residual connection</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Display all the weight parameters and their shapes of the first Transformer block
for k, v in model.items():
    if not k.startswith('layers'):
        continue
    if k.startswith('layers.1'):
        break
    print(k, v.shape)"><pre><span># Display all the weight parameters and their shapes of the first Transformer block</span>
<span>for</span> <span>k</span>, <span>v</span> <span>in</span> <span>model</span>.<span>items</span>():
    <span>if</span> <span>not</span> <span>k</span>.<span>startswith</span>(<span>'layers'</span>):
        <span>continue</span>
    <span>if</span> <span>k</span>.<span>startswith</span>(<span>'layers.1'</span>):
        <span>break</span>
    <span>print</span>(<span>k</span>, <span>v</span>.<span>shape</span>)</pre></div>
<div data-snippet-clipboard-copy-content="layers.0.attention.wq.weight torch.Size([4096, 4096])
layers.0.attention.wk.weight torch.Size([1024, 4096])
layers.0.attention.wv.weight torch.Size([1024, 4096])
layers.0.attention.wo.weight torch.Size([4096, 4096])
layers.0.feed_forward.w1.weight torch.Size([14336, 4096])
layers.0.feed_forward.w3.weight torch.Size([14336, 4096])
layers.0.feed_forward.w2.weight torch.Size([4096, 14336])
layers.0.attention_norm.weight torch.Size([4096])
layers.0.ffn_norm.weight torch.Size([4096])"><pre><code>layers.0.attention.wq.weight torch.Size([4096, 4096])
layers.0.attention.wk.weight torch.Size([1024, 4096])
layers.0.attention.wv.weight torch.Size([1024, 4096])
layers.0.attention.wo.weight torch.Size([4096, 4096])
layers.0.feed_forward.w1.weight torch.Size([14336, 4096])
layers.0.feed_forward.w3.weight torch.Size([14336, 4096])
layers.0.feed_forward.w2.weight torch.Size([4096, 14336])
layers.0.attention_norm.weight torch.Size([4096])
layers.0.ffn_norm.weight torch.Size([4096])
</code></pre></div>
<p dir="auto">There are two points to note here:</p>
<ol dir="auto">
<li>The shape of the weight matrix of a neural network is (output dimension, input dimension). During the calculation, the parameter matrix W will be transposed to (input dimension, output dimension) and then multiplied by the input X, i.e., the output Y = XW.T. You will see this in the subsequent calculations.</li>
<li>Since Llama3 uses the grouped attention mechanism, every 4 query heads will share a set of kv vectors (for details, see the section on the details of the configuration file above). Therefore, the dimension of the weight matrix of kv is [1024, 4096], which is 1/4 of that of q ([4096, 4096]).</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Normalization</h2><a id="user-content-normalization" aria-label="Permalink: Normalization" href="#normalization"></a></p>
<div dir="auto"><p>The normalization operation aims to constrain the scale differences in the data, avoiding issues such as unstable training process caused by excessive differences in vector values.
</p><p>
After normalization, the shape of the tensor remains [17x4096], the same as that of the embedding.</p></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/norm.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/norm.png" width="600"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using RMS normalization for embeddings</h3><a id="user-content-using-rms-normalization-for-embeddings" aria-label="Permalink: Using RMS normalization for embeddings" href="#using-rms-normalization-for-embeddings"></a></p>
<p dir="auto">Llama3 uses the Root Mean Square (RMS) normalization method, and its calculation formula is shown in the figure below.
<br>
It should be noted that we need a norm_eps parameter (from the configurations) because we don't want to accidentally set the RMS to 0 and perform a division by zero.
<br>
The formula is as follows:</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/rms.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/rms.png" width="600"></a>
</p>

<p dir="auto">In addition, you may have noticed the gi parameter in the formula. This is a scaling factor learned during the model training process, used to scale the normalization result of each dimension again to enhance the model's expressive ability. Its dimension is the same as the feature dimension of the embedding, i.e., [4096].</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Define the calculation function for RMS normalization
# Each token will be normalized independently
# norm_weights is the pre-trained scaling factor (i.e., gi in the formula) to enhance the model's representational ability. It can be loaded from the model file and has 4096 dimensions
# torch.rsqrt used to calculates the reciprocal of the square root of a tensor, i.e., 1/RMS(a)
def rms_norm(tensor, norm_weights):
    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights"><pre><span># Define the calculation function for RMS normalization</span>
<span># Each token will be normalized independently</span>
<span># norm_weights is the pre-trained scaling factor (i.e., gi in the formula) to enhance the model's representational ability. It can be loaded from the model file and has 4096 dimensions</span>
<span># torch.rsqrt used to calculates the reciprocal of the square root of a tensor, i.e., 1/RMS(a)</span>
<span>def</span> <span>rms_norm</span>(<span>tensor</span>, <span>norm_weights</span>):
    <span>return</span> (<span>tensor</span> <span>*</span> <span>torch</span>.<span>rsqrt</span>(<span>tensor</span>.<span>pow</span>(<span>2</span>).<span>mean</span>(<span>-</span><span>1</span>, <span>keepdim</span><span>=</span><span>True</span>) <span>+</span> <span>norm_eps</span>)) <span>*</span> <span>norm_weights</span></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Normalize the input
token_embeddings = rms_norm(token_embeddings_unnormalized, model[&quot;layers.0.attention_norm.weight&quot;])  # [17x4096] &amp; [4096] -> [17x4096]
model[&quot;layers.0.attention_norm.weight&quot;].shape, token_embeddings.shape"><pre><span># Normalize the input</span>
<span>token_embeddings</span> <span>=</span> <span>rms_norm</span>(<span>token_embeddings_unnormalized</span>, <span>model</span>[<span>"layers.0.attention_norm.weight"</span>])  <span># [17x4096] &amp; [4096] -&gt; [17x4096]</span>
<span>model</span>[<span>"layers.0.attention_norm.weight"</span>].<span>shape</span>, <span>token_embeddings</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="(torch.Size([4096]), torch.Size([17, 4096]))"><pre><code>(torch.Size([4096]), torch.Size([17, 4096]))
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Implementing the single-head attention mechanism from scratch</h2><a id="user-content-implementing-the-single-head-attention-mechanism-from-scratch" aria-label="Permalink: Implementing the single-head attention mechanism from scratch" href="#implementing-the-single-head-attention-mechanism-from-scratch"></a></p>
<p dir="auto">In the calculation of multi-head attention on each layer, 32 heads are involved. However, the calculation processes of these heads are completely identical and independent. Therefore, in this section, we will first implement the single-head attention calculation process, and expand it to multi-head calculation in the next section.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/qkv.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/qkv.png" width="600"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">
The core calculation of the attention mechanism is the calculation formula shown in the following figure.
</h3><a id="user-content-the-core-calculation-of-the-attention-mechanism-is-the-calculation-formula-shown-in-the-following-figure" aria-label="Permalink: 
The core calculation of the attention mechanism is the calculation formula shown in the following figure.
" href="#the-core-calculation-of-the-attention-mechanism-is-the-calculation-formula-shown-in-the-following-figure"></a></p>
<ol dir="auto">
<li>We need to obtain the query, key, and value vectors by performing a linear mapping on the input embeddings.</li>
<li>Subsequently, based on the QK vectors, we obtain the attention weights between tokens, that is, for each token, the scores of the importance or relevance of other tokens to it.</li>
<li>Finally, based on the attention weights, we weight the value vectors to obtain the attention results corresponding to each token.</li>
</ol>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/softmax.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/softmax.png" width="600px"></a>
</p>
<p dir="auto">Back to the point. Let's first load the attention heads of the first-layer Transformer.
<br>
&gt; When we load the query, key, value, and output weight matrices from the model (the output weight is used for information fusion among multiple heads to generate the final attention output), we will notice that their shapes are: [4096x4096], [1024x4096], [1024x4096], [4096x4096].
<br>
&gt; At first glance, this seems strange because ideally, we would like the q, k, v of each head to be independent of each other (in which case their shapes would be: 32x[128x4096], 8x[128x4096], 8x[128x4096]).
<br>
&gt; The author of the code binds them together because this helps parallelize the multiplication calculation of the attention heads.
<br>
&gt; But we will unfold everything...</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Show the shapes of the attention weight matrices of the current q, k, v and o.
print(
    model[&quot;layers.0.attention.wq.weight&quot;].shape,  # [4096x4096]
    model[&quot;layers.0.attention.wk.weight&quot;].shape,  # [1024x4096]
    model[&quot;layers.0.attention.wv.weight&quot;].shape,  # [1024x4096]
    model[&quot;layers.0.attention.wo.weight&quot;].shape   # [4096x4096]
)"><pre><span># Show the shapes of the attention weight matrices of the current q, k, v and o.</span>
<span>print</span>(
    <span>model</span>[<span>"layers.0.attention.wq.weight"</span>].<span>shape</span>,  <span># [4096x4096]</span>
    <span>model</span>[<span>"layers.0.attention.wk.weight"</span>].<span>shape</span>,  <span># [1024x4096]</span>
    <span>model</span>[<span>"layers.0.attention.wv.weight"</span>].<span>shape</span>,  <span># [1024x4096]</span>
    <span>model</span>[<span>"layers.0.attention.wo.weight"</span>].<span>shape</span>   <span># [4096x4096]</span>
)</pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])"><pre><code>torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Obtain the QKV vectors corresponding to the input tokens</h3><a id="user-content-obtain-the-qkv-vectors-corresponding-to-the-input-tokens" aria-label="Permalink: Obtain the QKV vectors corresponding to the input tokens" href="#obtain-the-qkv-vectors-corresponding-to-the-input-tokens"></a></p>
<p dir="auto">In this section, we will convert the input token embeddings into query, key, and value vectors for the purpose of attention mechanism computation.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Obtain the query vector</h4><a id="user-content-obtain-the-query-vector" aria-label="Permalink: Obtain the query vector" href="#obtain-the-query-vector"></a></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Unfold the query weight matrix</h5><a id="user-content-unfold-the-query-weight-matrix" aria-label="Permalink: Unfold the query weight matrix" href="#unfold-the-query-weight-matrix"></a></p>
<p dir="auto">We will first unfold the queries from multiple attention heads, and the final shape will be [32x128x4096].
<br>
Here, 32 is the number of attention heads in Llama3, 128 is the vector dimension of the query head, and 4096 is the dimension of the token embedding (the reason why the dimension of the embedding is in the last dimension is that when multiplying the input and the weight, it is usually = X*W.T, that is, multiplying by the transpose of the weight).</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Load and modify the shape of the query weight matrix of layers.0 to unfold it in the form of multiple heads
q_layer0 = model[&quot;layers.0.attention.wq.weight&quot;]  # Default shape is [4096x4096]
head_dim = q_layer0.shape[0] // n_heads  # Dimension of the attention head, 4096/32 = 128
q_layer0 = q_layer0.view(n_heads, head_dim, dim)  # Unfolded dimension, [32x128x4096]
q_layer0.shape"><pre><span># Load and modify the shape of the query weight matrix of layers.0 to unfold it in the form of multiple heads</span>
<span>q_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wq.weight"</span>]  <span># Default shape is [4096x4096]</span>
<span>head_dim</span> <span>=</span> <span>q_layer0</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_heads</span>  <span># Dimension of the attention head, 4096/32 = 128</span>
<span>q_layer0</span> <span>=</span> <span>q_layer0</span>.<span>view</span>(<span>n_heads</span>, <span>head_dim</span>, <span>dim</span>)  <span># Unfolded dimension, [32x128x4096]</span>
<span>q_layer0</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([32, 128, 4096])"><pre><code>torch.Size([32, 128, 4096])
</code></pre></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">Obtain the first head</h5><a id="user-content-obtain-the-first-head" aria-label="Permalink: Obtain the first head" href="#obtain-the-first-head"></a></p>
<p dir="auto">Here, I access the first head of the query weight matrix in the first layer. The shape of this query weight matrix is [128x4096].</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Extract the weights of the first head
q_layer0_head0 = q_layer0[0]  # [32x128x4096] -> [128x4096]
q_layer0_head0.shape"><pre><span># Extract the weights of the first head</span>
<span>q_layer0_head0</span> <span>=</span> <span>q_layer0</span>[<span>0</span>]  <span># [32x128x4096] -&gt; [128x4096]</span>
<span>q_layer0_head0</span>.<span>shape</span></pre></div>

<p dir="auto"><h5 tabindex="-1" dir="auto">Multiply the token embeddings by the query weights to obtain the query vectors corresponding to the tokens</h5><a id="user-content-multiply-the-token-embeddings-by-the-query-weights-to-obtain-the-query-vectors-corresponding-to-the-tokens" aria-label="Permalink: Multiply the token embeddings by the query weights to obtain the query vectors corresponding to the tokens" href="#multiply-the-token-embeddings-by-the-query-weights-to-obtain-the-query-vectors-corresponding-to-the-tokens"></a></p>
<p dir="auto">Here, you can see that the shape of the result is [17x128]. This is because we have 17 tokens, and for each token, there is a query vector of length 128.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/q_per_token.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/q_per_token.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the query values of inputs on the first query head
# Q0_head0 = XW0_Q_head0.T
q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T)  # [17x4096] x [4096x128] = [17x128]
q_per_token.shape"><pre><span># Calculate the query values of inputs on the first query head</span>
<span># Q0_head0 = XW0_Q_head0.T</span>
<span>q_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>q_layer0_head0</span>.<span>T</span>)  <span># [17x4096] x [4096x128] = [17x128]</span>
<span>q_per_token</span>.<span>shape</span></pre></div>

<p dir="auto"><h4 tabindex="-1" dir="auto">Obtain the key vector (almost the same as the query vector)</h4><a id="user-content-obtain-the-key-vector-almost-the-same-as-the-query-vector" aria-label="Permalink: Obtain the key vector (almost the same as the query vector)" href="#obtain-the-key-vector-almost-the-same-as-the-query-vector"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/keys.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/keys.png" width="600px"></a>
</p>
<p dir="auto">I want to take a lazy, so I won't elaborate on the calculation process of the key vector again. Orz. The only thing you need to remember is:
<br>
&gt; The key also generates a 128-dimensional vector.
<br>
&gt; The number of parameters of the weight matrix of the key is only 1/4 of that of the query, because the weight of each key is shared by 4 heads simultaneously to reduce the required amount of calculation.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Load and modify the shape of the key weight matrix of layers.0 to expand it in a multi-head form
# Different from the query weight matrix, the key has 8 attention heads, so the number of parameters is 1/4 of that of the query matrix
k_layer0 = model[&quot;layers.0.attention.wk.weight&quot;]  # [1024x4096]
k_layer0 = k_layer0.view(n_kv_heads, k_layer0.shape[0] // n_kv_heads, dim) # [8x128x4096]
k_layer0.shape"><pre><span># Load and modify the shape of the key weight matrix of layers.0 to expand it in a multi-head form</span>
<span># Different from the query weight matrix, the key has 8 attention heads, so the number of parameters is 1/4 of that of the query matrix</span>
<span>k_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wk.weight"</span>]  <span># [1024x4096]</span>
<span>k_layer0</span> <span>=</span> <span>k_layer0</span>.<span>view</span>(<span>n_kv_heads</span>, <span>k_layer0</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>) <span># [8x128x4096]</span>
<span>k_layer0</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([8, 128, 4096])"><pre><code>torch.Size([8, 128, 4096])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Extract the weights of the first head
k_layer0_head0 = k_layer0[0]  # [8x128x4096] -> [128x4096]
k_layer0_head0.shape"><pre><span># Extract the weights of the first head</span>
<span>k_layer0_head0</span> <span>=</span> <span>k_layer0</span>[<span>0</span>]  <span># [8x128x4096] -&gt; [128x4096]</span>
<span>k_layer0_head0</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the key vectors corresponding to the inputs of the first head
# K0_head0 = XW0_K_head0.T
k_per_token = torch.matmul(token_embeddings, k_layer0_head0.T)  # [17x4096] x [4096x128] = [17x128]
k_per_token.shape"><pre><span># Calculate the key vectors corresponding to the inputs of the first head</span>
<span># K0_head0 = XW0_K_head0.T</span>
<span>k_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>k_layer0_head0</span>.<span>T</span>)  <span># [17x4096] x [4096x128] = [17x128]</span>
<span>k_per_token</span>.<span>shape</span></pre></div>

<p dir="auto"><h4 tabindex="-1" dir="auto">Obtain the value vector (almost the same as the key vector)</h4><a id="user-content-obtain-the-value-vector-almost-the-same-as-the-key-vector" aria-label="Permalink: Obtain the value vector (almost the same as the key vector)" href="#obtain-the-value-vector-almost-the-same-as-the-key-vector"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/value.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/value.png" width="600px"></a>
</p>
<p dir="auto">&gt; Similar to the key weights, the value weights are also shared by every 4 attention heads (to save computation).
<br>
&gt; Therefore, the shape of the value weight matrix is [8x128x4096].</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Load and modify the shape of the value weight matrix of layers.0 to expand it in a multi-head form
# Similar to the key weight matrix, the value also has 8 attention heads, so the number of parameters is also 1/4 of that of the query matrix
v_layer0 = model[&quot;layers.0.attention.wv.weight&quot;]  # [1024x4096]
v_layer0 = v_layer0.view(n_kv_heads, v_layer0.shape[0] // n_kv_heads, dim)  # [1024x4096] -> [8x128x4096]
v_layer0.shape"><pre><span># Load and modify the shape of the value weight matrix of layers.0 to expand it in a multi-head form</span>
<span># Similar to the key weight matrix, the value also has 8 attention heads, so the number of parameters is also 1/4 of that of the query matrix</span>
<span>v_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wv.weight"</span>]  <span># [1024x4096]</span>
<span>v_layer0</span> <span>=</span> <span>v_layer0</span>.<span>view</span>(<span>n_kv_heads</span>, <span>v_layer0</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>)  <span># [1024x4096] -&gt; [8x128x4096]</span>
<span>v_layer0</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([8, 128, 4096])"><pre><code>torch.Size([8, 128, 4096])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Extract the weights of the first head
v_layer0_head0 = v_layer0[0]  # [8x128x4096] -> [128x4096]
v_layer0_head0.shape"><pre><span># Extract the weights of the first head</span>
<span>v_layer0_head0</span> <span>=</span> <span>v_layer0</span>[<span>0</span>]  <span># [8x128x4096] -&gt; [128x4096]</span>
<span>v_layer0_head0</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the value vectors corresponding to the inputs of the first head
# V0_head0 = XW0_V_head0.T
v_per_token = torch.matmul(token_embeddings, v_layer0_head0.T)  # [17x4096] x [4096x128] = [17x128]
v_per_token.shape"><pre><span># Calculate the value vectors corresponding to the inputs of the first head</span>
<span># V0_head0 = XW0_V_head0.T</span>
<span>v_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>v_layer0_head0</span>.<span>T</span>)  <span># [17x4096] x [4096x128] = [17x128]</span>
<span>v_per_token</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">Add positional information to the query and key vectors</h3><a id="user-content-add-positional-information-to-the-query-and-key-vectors" aria-label="Permalink: Add positional information to the query and key vectors" href="#add-positional-information-to-the-query-and-key-vectors"></a></p>
<ul dir="auto">
<li>For natural language, the sequential relationship and relative positions between words are extremely important. For example, "The dog bites the man" and "The man bites the dog" have completely different semantic information. Moreover, our intuition also tells us that the correlation between relatively close words is usually greater than that between distant words.</li>
<li>Therefore, we need to provide positional information between tokens during the attention calculation process, so that the model can better capture the dependencies in the sequence.</li>
<li>Why add it to the query and key vectors? Because the query and key vectors are used to calculate the attention weights, i.e., the importance of each token to other tokens. This requires both of them to know the positions and relative positional relationships of any two tokens when calculating the similarity between them.</li>
<li>Why not add it to the value vectors? Because the value vectors are only used for weighted summation. The positional information has already been considered in the interaction between the query and the key. Therefore, the value vectors only need to provide content information.</li>
</ul>
<p dir="auto">We will use RoPE (Rotary Position Encoding) to add positional information to these vectors.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Rotary Position Encoding (RoPE)</h4><a id="user-content-rotary-position-encoding-rope" aria-label="Permalink: Rotary Position Encoding (RoPE)" href="#rotary-position-encoding-rope"></a></p>
<div dir="auto"><p>You can watch this video to understand its mathematical principles in detail (this is also the one I watched):
<a href="https://www.youtube.com/watch?v=o29P0Kpobz0&amp;t=530s" rel="nofollow">https://www.youtube.com/watch?v=o29P0Kpobz0&amp;t=530s</a></p><p>
The general idea of RoPE is to regard the vector as being in the complex space, and then generate specific rotation matrix based on positions. By multiplying the vector with the rotation matrix, rotation in the complex space can be achieved, thereby adding the relative position information to the vector. (That is, taking the positional relationship of the input vectors as a rotation at different angles in a complex space.)
<br>
(Similar to the rotation of planar position coordinates around an axis through the multiplication of trigonometric-function-based matrices in robot kinematics.)
</p><p>
RoPE is usually applied to the query and key vectors in the self-attention mechanism. When calculating the attention scores, the query and key vectors are first rotated based on the corresponding rotation matrix of RoPE. Then, operations such as dot-product calculation and softmax normalization are performed. In this way, the Transformer can take positional information into account when calculating attentions and better capture the dependencies in the text.</p></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/rope.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/rope.png" width="600"></a>
</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">
The specific calculation process of RoPE is as follows:
</h3><a id="user-content-the-specific-calculation-process-of-rope-is-as-follows" aria-label="Permalink: 
The specific calculation process of RoPE is as follows:
" href="#the-specific-calculation-process-of-rope-is-as-follows"></a></p>
<ol dir="auto">
<li>Divide the dimensions of each vector into pairs (because the derivation of high-dimensional rotation matrices is complex, and excessively high dimensions will significantly increase the computational complexity, while the formulas for two-dimensional rotation are relatively mature and simple, making them easy to calculate).</li>
<li>For each pair, obtain <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\Large \theta=\frac{1}{rope\_theta^{i/D}}$</math-renderer>, where <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$i$</math-renderer> is the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$i$</math-renderer>-th pair and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$D$</math-renderer> is the total number of pairs. That is, the positional information of the current dimension pair within the vector.</li>
<li>For each vector, obtain <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\Large m$</math-renderer>, which represents that the vector corresponds to the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m$</math-renderer>-th token. That is, the positional information of the current vector within the entire input vectors.</li>
<li>For each pair, <a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/pmatrix.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/pmatrix.png" alt="png"></a> , where <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$res$</math-renderer> is the result of rotating the vector pair by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m\theta$</math-renderer> degrees in the complex space.</li>
<li>Perform the above calculations on all dimension pairs of all vectors to obtain the final RoPE result.</li>
</ol>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/qsplit.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/qsplit.png" width="600"></a>
</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">
In the actual code implementation, to simplify the calculation process, the above-mentioned calculation based on the rotation matrix (Step 4) will be converted into a calculation in the complex number domain. The principle is as follows:
</h3><a id="user-content-in-the-actual-code-implementation-to-simplify-the-calculation-process-the-above-mentioned-calculation-based-on-the-rotation-matrix-step-4-will-be-converted-into-a-calculation-in-the-complex-number-domain-the-principle-is-as-follows" aria-label="Permalink: 
In the actual code implementation, to simplify the calculation process, the above-mentioned calculation based on the rotation matrix (Step 4) will be converted into a calculation in the complex number domain. The principle is as follows:
" href="#in-the-actual-code-implementation-to-simplify-the-calculation-process-the-above-mentioned-calculation-based-on-the-rotation-matrix-step-4-will-be-converted-into-a-calculation-in-the-complex-number-domain-the-principle-is-as-follows"></a></p>
<ol dir="auto">
<li>The rectangular coordinates <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$(x, y)$</math-renderer> can be regarded as the coordinate representation of the complex number <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large x + yi$</math-renderer> on the complex plane.</li>
<li>The polar form of a complex number can be expressed as <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large re^{i\theta}$</math-renderer>, where <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$r$</math-renderer> is the modulus and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\theta$</math-renderer> is the angle.</li>
<li>The multiplication calculation in polar coordinates <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large r_1e^{i\theta_1} \times r_2e^{i\theta_2} = r_1r_2e^{i(\theta_1 + \theta_2)}$</math-renderer> can be regarded as increasing the length of coordinate_1 by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$r_2$</math-renderer> times and rotating the angle by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\theta_2$</math-renderer> degrees.</li>
<li>Therefore, if you want to rotate the coordinates by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m\theta$</math-renderer> degrees, you can define a rotation factor <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large e^{im\theta}$</math-renderer> with a modulus of 1 and an angle of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m\theta$</math-renderer>. Multiplying it by the coordinates will be equivalent to the rotation method based on the rotation matrix.</li>
<li>In addition, according to Euler's formula, we have <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large re^{i\theta} = r\cos\theta + r\sin{\theta i} = x + yi$</math-renderer> and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large e^{im\theta} = \cos{m\theta} + \sin{m\theta i}$</math-renderer>.</li>
<li>Therefore, rotating a two-dimensional coordinate <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$(x, y)$</math-renderer> by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m\theta$</math-renderer> degrees can be obtained through <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large re^{i\theta^\prime} \times e^{im\theta} = (x + yi) \times (\cos{m\theta} + \sin{m\theta i})$</math-renderer> (the product of two complex numbers).</li>
</ol>
<p dir="auto"><h4 tabindex="-1" dir="auto">Add positional information to the query vectors</h4><a id="user-content-add-positional-information-to-the-query-vectors" aria-label="Permalink: Add positional information to the query vectors" href="#add-positional-information-to-the-query-vectors"></a></p>
<div dir="auto"><p>In the following steps, we will first split the query vectors into pairs, and then perform angle rotation on each pair, as shown in the above steps.
</p><p>
Now we have a vector with a shape of [17x64x2]. This is obtained by splitting the 128-dimensional query vectors corresponding to each token in the prompt into 64 pairs, and each pair will be rotated by </p><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m\theta$</math-renderer><p> degrees.</p></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Split the query vectors in pairs along the dimension direction.
# .float() is for switch back to full precision to ensure the precision and numerical stability in the subsequent trigonometric function calculations.
# [17x128] -> [17x64x2]
q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)
q_per_token_split_into_pairs.shape"><pre><span># Split the query vectors in pairs along the dimension direction.</span>
<span># .float() is for switch back to full precision to ensure the precision and numerical stability in the subsequent trigonometric function calculations.</span>
<span># [17x128] -&gt; [17x64x2]</span>
<span>q_per_token_split_into_pairs</span> <span>=</span> <span>q_per_token</span>.<span>float</span>().<span>view</span>(<span>q_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)
<span>q_per_token_split_into_pairs</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">
Start to obtain the complex-domain representation of the rotation matrix.
</h3><a id="user-content-start-to-obtain-the-complex-domain-representation-of-the-rotation-matrix" aria-label="Permalink: 
Start to obtain the complex-domain representation of the rotation matrix.
" href="#start-to-obtain-the-complex-domain-representation-of-the-rotation-matrix"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/freq_cis.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/freq_cis.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate θ. Step 1: Obtain i/D.
# [64]
zero_to_one_split_into_64_parts = torch.tensor(range(64))/64  # Each feature has 64 dimension pairs after segmentation, so 64 θ values are required
zero_to_one_split_into_64_parts"><pre><span># Calculate θ. Step 1: Obtain i/D.</span>
<span># [64]</span>
<span>zero_to_one_split_into_64_parts</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>range</span>(<span>64</span>))<span>/</span><span>64</span>  <span># Each feature has 64 dimension pairs after segmentation, so 64 θ values are required</span>
<span>zero_to_one_split_into_64_parts</span></pre></div>
<div data-snippet-clipboard-copy-content="tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250,
        0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656,
        0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062,
        0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469,
        0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875,
        0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281,
        0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688,
        0.9844])"><pre><code>tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250,
        0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656,
        0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062,
        0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469,
        0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875,
        0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281,
        0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688,
        0.9844])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate θ. Step 2: Obtain θ.
# rope_theta is used to control information such as the periodicity of the position encoding.
# For details, please refer to the configuration information section.
freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)  # [64]
freqs"><pre><span># Calculate θ. Step 2: Obtain θ.</span>
<span># rope_theta is used to control information such as the periodicity of the position encoding.</span>
<span># For details, please refer to the configuration information section.</span>
<span>freqs</span> <span>=</span> <span>1.0</span> <span>/</span> (<span>rope_theta</span> <span>**</span> <span>zero_to_one_split_into_64_parts</span>)  <span># [64]</span>
<span>freqs</span></pre></div>
<div data-snippet-clipboard-copy-content="tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,
        2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,
        8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,
        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,
        7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,
        2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,
        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,
        1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,
        5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,
        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,
        4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06])"><pre><code>tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,
        2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,
        8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,
        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,
        7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,
        2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,
        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,
        1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,
        5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,
        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,
        4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate mθ
# 'outer' is for outer product calculation, and 'arange(17)' represents the m corresponding to each vector (since the input has 17 tokens, 17 m values are needed).
# The result has a shape of [17x64], meaning that each vector corresponding to a token has 64 mθ values, which are used to calculate the rotation of each of the 64 dimension pairs.
freqs_for_each_token = torch.outer(torch.arange(17), freqs)  # [17] &amp; [64] -> [17x64]"><pre><span># Calculate mθ</span>
<span># 'outer' is for outer product calculation, and 'arange(17)' represents the m corresponding to each vector (since the input has 17 tokens, 17 m values are needed).</span>
<span># The result has a shape of [17x64], meaning that each vector corresponding to a token has 64 mθ values, which are used to calculate the rotation of each of the 64 dimension pairs.</span>
<span>freqs_for_each_token</span> <span>=</span> <span>torch</span>.<span>outer</span>(<span>torch</span>.<span>arange</span>(<span>17</span>), <span>freqs</span>)  <span># [17] &amp; [64] -&gt; [17x64]</span></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Obtain (cosmθ + sinmθi), that is, convert mθ to the complex-number form
# Regard the rotation angle mθ as a polar-coordinate form with a modulus of 1, and then convert it to a complex-number representation
# The two inputs of 'polar' represent the modulus (set to 1, meaning only the angle is changed without affecting the length) and the angle (i.e., mθ) respectively
freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)  # [17x64] -> [17x64]
print(freqs_cis.shape)

# View freqs_cis at some positions, just for display
token_to_show = [1, 3, 5]  # View the 2nd, 4th, and 6th rows
fig, axs = plt.subplots(1, len(token_to_show), figsize=(5 * len(token_to_show), 4))  # Generate a figure window with 3 sub-plots in 1 row and 3 columns
for i, index in enumerate(token_to_show):
    value = freqs_cis[index]
    for j, element in enumerate(value):
        # Plot a blue line from the origin to the coordinate point, with the real part as the x-coordinate and the imaginary part as the y-coordinate.
        axs[i].plot([0, element.real], [0, element.imag], color='blue', linewidth=1, label=f&quot;Index: {j}&quot;)
        # Draw red numerical annotations to represent the i-th pair of dimensions.
        axs[i].annotate(f&quot;{j}&quot;, xy=(element.real, element.imag), color='red')
    axs[i].set_xlabel('Real')
    axs[i].set_ylabel('Imaginary')
    axs[i].set_title(f'Plot of {index + 1}th of freqs_cis')
plt.show()

&quot;&quot;&quot;
Note: As shown in the figures, tokens in later positions have larger rotation angles, but within a single token, earlier vector dimensions have larger rotation angles.
      You can explore further to see if there are any mathematical reasons behind this if you are interested. X_X
&quot;&quot;&quot;"><pre><span># Obtain (cosmθ + sinmθi), that is, convert mθ to the complex-number form</span>
<span># Regard the rotation angle mθ as a polar-coordinate form with a modulus of 1, and then convert it to a complex-number representation</span>
<span># The two inputs of 'polar' represent the modulus (set to 1, meaning only the angle is changed without affecting the length) and the angle (i.e., mθ) respectively</span>
<span>freqs_cis</span> <span>=</span> <span>torch</span>.<span>polar</span>(<span>torch</span>.<span>ones_like</span>(<span>freqs_for_each_token</span>), <span>freqs_for_each_token</span>)  <span># [17x64] -&gt; [17x64]</span>
<span>print</span>(<span>freqs_cis</span>.<span>shape</span>)

<span># View freqs_cis at some positions, just for display</span>
<span>token_to_show</span> <span>=</span> [<span>1</span>, <span>3</span>, <span>5</span>]  <span># View the 2nd, 4th, and 6th rows</span>
<span>fig</span>, <span>axs</span> <span>=</span> <span>plt</span>.<span>subplots</span>(<span>1</span>, <span>len</span>(<span>token_to_show</span>), <span>figsize</span><span>=</span>(<span>5</span> <span>*</span> <span>len</span>(<span>token_to_show</span>), <span>4</span>))  <span># Generate a figure window with 3 sub-plots in 1 row and 3 columns</span>
<span>for</span> <span>i</span>, <span>index</span> <span>in</span> <span>enumerate</span>(<span>token_to_show</span>):
    <span>value</span> <span>=</span> <span>freqs_cis</span>[<span>index</span>]
    <span>for</span> <span>j</span>, <span>element</span> <span>in</span> <span>enumerate</span>(<span>value</span>):
        <span># Plot a blue line from the origin to the coordinate point, with the real part as the x-coordinate and the imaginary part as the y-coordinate.</span>
        <span>axs</span>[<span>i</span>].<span>plot</span>([<span>0</span>, <span>element</span>.<span>real</span>], [<span>0</span>, <span>element</span>.<span>imag</span>], <span>color</span><span>=</span><span>'blue'</span>, <span>linewidth</span><span>=</span><span>1</span>, <span>label</span><span>=</span><span>f"Index: <span><span>{</span><span>j</span><span>}</span></span>"</span>)
        <span># Draw red numerical annotations to represent the i-th pair of dimensions.</span>
        <span>axs</span>[<span>i</span>].<span>annotate</span>(<span>f"<span><span>{</span><span>j</span><span>}</span></span>"</span>, <span>xy</span><span>=</span>(<span>element</span>.<span>real</span>, <span>element</span>.<span>imag</span>), <span>color</span><span>=</span><span>'red'</span>)
    <span>axs</span>[<span>i</span>].<span>set_xlabel</span>(<span>'Real'</span>)
    <span>axs</span>[<span>i</span>].<span>set_ylabel</span>(<span>'Imaginary'</span>)
    <span>axs</span>[<span>i</span>].<span>set_title</span>(<span>f'Plot of <span><span>{</span><span>index</span> <span>+</span> <span>1</span><span>}</span></span>th of freqs_cis'</span>)
<span>plt</span>.<span>show</span>()

<span>"""</span>
<span>Note: As shown in the figures, tokens in later positions have larger rotation angles, but within a single token, earlier vector dimensions have larger rotation angles.</span>
<span>      You can explore further to see if there are any mathematical reasons behind this if you are interested. X_X</span>
<span>"""</span></pre></div>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/output_47_1.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/output_47_1.png" alt="png"></a></p>
<div data-snippet-clipboard-copy-content="'\nNote: As shown in the figures, tokens in later positions have larger rotation angles, but within a single token, earlier vector dimensions have larger rotation angles.\n      You can explore further to see if there are any mathematical reasons behind this if you are interested. X_X\n'"><pre><code>'\nNote: As shown in the figures, tokens in later positions have larger rotation angles, but within a single token, earlier vector dimensions have larger rotation angles.\n      You can explore further to see if there are any mathematical reasons behind this if you are interested. X_X\n'
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">
Now we have provided a complex number (an angle-changing vector) for each dimension pair of the query vector corresponding to each token.
</h3><a id="user-content-now-we-have-provided-a-complex-number-an-angle-changing-vector-for-each-dimension-pair-of-the-query-vector-corresponding-to-each-token" aria-label="Permalink: 
Now we have provided a complex number (an angle-changing vector) for each dimension pair of the query vector corresponding to each token.
" href="#now-we-have-provided-a-complex-number-an-angle-changing-vector-for-each-dimension-pair-of-the-query-vector-corresponding-to-each-token"></a></p>
<br>
Now we can convert our query (the one divided into pairs) into complex numbers and then rotate these queries through dot-product calculation. :)
<div dir="auto" data-snippet-clipboard-copy-content="# Obtain (x + yi)
# That is, convert the dimension pairs into complex numbers. After the conversion, the shape of the dimensions will change from [17x64x2] to [17x64].
q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)  # [17x64x2] -> [17x64]
q_per_token_as_complex_numbers.shape"><pre><span># Obtain (x + yi)</span>
<span># That is, convert the dimension pairs into complex numbers. After the conversion, the shape of the dimensions will change from [17x64x2] to [17x64].</span>
<span>q_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>q_per_token_split_into_pairs</span>)  <span># [17x64x2] -&gt; [17x64]</span>
<span>q_per_token_as_complex_numbers</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Calculate (x + yi) * (cosmθ + sinmθi)
# That is, perform the rotation operation to obtain the final result.
q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis  # [17x64] * [17x64] = [17x64]
q_per_token_as_complex_numbers_rotated.shape"><pre><span># Calculate (x + yi) * (cosmθ + sinmθi)</span>
<span># That is, perform the rotation operation to obtain the final result.</span>
<span>q_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>q_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>  <span># [17x64] * [17x64] = [17x64]</span>
<span>q_per_token_as_complex_numbers_rotated</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">
Obtain the rotated vectors (restore the shape).
</h3><a id="user-content-obtain-the-rotated-vectors-restore-the-shape" aria-label="Permalink: 
Obtain the rotated vectors (restore the shape).
" href="#obtain-the-rotated-vectors-restore-the-shape"></a></p>
<br>
We can represent the complex numbers as real numbers again to obtain the query results in the form of dimension pairs.
<div dir="auto" data-snippet-clipboard-copy-content="# Convert the complex-number results back to the real-number dimension-pair form.
q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)  # [17x64] -> [17x64x2]
q_per_token_split_into_pairs_rotated.shape"><pre><span># Convert the complex-number results back to the real-number dimension-pair form.</span>
<span>q_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>q_per_token_as_complex_numbers_rotated</span>)  <span># [17x64] -&gt; [17x64x2]</span>
<span>q_per_token_split_into_pairs_rotated</span>.<span>shape</span></pre></div>

<p dir="auto">Merge the rotated dimensions. In this way, we obtain a new query vector (the rotated query vector) with a shape of [17x128], where 17 represents the number of tokens and 128 represents the dimension of the query vector.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Restore the dimension-pair results to the original form of the query vectors, and obtain the final query vector.
q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)  # [17x64x2] -> [17x128]
q_per_token_rotated.shape"><pre><span># Restore the dimension-pair results to the original form of the query vectors, and obtain the final query vector.</span>
<span>q_per_token_rotated</span> <span>=</span> <span>q_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>q_per_token</span>.<span>shape</span>)  <span># [17x64x2] -&gt; [17x128]</span>
<span>q_per_token_rotated</span>.<span>shape</span></pre></div>

<p dir="auto"><h4 tabindex="-1" dir="auto">Add positional information to the key vectors (same as the query)</h4><a id="user-content-add-positional-information-to-the-key-vectors-same-as-the-query" aria-label="Permalink: Add positional information to the key vectors (same as the query)" href="#add-positional-information-to-the-key-vectors-same-as-the-query"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Split the key vectors into pairs along the dimension direction to form dimension pairs (modify the shape).
k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)  # [17x128] -> [17x64x2]
k_per_token_split_into_pairs.shape"><pre><span># Split the key vectors into pairs along the dimension direction to form dimension pairs (modify the shape).</span>
<span>k_per_token_split_into_pairs</span> <span>=</span> <span>k_per_token</span>.<span>float</span>().<span>view</span>(<span>k_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)  <span># [17x128] -&gt; [17x64x2]</span>
<span>k_per_token_split_into_pairs</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Obtain (x + yi)
# That is, convert the dimension pairs into complex numbers. After the conversion, the shape of the dimensions will change from [17x64x2] to [17x64].
k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)  # [17x64x2] -> [17x64]
k_per_token_as_complex_numbers.shape"><pre><span># Obtain (x + yi)</span>
<span># That is, convert the dimension pairs into complex numbers. After the conversion, the shape of the dimensions will change from [17x64x2] to [17x64].</span>
<span>k_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>k_per_token_split_into_pairs</span>)  <span># [17x64x2] -&gt; [17x64]</span>
<span>k_per_token_as_complex_numbers</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Calculate (x + yi) * (cosmθ + sinmθi)
# That is, perform the rotation operation to obtain the final result.
# And convert the result back to the real-number form.
k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)  # [17x64] * [17x64] = [17x64] -> [17x64x2]
k_per_token_split_into_pairs_rotated.shape"><pre><span># Calculate (x + yi) * (cosmθ + sinmθi)</span>
<span># That is, perform the rotation operation to obtain the final result.</span>
<span># And convert the result back to the real-number form.</span>
<span>k_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>k_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>)  <span># [17x64] * [17x64] = [17x64] -&gt; [17x64x2]</span>
<span>k_per_token_split_into_pairs_rotated</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Restore the dimension-pair results to the original form of the key vectors, and obtain the final key vector.
k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)  # [17x64x2] -> [17x128]
k_per_token_rotated.shape"><pre><span># Restore the dimension-pair results to the original form of the key vectors, and obtain the final key vector.</span>
<span>k_per_token_rotated</span> <span>=</span> <span>k_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>k_per_token</span>.<span>shape</span>)  <span># [17x64x2] -&gt; [17x128]</span>
<span>k_per_token_rotated</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">
At this stage, we have the rotated query vectors and key vectors corresponding to each token.
</h3><a id="user-content-at-this-stage-we-have-the-rotated-query-vectors-and-key-vectors-corresponding-to-each-token" aria-label="Permalink: 
At this stage, we have the rotated query vectors and key vectors corresponding to each token.
" href="#at-this-stage-we-have-the-rotated-query-vectors-and-key-vectors-corresponding-to-each-token"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/keys0.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/keys0.png" width="600px"></a>
</p>
<p dir="auto">The shape of each query and key vector remains [17x128].</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Everything's ready. Let's start calculating the attention weights between tokens.</h3><a id="user-content-everythings-ready-lets-start-calculating-the-attention-weights-between-tokens" aria-label="Permalink: Everything's ready. Let's start calculating the attention weights between tokens." href="#everythings-ready-lets-start-calculating-the-attention-weights-between-tokens"></a></p>
<p dir="auto">This will involve a three-step process:</p>
<ol dir="auto">
<li>Calculate the attention scores: score = Q x K</li>
<li>Mask the future tokens: score = mask(score)</li>
<li>Calculate the attention weights: res = softmax(score)</li>
</ol>
<p dir="auto">Let's get started! :)</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Multiply the query and key vectors to obtain the attention scores.</h4><a id="user-content-multiply-the-query-and-key-vectors-to-obtain-the-attention-scores" aria-label="Permalink: Multiply the query and key vectors to obtain the attention scores." href="#multiply-the-query-and-key-vectors-to-obtain-the-attention-scores"></a></p>
<p dir="auto">In this way, we will get the score values between each token and all other tokens.
<br>
These scores represent how strongly each token's query relates to every other token's key.
<br>
This is the self-attention!
<br>
The shape of this attention score matrix (qk_per_token) is [17x17], where 17 is the number of tokens in the input prompt.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/qkmatmul.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/qkmatmul.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the attention score
# At the same time, perform normalization to prevent the subsequent softmax calculation results from being overly skewed towards 0 or 1,
# (the dot-product values may be too large when the dimensions are large),
# which could lead to vanishing gradients or exploding gradients, so as to maintain numerical stability.
qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(head_dim)**0.5  # [17x128] x [128x17] = [17x17]
qk_per_token.shape"><pre><span># Calculate the attention score</span>
<span># At the same time, perform normalization to prevent the subsequent softmax calculation results from being overly skewed towards 0 or 1,</span>
<span># (the dot-product values may be too large when the dimensions are large),</span>
<span># which could lead to vanishing gradients or exploding gradients, so as to maintain numerical stability.</span>
<span>qk_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>q_per_token_rotated</span>, <span>k_per_token_rotated</span>.<span>T</span>)<span>/</span>(<span>head_dim</span>)<span>**</span><span>0.5</span>  <span># [17x128] x [128x17] = [17x17]</span>
<span>qk_per_token</span>.<span>shape</span></pre></div>

<p dir="auto"><h4 tabindex="-1" dir="auto">Now we must mask the future query-key scores.</h4><a id="user-content-now-we-must-mask-the-future-query-key-scores" aria-label="Permalink: Now we must mask the future query-key scores." href="#now-we-must-mask-the-future-query-key-scores"></a></p>
<p dir="auto">During the training process of Llama 3, the QK scores of future tokens will be masked.
<br>
Why? Because during training, we only learn how to use past tokens to predict the current token. If we don't mask future tokens, it will lead to the leakage of prediction information.
<br>
Therefore, during the inference process, we also need to set the future tokens to 0 (to ensure the logical consistency between the training and inference processes).
<br></p>
<p dir="auto">Of course, if you're as curious as I am about what would happen without masking, you can check the results of the additional experiment I conducted in the last section after you've finished learning. (^_&lt;)</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/mask.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/mask.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# First, take a look at the score matrix before masking
def display_qk_heatmap(qk_per_token):
    _, ax = plt.subplots()  # Create a figure window

    # `imshow` is commonly used to display data in the form of a two-dimensional array or matrix,
    # it maps the matrix elements to grayscale or color values, so it can be used to draw heatmaps.
    # Convert the tensor back to full precision, then detach it from the computational graph to avoid potential gradient calculation and storage issues.
    # Specify to use the 'viridis' color mapping scheme to display the image (blue -> green -> yellow).
    im = ax.imshow(qk_per_token.to(float).detach(), cmap='viridis')

    # Set the number and labels of the x and y axis ticks to ensure correct one-to-one correspondence.
    ax.set_xticks(range(len(prompt_split_as_tokens)))
    ax.set_yticks(range(len(prompt_split_as_tokens)))
    ax.set_xticklabels(prompt_split_as_tokens)
    ax.set_yticklabels(prompt_split_as_tokens)

    # Add a color bar on the side.
    # Specify `im` to identify the correct color mapping and value range.
    # Specify the sub-plot it belongs to as `ax` (if there are multiple sub-plots, it would be `ax = ax[i]`).
    ax.figure.colorbar(im, ax=ax)
    
display_qk_heatmap(qk_per_token)"><pre><span># First, take a look at the score matrix before masking</span>
<span>def</span> <span>display_qk_heatmap</span>(<span>qk_per_token</span>):
    <span>_</span>, <span>ax</span> <span>=</span> <span>plt</span>.<span>subplots</span>()  <span># Create a figure window</span>

    <span># `imshow` is commonly used to display data in the form of a two-dimensional array or matrix,</span>
    <span># it maps the matrix elements to grayscale or color values, so it can be used to draw heatmaps.</span>
    <span># Convert the tensor back to full precision, then detach it from the computational graph to avoid potential gradient calculation and storage issues.</span>
    <span># Specify to use the 'viridis' color mapping scheme to display the image (blue -&gt; green -&gt; yellow).</span>
    <span>im</span> <span>=</span> <span>ax</span>.<span>imshow</span>(<span>qk_per_token</span>.<span>to</span>(<span>float</span>).<span>detach</span>(), <span>cmap</span><span>=</span><span>'viridis'</span>)

    <span># Set the number and labels of the x and y axis ticks to ensure correct one-to-one correspondence.</span>
    <span>ax</span>.<span>set_xticks</span>(<span>range</span>(<span>len</span>(<span>prompt_split_as_tokens</span>)))
    <span>ax</span>.<span>set_yticks</span>(<span>range</span>(<span>len</span>(<span>prompt_split_as_tokens</span>)))
    <span>ax</span>.<span>set_xticklabels</span>(<span>prompt_split_as_tokens</span>)
    <span>ax</span>.<span>set_yticklabels</span>(<span>prompt_split_as_tokens</span>)

    <span># Add a color bar on the side.</span>
    <span># Specify `im` to identify the correct color mapping and value range.</span>
    <span># Specify the sub-plot it belongs to as `ax` (if there are multiple sub-plots, it would be `ax = ax[i]`).</span>
    <span>ax</span>.<span>figure</span>.<span>colorbar</span>(<span>im</span>, <span>ax</span><span>=</span><span>ax</span>)
    
<span>display_qk_heatmap</span>(<span>qk_per_token</span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/output_65_0.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/output_65_0.png" alt="png"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Generate the masking matrix
# Set the positions of the elements to be masked to negative infinity, and set the positions that do not need to be masked to 0.
# Then, add it to the score matrix to achieve the masking effect (negative infinity will tend to 0 when calculating the softmax).

# `torch.full` is used to generate a tensor with a specified shape and filling value.
# Here, a [17x17] matrix filled with negative infinity is first generated.
# Specify that the device of this matrix is the same as that of the previous tokens to ensure that there are no errors in subsequent calculations,
# for example, if the previous tokens are on the GPU and the device is not specified here, the `mask` will be newly created on the CPU, and an error will occur when adding the two.
mask = torch.full((len(tokens), len(tokens)), float(&quot;-inf&quot;), device=tokens.device)  # [17x17]

# `torch.triu` is used to return the upper-triangular part of the matrix, and set the rest to 0 (use `torch.tril` to get the lower-triangular part).
# `diagonal` is the offset of the diagonal. When it's 1, it means taking the upper-triangular part starting from 1 position above the main diagonal to avoid masking the token itself.
mask = torch.triu(mask, diagonal=1)  # [17x17]

mask, mask.shape"><pre><span># Generate the masking matrix</span>
<span># Set the positions of the elements to be masked to negative infinity, and set the positions that do not need to be masked to 0.</span>
<span># Then, add it to the score matrix to achieve the masking effect (negative infinity will tend to 0 when calculating the softmax).</span>

<span># `torch.full` is used to generate a tensor with a specified shape and filling value.</span>
<span># Here, a [17x17] matrix filled with negative infinity is first generated.</span>
<span># Specify that the device of this matrix is the same as that of the previous tokens to ensure that there are no errors in subsequent calculations,</span>
<span># for example, if the previous tokens are on the GPU and the device is not specified here, the `mask` will be newly created on the CPU, and an error will occur when adding the two.</span>
<span>mask</span> <span>=</span> <span>torch</span>.<span>full</span>((<span>len</span>(<span>tokens</span>), <span>len</span>(<span>tokens</span>)), <span>float</span>(<span>"-inf"</span>), <span>device</span><span>=</span><span>tokens</span>.<span>device</span>)  <span># [17x17]</span>

<span># `torch.triu` is used to return the upper-triangular part of the matrix, and set the rest to 0 (use `torch.tril` to get the lower-triangular part).</span>
<span># `diagonal` is the offset of the diagonal. When it's 1, it means taking the upper-triangular part starting from 1 position above the main diagonal to avoid masking the token itself.</span>
<span>mask</span> <span>=</span> <span>torch</span>.<span>triu</span>(<span>mask</span>, <span>diagonal</span><span>=</span><span>1</span>)  <span># [17x17]</span>

<span>mask</span>, <span>mask</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="(tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),
 torch.Size([17, 17]))"><pre><code>(tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),
 torch.Size([17, 17]))
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Mask the scores of future tokens
qk_per_token_after_masking = qk_per_token + mask  # [17x17] + [17x17] = [17x17]
display_qk_heatmap(qk_per_token_after_masking)  # Display the attention scores after masking"><pre><span># Mask the scores of future tokens</span>
<span>qk_per_token_after_masking</span> <span>=</span> <span>qk_per_token</span> <span>+</span> <span>mask</span>  <span># [17x17] + [17x17] = [17x17]</span>
<span>display_qk_heatmap</span>(<span>qk_per_token_after_masking</span>)  <span># Display the attention scores after masking</span></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/output_67_0.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/output_67_0.png" alt="png"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Calculate the final attention weights, that is, softmax(score).</h4><a id="user-content-calculate-the-final-attention-weights-that-is-softmaxscore" aria-label="Permalink: Calculate the final attention weights, that is, softmax(score)." href="#calculate-the-final-attention-weights-that-is-softmaxscore"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/softmax.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/softmax.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the attention weights
# That is, calculate the softmax values of the scores.
# `dim = 1` indicates that the softmax calculation is performed row-by-row, and the result is converted to half-precision to be consistent with the subsequent value vectors.
qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)  # [17x17] -> [17x17]
display_qk_heatmap(qk_per_token_after_masking_after_softmax)"><pre><span># Calculate the attention weights</span>
<span># That is, calculate the softmax values of the scores.</span>
<span># `dim = 1` indicates that the softmax calculation is performed row-by-row, and the result is converted to half-precision to be consistent with the subsequent value vectors.</span>
<span>qk_per_token_after_masking_after_softmax</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>functional</span>.<span>softmax</span>(<span>qk_per_token_after_masking</span>, <span>dim</span><span>=</span><span>1</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)  <span># [17x17] -&gt; [17x17]</span>
<span>display_qk_heatmap</span>(<span>qk_per_token_after_masking_after_softmax</span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/output_69_0.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/output_69_0.png" alt="png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Finally! Calculate the final result of the single-head attention mechanism!</h3><a id="user-content-finally-calculate-the-final-result-of-the-single-head-attention-mechanism" aria-label="Permalink: Finally! Calculate the final result of the single-head attention mechanism!" href="#finally-calculate-the-final-result-of-the-single-head-attention-mechanism"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/attention.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/attention.png" width="600px"></a>
</p>
<p dir="auto">Principle: The previous attention weights (ranging from 0 to 1) are used to determine what proportion of each value vector should be used for each token (i.e., to weight the value vectors).</p>
<p dir="auto">Example: If the input consists of 3 tokens, the attention result of the first token might be: res = 0.6 * value_1 + 0.3 * value_2 + 0.1 * value_3</p>
<p dir="auto">The shape of the attention result after the multiplication of the weight matrix and the value matrix is [17x128].</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the final result of the single-head attention
qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)  # [17x17] x [17x128] = [17x128]
qkv_attention.shape"><pre><span># Calculate the final result of the single-head attention</span>
<span>qkv_attention</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>qk_per_token_after_masking_after_softmax</span>, <span>v_per_token</span>)  <span># [17x17] x [17x128] = [17x128]</span>
<span>qkv_attention</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Calculate the multi-head attention mechanism (a simple loop to repeat the above process)</h2><a id="user-content-calculate-the-multi-head-attention-mechanism-a-simple-loop-to-repeat-the-above-process" aria-label="Permalink: Calculate the multi-head attention mechanism (a simple loop to repeat the above process)" href="#calculate-the-multi-head-attention-mechanism-a-simple-loop-to-repeat-the-above-process"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/heads.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/heads.png" width="600px"></a>
</p>
<p dir="auto">We now have the attention values for the first head of the first layer.
<br></p>
<div dir="auto"><p>Now we need to run a loop to perform exactly the same mathematical process as in the previous cell, but for each head in the first layer.
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">
It's worth noting that in the <a href="https://github.com/meta-llama/llama3/blob/main/llama/model.py#L90">official Llama3 code implementation</a>, the multi-head attention calculation adopts the method of one-time matrix multiplication instead of time consuming for-loop calculations. The general process is as follows:
</h3><a id="user-content-its-worth-noting-that-in-the-official-llama3-code-implementation-the-multi-head-attention-calculation-adopts-the-method-of-one-time-matrix-multiplication-instead-of-time-consuming-for-loop-calculations-the-general-process-is-as-follows" aria-label="Permalink: 
It's worth noting that in the official Llama3 code implementation, the multi-head attention calculation adopts the method of one-time matrix multiplication instead of time consuming for-loop calculations. The general process is as follows:
" href="#its-worth-noting-that-in-the-official-llama3-code-implementation-the-multi-head-attention-calculation-adopts-the-method-of-one-time-matrix-multiplication-instead-of-time-consuming-for-loop-calculations-the-general-process-is-as-follows"></a></p>
<ol dir="auto">
<li>Based on matrix parallelism, calculate the QKV vectors: [17x4096] x [4096x4096] or [4096x1024] = [17x4096] or [17x1024], and then reshape them to [32x17x128] or [8x17x128].</li>
<li>After obtaining the QKV vectors, duplicate the internal parts of the K and V vectors to make their shapes consistent with the Q vector. At this time, the shapes of all of them are [32x17x128].</li>
<li>When calculating the scores, use the transpose method to exchange the positions of the last two dimensions of the tensors to complete the matrix multiplication. For example, <code>torch.matmul(q, k.transpose(1,2)) / head_dim ** 0.5</code>. At this time, it is [32x17x128] x [32x128x17] = [32x17x17].</li>
<li>The same principle applies to other matrix calculations.</li>
</ol>
<p dir="auto">Note: The matrix shape changes in each step of the above process are simplified versions, only for illustration to facilitate understanding, which are different from the change process in the official Llama3 implementation (the official implementation involves a large number of shape change processes).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Calculate the result for each head</h3><a id="user-content-calculate-the-result-for-each-head" aria-label="Permalink: Calculate the result for each head" href="#calculate-the-result-for-each-head"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the multi-head attention results
# That is, a loop of the previous single-head attention calculation process
qkv_attention_store = []

for head in range(n_heads):
    # Extract the QKV weight matrices corresponding to the current head
    q_layer0_head = q_layer0[head]  # [32x128x4096] -> [128x4096]
    k_layer0_head = k_layer0[head//4]  # Every 4 heads share one key weight, [8x128x4096] -> [128x4096]
    v_layer0_head = v_layer0[head//4]  # Every 4 heads share one value weight, [8x128x4096] -> [128x4096]
    
    # Calculate XW to obtain the QKV vectors
    # [17x4096] x [4096x128] = [17x128]
    q_per_token = torch.matmul(token_embeddings, q_layer0_head.T)
    k_per_token = torch.matmul(token_embeddings, k_layer0_head.T)
    v_per_token = torch.matmul(token_embeddings, v_layer0_head.T)
    
    # Add position information to the query vector (RoPE)
    q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)  # Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -> [17x64x2]
    q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)  # Convert to complex number representation, (x,y) -> (x+yi). [17x64x2] -> [17x64]
    q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis[:len(tokens)]  # Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]
    q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)  # Convert the result back to real number representation, (x+yi) -> (x,y). [17x64] -> [17x64x2]
    q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)  # Convert the result back to the original vector shape to obtain the final query vector. [17x64x2] -> [17x128]

    # Add position information to the key vector (RoPE)
    k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)  # Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -> [17x64x2]
    k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)  # Convert to complex number representation, (x,y) -> (x+yi). [17x64x2] -> [17x64]
    k_per_token_as_complex_numbers_rotated = k_per_token_as_complex_numbers * freqs_cis[:len(tokens)]  # Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]
    k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers_rotated)  # Convert the result back to real number representation, (x+yi) -> (x,y). [17x64] -> [17x64x2]
    k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)  # Convert the result back to the original vector shape to obtain the final key vector. [17x64x2] -> [17x128]

    # Calculate the attention scores and normalize the scores simultaneously (i.e., Q×K/sqrt(dim))
    qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(head_dim)**0.5  # [17x128] x [128x17] = [17x17]
    
    # Mask the scores of future tokens
    mask = torch.full(qk_per_token.shape, float(&quot;-inf&quot;), device=tokens.device)  # Create a matrix with the same shape as the attention scores, filled with negative infinity, and stored in the same device as other vectors to prevent errors in subsequent calculations. [17x17]
    mask = torch.triu(mask, diagonal=1)  # Keep the negative infinity in the upper-triangular part and set others to 0 (i.e., the upper-triangular area represents future tokens that need to be masked). The diagonal offset is 1 to avoid masking the token itself. [17x17]
    qk_per_token_after_masking = qk_per_token + mask  # Add the attention scores with the masking matrix, making the upper-triangular part of the score matrix become negative infinity, which will tend to 0 after the subsequent softmax operation. [17x17]
    
    # Calculate the attention weights (i.e., softmax(score))
    # Meanwhile, convert it back to half-precision (because it will be multiplied with the value vector v_per_token later, so the data types need to be the same).
    qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)  # Calculate the softmax row-by-row. [17x17]
    
    # Calculate the final result of the attention mechanism (i.e., softmax(score) × V)
    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)  # [17x17] × [17x128] = [17x128]
    
    # Record the result of this head
    qkv_attention_store.append(qkv_attention)

len(qkv_attention_store)"><pre><span># Calculate the multi-head attention results</span>
<span># That is, a loop of the previous single-head attention calculation process</span>
<span>qkv_attention_store</span> <span>=</span> []

<span>for</span> <span>head</span> <span>in</span> <span>range</span>(<span>n_heads</span>):
    <span># Extract the QKV weight matrices corresponding to the current head</span>
    <span>q_layer0_head</span> <span>=</span> <span>q_layer0</span>[<span>head</span>]  <span># [32x128x4096] -&gt; [128x4096]</span>
    <span>k_layer0_head</span> <span>=</span> <span>k_layer0</span>[<span>head</span><span>//</span><span>4</span>]  <span># Every 4 heads share one key weight, [8x128x4096] -&gt; [128x4096]</span>
    <span>v_layer0_head</span> <span>=</span> <span>v_layer0</span>[<span>head</span><span>//</span><span>4</span>]  <span># Every 4 heads share one value weight, [8x128x4096] -&gt; [128x4096]</span>
    
    <span># Calculate XW to obtain the QKV vectors</span>
    <span># [17x4096] x [4096x128] = [17x128]</span>
    <span>q_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>q_layer0_head</span>.<span>T</span>)
    <span>k_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>k_layer0_head</span>.<span>T</span>)
    <span>v_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>v_layer0_head</span>.<span>T</span>)
    
    <span># Add position information to the query vector (RoPE)</span>
    <span>q_per_token_split_into_pairs</span> <span>=</span> <span>q_per_token</span>.<span>float</span>().<span>view</span>(<span>q_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)  <span># Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -&gt; [17x64x2]</span>
    <span>q_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>q_per_token_split_into_pairs</span>)  <span># Convert to complex number representation, (x,y) -&gt; (x+yi). [17x64x2] -&gt; [17x64]</span>
    <span>q_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>q_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>[:<span>len</span>(<span>tokens</span>)]  <span># Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]</span>
    <span>q_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>q_per_token_as_complex_numbers_rotated</span>)  <span># Convert the result back to real number representation, (x+yi) -&gt; (x,y). [17x64] -&gt; [17x64x2]</span>
    <span>q_per_token_rotated</span> <span>=</span> <span>q_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>q_per_token</span>.<span>shape</span>)  <span># Convert the result back to the original vector shape to obtain the final query vector. [17x64x2] -&gt; [17x128]</span>

    <span># Add position information to the key vector (RoPE)</span>
    <span>k_per_token_split_into_pairs</span> <span>=</span> <span>k_per_token</span>.<span>float</span>().<span>view</span>(<span>k_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)  <span># Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -&gt; [17x64x2]</span>
    <span>k_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>k_per_token_split_into_pairs</span>)  <span># Convert to complex number representation, (x,y) -&gt; (x+yi). [17x64x2] -&gt; [17x64]</span>
    <span>k_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>k_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>[:<span>len</span>(<span>tokens</span>)]  <span># Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]</span>
    <span>k_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>k_per_token_as_complex_numbers_rotated</span>)  <span># Convert the result back to real number representation, (x+yi) -&gt; (x,y). [17x64] -&gt; [17x64x2]</span>
    <span>k_per_token_rotated</span> <span>=</span> <span>k_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>k_per_token</span>.<span>shape</span>)  <span># Convert the result back to the original vector shape to obtain the final key vector. [17x64x2] -&gt; [17x128]</span>

    <span># Calculate the attention scores and normalize the scores simultaneously (i.e., Q×K/sqrt(dim))</span>
    <span>qk_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>q_per_token_rotated</span>, <span>k_per_token_rotated</span>.<span>T</span>)<span>/</span>(<span>head_dim</span>)<span>**</span><span>0.5</span>  <span># [17x128] x [128x17] = [17x17]</span>
    
    <span># Mask the scores of future tokens</span>
    <span>mask</span> <span>=</span> <span>torch</span>.<span>full</span>(<span>qk_per_token</span>.<span>shape</span>, <span>float</span>(<span>"-inf"</span>), <span>device</span><span>=</span><span>tokens</span>.<span>device</span>)  <span># Create a matrix with the same shape as the attention scores, filled with negative infinity, and stored in the same device as other vectors to prevent errors in subsequent calculations. [17x17]</span>
    <span>mask</span> <span>=</span> <span>torch</span>.<span>triu</span>(<span>mask</span>, <span>diagonal</span><span>=</span><span>1</span>)  <span># Keep the negative infinity in the upper-triangular part and set others to 0 (i.e., the upper-triangular area represents future tokens that need to be masked). The diagonal offset is 1 to avoid masking the token itself. [17x17]</span>
    <span>qk_per_token_after_masking</span> <span>=</span> <span>qk_per_token</span> <span>+</span> <span>mask</span>  <span># Add the attention scores with the masking matrix, making the upper-triangular part of the score matrix become negative infinity, which will tend to 0 after the subsequent softmax operation. [17x17]</span>
    
    <span># Calculate the attention weights (i.e., softmax(score))</span>
    <span># Meanwhile, convert it back to half-precision (because it will be multiplied with the value vector v_per_token later, so the data types need to be the same).</span>
    <span>qk_per_token_after_masking_after_softmax</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>functional</span>.<span>softmax</span>(<span>qk_per_token_after_masking</span>, <span>dim</span><span>=</span><span>1</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)  <span># Calculate the softmax row-by-row. [17x17]</span>
    
    <span># Calculate the final result of the attention mechanism (i.e., softmax(score) × V)</span>
    <span>qkv_attention</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>qk_per_token_after_masking_after_softmax</span>, <span>v_per_token</span>)  <span># [17x17] × [17x128] = [17x128]</span>
    
    <span># Record the result of this head</span>
    <span>qkv_attention_store</span>.<span>append</span>(<span>qkv_attention</span>)

<span>len</span>(<span>qkv_attention_store</span>)</pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">Merge the results of each head into a large matrix</h3><a id="user-content-merge-the-results-of-each-head-into-a-large-matrix" aria-label="Permalink: Merge the results of each head into a large matrix" href="#merge-the-results-of-each-head-into-a-large-matrix"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/stacked.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/stacked.png" width="600px"></a>
</p>
Now we have the results of the attention mechanism for all 32 heads in the first layer. Next, we'll merge all the attention values into a large matrix with a shape of [17x4096].
<br>
We're almost done with the calculation of the attention layer :)
<div dir="auto" data-snippet-clipboard-copy-content="# Merge the multi-head attention matrices
stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)  # Concatenate along the second dimension, 32x[17x128] -> [17x4096]
stacked_qkv_attention.shape"><pre><span># Merge the multi-head attention matrices</span>
<span>stacked_qkv_attention</span> <span>=</span> <span>torch</span>.<span>cat</span>(<span>qkv_attention_store</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>)  <span># Concatenate along the second dimension, 32x[17x128] -&gt; [17x4096]</span>
<span>stacked_qkv_attention</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">Head-to-head information interaction (linear mapping), the final step of the self-attention layer!</h3><a id="user-content-head-to-head-information-interaction-linear-mapping-the-final-step-of-the-self-attention-layer" aria-label="Permalink: Head-to-head information interaction (linear mapping), the final step of the self-attention layer!" href="#head-to-head-information-interaction-linear-mapping-the-final-step-of-the-self-attention-layer"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/weightmatrix.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/weightmatrix.png" width="600px"></a>
</p>
The last step of the attention calculation for layer0 is to perform the final linear mapping, that is, multiply the combined attention matrix by the output weight matrix.
<div dir="auto" data-snippet-clipboard-copy-content="# Load the output weight matrix of layers.0
w_layer0 = model[&quot;layers.0.attention.wo.weight&quot;]  # [4096x4096]
w_layer0.shape"><pre><span># Load the output weight matrix of layers.0</span>
<span>w_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wo.weight"</span>]  <span># [4096x4096]</span>
<span>w_layer0</span>.<span>shape</span></pre></div>

<p dir="auto">This is just a simple linear layer, so we only need matrix multiplication.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Perform the linear mapping of the attention matrix
# This is the final output of the attention layer
embedding_delta = torch.matmul(stacked_qkv_attention, w_layer0.T)  # [17x4096] x [4096x4096] = [17x4096]
embedding_delta.shape"><pre><span># Perform the linear mapping of the attention matrix</span>
<span># This is the final output of the attention layer</span>
<span>embedding_delta</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>stacked_qkv_attention</span>, <span>w_layer0</span>.<span>T</span>)  <span># [17x4096] x [4096x4096] = [17x4096]</span>
<span>embedding_delta</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Perform the residual operation (add)</h2><a id="user-content-perform-the-residual-operation-add" aria-label="Permalink: Perform the residual operation (add)" href="#perform-the-residual-operation-add"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/afterattention.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/afterattention.png" width="600px"></a>
</p>
Now we have the value of the input vector after the attention mechanism is applied. At this time, we need to add the original input vector to it (i.e., the residual operation, to ensure that information is not easily lost and alleviate the problem of gradient vanishing).
<div dir="auto" data-snippet-clipboard-copy-content="# Add the output of the attention layer to the original input to complete the residual operation
embedding_after_edit = token_embeddings_unnormalized + embedding_delta  # [17x4096] + [17x4096] = [17x4096]
embedding_after_edit.shape"><pre><span># Add the output of the attention layer to the original input to complete the residual operation</span>
<span>embedding_after_edit</span> <span>=</span> <span>token_embeddings_unnormalized</span> <span>+</span> <span>embedding_delta</span>  <span># [17x4096] + [17x4096] = [17x4096]</span>
<span>embedding_after_edit</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Perform the second normalization operation</h2><a id="user-content-perform-the-second-normalization-operation" aria-label="Permalink: Perform the second normalization operation" href="#perform-the-second-normalization-operation"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/norm_after.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/norm_after.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Normalize the result of the residual operation
embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[&quot;layers.0.ffn_norm.weight&quot;])  # [17x4096] &amp; [4096] -> [17x4096]
embedding_after_edit_normalized.shape"><pre><span># Normalize the result of the residual operation</span>
<span>embedding_after_edit_normalized</span> <span>=</span> <span>rms_norm</span>(<span>embedding_after_edit</span>, <span>model</span>[<span>"layers.0.ffn_norm.weight"</span>])  <span># [17x4096] &amp; [4096] -&gt; [17x4096]</span>
<span>embedding_after_edit_normalized</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Perform the calculation of the FFN (Feed-Forward Neural Network) layer</h2><a id="user-content-perform-the-calculation-of-the-ffn-feed-forward-neural-network-layer" aria-label="Permalink: Perform the calculation of the FFN (Feed-Forward Neural Network) layer" href="#perform-the-calculation-of-the-ffn-feed-forward-neural-network-layer"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/swiglu.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/swiglu.png" width="600px"></a>
</p>
<br>
In Llama3, they used the SwiGLU feed-forward network. This network architecture can effectively increase nonlinear characteristics when the model needs.
<br>
Nowadays, this kind of feed-forward network architecture is very common in large language models.
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Introduce Nonlinear Layers:</h3><a id="user-content-why-introduce-nonlinear-layers" aria-label="Permalink: Why Introduce Nonlinear Layers:" href="#why-introduce-nonlinear-layers"></a></p>
<ul dir="auto">
<li>The Nonlinearity is at the core of why neural network models can be considered "universal function approximators". In traditional neural network models, we use nonlinear activation functions (such as sigmoid, ReLU, etc.) to increase the model's expressive power, enabling it to fit the complex patterns hidden in the training data.</li>
<li>However, in the Transformer, the attention mechanism is essentially a linear weighted sum of the value vectors (even though the weights are obtained through nonlinear calculation of the softmax function, it's still just a linear weighting for the values). Therefore, although it can capture global dependencies, its output is still only a linear combination of the input. At this time, the Transformer model is actually lacks nonlinear capabilities.</li>
<li>So, it is necessary to add an FFN network after the self-attention layer to introduce nonlinear transformation capabilities to the model, thus improving the model's ability to model complex semantic relationships.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Generally, introducing nonlinear layers can play the following roles:</h3><a id="user-content-generally-introducing-nonlinear-layers-can-play-the-following-roles" aria-label="Permalink: Generally, introducing nonlinear layers can play the following roles:" href="#generally-introducing-nonlinear-layers-can-play-the-following-roles"></a></p>
<ol dir="auto">
<li>Add nonlinear capabilities to the model to facilitate the model's learning and training.</li>
<li>Enhance the model's information abstraction ability, enabling the model to represent data features and patterns at different levels during the layer-by-layer learning process. For example, the lower-layer networks can identify basic language structures (such as part-of-speech), while the higher-layer networks can understand more complex semantic information (such as sentiment, intention).</li>
<li>In addition, a current view holds that the attention layer is mainly used for input context interaction, while the FFN layer is where the LLMs mainly stores and remembers general knowledge during training (given to its nonlinear representation ability), so that it can find answers to input questions from general knowledge.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">SwiGLU Network Structure:</h3><a id="user-content-swiglu-network-structure" aria-label="Permalink: SwiGLU Network Structure:" href="#swiglu-network-structure"></a></p>
<ol dir="auto">
<li>Perform a linear transformation on the input: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$X^\prime = XW_3$</math-renderer>
</li>
<li>Gating unit: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$GATE = Activation\_Function(XW_1)$</math-renderer>, which is used to selectively pass information. That is, assuming that the information in <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$X^\prime$</math-renderer> has different importance, so the information should be weighted and passed based on the score of the gating unit, thus improving the expressive ability of the model.</li>
<li>The activation function used is a Swish activation function (hence the network is called SwiGLU, which is a combination of the Swish activation function and the Gated Linear Unit (GLU)). The formula is: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$Swish = X \cdot \sigma(\beta X)$</math-renderer>, where <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\sigma$</math-renderer> is the sigmoid activation function. In SwiGLU, <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\beta$</math-renderer> is set to 1 (in the original formula, it is a learnable parameter).</li>
<li>Therefore, the specific calculation of the gating unit is: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$GATE = XW_1 \cdot \sigma(XW_1)$</math-renderer>. In PyTorch, this activation function is called silu, that is <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$GATE = silu(XW_1)$</math-renderer>.</li>
<li>Application of the gating mechanism: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$X^\prime = X^\prime \cdot GATE$</math-renderer>
</li>
<li>Perform a linear transformation again: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$Y = X^\prime W_2$</math-renderer>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Calculation of the Dimension Size of the Hidden Layer in the Feed-Forward Layer (Based on the Official Implementation Process of Llama3):</h3><a id="user-content-calculation-of-the-dimension-size-of-the-hidden-layer-in-the-feed-forward-layer-based-on-the-official-implementation-process-of-llama3" aria-label="Permalink: Calculation of the Dimension Size of the Hidden Layer in the Feed-Forward Layer (Based on the Official Implementation Process of Llama3):" href="#calculation-of-the-dimension-size-of-the-hidden-layer-in-the-feed-forward-layer-based-on-the-official-implementation-process-of-llama3"></a></p>
<ol dir="auto">
<li>Input dimension is dim = 4096</li>
<li>hidden_dim = 4 * dim = 16384  # First, magnify it by four times. When initializing the feed-forward layer in the Transformer block, the input hidden_dim is multiplied by four.</li>
<li>hidden_dim = int(2 * hidden_dim / 3) = 10922 # Then, magnify it by 2/3 times. Such scaling is first performed within the feed-forward layer.</li>
<li>hidden_dim = int(ffn_dim_multiplier * hidden_dim) = int(1.3 * 10922) = 14198  # Then, magnify it by ffn_dim_multiplier times. The ffn_dim_multiplier is defined as 1.3 in the model configuration file.</li>
<li>hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of) = 1024 * ((14198 + 1024 - 1) // 1024) = 14336  # Adjust it to an integer multiple of multiple_of. The multiple_of is defined as 1024 in the model configuration file to ensure that the dimensions of all hidden layers in the model are multiples of 1024, so as to improve the computational efficiency.</li>
<li>Finally, we get the dimension size of the hidden layer is 14336.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the feed-forward network layer
# The dimension size of the hidden layer is 14336
w1 = model[&quot;layers.0.feed_forward.w1.weight&quot;]  # [14336x4096]
w3 = model[&quot;layers.0.feed_forward.w3.weight&quot;]  # [14336x4096]
w2 = model[&quot;layers.0.feed_forward.w2.weight&quot;]  # [4096x14336]
print(w1.shape, w3.shape, w2.shape)

# output = (silu(XW1) * XW3)W2
# [17x4096] x [4096x14336] x [14336x4096] = [17x4096]
output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)
output_after_feedforward.shape"><pre><span># Calculate the feed-forward network layer</span>
<span># The dimension size of the hidden layer is 14336</span>
<span>w1</span> <span>=</span> <span>model</span>[<span>"layers.0.feed_forward.w1.weight"</span>]  <span># [14336x4096]</span>
<span>w3</span> <span>=</span> <span>model</span>[<span>"layers.0.feed_forward.w3.weight"</span>]  <span># [14336x4096]</span>
<span>w2</span> <span>=</span> <span>model</span>[<span>"layers.0.feed_forward.w2.weight"</span>]  <span># [4096x14336]</span>
<span>print</span>(<span>w1</span>.<span>shape</span>, <span>w3</span>.<span>shape</span>, <span>w2</span>.<span>shape</span>)

<span># output = (silu(XW1) * XW3)W2</span>
<span># [17x4096] x [4096x14336] x [14336x4096] = [17x4096]</span>
<span>output_after_feedforward</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>torch</span>.<span>functional</span>.<span>F</span>.<span>silu</span>(<span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w1</span>.<span>T</span>)) <span>*</span> <span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w3</span>.<span>T</span>), <span>w2</span>.<span>T</span>)
<span>output_after_feedforward</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([14336, 4096]) torch.Size([14336, 4096]) torch.Size([4096, 14336]) torch.Size([17, 4096])"><pre><code>torch.Size([14336, 4096]) torch.Size([14336, 4096]) torch.Size([4096, 14336]) torch.Size([17, 4096])
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Perform the residual operation again (Finally, we get the final output of the Transformer block!)</h2><a id="user-content-perform-the-residual-operation-again-finally-we-get-the-final-output-of-the-transformer-block" aria-label="Permalink: Perform the residual operation again (Finally, we get the final output of the Transformer block!)" href="#perform-the-residual-operation-again-finally-we-get-the-final-output-of-the-transformer-block"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Add the output of the feed-forward layer to the original input to complete the residual operation
# This is the final result of a Transformer block
layer_0_embedding = embedding_after_edit+output_after_feedforward  # [17x4096] + [17x4096] = [17x4096]
layer_0_embedding.shape"><pre><span># Add the output of the feed-forward layer to the original input to complete the residual operation</span>
<span># This is the final result of a Transformer block</span>
<span>layer_0_embedding</span> <span>=</span> <span>embedding_after_edit</span><span>+</span><span>output_after_feedforward</span>  <span># [17x4096] + [17x4096] = [17x4096]</span>
<span>layer_0_embedding</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">
Finally, we have the new embeddings of each token after passing through the first layer.
</h3><a id="user-content-finally-we-have-the-new-embeddings-of-each-token-after-passing-through-the-first-layer" aria-label="Permalink: 
Finally, we have the new embeddings of each token after passing through the first layer.
" href="#finally-we-have-the-new-embeddings-of-each-token-after-passing-through-the-first-layer"></a></p>
<br>
There are only 31 layers left to complete (just one for loop away).
<br>
You can imagine that this processed embedding contains all the information of the tokens proposed in the first layer.
<br>
Now, each layer will encode more complex queries in the asked question. Until the end, we will get an embedding that knows all the information about the next token we need.
<p dir="auto"><h2 tabindex="-1" dir="auto">Everything is here. Let's complete the calculation of all 32 Transformer blocks. Happy reading :)</h2><a id="user-content-everything-is-here-lets-complete-the-calculation-of-all-32-transformer-blocks-happy-reading-" aria-label="Permalink: Everything is here. Let's complete the calculation of all 32 Transformer blocks. Happy reading :)" href="#everything-is-here-lets-complete-the-calculation-of-all-32-transformer-blocks-happy-reading-"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/god.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/god.png" width="600px"></a>
</p>
<p dir="auto">Yes, that's it. All the work we've done before will be presented here at once to complete the calculation of each layer.
<br></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Now, let's start to complete the calculation of all 32 Transformer blocks!

# Use the embeddings of the input tokens as the initial input.
final_embedding = token_embeddings_unnormalized  # [17x4096]

# Perform layer-by-layer calculation for the 32-layer Transformer blocks
for layer in range(n_layers):
    #########################################################################################################################
    ################### Round 1: Normalization - Feature Transformation - Residual Operation ###############################
    
    ########################### The first normalization ###################################################
    
    # The first normalization
    layer_embedding_norm = rms_norm(final_embedding, model[f&quot;layers.{layer}.attention_norm.weight&quot;])  # [17x4096] &amp; [4096] -> [17x4096]
    
    ################ The first feature transformation - Multi-Head Self-Attention ########################
    
    # Obtain the qkv weight matrix of the attention mechanism for the current layer
    q_layer = model[f&quot;layers.{layer}.attention.wq.weight&quot;]  # [4096x4096]
    q_layer = q_layer.view(n_heads, q_layer.shape[0] // n_heads, dim)  # [32x128x4096]
    k_layer = model[f&quot;layers.{layer}.attention.wk.weight&quot;]  # [1024x4096]
    k_layer = k_layer.view(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)  # [8x128x4096]
    v_layer = model[f&quot;layers.{layer}.attention.wv.weight&quot;]  # [1024x4096]
    v_layer = v_layer.view(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)  # [8x128x4096]
    
    # Used to store the calculation results of the attention mechanism for each head
    qkv_attention_store = []
    
    # Calculate the attention mechanism results for each head
    for head in range(n_heads):
        # Extract the QKV weight matrices corresponding to the current head
        q_layer_head = q_layer[head]  # [32x128x4096] -> [128x4096]
        k_layer_head = k_layer[head//4]  # Every 4 heads share one key weight, [8x128x4096] -> [128x4096]
        v_layer_head = v_layer[head//4]  # Every 4 heads share one value weight, [8x128x4096] -> [128x4096]
        
        # Calculate XW to obtain the QKV vectors
        # [17x4096] x [4096x128] = [17x128]
        q_per_token = torch.matmul(layer_embedding_norm, q_layer_head.T)
        k_per_token = torch.matmul(layer_embedding_norm, k_layer_head.T)
        v_per_token = torch.matmul(layer_embedding_norm, v_layer_head.T)
        
        # Add position information to the query vector (RoPE)
        q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)  # Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -> [17x64x2]
        q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)  # Convert to complex number representation, (x,y) -> (x+yi). [17x64x2] -> [17x64]
        q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis  # Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]
        q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)  # Convert the result back to real number representation, (x+yi) -> (x,y). [17x64] -> [17x64x2]
        q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)  # Convert the result back to the original vector shape to obtain the final query vector. [17x64x2] -> [17x128]
        
        # Add position information to the key vector (RoPE)
        k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)  # Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -> [17x64x2]
        k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)  # Convert to complex number representation, (x,y) -> (x+yi). [17x64x2] -> [17x64]
        k_per_token_as_complex_numbers_rotated = k_per_token_as_complex_numbers * freqs_cis  # Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]
        k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers_rotated)  # Convert the result back to real number representation, (x+yi) -> (x,y). [17x64] -> [17x64x2]
        k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)  # Convert the result back to the original vector shape to obtain the final key vector. [17x64x2] -> [17x128]
        
        # Calculate the attention scores and normalize the scores simultaneously (i.e., Q×K/sqrt(dim))
        qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5  # [17x128] x [128x17] = [17x17]
        
        # Mask the scores of future tokens
        mask = torch.full(qk_per_token.shape, float(&quot;-inf&quot;), device=qk_per_token.device)  # Create a matrix with the same shape as the attention scores, filled with negative infinity, and stored in the same device as other vectors to prevent errors in subsequent calculations. [17x17]
        mask = torch.triu(mask, diagonal=1)  # Keep the negative infinity in the upper-triangular part and set others to 0 (i.e., the upper-triangular area represents future tokens that need to be masked). The diagonal offset is 1 to avoid masking the token itself. [17x17]
        qk_per_token_after_masking = qk_per_token + mask  # Add the attention scores with the masking matrix, making the upper-triangular part of the score matrix become negative infinity, which will tend to 0 after the subsequent softmax operation. [17x17]
        
        # Calculate the attention weights (i.e., softmax(score))
        # Meanwhile, convert it back to half-precision (because it will be multiplied with the value vector v_per_token later, so the data types need to be the same).
        qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)  # Calculate the softmax row-by-row. [17x17]
        
        # Calculate the final result of the attention mechanism (i.e., softmax(score) × V)
        qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)  # [17x17] x [17x128] = [17x128]
        
        # Record the result of this head
        qkv_attention_store.append(qkv_attention)
    
    # Merge the multi-head attention results
    stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)  # Merge the second dimension, that is, 32x[17x128] -> [17x4096]
    
    # Perform a linear mapping on the results to generate the final multi-head self-attention mechanism results
    o_layer = model[f&quot;layers.{layer}.attention.wo.weight&quot;]
    embedding_delta = torch.matmul(stacked_qkv_attention, o_layer.T)  # [17x4096] x [4096x4096] = [17x4096]

    ########################### The first residual operation ##############################################
    
    # The first Residual Operation
    # Add the output of the attention layer to the original input to complete the residual operation
    embedding_after_edit = final_embedding + embedding_delta  # [17x4096] + [17x4096] = [17x4096]
    
    
    #########################################################################################################################
    #################### Round 2: Normalization - Feature Transformation - Residual Operation ##############################
    
    ########################### The second normalization ##################################################
    
    # The second normalization
    embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[f&quot;layers.{layer}.ffn_norm.weight&quot;])  # [17x4096] &amp; [4096] -> [17x4096]
    
    ################## The second feature transformation - Feed-Forward Network ##########################
    
    # Load the parameter matrix of the feed-forward network (SwiGLU)
    w1 = model[f&quot;layers.{layer}.feed_forward.w1.weight&quot;]  # [14336x4096]
    w3 = model[f&quot;layers.{layer}.feed_forward.w3.weight&quot;]  # [14336x4096]
    w2 = model[f&quot;layers.{layer}.feed_forward.w2.weight&quot;]  # [4096x14336]
    
    # Calculate the results of the feed-forward network (output = (silu(XW1) * XW3)W2)
    # [17x4096] x [4096x14336] x [14336x4096] = [17x4096]
    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)
    
    ########################### The second residual operation ##############################################
    
    # The second residual operation, obtain the final output result of the current Transformer block
    # Add the output of the feed-forward layer to the original input to complete the residual operation
    final_embedding = embedding_after_edit+output_after_feedforward  # [17x4096] + [17x4096] = [17x4096]"><pre><span># Now, let's start to complete the calculation of all 32 Transformer blocks!</span>

<span># Use the embeddings of the input tokens as the initial input.</span>
<span>final_embedding</span> <span>=</span> <span>token_embeddings_unnormalized</span>  <span># [17x4096]</span>

<span># Perform layer-by-layer calculation for the 32-layer Transformer blocks</span>
<span>for</span> <span>layer</span> <span>in</span> <span>range</span>(<span>n_layers</span>):
    <span>#########################################################################################################################</span>
    <span>################### Round 1: Normalization - Feature Transformation - Residual Operation ###############################</span>
    
    <span>########################### The first normalization ###################################################</span>
    
    <span># The first normalization</span>
    <span>layer_embedding_norm</span> <span>=</span> <span>rms_norm</span>(<span>final_embedding</span>, <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention_norm.weight"</span>])  <span># [17x4096] &amp; [4096] -&gt; [17x4096]</span>
    
    <span>################ The first feature transformation - Multi-Head Self-Attention ########################</span>
    
    <span># Obtain the qkv weight matrix of the attention mechanism for the current layer</span>
    <span>q_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wq.weight"</span>]  <span># [4096x4096]</span>
    <span>q_layer</span> <span>=</span> <span>q_layer</span>.<span>view</span>(<span>n_heads</span>, <span>q_layer</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_heads</span>, <span>dim</span>)  <span># [32x128x4096]</span>
    <span>k_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wk.weight"</span>]  <span># [1024x4096]</span>
    <span>k_layer</span> <span>=</span> <span>k_layer</span>.<span>view</span>(<span>n_kv_heads</span>, <span>k_layer</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>)  <span># [8x128x4096]</span>
    <span>v_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wv.weight"</span>]  <span># [1024x4096]</span>
    <span>v_layer</span> <span>=</span> <span>v_layer</span>.<span>view</span>(<span>n_kv_heads</span>, <span>v_layer</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>)  <span># [8x128x4096]</span>
    
    <span># Used to store the calculation results of the attention mechanism for each head</span>
    <span>qkv_attention_store</span> <span>=</span> []
    
    <span># Calculate the attention mechanism results for each head</span>
    <span>for</span> <span>head</span> <span>in</span> <span>range</span>(<span>n_heads</span>):
        <span># Extract the QKV weight matrices corresponding to the current head</span>
        <span>q_layer_head</span> <span>=</span> <span>q_layer</span>[<span>head</span>]  <span># [32x128x4096] -&gt; [128x4096]</span>
        <span>k_layer_head</span> <span>=</span> <span>k_layer</span>[<span>head</span><span>//</span><span>4</span>]  <span># Every 4 heads share one key weight, [8x128x4096] -&gt; [128x4096]</span>
        <span>v_layer_head</span> <span>=</span> <span>v_layer</span>[<span>head</span><span>//</span><span>4</span>]  <span># Every 4 heads share one value weight, [8x128x4096] -&gt; [128x4096]</span>
        
        <span># Calculate XW to obtain the QKV vectors</span>
        <span># [17x4096] x [4096x128] = [17x128]</span>
        <span>q_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>layer_embedding_norm</span>, <span>q_layer_head</span>.<span>T</span>)
        <span>k_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>layer_embedding_norm</span>, <span>k_layer_head</span>.<span>T</span>)
        <span>v_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>layer_embedding_norm</span>, <span>v_layer_head</span>.<span>T</span>)
        
        <span># Add position information to the query vector (RoPE)</span>
        <span>q_per_token_split_into_pairs</span> <span>=</span> <span>q_per_token</span>.<span>float</span>().<span>view</span>(<span>q_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)  <span># Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -&gt; [17x64x2]</span>
        <span>q_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>q_per_token_split_into_pairs</span>)  <span># Convert to complex number representation, (x,y) -&gt; (x+yi). [17x64x2] -&gt; [17x64]</span>
        <span>q_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>q_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>  <span># Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]</span>
        <span>q_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>q_per_token_as_complex_numbers_rotated</span>)  <span># Convert the result back to real number representation, (x+yi) -&gt; (x,y). [17x64] -&gt; [17x64x2]</span>
        <span>q_per_token_rotated</span> <span>=</span> <span>q_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>q_per_token</span>.<span>shape</span>)  <span># Convert the result back to the original vector shape to obtain the final query vector. [17x64x2] -&gt; [17x128]</span>
        
        <span># Add position information to the key vector (RoPE)</span>
        <span>k_per_token_split_into_pairs</span> <span>=</span> <span>k_per_token</span>.<span>float</span>().<span>view</span>(<span>k_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)  <span># Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -&gt; [17x64x2]</span>
        <span>k_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>k_per_token_split_into_pairs</span>)  <span># Convert to complex number representation, (x,y) -&gt; (x+yi). [17x64x2] -&gt; [17x64]</span>
        <span>k_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>k_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>  <span># Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]</span>
        <span>k_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>k_per_token_as_complex_numbers_rotated</span>)  <span># Convert the result back to real number representation, (x+yi) -&gt; (x,y). [17x64] -&gt; [17x64x2]</span>
        <span>k_per_token_rotated</span> <span>=</span> <span>k_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>k_per_token</span>.<span>shape</span>)  <span># Convert the result back to the original vector shape to obtain the final key vector. [17x64x2] -&gt; [17x128]</span>
        
        <span># Calculate the attention scores and normalize the scores simultaneously (i.e., Q×K/sqrt(dim))</span>
        <span>qk_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>q_per_token_rotated</span>, <span>k_per_token_rotated</span>.<span>T</span>)<span>/</span>(<span>128</span>)<span>**</span><span>0.5</span>  <span># [17x128] x [128x17] = [17x17]</span>
        
        <span># Mask the scores of future tokens</span>
        <span>mask</span> <span>=</span> <span>torch</span>.<span>full</span>(<span>qk_per_token</span>.<span>shape</span>, <span>float</span>(<span>"-inf"</span>), <span>device</span><span>=</span><span>qk_per_token</span>.<span>device</span>)  <span># Create a matrix with the same shape as the attention scores, filled with negative infinity, and stored in the same device as other vectors to prevent errors in subsequent calculations. [17x17]</span>
        <span>mask</span> <span>=</span> <span>torch</span>.<span>triu</span>(<span>mask</span>, <span>diagonal</span><span>=</span><span>1</span>)  <span># Keep the negative infinity in the upper-triangular part and set others to 0 (i.e., the upper-triangular area represents future tokens that need to be masked). The diagonal offset is 1 to avoid masking the token itself. [17x17]</span>
        <span>qk_per_token_after_masking</span> <span>=</span> <span>qk_per_token</span> <span>+</span> <span>mask</span>  <span># Add the attention scores with the masking matrix, making the upper-triangular part of the score matrix become negative infinity, which will tend to 0 after the subsequent softmax operation. [17x17]</span>
        
        <span># Calculate the attention weights (i.e., softmax(score))</span>
        <span># Meanwhile, convert it back to half-precision (because it will be multiplied with the value vector v_per_token later, so the data types need to be the same).</span>
        <span>qk_per_token_after_masking_after_softmax</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>functional</span>.<span>softmax</span>(<span>qk_per_token_after_masking</span>, <span>dim</span><span>=</span><span>1</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)  <span># Calculate the softmax row-by-row. [17x17]</span>
        
        <span># Calculate the final result of the attention mechanism (i.e., softmax(score) × V)</span>
        <span>qkv_attention</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>qk_per_token_after_masking_after_softmax</span>, <span>v_per_token</span>)  <span># [17x17] x [17x128] = [17x128]</span>
        
        <span># Record the result of this head</span>
        <span>qkv_attention_store</span>.<span>append</span>(<span>qkv_attention</span>)
    
    <span># Merge the multi-head attention results</span>
    <span>stacked_qkv_attention</span> <span>=</span> <span>torch</span>.<span>cat</span>(<span>qkv_attention_store</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>)  <span># Merge the second dimension, that is, 32x[17x128] -&gt; [17x4096]</span>
    
    <span># Perform a linear mapping on the results to generate the final multi-head self-attention mechanism results</span>
    <span>o_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wo.weight"</span>]
    <span>embedding_delta</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>stacked_qkv_attention</span>, <span>o_layer</span>.<span>T</span>)  <span># [17x4096] x [4096x4096] = [17x4096]</span>

    <span>########################### The first residual operation ##############################################</span>
    
    <span># The first Residual Operation</span>
    <span># Add the output of the attention layer to the original input to complete the residual operation</span>
    <span>embedding_after_edit</span> <span>=</span> <span>final_embedding</span> <span>+</span> <span>embedding_delta</span>  <span># [17x4096] + [17x4096] = [17x4096]</span>
    
    
    <span>#########################################################################################################################</span>
    <span>#################### Round 2: Normalization - Feature Transformation - Residual Operation ##############################</span>
    
    <span>########################### The second normalization ##################################################</span>
    
    <span># The second normalization</span>
    <span>embedding_after_edit_normalized</span> <span>=</span> <span>rms_norm</span>(<span>embedding_after_edit</span>, <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.ffn_norm.weight"</span>])  <span># [17x4096] &amp; [4096] -&gt; [17x4096]</span>
    
    <span>################## The second feature transformation - Feed-Forward Network ##########################</span>
    
    <span># Load the parameter matrix of the feed-forward network (SwiGLU)</span>
    <span>w1</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.feed_forward.w1.weight"</span>]  <span># [14336x4096]</span>
    <span>w3</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.feed_forward.w3.weight"</span>]  <span># [14336x4096]</span>
    <span>w2</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.feed_forward.w2.weight"</span>]  <span># [4096x14336]</span>
    
    <span># Calculate the results of the feed-forward network (output = (silu(XW1) * XW3)W2)</span>
    <span># [17x4096] x [4096x14336] x [14336x4096] = [17x4096]</span>
    <span>output_after_feedforward</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>torch</span>.<span>functional</span>.<span>F</span>.<span>silu</span>(<span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w1</span>.<span>T</span>)) <span>*</span> <span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w3</span>.<span>T</span>), <span>w2</span>.<span>T</span>)
    
    <span>########################### The second residual operation ##############################################</span>
    
    <span># The second residual operation, obtain the final output result of the current Transformer block</span>
    <span># Add the output of the feed-forward layer to the original input to complete the residual operation</span>
    <span>final_embedding</span> <span>=</span> <span>embedding_after_edit</span><span>+</span><span>output_after_feedforward</span>  <span># [17x4096] + [17x4096] = [17x4096]</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Let's complete the last step and predict the next token</h2><a id="user-content-lets-complete-the-last-step-and-predict-the-next-token" aria-label="Permalink: Let's complete the last step and predict the next token" href="#lets-complete-the-last-step-and-predict-the-next-token"></a></p>
<p dir="auto">Now we have obtained the final embeddings, which contains all the information we needed to predict the next token.
<br>
The shape of this embedding is the same as that of the input token embedding, both being [17x4096], where 17 is the number of tokens and 4096 is the dimension of the embedding.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/last_norm.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/last_norm.png" width="600px"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">First, perform one last normalization on the output of the last Transformer layer</h2><a id="user-content-first-perform-one-last-normalization-on-the-output-of-the-last-transformer-layer" aria-label="Permalink: First, perform one last normalization on the output of the last Transformer layer" href="#first-perform-one-last-normalization-on-the-output-of-the-last-transformer-layer"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Perform the last normalization in the entire model
final_embedding = rms_norm(final_embedding, model[&quot;norm.weight&quot;])  # [17x4096] &amp; [4096] -> [17x4096]
final_embedding.shape"><pre><span># Perform the last normalization in the entire model</span>
<span>final_embedding</span> <span>=</span> <span>rms_norm</span>(<span>final_embedding</span>, <span>model</span>[<span>"norm.weight"</span>])  <span># [17x4096] &amp; [4096] -&gt; [17x4096]</span>
<span>final_embedding</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Then, make the prediction based on the embedding corresponding to the last token (perform a linear mapping to the vocabulary dimension)</h2><a id="user-content-then-make-the-prediction-based-on-the-embedding-corresponding-to-the-last-token-perform-a-linear-mapping-to-the-vocabulary-dimension" aria-label="Permalink: Then, make the prediction based on the embedding corresponding to the last token (perform a linear mapping to the vocabulary dimension)" href="#then-make-the-prediction-based-on-the-embedding-corresponding-to-the-last-token-perform-a-linear-mapping-to-the-vocabulary-dimension"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/finallayer.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/finallayer.png" width="600px"></a>
</p>
<br>
We will use the output decoder (a linear mapping layer) to convert the embedding vector of the last token into a prediction result for the next token (the dimension is the size of the vocabulary. If we apply a softmax function to the result, the value of each dimension represents the probability that the next token belongs to that word).
<div dir="auto"><p>Why do we only use the output vector of the last token to predict the next token?
<br>
Because during training, the model's objective is to predict the next token based on the current token and all previous tokens. Therefore, the output vector corresponding to each token is used to predict the next token relative to itself, rather than the next token for the entire input.
</p></div>
<p dir="auto">We hope the answer is 42 in our example :)
<br>
Note: 42 is the answer to "the answer to the ultimate question of life, the universe, and everything is " according to the book <em>The Hitchhiker's Guide to the Galaxy</em>. Most modern large language models will answer 42, which will verify the correctness of our entire code! Good luck to us :)</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Perform the last linear mapping to map the embeddings to the size of the vocabulary dimension as a prediction for the next token
logits = torch.matmul(final_embedding[-1], model[&quot;output.weight&quot;].T)  # [17x4096] -> [4096] -> [4096] x [4096x128256] = [128256]
logits.shape"><pre><span># Perform the last linear mapping to map the embeddings to the size of the vocabulary dimension as a prediction for the next token</span>
<span>logits</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>final_embedding</span>[<span>-</span><span>1</span>], <span>model</span>[<span>"output.weight"</span>].<span>T</span>)  <span># [17x4096] -&gt; [4096] -&gt; [4096] x [4096x128256] = [128256]</span>
<span>logits</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Here's the prediction result!</h2><a id="user-content-heres-the-prediction-result" aria-label="Permalink: Here's the prediction result!" href="#heres-the-prediction-result"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Extract the id corresponding to the dimension with the highest probability,
# is gonna be the predicted next token's id
next_token = torch.argmax(logits, dim=-1)  # Get the index corresponding to the maximum value, which is the predicted next token id. [128256] -> [1]
next_token"><pre><span># Extract the id corresponding to the dimension with the highest probability,</span>
<span># is gonna be the predicted next token's id</span>
<span>next_token</span> <span>=</span> <span>torch</span>.<span>argmax</span>(<span>logits</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>)  <span># Get the index corresponding to the maximum value, which is the predicted next token id. [128256] -&gt; [1]</span>
<span>next_token</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Based on the predicted id, restore it to the specific predicted value
tokenizer.decode([next_token.item()])"><pre><span># Based on the predicted id, restore it to the specific predicted value</span>
<span>tokenizer</span>.<span>decode</span>([<span>next_token</span>.<span>item</span>()])</pre></div>

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/42.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/42.png" width="600px"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Let's dive deeper and see how different embeddings or token masking strategies might affect the prediction results :)</h2><a id="user-content-lets-dive-deeper-and-see-how-different-embeddings-or-token-masking-strategies-might-affect-the-prediction-results-" aria-label="Permalink: Let's dive deeper and see how different embeddings or token masking strategies might affect the prediction results :)" href="#lets-dive-deeper-and-see-how-different-embeddings-or-token-masking-strategies-might-affect-the-prediction-results-"></a></p>
<p dir="auto">Now we've got the final prediction results. If you're still interested, let's explore some of the issues that might have been mentioned before~
<br></p>
<p dir="auto">We'll briefly explore three scenarios:</p>
<ol dir="auto">
<li>Apart from the top-1 result, what else is predicted in the current prediction, that is, the top-k results?</li>
<li>What can be predicted if we use the output embedding of other tokens for prediction?</li>
<li>If the future tokens were not masked during the attention calculation before, how would the prediction results differ?</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Let's first take a look at the top-k prediction results
logits_sort, logits_idx = torch.sort(logits, dim=-1, descending=True)  # Put the token with the highest probability prediction at the front, [128256]
[tokenizer.decode([i]) for i in logits_idx[:10]]  # View the top 10 high-probability results"><pre><span># Let's first take a look at the top-k prediction results</span>
<span>logits_sort</span>, <span>logits_idx</span> <span>=</span> <span>torch</span>.<span>sort</span>(<span>logits</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>, <span>descending</span><span>=</span><span>True</span>)  <span># Put the token with the highest probability prediction at the front, [128256]</span>
[<span>tokenizer</span>.<span>decode</span>([<span>i</span>]) <span>for</span> <span>i</span> <span>in</span> <span>logits_idx</span>[:<span>10</span>]]  <span># View the top 10 high-probability results</span></pre></div>
<div data-snippet-clipboard-copy-content="['42', '6', '43', '41', '4', '1', '45', '3', '2', '46']"><pre><code>['42', '6', '43', '41', '4', '1', '45', '3', '2', '46']
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Next, let's to see what can we get by using the embeddings of other tokens for prediction
logits_all_token = torch.matmul(final_embedding, model[&quot;output.weight&quot;].T)  # Map the embeddings to the same size as the vocabulary, [17x4096] x [4096x128256] = [17x128256]
logits_all_token_sort, logits_all_token_idx = torch.sort(logits_all_token, dim=-1, descending=True)  # Put the token with the highest probability prediction at the front, [17x128256]

print('Input tokens:', prompt_split_as_tokens)  # Display the input tokens, [17]

# Display the results of the next-token prediction based on the output embedding of each token
for i in range(len(final_embedding)):
    print(f'Predict results based on {i+1}th token:', [tokenizer.decode([j]) for j in logits_all_token_idx[i][:10]])  # Output the top 10 high-probability results
    
_=&quot;&quot;&quot;
It can be seen that when making predictions based on each token, the prediction result is the possible result of the next token after the &quot;current token&quot;,
rather than the prediction result of the entire complete input.
Therefore, in actual prediction, only the embedding of the last token will be used for prediction.
&quot;&quot;&quot;"><pre><span># Next, let's to see what can we get by using the embeddings of other tokens for prediction</span>
<span>logits_all_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>final_embedding</span>, <span>model</span>[<span>"output.weight"</span>].<span>T</span>)  <span># Map the embeddings to the same size as the vocabulary, [17x4096] x [4096x128256] = [17x128256]</span>
<span>logits_all_token_sort</span>, <span>logits_all_token_idx</span> <span>=</span> <span>torch</span>.<span>sort</span>(<span>logits_all_token</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>, <span>descending</span><span>=</span><span>True</span>)  <span># Put the token with the highest probability prediction at the front, [17x128256]</span>

<span>print</span>(<span>'Input tokens:'</span>, <span>prompt_split_as_tokens</span>)  <span># Display the input tokens, [17]</span>

<span># Display the results of the next-token prediction based on the output embedding of each token</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>len</span>(<span>final_embedding</span>)):
    <span>print</span>(<span>f'Predict results based on <span><span>{</span><span>i</span><span>+</span><span>1</span><span>}</span></span>th token:'</span>, [<span>tokenizer</span>.<span>decode</span>([<span>j</span>]) <span>for</span> <span>j</span> <span>in</span> <span>logits_all_token_idx</span>[<span>i</span>][:<span>10</span>]])  <span># Output the top 10 high-probability results</span>
    
<span>_</span><span>=</span><span>"""</span>
<span>It can be seen that when making predictions based on each token, the prediction result is the possible result of the next token after the "current token",</span>
<span>rather than the prediction result of the entire complete input.</span>
<span>Therefore, in actual prediction, only the embedding of the last token will be used for prediction.</span>
<span>"""</span></pre></div>
<div data-snippet-clipboard-copy-content="Input tokens: ['<|begin_of_text|>', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']
Predict results based on 1th token: ['Question', 'def', '#', 'The', 'import', 'Tags', 'A', 'package', 'Home', 'I']
Predict results based on 2th token: [' ', ' best', ' first', ' most', ' new', ' world', ' last', ' same', ' way', ' number']
Predict results based on 3th token: [' to', ' is', ' was', ' of', ' lies', ',', ' for', ' you', ' key', ' will']
Predict results based on 4th token: [' the', ' this', ' your', ' all', ' that', ' a', ' my', ' life', ' &quot;', ' everything']
Predict results based on 5th token: [' question', ' problem', ' above', ' ultimate', ' first', ' r', ' following', ' questions', ' most', ' previous']
Predict results based on 6th token: [' question', ' questions', ' mystery', '\xa0', ' quest', '\n', ' life', ' philosophical', ' qu', ' problem']
Predict results based on 7th token: [' of', '\n', ' to', ' is', '?\n', ',', '.\n', ':', '...\n', ' about']
Predict results based on 8th token: [' life', ' Life', ' the', '\xa0', ' everything', ' existence', '\n', ' LIFE', ' all', ' human']
Predict results based on 9th token: [',', ' the', '\n', ' and', ' is', ',\n', '.\n', '?\n', '...\n', '...']
Predict results based on 10th token: [' the', ' universe', ' and', ' etc', '\xa0', ' is', ' death', ' of', ' or', ' everything']
Predict results based on 11th token: [' universe', ' Universe', '\n', '\xa0', ' un', ' univers', ' uni', ' cosmos', ' universal', ' u']
Predict results based on 12th token: [',', ' and', ' &amp;', '\n', ',\n', ' ,', '...\n', ',and', '...', '\xa0']
Predict results based on 13th token: [' and', ' everything', ' &amp;', ' the', ' etc', '\xa0', ' is', ' or', ' ...\n', ' an']
Predict results based on 14th token: [' everything', '\xa0', ' the', ' every', '\n', ' ever', ' all', ' Everything', ' EVERY', '...']
Predict results based on 15th token: ['\n', ' is', '.\n', '.', '?\n', ',', ' (', '\n\n', '...\n', ' in']
Predict results based on 16th token: [' ', '\n', '...', '...\n', ':', ' forty', ' not', ' &quot;', '…', ' a']
Predict results based on 17th token: ['42', '6', '43', '41', '4', '1', '45', '3', '2', '46']"><pre><code>Input tokens: ['&lt;|begin_of_text|&gt;', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']
Predict results based on 1th token: ['Question', 'def', '#', 'The', 'import', 'Tags', 'A', 'package', 'Home', 'I']
Predict results based on 2th token: [' ', ' best', ' first', ' most', ' new', ' world', ' last', ' same', ' way', ' number']
Predict results based on 3th token: [' to', ' is', ' was', ' of', ' lies', ',', ' for', ' you', ' key', ' will']
Predict results based on 4th token: [' the', ' this', ' your', ' all', ' that', ' a', ' my', ' life', ' "', ' everything']
Predict results based on 5th token: [' question', ' problem', ' above', ' ultimate', ' first', ' r', ' following', ' questions', ' most', ' previous']
Predict results based on 6th token: [' question', ' questions', ' mystery', '\xa0', ' quest', '\n', ' life', ' philosophical', ' qu', ' problem']
Predict results based on 7th token: [' of', '\n', ' to', ' is', '?\n', ',', '.\n', ':', '...\n', ' about']
Predict results based on 8th token: [' life', ' Life', ' the', '\xa0', ' everything', ' existence', '\n', ' LIFE', ' all', ' human']
Predict results based on 9th token: [',', ' the', '\n', ' and', ' is', ',\n', '.\n', '?\n', '...\n', '...']
Predict results based on 10th token: [' the', ' universe', ' and', ' etc', '\xa0', ' is', ' death', ' of', ' or', ' everything']
Predict results based on 11th token: [' universe', ' Universe', '\n', '\xa0', ' un', ' univers', ' uni', ' cosmos', ' universal', ' u']
Predict results based on 12th token: [',', ' and', ' &amp;', '\n', ',\n', ' ,', '...\n', ',and', '...', '\xa0']
Predict results based on 13th token: [' and', ' everything', ' &amp;', ' the', ' etc', '\xa0', ' is', ' or', ' ...\n', ' an']
Predict results based on 14th token: [' everything', '\xa0', ' the', ' every', '\n', ' ever', ' all', ' Everything', ' EVERY', '...']
Predict results based on 15th token: ['\n', ' is', '.\n', '.', '?\n', ',', ' (', '\n\n', '...\n', ' in']
Predict results based on 16th token: [' ', '\n', '...', '...\n', ':', ' forty', ' not', ' "', '…', ' a']
Predict results based on 17th token: ['42', '6', '43', '41', '4', '1', '45', '3', '2', '46']
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Finally, let's take a look at what the prediction results will be if we don't mask future tokens when calculating attention
# At this time, the prediction results based on each token will be as follows
# It can be seen that due to the visibility of future tokens, the embeddings of each token will more accurately predict &quot;the next token for it&quot; (it's a bit like &quot;cheating&quot;) 

_=&quot;&quot;&quot;
Input tokens: ['<|begin_of_text|>', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']
Predict results based on 1th token: ['://', '.Forms', '_REF', ' Angeles', '.swing', '�', 'php', 'во', 'ysics', '�']
Predict results based on 2th token: [' answer', ' Hitch', ' universe', ' question', ' ultimate', ' meaning', ' hitch', ' Universe', ' Answer', ' reason']
Predict results based on 3th token: [' to', ' is', ',', ':', ' was', '\n', ' ', ' (', '\n\n', ' of']
Predict results based on 4th token: [' the', ' life', ' this', ' which', ' everything', ' that', ' how', ' why', ' ', ' all']
Predict results based on 5th token: [' ultimate', ' question', ' great', ' meaning', ' universe', ' Ultimate', ' everything', ' life', ' holy', ' greatest']
Predict results based on 6th token: [' question', ' answer', ' is', ' was', '\n', ' questions', ' mystery', '\n\n', ' what', ' Question']
Predict results based on 7th token: [' of', ' is', '\n', ',', ' about', ':', ' to', ' in', ' (', '<|end_of_text|>']
Predict results based on 8th token: [' life', ' existence', ' everything', ' Life', ' the', ' death', ' time', ' all', ' why', ' which']
Predict results based on 9th token: [',', ' is', ' the', '\n', ':', ' (', '...', ' and', ' ,', ' -']
Predict results based on 10th token: [' the', ' and', ' is', ' death', ' The', ' which', ' or', '\xa0', ' existence', ' don']
Predict results based on 11th token: [' universe', ' answer', ' cosmos', ' world', ' existence', ' Universe', ' everything', ' un', ' meaning', ' question']
Predict results based on 12th token: [',', ' and', ' is', ' &amp;', '\n', ' ,', '.', '...', ' (', ' ']
Predict results based on 13th token: [' and', ' &amp;', ' don', ' the', ' is', ' a', ' or', ' Douglas', '\xa0', '<|end_of_text|>']
Predict results based on 14th token: [' everything', ' dough', ' don', ' ever', ' deep', ' Douglas', ' the', ' every', ' all', ' death']
Predict results based on 15th token: ['\n', ' is', ',', '.', ' ', ' (', ':', '<|end_of_text|>', '\n\n', '.\n']
Predict results based on 16th token: [' ', '\n', ' forty', '...', ' &quot;', '42', ' the', ':', '\xa0', ' to']
Predict results based on 17th token: ['42', '6', '4', '41', '1', '2', '3', '7', '5', '43']
&quot;&quot;&quot;"><pre><span># Finally, let's take a look at what the prediction results will be if we don't mask future tokens when calculating attention</span>
<span># At this time, the prediction results based on each token will be as follows</span>
<span># It can be seen that due to the visibility of future tokens, the embeddings of each token will more accurately predict "the next token for it" (it's a bit like "cheating") </span>

<span>_</span><span>=</span><span>"""</span>
<span>Input tokens: ['&lt;|begin_of_text|&gt;', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']</span>
<span>Predict results based on 1th token: ['://', '.Forms', '_REF', ' Angeles', '.swing', '�', 'php', 'во', 'ysics', '�']</span>
<span>Predict results based on 2th token: [' answer', ' Hitch', ' universe', ' question', ' ultimate', ' meaning', ' hitch', ' Universe', ' Answer', ' reason']</span>
<span>Predict results based on 3th token: [' to', ' is', ',', ':', ' was', '<span>\n</span>', ' ', ' (', '<span>\n</span><span>\n</span>', ' of']</span>
<span>Predict results based on 4th token: [' the', ' life', ' this', ' which', ' everything', ' that', ' how', ' why', ' ', ' all']</span>
<span>Predict results based on 5th token: [' ultimate', ' question', ' great', ' meaning', ' universe', ' Ultimate', ' everything', ' life', ' holy', ' greatest']</span>
<span>Predict results based on 6th token: [' question', ' answer', ' is', ' was', '<span>\n</span>', ' questions', ' mystery', '<span>\n</span><span>\n</span>', ' what', ' Question']</span>
<span>Predict results based on 7th token: [' of', ' is', '<span>\n</span>', ',', ' about', ':', ' to', ' in', ' (', '&lt;|end_of_text|&gt;']</span>
<span>Predict results based on 8th token: [' life', ' existence', ' everything', ' Life', ' the', ' death', ' time', ' all', ' why', ' which']</span>
<span>Predict results based on 9th token: [',', ' is', ' the', '<span>\n</span>', ':', ' (', '...', ' and', ' ,', ' -']</span>
<span>Predict results based on 10th token: [' the', ' and', ' is', ' death', ' The', ' which', ' or', '<span>\xa0</span>', ' existence', ' don']</span>
<span>Predict results based on 11th token: [' universe', ' answer', ' cosmos', ' world', ' existence', ' Universe', ' everything', ' un', ' meaning', ' question']</span>
<span>Predict results based on 12th token: [',', ' and', ' is', ' &amp;', '<span>\n</span>', ' ,', '.', '...', ' (', ' ']</span>
<span>Predict results based on 13th token: [' and', ' &amp;', ' don', ' the', ' is', ' a', ' or', ' Douglas', '<span>\xa0</span>', '&lt;|end_of_text|&gt;']</span>
<span>Predict results based on 14th token: [' everything', ' dough', ' don', ' ever', ' deep', ' Douglas', ' the', ' every', ' all', ' death']</span>
<span>Predict results based on 15th token: ['<span>\n</span>', ' is', ',', '.', ' ', ' (', ':', '&lt;|end_of_text|&gt;', '<span>\n</span><span>\n</span>', '.<span>\n</span>']</span>
<span>Predict results based on 16th token: [' ', '<span>\n</span>', ' forty', '...', ' "', '42', ' the', ':', '<span>\xa0</span>', ' to']</span>
<span>Predict results based on 17th token: ['42', '6', '4', '41', '1', '2', '3', '7', '5', '43']</span>
<span>"""</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Need to predict multiple tokens? Just using KV-Cache! (It really took me a lot of effort to sort this out. Orz)</h2><a id="user-content-need-to-predict-multiple-tokens-just-using-kv-cache-it-really-took-me-a-lot-of-effort-to-sort-this-out-orz" aria-label="Permalink: Need to predict multiple tokens? Just using KV-Cache! (It really took me a lot of effort to sort this out. Orz)" href="#need-to-predict-multiple-tokens-just-using-kv-cache-it-really-took-me-a-lot-of-effort-to-sort-this-out-orz"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How to Continuously Predict Multiple Tokens</h3><a id="user-content-how-to-continuously-predict-multiple-tokens" aria-label="Permalink: How to Continuously Predict Multiple Tokens" href="#how-to-continuously-predict-multiple-tokens"></a></p>
<div dir="auto"><p>Now, we've completed the prediction of the next word for the input text. But what if our expected output requires multiple tokens?
<br>
For example, in practical llm applications, models usually don't output just one word. Instead, they often output a passage of text, or even a very long text. How is this ability achieved?
<br>
Actually, it's quite simple. We just need to repeatedly call the llm's prediction process to gradually generate a complete sentence or paragraph.
<br>
This process is like "snowballing". Each time we predict a word, we add this word to the current input sequence, and then call the model again for a new round of prediction. The prediction stops when we encounter a stop symbol (a special token "&lt;|end_of_text|&gt;" in llama3) or reach the maximum length limit (a hyperparameter max_seq_len).
</p><p>
Does this sound inefficient? Yes!
<br>
That's why there are well-known caching mechanisms like KV-Cache. By caching the KV vectors of historical tokens, we can reduce the input and computational load, thus improving the inference efficiency.
<br>
Thanks to the caching mechanism, when we use a large model for inference, you may notice that waiting for the first token to be output is often the most time-consuming. But once the first token is output, the output speed of subsequent tokens will increase significantly.
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Advantages and Disadvantages of KV-Cache</h3><a id="user-content-advantages-and-disadvantages-of-kv-cache" aria-label="Permalink: Advantages and Disadvantages of KV-Cache" href="#advantages-and-disadvantages-of-kv-cache"></a></p>
<div dir="auto"><p><strong>Advantage</strong>: When continuously predicting, we only need to input the new token each time instead of the entire text sequence. This greatly improves the calculation speed during inference.
<br>
<strong>Disadvantage</strong>: Due to the caching mechanism, it will consume more memory resources during inference.
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Principle Derivation of KV-Cache</h3><a id="user-content-principle-derivation-of-kv-cache" aria-label="Permalink: Principle Derivation of KV-Cache" href="#principle-derivation-of-kv-cache"></a></p>
<p dir="auto">KV-Cache comes from the observation and analysis of the above matrix calculation process. By analyzing the calculation process of each input token, we can find that in most calculation steps, the calculation of each token is actually relatively independent and rately involves interaction with other tokens. Only when calculating the attention mechanism will token-to-token interactions be involved, thus requiring the caching of historical KV vectors.
<br></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Here is the specific derivation logic of KV-Cache:</h3><a id="user-content-here-is-the-specific-derivation-logic-of-kv-cache" aria-label="Permalink: Here is the specific derivation logic of KV-Cache:" href="#here-is-the-specific-derivation-logic-of-kv-cache"></a></p>
<ol dir="auto">
<li><strong>Premise</strong>: To predict the next token, we only need to get the output result of the last token (just as we did in the prediction chapter).</li>
<li><strong>Non-attention parts only needs to calculate the new tokens</strong>: Except for the attention calculation, the calculations of all other parts are independent among tokens. So we only need to calculate the new tokens and don't need to input historical tokens (I'll expand the analysis below).</li>
<li><strong>Attention parts also only needs to calculate the new tokens</strong>: In the attention layer, due to the masking mechanism, the output results of historical tokens won't be affected by future new tokens. So their inputs and outputs at each layer are fixed, that is, the QKV vectors of historical tokens will not change because of the addition of new tokens. Thus, we only need to calculate the attention of the new tokens.</li>
<li><strong>Calculate the new token's attention mechanism</strong>: The attention layer is used to let the token obtain the context information of historical tokens. So, for each new token, we need to calculate the weighted sum using the value vectors of all tokens. Therefore, we need to store the values of historical tokens.</li>
<li><strong>Calculate the new token's attention weights</strong>: As known from point 4, we also need to obtain the importance information, i.e., weights, between the new tokens and historical tokens first. So we need to calculate the product of the key vectors of the new tokens with the key vectors of all tokens. Therefore, we need to store the keys of historical tokens.</li>
<li><strong>Acquisition of KV-Cache</strong>: As known from points 4 and 5, we need to store the KV vectors of historical tokens. Since the query vectors are not used, we don't need to store them. This is how the kv-cache came about.</li>
<li><strong>Efficiency of KV-Cache</strong>: As known from point 3, the historical KV vectors won't change. So they can be incrementally updated during the continuous prediction process without modifying the historical content. In this way, each time we predict, we only need to input and calculate the result of the newly added tokens instead of taking the complete sequence as input, thus greatly improving the inference efficiency.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Additional: Analysis of the Independence of Token Calculation in KV-Cache</h3><a id="user-content-additional-analysis-of-the-independence-of-token-calculation-in-kv-cache" aria-label="Permalink: Additional: Analysis of the Independence of Token Calculation in KV-Cache" href="#additional-analysis-of-the-independence-of-token-calculation-in-kv-cache"></a></p>
<p dir="auto"><strong>All components except the attention layer (no interaction among them)</strong>:</p>
<ol dir="auto">
<li><strong>Two times normalizations</strong>: Each token vector is normalized in its own feature dimension without using other tokens.</li>
<li><strong>Two times residual connections (add)</strong>: Each token vector adds its own output result to itself without using other tokens.</li>
<li><strong>Feed-forward network (FFN)</strong>: Each token vector is multiplied by the same weight matrices W1, W2, W3 to get the result, and other tokens are not used during this process. Imagine that if the number of input tokens is 17, the calculation of FFN can be simplified as: [17x4096] x [4096x14336] x [14336x4096] = [17x4096]. This is actually equivalent to inputting one token at a time and then concatenating the 17 results into a matrix, that is: 17 times ([1x4096] x [4096x14336] x [14336x4096] = [1x4096]) = 17x[1x4096] =&gt; [17x4096]. Therefore, when each token is calculated in the feed-forward layer, there is actually no interaction with other tokens.</li>
</ol>
<p dir="auto"><strong>Attention layer (only have one-way interaction between new tokens and historical tokens)</strong>:</p>
<ol dir="auto">
<li><strong>Calculate QKV vectors</strong>: Each token vector is multiplied by the same QKV weight matrices to get the result without using other tokens.</li>
<li><strong>Add positional information to QK vectors</strong>: Each token vector performs an independent rotation operation based on its own position without using the specific content of other tokens.</li>
<li><strong>Calculate attention weights</strong>: The attention weights represent the correlation between each token and every historical tokens preceding it, and are independent of future tokens. Therefore, the results of historical tokens are independent of new tokens. And new tokens need the key vector cache of historical tokens.</li>
<li><strong>Calculate the result of the attention mechanism</strong>: The attention mechanism calculates the weighted sum of value vectors based on attention weights. So, similar to the conclusion in the previous point, the results of historical tokens are also independent of new tokens. And new tokens need the value vector cache of historical tokens.
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Attention Calculation Process Based on KV-Cache</h3><a id="user-content-attention-calculation-process-based-on-kv-cache" aria-label="Permalink: Attention Calculation Process Based on KV-Cache" href="#attention-calculation-process-based-on-kv-cache"></a></p>
<p dir="auto">To clearly show the calculation process, we only derive the single-head scenario (the principle and process of extending it to the multi-head scenario are exactly the same as the previous multi-head attention implementation):</p>
<ol dir="auto">
<li>Assume that the historical input tokens are <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$S_1$</math-renderer> with a length of N. Based on KV-Cache, we will store the KV result matrix of each head. The shape of a single head is [Nxhead_dim] = [Nx128].</li>
<li>Assume that the newly added input tokens are <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$S_2$</math-renderer> with a length of M (it can be newly predicted tokens or the input of a new round of user dialogue or any other scenarios).</li>
<li>Calculate the QKV vectors of the new tokens: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$Q,K,V = S_2W_{Q,K,V}$</math-renderer> =&gt; [Mx4096] x [4096x128] = [Mx128].</li>
<li>Add positional information to the QK vectors: The positions of new tokens should start from N + 1, not from 0. [Mx128] -&gt; [Mx128].</li>
<li>Add the new KV values to the KV cache to get the updated KV matrix, that is, [Nx128] -&gt; [(N + M)x128].</li>
<li>Calculate the attention weights of the new tokens: Attention_weight = softmax(QK/sqrt(d) + mask) =&gt; [Mx128] x [128x(N + M)] = [Mx(N + M)].</li>
<li>Calculate the final result of the attention mechanism for the new tokens: Attention_weight x V =&gt; [Mx(N + M)] x [(N + M)x128] = [Mx128].</li>
<li>Concatenate the results of each head and perform a linear mapping to get the final output of the attention layer, with a shape of 32x[Mx128] -&gt; [Mx4096].
</li>
</ol>
<p dir="auto">Since our previous learning process has been quite comprehensive, we won't implement the code for the optimization scheme here (if you're interested, you can refer to the official code of Llama 3, which is relatively easy to implement). Just like the parallel calculation of multi-head attention mentioned before, knowing that the calculation process can be optimized is enough~</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Thank you all. Thanks for your continuous learning. Love you all :)</h2><a id="user-content-thank-you-all-thanks-for-your-continuous-learning-love-you-all-" aria-label="Permalink: Thank you all. Thanks for your continuous learning. Love you all :)" href="#thank-you-all-thanks-for-your-continuous-learning-love-you-all-"></a></p>
<p dir="auto">Our learning has come to an end. I hope you have also enjoyed this reading process!</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">From Me</h2><a id="user-content-from-me" aria-label="Permalink: From Me" href="#from-me"></a></p>
<p dir="auto">If you've come across this work, thank you for your trust and for learning all the way to this point. I'm glad to be of help to you~
<br></p>
<p dir="auto">If you'd like to support my work</p>
<ol dir="auto">
<li>give it a star⭐~ :)</li>
<li>buy me a coffee~ <a href="https://ko-fi.com/therealoliver" rel="nofollow">https://ko-fi.com/therealoliver</a></li>
</ol>

<p dir="auto"><h2 tabindex="-1" dir="auto">From the author of predecessor project</h2><a id="user-content-from-the-author-of-predecessor-project" aria-label="Permalink: From the author of predecessor project" href="#from-the-author-of-predecessor-project"></a></p>
<p dir="auto">If you want to support my work</p>
<ol dir="auto">
<li>follow me on twitter <a href="https://twitter.com/naklecha" rel="nofollow">https://twitter.com/naklecha</a></li>
<li>or, buy me a coffee <a href="https://www.buymeacoffee.com/naklecha" rel="nofollow">https://www.buymeacoffee.com/naklecha</a></li>
</ol>
<p dir="auto">Honestly, if you made it this far you already made my day :)</p>
<p dir="auto">what motivates me?</p>
<p dir="auto">My friends and I are on a mission - to make research more accessible!
We created a research lab called A10 - <a href="http://aaaaaaaaaa.org/" rel="nofollow">AAAAAAAAAA.org</a></p>
<p dir="auto">A10 twitter - <a href="https://twitter.com/aaaaaaaaaaorg" rel="nofollow">https://twitter.com/aaaaaaaaaaorg</a></p>
<p dir="auto">our thesis:</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/a10.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/a10.png" width="600px"></a>
</p>
<p>
Thanks again to the original author for the base code and illustrations, which also taught me a lot
</p><p dir="auto"><h2 tabindex="-1" dir="auto">LICENSE</h2><a id="user-content-license" aria-label="Permalink: LICENSE" href="#license"></a></p>
<p dir="auto">Copyright (c) 2025 Jinlong Zhang (<a href="https://github.com/therealoliver">https://github.com/therealoliver</a>)</p>
<p dir="auto">Copyright (c) 2024 Nishant Aklecha</p>
<p dir="auto">MIT</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>