<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 03 Jul 2024 20:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[A practical introduction to constraint programming using CP-SAT and Python (105 pts)]]></title>
            <link>https://pganalyze.com/blog/a-practical-introduction-to-constraint-programming-using-cp-sat</link>
            <guid>40867746</guid>
            <pubDate>Wed, 03 Jul 2024 16:48:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pganalyze.com/blog/a-practical-introduction-to-constraint-programming-using-cp-sat">https://pganalyze.com/blog/a-practical-introduction-to-constraint-programming-using-cp-sat</a>, See on <a href="https://news.ycombinator.com/item?id=40867746">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Imagine you're an e-commerce giant that would like to build a new warehouse to improve service to your customers, but you need to know what is the best location for it. Or you're a global shipping company that assigns packages to their delivery trucks and has to choose the best routes in order to save gas and reduce driver overtime. Or an airline that is looking to offer service to a new location, and needs to know which types of planes they should use and on what schedule, to maximize the resulting revenue.</p>
<p>These kinds of problems mentioned above are known as <strong>discrete optimization problems</strong>. There exist several methods that can be used to tackle such problems. In this article, we will discuss the theory and practice for one of them, called <strong><a href="https://en.wikipedia.org/wiki/Constraint_programming">constraint programming</a></strong>.</p>
<p>This is the first part in a two part series on constraint programming, used inside <a href="https://pganalyze.com/docs/indexing-engine/cp-model">pganalyze Indexing Engine</a> and <a href="https://pganalyze.com/docs/index-advisor/getting-started">pganalyze Index Advisor</a>. We're sharing this knowledge to help the wider programming community become familiar with how to utilize solvers like <a href="https://developers.google.com/optimization/cp/cp_solver">CP-SAT</a> in practice.</p>
<div>
<ul>
<li>
<p><a href="#a-declarative-paradigm">A declarative paradigm</a></p>
</li>
<li>
<p><a href="#the-basics-of-constraint-programming-cp">The basics of constraint programming (CP)</a></p>
</li>
<li>
<p><a href="#a-practical-example-with-python-and-cp-sat">A practical example with Python and CP-SAT</a></p>
<ul>
<li><a href="#an-empty-model">An empty model</a></li>
<li><a href="#the-data">The data</a></li>
<li><a href="#the-variables">The variables</a></li>
<li><a href="#the-constraints">The constraints</a></li>
<li><a href="#solving-the-model">Solving the model</a></li>
<li><a href="#adding-more-constraints">Adding more constraints</a></li>
<li><a href="#interlude-the-solver-status">Interlude: The solver status</a></li>
<li><a href="#sorry-emma">"Sorry, Emma"</a></li>
<li><a href="#objective-distributing-the-shifts-more-evenly">Objective: Distributing the shifts more evenly</a></li>
</ul>
</li>
<li>
<p><a href="#concluding-remarks">Concluding remarks</a></p>
</li>
</ul>
</div>
<h2 id="a-declarative-paradigm"><a href="#a-declarative-paradigm" aria-label="a declarative paradigm permalink"></a>A declarative paradigm</h2>
<p>Constraint programming (CP) is a declarative paradigm used to solve discrete optimization problems. This contrasts with the imperative paradigm that we are generally used to. When programming imperatively, we describe the steps necessary to reach a result. For example, suppose that we want to know who the adults are from a given list of people:</p>








<table><colgroup>
<col></colgroup><colgroup><col>
</colgroup><thead><tr><th scope="col">Name</th><th scope="col">Age</th></tr></thead><tbody><tr><td>Phil</td><td>20</td></tr><tr><td>Emma</td><td>17</td></tr><tr><td>David</td><td>11</td></tr><tr><td>Thomas</td><td>51</td></tr><tr><td>Sarah</td><td>45</td></tr><tr><td>Rebecca</td><td>6</td></tr></tbody></table>
<p>A typical imperative approach would explain the sequence of operations required to get the desired result:</p>
<div data-language="python"><pre><code>adult_people <span>=</span> <span>[</span><span>]</span>
<span>for</span> person <span>in</span> people<span>:</span>
    <span>if</span> person<span>.</span>Age <span>&gt;=</span> <span>18</span><span>:</span>
        adult_people <span>+=</span> person<span>.</span>Name</code></pre></div>

<p>Meanwhile, the same result can be described declaratively as:</p>
<div data-language="sql"><pre><code><span>SELECT</span> person_name <span>FROM</span> people <span>WHERE</span> age <span>&gt;=</span> <span>18</span><span>;</span></code></pre></div>

<p>Both approaches are equivalent in their outcome, but the two processes are different. In the imperative case, the program follows each step in sequence in order to reach the result. In the declarative case, the program is given a description of the desired result (using the available constructs of the language) and gets there by itself.</p>
<h2 id="the-basics-of-constraint-programming-cp"><a href="#the-basics-of-constraint-programming-cp" aria-label="the basics of constraint programming cp permalink"></a>The basics of constraint programming (CP)</h2>
<p>Similarly to the declarative example mentioned above, with CP we describe the desired result to a problem. This description is called a <strong>model</strong>. The main components of a model are variables and constraints. Variables represent <strong>what</strong> we are looking for, and each variable has an associated <strong>domain</strong> which is the set of values that this variable is allowed to take. Constraints describe <strong>relationships</strong> between variables.</p>
<p>A solution is an assignment of values to the variables (from their domains) such that the constraints are satisfied. Let's jump straight in with a simple example:</p>
<blockquote>
<div><p>Alice, Bob, and Carol each have $20, and they want to pool their money to purchase a candy bar worth $50 (yes, inflation is running wild). Alice has stated that she will put in at least as much money as Bob. Carol only has $5 bills, so her contribution will be a multiple of that. None of them want to contribute the exact same amount as any other.
</p><p>
How much should each of them chip in?</p></div>
</blockquote>
<p>Here, we are looking for the amount of money each person should contribute to the purchase of the candy bar. This means that we need one variable per person, indicating the amount of money that this person should contribute toward the purchase (<code>a</code> for Alice, <code>b</code> for Bob, and <code>c</code> for Carol). We start by setting the domains of these variables (the symbol <code>∈</code> means "in"):</p>
<div data-language="text"><pre><code>a ∈ {0, ..., 20}
b ∈ {0, ..., 20}
c ∈ {0, ..., 20}</code></pre></div>
<p>These domains ensure that the final values of the variables represent amounts of money that the contributors actually have in their pockets (that is, a maximum of $20). Next, we want to make sure that the combined contributions are enough to cover the price of the candy bar, so we add the constraint:</p>

<p>Alice will contribute at least as much as Bob, so we translate this into:</p>

<p>Carol's contribution must be a multiple of 5:</p>

<p>Finally, the amount of each of their contributions must be unique. We could model this using the following constraints:</p>

<p>We only have a handful of people in this example so these disequalities would work well. But what if they were hundreds, or thousands? It turns out that CP has a rich catalogue of expressive constraints that can encapsulate complex concepts, called <strong>global constraints</strong>. An alternative to the above would be to use the so-called <code>alldifferent</code> constraint, which ensures that a set of variables are all assigned different values:</p>

<p>This completes the model of this problem. You will note that we have not assigned any values to <code>a</code>, <code>b</code>, or <code>c</code> ourselves. We have simply defined three variables and their domains, and described the properties of the problem using constraints on those variables. Our job is done.</p>
<p>The piece of software that interprets this model and returns a solution is called a <strong>solver</strong>. The inner workings of a solver are outside the scope of this article, so for our purposes we will consider the solver as a black box that takes a model as input, and returns a valid solution:</p>

<p>The solution returned is valid as each variable takes a value from its domain and all the constraints are satisfied. However, we see that Carol contributes almost twice Bob's amount. Perhaps there exists another valid solution where all parties contribute more equally to the purchase?</p>
<p>We can add an <strong>objective</strong> to our CP model to try to reach this goal. Adding an objective to a model allows us to minimize or maximize an expression, without compromising the validity of the resulting solution (with respect to the constraints). If we can somehow minimize the amount of money spent by the largest contributor, this should push the three contributions closer together. Our objective will then be to find a valid solution where this value is minimal:</p>
<div data-language="text"><pre><code>x ∈ {0, ..., 20}
maximum(x, [a, b, c])
minimize: x</code></pre></div>
<p>To achieve this, we create a new variable <code>x</code> representing the amount of the largest contribution. The <code>maximum</code> constraint takes care of assigning to <code>x</code> the largest value from <code>[a, b, c]</code>. The objective is then to minimize <code>x</code>. The solution returned by the solver is now:</p>
<div data-language="text"><pre><code>a = 18
b = 17
c = 15
x = 18</code></pre></div>
<p>Previously there was a $9 difference between the largest and smallest contributions. With the objective we introduced this has now been reduced to $3, and this is as fair as this is going to get. Now that the basic concepts are cleared up, let's move on to a more challenging problem.</p>
<h2 id="a-practical-example-with-python-and-cp-sat"><a href="#a-practical-example-with-python-and-cp-sat" aria-label="a practical example with python and cp sat permalink"></a>A practical example with Python and CP-SAT</h2>
<p>Let's use this new CP knowledge to solve a more complex real-world example: the scheduling of employees for a small business.</p>
<blockquote>
<div><p>A store owner wishes to create the weekly work schedule for its employees. The store is open from 8AM to 8PM every day, and each day is divided into three shifts of 4 hours: morning, afternoon, and evening. There are two roles in the store: cashier and restocker.
</p></div>
<ul>
<li>
<p>Some employees are qualified to do either role, but others can only be a cashier, or a restocker.</p>
</li>
<li>
<p>There has to be a cashier scheduled at all times, but restocking only takes about 4 hours every day. Hence, for the restocking task we only need to schedule an employee for a single shift every day. This can be any shift, but two restocking shifts cannot be scheduled one after the other. If a restocking is scheduled on the evening shift on Tuesday, for example, we cannot schedule the Wednesday restocking on the morning shift.</p>
</li>
<li>
<p>An employee that is qualified in both roles can still only be assigned to one role per shift.</p>
</li>
<li>
<p>Employees cannot work more than 8 hours per day, which is 2 shifts. If they do work 2 shifts in a day, we must ensure that there is no idle time between these shifts—for example, we can't schedule them on both the morning and the evening shifts of the same day, as they would be idle for 4 hours during the afternoon shift.</p>
</li>
</ul>
</blockquote>
<p>This is the basic premise of the problem. Let's break this down into manageable parts.</p>
<h3 id="an-empty-model"><a href="#an-empty-model" aria-label="an empty model permalink"></a>An empty model</h3>
<p>We first start by creating an empty model using <a href="https://developers.google.com/optimization/cp/cp_solver">CP-SAT</a>, an open-source CP solver developed by Google as part of it's <a href="https://developers.google.com/optimization">OR-Tools</a> project.</p>
<div data-language="python"><pre><code><span>from</span> ortools<span>.</span>sat<span>.</span>python <span>import</span> cp_model

model <span>=</span> cp_model<span>.</span>CpModel<span>(</span><span>)</span></code></pre></div>
<h3 id="the-data"><a href="#the-data" aria-label="the data permalink"></a>The data</h3>
<blockquote>
<p>A store owner wishes to create the <span>weekly work schedule</span> for its <span>employees</span>. The store is open from 8AM to 8PM every day, and each day is divided into <span>three shifts</span> of 4 hours: <span>morning, afternoon, and evening</span>. There are <span>two roles</span> in the store: <span>cashier and restocker</span>. Some employees are <span>qualified</span> to do either role, but others can only be a cashier, or a restocker.</p>
</blockquote>
<p>Let's create a list of employees and the roles they are qualified for:</p>
<div data-language="python"><pre><code>employees <span>=</span> <span>{</span><span>"Phil"</span><span>:</span> <span>[</span><span>"Restocker"</span><span>]</span><span>,</span>
             <span>"Emma"</span><span>:</span> <span>[</span><span>"Cashier"</span><span>,</span> <span>"Restocker"</span><span>]</span><span>,</span>
             <span>"David"</span><span>:</span> <span>[</span><span>"Cashier"</span><span>,</span> <span>"Restocker"</span><span>]</span><span>,</span>
             <span>"Rebecca"</span><span>:</span> <span>[</span><span>"Cashier"</span><span>]</span><span>}</span></code></pre></div>
<p>The schedule is said to span a week, and we are told that there are three types of shifts, and two types of roles:</p>
<div data-language="python"><pre><code>days <span>=</span> <span>[</span><span>"Monday"</span><span>,</span>
        <span>"Tuesday"</span><span>,</span>
        <span>"Wednesday"</span><span>,</span>
        <span>"Thursday"</span><span>,</span>
        <span>"Friday"</span><span>,</span>
        <span>"Saturday"</span><span>,</span>
        <span>"Sunday"</span><span>]</span>

shifts <span>=</span> <span>[</span><span>"Morning"</span><span>,</span>
          <span>"Afternoon"</span><span>,</span>
          <span>"Evening"</span><span>]</span>

roles <span>=</span> <span>[</span><span>"Cashier"</span><span>,</span>
         <span>"Restocker"</span><span>]</span></code></pre></div>
<h3 id="the-variables"><a href="#the-variables" aria-label="the variables permalink"></a>The variables</h3>
<p>Now, let's define <strong>what</strong> we are looking for. To describe the schedule, we need to refer to employees, roles, days, and shifts: <strong>Does Emma work as a restocker on the Monday evening shift?</strong> This can be achieved using boolean variables. A boolean variable is a variable with a domain of <code>{0, 1}</code>.</p>
<div data-language="python"><pre><code>schedule <span>=</span> <span>{</span>e<span>:</span>
             <span>{</span>r<span>:</span>
               <span>{</span>d<span>:</span>
                 <span>{</span>s<span>:</span> model<span>.</span>new_bool_var<span>(</span><span><span>f"schedule_</span><span><span>{</span>e<span>}</span></span><span>_</span><span><span>{</span>r<span>}</span></span><span>_</span><span><span>{</span>d<span>}</span></span><span>_</span><span><span>{</span>s<span>}</span></span><span>"</span></span><span>)</span>
                   <span>for</span> s <span>in</span> shifts<span>}</span>
                 <span>for</span> d <span>in</span> days<span>}</span>
               <span>for</span> r <span>in</span> roles<span>}</span>
             <span>for</span> e <span>in</span> employees<span>}</span></code></pre></div>
<p>The function <code>model.new_bool_var()</code> creates and then returns a boolean variable, which we store in <code>schedule</code>. Within this structure, <code>schedule["Emma"]["Restocker"]["Monday"]["Evening"]</code> refers to one of those boolean variables. This variable is equal to <code>1</code> if Emma <strong>does</strong> work as a restocker on the Monday evening shift, or to <code>0</code> if she <strong>doesn't</strong>.</p>
<p>Our <code>schedule</code> variables are currently unconstrained. By following the problem description presented earlier and constraining these variables accordingly, we should be able to get a schedule that satisfies the requirements of the store owner.</p>
<h3 id="the-constraints"><a href="#the-constraints" aria-label="the constraints permalink"></a>The constraints</h3>
<p>Let's go through the rest of the problem description to correctly constrain the <code>schedule</code> variables. To add a new constraint to the model, we simply use <code>model.add(...)</code>.</p>
<blockquote>
<p>There has to be a <span>cashier</span> scheduled at <span>all times</span>.</p>
</blockquote>
<p>Since the schedule is comprised of boolean variables, it's easy to see how we can put limits on subsets of these variables by summing them and enforcing constraints on these sums:</p>
<div data-language="python"><pre><code><span>for</span> d <span>in</span> days<span>:</span>
    <span>for</span> s <span>in</span> shifts<span>:</span>
        model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span><span>"Cashier"</span><span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> e <span>in</span> employees<span>)</span> <span>==</span> <span>1</span><span>)</span></code></pre></div>
<p>If we need a cashier at all times, this means that for every day-shift pair, the sum of employees assigned the <code>"Cashier"</code> role has to be equal to <code>1</code>.</p>
<blockquote>
<p>For the <span>restocking</span> task we only need to schedule an employee for <span>a single shift every day</span>.</p>
</blockquote>
<p>Similarly to the previous constraint, we now want the sum of all employees assigned the <code>"Restocker"</code> role for all shifts of a given day to be equal to <code>1</code>:</p>
<div data-language="python"><pre><code><span>for</span> d <span>in</span> days<span>:</span>
    model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span><span>"Restocker"</span><span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> e <span>in</span> employees <span>for</span> s <span>in</span> shifts<span>)</span> <span>==</span> <span>1</span><span>)</span></code></pre></div>
<blockquote>
<p>This [restocking] can be any shift, but <span>two restocking shifts cannot be scheduled one after the other</span>.</p>
</blockquote>
<p>Because of the previous constraint, we already know that two restocking shifts cannot take place on the same day. The only way two restocking shifts could be scheduled one after the other would be to assign one on the evening shift of a day, and another one on the morning shift of the next day. By enforcing that the sum of restocking shifts for each evening-morning pair is not greater than <code>1</code>, we ensure that <strong>at most one</strong> restocking shift is scheduled for those pairs:</p>
<div data-language="python"><pre><code><span>for</span> i <span>in</span> <span>range</span><span>(</span><span>len</span><span>(</span>days<span>)</span><span>-</span><span>1</span><span>)</span><span>:</span>
    model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span><span>"Restocker"</span><span>]</span><span>[</span>days<span>[</span>i<span>]</span><span>]</span><span>[</span><span>"Evening"</span><span>]</span> <span>+</span> schedule<span>[</span>e<span>]</span><span>[</span><span>"Restocker"</span><span>]</span><span>[</span>days<span>[</span>i<span>+</span><span>1</span><span>]</span><span>]</span><span>[</span><span>"Morning"</span><span>]</span> <span>for</span> e <span>in</span> employees<span>)</span> <span>&lt;=</span> <span>1</span><span>)</span></code></pre></div>
<blockquote>
<p>An <span>employee</span> that is qualified in both roles can still <span>only be assigned to one role per shift</span>.</p>
</blockquote>
<p>For every employee, the sum of all assigned roles for all day-shift pairs is either going to be <code>1</code> (they work a <em>single</em> role on this day-shift slot), or <code>0</code> (they don't work on that day-shift slot):</p>
<div data-language="python"><pre><code><span>for</span> e <span>in</span> employees<span>:</span>
    <span>for</span> d <span>in</span> days<span>:</span>
        <span>for</span> s <span>in</span> shifts<span>:</span>
            model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles<span>)</span> <span>&lt;=</span> <span>1</span><span>)</span></code></pre></div>
<blockquote>
<p>Some employees are qualified to do either role, but <span>others can only be a cashier, or a restocker</span>.</p>
</blockquote>
<p>To prevent an employee from being assigned a role that they are not qualified for, we simply match the value of that role to 0 (or put differently, we're adding a constraint asserting that it's zero) everywhere for that employee:</p>
<div data-language="python"><pre><code><span>for</span> e <span>in</span> employees<span>:</span>
    <span>for</span> r <span>in</span> roles<span>:</span>
        <span>for</span> d <span>in</span> days<span>:</span>
            <span>for</span> s <span>in</span> shifts<span>:</span>
                <span>if</span> r <span>not</span> <span>in</span> employees<span>[</span>e<span>]</span><span>:</span>
                    model<span>.</span>add<span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>==</span> <span>0</span><span>)</span></code></pre></div>
<blockquote>
<p><span>Employees cannot work more</span> than 8 hours per day, which is <span>2 shifts</span>. If they do work 2 shifts in a day, we must ensure that there is <span>no idle time between these shifts</span>—in other words, we can't schedule them on both the morning and the evening shifts of the same day, as they would be idle for 4 hours during the afternoon shift.</p>
</blockquote>
<p>It turns out that a single constraint can take care of both of these requirements: An employee can work <strong>either</strong> the morning shift, <strong>or</strong> the evening shift, <strong>or</strong> neither of these shifts:</p>
<div data-language="python"><pre><code><span>for</span> e <span>in</span> employees<span>:</span>
    <span>for</span> d <span>in</span> days<span>:</span>
        model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span><span>"Morning"</span><span>]</span> <span>+</span> schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span><span>"Evening"</span><span>]</span> <span>for</span> r <span>in</span> roles<span>)</span> <span>&lt;=</span> <span>1</span><span>)</span></code></pre></div>
<p>Note that the above constraint does not need to specify anything about the afternoon shift. If the employee works in the morning, they can't work in the evening, and vice-versa. This both ensures that the employee works a maximum of two shifts per day, and also that there is no idle time, since there can only be idle time if the employee works both the morning <strong>and</strong> the evening shift.</p>
<p>The modeling of the problem is now complete. Let's see what the results are.</p>
<h3 id="solving-the-model"><a href="#solving-the-model" aria-label="solving the model permalink"></a>Solving the model</h3>
<p>To solve the model, we simply call a solver, with the model as an argument:</p>
<div data-language="python"><pre><code>solver <span>=</span> cp_model<span>.</span>CpSolver<span>(</span><span>)</span>

solver<span>.</span>solve<span>(</span>model<span>)</span></code></pre></div>
<p>After the solving process, we can get the resulting values of the <code>schedule</code> variables with <code>solver.value(...)</code>. We have organized these values in the schedule shown below:</p>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   7   |
Emma       |   |   |   | C |   |   | C |   |   | C |   |   | C |   |   | C |   |   | C |   |   |   6   |
David      | C |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   1   |
Rebecca    |   | C | C |   | C | C |   | C | C |   | C | C |   | C | C |   | C | C |   | C | C |  14   |</code></pre></div>
<p>The resulting schedule satisfies the constraints of the problem. Now let's think of additional real-world constraints that would make this more interesting.</p>
<h3 id="adding-more-constraints"><a href="#adding-more-constraints" aria-label="adding more constraints permalink"></a>Adding more constraints</h3>
<p>We see that Rebecca works 14 shifts for that week. The store owner is not keen on paying overtime wages, so they wish to cap each employee's schedule to a maximum of 40 hours per week (10 work shifts):</p>
<div data-language="python"><pre><code><span>for</span> e <span>in</span> employees<span>:</span>
    model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> days <span>for</span> s <span>in</span> shifts<span>)</span> <span>&lt;=</span> <span>10</span><span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   | R |   |   7   |
Emma       |   | C | C |   | C | C | C |   |   | C |   |   | C |   |   | C |   |   | C |   |   |   9   |
David      | C |   |   | C |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   2   |
Rebecca    |   |   |   |   |   |   |   | C | C |   | C | C |   | C | C |   | C | C |   | C | C |  10   |</code></pre></div>
<p>Phil is a full-time student and would like to work exactly 4 shifts per week. He also cannot work the morning and afternoon shifts during the week, in order to attend his classes.</p>
<div data-language="python"><pre><code>model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span><span>"Phil"</span><span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> days <span>for</span> s <span>in</span> shifts<span>)</span> <span>==</span> <span>4</span><span>)</span>

model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span><span>"Phil"</span><span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> days <span>if</span> d <span>not</span> <span>in</span> <span>[</span><span>"Saturday"</span><span>,</span> <span>"Sunday"</span><span>]</span> <span>for</span> s <span>in</span> shifts <span>if</span> s <span>in</span> <span>[</span><span>"Morning"</span><span>,</span> <span>"Afternoon"</span><span>]</span><span>)</span> <span>==</span> <span>0</span><span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   |   |   |   |   | R |   |   | R |   |   | R |   |   |   |   | R |   |   |   |   |   4   |
Emma       | C | R |   |   |   | C | C |   |   | C |   |   | C | R |   | C |   |   | C | R |   |  10   |
David      |   | C | C | C | C |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   4   |
Rebecca    |   |   |   |   |   |   |   | C | C |   | C | C |   | C | C |   | C | C |   | C | C |  10   |</code></pre></div>
<p>Phil and Emma do not get along very well, and as such we don't want them working the same shifts.</p>
<div data-language="python"><pre><code><span>for</span> d <span>in</span> days<span>:</span>
    <span>for</span> s <span>in</span> shifts<span>:</span>
        model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> e <span>in</span> <span>[</span><span>"Phil"</span><span>,</span> <span>"Emma"</span><span>]</span> <span>for</span> r <span>in</span> roles<span>)</span> <span>&lt;=</span> <span>1</span><span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   |   | R |   |   | R |   |   | R |   |   | R |   |   |   |   |   |   |   |   |   |   4   |
Emma       | C |   |   | C | C |   | C | C |   | C | C |   | C | C |   |   | C |   |   |   |   |  10   |
David      |   | C | C |   |   | C |   |   | C |   |   | C |   | R |   |   | R | C | C | R |   |  10   |
Rebecca    |   |   |   |   |   |   |   |   |   |   |   |   |   |   | C | C |   |   |   | C | C |   4   |</code></pre></div>
<p>No employee really likes to work on the weekend, so we would like to distribute these shifts equally between all employees. There are 8 such shifts (3 cashier shifts and 1 restocker shift on each day), and since we have 4 employees, we can give them all 2 shifts each for the weekend.</p>
<div data-language="python"><pre><code><span>for</span> e <span>in</span> employees<span>:</span>
    model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> <span>[</span><span>"Saturday"</span><span>,</span> <span>"Sunday"</span><span>]</span> <span>for</span> s <span>in</span> shifts<span>)</span> <span>==</span> <span>2</span><span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   |   |   |   |   | R |   |   | R |   |   |   |   |   |   | R |   |   | R |   |   |   4   |
Emma       | R | C |   | C | C |   | C |   |   |   |   | C |   | C | C |   |   |   |   | C | C |  10   |
David      | C |   |   |   |   | C |   | C | C | C | R |   | C | R |   |   |   | C | C |   |   |  10   |
Rebecca    |   |   | C |   |   |   |   |   |   |   | C |   |   |   |   | C | C |   |   |   |   |   4   |</code></pre></div>
<p>Emma would like to take Monday to Friday off, and asks us if this is possible. Let's see:</p>
<div data-language="python"><pre><code>model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span><span>"Emma"</span><span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> <span>[</span><span>"Monday"</span><span>,</span> <span>"Tuesday"</span><span>,</span> <span>"Wednesday"</span><span>,</span> <span>"Thursday"</span><span>,</span> <span>"Friday"</span><span>]</span> <span>for</span> s <span>in</span> shifts<span>)</span> <span>==</span> <span>0</span><span>)</span></code></pre></div>

<p>The solver returns a status of "infeasible". What does this mean?</p>
<h3 id="interlude-the-solver-status"><a href="#interlude-the-solver-status" aria-label="interlude the solver status permalink"></a>Interlude: The solver status</h3>
<p>The solver takes as input a model, and returns a status and a solution. Consider this simple case:</p>
<div data-language="text"><pre><code>x ∈ {0, ..., 10}
y ∈ {0, ..., 10}
x + y &gt;= 5
minimize: x + y

Status: OPTIMAL
x = 5
y = 0</code></pre></div>
<p>The variables <code>x</code> and <code>y</code> both have the domain <code>{0, ..., 10}</code>. The sum of the values assigned to them must be 5 or greater, and the objective is to minimize this sum. The solution <code>(x, y) = (5, 0)</code> is called <strong>optimal</strong>. A solution is optimal when there does not exist another solution that is <strong>strictly better</strong> than it. For instance, the solution <code>(x, y) = (3, 2)</code> would also be optimal.</p>
<p>Now consider the case:</p>
<div data-language="text"><pre><code>x ∈ {0, ..., 10}
x &gt;= 15

Status: INFEASIBLE</code></pre></div>
<p>An <strong>infeasible</strong> status means that there exists no assignment of values to the variables such that the constraints are satisfied. In the above example, it is not possible to assign a value to <code>x</code> that is at least as large as 15, since the largest value in its domain is 10.</p>
<p>There are also other possibilities. Let's take a look at this last example:</p>
<div data-language="text"><pre><code>x ∈ {0, ..., 10}
y ∈ {0, ..., 10}
...  // Many more variables
x + y &lt;= 12
x &gt;= 5
... // Many more constraints
minimize: x - 3*y + ...  // Very complex objective</code></pre></div>
<p>Sometimes, a problem is so large and complex, and takes so much time to solve, that we must interrupt the solver due to time constraints. In such a case, the solver will return one of two statuses:</p>
<ul>
<li>If a solution has been found, the status returned is <strong>feasible</strong>. A feasible solution means that the solution satisfies the constraints, but the solver does not know if that solution is optimal. In other words, we have a solution that works, but there may still exist a better one.</li>
<li>If no solution has been found, the status returned is <strong>unknown</strong>. This means that while the solver has not found a solution, it does not know whether one exists (feasible) or if the problem has no solution (infeasible).</li>
</ul>
<p>Let's get back to our schedule.</p>
<h3 id="sorry-emma"><a href="#sorry-emma" aria-label="sorry emma permalink"></a>"Sorry, Emma"</h3>
<p>With the infeasible status returned to us previously, we are forced to inform Emma that she can't take the whole week off, otherwise it would be impossible to fill the schedule without violating some of the other constraints. She decides to only take Monday to Wednesday off instead:</p>
<div data-language="python"><pre><code>model<span>.</span>add<span>(</span><span>sum</span><span>(</span>schedule<span>[</span><span>"Emma"</span><span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> <span>[</span><span>"Monday"</span><span>,</span> <span>"Tuesday"</span><span>,</span> <span>"Wednesday"</span><span>]</span> <span>for</span> s <span>in</span> shifts<span>)</span> <span>==</span> <span>0</span><span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   |   | R |   |   |   |   |   | R |   |   |   |   |   |   | R |   |   | R |   |   |   4   |
Emma       |   |   |   |   |   |   |   |   |   |   | R | C |   | R | C |   |   |   |   | C | C |   6   |
David      | C |   |   | C | R |   | C |   |   | C | C |   | C | C |   |   |   | C | C |   |   |  10   |
Rebecca    |   | C | C |   | C | C |   | C | C |   |   |   |   |   |   | C | C |   |   |   |   |   8   |</code></pre></div>
<p>Phil works exactly 4 shifts as he wants, but the other shifts are not very well distributed: Emma gets 6, David gets 10, and Rebecca gets 8. Let's see if we can improve this by adding an objective.</p>
<h3 id="objective-distributing-the-shifts-more-evenly"><a href="#objective-distributing-the-shifts-more-evenly" aria-label="objective distributing the shifts more evenly permalink"></a>Objective: Distributing the shifts more evenly</h3>
<p>We would like to distribute the shifts as fairly as possible between Emma, David, and Rebecca. We will recall that an objective allows us to minimize the value of an expression. We could, for example, minimize the shift difference between the employee who is assigned the fewest shifts, and the one who is assigned the most. Currently, Emma is assigned 6 shifts to David's 10, so this difference stands at 4 shifts.</p>
<p>We start by creating integer variables to track the number of shifts assigned to each employee, and enforcing the values of these variables:</p>
<div data-language="python"><pre><code><span># total_shifts[e] indicates the number of shifts worked by employee `e`</span>
total_shifts <span>=</span> <span>{</span>e<span>:</span> model<span>.</span>new_int_var<span>(</span><span>0</span><span>,</span> <span>10</span><span>,</span> <span><span>f"total_shifts_</span><span><span>{</span>e<span>}</span></span><span>"</span></span><span>)</span>
                <span>for</span> e <span>in</span> employees<span>}</span>

<span>for</span> e <span>in</span> employees<span>:</span>
    model<span>.</span>add<span>(</span>total_shifts<span>[</span>e<span>]</span> <span>==</span> <span>sum</span><span>(</span>schedule<span>[</span>e<span>]</span><span>[</span>r<span>]</span><span>[</span>d<span>]</span><span>[</span>s<span>]</span> <span>for</span> r <span>in</span> roles <span>for</span> d <span>in</span> days <span>for</span> s <span>in</span> shifts<span>)</span><span>)</span></code></pre></div>
<p>Integer variables are created with <code>model.new_int_var(...)</code>, and the parameters allow us to specify the lower and upper bounds of the domain (in this case, the domain is <code>{0, ..., 10}</code>). We then create variables to track the highest and lowest number of shifts assigned to any employee (with the exception of Phil, who works part-time):</p>
<div data-language="python"><pre><code>min_shifts <span>=</span> model<span>.</span>new_int_var<span>(</span><span>0</span><span>,</span> <span>10</span><span>,</span> <span>"min_shifts"</span><span>)</span>
model<span>.</span>add_min_equality<span>(</span>min_shifts<span>,</span> <span>[</span>total_shifts<span>[</span>e<span>]</span> <span>for</span> e <span>in</span> employees <span>if</span> e <span>!=</span> <span>"Phil"</span><span>]</span><span>)</span>

max_shifts <span>=</span> model<span>.</span>new_int_var<span>(</span><span>0</span><span>,</span> <span>10</span><span>,</span> <span>"max_shifts"</span><span>)</span>
model<span>.</span>add_max_equality<span>(</span>max_shifts<span>,</span> <span>[</span>total_shifts<span>[</span>e<span>]</span> <span>for</span> e <span>in</span> employees <span>if</span> e <span>!=</span> <span>"Phil"</span><span>]</span><span>)</span></code></pre></div>
<p>The constraints <code>model.add_min_equality(...)</code> and <code>model.add_max_equality(...)</code> work in the same fashion as the <code>maximum(...)</code> constraint presented in the candy bar example (with Alice, Bob, and Carol). For example, <code>model.add_min_equality(var, list)</code> assigns to integer variable <code>var</code> the smallest value among the integer variables in <code>list</code>.</p>
<p>Finally, we minimize the difference between <code>max_shifts</code> and <code>min_shifts</code>:</p>
<div data-language="python"><pre><code>model<span>.</span>minimize<span>(</span>max_shifts <span>-</span> min_shifts<span>)</span></code></pre></div>
<div data-language="text"><pre><code>           |  Monday   |  Tuesday  | Wednesday | Thursday  |  Friday   | Saturday  |  Sunday   | Total |
           | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E | M | A | E |       |
Phil       |   |   | R |   |   |   |   |   | R |   |   |   |   |   |   | R |   |   | R |   |   |   4   |
Emma       |   |   |   |   |   |   |   |   |   |   | R | C |   | R | C |   |   |   |   | C | C |   6   |
David      | C |   |   | C | R |   | C |   |   |   | C |   | C | C |   |   |   | C | C |   |   |   9   |
Rebecca    |   | C | C |   | C | C |   | C | C | C |   |   |   |   |   | C | C |   |   |   |   |   9   |</code></pre></div>
<p>This looks pretty good. David and Rebecca both get 9 shifts. Emma only gets 6, but that is understandable as she is taking 3 days off that week. Phil gets exactly 4 shifts, as he is supposed to. Hopefully, everyone will be happy with this schedule.</p>

<p>We have introduced the basics of constraint programming and discussed its main components. We have built a model to generate work schedules that can take into account the types of constraints that one could reasonably expect to be faced with in the real world.</p>
<p>Thanks to this model, we could easily see that it would not be possible for Emma to take five days off as she initially wanted, and we were able to distribute the work shifts as fairly as possible between the employees. In the end, we created a schedule that satisfied both the requiremends of the store owner, as well as the needs of the employees.</p>
<p>In the next article, we will discuss how to use constraint programming for index selection in Postgres.</p>
<p><strong>The code presented in this article can be found on the <a href="https://github.com/pganalyze/cp-sat-python-example">pganalyze GitHub</a>.</strong></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Do not taunt happy fun branch predictor (2023) (177 pts)]]></title>
            <link>https://www.mattkeeter.com/blog/2023-01-25-branch/</link>
            <guid>40866374</guid>
            <pubDate>Wed, 03 Jul 2024 14:42:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mattkeeter.com/blog/2023-01-25-branch/">https://www.mattkeeter.com/blog/2023-01-25-branch/</a>, See on <a href="https://news.ycombinator.com/item?id=40866374">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<!-- End header -->






<p>I've been writing a lot of AArch64 assembly, for <em>reasons</em>.</p>
<p>I recently came up with a "clever" idea to eliminate one jump from an inner
loop, and was surprised to find that it slowed things down.  Allow me to explain
my terrible error, so that you don't fall victim in the future.</p>
<p>A toy model of the relevant code looks something like this:</p>
<pre><code>float run(const float* data, size_t n) {
    float g = 0.0;
    while (n) {
        n--;
        const float f = *data++;
        foo(f, &amp;g);
    }
    return g;
}

static void foo(float f, float* g) {
    // do some stuff, modifying g
}
</code></pre>
<p>(eliding headers and the forward declaration of <code>foo</code> for space)</p>
<p>A simple translation into AArch64 assembly gives something like this:</p>
<pre><code>// x0: const float* data
// x1: size_t n
// Returns a single float in s0

// Prelude: store frame and link registers
stp   x29, x30, [sp, #-16]!

// Initialize g = 0.0
fmov s0, #0.0

loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

    bl foo   // call the function
    b loop   // keep looping

foo:
    // Do some work, reading from s1 and accumulating into s0
    // ...
    ret

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>Here, <code>foo</code> is kinda like a <a href="https://github.com/rust-lang/rfcs/blob/master/text/1201-naked-fns.md">naked
function</a>:
it uses the same stack frame and registers as the parent function, reads from
<code>s1</code>, and writes to <code>s0</code>.</p>
<p>The call to <code>foo</code> uses the the <code>bl</code> instruction, which is "branch and link":
it jumps to the given label, and stores the <strong>next</strong> instruction address in the
link register (<code>lr</code> or <code>x30</code>).</p>
<p>When <code>foo</code> is done, the <code>ret</code> instruction jumps to the address in the link
register, which is the instruction following the original <code>bl</code>.</p>
<p>Looking at this code, I was struck by the fact that it does two branches,
one after the other.  Surely, it would be more efficient to only branch once.</p>
<p>I had the clever idea to do so <strong>without changing <code>foo</code></strong>:</p>
<pre><code>stp   x29, x30, [sp, #-16]!
fmov s0, #0.0

bl loop // Set up x30 to point to the loop entrance
loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

foo:
    // Do some work, accumulating into `s0`
    // ...
    ret

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>This is a little subtle:</p>
<ul>
<li>The first call to <code>bl loop</code> stores the beginning of the <code>loop</code> block in <code>x30</code></li>
<li>After checking for loop termination, we fall through into the <code>foo</code> function
(without a branch!)</li>
<li><code>foo</code> still ends with <code>ret</code>, which returns to the <code>loop</code> block (because
that's what's in <code>x30</code>).</li>
</ul>
<p>Within the body of the loop, we never change <code>x30</code>, so the repeated <code>ret</code>
instructions always return to the same place.</p>
<p>I set up a benchmark using a very simple <code>foo</code>:</p>
<pre><code>foo:
    fadd s0, s0, s1
    ret
</code></pre>
<p>With this <code>foo</code>, the function as a whole sums the incoming array of <code>float</code>
values.</p>
<p>Benchmarking with <a href="https://docs.rs/criterion/latest/criterion/"><code>criterion</code></a>
(on an M1 Max CPU),
with a 1024-element array:</p>
<table>
<tbody><tr><th>Program</th><th>Time
</th></tr><tr><td>Original </td><td>969 ns
</td></tr><tr><td>"Optimized"</td><td>3.85 µs
</td></tr></tbody></table>
<p>The "optimized" code with one jump per loop is about <strong>4x slower</strong>
than the original version with two jumps per loop!</p>
<p>I found this surprising, so I asked a few colleagues about it.</p>
<p>Between <a href="https://hachyderm.io/@cliffle">Cliff</a> and
<a href="https://discuss.systems/@cross">Dan</a>,
the consensus was that mismatched <code>bl</code> / <code>ret</code>
pairs were confusing the
<a href="https://en.wikipedia.org/wiki/Branch_predictor">branch predictor</a>.</p>
<p>The <a href="https://developer.arm.com/documentation/102374/0101/Function-calls">ARM documentation</a> agrees:</p>
<blockquote>
<p>Why do we need a special function return instruction? Functionally, BR LR
would do the same job as RET. Using RET tells the processor that this is a
function return. Most modern processors, and all Cortex-A processors, support
branch prediction. Knowing that this is a function return allows processors to
more accurately predict the branch.</p>
<p>Branch predictors guess the direction the program flow will take across
branches. The guess is used to decide what to load into a pipeline with
instructions waiting to be processed. If the branch predictor guesses
correctly, the pipeline has the correct instructions and the processor does
not have to wait for instructions to be loaded from memory.</p>
</blockquote>
<p>More specifically, the branch predictor probably keeps an internal stack of
function return addresses, which is pushed to whenever a <code>bl</code> is executed. When
the branch predictor sees a <code>ret</code> coming down the pipeline, it assumes that
you're returning to the address associated with the most recent <code>bl</code> (and begins
prefetching / speculative execution / whatever), then pops that top address from
its internal stack.</p>
<p>This works if you've got matched <code>bl</code> / <code>ret</code> pairs, but the prediction will
fail if the same address is used by multiple <code>ret</code> instructions; you'll end up
with (<em>vague handwaving</em>) useless prefetching, incorrect speculative execution,
and pipeline stalls / flushes</p>
<p>Dan made the great suggestion of replacing <code>ret</code> with <code>br x30</code> to test this
theory.  Sure enough, this fixes the performance regression:</p>
<table>
<tbody><tr><th>Program</th><th>Time
</th></tr><tr><td>Matched <code>bl</code> / <code>ret</code> </td><td>969 ns
</td></tr><tr><td>One <code>bl</code>, many <code>ret</code></td><td>3.85 µs
</td></tr><tr><td>One <code>bl</code>, many <code>br x30</code></td><td>913 ns
</td></tr></tbody></table>
<p>In fact, it's slightly faster, probably because it's only doing one branch
per loop instead of two!</p>
<p>To further test the "branch predictor" theory, I opened up Instruments and
examined performance counters for the first two programs. Picking out the worst
offenders, the results seem conclusive:</p>
<table>
<tbody><tr><th>Counter</th><th>Matched <code>bl</code> / <code>ret</code></th><th>One <code>bl</code>, many <code>ret</code>
</th></tr><tr><td><code>BRANCH_RET_INDIR_MISPRED_NONSPECIFIC</code></td><td>92</td><td>928,644,975
</td></tr><tr><td><code>FETCH_RESTART</code></td><td>61,121</td><td>987,765,276
</td></tr><tr><td><code>MAP_DISPATCH_BUBBLE</code></td><td>1,155,632</td><td>7,350,085,139
</td></tr><tr><td><code>MAP_REWIND</code></td><td>6,412,734</td><td>2,789,499,545
</td></tr></tbody></table>
<p>These measurements are captured while summing an array of 1B elements.  We see
that with mismatched <code>bl</code> / <code>ret</code> pairs, the return branch predictor fails about
93% of the time!</p>
<p>Apple doesn't fully document these counters, but I'm guessing that the other
counters are downstream effects of bad branch prediction:</p>
<ul>
<li><code>FETCH_RESTART</code> is presumably bad prefetching</li>
<li><code>MAP_DISPATCH_BUBBLE</code> probably refers to <a href="https://en.wikipedia.org/wiki/Pipeline_stall">pipeline stalls</a></li>
<li><code>MAP_REWIND</code> might be bad speculative execution that needs to be rewound</li>
</ul>
<p>In conclusion,
<a href="https://www.youtube.com/watch?v=GmqeZl8OI2M">do not taunt happy fun branch predictor</a>
with asymmetric usage of <code>bl</code> and <code>ret</code> instructions.</p>
<hr>
<h2>Appendix: Going Fast</h2>
<p>Take a second look at this program:</p>
<pre><code>stp   x29, x30, [sp, #-16]!
fmov s0, #0.0

loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

    bl foo   // call the function
    b loop   // keep looping

foo:
    fadd s0, s0, s1
    ret

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>Upon seeing this program, it's a common reaction to ask "why is <code>foo</code> a
subroutine at all?"</p>
<p>The answer is "because this is a didactic example, not code that's trying
to go as fast as possible".</p>
<p>Still, it's a fair question.  You wanna go fast?  Let's go fast.</p>
<p>If we know the contents of <code>foo</code> when building this
function (and it's shorter than the maximum jump distance), we can remove the
<code>bl</code> and <code>ret</code> entirely:</p>
<pre><code>loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

    // foo is completely inlined here
    fadd s0, s0, s1

    b loop

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>This is a roughly 6% speedup: from 969 ns to 911 ns.</p>
<p>We can get faster still by trusting the compiler:</p>
<pre><code>pub fn sum_slice(f: &amp;[f32]) -&gt; f32 {
    f.iter().sum()
}
</code></pre>
<p>This brings us down to 833 ns, a significant improvement!</p>
<p><a href="https://godbolt.org/z/Kv77abW6c">Looking at the assembly</a>,
it's doing some loop unrolling.
However, even when compiled with <code>-C target-cpu=native</code>, it's not generating
<a href="https://developer.arm.com/Architectures/Neon">NEON SIMD instructions</a>.
Can we beat it?</p>
<p><strong>We sure can!</strong></p>
<pre><code>stp   x29, x30, [sp, #-16]!

fmov s0, #0.0
dup v1.4s, v0.s[0]
dup v2.4s, v0.s[0]

loop:  // 1x per loop
    ands xzr, x1, #3
    b.eq simd

    sub x1, x1, #1
    ldr s3, [x0], #4

    fadd s0, s0, s3
    b loop

simd:  // 4x SIMD per loop
    ands xzr, x1, #7
    b.eq simd2

    sub x1, x1, #4
    ldp d3, d4, [x0], #16
    mov v3.d[1], v4.d[0]

    fadd v1.4s, v1.4s, v3.4s

    b simd

simd2:  // 2 x 4x SIMD per loop
    cmp x1, #0
    b.eq exit

    sub x1, x1, #8

    ldp d3, d4, [x0], #16
    mov v3.d[1], v4.d[0]
    fadd v1.4s, v1.4s, v3.4s

    ldp d5, d6, [x0], #16
    mov v5.d[1], v6.d[0]
    fadd v2.4s, v2.4s, v5.4s

    b simd2

exit: // function exit
    fadd v2.4s, v2.4s, v1.4s
    mov s1, v2.s[0]
    fadd s0, s0, s1
    mov s1, v2.s[1]
    fadd s0, s0, s1
    mov s1, v2.s[2]
    fadd s0, s0, s1
    mov s1, v2.s[3]
    fadd s0, s0, s1

    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>This code includes three different loops:</p>
<ul>
<li>The first loop (<code>loop</code>) sums individual values
into <code>s0</code> until we have a multiple of four values remaining</li>
<li>The second loop (<code>simd</code>) uses SIMD instructions to sum 4 values at a time
into the vector register <code>v1</code>, until we have a multiple of 8 values remaining</li>
<li>The last loop (<code>simd2</code>) is the same as <code>simd</code>, but is unrolled 2x so it
handles 8 values per loop iteration, summing into <code>v1</code> and <code>v2</code></li>
</ul>
<p>At the function exit, we accumulate the values in the vector registers <code>v1</code>/<code>v2</code>
into <code>s0</code>, which is returned.</p>
<p>The type punning here is particularly cute:</p>
<pre><code>ldp d3, d4, [x0], #16
mov v3.d[1], v4.d[0]
fadd v1.4s, v1.4s, v3.4s
</code></pre>
<p>Remember, <code>x0</code> holds a <code>float*</code>.  We pretend that it's a <code>double*</code> to load 128
bits (i.e. 4x <code>float</code> values) into <code>d3</code> and <code>d4</code>.  Then, we move the "double" in <code>d4</code>
to occupy the top 64 bits of the <code>v3</code> vector register (of which <code>d3</code> is the
<em>lower</em> 64 bits).</p>
<p>Of course, each "double" is two floats, but that doesn't matter when shuffling
them around.  When summing with <code>fadd</code>, we tell the processor to treat them as
four floats (the <code>.4s</code> suffix), and everything works out fine.</p>
<p>How fast are we now?</p>
<p>This runs in 94 ns, or about <strong>8.8x faster</strong> than our previous best.</p>
<p>Here's a summary of performance:</p>
<table>
<tbody><tr><th>Program</th><th>Time
</th></tr><tr><td>Matched <code>bl</code> / <code>ret</code> </td><td>969 ns
</td></tr><tr><td>One <code>bl</code>, many <code>ret</code></td><td>3.85 µs
</td></tr><tr><td>One <code>bl</code>, many <code>br x30</code></td><td>913 ns
</td></tr><tr><td>Plain loop with <code>b</code></td><td>911 ns
</td></tr><tr><td>Rewrite it in Rust</td><td>833 ns
</td></tr><tr><td>SIMD + manual loop unrolling</td><td>94 ns
</td></tr></tbody></table>
<p>Could we get even faster?  I'm sure it's possible; I make no claims to being
the <a href="https://www.agner.org/optimize/">Agner Fog</a> of AArch64 assembly.</p>
<p>Still, this is a reasonable point to wrap up: we've demystified the initial
performance regression, and had some fun hand-writing assembly to go very
fast indeed.</p>
<p>The SIMD code does come with one asterisk, though: because floating-point
addition is not associative, and it performs the summation in a different
order, it <strong>may not get the same result</strong> as straight-line code.  In retrospect,
this is likely why the compiler doesn't generate SIMD instructions to compute
the sum!</p>
<p>Does this matter for your use case?  Only you can know!</p>
<hr>
<p>All of the code from this post is
<a href="https://github.com/mkeeter/arm64-test">published to GitHub</a>.</p>
<p>You can reproduce benchmarks by running <code>cargo bench</code> on an ARM64 machine.</p>

<!-- Begin footer -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Has anyone successfully pivoted from web dev to AI/ML development? (110 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40866311</link>
            <guid>40866311</guid>
            <pubDate>Wed, 03 Jul 2024 14:35:13 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40866311">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="40866311">
      <td><span></span></td>      <td><center><a id="up_40866311" href="https://news.ycombinator.com/vote?id=40866311&amp;how=up&amp;goto=item%3Fid%3D40866311"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=40866311">Ask HN: Has anyone successfully pivoted from web dev to AI/ML development?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_40866311">106 points</span> by <a href="https://news.ycombinator.com/user?id=ent_superpos">ent_superpos</a> <span title="2024-07-03T14:35:13"><a href="https://news.ycombinator.com/item?id=40866311">4 hours ago</a></span> <span id="unv_40866311"></span> | <a href="https://news.ycombinator.com/hide?id=40866311&amp;goto=item%3Fid%3D40866311">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Has%20anyone%20successfully%20pivoted%20from%20web%20dev%20to%20AI%2FML%20development%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=40866311&amp;auth=1344ad73a11754902765b72c2daad9533241ef0b">favorite</a> | <a href="https://news.ycombinator.com/item?id=40866311">65&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>I am currently working as a senior full-stack web software engineer. I have a BSc in Computer Science, and on my own, I've been learning more about AI/ML/deep learning. I really enjoy working with it, and I'd love to find a way to work on AI stuff professionally. The problem is that I've been working as a web developer professionally for about 10 years now, and I have no idea how I would pivot to more of a AI/data science role.</p><p>Does anyone have an experience of making this transition? As a web dev, I am senior level, but I'm sure I'd have to start from scratch on some things in the AI space. At least I have a good foundation of programming in general, math, and computer science.</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Proton launches its own version of Google Docs (315 pts)]]></title>
            <link>https://www.engadget.com/proton-launches-its-own-version-of-google-docs-100044471.html</link>
            <guid>40864914</guid>
            <pubDate>Wed, 03 Jul 2024 11:25:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/proton-launches-its-own-version-of-google-docs-100044471.html">https://www.engadget.com/proton-launches-its-own-version-of-google-docs-100044471.html</a>, See on <a href="https://news.ycombinator.com/item?id=40864914">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a data-i13n="cpos:1;pos:1" href="https://www.engadget.com/protons-windows-and-macos-mail-app-is-out-of-beta-and-available-now-110010822.html" data-ylk="slk:Proton;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas">Proton</a> now has its own version of Google Docs in its Drive cloud storage service, and like the company's other products, it comes with end-to-end encryption. The company says its flavor of Docs "offers a unique solution in a market where most popular products neglect privacy" and recommends it for use in the healthcare, media, finance and legal industries. <a data-i13n="elm:affiliate_link;sellerN:;elmt:;cpos:2;pos:1" href="https://shopping.yahoo.com/rdlw?siteId=us-engadget&amp;pageId=1p-autolink&amp;featureId=text-link&amp;custData=eyJzb3VyY2VOYW1lIjoiV2ViLURlc2t0b3AtVmVyaXpvbiIsImxhbmRpbmdVcmwiOiJodHRwczovL3Byb3Rvbi5tZS9ibG9nL2RvY3MtcHJvdG9uLWRyaXZlIiwiY29udGVudFV1aWQiOiJjMmYyMGEyYS05ZGEzLTQyZmMtYjhmMC03MGI3ZDRlMGFhNmYifQ&amp;signature=AQAAAWZaQ6nJO7zP4rH3JS2Y44qB7q_HbbOpt8RRVvU6nYbn&amp;gcReferrer=https%3A%2F%2Fproton.me%2Fblog%2Fdocs-proton-drive" rel="nofollow noopener" target="_blank" data-ylk="slk:Proton Docs;elm:affiliate_link;sellerN:;elmt:;cpos:2;pos:1;itc:0;sec:content-canvas">Proton Docs</a> has advanced formatting and image embed options like Google Docs has and can create, open and edit documents in multiple formats, including Microsoft .docx.</p><p>It has collaboration tools similar to Google Docs', as well. Users can invite anyone to view and edit their documents, though those without a Proton account will be prompted to create one first. The free tier of Proton Drive includes essential document features so people don't have to pay for the service if they don't want to. Participants will be able to add comments to the document, reply to them and resolve them. And users will see other participants' presence and their cursor placements in real time, so that they know who's working on which part of the document and so that their edits don't clash.</p><p>Proton didn't say whether the launch of Docs means it's going to roll out analogues of Google's other Workspace apps in the future, but the company did <a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/proton-encrypted-email-vpn-calendar-rebrand-103024950.html" data-ylk="slk:expand its offerings;cpos:3;pos:1;elm:context_link;itc:0;sec:content-canvas">expand its offerings</a> with several different products over the last few years. In addition to Drive cloud storage — and, of course, its email service — the company has a VPN, an encrypted calendar and even a <a data-i13n="cpos:4;pos:1" href="https://www.engadget.com/proton-launches-its-own-password-manager-115039870.html" data-ylk="slk:password manager;cpos:4;pos:1;elm:context_link;itc:0;sec:content-canvas">password manager</a>. Docs will make its way to Proton users over the coming days.</p><p>This article contains affiliate links; if you click such a link and make a purchase, we may earn a commission.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Jb / json.bash – Command-line tool (and bash library) that creates JSON (104 pts)]]></title>
            <link>https://github.com/h4l/json.bash</link>
            <guid>40864541</guid>
            <pubDate>Wed, 03 Jul 2024 10:18:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/h4l/json.bash">https://github.com/h4l/json.bash</a>, See on <a href="https://news.ycombinator.com/item?id=40864541">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto"><code>$ jb name=json.bash creates=JSON</code></h2><a id="user-content--jb-namejsonbash-createsjson" aria-label="Permalink: $ jb name=json.bash creates=JSON" href="#-jb-namejsonbash-createsjson"></a></p>
<p dir="auto"><code>json.bash</code> is a command-line tool and bash library that creates JSON.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb name=json.bash creates=JSON dependencies:[,]=Bash,Grep
{&quot;name&quot;:&quot;json.bash&quot;,&quot;creates&quot;:&quot;JSON&quot;,&quot;dependencies&quot;:[&quot;Bash&quot;,&quot;Grep&quot;]}

$ # Values are strings unless explicitly typed
$ jb id=42 size:number=42 surname=null data:null
{&quot;id&quot;:&quot;42&quot;,&quot;size&quot;:42,&quot;surname&quot;:&quot;null&quot;,&quot;data&quot;:null}

$ # Reference variables with @name
$ id=42 date=2023-06-23 jb @id created@date modified@date
{&quot;id&quot;:&quot;42&quot;,&quot;created&quot;:&quot;2023-06-23&quot;,&quot;modified&quot;:&quot;2023-06-23&quot;}

$ # Pull data from files
$ printf hunter2 > /tmp/password; jb @/tmp/password
{&quot;password&quot;:&quot;hunter2&quot;}

$ # Pull data from shell pipelines
$ jb sizes:number[]@<(seq 1 4)
{&quot;sizes&quot;:[1,2,3,4]}

$ # Nest jb calls
$ jb type=club members:json[]@<(jb name=Bob; jb name=Alice)
{&quot;type&quot;:&quot;club&quot;,&quot;members&quot;:[{&quot;name&quot;:&quot;Bob&quot;},{&quot;name&quot;:&quot;Alice&quot;}]}

$ # The Bash API can reference arrays and create JSON efficiently — without forking
$ source json.bash
$ out=people json name=Bob; out=people json name=Alice; sizes=(42 91 2)
$ id=&quot;abc.123&quot; json @id @sizes:number[] @people:json[]
{&quot;id&quot;:&quot;abc.123&quot;,&quot;sizes&quot;:[42,91,2],&quot;people&quot;:[{&quot;name&quot;:&quot;Bob&quot;},{&quot;name&quot;:&quot;Alice&quot;}]}"><pre>$ <span>jb name=json.bash creates=JSON dependencies:[,]=Bash,Grep</span>
<span>{"name":"json.bash","creates":"JSON","dependencies":["Bash","Grep"]}</span>

$ <span><span><span>#</span> Values are strings unless explicitly typed</span></span>
$ <span>jb id=42 size:number=42 surname=null data:null</span>
<span>{"id":"42","size":42,"surname":"null","data":null}</span>

$ <span><span><span>#</span> Reference variables with @name</span></span>
$ <span>id=42 date=2023-06-23 jb @id created@date modified@date</span>
<span>{"id":"42","created":"2023-06-23","modified":"2023-06-23"}</span>

$ <span><span><span>#</span> Pull data from files</span></span>
$ <span><span>printf</span> hunter2 <span>&gt;</span> /tmp/password<span>;</span> jb @/tmp/password</span>
<span>{"password":"hunter2"}</span>

$ <span><span><span>#</span> Pull data from shell pipelines</span></span>
$ <span>jb sizes:number[]@<span><span>&lt;(</span>seq 1 4<span>)</span></span></span>
<span>{"sizes":[1,2,3,4]}</span>

$ <span><span><span>#</span> Nest jb calls</span></span>
$ <span>jb type=club members:json[]@<span><span>&lt;(</span>jb name=Bob<span>;</span> jb name=Alice<span>)</span></span></span>
<span>{"type":"club","members":[{"name":"Bob"},{"name":"Alice"}]}</span>

$ <span><span><span>#</span> The Bash API can reference arrays and create JSON efficiently — without forking</span></span>
$ <span><span>source</span> json.bash</span>
$ <span>out=people json name=Bob<span>;</span> out=people json name=Alice<span>;</span> sizes=(42 91 2)</span>
$ <span>id=<span><span>"</span>abc.123<span>"</span></span> json @id @sizes:number[] @people:json[]</span>
<span>{"id":"abc.123","sizes":[42,91,2],"people":[{"name":"Bob"},{"name":"Alice"}]}</span></pre></div>
<p dir="auto"><code>json.bash</code>'s <em><a href="https://en.wikipedia.org/wiki/Unix_philosophy" rel="nofollow">one thing</a></em> is to get shell-native data (environment variables,
files, program output) to somewhere else, using JSON encapsulate it robustly.</p>
<p dir="auto">Creating JSON from the command line or a shell script can be useful when:</p>
<ul dir="auto">
<li>You need some ad-hoc JSON to interact with a JSON-consuming application</li>
<li>You need to bundle up some data to share or move elsewhere. JSON can a good
alternative to base64-encoding or a file archive.</li>
</ul>
<p dir="auto">It does no transformation or filtering itself, instead it pulls data from things
you <strong>already know how to use</strong>, like files, command-line arguments, environment
variables, shell pipelines and shell scripts. It glues together data from these
sources, giving it enough structure to make the data easy to consume reliably in
downstream programs.</p>
<p dir="auto">It's something like a reverse <code>tee</code> — it pulls together data sources, using JSON
to represent the aggregation. It's not an alternative to to data-processing
tools like <code>jq</code>, rather it helps assemble JSON to send into JSON-consuming tools
like <code>jq</code>.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contents</h2><a id="user-content-contents" aria-label="Permalink: Contents" href="#contents"></a></p>
<ol dir="auto">
<li><a href="#install">Install</a></li>
<li><a href="#how-to-guides">How-to guides</a></li>
<li><a href="#background--performance-notes">Background &amp; performance notes</a></li>
<li><a href="#credits">Credits</a></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Container image</h3><a id="user-content-container-image" aria-label="Permalink: Container image" href="#container-image"></a></p>
<p dir="auto">We publish the container image
<a href="https://github.com/h4l/json.bash/pkgs/container/json.bash%2Fjb"><code>ghcr.io/h4l/json.bash/jb</code></a>
with <code>jb-*</code> and <code>json.bash</code>, perhaps useful to try without installing.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ docker container run --rm ghcr.io/h4l/json.bash/jb msg=Hi
{&quot;msg&quot;:&quot;Hi&quot;}

$ # Get a bash shell to try things interactively
$ docker container run --rm -it ghcr.io/h4l/json.bash/jb
bash-5.2# jb os-release:{}@<(xargs < /etc/os-release env -i)
{&quot;os-release&quot;:{&quot;NAME&quot;:&quot;Alpine Linux&quot;,&quot;ID&quot;:&quot;alpine&quot;,&quot;VERSION_ID&quot;:&quot;3.18.2&quot;,&quot;PRETTY_NAME&quot;:&quot;Alpine Linux v3.18&quot;,&quot;HOME_URL&quot;:&quot;https://alpinelinux.org/&quot;,&quot;BUG_REPORT_URL&quot;:&quot;https://gitlab.alpinelinux.org/alpine/aports/-/issues&quot;}}"><pre>$ <span>docker container run --rm ghcr.io/h4l/json.bash/jb msg=Hi</span>
<span>{"msg":"Hi"}</span>

$ <span><span><span>#</span> Get a bash shell to try things interactively</span></span>
$ <span>docker container run --rm -it ghcr.io/h4l/json.bash/jb</span>
<span>bash-5.2# jb os-release:{}@&lt;(xargs &lt; /etc/os-release env -i)</span>
<span>{"os-release":{"NAME":"Alpine Linux","ID":"alpine","VERSION_ID":"3.18.2","PRETTY_NAME":"Alpine Linux v3.18","HOME_URL":"https://alpinelinux.org/","BUG_REPORT_URL":"https://gitlab.alpinelinux.org/alpine/aports/-/issues"}}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">OS Packages</h3><a id="user-content-os-packages" aria-label="Permalink: OS Packages" href="#os-packages"></a></p>
<p dir="auto">Package-manager files are available for any package manager supported by
<a href="https://fpm.readthedocs.io/" rel="nofollow"><code>fpm</code></a> (at least apk, deb, freebsd, rpm, sh (self extracting), tar,
possibly more).</p>
<p dir="auto">We publish the container image
<a href="https://github.com/h4l/json.bash/pkgs/container/json.bash%2Fpkg"><code>ghcr.io/h4l/json.bash/pkg</code></a>
that can generate a package file in whichever format you like:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ docker container run --rm -v &quot;$(pwd):/pkg&quot; ghcr.io/h4l/json.bash/pkg deb
Generating: /pkg/json.bash_0.2.2-dev.deb

$ ls
json.bash_0.2.2-dev.deb
$ dpkg -i /pkg/json.bash_0.2.2-dev.deb"><pre>$ <span>docker container run --rm -v <span><span>"</span><span><span>$(</span>pwd<span>)</span></span>:/pkg<span>"</span></span> ghcr.io/h4l/json.bash/pkg deb</span>
<span>Generating: /pkg/json.bash_0.2.2-dev.deb</span>

$ <span>ls</span>
<span>json.bash_0.2.2-dev.deb</span>
$ <span>dpkg -i /pkg/json.bash_0.2.2-dev.deb</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Manual install</h3><a id="user-content-manual-install" aria-label="Permalink: Manual install" href="#manual-install"></a></p>
<p dir="auto">Installing manually is quite straightforward.</p>
<details>
  <summary>Expand this for instructions</summary>
<div dir="auto" data-snippet-clipboard-copy-content="# Alternatively, use /usr/local/bin to install system-wide
cd ~/.local/bin
curl -fsSL -O &quot;https://raw.githubusercontent.com/h4l/json.bash/HEAD/json.bash&quot;
chmod +x json.bash
ln -s json.bash jb
ln -s json.bash jb-array

# If your shell is bash, you can alias jb and jb-array to the bash functions for
# better performance. You should add this line to your ~/.bashrc
source json.bash; alias jb=json jb-array=json.array

# Optional: if you'd also like jb-echo, jb-cat, jb-stream
for name in jb-echo jb-cat jb-stream; do
  curl -fsSL -O &quot;https://raw.githubusercontent.com/h4l/json.bash/HEAD/bin/${name:?}&quot;
  chmod +x &quot;${name:?}&quot;
done"><pre><span><span>#</span> Alternatively, use /usr/local/bin to install system-wide</span>
<span>cd</span> <span>~</span>/.local/bin
curl -fsSL -O <span><span>"</span>https://raw.githubusercontent.com/h4l/json.bash/HEAD/json.bash<span>"</span></span>
chmod +x json.bash
ln -s json.bash jb
ln -s json.bash jb-array

<span><span>#</span> If your shell is bash, you can alias jb and jb-array to the bash functions for</span>
<span><span>#</span> better performance. You should add this line to your ~/.bashrc</span>
<span>source</span> json.bash<span>;</span> <span>alias</span> jb=json jb-array=json.array

<span><span>#</span> Optional: if you'd also like jb-echo, jb-cat, jb-stream</span>
<span>for</span> <span>name</span> <span>in</span> jb-echo jb-cat jb-stream<span>;</span> <span>do</span>
  curl -fsSL -O <span><span>"</span>https://raw.githubusercontent.com/h4l/json.bash/HEAD/bin/<span>${name<span>:?</span>}</span><span>"</span></span>
  chmod +x <span><span>"</span><span>${name<span>:?</span>}</span><span>"</span></span>
<span>done</span></pre></div>
<p dir="auto">To uninstall, remove <code>json.bash</code>, <code>jb</code>, <code>jb-array</code>, <code>jb-echo</code>, <code>jb-cat</code> and
<code>jb-stream</code> from the directory you installed them to (run <code>which -a json.bash</code>
to find where it is).</p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">How-to guides</h2><a id="user-content-how-to-guides" aria-label="Permalink: How-to guides" href="#how-to-guides"></a></p>
<ol dir="auto">
<li><a href="#the-jsonbash-commands">The <code>json.bash</code> commands</a></li>
<li><a href="#object-keys">Object keys</a></li>
<li><a href="#object-values">Object values</a></li>
<li><a href="#arrays-mixed-types-fixed-length">Arrays (mixed types, fixed length)</a></li>
<li><a href="#argument-types">Argument types</a></li>
<li><a href="#array-values-uniform-types-variable-length">Array values (uniform types, variable length)</a></li>
<li><a href="#object-values-uniform-types-variable-length">Object values (uniform types, variable length)</a></li>
<li><a href="#-arguments-merge-entries-into-the-host-objectarray"><code>...</code> arguments (merge entries into the host object/array)</a></li>
<li><a href="#missing--empty-values">Missing / empty values</a></li>
<li><a href="#nested-json-with-json-and-raw-types">Nested JSON with <code>:json</code> and <code>:raw</code> types</a></li>
<li><a href="#file-references">File references</a></li>
<li><a href="#argument-structure">Argument structure</a></li>
<li><a href="#error-handling">Error handling</a></li>
<li><a href="#security-and-correctness">Security and correctness</a></li>
<li><a href="#jb-cat-jb-echo-jb-stream-utility-programs"><code>jb-cat</code>, <code>jb-echo</code>, <code>jb-stream</code> utility programs</a></li>
<li><a href="#streaming-output">Streaming output</a></li>
</ol>
<p dir="auto">These examples mostly use <code>jb</code>, which is the <code>json.bash</code> library run as a
stand-alone program. From within a bash script you get better performance by
running <code>source json.bash</code> and using the <code>json</code> bash function, which is a
superset of stand-alone <code>jb</code> and much faster because it doesn't execute new
child processes when called. See the
<a href="#background--performance-notes">Background &amp; performance notes</a> section for
more.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The <code>json.bash</code> commands</h3><a id="user-content-the-jsonbash-commands" aria-label="Permalink: The json.bash commands" href="#the-jsonbash-commands"></a></p>
<p dir="auto"><code>jb</code> / <code>jb-array</code> / <code>json</code> / <code>json.array</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # The jb program creates JSON objects
$ jb
{}

$ # The jb-array creates arrays, but otherwise works like jb.
$ jb-array :number=4
[4]

$ # From a bash shell or bash script, use the json and json.array functions
$ source json.bash  # no path is needed if json.bash is on $PATH
$ json
{}

$ # json.array creates arrays, but otherwise works like json
$ json.array
[]"><pre>$ <span><span><span>#</span> The jb program creates JSON objects</span></span>
$ <span>jb</span>
<span>{}</span>

$ <span><span><span>#</span> The jb-array creates arrays, but otherwise works like jb.</span></span>
$ <span>jb-array :number=4</span>
<span>[4]</span>

$ <span><span><span>#</span> From a bash shell or bash script, use the json and json.array functions</span></span>
$ <span><span>source</span> json.bash  <span><span>#</span> no path is needed if json.bash is on $PATH</span></span>
$ <span>json</span>
<span>{}</span>

$ <span><span><span>#</span> json.array creates arrays, but otherwise works like json</span></span>
$ <span>json.array</span>
<span>[]</span></pre></div>
<p dir="auto">Each argument defines an entry in the object or array. Arguments can contain a
key, type and value in this structure:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/h4l/json.bash/blob/main/docs/syntax-diagrams/minimal-argument.svg"><img width="100%" src="https://github.com/h4l/json.bash/raw/main/docs/syntax-diagrams/minimal-argument.svg" alt="A railroad syntax diagram showing a high-level summary of the key, type and value structure of an argument." title="Minimal Argument Structure Diagram"></a></p>
<p dir="auto">The <a href="#argument-structure">Argument structure</a> section has more details.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Object keys</h3><a id="user-content-object-keys" aria-label="Permalink: Object keys" href="#object-keys"></a></p>
<p dir="auto">Each argument creates an entry in the JSON object. The first part of each
argument defines the key.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb msg=hi
{&quot;msg&quot;:&quot;hi&quot;}

$ # Keys can contain most characters (except @:=, unless escaped)
$ jb &quot;🐚&quot;=JSON
{&quot;🐚&quot;:&quot;JSON&quot;}

$ # Key values can come from variables
$ key=&quot;The Message&quot; jb @key=hi
{&quot;The Message&quot;:&quot;hi&quot;}

$ # Key variables can contain any characters
$ key=&quot;@key:with=reserved-chars&quot; jb @key=hi
{&quot;@key:with=reserved-chars&quot;:&quot;hi&quot;}

$ # Each argument defines a key
$ var=c jb a=X b=Y @var=Z
{&quot;a&quot;:&quot;X&quot;,&quot;b&quot;:&quot;Y&quot;,&quot;c&quot;:&quot;Z&quot;}

$ # Keys may be reused, but should not be, because JSON parser behaviour for
$ # duplicate keys is undefined.
$ jb a=A a=B a=C
{&quot;a&quot;:&quot;A&quot;,&quot;a&quot;:&quot;B&quot;,&quot;a&quot;:&quot;C&quot;}

$ # The reserved characters can be escaped by doubling them
$ jb =@@handle=ok a::z=ok 1+1==2=ok
{&quot;@handle&quot;:&quot;ok&quot;,&quot;a:z&quot;:&quot;ok&quot;,&quot;1+1=2&quot;:&quot;ok&quot;}"><pre>$ <span>jb msg=hi</span>
<span>{"msg":"hi"}</span>

$ <span><span><span>#</span> Keys can contain most characters (except @:=, unless escaped)</span></span>
$ <span>jb <span><span>"</span>🐚<span>"</span></span>=JSON</span>
<span>{"🐚":"JSON"}</span>

$ <span><span><span>#</span> Key values can come from variables</span></span>
$ <span>key=<span><span>"</span>The Message<span>"</span></span> jb @key=hi</span>
<span>{"The Message":"hi"}</span>

$ <span><span><span>#</span> Key variables can contain any characters</span></span>
$ <span>key=<span><span>"</span>@key:with=reserved-chars<span>"</span></span> jb @key=hi</span>
<span>{"@key:with=reserved-chars":"hi"}</span>

$ <span><span><span>#</span> Each argument defines a key</span></span>
$ <span>var=c jb a=X b=Y @var=Z</span>
<span>{"a":"X","b":"Y","c":"Z"}</span>

$ <span><span><span>#</span> Keys may be reused, but should not be, because JSON parser behaviour for</span></span>
$ <span><span><span>#</span> duplicate keys is undefined.</span></span>
$ <span>jb a=A a=B a=C</span>
<span>{"a":"A","a":"B","a":"C"}</span>

$ <span><span><span>#</span> The reserved characters can be escaped by doubling them</span></span>
$ <span>jb =@@handle=ok a::z=ok 1+1==2=ok</span>
<span>{"@handle":"ok","a:z":"ok","1+1=2":"ok"}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Object values</h3><a id="user-content-object-values" aria-label="Permalink: Object values" href="#object-values"></a></p>
<p dir="auto">The last part of each argument after a <code>=</code> or <code>@</code> defines the value. Values can
contain their value directly, or reference a variable or file.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb message=&quot;Hello World&quot;
{&quot;message&quot;:&quot;Hello World&quot;}

$ greeting=&quot;Hi there&quot; jb message@greeting
{&quot;message&quot;:&quot;Hi there&quot;}

$ # Variable references without a value define the key and value in one go.
$ greeting=&quot;Hi&quot; name=Bob jb @greeting @name
{&quot;greeting&quot;:&quot;Hi&quot;,&quot;name&quot;:&quot;Bob&quot;}

$ # This also applies (less usefully) to inline entries.
$ jb message
{&quot;message&quot;:&quot;message&quot;}

$ # Inline values following a `=` have no content restrictions.
$ jb message=@value:with=reserved-chars
{&quot;message&quot;:&quot;@value:with=reserved-chars&quot;}

$ # @ values that begin with / or ./ are references to files
$ printf hunter2 > /tmp/password; jb secret@/tmp/password
{&quot;secret&quot;:&quot;hunter2&quot;}

$ # File references without a value define the key and value in one go.
$ jb @/tmp/password
{&quot;password&quot;:&quot;hunter2&quot;}"><pre>$ <span>jb message=<span><span>"</span>Hello World<span>"</span></span></span>
<span>{"message":"Hello World"}</span>

$ <span>greeting=<span><span>"</span>Hi there<span>"</span></span> jb message@greeting</span>
<span>{"message":"Hi there"}</span>

$ <span><span><span>#</span> Variable references without a value define the key and value in one go.</span></span>
$ <span>greeting=<span><span>"</span>Hi<span>"</span></span> name=Bob jb @greeting @name</span>
<span>{"greeting":"Hi","name":"Bob"}</span>

$ <span><span><span>#</span> This also applies (less usefully) to inline entries.</span></span>
$ <span>jb message</span>
<span>{"message":"message"}</span>

$ <span><span><span>#</span> Inline values following a `=` have no content restrictions.</span></span>
$ <span>jb message=@value:with=reserved-chars</span>
<span>{"message":"@value:with=reserved-chars"}</span>

$ <span><span><span>#</span> @ values that begin with / or ./ are references to files</span></span>
$ <span><span>printf</span> hunter2 <span>&gt;</span> /tmp/password<span>;</span> jb secret@/tmp/password</span>
<span>{"secret":"hunter2"}</span>

$ <span><span><span>#</span> File references without a value define the key and value in one go.</span></span>
$ <span>jb @/tmp/password</span>
<span>{"password":"hunter2"}</span></pre></div>
<p dir="auto">File references are more powerful than they might first appear, as they enable
all sorts of dynamic content to be pulled into JSON data, including nested <code>jb</code>
calls. See <a href="#file-references">File references</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Arrays (mixed types, fixed length)</h3><a id="user-content-arrays-mixed-types-fixed-length" aria-label="Permalink: Arrays (mixed types, fixed length)" href="#arrays-mixed-types-fixed-length"></a></p>
<p dir="auto">Creating arrays is much like creating objects — arguments hold values, either
directly, or referencing variables or files.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb-array Hi &quot;Bob Bobson&quot;
[&quot;Hi&quot;,&quot;Bob Bobson&quot;]

$ message=Hi name=&quot;Bob Bobson&quot; jb-array @message @name
[&quot;Hi&quot;,&quot;Bob Bobson&quot;]

$ printf 'Bob Bobson' > /tmp/name
$ jb-array Hi @/tmp/name
[&quot;Hi&quot;,&quot;Bob Bobson&quot;]

$ # Array values in arguments cannot contain @:= characters (unless escaped by
$ # doubling them), because they would clash with @variable and :type syntax.
$ # However, values following a = can contain anything, so long as they follow a
$ # key or type section.
$ jb-array :='@foo:bar=baz' :='{&quot;not&quot;:&quot;parsed&quot;}' =@@es::cap==ed
[&quot;@foo:bar=baz&quot;,&quot;{\&quot;not\&quot;:\&quot;parsed\&quot;}&quot;,&quot;@es:cap=ed&quot;]

$ # Values from variables have no restrictions. Arrays use the same argument
$ # syntax as objects, so values in the key or value position work the same.
$ s1='@foo:bar=baz' s2='{&quot;not&quot;:&quot;parsed&quot;}' jb-array @s1: :@s2
[&quot;@foo:bar=baz&quot;,&quot;{\&quot;not\&quot;:\&quot;parsed\&quot;}&quot;]

$ # It's possible to set a key as well as value for array entries, but the key
$ # is ignored.
$ a=A b=B jb-array @a@a @b=B c=C
[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;]"><pre>$ <span>jb-array Hi <span><span>"</span>Bob Bobson<span>"</span></span></span>
<span>["Hi","Bob Bobson"]</span>

$ <span>message=Hi name=<span><span>"</span>Bob Bobson<span>"</span></span> jb-array @message @name</span>
<span>["Hi","Bob Bobson"]</span>

$ <span><span>printf</span> <span><span>'</span>Bob Bobson<span>'</span></span> <span>&gt;</span> /tmp/name</span>
$ <span>jb-array Hi @/tmp/name</span>
<span>["Hi","Bob Bobson"]</span>

$ <span><span><span>#</span> Array values in arguments cannot contain @:= characters (unless escaped by</span></span>
$ <span><span><span>#</span> doubling them), because they would clash with @variable and :type syntax.</span></span>
$ <span><span><span>#</span> However, values following a = can contain anything, so long as they follow a</span></span>
$ <span><span><span>#</span> key or type section.</span></span>
$ <span>jb-array :=<span><span>'</span>@foo:bar=baz<span>'</span></span> :=<span><span>'</span>{"not":"parsed"}<span>'</span></span> =@@es::cap==ed</span>
<span>["@foo:bar=baz","{\"not\":\"parsed\"}","@es:cap=ed"]</span>

$ <span><span><span>#</span> Values from variables have no restrictions. Arrays use the same argument</span></span>
$ <span><span><span>#</span> syntax as objects, so values in the key or value position work the same.</span></span>
$ <span>s1=<span><span>'</span>@foo:bar=baz<span>'</span></span> s2=<span><span>'</span>{"not":"parsed"}<span>'</span></span> jb-array @s1: :@s2</span>
<span>["@foo:bar=baz","{\"not\":\"parsed\"}"]</span>

$ <span><span><span>#</span> It's possible to set a key as well as value for array entries, but the key</span></span>
$ <span><span><span>#</span> is ignored.</span></span>
$ <span>a=A b=B jb-array @a@a @b=B c=C</span>
<span>["A","B","C"]</span></pre></div>
<p dir="auto"><code>jb-array</code> is best for creating tuple-like arrays with a fixed number of entries
with a mix of types. Use
<a href="#value-arrays-uniform-types-variable-length">value arrays</a> to create
variable-length arrays containing the same type.</p>
<p dir="auto"><code>json.array</code> is the Bash API equivalent of <code>jb-array</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Argument types</h3><a id="user-content-argument-types" aria-label="Permalink: Argument types" href="#argument-types"></a></p>
<p dir="auto">Values are strings unless explicitly typed.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # These arguments are strings because they don't use a type
$ jb data=42 surname=null favourite_word=true
{&quot;data&quot;:&quot;42&quot;,&quot;surname&quot;:&quot;null&quot;,&quot;favourite_word&quot;:&quot;true&quot;}

$ # Non-string values need explicit types
$ jb size:number=42
{&quot;size&quot;:42}

$ # true/false/null have types which don't require redundant values
$ jb active:true enabled:false data:null
{&quot;active&quot;:true,&quot;enabled&quot;:false,&quot;data&quot;:null}

$ # Regardless, they can be given values if desired
$ jb active:true=true enabled:false=false data:null=null
{&quot;active&quot;:true,&quot;enabled&quot;:false,&quot;data&quot;:null}

$ # The bool type allows either true or false values.
$ active=true jb @active:bool enabled:bool=false
{&quot;active&quot;:true,&quot;enabled&quot;:false}

$ # The auto type outputs true/false/null and number values.
$ jb a:auto=42 b:auto=Hi c:auto=true d:auto=false e:auto=null f:auto=[] g:auto={}
{&quot;a&quot;:42,&quot;b&quot;:&quot;Hi&quot;,&quot;c&quot;:true,&quot;d&quot;:false,&quot;e&quot;:null,&quot;f&quot;:&quot;[]&quot;,&quot;g&quot;:&quot;{}&quot;}

$ # auto can be used selectively like other types
$ data=42 jb a=42 b:auto=42 c:auto@data
{&quot;a&quot;:&quot;42&quot;,&quot;b&quot;:42,&quot;c&quot;:42}

$ # In the Bash API (but not yet the jb CLI), the default type can be changed
$ # using the json_defaults option. First you create a named defaults set:
$ source json.bash
$ json.define_defaults num :number

$ # Then use the name with json_defaults when calling json to use your defaults
$ json_defaults=num json data=42
{&quot;data&quot;:42}

$ # In which case strings need to be explicitly typed
$ json_defaults=num json data=42 msg=Hi
json.encode_number(): not all inputs are numbers: 'Hi'
json(): Could not encode the value of argument 'msg=Hi' as a 'number' value. Read from inline value.
�␘

$ json_defaults=num json data=42 msg:string=Hi
{&quot;data&quot;:42,&quot;msg&quot;:&quot;Hi&quot;}"><pre>$ <span><span><span>#</span> These arguments are strings because they don't use a type</span></span>
$ <span>jb data=42 surname=null favourite_word=true</span>
<span>{"data":"42","surname":"null","favourite_word":"true"}</span>

$ <span><span><span>#</span> Non-string values need explicit types</span></span>
$ <span>jb size:number=42</span>
<span>{"size":42}</span>

$ <span><span><span>#</span> true/false/null have types which don't require redundant values</span></span>
$ <span>jb active:true enabled:false data:null</span>
<span>{"active":true,"enabled":false,"data":null}</span>

$ <span><span><span>#</span> Regardless, they can be given values if desired</span></span>
$ <span>jb active:true=true enabled:false=false data:null=null</span>
<span>{"active":true,"enabled":false,"data":null}</span>

$ <span><span><span>#</span> The bool type allows either true or false values.</span></span>
$ <span>active=true jb @active:bool enabled:bool=false</span>
<span>{"active":true,"enabled":false}</span>

$ <span><span><span>#</span> The auto type outputs true/false/null and number values.</span></span>
$ <span>jb a:auto=42 b:auto=Hi c:auto=true d:auto=false e:auto=null f:auto=[] g:auto={}</span>
<span>{"a":42,"b":"Hi","c":true,"d":false,"e":null,"f":"[]","g":"{}"}</span>

$ <span><span><span>#</span> auto can be used selectively like other types</span></span>
$ <span>data=42 jb a=42 b:auto=42 c:auto@data</span>
<span>{"a":"42","b":42,"c":42}</span>

$ <span><span><span>#</span> In the Bash API (but not yet the jb CLI), the default type can be changed</span></span>
$ <span><span><span>#</span> using the json_defaults option. First you create a named defaults set:</span></span>
$ <span><span>source</span> json.bash</span>
$ <span>json.define_defaults num :number</span>

$ <span><span><span>#</span> Then use the name with json_defaults when calling json to use your defaults</span></span>
$ <span>json_defaults=num json data=42</span>
<span>{"data":42}</span>

$ <span><span><span>#</span> In which case strings need to be explicitly typed</span></span>
$ <span>json_defaults=num json data=42 msg=Hi</span>
<span>json.encode_number(): not all inputs are numbers: 'Hi'</span>
<span>json(): Could not encode the value of argument 'msg=Hi' as a 'number' value. Read from inline value.</span>
<span>�␘</span>

$ <span>json_defaults=num json data=42 msg:string=Hi</span>
<span>{"data":42,"msg":"Hi"}</span></pre></div>
<details>
  <summary>Why does <code>json.bash</code> require explicit types?</summary>
  <p dir="auto"><h4 tabindex="-1" dir="auto">Why does <code>json.bash</code> require explicit types?</h4><a id="user-content-why-does-jsonbash-require-explicit-types" aria-label="Permalink: Why does json.bash require explicit types?" href="#why-does-jsonbash-require-explicit-types"></a></p>
  <p dir="auto">Type coercion can look good in demos, but my opinion is that in practice,
  fields are more commonly of a specific type than a union of several options,
  so coercing types by default makes it harder to achieve correct behaviour in
  the common case. The <a href="https://hitchdev.com/strictyaml/why/implicit-typing-removed/" rel="nofollow">Norway Problem</a>
  is worth reading about if you're not familiar with it.</p>
  <p dir="auto">Regardless, you can make the <code>:auto</code> type the default by using <code>json_defaults</code> when calling <code>json</code> from the
  Bash API (as demonstrated above). (This isn't yet exposed through the
  <code>jb</code> CLI.)</p>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Array values (uniform types, variable length)</h3><a id="user-content-array-values-uniform-types-variable-length" aria-label="Permalink: Array values (uniform types, variable length)" href="#array-values-uniform-types-variable-length"></a></p>
<p dir="auto">Arrays of values can be created using <code>[]</code> after the <code>:</code> type marker.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb sizes:number[]=42
{&quot;sizes&quot;:[42]}

$ # The value is split on the character inside the []
$ jb names:[,]=&quot;Alice,Bob,Dr Chris&quot;
{&quot;names&quot;:[&quot;Alice&quot;,&quot;Bob&quot;,&quot;Dr Chris&quot;]}

$ # Using a newline \n as the split character makes each line an array
$ # element. This integrates with line-oriented command-line tools:
$ jb sizes:number[$'\n']=&quot;$(seq 3)&quot;
{&quot;sizes&quot;:[1,2,3]}"><pre>$ <span>jb sizes:number[]=42</span>
<span>{"sizes":[42]}</span>

$ <span><span><span>#</span> The value is split on the character inside the []</span></span>
$ <span>jb names:[,]=<span><span>"</span>Alice,Bob,Dr Chris<span>"</span></span></span>
<span>{"names":["Alice","Bob","Dr Chris"]}</span>

$ <span><span><span>#</span> Using a newline \n as the split character makes each line an array</span></span>
$ <span><span><span>#</span> element. This integrates with line-oriented command-line tools:</span></span>
$ <span>jb sizes:number[<span><span>$'</span><span>\n</span><span>'</span></span>]=<span><span>"</span><span><span>$(</span>seq 3<span>)</span></span><span>"</span></span></span>
<span>{"sizes":[1,2,3]}</span></pre></div>
<p dir="auto"><a href="#file-references">File references</a> with process substitution are the a better
way to get the output of other programs into JSON though.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # The default type is used if the type name is left out
$ jb sizes:[,]=&quot;1,2,3&quot;
{&quot;sizes&quot;:[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;]}

$ # [:] is shorthand for /collection=array,split=:/
$ jb names:/collection=array,split=:/=&quot;Alice:Bob:Dr Chris&quot;
{&quot;names&quot;:[&quot;Alice&quot;,&quot;Bob&quot;,&quot;Dr Chris&quot;]}

$ # To split on null bytes, use split= (empty string). When used with inline and
$ # bash values this effectively inhibits splitting, because bash variables
$ # can't contain null bytes.
$ printf 'AB\nCD\x00EF\nGH\n\x00' | jb nullterm:[]/split=/@/dev/stdin
{&quot;nullterm&quot;:[&quot;AB\nCD&quot;,&quot;EF\nGH\n&quot;]}

$ # When using the Bash API, @var references can be bash arrays
$ source json.bash
$ names=(&quot;Bob Bobson&quot; &quot;Alice Alison&quot;) sizes=(42 55)
$ json @names:string[] @sizes:number[]
{&quot;names&quot;:[&quot;Bob Bobson&quot;,&quot;Alice Alison&quot;],&quot;sizes&quot;:[42,55]}

$ # json.array values can be arrays too
$ json.array @names:string[] @sizes:number[] :null[] :bool[]=true
[[&quot;Bob Bobson&quot;,&quot;Alice Alison&quot;],[42,55],[null],[true]]

$ # And jb-array values can be arrays as well
$ jb-array :[,]=&quot;Bob Bobson,Alice Alison&quot; :number[,]=42,55 :null[] :bool[]=true
[[&quot;Bob Bobson&quot;,&quot;Alice Alison&quot;],[42,55],[null],[true]]"><pre>$ <span><span><span>#</span> The default type is used if the type name is left out</span></span>
$ <span>jb sizes:[,]=<span><span>"</span>1,2,3<span>"</span></span></span>
<span>{"sizes":["1","2","3"]}</span>

$ <span><span><span>#</span> [:] is shorthand for /collection=array,split=:/</span></span>
$ <span>jb names:/collection=array,split=:/=<span><span>"</span>Alice:Bob:Dr Chris<span>"</span></span></span>
<span>{"names":["Alice","Bob","Dr Chris"]}</span>

$ <span><span><span>#</span> To split on null bytes, use split= (empty string). When used with inline and</span></span>
$ <span><span><span>#</span> bash values this effectively inhibits splitting, because bash variables</span></span>
$ <span><span><span>#</span> can't contain null bytes.</span></span>
$ <span><span>printf</span> <span><span>'</span>AB\nCD\x00EF\nGH\n\x00<span>'</span></span> <span>|</span> jb nullterm:[]/split=/@/dev/stdin</span>
<span>{"nullterm":["AB\nCD","EF\nGH\n"]}</span>

$ <span><span><span>#</span> When using the Bash API, @var references can be bash arrays</span></span>
$ <span><span>source</span> json.bash</span>
$ <span>names=(<span><span>"</span>Bob Bobson<span>"</span></span> <span><span>"</span>Alice Alison<span>"</span></span>) sizes=(42 55)</span>
$ <span>json @names:string[] @sizes:number[]</span>
<span>{"names":["Bob Bobson","Alice Alison"],"sizes":[42,55]}</span>

$ <span><span><span>#</span> json.array values can be arrays too</span></span>
$ <span>json.array @names:string[] @sizes:number[] :null[] :bool[]=true</span>
<span>[["Bob Bobson","Alice Alison"],[42,55],[null],[true]]</span>

$ <span><span><span>#</span> And jb-array values can be arrays as well</span></span>
$ <span>jb-array :[,]=<span><span>"</span>Bob Bobson,Alice Alison<span>"</span></span> :number[,]=42,55 :null[] :bool[]=true</span>
<span>[["Bob Bobson","Alice Alison"],[42,55],[null],[true]]</span></pre></div>
<p dir="auto">Arrays can be created from existing JSON arrays using the <code>[:json]</code> array
format:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb tags:[:json]=&quot;$(jb-array foo bar baz)&quot;
{&quot;tags&quot;:[&quot;foo&quot;,&quot;bar&quot;,&quot;baz&quot;]}

$ # The type of values in the argument's array must match the argument type
$ jb measures:number[:json]='[1,2,3,4]'
{&quot;measures&quot;:[1,2,3,4]}

$ # Otherwise an error occurs
$ jb measures:number[:json]='[1,2,&quot;oops&quot;]'
json.encode_array_entries_from_json(): provided entries are not all valid JSON arrays with 'number' values — '[1,2,&quot;oops&quot;]'
json(): Could not encode the value of argument 'measures:number[:json]=[1,2,&quot;oops&quot;]' as an array with 'number' values. Read from inline value, without splitting (one chunk), interpreted chunks with 'json' format.
�␘"><pre>$ <span>jb tags:[:json]=<span><span>"</span><span><span>$(</span>jb-array foo bar baz<span>)</span></span><span>"</span></span></span>
<span>{"tags":["foo","bar","baz"]}</span>

$ <span><span><span>#</span> The type of values in the argument's array must match the argument type</span></span>
$ <span>jb measures:number[:json]=<span><span>'</span>[1,2,3,4]<span>'</span></span></span>
<span>{"measures":[1,2,3,4]}</span>

$ <span><span><span>#</span> Otherwise an error occurs</span></span>
$ <span>jb measures:number[:json]=<span><span>'</span>[1,2,"oops"]<span>'</span></span></span>
<span>json.encode_array_entries_from_json(): provided entries are not all valid JSON arrays with 'number' values — '[1,2,"oops"]'</span>
<span>json(): Could not encode the value of argument 'measures:number[:json]=[1,2,"oops"]' as an array with 'number' values. Read from inline value, without splitting (one chunk), interpreted chunks with 'json' format.</span>
<span>�␘</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Object values (uniform types, variable length)</h3><a id="user-content-object-values-uniform-types-variable-length" aria-label="Permalink: Object values (uniform types, variable length)" href="#object-values-uniform-types-variable-length"></a></p>
<p dir="auto">Variable-length JSON objects can be created using <code>{}</code> after the <code>:</code> type
marker. Object values use the same <code>key=value</code> syntax used in arguments'
attributes section (<code>:/a=b,c=d/</code>).</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # The default type is used if the type name is left out
$ jb sizes:{}=small=s,medium=m,large=l
{&quot;sizes&quot;:{&quot;small&quot;:&quot;s&quot;,&quot;medium&quot;:&quot;m&quot;,&quot;large&quot;:&quot;l&quot;}}

$ jb measurements:number{}=small=5,medium=10,large=15
{&quot;measurements&quot;:{&quot;small&quot;:5,&quot;medium&quot;:10,&quot;large&quot;:15}}"><pre>$ <span><span><span>#</span> The default type is used if the type name is left out</span></span>
$ <span>jb sizes:{}=small=s,medium=m,large=l</span>
<span>{"sizes":{"small":"s","medium":"m","large":"l"}}</span>

$ <span>jb measurements:number{}=small=5,medium=10,large=15</span>
<span>{"measurements":{"small":5,"medium":10,"large":15}}</span></pre></div>
<p dir="auto">Like array values (<code>[]</code>), object values consume multiple lines of input when
reading files</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # env is a command-line tool that prints environment variables
$ env -i small=s medium=m large=l
small=s
medium=m
large=l

$ # We can encode variables from env as a JSON object
$ env -i small=s medium=m large=l | jb sizes:{}@/dev/stdin
{&quot;sizes&quot;:{&quot;small&quot;:&quot;s&quot;,&quot;medium&quot;:&quot;m&quot;,&quot;large&quot;:&quot;l&quot;}}"><pre>$ <span><span><span>#</span> env is a command-line tool that prints environment variables</span></span>
$ <span>env -i small=s medium=m large=l</span>
<span>small=s</span>
<span>medium=m</span>
<span>large=l</span>

$ <span><span><span>#</span> We can encode variables from env as a JSON object</span></span>
$ <span>env -i small=s medium=m large=l <span>|</span> jb sizes:{}@/dev/stdin</span>
<span>{"sizes":{"small":"s","medium":"m","large":"l"}}</span></pre></div>
<p dir="auto">As with array values, JSON data can be used as values:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb user=h4l repo=json.bash >> info
$ jb @./info:{:json}
{&quot;info&quot;:{&quot;user&quot;:&quot;h4l&quot;,&quot;repo&quot;:&quot;json.bash&quot;}}

$ jb file_types:string[,]=bash,md,hcl year_created:number=2023 >> info
$ # The values of the JSON objects are validated to match the argument's type,
$ # so the :json type must be used to consume arbitrary JSON
$ jb @./info:json{:json}
{&quot;info&quot;:{&quot;user&quot;:&quot;h4l&quot;,&quot;repo&quot;:&quot;json.bash&quot;,&quot;file_types&quot;:[&quot;bash&quot;,&quot;md&quot;,&quot;hcl&quot;],&quot;year_created&quot;:2023}}"><pre>$ <span>jb user=h4l repo=json.bash <span>&gt;&gt;</span> info</span>
$ <span>jb @./info:{:json}</span>
<span>{"info":{"user":"h4l","repo":"json.bash"}}</span>

$ <span>jb file_types:string[,]=bash,md,hcl year_created:number=2023 <span>&gt;&gt;</span> info</span>
$ <span><span><span>#</span> The values of the JSON objects are validated to match the argument's type,</span></span>
$ <span><span><span>#</span> so the :json type must be used to consume arbitrary JSON</span></span>
$ <span>jb @./info:json{:json}</span>
<span>{"info":{"user":"h4l","repo":"json.bash","file_types":["bash","md","hcl"],"year_created":2023}}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>...</code> arguments (merge entries into the host object/array)</h3><a id="user-content--arguments-merge-entries-into-the-host-objectarray" aria-label="Permalink: ... arguments (merge entries into the host object/array)" href="#-arguments-merge-entries-into-the-host-objectarray"></a></p>
<p dir="auto">An argument prefixed with <code>...</code> (commonly called splat, spread or unpacking in
programming languages) results in the argument's entries being merged directly
into the object or array being created.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ jb id=ab12 ...:=user=h4l,repo=json.bash ...:number=year=2023,min_radish_count=3
{&quot;id&quot;:&quot;ab12&quot;,&quot;user&quot;:&quot;h4l&quot;,&quot;repo&quot;:&quot;json.bash&quot;,&quot;year&quot;:2023,&quot;min_radish_count&quot;:3}

$ seq 5 8 | jb-array :number=0 ...:number[,]=1,2,3,4 ...:number@/dev/stdin
[0,1,2,3,4,5,6,7,8]"><pre>$ <span>jb id=ab12 ...:=user=h4l,repo=json.bash ...:number=year=2023,min_radish_count=3</span>
<span>{"id":"ab12","user":"h4l","repo":"json.bash","year":2023,"min_radish_count":3}</span>

$ <span>seq 5 8 <span>|</span> jb-array :number=0 ...:number[,]=1,2,3,4 ...:number@/dev/stdin</span>
<span>[0,1,2,3,4,5,6,7,8]</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Missing / empty values</h3><a id="user-content-missing--empty-values" aria-label="Permalink: Missing / empty values" href="#missing--empty-values"></a></p>
<p dir="auto">References to undefined variables, missing files or unreadable files are missing
values. Empty array variables, empty string variables, empty files and empty
argument values are empty values.</p>
<p dir="auto">Missing or empty keys or values are errors by default, apart from empty argument
values, like <code>foo=</code>.</p>
<p dir="auto">The flags <code>+</code> <code>~</code> <code>?</code> and <code>??</code> alter how missing/empty values behave.</p>
<table>
<thead>
<tr>
<th>Flag</th>
<th>Name</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>+</code></td>
<td>strict</td>
<td>All missing/empty values are errors.</td>
</tr>
<tr>
<td><code>~</code></td>
<td>optional</td>
<td>Missing files/variables are treated as empty.</td>
</tr>
<tr>
<td><code>?</code></td>
<td>substitute empty</td>
<td>Empty values are substituted with a default.</td>
</tr>
<tr>
<td><code>??</code></td>
<td>omit empty</td>
<td>Entries with an empty key or value are omitted.</td>
</tr>
</tbody>
</table>
<div dir="auto" data-snippet-clipboard-copy-content="$ # empty argument values are substituted by default
$ jb str= num:number= bool:bool= arr:[]= obj:{}=
{&quot;str&quot;:&quot;&quot;,&quot;num&quot;:0,&quot;bool&quot;:false,&quot;arr&quot;:[],&quot;obj&quot;:{}}

$ # Using ? substitutes the empty var for the default string, which is &quot;&quot;
$ empty= jb @empty?
{&quot;empty&quot;:&quot;&quot;}

$ # The empty attribute controls the default value. It's interpreted as JSON.
$ CI=true jb ci:bool/empty=false/?@CI
{&quot;ci&quot;:true}

$ CI= jb ci:true/empty=false/?@CI
{&quot;ci&quot;:false}

$ # empty_key controls the default value for empty keys
$ PROP= jb ?@PROP:true/empty_key='&quot;🤷&quot;'/
{&quot;🤷&quot;:true}

$ # The type= can be used to encode a raw value as JSON for empty attributes
$ PROP=👌 jb ?@PROP:true/empty_key=string=🤷/
{&quot;👌&quot;:true}

$ # ?? causes an empty value to be omitted entirely
$ CI= jb ci:bool??@CI
{}

$ # ~ causes a missing value to be empty. A ? is needed to prevent the empty
$ # value being an error.
$ jb github_actions:bool~?@GITHUB_ACTION
{&quot;github_actions&quot;:false}

$ # Empty variables are errors if ? isn't used.
$ empty= jb @empty
json.apply_empty_action(): The value of argument '@empty' must be non-empty but is empty.
json(): Could not encode the value of argument '@empty' as a 'string' value. Read from variable $empty. (Use the '?' flag after the :type to substitute the entry's empty value with a default, or the '??' flag to omit the entry when it has an empty value.)
�␘

$ # (Only the json Bash function, not the jb executable can access bash array variables.)
$ . json.bash
$ empty_array=()

$ # Using ? substitutes the empty array for the default, which is []
$ json @empty_array:[]?
{&quot;empty_array&quot;:[]}

$ # Empty arrays are errors without ?.
$ json @empty_array:[]
json.apply_empty_action(): The value of argument '@empty_array:[]' must be non-empty but is empty.
json(): Could not encode the value of argument '@empty_array:[]' as an array with 'string' values. Read from array-variable $empty_array. (Use the '?' flag after the :type to substitute the entry's empty value with a default, or the '??' flag to omit the entry when it has an empty value.)
�␘

$ # Missing / empty files work like variables
$ jb @./config:/empty=null/~?
{&quot;config&quot;:null}"><pre>$ <span><span><span>#</span> empty argument values are substituted by default</span></span>
$ <span>jb str= num:number= bool:bool= arr:[]= obj:{}=</span>
<span>{"str":"","num":0,"bool":false,"arr":[],"obj":{}}</span>

$ <span><span><span>#</span> Using ? substitutes the empty var for the default string, which is ""</span></span>
$ <span>empty= jb @empty<span>?</span></span>
<span>{"empty":""}</span>

$ <span><span><span>#</span> The empty attribute controls the default value. It's interpreted as JSON.</span></span>
$ <span>CI=true jb ci:bool/empty=false/<span>?</span>@CI</span>
<span>{"ci":true}</span>

$ <span>CI= jb ci:true/empty=false/<span>?</span>@CI</span>
<span>{"ci":false}</span>

$ <span><span><span>#</span> empty_key controls the default value for empty keys</span></span>
$ <span>PROP= jb <span>?</span>@PROP:true/empty_key=<span><span>'</span>"🤷"<span>'</span></span>/</span>
<span>{"🤷":true}</span>

$ <span><span><span>#</span> The type= can be used to encode a raw value as JSON for empty attributes</span></span>
$ <span>PROP=👌 jb <span>?</span>@PROP:true/empty_key=string=🤷/</span>
<span>{"👌":true}</span>

$ <span><span><span>#</span> ?? causes an empty value to be omitted entirely</span></span>
$ <span>CI= jb ci:bool<span>??</span>@CI</span>
<span>{}</span>

$ <span><span><span>#</span> ~ causes a missing value to be empty. A ? is needed to prevent the empty</span></span>
$ <span><span><span>#</span> value being an error.</span></span>
$ <span>jb github_actions:bool~<span>?</span>@GITHUB_ACTION</span>
<span>{"github_actions":false}</span>

$ <span><span><span>#</span> Empty variables are errors if ? isn't used.</span></span>
$ <span>empty= jb @empty</span>
<span>json.apply_empty_action(): The value of argument '@empty' must be non-empty but is empty.</span>
<span>json(): Could not encode the value of argument '@empty' as a 'string' value. Read from variable $empty. (Use the '?' flag after the :type to substitute the entry's empty value with a default, or the '??' flag to omit the entry when it has an empty value.)</span>
<span>�␘</span>

$ <span><span><span>#</span> (Only the json Bash function, not the jb executable can access bash array variables.)</span></span>
$ <span><span>.</span> json.bash</span>
$ <span>empty_array=()</span>

$ <span><span><span>#</span> Using ? substitutes the empty array for the default, which is []</span></span>
$ <span>json @empty_array:[]<span>?</span></span>
<span>{"empty_array":[]}</span>

$ <span><span><span>#</span> Empty arrays are errors without ?.</span></span>
$ <span>json @empty_array:[]</span>
<span>json.apply_empty_action(): The value of argument '@empty_array:[]' must be non-empty but is empty.</span>
<span>json(): Could not encode the value of argument '@empty_array:[]' as an array with 'string' values. Read from array-variable $empty_array. (Use the '?' flag after the :type to substitute the entry's empty value with a default, or the '??' flag to omit the entry when it has an empty value.)</span>
<span>�␘</span>

$ <span><span><span>#</span> Missing / empty files work like variables</span></span>
$ <span>jb @./config:/empty=null/~<span>?</span></span>
<span>{"config":null}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Nested JSON with <code>:json</code> and <code>:raw</code> types</h3><a id="user-content-nested-json-with-json-and-raw-types" aria-label="Permalink: Nested JSON with :json and :raw types" href="#nested-json-with-json-and-raw-types"></a></p>
<p dir="auto">Nested objects and arrays are created using the <code>:json</code> or <code>:raw</code> types. The
<code>:json</code> type validates the provided value(s) and fails if they're not actually
JSON, whereas the <code>:raw</code> type allow <em>any</em> value to be inserted (even invalid
JSON).</p>
<p dir="auto">The reason for both is that <code>:json</code> depends on grep (with PCRE) being present,
so <code>:raw</code> can be used in situations where only bash is available, and validation
isn't necessary (e.g. when passing the output of one <code>json.bash</code> call into
another). <code>:raw</code> also supports <a href="#streaming-output">streaming output</a>, which
<code>:json</code> does not.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # Like other types, :json and :raw values can be directly embedded in arguments
$ jb user:json='{&quot;name&quot;:&quot;Bob Bobson&quot;}'
{&quot;user&quot;:{&quot;name&quot;:&quot;Bob Bobson&quot;}}

$ # Or come from variable references
$ user='{&quot;name&quot;:&quot;Bob Bobson&quot;}' jb @user:json
{&quot;user&quot;:{&quot;name&quot;:&quot;Bob Bobson&quot;}}

$ # Or files
$ jb name=&quot;Bob Bobson&quot; > /tmp/user; jb @/tmp/user:json
{&quot;user&quot;:{&quot;name&quot;:&quot;Bob Bobson&quot;}}

$ # Arrays of JSON work the same way as other types.
$ jb users:json[$'\n']=&quot;$(jb name=Bob; jb name=Alice)&quot;
{&quot;users&quot;:[{&quot;name&quot;:&quot;Bob&quot;},{&quot;name&quot;:&quot;Alice&quot;}]}

$ # :json and :raw values are not formatted — whitespace in them is preserved
$ jb user:json=$'{\n  &quot;name&quot;: &quot;Bob Bobson&quot;\n}'
{&quot;user&quot;:{
  &quot;name&quot;: &quot;Bob Bobson&quot;
}}

$ # :json detects invalid JSON and fails with an error
$ jb oops:json='{&quot;truncated&quot;:'
json.encode_json(): not all inputs are valid JSON: '{&quot;truncated&quot;:'
json(): Could not encode the value of argument 'oops:json={&quot;truncated&quot;:' as a 'json' value. Read from inline value.
�␘

$ # However :raw performs no validation, so it must only be used with great care
$ # 🚨 This emits invalid JSON without failing! 🚨
$ jb broken:raw='{&quot;truncated&quot;:'
{&quot;broken&quot;:{&quot;truncated&quot;:}"><pre>$ <span><span><span>#</span> Like other types, :json and :raw values can be directly embedded in arguments</span></span>
$ <span>jb user:json=<span><span>'</span>{"name":"Bob Bobson"}<span>'</span></span></span>
<span>{"user":{"name":"Bob Bobson"}}</span>

$ <span><span><span>#</span> Or come from variable references</span></span>
$ <span>user=<span><span>'</span>{"name":"Bob Bobson"}<span>'</span></span> jb @user:json</span>
<span>{"user":{"name":"Bob Bobson"}}</span>

$ <span><span><span>#</span> Or files</span></span>
$ <span>jb name=<span><span>"</span>Bob Bobson<span>"</span></span> <span>&gt;</span> /tmp/user<span>;</span> jb @/tmp/user:json</span>
<span>{"user":{"name":"Bob Bobson"}}</span>

$ <span><span><span>#</span> Arrays of JSON work the same way as other types.</span></span>
$ <span>jb users:json[<span><span>$'</span><span>\n</span><span>'</span></span>]=<span><span>"</span><span><span>$(</span>jb name=Bob<span>;</span> jb name=Alice<span>)</span></span><span>"</span></span></span>
<span>{"users":[{"name":"Bob"},{"name":"Alice"}]}</span>

$ <span><span><span>#</span> :json and :raw values are not formatted — whitespace in them is preserved</span></span>
$ <span>jb user:json=<span><span>$'</span>{<span>\n</span>  "name": "Bob Bobson"<span>\n</span>}<span>'</span></span></span>
<span>{"user":{</span>
<span>  "name": "Bob Bobson"</span>
<span>}}</span>

$ <span><span><span>#</span> :json detects invalid JSON and fails with an error</span></span>
$ <span>jb oops:json=<span><span>'</span>{"truncated":<span>'</span></span></span>
<span>json.encode_json(): not all inputs are valid JSON: '{"truncated":'</span>
<span>json(): Could not encode the value of argument 'oops:json={"truncated":' as a 'json' value. Read from inline value.</span>
<span>�␘</span>

$ <span><span><span>#</span> However :raw performs no validation, so it must only be used with great care</span></span>
$ <span><span><span>#</span> 🚨 This emits invalid JSON without failing! 🚨</span></span>
$ <span>jb broken:raw=<span><span>'</span>{"truncated":<span>'</span></span></span>
<span>{"broken":{"truncated":}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">File references</h3><a id="user-content-file-references" aria-label="Permalink: File references" href="#file-references"></a></p>
<p dir="auto">The <code>@ref</code> syntax can be used to reference the content of files. If an <code> @ref</code>
starts with <code>/</code> or <code>./</code> it's taken to be a file (rather than a shell variable).</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ printf 'orange #3\nblue #5\n' > colours

$ jb my_colours@./colours
{&quot;my_colours&quot;:&quot;orange #3\nblue #5\n&quot;}

$ # The final path segment is used as the key if a key isn't set.
$ jb @./colours
{&quot;colours&quot;:&quot;orange #3\nblue #5\n&quot;}

$ # Array values split on newlines
$ jb @./colours:[]
{&quot;colours&quot;:[&quot;orange #3&quot;,&quot;blue #5&quot;]}

$ printf 'apple:pear:grape' > fruit

$ # The file can be split on a different character by naming it in the []
$ jb @./fruit:[:]
{&quot;fruit&quot;:[&quot;apple&quot;,&quot;pear&quot;,&quot;grape&quot;]}

$ # Which is shorthand for
$ jb @./fruit:/collection=array,split=:/
{&quot;fruit&quot;:[&quot;apple&quot;,&quot;pear&quot;,&quot;grape&quot;]}

$ # Split on null by setting split to the empty string
$ printf 'foo\nbar\n\x00bar baz\n\x00' > nullterminated
$ jb @./nullterminated:[]/split=/
{&quot;nullterminated&quot;:[&quot;foo\nbar\n&quot;,&quot;bar baz\n&quot;]}

$ # Read from stdin using the special /dev/stdin file
$ seq 3 | jb counts:number[]@/dev/stdin
{&quot;counts&quot;:[1,2,3]}"><pre>$ <span><span>printf</span> <span><span>'</span>orange #3\nblue #5\n<span>'</span></span> <span>&gt;</span> colours</span>

$ <span>jb my_colours@./colours</span>
<span>{"my_colours":"orange #3\nblue #5\n"}</span>

$ <span><span><span>#</span> The final path segment is used as the key if a key isn't set.</span></span>
$ <span>jb @./colours</span>
<span>{"colours":"orange #3\nblue #5\n"}</span>

$ <span><span><span>#</span> Array values split on newlines</span></span>
$ <span>jb @./colours:[]</span>
<span>{"colours":["orange #3","blue #5"]}</span>

$ <span><span>printf</span> <span><span>'</span>apple:pear:grape<span>'</span></span> <span>&gt;</span> fruit</span>

$ <span><span><span>#</span> The file can be split on a different character by naming it in the []</span></span>
$ <span>jb @./fruit:[:]</span>
<span>{"fruit":["apple","pear","grape"]}</span>

$ <span><span><span>#</span> Which is shorthand for</span></span>
$ <span>jb @./fruit:/collection=array,split=:/</span>
<span>{"fruit":["apple","pear","grape"]}</span>

$ <span><span><span>#</span> Split on null by setting split to the empty string</span></span>
$ <span><span>printf</span> <span><span>'</span>foo\nbar\n\x00bar baz\n\x00<span>'</span></span> <span>&gt;</span> nullterminated</span>
$ <span>jb @./nullterminated:[]/split=/</span>
<span>{"nullterminated":["foo\nbar\n","bar baz\n"]}</span>

$ <span><span><span>#</span> Read from stdin using the special /dev/stdin file</span></span>
$ <span>seq 3 <span>|</span> jb counts:number[]@/dev/stdin</span>
<span>{"counts":[1,2,3]}</span></pre></div>
<p dir="auto">File references become especially powerful when combined with process
substitution — a shell feature that provides a dynamic, temporary file
containing the output of another program.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # Use process substitution to nest jb calls and pull multiple shell pipelines
$ # into one JSON output.
$ jb counts:number[]@<(seq 3) \
>    people:json[]@<(jb name=Bob; jb name=Alice)
{&quot;counts&quot;:[1,2,3],&quot;people&quot;:[{&quot;name&quot;:&quot;Bob&quot;},{&quot;name&quot;:&quot;Alice&quot;}]}"><pre>$ <span><span><span>#</span> Use process substitution to nest jb calls and pull multiple shell pipelines</span></span>
$ <span><span><span>#</span> into one JSON output.</span></span>
$ <span>jb counts:number[]@<span><span>&lt;(</span>seq 3<span>)</span></span> \</span>
&gt;    <span>people:json[]@<span><span>&lt;(</span>jb name=Bob<span>;</span> jb name=Alice<span>)</span></span></span>
<span>{"counts":[1,2,3],"people":[{"name":"Bob"},{"name":"Alice"}]}</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Aside: Process substitution 101</h4><a id="user-content-aside-process-substitution-101" aria-label="Permalink: Aside: Process substitution 101" href="#aside-process-substitution-101"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # What's going on when we use process substitution? The <(...) syntax.
$ jb msg@<(printf &quot;Hi!&quot;)
{&quot;msg&quot;:&quot;Hi!&quot;}

$ # The shell replaces <(...) with a file path. That file contains the output of
$ # the command inside the <(...) when read. (But the catch is, the file only
$ # exists while the command runs, and it's not a normal file, so the contents
$ # isn't stored on disk.)

$ # We can see this if we echo the the substitution:
$ echo This is the substitution result: <(printf &quot;Hi!&quot;)
This is the substitution result: /dev/fd/...

$ # If we cat the substitution instead of echoing it, we read the file contents:
$ cat <(printf &quot;Hi!&quot;)
Hi!

$ # So when we use this with jb, it's as if we ran:  jb msg@/dev/fd/...

$ # We can see this in action by enabling tracing in Bash:
$ set -o xtrace;  jb msg@<(printf &quot;Hi!&quot;);  set +o xtrace
+ jb msg@/dev/fd/...
++ printf 'Hi!'
{&quot;msg&quot;:&quot;Hi!&quot;}
+ set +o xtrace"><pre>$ <span><span><span>#</span> What's going on when we use process substitution? The &lt;(...) syntax.</span></span>
$ <span>jb msg@<span><span>&lt;(</span>printf <span><span>"</span>Hi!<span>"</span></span><span>)</span></span></span>
<span>{"msg":"Hi!"}</span>

$ <span><span><span>#</span> The shell replaces &lt;(...) with a file path. That file contains the output of</span></span>
$ <span><span><span>#</span> the command inside the &lt;(...) when read. (But the catch is, the file only</span></span>
$ <span><span><span>#</span> exists while the command runs, and it's not a normal file, so the contents</span></span>
$ <span><span><span>#</span> isn't stored on disk.)</span></span>

$ <span><span><span>#</span> We can see this if we echo the the substitution:</span></span>
$ <span><span>echo</span> This is the substitution result: <span><span>&lt;(</span>printf <span><span>"</span>Hi!<span>"</span></span><span>)</span></span></span>
<span>This is the substitution result: /dev/fd/...</span>

$ <span><span><span>#</span> If we cat the substitution instead of echoing it, we read the file contents:</span></span>
$ <span>cat <span><span>&lt;(</span>printf <span><span>"</span>Hi!<span>"</span></span><span>)</span></span></span>
<span>Hi!</span>

$ <span><span><span>#</span> So when we use this with jb, it's as if we ran:  jb msg@/dev/fd/...</span></span>

$ <span><span><span>#</span> We can see this in action by enabling tracing in Bash:</span></span>
$ <span><span>set</span> -o xtrace<span>;</span>  jb msg@<span><span>&lt;(</span>printf <span><span>"</span>Hi!<span>"</span></span><span>)</span></span><span>;</span>  <span>set</span> +o xtrace</span>
<span>+ jb msg@/dev/fd/...</span>
<span>++ printf 'Hi!'</span>
<span>{"msg":"Hi!"}</span>
<span>+ set +o xtrace</span></pre></div>
<p dir="auto">Because <code>&lt;(...)</code> becomes a path, you don't <em>have</em> to quote it, which makes
forming commands a bit easier than using <em>command substitution</em> to do the same
thing (<code>echo "$(printf like this)"</code>). And you only pass a short file path as an
argument, not a potentially huge string.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Back to file references</h4><a id="user-content-back-to-file-references" aria-label="Permalink: Back to file references" href="#back-to-file-references"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # Process substitution can nest multiple times
$ jb owners:json@<(
>   jb people:json[]@<(jb name=Bob; jb name=Alice)
> )
{&quot;owners&quot;:{&quot;people&quot;:[{&quot;name&quot;:&quot;Bob&quot;},{&quot;name&quot;:&quot;Alice&quot;}]}}

$ # Files can be referenced indirectly using a shell variable.
$ # If @var is used and $var is not set, but $var_FILE is, the filename is read
$ # from $var_FILE and the content of the file is used.
$ printf 'secret123' > db_password
$ db_password_FILE=./db_password jb @db_password
{&quot;db_password&quot;:&quot;secret123&quot;}"><pre>$ <span><span><span>#</span> Process substitution can nest multiple times</span></span>
$ <span>jb owners:json@<span><span>&lt;(</span></span></span>
&gt;   <span>jb people:json[]@<span><span>&lt;(</span>jb name=Bob<span>;</span> jb name=Alice<span>)</span></span></span>
&gt; <span>)</span>
<span>{"owners":{"people":[{"name":"Bob"},{"name":"Alice"}]}}</span>

$ <span><span><span>#</span> Files can be referenced indirectly using a shell variable.</span></span>
$ <span><span><span>#</span> If @var is used and $var is not set, but $var_FILE is, the filename is read</span></span>
$ <span><span><span>#</span> from $var_FILE and the content of the file is used.</span></span>
$ <span><span>printf</span> <span><span>'</span>secret123<span>'</span></span> <span>&gt;</span> db_password</span>
$ <span>db_password_FILE=./db_password jb @db_password</span>
<span>{"db_password":"secret123"}</span></pre></div>
<p dir="auto">(This pattern is often used to securely pass secrets via environment variables,
<a href="#environment-variable-exposure">without directly exposing the secret's value itself in the environment</a>,
to avoid accidental exposure.)</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # Nesting lots of process substitution levels can become unwieldy, but we can
$ # flatten the nesting by holding the process substitution filenames in shell
$ #&nbsp;variables, using the _FILE var feature to reference them:
$ people_FILE=<(jb name=Bob; jb name=Alice) \
> owners_FILE=<(jb @people:json[]) \
> jb @owners:json
{&quot;owners&quot;:{&quot;people&quot;:[{&quot;name&quot;:&quot;Bob&quot;},{&quot;name&quot;:&quot;Alice&quot;}]}}"><pre>$ <span><span><span>#</span> Nesting lots of process substitution levels can become unwieldy, but we can</span></span>
$ <span><span><span>#</span> flatten the nesting by holding the process substitution filenames in shell</span></span>
$ <span><span><span>#</span>&nbsp;variables, using the _FILE var feature to reference them:</span></span>
$ <span>people_FILE=<span><span>&lt;(</span>jb name=Bob<span>;</span> jb name=Alice<span>)</span></span> \</span>
&gt; <span>owners_FILE=<span><span>&lt;(</span>jb @people:json[]<span>)</span></span> \</span>
&gt; <span>jb @owners:json</span>
<span>{"owners":{"people":[{"name":"Bob"},{"name":"Alice"}]}}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Argument structure</h3><a id="user-content-argument-structure" aria-label="Permalink: Argument structure" href="#argument-structure"></a></p>
<p dir="auto">Arguments have 3 main parts: a key, type and value. The structure (omitting some
details for clarity) is:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/h4l/json.bash/blob/main/docs/syntax-diagrams/approximate-argument.svg"><img width="100%" src="https://github.com/h4l/json.bash/raw/main/docs/syntax-diagrams/approximate-argument.svg" alt="A railroad syntax diagram showing the key, type and value structure of an argument, in more detail than the minimal argument diagram, but still omitting some details." title="Approximate Argument Structure Diagram"></a></p>
<p dir="auto">The <a href="https://github.com/h4l/json.bash/blob/main/docs/syntax.md">Argument syntax</a> page has more detail.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Error handling</h3><a id="user-content-error-handling" aria-label="Permalink: Error handling" href="#error-handling"></a></p>
<p dir="auto"><code>json.bash</code> aims to fail quickly, cleanly and clearly when problems happen.</p>
<blockquote>
<p dir="auto">Please open an issue if you discover a case where an error goes unreported, is
not reported clearly, or you find it's not easy to prevent incorrect data
getting generated.</p>
</blockquote>
<p dir="auto">Invalid values in typed arguments will cause an error — values are not coerced
if a type is specified. <code>:bool</code> and <code>:null</code> are pedantic — values must be
exactly <code>true</code> / <code>false</code> / <code>null</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ active=tRuE jb @active:bool
json.encode_bool(): not all inputs are bools: 'tRuE'
json(): Could not encode the value of argument '@active:bool' as a 'bool' value. Read from variable $active.
�␘"><pre>$ <span>active=tRuE jb @active:bool</span>
<span>json.encode_bool(): not all inputs are bools: 'tRuE'</span>
<span>json(): Could not encode the value of argument '@active:bool' as a 'bool' value. Read from variable $active.</span>
<span>�␘</span></pre></div>
<p dir="auto">Errors are reported with specific exit statuses:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # Errors in user-provided data fail with status 1
$ jb data:json='invalid'; echo status=$?
json.encode_json(): not all inputs are valid JSON: 'invalid'
json(): Could not encode the value of argument 'data:json=invalid' as a 'json' value. Read from inline value.
�␘
status=1

$ # Errors in developer-provided arguments fail with status 1
$ jb bad_arg:cheese; echo status=$?
json.parse_argument(): type name must be one of auto, bool, false, json, null, number, raw, string or true, but was 'cheese'
json(): Could not parse argument 'bad_arg:cheese'. Argument is not structured correctly, see --help for examples.
�␘
status=2

$ # Arguments referencing variables that don't exist fail with status 3
$ jb @missing; echo status=$?
json(): Could not process argument '@missing'. Its value references unbound variable $missing. (Use the '~' flag after the :type to treat a missing value as empty.)
�␘
status=3

$ # Arguments referencing files that don't exist fail with status 4
$ jb @/does/not/exist; echo status=$?
/.../bin/jb: line ...: /does/not/exist: No such file or directory
json(): Could not open the file '/does/not/exist' referenced as the value of argument '@/does/not/exist'.
�␘
status=4"><pre>$ <span><span><span>#</span> Errors in user-provided data fail with status 1</span></span>
$ <span>jb data:json=<span><span>'</span>invalid<span>'</span></span><span>;</span> <span>echo</span> status=<span>$?</span></span>
<span>json.encode_json(): not all inputs are valid JSON: 'invalid'</span>
<span>json(): Could not encode the value of argument 'data:json=invalid' as a 'json' value. Read from inline value.</span>
<span>�␘</span>
<span>status=1</span>

$ <span><span><span>#</span> Errors in developer-provided arguments fail with status 1</span></span>
$ <span>jb bad_arg:cheese<span>;</span> <span>echo</span> status=<span>$?</span></span>
<span>json.parse_argument(): type name must be one of auto, bool, false, json, null, number, raw, string or true, but was 'cheese'</span>
<span>json(): Could not parse argument 'bad_arg:cheese'. Argument is not structured correctly, see --help for examples.</span>
<span>�␘</span>
<span>status=2</span>

$ <span><span><span>#</span> Arguments referencing variables that don't exist fail with status 3</span></span>
$ <span>jb @missing<span>;</span> <span>echo</span> status=<span>$?</span></span>
<span>json(): Could not process argument '@missing'. Its value references unbound variable $missing. (Use the '~' flag after the :type to treat a missing value as empty.)</span>
<span>�␘</span>
<span>status=3</span>

$ <span><span><span>#</span> Arguments referencing files that don't exist fail with status 4</span></span>
$ <span>jb @/does/not/exist<span>;</span> <span>echo</span> status=<span>$?</span></span>
<span>/.../bin/jb: line ...: /does/not/exist: No such file or directory</span>
<span>json(): Could not open the file '/does/not/exist' referenced as the value of argument '@/does/not/exist'.</span>
<span>�␘</span>
<span>status=4</span></pre></div>
<p dir="auto"><code>jb</code> can detect errors in upstream <code>jb</code> calls that are pulled into a downstream
<code>jb</code> process, such as when several <code>jb</code> calls are fed into each other using
<a href="#file-references">process substitution</a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # The jb call 3 levels deep reading the missing file ./not-found fails
$ jb club:json@<(
>   jb name=&quot;jb Users&quot; members:json[]@<(
>     jb name=h4l; jb name@./not-found
>   )
> )
...: ./not-found: No such file or directory
json(): Could not open the file './not-found' referenced as the value of argument 'name@./not-found'.
json.encode_json(): not all inputs are valid JSON: '{&quot;name&quot;:&quot;h4l&quot;}' $'\030'
json(): Could not encode the value of argument 'members:json[]@/dev/fd/...' as an array with 'json' values. Read from file /dev/fd/..., split into chunks on $'\n', interpreted chunks with 'raw' format.
json.encode_json(): not all inputs are valid JSON: $'\030'
json(): Could not encode the value of argument 'club:json@/dev/fd/...' as a 'json' value. Read from file /dev/fd/..., up to the first 0x00 byte or end-of-file.
�␘"><pre>$ <span><span><span>#</span> The jb call 3 levels deep reading the missing file ./not-found fails</span></span>
$ <span>jb club:json@<span><span>&lt;(</span></span></span>
&gt;   <span>jb name=<span><span>"</span>jb Users<span>"</span></span> members:json[]@<span><span>&lt;(</span></span></span>
&gt;     <span>jb name=h4l<span>;</span> jb name@./not-found</span>
&gt;   <span>)</span>
&gt; <span>)</span>
<span>...: ./not-found: No such file or directory</span>
<span>json(): Could not open the file './not-found' referenced as the value of argument 'name@./not-found'.</span>
<span>json.encode_json(): not all inputs are valid JSON: '{"name":"h4l"}' $'\030'</span>
<span>json(): Could not encode the value of argument 'members:json[]@/dev/fd/...' as an array with 'json' values. Read from file /dev/fd/..., split into chunks on $'\n', interpreted chunks with 'raw' format.</span>
<span>json.encode_json(): not all inputs are valid JSON: $'\030'</span>
<span>json(): Could not encode the value of argument 'club:json@/dev/fd/...' as a 'json' value. Read from file /dev/fd/..., up to the first 0x00 byte or end-of-file.</span>
<span>�␘</span></pre></div>
<p dir="auto">Notice the <a href="https://en.wikipedia.org/wiki/Unicode_control_characters" rel="nofollow">␘</a> symbol in the output? It's the Unicode symbol for
the <a href="https://en.wikipedia.org/wiki/Cancel_character" rel="nofollow">Cancel control character</a>.</p>
<p dir="auto"><code>jb</code> propagates errors by emitting a <a href="https://en.wikipedia.org/wiki/Cancel_character" rel="nofollow">Cancel control character</a> when it
fails, which causes its output to be invalid JSON, which prevents the erroneous
output from being parsed by downstream JSON-consuming programs (<code>jb</code> or
otherwise). We call this Stream Poisoning, because the Cancel control character
poisons the output, and this poisoned output flows downstream until it's
detected.</p>
<p dir="auto">The result of this is that it's safe to pipe the output of a <code>jb</code> program into
another JSON-consuming program, with the knowledge that you'll get an error if
something has failed upstream, without needing to meticulously collect and check
the every exit status of every program contributing to the output.</p>
<p dir="auto">See <a href="https://github.com/h4l/json.bash/blob/main/docs/stream-poisoning.md">docs/stream-poisoning.md</a> for more on how this
works.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Security and correctness</h3><a id="user-content-security-and-correctness" aria-label="Permalink: Security and correctness" href="#security-and-correctness"></a></p>
<p dir="auto"><code>jb</code> can safely generate JSON from untrusted user input, but there are some ways
to get this wrong.</p>
<p dir="auto">It's safe to use untrusted input in:</p>
<ul dir="auto">
<li>
<p dir="auto">Inline values — after the value's <code>=</code> in an argument.</p>
<p dir="auto">With argument <code>key:type=value</code> Anything after the value's <code>=</code> in an argument
is used as-is and not interpreted/unescaped, so it can contain untrusted
input.</p>
<p dir="auto">The <code>=</code> must be preceded by a key or <code>:</code> (type section marker), otherwise an
argument starting with a <code>=</code> such as <code>=foo</code> is parsed as a key, which could
allow text inserted into the argument to be parsed as the value if not escaped
correctly.</p>
</li>
<li>
<p dir="auto">Variable references — the <em>value</em> held in a variable referenced by an
argument.</p>
<p dir="auto">With argument <code>@foo</code>, the value of <code>$foo</code> is is used as-is and not
interpreted/unescaped, so it can contain untrusted input.</p>
</li>
<li>
<p dir="auto">File references — the contents of a file referenced by a argument.</p>
<p dir="auto">The contents of files is not interpreted/unescaped, so they can contain
untrusted input.</p>
</li>
</ul>
<p dir="auto">In general, avoid inserting user-provided input into the argument string passed
to jb before the value's <code>=</code>. To create dynamic object property names from user
input, store the user-provided value in a variable or file, and use an <code>@ref</code> to
reference it:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ dynamic_prop='Untrusted' jb @dynamic_prop=value
{&quot;Untrusted&quot;:&quot;value&quot;}"><pre>$ <span>dynamic_prop=<span><span>'</span>Untrusted<span>'</span></span> jb @dynamic_prop=value</span>
<span>{"Untrusted":"value"}</span></pre></div>
<p dir="auto">If you format user-input into an argument string, they could insert an <code>@ref</code> of
their choice, and pull in a file or variable they shouldn't have access to. You
can escape special characters in argument values by doubling characters, but
it's safer to use an <code>@ref</code> — if you get an <code>@ref</code> wrong you get an error,
whereas if you get escaping wrong, you may create a vulnerability.</p>
<p dir="auto">References are not supported when specifying argument attributes, like
<code>/empty=x/</code>, so <code>,</code> in these values needs to be escaped by doubling it. E.g. to
use a comma as a split char, use <code>/empty=string='Why,, yes.'/</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ empty= jb msg:/empty=string='Why,, yes.'/??@empty
{&quot;msg&quot;:&quot;Why, yes.&quot;}"><pre>$ <span>empty= jb msg:/empty=string=<span><span>'</span>Why,, yes.<span>'</span></span>/<span>??</span>@empty</span>
<span>{"msg":"Why, yes."}</span></pre></div>
<p dir="auto">To pass a dynamic file location, use a <code>_FILE</code> variable reference or read the
file with a normal shell construct and redirect the input. You must also
separately validate that the referenced file should be accessible.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ printf 'Example\nContent\n' > /tmp/example
$ user_file=/tmp/example

$ user_specified_FILE=$user_file jb user_file_content@user_specified
{&quot;user_file_content&quot;:&quot;Example\nContent\n&quot;}

$ jb user_file_content@<(cat &quot;$user_file&quot;)
{&quot;user_file_content&quot;:&quot;Example\nContent\n&quot;}

$ jb user_file_content=&quot;$(<&quot;$user_file&quot;)&quot;  # $() strips the trailing newline
{&quot;user_file_content&quot;:&quot;Example\nContent&quot;}"><pre>$ <span><span>printf</span> <span><span>'</span>Example\nContent\n<span>'</span></span> <span>&gt;</span> /tmp/example</span>
$ <span>user_file=/tmp/example</span>

$ <span>user_specified_FILE=<span>$user_file</span> jb user_file_content@user_specified</span>
<span>{"user_file_content":"Example\nContent\n"}</span>

$ <span>jb user_file_content@<span><span>&lt;(</span>cat <span><span>"</span><span>$user_file</span><span>"</span></span><span>)</span></span></span>
<span>{"user_file_content":"Example\nContent\n"}</span>

$ <span>jb user_file_content=<span><span>"</span><span><span>$(</span><span>&lt;</span><span><span>"</span><span>$user_file</span><span>"</span></span><span>)</span></span><span>"</span></span>  <span><span>#</span> $() strips the trailing newline</span></span>
<span>{"user_file_content":"Example\nContent"}</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Environment variable exposure</h4><a id="user-content-environment-variable-exposure" aria-label="Permalink: Environment variable exposure" href="#environment-variable-exposure"></a></p>
<p dir="auto"><code>jb</code> <code>@var</code> refs have the advantage over normal shell <code>$var</code> refs in that they
are not expanded by the shell before executing the command, so sensitive values
in shell variables are not exposed as process arguments when using <code>@var</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ password=hunter2

$ # shell $var — secret's value is in process arguments
$ jb password=&quot;$password&quot; visible_args:[]/split=/@/proc/self/cmdline
{&quot;password&quot;:&quot;hunter2&quot;,&quot;visible_args&quot;:[&quot;bash&quot;,&quot;/.../bin/jb&quot;,&quot;password=hunter2&quot;,&quot;visible_args:[]/split=/@/proc/self/cmdline&quot;]}

$ # jb @var — only the variable name is in process arguments
$ password=$password jb @password visible_args:[]/split=/@/proc/self/cmdline
{&quot;password&quot;:&quot;hunter2&quot;,&quot;visible_args&quot;:[&quot;bash&quot;,&quot;/.../bin/jb&quot;,&quot;@password&quot;,&quot;visible_args:[]/split=/@/proc/self/cmdline&quot;]}"><pre>$ <span>password=hunter2</span>

$ <span><span><span>#</span> shell $var — secret's value is in process arguments</span></span>
$ <span>jb password=<span><span>"</span><span>$password</span><span>"</span></span> visible_args:[]/split=/@/proc/self/cmdline</span>
<span>{"password":"hunter2","visible_args":["bash","/.../bin/jb","password=hunter2","visible_args:[]/split=/@/proc/self/cmdline"]}</span>

$ <span><span><span>#</span> jb @var — only the variable name is in process arguments</span></span>
$ <span>password=<span>$password</span> jb @password visible_args:[]/split=/@/proc/self/cmdline</span>
<span>{"password":"hunter2","visible_args":["bash","/.../bin/jb","@password","visible_args:[]/split=/@/proc/self/cmdline"]}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>jb-cat</code>, <code>jb-echo</code>, <code>jb-stream</code> utility programs</h3><a id="user-content-jb-cat-jb-echo-jb-stream-utility-programs" aria-label="Permalink: jb-cat, jb-echo, jb-stream utility programs" href="#jb-cat-jb-echo-jb-stream-utility-programs"></a></p>
<p dir="auto"><code>json.bash</code> has a few single-purpose utility programs that were originally demo
programs for the Bash API, but could be use useful by themselves:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ # jb-echo is like echo, but each argument becomes a string element in a JSON array
$ jb-echo foo &quot;bar baz&quot; boz
[&quot;foo&quot;,&quot;bar baz&quot;,&quot;boz&quot;]

$ printf 'The Cat\nsat on\nthe mat.\n' > catmat
$ printf 'The Bat\nhid in\nthe hat.\n' > bathat

$ # jb-cat is like cat, but the output is stream-encoded as a single JSON string
$ jb-cat catmat bathat
&quot;The Cat\nsat on\nthe mat.\nThe Bat\nhid in\nthe hat.\n&quot;

$ # jb-stream is a filter program that encodes each input line as a JSON string
$ cat catmat bathat | jb-stream
&quot;The Cat&quot;
&quot;sat on&quot;
&quot;the mat.&quot;
&quot;The Bat&quot;
&quot;hid in&quot;
&quot;the hat.&quot;"><pre>$ <span><span><span>#</span> jb-echo is like echo, but each argument becomes a string element in a JSON array</span></span>
$ <span>jb-echo foo <span><span>"</span>bar baz<span>"</span></span> boz</span>
<span>["foo","bar baz","boz"]</span>

$ <span><span>printf</span> <span><span>'</span>The Cat\nsat on\nthe mat.\n<span>'</span></span> <span>&gt;</span> catmat</span>
$ <span><span>printf</span> <span><span>'</span>The Bat\nhid in\nthe hat.\n<span>'</span></span> <span>&gt;</span> bathat</span>

$ <span><span><span>#</span> jb-cat is like cat, but the output is stream-encoded as a single JSON string</span></span>
$ <span>jb-cat catmat bathat</span>
<span>"The Cat\nsat on\nthe mat.\nThe Bat\nhid in\nthe hat.\n"</span>

$ <span><span><span>#</span> jb-stream is a filter program that encodes each input line as a JSON string</span></span>
$ <span>cat catmat bathat <span>|</span> jb-stream</span>
<span>"The Cat"</span>
<span>"sat on"</span>
<span>"the mat."</span>
<span>"The Bat"</span>
<span>"hid in"</span>
<span>"the hat."</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Streaming output</h3><a id="user-content-streaming-output" aria-label="Permalink: Streaming output" href="#streaming-output"></a></p>
<p dir="auto">By default <code>jb</code> collects output in a buffer and outputs it all at once at the
end. This has the advantage that it does not emit partial output if an error
occurs mid-way through.</p>
<p dir="auto">However, setting the <code>JSON_BASH_STREAM=true</code> makes <code>jb</code> output content
incrementally. <code>jb</code> can stream-encode values it's pulling from file references:</p>
<ul dir="auto">
<li>Single string values from files are stream-encoded</li>
<li>Arrays of any type coming from files are stream-encoded (individual elements
are fully buffered), but elements are emitted incrementally</li>
<li><code>:raw</code> values from files are streamed</li>
</ul>
<p dir="auto"><code>:json</code> values can't be streamed unfortunately — <code>jb</code> (ab)uses grep to validate
JSON using PCRE's recursive matching features, but sadly grep buffers complete
inputs, even when backtracking and matched-region output are disabled.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Argument syntax details</h3><a id="user-content-argument-syntax-details" aria-label="Permalink: Argument syntax details" href="#argument-syntax-details"></a></p>
<p dir="auto">The full syntax of <code>jb</code> arguments is documented in a (pseudo) grammar in
<a href="https://github.com/h4l/json.bash/blob/main/hack/syntax_patterns.bash">hack/syntax_patterns.bash</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Background &amp; performance notes</h2><a id="user-content-background--performance-notes" aria-label="Permalink: Background &amp; performance notes" href="#background--performance-notes"></a></p>
<p dir="auto">Quite reasonably, you may be wondering why anyone would use Bash to implement a
JSON encoder. Won't that be ridiculously slow? I thought so too. Initially, I
just wanted to encode JSON strings from Bash without needing to depend on a
separate program. My initial few attempts at this were indeed hideously slow.
But after a few iterations I was able to get decent performance by operating
only on entire strings (or arrays of strings) (not byte-by-byte, or
string-by-string for arrays), and absolutely avoiding any forking of subshells.</p>
<p dir="auto">If you don't fork, and minimise the number of Bash-level operations, Bash can do
surprisingly well. Of course, performance still can't compare with a C program.
Well, that depends what you're measuring. Because starting a new process can be
surprisingly slow. So a race between <code>json.bash</code> and program like <a href="https://github.com/jqlang/jq"><code>jq</code></a> or
<a href="https://github.com/jpmens/jo"><code>jo</code></a> is a bit like a 100m race between a tortoise and a hare, where the
tortoise gets a 1 hour headstart.</p>
<p dir="auto">If you care about latency rather than throughput, calling <code>json</code> from an
already-running Bash script is a little faster than running a separate <code>jo</code>
process. And significantly faster than running <code>jq</code>, which is really slow to
start.</p>
<p dir="auto">There's a very basic benchmark script at
<a href="https://github.com/h4l/json.bash/blob/main/hack/hot_loop.bash">hack/hot_loop.bash</a>:</p>
<div data-snippet-clipboard-copy-content="$ time hack/hot_loop.bash json.bash 10000 > /dev/null
json.bash

real    0m8.193s
user    0m8.174s
sys     0m0.019s

$ time hack/hot_loop.bash jo 10000 > /dev/null
jo

real    0m9.393s
user    0m2.566s
sys     0m7.386s

$ # Note: 1000 not 10_000
$ time hack/hot_loop.bash jq 1000 > /dev/null
jq

real    0m20.453s
user    0m19.127s
sys     0m1.386s"><pre><code>$ time hack/hot_loop.bash json.bash 10000 &gt; /dev/null
json.bash

real    0m8.193s
user    0m8.174s
sys     0m0.019s

$ time hack/hot_loop.bash jo 10000 &gt; /dev/null
jo

real    0m9.393s
user    0m2.566s
sys     0m7.386s

$ # Note: 1000 not 10_000
$ time hack/hot_loop.bash jq 1000 &gt; /dev/null
jq

real    0m20.453s
user    0m19.127s
sys     0m1.386s
</code></pre></div>
<p dir="auto">If we just use <code>json.bash</code>'s <code>json.encode_string</code> encoding function to manually
construct the JSON (not the full argument parsing stuff) we can do a lot better
still:</p>
<div data-snippet-clipboard-copy-content="$ time hack/hot_loop.bash custom-json.bash 10000 > /dev/null
custom-json.bash

real    0m1.901s
user    0m1.891s
sys     0m0.011s"><pre><code>$ time hack/hot_loop.bash custom-json.bash 10000 &gt; /dev/null
custom-json.bash

real    0m1.901s
user    0m1.891s
sys     0m0.011s
</code></pre></div>
<p dir="auto">This kind of purpose-specific encoding is what I had in mind when I started
this. I was calling <code>jq</code> lots of times from a Bash script, finding it to be very
slow, and wondering if I could start a single <code>jq</code> process and make a kind of
tiny RPC protocol, sending it JSON from the Bash script to avoid the startup
delay on each operation. That would require some ability to encode JSON from
Bash.</p>
<p dir="auto">I wasn't planning to write something comparable to <code>jo</code> when I started, but idea
of a <code>jo</code>-like program that only depends on bash kind of appealed to me. Maybe I
should port it to a more suitable language though. The program is a now a lot
larger in size and scope than I originally anticipated when starting, I
certainly wouldn't have written it in bash if I'd known how large it'd become.
🙃</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<ul dir="auto">
<li><a href="https://github.com/jpmens/jo">jo</a> for the general idea of a command-line program that generates JSON</li>
<li><a href="https://github.com/OceanSprint/tesh">tesh</a> which automatically runs and tests the command-line output examples
here — it would not be at all practical to maintain these kind of examples
without it. With tesh the examples become a beneficial second layer of tests,
rather than a maintenance burdon.</li>
<li><a href="https://github.com/jqlang/jq">jq</a> for making it pleasant to use with JSON on the command-line and in shell
scripts</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Received an AI Email (642 pts)]]></title>
            <link>https://timharek.no/blog/i-received-an-ai-email</link>
            <guid>40862865</guid>
            <pubDate>Wed, 03 Jul 2024 05:05:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://timharek.no/blog/i-received-an-ai-email">https://timharek.no/blog/i-received-an-ai-email</a>, See on <a href="https://news.ycombinator.com/item?id=40862865">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Why AI Infrastructure Startups Are Insanely Hard to Build (176 pts)]]></title>
            <link>https://nextword.substack.com/p/why-ai-infrastructure-startups-are</link>
            <guid>40862436</guid>
            <pubDate>Wed, 03 Jul 2024 03:15:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nextword.substack.com/p/why-ai-infrastructure-startups-are">https://nextword.substack.com/p/why-ai-infrastructure-startups-are</a>, See on <a href="https://news.ycombinator.com/item?id=40862436">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Recently, </span><a href="http://adept.ai/" rel="">Adept AI</a><span> announced </span><a href="https://techcrunch.com/2024/06/28/amazon-hires-founders-away-from-ai-startup-adept/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAM7kyimVR-Nntc7w3SCp466ss9rI61B5U68ESaJExyUE65kA2h6sdH5pHKpdJ0oi6Y0SvXy7wg2OgsX1JTB-hZw9n0esnLkFCY_6JckUVqoIbgGFEy2gSjzMw4YJBYwbCFKJqPR19xgviwpcnO8cCVPa99I_tMkjrjBWHoQqPtTI" rel="">they are being acquired by Amazon</a><span>, and this solidified a somewhat controversial opinion I’ve held for a while - </span><strong>that AI infra startups are a tarpit idea</strong><span>, </span><strong><span>especially as a “venture-scale” business.</span></strong></p><p><span>The term “tarpit idea” refers to startup ideas that sound reasonable on the surface, but when put to test against reality or rigorous thought, fail to hold up.</span><br></p><div><p><span>I believe most AI infra startups will also fall into this category, where AI infra refers to the “building blocks” companies </span><strong>between the cloud layer and the application layer</strong><span> - RAG services, finetuning infrastructure, text processing services, TTS APIs, vector databases, etc. I won’t name specific names, but just think of any AI infra startup that raised sizable seed rounds off of open source or social media momentum.</span></p></div><p><span>I also believe </span><strong>many founders agree with this viewpoint</strong><span>, which explains the sale of Adept (to Amazon), </span><a href="https://openai.com/index/openai-acquires-rockset/" rel="">Rockset (to OpenAI)</a><span>, InflectionAI (to Microsoft), as well as the soon to be acquisitions of Stability (if it happens), </span><a href="http://character.ai/" rel="">Character</a><span>AI, etc. Every incumbent is looking at M&amp;A to paint an “end-to-end AI platform” story. Only a lucky few will get bought.</span><br></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;June 28 updated AI Infra market map&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="June 28 updated AI Infra market map" title="June 28 updated AI Infra market map" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Source: Bessemer Venture Partners</figcaption></figure></div><div><p><span>So why is selling AI infrastructure </span><em>as a startup</em><span> a tarpit idea? On paper, it’s perfectly reasonable to sell picks and shovels amidst proliferation of AI startups and enterprises building Gen AI features. After all, there’s over 30K “.ai” domains registered every month.</span></p></div><div><p><strong>In a nutshell, the new AI infra startups will struggle to succeed because they lack significant differentiation and capital to crack the enterprise segment.</strong><span> It’s not the startups’ fault, the real problem is competitive dynamics. There’s simply too many entities offering the same table stakes features within 1-3 months apart from each other, which creates a collective tarpit dynamic, where only the incumbents can keep swimming.</span></p></div><p>The argument goes:</p><ul><li><p>For AI infra startups to be “venture scale”, they will eventually need to win over enterprise customers. No question. That requires the startups to have some sustainable edge that separates their products from the incumbents’ (GCP, AWS, as well as the likes of Vercel, Databricks, Datadog, etc).</p></li><li><p>Unfortunately, most cutting edge innovation either comes from the incumbents or the research / OSS community - and incumbents are in a better position to commercialize the innovations because they have more usage data than startups, as well as the relationships.</p></li><li><p>To add salt to the injury, any good ideas that originate from startups get benchmarked and copied quickly. For example, I was quite surprised how quickly Databricks and Datadog caught up to the leading LLMOps products from the startup world (e.g. Arize AI). </p></li><li><p>Furthermore, OSS community can’t help but create OSS versions of other AI infra startups’ products - perhaps a testament to how easy it has become to write software.</p></li><li><p>Thus, startups struggle to maintain a sustainable lead over the incumbents to buy them time to win enterprise contracts.</p></li><li><p>And enterprise customers are incentivized to “hold off” on onboarding new vendors, because vendor products diminish in value so quickly because AI landscape changes every few months.</p></li><li><p>This ultimately lengthens sales cycles, and increases churn, which hurts startups more than the incumbents.</p></li></ul><div><p><span>There are also some other dynamics at play (to be discussed in the next section) - but essentially the AI infrastructure space becomes a grind that favors players with the longest runways.</span></p></div><div><p><span>My intention here is not to doom-post, but to highlight some real challenges, which I’m happy to be wrong on (DM me if you disagree). Also, I will end by offering some advice to AI infra startups.</span></p></div><p><em><span>To clarify, by “AI infra startup”, I’m referring to “venture scale” AI infrastructure startups. I’m sure founders can create essentially system integration agencies targeting SMB or mid market, and call themselves an AI infra company. But that’s a completely different business with a much smaller upside.</span><br></em></p><p>There’s three other major forces that’s worsening the competitive environment:</p><ol><li><p>Builders are now conditioned to “demand” composability, a.k.a making it easy to switch out your product for others’. This is great for application layer companies, but not infrastructure companies. Developers can rip out Langchain with Llamaindex, OpenAI models with Claude 3.5 through AWS Bedrock, etc. Every layer of the LLM training and inference stack has at least 10+ viable solutions, that it becomes difficult to create any type of lock-in.</p></li><li><p>The ongoing plummeting of inference costs also plays a role. The COGS are dropping fast, so AI infra players need to constantly price-match the incumbents who have the biggest economies of scale. Models or code have little perceived differentiation, so the consumption goes to the lowest cost providers (incumbents).</p></li><li><p>Incumbents seem to all have the same business strategy of creating an “end-to-end AI platform”. Databricks is getting into AI model training and business intelligence, competing with AWS Sagemaker and Tableau. Github Workspaces is getting into AI-powered security reviews, etc.</p><ol><li><p>Everyone’s default product strategy is to own all upstream and downstream workloads from their core product, which unintentionally makes startups’ lives more difficult, since it becomes hard to compete with a point solution.</p></li></ol></li></ol><p><br><span>With all these challenges, some AI infra startups have chosen to go vertical or move to the application layer. For example, I have been tracking a “Business Intelligence with Natural Language” startup since late 2022 that has pivoted three times already from:</span></p><ul><li><p>a general purpose “chat with data” platform, to</p></li><li><p>“chat with business intelligence data” platform, to</p></li><li><p>“chat with financial data” platform.</p></li></ul><div><p><span>The AI infra darlings LlamaIndex and Langchain also took this path of focus when it comes to their enterprise-oriented products. LlamaIndex is focusing on managed document parsing / OCR, whereas Langchain is focusing on LLMOps and agent building solutions. My guess is that both are working on narrowing their focus even further, since even selling a managed document parsing service is a huge scope for a seed-stage startup, given that Google and AWS already have existing vertical text extraction services. It’s not easy.</span></p></div><div><p><span>Narrowing the scope and going vertical is a typical response for AI infra startups - but I argue that these pivots rarely work out and cause new set of problems. Most importantly, these vertical pivots underestimate the importance of deep domain expertise once you go vertical, which many AI infra founders lack. Accumulating domain knowledge is time consuming. Also, your product may need to be heavily customized for the unique needs of the vertical, which means lower margins.</span></p></div><p><span>Not to mention, these application layer ecosystems have even worse competition (e.g. VCs’ LegalTech ecosystem maps ran out of space to put new logos long time ago). There’s not just the other AI startup competition, but competition from the legacy software companies. Pivoting to a vertical does not suddenly get rid of your competitors - you will just have new ones in that vertical who have been there before you. For example, legal tech industry has existed for ages, and many Legal AI companies are now competing with </span><strong>the legacy legal tech providers plus system integrators.</strong></p><div><p><span>So what’s the solution for AI infra startups? Should we all hope to be acquihired, or is it possible for startups to also stay independent for longer and find product market fit?</span></p><p><span>Here’s a somewhat anti-climatic answer, but the solution for startups goes back to the fundamentals: </span><strong>think deeply about how to be different from the incumbents.</strong><span> Here are four ways to iterate from here:</span></p></div><ul><li><p><strong>Narrow down the scope even further:</strong><span> focus on a very tiny segment of enterprise customers, as opposed to serving all customers. Don’t build all the integrations. Be a managed RAG service for customers using Salesforce with on-prem VMWare, as opposed to a general purpose RAG service. Startups don’t have the resources to solve for every environment, at least initially.</span></p></li><li><p><strong>Focus on just one workload:</strong><span> startups shouldn’t try to solve for too many workloads. Do one thing really well. Don’t try to be a platform for finetuning any LLM - there’s already too many of those. Instead, try to be the best platform for finetuning Tagalog models. The catch: the TAM might be too small.</span></p></li><li><p><strong>Raise more VC money than you think you need:</strong><span> long runways are non-negotiable. It can take a while for enterprises to be receptive to buying startup AI infra solutions, if ever. Be prepared for the worst case scenario.</span></p></li><li><p><strong>Or, don’t raise any VC money at all:</strong><span> raising VC money kind of forces you to orient business strategy around selling to the enterprise - which might be not something you can or want to do. You want the flexibility to work on more interesting and promising problems when they arise, given there’s constantly new changes in AI landscape.</span></p></li></ul><p><span>Lastly, AI startups should be open to being acquired by a larger player, even if it’s not a prestigious destination like OpenAI or Google. </span><strong>My view is that M&amp;A landscape for AI infrastructure sector will become worse, not better, over time.</strong></p><p><span>The acquisition market will become more “efficient” as the winners/losers emerge, and the workloads and enterprise needs become more clearly defined. Thus, in order to sell your startup at an “attractive” valuation, it needs to be marketed prior to the dust settles when the market is less efficient. Don’t wait for another 18 months to shop your startup, when all AI infra startups start running out of runway at the same time.</span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Bridges Don't Sink (253 pts)]]></title>
            <link>https://practical.engineering/blog/2024/7/2/why-bridges-dont-sink</link>
            <guid>40861520</guid>
            <pubDate>Tue, 02 Jul 2024 23:43:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://practical.engineering/blog/2024/7/2/why-bridges-dont-sink">https://practical.engineering/blog/2024/7/2/why-bridges-dont-sink</a>, See on <a href="https://news.ycombinator.com/item?id=40861520">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" id="item-668414c1a012432cf6a43a4b" data-layout-label="Post Body" data-type="item" data-updated-on="1719932256684">
  <p><em>[Note that this article is a transcript of the video embedded above.]</em></p><p>The essence of a bridge is not just that it goes over something, but that there’s clear space underneath for a river, railway, or road. Maybe this is already obvious to you, but bridges present a unique structural challenge. In a regular road, the forces are transferred directly into the ground. On a bridge, all those forces on the span get concentrated into the piers or abutments on either side. Because of that, bridge substructures are among the strongest engineered systems on the planet. And yet, bridge foundations are built in some of the least ideal places for heavy loading. Rivers and oceans have soft, mucky soils that can’t hold much weight. Plus, obviously, a lot of them are underwater.</p><p>What happens when you overload soil with a weight it can’t handle? In engineering-speak, it’s called a bearing failure, but it’s as simple as stepping in the mud. The foundation just sinks into the ground. But, what if you just keep loading it and causing it to sink deeper and deeper? Congratulations! You just invented one of the most widely used structural members on earth: the humble foundation pile. How do they work, and how can you install them underwater? I’m Grady, and this is Practical Engineering. Today we’re having piles of fun talking about deep foundations.</p><p>I did a video all about the different types of foundations used in engineering, but I didn’t go too deep into piles. A pile is a fairly simple structural member, just a long pole driven or drilled into the ground. But, behind that simplicity is a lot of terrifically complex engineering. Volume 1 of the Federal Highway Administration’s manual on the Design and Construction of Driven Pile Foundations is over 500 pages long. There are 11 pages of symbols, 2 pages of acronyms, and you don’t even get to the introduction until page 46. And just a little further than that, you get some history of driven piles. Namely that the history has been lost to time. Humans have been hammering sticks into the ground since way before we knew how to write about it. And that’s pretty much all a driven pile is.</p><p>The first piles were made from timber, and wood is still used all these years around the world. Timber piles are cheap, resilient to driving forces, and easy to install. But, wood rots, it has an upper limit on length from the size of the tree, and it’s not that strong compared to the alternatives. Concrete piles solve a lot of those problems. They come in a variety of sizes and shapes, and again, are widely used for deep foundations. One disadvantage of concrete piles is that they have to be pretty big to withstand the force required to drive them into ground. Some concrete piles can be upwards of 30 inches or 75 centimeters wide. It is hard to hit something that big hard enough to drive it downward into soil, and a lot of ground has to either get out of the way or compress in place to make room. Steel piles solve that problem since they can be a lot more slender. Pipe piles are just what they sound like, and the other major alternative is an H-pile. Your guess is as good as mine why the same steel shape is an <em>I</em>-beam but an <em>H</em>-pile. But, no matter the material, all driven piles are installed in basically the same way.&nbsp;</p><p>Newton’s third law applies to piles like everything else. To push one deep into the ground creates an equal and opposite reaction. You <em>would </em>need either an enormous weight to take advantage of gravity or some other strong structure attached to the ground to react against and develop the pushing force required to drive it downward. Instead of those two options, we usually just use a hammer. By dropping a comparatively small weight from a height, we convert the potential energy of the weight at that height into kinetic energy. The force required to stop the hammer as it falls gets transferred into the pile. Hopefully this is intuitive. It’s pretty hard to push a nail into wood, but it’s pretty easy to hammer it in... well, it’s <em>a little bit</em> easier to hammer it in. There are quite a few types of pile drivers, but most of them use a large hammer or vibratory head to create the forces required.</p><p>Maybe it goes without saying, but the main goal of a foundation is to not move. When you apply a load, you want it to stay put. Luckily, piles have two ways to do that (at least for vertical loads). The first is end-bearing. The end, or toe, of a pile can be driven down to a layer of strong soil or hard rock, making it able to withstand greater loads. But there’s not always a firm stratum at a reasonable depth below the ground. Quote-unquote “bedrock” is a simple idea, but in practice, geology is more complicated than that. Luckily, piles have a second type of resistance: skin friction, also known as shaft resistance. When you drive a pile, it compacts and densifies the surrounding soil, not only adding strength to the soil itself, but creating friction along the walls of the pile that hold it in place. The deeper you go, the more friction you get. Let me show you what I mean.</p><p>I have my own pipe pile in the backyard that I’ve marked with an arbitrary scale. When I drop the hammer at a prescribed height, the pile is driven a certain distance into the ground. Do this enough times, and eventually, you reach a point where the pile kind of stops moving with each successive hammer blow. In technical terms, the pile has reached refusal. I can graph the blow count required to drive the pile to each depth, and you get a pretty nice curve. It’s easy to see how it got stronger against vertical loads the deeper I drove it in. Toward the end, it barely moved with each hit. This is a really nice aspect of driven piles, you install them in a similar way to how they’ll be loaded by the final design. Of course, bridges and buildings don’t hammer on their foundations, but they do impose vertical loads. The tagline of the Pile Driving Contractors Association is “A Driven Pile is a Tested Pile” because, just by installing them, you’ve verified that they can withstand a certain amount of force. After all, you had to overcome that force to get them in the ground. And if you’re not seeing enough resistance, in most cases, you can just keep driving downward until you do!</p><p>But piles don’t just resist downward forces. Structures experience loads in other directions too. Buildings have horizontal, or lateral, loads from wind. Bridges see lateral loads from flowing water, and even ice or boats contacting the piers. Both can experience uplift forces that counteract gravity from floods due to buoyancy or strong winds. If you’ve ever hammered in a tent stake, you know that piles can withstand loading from all kinds of directions. And then there’s scour. The soil along a bridge might look like this right after the bridge is built, but after a few floods, it can look completely different. Engineers have to try and predict how the soil around a bridge will scour over time, from natural changes in the streambed and those created by the bridge itself. Then they make sure to design foundations that can accommodate those changes and stay strong over the long term. This is why bridge foundations sometimes look kind of funny. Loads transfer from the superstructure down into the piers. The piers sit on a pile cap that transfers and distributes loads into the piles themselves. Those piles can be vertical, but if the engineer is expecting serious lateral loads, some of the piles are often inclined, also called battered piles. Inclined piles take better advantage of the shaft resistance to make the foundation stronger against horizontal loads.</p><p>As important and beneficial as they are, driven piles have some limitations too. For one, they’re noisy and disruptive to install. Just last year, I had two friends on separate trips to Seattle who sent me a video of the exact same pile-driving operation. It’s good to have friends who know how much you like construction. But my point is, this type of construction is pretty much impossible to ignore. In dense urban areas, most people are just not willing to put up with the constant banging. Plus the vibrations from installing them can disrupt surrounding infrastructure. Pile driving is crude; in many cases, the piles aren’t designed to withstand the forces of the structure they’ll support but rather the forces they’ll have to experience during installation which are much higher. They can’t easily go through hard geological layers, cobbles, or boulders; they can wander off path, since you can’t really see where you’re going, and they can cause the ground to heave because you’re not removing any soil while you force them into the subsurface. The second major category of piles solves a lot of these problems.</p><p>And, wouldn’t you know it? There’s an FHWA manual that has all the juicy details - Drilled Shafts: Construction Procedures and Design Methods. This one a whopping 747 pages long. A drilled shaft is also exactly what it sounds like. The basic process is pretty simple. Drill a long hole into the ground. Place reinforcing steel in the hole. Then fill the whole thing with concrete. But, bridge piers are often, as you probably know, installed underwater. Pouring concrete underwater is a little tricky. Imagine trying to pour a smoothie at the bottom of a pool! Let me show you what I mean.</p><p>This is my garage-special bridge foundation simulator. It has transparent soil in the form of superabsorbent polymer beads… and you know we have to add some blue water too. You can probably imagine how easy it might be to drill a hole in this soil. It’s just going to collapse in on itself. We need a way to keep the hole open so the rebar and concrete can be installed. So, drilled shafts installed in soft soils or wet conditions usually rely on a casing to support the walls. Installing a casing usually happens while the hole is drilled, following the auger downward. I tried that myself, but I only have two hands, and it was pretty unwieldy. So, just for the sake of the demo, I’m advancing the casing into the soil ahead of time. Now I can drill out the soil to open the shaft. And now I’m realizing the limitations of my soil simulant. It was still pretty hard to do, even with the casing in place. It took a few tries, but I managed to get most of it out.</p><p>So now I have an open hole, but it’s still full of water. Even if your casing runs above the water surface, and you try to pump it out, you can still have water leaking in from the bottom. In ideal conditions, you can get a nice seal between the bottom of the casing and the soil, but even then, it’s pretty hard to keep water out of the hole, and luckily it doesn’t matter.</p><p>Instead of concrete, I’m using bentonite clay as a substitute. It’s got a similar density, and it’s perfect for this demo because you can push it through a small tube… if you get the proportions right. Ask me how I know. This is me pondering the life decisions that led up to me holding a gigantic syringe full of bentonite slurry in my garage. You can’t just drop this stuff through the water. It mixes and dilutes, just turning into a mess. Same is true for concrete. The ratio of water to cement in a concrete mix is essential to its strength and performance, so you can’t do anything that would add water to the mix. The trick is a little device called a tremie. Even though it has a funny name, it’s nothing more than a pipe that runs to the bottom of the hole. As long as you keep the end of the tremie below the surface of the concrete that you’re pumping in, or concrete simulant in my case, there’s no chance for it to mix with the water and dilute. I’m just pushing the clay into the casing with a big syringe, making sure to keep the end of the tube buried. Because concrete is a lot more dense than water, it just displaces it upward, out of the hole.&nbsp;</p><p>In underwater installations, the casing is often left in place. One advantage is that you can build a floating pile cap. Instead of building a big cofferdam and drying out the work area to construct a big concrete structure, sometimes you can raise the pile cap into or above the water surface, reducing the complexity of its construction. These “high rise” pile caps are used a lot in offshore wind turbines. But, not all casings are permanent.</p><p>In some situations, it’s possible to pull the casing once the hole is full of concrete, saving the sometimes enormous cost of each gigantic steel tube. I tried to show this in my demo. It’s not beautiful, but it did work. Again, the concrete is dense, so the pressure it exerts on the walls of the hole is enough to keep the soil from collapsing. And because drilled shafts can be much larger than driven piles, sometimes you don’t even need a group of them. Lots of structures, including wind turbines, highway signs, and more, are built on mono-pile foundations. Just a single drilled shaft deep in the ground, eliminating the need for a pile cap altogether. Another interesting aspect of drilled shafts is that you can ream out the bottom, creating an enlarged base that increases the surface area at the toe. This helps reduce a pile’s tendency to sink, and it can help with uplift resistance too.</p><p>Driven piles and drilled shafts are far from the only types of deep foundation systems. There are tons of variations on the idea that have been developed over the years to solve specific challenges: Continuous flight auger piles do the drilling and concreting in essentially one step, using a hollow-stem auger to fill the hole as it’s removed. Then reinforcement is lowered into the wet concrete. You can fill a hole with compacted aggregate instead of concrete, called a stone column or tradename Geopier if you’re only worried about compressive loads. Helical or screw piles twist into the ground, instead of being hammered, reducing vibrations and disturbance. Micropiles are like tiny drilled shafts used when there are access restrictions or geologic constraints. And of course, there are sheet piles that aren’t really used for foundations but are driven piles meant to create a wall or barrier. Let me know if I forgot to mention your favorite flavor of pile.</p><p>Even though they’re usually much stronger than shallow foundations, piles can and do fail. We’ve talked about San Francisco’s famous Millennium Tower in a previous video. That’s a skyscraper on a pile foundation that sank into the ground, causing the building to tilt. It seems like they mostly have it fixed now, but it’s still in the news every so often, so only time will tell. In 2004, a bridge pier on the Lee Roy Selmon Expressway in Tampa, Florida sank 11 feet (more than 3 meters) while it was still under construction because of the complicated geology. It cost 90 million dollars to fix and delayed the project’s completion by a year. These case studies highlight the complexity of geotechnical engineering when we ask the ground to hold up heavier and heavier loads. The science and technology that goes into designing deep foundations are enough to spend an entire career studying, but hopefully, this video gives you a little insight into how they work.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Illustrated Transformer (2018) (149 pts)]]></title>
            <link>https://jalammar.github.io/illustrated-transformer/</link>
            <guid>40861148</guid>
            <pubDate>Tue, 02 Jul 2024 22:42:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a>, See on <a href="https://news.ycombinator.com/item?id=40861148">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><span>Discussions:
<a href="https://news.ycombinator.com/item?id=18351674">Hacker News (65 points, 4 comments)</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/8uh2yz/p_the_illustrated_transformer_a_visual_look_at/">Reddit r/MachineLearning (29 points, 3 comments)</a>
</span>
<br>
<span>Translations: <a href="https://www.mundhor.site/post/post14">Arabic</a>, <a href="https://blog.csdn.net/yujianmin1990/article/details/85221271">Chinese (Simplified) 1</a>, <a href="https://blog.csdn.net/qq_36667170/article/details/124359818">Chinese (Simplified) 2</a>, <a href="https://a-coles.github.io/2020/11/15/transformer-illustre.html">French 1</a>, <a href="https://lbourdois.github.io/blog/nlp/Transformer/">French 2</a>, <a href="https://medium.com/@val.mannucci/il-transformer-illustrato-it-37a78e3e2348">Italian</a>, <a href="https://tips-memo.com/translation-jayalmmar-transformer">Japanese</a>, <a href="https://nlpinkorean.github.io/illustrated-transformer/">Korean</a>, <a href="http://dml.qom.ac.ir/2022/05/17/illustrated-transformer/">Persian</a>, <a href="https://habr.com/ru/post/486358/">Russian</a>, <a href="https://www.ibidemgroup.com/edu/transformer-ilustrado-jay-alammar/">Spanish 1</a>, <a href="https://hackernoon.com/el-transformador-ilustrado-una-traduccion-al-espanol-0y73wwp">Spanish 2</a>, <a href="https://trituenhantao.io/tin-tuc/minh-hoa-transformer/">Vietnamese</a></span>
<br>
<span>Watch: MIT’s <a href="https://youtu.be/53YvP6gdD7U?t=432">Deep Learning State of the Art</a> lecture referencing this post</span>
<br>
<span>Featured in courses at <a href="https://web.stanford.edu/class/cs224n/">Stanford</a>, <a href="https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/transformers">Harvard</a>, <a href="https://ocw.mit.edu/courses/6-s897-machine-learning-for-healthcare-spring-2019/d39a6eed387ee90b1f72a01949c35c7b_MIT6_S897S19_lec8.pdf">MIT</a>, <a href="https://www.cs.princeton.edu/courses/archive/fall22/cos597G/">Princeton</a>, <a href="https://www.cs.cmu.edu/~leili/course/mldl22w/14-Transformer.pdf">CMU</a> and others</span></p>

<p>In the <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">previous post, we looked at Attention</a> – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at <strong>The Transformer</strong> – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their <a href="https://cloud.google.com/tpu/">Cloud TPU</a> offering. So let’s try to break the model apart and look at how it functions.</p>

<p>The Transformer was proposed in the paper <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a>. A TensorFlow implementation of it is available as a part of the <a href="https://github.com/tensorflow/tensor2tensor">Tensor2Tensor</a> package. Harvard’s NLP group created a <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">guide annotating the paper with PyTorch implementation</a>. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.</p>

<p><strong>2020 Update</strong>: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:</p>

<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/-QH8fRhqFHM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p>
<h2 id="a-high-level-look">A High-Level Look</h2>
<p>Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.</p>

<p><img src="https://jalammar.github.io/images/t/the_transformer_3.png">
</p>

<!--more-->

<p>Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.</p>

<p><img src="https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png">
</p>

<p>The encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.</p>

<p><img src="https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png">
</p>

<p>The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:</p>

<p><img src="https://jalammar.github.io/images/t/Transformer_encoder.png">
</p>

<p>The encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.</p>

<p>The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.</p>

<p>The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">seq2seq models</a>).</p>

<p><img src="https://jalammar.github.io/images/t/Transformer_decoder.png">
</p>

<h2 id="bringing-the-tensors-into-the-picture">Bringing The Tensors Into The Picture</h2>

<p>Now that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.</p>

<p>As is the case in NLP applications in general, we begin by turning each input word into a vector using an <a href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca">embedding algorithm</a>.</p>



<p><img src="https://jalammar.github.io/images/t/embeddings.png">
  <br>
  Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes.
</p>

<p>The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.</p>

<p>After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.</p>

<p><img src="https://jalammar.github.io/images/t/encoder_with_tensors.png">
  <br>

</p>

<p>Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.</p>

<p>Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.</p>

<h2 id="now-were-encoding">Now We’re Encoding!</h2>

<p>As we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.</p>

<p><img src="https://jalammar.github.io/images/t/encoder_with_tensors_2.png">
  <br>
  The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.
</p>

<h2 id="self-attention-at-a-high-level">Self-Attention at a High Level</h2>
<p>Don’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.</p>

<p>Say the following sentence is an input sentence we want to translate:</p>

<p>”<code>The animal didn't cross the street because it was too tired</code>”</p>

<p>What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.</p>

<p>When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.</p>

<p>As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.</p>

<p>If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization.png">
  <br>
  As we are encoding the word "it" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on "The Animal", and baked a part of its representation into the encoding of "it".
</p>

<p>Be sure to check out the <a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb">Tensor2Tensor notebook</a> where you can load a Transformer model, and examine it using this interactive visualization.</p>

<h2 id="self-attention-in-detail">Self-Attention in Detail</h2>
<p>Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented – using matrices.</p>

<p>The <strong>first step</strong> in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.</p>

<p>Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_self_attention_vectors.png">
  <br>
  Multiplying <span>x1</span> by the <span>WQ</span> weight matrix produces <span>q1</span>, the "query" vector associated with that word. We end up creating a "query", a "key", and a "value" projection of each word in the input sentence.
</p>



<div><p>What are the “query”, “key”, and “value” vectors?
</p><p>

They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.</p></div>

<p>The <strong>second step</strong> in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.</p>

<p>The score is calculated by taking the dot product of the <span>query vector</span> with the <span>key vector</span> of the respective word we’re scoring. So if we’re processing the self-attention for the word in position <span>#1</span>, the first score would be the dot product of <span>q1</span> and <span>k1</span>. The second score would be the dot product of <span>q1</span> and <span>k2</span>.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_self_attention_score.png">
  <br>

</p>



<p>The <strong>third and fourth steps</strong> are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.</p>



<p><img src="https://jalammar.github.io/images/t/self-attention_softmax.png">
  <br>

</p>

<p>This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.</p>



<p>The <strong>fifth step</strong> is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).</p>

<p>The <strong>sixth step</strong> is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).</p>



<p><img src="https://jalammar.github.io/images/t/self-attention-output.png">
  <br>
</p>

<p>That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.</p>

<h2 id="matrix-calculation-of-self-attention">Matrix Calculation of Self-Attention</h2>
<p><strong>The first step</strong> is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix <span>X</span>, and multiplying it by the weight matrices we’ve trained (<span>WQ</span>, <span>WK</span>, <span>WV</span>).</p>

<p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png">
  <br>
  Every row in the <span>X</span> matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)
</p>



<p><strong>Finally</strong>, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.</p>

<p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png">
  <br>
  The self-attention calculation in matrix form
</p>





<h2 id="the-beast-with-many-heads">The Beast With Many Heads</h2>

<p>The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:</p>

<ol>
  <li>
    <p>It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.</p>
  </li>
  <li>
    <p>It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.</p>
  </li>
</ol>

<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png">
   <br>
   With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.
 </p>

<p><br>
If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices</p>

<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_z.png">
  <br>

</p>



<p>This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.</p>

<p>How do we do that? We concat the matrices then multiply them by an additional weights matrix WO.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png">
  <br>

</p>

<p>That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place</p>



<p><img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png">
  <br>

</p>



<p>Now that we have touched upon attention heads, let’s revisit our example from before to see where the different attention heads are focusing as we encode the word “it” in our example sentence:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png">
  <br>
  As we encode the word "it", one attention head is focusing most on "the animal", while another is focusing on "tired" -- in a sense, the model's representation of the word "it" bakes in some of the representation of both "animal" and "tired".
</p>



<p>If we add all the attention heads to the picture, however, things can be harder to interpret:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png">
  <br>
</p>

<h2 id="representing-the-order-of-the-sequence-using-positional-encoding">Representing The Order of The Sequence Using Positional Encoding</h2>
<p>One thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.</p>

<p>To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png">
  <br>
  To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.
</p>


<p>If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_example.png">
  <br>
  A real example of positional encoding with a toy embedding size of 4
</p>



<p>What might this pattern look like?</p>

<p>In the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png">
  <br>
  A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.
</p>

<p>The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in <a href="https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py"><code>get_timing_signal_1d()</code></a>. This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).</p>

<p><strong>July 2020 Update:</strong> 
The positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. <a href="https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb">Here’s the code to generate it</a>:</p>

<p><img src="https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png">
  <br>
</p>

<h2 id="the-residuals">The Residuals</h2>
<p>One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a <a href="https://arxiv.org/abs/1607.06450">layer-normalization</a> step.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png">
  <br>
</p>

<p>If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png">
  <br>
</p>

<p>This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png">
  <br>
</p>

<h2 id="the-decoder-side">The Decoder Side</h2>
<p>Now that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.</p>

<p>The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_decoding_1.gif">
  <br>
  After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).
</p>

<p>The following steps repeat the process until a special <end of="" sentence=""> symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.</end></p>

<p><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif">
  <br>

</p>

<p>The self attention layers in the decoder operate in a slightly different way than the one in the encoder:</p>

<p>In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to <code>-inf</code>) before the softmax step in the self-attention calculation.</p>

<p>The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.</p>

<h2 id="the-final-linear-and-softmax-layer">The Final Linear and Softmax Layer</h2>

<p>The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.</p>

<p>The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.</p>

<p>Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.</p>

<p>The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png">
  <br>
  This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.
</p>



<h2 id="recap-of-training">Recap Of Training</h2>
<p>Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.</p>

<p>During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.</p>

<p>To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “&lt;eos&gt;” (short for ‘end of sentence’)).</p>

<p><img src="https://jalammar.github.io/images/t/vocabulary.png">
   <br>
   The output vocabulary of our model is created in the preprocessing phase before we even begin training.
 </p>

<p>Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:</p>

<p><img src="https://jalammar.github.io/images/t/one-hot-vocabulary-example.png">
  <br>
  Example: one-hot encoding of our output vocabulary
</p>

<p>Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.</p>

<h2 id="the-loss-function">The Loss Function</h2>
<p>Say we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.</p>

<p>What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_logits_output_and_label.png">
  <br>
  Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output.
</p>



<p>How do you compare two probability distributions? We simply subtract one from the other. For more details, look at  <a href="https://colah.github.io/posts/2015-09-Visual-Information/">cross-entropy</a> and <a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">Kullback–Leibler divergence</a>.</p>

<p>But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:</p>

<ul>
  <li>Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)</li>
  <li>The first probability distribution has the highest probability at the cell associated with the word “i”</li>
  <li>The second probability distribution has the highest probability at the cell associated with the word “am”</li>
  <li>And so on, until the fifth output distribution indicates ‘<code>&lt;end of sentence&gt;</code>’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.</li>
</ul>

<p><img src="https://jalammar.github.io/images/t/output_target_probability_distributions.png">
   <br>
   The targeted probability distributions we'll train our model against in the training example for one sample sentence.
 </p>



<p>After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:</p>

<p><img src="https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png">
    <br>
    Hopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: <a href="https://www.youtube.com/watch?v=TIgfjmp-4BA">cross validation</a>). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process.
</p>

<p>Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with.</p>

<h2 id="go-forth-and-transform">Go Forth And Transform</h2>

<p>I hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:</p>

<ul>
  <li>Read the <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> paper, the Transformer blog post (<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer: A Novel Neural Network Architecture for Language Understanding</a>), and the <a href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html">Tensor2Tensor announcement</a>.</li>
  <li>Watch <a href="https://www.youtube.com/watch?v=rBCqOTEfxvg">Łukasz Kaiser’s talk</a> walking through the model and its details</li>
  <li>Play with the <a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb">Jupyter Notebook provided as part of the Tensor2Tensor repo</a></li>
  <li>Explore the <a href="https://github.com/tensorflow/tensor2tensor">Tensor2Tensor repo</a>.</li>
</ul>

<p>Follow-up works:</p>

<ul>
  <li><a href="https://arxiv.org/abs/1706.03059">Depthwise Separable Convolutions for Neural Machine Translation</a></li>
  <li><a href="https://arxiv.org/abs/1706.05137">One Model To Learn Them All</a></li>
  <li><a href="https://arxiv.org/abs/1801.09797">Discrete Autoencoders for Sequence Models</a></li>
  <li><a href="https://arxiv.org/abs/1801.10198">Generating Wikipedia by Summarizing Long Sequences</a></li>
  <li><a href="https://arxiv.org/abs/1802.05751">Image Transformer</a></li>
  <li><a href="https://arxiv.org/abs/1804.00247">Training Tips for the Transformer Model</a></li>
  <li><a href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations</a></li>
  <li><a href="https://arxiv.org/abs/1803.03382">Fast Decoding in Sequence Models using Discrete Latent Variables</a></li>
  <li><a href="https://arxiv.org/abs/1804.04235">Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</a></li>
</ul>

<h2 id="acknowledgements">Acknowledgements</h2>
<p>Thanks to <a href="https://twitter.com/ilblackdragon">Illia Polosukhin</a>, <a href="http://jakob.uszkoreit.net/">Jakob Uszkoreit</a>, <a href="https://www.linkedin.com/in/llion-jones-9ab3064b">Llion Jones </a>, <a href="https://ai.google/research/people/LukaszKaiser">Lukasz Kaiser</a>, <a href="https://twitter.com/nikiparmar09">Niki Parmar</a>, and <a href="https://dblp.org/pers/hd/s/Shazeer:Noam">Noam Shazeer</a> for providing feedback on earlier versions of this post.</p>

<p>Please hit me up on <a href="https://twitter.com/JayAlammar">Twitter</a> for any corrections or feedback.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Engine Sound Simulator (124 pts)]]></title>
            <link>https://markeasting.github.io/engine/</link>
            <guid>40861079</guid>
            <pubDate>Tue, 02 Jul 2024 22:32:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://markeasting.github.io/engine/">https://markeasting.github.io/engine/</a>, See on <a href="https://news.ycombinator.com/item?id=40861079">Hacker News</a></p>
<div id="readability-page-1" class="page">

    <h2>Engine sound simulator</h2>

    <p><i>Please click once to enable sounds :)</i></p>

    <h2>Controls:</h2>
    
    <p>Space: LETS GOOO</p>
    <p>Arrow up/down: change gear</p>
    <p>Numbers: change gear</p>
    <p>B: slow down</p>

    
  


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brazil data regulator bans Meta from mining data to train AI models (137 pts)]]></title>
            <link>https://apnews.com/article/brazil-tech-meta-privacy-data-93e00b2e0e26f7cc98795dd052aea8e1</link>
            <guid>40861057</guid>
            <pubDate>Tue, 02 Jul 2024 22:29:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/brazil-tech-meta-privacy-data-93e00b2e0e26f7cc98795dd052aea8e1">https://apnews.com/article/brazil-tech-meta-privacy-data-93e00b2e0e26f7cc98795dd052aea8e1</a>, See on <a href="https://news.ycombinator.com/item?id=40861057">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>RIO DE JANEIRO (AP) — Brazil’s national data protection authority determined on Tuesday that Meta, the parent company of Instagram and Facebook, cannot use data originating in the country to train its artificial intelligence. </p><p>Meta’s updated privacy policy enables the company to feed people’s public posts into its AI systems. That practice will not be permitted in Brazil, however.</p><p>The decision stems from “the imminent risk of serious and irreparable or difficult-to-repair damage to the fundamental rights of the affected data subjects,” the agency said in the nation’s official gazette. </p><p>Brazil is one of Meta’s biggest markets. Facebook alone has around 102 million active users in the country, the agency said in a statement. The nation has a population of 203 million, according to the country’s 2022 census.</p><p>A spokesperson for Meta said in a statement the company is “disappointed” and insists its method “complies with privacy laws and regulations in Brazil.”</p>
    

<p>“This is a step backwards for innovation, competition in AI development and further delays bringing the benefits of AI to people in Brazil,” the spokesperson added.</p>



<p>The social media company has also encountered resistance to its privacy policy update in Europe, where it recently put on hold its plans to start feeding people’s public posts into training AI systems — which was supposed to start last week.</p><p>In the U.S., where there’s no national law protecting online privacy, such training is already happening.</p>
    
<p>Meta said on its Brazilian blog in May that it could “use information that people have shared publicly about Meta’s products and services for some of our generative AI features,” which could include “public posts or photos and their captions.”</p><p>Refusing to partake is possible, Meta said in that statement. Despite that option, there are “excessive and unjustified obstacles to accessing the information and exercising” the right to opt out, the agency said in a statement. </p>
    

<p>Meta did not provide sufficient information to allow people to be aware of the possible consequences of using their personal data for the development of generative AI, it added.</p><p>Meta isn’t the only company that has sought to train its AI systems on data from Brazilians.</p><p>Human Rights Watch released a report last month that found that personal photos of identifiable Brazilian children sourced from a large database of online images — pulled from parent blogs, the websites of professional event photographers and video-sharing sites such as YouTube — were being used to create AI image-generator tools without families’ knowledge. In some cases, those tools have been used create AI-generated nude imagery.</p><p>Hye Jung Han, a Brazil-based researcher for the rights group, said in an email Tuesday that the regulator’s action “helps to protect children from worrying that their personal data, shared with friends and family on Meta’s platforms, might be used to inflict harm back on them in ways that are impossible to anticipate or guard against.”</p><p>But the decision regarding Meta will “very likely” encourage other companies to refrain from being transparent in the use of data in the future, said Ronaldo Lemos, of the Institute of Technology and Society of Rio de Janeiro, a think-tank. </p>
    

<p>“Meta was severely punished for being the only one among the Big Tech companies to clearly and in advance notify in its privacy policy that it would use data from its platforms to train artificial intelligence,” he said. </p><p>Compliance must be demonstrated by the company within five working days from the notification of the decision, and the agency established a daily fine of 50,000 reais ($8,820) for failure to do so.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[All I want for Christmas is a negative leap second (177 pts)]]></title>
            <link>https://qntm.org/leap</link>
            <guid>40860831</guid>
            <pubDate>Tue, 02 Jul 2024 22:01:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qntm.org/leap">https://qntm.org/leap</a>, See on <a href="https://news.ycombinator.com/item?id=40860831">Hacker News</a></p>
Couldn't get https://qntm.org/leap: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Python Modern Practices (105 pts)]]></title>
            <link>https://www.stuartellis.name/articles/python-modern-practices/</link>
            <guid>40860803</guid>
            <pubDate>Tue, 02 Jul 2024 21:56:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.stuartellis.name/articles/python-modern-practices/">https://www.stuartellis.name/articles/python-modern-practices/</a>, See on <a href="https://news.ycombinator.com/item?id=40860803">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><a href="https://www.python.org/" target="_blank" rel="noreferrer">Python</a> has a long history, and it has evolved over time. This article describes some agreed modern best practices.</p>
<h2 id="using-python">Using Python <span><a href="#using-python" aria-label="Anchor">#</a></span></h2><h3 id="install-python-with-tools-that-support-multiple-versions">Install Python With Tools That Support Multiple Versions <span><a href="#install-python-with-tools-that-support-multiple-versions" aria-label="Anchor">#</a></span></h3><p>Use a tool like <a href="https://mise.jdx.dev/" target="_blank" rel="noreferrer">mise</a> or <a href="https://github.com/pyenv/pyenv" target="_blank" rel="noreferrer">pyenv</a> to install Python on your development systems, so that you can switch between different versions of Python for your projects. This enables you to upgrade each project to a new version of Python without interfering with other tools and projects that use Python.</p>
<p>Alternatively, consider using <a href="https://containers.dev/" target="_blank" rel="noreferrer">Development Containers</a>, which enable you to define an isolated environment for a software project. This also allows you to use a separate version of Python for each project.</p>
<p>Ensure that the tool compiles Python, rather than downloading <a href="https://gregoryszorc.com/docs/python-build-standalone/main/" target="_blank" rel="noreferrer">standalone builds</a>. The standlone builds are modified versions of Python that are maintained by a third-party. Both the pyenv tool and the <a href="https://github.com/devcontainers/features/blob/main/src/python/README.md" target="_blank" rel="noreferrer">Visual Studio Code Dev Container feature</a> automatically compile Python, but you must <a href="https://mise.jdx.dev/lang/python.html#precompiled-python-binaries" target="_blank" rel="noreferrer">change the mise configuration</a> to use compilation. <a href="https://pdm-project.org/" target="_blank" rel="noreferrer">PDM</a> and <a href="https://hatch.pypa.io/" target="_blank" rel="noreferrer">Hatch</a> always download standalone builds when you use them to set up versions of Python.</p>
<p>If your operating system includes a Python installation, avoid using it. This Python installation is for operating system tools. It is likely to use an older version of Python, and may not include all of the standard features. An operating system copy of Python should be <a href="https://packaging.python.org/en/latest/specifications/externally-managed-environments/#externally-managed-environments" target="_blank" rel="noreferrer">marked</a> to prevent you from installing packages into it, but not all operating systems set the marker.</p>
<h3 id="use-the-most-recent-version-of-python-that-you-can">Use The Most Recent Version of Python That You Can <span><a href="#use-the-most-recent-version-of-python-that-you-can" aria-label="Anchor">#</a></span></h3><p>For new projects, choose the most recent stable version of Python 3. This ensures that you have the latest security fixes, as well as the fastest performance.</p>
<p>Upgrade your projects as new Python versions are released. The Python development team usually support each version for five years, but some Python libraries may only support each version of Python for a shorter period of time. If you use tools that support multiple versions of Python and automated testing, you can test your projects on new Python versions with little risk.</p>
<p><span>
    <span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M506.3 417l-213.3-364c-16.33-28-57.54-28-73.98 0l-213.2 364C-10.59 444.9 9.849 480 42.74 480h426.6C502.1 480 522.6 445 506.3 417zM232 168c0-13.25 10.75-24 24-24S280 154.8 280 168v128c0 13.25-10.75 24-23.1 24S232 309.3 232 296V168zM256 416c-17.36 0-31.44-14.08-31.44-31.44c0-17.36 14.07-31.44 31.44-31.44s31.44 14.08 31.44 31.44C287.4 401.9 273.4 416 256 416z"></path></svg>
</span>
  </span>
  <span><em>Avoid using Python 2.</em> It is not supported by the Python development team or by the developers of most popular Python libraries.</span>
</p>

<h3 id="use-pipx-to-run-developer-applications">Use pipx To Run Developer Applications <span><a href="#use-pipx-to-run-developer-applications" aria-label="Anchor">#</a></span></h3><p>Use <a href="https://pipx.pypa.io/" target="_blank" rel="noreferrer">pipx</a> to run Python applications on development systems, rather than installing the applications with <em>pip</em> or another method. This ensures that each application has the correct libraries, because <em>pipx</em> automatically puts the libraries for each application into a separate <a href="https://docs.python.org/3/tutorial/venv.html" target="_blank" rel="noreferrer">Python virtual environment</a>.</p>
<p>Consider using the <a href="https://pipx.pypa.io/stable/#walkthrough-running-an-application-in-a-temporary-virtual-environment" target="_blank" rel="noreferrer">pipx run</a> command, rather than <em>pipx install</em>. The <em>pipx run</em> command downloads and runs the application without installing it. Each application is cached for several days after the first download, which means that <em>pipx run</em> may not be slower than running a manually installed application.</p>
<p>The Python Packaging Authority maintain <em>pipx</em>, but it is not included with Python. To install <em>pipx</em>, run this command:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>python3 -m pip install --user pipx
</span></span></code></pre></div><p>This enables you to use any Python command-line application with <em>pipx</em>. For example, this command downloads and runs the latest version of <a href="https://github.com/aristocratos/bpytop" target="_blank" rel="noreferrer">bpytop</a>, a system monitoring tool:</p>
<blockquote>
<p><a href="https://peps.python.org/pep-0668/#guide-users-towards-virtual-environments" target="_blank" rel="noreferrer">PEP 668 - Marking Python base environments as “externally managed”</a> recommends that users install Python applications with pipx.</p>
</blockquote>
<h2 id="developing-python-projects">Developing Python Projects <span><a href="#developing-python-projects" aria-label="Anchor">#</a></span></h2><h3 id="avoid-using-poetry">Avoid Using Poetry <span><a href="#avoid-using-poetry" aria-label="Anchor">#</a></span></h3><p>Avoid using <a href="https://python-poetry.org/" target="_blank" rel="noreferrer">Poetry</a> for new projects. Poetry predates many standards for Python tooling. This means that it uses non-standard implementations of key features, such as the dependency resolver and configuration formats in <em>pyproject.toml</em> files.</p>
<p>If you would like to use a similar tool to develop your applications, consider using <a href="https://pdm-project.org/" target="_blank" rel="noreferrer">PDM</a>. <a href="https://hatch.pypa.io/" target="_blank" rel="noreferrer">Hatch</a> is another alternative to Poetry, but it is most useful for developing Python libraries. Both of these tools follow modern standards, which avoids compatibility issues.</p>
<h3 id="use-a-pyprojecttoml-file">Use a pyproject.toml File <span><a href="#use-a-pyprojecttoml-file" aria-label="Anchor">#</a></span></h3><p>Create a <em>pyproject.toml</em> file in the root directory of each Python project. Use this file as the central place to store configuration information about the project and the tools that it uses. The <a href="https://www.pyopensci.org/python-package-guide/package-structure-code/pyproject-toml-python-package-metadata.html" target="_blank" rel="noreferrer">pyOpenSci project documentation on pyproject.toml</a> provides an introduction to the file format.</p>
<p>Modern Python tools use the <em>pyproject.toml</em> file to store configuration. Some tools support <em>pyproject.toml</em>, but do not use it by default. Python project management tools like <a href="https://pdm-project.org/" target="_blank" rel="noreferrer">PDM</a> and <a href="https://hatch.pypa.io/" target="_blank" rel="noreferrer">Hatch</a> automatically create and use a <em>pyproject.toml</em> file.</p>
<blockquote>
<p>The various features of <em>pyproject.toml</em> files are defined these PEPs: <a href="https://peps.python.org/pep-0517/" target="_blank" rel="noreferrer">PEP 517</a>, <a href="https://peps.python.org/pep-0518/" target="_blank" rel="noreferrer">PEP 518</a>, <a href="https://peps.python.org/pep-0621/" target="_blank" rel="noreferrer">PEP 621</a> and <a href="https://peps.python.org/pep-0660/" target="_blank" rel="noreferrer">PEP 660</a>.</p>
</blockquote>
<h3 id="create-a-directory-structure-that-uses-the-src-layout">Create a Directory Structure That Uses the src Layout <span><a href="#create-a-directory-structure-that-uses-the-src-layout" aria-label="Anchor">#</a></span></h3><p>Python itself does not require a specific directory structure for your projects. The Python packaging documentation describes two popular directory structures: <a href="https://packaging.python.org/en/latest/discussions/src-layout-vs-flat-layout/" target="_blank" rel="noreferrer">the src layout and the flat layout</a>.
The <a href="https://www.pyopensci.org/python-package-guide/package-structure-code/python-package-structure.html" target="_blank" rel="noreferrer">pyOpenSci project documentation on directory structures</a> explains the practical differences between the two.</p>
<p>For modern Python projects, use the src layout. This requires you to use <a href="https://setuptools.pypa.io/en/latest/userguide/development_mode.html" target="_blank" rel="noreferrer">editable installs</a> of the packages in your project, but tools like <a href="https://pdm-project.org/" target="_blank" rel="noreferrer">PDM</a> and <a href="https://hatch.pypa.io/" target="_blank" rel="noreferrer">Hatch</a> will handle this for you.</p>
<h3 id="use-virtual-environments-for-development">Use Virtual Environments for Development <span><a href="#use-virtual-environments-for-development" aria-label="Anchor">#</a></span></h3><p>The <a href="https://docs.python.org/3/tutorial/venv.html" target="_blank" rel="noreferrer">virtual environments</a> feature enables you to define separate sets of packages for each Python project, so that the packages for a project do not conflict with any other Python packages on the system. Always use Python virtual environments for your projects.</p>
<p>Several tools automate creating and switching between virtual environments. The <a href="https://mise.jdx.dev/" target="_blank" rel="noreferrer">mise</a> version manager includes <a href="https://mise.jdx.dev/lang/python.html#automatic-virtualenv-activation" target="_blank" rel="noreferrer">support for virtual environments</a>. The <a href="https://github.com/pyenv/pyenv" target="_blank" rel="noreferrer">pyenv</a> version manager supports virtual environments with the <a href="https://github.com/pyenv/pyenv-virtualenv" target="_blank" rel="noreferrer">virtualenv plugin</a>. If you use a tool like <a href="https://pdm-project.org/" target="_blank" rel="noreferrer">PDM</a> or <a href="https://hatch.pypa.io/" target="_blank" rel="noreferrer">Hatch</a> to develop your projects, these also manage Python virtual environments for you.</p>
<p>You can set up and use virtual environments with <em>venv</em>, which is part of the Python standard library, but this is a manual process.</p>
<h3 id="use-requirementstxt-files-to-install-packages-into-environments">Use requirements.txt Files to Install Packages Into Environments <span><a href="#use-requirementstxt-files-to-install-packages-into-environments" aria-label="Anchor">#</a></span></h3><p>Avoid using <em>pip</em> commands to install packages into virtual environments. If you use <a href="https://pdm-project.org/" target="_blank" rel="noreferrer">PDM</a> or <a href="https://hatch.pypa.io/" target="_blank" rel="noreferrer">Hatch</a>, they manage packages in development and test environments. For other cases, create a <em>requirements.txt</em> file that specifies all of the packages that are required in the environment.</p>
<p>You can create <em>requirements.txt</em> files with whichever tool is appropriate. For example, PDM includes <a href="https://pdm-project.org/en/stable/usage/lockfile/#export-locked-packages-to-alternative-formats" target="_blank" rel="noreferrer">an export feature</a> that creates <em>requirements.txt</em> files. If you do not already have a tool to create <em>requirements.txt</em> files, use the <em>pip-compile</em> utility that is provided by <a href="https://github.com/jazzband/pip-tools/" target="_blank" rel="noreferrer">pip-tools</a>.</p>
<p>You can then use the <em>pip-sync</em> utility in <a href="https://github.com/jazzband/pip-tools/" target="_blank" rel="noreferrer">pip-tools</a> to add the packages that are specified in the <em>requirements.txt</em> file into a target virtual environment. The <em>pip-sync</em> utility ensures that the packages in a virtual environment match the list in the <em>requirements.txt</em> file.</p>
<p>If you need to install packages without using <em>pip-sync</em>, run <em>pip install</em> with a <em>requirements.txt</em> file. For example, these commands install the packages that are specified by the file <em>requirements-dev.txt</em> into the virtual environment <em>.venv</em>:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span><span>source</span> ./.venv/bin/activate
</span></span><span><span>python3 -m pip install -r requirements-dev.txt
</span></span></code></pre></div><blockquote>
<p>Ensure that your <em>requirements.txt</em> files include hashes for the packages. The <em>pip-compile</em> utility generates <em>requirements.txt</em> files with hashes if you specify the <em>generate-hashes</em> option.</p>
</blockquote>
<h3 id="format-your-code">Format Your Code <span><a href="#format-your-code" aria-label="Anchor">#</a></span></h3><p>Use a formatting tool with a plugin to your editor, so that your code is automatically formatted to a consistent style.</p>
<p><a href="https://black.readthedocs.io/en/stable/" target="_blank" rel="noreferrer">Black</a> is currently the most popular code formatting tool for Python, but consider using <a href="https://docs.astral.sh/ruff/" target="_blank" rel="noreferrer">Ruff</a>. Ruff provides both code formatting and quality checks for Python code.</p>
<p>Run the formatting tool with your CI system, so that it rejects any code that does not match the format for your project.</p>
<h3 id="use-a-code-linter">Use a Code Linter <span><a href="#use-a-code-linter" aria-label="Anchor">#</a></span></h3><p>Use a code linting tool with a plugin to your editor, so that your code is automatically checked for issues.</p>
<p><a href="https://flake8.pycqa.org/en/latest/" target="_blank" rel="noreferrer">flake8</a> is currently the most popular linter for Python, but consider using <a href="https://docs.astral.sh/ruff/" target="_blank" rel="noreferrer">Ruff</a>. Ruff includes the features of both flake8 itself and the most popular plugins for flake8.</p>
<p>Run the linting tool with your CI system, so that it rejects any code that does not meet the standards for your project.</p>
<h3 id="test-with-pytest">Test with pytest <span><a href="#test-with-pytest" aria-label="Anchor">#</a></span></h3><p>Use <a href="http://pytest.org/" target="_blank" rel="noreferrer">pytest</a> for testing. It has superseded <em>nose</em> as the most popular testing system for Python. Use the <em>unittest</em> module in the standard library for situations where you cannot add <em>pytest</em> to the project.</p>
<h3 id="package-your-applications">Package Your Applications <span><a href="#package-your-applications" aria-label="Anchor">#</a></span></h3><p>Use <em>wheel</em> packages for libraries, or for tools that are intended to be used with an existing installation of Python. If you publish your Python application as a <em>wheel</em>, other developers can use it with <em>pipx</em> and <em>pip-sync</em>. These packages cannot be used without a Python installation.</p>
<p>In most cases, you should package an application in a format that enables you to include your code, the dependencies and a copy of the required version of Python. This ensures that your code runs with the expected version of Python, and has the correct version of each dependency.</p>
<p>Use container images to package applications that provide a network service, such as a Web application. Use <a href="https://pyinstaller.org/" target="_blank" rel="noreferrer">PyInstaller</a> to publish desktop and command-line applications as a single executable file. Each container image and PyInstaller file includes a copy of Python, along with your code and the required dependencies.</p>
<h2 id="language-syntax">Language Syntax <span><a href="#language-syntax" aria-label="Anchor">#</a></span></h2><h3 id="use-type-hinting">Use Type Hinting <span><a href="#use-type-hinting" aria-label="Anchor">#</a></span></h3><p>Current versions of Python support type hinting. Consider using type hints in any critical application. If you develop a shared library, use type hints.</p>
<p>Once you add type hints, the <a href="http://www.mypy-lang.org/" target="_blank" rel="noreferrer">mypy</a> tool can check your code as you develop it. Code editors can also read type hints to display information about the code that you are working with.</p>
<p>If you use <a href="https://docs.pydantic.dev/" target="_blank" rel="noreferrer">Pydantic</a> in your application, it can work with type hints. Use the <a href="https://docs.pydantic.dev/latest/integrations/mypy/" target="_blank" rel="noreferrer">mypy plugin for Pydantic</a> to improve the integration between mypy and Pydantic.</p>
<blockquote>
<p><a href="https://peps.python.org/pep-0484/" target="_blank" rel="noreferrer">PEP 484 - Type Hints</a> and <a href="https://peps.python.org/pep-0526/" target="_blank" rel="noreferrer">PEP 526 – Syntax for Variable Annotations</a> define the notation for type hinting.</p>
</blockquote>
<h3 id="format-strings-with-f-strings">Format Strings with f-strings <span><a href="#format-strings-with-f-strings" aria-label="Anchor">#</a></span></h3><p>The new <a href="https://docs.python.org/3/reference/lexical_analysis.html#f-strings" target="_blank" rel="noreferrer">f-string</a> syntax is both more readable and has better performance than older methods. Use f-strings instead of <em>%</em> formatting, <em>str.format()</em> or <em>str.Template()</em>.</p>
<p>The older features for formatting strings will not be removed, to avoid breaking backward compatibility.</p>
<p>The f-strings feature was added in version 3.6 of Python. Alternate implementations of Python may include this specific feature, even when they do not support version 3.6 syntax.</p>
<blockquote>
<p><a href="https://www.python.org/dev/peps/pep-0498/" target="_blank" rel="noreferrer">PEP 498</a> explains f-strings in detail.</p>
</blockquote>
<h3 id="use-datetime-objects-with-time-zones">Use Datetime Objects with Time Zones <span><a href="#use-datetime-objects-with-time-zones" aria-label="Anchor">#</a></span></h3><p>Always use <em>datetime</em> objects that are <a href="https://docs.python.org/3/library/datetime.html?highlight=datetime#aware-and-naive-objects" target="_blank" rel="noreferrer">aware</a> of time zones. By default, Python creates <em>datetime</em> objects that do not include a time zone. The documentation refers to <em>datetime</em> objects without a time zone as <strong>naive</strong>.</p>
<p>Avoid using <em>date</em> objects, except where the time of day is completely irrelevant. The <em>date</em> objects are always <strong>naive</strong>, and do not include a time zone.</p>
<p>Use aware <em>datetime</em> objects with the UTC time zone for timestamps, logs and other internal features.</p>
<p>To get the current time and date in UTC as an aware <em>datetime</em> object, specify the UTC time zone with <em>now()</em>. For example:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> <span>datetime</span> <span>import</span> <span>datetime</span><span>,</span> <span>timezone</span>
</span></span><span><span>
</span></span><span><span><span>dt</span> <span>=</span> <span>datetime</span><span>.</span><span>now</span><span>(</span><span>timezone</span><span>.</span><span>utc</span><span>)</span>
</span></span></code></pre></div><p>Python 3.9 and above include the <strong>zoneinfo</strong> module. This provides access to the standard IANA database of time zones. Previous versions of Python require a third-party library for time zones.</p>
<blockquote>
<p><a href="https://www.python.org/dev/peps/pep-0615/" target="_blank" rel="noreferrer">PEP 615</a> describes support for the IANA time zone database with <strong>zoneinfo</strong>.</p>
</blockquote>
<h3 id="use-enum-or-named-tuples-for-immutable-sets-of-key-value-pairs">Use enum or Named Tuples for Immutable Sets of Key-Value Pairs <span><a href="#use-enum-or-named-tuples-for-immutable-sets-of-key-value-pairs" aria-label="Anchor">#</a></span></h3><p>Use the <em>enum</em> type in Python 3.4 or above for immutable collections of key-value pairs. Enums can use class inheritance.</p>
<p>Python 3 also has <em>collections.namedtuple()</em> for immutable key-value pairs. Named tuples do not use classes.</p>
<h3 id="create-data-classes-for-custom-data-objects">Create Data Classes for Custom Data Objects <span><a href="#create-data-classes-for-custom-data-objects" aria-label="Anchor">#</a></span></h3><p>The data classes feature enables you to reduce the amount of code that you need to define classes for objects that exist to store values. The new syntax for data classes does not affect the behavior of the classes that you define with it. Each data class is a standard Python class.</p>
<p>You can set a <em>frozen</em> option to make <a href="https://docs.python.org/3/library/dataclasses.html#frozen-instances" target="_blank" rel="noreferrer">frozen instances</a> of a data class.</p>
<p>Data classes were introduced in version 3.7 of Python.</p>
<blockquote>
<p><a href="https://www.python.org/dev/peps/pep-0557/" target="_blank" rel="noreferrer">PEP 557</a> describes data classes.</p>
</blockquote>
<h3 id="use-collectionsabc-for-custom-collection-types">Use collections.abc for Custom Collection Types <span><a href="#use-collectionsabc-for-custom-collection-types" aria-label="Anchor">#</a></span></h3><p>The abstract base classes in <em>collections.abc</em> provide the components for building your own custom collection types.</p>
<p>Use these classes, because they are fast and well-tested. The implementations in Python 3.7 and above are written in C, to provide better performance than Python code.</p>
<h3 id="use-breakpoint-for-debugging">Use breakpoint() for Debugging <span><a href="#use-breakpoint-for-debugging" aria-label="Anchor">#</a></span></h3><p>This function drops you into the debugger at the point where it is called. Both the <a href="https://docs.python.org/3/library/pdb.html" target="_blank" rel="noreferrer">built-in debugger</a> and external debuggers can use these breakpoints.</p>
<p>The <a href="https://docs.python.org/3/library/functions.html#breakpoint" target="_blank" rel="noreferrer">breakpoint()</a> feature was added in version 3.7 of Python.</p>
<blockquote>
<p><a href="https://www.python.org/dev/peps/pep-0553/" target="_blank" rel="noreferrer">PEP 553</a> describes the <em>breakpoint()</em> function.</p>
</blockquote>
<h2 id="application-design">Application Design <span><a href="#application-design" aria-label="Anchor">#</a></span></h2><h3 id="use-logging-for-diagnostic-messages-rather-than-print">Use Logging for Diagnostic Messages, Rather Than print() <span><a href="#use-logging-for-diagnostic-messages-rather-than-print" aria-label="Anchor">#</a></span></h3><p>The built-in <em>print()</em> statement is convenient for adding debugging information, but you should include logging in your scripts and applications. Use the <a href="https://docs.python.org/3/library/logging.html#logrecord-attributes" target="_blank" rel="noreferrer">logging</a> module in the standard library, or a third-party logging module.</p>
<h3 id="use-the-toml-format-for-configuration">Use The TOML Format for Configuration <span><a href="#use-the-toml-format-for-configuration" aria-label="Anchor">#</a></span></h3><p>Use <a href="https://toml.io/" target="_blank" rel="noreferrer">TOML</a> for data files that must be written or edited by human beings. Use the JSON format for data that is transferred between computer programs. Avoid using the INI or YAML formats.</p>
<p>Python 3.11 and above include <em>tomllib</em> to read the TOML format. Use <a href="https://pypi.org/project/tomli/" target="_blank" rel="noreferrer">tomli</a> to add support for reading TOML to applications that run on older versions of Python.</p>
<p>If your Python software needs to generate TOML, add <a href="https://pypi.org/project/tomli-w/" target="_blank" rel="noreferrer">Tomli-W</a>.</p>
<blockquote>
<p><a href="https://peps.python.org/pep-0680/" target="_blank" rel="noreferrer">PEP 680 - tomllib: Support for Parsing TOML in the Standard Library</a> explains why TOML is now included with Python.</p>
</blockquote>
<h3 id="only-use-async-where-it-makes-sense">Only Use async Where It Makes Sense <span><a href="#only-use-async-where-it-makes-sense" aria-label="Anchor">#</a></span></h3><p>The <a href="https://docs.python.org/3/library/asyncio.html" target="_blank" rel="noreferrer">asynchronous features of Python</a> enable a single process to avoid blocking on I/O operations. To achieve concurrency with Python, you must run multiple Python processes. Each of these processes may or may not use asynchronous I/O.</p>
<p>To run multiple application processes, either use a container system, with one container per process, or an application server like <a href="https://gunicorn.org/" target="_blank" rel="noreferrer">Gunicorn</a>. If you need to build a custom application that manages muliple processes, use the <a href="https://docs.python.org/3/library/multiprocessing.html" target="_blank" rel="noreferrer">multiprocessing</a> package in the Python standard library.</p>
<p>Code that uses asynchronous I/O must not call <em>any</em> function that uses synchronous I/O, such as <em>open()</em>, or the <em>logging</em> module in the standard library. Instead, you need to use either the equivalent functions from <em>asyncio</em> in the standard library or a third-party library that is designed to support asynchronous code.</p>
<p>The <a href="https://fastapi.tiangolo.com/" target="_blank" rel="noreferrer">FastAPI</a> Web framework supports <a href="https://fastapi.tiangolo.com/async/" target="_blank" rel="noreferrer">using both synchronous and asynchronous functions in the same application</a>. You must still ensure that asynchronous functions never call any synchronous function.</p>
<p>If you would like to work with <em>asyncio</em>, use Python 3.7 or above. Version 3.7 of Python introduced <a href="https://docs.python.org/3/library/contextvars.html" target="_blank" rel="noreferrer">context variables</a>, which enable you to have data that is local to a specific <em>task</em>, as well as the <em>asyncio.run()</em> function.</p>
<blockquote>
<p><a href="https://www.python.org/dev/peps/pep-0567/" target="_blank" rel="noreferrer">PEP 0567</a> describes context variables.</p>
</blockquote>
<h2 id="libraries">Libraries <span><a href="#libraries" aria-label="Anchor">#</a></span></h2><h3 id="handle-command-line-input-with-argparse">Handle Command-line Input with argparse <span><a href="#handle-command-line-input-with-argparse" aria-label="Anchor">#</a></span></h3><p>The <a href="https://docs.python.org/3/library/argparse.html" target="_blank" rel="noreferrer">argparse</a> module is now the recommended way to process command-line input. Use <em>argparse</em>, rather than the older <em>optparse</em> and <em>getopt</em>.</p>
<p>The <em>optparse</em> module is officially deprecated, so update code that uses <em>optparse</em> to use <em>argparse</em> instead.</p>
<p>Refer to <a href="https://docs.python.org/3/howto/argparse.html" target="_blank" rel="noreferrer">the argparse tutorial</a> in the official documentation for more details.</p>
<h3 id="use-pathlib-for-file-and-directory-paths">Use pathlib for File and Directory Paths <span><a href="#use-pathlib-for-file-and-directory-paths" aria-label="Anchor">#</a></span></h3><p>Use <a href="https://docs.python.org/3/library/pathlib.html" target="_blank" rel="noreferrer">pathlib</a> objects instead of strings whenever you need to work with file and directory pathnames.</p>
<p>Consider using the <a href="https://docs.python.org/3/library/pathlib.html#correspondence-to-tools-in-the-os-module" target="_blank" rel="noreferrer">the pathlib equivalents for os functions</a>.</p>
<p>The existing methods in the standard library have been updated to support Path objects.</p>
<p>To list all of the the files in a directory, use either the <em>.iterdir()</em> function of a Path object, or the <em>os.scandir()</em> function.</p>
<p>This <a href="https://realpython.com/working-with-files-in-python/#directory-listing-in-modern-python-versions" target="_blank" rel="noreferrer">RealPython article</a> provides a full explanation of the different Python functions for working with files and directories.</p>
<p>The <em>pathlib</em> module was added to the standard library in Python 3.4, and other standard library functions were updated to support Path objects in version 3.5 of Python.</p>
<h3 id="use-osscandir-instead-of-oslistdir">Use os.scandir() Instead of os.listdir() <span><a href="#use-osscandir-instead-of-oslistdir" aria-label="Anchor">#</a></span></h3><p>The <em>os.scandir()</em> function is significantly faster and more efficient than <em>os.listdir()</em>. Use <em>os.scandir()</em> wherever you previously used the <em>os.listdir()</em> function.</p>
<p>This function provides an iterator, and works with a context manager:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> <span>os</span>
</span></span><span><span>
</span></span><span><span><span>with</span> <span>os</span><span>.</span><span>scandir</span><span>(</span><span>'some_directory/'</span><span>)</span> <span>as</span> <span>entries</span><span>:</span>
</span></span><span><span>    <span>for</span> <span>entry</span> <span>in</span> <span>entries</span><span>:</span>
</span></span><span><span>        <span>print</span><span>(</span><span>entry</span><span>.</span><span>name</span><span>)</span>
</span></span></code></pre></div><p>The context manager frees resources as soon as the function completes. Use this option if you are concerned about performance or concurrency.</p>
<p>The <em>os.walk()</em> function now calls <em>os.scandir()</em>, so it automatically has the same improved performance as this function.</p>
<p>The <em>os.scandir()</em> function was added in version 3.5 of Python.</p>
<blockquote>
<p><a href="https://www.python.org/dev/peps/pep-0471/" target="_blank" rel="noreferrer">PEP 471</a> explains <em>os.scandir()</em>.</p>
</blockquote>
<h3 id="run-external-commands-with-subprocess">Run External Commands with subprocess <span><a href="#run-external-commands-with-subprocess" aria-label="Anchor">#</a></span></h3><p>The <a href="https://docs.python.org/3/library/subprocess.html" target="_blank" rel="noreferrer">subprocess</a> module provides a safe way to run external commands. Use <em>subprocess</em> rather than shell backquoting or the functions in <em>os</em>, such as <em>spawn</em>, <em>popen2</em> and <em>popen3</em>. The <em>subprocess.run()</em> function in current versions of Python is sufficient for most cases.</p>
<blockquote>
<p><a href="https://www.python.org/dev/peps/pep-0324/" target="_blank" rel="noreferrer">PEP 324</a> explains the technical details of subprocess in detail.</p>
</blockquote>
<h3 id="use-httpx-for-web-clients">Use httpx for Web Clients <span><a href="#use-httpx-for-web-clients" aria-label="Anchor">#</a></span></h3><p>Use <a href="https://www.python-httpx.org/" target="_blank" rel="noreferrer">httpx</a> for Web client applications. It <a href="https://www.python-httpx.org/http2/" target="_blank" rel="noreferrer">supports HTTP/2</a>, and <a href="https://www.python-httpx.org/async/" target="_blank" rel="noreferrer">async</a>. The httpx package supersedes <a href="https://requests.readthedocs.io/en/latest/" target="_blank" rel="noreferrer">requests</a>, which only supports HTTP 1.1.</p>
<p>Avoid using <em>urllib.request</em> from the Python standard library. It was designed as a low-level library, and lacks the features of httpx.</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tour de France: How professional cycling teams eat and cook on the road (122 pts)]]></title>
            <link>https://www.bbc.co.uk/sport/cycling/articles/cxxx568grlwo</link>
            <guid>40860364</guid>
            <pubDate>Tue, 02 Jul 2024 21:01:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.co.uk/sport/cycling/articles/cxxx568grlwo">https://www.bbc.co.uk/sport/cycling/articles/cxxx568grlwo</a>, See on <a href="https://news.ycombinator.com/item?id=40860364">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="text-block" width="wide"><p><b>A few weeks prior to a previous Tour de France, amid the maelstrom of planning involved with eight riders and more than a dozen support vehicles navigating the country, EF Education-EasyPost head performance chef Owen Blandy received notice of an issue at one of the hotels.</b></p><p>For reasons unexplained, Blandy was told he would not be allowed to use the hotel kitchen, nor even cook in his own food truck on site.</p><p>If he desired, he might be able to supervise the hotel's own chef in their preparations, but would not be permitted to do so from inside the kitchen.</p><p>For a man tasked with fuelling a professional cycling team throughout the most important race on the sport's calendar, it was not ideal news. But he was entirely unflustered.</p><p>"It was fine," shrugs Blandy. "I just had a challenging few days before settling into my own kitchen."</p><p>Personal experience gleaned from a cumulative total of more than a year on the road at major races has taught Blandy to roll with the punches.</p><p>"There are never perfect working conditions in cycling so you always have to adapt and be flexible," he says.</p><p>If a hotel bans the team chef from cooking food, then so be it.</p></div><div data-component="text-block" width="wide"><p>Not so long ago, the professional cycling world's approach to fuelling was remarkably basic.</p><p>Options for riders barely extended beyond a monotonous menu of pasta, rice or whatever fare that night's hotel kitchen decided to serve up.</p><p>These days, it is an entirely different prospect, with vast sums spent on custom-built food trucks, personalised nutrition apps and meticulously planned meal regimes all in the name of performance enhancement.</p><p>For the nutritionists and chefs tasked with providing sustenance to power their team's riders over 2,170 miles in the coming weeks there are principally two dilemmas: what food to prepare and how to do so in an ever-changing environment.</p><p>The answers are gleaned from a year-round process that begins in December during pre-season training.</p><p>While the riders are honing their bodies, ready for the multitude of races ahead, the number-crunchers eagerly gather data to better understand their nutritional needs.</p><p>"We know their individual bodies, their metabolism, how many calories they burn when resting and exactly what they will do in training, the intensity, how long and how many calories they will burn," says Visma-Lease a Bike head of nutrition Martijn Redegeld.</p><p>"Heart rate plays a role. We have that after each training ride. And at certain points in the season we test lactate measurements and breathing measurements in the lab to develop a good profile of each rider."</p><p>As one of three teams - alongside UAE Team Emirates and Ineos Grenadiers - whose budget tends to dwarf all others, Visma-Lease a Bike has strived to place itself at the forefront of nutritional advancement.</p><p>Partnerships with universities aim to ensure they are firmly aware of developments within the field "to keep that competitive edge over other teams", says Redegeld.</p><p>With riders burning an average of 6,000 calories per day during the Tour (around three times more than a resting adult), Visma-Lease a Bike have even begun using artificial intelligence to help determine precisely how much - and what type of - food each individual cyclist should consume.</p><p>Personalisation has become increasingly paramount, with the team <a href="https://www.theathletesfoodcoach.com/">developing its own app,<span>, <!-- -->external</span></a> where various algorithms are used to generate individualised nutrition plans.</p><p>When a rider comes back from a day on the bike, they simply open the app and are told exactly how many grams of each nutritional component (carbohydrates, proteins, fats etc) to put on their plate. No brain power is wasted beyond using the ubiquitous buffet table weighing scales.</p><p>While the methods used to generate precise nutritional needs vary between teams, all of them work to a broad five-meal daily plan of breakfast, pre-race snack, on-bike fuelling, recovery meal and dinner.</p><p>The core feeding principles remain the same across the peloton, although they are tweaked depending on the upcoming day's requirements and whether the rider in question is a climber or a sprinter, a domestique or a general classification contender.</p><p>Carbohydrates - usually in the form of rice or pasta - serve as the petrol, necessitating painfully high consumption levels.</p><p>Proteins - predominantly fish or chicken - are always unprocessed and fibre is kept low to minimise gut irritation and aid digestion, with fruit and vegetables often consumed in juice form.</p><p>Vegetarians tend to supplement themselves with protein shakes, in addition to plant-based proteins like tofu and seitan.</p><p>Riders might be allowed more vegetables and fibrous foods before flatter race days, when the body will be better equipped to break them down, while red meats are saved as a treat the evening before rest days.</p><p>On-bike fuelling comes courtesy of roadside soigneurs who load up musette bags with a variety of high-carbohydrate forms that can be selected or discarded based on personal preference.</p><p>Energy bars, gels, drinks and gummies provide quick hits on tough days, while more traditional food sources include wet rice cakes, brioches, jam sandwiches, flapjacks, sweet breads and cakes for easier days.</p><p>The required quantities are unenviably vast. Each rider consumes close to 1.5kg of rice or pasta every day and in the region of 120g of carbohydrates per hour when on the bike - the equivalent carbohydrate content of five hourly bananas. </p><p>One EF rider once went through four tubs of maple syrup during the three-week race.</p></div><div data-component="text-block" width="wide"><p>Blandy's laptop contains a treasure trove of nutritional information to enable his menu design.</p><p>One spreadsheet allows him to compare every food item's nutrient values to decide whether to cook with aubergines or parsnips, quinoa or couscous, chicken breast or chicken thigh.</p><p>Another document comprises the EF Education-EasyPost recipe bible, listing a myriad of soups, salads, carbohydrates, proteins, sides, desserts, post-race snacks and drinks. In a bid to combat flavour fatigue, repetition is kept to an absolute minimum across a three-week race.</p><p>"The food I make is all transparent," says Blandy. "There are no rich sauces, it's all plain, simple cooking with a light amount of seasoning, light amount of oil, fresh herbs and citrus.</p><p>"Instead of putting flavour in with cream, salt and butter we're adding it with herbs and citrus because they are low calorie and contain antioxidants."</p><p>It does not lend itself to the type of innovative kitchen artistry you might see on television shows or in fancy restaurants.</p><p>"When I'm teaching new chefs, I always say the only way they will mess it up is by being too 'cheffy,'" says Blandy.</p><p>"You need to swallow your chef ego and put it into a dessert or play around at the end of a race. Go wild then but don't mess with the simple stuff: the carbohydrates and proteins. Give the guys what they want and they will be happy.</p><p>"I've cooked risottos before and they've just asked for plain basmati rice. They aren't there on a holiday. They don't care about fancy food. They are literally there to fuel."</p></div><div data-component="text-block" width="wide"><p>Blandy estimates he has stayed - and therefore been tasked with cooking - at more than 300 hotels during his time working for EF Education-EasyPost. The transient nature of the job presents numerous logistical headaches.</p><p>A chef's day at the Tour de France begins around 06:00. They must prepare fresh breakfast items (all packaged food has already been set up the night before) for 08:00 before packing up and driving to the next hotel while the race is ongoing.</p><p>As well as cooking the food, they are also responsible for procuring it - a task that varies depending on team and, crucially, sponsor.</p><p>Blandy's experience of European supermarkets means he knows where to find the highest-quality food and shops personally for most of it, in addition to emailing hotels in advance to order some perishable items.</p><p>Conversely, Visma-Lease a Bike have been sponsored since 2014 by Dutch supermarket Jumbo, who provide all their food at every race, including the Tour de France.</p><p>"During a Grand Tour there are three times that a new delivery comes from the Netherlands to stock up on fresh produce," says Redegeld. "It's always the same Dutch food and the guys like that because they know what to expect and we know what products they like, so we can always have that available.</p><p>"It makes things a lot easier for the chefs who don't have to search in local supermarkets for things. For me as a nutritionist, we know the nutritional values of all the products so it makes the calculations a lot easier."</p><p>Upon arriving at a hotel, chefs will begin preparations for dinner and the following day's breakfast and snacks.</p><p>Professional cycling teams tend to adhere to one of two dining styles.</p><p>Most travel with customised kitchen trucks - a similar size to supermarket delivery vans - where food is stored and meals cooked. Food is then served up for the riders and wider team members in a private room inside the hotel.</p><p>A select few teams - including Ineos Grenadiers - instead choose to travel with a far bigger lorry, which contains a kitchen and dining room.</p><p>Camaraderie between chefs on rival teams is high. "Sometimes you're in a hotel with six teams, so the car parks are rammed," says Blandy.</p><p>"It's manic. Everyone is sharing water and electricity. So you have to scratch each other's backs. Chefs come to me and ask for an ingredient and I go to them. We help each other out."</p></div><div data-component="text-block" width="wide"><p>It is a world away from the three successive weeks of pasta with tomato sauce that riders just a generation ago were accustomed to stomaching throughout their Tour de France endeavours.</p><p>Redegeld predicts the nutrition evolution will continue, suggesting that within a decade or so teams will employ DNA analysis to take rider fuelling personalisation to the next level.</p><p>But all the analytics are worthless without someone to prepare the food.</p><p>Earlier this year, Blandy was all set for a quiet week at home when he received an SOS from the team.</p><p>He was given half an hour to pack his bags and jump in a taxi to the airport because a fellow EF Education-EasyPost chef had fallen ill before the Paris-Roubaix race.</p><p>"I rolled my knives up and threw them in a suitcase," he says. "I felt like chef special forces."</p><p>Cooking is serious business in the elite cycling world.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple poised to get OpenAI board observer role as part of AI pact (128 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-07-02/apple-to-get-openai-board-observer-role-as-part-of-ai-agreement</link>
            <guid>40860363</guid>
            <pubDate>Tue, 02 Jul 2024 21:01:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-07-02/apple-to-get-openai-board-observer-role-as-part-of-ai-agreement">https://www.bloomberg.com/news/articles/2024-07-02/apple-to-get-openai-board-observer-role-as-part-of-ai-agreement</a>, See on <a href="https://news.ycombinator.com/item?id=40860363">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
    </channel>
</rss>