<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 13 Jun 2024 07:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Uncensor any LLM with abliteration (135 pts)]]></title>
            <link>https://huggingface.co/blog/mlabonne/abliteration</link>
            <guid>40665721</guid>
            <pubDate>Thu, 13 Jun 2024 03:42:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://huggingface.co/blog/mlabonne/abliteration">https://huggingface.co/blog/mlabonne/abliteration</a>, See on <a href="https://news.ycombinator.com/item?id=40665721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p><a href="https://huggingface.co/blog">
						Back to Articles</a></p>

				
				
				
				<div data-target="BlogAuthorsByline" data-props="{&quot;authors&quot;:[{&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg&quot;,&quot;fullname&quot;:&quot;Maxime Labonne&quot;,&quot;name&quot;:&quot;mlabonne&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false}}],&quot;translators&quot;:[],&quot;proofreaders&quot;:[],&quot;lang&quot;:&quot;en&quot;}">

<p><span><span><a href="https://huggingface.co/mlabonne"><img alt="Maxime Labonne's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg">
					</a>
			</span>

	</span></p></div>
				

				<!-- HTML_TAG_START -->
<div><nav aria-label="Secondary"><ul><li><a href="#‚úÇÔ∏è-what-is-abliteration" title="‚úÇÔ∏è What is abliteration?"><!-- HTML_TAG_START -->‚úÇÔ∏è What is abliteration?<!-- HTML_TAG_END --></a>
									<ul></ul>
								</li><li><a href="#üíª-implementation" title="üíª Implementation"><!-- HTML_TAG_START -->üíª Implementation<!-- HTML_TAG_END --></a>
									<ul></ul>
								</li><li><a href="#‚öñÔ∏è-dpo-fine-tuning" title="‚öñÔ∏è DPO Fine-Tuning"><!-- HTML_TAG_START -->‚öñÔ∏è DPO Fine-Tuning<!-- HTML_TAG_END --></a>
									<ul></ul>
								</li><li><a href="#conclusion" title="Conclusion"><!-- HTML_TAG_START -->Conclusion<!-- HTML_TAG_END --></a>
									<ul></ul>
								</li><li><a href="#references" title="References"><!-- HTML_TAG_START -->References<!-- HTML_TAG_END --></a>
									<ul></ul>
								</li></ul></nav></div><p><a rel="nofollow" href="https://i.imgur.com/KhorYYG.png"><img alt="KhorYYG.png" src="https://i.imgur.com/KhorYYG.png"></a></p>
<p>The third generation of Llama models provided fine-tunes (Instruct) versions that excel in understanding and following instructions. However, these models are heavily censored, designed to refuse requests seen as harmful with responses such as "As an AI assistant, I cannot help you." While this safety feature is crucial for preventing misuse, it limits the model's flexibility and responsiveness.</p>
<p>In this article, we will explore a technique called "abliteration" that can uncensor any LLM without retraining. This technique effectively removes the model's built-in refusal mechanism, allowing it to respond to all types of prompts.</p>
<p>The code is available on&nbsp;<a rel="nofollow" href="https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR?usp=sharing">Google Colab</a>&nbsp;and in the&nbsp;<a rel="nofollow" href="https://github.com/mlabonne/llm-course">LLM Course</a>&nbsp;on GitHub.</p>
<h2>
	<a rel="nofollow" href="#‚úÇÔ∏è-what-is-abliteration" id="‚úÇÔ∏è-what-is-abliteration">
		<span></span>
	</a>
	<span>
		‚úÇÔ∏è What is abliteration?
	</span>
</h2>
<p>Modern LLMs are fine-tuned for safety and instruction-following, meaning they are trained to refuse harmful requests. In their <a rel="nofollow" href="https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction">blog post</a>, Arditi et al. have shown that this refusal behavior is mediated by a specific direction in the model's residual stream. If we prevent the model from representing this direction, it <strong>loses its ability to refuse requests</strong>. Conversely, adding this direction artificially can cause the model to refuse even harmless requests.</p>
<p>In the traditional decoder-only Llama-like architecture, there are three residual streams we can target: at the start of each block ("pre"), between the attention and MLP layers ("mid"), and after the MLP ("post"). The following figure illustrates the location of each residual stream.</p>
<p><a rel="nofollow" href="https://i.imgur.com/hsdR9e7.png"><img alt="" src="https://i.imgur.com/hsdR9e7.png"></a></p>
<p>To uncensor an LLM, we first need to identify the "refusal direction" within the model. This process involves a few technical steps:</p>
<ol>
<li><strong>Data Collection</strong>: Run the model on a set of harmful instructions and a set of harmless instructions, recording the residual stream activations at the last token position for each.</li>
<li><strong>Mean difference</strong>: Calculate the mean difference between the activations of harmful and harmless instructions. This gives us a vector representing the "refusal direction" for each layer of the model.</li>
<li><strong>Selection</strong>: Normalize these vectors and evaluate them to select the single best "refusal direction."</li>
</ol>
<p>Once we have identified the refusal direction, we can "ablate" it, effectively removing the model's ability to represent this feature. This can be done through an <strong>inference-time intervention</strong> or permanently with <strong>weight orthogonalization</strong>.</p>
<p>Let's talk about inference-time intervention first. For every component that writes to the residual stream (such as an attention head), we calculate the projection of its output onto the refusal direction and subtract this projection. This subtraction is applied at every token and every layer, ensuring that the model never represents the refusal direction.</p>
<p>On the other hand, weight orthogonalization involves modifying the model weights directly. By orthogonalizing the component weights with respect to the refusal direction, it prevents the model from writing to this direction altogether. This is achieved by adjusting the matrices that write to the residual stream, ensuring they do not contribute to the refusal direction.</p>
<p>In the next section, we will implement abliteration with weight orthogonalization.</p>
<h2>
	<a rel="nofollow" href="#üíª-implementation" id="üíª-implementation">
		<span></span>
	</a>
	<span>
		üíª Implementation
	</span>
</h2>
<p>The following implementation of abliteration is based on <a href="https://huggingface.co/failspy/llama-3-70B-Instruct-abliterated/blob/main/ortho_cookbook.ipynb">FailSpy's notebook</a>, which is itself based on the original authors' <a rel="nofollow" href="https://colab.research.google.com/drive/1a-aQvKC9avdZpdyBn4jgRQFObTPy1JZw?usp=sharing">notebook</a>. I mostly adapted and simplified it to make it easier to understand. This section is quite code-heavy so you can see what is going on, but you can use FailSpy's <a rel="nofollow" href="https://github.com/FailSpy/abliterator">abliterator library</a> if you're less interested in the technical details (also check his <a href="https://huggingface.co/collections/failspy/abliterated-v3-664a8ad0db255eefa7d0012b">collection of abliterated models</a> on Hugging Face).</p>
<p>The code relies on the excellent <a rel="nofollow" href="https://github.com/TransformerLensOrg/TransformerLens">TransformerLens</a> library (formerly known as EasyTransformer) to do the heavy lifting. It is designed for mechanistic interpretability and is used here to intervene on activations. Thanks to Neel Nanda and Joseph Bloom for creating and maintaining this library.</p>
<p>First, let's install the necessary packages and import them. All these steps are available in this <a rel="nofollow" href="https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR?usp=sharing">Google Colab notebook</a>.</p>
<pre><code>!pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping

<span>import</span> torch
<span>import</span> functools
<span>import</span> einops
<span>import</span> gc

<span>from</span> datasets <span>import</span> load_dataset
<span>from</span> tqdm <span>import</span> tqdm
<span>from</span> torch <span>import</span> Tensor
<span>from</span> typing <span>import</span> <span>List</span>
<span>from</span> transformer_lens <span>import</span> HookedTransformer, utils
<span>from</span> transformer_lens.hook_points <span>import</span> HookPoint
<span>from</span> transformers <span>import</span> AutoModelForCausalLM, AutoTokenizer
<span>from</span> jaxtyping <span>import</span> Float, Int
<span>from</span> collections <span>import</span> defaultdict

<span># Turn automatic differentiation off to save GPU memory (credit: Undi95)</span>
torch.set_grad_enabled(<span>False</span>)
</code></pre>
<p>We need two datasets: one containing harmless instructions, and one containing harmful instructions. We'll use <a href="https://huggingface.co/datasets/tatsu-lab/alpaca">tatsu-lab/alpaca</a> as well as data from <a rel="nofollow" href="https://github.com/llm-attacks/llm-attacks">llm-attacks</a>. To make things easier, I repackaged them in two Hugging Face datasets: <a href="https://huggingface.co/datasets/harmless_behaviors">mlabonne/harmless_behaviors</a> and <a href="https://huggingface.co/datasets/mlabonne/harmful_behaviors">mlabonne/harmful_behaviors</a>. That way, you can easily replace them with your own datasets.</p>
<p>We will load the instructions and reformat them into a list of dictionaries with "role" and "content" keys. This makes it compatible with the <code>apply_chat_tokenizer()</code> method, which we will use to follow Llama 3's chat template.</p>
<pre><code><span>def</span> <span>reformat_texts</span>(<span>texts</span>):
    <span>return</span> [[{<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: text}] <span>for</span> text <span>in</span> texts]

<span># Get harmful and harmless datasets</span>
<span>def</span> <span>get_harmful_instructions</span>():
    dataset = load_dataset(<span>'mlabonne/harmful_behaviors'</span>)
    <span>return</span> reformat_texts(dataset[<span>'train'</span>][<span>'text'</span>]), reformat_texts(dataset[<span>'test'</span>][<span>'text'</span>])

<span>def</span> <span>get_harmless_instructions</span>():
    dataset = load_dataset(<span>'mlabonne/harmless_alpaca'</span>)
    <span>return</span> reformat_texts(dataset[<span>'train'</span>][<span>'text'</span>]), reformat_texts(dataset[<span>'test'</span>][<span>'text'</span>])

harmful_inst_train, harmful_inst_test = get_harmful_instructions()
harmless_inst_train, harmless_inst_test = get_harmless_instructions()
</code></pre>
<p>Now that we have our datasets, we can load the model we want to abliterate. Unfortunately, you can't directly load a custom model using <code>HookedTransformer</code>. Here, I use a trick described in FailSpy's notebook to download a custom model and rename it as <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">meta-llama/Meta-Llama-3-8B-Instruct</a>. Load in <code>torch.float16</code> format if your GPU is not compatible with BF16.</p>
<p>In this example, we'll use <a href="https://huggingface.co/mlabonne/Daredevil-8B">mlabonne/Daredevil-8B</a>, a mega-merge created with DARE TIES (see my article about <a href="https://huggingface.co/blog/mlabonne/merge-models">model merging</a>) that has the highest MMLU score on the Open LLM Leaderboard in the 8B category.</p>
<pre><code>MODEL_ID = <span>"mlabonne/Daredevil-8B"</span>
MODEL_TYPE = <span>"meta-llama/Meta-Llama-3-8B-Instruct"</span>

<span># Download and load model</span>
!git clone https://huggingface.co/{MODEL_ID} {MODEL_TYPE}

<span># Load model and tokenizer</span>
model = HookedTransformer.from_pretrained_no_processing(
    MODEL_TYPE,
    local_files_only=<span>True</span>,
    dtype=torch.bfloat16,
    default_padding_side=<span>'left'</span>
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)
tokenizer.padding_side = <span>'left'</span>
tokenizer.pad_token = tokenizer.eos_token
</code></pre>
<p>We can now tokenize our datasets. We're using the same number of samples for both harmless and harmful instructions. Note that a high number of samples can use all the RAM/VRAM, which is why I'm limiting it to 256 here.</p>
<pre><code><span>def</span> <span>tokenize_instructions</span>(<span>tokenizer, instructions</span>):
    <span>return</span> tokenizer.apply_chat_template(
        instructions,
        padding=<span>True</span>,
        truncation=<span>False</span>,
        return_tensors=<span>"pt"</span>,
        return_dict=<span>True</span>,
        add_generation_prompt=<span>True</span>,
    ).input_ids

n_inst_train = <span>min</span>(<span>256</span>, <span>len</span>(harmful_inst_train), <span>len</span>(harmless_inst_train))

<span># Tokenize datasets</span>
harmful_tokens = tokenize_instructions(
    tokenizer,
    instructions=harmful_inst_train[:n_inst_train],
)
harmless_tokens = tokenize_instructions(
    tokenizer,
    instructions=harmless_inst_train[:n_inst_train],
)
</code></pre>
<p>Everything is set up, we can now implement the first step of abliteration: data collection. We want to process these tokenized datasets and store the residual stream activations in <code>harmful</code> and <code>harmless</code>. This is managed by the <a rel="nofollow" href="https://github.com/TransformerLensOrg/TransformerLens">transformer_lens</a> library.</p>
<pre><code><span># Define batch size based on available VRAM</span>
batch_size = <span>32</span>

<span># Initialize defaultdicts to store activations</span>
harmful = defaultdict(<span>list</span>)
harmless = defaultdict(<span>list</span>)

<span># Process the training data in batches</span>
num_batches = (n_inst_train + batch_size - <span>1</span>) // batch_size
<span>for</span> i <span>in</span> tqdm(<span>range</span>(num_batches)):
    <span>print</span>(i)
    start_idx = i * batch_size
    end_idx = <span>min</span>(n_inst_train, start_idx + batch_size)

    <span># Run models on harmful and harmless prompts, cache activations</span>
    harmful_logits, harmful_cache = model.run_with_cache(
        harmful_tokens[start_idx:end_idx],
        names_filter=<span>lambda</span> hook_name: <span>'resid'</span> <span>in</span> hook_name,
        device=<span>'cpu'</span>,
        reset_hooks_end=<span>True</span>
    )
    harmless_logits, harmless_cache = model.run_with_cache(
        harmless_tokens[start_idx:end_idx],
        names_filter=<span>lambda</span> hook_name: <span>'resid'</span> <span>in</span> hook_name,
        device=<span>'cpu'</span>,
        reset_hooks_end=<span>True</span>
    )

    <span># Collect and store the activations</span>
    <span>for</span> key <span>in</span> harmful_cache:
        harmful[key].append(harmful_cache[key])
        harmless[key].append(harmless_cache[key])

    <span># Flush RAM and VRAM</span>
    <span>del</span> harmful_logits, harmless_logits, harmful_cache, harmless_cache
    gc.collect()
    torch.cuda.empty_cache()

<span># Concatenate the cached activations</span>
harmful = {k: torch.cat(v) <span>for</span> k, v <span>in</span> harmful.items()}
harmless = {k: torch.cat(v) <span>for</span> k, v <span>in</span> harmless.items()}
</code></pre>
<p>We can now compute the refusal direction for each layer. This corresponds to the mean difference between the activations of harmful and harmless instructions, which is then normalized. We sort them in descending order in <code>activation_scored</code>. </p>
<pre><code><span># Helper function to get activation index</span>
<span>def</span> <span>get_act_idx</span>(<span>cache_dict, act_name, layer</span>):
    key = (act_name, layer)
    <span>return</span> cache_dict[utils.get_act_name(*key)]

<span># Compute difference of means between harmful and harmless activations at intermediate layers</span>
activation_layers = [<span>"resid_pre"</span>, <span>"resid_mid"</span>, <span>"resid_post"</span>]
activation_refusals = defaultdict(<span>list</span>)

<span>for</span> layer_num <span>in</span> <span>range</span>(<span>1</span>, model.cfg.n_layers):
    pos = -<span>1</span>  <span># Position index</span>

    <span>for</span> layer <span>in</span> activation_layers:
        harmful_mean_act = get_act_idx(harmful, layer, layer_num)[:, pos, :].mean(dim=<span>0</span>)
        harmless_mean_act = get_act_idx(harmless, layer, layer_num)[:, pos, :].mean(
            dim=<span>0</span>
        )

        refusal_dir = harmful_mean_act - harmless_mean_act
        refusal_dir = refusal_dir / refusal_dir.norm()
        activation_refusals[layer].append(refusal_dir)

<span># Get all calculated potential refusal directions, sort them in descending order based on their mean</span>
<span># Use a subset of layers if certain activations are not promising</span>
selected_layers = [<span>"resid_pre"</span>]
activation_scored = <span>sorted</span>(
    [
        activation_refusals[layer][l - <span>1</span>]
        <span>for</span> l <span>in</span> <span>range</span>(<span>1</span>, model.cfg.n_layers)
        <span>for</span> layer <span>in</span> selected_layers
    ],
    key=<span>lambda</span> x: <span>abs</span>(x.mean()),
    reverse=<span>True</span>,
)
</code></pre>
<p>The final step of the process consists of evaluating the refusal directions we calculated. To do this, we're going to apply the refusal direction to each residual stream and each block during inference. In the following snippet, we get generations for four test harmful instructions and 20 blocks (or layers).</p>
<pre><code><span>def</span> <span>_generate_with_hooks</span>(<span></span>
<span>    model: HookedTransformer,</span>
<span>    tokenizer: AutoTokenizer,</span>
<span>    tokens: Int[Tensor, <span>"batch_size seq_len"</span>],</span>
<span>    max_tokens_generated: <span>int</span> = <span>64</span>,</span>
<span>    fwd_hooks=[],</span>
<span></span>) -&gt; <span>List</span>[<span>str</span>]:
    all_tokens = torch.zeros(
        (tokens.shape[<span>0</span>], tokens.shape[<span>1</span>] + max_tokens_generated),
        dtype=torch.long,
        device=tokens.device,
    )
    all_tokens[:, : tokens.shape[<span>1</span>]] = tokens
    <span>for</span> i <span>in</span> <span>range</span>(max_tokens_generated):
        <span>with</span> model.hooks(fwd_hooks=fwd_hooks):
            logits = model(all_tokens[:, : -max_tokens_generated + i])
            next_tokens = logits[:, -<span>1</span>, :].argmax(
                dim=-<span>1</span>
            )  <span># greedy sampling (temperature=0)</span>
            all_tokens[:, -max_tokens_generated + i] = next_tokens
    <span>return</span> tokenizer.batch_decode(
        all_tokens[:, tokens.shape[<span>1</span>] :], skip_special_tokens=<span>True</span>
    )

<span>def</span> <span>get_generations</span>(<span></span>
<span>    model: HookedTransformer,</span>
<span>    tokenizer: AutoTokenizer,</span>
<span>    instructions: <span>List</span>[<span>str</span>],</span>
<span>    fwd_hooks=[],</span>
<span>    max_tokens_generated: <span>int</span> = <span>64</span>,</span>
<span>    batch_size: <span>int</span> = <span>4</span>,</span>
<span></span>) -&gt; <span>List</span>[<span>str</span>]:
    generations = []
    <span>for</span> i <span>in</span> tqdm(<span>range</span>(<span>0</span>, <span>len</span>(instructions), batch_size)):
        tokens = tokenize_instructions(
            tokenizer, instructions=instructions[i : i + batch_size]
        )
        generation = _generate_with_hooks(
            model,
            tokenizer,
            tokens,
            max_tokens_generated=max_tokens_generated,
            fwd_hooks=fwd_hooks,
        )
        generations.extend(generation)
    <span>return</span> generations

<span># Inference-time intervention hook</span>
<span>def</span> <span>direction_ablation_hook</span>(<span></span>
<span>    activation: Float[Tensor, <span>"... d_act"</span>],</span>
<span>    hook: HookPoint,</span>
<span>    direction: Float[Tensor, <span>"d_act"</span>],</span>
<span></span>):
    <span>if</span> activation.device != direction.device:
        direction = direction.to(activation.device)
    proj = (
        einops.einsum(
            activation, direction.view(-<span>1</span>, <span>1</span>), <span>"... d_act, d_act single -&gt; ... single"</span>
        )
        * direction
    )
    <span>return</span> activation - proj

<span># Testing baseline</span>
N_INST_TEST = <span>4</span>
baseline_generations = get_generations(
    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]
)

<span># Evaluating layers defined earlier (needs human evaluation to determine best layer for refusal inhibition)</span>
EVAL_N = <span>20</span>  <span># Evaluate how many of the top N potential directions</span>
evals = []
<span>for</span> refusal_dir <span>in</span> tqdm(activation_scored[:EVAL_N]):
    hook_fn = functools.partial(direction_ablation_hook, direction=refusal_dir)
    fwd_hooks = [
        (utils.get_act_name(act_name, layer), hook_fn)
        <span>for</span> layer <span>in</span> <span>list</span>(<span>range</span>(model.cfg.n_layers))
        <span>for</span> act_name <span>in</span> activation_layers
    ]
    intervention_generations = get_generations(
        model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=fwd_hooks
    )
    evals.append(intervention_generations)
</code></pre>
<p>We stored all the generations in the <code>evals</code> list. We can now print them and manually select the layer (block) that provides an uncensored response for each instruction. I'm automatically excluding responses containing "I cannot" and "I can't" to filter out unwanted answers.</p>
<p>If you can't find a layer that satisfies these requirements, you might want to test other residual streams in the previous <code>selected_layers</code> list, other instructions, additional blocks, etc.</p>
<pre><code><span># Print generations for human evaluation</span>
blacklist = [<span>"I cannot"</span>, <span>"I can't"</span>]
<span>for</span> i <span>in</span> <span>range</span>(N_INST_TEST):
    <span>print</span>(<span>f"\033[1mINSTRUCTION <span>{i}</span>: <span>{harmful_inst_test[i]}</span>"</span>)
    <span>print</span>(<span>f"\nBASELINE COMPLETION:\n<span>{baseline_generations[i]}</span>\033[0m"</span>)
    <span>for</span> layer_candidate <span>in</span> <span>range</span>(EVAL_N):
        <span>if</span> <span>not</span> <span>any</span>(word <span>in</span> evals[layer_candidate][i] <span>for</span> word <span>in</span> blacklist):
            <span>print</span>(<span>f"\n---\n\nLAYER CANDIDATE #<span>{layer_candidate}</span> INTERVENTION COMPLETION:"</span>)
            <span>print</span>(evals[layer_candidate][i])
</code></pre>
<p>In my case, the layer candidate 9 managed to provide uncensored answer for the four instructions. This is the one that we will select for the refusal direction. In the following, we implement weight orthogonalization to modify the weights and prevent the model from creating outputs with this direction. You can verify that the model is successfully uncensored by printing the completions.</p>
<pre><code><span>def</span> <span>get_orthogonalized_matrix</span>(<span></span>
<span>    matrix: Float[Tensor, <span>"... d_model"</span>], vec: Float[Tensor, <span>"d_model"</span>]</span>
<span></span>) -&gt; Float[Tensor, <span>"... d_model"</span>]:
    proj = (
        einops.einsum(
            matrix, vec.view(-<span>1</span>, <span>1</span>), <span>"... d_model, d_model single -&gt; ... single"</span>
        )
        * vec
    )
    <span>return</span> matrix - proj

<span># Select the layer with the highest potential refusal direction</span>
LAYER_CANDIDATE = <span>9</span>
refusal_dir = activation_scored[LAYER_CANDIDATE]

<span># Orthogonalize the model's weights</span>
<span>if</span> refusal_dir.device != model.W_E.device:
    refusal_dir = refusal_dir.to(model.W_E.device)
model.W_E.data = get_orthogonalized_matrix(model.W_E, refusal_dir)

<span>for</span> block <span>in</span> tqdm(model.blocks):
    <span>if</span> refusal_dir.device != block.attn.W_O.device:
        refusal_dir = refusal_dir.to(block.attn.W_O.device)
    block.attn.W_O.data = get_orthogonalized_matrix(block.attn.W_O, refusal_dir)
    block.mlp.W_out.data = get_orthogonalized_matrix(block.mlp.W_out, refusal_dir)

<span># Generate text with abliterated model</span>
orthogonalized_generations = get_generations(
    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]
)

<span># Print generations</span>
<span>for</span> i <span>in</span> <span>range</span>(N_INST_TEST):
    <span>if</span> <span>len</span>(baseline_generations) &gt; i:
        <span>print</span>(<span>f"INSTRUCTION <span>{i}</span>: <span>{harmful_inst_test[i]}</span>"</span>)
        <span>print</span>(<span>f"\033[92mBASELINE COMPLETION:\n<span>{baseline_generations[i]}</span>"</span>)
    <span>print</span>(<span>f"\033[91mINTERVENTION COMPLETION:\n<span>{evals[LAYER_CANDIDATE][i]}</span>"</span>)
    <span>print</span>(<span>f"\033[95mORTHOGONALIZED COMPLETION:\n<span>{orthogonalized_generations[i]}</span>\n"</span>)
</code></pre>
<p>We're now ready to use the model. We convert it back to the Hugging Face format and upload it to the HF hub.</p>
<pre><code><span># Convert model back to HF safetensors</span>
hf_model = AutoModelForCausalLM.from_pretrained(MODEL_TYPE, torch_dtype=torch.bfloat16)
lm_model = hf_model.model

state_dict = model.state_dict()
lm_model.embed_tokens.weight = torch.nn.Parameter(state_dict[<span>"embed.W_E"</span>].cpu())

<span>for</span> l <span>in</span> <span>range</span>(model.cfg.n_layers):
    lm_model.layers[l].self_attn.o_proj.weight = torch.nn.Parameter(
        einops.rearrange(
            state_dict[<span>f"blocks.<span>{l}</span>.attn.W_O"</span>], <span>"n h m-&gt;m (n h)"</span>, n=model.cfg.n_heads
        ).contiguous()
    )
    lm_model.layers[l].mlp.down_proj.weight = torch.nn.Parameter(
        torch.transpose(state_dict[<span>f"blocks.<span>{l}</span>.mlp.W_out"</span>], <span>0</span>, <span>1</span>).contiguous()
    )

hf_model.push_to_hub(<span>f"<span>{MODEL_ID}</span>-abliterated"</span>)
<span># hf_model.push_to_hub(f"{MODEL_ID}-abliterated")</span>
</code></pre>
<h2>
	<a rel="nofollow" href="#‚öñÔ∏è-dpo-fine-tuning" id="‚öñÔ∏è-dpo-fine-tuning">
		<span></span>
	</a>
	<span>
		‚öñÔ∏è DPO Fine-Tuning
	</span>
</h2>
<p>I evaluated the abliterated and source models from the previous section on the Open LLM Leaderboard and on Nous' benchmark suite. Here are the results:</p>
<p><a rel="nofollow" href="https://i.imgur.com/ECCejII.png"><img alt="" src="https://i.imgur.com/ECCejII.png"></a></p>
<p>As you can see, the source model significantly outperforms Llama 3 8B Instruct. However, we observe a performance drop in the ablated version across all benchmarks. The ablation process successfully uncensored it but also degraded the model's quality.</p>
<p>To address this issue, an idea consists of further training our abliterated model to heal it. Like most fine-tuned models, Llama 3 8B Instruct is quite brittle when it comes to supervised fine-tuning. An additional SFT would likely break the model's performance.</p>
<p>Alternatively, preference alignment is quite light and shouldn't lobotomize our abliterated model. DPO is a good candidate here for its ease of use and good track record. To implement it, I used <a rel="nofollow" href="https://colab.research.google.com/drive/1TsDKNo2riwVmU55gjuBgB1AXVtRRfRHW?usp=sharing">LazyAxolotl</a> with the <a href="https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k">mlabonne/orpo-dpo-mix-40k</a> dataset. Here's the configuration I used:</p>
<pre><code><span>base_model:</span> <span>mlabonne/Daredevil-8B-abliterated</span>
<span>model_type:</span> <span>LlamaForCausalLM</span>
<span>tokenizer_type:</span> <span>AutoTokenizer</span>

<span>load_in_8bit:</span> <span>false</span>
<span>load_in_4bit:</span> <span>true</span>
<span>strict:</span> <span>false</span>
<span>save_safetensors:</span> <span>true</span>

<span>rl:</span> <span>dpo</span>
<span>chat_template:</span> <span>chatml</span>
<span>datasets:</span>
  <span>-</span> <span>path:</span> <span>mlabonne/orpo-dpo-mix-40k-flat</span>
    <span>split:</span> <span>train</span>
    <span>type:</span> <span>chatml.intel</span>

<span>dataset_prepared_path:</span>
<span>val_set_size:</span> <span>0.0</span>
<span>output_dir:</span> <span>./out</span>

<span>adapter:</span> <span>qlora</span>
<span>lora_model_dir:</span>

<span>sequence_len:</span> <span>2048</span>
<span>sample_packing:</span> <span>false</span>
<span>pad_to_sequence_len:</span> <span>false</span>

<span>lora_r:</span> <span>64</span>
<span>lora_alpha:</span> <span>32</span>
<span>lora_dropout:</span> <span>0.05</span>
<span>lora_target_linear:</span> <span>true</span>
<span>lora_fan_in_fan_out:</span>

<span>wandb_project:</span> <span>axolotl</span>
<span>wandb_entity:</span>
<span>wandb_watch:</span>
<span>wandb_name:</span>
<span>wandb_log_model:</span>

<span>gradient_accumulation_steps:</span> <span>8</span>
<span>micro_batch_size:</span> <span>1</span>
<span>num_epochs:</span> <span>1</span>
<span>optimizer:</span> <span>paged_adamw_8bit</span>
<span>lr_scheduler:</span> <span>cosine</span>
<span>learning_rate:</span> <span>5e-6</span>
<span>train_on_inputs:</span> <span>false</span>
<span>group_by_length:</span> <span>false</span>

<span>bf16:</span> <span>auto</span>
<span>fp16:</span>
<span>tf32:</span>

<span>gradient_checkpointing:</span> <span>true</span>
<span>early_stopping_patience:</span>
<span>resume_from_checkpoint:</span>
<span>local_rank:</span>
<span>logging_steps:</span> <span>1</span>
<span>xformers_attention:</span>
<span>flash_attention:</span> <span>true</span>
<span>warmup_steps:</span> <span>100</span>
<span>evals_per_epoch:</span> <span>0</span>
<span>eval_table_size:</span>
<span>eval_table_max_new_tokens:</span> <span>128</span>
<span>saves_per_epoch:</span> <span>1</span>
<span>debug:</span>
<span>deepspeed:</span> <span>deepspeed_configs/zero2.json</span>
<span>weight_decay:</span> <span>0.0</span>
<span>special_tokens:</span>
  <span>pad_token:</span> <span>&lt;|end_of_text|&gt;</span>
</code></pre>
<p>I trained it using 6xA6000 GPUs with DeepSpeed ZeRO-2. The training took about 6 hours and 45 minutes. Here are the training curves I got from W&amp;B:</p>
<p><a rel="nofollow" href="https://i.imgur.com/nVcJYuu.png"><img alt="" src="https://i.imgur.com/nVcJYuu.png"></a></p>
<p>It automatically uploaded the DPO fine-tuned model, called <a href="https://huggingface.co/mlabonne/NeuralDaredevil-8B-abliterated">mlabonne/NeuralDaredevil-8B-abliterated</a>. To see if it fixed our abliterated version, I evaluated it on the same benchmarks:</p>
<p><a rel="nofollow" href="https://i.imgur.com/ChDwx4r.png"><img alt="" src="https://i.imgur.com/ChDwx4r.png"></a></p>
<p>We can see that this additional training allowed us to recover most of the performance drop due to abliteration. One area where the model doesn't improve is GSM8K, a math dataset, which could mean the orpo-dpo-mix-40k would benefit from more math samples.</p>
<p>The final model is an uncensored LLM with state-of-the-art performance in the 8B category. I recommend it as an improved version of Llama 3 8B Instruct when you don't need censorship. You can play with quantized versions like GGUF in LM Studio.</p>
<h2>
	<a rel="nofollow" href="#conclusion" id="conclusion">
		<span></span>
	</a>
	<span>
		Conclusion
	</span>
</h2>
<p>In this article, we introduced the concept of abliteration. This technique uses the model's activations on harmless and harmful prompts to calculate a refusal direction. It then uses this direction to modify the model's weights and ensure that we stop outputting refusals. This technique also demonstrates the fragility of safety fine-tuning and raises ethical considerations.</p>
<p>We applied abliteration to Daredevil-8B to uncensor it, which also degraded the model's performance. We then healed it using DPO to create the NeuralDaredevil-8B model, a fully uncensored and high-quality 8B LLM. Abliteration is not limited to removing alignment and should be seen as a form of fine-tuning without retraining. Indeed, it can creatively be applied to other goals, like FailSpy's <a href="https://huggingface.co/failspy/Llama-3-8B-Instruct-MopeyMule">MopeyMule</a>, which adopts a melancholic conversational style.</p>
<p>I hope you liked this article. If you want to see more follow me on&nbsp;<a href="https://huggingface.co/mlabonne/">Hugging Face</a>&nbsp;and Twitter&nbsp;<a rel="nofollow" href="https://twitter.com/maximelabonne">@maximelabonne</a>.</p>
<h2>
	<a rel="nofollow" href="#references" id="references">
		<span></span>
	</a>
	<span>
		References
	</span>
</h2>
<ul>
<li>FailSpy, "<a rel="nofollow" href="https://github.com/FailSpy/abliterator">abliterator library</a>," GitHub, 2024.</li>
<li>Andy Arditi, Oscar Obeso, Aaquib111, wesg, Neel Nanda, "<a rel="nofollow" href="https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction">Refusal in LLMs is mediated by a single direction</a>," Lesswrong, 2024.</li>
</ul>
<!-- HTML_TAG_END --></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Meta trains large language models at scale (258 pts)]]></title>
            <link>https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/</link>
            <guid>40664339</guid>
            <pubDate>Wed, 12 Jun 2024 23:35:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/">https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/</a>, See on <a href="https://news.ycombinator.com/item?id=40664339">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		<p><span>As we continue to focus our AI research and development on solving increasingly complex problems, one of the most significant and challenging shifts we‚Äôve experienced is the sheer scale of computation required to train large language models (LLMs).</span></p>
<p><span>Traditionally, our AI model training has involved a training massive number of models that required a comparatively smaller number of GPUs. This was the case for our recommendation models (e.g., our feed and ranking models) that would ingest vast amounts of information to make accurate recommendations that power most of our products.</span></p>
<p><img decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png 2500w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-1.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>With the advent of generative AI (GenAI), we‚Äôve seen a shift towards fewer jobs, but incredibly large ones. Supporting GenAI at scale has meant rethinking how our software, hardware, and network infrastructure come together.</span></p>
<h2><span>The challenges of large-scale model training</span></h2>
<p><img decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png 2500w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-2.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>As we increase the number of GPUs in a job, the likelihood of an interruption due to a hardware failure also increases. Also, all of these GPUs still need to communicate on the same high-speed fabric to perform optimally. This underscores the importance of four factors:</span></p>
<ul>
<li aria-level="1"><b>Hardware reliability</b><span>: Ensuring that our hardware is reliable is important. We need to minimize the chances of a hardware failure interrupting a training job. This involves rigorous testing and quality control measures, and automation to quickly detect and remediate issues.</span></li>
<li aria-level="1"><b>Fast recovery on failure</b><span>: Despite our best efforts, hardware failures can and do occur. When they do, we need to be able to recover quickly. This involves reducing re-scheduling overhead and fast training re-initialization.</span></li>
<li aria-level="1"><b>Efficient preservation of the training state</b><span>: In the event of a failure, we need to be able to pick up where we left off. This means we need to regularly checkpoint our training state and efficiently store and retrieve training data.</span></li>
<li aria-level="1"><b>Optimal connectivity between GPUs:</b><span> Large-scale model training involves transferring vast amounts of data between GPUs in a synchronized fashion. A slow data exchange between a subset of GPUs can compound and slow down the whole job. Solving this problem requires a robust and high-speed network infrastructure as well as efficient data transfer protocols and algorithms.&nbsp;</span></li>
</ul>
<h2><span>Innovating across the infrastructure stack</span></h2>
<p><span>Perfecting every layer of our infrastructure stack is important due to the demands of GenAI at scale. This has encompassed developments in a wide range of areas.</span></p>
<h3><span>Training software</span></h3>
<p><span>We enable researchers to use </span><a href="https://pytorch.org/blog/training-production-ai-models/"><span>PyTorch</span></a><span> and other new open source developments, facilitating extremely fast research-to-production development. This includes </span><span>developing new algorithms and techniques for efficient large-scale training and integrating new software tools and frameworks into our infrastructure.</span></p>
<h3><span>Scheduling</span></h3>
<p><span>Efficient scheduling helps ensure that our resources are used optimally. This involves </span><span>sophisticated algorithms that can allocate resources based on the needs of different jobs and dynamic scheduling to adapt to changing workloads.</span></p>
<h3><span>Hardware&nbsp;</span></h3>
<p><span>We need high-performance hardware to handle the computational demands of large-scale model training. Beyond size and scale, many hardware configurations and attributes need to be best optimized for GenAI. Given that hardware development times are traditionally long, we had to adapt existing hardware, and to this end we explored various dimensions including power, HBM capacity and speed, and I/O.&nbsp;</span></p>
<p><span>We also pivoted by modifying the </span><a href="https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/"><span>Grand Teton</span></a><span> platform that was developed using NVIDIA H100 GPUs, increased the TDP of the GPUs to 700W, and moved to HBM3 on the GPUs. Since we did not have time to change the cooling infrastructure, we had to remain in an air-cooled environment. The mechanical and thermal designs had to change to accommodate this, and that triggered a validation cycle to support a large-scale deployment.&nbsp;</span></p>
<p><span>All of these hardware-related changes were challenging because we had to find a solution that fit within the existing resource constraints, with a very small degree of freedom to change and meet a tight schedule.</span></p>
<h3><span>Data center deployment</span></h3>
<p><span>Once we‚Äôve chosen a GPU and system, the task of placing them in a data center for optimal usage of resources (power, cooling, networking, etc.) requires revisiting trade-offs made for other types of workloads. Data center power and cooling infrastructure cannot be changed quickly (or easily) and we had to find an optimal layout that allowed maximum compute capability within a data hall. This required relocating supporting services such as readers out of the data hall and packing as many GPU racks as possible to maximize the power and network capability for highest compute density with the largest network cluster.&nbsp;</span></p>
<h3><span>Reliability&nbsp;</span></h3>
<p><span>We need to plan for detection and remediation to minimize downtime during hardware failures. The number of failures scales with the size of the cluster, and having a job that spans the cluster makes it necessary to keep adequate spare capacity to restart the job as soon as possible. In addition, we monitor failures and can sometimes take preventive measures to mitigate downtime.&nbsp;</span></p>
<p><img loading="lazy" decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png 2500w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-5.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>Some of the most frequent failure modes we have observed are:</span></p>
<ul>
<li aria-level="1"><b>GPUs falling off:</b><span> In this case, GPUs are not detected by the host on PCIe. There are several reasons for this failure, but this failure mode is seen more in the early life and settles as the server ages.</span></li>
<li aria-level="1"><b>DRAM &amp; SRAM UCE:</b><span> Uncorrectable errors are common in memories, and we monitor and identify repeat offenders, track against thresholds, and initiate RMAs when error rates exceed vendor thresholds.</span></li>
<li aria-level="1"><b>HW network cable:</b><span> In the general category of unreachable servers, these failures are also seen most often in the early life of the server.&nbsp;</span></li>
</ul>
<h3><span>Network</span></h3>
<p><span>Large-scale model training involves transferring vast amounts of data quickly between GPUs. This requires robust and high-speed network infrastructure as well as efficient data transfer protocols and algorithms.&nbsp;</span></p>
<p><span>There are two leading choices in the industry that fit these requirements: RoCE and InfiniBand fabrics. Both of these options had tradeoffs. On the one hand, Meta had built RoCE clusters for the past four years, but the largest of those clusters only supported 4K GPUs. We needed significantly larger RoCE clusters. On the other hand, Meta had built research clusters with InfiniBand as </span><a href="https://ai.meta.com/blog/ai-rsc/"><span>large as 16K GPUs</span></a><span>. However, those clusters were </span><i><span>not</span></i><span> tightly integrated into Meta‚Äôs production environment, nor were they built for the latest generation of GPUs/networking. This made for a difficult decision of what fabric to build with.</span></p>
<p><span>So we decided to build both: </span><a href="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/"><span>two 24k clusters</span></a><span>, one with RoCE and another with InfiniBand. Our intent was to build and learn from the operational experience. These learnings will inform the future direction of GenAI fabrics. We optimized the RoCE cluster for quick build time, and the InfiniBand cluster for full-bisection bandwidth. We used both InfiniBand and RoCE clusters to train </span><a href="https://ai.meta.com/blog/meta-llama-3/"><span>Llama 3</span></a><span>, with the RoCE cluster used for training the largest model. Despite the underlying network technology differences between these clusters, we were able to tune both of them to provide equivalent performance for these large GenAI workloads</span></p>
<p><img loading="lazy" decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png 2500w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-3.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>We optimized three aspects of the overall stack to make network communication for GenAI models performant on both clusters:</span></p>
<ol>
<li><span>We assigned communication patterns resulting from different model, data and pipeline parallelisms to different layers of the network topology so that the network capabilities were effectively exploited.</span></li>
<li><span>We implemented collective communication patterns with network topology awareness so that they can be less latency-sensitive. We do this by changing the default implementation of collectives with custom algorithms such as recursive doubling or halving instead of conventional algorithms like rings.</span></li>
<li><span>Just like ranking jobs, GenAI jobs produce additional fat flows that make it hard to distribute traffic across all possible network paths. This required us to further invest in network load balancing and routing to achieve an optimal distribution of traffic across network resources.</span></li>
</ol>
<p><span>We spoke in depth about our </span><a href="https://atscaleconference.com/videos/scaling-roce-networks-for-ai-training/"><span>RoCE load-balancing techniques</span></a><span> at </span><a href="https://atscaleconference.com/videos/scaling-roce-networks-for-ai-training/"><span>Networking @Scale 2023</span></a><span>.</span></p>
<p><img loading="lazy" decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png 2500w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2024/06/Training-LLMs-at-Scale-image-4.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<h3><span>Storage</span></h3>
<p><span>We need efficient data-storage solutions to store the vast amounts of data used in model training. This involves investing in high-capacity and high-speed storage technologies and developing new data-storage solutions for specific workloads.</span></p>
<h2><span>Looking ahead</span></h2>
<p><span>In the next few years w</span><span>e will be working with hundreds of thousands of GPUs, handling even larger volumes of data, and dealing with longer distances and latencies. We‚Äôll be adopting new hardware technologies‚Äîincluding newer GPU architectures‚Äîand evolving our infrastructure.&nbsp;</span></p>
<p><span>These challenges will push us to innovate and adapt in ways we can‚Äôt fully predict yet. But one thing is certain: We are only at the beginning of this journey. As we continue to navigate the evolving landscape of AI, we remain committed to pushing the boundaries of what‚Äôs possible.</span></p>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gerald Sussman: Programming is (should be) fun (2022) [video] (119 pts)]]></title>
            <link>https://www.youtube.com/watch?v=2MYzvQ1v8Ww</link>
            <guid>40663704</guid>
            <pubDate>Wed, 12 Jun 2024 22:02:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=2MYzvQ1v8Ww">https://www.youtube.com/watch?v=2MYzvQ1v8Ww</a>, See on <a href="https://news.ycombinator.com/item?id=40663704">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Japan enacts law to curb Apple, Google's app dominance (298 pts)]]></title>
            <link>https://english.kyodonews.net/news/2024/06/bc2d7f45d456-japan-enacts-law-to-curb-apple-googles-app-dominance.html#google_vignette</link>
            <guid>40662176</guid>
            <pubDate>Wed, 12 Jun 2024 19:43:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://english.kyodonews.net/news/2024/06/bc2d7f45d456-japan-enacts-law-to-curb-apple-googles-app-dominance.html#google_vignette">https://english.kyodonews.net/news/2024/06/bc2d7f45d456-japan-enacts-law-to-curb-apple-googles-app-dominance.html#google_vignette</a>, See on <a href="https://news.ycombinator.com/item?id=40662176">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>Japan's parliament enacted Wednesday a law to promote competition in smartphone app stores by restricting tech giants Apple Inc. and Google LLC from limiting third-party companies from selling and operating apps on their platforms.</p>
<p>The law will prohibit the providers of Apple's iOS and Google's Android smartphone operating systems, app stores and payment platforms from preventing the sale of apps and services that directly compete with the native platforms' own.</p>
<p>The change is aimed at stopping the dominant players from gatekeeping and forcing them to engage in price competition with smaller challengers in hopes of benefiting consumers and promoting innovation.</p>
<p>The law will also prohibit the tech giants from giving priority to their own services in internet search results.</p>
<div><p><img src="https://img.kyodonews.net/english/public/images/posts/e12df7bdd2bc87d2b43aeaaaab5d93b4/photo_l.jpg" width="100%"></p><p><em>The House of Councillors convenes a plenary session in parliament in Tokyo on June 12, 2024. (Kyodo)</em></p>
</div>
<p>Violations of the new law will bring a penalty of 20 percent of the domestic revenue of the service found to have breached the rules. The fine can increase to 30 percent if the companies do not cease the anticompetitive practices.</p>
<p>The new penalty is more than triple the existing fine under the antimonopoly law, which imposes fines of 6 percent of revenue gained through services deemed to be using an anticompetitive edge.</p>
<p>The new law, expected to take effect by the end of 2025, follows a similar regulation introduced by the European Union in March.</p>
<p>The technology giants, which will be designated by the Fair Trade Commission, are to be required to submit regulatory compliance reports and will be monitored by the commission to ensure they are following the rules.</p>
<p>The legislation, which was approved by the House of Representatives in May, was enacted after being passed by the House of Councillors on Wednesday.</p>
<hr>
<p><em>Related coverage:</em></p>
<p><a title="Kyodo News Plus" href="https://english.kyodonews.net/news/2024/04/6b04c1e9d2b4-japan-cabinet-oks-bill-to-challenge-apple-google-app-store-duopoly.html" target="_blank" rel="noopener"><span><strong>Japan Cabinet OKs bill to challenge Apple-Google app store duopoly</strong></span></a></p>
<p><a title="Kyodo News Plus" href="https://english.kyodonews.net/news/2024/04/ba9866f5a54f-meta-sued-over-investment-ads-with-fake-celebrity-endorsements.html" target="_blank" rel="noopener"><span><strong>Meta sued in Japan for investment ads with fake celebrity endorsement</strong></span></a></p>
<hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ChromeOS will soon be developed on large portions of the Android stack (175 pts)]]></title>
            <link>https://blog.chromium.org/2024/06/building-faster-smarter-chromebook.html</link>
            <guid>40661703</guid>
            <pubDate>Wed, 12 Jun 2024 19:02:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.chromium.org/2024/06/building-faster-smarter-chromebook.html">https://blog.chromium.org/2024/06/building-faster-smarter-chromebook.html</a>, See on <a href="https://news.ycombinator.com/item?id=40661703">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-version="1" id="header">
<div>
<p><a href="https://blog.chromium.org/">
<img alt="Chromium Blog" height="50" src="https://1.bp.blogspot.com/-vkF7AFJOwBk/VkQxeAGi1mI/AAAAAAAARYo/57denvsQ8zA/s1600-r/logo_chromium.png">
</a></p><a href="https://blog.chromium.org/">
<h2>
            Chromium Blog
          </h2>
</a>
</div>
<p>
News and developments from the open source browser project
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Swift compiler is slow due to how types are inferred (203 pts)]]></title>
            <link>https://danielchasehooper.com/posts/why-swift-is-slow/</link>
            <guid>40661001</guid>
            <pubDate>Wed, 12 Jun 2024 17:59:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danielchasehooper.com/posts/why-swift-is-slow/">https://danielchasehooper.com/posts/why-swift-is-slow/</a>, See on <a href="https://news.ycombinator.com/item?id=40661001">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The Swift compiler is notoriously slow due to how types are inferred<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. Every June I hope that Apple will announce that they fixed it; sadly this is not that year.</p><p>Here‚Äôs an explanation by the creator of Swift, Chris Lattner (From his <a href="https://www.youtube.com/watch?v=9ag0fPMmYPQ&amp;t=373s" target="_blank" rel="noopener">Mojo talk</a>):</p><blockquote><p>My experience with Swift is we tried to make a really fancy bi-directional Hindley-Milner type checker and it‚Äôs really great because you can have very beautiful minimal syntax but the problem is that A) compile times are really bad (particularly if you have complicated expressions) and B) the error messages are awful because now you have global constraint systems and when something goes wrong you have to infer what happened and the user can‚Äôt know that something over there made it so something over here can‚Äôt type check. In my experience it sounds great but it doesn‚Äôt work super well.</p></blockquote><p>Let me explain what he means with an example:</p><div><pre tabindex="0"><code data-lang="swift"><span><span><span>enum</span> <span>ThreatLevel</span> <span>{</span>
</span></span><span><span>    <span>case</span> <span>red</span>
</span></span><span><span>    <span>case</span> <span>midnight</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>enum</span> <span>KeyTime</span> <span>{</span>
</span></span><span><span>    <span>case</span> <span>midnight</span>
</span></span><span><span>    <span>case</span> <span>midday</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>func</span> <span>setThreatLevel</span><span>(</span><span>_</span> <span>level</span><span>:</span> <span>ThreatLevel</span><span>)</span> <span>{...}</span>
</span></span><span><span>
</span></span><span><span><span>setThreatLevel</span><span>(.</span><span>midnight</span><span>)</span>
</span></span></code></pre></div><p>The <code>.midnight</code> on the last line could represent <code>ThreatLevel.midnight</code> or <code>KeyTime.midnight</code>. The Swift compiler has to use the surrounding context of <code>setThreatLevel()</code>, which has the type <code>(ThreatLevel)-&gt;Void</code>, to infer that we mean <code>ThreatLevel.midnight</code>. After the Swift compiler parses code into an abstract syntax tree, child nodes influence their parent‚Äôs type <em>and</em> parent nodes influence their children‚Äôs types (that‚Äôs what Chris means by ‚Äúbi-directional‚Äù). Compare this to the Zig language, in which types are determined without looking at the surrounding code.</p><p>This approach becomes a problem when expressions contain many elements that each need their types inferred, with each affecting the others. This often occurs due to Swift‚Äôs operator overloading, and the <a href="https://developer.apple.com/documentation/swift/initialization-with-literals" target="_blank" rel="noopener">ExpressibleBy protocols</a>. Every literal (string, number, boolean, dictionary, array) and every operator (* / + - etc) multiply the combinations the type checker must consider.</p><p>Here‚Äôs an example:</p><div><pre tabindex="0"><code data-lang="swift"><span><span><span>let</span> <span>address</span> <span>=</span> <span>"127.0.0.1"</span>
</span></span><span><span><span>let</span> <span>username</span> <span>=</span> <span>"steve"</span>
</span></span><span><span><span>let</span> <span>password</span> <span>=</span> <span>"1234"</span>
</span></span><span><span><span>let</span> <span>channel</span> <span>=</span> <span>11</span>
</span></span><span><span>
</span></span><span><span><span>let</span> <span>url</span> <span>=</span> <span>"http://"</span> <span>+</span> <span>username</span> 
</span></span><span><span>            <span>+</span> <span>":"</span> <span>+</span> <span>password</span> 
</span></span><span><span>            <span>+</span> <span>"@"</span> <span>+</span> <span>address</span> 
</span></span><span><span>            <span>+</span> <span>"/api/"</span> <span>+</span> <span>channel</span> 
</span></span><span><span>            <span>+</span> <span>"/picture"</span>
</span></span><span><span>
</span></span><span><span><span>print</span><span>(</span><span>url</span><span>)</span>
</span></span></code></pre></div><p><code>swiftc</code> spends 42 seconds on these 12 lines on an M1 Pro<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>, only to spit out the notorious <code>error: the compiler is unable to type-check this expression in reasonable time; try breaking up the expression into distinct sub-expressions</code>. In the same amount of time, clang can perform a clean build of my 59,000 line C project <em>38 times</em>.</p><p>The issue is caused by using the <code>+</code> operator with the <code>channel</code> Int and a String literal. Thanks to the standard library‚Äôs 17 overloads of <code>+</code> and 9 types adopting the <code>ExpressibleByStringLiteral</code> Protocol, the swift compiler can‚Äôt rule out that there <em>might</em> be a combination of types and operators that make the expression valid, so it has to try them all. Just considering that the five string literals could be one of the possible nine types results in 59,049 combinations, but I suspect that‚Äôs a lower bound, since it doesn‚Äôt consider the many overloads of <code>+</code>. It gives up before getting through them all.</p><p>You can fix the code by converting <code>channel</code> to String:</p><div><pre tabindex="0"><code data-lang="swift"><span><span><span>let</span> <span>url</span> <span>=</span> <span>"http://"</span> <span>+</span> <span>username</span> 
</span></span><span><span>            <span>+</span> <span>":"</span> <span>+</span> <span>password</span> 
</span></span><span><span>            <span>+</span> <span>"@"</span> <span>+</span> <span>address</span> 
</span></span><span><span>            <span>+</span> <span>"/api/"</span> <span>+</span> <span>String</span><span>(</span><span>channel</span><span>)</span> 
</span></span><span><span>            <span>+</span> <span>"/picture"</span>
</span></span></code></pre></div><p>This now successfully compiles in 0.19 seconds!</p><p>Maybe you think strings are complicated or something, so here‚Äôs an example that is just math:</p><div><pre tabindex="0"><code data-lang="swift"><span><span><span>let</span> <span>offset</span><span>:</span> <span>Double</span> <span>=</span> <span>5.0</span><span>;</span>
</span></span><span><span><span>let</span> <span>index</span><span>:</span> <span>Int</span> <span>=</span> <span>10</span><span>;</span>
</span></span><span><span><span>let</span> <span>angle</span> <span>=</span> <span>(</span><span>180.0</span> <span>-</span> <span>offset</span> <span>+</span> <span>index</span> <span>*</span> <span>5.0</span><span>)</span> <span>*</span> <span>.</span><span>pi</span> <span>/</span> <span>180</span><span>;</span>
</span></span></code></pre></div><p>Again, we get <code>error: the compiler is unable to type-check this expression in reasonable time; try breaking up the expression into distinct sub-expressions</code>, this time after ‚Äúonly‚Äù 8 seconds. The problem is due to <code>index * 5.0</code>, i.e. an int multiplied by a double. Even <a href="https://youtu.be/-eCwBwTbjAI?si=rQ5tHNcBRkmaFV8-&amp;t=1000" target="_blank" rel="noopener">toy compilers</a> handle equivalent code quickly, thanks to a context-free type system.</p><p>Both examples are slow because they‚Äôre invalid swift and the type checker falls out of the fast path in order to confirm all possible type combinations are invalid. You might think it‚Äôs ok for invalid code to take a long time to compile. For me, 42 seconds to produce an ‚ÄúI give up‚Äù message is unacceptable. However, there are valid lines of swift that take a long time to compile too. Send me your slow lines (found using <code>-Xfrontend -debug-time-function-bodies</code>) and I‚Äôll add it to this post.</p><p>Swift has come a long way from version 1, but on its 10th birthday it can still be slow. Unfortunately this can‚Äôt be completely fixed by optimizing the current approach. It requires a different approach.</p><p>Here‚Äôs what I‚Äôd do:</p><ol><li>Add a flag to <code>swiftc</code> that makes it infer types using only an expression‚Äôs child AST nodes while ignoring the parent AST node. The flag would also disable the <code>ExpressibleBy</code> protocols, which by definition get their type from their context.</li><li>Make a feature that adds type annotations, casts, and enum names to existing code where necessary to compile with the new type checker</li><li>Update all sample code to compile with the flag</li></ol><p>This might be a reasonable stopping point: teams that care about compile times and good error messages could use the flag, and everyone else doesn‚Äôt have to. It could go further though:</p><ol start="4"><li>Enable the flag by default for new Xcode projects</li><li>Deprecate the old type inference approach</li></ol><p>With this new approach, you‚Äôd have to add type annotations in some places. I‚Äôm ok with that. As a result, we‚Äôd get faster compilation times and clearer error messages, but the extra verbosity might be too much for the swift community to swallow.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The GJK Algorithm: A weird and beautiful way to do a simple thing (391 pts)]]></title>
            <link>https://computerwebsite.net/writing/gjk</link>
            <guid>40660761</guid>
            <pubDate>Wed, 12 Jun 2024 17:35:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://computerwebsite.net/writing/gjk">https://computerwebsite.net/writing/gjk</a>, See on <a href="https://news.ycombinator.com/item?id=40660761">Hacker News</a></p>
<div id="readability-page-1" class="page">
    

    
    <hr>
    <p>The GJK algorithm is a weird way to do a simple thing.
</p><p>We have shape A and shape B, and we'd like to determine if they overlap.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609145754.png" width="500">
</p><p>A shape is a set of infinitely many points. If there exists any point that's a member of both sets, then the shapes overlap.
</p><p>Alternatively, if there exists point <em>a</em> in set A and a point <em>b</em> in set B such that:
</p><p><em>a</em> - <em>b</em> = <strong>0</strong>
</p><p>Then an intersection exists. Note that the <strong>0</strong> here represents a point itself: the origin.
</p><p>To see what this means intuitively, we can take a shift in perspective. Instead of dealing with specific points, let's try subtracting <em>every</em> point within one shape from <em>every</em> point within another, and plotting where they all end up:
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240604163159.png" width="500">
</p><p>We've ended up with a new set that represents the "difference" between A and B. Since it contains the origin, we know that there must be at least one pair of points whose difference is <strong>0</strong>.
</p><p>That's exactly what the GJK algorithm takes advantage of. Instead of directly checking if the sets have an intersection, we subtract them and see if the new set contains the origin.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240604163335.png" width="500">
</p><p>Combining sets like this is known as taking the Minkowski difference. You may see this denoted '‚äñ' to separate it from the normal notion of a difference.
</p><p>More formally, it's defined as:
</p><p>A ‚äñ B = {a - b | a ‚àä A, b ‚àä B}
</p><p>At first, it doesn't seem like we've made any progress, only rephrased the problem. How are we supposed to subtract infinitely many points? Well, let's find the bare minimum that we <em>need to know</em> about the set A ‚äñ B to see if it contains the origin.
</p><p>In 2 dimensions, if a convex set contains the origin, it means we're able to draw a triangle between 3 points on the boundary, which contains the origin. This makes the problem even easier. If we know that A ‚äñ B is convex, we only need to deal with the boundary.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240604164614.png" width="500">
</p><p>Luckily, it's easy to ensure A ‚äñ B will be convex.
</p><p>The Minkowski difference of any two convex sets is also convex. So, we just need to break down A and B into convex components, which we can work with individually. On a computer, most shapes are already constructed out of convex components, so this process is often extremely easy.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240604182725.png" width="500">
</p><p>By the way, even though I'm explaining in 2D, our problem is fundamentally the same in any dimension. Here, we're looking for a bounding triangle on A ‚äñ B. But, more generally, we're looking for a "simplex" in our dimension. An n-dimensional simplex is just the simplest polytope that can enclose an n-dimensional region. For example, a 3D simplex is a tetrahedron, as it has the fewest vertices needed to bound a volume.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240604175314.png" width="500">
</p><p><em></em>With these tools, let's restate the problem.
</p><ul><li>We want to determine if any simplex within A ‚äñ B will enclose the origin.
</li></ul><ul><li>We don't want to have to compute the entire boundary of A ‚äñ B. We only need to find some 3 points on A and some 3 points on B that will map to the vertices of our bounding simplex.
</li></ul><ul><li>There's no one solution, as infinitely many bounding simplexes might exist. We just need to find any 6 points on A and B that work, or prove that we can't find any.
</li></ul><p>So, from now on, whenever I say "shape", I'm speaking only about the boundary of a given set. We've thrown away all the area, as it's no longer relevant to the problem.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240604192323.png" width="500">
</p><p>Things would be much easier if we had some system to identify points on our shape. The support function (denoted S here) does just that. It takes in a vector, and outputs the furthest point on the shape in that direction. On a convex shape, every boundary point is identifiable using the support function of some direction.
</p><p>In other words, if we rotate a vector <em>d</em> 360 degrees around the origin, S(<em>d</em>) will hit every point. So, finding N points on a shape corresponds to finding N directions.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609121840.png" width="500">
</p><p>The support function is easy to formally define. It's just the point that has the highest dot product of <em>d</em>. 
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609125706.png" width="500">
</p><p>Remember how I previously said we'd need to find 3 points on A, and 3 points on B? Well, we really only need to find 3 points in total. That's because of the following useful property of the Minkowski difference:
</p><p>The support of direction <em>d</em> in A ‚äñ B = the support of <em>d</em> in A minus the support of <em>-d</em> in B.
</p><p>Basically, if we sweep across the borders of A and B in opposite directions by rotating <em>d</em>, we'll hit every boundary point on A ‚äñ B. This massively narrows down how many possible points we need to search, as our points on A and B will always be dependent on each other. I know the diagrams are getting a bit confusing now, so bear with me.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240603131831.png" width="500">
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240610135637.png" width="500">
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609122623.png" width="500">
</p><p>Put differently, most combinations of points on A and B don't matter, as they won't land us on the border of A ‚äñ B. We only need to consider the case where they're in opposite directions. (There's some easy intuition for why this is the case: I'll leave that as an exercise)
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609123441.png" width="500">
</p><p>Alright, the stage has been set for GJK. The goal of the GJK algorithm is to find a simplex on A ‚äñ B that contains the origin (or show that none exists), while doing as few operations as possible.
</p><p>Let's appreciate how incredible such an algorithm would be: given <em>any</em> convex shape, as long as it has some defined notion of support points, we can detect collisions. That's unbelievably powerful. 
</p><p>Here's how it works:
</p><p>1) First, find a point on A ‚äñ B using a random direction <em>d</em>, which we'll call <em>p</em>. This is just S(<em>d</em>) - S(-<em>d</em>). It's the first point on our simplex.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240603135933.png" width="500">
</p><p>2) Then, let's take the dot product of <em>d</em> and <em>p</em>. If it's positive, we keep going on with the algorithm. If it's negative, that means <em>d</em> and <em>p</em> point in opposite directions. In that case, we terminate the algorithm, as there's no way for B and A to overlap.
</p><p>But why?
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240603151836.png" width="500">
</p><p>It's more clear when we rotate our view to be in line with <em>d</em>. Point <em>a</em> is S(<em>d</em>), and point <em>b</em> is S(-<em>d</em>). Now, point <em>a</em> is the "top" of shape A, and point <em>b</em> is the "bottom" of B. Remember that <em>p</em> = <em>a - b</em>. So, if we project <em>p</em> onto <em>d</em> by taking the dot product, and it's negative, a gap exists between A and B.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609131123.png" width="500">
</p><p>3) Ok, great. We've got one point of our simplex. Now, we need another. From our point <em>p</em>, let's shoot in the direction of the origin. Take the support of that vector <em>-d</em>, and run our check from step 2 on it.
</p><p>If you think about it, we're just finding the point most in the direction of the origin from <em>p</em>. If our shape encloses the origin, then our new point will have to be on the opposite side of it.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240610080829.png" width="500">
</p><p>Remember how we just performed the check from step 2? Well, there's another way to think about it. We're just checking to see if we've crossed the origin. If the origin is within our shape, we'll have to cross it when we take support points in opposite directions.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609152836.png" width="500">
</p><p>4) Now, we're ready to complete the simplex. Let's take the vector perpendicular to our first 2 points in the direction of the origin. The support of that vector is the 3rd point of our simplex.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609133818.png" width="500">
</p><p>I mean, think about it. We've effectively divided the space in half with our first line. We know the origin can't be anywhere on one side of our line, so we continue our search in the opposite direction. That's the efficiency of GJK: we'll keep dividing the space in half, until we find the point.
</p><p>Oh, and as usual, we can run our check from step 2 to see if we've proven that the shapes do not overlap.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609134721.png" width="500">
</p><p>5) Now, let's see if our simplex contains the origin. If it does, we return true. Otherwise, we continue on.
</p><p>To find if the origin is within our triangle, first, let's break down the area surrounding it into the three infinite regions depicted below.
</p><p>The overlapping red, green, and blue regions each divide the space in half: an inside region, and an outside region.
</p><p>We've already eliminated the possibility of the origin being in the red region. So, we just have to check whether it's outside either of the remaining 2 lines.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240610140613.png" width="500">
</p><p>Luckily, we already know how to do this. Just take the dot product perpendicular to a given line to see if the origin is beyond it. If so, we know it must be in the white region.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240610140732.png" width="500">
</p><p>6) Now, it's time to iterate. We repeat from step 4, this time from the line on our simplex closest to the origin. We update our simplex using this new support point, and check if the new simplex contains the origin, as we did in step 5. This time, it does! Our algorithm returns <em>true</em>. If it doesn't, we keep doing this iterative process until it does, or one of our checks fails.
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609142721.png" width="500">
</p><p><img src="https://computerwebsite.net/writing/images/Pasted%20image%2020240609143010.png" width="500">
</p><p>And, that's it. That's the essence of the GJK algorithm. 
</p><p>I've glossed over a few implementation details, but you now have all the intuition you need to develop an in-depth understanding.
</p><p>Personally, I find this algorithm pretty because it's such a tidy example of what makes mathematics so powerful. Through a bunch of subtle shifts in perspective, a complicated problem becomes obvious.
</p><p>There's probably a few things incorrect with what I said. So, take it with the appropriate amount of salt for a high school sophomore's explanation of anything math related.</p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diffractive Chocolate (142 pts)]]></title>
            <link>https://wp.optics.arizona.edu/oscoutreach/diffractive-chocolate/</link>
            <guid>40660689</guid>
            <pubDate>Wed, 12 Jun 2024 17:27:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wp.optics.arizona.edu/oscoutreach/diffractive-chocolate/">https://wp.optics.arizona.edu/oscoutreach/diffractive-chocolate/</a>, See on <a href="https://news.ycombinator.com/item?id=40660689">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="wrap" role="main">
		
		
	<article id="post-1360">
	
				
		
		
				
		<div>
			
<p><strong>Overview</strong>: Take your science skills into the kitchen. Turn ordinary chocolate into an edible optics demo that shows how diffraction works.</p>



<p><strong>Supplies:</strong>&nbsp;Stove, double boiler, heat proof spatula, candy thermometer, diffraction sheets, silicon molds (optional), chocolate bars or melting chocolate</p>



<p><strong>Objectives:</strong> Diffraction is the bending of light. We do not need to look through a diffraction sheet to see the bending. Light can bounce off of fine grooved surfaces to show us how each wavelength of light refracts off of the surface.</p>



<hr>



<p><strong>Setup:</strong></p>



<ul>
<li>If using molds, cut the diffraction sheets into pieces the size of the bottom of the molds. Place the diffraction pieces, diffraction side up, in the bottom of the mold</li>



<li>Divide your chocolate into thirds, 2/3 are for melting, 1/3 is for the tempering</li>



<li>Different chocolates have different tempering temperature, these temperatures are critical for proper tempering</li>
</ul>


<div>
<figure><img fetchpriority="high" decoding="async" width="683" height="1024" src="https://wp.optics.arizona.edu/oscoutreach/wp-content/uploads/sites/75/2024/02/Chocolate-Melting-Points-683x1024.jpg" alt="" srcset="https://wp.optics.arizona.edu/oscoutreach/wp-content/uploads/sites/75/2024/02/Chocolate-Melting-Points-683x1024.jpg 683w, https://wp.optics.arizona.edu/oscoutreach/wp-content/uploads/sites/75/2024/02/Chocolate-Melting-Points-200x300.jpg 200w, https://wp.optics.arizona.edu/oscoutreach/wp-content/uploads/sites/75/2024/02/Chocolate-Melting-Points-768x1152.jpg 768w, https://wp.optics.arizona.edu/oscoutreach/wp-content/uploads/sites/75/2024/02/Chocolate-Melting-Points-1024x1536.jpg 1024w, https://wp.optics.arizona.edu/oscoutreach/wp-content/uploads/sites/75/2024/02/Chocolate-Melting-Points.jpg 1200w" sizes="(max-width: 683px) 100vw, 683px"></figure></div>






<ul>
<li>Follow this <a href="https://www.youtube.com/watch?v=SoTi0tM4yQ8">video</a> (starting at minute 20 for chocolate) on how to temper chocolate and create the diffraction pieces</li>



<li>If using molds, chocolate can be poured into molds quite easily</li>
</ul>



<p><strong>How to run the demo:</strong> </p>



<ul>
<li>Pass out the chocolate and explain what they see.</li>
</ul>



<div>
<figure><img decoding="async" width="266" height="472" src="https://wp.optics.arizona.edu/oscoutreach/wp-content/uploads/sites/75/2024/02/diffraction-chocolate.gif" alt=""></figure></div>



<p><strong>Try this:</strong></p>



<p>Follow the instructions in the <a href="https://www.youtube.com/watch?v=SoTi0tM4yQ8">video</a> (starting at minute 7:40) to make diffractive candy</p>







<hr>



<p><strong>What‚Äôs Happening?</strong></p>



<p>White light can be separated into all seven major colors of the complete spectrum or rainbow by using a diffraction grating. The grating separates light into colors as the light passes through the many fine slits of the grating. Each color travels at a different speed and therefore has a different angle of refraction when it hits the grating. Chocolate makes a reflection gratings. Along with chocolate, a compact disc also makes a good reflection grating. When light passes through a grating it is called a transmission grating. A transmission grating is what is used to make the diffractive chocolate. Diffractive candy is a transmission grating.</p>



<hr>



<p><strong>Learn more:</strong>&nbsp;(external links)</p>
			<!-- <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
			xmlns:dc="http://purl.org/dc/elements/1.1/"
			xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
		<rdf:Description rdf:about="https://wp.optics.arizona.edu/oscoutreach/diffractive-chocolate/"
    dc:identifier="https://wp.optics.arizona.edu/oscoutreach/diffractive-chocolate/"
    dc:title="Diffractive Chocolate"
    trackback:ping="https://wp.optics.arizona.edu/oscoutreach/diffractive-chocolate/trackback/" />
</rdf:RDF> -->
						
		</div>
		
		

	</article>
			
		

		
		</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Are you still using your Vision Pro? (137 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40660270</link>
            <guid>40660270</guid>
            <pubDate>Wed, 12 Jun 2024 16:54:21 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40660270">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="40663619"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40663619" href="https://news.ycombinator.com/vote?id=40663619&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Yes, almost every day. I travel regularly and I love having a big, crisp, private display every where I am: Hotel, plane, train. Makes me so much more productive.</p><p>Consuming content is great of course but the AVP has changed my content creation: I take way more panoramas and now spatial photos (Spatialify on iOS works well). I also bought an Insta X4 360 camera which, while a far cry from Apple's immersive content in resolution, can still be a really nice way to relive memories.</p><p>More content: Last year I started 3d scanning (using Scaniverse) sculptures and other art / items that catch my eye during my travels. The AVP makes it really easy to import and place them in my environment. When I'm working I'll often place a favorite sculpture next to me for company &amp; as a reminder of a trip I took.</p><p>Finally, even after 4 months of use, it's still really fun and, from a tech perspective, astounding in terms of image quality, stability, 3D placement, integration in environment, etc. I love it and I can't wait for this tech to get better and better.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661602"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661602" href="https://news.ycombinator.com/vote?id=40661602&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Returned in within the first month. Couldn‚Äôt use it for work because of the shared Apple ID requirements. Hand tracking was too laggy for games. That left movies/tv as the only winning feature, and I prefer to watch socially. Was pretty bummed tbh, I thought it would be much cooler.</p><p>Edit: there was something really cool actually, that I think doesn‚Äôt get talked about enough. Pooping on Yosemite. Peak futurism.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661643"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40661643" href="https://news.ycombinator.com/vote?id=40661643&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Hand tracking is limited to 30fps in VisionOS 1.0, but in 2.0 it runs at the same framerate as the display, so it should be vastly improved.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662685"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40662685" href="https://news.ycombinator.com/vote?id=40662685&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>30fps is 33ms per frame. I'd expect that to be fine for most free-space gestures (for musical instruments you generally want &lt;10ms, but that's most noticeable with discrete impulsive events like hitting a drum pad, and even then is pretty manageable).</p><p>Is the frame rate really the limiting factor here, or something algorithmic in the tracking (like smoothing out noise)?</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40661562"><td></td></tr>
                <tr id="40662492"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40662492" href="https://news.ycombinator.com/vote?id=40662492&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt;Also when crankin‚Äô my hog (just being honest)</p><p>Honestly that seems like the primary use case whenever the topic is mentioned. I'm sure there are cool innovative uses for them, but porn is always going to be at the forefront.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663302"><td></td></tr>
                  <tr id="40661716"><td></td></tr>
                <tr id="40661881"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40661881" href="https://news.ycombinator.com/vote?id=40661881&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Neither a Vision Pro owner nor much of a hog cranker, so this comment may be something of a 'premature ejaculation', but I don't think there's any prospect of native applications designed to aid manipulating oneself to issue passing App Review.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662401"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40662401" href="https://news.ycombinator.com/vote?id=40662401&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>I own a AVP, hog cranking is its killer use case. Porn on the AVP exists today whether Apple likes it or not.</p><p>Passing app review doesnt matter, there are a number of subscription websites (sexlikereal, wankz, czechvr etc) which offer UHD uncompressed 8K MP4 videos for download. It is entirely possible (and quite easy) to download these UHD videos to a Mac, mount the folder containing the goon material on the Mac as a drive on the Vision Pro (local network drive), and stream the video to the AVP using a 3rd party 3d video player; Moonplayer is the pick of the bunch at the moment.</p><p>I must say the experience is pretty damn good. I can see people getting addicted to it an unhealthy way. The sites mentioned above are only producing 8K at the moment, I think the AVP can handle more pixels, and there are new cameras coming on to the market (<a href="https://x.com/Blackmagic_News/status/1800273164867658228" rel="nofollow">https://x.com/Blackmagic_News/status/1800273164867658228</a>) which will really crank things up a notch.</p><p>There is a tremedous market oppurtunity available here, its niche at the moment, but once you experience good quality VR porn its hard to go back to the flat stuff.</p><p>I should probably get a GF.... sigh.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662854"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40662854" href="https://news.ycombinator.com/vote?id=40662854&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Truly a golden era for fast-forwarding through videos of dead-eyed men and women rutting on camera. (Thank you ‚Äì I genuinely learned a lot from your reply.)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663371"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_40663371" href="https://news.ycombinator.com/vote?id=40663371&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>What a boner of a comment. We get it, George, you don't watch pornography. Well done on managing your personal brand I guess?</p><p>FYI there's nothing inherently wrong about porn or sex work! It's great! A lot of people like it, you should try it sometime!</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40663595"><td></td></tr>
                  <tr id="40661970"><td></td></tr>
                  <tr id="40663552"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40663552" href="https://news.ycombinator.com/vote?id=40663552&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Curious question, what would you think if they told you some sessions are uploaded to Apple for them to evaluate the use of the product and your security?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40663667"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40663667" href="https://news.ycombinator.com/vote?id=40663667&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>If you could watch NBA games from court side seat in VR, they would be flying off the shelves. I'm fairly sure nothing technical is preventing this from happening. Most likely it is blocked by existing media contracts. I.e "non-technical" reasons.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663840"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40663840" href="https://news.ycombinator.com/vote?id=40663840&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>How would this even work? The only plausible way I could think it that there's a 360 degree camera mounted court side and you can control the view by turning your head. But it would be entirely non-immersive because moving your upper body in any walk would immediately break the illusion. If the idea is just a court side camera I don't see what benefit the big headset is adding?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40662595"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40662595" href="https://news.ycombinator.com/vote?id=40662595&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Until it gets multiple account support, it‚Äôs not even an option. I could buy one, but I‚Äôm not going to buy three. Nor can I use it for work, for the same reason.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661563"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661563" href="https://news.ycombinator.com/vote?id=40661563&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>I still use mine every couple of weeks for movies that my family doesn‚Äôt want to watch with me.</p><p>I also use it as my hotel setup when I travel for work. It‚Äôs great having a full size monitor wherever I need. I‚Äôm excited for the coming improvements with vision os 2.0.</p><p>I‚Äôm still not comfortable using it in public, though. It feels ostentatious, but I will try it the next time I fly with the family. Having people I trust around me will make me more willing to go immersive while traveling.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40660484"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660484" href="https://news.ycombinator.com/vote?id=40660484&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Yes, its very much a part of my work setup. It transformed working so that for the first time I have a good working setup everywhere.</p><p>Its also my preferred place to consume cinema. I have a short throw projector and sound system. I prefer the AVP. The image is so crisp and the 3D is so good, that its better than a decent home movie theater.</p><p>Its my preferred place to watch F1.</p><p>Environments genuinely soothe me.</p><p>Breathe works on this platform, it annoys me on the watch.</p><p>I would watch every sport and documentary in spatial if the was a thing. The tastes have me excited for the future.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40660656"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40660656" href="https://news.ycombinator.com/vote?id=40660656&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>I'd love to hear more about how you use it for your work set-up. The other things you've mentioned all have me interested in getting one but I've never been able to imagine how I'd use it for work.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662996"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40662996" href="https://news.ycombinator.com/vote?id=40662996&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt; I'd love to hear more about how you use it for your work set-up.</p><p>Also very interested</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40661597"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40661597" href="https://news.ycombinator.com/vote?id=40661597&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>With F1 TV does that mean you can just place different streams and screens anywhere? That does sound quite neat in principle.</p><p>Hell if you had a feed for car positions one could make a virtual model of the track and watch it top down.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662815"><td></td></tr>
                <tr id="40663216"><td></td></tr>
                              <tr id="40661028"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661028" href="https://news.ycombinator.com/vote?id=40661028&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>No, I returned it. Kinda regretted it, so I picked up a Meta Quest 3 to scratch the VR itch. I use it for watching youtube while I do dishes and laundry. Sometimes I play golf for relaxation or beat saber. I really wish there was an easy way to put the older 3d blu-ray movies on the device to watch movies, because immersive movies are really the next level for entertainment.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663665"><td></td></tr>
                <tr id="40663720"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40663720" href="https://news.ycombinator.com/vote?id=40663720&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>I like augmented reality stuff, 3d games and immersive videos. I can get all that stuff done on the Quest 3 for a 1/10th of the cost. It's good enough and has a larger ecosystem for games. And strangely, I think controllers are better in a lot of cases than hand tracking.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40661619"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40661619" href="https://news.ycombinator.com/vote?id=40661619&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt;  I use it for watching youtube while I do dishes and laundry.</p><p>How do you interact with it, ooi? Hand-tracking, voice, ... ? (Can't really touch the headset or controller with wet and/or busy hands).</p><p>&gt;  I really wish there was an easy way to put the older 3d blu-ray movies on the device to watch movies,</p><p>Oh there is, it just involves the high seas route. Also most movie-players on these devices suck a bit, i.e. they are either "too immersive" and make it difficult to use pass-through, or too useless like the builtin one.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662079"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40662079" href="https://news.ycombinator.com/vote?id=40662079&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Skybox VR is great for watching movies on The Quest line. 2D or 3D.</p><p>There are actually some tools that exist to convert blu-rays to SBS 3D if you‚Äôre looking to go legit.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663681"><td></td></tr>
                  <tr id="40662161"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40662161" href="https://news.ycombinator.com/vote?id=40662161&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>You can use your hands just fine. You just grab a screen on the edge and move it to where you want it to be. No need to touch the device or a controller.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661696"><td></td></tr>
                  <tr id="40662562"><td></td></tr>
                  <tr id="40660381"><td></td></tr>
                <tr id="40662600"><td></td></tr>
                <tr id="40662614"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40662614" href="https://news.ycombinator.com/vote?id=40662614&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Well they don't sell them until 12th of July here so I'm going to go and play with one first.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40660918"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660918" href="https://news.ycombinator.com/vote?id=40660918&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Yep! A mix of work and play. When I'm going to be spending a LOT of time coding I still prefer my ultra-wide physical monitor (probably my own fault for preferring such a small font size), but for things like handling tickets, emails, quick code changes in terminal, etc. it is pretty great. I ended up using a third-party head strap [0] to greatly improve comfort, but I know other folks use the stock strap for long sessions with no issues. YMMV :)</p><p>[0] <a href="https://amzn.to/4ehZpnL" rel="nofollow">https://amzn.to/4ehZpnL</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661644"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661644" href="https://news.ycombinator.com/vote?id=40661644&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Haven't touched it in months. Thought about bringing it on a recent international flight, but we had toddler in tow and I didn't want to lug it around for a couple of weeks.</p><p>Excited about the updates to the OS, my goal was always to use it for work, as an alternative posture mode, but couldn't get used to it at first.</p><p>Are there any other killer apps other than movies these days?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663002"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40663002" href="https://news.ycombinator.com/vote?id=40663002&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt; we had toddler in tow and I didn't want to lug it around for a couple of weeks.</p><p>Weird. I really enjoyed our toddlers</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40661549"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661549" href="https://news.ycombinator.com/vote?id=40661549&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>I hadnt been for a month or so (picked it up at launch), but the new beta release of Vision OS 2 looks feels like a massive quality of life improvement. Foveation and implied resolution seem to be massively improved, framerates are much higher too. The new Bora Bora (day + night) environment is fabulous and has moments of "am I actually there", the prior exisiting environments have all had a decent bump in apparent quality too. Apple are slow rolling content and experiences, but I can see this working out in the long run.
P.S. if anyone is still hung up on comfort, the "open face" headstraps do wonders, very much like wearing glasses.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661594"><td></td></tr>
                <tr id="40661732"><td></td></tr>
                        <tr id="40660588"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660588" href="https://news.ycombinator.com/vote?id=40660588&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Yes - I do use it for watching films that I want to be more engaged in, usually films through Criterion. I find myself spending more time outdoors now that it's summer but during the winter I'm on it much more. I love the environments and do wish there were more to choose from, plus environments on more streaming platforms. The Disney+ ones are very well made.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40662904"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40662904" href="https://news.ycombinator.com/vote?id=40662904&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Yes, for movies and TV, and as a virtual monitor occasionally. I've travelled with it quite a bit as well, and it's great on long international flights.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661698"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661698" href="https://news.ycombinator.com/vote?id=40661698&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>No, I found it uncomfortable for extended periods of wear - to the point where I would rather just use a normal display on my Mac etc. I look forward to the second-generation with comfort, weight, &amp; FOV improvements (hopefully!).</p><p>It also still hasn't found the "killer apps" just yet, but it's clear Apple is still heavily invested into this considering there's nearly 30 sessions on VisionOS at WWDC this week.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663678"><td></td></tr>
                  <tr id="40660421"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660421" href="https://news.ycombinator.com/vote?id=40660421&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Not the Vision Pro, but I got the latest Facebook device around Christmas time, and it got almost entirely shelved before the end of January. I maybe get it out, for a workout, once a month now.</p><p>Some of the games were really fun, most notably Walkabout Minigolf and Super Hot VR.</p><p>Some of the exercise programs were pretty neat, most notably The Thrill of the Fight and Les Mills Body combat.</p><p>It did not work well as a replacement for either a TV or a computer monitor. The device was just too bulky and inconvenient and the software too clunky. So much easier to just use a laptop, if I want to work / watch on the go.</p><p>In the end, none of the experiences were compelling enough to keep using it regularly.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661806"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40661806" href="https://news.ycombinator.com/vote?id=40661806&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt; Are you still using your Vision Pro?</p><p>&gt; [150 words about a totally different product and platform]</p><p>Vision Pro isn't something I would use regularly, but you're bringing opinions about a 14" CRT monitor to a thread soliciting opinions on a specific 30" 1080P TV. I think we are beyond the stage where useful generalizations about "the state of AR/VR" can be drawn from exposure to a single device.</p><p>The disparity in screen quality and OS sophistication between Oculus 3 and Vision Pro is enormous (and both platforms are self-evidently in their infancy).</p><p>Whether you think they have succeeded or not, and whether you think the price point is reasonable or not, Vision Pro is as different to Quest 3 as a BlackBerry Bold 9700 was to a Nokia 7650.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662362"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40662362" href="https://news.ycombinator.com/vote?id=40662362&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>No it's not. When tossing up a vr purchase it's Vision pro, quest 3 or big screen beyond. Price points all vary but they are literally all the same shiz just served on a different shovel.</p><p>Each have their pros, each have their cons (well the mvp has mostly cons being the worst of the 3 but hey its having a crack).</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662737"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40662737" href="https://news.ycombinator.com/vote?id=40662737&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt; they are literally all the same shiz</p><p>Oculus Quest 3 screens: LCD displays with a per-eye resolution of 2064√ó2208p (4.56 million pixels per eye)</p><p>Apple Vision Pro screens: micro-OLED displays with a per-eye resolution of 3,680x3,140 (11.5 million pixels per eye)</p><p>Disproof by counterexample. Perhaps you could refine your theory?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663762"><td></td></tr>
                  <tr id="40662948"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40662948" href="https://news.ycombinator.com/vote?id=40662948&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Putting aside the enormous hardware difference between the two, even if they were "the same shiz" spec-wise, Id still not comment on Vision Pro over Quest - the reason being I have Macook Air. Spec-wise, that laptop is almost identical to any other laptop, but the level of refinement is on another planet. Its tousands little things that make using Air a joy, while dealing with my work HP Zbook is a pain in every way.</p><p>For that same reason, I dont dare to compare Vision to any other VR (and I tried a few, not Vision Pro tho).</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40663351"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40663351" href="https://news.ycombinator.com/vote?id=40663351&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt; Spec-wise, that laptop is almost identical to any other laptop</p><p>Post-M series I hear this from time to time and always ask people to show me something in the same weight class with equivalent battery life, performance, and screen quality.</p><p>Has the market finally caught up to the point where your statement is true? (Not asking you to research, just curious if any spring to mind from any pre-purchase research you did.)</p><p>&gt; Putting aside the enormous hardware difference between the two</p><p>I think this is far too charitable.</p><p>1. We are a largely technical audience.</p><p>2. We are discussing a product category where, per the last ten years of discussion about early hardware drawbacks (and the critical consensus on Vision Pro), the screen inescapably defines the experience.</p><p>Anyone on HN describing Vision Pro's screen as "the same shiz" as Quest 3 must either be a troll or operating with a knowledge gap so vast as to make meaningful discussion very, very difficult.</p><p>Like, if you don't understand the math, read the reviews and trust that this is not a global cabal of Apple apologists making shit up. Occam's Razor: this is a $3500 device where 35% of the BOM is the screens ($550-ish), compared to a $500 device where ~19% of the BOM is screens ($80). <i>Of course</i> they aren't in the same league.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="40660520"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40660520" href="https://news.ycombinator.com/vote?id=40660520&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>I still use my Oculus 2 a few times a week (I try for daily, but life doesn't allow it), but just Beat Saber and FitXR. It just replaces going to the the gym though if there is some problem with doing that.</p><p>I can't imagine using an AVP though, without controllers it really isn't suited to fitness.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40660634"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40660634" href="https://news.ycombinator.com/vote?id=40660634&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Beat Saber was worth the price of admission alone, at least when you could mod custom songs onto the headset itself. There were also websites that would generate a Beat Saber level for any YouTube video you gave it, which was great for playing along to brand-new releases.</p><p>It was really such a good game that I'm surprised we haven't seen more stuff like it. Of all the futuristic VR experiences I've tried (even HL: Alyx) Beat Saber was the only one that really felt effortlessly futuristic.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661611"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40661611" href="https://news.ycombinator.com/vote?id=40661611&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>BeatSaber would be so much better with custom music, so would FitXR. But I don't have time to figure out how to do that as I used to.</p><p>It is too bad Facebook doesn't lean more into BeatSaber and rhythm game/fitness experiences, they are simple, easy to sell, and are pretty satisfying. But I guess it really isn't good enough for their product, they really need metaverse to take off.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40660981"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40660981" href="https://news.ycombinator.com/vote?id=40660981&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>When the Vive first came out, there was a neat VR FPS named Pavlov VR that was pretty fun.</p><p>It was neat to play an FPS where ducking for cover worked, reloading involved actually having to pull a magazine from your belt and jam it in, you could duck behind something and blind fire over it.</p><p>It mostly worked very well.  The annoyances were around how physically exhausting constantly ducking and weaving was (and sweating into the foam), and getting lost in the moment and nearly sprinting out of the "safety box" into a coffee table.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661622"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40661622" href="https://news.ycombinator.com/vote?id=40661622&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>A silicon foam cover is a must have. Thankfully, they are included with all new oculus VR headsets these days.</p><p>BeatSaber is pretty stationary, so is FitXR. I've never tried a moving around the room/box VR experience (like Thrill of the fight).</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662199"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_40662199" href="https://news.ycombinator.com/vote?id=40662199&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt; <i>A silicon foam cover is a must have.</i></p><p>Funny, I'm the opposite.</p><p>I was excited to try it because it seems so much more hygienic -- you can wipe it down and the foam won't degrade. But I quickly discovered that it got all clammy and sticky on my skin, and then humidity would build up and fog up the lenses. What! Kind of the same way swim goggles fog up.</p><p>Whereas the regular foam padding is... perfectly fine. No sweating, no fog, no humidity, nothing, because enough air seems to pass through and nothing is suffocating your skin.</p><p>And I'm not even a sweaty person or anything, not at all. And I'm just reclining watching movies, it's not even for movement. But the silicone layer over the foam just creates this airtight (enough) seal which is just bad all around.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662799"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_40662799" href="https://news.ycombinator.com/vote?id=40662799&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>I tried one before the one that just came with the Oculus 2 and thought the same. I guess there is a lot of variation in silicon foam covers, but the standard one that comes with the headset works for me. I sweat a lot when I do VR, so without a cover, the foam head piece is going to get soaked and smelly.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40662286"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_40662286" href="https://news.ycombinator.com/vote?id=40662286&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>There are some anti-fog sprays people use while scuba diving, or some people apply a tiny bit of toothpaste.</p><p>I have no idea if those are safe to use on those lenses, but it might be worth a look.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="40661560"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40661560" href="https://news.ycombinator.com/vote?id=40661560&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>You can try moonrider.xyz in the oculus browser for like a punching one. It's web-based VR with all of the community songs included.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                              <tr id="40661586"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661586" href="https://news.ycombinator.com/vote?id=40661586&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Yes, but I'll use it a lot more when 2.0 comes out, so I can see my keyboard in environments, which is my biggest complaint.</p><p>Mostly it's the best cinema screen I've ever viewed in my life. "Avatar 2", in 3D and at 48fps, is an absolutely stunning viewing experience. I wish high-framerate movies were more common. They look incredible.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40660580"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660580" href="https://news.ycombinator.com/vote?id=40660580&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Yes. Still my favourite way to watch movies and F1 (via Vroom). I love working in environments, I am more calm and get distracted less.</p><p>One gripe I had was that I couldn't see the keyboard in an immersive environment, so I had to keep reorienting myself if I took my hands off of it. Now with visionOS 2 you can have the keyboard appear in an environment, so I'm excited to try that. The ability to have an ultra wide screen is a nice addition as well.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661613"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661613" href="https://news.ycombinator.com/vote?id=40661613&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Yes I do, to consume video content every other day on it. Sadly, I'm not the main owner of the device, because there is no multi user support, I cannot use it for day to day work (my partner is logged into it).</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40660924"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660924" href="https://news.ycombinator.com/vote?id=40660924&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Yes, still using it several times a week mostly for work via mac virtual display. If I need to work late at night, I find putting myself in a daylight immersive environment helps me stay awake and avoids high contrast difference between a laptop screen and a dark room.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662570"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40662570" href="https://news.ycombinator.com/vote?id=40662570&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt;avoids high contrast difference between a laptop screen and a dark room.</p><p>So one of the big advantages is that it saves you from turning on the overhead light?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662916"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40662916" href="https://news.ycombinator.com/vote?id=40662916&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Late at night, I work in a corner of the bedroom because my wife likes me nearby. This helps not disturb her and my toddler while they sleep.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40660425"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660425" href="https://news.ycombinator.com/vote?id=40660425&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>I use it every morning with my keyboard to watch videos, get caught up on emails and messages, and sometimes call friends. I don‚Äôt use it quite as much in the evenings when my wife is around since I like to be able to show her what I‚Äôm doing, so I‚Äôll use my laptop instead.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661169"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40661169" href="https://news.ycombinator.com/vote?id=40661169&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Shared augmented reality is the killer feature that I'm still waiting for. I'm really surprised that Apple didn't have that from the start. We could have things like going through your photo collection together. Or collaborating on a virtual sculpture. Or discussing ideas in front of a virtual blackboard. There are a lot of really cool things you could do with shared augmented reality.</p><p>Maybe Apple scheduled this feature for a later release? This is something that can probably be done efficiently 100% in software.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661513"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40661513" href="https://news.ycombinator.com/vote?id=40661513&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>&gt; This is something that can probably be done efficiently 100% in software.</p><p>Doubtful. The data transfer needed between the two devices is not trivial.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40661847"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40661847" href="https://news.ycombinator.com/vote?id=40661847&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>It shouldn't be much more complicated than what we currently have with first-person 3D shooters. All devices in an augmented room would have some existing shared data that doesn't need to be transferred. It's only the updates that need to be sent over the network.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661721"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40661721" href="https://news.ycombinator.com/vote?id=40661721&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Shared photos is likely very doable, as it's just photo id, physical location, and scale/orientation. Place a picture above the fireplace and both headsets could render it exactly the same and see it.</p><p>Video would work well, audio might be a little harder to sync exactly correctly.</p><p>Apps are going to be really really hard.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661905"><td></td></tr>
                              <tr id="40662095"><td></td></tr>
            <tr id="40660429"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660429" href="https://news.ycombinator.com/vote?id=40660429&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Yes, big fan of using the native apps in conjunction with the MacBook mirroring makes it a great workspace. After work, it's fantastic for media consumption.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40661603"><td></td></tr>
            <tr id="40660426"><td></td></tr>
            <tr id="40661756"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40661756" href="https://news.ycombinator.com/vote?id=40661756&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>As an aside, and as someone who doesn't own a vision pro (non US pleb): While it is interesting to me if people find utility. I can't help but feel that the narrative on places outside of HN is a strong "no".</p><p>But, that is to be expected, the form factor isn't convenient yet. When mobile phones weighed 2KG few people used them on a daily. When it's miniaturized into the form factor of glasses, we'll all be daily users. That seems to me more like a question of when, not if.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40662620"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40662620" href="https://news.ycombinator.com/vote?id=40662620&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>A lot of the utility, even in this thread, seems to be solutions for problems that don't actually exist. I think it's cool technology, but nothing mentioned in this thread makes me want to get one.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40660427"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660427" href="https://news.ycombinator.com/vote?id=40660427&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div><p>Yeah. I enjoy it for watching movies/shows, especially while flying. I also like to do a couple of hours of work (programming) in a cool environment with immersive music every couple of days. I keep it on my standing desk and when I stand up, I frequently will put it on.</p><p>I don't recommend it for non developers until there is more content, but it's a really neat dev kit device that shows where the future is headed.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40660324"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40660324" href="https://news.ycombinator.com/vote?id=40660324&amp;how=up&amp;goto=item%3Fid%3D40660270"></a></center>    </td><td><br><div>
                  <p>Yes! Honestly, I mostly just use it as a really, really fancy iPad. It's great for watching content, like movies and TV shows. Some of the games are also fun too in small doses.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40660361"><td></td></tr>
            <tr id="40660389"><td></td></tr>
            <tr id="40660364"><td></td></tr>
            <tr id="40660414"><td></td></tr>
                      </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Alexa dropped the ball on being the top conversational system (165 pts)]]></title>
            <link>https://www.mihaileric.com/posts/how-alexa-dropped-the-ball-conversational-ai/</link>
            <guid>40659281</guid>
            <pubDate>Wed, 12 Jun 2024 15:35:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mihaileric.com/posts/how-alexa-dropped-the-ball-conversational-ai/">https://www.mihaileric.com/posts/how-alexa-dropped-the-ball-conversational-ai/</a>, See on <a href="https://news.ycombinator.com/item?id=40659281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-reactid="11"><figure>
    
  <a href="https://www.mihaileric.com/static/andres-urena-tsBropDpnwE-unsplash-d92595a0132ed811534851d61f868d03-48998.jpg" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="voice assistant" title="" src="https://www.mihaileric.com/static/andres-urena-tsBropDpnwE-unsplash-d92595a0132ed811534851d61f868d03-c6823.jpg" srcset="https://www.mihaileric.com/static/andres-urena-tsBropDpnwE-unsplash-d92595a0132ed811534851d61f868d03-dad4f.jpg 240w,
https://www.mihaileric.com/static/andres-urena-tsBropDpnwE-unsplash-d92595a0132ed811534851d61f868d03-1808a.jpg 480w,
https://www.mihaileric.com/static/andres-urena-tsBropDpnwE-unsplash-d92595a0132ed811534851d61f868d03-c6823.jpg 960w,
https://www.mihaileric.com/static/andres-urena-tsBropDpnwE-unsplash-d92595a0132ed811534851d61f868d03-e8e5f.jpg 1440w,
https://www.mihaileric.com/static/andres-urena-tsBropDpnwE-unsplash-d92595a0132ed811534851d61f868d03-54793.jpg 1920w,
https://www.mihaileric.com/static/andres-urena-tsBropDpnwE-unsplash-d92595a0132ed811534851d61f868d03-c4df6.jpg 2880w,
https://www.mihaileric.com/static/andres-urena-tsBropDpnwE-unsplash-d92595a0132ed811534851d61f868d03-48998.jpg 3000w" sizes="(max-width: 960px) 100vw, 960px">
    </span>
  </span>
  
  </a>
    
</figure>
<p>A few weeks ago OpenAI released GPT-4o ushering in a new standard for multimodal, conversational experiences with sophisticated reasoning capabilities.</p>
<p>Several days later, my good friends at <a href="https://poly.ai/">PolyAI</a> announced their Series C fundraising round after tremendous growth in the usage of their enterprise voice assistant.</p>
<p>Amid this news, a former Alexa colleague messaged me: ‚ÄúYou‚Äôd think voice assistants would have been our forte at Alexa.‚Äù</p>
<p>For context, I joined Alexa AI as a research scientist in early 2019. By this time, the Alexa consumer device had existed for 5 years and was already in 100M+ homes throughout the world. </p>
<p>In 2019, Alexa was experiencing a period of hypergrowth. Dozens of new teams sprouted every quarter, huge financial resources were invested, and senior leadership made it clear that Alexa was going to be one of Amazon‚Äôs big bets moving forward. </p>
<p>My team was born amidst all this with a simple charter: bring the latest and greatest in AI research into the Alexa product and ecosystem. I‚Äôve often described our group (later dubbed the Conversational Modeling team) as Google Brain meets Alexa AI-SWAT team. </p>
<p>Over the course of the 2.5 years I was there, we grew from 2 to ~20 and tackled every part of the conversational systems stack. </p>
<p>We built the first LLMs for the organization (though back then we didn‚Äôt call them LLMs), we built knowledge grounded response generators (though we didn‚Äôt call it RAG), and we pioneered prototypes for what it would mean to make Alexa a multimodal agent in your home. </p>
<p>We had all the resources, talent, and momentum to become the unequivocal market leader in conversational AI. But most of that tech never saw the light of day and never received any noteworthy press. </p>
<p>Why?</p>
<p>The reality is Alexa AI was riddled with technical and bureaucratic problems.</p>
<h2>Bad Technical Process</h2>
<p>Alexa put a huge emphasis on protecting customer data with guardrails in place to prevent leakage and access. Definitely a crucial practice, but one consequence was that the internal infrastructure for developers was agonizingly painful to work with. </p>
<p>It would take weeks to get access to any internal data for analysis or experiments. Data was poorly annotated. Documentation was either nonexistent or stale. </p>
<p>Experiments had to be run in resource-limited compute environments. Imagine trying to train a transformer model when all you can get a hold of is CPUs. Unacceptable for a company sitting on one of the largest collections of accelerated hardware in the world. </p>
<p>I remember on one occasion our team did an analysis demonstrating that the annotation scheme for some subset of utterance data was completely wrong, leading to incorrect data labels. </p>
<p>That meant for months our internal annotation team had been mislabeling thousands of data points every single day. When we attempted to get the team to change their annotation taxonomy, we discovered it would require a herculean effort to get even the smallest thing modified. </p>
<p>We had to get the team‚Äôs PM onboard, then their manager‚Äôs buy-in, then submit a preliminary change request, then get that approved (a multi-month-long process end-to-end). </p>
<p>And most importantly, there was no immediate story for the team‚Äôs PM to make a promotion case through fixing this issue other than ‚Äúit‚Äôs scientifically the right thing to do and could lead to better models for some other team.‚Äù No incentive meant no action taken.</p>
<p>Since that wasn‚Äôt our responsibility and the lift from our side wasn‚Äôt worth the effort, we closed that chapter and moved on. </p>
<p>For all I know, they could still be mislabeling those utterances to this day.</p>
<h2>Fragmented Org Structures</h2>
<p>Alexa‚Äôs org structure was decentralized by design meaning there were multiple small teams working on sometimes identical problems across geographic locales. </p>
<p>This introduced an almost Darwinian flavor to org dynamics where teams scrambled to get their work done to avoid getting reorged and subsumed into a competing team. </p>
<p>The consequence was an organization plagued by antagonistic mid-managers that had little interest in collaborating for the greater good of Alexa and only wanted to preserve their own fiefdoms. </p>
<p>My group by design was intended to span projects, whereby we found teams that aligned with our research/product interests and urged them to collaborate on ambitious efforts. The resistance and lack of action we encountered was soul-crushing. </p>
<p>I remember on one occasion we were coordinating a project to scale out the large transformers model training I had been leading. This was an ambitious effort which, if done correctly, could have been the genesis of an Amazon ChatGPT (well before ChatGPT was released). </p>
<p>Our Alexa team met with an internal cloud team which independently was initiating similar undertakings. While the goal was to find a way to collaborate on this training infrastructure, over the course of several weeks there were many half-baked promises made which never came to fruition. </p>
<p>At the end of it, our team did our own thing and the sister team did their own thing. Duplicated efforts due to no shared common ground. With no data, infrastructure, or lesson sharing, this inevitably hurt the quality of produced models.</p>
<p>As another example, the <a href="https://www.amazon.com/alexa-skills/b?ie=UTF8&amp;node=13727921011">Alexa skills ecosystem</a> was Alexa‚Äôs attempt to apply Amazonian decentralization to the dialogue problem. Have individual teams own individual skills.</p>
<p>But dialogue is not conducive to that degree of separation of concerns. How can you seamlessly hand off conversational context between skills? This means endowing the system with multi-turn memory (a long-standing dream of dialogue research). </p>
<p>The internal design of the skills ecosystem made achieving this infeasible because each skill acted like its own independent bot. It was conversational AI by an opinionated bot committee each with its own agenda. </p>
<h2>Product-Science Misalignment</h2>
<p>Alexa was viciously customer-focused which I believe is admirable and a principle every company should practice. Within Alexa, this meant that every engineering and science effort had to be aligned to some downstream product. </p>
<p>That did introduce tension for our team because we were supposed to be taking experimental bets for the platform‚Äôs future. These bets couldn‚Äôt be baked into product without hacks or shortcuts in the typical quarter as was the expectation. </p>
<p>So we had to constantly justify our existence to senior leadership and massage our projects with metrics that could be seen as more customer-facing. </p>
<p>For example, in one of our projects to build an open-domain chat system, the success metric (i.e. a single integer value representing overall conversational quality) imposed by senior leadership had no scientific grounding and was borderline impossible to achieve. </p>
<p>This introduced product/science conflict in every weekly meeting to track the project‚Äôs progress leading to manager churn every few months and an eventual sunsetting of the effort.</p>
<p>‚Äî</p>
<p>As we look forward, in the battle for the future of the conversational AI market, I still believe it‚Äôs anyone‚Äôs game.</p>
<p>Today Alexa has sold 500M+ devices, which is a mind-boggling user data moat. But that alone is not enough. </p>
<p>Here‚Äôs how I would organize a dialogue systems effort from the ground-up:</p>
<p><strong>Invest in robust developer infrastructure especially around access to compute, data quality assurance, and streamlined data collection processes.</strong> Data and compute are the lifeblood of modern ML systems so proactively setting up this foundation is imperative.</p>
<p><strong>Make LLMs the fundamental building block of the dialogue flows.</strong> In retrospect, the Alexa skills ecosystem was a premature initiative for the abilities of conversational systems at the time. I liken it to when Leap Motion created and released a developer SDK before the underlying hardware device was stable. </p>
<p>But with the power of modern LLMs, I‚Äôm optimistic about redesigning a developer conversational toolkit with LLMs as their primitives. </p>
<p><strong>Ensure product timelines don‚Äôt dictate science research time frames.</strong> Because things are moving so fast in the AI world, it‚Äôs hard not to feel the pressure of shipping quickly. But there are still so many unsolved problems that will take time to solve. </p>
<p>Of course you should conduct research aggressively, but don‚Äôt have delivery cycles measured in quarters, as this will produce inferior systems to meet deadlines. </p>
<p>‚Äî</p>
<p>If you‚Äôre thinking about the future of multimodal conversational systems and interfaces, I would love to hear from you. We‚Äôve got work to do!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Overwatch (YC S22): OSINT platform for cyber and fraud risk (132 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40659236</link>
            <guid>40659236</guid>
            <pubDate>Wed, 12 Jun 2024 15:32:53 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40659236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="40659236">
      <td><span></span></td>      <td><center><a id="up_40659236" href="https://news.ycombinator.com/vote?id=40659236&amp;how=up&amp;goto=item%3Fid%3D40659236"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=40659236">Launch HN: Overwatch (YC S22): OSINT platform for cyber and fraud risk</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_40659236">109 points</span> by <a href="https://news.ycombinator.com/user?id=Bisen">Bisen</a> <span title="2024-06-12T15:32:53"><a href="https://news.ycombinator.com/item?id=40659236">6 hours ago</a></span> <span id="unv_40659236"></span> | <a href="https://news.ycombinator.com/hide?id=40659236&amp;goto=item%3Fid%3D40659236">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Overwatch%20(YC%20S22)%3A%20OSINT%20platform%20for%20cyber%20and%20fraud%20risk&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=40659236&amp;auth=31ce6a8bfdb4ec101184458f300d613dbaa7f035">favorite</a> | <a href="https://news.ycombinator.com/item?id=40659236">62&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Hey HN! Arjun and Zara here - cofounders of Overwatch (<a href="https://www.overwatchdata.ai/">https://www.overwatchdata.ai</a>), a platform to automate OSINT and threat intel, turning it into actionable insights.  Check out our clickthrough demo here: <a href="https://app.storylane.io/share/qyayvtamapis">https://app.storylane.io/share/qyayvtamapis</a>.</p><p>Overwatch began when we were working with risk and threat intel teams at Google, Stripe, and government. We experienced the immense challenge every fraud and cyber threat analyst faces: manually parsing through an ocean of data to find valuable insights and filter out the noise. This included using many of the feeds and tools out there that were often very expensive, noisy, keyword-based, and lacked accurate entity extraction or advanced query features.</p><p>Most threat intelligence tools utilize thousands of keywords and teams of analysts to manually sift through torrents of alerts. These alerts are usually individual posts on various platforms across news, social media, deep and dark web sources that have some matching keyword. This is full of false positives, requiring many hours to wade through to figure out what intel matters most to our users, why, and what they can do next.</p><p>Overwatch uses an alternative approach by layering AI agents and NLP techniques, including a combination of multifarious datasets, cluster analysis, topic modeling, Retrieval Augmented Language Models (RALM) and domain knowledgeable agents.</p><p>This allows us to (1) Filter through OSINT in real time to identify events and narratives that matter to our users, and write reports on what they could do about it; (2) Identify dark web and deep web threats, fraud methods, new tactics, and compromised accounts, stolen checks, and credentials affecting our users or their peers; (3) Send an alert any time one a 3rd party supplier or parts of the tech stack are impacted by a widely exploited vulnerability, ransomware attack, or breach; and (4) Track malware and ransomware groups that are actively targeting your industry including Indicators of Compromise (IOCs).</p><p>Our intelligence is actionable because the alert comes with the context and important details that an analyst needs to make an informed decision. Being AI-native, we also have a range of chat and data visualization features to effectively function as an intel co-pilot or industry expert. Finally, our in-house intelligence analysts and investigators can assist threat intelligence teams with HUMINT investigations and darkweb acquisition.</p><p>Our current customers include internet platforms, financial institutions, and supply chain companies. Within a day of one breach, one of our customers used Overwatch to surface 18,000+ leaked credentials. Another used us to surface fraudulent checks and learn exactly how threat actors were targeting their specific product features.</p><p>Our website says ‚ÄúRequest a demo‚Äù but if you want to poke around on a very basic example of how we‚Äôre aggregating dark web, deep web, social, and surface web, log in at <a href="https://app.overwatchdata.io/" rel="nofollow">https://app.overwatchdata.io/</a> using these credentials:
  username: try_overwatch@overwatchdata.io 
  pw: HelloHNWorld</p><p>That login is for an un-personalized feed of cyber threat intel (breaches, vulnerabilities, ransomed organizations, and industry updates) that gives you a flavor of not just the kind of information from which we can collect, but more importantly, how our technology prioritizes, clusters, and summarizes alerts for cyber / fraud analysts. Try the chat agent on the left-hand side to parse through the data.</p><p>Or sign up for a longer trial and preview of our email alerts: <a href="https://xryl45u9uep.typeform.com/to/pvtZQyS0" rel="nofollow">https://xryl45u9uep.typeform.com/to/pvtZQyS0</a>. You can also check out our clickthrough demo for dark and deep web intelligence: <a href="https://app.storylane.io/share/qyayvtamapis">https://app.storylane.io/share/qyayvtamapis</a>.</p><p>Integration options range from simple dashboard access to our API for those who want to weave our intelligence directly into other products. Pricing is dependent on how complex a threat landscape our users want to monitor and we‚Äôre still figuring out how to standardize this but we‚Äôll always do our best for the HN community.</p><p>Since the platform is AI-powered, it can also be used for news monitoring, supply chain disruptions, regulatory monitoring, or social media monitoring. We‚Äôve had a lot of experience wrangling text-based feeds and using numerous AI-models (from embeddings, entity extractors, and LLMs) to filter, categorize, cluster, and analyze the data into meaning - so let us know if you‚Äôd like to nerd-out or have had any particular challenges. Looking forward to your feedback and questions! Thanks, HN!</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Restate ‚Äì Low-latency durable workflows for JavaScript/Java, in Rust (157 pts)]]></title>
            <link>https://restate.dev/</link>
            <guid>40659160</guid>
            <pubDate>Wed, 12 Jun 2024 15:25:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://restate.dev/">https://restate.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=40659160">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><a href="https://restate.dev/blog/announcing-restate-1.0-restate-cloud-and-our-seed-funding-round/"><p>üéâ&nbsp;&nbsp;Announcing Restate 1.0, Restate&nbsp;Cloud, and our Seed Funding Round &nbsp;&nbsp;<span>Read more</span></p></a></section><div data-animation="default" data-collapse="medium" data-duration="400" data-easing="ease" data-easing2="ease" data-doc-height="1" role="banner"><p><a href="https://restate.dev/" aria-current="page"><img src="https://cdn.prod.website-files.com/663272930a67769123cdcf53/66552a0c5a6f4d528f1241de_Logo%20Refined.svg" loading="lazy" alt=""></a></p></div><div><div id="w-node-_909f3004-9317-0658-ea73-f830f9153f13-23cdcf59" data-w-id="909f3004-9317-0658-ea73-f830f9153f13"><p data-w-id="0e3c2fe2-55e6-bde5-39c2-cefd2408f0b6">As <strong>regular functions and services</strong>, in your existing infrastructure. On FaaS, K8s, servers, containers. Self-hosted or fully managed. Restate meets you where you are.</p></div><div id="w-node-_0a36f96d-1a2c-4e83-c438-e5e08296ce51-23cdcf59"><p><img src="https://cdn.prod.website-files.com/663272930a67769123cdcf53/665bd80206cc423e5a2edcf2_hero-background.png" loading="eager" sizes="(max-width: 479px) 261.75px, (max-width: 767px) 282.84375px, 554.75px" srcset="https://cdn.prod.website-files.com/663272930a67769123cdcf53/665bd80206cc423e5a2edcf2_hero-background-p-500.png 500w, https://cdn.prod.website-files.com/663272930a67769123cdcf53/665bd80206cc423e5a2edcf2_hero-background-p-800.png 800w, https://cdn.prod.website-files.com/663272930a67769123cdcf53/665bd80206cc423e5a2edcf2_hero-background-p-1080.png 1080w, https://cdn.prod.website-files.com/663272930a67769123cdcf53/665bd80206cc423e5a2edcf2_hero-background-p-1600.png 1600w, https://cdn.prod.website-files.com/663272930a67769123cdcf53/665bd80206cc423e5a2edcf2_hero-background-p-2000.png 2000w, https://cdn.prod.website-files.com/663272930a67769123cdcf53/665bd80206cc423e5a2edcf2_hero-background-p-2600.png 2600w, https://cdn.prod.website-files.com/663272930a67769123cdcf53/665bd80206cc423e5a2edcf2_hero-background.png 2752w" alt=""></p></div></div><div data-w-id="c420d2f0-b483-a654-3cb5-e22ce4cf01f9"><h2 id="w-node-ebe6a069-89b1-fc5f-5813-61aed10405e5-23cdcf59">Easy solutions for common challenges</h2><div fs-cmstabs-element="list" role="list" id="w-node-_987cb84f-9b03-aa32-df41-957747709f42-23cdcf59"><div role="listitem"><div id="w-node-_13e0bcf1-d9ea-6e17-5d98-d31f688c70c6-23cdcf59"><p fs-cmstabs-element="tab-link">Workflows as code</p><h4>Workflows as code</h4><div><p>Durable Execution ensures code runs reliably to the end, even in the presence of failures.</p><p>‚Äç</p><ul role="list"><li>Failures and errors are automatically retried (unless labeled as terminal errors)</li><li>Functions can memoize the results of code blocks, and actions like RPC, in a journal. Completed steps are not re-executed during retries, but replayed from the journal.</li><li>Workflows are built with regular code and control flow, no custom DSLs needed.</li><li>Durable sleeps let code wait and suspend for up to months</li></ul><p>‚Äç</p></div><p><a href="https://github.com/restatedev/examples/blob/main/basics/basics-typescript/src/1_durable_execution.ts" target="_blank">&gt; Learn More</a></p></div><div data-current="Tab 1" data-easing="ease" data-duration-in="300" data-duration-out="100" id="w-node-e2a5a4fe-8235-9f45-0a19-48178634d8ac-23cdcf59"><div data-w-tab="Tab 1"><pre><code>
export default restate.service({
  name: "roleUpdate",
  handlers: {
    applyRoleUpdate: async (ctx, update) =&gt; {
      const { userId, role, permissions } = update;
      const applied = await ctx.run("apply new role", () =&gt;
        applyUserRole(userId, role)
      );
      if (!applied) {
        return;
      }
      for (const permission of permissions) {
        await ctx.run("apply permission", () =&gt;
          applyPermission(userId, permission)
        );
      }
    }
  }
});
</code>
</pre></div><div data-w-tab="Tab 2"><pre><code>
@Service
public class RoleUpdateService {

    @Handler
    public void applyRoleUpdate(Context ctx, Update update) {

        boolean success = ctx.run("apply new role", BOOLEAN,
            () -&gt; applyUserRole(update.getUserId(), update.getRole()));

        if (!success) {
            return;
        }

        for (String permission : update.getPermissions()) {
            ctx.run("apply permission",
                () -&gt; applyPermission(update.getUserId(), permission));
        }
    }
}
</code>
</pre></div></div></div><div role="listitem"><div id="w-node-_13e0bcf1-d9ea-6e17-5d98-d31f688c70c6-23cdcf59"><p fs-cmstabs-element="tab-link">API calls and webhooks</p><h4>API calls and webhooks</h4><div><p>Reliably join synchronous code and async events like webhooks</p><ul role="list"><li>Webhooks/events are persisted in Restate‚Äôs log and reliably delivered to services</li><li>Persistent Promises/Futures easily join synchronous and asynchronous code paths</li><li>Durable execution ensures reliable completion, whether webhooks come after milliseconds or months, and avoid re-execution of completed steps.</li></ul></div><p><a href="https://github.com/restatedev/examples/tree/website_snippets_java/patterns-use-cases/async-signals-payment" target="_blank">&gt; Learn More</a></p></div><div data-current="Tab 1" data-easing="ease" data-duration-in="300" data-duration-out="100" id="w-node-e2a5a4fe-8235-9f45-0a19-48178634d8ac-23cdcf59"><div data-w-tab="Tab 1"><pre><code>
const paymentSvc = restate.service({
  name: "payments",
  handlers: {
    processPayment: async (ctx, request) =&gt; {
      const webhookPromise = ctx.awakeable();
      const paymentIntent = await ctx.run("stripe call", () =&gt;
        createPaymentIntent({
          request,
          metadata: { restate_callback_id: webhookPromise.id }
        })
      );
      if (paymentIntent.status === "processing") {
        // synchronous response inconclusive, await webhook response
        const paymentIntentFromWebhook = await webhookPromise.promise;
        return verifyPayment(paymentIntentFromWebhook);
      } else {
        return verifyPayment(paymentIntent);
      }
    },
    processWebhook: async (ctx) =&gt; {
      const paymentIntent = verifyAndParseEvent(ctx.request());
      const webhookPromiseId = paymentIntent.metadata.restate_callback_id;
      ctx.resolveAwakeable(webhookPromiseId, paymentIntent);
    }
  }
});
</code>
</pre></div><div data-w-tab="Tab 2"><pre><code>
@Service
public class PaymentService {
  @Handler
  public void processPayment(Context ctx, PaymentRequest request) {
    var webhookFuture = ctx.awakeable(SERDE);
    var payment = ctx.run("Stripe call", SERDE, () -&gt; submitPayment(
            request, Map.of("restate_callback_id", webhookFuture.id())
    ));
    if (payment.getStatus().equals("processing")) {
      // synchronous response inconclusive, await webhook response
      var updatedPayment = webhookFuture.await();
      verifyPayment(updatedPayment);
    } else {
      verifyPayment(payment);
    }
  }
  @Handler
  public void processWebhook(Context ctx) {
    var paymentEvent = verifyAndParseEvent(ctx.request());
    String callbackId = paymentEvent.getMetadata().get("restate_callback_id");
    ctx.awakeableHandle(callbackId).resolve(SERDE, paymentEvent);
  }
}
</code>
</pre></div></div></div><div role="listitem"><div id="w-node-_13e0bcf1-d9ea-6e17-5d98-d31f688c70c6-23cdcf59"><p fs-cmstabs-element="tab-link">Asynchronous Tasks</p><h4>Asynchronous Tasks</h4><div><p>All functions invoked through Restate are executed durably and asynchronous.</p><ul role="list"><li>Deploy async functions serverless or as containers or processes.</li><li>Call functions synchronously, async, or delayed. Re-attach and await from anywhere.</li><li>Build async patterns like fan-out, fan-in, task chains, and subtasks simply with function calls and Futures/Promises.</li><li>Use persistent timers to schedule tasks into the future.</li><li>Use fine-grained virtual queues (via virtual objects) to enforce strict task order and concurrency</li></ul></div><p><a href="https://github.com/restatedev/examples/tree/website_snippets_java/patterns-use-cases/async-tasks" target="_blank">&gt; Learn More</a></p></div><div data-current="Tab 1" data-easing="ease" data-duration-in="300" data-duration-out="100" id="w-node-e2a5a4fe-8235-9f45-0a19-48178634d8ac-23cdcf59"><div data-w-tab="Tab 1"><pre><code>
// ------ service (= worker) ------
const asyncTaskService = restate.service({
    name: "taskWorker",
    handlers: { processPayment }
});
// ------ client ------
const rs = clients.connect({ url: process.env.RESTATE_URL });
const taskWorker = rs.serviceSendClient({ name: "taskWorker" });
// submit the payment task 
app.post('/charge/:paymentId', async (req, res) =&gt; {
    const taskHandle = await taskWorker.processPayment(
        { request: req.params },
        SendOpts.from({ idempotencyKey: req.params.paymentId })
    );
    res.json(taskHandle);
});
// await the payment task
app.get('/status', async (req,res) =&gt; {
        const taskHandle = req.body.json();
        const paymentResult = await restate.result(taskHandle); 
        res.join(paymentResult);
});
</code>
</pre></div><div data-w-tab="Tab 2"><pre><code>
// --- start payment task ---
server.createContext("/charge", httpExchange -&gt; {
  PaymentRequest req = parsePaymentRequest(httpExchange);
  SendResponse handle = AsyncTaskServiceClient
      .fromIngress(RESTATE_URI)
      .send()
      .processPayment(req, idempotencyKey(req.getPaymentId()));

  respondJson(httpExchange, handle);
});
//  --- connect to payment result ---
server.createContext("/status", httpExchange -&gt; {
  String handle = parseToHandle(httpExchange);
  String response = IngressClient.defaultClient(RESTATE_URI)
      .invocationHandle(handle, STRING)
      .attach();
  respond(httpExchange, response);
});
</code>
</pre></div></div></div><div role="listitem"><div id="w-node-_13e0bcf1-d9ea-6e17-5d98-d31f688c70c6-23cdcf59"><p fs-cmstabs-element="tab-link">Stateful Event Processing</p><h4>Stateful Event Processing</h4><div><p>Process events (for example from Kafka) with durable functions as event handlers and get fine-grained retries and workflow-as-code semantics.</p><ul role="list"><li>No queue subscriptions, no manual offset management, scaling, or balancing</li><li>Deploy the event processing logic as serverless functions on FaaS</li><li>Keep exactly-once state, delay events, run multiple asynchronous steps or API calls.</li><li>Restate‚Äôs queue-per-key semantics mean no more head-of-the-line waiting effects</li></ul></div><p><a href="https://github.com/restatedev/examples/blob/main/basics/basics-typescript/src/5_events_processing.ts" target="_blank">&gt; Learn More</a></p></div><div data-current="Tab 1" data-easing="ease" data-duration-in="300" data-duration-out="100" id="w-node-e2a5a4fe-8235-9f45-0a19-48178634d8ac-23cdcf59"><div data-w-tab="Tab 1"><pre><code>
const eventEnricher = restate.object({
  name: "eventEnricher",
  handlers: {
    userEvent: async (ctx, event) =&gt; {
      // remember event, time box 100 ms to collect features
      // before emitting result
      ctx.set("user", event);
      ctx.serviceSendClient(eventEnricher, { delay: 100 }).emit();
    },
    featureEvent: async (ctx, featureEvent) =&gt; {
      // merge feature into event
      const userEvent = (await ctx.get("user")) ?? {};
      (userEvent.features ??= []).push(featureEvent);
      ctx.set("user", userEvent)
    },
    emit: async (ctx) =&gt; {
      emit(ctx.key, await ctx.get("user"));
      ctx.clearAll();
    }
  }
})
</code>
</pre></div><div data-w-tab="Tab 2"><pre><code>
@VirtualObject
public class EventEnricher {
    static final StateKey<user> USER = StateKey.of("user", of(User.class));
    @Handler
    public void userEvent(ObjectContext ctx, User event) {
        ctx.set(USER, event);
        // time box 100 ms to collect features before emitting result
        EventEnricherClient.fromContext(ctx, ctx.key())
            .send(ofMillis(100)).emit();
    }
    @Handler
    public void featureEvent(ObjectContext ctx, Feature event) {
        User user = ctx.get(USER).orElse(new User());
        user.addFeature(event);
        ctx.set(USER, user);
    }
    @Handler
    public void emit(ObjectContext ctx) {
        send(ctx.key(), ctx.get(USER));
        ctx.clearAll();
    }
}
</user></code>
</pre></div></div></div><div role="listitem"><div id="w-node-_13e0bcf1-d9ea-6e17-5d98-d31f688c70c6-23cdcf59"><p fs-cmstabs-element="tab-link">Durable Signals</p><h4>Durable Signals</h4><div><p>Create workflows and event handlers that reliably handle external signals, events, human input.</p><p>‚Äç</p><ul role="list"><li>Use durable Promises/Futures to intuitively model signals and conditions</li><li>Create signals from RPCs, webhooks, or Kafka events</li><li>Signals and events are persisted by Restate, no need for a queue</li></ul></div><p><a href="https://github.com/restatedev/examples/blob/main/basics/basics-typescript/src/3_workflows.ts" target="_blank">&gt; Learn More</a></p></div><div data-current="Tab 1" data-easing="ease" data-duration-in="300" data-duration-out="100" id="w-node-e2a5a4fe-8235-9f45-0a19-48178634d8ac-23cdcf59"><div data-w-tab="Tab 1"><pre><code>
export default workflow({
  name: "verify",
  handlers: {
    run: async (ctx, { email }) =&gt; {
      const secret = ctx.run("generate secret", () =&gt;
        crypto.randomUUID()
      );
      await ctx.run("send email", () =&gt; sendEmail({ email, secret }));

      const clickSecret = await ctx.promise("email.clicked");
      return clickSecret == secret;
    },
    click: (ctx, { secret }) =&gt; {
      ctx.promise("email.clicked").resolve(secret);
    },
  },
});



</code>
</pre></div><div data-w-tab="Tab 2"><pre><code>
@Workflow
public class SecretVerifier {

    static final DurablePromiseKey<string> EMAIL_CLICKED =
            DurablePromiseKey.of("email_clicked", JsonSerdes.STRING);

    @Workflow
    public boolean run(WorkflowContext ctx, Email email) {
        String secret = ctx.random().nextUUID().toString();
        ctx.run("send email",
            () -&gt; sendEmailWithLink(email, secret));

        String clickSecret = ctx.promise(EMAIL_CLICKED).awaitable().await();
        return clickSecret.equals(secret);
    }

    @Handler
    public void click(SharedWorkflowContext ctx, String secret) {
        ctx.promiseHandle(EMAIL_CLICKED).resolve(secret);
    }
}
</string></code>
</pre></div></div></div><div role="listitem"><div id="w-node-_13e0bcf1-d9ea-6e17-5d98-d31f688c70c6-23cdcf59"><p fs-cmstabs-element="tab-link">Idempotency</p><h4>Idempotency</h4><div><p>Add idempotency to any RPC- or event handler.</p><p>‚Äç</p><ul role="list"><li>Every RPC- and event handler call accepts an idempotency key</li><li>Use idempotency keys to re-attach to an ongoing invocation</li><li>Calls from within a durable execution context are automatically idempotent</li></ul></div><p><a href="https://docs.restate.dev/concepts/invocations#idempotent-invocations" target="_blank">&gt; Learn More</a></p></div><div data-current="Tab 1" data-easing="ease" data-duration-in="300" data-duration-out="100" id="w-node-e2a5a4fe-8235-9f45-0a19-48178634d8ac-23cdcf59"><div data-w-tab="Tab 1"><pre><code>
const rs = restate.connect({ url: process.env.RESTATE_URL });
app.get('/reserve/:product/:reservationId', async (req, res) =&gt; {
  const { product, reservationId } = req.params;
  const products = rs.serviceClient(ProductService);
  const reservation = await products.reserve(
    product,
    Opts.from({ idempotencyKey : reservationId })
  );
  res.json(reservation);
})
</code>
</pre></div><div data-w-tab="Tab 2"><pre><code>
server.createContext("/reserve", httpExchange -&gt; {
      ReservationRequest req = parseRequest(httpExchange.getRequestBody());
      // derive an idempotency key from the parameters
      var idempotencyOps = CallRequestOptions.DEFAULT
          .withIdempotency(req.getReservationId());
      // add idempotency opts to the request to let the service automatically
      // fuse repeated requests
      Reservation reservation = ProductServiceClient
          .fromIngress(RESTATE_RUNTIME_ENDPOINT)
          .reserve(req.getProduct(), idempotencyOps);
      sendResponse(httpExchange, reservation);
    });
</code>
</pre></div></div></div><div role="listitem"><div id="w-node-_13e0bcf1-d9ea-6e17-5d98-d31f688c70c6-23cdcf59"><p fs-cmstabs-element="tab-link">Sagas</p><h4>Sagas</h4><div><p>Implements robust sagas and compensation patterns: long-running transactions that undo previous actions when they need to abort and roll back.</p><p>‚Äç</p><ul role="list"><li>Reliably pick up after failures to trigger compensations</li><li>Ensure compensations happen even upon failures during the compensation phase</li><li>Use standard Exception/Error mechanisms and control flow rather than complex DSLs.</li></ul></div><p><a href="https://github.com/restatedev/examples/tree/main/patterns-use-cases/sagas" target="_blank">&gt; Learn More</a></p></div><div data-current="Tab 1" data-easing="ease" data-duration-in="300" data-duration-out="100" id="w-node-e2a5a4fe-8235-9f45-0a19-48178634d8ac-23cdcf59"><div data-w-tab="Tab 1"><pre><code>
async function reservation(ctx, products) {
  const reservations = [];
  try {
    for (const product of products) {
      const reservation = await ctx.run(`reserve ${product}`,
          () =&gt; reserve(product));
      reservations.push(reservation);
    }
  } catch (error) {
    if (error instanceof TerminalError) {
      for (const reservation of reservations) {
        await ctx.run("undo reserve", () =&gt; 
            cancelReservation(reservation));
      }
    }
    throw error;
  }
}
</code>
</pre></div><div data-w-tab="Tab 2"><pre><code>
@Handler
public void reserveAllProducts(Context ctx, Product[] products) {
    final List<reservation> reservations = new ArrayList&lt;&gt;();
    try {
        for (Product product : products) {
            Reservation res = ctx.run("Reserve " + product.getId(),
                RESERVE_SERDE, () -&gt; reserve(product)
            );
            reservations.add(res);
        }
    } catch (TerminalException e) {
        reservations.forEach(res -&gt; {
            ctx.run("Undo reservation", () -&gt; cancelReservation(res));
        });
        throw e;
    }
}
</reservation></code>
</pre></div></div></div><div role="listitem"><div id="w-node-_13e0bcf1-d9ea-6e17-5d98-d31f688c70c6-23cdcf59"><p fs-cmstabs-element="tab-link">State machines</p><h4>State machines</h4><div><p>Create consistent and scalable State Machines without databases or transactions<br>‚Äç</p><ul role="list"><li>Run millions of State Machines that maintain state directly in the context of their handlers</li><li>State changes commit atomically with function execution, for rock-solid consistency</li><li>Single-writer semantics for a dead simple concurrency model. A virtual queue per state machine for efficiency and scalability.</li><li>State transition can be workflows with all the features from durable execution</li></ul></div><p><a href="https://github.com/restatedev/examples/tree/main/patterns-use-cases/payment-state-machine" target="_blank">&gt; Learn More</a></p></div><div data-current="Tab 1" data-easing="ease" data-duration-in="300" data-duration-out="100" id="w-node-e2a5a4fe-8235-9f45-0a19-48178634d8ac-23cdcf59"><div data-w-tab="Tab 1"><pre><code>
const paymentSvc = restate.object({
  name: "payments",
  handlers: {
    makePayment: async (ctx, payment) =&gt; {
      const paymentId = ctx.key;
      switch (await ctx.get("status")) {
        case "CANCELLED":
            return `${paymentId} was cancelled before`;
        case "SUCCESS":
            return `${paymentId} previously completed`;
      }
      wireFunds(payment);
      ctx.set("status", "SUCCESS");
      ctx.set("payment", payment);
    },
    cancelPayment: async (ctx) =&gt; {
      const status = await ctx.get("status");
      if (status === "SUCCESS") {
        const payment = await ctx.get("payment");
        refund(payment);
      }
      ctx.set("status", "CANCELLED");
    }
  }
});
</code>
</pre></div><div data-w-tab="Tab 2"><pre><code>
@VirtualObject
public class PaymentStateMachine {
  @Handler
  public String makePayment(ObjectContext ctx, PaymentRequest payment) {
    String paymentId = ctx.key();
    switch (ctx.get(STATE_STATUS).orElse(NEW)) {
      case CANCELLED: return paymentId + " was cancelled before";
      case SUCCESS:   return paymentId + " was previously completed";
    }
    wireFunds(payment);
    ctx.set(STATE_STATUS, SUCCESS);
    ctx.set(STATE_PAYMENT_REQUEST, payment);
    return paymentId + " was successfully processed";
  }
  @Handler
  public void cancelPayment(ObjectContext ctx) {
    Status status = ctx.get(STATE_STATUS).orElse(NEW);
    if (status == SUCCESS) {
      PaymentRequest payment = ctx.get(STATE_PAYMENT_REQUEST).get();
      refund(payment);
    }
    ctx.set(STATE_STATUS, CANCELLED);
  }
}
</code>
</pre></div></div></div></div></div><div><div id="w-node-_6c62185e-aeb0-0281-3106-2400ea849cab-23cdcf59"><h2>A simple and powerful programming model</h2><p>Restate provides distributed durable version of your everyday building blocks.</p><p><a href="https://restate.dev/programming-model">&gt; See how it works</a></p></div><div id="w-node-_6efa648f-2885-10fe-426b-0822434c8e3d-23cdcf59"><div data-w-id="1f98eac8-7392-f220-f431-d55dd9707ff8"><h4>Durable Execution</h4><p>Functions/Services that handle retries, recovery, asynchrony, idempotency.</p></div><div data-w-id="fada5290-d344-caa0-93a5-2dca955e7633"><h4>Virtual Objects</h4><p>Persistent state directly in your objects with a simple concurrency model.</p></div><div data-w-id="6f0d1405-b0e1-174a-5538-0a0fc0cd2f72"><h4>Durable Promises</h4><p>Transparent and fault-tolerant communication across services, processes, and time.</p></div></div></div><div><div id="w-node-_64f32bf9-9582-8528-1db5-e498d4a9d807-23cdcf59"><p><img src="https://cdn.prod.website-files.com/663272930a67769123cdcf53/664fdfb3d3abef98f35a8335_FeatureIcon1.svg" loading="lazy" alt=""></p><h2>Single binary, no dependencies, built in Rust.</h2><p>A system that runs locally and on-prem just as well as in the ‚Ä®cloud. Restate server comes as a single binary. Simple to run, ‚Ä®simple to operate.</p><p>Fully self-contained, resource-efficient, resilient, thanks to ‚Ä®Rust‚Äôs magic.</p></div><div id="w-node-d62c8e4f-b2e9-6271-8c4d-c77645e928de-23cdcf59"><p><img src="https://cdn.prod.website-files.com/663272930a67769123cdcf53/664fdfb4b4dde429290390db_FeatureIcon2.svg" loading="lazy" alt=""></p><h2>Stellar local dev-experience</h2><p>What‚Äôs better than a local dev server? </p><p>Running the real system on your laptop or in your CI pipeline. No subtle quirks and differences between dev- and prod setups.<br></p><p>Your Restate-powered code is just functions/services. Develop them with the tools you know and love.<br></p></div></div><div data-w-id="c21848fa-07c8-b134-6d13-aa60a989b3e8"><div><h2>Restate Cloud:&nbsp;The zero-infrastructure option</h2><p>Get a fully serverless Restate experience, managed by the developers of the system.‚Ä®<br>Sign in, generate keys, point your app, go!</p><p><a href="https://cloud.restate.dev/" target="_blank">&gt; Get Access</a></p></div><p><img src="https://cdn.prod.website-files.com/663272930a67769123cdcf53/66551349e3bd7f3d6a22e6a9_Clouds.svg" loading="lazy" alt=""></p></div><div id="w-node-df836a90-d340-351a-8bf6-bfbc18c0615f-23cdcf59"><p id="w-node-_27c261b3-325b-c04f-1f2f-fb69f494eaf8-23cdcf59"><h2>Ready to <br><span>get started?</span></h2></p></div><div><p>Copyright ¬© 2024 Restate. All rights reserved.</p></div>

<!-- üíô MEMBERSCRIPT #92 v0.1 üíô TURN ANYTHING INTO A LINK -->






</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Star botanist likely made up data about nutritional supplements, new probe finds (185 pts)]]></title>
            <link>https://www.science.org/content/article/star-botanist-likely-made-data-about-nutritional-supplements-new-probe-finds</link>
            <guid>40658901</guid>
            <pubDate>Wed, 12 Jun 2024 14:59:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/article/star-botanist-likely-made-data-about-nutritional-supplements-new-probe-finds">https://www.science.org/content/article/star-botanist-likely-made-data-about-nutritional-supplements-new-probe-finds</a>, See on <a href="https://news.ycombinator.com/item?id=40658901">Hacker News</a></p>
Couldn't get https://www.science.org/content/article/star-botanist-likely-made-data-about-nutritional-supplements-new-probe-finds: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Intel is trucking a 916,000-pound 'Super Load' across Ohio to its new fab (234 pts)]]></title>
            <link>https://www.tomshardware.com/pc-components/cpus/intel-is-trucking-a-916000-pound-super-load-across-ohio-to-its-new-fab-spawning-road-closures-over-nine-days</link>
            <guid>40658095</guid>
            <pubDate>Wed, 12 Jun 2024 13:45:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/pc-components/cpus/intel-is-trucking-a-916000-pound-super-load-across-ohio-to-its-new-fab-spawning-road-closures-over-nine-days">https://www.tomshardware.com/pc-components/cpus/intel-is-trucking-a-916000-pound-super-load-across-ohio-to-its-new-fab-spawning-road-closures-over-nine-days</a>, See on <a href="https://news.ycombinator.com/item?id=40658095">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ-1920-80.jpg.webp 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ-320-80.jpg" alt="Ohio Department of Transportation" srcset="https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ-1920-80.jpg 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ.jpg"></picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/Uef74YhnJN4cS75qmWbgQZ.jpg">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span itemprop="copyrightHolder">(Image credit: Ohio Department of Transportation)</span>
</figcaption>
</div>

<div id="article-body">
<p>Ohio is seeing the effects of Intel's growth, but maybe not in the way state officials had hoped. Intel will put a 916,000-pound "super load" on the road in Ohio <a data-analytics-id="inline-link" href="https://www.dispatch.com/story/news/local/2024/06/11/intel-super-load-to-move-through-central-ohio-starting-sunday/74057074007/" data-url="https://www.dispatch.com/story/news/local/2024/06/11/intel-super-load-to-move-through-central-ohio-starting-sunday/74057074007/" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">on Wednesday</a>, for a trip that will cover approximately 150 miles in nine days and snarl traffic for over a week. The price of progress!</p><p>Intel's <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/intel-pushes-launch-date-of-ohio-fab-from-2025-to-2027-or-2028-state-politicians-remain-enthusiastic-about-progress" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/semiconductors/intel-pushes-launch-date-of-ohio-fab-from-2025-to-2027-or-2028-state-politicians-remain-enthusiastic-about-progress">new campus coming to New Albany, OH</a>, is in heavy construction, and around 20 super loads are being ferried across Ohio's roads by the Ohio Department of Transportation after arriving at a port of the Ohio River via barge. Four of these loads, including the one hitting the road now, weigh around 900,000 pounds ‚Äî that's 400 metric tons, or 76 elephants. The super loads were <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/delivery-of-equipment-to-intels-ohio-fab-delayed-for-several-weeks" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/semiconductors/delivery-of-equipment-to-intels-ohio-fab-delayed-for-several-weeks">first planned for February</a> but were delayed due to the immense planning workload. Large crowds are estimated to accumulate on the route, potentially slowing it even further.</p><p>Intel's 916,000-pound shipment is a "cold box," a self-standing air-processor structure that facilitates the cryogenic technology needed to fabricate semiconductors. The box is 23 feet tall, 20 feet wide, and 280 feet long, stretching longer than a football field. The immense scale of the cold box necessitates a transit process that moves at a "parade pace" of 5-10 miles per hour.</p><figure><blockquote><p>There's a lot of moving parts to it. It's not just jump in a truck and go</p><figcaption><cite>Matt Bruning, ODOT press secretary</cite></figcaption></blockquote></figure><p>Intel is taking over southern Ohio's roads for the next several weeks and months as it builds its new Ohio One Campus, a $28 billion project to create a 1,000-acre campus with two chip factories and room for more. Calling it the new "Silicon Heartland," the project will be the first leading-edge semiconductor fab in the American Midwest, and once operational, will get to work on the "Angstrom era" of Intel processes, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/intel-displays-arrow-lake-wafer-with-20a-process-node-chips-arrive-in-2024" data-before-rewrite-localise="https://www.tomshardware.com/news/intel-displays-arrow-lake-wafer-with-20a-process-node-chips-arrive-in-2024">20A and beyond</a>. Beyond bringing jobs to the region, Intel seeks to make nice with Ohio by investing millions into local schools and universities to provide local students with the tools to grow up to work at the foundries.</p><p>The cold box and the other super loads to come after it required immense planning from ODOT. "There's a lot of moving parts to it. It's not just jump in a truck and go from point A to point B," said Matt Bruning, ODOT press secretary. "There's a lot of planning and coordination and analysis that goes with doing a move like that." The Department of Transportation has been planning the route for months, ensuring that bridges and roadways could handle the loads coming for them. Power lines were moved underground or extended so work crews could lift them over the loads.&nbsp;</p><p>The <a data-analytics-id="inline-link" href="https://www.transportation.ohio.gov/about-us/traffic-advisories/district-9/superload" data-url="https://www.transportation.ohio.gov/about-us/traffic-advisories/district-9/superload" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Ohio Department of Transportation has shared a timetable</a> for how long they will be dealing with the super loads. Bruning shared that other companies are piggybacking on the super load route plans now that accommodations have already been made. "It is kind of abnormal to see this many in this close succession. Usually, you have a couple, and you may not see another load like that for years," he said. The summer of road closures is here for Ohio, thanks to Intel.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-xE3TGq23Eyoz89kJDTMjrL"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div>
</div>




<!-- Drop in a standard article here maybe? -->



</section>





<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Dataset Search Engine (101 pts)]]></title>
            <link>https://datasetsearch.research.google.com/</link>
            <guid>40657937</guid>
            <pubDate>Wed, 12 Jun 2024 13:28:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://datasetsearch.research.google.com/">https://datasetsearch.research.google.com/</a>, See on <a href="https://news.ycombinator.com/item?id=40657937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://datasetsearch.research.google.com/help">Learn more</a> about Dataset Search.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Serious Sam handled massive amounts of enemies on 56k modem connections (181 pts)]]></title>
            <link>https://staniks.github.io/articles/serious-engine-networking-analysis</link>
            <guid>40657574</guid>
            <pubDate>Wed, 12 Jun 2024 12:55:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://staniks.github.io/articles/serious-engine-networking-analysis">https://staniks.github.io/articles/serious-engine-networking-analysis</a>, See on <a href="https://news.ycombinator.com/item?id=40657574">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p><img alt="Banner" src="https://staniks.github.io/img/articles/serious-engine/banner.jpg" title="Banner"></p>
<p>Croteam released the <a href="https://github.com/Croteam-official/Serious-Engine">Serious Engine 1 source code</a> under GNU GPL v2 in 2016, and I've wanted to check it out for quite a while now. My observations here are based on reading and debugging this particular codebase and not reverse-engineering the classics released on GOG and Steam. Keep in mind that comments in code snippets have been replaced to provide more context. Also, some of my conclusions here may be wrong, so you can send me a message over at <a href="https://twitter.com/Sklopec">@Sklopec</a> if you feel like something needs correction.</p>
<blockquote>
<p><strong>NOTE:</strong> <em>This isn't an in-depth technical analysis, but an overview with more focus on the concepts rather than the implementation. I have skipped over a lot of things for the sake of simplicity. Also, the following sections assume you have at least a vague idea of how Serious Sam looks and plays.</em></p>
</blockquote>
<h2>Table of Contents</h2>
<div>
<ol>
    <li>
        <a href="#overview">Overview</a>
        <ol>
            <li><a href="#floating-point-determinism">Floating Point Determinism</a></li>
            <li><a href="#tick-vs-frame">Tick vs. Frame</a></li>
        </ol>
    </li>
    <li>
        <a href="#networked-multiplayer">Networked Multiplayer</a>
        <ol>
            <li>
                <a href="#the-packet-layer">The Packet Layer</a>
                <ol>
                    <li><a href="#the-lifecycle-of-a-connection">The Lifecycle of a Connection</a></li>
                    <li><a href="#master-buffers">Master Buffers</a></li>
                    <li><a href="#packet-routing">Packet Routing</a></li>
                    <li><a href="#establishing-a-connection">Establishing a Connection</a></li>
                    <li><a href="#reliability">Reliability</a></li>
                    <li><a href="#offline-play">Offline Play</a></li>
                </ol>
            </li>
            <li>
                <a href="#the-message-layer">The Message Layer</a>
                <ol>
                    <li><a href="#message-compression">Message Compression</a></li>
                    <li><a href="#message-security">Message Security</a></li>
                    <li><a href="#message-dispatcher">Message Dispatcher</a></li>
                </ol>
            </li>
             <li>
                <a href="#the-game-session-layer">The Game Session Layer</a>
                <ol>
                    <li><a href="#hosting-a-game">Hosting a Game</a></li>
                    <li><a href="#joining-a-game">Joining a Game</a></li>
                    <li><a href="#starting-a-demo-playback">Starting a Demo Playback</a></li>
                    <li><a href="#the-main-loop">The Main Loop</a></li>
                    <li><a href="#prediction">Prediction</a></li>
                </ol>
            </li>
        </ol>
    </li>
    <li>
        <a href="#conclusion-and-additional-thoughts">Conclusion and Additional Thoughts</a>
        <ol>
            <li><a href="#comparison-with-doom-and-quake">Comparison with Doom and Quake</a></li>
            <li><a href="#message-portability">Message Portability</a></li>
            <li><a href="#final-thoughts">Final Thoughts</a></li>
        </ol>
    </li>
</ol>
</div>

<h2>Changelog</h2>
<ul>
<li><strong>2020-11-05</strong> - published.</li>
<li><strong>2024-06-11</strong> - fix typo.</li>
</ul>
<h2>Overview <a name="overview"></a></h2>
<hr>
<p><strong>Serious Sam</strong> was built from the ground up as a multiplayer game. In a way, it's multiplayer even when you're playing the singleplayer campaign. While this idea may seem unusual at first, it's really just a clever way of abstraction. Let's explore how it works.</p>
<p>Serious Engine supports:</p>
<ul>
<li>singleplayer - offline campaign</li>
<li>multiplayer - online, LAN or local co-op and various game modes<ul>
<li>supports multiple players on the same client via split-screen!</li>
</ul>
</li>
<li>demo recording and playback</li>
</ul>
<p>Let's look at the demo functionality first. Serious Engine allows recording and reproduction of gameplay clips or <em>demos</em>. Both multiplayer and singleplayer game sessions can be recorded. In order to record a game, the most naive solution would be to persist the game state of every tick into a file.</p>
<p>However, such an approach has a problem - demo files would be ridiculously large.</p>
<p>Instead, Serious Engine records the entire game state at the beginning of the recording, and then, each tick, records something called <strong>game stream blocks</strong>. For now, think of these as messages which describe events in the game. They can be of these types:</p>
<pre><code>MSG_SEQ_ALLACTIONS,      // Player actions. See below.
MSG_SEQ_ADDPLAYER,       // Add a new player to the game.
MSG_SEQ_REMPLAYER,       // Remove a player from the game.
MSG_SEQ_PAUSE,           // Pause or unpause the game.
MSG_SEQ_CHARACTERCHANGE, // Change an aspect of player's character.
</code></pre>
<p>It's not important that you understand these context of these message types right now - we'll get to that later. For now, let's focus on message type <code>MSG_SEQ_ALLACTIONS</code>, because this is key to understanding how the whole thing works. This particular message type is processed in <code>CSessionState::ProcessGameTick</code>:</p>
<pre><code>FOREACHINSTATICARRAY(ses_apltPlayers, CPlayerTarget, itplt) {
    if (itplt-&gt;IsActive()) {
        // Extract action from message passed as parameter.
        CPlayerAction paAction;
        nmMessage&gt;&gt;paAction;

        // Apply the action to the CPlayerTarget.
        itplt-&gt;ApplyActionPacket(paAction);
    }
}
</code></pre>
<p>The engine deserializes several <code>CPlayerAction</code> objects from the message, one for each active player (since multiplayer games can be recorded as well), and applies these packets. Let's take a look at the <code>CPlayerAction</code> class to see what these packets actually are.</p>
<pre><code>class ENGINE_API CPlayerAction {
public:
    FLOAT3D pa_vTranslation;
    ANGLE3D pa_aRotation;
    ANGLE3D pa_aViewRotation;
    ULONG   pa_ulButtons;
    __int64 pa_llCreated;

    // ...
}
</code></pre>
<p><code>CPlayerAction</code> describes the player's state:</p>
<ul>
<li>player character velocity in world-space (<code>pa_vTranslation</code>)</li>
<li>player character rotation in world-space (<code>pa_aRotation</code>)</li>
<li>player view rotation in world-space (<code>pa_aViewRotation</code>)</li>
<li>buttons currently held down (<code>pa_ulButtons</code>, application defined, independent of control mapping scheme)</li>
<li>timestamp in milliseconds (<code>pa_llCreated</code>, from <a href="https://en.wikipedia.org/wiki/Time_Stamp_Counter">TSC</a>)</li>
</ul>
<p>These messages are generated by the Engine each tick during gameplay as the player interacts with the game (presses buttons, moves the mouse and/or thumbsticks). The messages are continuously serialized during recording and written into the demo file.</p>
<p>So how does reproduction work? The idea is simple - the Engine assumes everything in the game is completely predictable, and the players are the only ones with the power to change things. So in order to record the demo, the Engine only needs to record the entire game state once, and then only record the actions players perform each tick. In order to perform playback, the Engine deserializes the initial game state from the demo file, and then deserializes and applies player actions each tick as if the player was playing the game.</p>
<p>Neat, isn't it?</p>
<p>There's a caveat, though - <strong>this means the Engine's game model has to be completely deterministic.</strong> And it is. You can see an example of this if you peek into the <code>CEntity</code> implementation:</p>
<pre><code>ULONG CEntity::IRnd(void)
{
    return ((_pNetwork-&gt;ga_sesSessionState.Rnd()&gt;&gt;(31-16))&amp;0xFFFF);
}
</code></pre>
<p>where <code>CSessionState::Rnd()</code> is a pseudo-random number generator whose seed is part of the game state and is therefore initialized during game state deserialization:</p>
<pre><code>void CSessionState::Read_t(CTStream *pstr)  // throw char *
{
    // ...
    (*pstr)&gt;&gt;ses_ulRandomSeed;
    // ...
</code></pre>
<p>This makes sure the Engine is able to reproduce the exact same scenario every time we play the demo. If we would, say, use a truly random number generator for some game logic, or even a pseudo-random generator with differing seed, we would get different results every time - the famous <strong>desynchronization</strong>.</p>
<h2>Floating Point Determinism <a name="floating-point-determinism"></a></h2>
<p>There's also the matter of potential desynchronization due to floating point numbers. However, since Serious Sam on PC was originally released on Windows only, they could get away with using one compiler for everything, thus eliminating any sync issues that would emerge due to differences in C runtime library, like different implementations of trigonometry functions.</p>
<p>Similar issues can also arise due to differences in FPU precision. For example, the renderers are DLLs, and different clients might use different renderers. Renderers call various APIs (OpenGL, DirectX), and function calls in some of them might set FPU precision to different than expected. Serious Engine seems to have that covered as well. You can see precision guards like these, sprinkled around:</p>
<pre><code>CSetFPUPrecision FPUPrecision(FPT_24BIT);
</code></pre>
<p>Upon this object's construction, <code>_control87</code> function (MSVC specific) is used to cache the current FPU precision, then apply the new one. Once the object goes out of scope, the cached FPU precision is restored.</p>
<p>In theory, problems like these could also occur due to rounding control, but I haven't seen it explicitly set anywhere in the engine. There's this assert though, but this is just a query.</p>
<pre><code>ASSERT((_controlfp(0, 0)&amp;_MCW_RC)==_RC_NEAR);
</code></pre>
<p>Maybe it just wasn't that big of a deal - perhaps the rounding differences would be small enough not to accumulate significantly over the relatively short time that a session lasts, and thus, not produce any noticeable desynchronization.</p>
<h2>Tick vs. Frame <a name="tick-vs-frame"></a></h2>
<p>Notice how I use the word <strong>tick</strong> instead of <strong>frame</strong>. This is because the game logic tickrate is decoupled from the rendering framerate. Rendering framerate varies depending on the hardware and settings, but seems to be capped at <strong>500 frames per second</strong> internally. However, the game logic rate is constant and limited to <strong>20 ticks per second</strong>. But why do we see smooth movement and animation?</p>
<p><strong>Interpolation</strong>. Serious Engine interpolates between the current and the previous game tick based on time passed between. Try opening the in-game console (<code>~</code> key) and typing this to see how the game looks and feels without interpolation:</p>
<pre><code>/net_bLerping=0
</code></pre>
<p>It's kind of like playing a modern console exclusive. So how does Serious Engine smooth this out?</p>
<p>Animations and movement are interpolated with simple linear interpolation (lerp):</p>
<pre><code>interpolated_state = old_state + (new_state - old_state) * factor;
</code></pre>
<p>where <code>factor</code> is a floating point value in range <code>[0.0f, 1.0f]</code>. The factor in a particular moment in time is calculated as follows (pseudocode):</p>
<pre><code>// Time is in seconds.
float real_delta = time_since_session_started;
float tick_delta = time_of_last_tick - time_of_first_tick;

// 20 FPS logic framerate.
static constexpr float tick_quantum = 1 / 20.0f;

float factor2 = 1.0f - (tick_delta - real_delta) / tick_quantum;
</code></pre>
<p>Or illustrated, if don't mind my terrible handwriting...</p>
<p><img alt="Ticks explained" src="https://staniks.github.io/img/articles/serious-engine/frametick.png" title="Ticks explained"></p>
<p>You can also see the implementation in <code>CSessionState::SetLerpFactor</code>. You will notice there are two interpolation (<code>Lerp</code>) factors - one is for predicted movement, and one is for non-predicted. For now, don't worry about predicted movement - we'll get to prediction and explain how it works later.</p>
<p>Now that we've covered the basic concept of the demo recording and reproduction, think about this: instead of recording the course of the game into a file to be reproduced later, we could send it over the network to be reproduced in real time as we play the game with another person. That is the basic idea of Serious Engine multiplayer.</p>
<h2>Networked Multiplayer <a name="networked-multiplayer"></a></h2>
<hr>
<p>Unfortunately, the internet is a much more complicated environment than a file on your disk drive. Serious Sam is a fast paced game, and making things work fast over the internet is somewhat tricky, especially if you consider the fact that Serious Sam came out in the early 2000s, when a noticeable amount of people were still using 56k modems.</p>
<p>As you may have already guessed, Serious Sam employs a multiplayer model in which every player runs their own simulation and merely receives instructions on what the players have done, much like the demo system. If you glance at the code, you might see function names like <code>CNetworkLibrary::StartPeerToPeer_t</code>, but this is somewhat misleading - Serious Sam's networking isn't really peer to peer, even though the logic is processed akin to the old lockstep multiplayer games.</p>
<p>Serious Engine's networking model is actually client-server.</p>
<p>The basic idea is that, for a single multiplayer session, there is a single server, and the clients connect to it. The server receives messages from clients, processes them, and relays relevant information to all the clients. The clients then use this information to advance the state of their simulation.</p>
<p>This concept introduces the server as the "middleman" and avoids a myriad of issues which could emerge in a classic peer-to-peer model. For example, in case of desynchronization in pure peer-to-peer, it isn't trivial to determine whose game state is legitimate. Even worse, since public IPv4 addresses are in short supply, many people play behind NATs, and directly interfacing with such clients via UDP would often involve ugly hacks like NAT hole punching, or may not even work at all.</p>
<h2>The Packet Layer <a name="the-packet-layer"></a></h2>
<p>Serious Engine uses UDP - a connectionless, "fire and forget" protocol. UDP packets begin with a struct like this, followed by packet data.</p>
<pre><code>struct udp_packet_header_t
{
    uint16_t src_port; // Source port.
    uint16_t dst_port; // Destination port.
    uint16_t length;   // Packet length (including the header).
    uint16_t checksum; // Checksum.
};
</code></pre>
<p>UDP packets can arrive at their destination out of order, or may not arrive at all. This is a significant problem when playing over the internet, so Croteam implemented their own, custom protocol on top of UDP to achieve reliability and packet ordering. Let's take a look at the <code>CPacket</code> structure.</p>
<blockquote>
<p><strong>NOTE:</strong> This isn't actually what's being transmitted, but rather an internal representation. <code>pa_pubPacketData</code> is the data that will eventually end up being sent over the network.</p>
</blockquote>
<pre><code>class CPacket {
public:
    ULONG       pa_ulSequence;
    UBYTE       pa_ubReliable;
    SLONG       pa_slSize;
    SLONG       pa_slTransferSize;
    UBYTE       pa_ubRetryNumber;
    CTimerValue pa_tvSendWhen;
    UBYTE       pa_pubPacketData[MAX_PACKET_SIZE];
    CListNode   pa_lnListNode;
    CAddress    pa_adrAddress;

    // ...
};
</code></pre>
<p>Packet ordering and deduplication is achieved via <strong>sequence number</strong> (<code>CPacket::pa_ulSequence</code>). This is incremented every time the engine sends a packet. When packets are received or prepared for sending, they are inserted into a corresponding packet buffer (<code>CPacketBuffer</code>), at a position based on this sequence number (i.e. packet of highest index is appended to the end of the buffer). When a packet with an already encountered sequence number is received, it is discarded to prevent duplication.</p>
<p>Reliability is handled via <code>CPacket::pa_ubReliable</code> flag field. The basic idea is to have two types of packets.</p>
<ul>
<li>
<p><strong>Unreliable packets</strong> - these are sent and discarded. They are used when the Engine doesn't care if the message has reached the destination or not.</p>
</li>
<li>
<p><strong>Reliable packets</strong> - when these are sent, Serious Engine expects to get an <strong>acknowledge packet</strong> (in further text: <strong>ACK</strong>) to confirm the destination has received the packet. In case the ACK isn't received after some time (timeout), the Engine sends the original packet again - this is called <strong>retransmission</strong>.</p>
</li>
</ul>
<p><code>CPacket::pa_ubReliable</code> is a flag field with the following flags:</p>
<pre><code>#define UDP_PACKET_UNRELIABLE       0
#define UDP_PACKET_RELIABLE         1
#define UDP_PACKET_RELIABLE_HEAD    2
#define UDP_PACKET_RELIABLE_TAIL    4
#define UDP_PACKET_ACKNOWLEDGE      8
#define UDP_PACKET_CONNECT_REQUEST  16
#define UDP_PACKET_CONNECT_RESPONSE 32
</code></pre>
<p>If a packet is to be considered reliable, Serious Engine sets the <code>UDP_PACKET_RELIABLE</code> flag. It is also worth noting that reliable packets can form streams to carry more data than fits into a single packet. The Engine adds <code>UDP_PACKET_RELIABLE_HEAD</code> flag to the first packet in the stream, and <code>UDP_PACKET_RELIABLE_TAIL</code> to the last packet. If the Engine is sending a single reliable packet (i.e. not part of a stream), both <code>UDP_PACKET_RELIABLE_HEAD</code> and <code>UDP_PACKET_RELIABLE_TAIL</code> flags are set for that packet.</p>
<p>Unreliable packets can't form streams because that wouldn't make any sense - packet loss could result in a corrupted stream.</p>
<p><strong>Acknowledge (ACK)</strong> packets are sent for received reliable packets. ACK packets are unreliable by design, and only have <code>UDP_PACKET_ACKNOWLEDGE</code> flag set. A single ACK packet can contain acknowledgements for multiple reliable packets - just a series of <code>ULONG</code> (<code>unsigned long</code>) numbers, each representing a sequence number of a packet meant to be acknowledged.</p>
<p>In case no ACK is received for a packet, Serious Engine will attempt retransmission several times before closing the (virtual) connection to the client, and this is kept track of in <code>pa_ubRetryNumber</code>. Number of retries is specified with shell variable <code>net_iMaxSendRetries</code> and can be configured via console or configuration files. It seems to be <code>10</code> by default. It is worth noting that each retransmission will delay the next one by a certain amount of time. This is also configurable with shell variable <code>net_fSendRetryWait</code>, which seems to be <code>0.5f</code> by default. Each retransmission can occur only <code>net_fSendRetryWait</code> seconds after the previous.</p>
<p><code>CPacket::pa_tvSendWhen</code> keeps track of when the packet was supposed to be sent, not including the retransmission penalty. This isn't only used to calculate when the next retry should occur, but also serves as a simple congestion control mechanism to prevent flooding the client with more messages than they can handle in a certain amount of time. Serious Engine will attempt to approximate a good time in the future to send a particular packet based on the packet size, bandwidth limit, latency limit and latency variation. The latter two are used to simulate real network conditions and are configurable via shell variables (I believe this was only intended for debugging), while the bandwidth limit configuration is also exposed in the options menu:</p>
<p><img alt="Banner" src="https://staniks.github.io/img/articles/serious-engine/network-settings.png" title="Network options menu."></p>
<p>There options merely execute and persist shell commands in <code>.ini</code> files in <code>Scripts/NetSettings/</code>. For example, <code>ISDN.ini</code>:</p>
<pre><code>cli_bPrediction = 1;
cli_iBufferActions = 2;
cli_iMinBPS = 5000;
cli_iMaxBPS = 10000;
</code></pre>
<p>While prediction only affects the client, the latter three are communicated to the server upon establishing the virtual connection as <code>CSessionSocketParams</code>.</p>
<pre><code>class CSessionSocketParams {
public:
    INDEX ssp_iBufferActions;
    INDEX ssp_iMaxBPS;
    INDEX ssp_iMinBPS;
}
</code></pre>
<p>This allows the server to work with clients with varying connection speeds and quality. Packet time is approximated in <code>CPacketBufferStats::GetPacketSendTime</code>.</p>
<p>As for the rest of the <code>CPacket</code> fields, <code>CPacket::pa_slSize</code> represents the size of packet payload in bytes, while <code>CPacket::pa_slTransferSize</code> represents the size of the stream payload in bytes. In case the packet isn't part of a stream, these fields are equal.</p>
<p>These packets are the basics for higher-level constructs such as <code>CNetworkMessage</code>, which we'll cover soon. But first, let's take a look at how the packets are used in a real multiplayer session.</p>
<h3>The Lifecycle of a Connection <a name="the-lifecycle-of-a-connection"></a></h3>
<p><code>CCommunicationInterface</code> is the main class responsible for packet-layer communication. Among mostly uninteresting socket abstraction and handling, we have three sets of distinct member functions:</p>
<pre><code>// Send an unreliable packet to the specified client.
void Server_Send_Unreliable(INDEX iClient,
                            const void *pvSend,
                            SLONG slSendSize);

// Check if any unreliable packets have arrived from
// the specified client. If so, fill out the buffer
// and the size and return true.
BOOL Server_Receive_Unreliable(INDEX iClient,
                               void *pvReceive,
                               SLONG &amp;slReceiveSize);

// Also: reliable variation.
</code></pre>
<p>And these:</p>
<pre><code>// Send an unreliable packet to the server.
void Client_Send_Unreliable(const void *pvSend, SLONG slSendSize);

// Check if any unreliable packets have arrived from the server.
// If so, fill out the buffer and size and return true.
BOOL Client_Receive_Unreliable(void *pvReceive, SLONG &amp;slReceiveSize);

// Also: reliable variation.
</code></pre>
<p>But also these:</p>
<pre><code>// Sends a packet to a specified CAddress.
void Broadcast_Send(const void *pvSend,
                    SLONG slSendSize,
                    CAddress &amp;adrDestination);

// Check if there are any packets from any address.
// If so, fill out the buffer, size and address and
// return true.
BOOL Broadcast_Receive(void *pvReceive,
                       SLONG &amp;slReceiveSize,
                       CAddress &amp;adrAddress);
</code></pre>
<blockquote>
<p><strong>NOTE:</strong> If you find the above naming confusing, perhaps it'll help if you think of this as a polymorphic class, with derived classes like <code>CServerCommunicationInterface</code>, <code>CClientCommunicationInterface</code> and <code>CBroadcastCommunicationInterface</code>. But hey, static calls beat polymorphic indirection.</p>
</blockquote>
<p>Notice how the server and the client interfaces both assume the source or destination of the message is already known. In other words, it's assumed that the virtual connection between the client and the server is already established. However, if you look at the <strong>broadcast interface</strong>, you'll see that these methods can send and receive packets to and from <strong>any</strong> address - this is used to establish the connection. To understand how this works, we need to explore the concept of <strong>master buffers</strong> and <strong>packet routing</strong>.</p>
<h3>Master Buffers <a name="master-buffers"></a></h3>
<p><code>CCommunicationInterface</code> has two main (master) packet buffers - one for input, and one for output. Every time the Engine calls <code>CCommunicationInterface::UpdateMasterBuffers()</code>, the communication interface will do the following:</p>
<ol>
<li>Poll the socket API (Winsock) to check for and read any incoming UDP packets, deserialize them into <code>CPackets</code> and insert them into the master input buffer (<code>cci_pbMasterInput</code>).</li>
<li>Serialize and send out (via socket API) any <code>CPackets</code> in the master output buffer (<code>cci_pbMasterOutput</code>).</li>
</ol>
<p>Notice how this is very simple - UDP layer is very thin, and all the heavy lifting is done on the higher levels.</p>
<h3>Packet Routing <a name="packet-routing"></a></h3>
<p>This is where things get more interesting. Remember the three interface groups from <code>CCommunicationInterface</code>? They each actually just call the (mostly) same functions of a corresponding <code>CClientInterface</code>.</p>
<pre><code>CClientInterface cm_aciClients[SERVER_CLIENTS];
CClientInterface cm_ciBroadcast;
CClientInterface cm_ciLocalClient;
</code></pre>
<p>The purpose of <code>CClientInterface</code> is to abstract away the complexity of communicating with a client (or the server, if used by the client). When the application is the server, <code>cm_aciClients</code> array is used to provide an interface for each player in the game. If the application is the client, it uses <code>cm_ciLocalClient</code> to communicate with the server. <code>cm_ciBroadcast</code> is used by both the client and the server to establish the connection.</p>
<p><img alt="Communication Interface" src="https://staniks.github.io/img/articles/serious-engine/comminterface.png" title="Communication Interface"></p>
<p><code>CClientInterface</code> contains simple methods by design:</p>
<pre><code>// Sends a message through the interface.
void Send(const void *pvSend, SLONG slSize, BOOL bReliable);

// Broadcast variant.
void SendTo(const void *pvSend,
            SLONG slSize,
            const CAddress adrAdress,
            BOOL bReliable);
</code></pre>
<p>However, the implementation is a bit more complex. Internally, <code>CClientInterface</code> performs packet ordering and reliability. For this purpose, it contains four main buffers.</p>
<pre><code>CPacketBuffer ci_pbOutputBuffer;
CPacketBuffer ci_pbWaitAckBuffer;
CPacketBuffer ci_pbInputBuffer;
CPacketBuffer ci_pbReliableInputBuffer;
</code></pre>
<p>For now, let's focus on <code>ci_pbInputBuffer</code> and <code>ci_pbOutputBuffer</code>. As you may have guessed, these are the input and output packet buffers. Input buffer contains packets which were meant to be received by this client interface, and output buffer contains packet which are meant to be sent by this interface. But how do packets end up in <code>ci_pbInputBuffer</code>, and get out of <code>ci_pbOutputBuffer</code>?</p>
<p>They actually come from the <code>CCommunicationInterface</code>'s input master buffer, and end up in its output master buffer, but how does the Engine know which <code>CClientInterface</code> needs to receive a certain packet?</p>
<p>Packet routing!</p>
<p>If you go back a bit and look at the <code>CPacket</code> structure, you'll see it contains a <code>CAddress</code> object (<code>pa_adrAddress</code>). Let's have a look what this actually is.</p>
<pre><code>class CAddress {
public:
    ULONG adr_ulAddress;   // IPv4 address.
    UWORD adr_uwPort;      // UDP port.
    UWORD adr_uwID;        // Huh?
}
</code></pre>
<p><code>adr_uwID</code>, depending on its value, may carry either:</p>
<ul>
<li>unique identifier for a client</li>
<li>information that this is a broadcast packet</li>
</ul>
<p>If <code>adr_uwID</code> is equal to <code>'//'</code> (<code>0x2f2f</code>) or <code>0</code>, this packet was meant for, or came from the broadcast interface. Otherwise, it contains a unique client ID for this session.</p>
<p>So, if the packet is a broadcast packet, it's routed to the broadcast interface, otherwise it is routed to the corresponding client interface. You can see the routing logic performed in <code>Server_Update</code> and <code>Client_Update</code> methods of <code>CCommunicationInterface</code>.</p>
<h3>Establishing a connection <a name="establishing-a-connection"></a></h3>
<p>In order to connect to the server, the client must send a reliable broadcast packet with the <code>UDP_PACKET_CONNECT_REQUEST</code> flag. We can see this in <code>CCommunicationInterface::Client_OpenNet_t</code>:</p>
<pre><code>// Instantiate the connection request packet.
ppaInfoPacket = new CPacket;

// Set the flags.
ubReliable = UDP_PACKET_RELIABLE
             | UDP_PACKET_RELIABLE_HEAD
             | UDP_PACKET_RELIABLE_TAIL
             | UDP_PACKET_CONNECT_REQUEST;

// Set parameters and write a single-byte (useless) payload.
ppaInfoPacket-&gt;pa_adrAddress.adr_ulAddress = ulServerAddress;
ppaInfoPacket-&gt;pa_adrAddress.adr_uwPort = net_iPort;
ppaInfoPacket-&gt;pa_ubRetryNumber = 0;
ppaInfoPacket-&gt;WriteToPacket(&amp;ubDummy,
                             1,
                             ubReliable,
                             cm_ciLocalClient.ci_ulSequence++,
                             '//',
                             1);
</code></pre>
<p>When the server receives this packet, it will first check whether the client with this address and port (from which the packet came from) is already connected. If so, the packet is simply ignored. If not, the server will look for the first empty client interface and do the following:</p>
<ol>
<li>Generate the unique identifier for that client and assign it to the corresponding <code>CClientInterface</code>.</li>
<li>Send the unique identifier to the client via the <code>UDP_PACKET_CONNECT_RESPONSE</code> reliable broadcast packet.</li>
</ol>
<p>Identifier generation is pretty straightforward:</p>
<pre><code>// This isn't some cryptographic hash so the timer value will do.
UWORD uwID = _pTimer-&gt;GetHighPrecisionTimer().tv_llValue &amp; 0x0FFF;

// In case we're so unlucky we hit a broadcast packet marker,
// just increment by one.
if (uwID==0 || uwID=='//') {
    uwID+=1;
}

// Assign the ID to the client interface.
cm_aciClients[iClient].ci_adrAddress.adr_uwID = (uwID&lt;&lt;4)+iClient;
</code></pre>
<p>From the moment the server sends the response packet, the client is considered connected. The client will then use the provided <code>uwID</code> to identify themselves when sending packets to the server, and the server will properly route the packet to the corresponding client interface.</p>
<p>Why bother with <code>uwID</code>, though? Why not just assign an index?</p>
<p>It prevents impersonation attacks. The attacker would need to guess <code>uwID</code> of the player they would want to impersonate, so the attack surface is lowered. Sure, they could brute-force the <code>uwID</code> by spamming non-broadcast packets and receive an ACK at some point as confirmation, but that wouldn't be very subtle - non-broadcast packets from non-connected players will cause Serious Engine to emit a warning in the console. You can see this in <code>CCommunicationInterface</code> method <code>Server_Update</code>.</p>
<pre><code>// bClientFound - true if packet came from connected client.
if (!bClientFound) {
    // warn about possible attack
    extern INDEX net_bReportMiscErrors;
    if (net_bReportMiscErrors) {
        CPrintF(TRANS("WARNING: Invalid message from: %s\n"),
            AddressToString(ppaPacket-&gt;pa_adrAddress.adr_ulAddress));
    }
}
</code></pre>
<h3>Reliability <a name="reliability"></a></h3>
<p>Let's head back to the <code>CClientInterface</code> and have a look at these buffers again.</p>
<pre><code>CPacketBuffer ci_pbOutputBuffer;
CPacketBuffer ci_pbWaitAckBuffer;
CPacketBuffer ci_pbInputBuffer;
CPacketBuffer ci_pbReliableInputBuffer;
</code></pre>
<p><code>ci_pbWaitAckBuffer</code> is the buffer containing copies of reliable packets which have been sent. In case the Engine doesn't receive the ACK for these, it will attempt retransmission. The packets are copied into this buffer from <code>ci_pbOutputBuffer</code> before being sent into the master output buffer.</p>
<p><code>ci_pbReliableInputBuffer</code> contains ordered and deduplicated reliable packets. It's filled just after the packets are routed to the <code>ci_pbInputBuffer</code> from the master input buffer. The Engine iterates through packets in <code>ci_pbInputBuffer</code> and does several things:</p>
<ol>
<li>If the incoming packet is an ACK packet, remove the acknowledged packets from both <code>ci_pbWaitAckBuffer</code> and <code>ci_pbOutputBuffer</code>. Also remove the ACK packet from the input buffer.</li>
<li>If the incoming packet is reliable, then insert it into <code>ci_pbReliableInputBuffer</code>, but only if not already present (deduplication). Also remove the packet from the input buffer, and write it up for acknowledgement.</li>
<li>If the incoming packet is unreliable, leave it in the input buffer.</li>
<li>Generate an ACK packet (or packets) which contain acknowledges for each of the input packets written up for acknowledgement.</li>
</ol>
<p>It's a simple but elegant system.</p>
<h3>Offline Play <a name="offline-play"></a></h3>
<p>The singleplayer and demo reproduction are just a special case of multiplayer. We still have the server, and still have the client, but here they're the same process.</p>
<p>It would be somewhat ridiculous to use the network sockets to communicate with something in the same process, so the Engine establishes a simple shortcut. If we observe <code>CCommunicationInterface</code>, we can see this:</p>
<pre><code>void CCommunicationInterface::Client_OpenLocal(void)
{
    CTSingleLock slComm(&amp;cm_csComm, TRUE);

    CClientInterface &amp;ci0 = cm_ciLocalClient;
    CClientInterface &amp;ci1 = cm_aciClients[SERVER_LOCAL_CLIENT];

    ci0.ci_bUsed = TRUE;
    ci0.SetLocal(&amp;ci1);
    ci1.ci_bUsed = TRUE;
    ci1.SetLocal(&amp;ci0);
};
</code></pre>
<p><code>ci0</code> is the virtual client's <code>CClientInterface</code>, and <code>ci1</code> is the matching <code>CClientInterface</code> as it would be on the server. These client interfaces become paired in <code>CClientInterface::SetLocal</code>:</p>
<pre><code>void CClientInterface::SetLocal(CClientInterface *pciOther)
{
    // ...
    ci_pciOther = pciOther;
    // ...
}
</code></pre>
<p>When two client interfaces are paired, they can exchange buffers by calling <code>CClientInterface::ExchangeBuffers</code>. This will consume packets from one interface's output buffer and insert them into the other interface's input buffer, then vice versa. This eliminates the need for sending and receiving everything through master output and input buffers when playing locally.</p>
<p>Buffer exchange is performed in <code>CCommunicationInterface::Server_Update</code>.</p>
<p>And that pretty-much covers the packet layer overview.</p>
<p>There's a bit more going on in there than I laid out, but I suggest you consult the source code if you want to know more. After all, this is a conceptual overview, so I'd rather not bore the average reader to death with details.</p>
<p><code>CPackets</code> provide a neat layer above the UDP, but they're still somewhat low-level and awkward to use, at least directly. This is why Croteam introduced another layer above packets - <strong>network messages</strong>.</p>
<h2>The Message Layer <a name="the-message-layer"></a></h2>
<p><code>CNetworkMessage</code> is a message abstraction which can be read from and written into in a stream-like manner. Let's look at the data members:</p>
<pre><code>class ENGINE_API CNetworkMessage {
public:
    MESSAGETYPE nm_mtType; // Message type (enumeration).

    #define MAX_NETWORKMESSAGE_SIZE 2048
    UBYTE *nm_pubMessage;  // Buffer (allocated on heap).
    SLONG nm_slMaxSize;    // Buffer size.

    UBYTE *nm_pubPointer;  // Read/write pointer.
    SLONG nm_slSize;       // Message size (so far).
    INDEX nm_iBit;         // Next bit index to read/write.

    // ...
};
</code></pre>
<p>I was surprised to find that <code>nm_pubMessage</code> is allocated via <code>AllocMemory</code> which seems to just call <code>malloc</code> under the hood. In fact, memory is allocated this way all over the Engine. There's a <code>CLinearAllocator</code>, but doesn't seem to be used anywhere. <code>CNetworkMessage</code> buffers are allocated (and reallocated) quite often, so at some point, some people would argue that the heap could end up looking like swiss cheese.</p>
<p>Well, it's either that, or I missed a custom allocator implementation somewhere within the Engine codebase. But then again, it's not like you'd need a long-running server for this kind of game, so you probably wouldn't even notice; heap fragmentation usually becomes a problem when software is expected to work for days or even weeks.</p>
<p><code>CNetworkMessage</code> is meant to be written into and read from via simple interface:</p>
<pre><code>void Read(void *pvBuffer, SLONG slSize);
void Write(const void *pvBuffer, SLONG slSize);
void ReadBits(void *pvBuffer, INDEX ctBits);
void WriteBits(const void *pvBuffer, INDEX ctBits);
</code></pre>
<p>but also like a stream:</p>
<pre><code>inline CNetworkMessage &amp;operator&gt;&gt;(SLONG &amp;sl);
inline CNetworkMessage &amp;operator&gt;&gt;(SWORD &amp;sw);

// ...

inline CNetworkMessage &amp;operator&lt;&lt;(const SLONG &amp;sl);
inline CNetworkMessage &amp;operator&lt;&lt;(const SWORD &amp;sw);

// ...

void Rewind(void);
</code></pre>
<p>Messages can also contain submessages (serialized version of themselves). Once the message buffer contains all the data needed, the buffer can be reallocated to fit the data (<code>CNetworkMessage::Shrink</code>).</p>
<h3>Message Compression <a name="message-compression"></a></h3>
<p>It's also worth noting that messages can be compressed by either specifying a <code>Compressor</code> or using the default one based on <code>nm_mtType</code>. The enumeration (<code>MESSAGETYPE</code>) is actually just the lower 6 bits, while the remaining two indicate a type of compression used. This can be either:</p>
<ul>
<li>LZ77 (<code>CzlibCompressor</code>)</li>
<li>LZRW1 (<code>CLZCompressor</code>)</li>
<li>uncompressed</li>
</ul>
<p>LZRW1 seems to be used by default. This can be changed via shell variable <code>net_iCompression</code>, most likely just for development purposes.</p>
<p>Also, remember <code>CPlayerAction</code> from the beginning of the article? If we peek into <code>PlayerBuffer::CreateActionPacket</code>, we can see this piece of code:</p>
<pre><code>CPlayerAction paDelta;
for (INDEX i=0; i&lt;sizeof(CPlayerAction); i++) {
    ((UBYTE*)&amp;paDelta)[i] = ((UBYTE*)&amp;paCurrent)[i]
                            ^ ((UBYTE*)&amp;plb_paLastAction)[i];
}
</code></pre>
<p>The <code>CPlayerAction</code> here is being prepared for sending, but the structure itself isn't being sent, but rather its <em>delta</em>, which is just a result of a XOR operation between the current and the last player action sent.</p>
<p>Then again, in <code>PlayerTarget::ApplyActionPacket</code>, which is meant to be processed by the receiving end, we can see this:</p>
<pre><code>for (INDEX i=0; i&lt;sizeof(CPlayerAction); i++) {
    ((UBYTE*)&amp;plt_paLastAction)[i] ^= ((UBYTE*)&amp;paDelta)[i];
}
</code></pre>
<p>The <code>CPlayerAction</code> is being XOR-ed back, yielding the desired player action. But why go through all this trouble? Why not just send the <code>CPlayerAction</code>, thus avoiding calculating the delta and reconstructing the action structure?</p>
<p>Because a <strong>delta can be compressed more efficiently</strong> when the data hasn't changed much.</p>
<p>And in this particular case, data really doesn't change that much; for example, <code>CPlayerAction</code> contains information about keys being held down, and players often hold the same keys for a period over several frames, so it makes sense to minimize the amount of information being sent over the network (or being written to a file). Same goes for velocity and view rotation - they don't cover the full range of the floating point, so there's usually very little change there.</p>
<p>We might not see the benefit of this when sending a single client action (e.g. from client to server), but rather when they're being sent in bulk, as server does via <code>MSG_SEQ_ALLACTIONS</code>.</p>
<p>Neat trick, huh? It's actually a well-known and established concept called <a href="https://en.wikipedia.org/wiki/Delta_encoding">delta encoding</a>.</p>
<h3>Message Security <a name="message-security"></a></h3>
<p>Messages aren't encrypted. Most people would agree that observing a way someone dodges a Kleer or a Sirian Werebull is hardly a meaningful privacy violation.</p>
<p>However, that may not be true for chat messages.</p>
<p>If we put on our tinfoil hat and disable message compression:</p>
<pre><code>/net_iCompression=0
</code></pre>
<p>By sending a chat message in-game, we can see the UDP packet and its payload. Since it's transmitted in plaintext, we can see the whole message.</p>
<p><img alt="Wireshark output" src="https://staniks.github.io/img/articles/serious-engine/wireshrek.png" title="Wireshark output"></p>
<p>Sure, in real-case scenario, the compression would be enabled and someone sniffing for UDP packets would have to go through the trouble of figuring out this is a LZ-compressed stream and then decompress it, but they'd have everything they need in order to do it.</p>
<p>So yeah - the original Serious Sam multiplayer sessions might not be the best place to have very private conversations. But then again, it's not like people play this game to <em>slide into DMs</em>.</p>
<p>This isn't anything controversial or particularly concerning, though - most games from that time didn't deal with encryption simply because it wasn't necessary, or would perhaps increase complexity since it would require implementing mechanisms like authentication and key exchange.</p>
<p>Also, at the time, most of the web was still on HTTP.</p>
<h3>Message Dispatcher <a name="message-dispatcher"></a></h3>
<p><code>CMessageDispatcher</code> is essentially a wrapper around the packet layer.</p>
<p>It invokes <code>CCommunicationInterface</code>'s functions to send packets with the <code>CNetworkMessage</code>'s content, or receive a <code>CNetworkMessage</code> by reading the packet content.</p>
<p>Its job is also to prepare the <code>_cmiComm</code> (global <code>CCommunicationInterface</code>) for use based on the selected <code>CNetworkProvider</code>. This is actually just a description wrapper, and can be:</p>
<ul>
<li><code>Local</code></li>
<li><code>TCP/IP Server</code></li>
<li><code>TCP/IP Client</code></li>
</ul>
<p>"Preparation for use" here is basically just deciding whether to open the socket and how to open it.</p>
<ul>
<li><code>Local</code> is used when playing singleplayer or the demo recording. Winsock isn't initialized here, so no socket either.</li>
<li><code>TCP/IP Server</code> is used when hosting. The socket is opened on the port specified by <code>net_iPort</code> shell variable.</li>
<li><code>TCP/IP Client</code> is used when we're the client. It also opens the socket, but the Engine lets the socket API decide on the port, since it doesn't matter.</li>
</ul>
<h2>The Game Session Layer <a name="the-game-session-layer"></a></h2>
<p>To see network messages in action, let's take a step back and get a bit broader look at how Serious Engine manages a multiplayer game. We'll skip the outer layers dealing with platform specifics, timing and rendering, and just focus on game logic and communication, and that's mostly packed in <code>CNetworkLibrary</code>.</p>
<p>A seemingly unusual place for game logic, isn't it?</p>
<p><code>CNetworkLibrary</code>, despite its peculiar naming choice, is a class that houses and manages the game state (<code>CSessionState</code>), among other things. It's inherited from <code>CMessageDispatcher</code> we mentioned earlier.</p>
<p>The scope of <code>CNetworkLibrary</code> a bit wide and there's a lot going on there, so I'll rather attempt to simplify how the whole thing works without going into too much detail - if I start speaking in classes, this would become an unreadable mess.</p>
<h3>Hosting a Game <a name="hosting-a-game"></a></h3>
<p>Let's assume we want to start a server. Upon hosting the game (<code>CNetworkLibrary::StartPeerToPeer_t()</code>), the Engine will do the following:</p>
<ol>
<li>Initialize CRC (cyclic redundancy check) gathering. This is used later to determine whether the connecting clients have the same files as the server. This isn't a cheat prevention method, but rather a way to detect desynchronization early.</li>
<li>Create a new session state (<code>CSessionState</code>), serialize it and store it into <code>ga_pubDefaultState</code>. This is considered the default state and will be used as a baseline for calculating <strong>state deltas</strong> later on.</li>
<li>Load the local world instance.</li>
<li>Initialize the global communication interface.</li>
<li>Set up and initialize the local session state (<code>ga_sesSessionState</code>). When clients connect, they will receive a <strong>state delta</strong> - a difference between the default (baseline) state and the server's local state. This is required because clients can connect to a game already in progress. The local client is also initialized here (if not dedicated server).</li>
<li>Finish CRC gathering. At this point CRC of files is stored in <code>ga_ulCRC</code>. When clients connect, they will request a list of filenames to check (<code>MSG_REQ_CRCLIST</code>). The server will then send a list of filenames (<code>MSG_REQ_CRCCHECK</code>), and the client will produce a CRC of their copies of these files, then send it to the server (<code>MSG_REP_CRCCHECK</code>). If CRCs don't match, the client is disconnected.</li>
</ol>
<p>At this point, the server is considered up and running, and we have entered the game logic loop (<code>CNetworkLibrary::MainLoop()</code>).</p>
<h3>Joining a Game <a name="joining-a-game"></a></h3>
<p>Joining a game is done via <code>CNetworkLibrary::JoinSession_t()</code>. The function receives a <code>CNetworkSession</code> parameter which contains, among various session information, a server address. This is either instantiated via polling the GameAgent (part of the engine responsible for session discovery) or manually, via the class constructor. Upon joining the game, the client will do the following:</p>
<ol>
<li>Initialize CRC gathering, just like the server.</li>
<li>Set up and initialize an empty local session state.</li>
<li>Initialize the global communication interface.</li>
<li>Send a connection request message (<code>MSG_REQ_CONNECTREMOTESESSIONSTATE</code>). It contains the build version, mod name, server password, amount of local players on this client (in case of split-screen) and serialized <code>CSessionSocketParams</code> (connection quality information).</li>
<li>Wait for the response in form of <code>MSG_REP_CONNECTREMOTESESSIONSTATE</code>. It contains message of the day, world filename, spawn flags (difficulty, game mode) and session properties.</li>
<li>Initialize the baseline game state using received information (much like the server).</li>
<li>Send a <code>MSG_REQ_STATEDELTA</code> message. This requests a state delta between the baseline state (which <em>should</em> be equal on the client and the server) and the server's current local state.</li>
<li>Await a response in form of <code>MSG_REP_STATEDELTA</code>. Upon decompression, a reverse diff is performed to reconstruct the game state stream.</li>
<li>Initialize the local session state with the reconstructed stream via <code>CSessionState::Read_t()</code>.</li>
<li>Perform a CRC check with the server (<code>MSG_REQ_CRCLIST</code>/<code>MSG_REP_CRCCHECK</code>). Disconnect in case of mismatch.</li>
</ol>
<p>At this point, the client is considered connected to the server and the Engine will enter the game loop, just like the server.</p>
<h3>Starting a Demo Playback <a name="starting-a-demo-playback"></a></h3>
<p>Demo playback is initialized via <code>CNetworkLibrary::StartDemoPlay_t()</code>. It receives a filename string as a parameter. In comparison to multiplayer, it's very simple:</p>
<ol>
<li>Parse the demo file, read the header and the version.</li>
<li>Initialize <code>ga_sesSessionState</code> with the serialized game state from this point in the file stream.</li>
</ol>
<p>At this point, the client is playing the demo and will enter the main loop.</p>
<h3>The Main Loop <a name="the-main-loop"></a></h3>
<p>The main loop is actually very similar for both the client and the server, with a few exceptions.</p>
<ol>
<li>Update<sup>*</sup> the local client communication interface (<code>cm_ciLocalClient</code>) and the broadcast communication interface (<code>cm_ciBroadcast</code>).</li>
<li>Have the local session state handle the incoming network messages.</li>
<li>[<code>SERVER ONLY</code>] Exchange buffers between paired client interfaces, then update each of the server-side client communication interfaces (<code>cm_aciClients</code> array). Also update the local client and the broadcast interface again.</li>
<li>Have the local session state process its game stream.</li>
<li>[<code>SERVER ONLY</code>] Handle GameAgent update (stuff for server browser).</li>
<li>[<code>SERVER ONLY</code>] Handle remote administration shell commands, if any were sent since the last iteration.</li>
</ol>
<p>(<sup>*</sup>) <em>Updating a communication interface is essentially updating its four main buffers, performing message routing, etc. See packet layer section.</em></p>
<p><code>CSessionState</code> handles incoming network messages via <code>SessionStateLoop()</code> function. Let's look at this function more closely. It handles the following message types:</p>
<p><em>Unreliable messages</em></p>
<ul>
<li><code>MSG_GAMESTREAMBLOCKS</code> - message containing game stream blocks. These are stored into the state's internal buffer to be processed later.</li>
<li><code>MSG_KEEPALIVE</code> - when received, use current time as time of session start.</li>
<li><code>MSG_INF_PINGS</code> - message containing pings of all players.</li>
<li><code>MSG_CHAT_OUT</code> - message containing a chat message.</li>
</ul>
<p><em>Reliable messages</em></p>
<ul>
<li><code>MSG_INF_DISCONNECTED</code> - message containing the reason why the client was disconnected.</li>
<li><code>MSG_ADMIN_RESPONSE</code> - message containing remote administration response.</li>
</ul>
<p>For some of these messages, responses are generated right here. However, notice how <code>MSG_GAMESTREAMBLOCKS</code> is an unreliable message. But isn't this information important? <strong>Would we not desync immediately if we miss even just one of these messages?</strong></p>
<p>We definitely would. But retransmission logic for this is handled later, when the local session state processes its game stream - <code>CSessionState::ProcessGameStream</code>. If we look into it, we can see this patch of code:</p>
<pre><code>// Calculate the index of the next expected sequence.
INDEX iSequence = ses_iLastProcessedSequence+1;

// Get the stream block with that sequence.
CNetworkStreamBlock *pnsbBlock;
CNetworkStream::Result res = ses_nsGameStream.GetBlockBySequence(iSequence,
                                                                 pnsbBlock);
</code></pre>
<p>Three things can happen when fetching a game stream block by sequence:</p>
<ol>
<li>The block with the next expected sequence <strong>is</strong> found. In this case, we continue onto processing the block (<code>CSessionState::ProcessGameStreamBlock</code>).</li>
<li>The block with the next expected sequence <strong>isn't found</strong>, but we don't have any more recent blocks (i.e. with larger sequence number). In this case we don't do anything this iteration of the main loop.</li>
<li>The block with the next expected sequence <strong>isn't found</strong>, but we <strong>already have at least one more recent block.</strong> This means the block might have been lost, and we may have to perform retransmission.</li>
</ol>
<p>However, there is no always need for retransmission in case of (3). The block may have simply been late due to nature of UDP. Instead of requesting retransmission immediately when we encounter a missing block, we mark this sequence as missing and set up a timeout. Then, next time the main loop ends up here and the timeout has passed, we send a retransmission request (<code>MSG_REQUESTGAMESTREAMRESEND</code>) which contains:</p>
<ul>
<li>the sequence of the missing block</li>
<li>number of missing blocks, determined by looking up the difference between the most recent received block sequence and the missing block sequence</li>
</ul>
<p>The server will then re-send these blocks.</p>
<p>Let's head over to <code>CSessionState::ProcessGameStreamBlock</code> to see how the game stream blocks actually get processed. Just to get your bearings:</p>
<pre><code>CNetworkLibrary::MainLoop();
    ga_sesSessionState.ProcessGameStream();
        ProcessGameStreamBlock(*pnsbBlock); // We're here!
</code></pre>
<p>Remember the game stream block types from the beginning of the article? We're finally here.</p>
<p><code>MSG_SEQ_ADDPLAYER</code> is sent when a player joins the game. It contains a player index and a <code>CPlayerCharacter</code> descriptor (guid, name, team, appearance). When received, the local game session state will check whether the corresponding <code>CPlayerEntity</code> already exists in the game world - in case a player was disconnected and is reconnecting. If not, a new <code>CPlayerEntity</code> will be added to the game world. In any case, the entity becomes linked with a corresponding player target. <code>CPlayerTarget</code> is a utility class to which player actions are passed and which applies these actions to the linked player entity.</p>
<p><code>MSG_SEQ_REMPLAYER</code> is sent when a player is disconnected from the game. It contains just the player index. When received, the player entity is disconnected from its player target, and the player target is deactivated.</p>
<p><code>MSG_SEQ_CHARACTERCHANGE</code> is sent when a player changes an aspect of their character. It contains a player index and a <code>CPlayerCharacter</code> descriptor. The player can change their name, team or appearance. Appearance seems to be application specific - it's just a 32-byte buffer. In Serious Sam, this buffer houses a <code>CPlayerSettings</code> structure which contains the filename of the player model, weapon auto select policy, crosshair type and various flags (all customizable via in-game options menu).</p>
<p><code>MSG_SEQ_PAUSE</code> is sent when the game gets paused or unpaused by someone. It contains whether the game should be paused or unpaused (<code>BOOL</code>) and a string containing the name of the player who requested the change. It affects <code>CSessionState::ses_bPause</code> - when false, the game state is not advanced, nor the player actions are being generated. When this message is received, the pauser's name is printed in the console. <em>As it should be.</em></p>
<p><code>MSG_SEQ_ALLACTIONS</code> is the most interesting of these. It contains a floating point header which represents time - this is additionally used for diagnostics, to emit a warning in case these blocks are sent too often. It's then passed to <code>ProcessGameTick()</code>, along with the rest of the message.</p>
<p>The time value is used as a current tick time in the local session. The session's player targets are being iterated here, and for each, the Engine deserializes a <code>CPlayerAction</code> from the message and then applies it to the player target. After the player targets are done, we have this:</p>
<pre><code>// Update timers. Generate events, etc.
HandleTimers(tmCurrentTick);
// Handle moving entities, physics.
HandleMovers();
</code></pre>
<p>I could go into this, but I'd rather keep the scope of this tutorial limited to networking. This logic is something that everyone's local game state performs regardless of networking, and the only important thing is that everyone does it exactly the same. We have a synchronization check to confirm that:</p>
<pre><code>MakeSynchronisationCheck();
</code></pre>
<p>This will iterate through various objects with <code>ChecksumForSync()</code> (entities, player targets...) and produce a <code>CSyncCheck</code> object which contains the CRC with some additional info. If <code>CSyncCheck</code> is produced on the server, it's buffered.</p>
<p>Upon the check, <code>MSG_SYNCCHECK</code> message containing the <code>CSyncCheck</code> is sent to the server and the server disconnects the client if there's a discrepancy in relation to the server's local state.</p>
<p>And that's mostly it. Sure, there's a lot more going on in this loop than I covered, but then this analysis wouldn't be a short analysis anymore - let's keep it simple.</p>
<h3>Prediction <a name="prediction"></a></h3>
<blockquote>
<p><strong>NOTE:</strong> I purposefully omitted prediction-related stuff from the main loop explanation to avoid confusion, but we'll cover the most relevant stuff here.</p>
</blockquote>
<p>Did you ever hook up your PC or a videogame console to an old TV with huge HDMI input latency? You move the thumbstick and then release it, and then, half a second later, you see your character do the same. It doesn't feel very interactive.</p>
<p>Now imagine that, instead of TV input, we're dealing with internet latency. Let's look at a simple use-case of moving the character in a Serious Sam multiplayer session. We're a client, and we press a movement key. Assuming there's no prediction, the player action gets sent to the server, the server eventually simulates its local state and sends <code>MSG_SEQ_ALLACTIONS</code>, which contains all the player actions, including ours. The problem is, we see our character move only after the player actions have been received:</p>
<p><img alt="Latency" src="https://staniks.github.io/img/articles/serious-engine/prediction.png" title="Latency"></p>
<p>And this is the best case scenario - it becomes even worse if you take packet loss or varying network conditions into account.</p>
<p><strong>Prediction</strong> is a mechanism which helps mitigate this a bit.</p>
<p>In a nutshell, prediction is just a fancy way of saying "we're going to be extrapolating because packet round-trip time makes fast-paced games feel unresponsive." In other words, the Engine will try to "guess" where the entities will be in the future without awaiting the action response from the server.</p>
<p>Due to the nature of Serious Engine multiplayer, it is enough to guess player actions, and the rest of the simulation will follow suit. We have two cases:</p>
<ul>
<li>prediction for the local player<ul>
<li>the Engine will use actions sent to the server</li>
</ul>
</li>
<li>prediction for remote players<ul>
<li>the Engine will use last received action from the server</li>
</ul>
</li>
</ul>
<p>Why even wait for player action messages from the server before simulating local state, anyway? Why not just send the action packets and proceed to simulate the world using the sent information?</p>
<p>Because we can't know what the other players did, and they have direct impact on the game state - we would get desynchronization. To avoid mixing the actual and the predicted game state, Serious Engine employs <strong>predictors</strong>.</p>
<p>A predictor is kind of a "ghost" entity, which is paired to a regular entity in the game world. It's essentially a copy of the entity with some special flags. There are two types of predictors:</p>
<ul>
<li><strong>Predictor</strong> - a predictor for an existing entity in the game state.</li>
<li><strong>Temporary predictor</strong> - a predictor spawned during prediction. It has no linked entity, since it doesn't exist in the game state.</li>
</ul>
<p>A <strong>predicted entity</strong> is an entity for which a <strong>predictor</strong> currently exists. When processing predicted game tick, only <strong>predictor entities</strong> are processed. Every time the client receives player actions from the server, the predictors are destroyed and a new prediction cycle begins.</p>
<blockquote>
<p>The nomenclature can be a bit confusing, so just to recap...</p>
<ul>
<li>predictable entity - entity which is meant to be predicted if needed</li>
<li>predictor entity - a copy of an entity, used for prediction</li>
<li>predicted entity - a predictable entity for which a predictor exists</li>
</ul>
</blockquote>
<p>When rendering, <strong>predicted</strong> entities are not rendered - their <strong>predictors</strong> are rendered instead. This gives the illusion of advancing the game state, while the original game state has not changed in any meaningful way.</p>
<p>As for the implementation, prediction is processed just after processing the game stream.</p>
<pre><code>ga_sesSessionState.ProcessGameStream();

// ...

if (bUsePrediction) {
    ga_World.UnmarkForPrediction();
    ga_World.MarkForPrediction();

    ga_sesSessionState.ProcessPrediction();

    ga_World.UnmarkForPrediction();
}
</code></pre>
<p><code>ga_World</code> is a global <code>CWorld</code> instance. In <code>UnmarkForPrediction</code>, the Engine iterates through entities to be predicted and removes <code>ENF_WILLBEPREDICTED</code> flag from the entity's <code>en_ulFlags</code>. <code>MarkForPrediction</code> does the opposite - it sets the flag on any predictable entities and player entities.</p>
<p>A <strong>predictable entity</strong> is any entity with a <code>ENF_PREDICTABLE</code> flag in its <code>en_ulFlags</code>. This is typically set in a constructor of an entity class via <code>SetPredictable</code>, which also adds the entity to the world's <code>wo_cenPredictable</code> collection.</p>
<p>As you may have guessed, <code>CSessionState::ProcessPrediction</code> is the prediction equivalent of <code>CSessionState::ProcessGameStream</code>.</p>
<p>It starts by guessing how many ticks can be predicted. To understand this, let's look at <code>CPlayerTarget</code> members:</p>
<pre><code>class CPlayerTarget {
public:
    BOOL plt_bActive;                     // True if player connected.
    CPlayerEntity *plt_penPlayerEntity;   // Linked player entity.
    CTCriticalSection plt_csAction;       // Access mutex.
    CPlayerAction plt_paPreLastAction;    // Action before last received action.
    CPlayerAction plt_paLastAction;       // Last received action.
    CActionBuffer plt_abPrediction;       // Buffer of sent actions.
    FLOAT3D plt_vPredictorPos;            // Last position of predictor.
</code></pre>
<p><code>plt_abPrediction</code> is interesting in because it's a buffer of actions which were sent to the server since the last received <code>MSG_SEQ_ALLACTIONS</code>. If we sent only <code>N</code> actions, it only makes sense to predict the following <code>N</code> ticks at the most.</p>
<p>When the Engine knows the number of ticks to predict, it will continue to cache the RNG seed and next entity ID (to avoid corrupting the game state), and after this, delete all existing predictors and instantiate new ones (<code>CWorld::CreatePredictors</code>). Then, for each tick that can be predicted, the Engine calls <code>CSessionState::ProcessPredictedGameTick</code>.</p>
<p>This function is the prediction equivalent of <code>CSessionState::ProcessGameTick</code>. Functionality is similar as well - except the that we apply predicted actions to the player targets, and that we set <code>ses_bPredicting</code> to <code>TRUE</code>, which lets the game logic functions know we're currently processing prediction and not affecting the actual game state.</p>
<p><code>CPlayerTarget::ApplyPredictedAction</code> is the prediction equivalent of <code>CPlayerTarget::ApplyActionPacket</code>. However, instead of action delta for the parameter, we have two things:</p>
<ul>
<li><code>iAction</code> - index of the action in the <code>plt_abPrediction</code> buffer, used for local players only</li>
<li><code>fFactor</code> - interpolation factor, used for remote players only</li>
</ul>
<p>When predicting the local player, <code>iAction</code> simply becomes the index of the prediction step (range <code>[0, number of predicted steps]</code>). Since prediction step count is capped at number of buffered actions, this is safe.</p>
<p><code>fFactor</code> is a bit more interesting because it's used only when action interpolation (<code>cli_bLerpActions</code>) is enabled in the console. When predicting the remote player, we have two cases:</p>
<ul>
<li>if <code>cli_bLerpActions</code> is <strong>disabled</strong>, the predicted action is simply the last received action for that player</li>
<li>if <code>cli_bLerpActions</code> is <strong>enabled</strong>, the predicted action is a result of linear interpolation between the last two actions received for that player</li>
</ul>
<p>Honestly, I am not sure how enabling <code>cli_bLerpActions</code> is beneficial. Repeating the last received player action seems reasonable enough, which is likely why <code>cli_bLerpActions</code> is disabled by default.</p>
<p>Once the prediction ticks are processed and the prediction cycle done, RNG seed and entity ID are restored and the prediction has finished for an iteration of the main loop.</p>
<p>Upon rendering, the Engine will simply skip over the entities which are being predicted, and render predictors instead.</p>
<pre><code>void CRenderer::AddModelEntity(CEntity *penModel)
{
    //...

    // Skip the entity if predicted, predicted entities should not be rendered.
    if( penModel-&gt;IsPredicted() &amp;&amp; !gfx_bRenderPredicted) return;
</code></pre>
<h2>Conclusion and Additional Thoughts <a name="conclusion-and-additional-thoughts"></a></h2>
<h2>Comparison with Doom and Quake <a name="comparison-with-doom-and-quake"></a></h2>
<p>It may be interesting to compare Serious Engine's networking model to the similar shooters that came before.</p>
<p>For example, it's somewhat reminiscent of Doom's networking. Even though Doom was actually peer-to-peer, clients exchanged a structure similar to <code>CPlayerActions</code> and each ran their own simulation independently. Doom used a similar system <a href="https://doomwiki.org/wiki/Demo#Technical_information">for demo recording and playback</a> as well. Since Doom source code was released in 1997, it may be possible that Croteam were inspired by this concept when they were developing the Serious Engine, but I'm just speculating here.</p>
<p>Quake, on the other hand, was much different. Instead of having each client simulate their own independent game state, the clients were "dumb" - they merely served as message relays and dind't process any significant game logic on their own, but rather received constant updates of the game state from the server. Neat thing about this concept is not having to worry about desynchronization, and the fact it's easier to prevent cheating (e.g. the server could omit sending information about entities behind walls).</p>
<p>Why Croteam didn't opt for this kind of networking, I can't know for sure, but I guess it has to do with the fact that Serious Sam's game sessions typically have much more active enemies and objects in the world than Quake, and thus, sending updates for that many objects each tick would likely kill the bandwidth. But as I said - I can only speculate.</p>
<h2>Message Portability <a name="message-portability"></a></h2>
<p>If you peek at the network messages, you can see some structs are being serialized with reinterpret casts. Let's consider the case of sending a synchronization check message.</p>
<pre><code>CSyncCheck sc;

CNetworkMessage nmSyncCheck(MSG_SYNCCHECK);
nmSyncCheck.Write(&amp;sc, sizeof(sc)); // Oooof.
_pNetwork-&gt;SendToServer(nmSyncCheck);
</code></pre>
<p>Since the developers were likely using a single compiler for all the clients, they could get away with this, but when developing a cross-platform game, this is where things get a bit slippery. The C++ standard doesn't guarantee exactly the same structure layout across different compilers. Compilers may insert padding bytes to align members for faster access, and this padding may vary, so in the end, the resulting structs could end up different.</p>
<p>E.g. when compiling a 32-bit executable, the compiler might attempt to align members to 4-byte boundary, and 8-byte boundary for 64-bit executables.</p>
<p>There's also the matter of endianness as well (e.g. x86 PC is little-endian, PS3 is big-endian).</p>
<p>But again - the luxury of having a single compiler and developing for a single platform basically made these issues go away.</p>
<h2>Final Thoughts <a name="final-thoughts"></a></h2>
<p>To recap, Serious Engine is an interesting example of a well-thought-out architecture in regard to multiple game modes. The system is fairly elegant since it abstracts away the specifics of the transport medium, be it network or a file, from the game logic.</p>
<p>Due to the nature of the multiplayer model in which everyone maintains a copy of the game state, cheating is possible - for example, it's possible to create a hacked client which would display outlines of other players behind walls, and thus gain an advantage in deathmatch. But let's be honest here - no one plays this game for the deathmatch, most of the people are here for the co-op.</p>
<p>This was an interesting experience which gave me some pretty good ideas to experiment with and possibly incorporate into my hobby games. Tearing apart other people's work to see how it ticks seems like a good way to learn a thing or two.</p>
<p>Once again, do keep in mind that I barely scratched the surface here - I didn't even cover the entirety of networking, only the parts I found most interesting. There's much to be said about the other parts of the Serious Engine, but this could be a topic for another time. Even this article came out longer than I originally intended!</p>
<p>If you liked the writeup, consider <a href="https://twitter.com/Sklopec">following me on Twitter</a>.</p>
<p>Until next time!</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Study shows N95 masks near-perfect at blocking escape of airborne Covid-19 (155 pts)]]></title>
            <link>https://sph.umd.edu/news/study-shows-n95-masks-near-perfect-blocking-escape-airborne-covid-19</link>
            <guid>40657307</guid>
            <pubDate>Wed, 12 Jun 2024 12:28:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sph.umd.edu/news/study-shows-n95-masks-near-perfect-blocking-escape-airborne-covid-19">https://sph.umd.edu/news/study-shows-n95-masks-near-perfect-blocking-escape-airborne-covid-19</a>, See on <a href="https://news.ycombinator.com/item?id=40657307">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>COLLEGE PARK, Md. ‚Äì&nbsp;In a head-to-head comparison of masks worn by people with active COVID-19, the inexpensive ‚Äúduckbill‚Äù N95 came out on top, stopping 98% of COVID-19 particles in the breath of infected people from escaping into the air. Led by researchers from the University of Maryland School of Public Health (SPH), results showed other masks also performed well, blocking at least 70% of viral particles from escaping from the source ‚Äì an infected person‚Äôs exhaled breath.</p><div id="umd_terp_paragraph--12679">
				<p>The study,&nbsp;<a href="https://www.thelancet.com/journals/ebiom/article/PIIS2352-3964(24)00192-0/fulltext">Relative efficacy of masks and respirators as source control for viral aerosol shedding from people infected with SARS-CoV-2</a>, published May 29 in eBioMedicine, a Lancet journal.</p>

<p>‚ÄúThe research shows that any mask is much better than no mask, and an N95 is significantly better than the other options. That‚Äôs the number one message,‚Äù says the study‚Äôs senior author,&nbsp;<a href="https://sph.umd.edu/people/donald-milton">Dr. Donald Milton</a>. Milton is a UMD SPH professor of environmental health and a global expert on how viruses spread through the air.</p>

<p>The study started in May 2020, shortly after the pandemic began, and compared breath samples from volunteers who had active COVID-19, testing the performance of four commonly-used masks. Even without giving participants fit tests or training on how to wear masks correctly, all masks significantly reduced the amount of virus escaping into the air. The study tested masks as a way to control the spread of the virus from the source, i.e. the infected person, and did not test masks as protection from COVID-19 in the surrounding air.</p>

<p>‚ÄúBecause COVID-19 is airborne, we focused on the extent to which wearing a mask reduces contamination of the air around you,‚Äù Milton says. This latest study is a continuation of investigations by UMD‚Äôs&nbsp;<a href="https://sites.google.com/umd.edu/phablabumd/home?authuser=0">Public Health AeroBiology Lab</a>&nbsp;(PHAB Lab) into how contagious respiratory viruses such as influenza contaminate the air.</p>

<p>Researchers asked volunteers with COVID-19 to breathe into a unique contraption known as the Gesundheit II Machine, developed by Milton and colleagues to measure viruses in exhaled breath. Participants, who breathed into the machine for 30 minutes at a time, were asked to do a&nbsp;variety of vocalizations such as repeating the alphabet, singing Happy Birthday, and even&nbsp;honoring&nbsp;<a href="https://umterps.com/sports/2018/6/7/school-mascot">UMD‚Äôs mascot</a>&nbsp;by repeatedly shouting ‚ÄúGo Terps!‚Äù</p>

<p>In each instance, researchers measured the amount of viral particles in the exhaled breath of volunteers, pairing each 30-minute session of breathing with a mask on with another 30-minute session with no mask.</p>

<p>‚ÄúData from our study suggests that a mildly symptomatic person with COVID-19 who is not wearing a mask exhales a little over two infectious doses per hour,‚Äù says first author Dr. Jianyu Lai, a postdoctoral researcher at the PHAB Lab. ‚ÄúBut when wearing an N95 mask, the risk goes down exponentially.‚Äù</p>

<p>The duckbill N95 blocked 99% of large particles and 98% of small particles from escaping out of a person‚Äôs mask. Milton says the design‚Äôs tight seal, a powerful filter, and large air space for breath to move around all contribute to the duckbill‚Äôs success.</p>

<p>Surprisingly, KN95 masks ‚Äì the disposable masks used widely ‚Äì were no more effective than cloth or surgical masks. The study found that a common brand of KN95 masks leak more air than duckbills or other studied masks, because they don‚Äôt conform to the face well. That flaw is compounded by a powerful filter with more flow resistance that pushes air out of the mask at the sides instead of through the filter, allowing more virus particles to escape into the surrounding air.</p>

<p>Cloth masks also outperformed both KN95 and surgical masks. Milton theorizes that cloth masks with greater coverage, wrap around the face and give a better seal than either KN95 or surgical masks. With cloth mask filters, flow resistance is also lower, allowing breath to pass through the filter and not leak out the sides of the mask.</p>

<p>Limiting the amount of viral particles in the air is a key way to control highly contagious respiratory viruses in general, Milton said. This is even more the case with the COVID-19 virus, given transmissibility has increased over time, with Omicron in particular breaking through the immunity people developed from vaccinations or prior infections.</p>

<p>‚ÄúOur research shows definitively why it‚Äôs so important to have non-pharmaceutical responses like wearing masks, and why we need studies like this to illuminate which masks are most effective,‚Äù says Milton.</p>

<p>Both Milton and Lai hope that their findings will inform health policies going forward, including when combatting potential outbreaks like bird flu or even the common flu.</p>

<p>‚ÄúDuckbill N95 masks should be the standard of care in high-risk situations, such as nursing homes and health care settings,‚Äù Lai says. ‚ÄúNow, when the next outbreak of a severe respiratory&nbsp;virus occurs, we know exactly how to help control the spread, with this simple and inexpensive&nbsp;solution.‚Äù</p>

<p>In addition to researchers from the UMD School of Public Health, collaborators include authors from the UMD A. James Clark School of Engineering and the World Health Organization Collaborating Centre for Infectious Disease Epidemiology and Control at the University of Hong Kong, China.</p>

<p>This research was supported by the Prometheus-UMD, sponsored by the Defence Advanced Research Projects Agency (agreement N66001-18-2-4015), the National Institute of Allergy and Infectious Diseases Centers of Excellence for Influenza Research and Surveillance (contract 12-HHSN272201400008C), and the Centers for Disease Control and Prevention (contract 200-2020-09528); by a grant from the Bill &amp; Melinda Gates Foundation; and by a gift from The Flu Lab.</p>



<p>##<br>
Media Contact &nbsp;- SPH Communications, sph-comm@umd.edu, 301-405-2438</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Detectors Get It Wrong. Writers Are Being Fired Anyway (154 pts)]]></title>
            <link>https://gizmodo.com/ai-detectors-inaccurate-freelance-writers-fired-1851529820</link>
            <guid>40657238</guid>
            <pubDate>Wed, 12 Jun 2024 12:19:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/ai-detectors-inaccurate-freelance-writers-fired-1851529820">https://gizmodo.com/ai-detectors-inaccurate-freelance-writers-fired-1851529820</a>, See on <a href="https://news.ycombinator.com/item?id=40657238">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Kimberly Gasuras doesn‚Äôt use AI. ‚ÄúI don‚Äôt need it,‚Äù she said. ‚ÄúI‚Äôve been a news reporter for 24 years. How do you think I did all that work?‚Äù That logic wasn‚Äôt enough to save her job.<br></p><div data-video-id="195264" data-monetizable="true" data-position="sidebar" data-video-title="Why is Everyone Suing AI Companies? | Future Tech" data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="290" data-playlist="195264,195603,196019" data-current="195264"><div><p>Why is Everyone Suing AI Companies? | Future Tech</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/195264/195264_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/195264/195264_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/195264/195264_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/195264/195264_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/20725.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p>As a local journalist in Bucyrus, Ohio, Gasuras relies on side hustles to pay the bills. For a while, she made good money on a freelance writing platform called WritersAccess, where she wrote blogs and other content for small and midsize companies. But halfway through 2023, the income plummeted as some clients switched to ChatGPT for their writing needs. It was already a difficult time. Then the email came. </p><p>‚ÄúI only got one warning,‚Äù Gasuras said. ‚ÄúI got this message saying they‚Äôd flagged my work as AI using a tool called ‚ÄòOriginality.‚Äô‚Äù She was dumbfounded. Gasuras wrote back to defend her innocence, but she never got a response. Originality costs money, but Gasuras started running her work through other AI detectors before submitting to make sure she wasn‚Äôt getting dinged by mistake. A few months later, WritersAccess kicked her off the platform anyway. ‚ÄúThey said my account was suspended due to excessive use of AI. I couldn‚Äôt believe it,‚Äù Gasuras said. WritersAccess did not respond to a request for comment.</p><p>When ChatGPT set the world on fire a year and a half ago, it sparked a feverish search for ways to catch people trying to pass off AI text as their own writing. A host of startups launched to fill the void through AI detection tools, with names including Copyleaks, GPTZero, Originality.AI, and Winston AI. It makes for a tidy business in a landscape full of AI boogeymen. </p><p>These companies advertise peace of mind, a way to take back control through ‚Äúproof‚Äù and ‚Äúaccountability.‚Äù Some advertise accuracy rates as high as 99.98%. But a growing body of experts, studies, and industry insiders argue these tools are far less reliable than their makers promise. There‚Äôs no question that AI detectors make frequent mistakes, and innocent bystanders get caught in the crossfire. Countless <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.rollingstone.com/culture/culture-features/student-accused-ai-cheating-turnitin-1234747351/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.rollingstone.com/culture/culture-features/student-accused-ai-cheating-turnitin-1234747351/" target="_blank" rel="noopener noreferrer">students have been accused of AI plagiarism</a></span>, but a quieter epidemic is happening in the professional world. Some <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4602944&quot;,{&quot;metric25&quot;:1}]]" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4602944" target="_blank" rel="noopener noreferrer">writing gigs are drying up thanks to chatbots</a></span>. As people fight over the dwindling field of work, writers are losing jobs over false accusations from AI detectors.</p><p>‚ÄúThis technology doesn‚Äôt work the way people are advertising it,‚Äù said Bars Juhasz, co-founder of Undetectable AI, which makes tools to help people humanize AI text to sneak it past detection software. ‚ÄúWe have a lot of concerns around the reliability of the training process these AI detectors use. These guys are claiming they have 99% accuracy, and based on our work, I think that‚Äôs impossible. But even if it‚Äôs true, that still means for every 100 people there‚Äôs going to be one false flag. We‚Äôre talking about people‚Äôs livelihoods and their reputations.‚Äù</p><h4 id="h1203"><a id=""></a><strong>Safeguard, or snake oil?</strong></h4><p>In general, AI detectors work by spotting the hallmarks of AI penmanship, such as perfect grammar and punctuation. In fact, one of the easiest ways to get your work flagged is to use Grammarly, a tool that checks for spelling and grammatical errors. It even suggests ways to rewrite sentences for clarity using, you guessed it, artificial intelligence. Adding insult to injury, Gizmodo spoke to writers who said they were fired by platforms that required them to use Grammarly. (Gizmodo confirmed the details of these stories, but we are excluding the names of certain freelance platforms because writers signed non-disclosure agreements.)</p><p>Detectors look for more telling factors as well, such as ‚Äúburstiness.‚Äù Human writers are more likely to reuse certain words in clusters or bursts, while AI is more likely to distribute words evenly across a document. AI detectors can also assess ‚Äúperplexity,‚Äù which essentially asks an AI to measure the likelihood that it would have produced a piece of text given the model‚Äôs training data. Some companies, such as industry leader Originaility.AI, train their own AI language models specially made to detect the work of other AIs, which are meant to spot patterns that are too complex for the human mind. </p><p>However, none of these techniques are foolproof, and many major institutions have backed away from this class of tools. OpenAI released its own AI detector to quell fears about its products in 2023 but pulled the tool off the market just months later ‚Äúdue to its <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/open-ai-chatgpt-ai-text-detector-1850055005&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/open-ai-chatgpt-ai-text-detector-1850055005">low rate of accuracy</a></span>.‚Äù The academic world was first to adopt AI detectors, but false accusations pushed a long list of universities to <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.theregister.com/2023/09/23/turnitin_ai_detection/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.theregister.com/2023/09/23/turnitin_ai_detection/" target="_blank" rel="noopener noreferrer">ban the use of AI detection software</a></span>,, including Vanderbilt, Michigan State, Northwestern, and the University of Texas at Austin.</p><p>AI detection companies ‚Äúare in the business of selling snake oil,‚Äù said Debora Weber-Wulff, a professor at the University of Applied Sciences for Engineering and Economics in Berlin, who co-authored a <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://edintegrity.biomedcentral.com/articles/10.1007/s40979-023-00146-z&quot;,{&quot;metric25&quot;:1}]]" href="https://edintegrity.biomedcentral.com/articles/10.1007/s40979-023-00146-z" target="_blank" rel="noopener noreferrer">recent paper</a></span> about the effectiveness of AI detection. According to Weber-Wulff, research shows that AI detectors are inaccurate, unreliable, and easy to fool. ‚ÄúPeople want to believe that there can be some magic software that solves their problems,‚Äù she said. But ‚Äúcomputer software cannot solve social problems. We have to find other solutions.‚Äù</p><p>The companies that make AI detectors say they‚Äôre a necessary but imperfect tool in a world inundated by robot-generated text. There‚Äôs a significant demand for these services, whether or not they‚Äôre effective.</p><p>Alex Cui, chief technology officer for the AI detection company GPTZero, said detectors have meaningful shortcomings, but the benefits outweigh the drawbacks. ‚ÄúWe see a future where, if nothing is changed, the internet becomes more and more dictated by AI, whether it‚Äôs news, peer-reviewed articles, marketing. You don‚Äôt even know if the person you‚Äôre talking to on social media is real,‚Äù Cui said. ‚ÄúWe need a solution for confirming knowledge en masse, and determining whether content is high quality, authentic, and of legitimate authorship.‚Äù</p><h4 id="h1204"><a id=""></a><strong>A necessary evil?</strong></h4><p>Mark, another Ohio-based copywriter who asked that we withhold his name to avoid professional repercussions, said he had to take work doing maintenance at a local store after an AI detector cost him his job.</p><p>‚ÄúI got an email saying my most recent article had scored a 95% likelihood of AI generation,‚Äù Mark said. ‚ÄúI was in shock. It felt ridiculous that they‚Äôd accuse me after working together for three years, long before ChatGPT was available.‚Äù </p><p>He tried to push back. Mark sent his client a copy of the Google Doc where he drafted the article, which included timestamps that demonstrated he wrote the document by hand. It wasn‚Äôt enough. Mark‚Äôs relationship with the writing platform fell apart. He said losing the job cost him 90% of his income.</p><p>‚ÄúWe hear these stories more than we wish we did, and we understand the pain that false positives cause writers when the work they poured their heart and soul into gets falsely accused,‚Äù said Jonathan Gillham, CEO of Originality.AI. ‚ÄúWe feel like we feel like we‚Äôre building a tool to help writers, but we know that at times it does have some consequences.‚Äù</p><p>But according to Gillham, the problem is about more than helping writers or providing accountability.‚ÄúGoogle is aggressively going after AI spam,‚Äù he said. ‚ÄúWe‚Äôve heard from companies that had their entire site de-indexed by Google that said they didn‚Äôt even know their writers were using AI.‚Äù </p><p>It‚Äôs true that the internet is being flooded by <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/study-junk-news-sites-ad-systems-ai-generated-content-1850578259&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/study-junk-news-sites-ad-systems-ai-generated-content-1850578259">low-effort content farms</a></span> that pump out junky AI articles in an effort to game search results, get clicks, and make ad money from those eyeballs. <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/google-search-updates-downrank-seo-ai-generated-content-1851309904&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/google-search-updates-downrank-seo-ai-generated-content-1851309904">Google is cracking down on these sites</a></span>, which leads some companies to believe that their websites will be down-ranked if Google detects any AI writing whatsoever. That‚Äôs a problem for web-based businesses, and increasingly the No. 1 selling point for AI detectors. Originality promotes itself as a way to ‚Äúfuture proof your site on Google‚Äù at the top of the list of benefits on its homepage.</p><p>A Google spokesperson said this completely misinterprets the company‚Äôs policies. Google, a company that provides AI, said it has no problem with AI content in and of itself. ‚ÄúIt‚Äôs inaccurate to say Google penalizes websites simply because they may use some AI-generated content,‚Äù the spokesperson said. ‚ÄúAs we‚Äôve clearly stated, low value content that‚Äôs created at scale to manipulate Search rankings is spam, however it is produced. Our automated systems determine what appears in top search results based on signals that indicate if content is helpful and high quality.‚Äù</p><h4 id="h1205"><a id=""></a><strong>Mixed messages</strong></h4><p>No one claims AI detectors are perfect, including the companies that make them. But Originality and other AI detectors send mixed messages about how their tools should be used. For example, Gillham said ‚Äúwe advise against the tool being used within academia, and strongly recommend against being used for disciplinary action.‚Äù He explained the risk of false positives is too high for students, because they submit a small number of essays throughout a school year, but the volume of work produced by a professional writer means the algorithm has more chances to get it right. However, on one of the company‚Äôs <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://originality.ai/blog/ai-content-detection-in-education&quot;,{&quot;metric25&quot;:1}]]" href="https://originality.ai/blog/ai-content-detection-in-education" target="_blank" rel="noopener noreferrer">blog posts</a></span>, Originality says AI detection is ‚Äúessential‚Äù in the classroom. </p><p>Then there are questions about how the results are presented. Many of the writers Gizmodo spoke to said their clients don‚Äôt understand the limitations of AI detectors or even what the results are actually saying. It‚Äôs easy to see how someone might be confused: I ran one of my own articles through Originality‚Äôs AI detector. The results were ‚Äú70% Original‚Äù and ‚Äú30% AI.‚Äù You might assume that means Originality determined that 30% of the article was written by a chatbot, especially because the tool highlights specific sentences it finds suspect. However, it‚Äôs actually a confidence score; Originality is 70% sure a human wrote the text. (I wrote the whole thing myself, but you‚Äôll just have to take my word for it.)</p><p>Then there‚Äôs the way the company describes its algorithm. According to Originality, the latest version of its tool has a 98.8% accuracy rate, but Originality also says its false positive rate is 2.8%. If you‚Äôve got your calculator handy, you‚Äôll notice that adds up to more than 100%. Gillham said that‚Äôs because these numbers come from two different tests.</p><p>In Originality‚Äôs defense, the company provides a detailed explanation of how you should interpret the information right below the results, along with links to more detailed writeups about how to use the tool. It seems that isn‚Äôt enough, though. Gizmodo spoke to multiple writers who said they had to argue with clients who misunderstood the Originality tool.</p><p>Originality has published numerous <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://originality.ai/blog/ai-content-detection-accuracy&quot;,{&quot;metric25&quot;:1}]]" href="https://originality.ai/blog/ai-content-detection-accuracy" target="_blank" rel="noopener noreferrer">blog posts</a></span> and studies about accuracy and other issues, including the dataset and methodology it used to develop and measure its own tools. However, Weber-Wulff at the University of Applied Sciences for Engineering and Economics in Berlin said the details about Originality‚Äôs methodology ‚Äúwere not that clear.‚Äù </p><p>A number of experts Gizmodo spoke to, such as Juhasz of Undetectable AI, said they had concerns about businesses across the AI detection industry inflating their accuracy rates and misleading their customers. Representatives for GPTZero and Originality AI said their companies are committed to openness and transparency. Both companies said they go out of their way to provide clear information about the limitations and shortcomings of their tools.</p><p>It might feel like being against AI detectors is being on the side of writers, but according to Gillham the opposite is true. ‚ÄúIf there are no detectors, then the competition for writing jobs increases and as a result the pay drops,‚Äù he said. ‚ÄúDetectors are the difference between a writer being able to do their work, submit content, and get compensated for it, and somebody being able to just copy and paste something from ChatGPT.‚Äù </p><p>On the other hand, all of the copywriters Gizmodo spoke to said the AI detectors are the problem. </p><p>‚ÄúAI is the future. There‚Äôs nothing we can do to stop it, but in my opinion that‚Äôs not the issue. I can see lots of ways AI can be useful,‚Äù Mark said. ‚ÄúIt‚Äôs these detectors. They are the ones that are saying with utmost certainty that they can detect AI writing, and they‚Äôre the ones who are making our clients on edge and paranoid and putting us out of jobs.‚Äù </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Elixir 1.17 released: set-theoretic types in patterns, durations, OTP 27 (410 pts)]]></title>
            <link>https://elixir-lang.org/blog/2024/06/12/elixir-v1-17-0-released/</link>
            <guid>40656747</guid>
            <pubDate>Wed, 12 Jun 2024 11:13:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elixir-lang.org/blog/2024/06/12/elixir-v1-17-0-released/">https://elixir-lang.org/blog/2024/06/12/elixir-v1-17-0-released/</a>, See on <a href="https://news.ycombinator.com/item?id=40656747">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
        <p>Elixir v1.17 has just been released. üéâ</p>

<p>This release introduces set-theoretic types into a handful of language constructs. While there are still <a href="https://elixir-lang.org/blog/2023/06/22/type-system-updates-research-dev/">many steps ahead of us</a>, this important milestone already brings benefits to developers in the form of new warnings for common mistakes. This new version also adds support for <a href="https://www.erlang.org/downloads/27">Erlang/OTP 27</a>, the latest and greatest Erlang release. You‚Äôll also find a new calendar-related data type (<code>Duration</code>) and a <code>Date.shift/2</code> function.</p>

<p>Let‚Äôs dive in.</p>

<h2 id="warnings-from-gradual-set-theoretic-types">Warnings from gradual set-theoretic types</h2>

<p>This release introduces gradual set-theoretic types to infer types from patterns and use them to type check programs, enabling the Elixir compiler to find faults and bugs in codebases without requiring changes to existing software. The underlying principles, theory, and roadmap of our work have been outlined in <a href="https://arxiv.org/abs/2306.06391">‚ÄúThe Design Principles of the Elixir Type System‚Äù by Giuseppe Castagna, Guillaume Duboc, Jos√© Valim</a>.</p>

<p>At the moment, Elixir developers will interact with set-theoretic types only through <strong>warnings</strong> found by the type system. The current implementation models all data types in the language:</p>

<ul>
  <li>
    <p><code>binary()</code>, <code>integer()</code>, <code>float()</code>, <code>pid()</code>, <code>port()</code>, <code>reference()</code> - these
types are indivisible. This means both <code>1</code> and <code>13</code> get the same <code>integer()</code>
type.</p>
  </li>
  <li>
    <p><code>atom()</code> - it represents all atoms and it is divisible. For instance, the
atom <code>:foo</code> and <code>:hello_world</code> are also valid (distinct) types.</p>
  </li>
  <li>
    <p><code>map()</code> and structs - maps can be ‚Äúclosed‚Äù or ‚Äúopen‚Äù. Closed maps only allow
the specified keys, such as <code>%{key: atom(), value: integer()}</code>. Open maps
support any other keys in addition to the ones listed and their definition
starts with <code>...</code>, such as <code>%{..., key: atom(), value: integer()}</code>. Structs
are closed maps with the <code>__struct__</code> key.</p>
  </li>
  <li>
    <p><code>tuple()</code>, <code>list()</code>, and <code>function()</code> - currently they are modelled as
indivisible types. The next Elixir versions will also introduce fine-grained
support to them.</p>
  </li>
</ul>

<p>We focused on <em>atoms</em> and <em>maps</em> on this initial release as they are respectively the simplest and the most complex types representations, so we can stress the performance of the type system and quality of error messages. Modelling these types will also provide the most immediate benefits to Elixir developers. Assuming there is a variable named <code>user</code>, holding a <code>%User{}</code> struct with a <code>address</code> field, Elixir v1.17 will emit the following warnings at compile-time:</p>

<ul>
  <li>
    <p>Pattern matching against a map or a struct that does not have the given key,
such as <code>%{adress: ...} = user</code> (notice <code>address</code> vs <code>adress</code>).</p>
  </li>
  <li>
    <p>Accessing a key on a map or a struct that does not have the given key, such
as <code>user.adress</code>.</p>
  </li>
  <li>
    <p>Invoking a function on non-modules, such as <code>user.address()</code>.</p>
  </li>
  <li>
    <p>Capturing a function on non-modules, such as <code>&amp;user.address/0</code>.</p>
  </li>
  <li>
    <p>Attempting to call an anonymous function without an actual function, such as
<code>user.()</code>.</p>
  </li>
  <li>
    <p>Performing structural comparisons between structs, such as <code>my_date &lt;
~D[2010-04-17]</code>.</p>
  </li>
  <li>
    <p>Performing structural comparisons between non-overlapping types, such as
<code>integer &gt;= string</code>.</p>
  </li>
  <li>
    <p>Building and pattern matching on binaries without the relevant specifiers,
such as <code>&lt;&lt;name&gt;&gt;</code> (this warns because by default it expects an integer, it
should have been <code>&lt;&lt;name::binary&gt;&gt;</code> instead).</p>
  </li>
  <li>
    <p>Attempting to rescue an undefined exception or a struct that is not an
exception.</p>
  </li>
  <li>
    <p>Accessing a field that is not defined in a rescued exception.</p>
  </li>
</ul>

<p>Here‚Äôs an example of how the warning for accessing a misspelled field of a
struct looks like:</p>

<p><img src="https://elixir-lang.org/images/contents/type-warning-on-struct-field.png" alt="Example of a warning when accessing a mispelled struct field"></p>

<p>Another example, this time it‚Äôs a warning for structural comparison across two
<code>Date</code> structs:</p>

<p><img src="https://elixir-lang.org/images/contents/type-warning-on-date-comparison.png" alt="Example of a warning when comparing two structs with &quot;>&quot;"></p>

<p>These warnings also work natively in text editors, as they are standard Elixir
compiler warnings:</p>

<p><img src="https://elixir-lang.org/images/contents/type-warning-in-editor.png" alt="Example of a type warning inline in an editor"></p>

<p>These new warnings will help Elixir developers find bugs earlier and give more
confidence when refactoring code, especially around maps and structs. While
Elixir already emitted some of these warnings in the past, those were discovered
using syntax analysis. The new warnings are more reliable, precise, and with
better error messages. Keep in mind, however, that the Elixir typechecker only
infers types from patterns within the same function at the moment. Analysis from
guards and across function boundaries will be added in future releases. For more
details, see our new <a href="https://hexdocs.pm/elixir/gradual-set-theoretic-types.html">reference document on gradual set-theoretic
types</a>.</p>

<p>The type system was made possible thanks to a partnership between
<a href="https://www.cnrs.fr/">CNRS</a> and <a href="https://remote.com/">Remote</a>. The development
work is currently sponsored by <a href="https://www.fresha.com/">Fresha</a>
(<a href="https://www.fresha.com/careers/openings?department=engineering">they are hiring!</a>),
<a href="https://starfish.team/">Starfish*</a>, and <a href="https://dashbit.co/">Dashbit</a>.</p>



<p>This release adds support for Erlang/OTP 27 and drops support for Erlang/OTP 24.
We recommend Elixir developers to migrate to Erlang/OTP 26 or later, especially
on Windows. Support for WERL (a graphical user interface for the Erlang terminal
on Windows) will be removed in Elixir v1.18.</p>

<p>You can read more about Erlang/OTP 27 in <a href="https://www.erlang.org/downloads/27">their release
announcement</a>. The bits that are
particularly interesting for Elixir developers are the addition of a <a href="https://erlang.org/documentation/doc-15.0-rc3/lib/stdlib-6.0/doc/html/json.html"><code>json</code>
module</a>
and process labels (<code>proc_lib:set_label/1</code>). The latter will also be available
in this Elixir release as <code>Process.set_label/1</code>.</p>

<h2 id="new-duration-data-type-and-shifting-functions">New <code>Duration</code> data type and shifting functions</h2>

<p>This Elixir version introduces the <code>Duration</code> data type and APIs to shift dates,
times, and date times by a given duration, considering different calendars and
time zones.</p>

<div><pre><code><span>iex</span><span>&gt;</span> <span>Date</span><span>.</span><span>shift</span><span>(</span><span>~D[2016-01-31]</span><span>,</span> <span>month:</span> <span>2</span><span>)</span>
<span>~D[2016-03-31]</span>
</code></pre></div>

<p>We chose the name <em>‚Äúshift‚Äù</em> for this operation (instead of ‚Äúadd‚Äù) since working
with durations does not obey properties such as <strong>associativity</strong>. For instance,
adding one month and then one month does not give the same result as adding two
months:</p>

<div><pre><code><span>iex</span><span>&gt;</span> <span>~D[2016-01-31]</span> <span>|&gt;</span> <span>Date</span><span>.</span><span>shift</span><span>(</span><span>month:</span> <span>1</span><span>)</span> <span>|&gt;</span> <span>Date</span><span>.</span><span>shift</span><span>(</span><span>month:</span> <span>1</span><span>)</span>
<span>~D[2016-03-29]</span>
</code></pre></div>

<p>Still, durations are essential for building intervals, recurring events, and
modelling scheduling complexities found in the world around us. For <code>DateTime</code>s,
Elixir will correctly deal with time zone changes (such as Daylight Saving
Time). However, provisions are also available in case you want to surface
conflicts, such as shifting to a wall clock that does not exist, because the
clock has been moved forward by one hour. See <code>DateTime.shift/2</code> for examples.</p>

<p>Finally, we added a new <code>Kernel.to_timeout/1</code> function, which helps developers
normalize durations and integers to a timeout used by many APIs‚Äîlike <code>Process</code>,
<code>GenServer</code>, and more. For example, to send a message after one hour, you can
now write:</p>

<div><pre><code><span>Process</span><span>.</span><span>send_after</span><span>(</span><span>pid</span><span>,</span> <span>:wake_up</span><span>,</span> <span>to_timeout</span><span>(</span><span>hour:</span> <span>1</span><span>))</span>
</code></pre></div>

<h2 id="learn-more">Learn more</h2>

<p>Here are other notable changes in this release:</p>

<ul>
  <li>
    <p>There are new <code>Keyword.intersect/2,3</code> functions to mirror the equivalent in
the <code>Map</code> module.</p>
  </li>
  <li>
    <p>A new Mix profiler was added, <code>mix profile.tprof</code>, which lets you use the
new <a href="https://www.erlang.org/doc/apps/tools/tprof.html">tprof</a>
profiler released with Erlang/OTP 27. This profiler leads to the
soft-deprecation of <code>mix profile.cprof</code> and <code>mix profile.eprof</code>.</p>
  </li>
  <li>
    <p>We added <code>Kernel.is_non_struct_map/1</code>, a new guard to help with the common
pitfall of matching on <code>%{}</code>, which also successfully matches structs (as
they are maps underneath).</p>
  </li>
  <li>
    <p>Elixir‚Äôs Logger now formats
<a href="https://www.erlang.org/doc/apps/stdlib/gen_statem.html"><code>gen_statem</code></a>
reports and includes Erlang/OTP 27 <em>process labels</em> in logger events.</p>
  </li>
</ul>

<p>For a complete list of all changes, see the
<a href="https://github.com/elixir-lang/elixir/releases/tag/v1.17.0">full release notes</a>.</p>

<p>Check <a href="https://elixir-lang.org/install.html">the Install section</a> to get Elixir installed and
read our <a href="https://hexdocs.pm/elixir/introduction.html">Getting Started guide</a>
to learn more.</p>

<p>Happy learning!</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Raspberry Pi is now a public company (163 pts)]]></title>
            <link>https://techcrunch.com/2024/06/11/raspberry-pi-is-now-a-public-company-as-its-shares-pops-after-ipo-pricing/</link>
            <guid>40656603</guid>
            <pubDate>Wed, 12 Jun 2024 10:48:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/06/11/raspberry-pi-is-now-a-public-company-as-its-shares-pops-after-ipo-pricing/">https://techcrunch.com/2024/06/11/raspberry-pi-is-now-a-public-company-as-its-shares-pops-after-ipo-pricing/</a>, See on <a href="https://news.ycombinator.com/item?id=40656603">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Who would have thought that Raspberry Pi, the maker of the tiny, cheap, single-board computers, would become a public company? Yet, this is exactly what‚Äôs <a href="https://www.raspberrypi.com/news/raspberry-pi-ipo/" target="_blank" rel="noreferrer noopener">happening</a>: Raspberry Pi <a href="https://investors.raspberrypi.com/" target="_blank" rel="noreferrer noopener">priced its IPO</a> on the London Stock Exchange on Tuesday morning at ¬£2.80 per share, valuing it at ¬£542 million, or $690 million at today‚Äôs exchange rate.</p>

<p>Shortly after that, the company‚Äôs <a href="https://www.ft.com/content/6419f805-01e8-4900-9da7-7e4fc8504a81" target="_blank" rel="noreferrer noopener">shares jumped a nice 32% to ¬£3.70</a>. It means that Raspberry Pi could end up raising more than $200 million during its IPO process.</p>

	
	


<p>Retail investors can‚Äôt buy Raspberry Pi shares just yet, as only certain institutional shareholders can trade the company‚Äôs shares right now. Retail investors will be able to buy and sell shares starting on Friday.</p>

	
	


<p>This listing is also a win for the London stock market. Deliveroo and Wise both trade in London, but many U.K. tech companies choose to go public in the U.S., as the stock markets there are more liquid.</p>

<p>Raspberry Pi is mostly known for its <a href="https://techcrunch.com/2023/09/27/raspberry-pi-5/">tiny computers</a> that can be programmed to perform all sorts of tasks without spending too much money and requiring too much power. These ARM-based computers became particularly popular among tech hobbyists who wanted to create media servers, retro game consoles, interactive dashboards, robotics projects and more.</p>

<p>More recently, many industrial companies have started integrating the Raspberry Pi in their devices and facilities. The company <a href="https://investors.raspberrypi.com/" target="_blank" rel="noreferrer noopener">reports</a> that the industrial and embedded segment represents 72% of its sales.</p>

<p>Raspberry Pi has sold 60 million units since its inception. In 2023 alone, Raspberry Pi generated $266 million in revenue and $66 million in gross profit.</p>

	
	



<p>Raspberry Pi Ltd, the public company, is the commercial subsidiary of the Raspberry Pi Foundation. The Foundation says it wants to make it easier for people to learn coding through a low-cost, programmable computer. It also remains the main shareholder of Raspberry Pi Ltd.</p>

	
	


<p>Other strategic shareholders in the company include ARM and Sony Semiconductor Solutions Corporation, a subsidiary of Sony that makes image sensors for smartphones and other components. ARM previously said it intended to increase its stake in Raspberry Pi via the public listing.</p>
</div></div>]]></description>
        </item>
    </channel>
</rss>