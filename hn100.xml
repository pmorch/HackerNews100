<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 20 Jan 2025 23:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Did Elon Musk Appear to Sieg Heil at Trump Inauguration? (137 pts)]]></title>
            <link>https://www.jpost.com/international/article-838444</link>
            <guid>42772995</guid>
            <pubDate>Mon, 20 Jan 2025 21:02:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jpost.com/international/article-838444">https://www.jpost.com/international/article-838444</a>, See on <a href="https://news.ycombinator.com/item?id=42772995">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            
<section>
	<section>
		


    <section>
        <h2>Musk was seen making the gesture a total of three times on live television.</h2>
    </section>

<section>
            <section>
                <section>
                    <time datetime="2025-01-20T22:29:54+02:00">
                        JANUARY 20, 2025 22:29
                    </time>
                </section>
                    <section><b>Updated:</b> JANUARY 20, 2025 23:30</section>
            </section>
            <section data-share-url="https://www.jpost.com/international/article-838444" data-share-title="Did Elon Musk Sieg Heil at Trump's inauguration?">
                    <nav>
                        <ul>
                            <li data-share-url="https://www.jpost.com/international/article-838444#838444" data-share-title="Did Elon Musk Sieg Heil at Trump's inauguration?">
                                
                            </li>
                            <li data-share-url="https://www.jpost.com/international/article-838444#838444" data-share-title="Did Elon Musk Sieg Heil at Trump's inauguration?">
                                
                            </li>
                            <li data-share-url="https://www.jpost.com/international/article-838444#838444" data-share-title="Did Elon Musk Sieg Heil at Trump's inauguration?">
                                
                            </li>
                            <li data-share-url="https://www.jpost.com/international/article-838444#838444" data-share-title="Did Elon Musk Sieg Heil at Trump's inauguration?">
                                
                            </li>
                        </ul>
                    </nav>
                </section>
        </section>


	</section>
	<section>
			<figure>
				<img src="https://images.jpost.com/image/upload/q_auto/c_fill,g_faces:center,h_537,w_822/644997" width="290" height="260" alt=" Elon Musk makes controversial gesture at Washington DC arena (photo credit: SCREENSHOT/X)" title=" Elon Musk makes controversial gesture at Washington DC arena">
				<figcaption>
					<section>
						<section> Elon Musk makes controversial gesture at Washington DC arena</section>
						<section>(photo credit: SCREENSHOT/X)</section>
					</section>
				</figcaption>
			</figure>
	</section>

</section>
<section>
			
			<section itemprop="articleBody" id="startBannerSticky">
				<p>US billionaire <a href="https://www.jpost.com/tags/elon-musk">Elon Musk</a> appeared to make a Heil Hitler salute at the Washington DC Trump parade on Monday, following Trump's inauguration.&nbsp;</p><p><a href="https://www.jpost.com/middle-east/article-837785">Musk</a> was seen making the gesture a total of three times on live television.</p><blockquote data-media-max-width="560"><p lang="en" dir="ltr">Elon Musk does what looks like a Hitler salute after talking of victory at Trump inauguration, thanking supporters for assuring "the future of civilisation" <a href="https://t.co/xp0kmJ5dFQ" rel="nofollow">pic.twitter.com/xp0kmJ5dFQ</a></p>— James Jackson (@derJamesJackson) <a href="https://twitter.com/derJamesJackson/status/1881436166144262376?ref_src=twsrc%5Etfw" rel="nofollow">January 20, 2025</a></blockquote><p>He then appeared on stage at the Capital One Area in front of 20,000 Trump supporters, where he thanked supporters before making the gesture.</p><blockquote><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/Breaking?src=hash&amp;ref_src=twsrc%5Etfw" rel="nofollow">#Breaking</a>: Senior Trump administration official Elon Musk thanks supporters with a Nazi salute. <a href="https://t.co/WzSZFUYvEG" rel="nofollow">pic.twitter.com/WzSZFUYvEG</a></p>— Noga Tarnopolsky נגה טרנופולסקי نوغا ترنوبولسكي (@NTarnopolsky) <a href="https://twitter.com/NTarnopolsky/status/1881436487558090831?ref_src=twsrc%5Etfw" rel="nofollow">January 20, 2025</a></blockquote><p>Social media users reacted with horror, with one writing, "Remember when Democrats called MAGA rallies "Nazi rallies?" President un-elect Elon Musk just did the Nazi Sieg Heil salute."</p><h3>Mars space travel</h3><p>The Tesla and Space X owner appeared excited by Trump's mention of Mars in his inaugural speech, given he has reportedly urged NASA to drop its plans to return to the moon and go straight to Mars, according to Politico.</p><p>President Donald Trump said the US would launch astronauts to plant the “stars and stripes” on Mars.</p><p>"We're gonna take DOGE to Mars!" said Musk in his speech, "Can you imagine how awesome it will be to have American astronauts plant the flag on another planet for the first time! How inspiring would that be?!"</p><section><hr><div>            <p>Stay updated with the latest news!</p>            <p>Subscribe to The Jerusalem Post Newsletter</p>        </div><hr></section><p>This comes amid a Washington Post report that Donald Trump's government <a href="https://www.jpost.com/american-politics/article-837770">advisory panel</a>, led by Elon Musk, will be sued soon after the incoming US president is sworn in on Monday.
				</p>
			</section>
		</section>



            
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I am (not) a failure: Lessons learned from six failed startup attempts (261 pts)]]></title>
            <link>http://blog.rongarret.info/2025/01/i-am-not-failure-lessons-learned-from.html</link>
            <guid>42771676</guid>
            <pubDate>Mon, 20 Jan 2025 18:40:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://blog.rongarret.info/2025/01/i-am-not-failure-lessons-learned-from.html">http://blog.rongarret.info/2025/01/i-am-not-failure-lessons-learned-from.html</a>, See on <a href="https://news.ycombinator.com/item?id=42771676">Hacker News</a></p>
Couldn't get http://blog.rongarret.info/2025/01/i-am-not-failure-lessons-learned-from.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: I'm Peter Roberts, immigration attorney, who does work for YC and startups. AMA (171 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42770125</link>
            <guid>42770125</guid>
            <pubDate>Mon, 20 Jan 2025 16:20:29 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42770125">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=42770125: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Mixxx: GPL DJ Software (358 pts)]]></title>
            <link>https://mixxx.org/</link>
            <guid>42769871</guid>
            <pubDate>Mon, 20 Jan 2025 15:53:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mixxx.org/">https://mixxx.org/</a>, See on <a href="https://news.ycombinator.com/item?id=42769871">Hacker News</a></p>
Couldn't get https://mixxx.org/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek-R1 (937 pts)]]></title>
            <link>https://github.com/deepseek-ai/DeepSeek-R1</link>
            <guid>42768072</guid>
            <pubDate>Mon, 20 Jan 2025 12:37:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/DeepSeek-R1">https://github.com/deepseek-ai/DeepSeek-R1</a>, See on <a href="https://news.ycombinator.com/item?id=42768072">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DeepSeek-R1</h2><a id="user-content-deepseek-r1" aria-label="Permalink: DeepSeek-R1" href="#deepseek-r1"></a></p>



<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true"><img src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3"></a>
</p>
<hr>
<p><a href="https://www.deepseek.com/" rel="nofollow">
    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/badge.svg?raw=true">
  </a>
  <a href="https://chat.deepseek.com/" rel="nofollow">
    <img alt="Chat" src="https://camo.githubusercontent.com/a8ed26619b0338e36bfd80f920c9fe96127ff7f12f25a0190e2a94d00a4fa5b9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa496253230436861742d446565705365656b25323052312d3533366166353f636f6c6f723d353336616635266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&amp;logoColor=white">
  </a>
  <a href="https://huggingface.co/deepseek-ai" rel="nofollow">
    <img alt="Hugging Face" src="https://camo.githubusercontent.com/5e3115539d4583e22d65cb89eb1759e767cb9e1d70772923292fcfc80a654be4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d446565705365656b25323041492d6666633130373f636f6c6f723d666663313037266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;logoColor=white">
  </a>
</p>
<p><a href="https://discord.gg/Tc7c45Zzu5" rel="nofollow">
    <img alt="Discord" src="https://camo.githubusercontent.com/e227481a149714ed5187e4fd0b60b9f736099c2dd2083e6c091e29f1446cbb1a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d446565705365656b25323041492d3732383964613f6c6f676f3d646973636f7264266c6f676f436f6c6f723d776869746526636f6c6f723d373238396461" data-canonical-src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da">
  </a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true">
    <img alt="Wechat" src="https://camo.githubusercontent.com/562efc618da65f0a69bc804395005b8124f5c2ed2eb73441c4e359185cc01467/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5765436861742d446565705365656b25323041492d627269676874677265656e3f6c6f676f3d776563686174266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;logoColor=white">
  </a>
  <a href="https://twitter.com/deepseek_ai" rel="nofollow">
    <img alt="Twitter Follow" src="https://camo.githubusercontent.com/8272710ecd020c821b4f62c1c455efb89e0db4eb179c5f5f971c3c1f69452c54/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547769747465722d646565707365656b5f61692d77686974653f6c6f676f3d78266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;logoColor=white">
  </a>
</p>
<p><a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-CODE">
    <img alt="Code License" src="https://camo.githubusercontent.com/d8caf1d64169802e8094bcf0013f0b54d3a9547263b4e59eb43531d7d77993e4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f64655f4c6963656e73652d4d49542d6635646535333f26636f6c6f723d663564653533" data-canonical-src="https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;color=f5de53">
  </a>
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-MODEL">
    <img alt="Model License" src="https://camo.githubusercontent.com/edb8b566827851c52a732ee62c71e1c0231f588d5181c5925a518d98f35b4ff4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d6f64656c5f4c6963656e73652d4d6f64656c5f41677265656d656e742d6635646535333f26636f6c6f723d663564653533" data-canonical-src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;color=f5de53">
  </a>
</p>
<p dir="auto">
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Paper Link</b>👁️</a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">1. Introduction</h2><a id="user-content-1-introduction" aria-label="Permalink: 1. Introduction" href="#1-introduction"></a></p>
<p dir="auto">We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.
DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.
With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.
However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,
we introduce DeepSeek-R1, which incorporates cold-start data before RL.
DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.
To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/figures/benchmark.jpg"><img width="80%" src="https://github.com/deepseek-ai/DeepSeek-R1/raw/main/figures/benchmark.jpg"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">2. Model Summary</h2><a id="user-content-2-model-summary" aria-label="Permalink: 2. Model Summary" href="#2-model-summary"></a></p>
<hr>
<p dir="auto"><strong>Post-Training: Large-Scale Reinforcement Learning on the Base Model</strong></p>
<ul dir="auto">
<li>
<p dir="auto">We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.</p>
</li>
<li>
<p dir="auto">We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.
We believe the pipeline will benefit the industry by creating better models.</p>
</li>
</ul>
<hr>
<p dir="auto"><strong>Distillation: Smaller Models Can Be Powerful Too</strong></p>
<ul dir="auto">
<li>We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.</li>
<li>Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">3. Model Downloads</h2><a id="user-content-3-model-downloads" aria-label="Permalink: 3. Model Downloads" href="#3-model-downloads"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DeepSeek-R1 Models</h3><a id="user-content-deepseek-r1-models" aria-label="Permalink: DeepSeek-R1 Models" href="#deepseek-r1-models"></a></p>
<div dir="auto">
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><strong>Model</strong></th>
<th><strong>#Total Params</strong></th>
<th><strong>#Activated Params</strong></th>
<th><strong>Context Length</strong></th>
<th><strong>Download</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>DeepSeek-R1-Zero</td>
<td>671B</td>
<td>37B</td>
<td>128K</td>
<td><a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero" rel="nofollow">🤗 HuggingFace</a></td>
</tr>
<tr>
<td>DeepSeek-R1</td>
<td>671B</td>
<td>37B</td>
<td>128K</td>
<td><a href="https://huggingface.co/deepseek-ai/DeepSeek-R1" rel="nofollow">🤗 HuggingFace</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</div>
<p dir="auto">DeepSeek-R1-Zero &amp; DeepSeek-R1 are trained based on DeepSeek-V3-Base.
For more details regrading the model architecture, please refer to <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> repository.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DeepSeek-R1-Distill Models</h3><a id="user-content-deepseek-r1-distill-models" aria-label="Permalink: DeepSeek-R1-Distill Models" href="#deepseek-r1-distill-models"></a></p>

<p dir="auto">DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.
We slightly change their configs and tokenizers. Please use our setting to run these models.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">4. Evaluation Results</h2><a id="user-content-4-evaluation-results" aria-label="Permalink: 4. Evaluation Results" href="#4-evaluation-results"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DeepSeek-R1-Evaluation</h3><a id="user-content-deepseek-r1-evaluation" aria-label="Permalink: DeepSeek-R1-Evaluation" href="#deepseek-r1-evaluation"></a></p>
<p dir="auto">For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="570d04263f45e653820916ae742049fc">$0.6$</math-renderer>, a top-p value of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="570d04263f45e653820916ae742049fc">$0.95$</math-renderer>, and generate 64 responses per query to estimate pass@1.</p>
<div dir="auto">
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Category</th>
<th>Benchmark (Metric)</th>
<th>Claude-3.5-Sonnet-1022</th>
<th>GPT-4o 0513</th>
<th>DeepSeek V3</th>
<th>OpenAI o1-mini</th>
<th>OpenAI o1-1217</th>
<th>DeepSeek R1</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Architecture</td>
<td>-</td>
<td>-</td>
<td>MoE</td>
<td>-</td>
<td>-</td>
<td>MoE</td>
</tr>
<tr>
<td></td>
<td># Activated Params</td>
<td>-</td>
<td>-</td>
<td>37B</td>
<td>-</td>
<td>-</td>
<td>37B</td>
</tr>
<tr>
<td></td>
<td># Total Params</td>
<td>-</td>
<td>-</td>
<td>671B</td>
<td>-</td>
<td>-</td>
<td>671B</td>
</tr>
<tr>
<td>English</td>
<td>MMLU (Pass@1)</td>
<td>88.3</td>
<td>87.2</td>
<td>88.5</td>
<td>85.2</td>
<td><strong>91.8</strong></td>
<td>90.8</td>
</tr>
<tr>
<td></td>
<td>MMLU-Redux (EM)</td>
<td>88.9</td>
<td>88.0</td>
<td>89.1</td>
<td>86.7</td>
<td>-</td>
<td><strong>92.9</strong></td>
</tr>
<tr>
<td></td>
<td>MMLU-Pro (EM)</td>
<td>78.0</td>
<td>72.6</td>
<td>75.9</td>
<td>80.3</td>
<td>-</td>
<td><strong>84.0</strong></td>
</tr>
<tr>
<td></td>
<td>DROP (3-shot F1)</td>
<td>88.3</td>
<td>83.7</td>
<td>91.6</td>
<td>83.9</td>
<td>90.2</td>
<td><strong>92.2</strong></td>
</tr>
<tr>
<td></td>
<td>IF-Eval (Prompt Strict)</td>
<td><strong>86.5</strong></td>
<td>84.3</td>
<td>86.1</td>
<td>84.8</td>
<td>-</td>
<td>83.3</td>
</tr>
<tr>
<td></td>
<td>GPQA-Diamond (Pass@1)</td>
<td>65.0</td>
<td>49.9</td>
<td>59.1</td>
<td>60.0</td>
<td><strong>75.7</strong></td>
<td>71.5</td>
</tr>
<tr>
<td></td>
<td>SimpleQA (Correct)</td>
<td>28.4</td>
<td>38.2</td>
<td>24.9</td>
<td>7.0</td>
<td><strong>47.0</strong></td>
<td>30.1</td>
</tr>
<tr>
<td></td>
<td>FRAMES (Acc.)</td>
<td>72.5</td>
<td>80.5</td>
<td>73.3</td>
<td>76.9</td>
<td>-</td>
<td><strong>82.5</strong></td>
</tr>
<tr>
<td></td>
<td>AlpacaEval2.0 (LC-winrate)</td>
<td>52.0</td>
<td>51.1</td>
<td>70.0</td>
<td>57.8</td>
<td>-</td>
<td><strong>87.6</strong></td>
</tr>
<tr>
<td></td>
<td>ArenaHard (GPT-4-1106)</td>
<td>85.2</td>
<td>80.4</td>
<td>85.5</td>
<td>92.0</td>
<td>-</td>
<td><strong>92.3</strong></td>
</tr>
<tr>
<td>Code</td>
<td>LiveCodeBench (Pass@1-COT)</td>
<td>33.8</td>
<td>34.2</td>
<td>-</td>
<td>53.8</td>
<td>63.4</td>
<td><strong>65.9</strong></td>
</tr>
<tr>
<td></td>
<td>Codeforces (Percentile)</td>
<td>20.3</td>
<td>23.6</td>
<td>58.7</td>
<td>93.4</td>
<td><strong>96.6</strong></td>
<td>96.3</td>
</tr>
<tr>
<td></td>
<td>Codeforces (Rating)</td>
<td>717</td>
<td>759</td>
<td>1134</td>
<td>1820</td>
<td><strong>2061</strong></td>
<td>2029</td>
</tr>
<tr>
<td></td>
<td>SWE Verified (Resolved)</td>
<td><strong>50.8</strong></td>
<td>38.8</td>
<td>42.0</td>
<td>41.6</td>
<td>48.9</td>
<td>49.2</td>
</tr>
<tr>
<td></td>
<td>Aider-Polyglot (Acc.)</td>
<td>45.3</td>
<td>16.0</td>
<td>49.6</td>
<td>32.9</td>
<td><strong>61.7</strong></td>
<td>53.3</td>
</tr>
<tr>
<td>Math</td>
<td>AIME 2024 (Pass@1)</td>
<td>16.0</td>
<td>9.3</td>
<td>39.2</td>
<td>63.6</td>
<td>79.2</td>
<td><strong>79.8</strong></td>
</tr>
<tr>
<td></td>
<td>MATH-500 (Pass@1)</td>
<td>78.3</td>
<td>74.6</td>
<td>90.2</td>
<td>90.0</td>
<td>96.4</td>
<td><strong>97.3</strong></td>
</tr>
<tr>
<td></td>
<td>CNMO 2024 (Pass@1)</td>
<td>13.1</td>
<td>10.8</td>
<td>43.2</td>
<td>67.6</td>
<td>-</td>
<td><strong>78.8</strong></td>
</tr>
<tr>
<td>Chinese</td>
<td>CLUEWSC (EM)</td>
<td>85.4</td>
<td>87.9</td>
<td>90.9</td>
<td>89.9</td>
<td>-</td>
<td><strong>92.8</strong></td>
</tr>
<tr>
<td></td>
<td>C-Eval (EM)</td>
<td>76.7</td>
<td>76.0</td>
<td>86.5</td>
<td>68.9</td>
<td>-</td>
<td><strong>91.8</strong></td>
</tr>
<tr>
<td></td>
<td>C-SimpleQA (Correct)</td>
<td>55.4</td>
<td>58.7</td>
<td><strong>68.0</strong></td>
<td>40.3</td>
<td>-</td>
<td>63.7</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Distilled Model Evaluation</h3><a id="user-content-distilled-model-evaluation" aria-label="Permalink: Distilled Model Evaluation" href="#distilled-model-evaluation"></a></p>
<div dir="auto">
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>AIME 2024 pass@1</th>
<th>AIME 2024 cons@64</th>
<th>MATH-500 pass@1</th>
<th>GPQA Diamond pass@1</th>
<th>LiveCodeBench pass@1</th>
<th>CodeForces rating</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4o-0513</td>
<td>9.3</td>
<td>13.4</td>
<td>74.6</td>
<td>49.9</td>
<td>32.9</td>
<td>759</td>
</tr>
<tr>
<td>Claude-3.5-Sonnet-1022</td>
<td>16.0</td>
<td>26.7</td>
<td>78.3</td>
<td>65.0</td>
<td>38.9</td>
<td>717</td>
</tr>
<tr>
<td>o1-mini</td>
<td>63.6</td>
<td>80.0</td>
<td>90.0</td>
<td>60.0</td>
<td>53.8</td>
<td><strong>1820</strong></td>
</tr>
<tr>
<td>QwQ-32B-Preview</td>
<td>44.0</td>
<td>60.0</td>
<td>90.6</td>
<td>54.5</td>
<td>41.9</td>
<td>1316</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Qwen-1.5B</td>
<td>28.9</td>
<td>52.7</td>
<td>83.9</td>
<td>33.8</td>
<td>16.9</td>
<td>954</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Qwen-7B</td>
<td>55.5</td>
<td>83.3</td>
<td>92.8</td>
<td>49.1</td>
<td>37.6</td>
<td>1189</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Qwen-14B</td>
<td>69.7</td>
<td>80.0</td>
<td>93.9</td>
<td>59.1</td>
<td>53.1</td>
<td>1481</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Qwen-32B</td>
<td><strong>72.6</strong></td>
<td>83.3</td>
<td>94.3</td>
<td>62.1</td>
<td>57.2</td>
<td>1691</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Llama-8B</td>
<td>50.4</td>
<td>80.0</td>
<td>89.1</td>
<td>49.0</td>
<td>39.6</td>
<td>1205</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Llama-70B</td>
<td>70.0</td>
<td><strong>86.7</strong></td>
<td><strong>94.5</strong></td>
<td><strong>65.2</strong></td>
<td><strong>57.5</strong></td>
<td>1633</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">5. Chat Website &amp; API Platform</h2><a id="user-content-5-chat-website--api-platform" aria-label="Permalink: 5. Chat Website &amp; API Platform" href="#5-chat-website--api-platform"></a></p>
<p dir="auto">You can chat with DeepSeek-R1 on DeepSeek's official website: <a href="https://chat.deepseek.com/" rel="nofollow">chat.deepseek.com</a>, and switch on the button "DeepThink"</p>
<p dir="auto">We also provide OpenAI-Compatible API at DeepSeek Platform: <a href="https://platform.deepseek.com/" rel="nofollow">platform.deepseek.com</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">6. How to Run Locally</h2><a id="user-content-6-how-to-run-locally" aria-label="Permalink: 6. How to Run Locally" href="#6-how-to-run-locally"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DeepSeek-R1 Models</h3><a id="user-content-deepseek-r1-models-1" aria-label="Permalink: DeepSeek-R1 Models" href="#deepseek-r1-models-1"></a></p>
<p dir="auto">Please visit <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> repo for more information about running DeepSeek-R1 locally.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DeepSeek-R1-Distill Models</h3><a id="user-content-deepseek-r1-distill-models-1" aria-label="Permalink: DeepSeek-R1-Distill Models" href="#deepseek-r1-distill-models-1"></a></p>
<p dir="auto">DeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.</p>
<p dir="auto">For instance, you can easily start a service using <a href="https://github.com/vllm-project/vllm">vLLM</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager"><pre>vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager</pre></div>
<p dir="auto">You can also easily start a service using <a href="https://github.com/sgl-project/sglang">SGLang</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2"><pre>python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2</pre></div>
<p dir="auto"><strong>NOTE: We recommend setting an appropriate temperature (between 0.5 and 0.7) when running these models, otherwise you may encounter issues with endless repetition or incoherent output.</strong></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">7. License</h2><a id="user-content-7-license" aria-label="Permalink: 7. License" href="#7-license"></a></p>
<p dir="auto">This code repository and the model weights are licensed under the <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE">MIT License</a>.
DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:</p>
<ul dir="auto">
<li>DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from <a href="https://github.com/QwenLM/Qwen2.5">Qwen-2.5 series</a>, which are originally licensed under <a href="https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE" rel="nofollow">Apache 2.0 License</a>, and now finetuned with 800k samples curated with DeepSeek-R1.</li>
<li>DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under <a href="https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE" rel="nofollow">llama3.1 license</a>.</li>
<li>DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under <a href="https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE" rel="nofollow">llama3.3 license</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">8. Citation</h2><a id="user-content-8-citation" aria-label="Permalink: 8. Citation" href="#8-citation"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">9. Contact</h2><a id="user-content-9-contact" aria-label="Permalink: 9. Contact" href="#9-contact"></a></p>
<p dir="auto">If you have any questions, please raise an issue or contact us at <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/service@deepseek.com">service@deepseek.com</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Celestial Navigation for Drones (132 pts)]]></title>
            <link>https://www.mdpi.com/2504-446X/8/11/652</link>
            <guid>42767797</guid>
            <pubDate>Mon, 20 Jan 2025 12:02:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mdpi.com/2504-446X/8/11/652">https://www.mdpi.com/2504-446X/8/11/652</a>, See on <a href="https://news.ycombinator.com/item?id=42767797">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <section id="sec1-drones-08-00652" type="intro"><h2 data-nested="1">  1. Introduction</h2><p>Celestial navigation is among the oldest forms of navigation in aviation [<a href="#B1-drones-08-00652">1</a>]. The abundance of salient stars, known to high levels of precision, make them a useful cue for navigators when operating in clear conditions. The elevation of a star above the horizon would be measured, yielding a ‘line of position’. This process was repeated with different stars to fully determine the navigator’s position. The advancement of imaging and computing hardware saw the integration of autonomous star trackers into manned aircraft, as seen, for example, in Lockheed’s SR-71. These autonomous star trackers consisted of a mechanically stabilized telescope and inertial sensors, whose observations, when combined, could produce position estimates to within 0.3 nautical miles for up to 10 h of operation. While accurate, these sensors tended to be both heavy and voluminous, making them undesirable for more modern Size, Weight and Power Constrained (SWAP-C) applications.</p><p>By the advent of the 21st century, Global Positioning System (GPS) had become ubiquitous in avionic navigation. The introduction of GPS caused the interest in celestial navigation to wither due to its relative inaccuracy. Consequently, celestial navigation is primarily seen only in space-based systems, whose orientation must be known to high levels of precision. Nonetheless, celestial navigation was identified as a desirable alternative to GPS [<a href="#B2-drones-08-00652">2</a>], primarily due its robustness against potential jamming. Critically, few GPS-denied alternatives exist that are capable of using passive sensors to estimate global position at night or over the ocean. For this reason, celestial navigation remains an important topic of research. The methodology presented in this paper demonstrates how celestial navigation may be utilized on low cost Uncrewed Aerial Vehicles (UAVs) that lack the precision of an expensive Attitude and Heading Reference System (AHRS).</p><p>Existing works on strapdown celestial navigation tend to be focused on simulation. One such example derives the equations for a celestial camera mounted directly to a strapdown inertial system [<a href="#B3-drones-08-00652">3</a>]. Similarly, a more recent study identified that a strapdown celestial system is contingent on the initial conditions and will tend to diverge over time [<a href="#B4-drones-08-00652">4</a>]. While some promise has been shown in the utility of such systems [<a href="#B5-drones-08-00652">5</a>], the primary problem with the navigation method is that the strapdown system must estimate the camera biases and delineate these from true position error. This is difficult to achieve, as camera bias is perceptually identical to motion over the Earth. Utilizing the celestial system as a highly accurate attitude reference tends to produce useful results in high altitude aircraft [<a href="#B6-drones-08-00652">6</a>] and spacecraft [<a href="#B7-drones-08-00652">7</a>,<a href="#B8-drones-08-00652">8</a>,<a href="#B9-drones-08-00652">9</a>,<a href="#B10-drones-08-00652">10</a>,<a href="#B11-drones-08-00652">11</a>]; however, as a modular solution, the celestial system must have a direct input to the AHRS to improve the inertial position estimate. Alternatively, it has been theoretically shown that atmospheric refraction can be observed to resolve positions [<a href="#B12-drones-08-00652">12</a>]. This technique relies on the observation of the atmospheric refraction of starlight, which occurs most significantly closest to the horizon. The angle of refraction tends to be minimal and difficult to observe, requiring a highly stabilized viewing platform.</p><p>There are a number of reasons why celestial navigation has not become ubiquitous in aviation and particularly in UAV applications. Firstly, there is significant complexity in designing and configuring the hardware. A celestial system must typically be gimballed to within arcminutes of precision, such that a narrow field of a view sensor may lock onto a single star. This could be achieved through gyroscopic stabilization; however, the inclusion of multiple kilograms of stabilization hardware is rarely justified to achieve a navigation outcome with significantly worse performance than GPS. Secondly, celestial systems require a clear view of the night sky, which places practical limitations on their use. Thirdly, alongside the mechanical complexity, there exists a significant computational complexity required to integrate a celestial payload into an existing system. A star database must be maintained, image processing algorithms must be implemented to identify and track stars, and the system may or may not require an interface for the existing AHRS to time and orientate data. These reasons have overshadowed the benefit of having an additional GPS-denied modality for navigation available to an aircraft.</p><p>Previous research has addressed the latter of these points, demonstrating that modern computer vision libraries and embedded hardware are capable of handling the computational requirements of celestial navigation [<a href="#B13-drones-08-00652">13</a>,<a href="#B14-drones-08-00652">14</a>]. We address the first of these points in this study by demonstrating the use of a strapdown celestial system for navigation. Contrary to the stabilized alternative, a strapdown celestial system contains low mechanical complexity, is lightweight, and can be implemented at a low cost. The primary trade-off is in the accuracy of the navigation system. While high levels of accuracy are theoretically achievable (depending on the quality of the optical system), the limiting factor with strapdown celestial navigation is the accuracy of the AHRS. As a rough guide, an attitude error of 1° correlates with a position error of approximately 100&nbsp;km. It is not uncommon for low cost autopilots (such as the Cube Orange running ArduPlane v4.5 firmware) to produce attitude errors in the vicinity of multiple degrees. Such biases would lead to positional offsets that are far too significant for use in any real application.</p><p>We address this shortcoming by demonstrating how a simple orbital motion can significantly improve a celestial position estimate. This technique has been used on the ground to correct for boresight errors [<a href="#B15-drones-08-00652">15</a>], and we demonstrate through experimentation that a similar technique may be used on aircraft to correct for attitude errors. The principle behind this maneuver is that a misaligned boresight will trace a circle about the true boresight if a full azimuthal revolution is performed. The misalignment presents itself in the navigation frame as a circular error in latitude and longitude, enabling the averaging of the results to attain an improved position estimate. We demonstrate that this technique is effective even when the camera is misaligned with the AHRS, offering a reliable method for estimating the position to within 4&nbsp;km from an unknown state. While lacking precision, this method of localization is absolute, cheap to implement, and relatively lightweight.</p><p>To our knowledge, no such technique currently exists in the literature. The methods proposed here are intended to demonstrate the application of strapdown celestial navigation on a fixed-wing drone platform, while addressing the key practical difficulties in doing so. Existing works are iterative, relying on the integration of celestial measurements with inertial measurements using a filter such as an Extended Kalman Filter (EKF) or Unscented Kalman Filter (UKF). This research demonstrates that a strapdown system can be treated as a stand-alone, modular addition to an inertial system, such that the measurements do not need to be integrated into the filter. Independence from an inertial filter enables the celestial system to produce true global position measurements that are not affected by initial conditions. Provided the use of an accurate clock, the results presented in this paper will not degrade over time.</p><p>The remainder of this paper addresses the theory, implementation, and results of the proposed method. We outline the equations for observing stars and estimating global positions in <a href="#sec2-drones-08-00652">Section 2</a>, we describe the methdology used in star detection/tracking and the experimental configuration, we explicitly define the equations for computing the mean position in <a href="#sec3-drones-08-00652">Section 3</a>, we present the results from the flight trial and simulation in <a href="#sec4-drones-08-00652">Section 4</a>, we discuss the results in <a href="#sec5-drones-08-00652">Section 5</a>, and we conclude in <a href="#sec6-drones-08-00652">Section 6</a>.</p></section><section id="sec2-drones-08-00652" type=""><h2 data-nested="1">  2. Theory</h2><section id="sec2dot1-drones-08-00652" type=""><h4 data-nested="2">  2.1. Star Observation</h4><div><p>An observer may estimate their position with knowledge of the current sidereal time and direction vectors to three or more stars in the local North East Down (NED) frame. A camera system is mounted relative to the body frame of the aircraft, with Direction Cosine Matrix (DCM) </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>b</mi>
      <mo>/</mo>
      <mi>c</mi>
    </mrow>
  </msub>
</semantics></math><p> describing the orientation of the camera with respect to the aircraft. Given an aircraft with the orientation described by the DCM </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p>, an observation </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">X</mi>
    <mi>c</mi>
  </msup>
</semantics></math><p> in the camera frame may be transformed to the local NED frame by the following equation:</p><div id="FD1-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msup>
      <mi mathvariant="bold">X</mi>
      <mi>l</mi>
    </msup>
    <mo>=</mo>
    <msub>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>l</mi>
        <mo>/</mo>
        <mi>b</mi>
      </mrow>
    </msub>
    <msub>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>b</mi>
        <mo>/</mo>
        <mi>c</mi>
      </mrow>
    </msub>
    <msup>
      <mi mathvariant="bold">X</mi>
      <mi>c</mi>
    </msup>
  </mrow>
</semantics></math>
      </p>
      <p><label>(1)</label>
      </p>
    </div></div><div><p>Multiple stars are observed simultaneously through the imaging system. A calibrated camera may be described by the pinhole projection model, such that
        </p><p>
        where </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">x</mi>
    <mi>p</mi>
  </msup>
</semantics></math><p> contains the homogeneous pixel coordinates of the projected star, </p><math display="inline"><semantics>
  <mi mathvariant="bold">K</mi>
</semantics></math><p> is the camera intrinsic matrix, and </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">X</mi>
    <mi>c</mi>
  </msup>
</semantics></math><p> is the direction vector of the star in the camera coordinate frame. Consequently, with knowledge of the camera intrinsic matrix, given an observation </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">x</mi>
    <mi>p</mi>
  </msup>
</semantics></math><p>, the vector </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">X</mi>
    <mi>c</mi>
  </msup>
</semantics></math><p> may be computed as follows:</p><p>
        where the scale factor </p><math display="inline"><semantics>
  <mi>α</mi>
</semantics></math><p> may be computed with the knowledge that </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">X</mi>
    <mi>c</mi>
  </msup>
</semantics></math><p> is unitary. Given that </p><math display="inline"><semantics>
  <mrow>
    <mrow>
      <mo>|</mo>
    </mrow>
    <msup>
      <mi mathvariant="bold">X</mi>
      <mi>c</mi>
    </msup>
    <mrow>
      <mo>|</mo>
      <mo>&nbsp;</mo>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
  </mrow>
</semantics></math><p>, it follows that </p><math display="inline"><semantics>
  <mrow>
    <mrow>
      <mi>α</mi>
      <mo>|</mo>
    </mrow>
    <msup>
      <mi mathvariant="bold">K</mi>
      <mrow>
        <mo>−</mo>
        <mn>1</mn>
      </mrow>
    </msup>
    <msup>
      <mi mathvariant="bold">x</mi>
      <mi>p</mi>
    </msup>
    <mrow>
      <mo>|</mo>
      <mo>&nbsp;</mo>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
  </mrow>
</semantics></math><p>. Therefore, </p><math display="inline"><semantics>
  <mi>α</mi>
</semantics></math><p> can be calculated as follows:</p></div></section><section id="sec2dot2-drones-08-00652" type=""><h4 data-nested="2">  2.2. Position Estimation</h4><div><p>The global position may be calculated using the zenith-angle, </p><math display="inline"><semantics>
  <mi>ζ</mi>
</semantics></math><p>, of stars. Following the methodology presented by [<a href="#B16-drones-08-00652">16</a>], each measurement </p><math display="inline"><semantics>
  <mi>ζ</mi>
</semantics></math><p> generates a plane that intersects with the terrestrial sphere, yielding a circle on which the observer may be located. Two observations generate a line, of which only two points on this line intersect with the terrestrial sphere. This case is depicted in <a href="#drones-08-00652-f001">Figure 1</a>. Three observations precisely define the location of the observer, and more than three observations can create an over-constrained system. We may utilize redundant measurements by estimating the least-squares approximation from three or more star observations.</p></div><div><p>Given <span>n</span> stars, with right ascension </p><math display="inline"><semantics>
  <mi>α</mi>
</semantics></math><p> and declination </p><math display="inline"><semantics>
  <mi>δ</mi>
</semantics></math><p>, observed at a zenith angle </p><math display="inline"><semantics>
  <mi>ζ</mi>
</semantics></math><p>, we may define <span>n</span> planes within the geographic coordinate system. If we define </p><math display="inline"><semantics>
  <mi>λ</mi>
</semantics></math><p> as the longitude of an observer whose zenith is directed towards the star, which can be expressed as the difference between the star’s right ascension and the current Greenwich hour angle of Aries, </p><math display="inline"><semantics>
  <mrow>
    <mi>λ</mi>
    <mo>=</mo>
    <mi>α</mi>
    <mo>−</mo>
    <mi>G</mi>
    <mi>H</mi>
    <mi>A</mi>
    <mo>Γ</mo>
  </mrow>
</semantics></math><p>, then it can be seen that the equation of a plane whose normal vector is directed towards the star is given by
        </p><div id="FD5-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mi>a</mi>
    <mi>x</mi>
    <mo>+</mo>
    <mi>b</mi>
    <mi>y</mi>
    <mo>+</mo>
    <mi>c</mi>
    <mi>z</mi>
    <mo>−</mo>
    <mi>p</mi>
    <mo>=</mo>
    <mn>0</mn>
  </mrow>
</semantics></math>
      </p>
      <p><label>(5)</label>
      </p>
    </div><p>
        where
        </p><div id="FD6-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mi>a</mi>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mo form="prefix">cos</mo>
          <mi>λ</mi>
          <mo form="prefix">cos</mo>
          <mi>δ</mi>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mi>b</mi>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mo form="prefix">sin</mo>
          <mi>λ</mi>
          <mo form="prefix">cos</mo>
          <mi>δ</mi>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mi>c</mi>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mo form="prefix">sin</mo>
          <mi>δ</mi>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mi>p</mi>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mo form="prefix">cos</mo>
          <mi>ζ</mi>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</semantics></math>
      </p>
      <p><label>(6)</label>
      </p>
    </div></div><div><p>Therefore, given a minimum of three star observations, we can build matrices </p><math display="inline"><semantics>
  <mi mathvariant="bold">A</mi>
</semantics></math><p> and </p><math display="inline"><semantics>
  <mi mathvariant="bold">p</mi>
</semantics></math><p> to formulate a least-squares approximation of the intersection of these planes. Vertically stacking each equation yields
        </p><p>
        where
        </p><div id="">
      <p>
        <math display="block"><semantics>
  <mtable displaystyle="true">
    <mtr>
      <mtd>
        <mrow>
          <mi mathvariant="bold">A</mi>
          <mo>=</mo>
          <mfenced open="[" close="]">
            <mtable>
              <mtr>
                <mtd>
                  <msub>
                    <mi>a</mi>
                    <mn>1</mn>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>b</mi>
                    <mn>1</mn>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>c</mi>
                    <mn>1</mn>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>a</mi>
                    <mn>2</mn>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>b</mi>
                    <mn>2</mn>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>c</mi>
                    <mn>2</mn>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mo>⋮</mo>
                </mtd>
                <mtd>
                  <mo>⋮</mo>
                </mtd>
                <mtd>
                  <mo>⋮</mo>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>a</mi>
                    <mi>n</mi>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>b</mi>
                    <mi>n</mi>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>c</mi>
                    <mi>n</mi>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
          <mo>,</mo>
          <mspace width="4pt"></mspace>
          <mi mathvariant="bold">x</mi>
          <mo>=</mo>
          <mfenced open="[" close="]">
            <mtable>
              <mtr>
                <mtd>
                  <mi>x</mi>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mi>y</mi>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mi>z</mi>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
          <mo>,</mo>
          <mspace width="4pt"></mspace>
          <mi mathvariant="bold">p</mi>
          <mo>=</mo>
          <mfenced open="[" close="]">
            <mtable>
              <mtr>
                <mtd>
                  <msub>
                    <mi>p</mi>
                    <mn>1</mn>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>p</mi>
                    <mn>2</mn>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mo>⋮</mo>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>p</mi>
                    <mi>n</mi>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</semantics></math>
      </p>
      
    </div></div><div><p>We apply the standard least-squares solution to find the point of intersection of the planes, </p><math display="inline"><semantics>
  <mi mathvariant="bold">x</mi>
</semantics></math><p>, as follows:</p><div id="FD8-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mi mathvariant="bold">x</mi>
    <mo>=</mo>
    <msup>
      <mrow>
        <mo>(</mo>
        <msup>
          <mi mathvariant="bold">A</mi>
          <mi>T</mi>
        </msup>
        <mi mathvariant="bold">A</mi>
        <mo>)</mo>
      </mrow>
      <mrow>
        <mo>−</mo>
        <mn>1</mn>
      </mrow>
    </msup>
    <msup>
      <mi mathvariant="bold">A</mi>
      <mi>T</mi>
    </msup>
    <mi mathvariant="bold">p</mi>
  </mrow>
</semantics></math>
      </p>
      <p><label>(8)</label>
      </p>
    </div></div><div><p>Putting </p><math display="inline"><semantics>
  <mi mathvariant="bold">x</mi>
</semantics></math><p> back into geographical coordinates, we find the latitude and longitude which best fits the star observations:</p><div id="FD9-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mtable displaystyle="true">
    <mtr>
      <mtd>
        <mrow>
          <mi>ϕ</mi>
          <mo>=</mo>
          <msup>
            <mo form="prefix">tan</mo>
            <mrow>
              <mo>−</mo>
              <mn>1</mn>
            </mrow>
          </msup>
          <mfenced open="(" close=")">
            <mstyle scriptlevel="0" displaystyle="true">
              <mfrac>
                <mi>z</mi>
                <msqrt>
                  <mrow>
                    <msup>
                      <mi>x</mi>
                      <mn>2</mn>
                    </msup>
                    <mo>+</mo>
                    <msup>
                      <mi>y</mi>
                      <mn>2</mn>
                    </msup>
                  </mrow>
                </msqrt>
              </mfrac>
            </mstyle>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</semantics></math>
      </p>
      <p><label>(9)</label>
      </p>
    </div></div><p>This value for the latitude and longitude minimizes the squared error in the position vectors. This will be used in <a href="#sec3-drones-08-00652">Section 3</a> for converting many observations into a single position estimate.</p></section></section><section id="sec3-drones-08-00652" type="methods"><h2 data-nested="1">  3. Methods</h2><div><p>A modular celestial navigation system should be mounted rigidly with respect to the AHRS. The orientation of the aircraft, </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p>, is provided by the autopilot. The orientation of the camera, </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>b</mi>
      <mo>/</mo>
      <mi>c</mi>
    </mrow>
  </msub>
</semantics></math><p>, is unknown and must be calibrated to obtain accurate positional information. For the purposes of this study, we assume that the precise orientation of the camera remains unknown, and we consequently accept that the individual position estimates will be erroneous. It will be shown later that this misalignment may be overcome through the use of averaging. This is particularly useful given that the camera itself may be mounted separately from the autopilot, thus being subjected to factors such as vibration, aerodynamic loading, or changes in AHRS biases, which cause the relative orientation between the AHRS and the camera to change over time.</p></div><section id="sec3dot1-drones-08-00652" type=""><h4 data-nested="2">  3.1. Star Detection and Tracking</h4><div><p>Stars are detected within an image using a basic binary thresholding operation. Given a mean pixel value within the frame, </p><math display="inline"><semantics>
  <mi>μ</mi>
</semantics></math><p>, and standard deviation, </p><math display="inline"><semantics>
  <mi>σ</mi>
</semantics></math><p>, a binary threshold is set at </p><math display="inline"><semantics>
  <mrow>
    <mi>μ</mi>
    <mo>+</mo>
    <mn>5</mn>
    <mi>σ</mi>
  </mrow>
</semantics></math><p>, as advised in [<a href="#B17-drones-08-00652">17</a>]. The contours are extracted, and an agglomerative clustering algorithm is applied to cluster redundant detections.</p></div><div><p>Once detected, a standard Kalman filter is used to track individual stars between frames. Each star is defined by its state vector:</p><p>
        and a constant sized bounding box. The states </p><math display="inline"><semantics>
  <mrow>
    <mi>u</mi>
    <mo>,</mo>
    <mi>v</mi>
    <mo>,</mo>
    <mover accent="true">
      <mi>u</mi>
      <mo>˙</mo>
    </mover>
    <mo>,</mo>
    <mi>and</mi>
    <mover accent="true">
      <mi>v</mi>
      <mo>˙</mo>
    </mover>
  </mrow>
</semantics></math><p> describe the x position, y position, x velocity, and y velocity of the star on the image plane, respectively. The position of the star is taken to be the subpixel maxima, computed as a weighted average of the </p><math display="inline"><semantics>
  <mrow>
    <mn>3</mn>
    <mo>×</mo>
    <mn>3</mn>
  </mrow>
</semantics></math><p> Region of Interest (ROI) centered on the peak pixel value, as described by [<a href="#B18-drones-08-00652">18</a>]
        </p><div id="FD12-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mrow>
      <mo>(</mo>
      <mi>u</mi>
      <mo>,</mo>
      <mi>v</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mfenced separators="" open="(" close=")">
      <mstyle scriptlevel="0" displaystyle="true">
        <mfrac>
          <mrow>
            <msub>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
            <msub>
              <mi>I</mi>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
            <msub>
              <mi>u</mi>
              <mi>i</mi>
            </msub>
          </mrow>
          <mrow>
            <msub>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
            <msub>
              <mi>I</mi>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
          </mrow>
        </mfrac>
      </mstyle>
      <mo>,</mo>
      <mstyle scriptlevel="0" displaystyle="true">
        <mfrac>
          <mrow>
            <msub>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
            <msub>
              <mi>I</mi>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
            <msub>
              <mi>v</mi>
              <mi>j</mi>
            </msub>
          </mrow>
          <mrow>
            <msub>
              <mo>∑</mo>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
            <msub>
              <mi>I</mi>
              <mrow>
                <mi>i</mi>
                <mo>,</mo>
                <mi>j</mi>
              </mrow>
            </msub>
          </mrow>
        </mfrac>
      </mstyle>
    </mfenced>
  </mrow>
</semantics></math>
      </p>
      <p><label>(12)</label>
      </p>
    </div></div><div><p>The state transition matrix </p><math display="inline"><semantics>
  <mi mathvariant="bold">F</mi>
</semantics></math><p> is defined assuming a constant velocity as follows:</p><div id="FD13-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mi mathvariant="bold">F</mi>
    <mo>=</mo>
    <mfenced open="[" close="]">
      <mtable>
        <mtr>
          <mtd>
            <mn>1</mn>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mrow>
              <mo>Δ</mo>
              <mi>t</mi>
            </mrow>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mn>1</mn>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mrow>
              <mo>Δ</mo>
              <mi>t</mi>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mn>1</mn>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mn>1</mn>
          </mtd>
        </mtr>
      </mtable>
    </mfenced>
  </mrow>
</semantics></math>
      </p>
      <p><label>(13)</label>
      </p>
    </div></div><div><p>The position and velocity of the stars are observed directly; therefore, the observation matrix </p><math display="inline"><semantics>
  <mi mathvariant="bold">H</mi>
</semantics></math><p> is defined as the </p><math display="inline"><semantics>
  <mrow>
    <mn>4</mn>
    <mo>×</mo>
    <mn>4</mn>
  </mrow>
</semantics></math><p> identity, such that the observation equation is simply given&nbsp;by
        </p></div><div><p>Following the standard Kalman filter equations, the a priori state and covariance estimates are computed:</p><div id="FD16-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mover accent="true">
        <mi mathvariant="bold">P</mi>
        <mo>^</mo>
      </mover>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <mo>=</mo>
    <mi mathvariant="bold">F</mi>
    <msub>
      <mi mathvariant="bold">P</mi>
      <mi>k</mi>
    </msub>
    <msup>
      <mi mathvariant="bold">F</mi>
      <mi>T</mi>
    </msup>
    <mo>+</mo>
    <mi mathvariant="bold">Q</mi>
  </mrow>
</semantics></math>
      </p>
      <p><label>(16)</label>
      </p>
    </div><p>
        for the covariance matrix </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">P</mi>
    <mi>k</mi>
  </msub>
</semantics></math><p> with process noise </p><math display="inline"><semantics>
  <mi mathvariant="bold">Q</mi>
</semantics></math><p>. The innovation is calculated as follows:</p><div id="FD17-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mi mathvariant="bold">y</mi>
      <mi>k</mi>
    </msub>
    <mo>=</mo>
    <msub>
      <mi mathvariant="bold">z</mi>
      <mi>k</mi>
    </msub>
    <mo>−</mo>
    <msub>
      <mover accent="true">
        <mi mathvariant="bold">x</mi>
        <mo>^</mo>
      </mover>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
  </mrow>
</semantics></math>
      </p>
      <p><label>(17)</label>
      </p>
    </div><p>
        with covariance, </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">S</mi>
    <mi mathvariant="bold">k</mi>
  </msub>
</semantics></math><p>, given by
        </p><div id="FD18-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mi mathvariant="bold">S</mi>
      <mi>k</mi>
    </msub>
    <mo>=</mo>
    <msub>
      <mover accent="true">
        <mi mathvariant="bold">P</mi>
        <mo>^</mo>
      </mover>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <mo>+</mo>
    <msub>
      <mi mathvariant="bold">R</mi>
      <mi>k</mi>
    </msub>
  </mrow>
</semantics></math>
      </p>
      <p><label>(18)</label>
      </p>
    </div><p>
        where </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">R</mi>
    <mi>k</mi>
  </msub>
</semantics></math><p> is the measurement noise. The Kalman gain is computed as follows:</p><div id="FD19-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mi mathvariant="bold">K</mi>
      <mi>k</mi>
    </msub>
    <mo>=</mo>
    <msub>
      <mover accent="true">
        <mi mathvariant="bold">P</mi>
        <mo>^</mo>
      </mover>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <msubsup>
      <mi mathvariant="bold">S</mi>
      <mi>k</mi>
      <mrow>
        <mo>−</mo>
        <mn>1</mn>
      </mrow>
    </msubsup>
  </mrow>
</semantics></math>
      </p>
      <p><label>(19)</label>
      </p>
    </div><p>
        and the posterior updates are applied to the a priori estimates:</p><div id="FD20-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mi mathvariant="bold">x</mi>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <mo>=</mo>
    <msub>
      <mover accent="true">
        <mi mathvariant="bold">x</mi>
        <mo>^</mo>
      </mover>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <mo>+</mo>
    <msub>
      <mi mathvariant="bold">K</mi>
      <mi>k</mi>
    </msub>
    <msub>
      <mi mathvariant="bold">y</mi>
      <mi>k</mi>
    </msub>
  </mrow>
</semantics></math>
      </p>
      <p><label>(20)</label>
      </p>
    </div><div id="FD21-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mi mathvariant="bold">P</mi>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <mo>=</mo>
    <mrow>
      <mo>(</mo>
      <mi mathvariant="bold">I</mi>
      <mo>−</mo>
      <msub>
        <mi mathvariant="bold">K</mi>
        <mi>k</mi>
      </msub>
      <mo>)</mo>
    </mrow>
    <msub>
      <mover accent="true">
        <mi mathvariant="bold">P</mi>
        <mo>^</mo>
      </mover>
      <mrow>
        <mi>k</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
  </mrow>
</semantics></math>
      </p>
      <p><label>(21)</label>
      </p>
    </div></div><div><p>The measurement noise, </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">R</mi>
    <mi>k</mi>
  </msub>
</semantics></math><p>, is defined as a diagonal matrix, with elements equal to </p><math display="inline"><semantics>
  <mrow>
    <mo>[</mo>
    <mn>0.5</mn>
    <mspace width="4pt"></mspace>
    <mspace width="4pt"></mspace>
    <mn>0.5</mn>
    <mspace width="4pt"></mspace>
    <mspace width="4pt"></mspace>
    <mn>1.0</mn>
    <mspace width="4pt"></mspace>
    <mspace width="4pt"></mspace>
    <mn>1.0</mn>
    <mo>]</mo>
  </mrow>
</semantics></math><p>, and the process noise is also defined as a diagonal matrix, with elements equal to </p><math display="inline"><semantics>
  <mrow>
    <mo>[</mo>
    <mn>4.0</mn>
    <mspace width="4pt"></mspace>
    <mspace width="4pt"></mspace>
    <mn>4.0</mn>
    <mspace width="4pt"></mspace>
    <mspace width="4pt"></mspace>
    <mn>2.0</mn>
    <mspace width="4pt"></mspace>
    <mspace width="4pt"></mspace>
    <mn>2.0</mn>
    <mo>]</mo>
  </mrow>
</semantics></math><p>. The measurement noise was selected based on the calibration accuracy of the camera, and the process noise was experimentally tuned to minimize the occurrence of lost tracks under high motion conditions. The a priori state estimate is used to propagate the bounding box containing the region of interest, within which the peak is detected using Equation (<a href="#FD12-drones-08-00652">12</a>). The bounding box is centered on </p><math display="inline"><semantics>
  <mrow>
    <mo>[</mo>
    <mi>u</mi>
    <mo>,</mo>
    <mi>v</mi>
    <mo>]</mo>
  </mrow>
</semantics></math><p>, rounded to the nearest pixel. The width and height of the bounding box were fixed at 21 pixels for this imaging system, given an image resolution of </p><math display="inline"><semantics>
  <mrow>
    <mn>1936</mn>
    <mo>×</mo>
    <mn>1216</mn>
  </mrow>
</semantics></math><p> and a field of view of 53.5&nbsp;deg. A visual snapshot of the tracking system can be seen in <a href="#drones-08-00652-f002">Figure 2</a>.</p></div></section><section id="sec3dot2-drones-08-00652" type=""><h4 data-nested="2">  3.2. Experimental Configuration</h4><p>Celestial imagery was captured in-flight from a 4 m flying wing UAV. The selected autopilot (Cube Orange) was mounted at the center of mass, and the celestial payload was mounted in the shoulder of the aircraft (see <a href="#drones-08-00652-f003">Figure 3</a>). The celestial payload collected imagery at a rate of 10 &nbsp;Hz, as well as the attitude and position data from the autopilot, enabling post-analysis. A Raspberry Pi 5 (Raspberry Pi Ltd., Cambridge, UK) was used as the companion computer, interfacing with the celestial camera and the autopilot. An Alvium 1800 U-240 (Allied Vision, Stradtroda, Germany) monochrome sensor fitted with a f/1.4 6 mm wide angle lens was chosen for the imaging system. A serial Universal Asynchronous Receiver Transmitter (UART) link between the Raspberry Pi and the Cube Orange facilitated the transport of MAVLink v2.0 messages. The AHRS data from ArduPlane’s EKF3 was recorded at a rate of 30&nbsp;Hz, and the ground truth GPS position data were recorded at a rate of 10&nbsp;Hz. The Raspberry Pi clock was synchronized with GPS time prior to takeoff. The Raspberry Pi and the camera were mounted on a PLA 3D-printed structure (see <a href="#drones-08-00652-f004">Figure 4</a>) for integration into the airframe.</p><p>The test flight was conducted on a moonless night. The wind was modest, typically remaining below 5&nbsp;m/s. The flight plan consisted of both straight legs and orbital trajectories with varying radii. An overview of the flight plan can be seen in <a href="#drones-08-00652-t001">Table 1</a>.</p><p>The total flight lasted 72 min. Astronomical twilight ceased at 19:32, and takeoff was conducted at 19:53. The GPS receiver in the aircraft was not allowed to be switched off to enable emergency failsafes and prevent fence breaches. We address the consequences of this in <a href="#sec4dot4-drones-08-00652">Section 4.4</a>.</p></section><section id="sec3dot3-drones-08-00652" type=""><h4 data-nested="2">  3.3. Position Estimation</h4><div><p>The star tracker in <a href="#sec3dot1-drones-08-00652">Section 3.1</a> operates on distorted images. The pixel location from each star tracker is extracted. rectified, and subsequently converted to a unit vector in NED coordinates according to <a href="#sec2dot1-drones-08-00652">Section 2.1</a>. Given an observation </p><math display="inline"><semantics>
  <mrow>
    <msup>
      <mi mathvariant="bold">X</mi>
      <mi>l</mi>
    </msup>
    <mo>=</mo>
    <msup>
      <mrow>
        <mo>[</mo>
        <mi>x</mi>
        <mspace width="4pt"></mspace>
        <mi>y</mi>
        <mspace width="4pt"></mspace>
        <mi>z</mi>
        <mo>]</mo>
      </mrow>
      <mi>T</mi>
    </msup>
  </mrow>
</semantics></math><p>, the elevation of a star above the horizon is calculated as
        </p><div id="FD22-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mi>e</mi>
    <mi>l</mi>
    <mo>=</mo>
    <mo>−</mo>
    <msup>
      <mo form="prefix">tan</mo>
      <mrow>
        <mo>−</mo>
        <mn>1</mn>
      </mrow>
    </msup>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac>
        <mi>y</mi>
        <msqrt>
          <mrow>
            <msup>
              <mi>x</mi>
              <mn>2</mn>
            </msup>
            <mo>+</mo>
            <msup>
              <mi>y</mi>
              <mn>2</mn>
            </msup>
          </mrow>
        </msqrt>
      </mfrac>
    </mstyle>
  </mrow>
</semantics></math>
      </p>
      <p><label>(22)</label>
      </p>
    </div><p>
        and, subsequently, the zenith-angle </p><math display="inline"><semantics>
  <mi>ζ</mi>
</semantics></math><p> is calculated as </p><math display="inline"><semantics>
  <mrow>
    <mn>90</mn>
    <mo>−</mo>
    <mi>e</mi>
    <mi>l</mi>
  </mrow>
</semantics></math><p>.</p></div><div><p>In the initial case, the camera orientation </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p> is not known to a high level of precision. Assuming that an attempt is made to mount the camera in alignment with the autopilot, an initial DCM may be formulated from some combination of 90° Euler rotations. It will be seen that as long as this orientation is accurate to within a hemisphere of tolerance, the exact value does not matter, as it will be recalculated in flight.</p></div><div><p>If a minimum of six stars are visible within the frame, a Random Sample Consensus (RANSAC) approach to position estimation may be used to remove bias from false detections. RANSAC is a technique used to identify outliers in sample data and omit them from the estimation process. This is useful for detecting misidentified stars or for removing non-star detections (such as satellites or overhead planes). In the context of position estimation, the RANSAC algorithm randomly selects three stars to generate a position estimate. Subsequently, the algorithm compares the location of the remaining stars against the position estimate. If the remaining stars are within some heuristic tolerance, they are considered inliers. If they are outside of tolerance, they are considered outliers. The position estimate which minimizes the mean squared error of the inliers is chosen as the estimate. A pseudocode implementation of this RANSAC position estimation is outlined in Algorithm 1.
        </p><table><tbody><tr><td><b>Algorithm 1</b> Position-RANSAC</td></tr><tr><td><p><math display="inline"><semantics>
  <mrow>
    <msub>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>l</mi>
        <mo>/</mo>
        <mi>b</mi>
      </mrow>
    </msub>
    <mo>,</mo>
    <msub>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>b</mi>
        <mo>/</mo>
        <mi>c</mi>
      </mrow>
    </msub>
  </mrow>
</semantics></math></p><p><math display="inline"><semantics>
  <mrow>
    <mi>G</mi>
    <mi>H</mi>
    <mi>A</mi>
    <mo>Γ</mo>
    <mo>←</mo>
    <mi>g</mi>
    <mi>e</mi>
    <mi>t</mi>
    <mi>G</mi>
    <mi>H</mi>
    <mi>A</mi>
    <mi>A</mi>
    <mi>r</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mo>(</mo>
    <mo>)</mo>
  </mrow>
</semantics></math></p><p><math display="inline"><semantics>
  <mrow>
    <mi>A</mi>
    <mi>s</mi>
    <mo>←</mo>
    <mo>[</mo>
    <mspace width="4pt"></mspace>
    <mo>]</mo>
  </mrow>
</semantics></math></p><p><math display="inline"><semantics>
  <mrow>
    <mi>P</mi>
    <mi>s</mi>
    <mo>←</mo>
    <mo>[</mo>
    <mspace width="4pt"></mspace>
    <mo>]</mo>
  </mrow>
</semantics></math></p><div><p><b>for</b></p><math display="inline"><semantics>
  <mrow>
    <mo>&nbsp;</mo>
    <mi>o</mi>
    <mi>b</mi>
    <mi>s</mi>
  </mrow>
</semantics></math><p> in </p><math display="inline"><semantics>
  <mrow>
    <mi>o</mi>
    <mi>b</mi>
    <mi>s</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>v</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>s</mi>
  </mrow>
</semantics></math>&nbsp;<p><b>do</b></p></div><p>&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>a</mi>
    <mi>z</mi>
    <mo>,</mo>
    <mi>e</mi>
    <mi>l</mi>
    <mo>=</mo>
    <mi>c</mi>
    <mi>o</mi>
    <mi>m</mi>
    <mi>p</mi>
    <mi>u</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>A</mi>
    <mi>z</mi>
    <mi>E</mi>
    <mi>l</mi>
    <mo>(</mo>
    <mi>o</mi>
    <mi>b</mi>
    <mi>s</mi>
    <mo>,</mo>
    <msub>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>l</mi>
        <mo>/</mo>
        <mi>b</mi>
      </mrow>
    </msub>
    <mo>,</mo>
    <msub>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>b</mi>
        <mo>/</mo>
        <mi>c</mi>
      </mrow>
    </msub>
    <mo>)</mo>
  </mrow>
</semantics></math></p><p>&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>a</mi>
    <mo>,</mo>
    <mi>b</mi>
    <mo>,</mo>
    <mi>c</mi>
    <mo>,</mo>
    <mi>p</mi>
    <mo>=</mo>
    <mi>c</mi>
    <mi>o</mi>
    <mi>m</mi>
    <mi>p</mi>
    <mi>u</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>P</mi>
    <mi>o</mi>
    <mi>s</mi>
    <mi>i</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>C</mi>
    <mi>o</mi>
    <mi>e</mi>
    <mi>f</mi>
    <mi>f</mi>
    <mi>i</mi>
    <mi>c</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>t</mi>
    <mi>s</mi>
    <mo>(</mo>
    <mi>G</mi>
    <mi>H</mi>
    <mi>A</mi>
    <mo>Γ</mo>
    <mo>,</mo>
    <mi>a</mi>
    <mi>z</mi>
    <mo>,</mo>
    <mi>e</mi>
    <mi>l</mi>
    <mo>,</mo>
    <mi>o</mi>
    <mi>b</mi>
    <mi>s</mi>
    <mo>.</mo>
    <mi>r</mi>
    <mi>a</mi>
    <mo>,</mo>
    <mi>o</mi>
    <mi>b</mi>
    <mi>s</mi>
    <mo>.</mo>
    <mi>d</mi>
    <mi>e</mi>
    <mi>c</mi>
    <mo>)</mo>
  </mrow>
</semantics></math></p><p>&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>A</mi>
    <mi>s</mi>
    <mo>.</mo>
    <mi>a</mi>
    <mi>p</mi>
    <mi>p</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>d</mi>
    <mo>(</mo>
    <mo>[</mo>
    <mi>a</mi>
    <mo>,</mo>
    <mi>b</mi>
    <mo>,</mo>
    <mi>c</mi>
    <mo>]</mo>
    <mo>)</mo>
  </mrow>
</semantics></math></p><p>&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>P</mi>
    <mi>s</mi>
    <mo>.</mo>
    <mi>a</mi>
    <mi>p</mi>
    <mi>p</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>d</mi>
    <mo>(</mo>
    <mo>[</mo>
    <mi>p</mi>
    <mo>]</mo>
    <mo>)</mo>
  </mrow>
</semantics></math></p><p><b>end for</b></p><p><math display="inline"><semantics>
  <mrow>
    <mi>i</mi>
    <mi>n</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>s</mi>
    <mo>←</mo>
    <mn>0</mn>
  </mrow>
</semantics></math></p><p><math display="inline"><semantics>
  <mrow>
    <mi>o</mi>
    <mi>u</mi>
    <mi>t</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>s</mi>
    <mo>←</mo>
    <mn>0</mn>
  </mrow>
</semantics></math></p><p><math display="inline"><semantics>
  <mrow>
    <mi>b</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mo>←</mo>
    <mo movablelimits="true" form="prefix">inf</mo>
  </mrow>
</semantics></math></p><p><math display="inline"><semantics>
  <mrow>
    <mi mathvariant="bold">x</mi>
    <mo>←</mo>
    <mo>[</mo>
    <mn>0</mn>
    <mo>,</mo>
    <mn>0</mn>
    <mo>,</mo>
    <mn>0</mn>
    <mo>]</mo>
  </mrow>
</semantics></math></p><div><p><b>for</b></p><math display="inline"><semantics>
  <mrow>
    <mn>0</mn>
    <mo>&lt;</mo>
    <mi>i</mi>
    <mo>&lt;</mo>
    <mi>i</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>s</mi>
  </mrow>
</semantics></math>&nbsp;
                      <p><b>do</b></p></div><div>&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>j</mi>
    <mo>←</mo>
  </mrow>
</semantics></math><p> 3 randomly selected indexes</p></div><div><p>&nbsp;&nbsp;&nbsp;&nbsp;build </p><math display="inline"><semantics>
  <mi mathvariant="bold">A</mi>
</semantics></math><p> and </p><math display="inline"><semantics>
  <mi mathvariant="bold">p</mi>
</semantics></math><p> using </p><math display="inline"><semantics>
  <mrow>
    <mi>A</mi>
    <mi>s</mi>
    <mo>[</mo>
    <mi>j</mi>
    <mo>]</mo>
  </mrow>
</semantics></math><p> and </p><math display="inline"><semantics>
  <mrow>
    <mi>P</mi>
    <mi>s</mi>
    <mo>[</mo>
    <mi>j</mi>
    <mo>]</mo>
  </mrow>
</semantics></math></div><p>&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mover accent="true">
      <mi mathvariant="bold">x</mi>
      <mo>^</mo>
    </mover>
    <mo>←</mo>
    <msup>
      <mrow>
        <mo>(</mo>
        <msup>
          <mi mathvariant="bold">A</mi>
          <mi>T</mi>
        </msup>
        <mi mathvariant="bold">A</mi>
        <mo>)</mo>
      </mrow>
      <mrow>
        <mo>−</mo>
        <mn>1</mn>
      </mrow>
    </msup>
    <msup>
      <mi mathvariant="bold">A</mi>
      <mi>T</mi>
    </msup>
    <mi mathvariant="bold">p</mi>
  </mrow>
</semantics></math></p><p>&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>i</mi>
    <mi>n</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mo>←</mo>
    <mn>0</mn>
  </mrow>
</semantics></math></p><p><b>for</b> all remaining indexes <span>k</span>&nbsp;<b>do</b></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi mathvariant="script">E</mi>
    <mo>←</mo>
    <mi>P</mi>
    <mi>s</mi>
    <mrow>
      <mo>[</mo>
      <mi>k</mi>
      <mo>]</mo>
    </mrow>
    <mo>−</mo>
    <mi>A</mi>
    <mi>s</mi>
    <mrow>
      <mo>[</mo>
      <mi>k</mi>
      <mo>]</mo>
    </mrow>
    <mover accent="true">
      <mi mathvariant="bold">x</mi>
      <mo>^</mo>
    </mover>
  </mrow>
</semantics></math></p><div>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p><b>if</b></p><math display="inline"><semantics>
  <mrow>
    <mi mathvariant="script">E</mi>
    <mo>&lt;</mo>
    <mi>t</mi>
    <mi>o</mi>
    <mi>l</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>c</mi>
    <mi>e</mi>
  </mrow>
</semantics></math>&nbsp;<p><b>then</b></p></div><div>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>i</mi>
    <mi>n</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
  </mrow>
</semantics></math><p> += </p><math display="inline"><semantics>
  <mi mathvariant="script">E</mi>
</semantics></math></div><p><b>else</b></p><p><b>end if</b></p><p><b>end for</b></p><div>&nbsp;&nbsp;&nbsp;&nbsp;<p><b>if</b></p><math display="inline"><semantics>
  <mrow>
    <mi>i</mi>
    <mi>n</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>s</mi>
    <mo>:</mo>
    <mi>o</mi>
    <mi>u</mi>
    <mi>t</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>s</mi>
    <mo>&gt;</mo>
    <mi>a</mi>
    <mi>c</mi>
    <mi>c</mi>
    <mi>e</mi>
    <mi>p</mi>
    <mi>t</mi>
    <mi>a</mi>
    <mi>b</mi>
    <mi>l</mi>
    <mi>e</mi>
    <mspace width="4pt"></mspace>
    <mi>r</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
  </mrow>
</semantics></math>&nbsp;<p><b>then</b></p></div><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>m</mi>
    <mi>e</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mo>=</mo>
    <mi>i</mi>
    <mi>n</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mo>/</mo>
    <mi>i</mi>
    <mi>n</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>s</mi>
  </mrow>
</semantics></math></p><div>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p><b>if</b></p><math display="inline"><semantics>
  <mrow>
    <mi>m</mi>
    <mi>e</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mo>&lt;</mo>
    <mi>b</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
  </mrow>
</semantics></math>&nbsp;<p><b>then</b></p></div><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi>b</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mo>=</mo>
    <mi>m</mi>
    <mi>e</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mspace width="4pt"></mspace>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
  </mrow>
</semantics></math></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math display="inline"><semantics>
  <mrow>
    <mi mathvariant="bold">x</mi>
    <mo>←</mo>
    <mover accent="true">
      <mi mathvariant="bold">x</mi>
      <mo>^</mo>
    </mover>
  </mrow>
</semantics></math></p><p><b>end if</b></p><p><b>end if</b></p><p><b>end for</b></p><p><math display="inline"><semantics>
  <mrow>
    <mi>ϕ</mi>
    <mo>←</mo>
    <msup>
      <mo form="prefix">tan</mo>
      <mrow>
        <mo>−</mo>
        <mn>1</mn>
      </mrow>
    </msup>
    <mfenced open="(" close=")">
      <mstyle scriptlevel="0" displaystyle="true">
        <mfrac>
          <mrow>
            <mi mathvariant="bold">x</mi>
            <mo>[</mo>
            <mn>2</mn>
            <mo>]</mo>
          </mrow>
          <msqrt>
            <mrow>
              <mi mathvariant="bold">x</mi>
              <msup>
                <mrow>
                  <mo>[</mo>
                  <mn>0</mn>
                  <mo>]</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>+</mo>
              <mi mathvariant="bold">x</mi>
              <msup>
                <mrow>
                  <mo>[</mo>
                  <mn>1</mn>
                  <mo>]</mo>
                </mrow>
                <mn>2</mn>
              </msup>
            </mrow>
          </msqrt>
        </mfrac>
      </mstyle>
    </mfenced>
  </mrow>
</semantics></math></p><p><math display="inline"><semantics>
  <mrow>
    <mi>λ</mi>
    <mo>←</mo>
    <msup>
      <mo form="prefix">tan</mo>
      <mrow>
        <mo>−</mo>
        <mn>1</mn>
      </mrow>
    </msup>
    <mfenced open="(" close=")">
      <mstyle scriptlevel="0" displaystyle="true">
        <mfrac>
          <mrow>
            <mi mathvariant="bold">x</mi>
            <mo>[</mo>
            <mn>1</mn>
            <mo>]</mo>
          </mrow>
          <mrow>
            <mi mathvariant="bold">x</mi>
            <mo>[</mo>
            <mn>0</mn>
            <mo>]</mo>
          </mrow>
        </mfrac>
      </mstyle>
    </mfenced>
  </mrow>
</semantics></math></p></td></tr></tbody></table></div><div><p>We first demonstrate that, in the general case, the DCM </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p> is not fixed during flight. In an integrated solution, the celestial system would provide attitude measurements to the EKF, and the offset from this attitude would be estimated. As a modular solution, however, this is infeasible. Taking the output from the EKF to be the true orientation of the aircraft, we can use the true GPS position to compute </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p>. We use the Kabsch-RANSAC methodology presented in [<a href="#B19-drones-08-00652">19</a>] to calculate the ideal rotation between a star’s theoretical location and its actual location. In conjunction with the autopilot output, this rotation yields </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p>. Testing over an 89 s section of flat and level flight, it can be seen in <a href="#drones-08-00652-f005">Figure 5</a> that the camera roll angle shifts by approximately 0.2°, the pitch angle shifts by approximately 0.05°, and the yaw angle shifts by approximately 0.3°. In the context of celestial navigation, these are significant perturbances which, had the offsets been attributed to position as opposed to camera orientation, would have been interpreted as around 30&nbsp;km of positional offset. Indeed, by assuming that </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p> is fixed, we can see in <a href="#drones-08-00652-f006">Figure 6</a> that the latitude drifts by approximately 0.2° over the same short window of flat and level flight. Over an extended period (hours), the orientation may shift such that positional errors are in the range of hundreds of kilometers.</p></div><p>It is clear that a stand-alone celestial module cannot function in the conventional manner with consumer-grade hardware, unless the autopilot is capable of integrating the attitude output from the celestial system into its own filter and estimating the camera orientation. We will demonstrate now a method which, given a very rough initial estimate of the camera orientation, is capable of returning position estimates to within 4&nbsp;km. This is significant in the context of long flights, where alternative dead-reckoning solutions would experience positional drift that is either linear (assuming velocity measurements are available) or quadratic (assuming only acceleration measurements) as a function of time.</p><div><p>By performing a rotation through 360° of compass heading, at an approximately constant yaw rate, the position estimates can be averaged to find an approximation of the orbital center. Provided <span>n</span> images throughout an orbit are available, there exists <span>n</span> latitude and longitude estimates, which may be expressed as unit vectors in the Earth Centred Earth Fixed (ECEF) frame </p><math display="inline"><semantics>
  <msubsup>
    <mi mathvariant="bold">p</mi>
    <mi>i</mi>
    <mrow>
      <mi>e</mi>
      <mi>c</mi>
      <mi>e</mi>
      <mi>f</mi>
    </mrow>
  </msubsup>
</semantics></math><p>. The mean position is taken to be the mean of these vectors:</p><div id="FD23-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msup>
      <mover accent="true">
        <mi mathvariant="bold">p</mi>
        <mo>¯</mo>
      </mover>
      <mrow>
        <mi>e</mi>
        <mi>c</mi>
        <mi>e</mi>
        <mi>f</mi>
      </mrow>
    </msup>
    <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac>
        <mn>1</mn>
        <mi>n</mi>
      </mfrac>
    </mstyle>
    <munderover>
      <mo>∑</mo>
      <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
      </mrow>
      <mi>n</mi>
    </munderover>
    <msubsup>
      <mi mathvariant="bold">p</mi>
      <mi>i</mi>
      <mrow>
        <mi>e</mi>
        <mi>c</mi>
        <mi>e</mi>
        <mi>f</mi>
      </mrow>
    </msubsup>
  </mrow>
</semantics></math>
      </p>
      <p><label>(23)</label>
      </p>
    </div></div><div><p>The latitude and longitude are simply calculated as
        </p><p>
        where <span>x</span>, <span>y</span>, and <span>z</span> are the elements of the mean position vector.</p></div><div><p>Once an initial position estimate has been formulated, it may be used to calculate a more precise estimate of the camera orientation </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>b</mi>
      <mo>/</mo>
      <mi>l</mi>
    </mrow>
  </msub>
</semantics></math><p>. The aim of orientation estimation is to find the rotation matrix </p><math display="inline"><semantics>
  <mi mathvariant="bold">R</mi>
</semantics></math><p> that minimizes the residual error between a set of observations </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">u</mi>
    <mi>c</mi>
  </msup>
</semantics></math><p> represented in the camera frame of reference and a set of theoretical unit vectors </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">v</mi>
    <mi>l</mi>
  </msup>
</semantics></math><p> in the NED frame of reference:</p><div id="FD26-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mi mathvariant="script">E</mi>
    <mo>=</mo>
    <munderover>
      <mo>∑</mo>
      <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
      </mrow>
      <mi>n</mi>
    </munderover>
    <mrow>
      <mo stretchy="false">‖</mo>
    </mrow>
    <msubsup>
      <mi mathvariant="bold">v</mi>
      <mi>i</mi>
      <mi>l</mi>
    </msubsup>
    <mo>−</mo>
    <mi mathvariant="bold">R</mi>
    <msubsup>
      <mi mathvariant="bold">u</mi>
      <mi>i</mi>
      <mi>c</mi>
    </msubsup>
    <msup>
      <mrow>
        <mo stretchy="false">‖</mo>
      </mrow>
      <mn>2</mn>
    </msup>
    <mo>,</mo>
  </mrow>
</semantics></math>
      </p>
      <p><label>(26)</label>
      </p>
    </div></div><div><p>In this case, the rotation matrix </p><math display="inline"><semantics>
  <mi mathvariant="bold">R</mi>
</semantics></math><p> is composed of the aircraft DCM, </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p>, and the camera DCM, </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>b</mi>
      <mo>/</mo>
      <mi>c</mi>
    </mrow>
  </msub>
</semantics></math><p>:</p><p>
        where we accept </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p> as a deterministic output from the autopilot, and so the camera orientation may be found given </p><math display="inline"><semantics>
  <mi mathvariant="bold">R</mi>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>l</mi>
      <mo>/</mo>
      <mi>b</mi>
    </mrow>
  </msub>
</semantics></math><p>:</p><div id="FD28-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>b</mi>
        <mo>/</mo>
        <mi>c</mi>
      </mrow>
    </msub>
    <mo>=</mo>
    <msubsup>
      <mi mathvariant="bold">C</mi>
      <mrow>
        <mi>l</mi>
        <mo>/</mo>
        <mi>b</mi>
      </mrow>
      <mi>T</mi>
    </msubsup>
    <mi mathvariant="bold">R</mi>
  </mrow>
</semantics></math>
      </p>
      <p><label>(28)</label>
      </p>
    </div></div><div><p>Each observation </p><math display="inline"><semantics>
  <msubsup>
    <mi mathvariant="bold">u</mi>
    <mi>i</mi>
    <mi>c</mi>
  </msubsup>
</semantics></math><p> is correlated with a database star. This correlation is obtained through star identification. During the instantiation of the star tracker, a lost-in-space log-polar star identification algorithm is used to determine the IDs of each star in the frame [<a href="#B20-drones-08-00652">20</a>]. The theoretical position vectors </p><math display="inline"><semantics>
  <msubsup>
    <mi mathvariant="bold">v</mi>
    <mi>i</mi>
    <mi>l</mi>
  </msubsup>
</semantics></math><p> in the NED coordinates are generated from the star’s right ascension and declination, the time of observation, and the estimated latitude and longitude [<a href="#B13-drones-08-00652">13</a>]. This allows for the use of the Kabsch algorithm [<a href="#B21-drones-08-00652">21</a>] to find the rotation </p><math display="inline"><semantics>
  <mi mathvariant="bold">R</mi>
</semantics></math><p> between the observed stars and the database stars. Following the implementation in [<a href="#B19-drones-08-00652">19</a>], the algorithm is provided in Algorithm 2, where </p><math display="inline"><semantics>
  <mi mathvariant="bold">A</mi>
</semantics></math><p> is the matrix of <span>n</span> observation vectors with dimension </p><math display="inline"><semantics>
  <mrow>
    <mo>[</mo>
    <mi>n</mi>
    <mo>×</mo>
    <mn>3</mn>
    <mo>]</mo>
  </mrow>
</semantics></math><p> and </p><math display="inline"><semantics>
  <mi mathvariant="bold">B</mi>
</semantics></math><p> is the matrix of <span>n</span> theoretical vectors, also with dimension </p><math display="inline"><semantics>
  <mrow>
    <mo>[</mo>
    <mi>n</mi>
    <mo>×</mo>
    <mn>3</mn>
    <mo>]</mo>
  </mrow>
</semantics></math><p>. The resulting rotation </p><math display="inline"><semantics>
  <mi mathvariant="bold">R</mi>
</semantics></math><p> is used in conjunction with the most recent autopilot attitude data to find the camera orientation </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>b</mi>
      <mo>/</mo>
      <mi>c</mi>
    </mrow>
  </msub>
</semantics></math><p>.
        </p><table><tbody><tr><td><b>Algorithm 2</b> Kabsch</td></tr><tr><td><div><p>Translate vectors in </p><math display="inline"><semantics>
  <mi mathvariant="bold">A</mi>
</semantics></math><p> and </p><math display="inline"><semantics>
  <mi mathvariant="bold">B</mi>
</semantics></math><p> such that centroid is at origin</p></div><div><p>Compute the matrix </p><math display="inline"><semantics>
  <mrow>
    <mi mathvariant="bold">C</mi>
    <mo>=</mo>
    <msup>
      <mi mathvariant="bold">A</mi>
      <mi>T</mi>
    </msup>
    <mi mathvariant="bold">B</mi>
  </mrow>
</semantics></math></div><div><p>Compute the singular value decomposition </p><math display="inline"><semantics>
  <mrow>
    <mi mathvariant="bold">U</mi>
    <mo>,</mo>
    <mo>Σ</mo>
    <mo>,</mo>
    <mi mathvariant="bold">V</mi>
    <mo>=</mo>
    <mi>SVD</mi>
    <mo>(</mo>
    <mi mathvariant="bold">C</mi>
    <mo>)</mo>
  </mrow>
</semantics></math><p> such that </p><math display="inline"><semantics>
  <mrow>
    <mi mathvariant="bold">C</mi>
    <mo>=</mo>
    <mi mathvariant="bold">U</mi>
    <mo>Σ</mo>
    <msup>
      <mi mathvariant="bold">V</mi>
      <mi>T</mi>
    </msup>
  </mrow>
</semantics></math></div><div><p>Set diagonal elements </p><math display="inline"><semantics>
  <msub>
    <mo>Σ</mo>
    <mn>1</mn>
  </msub>
</semantics></math><p> through to </p><math display="inline"><semantics>
  <msub>
    <mo>Σ</mo>
    <mrow>
      <mi>n</mi>
      <mo>−</mo>
      <mn>1</mn>
    </mrow>
  </msub>
</semantics></math><p> = 1.</p></div><p><b>else</b></p><p><b>end if</b></p><div><p>Compute the rotation matrix </p><math display="inline"><semantics>
  <mrow>
    <mi mathvariant="bold">R</mi>
    <mo>=</mo>
    <mi mathvariant="bold">V</mi>
    <mo>Σ</mo>
    <msup>
      <mi mathvariant="bold">U</mi>
      <mi>T</mi>
    </msup>
  </mrow>
</semantics></math></div></td></tr></tbody></table></div><div><p>Once the camera orientation has been found, two options are presented, depending on the computational power of the flight computer. If the computer is capable of parallel processing, then the set of theoretical star locations </p><math display="inline"><semantics>
  <msup>
    <mi mathvariant="bold">v</mi>
    <mi>l</mi>
  </msup>
</semantics></math><p> may be re-calculated using the updated latitude and longitude, and the estimated positions may be re-calculated using the updated camera DCM </p><math display="inline"><semantics>
  <msub>
    <mi mathvariant="bold">C</mi>
    <mrow>
      <mi>b</mi>
      <mo>/</mo>
      <mi>c</mi>
    </mrow>
  </msub>
</semantics></math><p>. This enables the system to recursively converge on a position estimate from a single set of orbital data. Alternatively, it is possible to repeat an orbit and calculate a new set of latitudes and longitudes. This method does not require post hoc processing and may be better suited to real-time applications. For the purposes of this study, we use the former method, as this allows us to treat each orbit as an independent sample, yielding an independent position estimate, thus giving us deeper insight into how the characteristics of a given orbit affect the position estimate. An example of this process can be seen in <a href="#drones-08-00652-f007">Figure 7</a>.</p></div></section></section><section id="sec4-drones-08-00652" type="results"><h2 data-nested="1">  4. Results</h2><section id="sec4dot1-drones-08-00652" type=""><h4 data-nested="2">  4.1. Position Estimation</h4><div><p>Following the experimental configuration in <a href="#sec3-drones-08-00652">Section 3</a>, the position of the aircraft is estimated for each orbit. The camera was initially aligned using a rotation matrix built from Euler angles of </p><math display="inline"><semantics>
  <mrow>
    <mo>−</mo>
    <msup>
      <mn>90</mn>
      <mo>∘</mo>
    </msup>
    <mo>,</mo>
    <msup>
      <mn>0</mn>
      <mo>∘</mo>
    </msup>
  </mrow>
</semantics></math><p>, and </p><math display="inline"><semantics>
  <msup>
    <mn>180</mn>
    <mo>∘</mo>
  </msup>
</semantics></math><p> in yaw, pitch, and roll, respectively, for a yaw–pitch–roll rotation sequence. This is a coarse approximation based on the orientation with which the camera was mounted to the airframe. Of course, the true orientation differs from this; as can be seen in <a href="#drones-08-00652-f005">Figure 5</a>, there is approximately a 5° error in this initial orientation.</p></div><p>Each orbit is treated as an independent observation. This is achieved by resetting the camera orientation at the initialization of every orbit, thus requiring the algorithm to recalculate the camera DCM. The output from each of the orbital motions listed in <a href="#drones-08-00652-t001">Table 1</a> is shown in <a href="#drones-08-00652-t002">Table 2</a>. The position error is measured as the distance between the final celestial position estimate and the center of the orbit, calculated using the Haversine function.</p></section><section id="sec4dot2-drones-08-00652" type=""><h4 data-nested="2">  4.2. Estimation Accuracy</h4><div><p>It can be seen that the orbits which are neither climbing nor descending produced more accurate positional data. These orbits had larger radii, which consequently produced greater numbers of position estimates per orbit. Additionally, the variance in the pitch and roll axis during ascent/descent tends to be greater, as the total energy control system utilizes the pitch axis to throttle the climb/descent rate. By taking these two factors into account, we calculate the standard error as a function of the covariance in pitch and roll, as well as the number of samples. The generalized variance is calculated by taking the determinant of the covariance matrix in pitch and roll, where this matrix is given by
        </p><div id="FD29-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <msub>
      <mo>Σ</mo>
      <mrow>
        <mi>θ</mi>
        <mo>,</mo>
        <mi>ϕ</mi>
      </mrow>
    </msub>
    <mo>=</mo>
    <mfenced open="[" close="]">
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mi>COV</mi>
              <mo>(</mo>
              <mi>θ</mi>
              <mo>,</mo>
              <mi>θ</mi>
              <mo>)</mo>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mi>COV</mi>
              <mo>(</mo>
              <mi>θ</mi>
              <mo>,</mo>
              <mi>ϕ</mi>
              <mo>)</mo>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mi>COV</mi>
              <mo>(</mo>
              <mi>ϕ</mi>
              <mo>,</mo>
              <mi>θ</mi>
              <mo>)</mo>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mi>COV</mi>
              <mo>(</mo>
              <mi>ϕ</mi>
              <mo>,</mo>
              <mi>ϕ</mi>
              <mo>)</mo>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mfenced>
  </mrow>
</semantics></math>
      </p>
      <p><label>(29)</label>
      </p>
    </div><p>
        where the function </p><math display="inline"><semantics>
  <mrow>
    <mi>COV</mi>
    <mo>(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo>)</mo>
  </mrow>
</semantics></math><p> describes the covariance between sets <span>x</span> and <span>y</span>:</p><div id="FD30-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mi>COV</mi>
    <mrow>
      <mo>(</mo>
      <mi>x</mi>
      <mo>,</mo>
      <mi>y</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac>
        <mrow>
          <msubsup>
            <mo>∑</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>n</mi>
          </msubsup>
          <mrow>
            <mo>(</mo>
            <msub>
              <mi>x</mi>
              <mi>i</mi>
            </msub>
            <mo>−</mo>
            <mover accent="true">
              <mi>x</mi>
              <mo>¯</mo>
            </mover>
            <mo>)</mo>
          </mrow>
          <mrow>
            <mo>(</mo>
            <msub>
              <mi>y</mi>
              <mi>i</mi>
            </msub>
            <mo>−</mo>
            <mover accent="true">
              <mi>y</mi>
              <mo>¯</mo>
            </mover>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mi>n</mi>
      </mfrac>
    </mstyle>
  </mrow>
</semantics></math>
      </p>
      <p><label>(30)</label>
      </p>
    </div><p>
        given <span>n</span> samples. The standard deviation is found given the generalized variance:</p><p>
        and thus the standard error in roll and pitch is calculated given the standard deviation and the number of samples as follows:</p></div><div><p>It can be seen in <a href="#drones-08-00652-f008">Figure 8</a> that the relationship between the estimated standard error and the true error is approximately linear. The accuracy of the first orbit is likely due to chance; this is a good indicator that the factors dictating an accurate positional estimate are indeed the variance in pitch and roll throughout the orbit and the total number of samples. The inverse of the trend-line may be used as an estimate for the Circular Error Probable (CEP), where
        </p><div id="FD33-drones-08-00652">
      <p>
        <math display="block"><semantics>
  <mrow>
    <mi>C</mi>
    <mi>E</mi>
    <mi>P</mi>
    <mo>=</mo>
    <mn>1205</mn>
    <mo>×</mo>
    <mi>S</mi>
    <mi>E</mi>
    <mo>−</mo>
    <mn>0.567</mn>
  </mrow>
</semantics></math>
      </p>
      <p><label>(33)</label>
      </p>
    </div><p>
        and </p><math display="inline"><semantics>
  <mrow>
    <mi>C</mi>
    <mi>E</mi>
    <mi>P</mi>
  </mrow>
</semantics></math><p> is estimated in kilometers.</p></div></section><section id="sec4dot3-drones-08-00652" type=""><h4 data-nested="2">  4.3. Initial Conditions</h4><div><p>We demonstrate here that the initial value of the camera rotation matrix has little effect on the final position estimate. This is significant because, as shown in <a href="#sec3-drones-08-00652">Section 3</a>, many factors may cause changes in the relative orientation between the camera and the AHRS. Choosing orbit 5 as an example, we show that as long as the estimated boresight of the camera is accurate to within a hemisphere of tolerance (that is, the estimated boresight is within </p><math display="inline"><semantics>
  <msup>
    <mn>90</mn>
    <mo>∘</mo>
  </msup>
</semantics></math><p> of the true boresight), the algorithm will converge near the true location. A graphical representation of the first iteration given various camera calibrations is shown in <a href="#drones-08-00652-f009">Figure 9</a>. In each case, with a position error less than 90°, the position estimate converged to within the estimated CEP. It can be seen in <a href="#drones-08-00652-t003">Table 3</a> that an initial calibration error beyond 90° results in a position estimate on the opposite side of the Earth.</p></div></section><section id="sec4dot4-drones-08-00652" type=""><h4 data-nested="2">  4.4. Simulation of Wind Effects</h4><p>The flight trial was conducted with modest amounts of wind, utilizing GPS to conduct an orbit. We postulate that in the presence of wind, the position is better attained by fixing the pitch and roll of the aircraft, such that a constant yaw rate is achieved in the local NED frame. This approach does not require GPS to perform an orbit and only relies on compass heading and inertial attitude sensors to follow a trajectory. Simulation results strongly indicate that GPS is not required but is in fact detrimental to the orbital method of position estimation. As a consequence of not using GPS, the aircraft is subjected to lateral drift in the local NED frame. The amount of drift experienced by the aircraft during an orbit is, however, minimal in comparison to the scale to which position is being estimated.</p><p>We used JSBSim to simulate the aircraft’s flight dynamics, in conjunction with Ardupilot’s software-in-the-loop simulator to generate synthesized motion-blurred imagery. The images were generated following the methodology in [<a href="#B13-drones-08-00652">13</a>], factoring in the motion blur experienced by the camera. A strong southerly wind of 54 km/h was added to the simulation. An orbit was performed using GPS, and another was performed using a fixed pitch and roll angle with GPS disabled. During the fixed-attitude orbit, the aircraft drifted 1.05 km. The simulated trajectory of the GPS-denied orbit can be seen in <a href="#drones-08-00652-f010">Figure 10</a>.</p><p>Applying the same methodology, the position was calculated for both the GPS-guided orbit and the GPS-denied orbit. The control protocol of the GPS-guided orbit was to maintain a ground track at a fixed radius about a point in an identical manner to what was performed in the real flight test. This method adjusts the pitch and roll of the vehicle to compensate for wind and altitude. Up-wind sections of the orbit are prolonged in time, and down-wind sections are contracted in time. The resulting position estimates are heavily biased towards the prolonged portion of the orbit, which consequently skews the mean calculation. Under high wind conditions, it can be seen that following a fixed-radius ground track is not optimal for position calculation. The resulting position error was 18.27 km, as seen in <a href="#drones-08-00652-f011">Figure 11</a>. By contrast, the fixed-attitude orbit achieved a position error of 2.29 km, despite being subjected to over 1 km of drift. This highlights the importance of fixing the aircraft attitude rather than following a constant-radius orbit.</p></section></section><section id="sec5-drones-08-00652" type="discussion"><h2 data-nested="1">  5. Discussion</h2><p>The results presented in <a href="#sec4-drones-08-00652">Section 4</a> demonstrate that strapdown celestial navigation as a modular solution has potential use in Global Navigation Satellite System (GNSS)-denied UAV navigation. The celestial camera was mounted to the airframe, separate from the autopilot, but still utilizing the AHRS orientation information coming from the autopilot. This has significant implications for integration into SWAP-C airframes, in which the inclusion of stabilization hardware adds unwanted mass. With a modern GPS receiver weighing only a few grams and producing estimates to within 1m accuracy, it is understandable that, given the choice, an alternative celestial navigation solution would not be included. Yet, GNSS denial is increasing in prevalence, and alternative navigation solutions must be explored if a UAV is to operate under such circumstances.</p><p>The most profound outcome from this study is the celestial system’s independence from the initial conditions. Provided a functional AHRS and navigation system which is capable of performing an orbit, the celestial system can produce a position estimate from any unknown position on the globe, with a camera that is aligned to within 90°. The remaining source of error lies in the clock drift, which is typically equal to 3 ppm (0.3 s per 24 h) for a modern real-time clock. This has significant implications for long endurance aircraft, which may need to operate for many hours in RF contested environments. It has been shown that the rate of divergence in dead reckoning navigation is substantial without a velocity estimate [<a href="#B22-drones-08-00652">22</a>], particularly in consumer grade systems. Even with velocity measurements, in the presence of wind, a tactical grade aircraft may drift by 10 km per hour. In such cases, over the course of multiple hours, the precision offered by the proposed method is a vast improvement.</p><p>For loitering aircraft, this method of localization is convenient, as the flight plan need not change. For aircraft travelling significant distances, it may be sufficient to intermittently perform a single orbit, and utilize the more erroneous position estimate for course correction. The act of performing an orbit through a full compass rotation effectively nullifies the biases and offsets between the AHRS and the celestial camera. The main source of error for each independent position estimate during an orbit, is the misalignment of the estimated zenith with the true zenith. This may be caused by a number of factors, such as aerodynamic loading causing minor perturbations in the camera’s orientation relative to the autopilot, improper alignment or calibration of the inertial system, or simply due to estimation errors caused by improperly estimated centrifugal acceleration. In each of these cases, it is expected that the error remains approximately constant throughout an orbit. Consequently, zenith errors at one particular azimuth are cancelled out by the zenith errors at the opposing (180° offset) azimuth. This is the reason why such a method is capable of working with almost arbitrary levels of initial error.</p><p>This flight was conducted using GPS to maintain the orbital position. We recognize that, in a true GNSS denied environment, wind errors will cause the aircraft to drift during an orbit. While this may introduce some error into the position estimate, we show in <a href="#sec4-drones-08-00652">Section 4</a> that the primary factor governing the accuracy of a position estimate is the variance in the zenith-angle throughout the orbit. The circular position errors seen for example in <a href="#drones-08-00652-f007">Figure 7</a>, are not caused by the change in aircraft position. They are caused by a misalignment of the optical system, and consequently may be reproduced by simply maintaining constant roll and pitch throughout a full compass rotation. In <a href="#sec4dot4-drones-08-00652">Section 4.4</a> we show that even in high wind conditions, the appropriate strategy is to maintain roll rate and accept that there will be positional drift throughout the orbit. This strongly suggests that the presence of GPS during the flight test offered no advantages to the algorithm.</p><p>An obvious limitation of this method is its dependence on sky visibility. Some research has shown that short-wave infrared cameras offer a daylight visible alternative to visible spectrum cameras [<a href="#B23-drones-08-00652">23</a>,<a href="#B24-drones-08-00652">24</a>]. These tend to have far lower signal-to-noise ratios, resulting in more erroneous measurements. It may be the case that the proposed positioning method is capable of nullifying the increased observational error from short-wave infrared observations. This may be a potential topic for future research.</p></section><section id="sec6-drones-08-00652" type="conclusions"><h2 data-nested="1">  6. Conclusions</h2><p>This study proposed a method for obtaining more accurate positional estimates from a modular strapdown celestial navigation system. We hypothesized that by flying in an orbital motion, the errors in estimated zenith angles would cancel one another at opposing azimuths. The methodology was tested in a real flight, demonstrating that the position can be routinely estimated to within 4 km by performing orbits at a fixed altitude and airspeed. Throughout each orbit, positional estimates are generated from the individual celestial images, and these positions are averaged at the conclusion of the orbit. We show that by recursively estimating the mean position and using this position to recalibrate the orientation of the camera, the algorithm is capable of converging near the true location. Testing found that the algorithm is robust against initial conditions, requiring no knowledge of the prior position and only requiring the camera to be aligned to within a hemisphere of tolerance.</p></section>
  </div><div>
    <section><h2>Author Contributions</h2><p>Conceptualization, S.T. and J.C.; methodology, S.T.; software, S.T.; validation, S.T.; formal analysis, S.T.; investigation, S.T.; resources, J.C.; data curation, S.T. and J.C.; writing—original draft preparation, S.T.; writing—review and editing, J.C.; visualization, S.T.; supervision, J.C.; project administration, J.C. All authors have read and agreed to the published version of the manuscript.</p></section><section><h2>Funding</h2><p>This research received no external funding.</p></section><section><h2>Data Availability Statement</h2><p>The data presented in this study are available on request from the corresponding author.</p></section><section id="html-ack"><h2>Acknowledgments</h2><p>This work was supported by Scope Global Pty Ltd. under the Commonwealth Scholarships Program, and the Commonwealth of South Australia under the Australian Government Research Training Program.</p></section><section><h2>Conflicts of Interest</h2><p>The authors declare no conflict of interest.</p></section><section id="html-references_list"><h2>References</h2><ol><li id="B1-drones-08-00652" data-content="1.">Gatty, H. Aerial Navigation—Methods and Equipment. <span>SAE Trans.</span> <b>1932</b>, <span>27</span>, 153–170. [<a href="https://scholar.google.com/scholar_lookup?title=Aerial+Navigation%E2%80%94Methods+and+Equipment&amp;author=Gatty,+H.&amp;publication_year=1932&amp;journal=SAE+Trans.&amp;volume=27&amp;pages=153%E2%80%93170&amp;doi=10.4271/320042" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.4271/320042" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B2-drones-08-00652" data-content="2.">Pappalardi, F.; Dunham, S.; LeBlang, M.; Jones, T.; Bangert, J.; Kaplan, G. Alternatives to GPS. In Proceedings of the MTS/IEEE Oceans 2001. An Ocean Odyssey. Conference Proceedings (IEEE Cat. No. 01CH37295), Honolulu, HI, USA, 5–8 November 2001; IEEE: New York, NY, USA, 2001; Volume 3, pp. 1452–1459. [<a href="https://scholar.google.com/scholar_lookup?title=Alternatives+to+GPS&amp;conference=Proceedings+of+the+MTS/IEEE+Oceans+2001.+An+Ocean+Odyssey.+Conference+Proceedings+(IEEE+Cat.+No.+01CH37295)&amp;author=Pappalardi,+F.&amp;author=Dunham,+S.&amp;author=LeBlang,+M.&amp;author=Jones,+T.&amp;author=Bangert,+J.&amp;author=Kaplan,+G.&amp;publication_year=2001&amp;pages=1452%E2%80%931459&amp;doi=10.1109/OCEANS.2001.968047" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1109/OCEANS.2001.968047" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B3-drones-08-00652" data-content="3.">Ali, J.; Zhang, C.; Fang, J. An algorithm for astro-inertial navigation using CCD star sensors. <span>Aerosp. Sci. Technol.</span> <b>2006</b>, <span>10</span>, 449–454. [<a href="https://scholar.google.com/scholar_lookup?title=An+algorithm+for+astro-inertial+navigation+using+CCD+star+sensors&amp;author=Ali,+J.&amp;author=Zhang,+C.&amp;author=Fang,+J.&amp;publication_year=2006&amp;journal=Aerosp.+Sci.+Technol.&amp;volume=10&amp;pages=449%E2%80%93454&amp;doi=10.1016/j.ast.2006.01.004" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1016/j.ast.2006.01.004" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B4-drones-08-00652" data-content="4.">Ting, F.; Xiaoming, H. Inertial/celestial integrated navigation algorithm for long endurance unmanned aerial vehicle. <span>Acta Tech. CSAV (Ceskoslov. Akad. Ved.)</span> <b>2017</b>, <span>62</span>, 205–217. [<a href="https://scholar.google.com/scholar_lookup?title=Inertial/celestial+integrated+navigation+algorithm+for+long+endurance+unmanned+aerial+vehicle&amp;author=Ting,+F.&amp;author=Xiaoming,+H.&amp;publication_year=2017&amp;journal=Acta+Tech.+CSAV+(Ceskoslov.+Akad.+Ved.)&amp;volume=62&amp;pages=205%E2%80%93217" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li><li id="B5-drones-08-00652" data-content="5.">Levine, S.; Dennis, R.; Bachman, K.L. Strapdown Astro-Inertial Navigation Utilizing the Optical Wide-angle Lens Startracker. <span>Navigation</span> <b>1990</b>, <span>37</span>, 347–362. [<a href="https://scholar.google.com/scholar_lookup?title=Strapdown+Astro-Inertial+Navigation+Utilizing+the+Optical+Wide-angle+Lens+Startracker&amp;author=Levine,+S.&amp;author=Dennis,+R.&amp;author=Bachman,+K.L.&amp;publication_year=1990&amp;journal=Navigation&amp;volume=37&amp;pages=347%E2%80%93362&amp;doi=10.1002/j.2161-4296.1990.tb01561.x" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1002/j.2161-4296.1990.tb01561.x" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B6-drones-08-00652" data-content="6.">Chen, H.; Gao, H.; Zhang, H. Integrated navigation approaches of vehicle aided by the strapdown celestial angles. <span>Int. J. Adv. Robot. Syst.</span> <b>2020</b>, <span>17</span>, 1729881420932008. [<a href="https://scholar.google.com/scholar_lookup?title=Integrated+navigation+approaches+of+vehicle+aided+by+the+strapdown+celestial+angles&amp;author=Chen,+H.&amp;author=Gao,+H.&amp;author=Zhang,+H.&amp;publication_year=2020&amp;journal=Int.+J.+Adv.+Robot.+Syst.&amp;volume=17&amp;pages=1729881420932008&amp;doi=10.1177/1729881420932008" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1177/1729881420932008" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B7-drones-08-00652" data-content="7.">Gai, E.; Daly, K.; Harrison, J.; Lemos, L. Star-sensor-based satellite attitude/attitude rate estimator. <span>J. Guid. Control Dyn.</span> <b>1985</b>, <span>8</span>, 560–565. [<a href="https://scholar.google.com/scholar_lookup?title=Star-sensor-based+satellite+attitude/attitude+rate+estimator&amp;author=Gai,+E.&amp;author=Daly,+K.&amp;author=Harrison,+J.&amp;author=Lemos,+L.&amp;publication_year=1985&amp;journal=J.+Guid.+Control+Dyn.&amp;volume=8&amp;pages=560%E2%80%93565&amp;doi=10.2514/3.56393" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.2514/3.56393" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B8-drones-08-00652" data-content="8.">Wang, J.; Chun, J. Attitude determination using a single star sensor and a star density table. <span>J. Guid. Control Dyn.</span> <b>2006</b>, <span>29</span>, 1329–1338. [<a href="https://scholar.google.com/scholar_lookup?title=Attitude+determination+using+a+single+star+sensor+and+a+star+density+table&amp;author=Wang,+J.&amp;author=Chun,+J.&amp;publication_year=2006&amp;journal=J.+Guid.+Control+Dyn.&amp;volume=29&amp;pages=1329%E2%80%931338&amp;doi=10.2514/1.17249" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.2514/1.17249" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B9-drones-08-00652" data-content="9.">Mao, X.; Du, X.; Fang, H. Precise attitude determination strategy for spacecraft based on information fusion of attitude sensors: Gyros/GPS/Star-sensor. <span>Int. J. Aeronaut. Space Sci.</span> <b>2013</b>, <span>14</span>, 91–98. [<a href="https://scholar.google.com/scholar_lookup?title=Precise+attitude+determination+strategy+for+spacecraft+based+on+information+fusion+of+attitude+sensors:+Gyros/GPS/Star-sensor&amp;author=Mao,+X.&amp;author=Du,+X.&amp;author=Fang,+H.&amp;publication_year=2013&amp;journal=Int.+J.+Aeronaut.+Space+Sci.&amp;volume=14&amp;pages=91%E2%80%9398&amp;doi=10.5139/IJASS.2013.14.1.91" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.5139/IJASS.2013.14.1.91" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B10-drones-08-00652" data-content="10.">Guo, C.; Tong, X.; Liu, S.; Lu, X.; Chen, P.; Jin, Y.; Xie, H. High-precision attitude estimation method of star sensors and gyro based on complementary filter and unscented Kalman filter. <span>Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci.</span> <b>2017</b>, <span>42</span>, 49–53. [<a href="https://scholar.google.com/scholar_lookup?title=High-precision+attitude+estimation+method+of+star+sensors+and+gyro+based+on+complementary+filter+and+unscented+Kalman+filter&amp;author=Guo,+C.&amp;author=Tong,+X.&amp;author=Liu,+S.&amp;author=Lu,+X.&amp;author=Chen,+P.&amp;author=Jin,+Y.&amp;author=Xie,+H.&amp;publication_year=2017&amp;journal=Int.+Arch.+Photogramm.+Remote+Sens.+Spat.+Inf.+Sci.&amp;volume=42&amp;pages=49%E2%80%9353&amp;doi=10.5194/isprs-archives-XLII-3-W1-49-2017" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.5194/isprs-archives-XLII-3-W1-49-2017" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B11-drones-08-00652" data-content="11.">De Almeida Martins, F.; Carrara, V.; d’Amore, R. Positionless Attitude Estimation with Integrated Star and Horizon Sensors. <span>IEEE Access</span> <b>2023</b>, <span>12</span>, 2340–2348. [<a href="https://scholar.google.com/scholar_lookup?title=Positionless+Attitude+Estimation+with+Integrated+Star+and+Horizon+Sensors&amp;author=De+Almeida+Martins,+F.&amp;author=Carrara,+V.&amp;author=d%E2%80%99Amore,+R.&amp;publication_year=2023&amp;journal=IEEE+Access&amp;volume=12&amp;pages=2340%E2%80%932348&amp;doi=10.1109/ACCESS.2023.3348077" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1109/ACCESS.2023.3348077" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B12-drones-08-00652" data-content="12.">Gao, Z.; Wang, H.; Wang, W.; Xu, Y. SIMU/Triple star sensors integrated navigation method of HALE UAV based on atmospheric refraction correction. <span>J. Navig.</span> <b>2022</b>, <span>75</span>, 704–726. [<a href="https://scholar.google.com/scholar_lookup?title=SIMU/Triple+star+sensors+integrated+navigation+method+of+HALE+UAV+based+on+atmospheric+refraction+correction&amp;author=Gao,+Z.&amp;author=Wang,+H.&amp;author=Wang,+W.&amp;author=Xu,+Y.&amp;publication_year=2022&amp;journal=J.+Navig.&amp;volume=75&amp;pages=704%E2%80%93726&amp;doi=10.1017/S037346332100093X" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1017/S037346332100093X" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B13-drones-08-00652" data-content="13.">Teague, S.; Chahl, J. Imagery synthesis for drone celestial navigation simulation. <span>Drones</span> <b>2022</b>, <span>6</span>, 207. [<a href="https://scholar.google.com/scholar_lookup?title=Imagery+synthesis+for+drone+celestial+navigation+simulation&amp;author=Teague,+S.&amp;author=Chahl,+J.&amp;publication_year=2022&amp;journal=Drones&amp;volume=6&amp;pages=207&amp;doi=10.3390/drones6080207" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.3390/drones6080207" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B14-drones-08-00652" data-content="14.">Teague, S.; Chahl, J. Strapdown Celestial Attitude Estimation from Long Exposure Images for UAV Navigation. <span>Drones</span> <b>2023</b>, <span>7</span>, 52. [<a href="https://scholar.google.com/scholar_lookup?title=Strapdown+Celestial+Attitude+Estimation+from+Long+Exposure+Images+for+UAV+Navigation&amp;author=Teague,+S.&amp;author=Chahl,+J.&amp;publication_year=2023&amp;journal=Drones&amp;volume=7&amp;pages=52&amp;doi=10.3390/drones7010052" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.3390/drones7010052" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B15-drones-08-00652" data-content="15.">Zhang, H.; Zhang, C.; Tong, S.; Wang, R.; Li, C.; Tian, Y.; He, D.; Jiang, D.; Pu, J. Celestial Navigation and Positioning Method Based on Super-Large Field of View Star Sensors. In Proceedings of the China Satellite Navigation Conference, Jinan, China, 22–24 May 2024; Springer: Berlin/Heidelberg, Germany, 2023; pp. 475–487. [<a href="https://scholar.google.com/scholar_lookup?title=Celestial+Navigation+and+Positioning+Method+Based+on+Super-Large+Field+of+View+Star+Sensors&amp;conference=Proceedings+of+the+China+Satellite+Navigation+Conference&amp;author=Zhang,+H.&amp;author=Zhang,+C.&amp;author=Tong,+S.&amp;author=Wang,+R.&amp;author=Li,+C.&amp;author=Tian,+Y.&amp;author=He,+D.&amp;author=Jiang,+D.&amp;author=Pu,+J.&amp;publication_year=2023&amp;pages=475%E2%80%93487&amp;doi=10.1007/978-981-99-6928-9_41" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1007/978-981-99-6928-9_41" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B16-drones-08-00652" data-content="16.">Van Allen, J.A. Basic principles of celestial navigation. <span>Am. J. Phys.</span> <b>2004</b>, <span>72</span>, 1418–1424. [<a href="https://scholar.google.com/scholar_lookup?title=Basic+principles+of+celestial+navigation&amp;author=Van+Allen,+J.A.&amp;publication_year=2004&amp;journal=Am.+J.+Phys.&amp;volume=72&amp;pages=1418%E2%80%931424&amp;doi=10.1119/1.1778391" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1119/1.1778391" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B17-drones-08-00652" data-content="17.">Liebe, C.C. Accuracy performance of star trackers-a tutorial. <span>IEEE Trans. Aerosp. Electron. Syst.</span> <b>2002</b>, <span>38</span>, 587–599. [<a href="https://scholar.google.com/scholar_lookup?title=Accuracy+performance+of+star+trackers-a+tutorial&amp;author=Liebe,+C.C.&amp;publication_year=2002&amp;journal=IEEE+Trans.+Aerosp.+Electron.+Syst.&amp;volume=38&amp;pages=587%E2%80%93599&amp;doi=10.1109/TAES.2002.1008988" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1109/TAES.2002.1008988" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B18-drones-08-00652" data-content="18.">Delabie, T.; Schutter, J.D.; Vandenbussche, B. An accurate and efficient gaussian fit centroiding algorithm for star trackers. <span>J. Astronaut. Sci.</span> <b>2014</b>, <span>61</span>, 60–84. [<a href="https://scholar.google.com/scholar_lookup?title=An+accurate+and+efficient+gaussian+fit+centroiding+algorithm+for+star+trackers&amp;author=Delabie,+T.&amp;author=Schutter,+J.D.&amp;author=Vandenbussche,+B.&amp;publication_year=2014&amp;journal=J.+Astronaut.+Sci.&amp;volume=61&amp;pages=60%E2%80%9384&amp;doi=10.1007/s40295-015-0034-4" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1007/s40295-015-0034-4" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B19-drones-08-00652" data-content="19.">Teague, S.; Chahl, J. Bootstrap geometric ground calibration method for wide angle star sensors. <span>JOSA A</span> <b>2024</b>, <span>41</span>, 654–663. [<a href="https://scholar.google.com/scholar_lookup?title=Bootstrap+geometric+ground+calibration+method+for+wide+angle+star+sensors&amp;author=Teague,+S.&amp;author=Chahl,+J.&amp;publication_year=2024&amp;journal=JOSA+A&amp;volume=41&amp;pages=654%E2%80%93663&amp;doi=10.1364/JOSAA.517943" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1364/JOSAA.517943" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B20-drones-08-00652" data-content="20.">Wei, X.; Zhang, G.; Jiang, J. Star identification algorithm based on log-polar transform. <span>J. Aerosp. Comput. Inf. Commun.</span> <b>2009</b>, <span>6</span>, 483–490. [<a href="https://scholar.google.com/scholar_lookup?title=Star+identification+algorithm+based+on+log-polar+transform&amp;author=Wei,+X.&amp;author=Zhang,+G.&amp;author=Jiang,+J.&amp;publication_year=2009&amp;journal=J.+Aerosp.+Comput.+Inf.+Commun.&amp;volume=6&amp;pages=483%E2%80%93490&amp;doi=10.2514/1.30393" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.2514/1.30393" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B21-drones-08-00652" data-content="21.">Lawrence, J.; Bernal, J.; Witzgall, C. A purely algebraic justification of the Kabsch-Umeyama algorithm. <span>J. Res. Natl. Inst. Stand. Technol.</span> <b>2019</b>, <span>124</span>, 1. [<a href="https://scholar.google.com/scholar_lookup?title=A+purely+algebraic+justification+of+the+Kabsch-Umeyama+algorithm&amp;author=Lawrence,+J.&amp;author=Bernal,+J.&amp;author=Witzgall,+C.&amp;publication_year=2019&amp;journal=J.+Res.+Natl.+Inst.+Stand.+Technol.&amp;volume=124&amp;pages=1&amp;doi=10.6028/jres.124.028" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.6028/jres.124.028" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B22-drones-08-00652" data-content="22.">Gebre-Egziabher, D.; Powell, J.; Enge, P. Design and performance analysis of a low-cost aided dead reckoning navigation system. <span>Gyroscopy Navig.</span> <b>2001</b>, <span>4</span>, 83–92. [<a href="https://scholar.google.com/scholar_lookup?title=Design+and+performance+analysis+of+a+low-cost+aided+dead+reckoning+navigation+system&amp;author=Gebre-Egziabher,+D.&amp;author=Powell,+J.&amp;author=Enge,+P.&amp;publication_year=2001&amp;journal=Gyroscopy+Navig.&amp;volume=4&amp;pages=83%E2%80%9392" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li><li id="B23-drones-08-00652" data-content="23.">Wang, W.; Wei, X.; Li, J.; Zhang, G. Guide star catalog generation for short-wave infrared (SWIR) all-time star sensor. <span>Rev. Sci. Instrum.</span> <b>2018</b>, <span>89</span>. [<a href="https://scholar.google.com/scholar_lookup?title=Guide+star+catalog+generation+for+short-wave+infrared+(SWIR)+all-time+star+sensor&amp;author=Wang,+W.&amp;author=Wei,+X.&amp;author=Li,+J.&amp;author=Zhang,+G.&amp;publication_year=2018&amp;journal=Rev.+Sci.+Instrum.&amp;volume=89&amp;doi=10.1063/1.5023157" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1063/1.5023157" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li><li id="B24-drones-08-00652" data-content="24.">Ni, Y.; Wang, X.; Dai, D.; Tan, W.; Qin, S. Adaptive section non-uniformity correction method of short-wave infrared star images for a star tracker. <span>Appl. Opt.</span> <b>2022</b>, <span>61</span>, 6992–6999. [<a href="https://scholar.google.com/scholar_lookup?title=Adaptive+section+non-uniformity+correction+method+of+short-wave+infrared+star+images+for+a+star+tracker&amp;author=Ni,+Y.&amp;author=Wang,+X.&amp;author=Dai,+D.&amp;author=Tan,+W.&amp;author=Qin,+S.&amp;publication_year=2022&amp;journal=Appl.+Opt.&amp;volume=61&amp;pages=6992%E2%80%936999&amp;doi=10.1364/AO.457458" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="https://doi.org/10.1364/AO.457458" target="_blank" rel="noopener noreferrer">CrossRef</a>]</li></ol></section><section id="FiguresandTables" type="display-objects"><div id="drones-08-00652-f001">
    <p><b>Figure 1.</b>
      The circles around points </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p> show the intersection of their respective planes with the terrestrial sphere, forming circles about which the observation could have been made. Points </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p> are taken from the observed stars, located such that their respective star is observed at the zenith. The zenith-angles </p><math display="inline"><semantics>
  <msub>
    <mi>ζ</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>ζ</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p> represent the angle at which the stars were actually observed. In this case, two stars were observed, resulting in two potential points (</p><math display="inline"><semantics>
  <msub>
    <mi>P</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>P</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p>) at which this observation could have been made. Additional observations serve to reduce this ambiguity.
</p><!--     <p><a class="html-figpopup" href="#fig_body_display_drones-08-00652-f001">
      Click here to enlarge figure
    </a></p> -->

  </div>
<div id="fig_body_display_drones-08-00652-f001">
  <div> <p><b>Figure 1.</b>
      The circles around points </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p> show the intersection of their respective planes with the terrestrial sphere, forming circles about which the observation could have been made. Points </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>S</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p> are taken from the observed stars, located such that their respective star is observed at the zenith. The zenith-angles </p><math display="inline"><semantics>
  <msub>
    <mi>ζ</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>ζ</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p> represent the angle at which the stars were actually observed. In this case, two stars were observed, resulting in two potential points (</p><math display="inline"><semantics>
  <msub>
    <mi>P</mi>
    <mn>1</mn>
  </msub>
</semantics></math><p> and </p><math display="inline"><semantics>
  <msub>
    <mi>P</mi>
    <mn>2</mn>
  </msub>
</semantics></math><p>) at which this observation could have been made. Additional observations serve to reduce this ambiguity.</p></div>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g001.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g001.png" alt="Drones 08 00652 g001" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g001.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g001.png"></p>
</div>
<div id="drones-08-00652-f002">
  
  <p><b>Figure 2.</b>
      Star tracker operating on video footage captured in-flight. Image intensity is amplified 10×.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f002">
  <p><b>Figure 2.</b>
      Star tracker operating on video footage captured in-flight. Image intensity is amplified 10×.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g002.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g002.png" alt="Drones 08 00652 g002" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g002.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g002.png"></p>
</div>
<div id="drones-08-00652-f003">
  
  <p><b>Figure 3.</b>
      Platform layout of the autopilot and celestial payload within the airframe.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f003">
  <p><b>Figure 3.</b>
      Platform layout of the autopilot and celestial payload within the airframe.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g003.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g003.png" alt="Drones 08 00652 g003" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g003.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g003.png"></p>
</div>
<div id="drones-08-00652-f004">
  
  <p><b>Figure 4.</b>
      The celestial payload, consisting of a Raspberry Pi 5 and an Alvium 1800 U-240 monochrome sensor fitted with a 6 mm f/1.4 wide angle lens.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f004">
  <p><b>Figure 4.</b>
      The celestial payload, consisting of a Raspberry Pi 5 and an Alvium 1800 U-240 monochrome sensor fitted with a 6 mm f/1.4 wide angle lens.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g004.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g004.png" alt="Drones 08 00652 g004" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g004.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g004.png"></p>
</div>
<div id="drones-08-00652-f005">
  
  <p><b>Figure 5.</b>
      True camera orientation in the aircraft body frame, as calculated using a combination of stellar observations, aircraft attitude, and GPS data. Data are captured from a straight segment of flight over 89.5 s (895 video frames). <b>Top</b>: Calculated camera yaw angle in aircraft body frame. <b>Middle</b>: Calculated camera pitch angle in aircraft body frame. <b>Bottom</b>: Calculated camera roll angle in aircraft body frame.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f005">
  <p><b>Figure 5.</b>
      True camera orientation in the aircraft body frame, as calculated using a combination of stellar observations, aircraft attitude, and GPS data. Data are captured from a straight segment of flight over 89.5 s (895 video frames). <b>Top</b>: Calculated camera yaw angle in aircraft body frame. <b>Middle</b>: Calculated camera pitch angle in aircraft body frame. <b>Bottom</b>: Calculated camera roll angle in aircraft body frame.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g005.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g005.png" alt="Drones 08 00652 g005" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g005.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g005.png"></p>
</div>
<div id="drones-08-00652-f006">
  
  <p><b>Figure 6.</b>
      Position estimation over a an 89.5 s straight segment of flight. It can be seen that, by assuming the camera orientation is fixed, significant positional error makes its way into the estimate. <b>Top</b>: Latitude estimated using celestial imagery with Ardupilot AHRS. <b>Bottom</b>: Longitude estimated using celestial imagery with Ardupilot AHRS.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f006">
  <p><b>Figure 6.</b>
      Position estimation over a an 89.5 s straight segment of flight. It can be seen that, by assuming the camera orientation is fixed, significant positional error makes its way into the estimate. <b>Top</b>: Latitude estimated using celestial imagery with Ardupilot AHRS. <b>Bottom</b>: Longitude estimated using celestial imagery with Ardupilot AHRS.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g006.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g006.png" alt="Drones 08 00652 g006" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g006.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g006.png"></p>
</div>
<div id="drones-08-00652-f007">
  
  <p><b>Figure 7.</b>
      Visualization of the position estimates as the algorithm converges on the location of the aircraft. Each blue point represents an independent position that was calculated from an image frame, the red dot represent the estimated position taken from the mean of the individual estimates, and the yellow dot represents the true center of the aircraft’s orbit.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f007">
  <p><b>Figure 7.</b>
      Visualization of the position estimates as the algorithm converges on the location of the aircraft. Each blue point represents an independent position that was calculated from an image frame, the red dot represent the estimated position taken from the mean of the individual estimates, and the yellow dot represents the true center of the aircraft’s orbit.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g007.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g007.png" alt="Drones 08 00652 g007" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g007.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g007.png"></p>
</div>
<div id="drones-08-00652-f008">
  
  <p><b>Figure 8.</b>
      Plot of the standard error in pitch and variance versus the true error. It can be seen that the relationship is approximately linear.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f008">
  <p><b>Figure 8.</b>
      Plot of the standard error in pitch and variance versus the true error. It can be seen that the relationship is approximately linear.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g008.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g008.png" alt="Drones 08 00652 g008" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g008.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g008.png"></p>
</div>
<div id="drones-08-00652-f009">
  
  <p><b>Figure 9.</b>
      Visualization of position estimates for a given camera misalignment. It can be seen that as long as the camera is aligned to within 90°, the mean estimated position provides a good approximation of the true position.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f009">
  <p><b>Figure 9.</b>
      Visualization of position estimates for a given camera misalignment. It can be seen that as long as the camera is aligned to within 90°, the mean estimated position provides a good approximation of the true position.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g009.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g009.png" alt="Drones 08 00652 g009" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g009.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g009.png"></p>
</div>
<div id="drones-08-00652-f010">
  
  <p><b>Figure 10.</b>
      Simulated trajectory of the aircraft during a GPS-denied orbit in high wind conditions.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f010">
  <p><b>Figure 10.</b>
      Simulated trajectory of the aircraft during a GPS-denied orbit in high wind conditions.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g010.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g010.png" alt="Drones 08 00652 g010" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g010.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g010.png"></p>
</div>
<div id="drones-08-00652-f011">
  
  <p><b>Figure 11.</b>
      Comparison of position estimation in high wind conditions. It can be seen that the course followed by a GPS-guided orbit introduces errors in the estimation process. The GPS-denied orbit follows a fixed-attitude orbit, resulting in drift, but ultimately yielding a more accurate position.
</p>
</div>
<div id="fig_body_display_drones-08-00652-f011">
  <p><b>Figure 11.</b>
      Comparison of position estimation in high wind conditions. It can be seen that the course followed by a GPS-guided orbit introduces errors in the estimation process. The GPS-denied orbit follows a fixed-attitude orbit, resulting in drift, but ultimately yielding a more accurate position.</p>
  <p><img data-large="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g011.png" data-original="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g011.png" alt="Drones 08 00652 g011" data-lsrc="/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g011.png" src="https://www.mdpi.com/drones/drones-08-00652/article_deploy/html/images/drones-08-00652-g011.png"></p>
</div>
<div id="drones-08-00652-t001">
  
  <p><b>Table 1.</b>
    Flight plan for the test flight.
  </p>
</div>
<div id="table_body_display_drones-08-00652-t001">
  

      <p><b>Table 1.</b>
    Flight plan for the test flight.</p>
      <table>
        <thead><tr><th>Description</th><th>Direction</th><th>Repeats</th><th>Radius (m)</th><th>Altitude (m)</th></tr></thead><tbody><tr><td>Takeoff</td><td> </td><td> </td><td> </td><td>30</td></tr><tr><td>Orbit to Altitude</td><td>CCW</td><td>4</td><td>300</td><td>800</td></tr><tr><td>Straight Legs</td><td>N/S</td><td>6</td><td>2500</td><td>800</td></tr><tr><td>Orbit</td><td>CW</td><td>1</td><td>1200</td><td>800</td></tr><tr><td>Orbit</td><td>CW</td><td>2</td><td>600</td><td>800</td></tr><tr><td>Orbit</td><td>CCW</td><td>1</td><td>600</td><td>800</td></tr><tr><td>Orbit</td><td>CCW</td><td>1</td><td>1200</td><td>800</td></tr><tr><td>Orbit to Altitude</td><td>CCW</td><td>1</td><td>300</td><td>100</td></tr><tr><td>Land</td><td> </td><td> </td><td> </td><td>0</td></tr></tbody>
      </table>



</div>
<div id="drones-08-00652-t002">
  
  <p><b>Table 2.</b>
    Celestial positioning results.
  </p>
</div>
<div id="table_body_display_drones-08-00652-t002">
  

      <p><b>Table 2.</b>
    Celestial positioning results.</p>
      <table>
        <thead><tr><th>Description</th><th>Direction</th><th>Radius (m)</th><th>Pos. Error (km)</th><th>Iterations</th></tr></thead><tbody><tr><td>Orbit to Altitude (1)</td><td>CCW</td><td>300</td><td>3.56</td><td>5</td></tr><tr><td>Orbit to Altitude (2)</td><td>CCW</td><td>300</td><td>7.18</td><td>4</td></tr><tr><td>Orbit to Altitude (3)</td><td>CCW</td><td>300</td><td>9.67</td><td>4</td></tr><tr><td>Orbit to Altitude (4)</td><td>CCW</td><td>300</td><td>9.89</td><td>4</td></tr><tr><td>Orbit (5)</td><td>CW</td><td>1200</td><td>2.21</td><td>4</td></tr><tr><td>Orbit (6)</td><td>CW</td><td>600</td><td>3.48</td><td>5</td></tr><tr><td>Orbit (7)</td><td>CW</td><td>600</td><td>2.54</td><td>5</td></tr><tr><td>Orbit (8)</td><td>CCW</td><td>600</td><td>1.73</td><td>5</td></tr><tr><td>Orbit (9)</td><td>CCW</td><td>1200</td><td>2.90</td><td>4</td></tr><tr><td>Orbit to Altitude (10)</td><td>CCW</td><td>300</td><td>7.16</td><td>6</td></tr></tbody>
      </table>



</div>
<div id="drones-08-00652-t003">
  
  <p><b>Table 3.</b>
    Sensitivity to initial camera calibration.
  </p>
</div>
<div id="table_body_display_drones-08-00652-t003">
  

      <p><b>Table 3.</b>
    Sensitivity to initial camera calibration.</p>
      <table>
        <thead><tr><th>Initial Orientation Error</th><th>Final Pos. Error (km)</th><th>Iterations</th></tr></thead><tbody><tr><td>Nil (calibrated)</td><td>2.47</td><td>2</td></tr><tr><td>Initial Guess (<math display="inline"><semantics>
  <mrow>
    <mo>≈</mo>
    <msup>
      <mn>5</mn>
      <mo>∘</mo>
    </msup>
  </mrow>
</semantics></math>)</td><td>2.49</td><td>4</td></tr><tr><td>45°</td><td>2.46</td><td>5</td></tr><tr><td>60°</td><td>2.51</td><td>5</td></tr><tr><td>85°</td><td>2.49</td><td>6</td></tr><tr><td>120°</td><td>19,987.66</td><td>5</td></tr></tbody>
      </table>



</div>
</section><section><table><tbody><tr id=""><td></td><td><p><b>Disclaimer/Publisher’s Note:</b> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></td></tr></tbody></table></section>
    <section id="html-copyright"><br>© 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer">https://creativecommons.org/licenses/by/4.0/</a>).</section>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using eSIMs with devices that only have a physical SIM slot via a 9eSIM SIM car (251 pts)]]></title>
            <link>https://neilzone.co.uk/2025/01/using-esims-with-devices-that-only-have-a-physical-sim-slot-via-a-9esim-sim-card-with-android-and-linux/</link>
            <guid>42767584</guid>
            <pubDate>Mon, 20 Jan 2025 11:33:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neilzone.co.uk/2025/01/using-esims-with-devices-that-only-have-a-physical-sim-slot-via-a-9esim-sim-card-with-android-and-linux/">https://neilzone.co.uk/2025/01/using-esims-with-devices-that-only-have-a-physical-sim-slot-via-a-9esim-sim-card-with-android-and-linux/</a>, See on <a href="https://news.ycombinator.com/item?id=42767584">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<article>
    <p>Do you have a phone, tablet, or laptop (or, well, any device…) which will only take a physical SIM, but with which you’d like to use eSIMs?</p>
<p>Then this is a blogpost for you, as that’s exactly what this is: a physical SIM, onto which one can provision eSIMs, using software to swap between them.</p>
<h2 id="what-i-bought-from-9esim">What I bought from 9eSIM</h2>
<p>I bought the bundle - SIM and smartcard reader - from <a href="https://www.9esim.com/">9eSIM</a>.</p>
<p>The first shipment from China got lost but, after a bit of waiting, they posted another one without complaint. That was shipped by a different delivery company, and it arrived in just over a week.</p>
<p>In the box, I got:</p>
<ul>
<li>
<p>A SIM card, with the usual push-out sections to change it from normal to micro to nano.</p>
<ul>
<li>The SIM was in a blue envelope, not attached to the cardboard saying “9eSIM v2” - at first, I thought “they’ve forgotten to put in the SIM card”.</li>
<li>If you want to use the smartcard reader, <em>do not discard the SIM’s packaging</em>. You will need it to make the smartcard reader work reliably.</li>
</ul>
</li>
<li>
<p>A SIM card adapter.</p>
</li>
<li>
<p>A USB smartcard reader, and a USB-A to USB-C adapter.</p>
</li>
</ul>
<p>Including delivery, it came to about £30.</p>
<h2 id="use-the-sims-packaging-to-make-it-fit-in-the-smartcard-reader">Use the SIM’s packaging to make it fit in the smartcard reader</h2>
<p>I spent a <em>lot</em> of time trying to get the SIM and smartcard reader working.</p>
<p>The solution was a frustratingly simple one: the best way to use the supplied smartcard reader is to use the original packaging for the SIM.</p>
<p>This packaging is the right size to slide into the reader while positioning the SIM’s contacts over the contact points on the reader. I will keep that together with the smartcard reader.</p>
<p>When I realised that - rather than trying to slide the popped-out SIM into the right place and keep it there - it Just Worked.</p>
<h2 id="adding-and-switching-esims">Adding and switching eSIMs</h2>
<p>The SIM card is advertised as have “a memory capacity of 1.6M, [which] can store up to 50 groups of eSIM profile data”.</p>
<p>To make use of it, one needs to download one or more eSIMs to the SIM.</p>
<p>To do this, and to switch between profiles, one needs to use a “Local Profile Agent” (or “LPA”).</p>
<p>I tested the process for this using both Android and Linux, and both worked just fine.</p>
<h2 id="test-esim-profiles-you-can-try-to-for-free">Test eSIM profiles you can try to for free</h2>
<p>While I was getting used to how it worked, I didn’t want to buy an actual eSIM.</p>
<p>Fortunately, there are a couple of options.</p>
<p>First, there are four official <a href="https://source.android.com/docs/core/connect/esim-test-profiles">Android test eSIM profiles</a></p>
<p>While I could start to provision these, I could not download them, with what appears to be a TLS error.</p>
<p>Second, osmocom has a <a href="https://euicc-manual.osmocom.org/docs/rsp/known-test-profile/">very useful page of other test profiles</a>.</p>
<p>I was able to install test eSIM profiles for <a href="https://qr.esim.tf/1$rsp.truphone.com$QR-G-5C-1LS-1W1Z9P7">TruPhone</a> and <a href="https://qr.esim.tf/1$rsp.truphone.com$QRF-SPEEDTEST">TruPhone / Speedtest</a>.</p>
<p>These provisioned correctly onto the SIM. I could not use them to make or receive calls or start a data session - which was fine - but they did show up in Android’s SIM card manager as available eSIMs.</p>
<h2 id="adding-and-switching-esims-via-android">Adding and switching eSIMs via Android</h2>
<p>There is an .apk from the 9eSIM site.</p>
<p>I wasn’t too keen on using this but, well, it worked, and is probably the simplest option.</p>
<p>I tested it first by putting the SIM into my phone, replacing my main physical SIM. This was recognised by the app immediately.</p>
<p>Once I had worked out how to correctly seat the SIM in the reader, I tried that too, using a USB-C hub to connect the smartcard reader to my phone. That worked fine too.</p>
<p>Provisioning eSIMs using the Android application was easy as long as the QR code was on a different screen, since I could just scan the QR code for the eSIM using the camera. I’ve not looked to see if there is a way to use Android to import a QR code on its own screen.</p>
<p>Using the 9eSIM application, one can enable and disable eSIMs, and swap between them. Once enabled, they appeared in Android’s own SIM management settings. I did not need to reboot.</p>
<p>Deleting an eSIM is also easy. One needs to type a security phrase - the name of the eSIM - to trigger deletion, which is a simple means of avoiding accidental deletion.</p>
<h2 id="adding-and-switching-esims-via-linux">Adding and switching eSIMs via Linux</h2>
<p>Since I want to use the SIM with the integrated WWAN modem of a laptop running Linux, I was keen to see if I could get this all to work using Linux and Free software.</p>
<p>So far, I have not found a way of writing profiles to the SIM while it is in the laptop - I need to take it out and put it in the smartcard reader.</p>
<p>And, if I’m going to do that then, from a practical point of view, it is little more effort to hook it up to my phone and swap and provisions eSIMs from there.</p>
<p>Still, I wanted to get it working within Linux and FOSS just because.</p>
<h3 id="the-smartcard-reader-and-linux">The smartcard reader and Linux</h3>
<p>Connecting the smartcard reader and running <code>lsusb</code>:</p>
<pre tabindex="0"><code>Bus 001 Device 011: ID 058f:9540 Alcor Micro Corp. AU9540 Smartcard Reader
</code></pre><p>It looks like this is a quite common smartcard reader, sometimes built into laptops.</p>
<p><code>lsusb -s 1:11 -v</code>:</p>
<pre tabindex="0"><code>Bus 001 Device 011: ID 058f:9540 Alcor Micro Corp. AU9540 Smartcard Reader
Negotiated speed: Full Speed (12Mbps)
Device Descriptor:
  bLength                18
  bDescriptorType         1
  bcdUSB               2.01
  bDeviceClass            0 [unknown]
  bDeviceSubClass         0 [unknown]
  bDeviceProtocol         0 
  bMaxPacketSize0         8
  idVendor           0x058f Alcor Micro Corp.
  idProduct          0x9540 AU9540 Smartcard Reader
  bcdDevice            1.20
  iManufacturer           1 Generic
  iProduct                2 EMV Smartcard Reader
  iSerial                 0 
  bNumConfigurations      1
  Configuration Descriptor:
    bLength                 9
    bDescriptorType         2
    wTotalLength       0x005d
    bNumInterfaces          1
    bConfigurationValue     1
    iConfiguration          0 
    bmAttributes         0xa0
      (Bus Powered)
      Remote Wakeup
    MaxPower               50mA
    Interface Descriptor:
      bLength                 9
      bDescriptorType         4
      bInterfaceNumber        0
      bAlternateSetting       0
      bNumEndpoints           3
      bInterfaceClass        11 Chip/SmartCard
      bInterfaceSubClass      0 [unknown]
      bInterfaceProtocol      0 
      iInterface              0 
      ChipCard Interface Descriptor:
        bLength                54
        bDescriptorType        33
        bcdCCID              1.10
        nMaxSlotIndex           0
        bVoltageSupport         7  5.0V 3.0V 1.8V 
        dwProtocols             3  T=0 T=1
        dwDefaultClock       3700
        dwMaxiumumClock     12000
        bNumClockSupported      3
        dwDataRate           9946 bps
        dwMaxDataRate      688172 bps
        bNumDataRatesSupp.    138
        dwMaxIFSD             254
        dwSyncProtocols  00000007  2-wire 3-wire I2C
        dwMechanical     00000000 
        dwFeatures       000404BE
          Auto configuration based on ATR
          Auto activation on insert
          Auto voltage selection
          Auto clock change
          Auto baud rate change
          Auto PPS made by CCID
          Auto IFSD exchange (T=1)
          Short and extended APDU level exchange
        dwMaxCCIDMsgLen       272
        bClassGetResponse    echo
        bClassEnvelope       echo
        wlcdLayout           none
        bPINSupport             0 
        bMaxCCIDBusySlots       1
      Endpoint Descriptor:
        bLength                 7
        bDescriptorType         5
        bEndpointAddress     0x81  EP 1 IN
        bmAttributes            3
          Transfer Type            Interrupt
          Synch Type               None
          Usage Type               Data
        wMaxPacketSize     0x0004  1x 4 bytes
        bInterval               1
      Endpoint Descriptor:
        bLength                 7
        bDescriptorType         5
        bEndpointAddress     0x02  EP 2 OUT
        bmAttributes            2
          Transfer Type            Bulk
          Synch Type               None
          Usage Type               Data
        wMaxPacketSize     0x0010  1x 16 bytes
        bInterval               0
      Endpoint Descriptor:
        bLength                 7
        bDescriptorType         5
        bEndpointAddress     0x83  EP 3 IN
        bmAttributes            2
          Transfer Type            Bulk
          Synch Type               None
          Usage Type               Data
        wMaxPacketSize     0x0010  1x 16 bytes
        bInterval               0
Binary Object Store Descriptor:
  bLength                 5
  bDescriptorType        15
  wTotalLength       0x000c
  bNumDeviceCaps          1
  USB 2.0 Extension Device Capability:
    bLength                 7
    bDescriptorType        16
    bDevCapabilityType      2
    bmAttributes   0x00000002
      HIRD Link Power Management (LPM) Supported
Device Status:     0x0000
  (Bus Powered)
</code></pre><p>I spent quite a long time trying to work out why it would not detect the SIM in the smartcard reader (which, as above, was solved by using the supplied packaging material).</p>
<p>I have not reproduced all my troubleshooting here, but it was reasonably obvious from the error messages that, while my laptop recognised and could talk to the smartcard reader, the SIM was not recognised.</p>
<h3 id="lpac"><code>lpac</code></h3>
<p>I started with a command line tool called <a href="https://github.com/estkme-group/lpac"><code>lpac</code></a>.</p>
<p>It has a <a href="https://github.com/estkme-group/lpac/releases/download/v2.2.1/lpac_2.1.0_amd64.deb">.deb release version</a>, which I installed with:</p>
<pre tabindex="0"><code>sudo apt install lpac_2.1.0_amd64.deb -y
</code></pre><p>It installed without error to <code>/usr/bin/lpac</code>.</p>
<p><code>lpac</code>’s output is in json, so run it through <code>jq</code> to prettify it.</p>
<p><code>lpac chip info | jq</code> shows information about the SIM card / eUICC:</p>
<p>(I’ve removed some potentially sensitive/personal bits)</p>
<pre tabindex="0"><code>{
  "type": "lpa",
  "payload": {
    "code": 0,
    "message": "success",
    "data": {
      "eidValue": "8904xxxxx",
      "EuiccConfiguredAddresses": {
        "defaultDpAddress": "smdp-plus-0.eu.cd.rsp.kigen.com",
        "rootDsAddress": "lpa.ds.gsma.com"
      },
      "EUICCInfo2": {
        "profileVersion": "2.3.1",
        "svn": "2.3.0",
        "euiccFirmwareVer": "36.17.4",
        "extCardResource": {
          "installedApplication": 11,
          "freeNonVolatileMemory": 1528320,
          "freeVolatileMemory": 32739
        },
        "uiccCapability": [
          "usimSupport",
          "isimSupport",
          "csimSupport",
          "akaMilenage",
          "akaCave",
          "akaTuak128",
          "akaTuak256",
          "gbaAuthenUsim",
          "gbaAuthenISim",
          "eapClient",
          "javacard",
          "multipleUsimSupport",
          "multipleIsimSupport",
          "multipleCsimSupport"
        ],
        "ts102241Version": "15.1.0",
        "globalplatformVersion": "2.3.0",
        "rspCapability": [
          "additionalProfile",
          "testProfileSupport"
        ],
        "euiccCiPKIdListForVerification": [
          "8137xxxxx"
        ],
        "euiccCiPKIdListForSigning": [
          "8137xxxxx"
        ],
        "euiccCategory": null,
        "ppVersion": "1.0.0",
        "sasAcreditationNumber": "KN-DN-UP-0924",
        "certificationDataObject": {
          "platformLabel": null,
          "discoveryBaseURL": null
        }
      },
      "rulesAuthorisationTable": [
        {
          "pprIds": [
            "ppr1",
            "ppr2"
          ],
          "allowedOperators": [
            {
              "plmn": "eeeeee",
              "gid1": null,
              "gid2": null
            }
          ],
          "pprFlags": []
        }
      ]
    }
  }
}
</code></pre><p>It appears to use a platform run by Kigen for remote SIM provisioning, so it looks like there is a dependency on a third party’s infrastructure to make all this work.</p>
<p>Use <code>lpac profile list | jq</code> to list the downloaded SIMs, and see their ICCID and AID.</p>
<p>To enable / disable downloaded eSIMs, use <code>lpac profile {enable,disable} ICCID/AID | jq</code>.</p>
<p>Using <a href="https://qr.esim.tf/1$rsp.truphone.com$QRF-SPEEDTEST">this test profile</a>, I tried adding/downloading SIMs via <code>lpac</code>.</p>
<p>Specifying the SM-DP+ address (<code>-s</code>) and the matching ID (<code>-m</code>) worked:</p>
<pre tabindex="0"><code>lpac profile download -s rsp.truphone.com -m QRF-SPEEDTEST`
</code></pre><p>Specifying the activation code (<code>-a</code>) in the manner set out in the documentation failed <em>(Edit: quoting issue; see below)</em>:</p>
<pre tabindex="0"><code>lpac profile download -a LPA:1$rsp.truphone.com$QRF-SPEEDTEST | jq
</code></pre><p>It returned:</p>
<pre tabindex="0"><code>{
  "type": "progress",
  "payload": {
    "code": 0,
    "message": "es10b_cancel_session",
    "data": null
  }
}
{
  "type": "progress",
  "payload": {
    "code": 0,
    "message": "es9p_cancel_session",
    "data": null
  }
}
{
  "type": "lpa",
  "payload": {
    "code": -1,
    "message": "activation_code",
    "data": "invalid"
  }
}
</code></pre><p>To check this was not just an issue with the test profiles, I bought a <a href="https://www.lycamobile.co.uk/en/bundles/sim-only-deals/#30-day-plans">30 day, £2.50 LycaMobile eSIM</a> for testing, so that I could try it with an “actual” eSIM, rather than a test eSIM.</p>
<p>Lyca’s email gave me a QR code, but did not specify (explicitly) the SM-DP+ address or matching ID.</p>
<p>The image’s alt-text was just my order number with Lyca.</p>
<p>I used GNOME’s <a href="https://apps.gnome.org/en-GB/Decoder/">“Decoder”</a>, which lets one scan a QR card either from the device’s camera or from a screenshot.</p>
<p>That gave me the full activation code, which looked like this:</p>
<pre tabindex="0"><code>LPA:1$dp-plus-par07-01.oasis-smartsim.com$XHY48-xxxxx-xxxxx-xxxxx$x.x.x.x.x.x.xxxxx.x.x.x.x
</code></pre><p>(I don’t know how much of it is sensitive!)</p>
<p>This too failed using <code>lpac</code>.</p>
<p>I have not dug into <em>why</em> it failed.</p>
<p>I missed the opportunity to test extracting the parameters and pushing them manually into <code>lpac</code> via <code>-s</code> and <code>-m</code> as I had already used EasyLPAC at that point. I suspect that that would have worked.</p>
<p><em>Update</em>: doh, it is just a matter of quoting the string, with single quotes. So:</p>
<pre tabindex="0"><code>lpac profile download -a 'LPA:1$rsp.truphone.com$QRF-BETTERROAMING-PMRDGIR2EARDEIT5'
</code></pre><p>How embarrassing.</p>
<p>I tried to update the <code>lpac</code> documentation to make this more obvious, but <a href="https://github.com/estkme-group/lpac/pull/183#issuecomment-2602304507">the maintainer decided that it wasn’t needed</a>. So I guess this blogpost will have to do :)</p>
<p>Thanks to <a href="https://toot.wales/@tswsl">Tom</a> for that :)</p>
<h3 id="linux-gui-easylpac">Linux GUI: EasyLPAC</h3>
<p>There is a GUI for <code>lpac</code>, called <a href="https://github.com/creamlike1024/EasyLPAC">EasyLPAC</a>.</p>
<p>I used <a href="https://github.com/creamlike1024/EasyLPAC/releases/tag/0.7.7.2">the latest Github release</a>, with the version of <code>lpac</code> that I had already installed.</p>
<p>I did not install it; I just ran it, and it worked.</p>
<p>I tested, on a different machine, the version of EasyLPAC which comes with <code>lpac</code> included, but I could not get this to work. I did not pursue this.</p>
<p>I was able to download an eSIM using EasyLPAC in a few different ways:</p>
<ul>
<li>having an LPA:1 activation code on the system clipboard</li>
<li>having a QR code image on the system clipboard</li>
<li>“scanning an image file”, which consisted of opening a QR code saved as an image file on my computer</li>
</ul>
<p>It was really quite straightforward.</p>
<p>I noted that the instructions say:</p>
<blockquote>
<p>Note: Reading LPA activation code and QRCode from clipboard not working in Wayland</p>
</blockquote>
<p>But they did work for me, using Wayland.</p>
<p>Importing the LycaMobile eSIM, using the details extracted from the QR code, worked.</p>
<h2 id="connectivity-worked-fine-via-linux">Connectivity worked fine via Linux</h2>
<p>I put the 9eSIM into my laptop’s SIM slot, and booted it.</p>
<p>In GNOME’s “Mobile Network” settings, it was recognised as a LycaMobile SIM.</p>
<p>I had to set the APN manually.</p>
<p>It took a couple of minutes for the eSIM to activate and, once activated, I was able to browse the Internet.</p>
<p>So there we go: an eSIM on a physical SIM on a laptop running Debian, with very little effort.</p>
<h2 id="other-options">Other options</h2>
<p>I quite like the look of <a href="https://shop.sysmocom.de/sysmoEUICC1-eUICC-for-consumer-eSIM-RSP/sysmoEUICC1">this Sysmocom SIM</a>.</p>
<p>I’d be interested in giving that a try some point.</p>



  <div>
	<hr>   
	<h2>You may also like:</h2>
	
    
</div>


</article>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Met Paul Graham Once (570 pts)]]></title>
            <link>http://okayfail.com/2025/i-met-pg-once.html</link>
            <guid>42767507</guid>
            <pubDate>Mon, 20 Jan 2025 11:20:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://okayfail.com/2025/i-met-pg-once.html">http://okayfail.com/2025/i-met-pg-once.html</a>, See on <a href="https://news.ycombinator.com/item?id=42767507">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <h5><a href="http://okayfail.com/2025/i-met-pg-once.html">January 16, 2025</a> | <a href="http://okayfail.com/tags/tech.html">#tech</a>, <a href="http://okayfail.com/tags/queer.html">#queer</a> </h5>
    <p data-sourcepos="1:1-1:346"><em>Nota bene: I’ve had a rough 2025 so far. I’m worried that people who used to support, or at least tolerate me, will turn against me out of a desire to conform, to show their obeisance, to the current prevailing winds. I found myself writing this essay to explain why I’ve been feeling so miserable. I sent it to Paul before I published it here.</em></p>
<p data-sourcepos="3:1-3:23">I met Paul Graham once.</p>
<p data-sourcepos="5:1-5:217">It was the summer of 2015, and we were attending Y Combinator, the premier finishing school for startup founders. It was a long and stressful summer, holed up in our apartment in Mountain View, and a great experience.</p>
<p data-sourcepos="7:1-7:44">We benefitted immensely from our time there.</p>
<p data-sourcepos="9:1-9:322">At Y Combinator, there was sort of a curriculum, but they didn’t teach you anything per se. You’re assigned mentors, who you meet weekly, and you are free to book office hours – thirty minutes at a time – with a rotating cast of partners, each and every one of them formidable people, near or at the top of the field.</p>
<p data-sourcepos="11:1-11:74">The mentors applied a neat and very effective trick: they believed in you.</p>
<p data-sourcepos="13:1-13:250">Out of thousands of applicants, you had been chosen, plucked from obscurity, and flown out, and now you were here, in the centre of the (software) universe. They had invested in you, you personally, above and beyond your specific idea for a business.</p>
<p data-sourcepos="15:1-15:182">Now every week you spoke with them, and presented your metrics, and discussed your problems, and, well, you could be doing better, couldn’t you? These results, they were kind of mid.</p>
<p data-sourcepos="17:1-17:299">They didn’t boss us around, or tell us what to do, but we didn’t want to disappoint teacher. Their approval meant a lot. Doors would open. Success was at our fingertips: the summer culminated in Demo Day, a cattle call where we would be presented to investors, and our future might change forever.</p>
<p data-sourcepos="19:1-19:214">We worked harder. We learned how to hustle. We swallowed rejection. We doubled our efforts, bent over our desks, working until we could barely see straight, trying to figure out how to <em>make something people want</em>.</p>
<hr data-sourcepos="21:1-22:0">
<p data-sourcepos="23:1-23:255">One day, we booked a meeting with pg (Paul Graham). He had just retired from the day to day running of the show, handed the reigns over to sama (Sam Altman), and now roamed the halls as an elder statesmen, a congenial and affable uncle, dispensing advice.</p>
<p data-sourcepos="25:1-25:443">We were excited. I don’t think he was ever a <em>hero</em> of mine, but he certainly had influence, made a big impression on me. I had read his essays eight or nine years earlier, during my undergraduate, hunched over a screen in my university’s computer lab, nodding along. I spent way too much time on hackernews. Lisp was cool, I was special for just taking an interest in startups and software, we could do anything if we worked hard enough.</p>
<p data-sourcepos="27:1-27:142">We explained our pitch to him: Appcanary monitors your apps and servers, and lets you know when you’re affected by a security vulnerability.</p>
<p data-sourcepos="29:1-29:278">He thought it was a decent enough idea, but the name, Appcanary, he wasn’t crazy about the name. He was very good at naming companies. He thought about it and told us that, really, we ought to be named Oracle, that would be a great name for us. Descriptive, simple, memorable.</p>
<p data-sourcepos="31:1-31:27">Shame it was taken, though.</p>
<p data-sourcepos="33:1-33:78">We nodded and thanked him, and laughed about it later. No one bats a thousand.</p>
<hr data-sourcepos="35:1-36:0">
<p data-sourcepos="37:1-37:41">YC had a huge positive impact on my life.</p>
<p data-sourcepos="39:1-39:174">People took us more seriously now. Before we were randos, misfits even, but now we were Princes of the Universe. We had rubbed shoulders with royalty, or at least sovereigns.</p>
<p data-sourcepos="41:1-41:87">Someone had believed in us, and pushed us until we understood how to make our own luck.</p>
<p data-sourcepos="43:1-43:360">We grew a lot that summer, but we didn’t raise that much money. A bit, enough to keep us going. I loved California, I liked San Francisco, but I didn’t feel comfortable trying to live there. I felt gross being part of a monied class in a city rapidly hollowing out. I’d had a precarious childhood and early adulthood, and I craved some kind of stability.</p>
<p data-sourcepos="45:1-45:276">Back home, we lived in the Best Neighbourhood in the Last Affordable Apartment In West Toronto. We knew it would reduce our chances for success, but it would double our runway. In the winter, I had started dating this most amazing girl, and I wanted to see where it would go.</p>
<p data-sourcepos="47:1-47:423">Two years later, we realized that everyone thought our product was useful but not <em>that</em> useful. We could get people to pay us hundreds of dollars per month, but not thousands. That’s the death knell of a vc-backed b2b saas sales model. High-touch sales to large companies is not worth it unless you can score thousands of dollars per month. We were too burnt out to pivot to another business idea, and we quietly folded.</p>
<p data-sourcepos="49:1-49:490">It was good timing: almost immediately after, GitHub announced they were going to provide our startup’s features <strong>for free</strong>. We gave them a call, we got them excited. They “acquihired” us as subject matter experts, and paid us a small fee for the intellectual property. We returned something like 40 or 45 cents on the dollar to our investors. Not a great result, but about par for the course. Most startups fail. Some of our Summer 2015 cohort flamed out faster, or more spectacularly.</p>
<p data-sourcepos="51:1-51:95">One of our investors commended us for our ethical behaviour – not everyone returns the money.</p>
<hr data-sourcepos="53:1-54:0">
<p data-sourcepos="55:1-55:137">It’s been almost ten years. After we got hired, Microsoft bought GitHub. I’m still with that amazing girl. We have two kids, a house.</p>
<p data-sourcepos="57:1-57:22">I’m transgender now.</p>
<p data-sourcepos="59:1-59:423">It turns out that I like women so much I’d like to be one of them. Or as close as I can get. I’m happier now, more joyful. I feel in touch with my body, and how I move through the world, in ways I didn’t before. Up until very recently, very few people would confuse me for a woman. It took me a while to remove my facial hair, I still haven’t trained my voice. That’s OK. What is a woman, exactly? I don’t know.</p>
<p data-sourcepos="61:1-61:258">I don’t want to make anyone uncomfortable. For that reason, I like to say that I am “non-binary trans femme”. It’s a mouthful, I know. But asking to be called “they” feels less burdensome, less of a polite fiction, than to ask to be called “she”.</p>
<p data-sourcepos="63:1-63:92">I just want to be treated with respect, and kindness. I don’t think I’m asking for much.</p>
<p data-sourcepos="65:1-65:77">Why is it wrong for me to have more joy in my life? I’m not hurting anyone.</p>
<hr data-sourcepos="67:1-68:0">
<p data-sourcepos="69:1-69:65">A few days ago, Paul Graham published an essay on “Wokeness”.</p>
<p data-sourcepos="71:1-71:436">I skimmed it. I couldn’t finish reading it, it made me too upset. It came a few days after Mark Zuckerberg announced he was going to increase the hate speech people like me receive. It’s not OK to imply someone has a mental disability – unless it’s because they’re queer. He also quietly removed some trivial accomodations he had made for his transgender employees.<sup><a href="http://okayfail.com/2025/i-met-pg-once.html#fn-accomodations" id="fnref-accomodations" data-footnote-ref="">1</a></sup>  <em>That</em> stung. That felt personal, targeted.</p>
<p data-sourcepos="75:1-75:93">I’ve been feeling quite anxious ever since. It feels like the world is crumbling around me.</p>
<p data-sourcepos="77:1-77:352">I’m still not sure what pg thinks “Wokeness” means.<sup><a href="http://okayfail.com/2025/i-met-pg-once.html#fn-performative" id="fnref-performative" data-footnote-ref="">2</a></sup> I know for a fact, that for most people – including many of the people he hangs out with – it just means “left-wing thing I dislike”. I got the impression that he thinks it’s bad, and that companies should purge people who are too woke. Maybe I’m being unfair to him.</p>
<p data-sourcepos="81:1-81:255">The irony is I too dislike nagging, hollow, corporate DEI exercises. In the abstract I was glad they existed<sup><a href="http://okayfail.com/2025/i-met-pg-once.html#fn-dei" id="fnref-dei" data-footnote-ref="">3</a></sup> but the insincerity was palpable. Are “identity politics” just a status game that economically advantaged elites play? I could be convinced.</p>
<hr data-sourcepos="85:1-86:0">
<p data-sourcepos="87:1-87:161">In the 2021 novel “Detransition Baby” the author shares a joke. Transgender women only have one of three jobs: computer programmer, aesthetician, prostitute.</p>
<p data-sourcepos="89:1-89:518">It’s an old joke. More of an observation, really. The director of “Vestidas de Azul”, <a href="https://thefilmstage.com/dressed-in-blue-review-a-long-lost-essential-document-of-spanish-and-lgbtqi-cinema-history-gets-new-life/">a 1983 documentary about trans women in Madrid</a>, had hoped to make a movie about trans women who were lawyers or held important roles in society. Instead he discovered that most trans women were <strong>forced</strong> to be artists, hair dressers, or sex workers – so that’s who he made his movie about.</p>
<p data-sourcepos="91:1-91:79">I’m glad I can be a computer programmer; no one gets to be an artist anymore.</p>
<p data-sourcepos="93:1-93:223">If you haven’t met many trans women, that might sound over-the-top, hyperbolic. But for so long, people like me were strongly discriminated against. Until very recently, we were treated as bywords for freaks, or psycopaths.</p>
<p data-sourcepos="95:1-95:239">In this vein, the other day I saw a scorching, sizzling hot take on Mastodon that read something like: the reason why conservative women are so mad about trans women is because they don’t want to share washrooms with the sex slave caste.</p>
<hr data-sourcepos="97:1-98:0">
<p data-sourcepos="99:1-99:255">The reason why pg’s essay made me so upset, made me feel so dispirited, is because I benefitted directly from his largesse, from a system he set up. His “school” took a chance on me, taught me how to hustle, how to become a Princess of the Universe.</p>
<p data-sourcepos="101:1-101:73">I’m immensely glad for the opportunity. Would I receive it again today?</p>
<p data-sourcepos="103:1-103:262">In many people’s imagination, the word “woke” invokes someone exactly like me. I’m the person who is annoying about their pronouns. I’m the person who feels more comfortable using gender neutral bathrooms.<sup><a href="http://okayfail.com/2025/i-met-pg-once.html#fn-bathrooms" id="fnref-bathrooms" data-footnote-ref="">4</a></sup> I have a passing interest in social justice.</p>
<p data-sourcepos="107:1-107:476">I’m concerned he, or rather, the people who succeeded him, would take one look at me, and decide that I am “too woke”, whatever that means. I worry that my existence, that my living in joy, in a state of grace, is “too woke” to be worth employing. I’m certain he wouldn’t be rude to my face, but he might quietly discriminate against me, say no thanks. He might not even think of it as discrimination, only that I don’t have what it takes. Whatever that means.</p>
<p data-sourcepos="109:1-109:70">I think this is why the current turn in the industry is so unsettling.</p>
<p data-sourcepos="111:1-111:635">It’s mean, and unkind. It’s malicious. Moves like Mark’s, and essays like pg’s, create the permission structure for people to discriminate against me. I’ve already been pushed out of Twitter; the <a href="https://www.theguardian.com/technology/article/2024/sep/05/racism-misogyny-lies-how-did-x-become-so-full-of-hatred-and-is-it-ethical-to-keep-using-it">hate speech</a> and <a href="https://www.salon.com/2024/05/24/twitter-considers-cisgender-a-slur-and-moderates-it-over-actual-slurs/">censorship</a> was too much to deal with. A lot of people feel that treating someone like me with respect is just a trendy moral fashion.<sup><a href="http://okayfail.com/2025/i-met-pg-once.html#fn-conventionally-minded" id="fnref-conventionally-minded" data-footnote-ref="">5</a></sup></p>
<p data-sourcepos="115:1-115:164">Will my next promotion be silently denied? Will a coworker try to disrespect me out of spite? Will I be shut out of big tech? Will anyone invest in my next startup?</p>
<p data-sourcepos="117:1-117:117">I’m better at my job than most. I’d be a better startup founder today than I was in 2015. None of that will matter.</p>
<p data-sourcepos="119:1-119:206">It feels as if people like pg, or at least people he hangs out with, who once upon a time believed in me, who lifted me up, recognized my talent, would now prefer that I be relegated to the sex slave caste.</p>
<p data-sourcepos="121:1-121:9">It hurts.</p>
<section data-footnotes="">
<ol>
<li id="fn-accomodations">
<p data-sourcepos="73:19-73:538">As reported by the <a href="https://www.nytimes.com/2025/01/10/technology/meta-mark-zuckerberg-trump.html">nytimes</a>, </p>
<blockquote>That same day at Meta’s offices in Silicon Valley, Texas and New York, facilities managers were instructed to remove tampons from men’s bathrooms, which the company had provided for nonbinary and transgender employees who use the men’s room and who may have required sanitary pads, two employees said.</blockquote> Why go out of your way to remove them? Do tampons drain your masculine energy? <a href="http://okayfail.com/2025/i-met-pg-once.html#fnref-accomodations" data-footnote-backref="" aria-label="Back to content">↩</a>
</li>
<li id="fn-performative">
<p data-sourcepos="79:18-79:404">He does provide a definition: an aggressively performative focus on social justice. Who decides what is a “performative” focus? That seems to be the question. All sorts of things are a “performance”, cf Judith Butler. Artifacts – our art, our technology, our material culture – express politics, cf Langdon Winner. Racism is bad, but you musn’t be annoying about it? It’s incoherent. <a href="http://okayfail.com/2025/i-met-pg-once.html#fnref-performative" data-footnote-backref="" aria-label="Back to content">↩</a></p>
</li>
<li id="fn-dei">
<p data-sourcepos="83:9-83:78">A small minority of people really do need to be taught how to be kind. <a href="http://okayfail.com/2025/i-met-pg-once.html#fnref-dei" data-footnote-backref="" aria-label="Back to content">↩</a></p>
</li>
<li id="fn-bathrooms">
<p data-sourcepos="105:15-105:213">In bathrooms, sometimes men flinch when they see me, afraid that they walked in through the wrong door. In an airport, it can be charming, affirming even. In a bar with drunk people, it can be scary. <a href="http://okayfail.com/2025/i-met-pg-once.html#fnref-bathrooms" data-footnote-backref="" aria-label="Back to content">↩</a></p>
</li>
<li id="fn-conventionally-minded">
<p data-sourcepos="113:27-113:215">Some frame this as the “aggressively conventional-minded” shutting down free inquiry. I ask you, is there <em>anything</em> more “aggressively independent-minded” than being gender-non-conforming? <a href="http://okayfail.com/2025/i-met-pg-once.html#fnref-conventionally-minded" data-footnote-backref="" aria-label="Back to content">↩</a></p>
</li>
</ol>
</section>
    <p><a href="http://okayfail.com/2025/i-met-pg-once.html"># 2025-01-16</a></p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zork: The Great Inner Workings (2020) (133 pts)]]></title>
            <link>https://medium.com/swlh/zork-the-great-inner-workings-b68012952bdc</link>
            <guid>42767132</guid>
            <pubDate>Mon, 20 Jan 2025 10:23:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/swlh/zork-the-great-inner-workings-b68012952bdc">https://medium.com/swlh/zork-the-great-inner-workings-b68012952bdc</a>, See on <a href="https://news.ycombinator.com/item?id=42767132">Hacker News</a></p>
Couldn't get https://medium.com/swlh/zork-the-great-inner-workings-b68012952bdc: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[TypeScript enums: use cases and alternatives (137 pts)]]></title>
            <link>https://2ality.com/2025/01/typescript-enum-patterns.html</link>
            <guid>42766729</guid>
            <pubDate>Mon, 20 Jan 2025 09:30:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://2ality.com/2025/01/typescript-enum-patterns.html">https://2ality.com/2025/01/typescript-enum-patterns.html</a>, See on <a href="https://news.ycombinator.com/item?id=42766729">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In this blog post, we take a closer look at TypeScript enums:</p>
<ul>
<li>How do they work?</li>
<li>What are their use cases?</li>
<li>What are the alternatives if we don’t want to use them?</li>
</ul>
<p>The blog post concludes with recommendations for what to use when.</p>
<!--more-->
<nav>
<ul>
  <li>
    <a href="#notation">Notation</a>
  </li>
  <li>
    <a href="#the-basics-of-typescript-enums">The basics of TypeScript enums</a>
    <ul>
      <li>
        <a href="#an-enum-defines-an-object">An enum defines an object</a>
      </li>
      <li>
        <a href="#an-enum-defines-a-type">An enum defines a type</a>
      </li>
      <li>
        <a href="#we-can-check-exhaustiveness-for-enums">We can check exhaustiveness for enums</a>
      </li>
      <li>
        <a href="#enumerating-members">Enumerating members</a>
      </li>
      <li>
        <a href="#enums-without-explicitly-specified-values">Enums without explicitly specified values</a>
      </li>
    </ul>
  </li>
  <li>
    <a href="#use-cases-for-enums">Use cases for enums</a>
  </li>
  <li>
    <a href="#use-case%3A-namespace-for-constants-with-primitive-values">Use case: namespace for constants with primitive values</a>
    <ul>
      <li>
        <a href="#enum-as-namespace-for-constants-with-primitive-values">Enum as namespace for constants with primitive values</a>
      </li>
      <li>
        <a href="#alternative-to-enum%3A-object-literal">Alternative to enum: object literal</a>
      </li>
    </ul>
  </li>
  <li>
    <a href="#use-case%3A-custom-type-with-unique-values">Use case: custom type with unique values</a>
    <ul>
      <li>
        <a href="#enum-as-custom-type-with-unique-values">Enum as custom type with unique values</a>
      </li>
      <li>
        <a href="#alternative-to-enum%3A-object-literal-1">Alternative to enum: object literal</a>
      </li>
      <li>
        <a href="#alternative-to-enum%3A-union-of-string-literal-types">Alternative to enum: union of string literal types</a>
      </li>
    </ul>
  </li>
  <li>
    <a href="#use-case%3A-namespace-for-constants-with-object-values">Use case: namespace for constants with object values</a>
    <ul>
      <li>
        <a href="#object-literal-whose-property-values-are-objects">Object literal whose property values are objects</a>
      </li>
      <li>
        <a href="#enum-class">Enum class</a>
      </li>
    </ul>
  </li>
  <li>
    <a href="#mapping-to-and-from-an-enum">Mapping to and from an enum</a>
  </li>
  <li>
    <a href="#recommendations">Recommendations</a>
  </li>
</ul>
</nav>
<h2 id="notation" tabindex="-1">Notation&nbsp;&nbsp;</h2>
<p>For showing inferred types in the source code, I use the npm package <a href="https://github.com/TypeStrong/ts-expect"><code>ts-expect</code></a> – e.g.:</p>
<pre><code><span>// Types of values</span>
expectType&lt;<span>string</span>&gt;(<span>'abc'</span>);
expectType&lt;<span>number</span>&gt;(<span>123</span>);

<span>// Equality of types</span>
<span>type</span> <span>Pair</span>&lt;T&gt; = [T, T];
expectType&lt;<span>TypeEqual</span>&lt;<span>Pair</span>&lt;<span>string</span>&gt;, [<span>string</span>,<span>string</span>]&gt;&gt;(<span>true</span>);
</code></pre>
<h2 id="the-basics-of-typescript-enums" tabindex="-1">The basics of TypeScript enums&nbsp;&nbsp;</h2>
<p>There are many different kinds of <em>enums</em> in various programming languages. In TypeScript, an enum defines two things:</p>
<ul>
<li>an object that maps member keys to member values</li>
<li>a type that only contains the member values</li>
</ul>
<p>Note that we are ignoring <a href="https://exploringjs.com/tackling-ts/ch_enums.html#const-enums">const enums</a> in this blog post.</p>
<p>Next, we’ll look at various aspects of enums in more detail.</p>
<h3 id="an-enum-defines-an-object" tabindex="-1">An enum defines an object&nbsp;&nbsp;</h3>
<p>On one hand, an enum is an object that maps member keys to member values. In that way, it works much like an object literal:</p>
<pre><code><span>enum</span> <span>Color</span> {
  <span>Red</span> = <span>0</span>,
  <span>Green</span> = <span>'GREEN'</span>,
}
assert.<span>equal</span>(<span>Color</span>.<span>Red</span>, <span>0</span>);
assert.<span>equal</span>(<span>Color</span>.<span>Green</span>, <span>'GREEN'</span>);
assert.<span>equal</span>(<span>Color</span>[<span>'Green'</span>], <span>'GREEN'</span>);
</code></pre>
<p>One limitation is that only numbers and strings are allowed as member values.</p>
<h3 id="an-enum-defines-a-type" tabindex="-1">An enum defines a type&nbsp;&nbsp;</h3>
<p>On the other hand, an enum is also a type that only contains the member values:</p>
<pre><code><span>let</span> <span>color</span>: <span>Color</span>;
color = <span>Color</span>.<span>Red</span>;
color = <span>Color</span>.<span>Green</span>;
<span>// @ts-expect-error: Type 'true' is not assignable to type 'Color'.</span>
color = <span>true</span>;
</code></pre>
<p>There is one important difference between string members and number members: We cannot assign plain strings to <code>color</code>:</p>
<pre><code><span>// @ts-expect-error: Type '"GREEN"' is not assignable to</span>
<span>// type 'Color'.</span>
color = <span>'GREEN'</span>;
</code></pre>
<p>But we can assign a plain number to <code>color</code> – if it is the value of a member:</p>
<pre><code>color = <span>0</span>;
<span>// @ts-expect-error: Type '123' is not assignable to type 'Color'.</span>
color = <span>123</span>;
</code></pre>
<h3 id="we-can-check-exhaustiveness-for-enums" tabindex="-1">We can check exhaustiveness for enums&nbsp;&nbsp;</h3>
<p>Consider the following enum:</p>
<pre><code><span>enum</span> <span>Color</span> {
  <span>Red</span> = <span>0</span>,
  <span>Green</span> = <span>'GREEN'</span>,
}
</code></pre>
<p>If we handle the values that a variable of type <code>Color</code> may have, then TypeScript can warn us if we forget one of them. In other words: If we didn’t handle all cases “exhaustively”. That is called an <em>exhaustiveness check</em>. To see how that works, let’s start with the following code:</p>
<pre><code><span>// @ts-expect-error: Not all code paths return a value.</span>
<span>function</span> <span>colorToString</span>(<span>color: Color</span>) {
  expectType&lt;<span>Color</span>&gt;(color); <span>// (A)</span>
  <span>if</span> (color === <span>Color</span>.<span>Red</span>) {
    <span>return</span> <span>'red'</span>;
  }
  expectType&lt;<span>Color</span>.<span>Green</span>&gt;(color); <span>// (B)</span>
  <span>if</span> (color === <span>Color</span>.<span>Green</span>) {
    <span>return</span> <span>'green'</span>;
  }
  expectType&lt;<span>never</span>&gt;(color); <span>// (C)</span>
}
</code></pre>
<p>In line A, <code>color</code> can still have any value. In line B, we have crossed off <code>Color.Red</code> and <code>color</code> can only have the value <code>Color.Green</code>. In line C, <code>color</code> can’t have any value – which explains its type <code>never</code>.</p>
<p>If <code>color</code> is <em>not</em> <code>never</code> in line C then we have forgotten a member. We can let TypeScript report an error at compile time like this:</p>
<pre><code><span>function</span> <span>colorToString</span>(<span>color: Color</span>) {
  <span>if</span> (color === <span>Color</span>.<span>Red</span>) {
    <span>return</span> <span>'red'</span>;
  }
  <span>if</span> (color === <span>Color</span>.<span>Green</span>) {
    <span>return</span> <span>'green'</span>;
  }
  <span>throw</span> <span>new</span> <span>UnsupportedValueError</span>(color);
}
</code></pre>
<p>How does that work? The <code>value</code> we pass to <code>UnsupportedValueError</code> must have the type <code>never</code>:</p>
<pre><code><span>class</span> <span>UnsupportedValueError</span> <span>extends</span> <span>Error</span> {
  <span>constructor</span>(<span>value: <span>never</span>, message = <span>`Unsupported value: <span>${value}</span>`</span></span>) {
    <span>super</span>(message)
  }
}
</code></pre>
<p>This is what happens if we forget the second case:</p>
<pre><code><span>function</span> <span>colorToString</span>(<span>color: Color</span>) {
  <span>if</span> (color === <span>Color</span>.<span>Red</span>) {
    <span>return</span> <span>'red'</span>;
  }
  <span>// @ts-expect-error: Argument of type 'Color.Green'</span>
  <span>// is not assignable to parameter of type 'never'.</span>
  <span>throw</span> <span>new</span> <span>UnsupportedValueError</span>(color);
}
</code></pre>
<p>Exhaustiveness checking works just as well with <code>case</code> statements:</p>
<pre><code><span>function</span> <span>colorToString</span>(<span>color: Color</span>) {
  <span>switch</span> (color) {
    <span>case</span> <span>Color</span>.<span>Red</span>:
      <span>return</span> <span>'red'</span>;
    <span>case</span> <span>Color</span>.<span>Green</span>:
      <span>return</span> <span>'green'</span>;
    <span>default</span>:
      <span>throw</span> <span>new</span> <span>UnsupportedValueError</span>(color);
  }
}
</code></pre>
<p>Another way to check exhaustiveness is by specifying a return type for the function:</p>
<pre><code><span>// @ts-expect-error: Function lacks ending return statement and</span>
<span>// return type does not include 'undefined'.</span>
<span>function</span> <span>colorToString</span>(<span>color: Color</span>): <span>string</span> {
  <span>switch</span> (color) {
    <span>case</span> <span>Color</span>.<span>Red</span>:
      <span>return</span> <span>'red'</span>;
  }
}
</code></pre>
<p>In my code, I usually do that but additionally throw an <code>UnsupportedValueError</code> because I like having a check that also works at runtime.</p>
<h3 id="enumerating-members" tabindex="-1">Enumerating members&nbsp;&nbsp;</h3>
<p>One operation that is occasionally useful is enumerating the members of an enum. Can we do that with TypeScript enums? Let’s use our previous enum:</p>
<pre><code><span>enum</span> <span>Color</span> {
  <span>Red</span> = <span>0</span>,
  <span>Green</span> = <span>'GREEN'</span>,
}
</code></pre>
<p>Compiled to JavaScript, <code>Color</code> looks like this:</p>
<pre><code><span>var</span> <span>Color</span>;
(<span>function</span> (<span>Color</span>) {
    <span>Color</span>[<span>Color</span>[<span>"Red"</span>] = <span>0</span>] = <span>"Red"</span>; <span>// (A)</span>
    <span>Color</span>[<span>"Green"</span>] = <span>"GREEN"</span>; <span>// (B)</span>
})(<span>Color</span> || (<span>Color</span> = {}));
</code></pre>
<p>This is a function that is immediately invoked and adds properties to an object <code>Color</code>.</p>
<p>The code for the string member <code>Green</code> in line B is straightforward: It maps from key to value.</p>
<p>The code for the number member <code>Red</code> in line A adds two properties for <code>Red</code> instead of one – a mapping from key to value and a mapping from value to key:</p>
<pre><code><span>Color</span>[<span>"Red"</span>] = <span>0</span>;
<span>Color</span>[<span>0</span>] = <span>"Red"</span>;
</code></pre>
<p>Note that the zero in the second line is coerced to a string (a property key can only be a string or a symbol). Thus, we can’t even really look up a number this way. We have to convert it to a string first.</p>
<p>Therefore, the number member prevents us from enumerating the keys or values of this enum:</p>
<pre><code>assert.<span>deepEqual</span>(
  <span>Object</span>.<span>keys</span>(<span>Color</span>), [<span>'0'</span>, <span>'Red'</span>, <span>'Green'</span>]
);
assert.<span>deepEqual</span>(
  <span>Object</span>.<span>values</span>(<span>Color</span>), [<span>'Red'</span>, <span>0</span>, <span>'GREEN'</span>]
);
</code></pre>
<p>If we switch to only string members then enumeration works:</p>
<pre><code><span>enum</span> <span>Color</span> {
  <span>Red</span> = <span>'RED'</span>,
  <span>Green</span> = <span>'GREEN'</span>,
}

assert.<span>deepEqual</span>(
  <span>Object</span>.<span>keys</span>(<span>Color</span>), [<span>'Red'</span>, <span>'Green'</span>]
);
assert.<span>deepEqual</span>(
  <span>Object</span>.<span>values</span>(<span>Color</span>), [<span>'RED'</span>, <span>'GREEN'</span>]
);
</code></pre>
<h3 id="enums-without-explicitly-specified-values" tabindex="-1">Enums without explicitly specified values&nbsp;&nbsp;</h3>
<p>We can also create enums without explicitly specifying member values. Then TypeScript specifies them for us and uses numbers:</p>
<pre><code><span>enum</span> <span>Color</span> {
  <span>Red</span>, <span>// implicitly = 0</span>
  <span>Green</span>, <span>// implicitly = 1</span>
}
assert.<span>equal</span>(<span>Color</span>.<span>Red</span>, <span>0</span>);
assert.<span>equal</span>(<span>Color</span>.<span>Green</span>, <span>1</span>);
</code></pre>
<h2 id="use-cases-for-enums" tabindex="-1">Use cases for enums&nbsp;&nbsp;</h2>
<p>Making sense of enums and enum-related patterns can quickly get confusing because what an enum is varies widely between programming languages – e.g.:</p>
<ul>
<li>Java’s enums are classes with a fixed set of instances.</li>
<li>Rust’s enums are more like algebraic datatypes in functional programming languages. They are loosely related to <a href="https://exploringjs.com/tackling-ts/ch_enum-alternatives.html#discriminated-union">discriminated unions</a> in TypeScript.</li>
</ul>
<p>Therefore, we benefit from a narrow definition of the term <em>enum</em>:</p>
<ul>
<li>A fixed set of values.</li>
<li>That can be accessed via the keys of an object.</li>
</ul>
<p>In the following sections, we’ll go through the following use cases for this kind of enum:</p>
<ol>
<li>Namespace for constants with primitive values</li>
<li>Custom type with unique values</li>
<li>Namespace for constants with object values</li>
</ol>
<p>We’ll consider how well TypeScript enums work for these use cases. (Spoiler: they work reasonably well for #1 and #2 but can’t be used for #3.) And we’ll look at enum-like patterns that we can use instead.</p>
<p>For each option, we’ll also examine:</p>
<ul>
<li>Can exhaustiveness checks be performed?</li>
<li>Can members be enumerated?</li>
</ul>
<h2 id="use-case%3A-namespace-for-constants-with-primitive-values" tabindex="-1">Use case: namespace for constants with primitive values&nbsp;&nbsp;</h2>
<p>One way in which enums (or enum-like objects) are sometimes used is simply as a namespace for constants – e.g., the Node.js function <a href="https://nodejs.org/api/fs.html#fsaccesspath-mode-callback"><code>fs.access()</code></a> has a parameter <code>mode</code> whose values are provided via an object that is similar to the following enum:</p>
<pre><code><span>enum</span> constants {
  <span>F_OK</span> = <span>0</span>,
  <span>R_OK</span> = <span>4</span>,
  <span>W_OK</span> = <span>2</span>,
  <span>X_OK</span> = <span>1</span>,
  <span>// ...</span>
}
</code></pre>
<p>Except for the first value, these are bits that are combined via bitwise Or:</p>
<pre><code><span>const</span> readOrWrite = constants.<span>R_OK</span> | constants.<span>W_OK</span>;
</code></pre>
<h3 id="enum-as-namespace-for-constants-with-primitive-values" tabindex="-1">Enum as namespace for constants with primitive values&nbsp;&nbsp;</h3>
<p>Which enum features are relevant for this use case?</p>
<ul>
<li>One major limitation of enums is that values can only be number or strings.</li>
<li>The enum as a type doesn’t matter: The type of parameter <code>mode</code> is <code>number</code>, not <code>constants</code> or something similar. That’s because the values of <code>constants</code> are not an exhaustive list of all possible values of <code>mode</code>.</li>
<li>For the same reason, exhaustiveness checks are not relevant in this case.</li>
<li>Enumerating members isn’t desirable either. And it’s not something that we can do with number-valued enums anyway.</li>
</ul>
<h3 id="alternative-to-enum%3A-object-literal" tabindex="-1">Alternative to enum: object literal&nbsp;&nbsp;</h3>
<p>For this use case, an object literal is a very good alternative:</p>
<pre><code><span>const</span> constants = {
  <span>__proto__</span>: <span>null</span>,
  <span>F_OK</span>: <span>0</span>,
  <span>R_OK</span>: <span>4</span>,
  <span>W_OK</span>: <span>2</span>,
  <span>X_OK</span>: <span>1</span>,
};
</code></pre>
<p>We use the pseudo property key <code>__proto__</code> to set the prototype of <code>constants</code> to <code>null</code>. That is a good practice because then we don’t have to deal with inherited properties:</p>
<ul>
<li>The main benefit is that we can use the <code>in</code> operator to check if <code>constants</code> has a given key without worrying about properties inherited from <code>Object.prototype</code> such as <code>.toString</code>.</li>
<li>However, <code>Object.keys()</code> and <code>Object.values()</code> ignore inherited properties anyway, so we don’t gain anything there.</li>
</ul>
<p>Note that <code>__proto__</code> also exists as a getter and a setter in <code>Object.prototype</code>. This feature is deprecated in favor of <code>Object.getPrototypeOf()</code> and <code>Object.setPrototypeOf()</code>. However, that is different from using this name in an object literal – which is not deprecated.</p>
<p>For more information, check out these sections of “Exploring JavaScript”:</p>
<ul>
<li><a href="https://exploringjs.com/js/book/ch_objects.html#the-pitfalls-of-using-an-object-as-a-dictionary">“The pitfalls of using an object as a dictionary”</a></li>
<li><a href="https://exploringjs.com/js/book/ch_objects.html#tips-for-working-with-prototypes">“Tips for working with prototypes”</a></li>
<li><a href="https://exploringjs.com/js/book/ch_classes.html#Object.prototype.__proto__">“<code>Object.prototype.__proto__</code> (accessor)”</a></li>
</ul>
<h2 id="use-case%3A-custom-type-with-unique-values" tabindex="-1">Use case: custom type with unique values&nbsp;&nbsp;</h2>
<p>Sometimes we may want to define our own custom type that has a fixed set of values. For example, booleans don’t always express intentions well. Then an enum can do a better job:</p>
<pre><code><span>enum</span> <span>Activation</span> {
  <span>Active</span> = <span>'Active'</span>,
  <span>Inactive</span> = <span>'Inactive'</span>,
}
</code></pre>
<p>It’s a good practice to explicitly specify string values via <code>=</code>:</p>
<ul>
<li>We get more type safety and can’t accidentally provide numbers where an <code>Activation</code> is expected.</li>
<li>We can enumerate the keys and values of <code>Activation</code>.</li>
</ul>
<h3 id="enum-as-custom-type-with-unique-values" tabindex="-1">Enum as custom type with unique values&nbsp;&nbsp;</h3>
<p>Which enum features are relevant for this use case?</p>
<ul>
<li>We will use the type defined by <code>Activation</code>.</li>
<li>Exhaustiveness checks are possible and useful.</li>
<li>We also may want to enumerate keys or values.</li>
</ul>
<h3 id="alternative-to-enum%3A-object-literal-1" tabindex="-1">Alternative to enum: object literal&nbsp;&nbsp;</h3>
<p>Let’s use an object literal to define the value part of an enum (we’ll get to the type part next):</p>
<pre><code><span>const</span> <span>Activation</span> = {
  <span>__proto__</span>: <span>null</span>,
  <span>Active</span>: <span>'Active'</span>,
  <span>Inactive</span>: <span>'Inactive'</span>,
} <span>as</span> <span>const</span>; <span>// (A)</span>

<span>// Without `as const`, this type would be `string`:</span>
expectType&lt;<span>'Active'</span>&gt;(<span>Activation</span>.<span>Active</span>);

<span>type</span> <span>ActivationType</span> = <span>PropertyValues</span>&lt;<span>typeof</span> <span>Activation</span>&gt;;
expectType&lt;
  <span>TypeEqual</span>&lt;<span>ActivationType</span>, <span>'Active'</span> | <span>'Inactive'</span>&gt;
&gt;(<span>true</span>);
</code></pre>
<p>The <code>as const</code> in line A enables us to derive <code>ActivationType</code> from <code>Activation</code> via the helper type <code>PropertyValues</code> (which is defined below).</p>
<p>Why is this type called <code>ActivationType</code> and not <code>Activation</code>? Since the namespaces of values and types are separate in TypeScript, we could indeed use the same name. However, I’ve had issues when using Visual Studio Code to rename value and type: It got confused because importing <code>Activation</code> imported both value and type. That’s why I’m using different names – for now.</p>
<p>The helper type <code>PropertyValues</code> looks like this:</p>
<pre><code><span>type</span> <span>PropertyValues</span>&lt;<span>Obj</span>&gt; = <span>Obj</span>[<span>Exclude</span>&lt;keyof <span>Obj</span>, <span>'__proto__'</span>&gt;];
</code></pre>
<ul>
<li>The type <code>Obj[K]</code> contains the values of all properties whose keys are in <code>K</code>.</li>
<li>We exclude the key <code>'__proto__'</code> from <code>keyof Obj</code> because TypeScript treats that key as a normal property and that’s not what we want (<a href="https://github.com/microsoft/TypeScript/issues/38385">related GitHub issue</a>).</li>
</ul>
<p>Let’s explore what the derived type looks like if we don’t use <code>as const</code>:</p>
<pre><code><span>const</span> <span>Activation</span> = {
  <span>__proto__</span>: <span>null</span>,
  <span>Active</span>: <span>'Active'</span>,
  <span>Inactive</span>: <span>'Inactive'</span>,
};

expectType&lt;<span>string</span>&gt;(<span>Activation</span>.<span>Active</span>);
expectType&lt;<span>string</span>&gt;(<span>Activation</span>.<span>Inactive</span>);

<span>type</span> <span>ActivationType</span> = <span>PropertyValues</span>&lt;<span>typeof</span> <span>Activation</span>&gt;;
expectType&lt;
  <span>TypeEqual</span>&lt;<span>ActivationType</span>, <span>string</span>&gt;
&gt;(<span>true</span>);
</code></pre>
<h4 id="exhaustiveness-checks" tabindex="-1">Exhaustiveness checks&nbsp;&nbsp;</h4>
<p>TypeScript supports exhaustiveness checks for unions of literal types. And that’s what <code>ActivationType</code> is. Therefore, we can use the same pattern as we did with enums:</p>
<pre><code><span>function</span> <span>activationToString</span>(<span>activation: ActivationType</span>): <span>string</span> {
  <span>switch</span> (activation) {
    <span>case</span> <span>Activation</span>.<span>Active</span>:
      <span>return</span> <span>'ACTIVE'</span>;
    <span>case</span> <span>Activation</span>.<span>Inactive</span>:
      <span>return</span> <span>'INACTIVE'</span>;
    <span>default</span>:
      <span>throw</span> <span>new</span> <span>UnsupportedValueError</span>(activation);
  }
}
</code></pre>
<h4 id="enumerating-members-1" tabindex="-1">Enumerating members&nbsp;&nbsp;</h4>
<p>We can use <code>Object.keys()</code> and <code>Object.values()</code> to enumerate the members of the object <code>Activation</code>:</p>
<pre><code><span>for</span> (<span>const</span> value <span>of</span> <span>Object</span>.<span>values</span>(<span>Activation</span>)) {
  <span>console</span>.<span>log</span>(value);
}
</code></pre>
<p>Output:</p>
<pre><code>Active
Inactive
</code></pre>
<h4 id="using-symbols-as-property-values" tabindex="-1">Using symbols as property values&nbsp;&nbsp;</h4>
<p>One downside of using strings as property values is that <code>ActivationType</code> does not exclude arbitrary strings from being used. We can get more type safety if we use symbols:</p>
<pre><code><span>const</span> <span>Active</span> = <span>Symbol</span>(<span>'Active'</span>);
<span>const</span> <span>Inactive</span> = <span>Symbol</span>(<span>'Inactive'</span>);

<span>const</span> <span>Activation</span> = {
  <span>__proto__</span>: <span>null</span>,
  <span>Active</span>,
  <span>Inactive</span>,
} <span>as</span> <span>const</span>;

expectType&lt;<span>typeof</span> <span>Active</span>&gt;(<span>Activation</span>.<span>Active</span>);

<span>type</span> <span>ActivationType</span> = <span>PropertyValues</span>&lt;<span>typeof</span> <span>Activation</span>&gt;;
expectType&lt;
  <span>TypeEqual</span>&lt;
    <span>ActivationType</span>, <span>typeof</span> <span>Active</span> | <span>typeof</span> <span>Inactive</span>
  &gt;
&gt;(<span>true</span>);
</code></pre>
<p>This seems overly complicated: Why the intermediate step of first declaring variables for the symbols before we use them? Why not create the symbols inside the object literal? Alas, that’s a current limitation of <code>as const</code> for symbols: They are not recognized as unique (<a href="https://github.com/microsoft/TypeScript/issues/54100">related GitHub issue</a>):</p>
<pre><code><span>const</span> <span>Activation</span> = {
  <span>__proto__</span>: <span>null</span>,
  <span>Active</span>: <span>Symbol</span>(<span>'Active'</span>),
  <span>Inactive</span>: <span>Symbol</span>(<span>'Inactive'</span>),
} <span>as</span> <span>const</span>;

<span>// Alas, the type of Activation.Active is not `typeof Active`</span>
expectType&lt;<span>symbol</span>&gt;(<span>Activation</span>.<span>Active</span>);

<span>type</span> <span>ActivationType</span> = <span>PropertyValues</span>&lt;<span>typeof</span> <span>Activation</span>&gt;;
expectType&lt;
  <span>TypeEqual</span>&lt;<span>ActivationType</span>, <span>symbol</span>&gt;
&gt;(<span>true</span>);
</code></pre>
<h3 id="alternative-to-enum%3A-union-of-string-literal-types" tabindex="-1">Alternative to enum: union of string literal types&nbsp;&nbsp;</h3>
<p>A union of string literal types is an interesting alternative to an enum when it comes to defining a type with a fixed set of members:</p>
<pre><code><span>type</span> <span>Activation</span> = <span>'Active'</span> | <span>'Inactive'</span>;
</code></pre>
<p>How does such a type compare to an enum?</p>
<p>Pros:</p>
<ul>
<li>It’s a quick and simple solution.</li>
<li>It supports exhaustiveness checks.</li>
<li>Renaming members works reasonably well in Visual Studio Code.</li>
</ul>
<p>Cons:</p>
<ul>
<li>The type members are not unique. We could change that by using symbols but then we’d lose some of the convenience of string literal union types – e.g., we’d have to import the values.</li>
<li>We can’t enumerate the members. The next section explains how to change that.</li>
</ul>
<h4 id="reifying-string-literal-unions" tabindex="-1">Reifying string literal unions via Sets&nbsp;&nbsp;</h4>
<p><em>Reification</em> means creating an entity at the object level (think JavaScript values) for an entity that exists at the meta level (think TypeScript types).</p>
<p>We can use a Set to reify a string literal union type:</p>
<pre><code><span>const</span> activation = <span>new</span> <span>Set</span>([
  <span>'Active'</span>,
  <span>'Inactive'</span>,
] <span>as</span> <span>const</span>);
expectType&lt;<span>Set</span>&lt;<span>'Active'</span> | <span>'Inactive'</span>&gt;&gt;(activation);

<span>// @ts-expect-error: Argument of type '"abc"' is not assignable to</span>
<span>// parameter of type '"Active" | "Inactive"'.</span>
activation.<span>has</span>(<span>'abc'</span>);
  <span>// Auto-completion works for arguments of .has(), .delete() etc.</span>

<span>// Let’s turn the Set into a string literal union</span>
<span>type</span> <span>Activation</span> = <span>SetElementType</span>&lt;<span>typeof</span> activation&gt;;
expectType&lt;
  <span>TypeEqual</span>&lt;<span>Activation</span>, <span>'Active'</span> | <span>'Inactive'</span>&gt;
&gt;(<span>true</span>);

<span>type</span> <span>SetElementType</span>&lt;S <span>extends</span> <span>Set</span>&lt;<span>any</span>&gt;&gt; =
  S <span>extends</span> <span>Set</span>&lt;infer <span>Elem</span>&gt; ? <span>Elem</span> : <span>never</span>;
</code></pre>
<h2 id="use-case%3A-namespace-for-constants-with-object-values" tabindex="-1">Use case: namespace for constants with object values&nbsp;&nbsp;</h2>
<p>Sometimes, it’s useful to have an enum-like construct for looking up richer data – stored in objects. We can’t use objects as enum values, so we’ll have to use other solutions.</p>
<h3 id="object-literal-whose-property-values-are-objects" tabindex="-1">Object literal whose property values are objects&nbsp;&nbsp;</h3>
<p>This is an example of using an object literal as an enum for objects:</p>
<pre><code><span>// This type is optional: It constrains the property values</span>
<span>// of `TextStyle` but has no other use.</span>
<span>type</span> <span>TTextStyle</span> = {
  <span>key</span>: <span>string</span>,
  <span>html</span>: <span>string</span>,
  <span>latex</span>: <span>string</span>,
};
<span>const</span> <span>TextStyle</span> = {
  <span>Bold</span>: {
    <span>key</span>: <span>'Bold'</span>,
    <span>html</span>: <span>'b'</span>,
    <span>latex</span>: <span>'textbf'</span>,
  },
  <span>Italics</span>: {
    <span>key</span>: <span>'Italics'</span>,
    <span>html</span>: <span>'i'</span>,
    <span>latex</span>: <span>'textit'</span>,
  },
} <span>as</span> <span>const</span> satisfies <span>Record</span>&lt;<span>string</span>, <span>TTextStyle</span>&gt;;

<span>type</span> <span>TextStyleType</span> = <span>PropertyValues</span>&lt;<span>typeof</span> <span>TextStyle</span>&gt;;
<span>type</span> <span>PropertyValues</span>&lt;<span>Obj</span>&gt; = <span>Obj</span>[<span>Exclude</span>&lt;keyof <span>Obj</span>, <span>'__proto__'</span>&gt;];
</code></pre>
<h4 id="exhaustiveness-check" tabindex="-1">Exhaustiveness check&nbsp;&nbsp;</h4>
<p>Why do the property values of <code>TextStyle</code> have the property <code>.key</code>? That property lets us do exhaustiveness checks because the property values form a <a href="https://exploringjs.com/tackling-ts/ch_enum-alternatives.html#discriminated-union">discriminated union</a>.</p>
<pre><code><span>function</span> <span>f</span>(<span>textStyle: TextStyleType</span>): <span>string</span> {
  <span>switch</span> (textStyle.<span>key</span>) {
    <span>case</span> <span>TextStyle</span>.<span>Bold</span>.<span>key</span>:
      <span>return</span> <span>'BOLD'</span>;
    <span>case</span> <span>TextStyle</span>.<span>Italics</span>.<span>key</span>:
      <span>return</span> <span>'ITALICS'</span>;
    <span>default</span>:
      <span>throw</span> <span>new</span> <span>UnsupportedValueError</span>(textStyle); <span>// No `.key`!</span>
  }
}
</code></pre>
<p>For comparison, this is what <code>f()</code> would look like if <code>TextStyle</code> were an enum:</p>
<pre><code><span>enum</span> <span>TextStyle2</span> { <span>Bold</span>, <span>Italics</span> }
<span>function</span> <span>f2</span>(<span>textStyle: TextStyle2</span>): <span>string</span> {
  <span>switch</span> (textStyle) {
    <span>case</span> <span>TextStyle2</span>.<span>Bold</span>:
      <span>return</span> <span>'BOLD'</span>;
    <span>case</span> <span>TextStyle2</span>.<span>Italics</span>:
      <span>return</span> <span>'ITALICS'</span>;
    <span>default</span>:
      <span>throw</span> <span>new</span> <span>UnsupportedValueError</span>(textStyle);
  }
}
</code></pre>
<h3 id="enum-class" tabindex="-1">Enum class&nbsp;&nbsp;</h3>
<p>We can also use a class as an enum – a pattern that is borrowed from Java:</p>
<pre><code><span>class</span> <span>TextStyle</span> {
  <span>static</span> <span>Bold</span> = <span>new</span> <span>TextStyle</span>(<span>/*...*/</span>);
  <span>static</span> <span>Italics</span> = <span>new</span> <span>TextStyle</span>(<span>/*...*/</span>);
}
<span>type</span> <span>TextStyleKeys</span> = <span>EnumKeys</span>&lt;<span>typeof</span> <span>TextStyle</span>&gt;;
expectType&lt;
  <span>TypeEqual</span>&lt;<span>TextStyleKeys</span>, <span>'Bold'</span> | <span>'Italics'</span>&gt;
&gt;(<span>true</span>);

<span>type</span> <span>EnumKeys</span>&lt;T&gt; = <span>Exclude</span>&lt;keyof T, <span>'prototype'</span>&gt;;
</code></pre>
<p>One pro of this pattern is that we can use methods to add behavior to the enum values. A con is that there is no simple way to get an exhaustiveness check.</p>
<p><code>Object.keys()</code> and <code>Object.values()</code> ignore non-enumerable properties of <code>TextStyle</code> such as <code>.prototype</code> – which is why we can use them to enumerate keys and values – e.g.:</p>
<pre><code>assert.<span>deepEqual</span>(
  <span>// TextStyle.prototype is non-enumerable</span>
  <span>Object</span>.<span>keys</span>(<span>TextStyle</span>),
  [<span>'Bold'</span>, <span>'Italics'</span>]
);
</code></pre>
<h2 id="mapping-to-and-from-an-enum" tabindex="-1">Mapping to and from an enum&nbsp;&nbsp;</h2>
<p>Sometimes we want to translate enum values to other values or vice versa – e.g. when serializing them to JSON or deserializing them from JSON. If we do so via a <code>Map</code>, we can use TypeScript to get a warning if we forget an enum value.</p>
<p>To explore how that works, we’ll use the following enum pattern type:</p>
<pre><code><span>const</span> <span>Pending</span> = <span>Symbol</span>(<span>'Pending'</span>);
<span>const</span> <span>Ongoing</span> = <span>Symbol</span>(<span>'Ongoing'</span>);
<span>const</span> <span>Finished</span> = <span>Symbol</span>(<span>'Finished'</span>);
<span>const</span> <span>TaskStatus</span> = {
  <span>__proto__</span>: <span>null</span>,
  <span>Pending</span>,
  <span>Ongoing</span>,
  <span>Finished</span>,
} <span>as</span> <span>const</span>;
<span>type</span> <span>TaskStatusType</span> = <span>PropertyValues</span>&lt;<span>typeof</span> <span>TaskStatus</span>&gt;;
<span>type</span> <span>PropertyValues</span>&lt;<span>Obj</span>&gt; = <span>Obj</span>[<span>Exclude</span>&lt;keyof <span>Obj</span>, <span>'__proto__'</span>&gt;];
</code></pre>
<p>This is the Map:</p>
<pre><code><span>const</span> taskPairs = [
  [<span>TaskStatus</span>.<span>Pending</span>, <span>'not yet'</span>],
  [<span>TaskStatus</span>.<span>Ongoing</span>, <span>'working on it'</span>],
  [<span>TaskStatus</span>.<span>Finished</span>, <span>'finished'</span>],
] <span>as</span> <span>const</span>;

<span>type</span> <span>Key</span> = (<span>typeof</span> taskPairs)[<span>number</span>][<span>0</span>];
<span>const</span> taskMap = <span>new</span> <span>Map</span>&lt;<span>Key</span>, <span>string</span>&gt;(taskPairs);
</code></pre>
<p>If you are wondering why we didn’t directly use the value of <code>taskPairs</code> as the argument of <code>new Map()</code> and omit the type parameters: TypeScript doesn’t seem to be able to infer the type parameters if the keys are symbols and reports a compile-time error. With strings, the code would be simpler:</p>
<pre><code><span>const</span> taskPairs = [
  [<span>'Pending'</span>, <span>'not yet'</span>],
  [<span>'Ongoing'</span>, <span>'working on it'</span>],
  [<span>'Finished'</span>, <span>'finished'</span>],
] <span>as</span> <span>const</span>;
<span>const</span> taskMap = <span>new</span> <span>Map</span>(taskPairs); <span>// no type parameters!</span>
</code></pre>
<p>The final step is to check if we forgot only of the values of <code>TaskStatus</code>:</p>
<pre><code>expectType&lt;
  <span>TypeEqual</span>&lt;<span>MapKey</span>&lt;<span>typeof</span> taskMap&gt;, <span>TaskStatusType</span>&gt;
&gt;(<span>true</span>);
<span>type</span> <span>MapKey</span>&lt;M <span>extends</span> <span>Map</span>&lt;<span>any</span>, <span>any</span>&gt;&gt; =
  M <span>extends</span> <span>Map</span>&lt;infer K, <span>any</span>&gt; ? K : <span>never</span>;
</code></pre>
<h2 id="recommendations" tabindex="-1">Recommendations&nbsp;&nbsp;</h2>
<p>When should we use an enum and when an alternative pattern?</p>
<p><strong>TypeScript enums are not JavaScript:</strong> Enums are one of the few TypeScript language constructs (vs. type constructs) that have no corresponding JavaScript features. That can matter in two ways:</p>
<ul>
<li>The transpiled code looks a bit strange – especially if some enum members are numbers.</li>
<li>If a tool doesn’t transpile TypeScript but only strips types then it won’t support enums. That’s not (yet?) that common but one prominent example is <a href="https://2ality.com/2025/01/nodejs-strip-type.html">Node’s current built-in support for TypeScript</a>.</li>
</ul>
<p><strong>Performance of strings:</strong> One thing to keep in mind is that comparing strings is usually slower than comparing numbers or symbols. Therefore, enums or enum patterns where values are strings, are slower. Note that applies to string literal unions too. But such a performance cost only matters if we do many comparisons.</p>
<p><strong>What is the use case?</strong> Looking at the use cases can help us make a decision:</p>
<ol>
<li>Namespace for constants with primitive values:
<ul>
<li>If the primitive values are numbers or strings, we can use a TypeScript enum.
<ul>
<li>Alas, number values aren’t great because each member produces two properties: a mapping from key to value and a reverse mapping.</li>
</ul>
</li>
<li>Otherwise (or if we don’t want to use an enum) we can use an object literal.</li>
</ul>
</li>
<li>Custom type with unique values:
<ul>
<li>If we use an enum, it should have string values because that gives us more type safety and lets us iterate over keys and values.</li>
<li>A union of string literal types is a lightweight, quick solution. Its downsides are: less type safety and no namespace object for easy lookup.
<ul>
<li>If we want to access the string literal values at runtime, we can use a Set to reify them.</li>
</ul>
</li>
<li>If we want a solid, slightly verbose solution, we can use an object literal with symbol property values.</li>
</ul>
</li>
<li>Namespace for constants with object values:
<ul>
<li>We can’t use enums for this use case.</li>
<li>We can use an object literal whose property values are objects. The upside of this solution is that we can check exhaustiveness.</li>
<li>If we want enum values to have methods, we can use an enum class. However that means that we can’t check exhaustiveness.</li>
</ul>
</li>
</ol>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Parinfer: Simpler Lisp Editing (129 pts)]]></title>
            <link>https://shaunlebron.github.io/parinfer/</link>
            <guid>42766205</guid>
            <pubDate>Mon, 20 Jan 2025 08:21:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shaunlebron.github.io/parinfer/">https://shaunlebron.github.io/parinfer/</a>, See on <a href="https://news.ycombinator.com/item?id=42766205">Hacker News</a></p>
Couldn't get https://shaunlebron.github.io/parinfer/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[What does "supports DRM and may not be fully accessible" mean for SATA SDDs? (173 pts)]]></title>
            <link>https://unix.stackexchange.com/questions/789838/what-does-supports-drm-functions-and-may-not-be-fully-accessible-mean-for-sata</link>
            <guid>42765480</guid>
            <pubDate>Mon, 20 Jan 2025 06:09:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://unix.stackexchange.com/questions/789838/what-does-supports-drm-functions-and-may-not-be-fully-accessible-mean-for-sata">https://unix.stackexchange.com/questions/789838/what-does-supports-drm-functions-and-may-not-be-fully-accessible-mean-for-sata</a>, See on <a href="https://news.ycombinator.com/item?id=42765480">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<blockquote>
<p>supports DRM functions and may not be fully accessible</p>
</blockquote>
<p>There's an extension of the ATA protocol (the "language" spoken between your mainboard and your SATA SSDs), which allows for the storage device to reply differently, depending on whether a request was signed by the trusted platform module on the mainboard. That way, things like video player devices that only give access to the stored data when the software has proven itself to be unmodified can be implemented. (I can find much more interesting use cases, but digital rights management seems to be what the kernel authors saw as the application for this when they implemented that warning.) That also means that Linux might simply get an "incomplete" view of the SSD, hence it warns you about that, in case you wonder.</p>
<blockquote>
<p>read cache: enabled, doesn't support DPO or FUA</p>
</blockquote>
<p>uff, this just tells you that the device has a read cache: the computer fetches some smaller unit of data, and the device keeps that (and potentially more of its surroundings) around in a smaller, faster "transparent" piece of memory. That makes sense, because it's more common that data that was recently read is read again than some data that nobody cared about for a long time.</p>
<p>DPO and FUA are just really old techniques to tell SCSI (and consequently, ATAPI) devices to bypass that cache (FUA was supposed to force reading from the spinning platter without using what's in the cache, whereas DPO was meant to say, hey, this is probably a one-off read; don't care to start caching what I read here). Makes no sense for an SSD in how they were thought out in the early 1990s, so these extensions are not available for your SSD.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'll think twice before using GitHub Actions again (124 pts)]]></title>
            <link>https://ninkovic.dev/blog/2025/think-twice-before-using-github-actions</link>
            <guid>42764762</guid>
            <pubDate>Mon, 20 Jan 2025 03:41:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ninkovic.dev/blog/2025/think-twice-before-using-github-actions">https://ninkovic.dev/blog/2025/think-twice-before-using-github-actions</a>, See on <a href="https://news.ycombinator.com/item?id=42764762">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Before I rant about GitHub Actions, I'd like to set the context on where this dissatisfaction comes from. My team consists of about 15 engineers constantly pushing to the main branch. Our code lives in a monorepo split per module, which, through <a href="https://trunkbaseddevelopment.com/" rel="noopener noreferrer" target="_blank">trunk based development</a>, gets deployed multiple times a day.</p><p>I want to emphasize that your mileage may vary. There will be folks who say GitHub Actions are great (I also use them for smaller projects), but as with any tool, it has limits and might not be suitable for all problems. Let's look at some of them.</p><br><h2 id="pull-request-and-required-checks">Pull request and required checks</h2><p>Our code sits in a monorepo which is further divided into folders. Every folder is independent of each other and can be tested, built, and deployed separately.</p><pre><code>monorepo/
    ├─ api1/
    ├─ api2/
    ├─ web-app1/
    ├─ web-app2/</code></pre><p>Each of these folders has its pipeline. We leverage <a href="https://docs.github.com/en/actions/writing-workflows/workflow-syntax-for-github-actions#onpushpull_requestpull_request_targetpathspaths-ignore" rel="noopener noreferrer" target="_blank">GitHub Actions paths</a> to trigger a pipeline only when there is a code change within that folder.</p><p>A good practice is not to allow pull requests to be merged unless all checks are green. However trivial it might sound, this becomes notoriously difficult with the above monorepo setup.</p><p>In GitHub you can specify a "required check", the name of the step in your pipeline that always has to be green before a pull request is merged. As an example, I can say that <code>web-app1 - Unit tests</code> are required to pass. The problem is that this step will only run when I change something in the <code>web-app1</code> folder. So if my pull request only made changes in <code>api1</code> I will never be able to merge my pull request! 🤯</p><p>In these two GitHub threads <a href="https://github.com/orgs/community/discussions/44490" rel="noopener noreferrer" target="_blank">1</a>, <a href="https://github.com/orgs/community/discussions/13690" rel="noopener noreferrer" target="_blank">2</a> you can see the impact. The bottom line is, that working around this limitation is hacky, difficult to maintain, and costly since you have to run additional pipelines just to determine if a pull request can be merged or not!</p><p>GitHub should not rely on specific names for the required checks. They can just say - all checks have to pass before you merge. That way, whatever pipelines and checks your pull requests have triggered will be considered mandatory. It's been almost 3 years since these issues were raised, and nothing changed yet!</p><br><h2 id="reusability-and-yaml">Reusability and YAML</h2><p>My impression is that when your pipeline grows, it becomes more and more difficult to manage it with GitHub Actions. Here is an example workflow that can be called from other workflows, can be triggered manually, and triggers when someone pushes to the master branch.</p><pre><code>name: CD - api1

on:
  push:
    paths:
      - 'api1/**'
    branches:
      - master
  workflow_dispatch:
    inputs:
      target_environment:
        type: environment
        default: 'staging'
        required: true
  workflow_call:
    inputs:
      target_environment:
        type: string
        required: true

jobs:
  deploy-api1:
    environment:
      name: ${{ github.event_name == 'push' &amp;&amp; 'production' || inputs.target_environment }}

  smoke-tests:
    if: ${{ github.event_name == 'push' || inputs.target_environment == 'production' }}
    name: Smoke Tests
    uses: ./.github/workflows/smoke-tests.yml
    with:
      target_environment: 'production'
    secrets: inherit
</code>
        </pre><p>What I notice doing more and more is that I have to add lots of if statements such as this.</p><pre><code> if: ${{ github.event_name == 'push' || inputs.target_environment == 'production' }}</code></pre><p>While I could split this into multiple workflows with different triggers, then I'm getting more and more files to maintain. A workflow reuse should be a one-liner but I always have to write more lines and a lot of duplicated statements such as workflow name, <code>secrets: inherit</code> etc. Our <code>.github</code> folder already contains 30+ files.</p><p>Another pitfall that happens often is the <code>needs</code> clause. When you are refactoring and removing jobs, it's easy to forget to update this clause and subsequent steps. While there are linters available, they are not perfect. The sad part is that I can only see mistakes when I push the workflow, I'd expect to know about this way earlier.</p><pre><code>needs: [ build, deploy, generate-release-tag ]
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/create-git-tag
        with:
          release_tag: ${{ needs.generate-release-tag.outputs.release_tag }}   
</code></pre><br><h2 id="lack-of-local-development">Lack of local development</h2><p>It's a known thing that there is no way of running GitHub Actions locally. There is a tool called <a href="https://github.com/nektos/act" rel="noopener noreferrer" target="_blank">act</a> but in my experience it's subpar.</p><br><h2 id="github-doesnt-care">GitHub doesn't care</h2><p>Of all the pain points, this is the worst one. It seems that GitHub doesn't care about fixing any of these issues or improving its product. Some of the threads have been open for years without any action taken by GitHub. A lot of these issues have <a href="https://github.com/github/roadmap/discussions/1014" rel="noopener noreferrer" target="_blank">been recently closed by GitHub</a> causing a backlash from the community. There are no signs that these will be addressed based on their <a href="https://github.com/orgs/github/projects/4247/views/1" rel="noopener noreferrer" target="_blank">public roadmap</a>.</p><br><h2 id="options">Options</h2><p>Considering all the problems listed, with the lack of motivation from GitHub I'd think twice before using GitHub Actions again. The CI/CD product space offers a lot of options such as Gitlab, Jenkins, TeamCity, etc. I used some of these and at this moment, they offer a better service for the money. Things like <a href="https://dagger.io/" rel="noopener noreferrer" target="_blank">Dagger</a> are also worth evaluating since they bring new perspectives into the game.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reverse Engineering Bambu Connect (486 pts)]]></title>
            <link>https://wiki.rossmanngroup.com/wiki/Reverse_Engineering_Bambu_Connect</link>
            <guid>42764602</guid>
            <pubDate>Mon, 20 Jan 2025 03:08:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wiki.rossmanngroup.com/wiki/Reverse_Engineering_Bambu_Connect">https://wiki.rossmanngroup.com/wiki/Reverse_Engineering_Bambu_Connect</a>, See on <a href="https://news.ycombinator.com/item?id=42764602">Hacker News</a></p>
Couldn't get https://wiki.rossmanngroup.com/wiki/Reverse_Engineering_Bambu_Connect: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Is anyone making money selling traditional downloadable software? (170 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42764185</link>
            <guid>42764185</guid>
            <pubDate>Mon, 20 Jan 2025 01:56:11 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42764185">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=42764185: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[UK's hardware talent is being wasted (568 pts)]]></title>
            <link>https://josef.cn/blog/uk-talent</link>
            <guid>42763386</guid>
            <pubDate>Sun, 19 Jan 2025 23:52:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://josef.cn/blog/uk-talent">https://josef.cn/blog/uk-talent</a>, See on <a href="https://news.ycombinator.com/item?id=42763386">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p>Imperial, Oxford, and Cambridge produce world-class engineers. Yet post-graduation, their trajectory is an economic tragedy - and a hidden arbitrage opportunity.</p><p><strong>The Stark Reality:</strong></p><ul><li data-preset-tag="p"><p>Top London hardware engineer graduates: £30,000-£50,000</p></li><li data-preset-tag="p"><p>Silicon Valley equivalent: $150,000+</p></li></ul><p>The reality for most graduates is even grimmer:</p><ul><li data-preset-tag="p"><p>£25,000 starting salaries at traditional engineering firms</p></li><li data-preset-tag="p"><p>Exodus to consulting or finance just because it's compensated better</p></li></ul><p>Meanwhile computer science graduates land lucrative jobs in big tech or quant trading, often starting at £100,000+</p><p>Examples of wasted potential:</p><ul><li data-preset-tag="p"><p>Sarah: Built a fusion reactor at 16. Now? Debugging fintech payment systems.</p></li><li data-preset-tag="p"><p>James: 3D-printed prosthetic limbs for A-levels. Today? Writing credit risk reports.</p></li><li data-preset-tag="p"><p>Alex: Developed AI drone swarms for disaster relief at 18. Graduated with top honours from Imperial. His job? Tweaking a single button's ergonomics on home appliances.</p></li></ul><p>These aren't outliers. They're a generation of engineering prodigies whose talents are being squandered.</p><p>This isn't just wage disparity. <strong>It's misallocation of human capital on a national scale.</strong></p><p>As a hardware founder in London, I've witnessed this firsthand. We have the talent for groundbreaking innovation, but lack the means to realise it.</p><p><strong>Root Causes:</strong></p><ol><li data-preset-tag="p"><p><strong>Geographical Constraints</strong>: Unlike lucrative software jobs, hardware engineering demands physical presence.</p></li><li data-preset-tag="p"><p><strong>Venture Capital: </strong>European VCs, mostly bullish on fintech and SaaS, remain wary of hardware. Result? A feedback loop of underinvestment and missed opportunities.</p></li><li data-preset-tag="p"><p><strong>Industrial Stagnation:</strong> Traditional engineering firms fail to innovate in talent strategies and match compensation, accelerating brain drain.</p></li></ol><p><strong>Consequences:</strong></p><ol><li data-preset-tag="p"><p><strong>Innovation Stagnation:</strong> We're not just losing salary differences; we're missing out on the next ARM or Tesla.</p></li><li data-preset-tag="p"><p><strong>Economic Ripple Effects:</strong> One successful hardware company can spawn dozens of ancillary businesses. We're losing these compounding effects.</p></li><li data-preset-tag="p"><p><strong>National Security Implications: </strong>In an era where technological edge equals geopolitical power, can we afford to let our best hardware talent languish?</p></li><li data-preset-tag="p"><p><strong>Brain Drain Acceleration: </strong>We risk losing our top talent permanently to overseas markets.</p></li></ol><p><strong>Debunking Common Myths</strong></p><blockquote><p>"London's lower living costs justify lower salaries."</p></blockquote><p>False. London is around the same as NYC and more expensive than most parts of California and definitely Texas. This also ignores:</p><ul><li data-preset-tag="p"><p>Wealth Creation and Ecosystem Acceleration: High salaries and successful exits compound dramatically over time, that's why the US has so much more VC and angel capital.</p></li><li data-preset-tag="p"><p>Talent attraction: Top jobs draw global talent. Example: Google's entry into London with competitive salaries reshaped the entire tech ecosystem.</p></li></ul><blockquote><p>"UK's small market limits growth."</p></blockquote><p>Outdated thinking. Consider:</p><ul><li data-preset-tag="p"><p>Dyson: From a Wiltshire barn to a global technology powerhouse, now innovating in Singapore and Malaysia.</p></li><li data-preset-tag="p"><p>Ocado: Online grocer turned global automation technology provider, with robotics solutions deployed across Europe and North America.</p></li><li data-preset-tag="p"><p>ARM: Powering 95% of smartphones globally.</p></li></ul><blockquote><p>"Hardware is riskier than software."</p></blockquote><p>No longer true:</p><ul><li data-preset-tag="p"><p>Development speed: 3D prints and PCB prototypes now available in 24 hours, rivalling software iteration speeds.</p></li><li data-preset-tag="p"><p>Moat strength: Apple's hardware-software ecosystem is far more defensible than most pure software plays.</p></li><li data-preset-tag="p"><p>Massive hardware exits: </p><ul><li data-preset-tag="p"><p>ARM: Sold to SoftBank for $32B in 2016, now worth $140B+.</p></li><li data-preset-tag="p"><p>CSR (Cambridge Silicon Radio): Acquired by Qualcomm for $2.5B in 2015.</p></li><li data-preset-tag="p"><p>Dyson: While not an exit, it's valued at over £20B as of 2023.</p></li></ul></li></ul><p><strong>The Arbitrage Play:</strong></p><p>It isn't about costs. It's about ambition.</p><p>While software talent flows freely globally, ambitious UK hardware startups can exclusively tap into a world-class, locally-bound talent pool.</p><ol><li data-preset-tag="p"><p><strong>The Software Brain Drain:</strong> </p><ul><li data-preset-tag="p"><p>US tech giants easily poach UK software talent</p></li><li data-preset-tag="p"><p>Remote work erases geographical boundaries</p></li><li data-preset-tag="p"><p>Result: Constant outflow of top software engineers</p></li></ul></li><li data-preset-tag="p"><p><strong>The Hardware Opportunity:</strong> </p><ul><li data-preset-tag="p"><p>Physical presence matters - can't build rockets remotely</p></li><li data-preset-tag="p"><p>UK hardware talent largely untapped by global competition</p></li><li data-preset-tag="p"><p>Build something ambitious, attract local engineering superstars</p></li></ul></li><li data-preset-tag="p"><p><strong>The Talent Trap:</strong> </p><ul><li data-preset-tag="p"><p>Brilliant minds wasting away in soul-crushing corporate jobs</p></li><li data-preset-tag="p"><p>Your future "10x engineer" is someone else's bored employee</p></li></ul></li><li data-preset-tag="p"><p><strong>The Hardware Advantage:</strong></p><ul><li data-preset-tag="p"><p>Forget software. Hardware is the new frontier.</p></li><li data-preset-tag="p"><p>Build the next ARM or Dyson, not another fintech app</p></li><li data-preset-tag="p"><p>Leverage UK's world-class research institutions</p></li></ul></li><li data-preset-tag="p"><p><strong>Why Now:</strong></p><ul><li data-preset-tag="p"><p>Incumbents are unambitious, startups are few (for now)</p></li><li data-preset-tag="p"><p>Top-tier VCs awakening to UK hardware potential</p></li><li data-preset-tag="p"><p>First movers will have pick of the talent pool</p></li></ul></li></ol><p><strong>The Window is Closing</strong> </p><p>This arbitrage won't last forever. As you read this, others are waking up to the opportunity. The first movers will reap the rewards. The followers will wonder why they didn't see it sooner.</p><p><strong>The Hardware Revolution Starts Now</strong></p><p>Wake up, UK. Our engineering talent is our nuclear fusion. Ignite it or lose the future.</p><p>VCs:</p><p>Your next unicorn isn't code. It's cobalt and circuits. Back the tangible.</p><p>Founders:</p><p>Stop fleeing to the US. London can be the hardware capital of the world. We have the talent. We have the creativity. What we need is your audacity.</p><p>Engineers:</p><p>Your brain's worth billions. Build empires, not apps.</p><p>While the world obsesses over the next GPT wrapper, we'll forge the next industrial revolution.</p><p>This isn't a pipe dream. It's an imperative.</p><p>UK, it's time to build.</p></div></div>]]></description>
        </item>
    </channel>
</rss>