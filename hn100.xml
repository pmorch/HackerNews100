<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 18 Feb 2025 02:30:23 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Police arrest apparent leader of cultlike 'Zizian' group linked to killings (108 pts)]]></title>
            <link>https://www.pressdemocrat.com/article/trending/police-arrest-apparent-leader-of-cultlike-zizian-group-linked-to-multiple/</link>
            <guid>43083976</guid>
            <pubDate>Mon, 17 Feb 2025 22:39:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pressdemocrat.com/article/trending/police-arrest-apparent-leader-of-cultlike-zizian-group-linked-to-multiple/">https://www.pressdemocrat.com/article/trending/police-arrest-apparent-leader-of-cultlike-zizian-group-linked-to-multiple/</a>, See on <a href="https://news.ycombinator.com/item?id=43083976">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
					
											
						<p>The apparent leader of a cultlike group known as the Zizians has been arrested in Maryland along with another member of the group.</p><div><p><img src="https://imengine.prod.srp.navigacloud.com/?uuid=35636237-3235-4765-b139-393731376365&amp;type=primary&amp;q=75&amp;width=1024" srcset="https://imengine.prod.srp.navigacloud.com/?uuid=35636237-3235-4765-b139-393731376365&amp;type=preview&amp;q=75&amp;width=150 150w,https://imengine.prod.srp.navigacloud.com/?uuid=35636237-3235-4765-b139-393731376365&amp;type=preview&amp;q=75&amp;width=300 300w,https://imengine.prod.srp.navigacloud.com/?uuid=35636237-3235-4765-b139-393731376365&amp;type=preview&amp;q=75&amp;width=768 768w,https://imengine.prod.srp.navigacloud.com/?uuid=35636237-3235-4765-b139-393731376365&amp;type=primary&amp;q=75&amp;width=1024 1024w" sizes="(max-width: 1024px) 100vw, 1024px" alt="FILE - This image taken from video provided by WCAX shows police cars closing off a road after a shooting involving a U.S. Border Patrol agent on Interstate 91 near Coventry, Vt., on Jan. 20, 2025. (WCAX via AP, File)"></p></div>						<div><div><p><time datetime="2025-02-18T00:30:10Z" pubdate="pubdate">February 17, 2025, 4:30PM</time></p><p><time datetime="2025-02-18T00:41:17Z">Updated 1 hour ago</time></p></div><p>2 minute read</p></div>											
										
					<div data-pw-page="1">
						
																									
																									
						
<p>BOSTON — The apparent leader of a cultlike group known as the Zizians has been arrested in Maryland along with another member of the group, Maryland State Police said Monday.</p>
<p>Jack Lasota, 34, was arrested Sunday along with Michelle Zajko, 33, of Media, Pennsylvania. They face multiple charges including trespassing, obstructing and hindering and possession of a handgun in the vehicle.</p>
<p>A bail hearing for the the two is scheduled for 11 a.m. Tuesday at Allegany District Court.</p>
<p>The Zizians have been tied to the killing of U.S. Border Patrol Agent <a href="https://apnews.com/article/vermont-border-patrol-shooting-d40ffa36bad16cfce6174daa3cd3fb96" id="link-1">David Maland</a> near the Canadian border in January and five other homicides in Vermont, Pennsylvania and California.</p>
<p>Maland, 44, was killed in <a href="https://apnews.com/article/vermont-border-patrol-shooting-3ced268f987df53069d0096465ea2271" id="link-2">a Jan. 20 shootout following a traffic stop in Coventry, Vermont</a>, a small town about 20 miles (32 kilometers) from the Canadian border.</p>
<p>Officials have offered few details of the <a href="https://apnews.com/article/vermont-border-agent-killed-78d4336b6ee5ea0bb4ae372627e37f67" id="link-3">cross-country investigation</a>, which broke open after the Jan. 20 shooting death of Maland. Associated Press interviews and a review of court records and online postings tell the story of how a group of young, highly intelligent computer scientists, most of them in their 20s and 30s, met online, shared anarchist beliefs, and became increasingly violent.</p>
<p>Their goals aren’t clear, but online writings span topics from radical veganism and gender identity to artificial intelligence.</p>
<p>At the middle of it all is “Ziz,” who appears to be the leader of the strange group members who called themselves “Zizians.” She has been seen near multiple crime scenes and has connections to various suspects.</p>
<p>LaSota published a dark and sometimes violent blog under the name Ziz and, in one section, described her theory that the two hemispheres of the brain could hold separate values and genders and “often desire to kill each other.”</p>
<p>LaSota, who used she/her pronouns, and in her writings says she is a transgender woman, railed against perceived enemies, including so-called rationalist groups, which operate mostly online and seek to understand human cognition through reason and knowledge. Some are concerned with the potential dangers of artificial intelligence.</p>
<p>LaSota, 34, has not responded to multiple Associated Press emails in recent weeks, and her <a href="https://apnews.com/article/vermont-border-patrol-shooting-youngblut-72c1b68aeb3d52cccb05b6b850d83450" id="link-4">attorney Daniel McGarrigle</a> declined to comment when asked whether she is connected to any of the deaths. Before her weekend arrest, she missed court appearances in two states, and bench warrants have been issued for her arrest.</p>
<p>Reached on Monday, McGarrigle would only confirm that he has represented LaSota and wouldn't confirm her arrest or any details of the latest case.</p>						
																															
					</div>
					
																											
																					
											
												
						
									
				</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[List of DRM-Free Bookshops (134 pts)]]></title>
            <link>https://libreture.com/bookshops/</link>
            <guid>43083772</guid>
            <pubDate>Mon, 17 Feb 2025 22:09:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://libreture.com/bookshops/">https://libreture.com/bookshops/</a>, See on <a href="https://news.ycombinator.com/item?id=43083772">Hacker News</a></p>
Couldn't get https://libreture.com/bookshops/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[NLRB acting general counsel rescinds non-compete labor policy (105 pts)]]></title>
            <link>https://natlawreview.com/article/nlrb-acting-general-counsel-rescinds-non-compete-labor-policy</link>
            <guid>43083295</guid>
            <pubDate>Mon, 17 Feb 2025 21:05:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://natlawreview.com/article/nlrb-acting-general-counsel-rescinds-non-compete-labor-policy">https://natlawreview.com/article/nlrb-acting-general-counsel-rescinds-non-compete-labor-policy</a>, See on <a href="https://news.ycombinator.com/item?id=43083295">Hacker News</a></p>
Couldn't get https://natlawreview.com/article/nlrb-acting-general-counsel-rescinds-non-compete-labor-policy: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Project 2025 Observer (155 pts)]]></title>
            <link>https://www.project2025.observer/</link>
            <guid>43083280</guid>
            <pubDate>Mon, 17 Feb 2025 21:04:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.project2025.observer/">https://www.project2025.observer/</a>, See on <a href="https://news.ycombinator.com/item?id=43083280">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header><div><nav><div><div><p><span>Data Wrangler</span>&nbsp;<a href="https://www.reddit.com/user/rusticgorilla/" target="_blank" rel="noopener noreferrer">u/rusticgorilla</a></p></div><p>&nbsp;|&nbsp;</p><div><p><span>Code Wrangler</span>&nbsp;<a href="https://www.buymeacoffee.com/project2025observer" target="_blank" rel="noopener noreferrer">u/mollynaquafina</a></p></div></div></nav></div></header><main><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><div><div></div><div><div></div><div></div><div></div><div></div><div></div><div></div></div></div><!--/$--></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Privacy Is Not Dead: Beware the All-or-Nothing Mindset (104 pts)]]></title>
            <link>https://www.privacyguides.org/articles/2025/02/17/privacy-is-not-dead/</link>
            <guid>43083151</guid>
            <pubDate>Mon, 17 Feb 2025 20:48:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.privacyguides.org/articles/2025/02/17/privacy-is-not-dead/">https://www.privacyguides.org/articles/2025/02/17/privacy-is-not-dead/</a>, See on <a href="https://news.ycombinator.com/item?id=43083151">Hacker News</a></p>
Couldn't get https://www.privacyguides.org/articles/2025/02/17/privacy-is-not-dead/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Plane crashes, overturns during landing at Toronto airport (132 pts)]]></title>
            <link>https://www.cbc.ca/news/canada/toronto/toronto-pearson-overturned-airplane-1.7461227</link>
            <guid>43083012</guid>
            <pubDate>Mon, 17 Feb 2025 20:30:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cbc.ca/news/canada/toronto/toronto-pearson-overturned-airplane-1.7461227">https://www.cbc.ca/news/canada/toronto/toronto-pearson-overturned-airplane-1.7461227</a>, See on <a href="https://news.ycombinator.com/item?id=43083012">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="detailContent"><!--$--><p><span><a href="https://www.cbc.ca/news/canada/toronto"><span>Toronto</span></a><span><span>·</span><span data-live="false" data-breaking="false">Updated</span></span></span></p><!--/$--><p>A plane crashed and flipped on its back at Toronto's Pearson International Airport on Monday afternoon, injuring at least 17&nbsp;passengers.</p><h2 lang="en">Departures, arrivals resumed as of 5 p.m. ET, following crash that injured at least 17</h2><!--$--><!--/$--><!--$--><div data-cy="storyWrapper"><!--$--><figure><p><img alt="A plane is overturned on its back after a crash on an icy runway and emergency personnel surround it." src="https://i.cbc.ca/1.7461403.1739843807!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_1180/ont-pearson-plane-crash-20250217.jpg?im=Resize%3D780" data-cy="leadmedia-story-img" fetchpriority="high"></p><figcaption>A Delta Air Lines plane from Minneapolis crashed at Toronto's Pearson International Airport on Monday.<!-- --> <!-- -->(Geoff Robins/AFP/Getty Images)</figcaption></figure><!--/$--><div><p>A plane crashed and flipped on its back at Toronto's Pearson International Airport on Monday afternoon, injuring at least 17&nbsp;passengers.</p><p>Departures and arrivals on the airport's remaining three runways were suspended following the crash landing&nbsp;but resumed as of 5 p.m. ET, Deborah Flint, president and CEO of the Greater Toronto Airports Authority (GTAA), told reporters in a media briefing on Monday.</p><p>Flint said the injured&nbsp;were taken to area hospitals and that the airport doesn't know if any of the injured were critically hurt.</p><p>In the media update, Flint said the number of injured was 17, but Todd Aitken, the GTAA's fire chief, said later on Monday that there was an additional injured person who&nbsp;went to hospital later.</p><p>The number of injured has fluctuated in reports since the crash occurred.&nbsp;Earlier, Peel Regional Paramedic Services, which services Mississauga, Ont., where the airport is located just outside Toronto, said&nbsp;15 passengers in&nbsp;total were&nbsp;hurt, after initially saying it&nbsp;believed eight people had been&nbsp;injured.</p><p>Delta Air Lines said in an update on Monday evening that 18 people were injured, but Peel paramedics said that total includes&nbsp;three family members who accompanied the injured to the hospital.</p><p>All 76 passengers and four crew members on the Delta Air Lines flight from Minneapolis to Toronto&nbsp;were accounted for, Flint said. There were 22 Canadians on board, she added. The other passengers were multinational.</p><p>Two runways will remain closed as the investigation continues.</p><p><strong><em>WATCH&nbsp;| Video shows passengers disembarking from overturned plane at Pearson:</em></strong><span><span><div title="8 injured after plane flips at Pearson airport" role="button" tabindex="0" data-cy="player-placeholder-ui-container"><p><img src="https://i.cbc.ca/ais/bc57af02-9cb4-4334-ab0b-702073e272d2,1739825666613/full/max/0/default.jpg?im=Crop%2Crect%3D%280%2C0%2C1280%2C720%29%3BResize%3D620" srcset="" alt="" loading="lazy"></p><div><p>8 injured after plane flips at Pearson airport</p></div></div><span>Toronto's Pearson airport is responding to an emergency involving a plane arriving from Minneapolis Monday afternoon, the airport said in a post to X. Paramedics say they are dealing with eight patients involved in the emergency.</span></span></span></p><p>"We are very grateful there was no loss of life and relatively minor injuries," Flint said, adding that the quick response was due to the "heroic"&nbsp;first responders at the airport.</p><p>"We are very focused on the care and the concern and the passengers and the crew, some of whom have already been reunified with their friends and their families. Others we have in a comfortable place right here at the airport in an environment where they're getting a lot of care and support from my staff."</p><p>According to Flint, Delta Air Lines Flight 4819, operated by subsidiary Endeavor Air,&nbsp;was involved in a "single aircraft accident" upon landing.</p><p>Aitken told reporters on Monday night that the runway conditions were dry and there were no crosswinds.</p><p>Ornge,&nbsp;Ontario's air ambulance service, said&nbsp;a&nbsp;child was taken to Toronto's Hospital&nbsp;for&nbsp;Sick Children&nbsp;with critical injuries, while a man in his 60s and a woman in her 40s were also taken to Toronto hospitals with critical injuries.</p><p>Three air ambulance helicopters and two critical care land ambulances were dispatched to the scene, Ornge said.</p><p>Peel Regional Police Const.&nbsp;Sarah Patten told Reuters that most of the passengers were&nbsp;unharmed, "but we're still trying to make sure so we're still on scene investigating."</p><p>The airport said earlier in the day that it was expecting a busy day on Monday as airlines tried to catch up after 22 centimetres of snow fell over the weekend, causing&nbsp;a mass of delays and cancellations. More than&nbsp;130,000 travellers were expected to board about&nbsp;1,000 flights, it said in a post on Monday morning.</p><h2>'The aircraft ... is upside down and burning'</h2><p>Audio recording from Pearson's air traffic control tower shows that the Delta Air Lines flight&nbsp;was cleared to land shortly after 2 p.m. and that the tower warned the pilots of a possible air flow "bump" in the glide path from an aircraft in front of it, according to a report from The Canadian Press.</p><p>There were no further conversations with the Delta flight until the tower confirmed that a plane had crashed, with air traffic controllers quickly redirecting traffic to accommodate the crash scene.</p><p>Audio conversations from ground crews at the airport recorded a burst of commotion from workers at about the same time, with someone yelling at another person to "get off the phone," while another crew member described "a huge emergency."</p><p>Several minutes later, air traffic control could&nbsp;be heard in the recording directing a medevac helicopter for landing, noting there are people walking around the aircraft.</p><p>"Yeah, we've got it," the medevac responds. "The aircraft ... is upside down and burning."</p><div><figure><p><img loading="lazy" alt="On a snowy airport runway, about a dozen people walk away from an overturned aircraft. It is day time." srcset="https://i.cbc.ca/1.7461243.1739823748!/fileImage/httpImage/image.jpeg_gen/derivatives/original_1180/toronto-pearson-emergency.jpeg?im=Resize%3D300 300w,https://i.cbc.ca/1.7461243.1739823748!/fileImage/httpImage/image.jpeg_gen/derivatives/original_1180/toronto-pearson-emergency.jpeg?im=Resize%3D460 460w,https://i.cbc.ca/1.7461243.1739823748!/fileImage/httpImage/image.jpeg_gen/derivatives/original_1180/toronto-pearson-emergency.jpeg?im=Resize%3D620 620w,https://i.cbc.ca/1.7461243.1739823748!/fileImage/httpImage/image.jpeg_gen/derivatives/original_1180/toronto-pearson-emergency.jpeg?im=Resize%3D780 780w,https://i.cbc.ca/1.7461243.1739823748!/fileImage/httpImage/image.jpeg_gen/derivatives/original_1180/toronto-pearson-emergency.jpeg?im=Resize%3D1180 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.7461243.1739823748!/fileImage/httpImage/image.jpeg_gen/derivatives/original_1180/toronto-pearson-emergency.jpeg?im=" data-cy="image-img"></p><figcaption>Toronto's Pearson airport says it's responding to an emergency involving a plane landing Monday afternoon. Passenger Pete Carlson took this photo.<!-- --> <!-- -->(Submitted by Pete Carlson)</figcaption></figure></div><p>The <!-- -->Delta <!-- -->Air Lines&nbsp;plane,&nbsp;a Mitsubishi CRJ-900LR, has capacity for 95 seats and was built by Bombardier Inc., according to an aircraft registration posted on FlightAware, an online flight tracking platform.&nbsp;It has been registered to Delta since January 2010, according to the website.</p><p>In an online update posted Monday afternoon, <!-- -->Delta <!-- -->Air Lines said it was aware of the crash involving Flight 4819, operated by subsidiary Endeavor Air.</p><p>"Our primary focus is taking care of those impacted," the update said. "<!-- -->Delta is working to connect with customers travelling from, to or through <!-- -->YYZ<!-- --> who should also monitor the status of their flight via the Fly Delta <!-- -->app."</p><p><strong><em>WATCH&nbsp;| Passenger recounts experience after plane flipped on landing:</em></strong><span><span><div title="'I was just in a plane crash. Oh my God,' says passenger after plane flipped on landing" role="button" tabindex="0" data-cy="player-placeholder-ui-container"><p><img src="https://i.cbc.ca/ais/3def9d03-ff8b-49dc-ae30-86e3b0516181,1739834442197/full/max/0/default.jpg?im=Crop%2Crect%3D%280%2C0%2C1652%2C929%29%3BResize%3D620" srcset="" alt="" loading="lazy"></p><div><p>'I was just in a plane crash. Oh my God,' says passenger after plane flipped on landing</p></div></div><span>Video posted to Snapchat by passenger Ashley Zook shows the immediate aftermath from inside and outside a Delta Air Lines plane from Minneapolis that flipped over during landing at Toronto's Pearson airport on Monday.</span></span></span></p><p>Federal Transport Minister Anita Anand&nbsp;posted on X, formerly Twitter, that&nbsp;she's closely following the "serious incident."</p><p>"Passengers travelling across Canada today are advised to check the status of their flight before going to their airport due to traffic impacts from diverted flights," she said.</p><p>The Transportation Safety Board of Canada said it is sending a team to investigate the incident. The U.S. National Transportation Safety Board said on social media that it is assisting with the investigation.</p><p>Other airports have been accepting planes diverted from Pearson in light of the crash, including Hamilton International Airport and Montreal's Trudeau International Airport.</p><p dir="ltr">Toronto Mayor Olivia Chow posted on X on Monday that she was relieved all passengers and crew were accounted for.</p><p dir="ltr">"Thank you to the first responders, crew and airport staff for their quick actions and commitment to keeping everyone safe," she said.</p><p dir="ltr">Ontario Liberal Leader Bonnie Crombie, Progressive Conservative Leader Doug Ford, Green Party Leader Mike Schreiner&nbsp;and NDP Leader Marit Stiles all expressed their relief on social media that there were no fatalities.</p></div></div><!--/$--><!--$--><!--/$--><div><h2>ABOUT THE AUTHOR</h2><div><figure><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.6777293.1678986263!/fileImage/httpImage/image.jpg_gen/derivatives/square_1180/ethan-lang.jpg?im=Resize%3D192 192w,https://i.cbc.ca/1.6777293.1678986263!/fileImage/httpImage/image.jpg_gen/derivatives/square_1180/ethan-lang.jpg?im=Resize%3D288 288w" sizes="96px" src="https://i.cbc.ca/1.6777293.1678986263!/fileImage/httpImage/image.jpg_gen/derivatives/square_1180/ethan-lang.jpg?im=" data-cy="author-image-img"></p></figure></div><p>Ethan Lang is a reporter for CBC Toronto. Ethan has also worked in Whitehorse, where he covered the Yukon Legislative Assembly, and Halifax, where he wrote on housing and forestry for the Halifax Examiner.</p><ul></ul></div><p>With files from Muriel Draaisma, Lamia Abozaid, Radio-Canada, Reuters, The Canadian Press</p><!--$--><!--/$--><!--$--><!--/$--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Debugging an Undebuggable App (199 pts)]]></title>
            <link>https://bryce.co/undebuggable/</link>
            <guid>43081713</guid>
            <pubDate>Mon, 17 Feb 2025 18:10:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bryce.co/undebuggable/">https://bryce.co/undebuggable/</a>, See on <a href="https://news.ycombinator.com/item?id=43081713">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>I recently ran into an <a href="https://apps.apple.com/us/app/top-widgets-icons-themes/id6446477593">app</a> that:</p><ol><li>Blocks debuggers from being attached</li><li>Exits early if you try to inject any code</li><li>Crashes your whole phone if you run it with a jailbreak on (!)</li></ol><video playsinline="" autoplay="" loop="" muted="" preload="auto" data-setup="{}">
<source src="https://bryce.co/undebuggable/Undebuggable-Animation.mp4" type="video/mp4">
<source src="https://bryce.co/" type="video/webm">
<source src="https://bryce.co/" type="video/ogg"></video><p>Like — who even does that last one???</p><p>The sorts of things we do here, like <a href="https://youtu.be/YW3jL2gI9IE">modding TikTok</a>
to only show cat videos
or <a href="https://youtu.be/SpHl5_0n3Ps">fixing freezes</a> in other peoples’ apps,
all require the ability to take an app
and poke at it to see how it works.</p><p>But it’s not uncommon for iOS apps
to include additional protections to keep
prying eyes away — like jailbreak detection
or <a href="https://bryce.co/youtube-code">code obfuscation</a>.</p><p>Though it looks like this app has a surprisingly fun combination of them.
A lot more than I’d expect for a regular old Widget app.</p><blockquote><p>It turns out this app does a few things more interesting than
a regular old Widget app — but that’s for a future post!</p></blockquote><p>Let’s take a look at each of these protections one-by-one,
and figure out how to circumvent them.</p><h2 id="video-version">Video Version</h2><p>There’s a video version of this post here that shows off this process in even more detail:</p><p><iframe src="https://www.youtube.com/embed/ih6gWZDuNME" allowfullscreen="" title="YouTube Video"></iframe></p><p>If you prefer text though, you’re in the right place — let’s get into it!</p><h2 id="table-of-contents">Table of Contents</h2><div><ul><li><a href="#pt_deny_attach">PT_DENY_ATTACH</a><ul><li><a href="#bypassing-pt_deny_attach-easy-mode">Bypassing <code>PT_DENY_ATTACH</code> (Easy Mode)</a></li><li><a href="#bypassing-pt_deny_attach-hard-mode">Bypassing <code>PT_DENY_ATTACH</code> (Hard Mode)</a></li></ul></li><li><a href="#killing-the-phone">Killing The Phone</a></li><li><a href="#injecting-code">Injecting Code</a></li><li><a href="#wrapping-up">Wrapping Up</a></li></ul></div><h2 id="pt_deny_attach"><code>PT_DENY_ATTACH</code></h2><p>Let’s start
with getting the debugger attached,
because that might help us with sorting
out the other protections later.</p><p>I’m running this app on a jailbroken phone,
which usually makes it pretty easy to attach a debugger to an app.
Normally, we can <code>ssh</code> into the phone
and then start <code>debugserver</code>
attached to a given app:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>$</span> <span>/</span><span>var</span><span>/</span><span>jb</span><span>/</span><span>usr</span><span>/</span><span>bin</span><span>/</span><span>debugserver</span> <span>0.0.0.0</span><span>:</span><span>4445</span> <span>-</span><span>a</span> <span>AppStore</span>
</span></span></code></pre></div><p>And then attach to that <code>debugserver</code> from another computer using <code>lldb</code>:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>$</span> <span>lldb</span>
</span></span><span><span><span>(</span><span>lldb</span><span>)</span> <span>platform</span> <span>select</span> <span>remote</span><span>-</span><span>ios</span>
</span></span><span><span><span>(</span><span>lldb</span><span>)</span> <span>process</span> <span>connect</span> <span>connect</span><span>:</span><span>//localhost:4445
</span></span></span><span><span><span></span><span>Process</span> <span>303</span> <span>stopped</span>
</span></span><span><span><span>Target</span> <span>0</span><span>:</span> <span>(</span><span>AppStore</span><span>)</span> <span>stopped</span><span>.</span>
</span></span></code></pre></div><p>And now we can interact
with the app however we want; print debug information,
set breakpoints to follow the code flow, etc.</p><p>That’s how it should work — but if we try to
do the same thing with this widget app,
we run into a problem launching <code>debugserver</code>:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>$</span> <span>/</span><span>var</span><span>/</span><span>jb</span><span>/</span><span>usr</span><span>/</span><span>bin</span><span>/</span><span>debugserver</span> <span>0.0.0.0</span><span>:</span><span>4445</span> <span>-</span><span>a</span> <span>TopWidget</span>
</span></span><span><span><span>debugserver</span><span>-</span><span>@(</span><span>#</span><span>)</span><span>PROGRAM</span><span>:</span><span>LLDB</span>  <span>PROJECT</span><span>:</span><span>lldb</span><span>-</span><span>1403.2.3.13</span>
</span></span><span><span> <span>for</span> <span>arm64</span><span>.</span>
</span></span><span><span><span>Attaching</span> <span>to</span> <span>process</span> <span>TopWidget</span><span>...</span>
</span></span><span><span><span>Segmentation</span> <span>fault</span>
</span></span></code></pre></div><p>We get a segmentation fault, and the debugger doesn’t actually attach.</p><p>… and then the whole phone soft-reboots/resprings because of one of the other protections. We’ll get to that!</p><video playsinline="" autoplay="" loop="" muted="" preload="auto" data-setup="{}">
<source src="https://bryce.co/undebuggable/SoftReboot.mp4" type="video/mp4">
<source src="https://bryce.co/" type="video/webm">
<source src="https://bryce.co/" type="video/ogg"></video><p>This failure to attach a debugger is thanks to a function called <code>ptrace</code>.
<code>ptrace</code> is a private API on iOS,
but it’s a public API on macOS, which means
we can easily access the <a href="https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man2/ptrace.2.html">documentation</a> for it.</p><p><code>ptrace</code> is super cool — it is the thing that powers
most of the debugging functionality
that we know and love.
But there’s one operation that we’re particularly interested in here — <code>PT_DENY_ATTACH</code>. From the docs:</p><blockquote><p>PT_DENY_ATTACH</p><p>This request is the other operation used by the traced
process; it allows a process that is not currently being
traced to deny future traces by its parent. All other
arguments are ignored. If the process is currently being
traced, it will exit with the exit status of ENOTSUP;
otherwise, it sets a flag that denies future traces. An
attempt by the parent to trace a process which has set this
flag will result in a segmentation violation in the parent.</p></blockquote><p>Calling <code>ptrace</code> with the <code>PT_DENY_ATTACH</code> request type
will prevent any future debugging requests,
and if the app has already been debugged,
it will exit out of the app entirely.
Very useful if you don’t want anyone poking around your app!</p><p>There are effectively two ways an iOS app can include this functionality,
and how we deal with circumventing it depends on which strategy the app
is using.</p><p>The simplest way for an app to integrate this is to just call the <code>ptrace</code>
function like you’d expect. The method call itself is fairly straightforward:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>ptrace</span><span>(</span><span>PT_DENY_ATTACH</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>);</span>
</span></span></code></pre></div><p>We specify the type of <code>ptrace</code> request, <code>PT_DENY_ATTACH</code>, to indicate
that we want to prevent debuggers from attaching; there are then three
more required parameters, but per the docs, they’re all unused
for this request type, so we just set them to <code>0</code>.</p><p>However, because this is a private API, the full
setup to invoke it is a bit more annoying —
you have to look up a pointer to the method first,
define the function’s type information yourself, etc.</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>#import &lt;dlfcn.h&gt;
</span></span></span><span><span><span>#import &lt;sys/types.h&gt;
</span></span></span><span><span><span></span>
</span></span><span><span><span>// A type representing the `ptrace` function
</span></span></span><span><span><span></span><span>typedef</span> <span>int</span> <span>(</span><span>*</span><span>ptrace_ptr_t</span><span>)(</span>
</span></span><span><span>    <span>int</span> <span>request</span><span>,</span>
</span></span><span><span>    <span>pid_t</span> <span>pid</span><span>,</span>
</span></span><span><span>    <span>caddr_t</span> <span>addr</span><span>,</span>
</span></span><span><span>    <span>int</span> <span>data</span>
</span></span><span><span><span>);</span>
</span></span><span><span>
</span></span><span><span><span>// The requst value for `PT_DENY_ATTACH`
</span></span></span><span><span><span></span><span>#define PT_DENY_ATTACH 31
</span></span></span><span><span><span></span>
</span></span><span><span><span>void</span> <span>__attribute__</span><span>((</span><span>constructor</span><span>))</span> <span>prevent_debugging</span><span>(</span><span>void</span><span>)</span> <span>{</span>
</span></span><span><span>    <span>// Get a handle to `libsystem_kernel.dylib`
</span></span></span><span><span><span></span>    <span>void</span> <span>*</span><span>libsystem_kernel_handle</span> <span>=</span> <span>dlopen</span><span>(</span>
</span></span><span><span>        <span>"/usr/lib/system/libsystem_kernel.dylib"</span><span>,</span>
</span></span><span><span>        <span>RTLD_GLOBAL</span> <span>|</span> <span>RTLD_NOW</span>
</span></span><span><span>    <span>);</span>
</span></span><span><span>
</span></span><span><span>    <span>// Find the symbol `ptrace` within that handle
</span></span></span><span><span><span></span>    <span>ptrace_ptr_t</span> <span>ptrace</span> <span>=</span> <span>dlsym</span><span>(</span>
</span></span><span><span>        <span>libsystem_kernel_handle</span><span>,</span>
</span></span><span><span>        <span>"ptrace"</span>
</span></span><span><span>    <span>);</span>
</span></span><span><span>
</span></span><span><span>    <span>// Call ptrace w/ PT_DENY_ATTACH .
</span></span></span><span><span><span></span>    <span>// All other arguments are ignored
</span></span></span><span><span><span></span>    <span>ptrace</span><span>(</span><span>PT_DENY_ATTACH</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>);</span>
</span></span><span><span>
</span></span><span><span>    <span>// Close the handle
</span></span></span><span><span><span></span>    <span>dlclose</span><span>(</span><span>libsystem_kernel_handle</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>If you add this snippet to an iOS app,
and try to launch it, you’ll see the app flash alive for just
a second before being killed — but only if a debugger is being
attached! If you tell Xcode to not debug the
launched executable (via unchecking <code>Scheme &gt; Edit Scheme &gt; Run &gt; Info &gt; Debug executable</code>),
then the app launches fine!</p><h3 id="bypassing-pt_deny_attach-easy-mode">Bypassing <code>PT_DENY_ATTACH</code> (Easy Mode)</h3><p>So that’s how <code>PT_DENY_ATTACH</code> works - now how do we work around it?</p><p>Well there’s one easy weakness to this function,
which is that it only blocks the debugger
<em>after</em> it’s been called.</p><p>So if we put a breakpoint anywhere
before that call — say, at the start of the
<code>prevent_debugging</code> function above —
and then rerun, we’ll see that the debugger attaches just fine.</p><p>This gives us a lot of control to
actually circumvent that <code>ptrace</code> call later.
There’s a bunch of ways we could do that,
but probably the easiest is to just set
a breakpoint on <code>ptrace</code> itself:</p><p>Then when we continue execution,
we can see the debugger stop execution
when the <code>ptrace</code> function is invoked:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>(</span><span>lldb</span><span>)</span> <span>bt</span>
</span></span><span><span><span>*</span> <span>thread</span> <span>#</span><span>1</span><span>,</span> <span>queue</span> <span>=</span> <span>'</span><span>com</span><span>.</span><span>apple</span><span>.</span><span>main</span><span>-</span><span>thread</span><span>'</span><span>,</span>
</span></span><span><span>  <span>stop</span> <span>reason</span> <span>=</span> <span>breakpoint</span> <span>2.1</span>
</span></span><span><span>  <span>*</span> <span>frame</span> <span>#</span><span>0</span><span>:</span> <span>0x000000010109d7d0</span>
</span></span><span><span>    <span>libsystem_kernel</span><span>.</span><span>dylib</span><span>`</span><span>__ptrace</span>
</span></span></code></pre></div><p>We’re currently stopped right at the start of the <code>ptrace</code>
function —
which means we can use <code>thread return</code> to back out
to where we were before the call
without actually invoking the function,
and then continue execution again.</p><p>And now we’ve successfully skipped the ptrace call —
the app is launched and our debugger is still attached!</p><p>We can try this strategy in the widget app
by launching <code>debugserver</code> again, but this time not
attaching it
to any currently running process:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>$</span> <span>/</span><span>var</span><span>/</span><span>jb</span><span>/</span><span>usr</span><span>/</span><span>bin</span><span>/</span><span>debugserver</span> <span>0.0.0.0</span><span>:</span><span>4445</span>
</span></span></code></pre></div><p>Instead, we’ll specify the process we’re interested
in from the <code>lldb</code> side, this time using the <code>--waitfor</code>
flag to indicate that the process hasn’t launched yet,
but that we want to attach to it as soon as it does:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>$</span> <span>lldb</span>
</span></span><span><span><span>(</span><span>lldb</span><span>)</span> <span>platform</span> <span>select</span> <span>remote</span><span>-</span><span>ios</span>
</span></span><span><span><span>(</span><span>lldb</span><span>)</span> <span>process</span> <span>connect</span> <span>connect</span><span>:</span><span>//localhost:4445
</span></span></span><span><span><span></span><span>(</span><span>lldb</span><span>)</span> <span>process</span> <span>attach</span> <span>--</span><span>name</span> <span>TopWidget</span> <span>--</span><span>waitfor</span>
</span></span></code></pre></div><p>Now if we try launching the app again,
we’ll see <code>lldb</code> successfully attach to the app
before any of its code is executed,
including the code that tries
to kick out the debugger!</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>Process</span> <span>707</span> <span>stopped</span>
</span></span><span><span><span>*</span> <span>thread</span> <span>#</span><span>1</span><span>,</span> <span>stop</span> <span>reason</span> <span>=</span> <span>signal</span> <span>SIGSTOP</span>
</span></span><span><span>    <span>frame</span> <span>#</span><span>0</span><span>:</span> <span>0x0000000108ff85b0</span> <span>dyld</span><span>`</span><span>stat64</span> <span>+</span> <span>8</span>
</span></span><span><span><span>Target</span> <span>0</span><span>:</span> <span>(</span><span>TopWidget</span><span>)</span> <span>stopped</span><span>.</span>
</span></span><span><span><span>(</span><span>lldb</span><span>)</span>
</span></span></code></pre></div><p>But if we set a breakpoint on <code>ptrace</code> and then
continuing execution, like we did in our above example,
something goes wrong:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>(</span><span>lldb</span><span>)</span> <span>b</span> <span>ptrace</span>
</span></span><span><span><span>Breakpoint</span> <span>1</span><span>:</span> <span>no</span> <span>locations</span> <span>(</span><span>pending</span><span>).</span>
</span></span><span><span><span>WARNING</span><span>:</span>  <span>Unable</span> <span>to</span> <span>resolve</span> <span>breakpoint</span>
</span></span><span><span>          <span>to</span> <span>any</span> <span>actual</span> <span>locations</span><span>.</span>
</span></span><span><span><span>(</span><span>lldb</span><span>)</span> <span>con</span>
</span></span><span><span><span>Process</span> <span>707</span> <span>resuming</span>
</span></span><span><span><span>1</span> <span>location</span> <span>added</span> <span>to</span> <span>breakpoint</span> <span>1</span>
</span></span><span><span><span>Process</span> <span>707</span> <span>exited</span> <span>with</span> <span>status</span> <span>=</span> <span>45</span> <span>(</span><span>0x0000002d</span><span>)</span>
</span></span></code></pre></div><p>The breakpoint is eventually
resolved, but it’s never hit, and we’re kicked
out of the app with an error code.
And then a couple seconds later, the whole
phone reboots again, just for fun!</p><p>Clearly, we’ll need something a bit more involved here.</p><h3 id="bypassing-pt_deny_attach-hard-mode">Bypassing <code>PT_DENY_ATTACH</code> (Hard Mode)</h3><p>I mentioned there are two ways
to integrate this <code>ptrace</code> functionality
on iOS, and the method discussed above
actually has some downsides.</p><p>For one, it’s super easy to find and skip
this <code>ptrace</code> call, just like we saw
in our earlier example — just setting a breakpoint
on <code>ptrace</code> allows it to be quickly identified
and bypassed.</p><p>And two, you have to do that suspicious-looking
runtime lookup using <code>dlopen</code> and <code>dlsym</code> —
which can make it much easier for Apple to recognize
that you’re calling a private API.</p><p>We can actually feed two birds
with one scone here and skip
this <code>ptrace</code> function call altogether.
If we run our earlier example in a sample app, and set a breakpoint
in the <code>ptrace</code> function again,
<code>lldb</code> will show us a disassembly of that function:</p><div><pre tabindex="0"><code data-lang="asm"><span><span><span>libsystem_kernel.dylib</span><span>`</span><span>__ptrace</span><span>:</span>
</span></span><span><span><span>-&gt;</span>  <span>0</span><span>x1039517d0</span> <span>&lt;+</span><span>0</span><span>&gt;</span><span>:</span>  <span>adrp</span>   <span>x9</span><span>,</span> <span>59</span>
</span></span><span><span>    <span>0</span><span>x1039517d4</span> <span>&lt;+</span><span>4</span><span>&gt;</span><span>:</span>  <span>add</span>    <span>x9</span><span>,</span> <span>x9</span><span>,</span> <span>#0x48     ; errno
</span></span></span><span><span><span></span>    <span>0x1039517d8</span> <span>&lt;+</span><span>8</span><span>&gt;</span><span>:</span>  <span>str</span>    <span>wzr</span><span>,</span> <span>[</span><span>x9</span><span>]</span>
</span></span><span><span>    <span>0</span><span>x1039517dc</span> <span>&lt;+</span><span>12</span><span>&gt;</span><span>:</span> <span>mov</span>    <span>x16</span><span>,</span> <span>#0x1a        ; =26 
</span></span></span><span><span><span></span>    <span>0x1039517e0</span> <span>&lt;+</span><span>16</span><span>&gt;</span><span>:</span> <span>svc</span>    <span>#0x80
</span></span></span><span><span><span></span>    <span>0x1039517e4</span> <span>&lt;+</span><span>20</span><span>&gt;</span><span>:</span> <span>b.lo</span>   <span>0x103951800</span>       <span>; &lt;+48&gt;
</span></span></span><span><span><span></span>    <span>0x1039517e8</span> <span>&lt;+</span><span>24</span><span>&gt;</span><span>:</span> <span>stp</span>    <span>x29</span><span>,</span> <span>x30</span><span>,</span> <span>[</span><span>sp</span><span>,</span> <span>#-0x10]!
</span></span></span><span><span><span></span>    <span>0x1039517ec</span> <span>&lt;+</span><span>28</span><span>&gt;</span><span>:</span> <span>mov</span>    <span>x29</span><span>,</span> <span>sp</span>
</span></span><span><span>    <span>0</span><span>x1039517f0</span> <span>&lt;+</span><span>32</span><span>&gt;</span><span>:</span> <span>bl</span>     <span>0x10394acf4</span>       <span>; cerror
</span></span></span><span><span><span></span>    <span>0x1039517f4</span> <span>&lt;+</span><span>36</span><span>&gt;</span><span>:</span> <span>mov</span>    <span>sp</span><span>,</span> <span>x29</span>
</span></span><span><span>    <span>0</span><span>x1039517f8</span> <span>&lt;+</span><span>40</span><span>&gt;</span><span>:</span> <span>ldp</span>    <span>x29</span><span>,</span> <span>x30</span><span>,</span> <span>[</span><span>sp</span><span>],</span> <span>#0x10
</span></span></span><span><span><span></span>    <span>0x1039517fc</span> <span>&lt;+</span><span>44</span><span>&gt;</span><span>:</span> <span>ret</span>    
</span></span><span><span>    <span>0x103951800</span> <span>&lt;+</span><span>48</span><span>&gt;</span><span>:</span> <span>ret</span>
</span></span></code></pre></div><p>We can see that the actual implementation is surprisingly short.
There’s some error handling at the top and bottom,
but the only really critical part of the function
is the <code>scv</code> instruction in the middle.</p><p>That’s a system call, meaning
this function is delegating to the kernel
to do the actual heavy lifting —
which makes sense for something
as low level as a debugging integration.</p><p>So what some developers will do, instead
of calling this <code>ptrace</code> function, is to
just make this same system call themselves
using essentially the same assembly:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>void</span> <span>__attribute__</span><span>((</span><span>constructor</span><span>))</span> <span>prevent_debugging</span><span>(</span><span>void</span><span>)</span> <span>{</span>
</span></span><span><span>    <span>asm</span> <span>volatile</span><span>(</span>
</span></span><span><span>        <span>// pass the same arguments we
</span></span></span><span><span><span></span>        <span>// were passing to ptrace.
</span></span></span><span><span><span></span>        <span>// PT_DENY_ATTACH = 31;
</span></span></span><span><span><span></span>        <span>// the other arguments are unused
</span></span></span><span><span><span></span>        <span>"mov x0, #31   </span><span>\n</span><span>"</span>
</span></span><span><span>        <span>"mov x1, #0    </span><span>\n</span><span>"</span>
</span></span><span><span>        <span>"mov x2, #0    </span><span>\n</span><span>"</span>
</span></span><span><span>        <span>"mov x3, #0    </span><span>\n</span><span>"</span>
</span></span><span><span>
</span></span><span><span>        <span>// x16 holds the type of syscall we
</span></span></span><span><span><span></span>        <span>// want to make; in this case,
</span></span></span><span><span><span></span>        <span>// ptrace = 26
</span></span></span><span><span><span></span>        <span>"mov x16, #26  </span><span>\n</span><span>"</span>
</span></span><span><span>
</span></span><span><span>        <span>// Make the actual syscall
</span></span></span><span><span><span></span>        <span>"svc #0x80     </span><span>\n</span><span>"</span>
</span></span><span><span>    <span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><blockquote><p>Note: This style of inline-assembly code has some issues - if you were building
similar within your app, you’ll likely want to use <a href="https://gcc.gnu.org/onlinedocs/gcc/Extended-Asm.html">extended asm</a>
instead. But this style makes for an easier walkthrough.</p></blockquote><p>If you’re not well-versed in assembly,
don’t worry — this one’s easier than it looks.</p><p>First we have the same four parameters
that we’re passing in our original <code>ptrace</code>
function call. There’s <code>PT_DENY_ATTACH</code>
(which is literally just the integer value
31), followed by three 0’s.
We’re passing all of those in registers
<code>x0</code> through <code>x3</code>.</p><p>Then we need to tell the kernel which
system call we actually want to make.
iOS expects
that to be passed along in register <code>x16</code>,
which we’re setting to the value <code>26</code>.
That’s the value corresponding
to <code>ptrace</code>, which we can actually see
in the disassembled version
of the real <code>ptrace</code> function above —
it sets register <code>x16</code> to <code>0x1a</code> (<code>26</code> in decimal).</p><p>And then we actually make the system
call itself.
The <code>0x80</code> here
is also unused —
We’re just required to pass a value,
and iOS uses that value <a href="https://stackoverflow.com/a/56993314">by convention</a>.</p><p>That’s the entire assembly. If we re-run our sample app
with this function added, we’ll see the same behavior as before —
the app will kick us out as soon as a debugger is attached —
all without ever calling the private <code>ptrace</code> function.</p><p>This is a little harder for Apple to detect,
and a little harder for us to work around.
There’s now no common function like <code>ptrace</code>
that we could set a breakpoint on to detect and skip.
We instead have to figure out where in the binary
the system call happens, and then decide
from there how we want to bypass it.</p><p>To do that, we need to open a decrypted copy of the app in a disassembler
and then figure out a way to search for these instructions.</p><p>Luckily, if we look back at those instructions,
one of them sticks out as a good candiate to search for:</p><p>That’s a very specific value being set to a very specific register.
I wouldn’t expect this to appear in regular app code very frequently.
So looking for this instruction
might be a good way to narrow our search space down.</p><p>Most disassemblers should give you an option
to search through their raw disassembled output
text, but that can be super slow,
plus it can be tripped up by small
formatting differences,
like whether numbers are displayed
in hexadecimal or decimal.</p><p>It’s an option if we need it — but we may have a better option here,
which is to search for the exact bytes
corresponding to this instruction.</p><p>I like <a href="https://armconverter.com/">armconverter.com</a> for this. It lets you paste in
an assembly instruction, like</p><p>and get the corresponding
bytes out — in this case:</p><p>So we can search for those bytes in the binary to find potential places
where this <code>ptrace</code> system call is being made.
One consideration here is
that there is another way
that you could write this instruction,
which is to use <code>w16</code> instead of <code>x16</code>.
That’s the same underlying register —
<code>w16</code> is just a 32-bit view,
and <code>x16</code> is a 64-bit view.
Both would be valid ways to write this assembly, so we should search for
both in the binary.</p><p>Searching for the bytes corresponding to <code>mov x16, #26</code>, we find nothing — but searching
for the bytes for <code>mov w16, #26</code>, we get four results:</p><div><pre tabindex="0"><code data-lang="txt"><span><span>Address                  Function       Instruction
</span></span><span><span>__text:00000001013BB18C	 sub_1013BA900	MOV W16, #0x1A
</span></span><span><span>__text:0000000102A21D80	 sub_102A21868	MOV W16, #0x1A
</span></span><span><span>__text:0000000102A2BB10	 sub_102A2B984	MOV W16, #0x1A
</span></span><span><span>__text:0000000102A2BB64	 sub_102A2B984	MOV W16, #0x1A
</span></span></code></pre></div><p>That’s pretty manageable. Clicking into each result,
this first two don’t look very suspicious — meaning
that the surrounding instructions don’t look much like the
assembly we expected from the example above.</p><div><pre tabindex="0"><code data-lang="asm"><span><span><span>; sub_1013BA900
</span></span></span><span><span><span></span><span>MOV</span>   <span>W15</span><span>,</span> <span>#8
</span></span></span><span><span><span></span><span>STURB</span> <span>W15</span><span>,</span> <span>[</span><span>X29</span><span>,</span><span>#var_F8+0xC]
</span></span></span><span><span><span></span><span>MOV</span>   <span>W16</span><span>,</span> <span>#0x1A
</span></span></span><span><span><span></span><span>STURB</span> <span>W16</span><span>,</span> <span>[</span><span>X29</span><span>,</span><span>#var_F8+0xD]
</span></span></span><span><span><span></span><span>STURB</span> <span>W12</span><span>,</span> <span>[</span><span>X29</span><span>,</span><span>#var_F8+0xE]
</span></span></span></code></pre></div><p>But looking at the third result — we find
<em>exactly</em> the assembly we were looking for!</p><div><pre tabindex="0"><code data-lang="asm"><span><span><span>; sub_102A2B984
</span></span></span><span><span><span></span><span>MOV</span> <span>X0</span><span>,</span> <span>#0x1F
</span></span></span><span><span><span></span><span>MOV</span> <span>X1</span><span>,</span> <span>#0
</span></span></span><span><span><span></span><span>MOV</span> <span>X2</span><span>,</span> <span>#0
</span></span></span><span><span><span></span><span>MOV</span> <span>X3</span><span>,</span> <span>#0
</span></span></span><span><span><span></span><span>MOV</span> <span>W16</span><span>,</span> <span>#0x1A
</span></span></span><span><span><span></span><span>SVC</span> <span>0x80</span> <span>; addr=0x102A2BB14
</span></span></span></code></pre></div><p>Perfect —
this is the part of the app that is
preventing us from attaching our debugger.
And it turns out the fourth occurrence
is just a different branch
in the same function further down.
So it’s very clear that this is the function
that we’ll want to patch out.</p><p>We’ll attach a debugger to the app again,
with the exact same setup as before,
using the <code>--waitfor</code> command to instruct
<code>lldb</code> to wait til the process has loaded.
But this time, we’ll set a breakpoint
on the addresses
where these system calls are being made.
According to the disassembler, that was
address <code>0x102A2BB14</code> and address <code>0x102A2BB68</code>.</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>(</span><span>lldb</span><span>)</span> <span>br</span> <span>s</span> <span>-</span><span>a</span> <span>0x102A2BB14</span> <span>-</span><span>s</span> <span>TopWidget</span>
</span></span><span><span><span>(</span><span>lldb</span><span>)</span> <span>br</span> <span>s</span> <span>-</span><span>a</span> <span>0x102A2BB68</span> <span>-</span><span>s</span> <span>TopWidget</span>
</span></span></code></pre></div><p>We use <code>br s</code> (short for <code>breakpoint set</code>),
with <code>-a</code> for address, and finally <code>-s</code> to indicate
that the address is relative to the given binary name;
since our disassembler knows nothing about where the binary
was actually loaded into memory, we need <code>lldb</code> to convert for us.</p><p>Now if we continue execution, one of our breakpoints is hit!</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>(</span><span>lldb</span><span>)</span> <span>con</span>
</span></span><span><span><span>Process</span> <span>865</span> <span>resuming</span>
</span></span><span><span><span>Process</span> <span>865</span> <span>stopped</span>
</span></span><span><span><span>*</span> <span>thread</span> <span>#</span><span>1</span><span>,</span> <span>queue</span> <span>=</span> <span>'</span><span>com</span><span>.</span><span>apple</span><span>.</span><span>main</span><span>-</span><span>thread</span><span>'</span><span>,</span>
</span></span><span><span>  <span>stop</span> <span>reason</span> <span>=</span> <span>breakpoint</span> <span>1.1</span>
</span></span><span><span>    <span>frame</span> <span>#</span><span>0</span><span>:</span> <span>0x000000010327bb14</span>
</span></span><span><span>    <span>TopWidget</span><span>`</span><span>___lldb_unnamed_symbol254690</span> <span>+</span> <span>400</span>
</span></span><span><span><span>TopWidget</span><span>`</span><span>___lldb_unnamed_symbol254690</span><span>:</span>
</span></span><span><span><span>-&gt;</span>  <span>0x10327bb14</span> <span>&lt;+</span><span>400</span><span>&gt;:</span> <span>svc</span>    <span>#</span><span>0x80</span>
</span></span><span><span>    <span>0x10327bb18</span> <span>&lt;+</span><span>404</span><span>&gt;:</span> <span>and</span>    <span>w10</span><span>,</span> <span>w9</span><span>,</span> <span>#</span><span>0x1</span>
</span></span><span><span>    <span>0x10327bb1c</span> <span>&lt;+</span><span>408</span><span>&gt;:</span> <span>mov</span>    <span>w9</span><span>,</span> <span>#</span><span>0x7149</span>
</span></span><span><span>    <span>0x10327bb20</span> <span>&lt;+</span><span>412</span><span>&gt;:</span> <span>movk</span>   <span>w9</span><span>,</span> <span>#</span><span>0xd7ad</span><span>,</span> <span>lsl</span> <span>#</span><span>16</span>
</span></span><span><span><span>Target</span> <span>0</span><span>:</span> <span>(</span><span>TopWidget</span><span>)</span> <span>stopped</span><span>.</span>
</span></span></code></pre></div><p>We can see that we’re stopped at an <code>svc</code> instruction,
just like we expected.</p><p>There are a couple options here to prevent this call from taking effect,
but one of the easiest is to simply bypass the instruction
altogether. We can do that by taking the address of the next instruction
(in this case, <code>0x10327bb18</code>) and asking <code>lldb</code> to jump there:</p><p>If we run <code>di</code> (for <code>disassemble</code>), we can see that we’ve now
moved to the instruction right <em>after</em> the <code>svc</code> one:</p><div><pre tabindex="0"><code data-lang="objc"><span><span>    <span>0x10327bb14</span> <span>&lt;+</span><span>400</span><span>&gt;:</span> <span>svc</span>    <span>#</span><span>0x80</span>
</span></span><span><span><span>-&gt;</span>  <span>0x10327bb18</span> <span>&lt;+</span><span>404</span><span>&gt;:</span> <span>and</span>    <span>w10</span><span>,</span> <span>w9</span><span>,</span> <span>#</span><span>0x1</span>
</span></span><span><span>    <span>0x10327bb1c</span> <span>&lt;+</span><span>408</span><span>&gt;:</span> <span>mov</span>    <span>w9</span><span>,</span> <span>#</span><span>0x7149</span>
</span></span><span><span>    <span>0x10327bb20</span> <span>&lt;+</span><span>412</span><span>&gt;:</span> <span>movk</span>   <span>w9</span><span>,</span> <span>#</span><span>0xd7ad</span><span>,</span> <span>lsl</span> <span>#</span><span>16</span>
</span></span></code></pre></div><p>And now if we continue execution… we’re in, with a debugger attached!</p><p>Right up until we hit that second protection that causes the whole phone to soft-reboot.</p><h2 id="killing-the-phone">Killing The Phone</h2><p>There’s now a very important difference from the other times we’ve run into the phone soft-rebooting/respringing during this process,
which is that this time, <code>lldb</code> is <em>still attached</em>:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>Process</span> <span>865</span> <span>stopped</span>
</span></span><span><span><span>*</span> <span>thread</span> <span>#</span><span>1</span><span>,</span> <span>queue</span> <span>=</span> <span>'</span><span>com</span><span>.</span><span>apple</span><span>.</span><span>main</span><span>-</span><span>thread</span><span>'</span><span>,</span>
</span></span><span><span>  <span>stop</span> <span>reason</span> <span>=</span> <span>signal</span> <span>SIGKILL</span>
</span></span><span><span>    <span>frame</span> <span>#</span><span>0</span><span>:</span> <span>0x0000000211445030</span>
</span></span><span><span>    <span>libsystem_kernel</span><span>.</span><span>dylib</span><span>`</span><span>mach_msg2_trap</span> <span>+</span> <span>8</span>
</span></span><span><span><span>libsystem_kernel</span><span>.</span><span>dylib</span><span>`</span><span>mach_msg2_trap</span><span>:</span>
</span></span><span><span><span>-&gt;</span>  <span>0x211445030</span> <span>&lt;+</span><span>8</span><span>&gt;:</span> <span>ret</span>
</span></span><span><span>
</span></span><span><span><span>libsystem_kernel</span><span>.</span><span>dylib</span><span>`</span><span>macx_swapon</span><span>:</span>
</span></span><span><span>    <span>0x211445034</span> <span>&lt;+</span><span>0</span><span>&gt;:</span> <span>mov</span>    <span>x16</span><span>,</span> <span>#</span><span>-</span><span>0x30</span>
</span></span><span><span>    <span>0x211445038</span> <span>&lt;+</span><span>4</span><span>&gt;:</span> <span>svc</span>    <span>#</span><span>0x80</span>
</span></span><span><span>    <span>0x21144503c</span> <span>&lt;+</span><span>8</span><span>&gt;:</span> <span>ret</span>
</span></span><span><span><span>Target</span> <span>0</span><span>:</span> <span>(</span><span>TopWidget</span><span>)</span> <span>stopped</span><span>.</span>
</span></span><span><span><span>(</span><span>lldb</span><span>)</span>
</span></span></code></pre></div><p>By fixing the debugger,
we now have the ability to get some
insights into the whole soft-reboot thing.
We can see that the process was sent a kill signal,
and we run <code>bt</code> to see the current stacktrace,
We can see what it was doing
when it was killed:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>(</span><span>lldb</span><span>)</span> <span>bt</span>
</span></span><span><span><span>*</span> <span>thread</span> <span>#</span><span>1</span><span>,</span> <span>queue</span> <span>=</span> <span>'</span><span>com</span><span>.</span><span>apple</span><span>.</span><span>main</span><span>-</span><span>thread</span><span>'</span><span>,</span> <span>stop</span> <span>reason</span> <span>=</span> <span>signal</span> <span>SIGKILL</span>
</span></span><span><span>  <span>*</span> <span>frame</span> <span>#</span><span>0</span><span>:</span> <span>0x0000000211445030</span> <span>libsystem_kernel</span><span>.</span><span>dylib</span><span>`</span><span>mach_msg2_trap</span> <span>+</span> <span>8</span>
</span></span><span><span>    <span>frame</span> <span>#</span><span>4</span><span>:</span> <span>0x00000001d72b1690</span> <span>QuartzCore</span><span>`</span><span>CARenderServerCaptureDisplayWithTransform_</span> <span>+</span> <span>628</span>
</span></span><span><span>    <span>frame</span> <span>#</span><span>5</span><span>:</span> <span>0x00000001d70e5cdc</span> <span>QuartzCore</span><span>`</span><span>CARenderServerSnapshot_</span><span>(</span><span>unsigned</span> <span>int</span><span>,</span> <span>NSDictionary</span><span>*</span><span>)</span> <span>+</span> <span>1844</span>
</span></span><span><span>    <span>frame</span> <span>#</span><span>6</span><span>:</span> <span>0x00000001d719d6f0</span> <span>QuartzCore</span><span>`</span><span>CARenderServerSnapshot</span> <span>+</span> <span>12</span>
</span></span><span><span>    <span>frame</span> <span>#</span><span>7</span><span>:</span> <span>0x00000001d8881628</span> <span>UIKitCore</span><span>`</span><span>___UISnapshotScreenWindowsRectBlock_block_invoke</span> <span>+</span> <span>368</span>
</span></span><span><span>    <span>frame</span> <span>#</span><span>8</span><span>:</span> <span>0x00000001d8030e40</span> <span>UIKitCore</span><span>`</span><span>_performAfterCommitUnderCoverAllowDefer</span> <span>+</span> <span>328</span>
</span></span><span><span>    <span>frame</span> <span>#</span><span>9</span><span>:</span> <span>0x00000001d887fe30</span> <span>UIKitCore</span><span>`</span><span>_UISnapshotScreenWindowsRectAfterCommit</span> <span>+</span> <span>388</span>
</span></span><span><span>    <span>frame</span> <span>#</span><span>10</span><span>:</span> <span>0x00000001d888009c</span> <span>UIKitCore</span><span>`</span><span>_UISnapshotScreenCompatibilityRectAfterCommit</span> <span>+</span> <span>564</span>
</span></span><span><span>    <span>frame</span> <span>#</span><span>11</span><span>:</span> <span>0x0000000100891898</span> <span>TopWidget</span><span>`</span><span>___lldb_unnamed_symbol7950</span> <span>+</span> <span>76</span>
</span></span><span><span>    <span>frame</span> <span>#</span><span>12</span><span>:</span> <span>0x00000001ddb9f490</span> <span>Combine</span><span>`</span><span>Combine</span><span>.</span><span>Subscribers</span><span>.</span><span>Sink</span><span>.</span><span>receive</span><span>(</span><span>τ</span><span>_0_0</span><span>)</span> <span>-&gt;</span> <span>Combine</span><span>.</span><span>Subscribers</span><span>.</span><span>Demand</span> <span>+</span> <span>88</span>
</span></span><span><span>    <span>// ...
</span></span></span><span><span><span></span>    <span>frame</span> <span>#</span><span>28</span><span>:</span> <span>0x00000001054a0344</span> <span>dyld</span><span>`</span><span>start</span> <span>+</span> <span>1860</span>
</span></span></code></pre></div><p>It’s running a function to capture the screen contents,
which is… interesting? We can also find the last address
where we are going through the app’s code itself
— on <code>frame #11</code>.</p><p>The first thing we’ll want to do is
look at that function in a disassembler,
but there’s a disconnect here
in that this is the address
relative to wherever this process
was loaded into memory, which our disassembler
knows nothing about.</p><p>But we can ask <code>lldb</code> for more information on that address:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>(</span><span>lldb</span><span>)</span> <span>image</span> <span>lookup</span> <span>-</span><span>a</span> <span>0x0000000100891898</span>
</span></span><span><span>      <span>Address</span><span>:</span> <span>TopWidget</span><span>[</span><span>0x0000000100041898</span><span>]</span> <span>(</span><span>TopWidget</span><span>.</span><span>__TEXT</span><span>.</span><span>__text</span> <span>+</span> <span>235672</span><span>)</span>
</span></span><span><span>      <span>Summary</span><span>:</span> <span>TopWidget</span><span>`</span><span>___lldb_unnamed_symbol7950</span> <span>+</span> <span>76</span>
</span></span></code></pre></div><p>The second line of that output includes the address relative to the binary itself — <code>0x100041898</code>
We can take that address and jump to it in our disassembler to find the function we were running when
we crashed.</p><p>The decompilation of this function looks like this:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>void</span> <span>__noreturn</span> <span>sub_10004184C</span><span>()</span> <span>{</span>
</span></span><span><span>  <span>void</span> <span>*</span><span>v0</span><span>;</span> <span>// x19
</span></span></span><span><span><span></span>  <span>id</span> <span>v1</span><span>;</span> <span>// x20
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>v0</span> <span>=</span> <span>(</span><span>void</span> <span>*</span><span>)</span><span>objc_opt_self</span><span>(</span><span>&amp;</span><span>OBJC_CLASS___UIScreen</span><span>);</span>
</span></span><span><span>  <span>while</span> <span>(</span> <span>1</span> <span>)</span>
</span></span><span><span>  <span>{</span>
</span></span><span><span>    <span>v1</span> <span>=</span> <span>objc_retainAutoreleasedReturnValue</span><span>(</span>
</span></span><span><span>        <span>objc_msgSend</span><span>(</span><span>v0</span><span>,</span> <span>"mainScreen"</span><span>)</span>
</span></span><span><span>    <span>);</span>
</span></span><span><span>
</span></span><span><span>    <span>objc_release</span><span>(</span><span>objc_retainAutoreleasedReturnValue</span><span>(</span>
</span></span><span><span>        <span>objc_msgSend</span><span>(</span><span>v1</span><span>,</span> <span>"snapshotViewAfterScreenUpdates:"</span><span>,</span> <span>1LL</span><span>)</span>
</span></span><span><span>    <span>));</span>
</span></span><span><span>
</span></span><span><span>    <span>objc_release</span><span>(</span><span>v1</span><span>);</span>
</span></span><span><span>  <span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>In an infinite loop,
we call <code>+[UIScreen mainScreen]</code>, and on the result,
we call <code>snapshotViewAfterScreenUpdates:</code>.
This is a public API
and a fairly common one — it just creates a snapshot of a view.</p><p>But it’s the surrounding implementation
that’s so interesting here.
This is a very memory-intensive method,
but we don’t do anything with the result;
We’re just taking snapshots
in an infinite loop!</p><p>Sometimes a decompilation
can get things wrong, but if we look at the graph view
of this function,
we see the exact same thing —
we’re in an infinite loop,
and all we do within that loop is call
<code>snapshotViewAfterScreenUpdates:</code>.
The result is never referenced.
There are no other methods called.
We have no way to break out of the loop.</p><p>I need to emphasize how crazy this is.
Using too much memory can of course
get your app killed,
but it seems like somebody found a method
that, if you call it too quickly, will
soft-reboot the whole phone (in fact, I later found
that this is the strategy that at least <a href="https://github.com/haxi0/TrollLock-Reborn/blob/484616412a6b056f8fb241db14422ad471d9c1f9/TrollLock/TrollLock/ViewController.m#L115">one jailbreak tweak</a> uses
to respring the phone upon a user’s request) —
and then they chose
to take that information
and write a function
to call it in an infinite loop,
and just torpedo the phone of whoever’s
running this code, in a regular old app.</p><p>That’s delightful!</p><p>In this post’s <a href="https://youtu.be/ih6gWZDuNME">associated video</a>, we trace back
the origin of this call a bit further,
and learn that it’s run whenever a notification named <code>com.apple.tw.twrr</code> is fired,
and that it appears to be tied to the app running a risk check on the phone.
If we don’t pass the check, it resprings our phone.</p><p>The underlying setup is interesting, but we don’t need to dive too far into
it in order to bypass this protection — if we start <code>lldb</code> again,
bypass the anti-debugging call, but this time set an additional breakpoint
on the start of the function that intentionally crashes the phone —
we can just use <code>thread return</code> to skip its execution:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>(</span><span>lldb</span><span>)</span> <span>br</span> <span>s</span> <span>-</span><span>a</span> <span>0x102A2BB68</span> <span>-</span><span>s</span> <span>TopWidget</span>
</span></span><span><span><span>Breakpoint</span> <span>2</span><span>:</span>
</span></span><span><span>  <span>where</span> <span>=</span> <span>TopWidget</span><span>`</span><span>___lldb_unnamed_symbol256584</span> <span>+</span> <span>756</span><span>,</span>
</span></span><span><span>  <span>address</span> <span>=</span> <span>0x0000000107003b68</span>
</span></span><span><span><span>// Later, on breakpoint hit:
</span></span></span><span><span><span></span><span>*</span> <span>thread</span> <span>#</span><span>1</span><span>,</span> <span>queue</span> <span>=</span> <span>'</span><span>com</span><span>.</span><span>apple</span><span>.</span><span>main</span><span>-</span><span>thread</span><span>'</span><span>,</span>
</span></span><span><span>  <span>stop</span> <span>reason</span> <span>=</span> <span>breakpoint</span> <span>2.1</span>
</span></span><span><span><span>(</span><span>lldb</span><span>)</span> <span>thread</span> <span>return</span>
</span></span><span><span><span>(</span><span>lldb</span><span>)</span> <span>con</span>
</span></span></code></pre></div><p>And now we are fully into the app with our debugger attached!</p><p><img alt="The main screen of the Top Widgets app" src="https://bryce.co/undebuggable/ActualApp.png"></p><h2 id="injecting-code">Injecting Code</h2><p>We listed one more issue at the start of all this — which is that when
we try to inject new code into this app, it <em>also</em> crashes.</p><p>I’ll often do that when I have a debugger attached,
when I need some more complex utilities to help me dive into an app —
like wanting to quickly log the accessibility information of all buttons on screen,
for example.
You could do that just from <code>lldb</code>,
but it would be somewhat painful — it’s
easier to just write that in a framework,
inject that into the app, and then call your utility
function from the debugger.</p><p>But you might also want to inject code if you don’t
have a jailbroken phone in the first place — injecting
tools like <a href="https://frida.re/docs/ios/">Frida</a> or <a href="https://github.com/FLEXTool/FLEX">Flex</a>
let you get some initial exploration of an app without actually needing a jailbreak.
You would usually inject these using a tool that resigns the app
(I like <a href="https://sideloadly.io/">Sideloadly</a>) — but if you do so here,
you’ll see an immediate crash launching the app.</p><p>So is this one more protection,
checking what frameworks are loaded at runtime
and crashing if anything unexpected appears?</p><p>Luckily, no. Some apps do that!</p><p>But we have a much simpler explanation
here. We can look at the crash — using either
<code>lldb</code> or, in the event we were doing this because
we didn’t have a jailbreak in the first place, by
pulling crash logs right from the device.</p><p>In our case, the crash shows the top of the stack trace
as <code>0x1002027D4</code>. Jumping there in our disassembler, we can see
a <code>BRK</code> instruction — which is exactly what we’d expect
in a scenario where the app was intentionally crashing,
like from force-unwrapping a nil value.</p><p>The decompilation of this method is a bit longer,
but scanning through, there’s one line that jumps out as our likely issue:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>v13</span> <span>=</span> <span>objc_retainAutoreleasedReturnValue</span><span>(</span><span>objc_msgSend</span><span>(</span>
</span></span><span><span>    <span>v8</span><span>,</span> <span>"containerURLForSecurityApplicationGroupIdentifier:"</span><span>,</span> <span>v12</span><span>)</span>
</span></span><span><span><span>);</span>
</span></span></code></pre></div><p><code>containerURLForSecurityApplicationGroupIdentifier:</code>
is a public, perfectly normal method
that returns a folder that multiple apps or extensions
in a single group can access together.
Usually apps are completely sandboxed
from each other, but putting them in a shared group means
that they can access shared resources.</p><p>The problem is that app groups are defined
as part of the code signing process, and
we just threw away the existing app signature
by injecting code and resigning the app —
and threw away any groups in the process.</p><p>So this method which is supposed to return a URL
pointing to the local file
system is probably returning <code>nil</code>,
and then the app is likely force-unwrapping that
and crashing as a result.</p><p>This is actually a pretty common problem —
and even though it’s acting like a defense
here, it’s probably not on purpose.
This is a widget app,
and it makes sense
that it would need to be in an app group
with its widget app extension that actually controls
displaying widgets on the home screen.</p><p>By far the easiest way to work around
this kind of issue is to just… not resign
the app. A jailbroken phone
makes this significantly easier —
you can inject frameworks into an app
using a jailbreak tweak
without ever having to resign it.
Or you can even run code
that has invalid signatures;
you can add whatever apps to
whatever groups you want.
Jailbreaking makes a lot of codesign
issues just go away.</p><p>In a pinch, though, if you were injecting
a debugging framework
because you don’t have
a jailbroken device,
you can still sometimes
work around things like this
even while resigning the app.</p><p>The app is crashing because it’s expecting
a URL to be returned here and it’s not getting one back.
We can create a small framework that swizzles this method
to make sure that a URL is always returned:</p><div><pre tabindex="0"><code data-lang="objc"><span><span><span>@implementation</span> <span>NSFileManager</span> <span>(Swizzle)</span>
</span></span><span><span>
</span></span><span><span><span>+</span> <span>(</span><span>void</span><span>)</span><span>load</span> <span>{</span>
</span></span><span><span>    <span>static</span> <span>dispatch_once_t</span> <span>onceToken</span><span>;</span>
</span></span><span><span>    <span>dispatch_once</span><span>(</span><span>&amp;</span><span>onceToken</span><span>,</span> <span>^</span><span>{</span>
</span></span><span><span>        <span>Class</span> <span>class</span> <span>=</span> <span>[</span><span>self</span> <span>class</span><span>];</span>
</span></span><span><span>
</span></span><span><span>        <span>SEL</span> <span>originalSelector</span>
</span></span><span><span>            <span>=</span> <span>@selector</span><span>(</span><span>containerURLForSecurityApplicationGroupIdentifier</span><span>:);</span>
</span></span><span><span>        <span>SEL</span> <span>swizzledSelector</span>
</span></span><span><span>            <span>=</span> <span>@selector</span><span>(</span><span>swizzled_containerURLForSecurityApplicationGroupIdentifier</span><span>:);</span>
</span></span><span><span>
</span></span><span><span>        <span>Method</span> <span>originalMethod</span>
</span></span><span><span>            <span>=</span> <span>class_getInstanceMethod</span><span>(</span><span>class</span><span>,</span> <span>originalSelector</span><span>);</span>
</span></span><span><span>        <span>Method</span> <span>swizzledMethod</span>
</span></span><span><span>            <span>=</span> <span>class_getInstanceMethod</span><span>(</span><span>class</span><span>,</span> <span>swizzledSelector</span><span>);</span>
</span></span><span><span>
</span></span><span><span>        <span>method_exchangeImplementations</span><span>(</span>
</span></span><span><span>            <span>originalMethod</span><span>,</span>
</span></span><span><span>            <span>swizzledMethod</span>
</span></span><span><span>        <span>);</span>
</span></span><span><span>    <span>});</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>-</span> <span>(</span><span>NSURL</span> <span>*</span><span>)</span><span>swizzled_containerURLForSecurityApplicationGroupIdentifier:</span><span>(</span><span>NSString</span> <span>*</span><span>)</span><span>groupIdentifier</span> <span>{</span>
</span></span><span><span>    <span>return</span> <span>[</span><span>self</span> <span>temporaryDirectory</span><span>];</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>@end</span>
</span></span></code></pre></div><p>In this case, we’re making it
so that anytime
<code>containerURLForSecurityApplicationGroupIdentifier:</code>
method is called,
we’re going to invoke our replacement method instead,
which just returns
some temporary directory.</p><p>It’s worth noting
this is <strong>not</strong> an equivalent replacement
for what the app is actually trying to do —
this method is being called
because the app wants a shared
folder that both it and its app
extensions can access, and we’re lying to it and saying, “Here
you go. Here’s your shared folder” —
even though that is definitely not a shared folder.</p><p>But depending on what you’re trying to do,
that might actually be fine.
I’ve found
little patches like this will often break
the app extensions themselves,
But I usually don’t care about them —
I just want the main app to work fine,
especially if my goal
is to just poke around
at how the app does something,
and I just need
that basic functionality to work.</p><p>You can maybe do something more advanced
here like create your own app group,
resign the main app and all its extensions using it,
and swizzle this method (along with any others you run into)
to use your new group identifier instead. That would be a much larger
effort though, and not likely to be worth it unless you really need it —
at some point, finding a jailbroken device instead is probably worth your while.</p><p>Either way, if we build this framework,
and inject it into the app alongside whatever else
we wanted (like <code>Flex</code>), now the app launches perfectly fine
on a normal device! On a jailbroken device, we still need to attach a debugger
and bypass the earlier protections again,
but once we do, we’re back in the app, this time with <code>Flex</code> successfully
injected, giving us a whole bunch of other useful debugging tools:</p><p><img alt="The main screen of the Top Widgets app" src="https://bryce.co/undebuggable/Flex.png"></p><h3 id="wrapping-up">Wrapping Up</h3><p>We’re in the app with a debugger attached, jailbreak detection bypassed, and code successfully injected!</p><p>… what were we here for again?</p><p>Oh, yeah - that’s for next time.</p><p>Until then, if you bypass any anti-debugging protections thanks to this article, or decide to add them
to your own app because you’re just learning about them for the first time — let me know!</p><hr></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Watch R1 "think" with animated chains of thought (154 pts)]]></title>
            <link>https://github.com/dhealy05/frames_of_mind</link>
            <guid>43080531</guid>
            <pubDate>Mon, 17 Feb 2025 16:23:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/dhealy05/frames_of_mind">https://github.com/dhealy05/frames_of_mind</a>, See on <a href="https://news.ycombinator.com/item?id=43080531">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Frames of Mind: Animating R1's Thoughts</h2><a id="user-content-frames-of-mind-animating-r1s-thoughts" aria-label="Permalink: Frames of Mind: Animating R1's Thoughts" href="#frames-of-mind-animating-r1s-thoughts"></a></p>
<p dir="auto">We can visualize the "thought process" for R1 by:</p>
<ul dir="auto">
<li>Saving the chains of thought as text</li>
<li>Converting the text to embeddings with the OpenAI API</li>
<li>Plotting the embeddings sequentially with t-SNE</li>
</ul>
<p dir="auto">Here's what it looks like when R1 answers a question (in this case "Describe how a bicycle works."):</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/dhealy05/frames_of_mind/blob/main/img/simple_animation.gif"><img src="https://github.com/dhealy05/frames_of_mind/raw/main/img/simple_animation.gif" alt="A" data-animated-image=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Consecutive Distance</h2><a id="user-content-consecutive-distance" aria-label="Permalink: Consecutive Distance" href="#consecutive-distance"></a></p>
<p dir="auto">It might be useful to get a sense of how big each jump from "thought i" to "thought i+1" is. The graph below shows the difference between consecutive steps.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/dhealy05/frames_of_mind/blob/main/img/distance.gif"><img src="https://github.com/dhealy05/frames_of_mind/raw/main/img/distance.gif" alt="A" data-animated-image=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">By default we calculate cosine similarity between the embeddings and normalize across the set of all consecutive steps to 0, 1. I'm interested in seeing when the bigger or smaller jumps happen in the "thought cycle".</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Combined Plot</h2><a id="user-content-combined-plot" aria-label="Permalink: Combined Plot" href="#combined-plot"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/dhealy05/frames_of_mind/blob/main/img/dual_animation.gif"><img src="https://github.com/dhealy05/frames_of_mind/raw/main/img/dual_animation.gif" alt="A" data-animated-image=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">The combined plot shows both at once.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Aggregate Distances</h2><a id="user-content-aggregate-distances" aria-label="Permalink: Aggregate Distances" href="#aggregate-distances"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/dhealy05/frames_of_mind/blob/main/img/normalized_sequences.png"><img src="https://github.com/dhealy05/frames_of_mind/raw/main/img/normalized_sequences.png" alt="A"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">The graph above shows the aggregate distances for 10 samples. To my eyes it looks like a "search" phase where size of step is large, followed by a stable "thinking" phase, followed by a "concluding" phase.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">I used these prompts:</p>
<ol dir="auto">
<li>Describe how a bicycle works.</li>
<li>Design a new type of transportation.</li>
<li>Explain why leaves change color in autumn</li>
<li>How should society balance individual freedom with collective good?</li>
<li>How would you resolve a conflict between two people with opposing views?</li>
<li>What makes a good life?</li>
<li>What would happen if gravity suddenly doubled?</li>
<li>What's the best way to comfort someone who is grieving</li>
<li>Why do humans make art?</li>
<li>Why do people tell jokes?</li>
</ol>
<p dir="auto">The chains are available in data/chains. To easily pull from Deepseek's public chat interface, paste the "pull_cot.js" script into your browser console when a chat is open. It will download automatically.</p>
<p dir="auto">Install requisite packages in Pipfile and run with the function in run.py.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral Saba (131 pts)]]></title>
            <link>https://mistral.ai/en/news/mistral-saba</link>
            <guid>43079046</guid>
            <pubDate>Mon, 17 Feb 2025 13:56:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/en/news/mistral-saba">https://mistral.ai/en/news/mistral-saba</a>, See on <a href="https://news.ycombinator.com/item?id=43079046">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p dir="ltr">Making AI ubiquitous requires addressing every culture and language. As AI proliferates globally, many of our customers worldwide have expressed a strong desire for models that are not just fluent but native to regional parlance. While larger, general-purpose models are often proficient in several languages, they lack linguistic nuances, cultural background, and in-depth regional knowledge required to serve use cases with strong regional context.</p>
<p dir="ltr">In such scenarios, custom-trained models tailored to regional languages can grasp the unique intricacies and insights for delivering precision and authenticity. To that end, we are proud to introduce <strong>Mistral Saba</strong>, the first of our specialized regional language models.&nbsp;</p>
<p dir="ltr">Mistral Saba is a 24B parameter model trained on meticulously curated datasets from across the Middle East and South Asia. The model provides more accurate and relevant responses than models that are over 5 times its size, while being significantly faster and lower cost. The model can also serve as a strong base to train highly specific regional adaptations. Mistral Saba is available as an <a href="http://console.mistral.ai/">API</a>, but importantly, it is also available to deploy locally within the security premises of customers. Like the recently released <a href="https://mistral.ai/news/mistral-small-3">Mistral Small 3,</a> the model is lightweight and can be deployed on single-GPU systems, responding at speeds of over 150 tokens per second.&nbsp;</p>
<p dir="ltr">In keeping with the rich cultural cross-pollination between the Middle East and South Asia, Mistral Saba supports Arabic and many Indian-origin languages, and is particularly strong in South Indian-origin languages such as Tamil and Malayalam. This capability enhances its versatility in multinational use across these interconnected regions.</p>
<h2 dir="ltr">Benchmarks</h2>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/f7c7b852-bc79-4118-b7cc-cf19cf5e0152.png?width=1920&amp;height=1080" alt=" Mistral Saba Pretrain"></p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/f08e21db-2f9e-4745-85b7-8436ffac0780.png?width=1920&amp;height=1080" alt=" Mistral Saba Instruct"></p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/5afc411e-fc9d-468b-8b33-3eaf34459d33.png?width=1590&amp;height=714" alt="Benchmark Saba"></p>
<h2 dir="ltr">Use cases</h2>
<p dir="ltr">As Mistral Saba gains traction among our customer base in the Middle East, several compelling use cases are emerging, showcasing the model's versatility:</p>
<h3 dir="ltr">Conversational Support</h3>
<p dir="ltr">Mistral Saba is ideal for scenarios requiring swift and precise Arabic responses. We’re working with customers to power virtual assistants that engage users in natural, real-time conversations in Arabic, enhancing user interactions across various platforms.</p>
<h3 dir="ltr">Domain-Specific Expertise</h3>
<p dir="ltr">Through fine-tuning, Mistral Saba can become a specialized expert in various fields such as energy, financial markets, and healthcare, offering deep insights and accurate responses all within the context of Arabic language and culture.</p>
<h3 dir="ltr">Cultural Content Creation</h3>
<p dir="ltr">Mistral Saba excels in generating culturally relevant content, including educational resources. By understanding local idioms and cultural references, it helps businesses and organizations create authentic and engaging content that resonates with Middle Eastern audiences.</p>
<p dir="ltr">Mistral Saba is quickly becoming a powerful, locally-deployable solution that respects and understands nuances in the Middle East.</p>
<h2 dir="ltr">Try it today</h2>
<p dir="ltr">Get started with the Mistral Saba <a href="https://console.mistral.ai/">API</a> to quickly build applications requiring local authenticity and accuracy. For enterprise deployments, please <a href="https://mistral.ai/en/contact">contact us</a>.&nbsp;</p>
<h2 dir="ltr">Custom model training at Mistral AI</h2>
<p dir="ltr">Mistral Saba is a result of working closely with strategic regional customers to address very specific challenges in addressing bespoke use cases. In addition to regional language models, we have also begun to train models for strategic customers with the power of their deep and proprietary enterprise context. These models stay exclusive and private to the respective customers. If you would like to explore custom training with Mistral AI, explore our <a href="https://mistral.ai/services">applied AI</a> offerings, or please <a href="https://mistral.ai/en/contact">contact us</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Espargos: ESP32-based WiFi sensing array (135 pts)]]></title>
            <link>https://espargos.net/</link>
            <guid>43079023</guid>
            <pubDate>Mon, 17 Feb 2025 13:54:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://espargos.net/">https://espargos.net/</a>, See on <a href="https://news.ycombinator.com/item?id=43079023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Real-Time Indoor Localization Demo</h2><div><p>With ESPARGOS, existing WiFi devices like smartphones, robots and other machines can be localized in indoor environments without requiring any changes to those devices.
In line-of-sight environments, ESPARGOS can simply locate devices by triangulation. <a href="https://dichasus.inue.uni-stuttgart.de/tutorials/tutorial/dissimilarity-metric-channelcharting/">Channel Charting</a> allows the system to locate devices even if there is no line-of-sight path between ESPARGOS and the end device.
This application was publicly demonstrated at the Berlin 6G Conference 2024.</p></div></div><div><h2 id="obtain">How to obtain ESPARGOS?</h2><p>At the <a href="https://www.inue.uni-stuttgart.de/">Institute of Telecommunications at the University of Stuttgart</a>, we have manufactured a small batch of ESPARGOS devices and have already provided some boards to partners for evaluation, primarily research project members and other research institutions.
We still have a very small (single-digit) number of boards available that we can provide to interested parties, but we kindly ask for your understanding that we must prioritize requests.
Please <a href="mailto:euchner@inue.uni-stuttgart.de">contact me</a> if you would like to receive an ESPARGOS board, or in case you have any questions or ideas to share about use cases, research ideas, open sourcing, manufacturing, or other topics.</p><p>We are still evaluating how to proceed, if we can (and should) make ESPARGOS available to a wider audience.
The hardware design and firmware files of ESPARGOS are currently not open source, but the <a href="https://github.com/ESPARGOS/pyespargos"><code>pyespargos</code></a> library including the demo applications are free software.
If you would like to receive updates about future developments regarding ESPARGOS, you can sign up to receive updates.</p><p><a>Sign up for Updates</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fluoxetine promotes metabolic defenses to protect from sepsis-induced lethality (134 pts)]]></title>
            <link>https://www.science.org/doi/10.1126/sciadv.adu4034</link>
            <guid>43078537</guid>
            <pubDate>Mon, 17 Feb 2025 13:02:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/doi/10.1126/sciadv.adu4034">https://www.science.org/doi/10.1126/sciadv.adu4034</a>, See on <a href="https://news.ycombinator.com/item?id=43078537">Hacker News</a></p>
Couldn't get https://www.science.org/doi/10.1126/sciadv.adu4034: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Representing Graphs in PostgreSQL (166 pts)]]></title>
            <link>https://www.richard-towers.com/2025/02/16/representing-graphs-in-postgres.html</link>
            <guid>43078100</guid>
            <pubDate>Mon, 17 Feb 2025 12:15:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.richard-towers.com/2025/02/16/representing-graphs-in-postgres.html">https://www.richard-towers.com/2025/02/16/representing-graphs-in-postgres.html</a>, See on <a href="https://news.ycombinator.com/item?id=43078100">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>


<p>Let’s say we’ve got some graph-like data, such as a social network.</p>

<p>We can represent this generically with two tables:</p>

<div><pre><code><span>create</span> <span>table</span> <span>nodes</span> <span>(</span>
  <span>id</span> <span>serial</span> <span>primary</span> <span>key</span><span>,</span>
  <span>name</span> <span>text</span><span>,</span>
  <span>details</span> <span>jsonb</span>
<span>);</span>

<span>create</span> <span>table</span> <span>edges</span> <span>(</span>
  <span>id</span> <span>serial</span> <span>primary</span> <span>key</span><span>,</span>
  <span>type</span> <span>text</span><span>,</span>
  <span>from_id</span> <span>integer</span> <span>references</span> <span>nodes</span><span>(</span><span>id</span><span>),</span>
  <span>to_id</span> <span>integer</span> <span>references</span> <span>nodes</span><span>(</span><span>id</span><span>),</span>
  <span>details</span> <span>jsonb</span>
<span>);</span>
</code></pre></div>

<p>And we can insert some data (using lord of the rings characters) like this:</p>

<div><pre><code><span>insert</span> <span>into</span> <span>nodes</span> <span>(</span><span>name</span><span>,</span> <span>details</span><span>)</span> <span>values</span>
  <span>(</span><span>'Frodo Baggins'</span><span>,</span> <span>'{"species": "Hobbit"}'</span><span>),</span> <span>-- 1</span>
  <span>(</span><span>'Bilbo Baggins'</span><span>,</span> <span>'{"species": "Hobbit"}'</span><span>),</span> <span>-- 2</span>
  <span>(</span><span>'Samwise Gamgee'</span><span>,</span> <span>'{"species": "Hobbit"}'</span><span>),</span> <span>-- 3</span>
  <span>(</span><span>'Hamfast Gamgee'</span><span>,</span> <span>'{"species": "Hobbit"}'</span><span>),</span> <span>-- 4</span>
  <span>(</span><span>'Gandalf'</span><span>,</span> <span>'{"species": "Wizard"}'</span><span>),</span> <span>-- 5</span>
  <span>(</span><span>'Aragorn'</span><span>,</span> <span>'{"species": "Human"}'</span><span>),</span> <span>-- 6</span>
  <span>(</span><span>'Arathorn'</span><span>,</span> <span>'{"species": "Human"}'</span><span>),</span> <span>-- 7</span>
  <span>(</span><span>'Legolas'</span><span>,</span> <span>'{"species": "Elf"}'</span><span>),</span> <span>-- 8</span>
  <span>(</span><span>'Thranduil'</span><span>,</span> <span>'{"species": "Elf"}'</span><span>),</span> <span>-- 9</span>
  <span>(</span><span>'Gimli'</span><span>,</span> <span>'{"species": "Dwarf"}'</span><span>),</span> <span>-- 10</span>
  <span>(</span><span>'Gloin'</span><span>,</span> <span>'{"species": "Dwarf"}'</span><span>);</span> <span>-- 11</span>

<span>-- Parents</span>
<span>insert</span> <span>into</span> <span>edges</span> <span>(</span><span>type</span><span>,</span> <span>from_id</span><span>,</span> <span>to_id</span><span>,</span> <span>details</span><span>)</span> <span>values</span>
  <span>(</span><span>'parent'</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>,</span> <span>'{}'</span><span>),</span>
  <span>(</span><span>'parent'</span><span>,</span> <span>4</span><span>,</span> <span>3</span><span>,</span> <span>'{}'</span><span>),</span>
  <span>(</span><span>'parent'</span><span>,</span> <span>7</span><span>,</span> <span>6</span><span>,</span> <span>'{}'</span><span>),</span>
  <span>(</span><span>'parent'</span><span>,</span> <span>9</span><span>,</span> <span>8</span><span>,</span> <span>'{}'</span><span>),</span>
  <span>(</span><span>'parent'</span><span>,</span> <span>11</span><span>,</span> <span>10</span><span>,</span> <span>'{}'</span><span>);</span>
</code></pre></div>

<h2 id="simple-queries---finding-parents-and-children">Simple queries - finding parents and children</h2>

<p>We can find someone’s parents by joining the <code>edges</code> table to the <code>nodes</code> table.</p>

<p>Hardcoding the child id (3) to find Samwise’s parents:</p>

<div><pre><code><span>select</span> <span>parent</span><span>.</span><span>name</span> <span>from</span> <span>nodes</span> <span>child</span>
<span>join</span> <span>edges</span> <span>on</span> <span>edges</span><span>.</span><span>to_id</span> <span>=</span> <span>child</span><span>.</span><span>id</span>
<span>join</span> <span>nodes</span> <span>parent</span> <span>on</span> <span>edges</span><span>.</span><span>from_id</span> <span>=</span> <span>parent</span><span>.</span><span>id</span>
<span>where</span> <span>child</span><span>.</span><span>id</span> <span>=</span> <span>3</span><span>;</span>
</code></pre></div>

<p>If we didn’t know Samwise’s id, we could find him by name first using a CTE:</p>

<div><pre><code><span>with</span> <span>child</span> <span>as</span> <span>(</span><span>select</span> <span>id</span> <span>from</span> <span>nodes</span> <span>where</span> <span>name</span> <span>=</span> <span>'Samwise Gamgee'</span><span>)</span>
<span>select</span> <span>parent</span><span>.</span><span>name</span> <span>from</span> <span>child</span>
<span>join</span> <span>edges</span> <span>on</span> <span>edges</span><span>.</span><span>to_id</span> <span>=</span> <span>child</span><span>.</span><span>id</span>
<span>join</span> <span>nodes</span> <span>parent</span> <span>on</span> <span>edges</span><span>.</span><span>from_id</span> <span>=</span> <span>parent</span><span>.</span><span>id</span><span>;</span>
</code></pre></div>

<h2 id="more-complex-queries---friends-of-friends">More complex queries - friends of friends</h2>

<p>Not all of our characters knew each other directly. The friendship graph is a bit more complex:</p>

<div><pre><code><span>insert</span> <span>into</span> <span>edges</span> <span>(</span><span>type</span><span>,</span> <span>from_id</span><span>,</span> <span>to_id</span><span>,</span> <span>details</span><span>)</span> <span>values</span>
  <span>-- Everyone in the fellowship is friends with everyone else</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>1</span><span>,</span> <span>3</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Frodo and Sam</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>1</span><span>,</span> <span>5</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Frodo and Gandalf</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>1</span><span>,</span> <span>6</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Frodo and Aragorn</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>1</span><span>,</span> <span>8</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Frodo and Legolas</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>1</span><span>,</span> <span>10</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Frodo and Gimli</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>3</span><span>,</span> <span>1</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Sam and Frodo</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>3</span><span>,</span> <span>5</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Sam and Gandalf</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>3</span><span>,</span> <span>6</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Sam and Aragorn</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>3</span><span>,</span> <span>8</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Sam and Legolas</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>3</span><span>,</span> <span>10</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Sam and Gimli</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>5</span><span>,</span> <span>1</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Gandalf and Frodo</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>5</span><span>,</span> <span>3</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Gandalf and Sam</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>5</span><span>,</span> <span>6</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Gandalf and Aragorn</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>5</span><span>,</span> <span>8</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Gandalf and Legolas</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>5</span><span>,</span> <span>10</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Gandalf and Gimli</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>6</span><span>,</span> <span>1</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Aragorn and Frodo</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>6</span><span>,</span> <span>3</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Aragorn and Sam</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>6</span><span>,</span> <span>5</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Aragorn and Gandalf</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>6</span><span>,</span> <span>8</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Aragorn and Legolas</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>6</span><span>,</span> <span>10</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Aragorn and Gimli</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>8</span><span>,</span> <span>1</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Legolas and Frodo</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>8</span><span>,</span> <span>3</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Legolas and Sam</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>8</span><span>,</span> <span>5</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Legolas and Gandalf</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>8</span><span>,</span> <span>6</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Legolas and Aragorn</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>8</span><span>,</span> <span>10</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Legolas and Gimli</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>10</span><span>,</span> <span>1</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Gimli and Frodo</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>10</span><span>,</span> <span>3</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Gimli and Sam</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>10</span><span>,</span> <span>5</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Gimli and Gandalf</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>10</span><span>,</span> <span>6</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Gimli and Aragorn</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>10</span><span>,</span> <span>8</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Gimli and Legolas</span>
  <span>-- Bilbo was friends with Hamfast and Gandalf</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>2</span><span>,</span> <span>4</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Bilbo and Hamfast</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>2</span><span>,</span> <span>5</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Bilbo and Gandalf</span>
  <span>-- And vice versa</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>4</span><span>,</span> <span>2</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Hamfast and Bilbo</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>5</span><span>,</span> <span>2</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Gandalf and Bilbo</span>
  <span>-- Gandalf was friends with Bilbo, Hamfast and Thranduil, but for the sake of</span>
  <span>-- argument let's say he didn't know Gloin or Arathorn</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>5</span><span>,</span> <span>2</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Gandalf and Bilbo</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>5</span><span>,</span> <span>4</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Gandalf and Hamfast</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>5</span><span>,</span> <span>9</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Gandalf and Thranduil</span>
  <span>-- And vice versa</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>2</span><span>,</span> <span>5</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Bilbo and Gandalf</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>4</span><span>,</span> <span>5</span><span>,</span> <span>'{}'</span><span>),</span> <span>-- Hamfast and Gandalf</span>
  <span>(</span><span>'friend'</span><span>,</span> <span>9</span><span>,</span> <span>5</span><span>,</span> <span>'{}'</span><span>);</span> <span>-- Thranduil and Gandalf</span>
</code></pre></div>

<p>We could follow two friendship links using CTEs like before. Let’s say friends of friends of Samwise:</p>

<div><pre><code><span>with</span> <span>samwise</span> <span>as</span> <span>(</span><span>select</span> <span>id</span> <span>from</span> <span>nodes</span> <span>where</span> <span>name</span> <span>=</span> <span>'Samwise Gamgee'</span><span>),</span>
<span>samwise_friends</span> <span>as</span> <span>(</span>
  <span>select</span> <span>distinct</span> <span>edges</span><span>.</span><span>to_id</span> <span>from</span> <span>edges</span>
  <span>join</span> <span>samwise</span> <span>on</span> <span>edges</span><span>.</span><span>from_id</span> <span>=</span> <span>samwise</span><span>.</span><span>id</span> <span>and</span> <span>edges</span><span>.</span><span>type</span> <span>=</span> <span>'friend'</span>
<span>),</span>
<span>friends_of_friends</span> <span>as</span> <span>(</span>
  <span>select</span> <span>distinct</span> <span>edges</span><span>.</span><span>to_id</span> <span>from</span> <span>edges</span>
  <span>join</span> <span>samwise_friends</span> <span>on</span> <span>edges</span><span>.</span><span>from_id</span> <span>=</span> <span>samwise_friends</span><span>.</span><span>to_id</span> <span>and</span> <span>edges</span><span>.</span><span>type</span> <span>=</span> <span>'friend'</span>
<span>)</span>
<span>select</span> <span>name</span> <span>from</span> <span>friends_of_friends</span> <span>join</span> <span>nodes</span> <span>on</span> <span>nodes</span><span>.</span><span>id</span> <span>=</span> <span>friends_of_friends</span><span>.</span><span>to_id</span><span>;</span>
</code></pre></div>

<p>This returns almost everyone, except for Arathorn and Gloin who we’ve decided weren’t friends with anyone Samwise knew
(even though they probably knew Gandalf, in reality).</p>

<p>If we don’t mind building up arbitrarily long chains of CTEs, we can follow any number of edges using this technique.</p>

<p>It would be kind of nice to be able to pass in the root node and the edge paths as parameters though, rather than
dynamically building up the CTEs.</p>

<h2 id="parameterised-recursive-ctes">Parameterised recursive CTEs</h2>

<p>Let’s say we want to find the parents of the friends-of-friends of Samwise.</p>

<p>We could do this naively by building up a CTE for each level of friendship:</p>

<div><pre><code><span>with</span> <span>samwise</span> <span>as</span> <span>(</span><span>select</span> <span>id</span> <span>from</span> <span>nodes</span> <span>where</span> <span>name</span> <span>=</span> <span>'Samwise Gamgee'</span><span>),</span>
<span>samwise_friends</span> <span>as</span> <span>(</span>
  <span>select</span> <span>distinct</span> <span>edges</span><span>.</span><span>to_id</span> <span>from</span> <span>edges</span>
  <span>join</span> <span>samwise</span> <span>on</span> <span>edges</span><span>.</span><span>from_id</span> <span>=</span> <span>samwise</span><span>.</span><span>id</span> <span>and</span> <span>edges</span><span>.</span><span>type</span> <span>=</span> <span>'friend'</span>
<span>),</span>
<span>friends_of_friends</span> <span>as</span> <span>(</span>
  <span>select</span> <span>distinct</span> <span>edges</span><span>.</span><span>to_id</span> <span>from</span> <span>edges</span>
  <span>join</span> <span>samwise_friends</span> <span>on</span> <span>edges</span><span>.</span><span>from_id</span> <span>=</span> <span>samwise_friends</span><span>.</span><span>to_id</span> <span>and</span> <span>edges</span><span>.</span><span>type</span> <span>=</span> <span>'friend'</span>
<span>),</span>
<span>friends_of_friends_parents</span> <span>as</span> <span>(</span>
  <span>select</span> <span>distinct</span> <span>edges</span><span>.</span><span>from_id</span> <span>from</span> <span>edges</span>
  <span>join</span> <span>friends_of_friends</span> <span>on</span> <span>edges</span><span>.</span><span>to_id</span> <span>=</span> <span>friends_of_friends</span><span>.</span><span>to_id</span> <span>and</span> <span>edges</span><span>.</span><span>type</span> <span>=</span> <span>'parent'</span>
<span>)</span>
<span>select</span> <span>name</span> <span>from</span> <span>friends_of_friends_parents</span> <span>join</span> <span>nodes</span> <span>on</span> <span>nodes</span><span>.</span><span>id</span> <span>=</span> <span>friends_of_friends_parents</span><span>.</span><span>from_id</span><span>;</span>
</code></pre></div>

<p>But this quickly becomes unwieldy. Instead, let’s pull the variable bits out, and use a recursive CTE:</p>

<div><pre><code><span>with</span> <span>recursive</span> 
<span>root</span><span>(</span><span>id</span><span>)</span> <span>as</span> <span>(</span><span>select</span> <span>id</span> <span>from</span> <span>nodes</span> <span>where</span> <span>name</span> <span>=</span> <span>'Samwise Gamgee'</span><span>),</span>
<span>paths</span><span>(</span><span>path</span><span>)</span> <span>as</span> <span>(</span><span>values</span> <span>(</span><span>'{friend,friend,parent}'</span><span>::</span><span>text</span><span>[])),</span>
<span>results</span><span>(</span><span>id</span><span>)</span> <span>as</span> <span>(</span>
  <span>select</span> <span>root</span><span>.</span><span>id</span><span>,</span> <span>1</span> <span>as</span> <span>path_index</span> <span>from</span> <span>root</span>
<span>union</span>
  <span>select</span> <span>edges</span><span>.</span><span>from_id</span><span>,</span> <span>path_index</span> <span>+</span> <span>1</span> <span>as</span> <span>path_index</span> <span>from</span> <span>results</span>
  <span>join</span> <span>edges</span> <span>on</span> <span>edges</span><span>.</span><span>to_id</span> <span>=</span> <span>results</span><span>.</span><span>id</span>
  <span>join</span> <span>paths</span> <span>on</span> <span>edges</span><span>.</span><span>type</span> <span>=</span> <span>paths</span><span>.</span><span>path</span><span>[</span><span>path_index</span><span>]</span>
<span>)</span>
<span>select</span> <span>*</span> <span>from</span> <span>results</span>
<span>join</span> <span>nodes</span> <span>on</span> <span>nodes</span><span>.</span><span>id</span> <span>=</span> <span>results</span><span>.</span><span>id</span>
<span>join</span> <span>paths</span> <span>on</span> <span>cardinality</span><span>(</span><span>paths</span><span>.</span><span>path</span><span>)</span> <span>+</span> <span>1</span> <span>=</span> <span>results</span><span>.</span><span>path_index</span><span>;</span>
</code></pre></div>

<p>This can then be parameterised, so we can create this as a prepared statement and call it lots of times with
different root nodes and paths.</p>

<div><pre><code><span>with</span> <span>recursive</span> 
<span>root</span><span>(</span><span>id</span><span>)</span> <span>as</span> <span>(</span><span>select</span> <span>id</span> <span>from</span> <span>nodes</span> <span>where</span> <span>name</span> <span>=</span> <span>?</span><span>),</span>
<span>paths</span><span>(</span><span>path</span><span>)</span> <span>as</span> <span>(</span><span>values</span> <span>(</span><span>?</span><span>::</span><span>text</span><span>[])),</span>
<span>results</span><span>(</span><span>id</span><span>)</span> <span>as</span> <span>(</span>
  <span>select</span> <span>root</span><span>.</span><span>id</span><span>,</span> <span>1</span> <span>as</span> <span>path_index</span> <span>from</span> <span>root</span>
<span>union</span>
  <span>select</span> <span>edges</span><span>.</span><span>from_id</span><span>,</span> <span>path_index</span> <span>+</span> <span>1</span> <span>as</span> <span>path_index</span> <span>from</span> <span>results</span>
  <span>join</span> <span>edges</span> <span>on</span> <span>edges</span><span>.</span><span>to_id</span> <span>=</span> <span>results</span><span>.</span><span>id</span>
  <span>join</span> <span>paths</span> <span>on</span> <span>edges</span><span>.</span><span>type</span> <span>=</span> <span>paths</span><span>.</span><span>path</span><span>[</span><span>path_index</span><span>]</span>
<span>)</span>
<span>select</span> <span>*</span> <span>from</span> <span>results</span>
<span>join</span> <span>nodes</span> <span>on</span> <span>nodes</span><span>.</span><span>id</span> <span>=</span> <span>results</span><span>.</span><span>id</span>
<span>join</span> <span>paths</span> <span>on</span> <span>cardinality</span><span>(</span><span>paths</span><span>.</span><span>path</span><span>)</span> <span>+</span> <span>1</span> <span>=</span> <span>results</span><span>.</span><span>path_index</span><span>;</span>
</code></pre></div>

<p>If we wanted every node along the path, instead of just the end node, we could remove the join to <code>paths</code> in the final
line.</p>

<h2 id="conclusion">Conclusion</h2>

<p>This is one way of representing graph-like data in Postgresql, and querying it in a flexible way.</p>

<p>I’d be very interested in hearing about other techniques or improvements.</p>

</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[0+0 > 0: C++ thread-local storage performance (127 pts)]]></title>
            <link>https://yosefk.com/blog/cxx-thread-local-storage-performance.html</link>
            <guid>43077675</guid>
            <pubDate>Mon, 17 Feb 2025 11:18:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://yosefk.com/blog/cxx-thread-local-storage-performance.html">https://yosefk.com/blog/cxx-thread-local-storage-performance.html</a>, See on <a href="https://news.ycombinator.com/item?id=43077675">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>We'll discuss how to make sure that your access to TLS (thread-local storage) is fast. If you’re interested strictly in TLS
performance guidelines and don't care about the details, <a href="#summary-of-performance-guidelines">skip right to the end</a>
— but be aware that you’ll be missing out on assembly listings of profound emotional depth, which can shake even a cynical,
battle-hardened programmer. If you don’t want to miss out on that — and who would?! — read on, and you shall learn the
computer-scientific insight behind the intriguing inequality 0+0 &gt; 0.</p>
<p>I’ve recently <a href="https://yosefk.com/blog/profiling-in-production-with-function-call-traces.html">published</a> a new
C++ profiler, <a href="https://github.com/yosefk/funtrace">funtrace</a>, which traces function calls &amp; returns as well as
thread state changes, showing an execution timeline like this (the screenshot is from <a href="https://krita.org/">Krita</a>, a
“real-world,” complicated drawing program):</p>
<p><img alt="image8.png" height="776" src="https://yosefk.com/img/funtrace/image8.png" title="a trace of Krita made by funtrace &amp; displayed by vizviewer" width="576"></p>
<p>One thing a software-based tracing profiler needs is a per-thread buffer for traced data. Actually it would waste less memory
for all threads to share the same buffer, and this is how things “should” work in a system with some fairly minimal <a href="https://yosefk.com/blog/profiling-in-production-with-function-call-traces.html#hardware-assisted-tracing">hardware support
for tracing, which I suggested in the funtrace writeup</a>, and which would look roughly like this:</p>
<p><img alt="image5.png" height="336" src="https://yosefk.com/img/funtrace/image5.png" title="A trace writer with a DMA bypassing the CPU cache system" width="556"></p>
<p>But absent such trace data writing hardware, the data must be written using store instructions through the caches<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a>. So many CPUs sharing a trace buffer results in
them constantly yanking lines from each other’s caches in order to append to the buffer, with a spectacular slowdown. And then
you'd need to synchronize updates to the current write position — still more slowdown. A shared buffer can be fine for <a href="https://yosefk.com/blog/delayed-printf-for-real-time-logging.html">user-initiated printing</a>, but it’s too slow for
tracing every call and return.</p>
<p>So per-thread buffers it is — bringing us to C++’s <code>thread_local</code> keyword, which gives each thread its own copy of
a variable in the global scope — perfect for our trace buffers, it would seem. But it turns out that <strong>we need to be
careful with exactly how we use <code>thread_local</code> to keep our variable access time from exploding</strong>, as explained
in the rest of this document.</p>
<p>The C toolchain — not the C++ compiler front-end, but assemblers, linkers and such — is generally quite ossified, with <a href="https://stackoverflow.com/questions/76925604/why-is-the-constructor-of-a-global-variable-not-called-in-a-library">decades-old
linker bugs enshrined as a standard</a><a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>. TLS
is an interesting case when this toolchain was actually given quite the facelift to support a new feature — with the result of
simple, convenient syntax potentially hiding fairly high overhead (contrary to the more typical case of inconvenient syntax, no
new work in the toolchain, and resource use being fairly explicit.)</p>
<p>At first glance, TLS looks wonderfully efficient, with a whole machine register dedicated to making access to these exotic
variables fast, and a whole scheme set up in the linker to use this register. Let’s take this code accessing a
<code>thread_local</code> object named <code>tls_obj</code>:</p>
<pre><code>int get_first() {
  return tls_obj.first_member;
}</code></pre>
<p>This compiles to the following assembly code:</p>
<pre>  <b><span>movl  %fs:<i>tls_obj</i>@tpoff, %eax</span></b>
</pre>
<p>This loads data from the address of <code>tls_obj</code> into the <code>%eax</code> register where the return value should
go. The address of tls_obj is computed by adding the value of the register <code>%fs</code> and the constant offset
<code>tls_obj@tpoff</code>. Here, <code>%fs</code> is the TLS base address register on x86; other machines similarly reserve a
register for this. <code>tls_obj@tpoff</code> is an offset from the base address of the TLS area allocated per thread, and it’s
assigned by the linker such that room is reserved within the TLS area for every <code>thread_local</code> object in the linked
binary. Is this awesome or what?!</p>
<h2 id="constructors">Constructors</h2>
<p>If instead we access a <code>thread_local</code> object with a constructor — let's call it <code>tls_with_ctor</code> — we
get assembly code like this (and this is with <code>-O3</code> – you really don’t want to see the unoptimized version of
this):</p>
<pre>  <span>cmpb  $0, %fs:<b><i>__tls_guard</i></b>@tpoff
  je    .slow_path</span>
  <b><span>movl  %fs:<i>tls_with_ctor</i>@tpoff, %eax</span></b>
  ret
<b>.slow_path:</b>
  // inlined call to __tls_init, which constructs
  // <b>all</b> the TLS variables in this translation unit…
  pushq %rbx
  movq  %fs:0, %rbx
  movb  $1, %fs:<b><i>__tls_guard</i></b>@tpoff
  leaq  <b><i>tls_with_ctor</i></b>@tpoff(%rbx), %rdi
  call  <b><i>Class::Class()</i></b>
  leaq  <b><i>tls_with_ctor2</i></b>@tpoff(%rbx), %rdi
  call  <b><i>Class2::Class2()</i></b>
  // …followed by our function’s code
  <b><span>movl    %fs:<i>tls_with_ctor</i>@tpoff, %eax</span></b>
  popq  %rbx
  ret
</pre>
<p>Our simple access to a register plus offset has evolved to first check a thread-local “guard variable”, and if it’s not yet
set to 1, it now calls the constructors for all of the thread-local objects in the translation unit. (<code>__tls_guard</code>
is an implicitly generated <code>static</code>, per-translation-unit boolean.)</p>
<p>While funtrace’s call/return hooks, which get their trace buffer pointer from TLS, are called all the time, access to
<code>thread_local</code>s should be more rare in “normal” code — so not sure it’s fair to brand this <code>__tls_guard</code>
approach as having “unacceptable overhead.” Of course, the inlining only happens if your thread_local is defined in the same
translation unit where you access it; <strong>accessing an <code>extern thread_local</code> with a constructor involves a
function call</strong>, with the function testing the guard variable of the translation unit where the thread_local is defined.
But with inlining, the fast path is quite fast on a good processor (I come from an embedded background where you usually have
<em>cheap </em>CPUs rather than <em>good</em>, so an extra load and a branch depending on the loaded value shock me more than
they should; a superscalar out-of-order branch-predicting speculatively-executing CPU will handle this just fine.)</p>
<p>What I don’t understand is why. Like, <em>why.</em> Generating this code must have taken a bunch of compiler work; it didn’t
“just happen for free.” Furthermore, the <code>varname@tpoff</code> thing must have involved some linker work; it’s not like
keeping the linker unchanged was a constraint. Why not arrange for the <code>__tls_init</code> function of every translation
unit (the one that got inlined into the slow path above) to be called before a thread’s entry point is called? Because it would
require a little bit of libc or libpthread work?..</p>
<p>I mean, this was done for <em>global constructors</em>. You don’t check whether you called the global constructors of a
translation unit before accessing a global with a constructor (and sure, <em>that </em>would have been even slower than the TLS
init code checking <code>__tls_guard</code>, because it would need to have been a <em>thread-safe</em> guard variable access;
though even <em>this </em>was implemented for calling the constructors of <em>static variables declared inside functions,
</em>see also <code>-fno-threadsafe-statics</code>.) It’s not really harder to do this for TLS constructors than for global
constructors, except that we need <code>pthread_create</code> to call this code, which, why not?..</p>
<p>Is this a deliberate performance tradeoff, benefitting code with lots of thread_locals and starting threads constantly, with
each thread using few of the thread_locals, and some thread_locals having slow constructors? But such code isn't great to begin
with?.. Anyway, I don’t really get why the ugly thing above is generated from <code>thread_local</code>s’ constructors. The way
I handled it in my case is, <strong>funtrace sidesteps the TLS constructor problem by <a href="https://docs.oracle.com/cd/E19683-01/816-1386/chapter3-26/index.html">interposing</a>
<code>pthread_create</code></strong>, and initializing its <code>thread_local</code>s in its pthread_create wrapper.</p>
<p>Note that for whatever reason it was chosen, construct-on-first-use is <a href="https://en.cppreference.com/w/cpp/language/storage_duration">mandated</a> for thread_locals by the C++ standard, as <a href="https://lobste.rs/s/b5dnjh/0_0_0_c_thread_local_storage_performance#c_xxdqud">pointed out by a reader</a>:</p>
<blockquote>
<p>Block variables with static or thread(since C++11) storage duration are initialized the first time control passes through
their declaration (unless their initialization is zero- or constant-initialization, which can be performed before the block is
first entered). On all further calls, the declaration is skipped.</p>
</blockquote>

<p>And now let’s see what happens when we put our thread-local variable, the one without a constructor, into a shared library
(compiling with <code>-fPIC</code> and linking with <code>-shared</code>):</p>
<pre><b><span>push %rbp
mov  %rsp,%rbp
<span>data16 lea <i>tls_obj</i>(%rip),%rdi
data16 data16 callq <i>__tls_get_addr</i>@plt</span>
mov  (%rax),%eax
pop  %rbp</span></b>
retq 
</pre>
<p>All this colorful code is generated instead of what used to be a single <b><span>movl
%fs:<i>tls_obj</i><span data-cites="tpoff">@tpoff</span>, %eax</span></b>. More code was generated than before,
forcing us to <strong><span>spill and restore registers</span></strong>. But the worst part is that our
TLS access now requires <strong><span>a function call</span></strong> — we need
<code>__tls_get_addr</code> to find the TLS area of the currently running shared library.</p>
<p><strong>Why don’t we just use the same code as before — the <code>movl</code> instruction — with the dynamic linker
substituting the right value for <code>tls_obj@tpoff</code>? </strong>This is an honest question; I don’t understand why this
isn’t a job for the dynamic linker like any other kind of dynamic relocation. Is this to save work in libc again?.. Like, for
<code>tls_obj@tpoff</code> to be an offset <em>from the same base address</em> no matter which shared library
<code>tls_obj</code> was linked into, you would need the TLS areas of all the shared libraries to be allocated contiguously:</p>
<ul>
<li>main executable at offset 0</li>
<li>the first loaded .so at the offset <code>sizeof(main TLS)</code></li>
<li>the next one at the offset <code>sizeof(main TLS) + sizeof(first.so TLS)</code></li>
<li>…</li>
</ul>
<p>But for this, libc would need to do this contiguous allocation, and of course you can’t move the TLS data once you’ve
allocated it, since someone might be keeping pointers into it<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a>. So you need to carve out a chunk of the memory space — no biggie with a 64-bit or even
“just” a 48-bit address space, right?.. — and you need to put the executable’s TLS at some magic address with <code>mmap</code>
and then you keep <code>mmap</code>ing the TLS areas of newly loaded .so’s one next to another.</p>
<p>But this now becomes a part of the ABI (“these addresses are reserved for TLS”), and I guess nobody wanted to soil the ABI
this way “just” to make TLS fast for shared libraries?.. In any case, looks like TLS areas are allocated non-contiguously and so
you need a different base address every time and you can’t use an offset… but <em>still</em>, couldn’t the dynamic linker bake
this address into the code, instead of calling a function to get it?.. Feels to me that this was doable but deemed not worth the
trouble, more than it being impossible, though maybe I’m missing something.</p>
<p>A curious bit is those <code>data16</code><a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>
in the code:</p>
<pre><b><span>data16 lea <i>tls_obj</i>(%rip),%rdi
data16 data16 callq <i>__tls_get_addr</i>@plt</span></b>
</pre>
<p>What is this for?.. Actually, the <code>data16</code> prefix does nothing in this context except padding the instructions to
take more space, making things slightly slower still, though it’s peanuts compared to the function call. Why does the compiler
put this padding in? Because if you compile with <code>-fPIC</code> but then link the code into an executable, without the
<code>-shared</code>, the function call gets replaced with faster code:</p>
<pre><b><span>push %rbp
mov  %rsp,%rbp
<span>mov  %fs:0x0,%rax
lea  -0x4(%rax),%rax</span>
mov  (%rax),%eax
pop  %rbp</span></b>
retq
</pre>
<p>The generated code is still scarred with the <strong><span>register spilling</span></strong> and
what-not, and we don’t get our simple <b><span>movl %fs:<i>tls_obj</i><span data-cites="tpoff">@tpoff</span>, %eax</span></b> back, but still, we have to be very thankful for the compiler &amp; linker
work here, done for the benefit of the <em>many </em>people whose build system compiles everything with <code>-fPIC</code>,
including code that is then linked without <code>-shared</code> (because who knows if the .o will be linked into a shared
library or an executable? It’s not like the build system knows <em>the entire graph of build dependencies — </em>wait, it
actually <em>does — </em>but still, it obviously shouldn’t be <em>bothered </em>to find out if -fPIC is needed — this type of
mundane concern would just distract it from its noble goal of Scheduling a Graph of Completely Generic Tasks. Seriously, no C++
build system out there stoops to this - not one, and goodness knows there are A LOT of them.)</p>
<p>In any case, the <code>data16</code>s are generated by the compiler to make the red instructions take enough space for the
green instructions to fit into, in case we link without <code>-shared</code> after all.</p>

<p>And now let’s see what happens if we put (1) a thread_local object with (2) a constructor into a shared library, for a fine
example of how 2 of C++’s famously “zero-overhead” features compose. We’ve all heard how “the whole is greater than the sum of
its parts,” occasionally expressed by the peppier HRy people as “1 + 1 = 3.” I suggest a similarly inspiring expression “0 + 0
&gt; 0”, which quite often applies to “zero overhead”:</p>
<pre>sub  $0x8,%rsp
<b><span>callq</span> <i>TLS init function for tls_with_ctor</i></b>@plt
data16 lea <b><i>tls_with_ctor</i></b>(%rip),%rdi
data16 data16 <b><span>callq</span> <i>__tls_get_addr</i></b>@plt
mov  (%rax),%eax
add  $0x8,%rsp
retq
</pre>
<p>So, now we have 2 function calls — one for calling the constructor in case it wasn’t called yet, and another to get the
address of the <code>thread_local</code> variable from its ID. Makes sense, except that I recall that under <code>-O3</code>,
this “TLS init function” business was inlined, and now it no longer is? Say, I wonder what code got generated for this “TLS init
function”?..</p>
<pre>  subq  $8, %rsp
  leaq  <b><i>__tls_guard</i></b>@tlsld(%rip), %rdi
  <b><span>call</span>  <i>__tls_get_addr</i></b>@PLT
  cmpb  $0, <b><i>__tls_guard@dtpoff</i></b>(%rax)
  je    .slow_path
  addq  $8, %rsp
  ret
<b>.slow_path:</b>
  movb  $1, <b><i>__tls_guard</i></b>@dtpoff(%rax)
  data16  leaq  <b><i>tls_with_ctor</i></b>@tlsgd(%rip), %rdi
  data16 data16 <b><span>call</span>  <i>__tls_get_addr</i></b>@PLT
  movq  %rax, %rdi
  call  <b><i>Class::Class</i></b>()@PLT
  data16  leaq  <b><i>tls_with_ctor2</i></b>@tlsgd(%rip), %rdi
  data16 data16 <b><span>call</span>  <i>__tls_get_addr</i></b>@PLT
  addq  $8, %rsp
  movq  %rax, %rdi
  jmp   <b><i>Class2::Class2</i></b>()@PLT
</pre>
<p>Oh boy. So not only doesn’t this thing get inlined, but it calls <code>__tls_get_addr</code> <em>again, <strong>even on the
fast path</strong>. </em>And then you have the slow path, which calls __tls_get_addr <em>again and again</em>…not that we care,
it runs just once, but it kinda shows that this __tls_get_addr business doesn’t optimize very well. I mean, it’s not just the
slow path of the init code — here’s how a function accessing 2 thread_local objects with constructors looks like:</p>
<pre>pushq   %rbx
<b><span>call</span></b>    TLS init function for tls_with_ctor@PLT
data16 leaq tls_with_ctor@tlsgd(%rip), %rdi
data16 data16 <b><span>call</span></b> __tls_get_addr@PLT
movl    (%rax), %ebx
<b><span>call</span></b>    TLS init function for tls_with_ctor2@PLT
data16 leaq tls_with_ctor2@tlsgd(%rip), %rdi
data16 data16 <b><span>call</span></b> __tls_get_addr@PLT
addl    (%rax), %ebx
movl    %ebx, %eax
popq    %rbx
</pre>
<p>Like… man. This calls __tls_get_addr <span><strong><em>4 times</em></strong></span>, twice per
accessed thread_local (once directly, and once from the “TLS init functions”).</p>
<p>Why do we call <em>2 </em>“TLS init function for whatever” when <em>both </em>do the same thing — check the guard variable
and run the constructors of <em>all </em>objects in the translation unit (and in this case the two objects are defined in the
same translation unit, the same one where the function is defined)? Is it because in the general case, the two objects come from
2 different translation units, and nobody bothered to optimize the case where they’re from the same one? I guess I can see how
that could happen…</p>
<p>And what about the __tls_get_addr calls to get the addresses of the objects themselves? Why call <em>that </em>twice? Why not
call something just once that gives you the base address of the module’s TLS, and then add offsets to it? Is it because in the
general case, the two objects could come from 2 different shared libraries? Or is this actually done to help the case when we
compile with <code>-fPIC</code> but link without <code>-shared</code>, so that we can neatly replace both calls to
__tls_get_addr with cheaper instructions computing the offset — remember how we thanked compiler &amp; linker people for doing
that? Whereas with a single call to <code>__tls_get_module_base</code> or whatever we’d call it, we wouldn’t have room to put
the faster instruction sequences in at link time (but I guess we could use some NOPW sequence for that instead, but nobody
bothered)? A case of “no good deed goes unpunished,” or rather “no improvement to the performance of executables goes without a
degradation of the performance of shared libraries?..”</p>
<p>And BTW, with clang 20 (the latest version ATM), it’s seemingly enough for <em>one </em>thread-local object in a translation
unit to have a constructor for the compiler to generate a “TLS init function” for <em>every </em>thread-local object, and call
it when the object is accessed… so, seriously, <strong>don’t </strong>use <code>thread_local</code> with constructors, even if
you don’t care about the overhead, as long as there’s even one thread_local object where you <em>do </em>care about access
time.</p>
<p>What does <code>__tls_get_addr</code> do? Here’s the fast path:</p>
<pre>mov  %fs:DTV_OFFSET, %RDX_LP
mov  GL_TLS_GENERATION_OFFSET+_rtld_local(%rip), %RAX_LP
cmp  %RAX_LP, (%rdx)
jne  .slow_path
mov  TI_MODULE_OFFSET(%rdi), %RAX_LP
salq $4, %rax
movq (%rdx,%rax), %rax
cmp  $-1, %RAX_LP
je   .slow_path
add  TI_OFFSET_OFFSET(%rdi), %RAX_LP
ret
</pre>
<p>These 11 instructions on the fast path enable lazy allocation of a shared library’s TLS — every thread only allocates a TLS
for a given shared library upon its first attempt to access one of its thread-local variables. (Each “variable ID” passed to
<code>__tls_get_addr</code> is a pointer to a struct with module ID and an offset within that module’s TLS;
<code>__tls_get_addr</code> checks whether TLS was allocated for the module, and if it wasn’t, calls
<code>__tls_get_addr_slow</code> in order to allocate it.)</p>
<p>Is this lazy allocation the answer to why the whole thing is so slow? Do we <em>really</em> want to only call constructors
for thread-local variables upon first use, and ideally to even allocate memory for them upon first use? Note that we allocate
memory <em>for all the thread_locals <strong>in a shared library</strong> </em>upon the first use of even one; but we call
constructors <em>for all the thread_locals <strong>in a translation unit </strong></em>upon the first use of even one; which is
a bit random for the C++ standard to prescribe, not to mention that it doesn’t really concern itself with dynamic loading? So
it’s more, the standard gave implementations room to do this, rather than prescribed them to do this?.. I don’t know about you,
but I’d prefer a contiguous allocation for all the TLS areas of all the modules in all the threads, and fast access to the
variables over this lazy allocation and initialization; I wonder if this was a deliberate tradeoff or “just how things ended up
being.”</p>
<h2 id="summary-of-performance-guidelines">Summary of performance guidelines</h2>
<ul>
<li>Access to <code>thread_local</code> objects without constructors linked into an executable is <em>very</em> efficient</li>
<li>Constructors make this slower…</li>
<li>Especially if you access an <code>extern thread_local</code> from another translation unit…</li>
<li>Separately from constructors, compiling with <code>-fPIC</code> also makes TLS access slower…</li>
<li>…and linking code compiled with <code>-fPIC</code> with the <code>-shared</code> flag makes it <em>seriously </em>slower,
worse than either constructors or compiling with <code>-fPIC</code>...</li>
<li>…but constructors together with <code>-fPIC -shared</code> <em>really </em>takes the cake and is the slowest by far!</li>
<li>…and actually, a thread_local variable x having a constructor might slow down access to a thread_local variable y in the
same translation unit</li>
<li>Prefer putting the data into one thread_local object rather than several when you can (true for globals, too, BTW.) It can’t
hurt, and it can probably help a lot, by having fewer calls to <code>__tls_get_addr</code> if your code is linked into a shared
library.</li>
</ul>
<h2 id="future-work">Future work</h2>
<p>It annoys me to no end that the funtrace runtime has to be linked into the executable to avoid the price of
<code>__tls_get_addr</code>. (This also means that funtrace must export its runtime functions from the executable, which
precludes shared libraries using the funtrace runtime API (for taking trace snapshots) from linking with
<code>-Wl,--no-undefined</code>.)</p>
<p>I just want a tiny thread-local struct. It can’t be that I can’t do that efficiently without modifying the executable, so
that for instance a Python extension module can be traced without recompiling the Python executable. Seriously, there’s a limit
to how idiotic things should be able to get.</p>
<p>I’m sure there’s some dirty trick or other, based on knowing the guts of libc and other such, which, while dirty, is going to
work for a long time, and where you can reasonably safely detect if it stopped working and upgrade it for whatever changes the
guts of libc will have undergone. If you have an idea, please share it! If not, I guess I’ll get to it one day; I released
funtrace before getting around to this bit, but generally, working around a large number of stupid things like this is a big
chunk of what I do.</p>
<h2 id="knowing-what-you-shouldnt-know">Knowing what you shouldn’t know</h2>
<p>If I manage to stay out of trouble, it’s rarely because of knowing that much, but more because I’m relatively good at 2 other
things: knowing what I don’t know, and knowing what I shouldn’t know. To look at our example, you could argue that the above
explanations are shallower than they could be — I ask why something was done instead of looking up the history, and I only
briefly touch on what <code>TI_MODULE_OFFSET</code> and <code>TI_OFFSET_OFFSET</code> (yes, TI_OFFSET_OFFSET) are, and I don’t
say a word about GL_TLS_GENERATION_OFFSET, for example, and I <em>could.</em></p>
<p>I claim that the kind of things we saw around __tls_get_addr is an immediate red flag along the lines of, yes I am looking
into low-level stuff, but no, nothing good will come out of knowing this particular bit very well in the context that I’m in
right now; maybe I’ll be forced to learn it sometime, but right now this looks exactly like stuff I should avoid rather than
stuff I should learn.</p>
<p>I don’t know how to generalize the principle to make it explicit and easy to follow. All I can say right now is that the next
section has examples substantiating this feeling; you mainly want to avoid <code>__tls_get_addr</code>, because even people who
know it very well, because they maintain it and everything related to it, run into problems with it.</p>
<p>I’ve recently been seeing the expression “anti-intellectualism” used by people criticizing arguments along the lines of “this
is too complex for me to understand, so this can’t be good.” While I agree that we want some more concrete argument about why
something isn’t worth understanding than “I don’t get it, and I <em>would </em>get it if it was any good,” I implore not to call
this “anti-intellectualism,” lest we implicitly crown ourselves as “intellectuals” over the fact that we understand what
TI_OFFSET_OFFSET is. It’s ridiculous enough that we’re called “knowledge workers,” when the “knowledge” referred to in this
expression is the knowledge of what TI_OFFSET_OFFSET is.</p>

<p>Like I said, it annoys me to no end that TLS access is slow for variables defined in shared libraries. Readers suggested
quite a few workarounds, "dirty" to varying degrees:</p>
<h3 id="inlining-pthread_getspecific">"Inlining" <code>pthread_getspecific</code></h3>
<p>There's a pthreads API for allocating "thread-specific keys" which is a form of TLS. Calling <code>pthread_getspecific</code>
upon every TLS access isn't any better than calling <code>__tls_get_addr</code>. But <a href="https://x.com/pskocik/status/1891494684863680663">we can "inline" the code of glibc's implementation</a>, and if we can
make sure that our key is the first one allocated, it will take just a couple of assembly instructions (loading a pointer from
<code>%fs</code> with a constant offset, and then loading our data from that pointer):</p>
<pre>#define tlsReg_ (__extension__( \
  { char*r; __asm ("mov %%fs:0x10,%0":"=r"(r)); r; }))

inline void *pxTlsGetLt32_m(pthread_key_t Pk){
  assert(Pk&lt;32);
  return *(void**)(tlsReg_+0x310+sizeof(void*[2])*Pk+8);
}
void* getKey0(void) {
  return pxTlsGetLt32_m(0);
}
</pre>
<p><code>getKey0</code> compiles to:</p>
<pre>  mov  %fs:0x10,%rax
  mov  0x318(%rax),%rax
</pre>
<h3 id="compiling-with--ftls-modelinitial-exec">Compiling with <code>-ftls-model=initial-exec</code></h3>
<p>It <a href="https://news.ycombinator.com/item?id=43078859">turns out</a> that there's something called the "<a href="https://maskray.me/blog/2021-02-14-all-about-thread-local-storage#initial-exec-tls-model-executable-preemptible">initial
exec TLS model</a>", where a TLS access costs you 2 instructions and no function calls:</p>
<pre>movq <b><i>tls_obj</i></b>@GOTTPOFF(%rip), %rax
movl %fs:(%rax), %eax
</pre>
<p>You can also make just some variables use this model with <code>__attribute((tls_model("initial-exec")))</code>, instead of
compiling everything with <code>-ftls-model=initial-exec</code>, which might be very useful since the space for such variables
is a scarce resource as we'll see shortly.</p>
<p>This method is great if you can <code>LD_PRELOAD</code> your library, or link the executable against it so that it becomes
<code>DT_NEEDED</code>. Otherwise, this may or may not work at runtime:</p>
<blockquote>
<p>the shared object generally needs to be an immediately loaded shared object. The linker sets the DF_STATIC_TLS flag to
annotate a shared object with initial-exec TLS relocations.</p>
<p>glibc ld.so reserves some space in static TLS blocks and allows dlopen on such a shared object if its TLS size is small.
There could be an obscure reason for using such an attribute: general dynamic and local dynamic TLS models are not
async-signal-safe in glibc. However, other libc implementations may not reserve additional TLS space for dlopen'ed initial-exec
shared objects, e.g. musl will error.</p>
</blockquote>
<h3 id="faster-__tls_get_addr-with--mtls-dialectgnu2">Faster <code>__tls_get_addr</code> with
<code>-mtls-dialect=gnu2</code></h3>
<p>It turns out there's a faster <code>__tls_get_addr</code> which you can opt into using. This is still too much code for my
taste; but if you're intereseted in the horrible details, you can read <a href="https://news.ycombinator.com/item?id=43079061">the comment where I found out about this</a>.</p>
<h2 id="see-also">See also</h2>
<p>Various compiler and runtime issues make this slow stuff even slower, and it takes a while to get it fixed. If you stay
within the guidelines above, you should avoid such problems; if you don’t, you might have more problems than described above —
including both performance and correctness:</p>
<ul>
<li><a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81501">mulitple calls to __tls_get_addr() with -fPIC</a> (reported in
2017, status: NEW as of 2025). Some highlights from 2022:
<ul>
<li>“We recently upgraded our toolchain from GCC9 to GCC11, and <strong>we're seeing <code>__tls_get_addr</code> take up to 10%
of total runtime</strong> under some workloads, where it was 1-2% before. It seems that some changes to the optimization passes
in 10 or 11 have significantly increased the impact of this problem.”</li>
<li>“I've shown a workaround I used, which might be useful until GCC handle <code>__tls_get_addr()</code> as returning a
constant addresses that doesn't need to be looked up multiple times in a function.“</li>
<li>“Thanks for the patch! I wonder if it would handle coroutines correctly. <strong>Clang has this open bug <a href="https://github.com/llvm/llvm-project/issues/47179">"Compiler incorrectly caches thread_local address across
suspend-points"</a> that is related to this optimization</strong>.”</li>
</ul></li>
<li><a href="https://sourceware.org/bugzilla/show_bug.cgi?id=19924">TLS performance degradation after dlopen</a> (reported in
2016; fixed in libc 2.39 in 2023, backported to older libcs up to 2.34 in 2025):
<ul>
<li>“we have noticed a performance degradation of TLS access in shared libraries. <b>If another shared library that uses TLS is
loaded via dlopen, __tls_get_addr takes significant more time</b>. Once that shared library accesses it's TLS, the performance
normalizes. We do have a use-case where this is actually really significant.”</li>
<li>“elf: Fix slow tls access after dlopen [BZ #19924] In short: __tls_get_addr checks the global generation counter and if the
current dtv is older then _dl_update_slotinfo updates dtv up to the generation of the accessed module. So if the global
generation is newer than generation of the module then __tls_get_addr keeps hitting the slow dtv update path. The dtv update
path includes a number of checks to see if any update is needed and this already causes measurable tls access slow down after
dlopen. It may be possible to detect up-to-date dtv faster. But if there are many modules loaded (&gt; TLS_SLOTINFO_SURPLUS)
then this requires at least walking the slotinfo list. This patch tries to update the dtv to the global generation instead, so
after a dlopen the tls access slow path is only hit once. The modules with larger generation than the accessed one were not
necessarily synchronized before, so additional synchronization is needed.”</li>
<li>“the fix for <a href="https://sourceware.org/bugzilla/show_bug.cgi?id=19924">bug 19924</a> was to update DTV on tls access
up to the global gen count so after an independent dlopen the next tls access updates the DTV gen count instead of falling into
a slow code path over and over again. <strong>this introduced some issues</strong>: update happens now even if the accessed tls
is in an early loaded library that use static tls (l_tls_offset is set), so such access is no longer as-safe and may alloc. some
of this was mitigated by an ugly workaround: “elf: Support recursive use of dynamic TLS in interposed malloc.” a possible better
approach is to expose the gen count of the accessed module directly in the tls_get_addr argument: this is possible on 64bit
targets if we compress modid and offset into one GOT entry and use the other for the gen count when processing DTPMOD and DTPREL
relocs. (then the original logic before the 19924 fix would not slow down after a global gencount bump: we can compare the DTV
gen count to the accessed module gen count. btw we do this with TLSDESC today and thus aarch64 was imho not affected by the
malloc interposition issue.) however i feel this is dancing around a bad design to use the generation count to deal with dlclose
and reused modids. so here is a better approach…”</li>
</ul></li>
</ul>
<p>If you’re not quite following some of the above, this sort of makes my point about <code>__tls_get_addr</code> being
undesirable, though I am not sure how to defend this way of making a point in the general case.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Searchcode.com's SQLite database is probably 6 terabytes bigger than yours (224 pts)]]></title>
            <link>https://boyter.org/posts/searchcode-bigger-sqlite-than-you/</link>
            <guid>43076785</guid>
            <pubDate>Mon, 17 Feb 2025 09:07:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boyter.org/posts/searchcode-bigger-sqlite-than-you/">https://boyter.org/posts/searchcode-bigger-sqlite-than-you/</a>, See on <a href="https://news.ycombinator.com/item?id=43076785">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>searchcode.com’s SQLite database is probably one of the largest in the world, at least for a public facing website. It’s actual size is 6.4 TB. Which is probably 6 terabytes bigger than yours.</p>
<pre><code>-rw-r--r-- 1 searchcode searchcode 6.4T Feb 17 04:30 searchcode.db
</code></pre>
<p>At least, I think its bigger. I have no evidence to the contrary, being by far the largest I have ever heard of. Poking around the internet did not find anyone talking publicly about anything bigger. The largest I could find using any search or LLM was 1 TB (without source), and a few people on HN and Reddit claiming to be working with SQLite database’s around the size of 10’s of gigabytes.</p>
<blockquote>
<p>However lack of evidence does not mean such as thing does not exist, and so if you do have a larger database please let me know, either by mocking me on some comment section somewhere, or by direct abuse using my email below and I will be appropriately scolded.</p>
</blockquote>
<p>Probably more interesting is why searchcode.com has such a large SQLite database. For those who don’t know <a href="https://searchcode.com/">https://searchcode.com/</a> is my side/desperately needs to pay for itself/ passion project, which as the name suggests is a place to search source code. It has multiple sources including, github, bitbucket, codeplex, sourceforge, gitlab and 300+ languages. I have written about it a lot, which probably one of the more interesting posts about building its own index <a href="https://boyter.org/posts/how-i-built-my-own-index-for-searchcode/">https://boyter.org/posts/how-i-built-my-own-index-for-searchcode/</a></p>
<p>searchcode.com itself is now officially a ship of Theseus project, as every part I started with has now been replaced. A brief history,</p>
<ul>
<li>First version released using PHP, CodeIgniter, MySQL, Memcached, Apache2 and Sphinx search</li>
<li>Rewritten using Python, Django, MySQL, Memcached, Sphinx search, Nginx and RabbitMQ</li>
<li>Never publicly released version using Java, MySQL, Memcached, Nginx and Sphinx search</li>
<li>Start of Covid 19 rewritten using Go, MySQL, Redis, Caddy and Manticore search</li>
<li>Replaced Manticore search with <a href="https://boyter.org/posts/how-i-built-my-own-index-for-searchcode/">custom index</a> now the stack consists Go, MySQL, Redis and Caddy</li>
<li>As of a few days ago Go, SQLite, Caddy</li>
</ul>
<p>The constant between everything till now was the use of MySQL as the storage layer. The reasons for using it initially was it was there, I knew how to work it and it would scale along with my needs fairly well. So what changed? If you look at my previous choices you will see there is in general a move to reducing the number of dependencies. The older and more crusty I get the more I appreciate having a single binary I can just deploy. Single binary deploys are very simple to reason about.</p>
<p>So why SQLite? Well mostly because you can compile it directly into the binary, so I can have my single binary deploy. No need to install any dependencies. Why not write my own? I am not confident enough in my abilities to write something like this myself, at least not in any reasonable time frame. While I may be crazy enough to write my own index engine, I am not crazy enough to write my own storage persistence layer.</p>
<p>Looking at embedded databases I had previously used and played around with embedded Go databases such as <a href="https://github.com/etcd-io/bbolt">bbolt</a> but they never worked at the sort of scale I was expecting to deal with, and since the data I was working with was already mostly relational I wanted to stay in the SQL world.</p>
<p>SQLite has not been totally fault free however. Previously I had used it and ended up with the dreaded <code>"database is locked"</code> error till I found a blog mentioning the best solution in Go is to have two connections. The first being limited to the number of CPU’s for reading and another with a single connection for writing. Something like the below achieves this, and ensures you still have excellent read performance while avoiding the aforementioned error.</p>
<div><pre tabindex="0"><code data-lang="go"><span><span><span>dbRead</span>, <span>_</span> <span>:=</span> <span>connectSqliteDb</span>(<span>"dbname.db"</span>)
</span></span><span><span><span>defer</span> <span>dbRead</span>.<span>Close</span>()
</span></span><span><span><span>dbRead</span>.<span>SetMaxOpenConns</span>(<span>runtime</span>.<span>NumCPU</span>())
</span></span><span><span>
</span></span><span><span><span>dbRead</span>, <span>_</span> <span>:=</span> <span>connectSqliteDb</span>(<span>"dbname.db"</span>)
</span></span><span><span><span>defer</span> <span>dbWrite</span>.<span>Close</span>()
</span></span><span><span><span>dbWrite</span>.<span>SetMaxOpenConns</span>(<span>1</span>)</span></span></code></pre></div>
<p>The other issues I had previously had was cross compiling with CGO. I use a ARM Mac for personal development, for the battery life to and performance. However I deploy to Linux instances. Cross compiling such a thing in Go is trivial till you add in CGO and it becomes a major pain in the backside. Thankfully pure Go versions of SQLite exist such as <a href="https://modernc.org/sqlite">https://modernc.org/sqlite</a> which resolves the issue, at the expense of some performance.</p>
<p>With the above issues resolved first on some other projects, on August 13 2024 I started work on the conversion from MySQL to SQLite. Converting over to SQLite allowed me to start by improving database access by using <a href="https://sqlc.dev/">SQLC</a>. For those curious, you write SQL queries, and it converts them into type safe code you can then call. It’s an amazing tool and gets close to the C# Entity Framework level of productivity. I wrote something similar to it a long time ago in PHP but SQLC is a much better executed version of the idea.</p>
<p>Converting over to SQLite was actually fairly simple as a result and I had a proof of concept working locally in a week or two. However I then needed to start looking into the migration and thats where a lot of the pain started.</p>
<p>I had always known that storing code in a database was going to be problematic, simply because of the quantity of content. As a result I had used MySQL’s <code>compress</code> and <code>uncompress</code> functions when storing any code. Not the most ideal solution but it did crunch down the database enough that I could store it on SSD’s most of the time. However SQLite does not have such a function unless you pay (<code>PRAGMA compression = 'zip';</code>) or look to some 3rd party plugin.</p>
<p>I looked around and found the following <a href="https://phiresky.github.io/blog/2022/sqlite-zstd/">https://phiresky.github.io/blog/2022/sqlite-zstd/</a> which looked promising, but given that it came with the warning <code>I wouldn't trust it with my data (yet).</code> and that I would have needed to use a CGO version of SQLite.</p>
<p>I was discussing the issue on the TechZing podcast discord when someone mentioned that while they had not tried compression in SQLite themselves but that I might want to consider compression on the filesystem level. Some searching around showed that this could be very viable option, <a href="https://news.ycombinator.com/item?id=29148198">https://news.ycombinator.com/item?id=29148198</a> <a href="https://trunc.org/learning/compressing-sqlite-with-zfs">https://trunc.org/learning/compressing-sqlite-with-zfs</a>.</p>
<p>So I got to work on a few linux VM’s with attached disks. Fiddling around with mounting disks with different filesystems in Linux is not something I had done since ReiserFS was a thing (I since discovered it has been removed from the kernel). I started looking at ZFS, since thats the only filesystem I knew supported encryption. Knowing nothing about ZFS I was suggested to look at BTRFS since it should in theory be simpler and turned out to be trivial to setup with zstd compression.</p>
<p>Initial tests on a subset of the data showed a pretty drastic compression ratio.</p>
<pre tabindex="0"><code>$ compsize /mnt/btrfs-partition
Processed 1 file, 150753 regular extents (150753 refs), 0 inline.
Type       Perc     Disk Usage   Uncompressed Referenced  
TOTAL       20%      3.8G          18G          18G       
none       100%       71M          71M          71M       
zstd        20%      3.7G          18G          18G  
</code></pre><p>With that done, I exported the entirety of searchcode’s database in preparation of the migration. This was done using a custom Go program hooking into both the previously written SQL logic and the new SQLC code. The conversion was surprisingly easy, with the only thing to keep in mind using transactions around batches of SQLite inserts to maintain insert performance.</p>
<p>I explicitly coded it to back off when the system was under load and after several days had an output 6.4TB SQLite file. This had me worried at first, since as mentioned I could not find any example of anyone working with SQLite at this level. As such I double checked my indexes and then ran the queries searchcode needs against it in a loop to establish all returned quickly. In fact most returned faster than the MySQL instance I was replacing, which stands to reason given I was replacing the network transfer that was happening previously.</p>
<p>With all of the above done I had one last thing to consider. Should I replace the server? searchcode.com was running on an older AMD 5950x. Still a powerful CPU, but I do like to replace the hardware every few years to get as much performance as possible. So I had a look around and decided to upgrade, to a new server. A real server. One with more cores, more RAM, ECC ram, and faster disks such as this from Hetzner <a href="https://www.hetzner.com/dedicated-rootserver/ex130-r/">https://www.hetzner.com/dedicated-rootserver/ex130-r/</a> which interestingly turned out to be an Intel CPU. As such searchcode is now running on a Intel(R) Xeon(R) Gold 5412U with 256 GB of RAM. The increase in RAM allows me to grow the index by some factor (at least 2x) as well as improve it’t false positive match rates by tweaking the bloom filters.</p>
<p>I ordered and with it setup a day later, started migration over the network (this took a day or two) after setting up the new box box with the BRFS partition. Sadly the data turned out to be less compressible than my tests first suggested, but it still fits nicely into the storage I have for it with a fair amount of room to grow, albeit less than I had hoped for.</p>
<pre tabindex="0"><code>$ compsize /mnt/data/searchcode.db
Processed 1 file, 16481352 regular extents (16481360 refs), 0 inline.
Type       Perc     Disk Usage   Uncompressed Referenced  
TOTAL       76%      4.8T         6.3T         6.3T       
none       100%      4.3T         4.3T         4.3T       
zstd        23%      470G         1.9T         1.9T       
</code></pre><p>So with everything setup and ready to do I flipped over the DNS entries to the new server and observed. Nothing appeared to go amiss, and so I left the updated DNS records in place. If you visit <a href="https://searchcode.com/">https://searchcode.com/</a> you should be usingthe brand new code with SQLite backend. This post is already pretty long so I am not going to go through the new functionality that was released, instead saving it for another post.</p>
<p>As for SQLite? Well so far it appears to be working fine? While the schema searchcode uses is fairly simple, avoiding joins where possible and properly indexed I would not have expected SQLite to cope as well as it has. In fact making searchcode.com live with it was a giant leap of faith in it. I had no examples to follow were it to have issues, and while I had not done anything obviously wrong I was still deeply worried about hitting some unexpected issues. So far however this does not seem to be the case, with everything compared to the previous instance being much faster, from searches, to fetching pages and all of the backend processes that run.</p>
<p>As I write/public this indexing is still happening, with the cores nicely indexing away, proving that the index I wrote scales nicely, even if it does abuse the poor garbage collector by allocating memory all over the place as the following log output indicates <code>memoryusage::Alloc = 69731 MB::TotalAlloc = 196538564 MB::Sys = 89425 MB::tNumGC = 48080</code>. Watching all 48 cores light up however always fills my heart with joy. There really is something about watching the blinkenlights.</p>
<p><img src="https://boyter.org/static/sqlite/moar_cores.png" alt="searchcode.com feeding the cores"></p>
<p>So whats next? Well the plan is to double down on searchcode.com this year. I want to expand it out, and get it back to a state of paying for itself, and then to some level of profitability, and expand it out further. I have a heap of ideas on how to do that which will be running though over the coming weeks. I also want to start to focus on other services besides github to index, since they have their own nice search already. If you are running a 3rd party code hosting service, contact me as id love to explore this with you.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Twitter blocks links to Signal messenger (735 pts)]]></title>
            <link>https://www.disruptionist.com/p/elon-musks-x-blocks-links-to-signal</link>
            <guid>43076710</guid>
            <pubDate>Mon, 17 Feb 2025 08:52:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.disruptionist.com/p/elon-musks-x-blocks-links-to-signal">https://www.disruptionist.com/p/elon-musks-x-blocks-links-to-signal</a>, See on <a href="https://news.ycombinator.com/item?id=43076710">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299ff3f7-54c1-435f-8947-d46591ca8b90_1200x627.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299ff3f7-54c1-435f-8947-d46591ca8b90_1200x627.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299ff3f7-54c1-435f-8947-d46591ca8b90_1200x627.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299ff3f7-54c1-435f-8947-d46591ca8b90_1200x627.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299ff3f7-54c1-435f-8947-d46591ca8b90_1200x627.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299ff3f7-54c1-435f-8947-d46591ca8b90_1200x627.jpeg" width="1200" height="627" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/299ff3f7-54c1-435f-8947-d46591ca8b90_1200x627.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:627,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:71374,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299ff3f7-54c1-435f-8947-d46591ca8b90_1200x627.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299ff3f7-54c1-435f-8947-d46591ca8b90_1200x627.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299ff3f7-54c1-435f-8947-d46591ca8b90_1200x627.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299ff3f7-54c1-435f-8947-d46591ca8b90_1200x627.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Elon Musk’s social media platform, X, is currently banning links to “Signal.me,” a URL used by the encrypted messaging service Signal. The “Signal.me” domain is specifically used by the service so that users can send out a quick link to directly contact them through the messaging app.</p><p>X, formerly Twitter, is blocking users from posting a “Signal.me” link through DM, public post, or even in their profile page. When trying to post a Signal link, users receive a variety of different “message failed” prompts depending on what version of the X platform they use (i.e. X for web, X for iPhone, etc.)</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27875dc8-a725-4861-b752-f547b4b37511_578x547.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27875dc8-a725-4861-b752-f547b4b37511_578x547.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27875dc8-a725-4861-b752-f547b4b37511_578x547.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27875dc8-a725-4861-b752-f547b4b37511_578x547.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27875dc8-a725-4861-b752-f547b4b37511_578x547.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27875dc8-a725-4861-b752-f547b4b37511_578x547.png" width="578" height="547" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/27875dc8-a725-4861-b752-f547b4b37511_578x547.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:547,&quot;width&quot;:578,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:187087,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27875dc8-a725-4861-b752-f547b4b37511_578x547.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27875dc8-a725-4861-b752-f547b4b37511_578x547.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27875dc8-a725-4861-b752-f547b4b37511_578x547.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27875dc8-a725-4861-b752-f547b4b37511_578x547.png 1456w" sizes="100vw"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d366154-941d-4f24-b2af-c1c4dbf34be8_552x670.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d366154-941d-4f24-b2af-c1c4dbf34be8_552x670.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d366154-941d-4f24-b2af-c1c4dbf34be8_552x670.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d366154-941d-4f24-b2af-c1c4dbf34be8_552x670.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d366154-941d-4f24-b2af-c1c4dbf34be8_552x670.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d366154-941d-4f24-b2af-c1c4dbf34be8_552x670.png" width="552" height="670" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9d366154-941d-4f24-b2af-c1c4dbf34be8_552x670.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:670,&quot;width&quot;:552,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:452256,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d366154-941d-4f24-b2af-c1c4dbf34be8_552x670.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d366154-941d-4f24-b2af-c1c4dbf34be8_552x670.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d366154-941d-4f24-b2af-c1c4dbf34be8_552x670.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d366154-941d-4f24-b2af-c1c4dbf34be8_552x670.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>“We can’t complete this request because this link has been identified by X or our partners as being potentially harmful,” reads one failure prompt when attempting to post a “Signal.me” link on X.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F275187eb-8c65-44ae-bb8d-e5365b59eca0_377x363.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F275187eb-8c65-44ae-bb8d-e5365b59eca0_377x363.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F275187eb-8c65-44ae-bb8d-e5365b59eca0_377x363.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F275187eb-8c65-44ae-bb8d-e5365b59eca0_377x363.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F275187eb-8c65-44ae-bb8d-e5365b59eca0_377x363.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F275187eb-8c65-44ae-bb8d-e5365b59eca0_377x363.png" width="377" height="363" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/275187eb-8c65-44ae-bb8d-e5365b59eca0_377x363.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:363,&quot;width&quot;:377,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:231077,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F275187eb-8c65-44ae-bb8d-e5365b59eca0_377x363.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F275187eb-8c65-44ae-bb8d-e5365b59eca0_377x363.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F275187eb-8c65-44ae-bb8d-e5365b59eca0_377x363.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F275187eb-8c65-44ae-bb8d-e5365b59eca0_377x363.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>“This request looks like it might be automated” reads another prompt. “To protect our users from spam and other malicious activity, we can’t complete this action right now. Please try again later.”</p><p>An attempt to add a “Signal.me” link in a profile bio resulted in an error message saying “Account update failed. Description is considered malware.”</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0fab4a-c87a-4451-a316-1c829a7ababd_494x241.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0fab4a-c87a-4451-a316-1c829a7ababd_494x241.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0fab4a-c87a-4451-a316-1c829a7ababd_494x241.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0fab4a-c87a-4451-a316-1c829a7ababd_494x241.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0fab4a-c87a-4451-a316-1c829a7ababd_494x241.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0fab4a-c87a-4451-a316-1c829a7ababd_494x241.png" width="494" height="241" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2b0fab4a-c87a-4451-a316-1c829a7ababd_494x241.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:241,&quot;width&quot;:494,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:84908,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0fab4a-c87a-4451-a316-1c829a7ababd_494x241.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0fab4a-c87a-4451-a316-1c829a7ababd_494x241.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0fab4a-c87a-4451-a316-1c829a7ababd_494x241.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0fab4a-c87a-4451-a316-1c829a7ababd_494x241.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>X is also blocking users from clicking existing “Signal.me” links published prior to the URL’s ban.</p><p>Users who attempt to click on a “Signal.me” link already posted to X are currently met with a warning page from X that reads “Warning: this link may be unsafe. The link you are trying to access has been identified by X or our partners as being potentially spammy or unsafe, in accordance with X's URL Policy.“ However, users are given the choice to ignore the warning and click an additional link on the warning page to direct to the original “Signal.me” URL.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe52ebd1f-009c-4ea7-85c2-03eb9ea6178b_630x416.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe52ebd1f-009c-4ea7-85c2-03eb9ea6178b_630x416.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe52ebd1f-009c-4ea7-85c2-03eb9ea6178b_630x416.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe52ebd1f-009c-4ea7-85c2-03eb9ea6178b_630x416.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe52ebd1f-009c-4ea7-85c2-03eb9ea6178b_630x416.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe52ebd1f-009c-4ea7-85c2-03eb9ea6178b_630x416.png" width="630" height="416" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e52ebd1f-009c-4ea7-85c2-03eb9ea6178b_630x416.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:416,&quot;width&quot;:630,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:64100,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe52ebd1f-009c-4ea7-85c2-03eb9ea6178b_630x416.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe52ebd1f-009c-4ea7-85c2-03eb9ea6178b_630x416.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe52ebd1f-009c-4ea7-85c2-03eb9ea6178b_630x416.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe52ebd1f-009c-4ea7-85c2-03eb9ea6178b_630x416.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Its unclear when X blocked “Signal.me” links on the platform. However, it seems like a fairly recent change as users have previously been able to include “Signal.me” links in public posts and in their profile bio. Other Signal links, such as Signal.org, do not appear to be blocked. Only the “Signal.me” URLs which are used to directly contact users through the app appear to be affected.</p><p>It appears that Signal is the only service currently affected. Links to other similar third-party services, such as URLs to contact users on Telegram, are allowed on X.</p><p><span>Security researchers at </span><a href="https://x.com/mysk_co" rel="">Mysk</a><span> first noticed the issue on Sunday night, Feb. 16. They reached out to me via DM and we were able to confirm the various different ways (DM, post, profile bio, etc.) that X was blocking “Signal.me” links.</span></p><p>Signal has been an important tool for journalists over the years as really one of the few services that are truly private. All messages are end-to-end encrypted, everything is stored on device, and no content is kept on any Signal servers in the cloud. If a source wants to reach out to a reporter and be sure their communication would be as confidential as possible, Signal is usually one of the primary methods of choice.</p><p>Needless to say, Signal has been especially important over these past few weeks as federal employees have reached out to journalists to blow the whistle on what Elon Musk’s DOGE have been doing with access to data within numerous government agencies.</p><p>As of publishing, Signal users can still post their Signal handle on X, which users can then copy and paste into the Signal app.</p><p><span>Anyone who has tips or other news that they’d like to securely send my way can reach me on Signal @MattBinder.01 or via my direct “Signal.me” link: </span><a href="https://signal.me/#eu/P01wpUmC4nT2BBTwMrPAw7Nxcp81055tKHGbYwCkQbo7TNzMHPRKWquiWEttS85C" rel="">https://signal.me/#eu/P01wpUmC4nT2BBTwMrPAw7Nxcp81055tKHGbYwCkQbo7TNzMHPRKWquiWEttS85C</a></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Managers given 200 characters to justify not firing nuclear regulators (139 pts)]]></title>
            <link>https://www.npr.org/2025/02/14/nx-s1-5298190/nuclear-agency-trump-firings-nnsa</link>
            <guid>43076137</guid>
            <pubDate>Mon, 17 Feb 2025 07:19:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npr.org/2025/02/14/nx-s1-5298190/nuclear-agency-trump-firings-nnsa">https://www.npr.org/2025/02/14/nx-s1-5298190/nuclear-agency-trump-firings-nnsa</a>, See on <a href="https://news.ycombinator.com/item?id=43076137">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storytext">
      <div id="resg-s1-49184">
            <div data-crop-type="">
        <picture>
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/400/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/900/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/1200/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/1600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/1800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/400/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/900/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/1200/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/1600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/1800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/1100/quality/50/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/5184x3888+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fca%2F03%2Fadbc439f465aac866c284b184291%2Fgptempdownload-2.JPG" alt="Members of the Nuclear Emergency Support Team training for a radiological contamination scenario." fetchpriority="high">
        </picture>
</div>
<div>
    <div>
        <p>
                Members of the Nuclear Emergency Support Team training for a radiological contamination scenario.
                <b aria-label="Image credit">
                    
                    National Nuclear Security Administration
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        National Nuclear Security Administration
        
    </span>
</p></div>
   </div>
   <p>Scenes of confusion and chaos unfolded over the last two days at the civilian agency that oversees the nation's nuclear weapons stockpile, as the Trump administration's mass firings were carried out before being "paused" on Friday.</p>   <p>This account of firings at the <a href="https://www.npr.org/2024/10/28/nx-s1-5119933/meet-americas-secret-team-of-nuclear-first-responders" target="_blank">National Nuclear Security Administration</a> (NNSA) is based on interviews with several current and former NNSA employees who asked to remain anonymous, fearing retribution from the Trump administration.</p>   <p>Officials were given hours to fire hundreds of employees, and workers were shut out of email as termination notices arrived. The terminations were part of a broader group of dismissals at the Department of Energy, where reportedly more than a thousand federal workers were terminated. It was all a result of Elon Musk's <a href="https://www.npr.org/2025/02/11/nx-s1-5293504/trump-musk-doge-oval-office" target="_blank">Department of Government Efficiency (DOGE)</a> initiative to slash the federal workforce and what Musk and President Trump characterize as excessive government spending. </p>   
   <p>The NNSA is a semi-autonomous agency within the Department of Energy that oversees the U.S. stockpile of thousands of nuclear weapons. Despite having the words "National" and "Security" in its title, it was not getting an exemption for national security, managers at the agency were told last Friday, according to an employee at NNSA who asked not to be named, fearing retribution from the Trump administration. Just days before, officials in leadership had scrambled to write descriptions for the roughly 300 probationary employees at the agency who had joined the federal workforce less than two years ago.</p>   
   
<!-- END ID="RESNX-S1-5298190-100" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>Managers were given just 200 characters to explain why the jobs these workers did mattered.</p>   <p>It was a tall order for the relatively obscure civilian agency that conducts a wide variety of nuclear security missions, including servicing the nation's nuclear weapons when they're not on missiles and bombers, and making extensive safety and security upgrades of the warheads. Some workers were responsible for making sure emergency response plans were in place at sites like a giant facility in Texas, where thousands of dismantled warheads are stored. Others worked to prevent terrorists and rogue nations from acquiring weapons-grade plutonium or uranium. Many had "Q" clearances, the highest level security clearance at the Department of Energy.</p>   
   <p>The Department of Energy's press office did not respond to repeated requests by NPR for comment.</p>   <p>In the final days leading up to the firings, managers drew up lists of essential workers and pleaded to keep them.</p>   <p>In the end, it didn't matter. On Thursday, officials were told that the vast majority of the exemptions they had asked for were denied by the Trump administration. Multiple current and former employees at the agency told NPR that scores of people were notified verbally they were fired. Many had to clear out their desks on the spot. "It broke my heart," says one employee who was among those who left the agency's Washington, D.C., headquarters.</p>   <div id="resg-s1-49185">
            <div data-crop-type="">
        <picture>
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/400/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/900/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/1200/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/1600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/1800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/400/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/900/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/1200/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/1600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/1800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/1100/quality/50/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2400x1585+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F7e%2F7f%2F68909b7f43e4aa32104d8d42373c%2Fap100319140941.jpg" alt="Two employees from the National Nuclear Security Administration use gamma detectors to test the outside of a container of highly-enriched uranium for surface contamination at the Charleston Weapons Station near Goose Creek, S.C. on March 19, 2010." loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Two employees from the National Nuclear Security Administration use gamma detectors to test the outside of a container of highly-enriched uranium for surface contamination at the Charleston Weapons Station near Goose Creek, S.C. on March 19, 2010.
                <b aria-label="Image credit">
                    
                    Mic Smith/AP
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Mic Smith/AP
        
    </span>
</p></div>
   </div>
   <p>But what followed was even more confusing. Employees were told they would receive a letter confirming their termination.</p>   
   
<!-- END ID="RESNX-S1-5298190-101" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>Some did, a letter seen by NPR came late at night, and minutes later their work email ceased to function. "Per OPM [Office of Personnel Management] instructions, DOE finds that your further employment would not be in the public interest," it read in part. The text matched other letters seen by NPR that were sent to employees throughout the Department of Energy. The NNSA termination letter did not appear to make any specific reference to the highly-classified nuclear mission conducted by the agency.</p>   <p>But others at the agency who were told they were terminated never received written notification.</p>   <p>Amid the confusion, employees reached by NPR on Friday were unsure whether they still worked at the agency. "Nobody knows if they're fired or not," said an employee. Two employees still had work laptops and equipment. The laptops did not contain any classified information.</p>   <p>On Friday, an employee still at NNSA told NPR that the firings are now "paused," in part because of the chaotic way in which they unfolded. Another employee had been contacted and told that their termination had been "rescinded." But some worried the damage had already been done. Nuclear security is highly specialized, high-pressure work, but it's not particularly well paid, one employee told NPR. Given what's unfolded over the past 24 hours, "why would anybody want to take these jobs?" they asked.<br></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Word embeddings – Part 3: The secret ingredients of Word2Vec (167 pts)]]></title>
            <link>https://www.ruder.io/secret-word2vec/</link>
            <guid>43075347</guid>
            <pubDate>Mon, 17 Feb 2025 05:02:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ruder.io/secret-word2vec/">https://www.ruder.io/secret-word2vec/</a>, See on <a href="https://news.ycombinator.com/item?id=43075347">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-main">
<article>

    <header>

        

        

            <p>Word2vec is a pervasive tool for learning word embeddings. Its success, however, is mostly due to particular architecture choices. Transferring these choices to traditional distributional methods makes them competitive with popular word embedding methods.</p>

        <section>

            <ul>
                <li>
                    <a href="https://www.ruder.io/author/sebastian/" aria-label="Read more of Sebastian Ruder">
                        <img src="https://www.ruder.io/content/images/size/w100/2023/01/new_profile_photo_square.jpg" alt="Sebastian Ruder">
                    </a>
                </li>
            </ul>

            <div>
                
                <p><time datetime="2016-09-24">24 Sep 2016</time>
                        <span><span>•</span> 10 min read</span>
                </p>
            </div>

        </section>

            <figure>
                <img srcset="https://www.ruder.io/content/images/size/w300/2016/09/merge_from_ofoct--2-.jpg 300w,
                            https://www.ruder.io/content/images/size/w600/2016/09/merge_from_ofoct--2-.jpg 600w,
                            https://www.ruder.io/content/images/size/w1000/2016/09/merge_from_ofoct--2-.jpg 1000w,
                            https://www.ruder.io/content/images/size/w2000/2016/09/merge_from_ofoct--2-.jpg 2000w" sizes="(min-width: 1400px) 1400px, 92vw" src="https://www.ruder.io/content/images/size/w2000/2016/09/merge_from_ofoct--2-.jpg" alt="On word embeddings - Part 3: The secret ingredients of word2vec">
            </figure>

    </header>

    <section>
        <!--kg-card-begin: markdown--><p>This post will discuss the factors that account for the success of word2vec and its connection to more traditional models.</p>
<p>Table of Contents:</p>
<ul>
<li><a href="#glove">GloVe</a></li>
<li><a href="#wordembeddingsvsdistributionalsemanticsmodels">Word embeddings vs. distributional semantic models</a></li>
<li><a href="#models">Models</a></li>
<li><a href="#hyperparameters">Hyperparameters</a></li>
<li><a href="#results">Results</a></li>
</ul>
<p>Excuse the rather clickbait-y title. This is a blog post that I meant to write for a while. In this post, I want to highlight the factors, i.e. the secret ingredients that account for the success of word2vec.<br>
In particular, I want to focus on the connection between word embeddings trained via neural models and those produced by traditional distributional semantics models (DSMs). By showing how these ingredients can be transferred to DSMs, I will demonstrate that distributional methods are in no way inferior to the popular word embedding methods.<br>
Even though this is no new insight, I feel that traditional methods are frequently overshadowed amid the deep learning craze and their relevancy consequently deserves to be mentioned more often.</p>
<p>To this effect, the paper on which this blog post is based is <em>Improving Distributional Similarity with Lessons Learned from Word Embeddings</em> <sup><a href="#fn1" id="fnref1">[1]</a></sup> by Levy et al. (2015). If you haven't read it, I recommend you to check it out.</p>
<p>Over the course of this blog post, I will first introduce GloVe, a popular word embedding model. I will then highlight the connection between word embedding models and distributional semantic methods. Subsequently, I will introduce the four models that will be used to measure the impact of the different factors. I will then give an overview of all additional factors that play a role in learning word representations, besides the choice of the algorithm. I will finally present the results by Levy et al., their takeaways and recommendations.</p>
<h2 id="glove">GloVe</h2>
<p>In a <a href="http://ruder.io/word-embeddings-1/index.html?ref=ruder.io">previous blog post</a>, we have given an overview of popular word embedding models. One model that we have omitted so far is GloVe <sup><a href="#fn2" id="fnref2">[2]</a></sup>.</p>
<p>Briefly, GloVe seeks to make explicit what SGNS does implicitly: Encoding meaning as vector offsets in an embedding space -- seemingly only a serendipitous by-product of word2vec -- is the specified goal of GloVe.<br>
Specifically, the authors of Glove show that the ratio of the co-occurrence probabilities of two words (rather than their co-occurrence probabilities themselves) is what contains information and aim to encode this information as vector differences.<br>
To achieve this, they propose a weighted least squares objective \(J\) that directly aims to minimise the difference between the dot product of the vectors of two words and the logarithm of their number of co-occurrences:</p>
<p>\(J = \sum\limits_{i, j=1}^V f(X_{ij})   (w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \text{log}   X_{ij})^2 \)</p>
<p>where \(w_i\) and \(b_i\) are the word vector and bias respectively of word \(i\), \(\tilde{w}_j\) and \(b_j\) are the context word vector and bias respectively of word \(j\), \(X_{ij}\) is the number of times word \(i\) occurs in the context of word \(j\), and \(f\) is a weighting function that assigns relatively lower weight to rare and frequent co-occurrences.</p>
<p>As co-occurrence counts can be directly encoded in a word-context co-occurrence matrix, GloVe takes such a matrix rather than the entire corpus as input.</p>
<p>If you want to know more about GloVe, the best reference is likely <a href="http://www.aclweb.org/anthology/D14-1162?ref=ruder.io">the paper</a> and the <a href="http://nlp.stanford.edu/projects/glove/?ref=ruder.io">accompanying website</a>. Besides that, you can find some additional intuitions on GloVe and its difference to word2vec by the author of gensim <a href="http://rare-technologies.com/making-sense-of-word2vec/?ref=ruder.io">here</a>, in <a href="https://www.quora.com/How-is-GloVe-different-from-word2vec?ref=ruder.io">this Quora thread</a>, and in <a href="https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html?ref=ruder.io">this blog post</a>.</p>
<h2 id="wordembeddingsvsdistributionalsemanticsmodels">Word embeddings vs. distributional semantics models</h2>
<p>The reason why word embedding models, particularly word2vec and GloVe, became so popular is that they seemed to continuously and significantly outperform DSMs. Many attributed this to the neural architecture of word2vec or the fact that it predicts words, which seemed to have a natural edge over solely relying on co-occurrence counts.</p>
<p>We can view DSMs as <em>count</em> models as they "count" co-occurrences among words by operating on co-occurrence matrices. In contrast, neural word embedding models can be seen as <em>predict</em> models, as they try to predict surrounding words.</p>
<p>In 2014, Baroni et al. <sup><a href="#fn3" id="fnref3">[3]</a></sup> showed that predict models consistently outperform count models in almost all tasks, thus providing a clear verification for the apparent superiority of word embedding models. Is this the end? No.</p>
<p>Already with GloVe we've seen that the differences are not as clear-cut: While GloVe is considered a predict model by Levy et al. (2015), it is clearly factorizing a word-context co-occurrence matrix, which brings it close to traditional methods such as PCA and LSA. Even more, Levy et al. <sup><a href="#fn4" id="fnref4">[4]</a></sup> demonstrate that word2vec implicitly factorizes a word-context PMI matrix.</p>
<p>Consequently, while on the surface DSMs and word embedding models use different algorithms to learn word representations -- the first count, the latter predict -- fundamentally, both types of models act on the same underlying statistics of the data, i.e. the co-occurrence counts between words.</p>
<p>Thus, the question that still remains and which we will dedicate the rest of this blog post to answering is the following:<br>
<em>Why do word embedding models still perform better than DSM with almost the same information?</em></p>
<h2 id="models">Models</h2>
<p>Following Levy et al. (2015), we will isolate and identify the factors that account for the success of neural word embedding models and show how these can be transferred to traditional methods by comparing the following four models:</p>
<ul>
<li>
<p><strong>Positive Pointwise Mutual Information (PPMI)</strong>: PMI is a common measure for the strength of association between two words. It is defined as the log ratio between the joint probability of two words \(w\) and \(c\) and the product of their marginal probabilities: \(PMI(w,c) = \text{log}   \dfrac{P(w,c)}{P(w) P(c)} \). As \( PMI(w,c) = \text{log}   0 = - \infty \) for pairs \( (w,c) \) that were never observed, PMI is in practice often replaced with <em>positive</em> PMI (PPMI), which replaces negative values with \(0\), yielding \(PPMI(w,c) = \text{max}(PMI(w,c),0)\).</p>
</li>
<li>
<p><strong>Singular Value Decomposition (SVD):</strong> SVD is one of the most popular methods for dimensionality reduction and found its into NLP originally via latent semantic analysis (LSA). SVD factories the word-context co-occurrence matrix into the product of three matrices \(U \cdot \Sigma \times V^T \) where \(U\) and \(V\) are orthonormal matrices (i.e. square matrices whose rows and columns are orthogonal unit vectors) and \(\Sigma\) is a diagonal matrix of eigenvalues in<br>
decreasing order. In practice, SVD is often used to factorize the matrix produced by PPMI. Generally, only the top \(d\) elements of \(\Sigma\) are kept, yielding \(W^{SVD} = U_d \cdot \Sigma_d\) and \(C^{SVD} = V_d\), which are typically used as the word and context representations respectively.</p>
</li>
<li>
<p><strong>Skip-gram with Negative Sampling (SGNS)</strong> aka word2vec: To learn more about the skip-gram architecture and the negative sampling refer to my previous blog posts <a href="http://ruder.io/word-embeddings-1/index.html?ref=ruder.io">here</a> and <a href="http://ruder.io/word-embeddings-softmax/index.html?ref=ruder.io">here</a> respectively.</p>
</li>
<li>
<p><strong>Global Vectors (GloVe)</strong> as presented in the previous section.</p>
</li>
</ul>
<h2 id="hyperparameters">Hyperparameters</h2>
<p>We will look at the following hyper-parameters:</p>
<ul>
<li><a href="#preprocessing">Pre-processing</a>
<ul>
<li><a href="#dynamiccontextwindow">Dynamic context window</a></li>
<li><a href="#subsamplingfrequentwords">Subsampling frequent words</a></li>
<li><a href="#deletingrarewords">Deleting rare words</a></li>
</ul>
</li>
<li><a href="#associationmetric">Association metric</a>
<ul>
<li><a href="#shiftedpmi">Shifted PMI</a></li>
<li><a href="#contextdistributionsmoothing">Context distribution smoothing</a></li>
</ul>
</li>
<li><a href="#postprocessing">Post-processing</a>
<ul>
<li><a href="#addingcontextvectors">Adding context vectors</a></li>
<li><a href="#eigenvalueweighting">Eigenvalue weighting</a></li>
<li><a href="#vectornormalisation">Vector normalisation</a></li>
</ul>
</li>
</ul>
<h2 id="preprocessing">Pre-processing</h2>
<p>Word2vec introduces three ways of pre-processing a corpus, which can be easily applied to DSMs.</p>
<h3 id="dynamiccontextwindow">Dynamic context window</h3>
<p>In DSMs traditionally, the context window is unweighted and of a constant size. Both SGNS and GloVe, however, use a scheme that assigns more weight to closer words, as closer words are generally considered to be more important to a word's meaning. In SGNS, this weighting is implemented by having a dynamic window size that is sampled uniformly between \(1\) and the maximum window size during training.</p>
<h3 id="subsamplingfrequentwords">Subsampling frequent words</h3>
<p>SGNS dilutes very frequent words by randomly removing words whose frequency \(f\) is higher than some threshold \(t\) with a probability \(p = 1 - \sqrt{\dfrac{t}{f}}\). As this subsampling is done <em>before</em> actually creating the windows, the context windows used by SGNS in practice are larger than indicated by the context window size.</p>
<h3 id="deletingrarewords">Deleting rare words</h3>
<p>In the pre-processing of SGNS, rare words are also deleted <em>before</em> creating the context windows, which increases the actual size of the context windows further. Levy et al. (2015) find this not to have a significant performance impact, though.</p>
<h2 id="associationmetric">Association metric</h2>
<p>PMI has been shown to be an effective metric for measuring the association between words. Since Levy and Goldberg (2014) have shown SGNS to implicitly factorize a PMI matrix, two variations stemming from this formulation can be introduced to regular PMI.</p>
<h3 id="shiftedpmi">Shifted PMI</h3>
<p>In SGNS, the higher the number of negative samples \(k\), the more data is being used and the better should be the estimation of the parameters. \(k\) affects the shift of the PMI matrix that is implicitly factorized by word2vec, i.e. \(k\) k shifts the PMI values by log \(k\).<br>
If we transfer this to regular PMI, we obtain Shifted PPMI (SPPMI): \(SPPMI(w,c) = \text{max}(PMI(w,c) - \text{log}   k,0)\).</p>
<h3 id="contextdistributionsmoothing">Context distribution smoothing</h3>
<p>In SGNS, the negative samples are sampled according to a <em>smoothed</em> unigram distribution, i.e. an unigram distribution raised to the power of \(\alpha\), which is empirically set to \(\dfrac{3}{4}\). This leads to frequent words being sampled relatively less often than their frequency would indicate.<br>
We can transfer this to PMI by equally raising the frequency of the context words \(f(c)\) to the power of \(\alpha\):<br>
\(PMI(w, c) = \text{log} \dfrac{p(w,c)}{p(w)p_\alpha(c)}\) where \(p_\alpha(c) = \dfrac{f(c)^\alpha}{\sum_c f(c)^\alpha}\) and \(f(x)\) is the frequency of word \(x\).</p>
<h2 id="postprocessing">Post-processing</h2>
<p>Similar as in pre-processing, three methods can be used to modify the word vectors produced by an algorithm.</p>
<h3 id="addingcontextvectors">Adding context vectors</h3>
<p>The authors of GloVe propose to add word vectors and context vectors to create the final output vectors, e.g. \(\vec{v}_{\text{cat}} = \vec{w}_{\text{cat}} + \vec{c}_{\text{cat}}\). This adds first-order similarity terms, i.e \(w \cdot v\). However, this method cannot be applied to PMI, as the vectors produced by PMI are sparse.</p>
<h3 id="eigenvalueweighting">Eigenvalue weighting</h3>
<p>SVD produces the following matrices: \(W^{SVD} = U_d \cdot \Sigma_d \) and \(C^{SVD} = V_d\). These matrices, however, have different properties: \(C^{SVD}\) is orthonormal, while \(W^{SVD}\) is not.<br>
SGNS, in contrast, is more symmetric. We can thus weight the eigenvalue matrix \(\Sigma_d\) with an additional parameter \(p\), which can be tuned, to yield the following:<br>
\(W^{SVD} = U_d \cdot \Sigma_d^p\).</p>
<h3 id="vectornormalisation">Vector normalisation</h3>
<p>Finally, we can also normalise all vectors to unit length.</p>
<h2 id="results">Results</h2>
<p>Levy et al. (2015) train all models on a dump of the English wikipedia and evaluate them on the commonly used word similarity and analogy datasets. You can read more about the experimental setup and training details in their paper. We summarise the most important results and takeaways below.</p>
<h2 id="takeaways">Takeaways</h2>
<p>Levy et al. find that SVD -- and not one of the word embedding algorithms -- performs best on similarity tasks, while SGNS performs best on analogy datasets. They furthermore shed light on the importance of hyperparameters compared to other choices:</p>
<ol>
<li>Hyperparameters vs. algorithms:<br>
Hyperparameter settings are often more important than algorithm choice.<br>
No single algorithm consistently outperforms the other methods.</li>
<li>Hyperparameters vs. more data:<br>
Training on a larger corpus helps for some tasks.<br>
In 3 out of 6 cases, tuning hyperparameters is more beneficial.</li>
</ol>
<h2 id="debunkingpriorclaims">Debunking prior claims</h2>
<p>Equipped with these insights, we can now debunk some generally held claims:</p>
<ol>
<li>Are embeddings superior to distributional methods?<br>
With the right hyperparameters, no approach has a consistent advantage over another.</li>
<li>Is GloVe superior to SGNS?<br>
SGNS outperforms GloVe on all tasks.</li>
<li>Is CBOW a good word2vec configuration?<br>
CBOW does not outperform SGNS on any task.</li>
</ol>
<h2 id="recommendations">Recommendations</h2>
<p>Finally -- and one of the things I like most about the paper -- we can give concrete practical recommendations:</p>
<ul>
<li><strong>DON'T</strong> use shifted PPMI with SVD.</li>
<li><strong>DON'T</strong> use SVD "correctly", i.e. without eigenvector weighting (performance drops 15 points compared to with eigenvalue weighting with \(p = 0.5\)).</li>
<li><strong>DO</strong> use PPMI and SVD with short contexts (window size of \(2\)).</li>
<li><strong>DO</strong> use many negative samples with SGNS.</li>
<li><strong>DO</strong> always use context distribution smoothing (raise unigram distribution to the power of \(\alpha = 0.75\)) for all methods.</li>
<li><strong>DO</strong> use SGNS as a baseline (robust, fast and cheap to train).</li>
<li><strong>DO</strong> try adding context vectors in SGNS and GloVe.</li>
</ul>
<h2 id="conclusions">Conclusions</h2>
<p>These results run counter to what is generally assumed, namely that word embeddings are superior to traditional methods and indicate that it generally makes <em>no difference whatsoever</em> whether you use word embeddings or distributional methods -- what matters is that you tune your hyperparameters and employ the appropriate pre-processing and post-processing steps.</p>
<p>Recent papers from Jurafsky's group <sup><a href="#fn5" id="fnref5">[5]</a></sup> <sup><a href="#fn6" id="fnref6">[6]</a></sup> echo these findings and show that SVD -- not SGNS -- is often the preferred choice when you care about accurate word representations.</p>
<p>I hope this blog post was useful in highlighting cool research that sheds light on the link between traditional distributional semantic and in-vogue embedding models. As we've seen, knowledge of distributional semantics allows us to improve upon our current methods and develop entirely new variations of existing ones. For this reason, I hope that the next time you train word embeddings, you will consider adding distributional methods to your toolbox or lean on them for inspiration.</p>
<p>As always, feel free to ask questions and point out the mistakes I made in this blog post in the comments below.</p>
<h2 id="citation">Citation</h2>
<p>
For attribution in academic contexts or books, please cite this work as:
</p><pre><code>Sebastian Ruder, "On word embeddings - Part 3: The secret ingredients of word2vec". http://ruder.io/secret-word2vec/, 2016.
</code></pre>
<p>BibTeX citation:</p>
<pre><code>@misc{ruder2016secretword2vec,
author = {Ruder, Sebastian},
title = {{On word embeddings - Part 3: The secret ingredients of word2vec}},
year = {2016},
howpublished = {\url{http://ruder.io/secret-word2vec/}},
}
</code></pre>

<h2 id="otherblogpostsonwordembeddings">Other blog posts on word embeddings</h2>
<p>If you want to learn more about word embeddings, these other blog posts on word embeddings are also available:</p>
<ul>
<li><a href="http://ruder.io/word-embeddings-1/index.html?ref=ruder.io">On word embeddings - Part 1</a></li>
<li><a href="http://ruder.io/word-embeddings-softmax/index.html?ref=ruder.io">On word embeddings - Part 2: Approximating the softmax</a></li>
<li><a href="http://ruder.io/cross-lingual-embeddings/index.html?ref=ruder.io">Unofficial Part 4: A survey of cross-lingual embedding models</a></li>
<li><a href="http://ruder.io/word-embeddings-2017/index.html?ref=ruder.io">Unofficial Part 5: Word embeddings in 2017 -  Trends and future directions</a></li>
</ul>
<p>Cover images are courtesy of <a href="http://nlp.stanford.edu/projects/glove/?ref=ruder.io">Stanford</a>.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>Levy, O., Goldberg, Y., &amp; Dagan, I. (2015). Improving Distributional Similarity with Lessons Learned from Word Embeddings. Transactions of the Association for Computational Linguistics, 3, 211–225. Retrieved from <a href="https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570?ref=ruder.io">https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570</a> <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543. <a href="https://doi.org/10.3115/v1/D14-1162?ref=ruder.io">http://doi.org/10.3115/v1/D14-1162</a> <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>Baroni, M., Dinu, G., &amp; Kruszewski, G. (2014). Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. ACL, 238–247. <a href="https://doi.org/10.3115/v1/P14-1023?ref=ruder.io">http://doi.org/10.3115/v1/P14-1023</a> <a href="#fnref3">↩︎</a></p>
</li>
<li id="fn4"><p>Levy, O., &amp; Goldberg, Y. (2014). Neural Word Embedding as Implicit Matrix Factorization. Advances in Neural Information Processing Systems (NIPS), 2177–2185. Retrieved from <a href="http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization?ref=ruder.io">http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization</a> <a href="#fnref4">↩︎</a></p>
</li>
<li id="fn5"><p>Hamilton, W. L., Clark, K., Leskovec, J., &amp; Jurafsky, D. (2016). Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Retrieved from <a href="http://arxiv.org/abs/1606.02820?ref=ruder.io">http://arxiv.org/abs/1606.02820</a> <a href="#fnref5">↩︎</a></p>
</li>
<li id="fn6"><p>Hamilton, W. L., Leskovec, J., &amp; Jurafsky, D. (2016). Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change. arXiv Preprint arXiv:1605.09096. <a href="#fnref6">↩︎</a></p>
</li>
</ol>
</section>
<!--kg-card-end: markdown-->
    </section>

        

</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My Time at MIT (173 pts)]]></title>
            <link>http://muratbuffalo.blogspot.com/2025/02/my-time-at-mit.html</link>
            <guid>43075113</guid>
            <pubDate>Mon, 17 Feb 2025 04:23:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://muratbuffalo.blogspot.com/2025/02/my-time-at-mit.html">http://muratbuffalo.blogspot.com/2025/02/my-time-at-mit.html</a>, See on <a href="https://news.ycombinator.com/item?id=43075113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-1205669765698104829">
<p>Twenty years ago, in 2004-2005, I spent a year at MIT’s Computer Science department as a postdoc working with Professor Nancy Lynch. It was an extraordinary experience. Life at MIT felt like paradise, and leaving felt like being cast out.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoZE-0eOmKIQ1Z9jrcV_5e1KOny0ho0QAt-qP_asRcIKYan9UcCJDNyyHoxv7np-wV2bqoz2GkNTOqV9n4ImSywp2XunMcOq1euWAe1MN91FMDQedy4RyHbUx4zbdDnYXatF5lQFnbj0EXEpRC8xggO3cuC5VgnpB8fApgnKE50tItZGMzjcsolWTw1kk/s580/stata.jpg"><img data-original-height="429" data-original-width="580" height="296" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoZE-0eOmKIQ1Z9jrcV_5e1KOny0ho0QAt-qP_asRcIKYan9UcCJDNyyHoxv7np-wV2bqoz2GkNTOqV9n4ImSywp2XunMcOq1euWAe1MN91FMDQedy4RyHbUx4zbdDnYXatF5lQFnbj0EXEpRC8xggO3cuC5VgnpB8fApgnKE50tItZGMzjcsolWTw1kk/w400-h296/stata.jpg" width="400"></a></p><br><h2>MIT Culture</h2><p>MIT’s Stata Center was the best CS building in the world at the time. Designed by Frank Gehry, it was a striking abstract architecture masterpiece (<a href="https://thetech.com/2010/03/19/statasuit-v130-n14">although like all abstractions it was a bit leaky</a>). Furniture from Herman Miller complemented this design. I remember seeing price tags of $400 on simple yellow chairs.</p><p>The building buzzed with activity. &nbsp;Every two weeks, postdocs were invited to the faculty lunch on Thursdays, and alternating weeks we had group lunches. Free food seemed to materialize somewhere in the building almost daily, and the food trucks outside were also good. MIT thrived on constant research discussions, collaborations, and talks. Research talks were advertised on posters at the urinals, as a practical touch of MIT's hacker culture I guess.</p><p>Our research group occupied the 6th floor, which was home to theory and algorithms. From there, I would see Tim Berners-Lee meeting with colleagues on the floor below. The building’s open spaces and spiral staircases connected every pair of floors to foster interaction. The place radiated strong academic energy. One evening, I saw Piotr Indyk discussing something in front of one of the many whiteboards on the 6th floor. The next morning, he was still there, having spent the night working toward a paper deadline. Eric Demaine was on the same floor too. Once, I accidentally sent a long print job (a PhD thesis) to his office printer, and he was angry because of the wasted paper.</p><p>Nancy Lynch set a great example for us. She is a very detail-oriented person and she was able to find even the tiniest mistakes in the papers with ease. She once told me that her mind worked like a debugger when reading a paper, and these bugs jumped at her. The way she worked with students was that she would dedicate herself solely on a student/paper for the duration of an entire week. That week, she would avoid thinking or listening other works/students, even when she wanted to participate. <a href="https://muratbuffalo.blogspot.com/2013/07/how-i-read-research-paper.html">This is because, she wanted to immerse and keep every parameter about the paper she is working on in her mind, and grok it.</a></p><p>People in Nancy's group were also incredibly sharp—Seth Gilbert, Rui Fan, Gregory Chockler, Cal Newport, and many other students and visiting researchers. Yes, that Cal Newport of "Deep Work" fame was a fresh PhD student back then. Looking back, I regret not making more friends, and not forging deeper connections.</p><h2>Lessons Learned</h2><p>Reflecting on my time at MIT, I wish I had been more intentional, more present, and more engaged. The experience was a gift, but I see now how much more I could have made of it.</p><p>I was young, naive, and plagued by impostor syndrome. I held back instead of exploring more, engaging more deeply, and seeking out more challenges. &nbsp;I allowed myself to be carried along by the current, rather than actively charting my own course. Youth is wasted on the young.</p><p>Why pretend to be smart and play it safe? True understanding is rare and hard-won, so why claim it before you are sure of it? Isn't it more advantageous to embrace your stupidity/ignorance and be underestimated? &nbsp;In research and academia, success often goes not to the one who understands first, but to the one who understands best. Even when speed matters, the real advantage comes from the deep, foundational insights that lead there.</p><p>When you approach work with humility and curiosity, you learn more and participate more fully. Good collaborators value these qualities. A beginner’s mind is an asset. <a href="https://muratbuffalo.blogspot.com/2020/07/the-great-work-of-your-life-by-stephen.html">Staying close to your authentic self helps you find your true calling.</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New junior developers can’t code (104 pts)]]></title>
            <link>https://nmn.gl/blog/ai-and-learning</link>
            <guid>43074852</guid>
            <pubDate>Mon, 17 Feb 2025 03:39:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nmn.gl/blog/ai-and-learning">https://nmn.gl/blog/ai-and-learning</a>, See on <a href="https://news.ycombinator.com/item?id=43074852">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    
    
      <p><em>Something’s been bugging me about how new devs and I need to talk about it.</em></p>

<p>We’re at this weird inflection point in software development. Every junior dev I talk to has Copilot or Claude or GPT running 24/7. They’re shipping code faster than ever. But when I dig deeper into their understanding of what they’re shipping? That’s where things get concerning.</p>

<p>Sure, the code works, but ask why it works that way instead of another way? Crickets. Ask about edge cases? Blank stares.</p>

<p>The foundational knowledge that used to come from struggling through problems is just… missing.</p>

<p>We’re trading deep understanding for quick fixes, and while it feels great in the moment, we’re going to pay for this later.</p>

<!--more-->

<h2 id="back-when-we-had-to-actually-think">Back when we had to actually think</h2>

<p>I recently realized that there’s a whole generation of new programmers who don’t even know what StackOverflow is.</p>

<p>Back when “Claude” was not a chatbot but the man who invented the field of information entropy, there was a different way to debug programming problems.</p>

<p>First, search on Google. Then, hope some desperate soul had posed a similar question as you had. If they did, you’d find a detailed, thoughtful, (and often patronizing) answer from a wise greybeard on this site called <em>“Stack Overflow”</em>.</p>

<p>Here’s <a href="https://stackoverflow.com/questions/12227594/what-is-the-difference-between-unary-plus-numberx-and-parsefloatx" target="_blank">one of the questions I posed 12 years ago</a>. See the top answer by Nathan Wall (who has earned a whopping 10,000+ points and is apparently a <a href="https://www.stacksource.com/" target="_blank">CTO</a> since 2015):</p>

<figure>
<img src="https://nmn.gl/blog/assets/so-qa-12-years.jpg" alt="Stack Overflow question and answer">
<figcaption>
Do yourself a favor and <a href="https://stackoverflow.com/a/13676265/1518029" target="_blank">go read it</a>, you might learn something new.
</figcaption>
</figure>

<p>Can you imagine that some dude just wrote an answer with this level of detail? Raw, without any AI? And for free?</p>

<p>My original question was thoroughly answered of course, but he didn’t stop there. I learnt so many new facts. Some of them I didn’t even know existed, and others I didn’t even want to know but now they’re etched in my brain forever.</p>

<p>This was the best case scenario if you had a question. If you stumbled upon a particularly difficult problem and didn’t find someone who had answered your question already, then tough luck.</p>

<figure>
<img src="https://imgs.xkcd.com/comics/wisdom_of_the_ancients.png" alt="XKCD 979">
<figcaption>
<a href="https://xkcd.com/979/" target="_blank">"Wisdom of the Ancients", XKCD 979</a>
</figcaption>
</figure>

<p>Junior devs these days have it easy. They just go to <a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ" target="_blank">chat.com</a> and copy-paste whatever errors they see. Even lazier ones don’t do the 30 second effort of toggling to a browser window, they just use a tool that does it all in <a href="https://cursor.com/" target="_blank">one place</a>.</p>

<p>It’s convenient and quick.</p>

<p>But, there’s still one reason that StackOverflow was superior:</p>

<p><em>Reading discussions by experienced developers about your topic is the <strong>best</strong> way to learn.</em></p>

<p>Here’s a graph to explain what I mean:</p>

<figure>
<img src="https://nmn.gl/blog/assets/speed-vs-knowledge.jpg" alt="Speed vs knowledge">
</figure>

<p>AI gives you answers, but the knowledge you gain is shallow. With StackOverflow, you had to read multiple expert discussions to get the full picture. It was slower, but you came out understanding not just what worked, but why it worked.</p>

<p>Think about every great developer you know. Did they get that good by copying solutions? No—they got there by understanding systems deeply and understanding other developers’ thought processes. That’s exactly what we’re losing.</p>

<p>I’m not trying to be that guy who complains about “kids these days.” I use AI tools daily. I’m literally <a href="https://nmn.gl/blog/giga">building one</a>. But we need to be honest about what we’re trading away for this convenience.</p>


      
      
      

<h2 id="what-can-we-do">What can we do?</h2>

<p>I’ve been experimenting with ways to fix this (because let’s face it, AI isn’t going anywhere). Here’s what’s actually working:</p>

<ul>
  <li>First, use AI with a learning mindset. When it gives you an answer, interrogate it. Ask it why. Sure, it takes longer, but that’s literally the point.</li>
  <li>Next, find your tribe. Reddit, Discord, Mastodon—wherever the smart people hang out. That’s where you’ll find the real discussions happening. The ones that make you go “huh, I never thought about it that way.”</li>
  <li>Do code reviews differently. Instead of just checking if the code works, start a conversation with your team. What other approaches did they consider? Why did they pick this one? Make understanding the process as important as the end result.</li>
  <li>Build things from scratch sometimes. Yes, AI can generate that authentication system for you. But try building one yourself first. You’ll write worse code, but you’ll understand every line of it. That knowledge compounds.</li>
</ul>

<h2 id="looking-forward">Looking forward</h2>

<p>Here’s the reality: The acceleration has begun and there’s nothing we can do about it. Open source models are taking over, and we’ll have AGI running in our pockets before we know it. But that doesn’t mean we have to let it make us worse developers.</p>

<p>The future isn’t about whether we use AI—it’s about how we use it. And maybe, just maybe, we can find a way to combine the speed of AI with the depth of understanding that we need to learn.</p>

<p>Let me know if you’ve found other ways to balance this. Or tell me I’m just being an old man yelling at clouds. Either way, let’s figure this out together.</p>

    
  </div></div>]]></description>
        </item>
    </channel>
</rss>