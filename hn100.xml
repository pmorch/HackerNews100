<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 19 Jun 2025 10:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[SpaceX Starship 36 Anomaly (152 pts)]]></title>
            <link>https://twitter.com/NASASpaceflight/status/1935548909805601020</link>
            <guid>44315529</guid>
            <pubDate>Thu, 19 Jun 2025 04:49:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/NASASpaceflight/status/1935548909805601020">https://twitter.com/NASASpaceflight/status/1935548909805601020</a>, See on <a href="https://news.ycombinator.com/item?id=44315529">Hacker News</a></p>
Couldn't get https://twitter.com/NASASpaceflight/status/1935548909805601020: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[The Zed Debugger Is Here (276 pts)]]></title>
            <link>https://zed.dev/blog/debugger</link>
            <guid>44314977</guid>
            <pubDate>Thu, 19 Jun 2025 02:42:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zed.dev/blog/debugger">https://zed.dev/blog/debugger</a>, See on <a href="https://news.ycombinator.com/item?id=44314977">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>Over 2,000 developers asked, and we delivered.</p>
<p>Debugging in Zed is now a reality—and it's a big leap toward Zed 1.0.</p>
<h2 id="overview"><a href="#overview" aria-label="Copy heading link"><span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="4" x2="20" y1="9" y2="9"></line><line x1="4" x2="20" y1="15" y2="15"></line><line x1="10" x2="8" y1="3" y2="21"></line><line x1="16" x2="14" y1="3" y2="21"></line></svg></span><span>Overview</span></a></h2>
<p>We set out to build a debugger with three primary focuses:</p>
<ul>
<li>Fast: Spend less time context switching and more time debugging</li>
<li>Familiar: In line with Zed's design language and supports everything expected from a typical debugger flow</li>
<li>Configurable: You're able to customize the UI, keybindings, debug configurations and more</li>
</ul>
<p>Out of the box, Zed supports debugging popular languages including Rust, C/C++, JavaScript, Go, and Python.
With our extension system, Zed can support any debug adapter that implements the <a href="https://microsoft.github.io/debug-adapter-protocol/">Debug Adapter Protocol (DAP)</a>.</p>
<p>To simplify the setup process, we've introduced locators, a system that translates build configurations into debug configurations. Meaning that you can write a build task once in <code>tasks.json</code> and reference it from <code>debug.json</code> — or, even better, rely on Zed's automatic configuration.</p>
<p>Zed automatically runs locators on built-in or language server-generated runnables, so in many cases you won't even need to write a debug configuration to get up and running.</p>
<p>We currently support locators for Cargo, Python, JavaScript, and Go, with more coming in the future.
For more information on configuring a debug session, <a href="https://zed.dev/docs/debugger">see our documentation</a>.</p>
<p>Once in a debug session, Zed makes it easy to inspect your program's state, such as threads, variables, breakpoints, the call stack, and more.</p>
<p><figure><video src="https://customer-snccc0j9v3kfzkif.cloudflarestream.com/fd34bd93b1ec8d7c5554daf0ffdb909e/downloads/default.mp4" width="3280" height="2160" poster="https://zed.dev/img/debugger/zero-setup-poster.webp" controls=""></video><figcaption>Setting some breakpoints and running the test in a debug session.</figcaption></figure></p>
<p>The debugger panel is fully customizable too, just drag and rearrange tabs in whatever order you want; you can even move the debug panel around so it fits your workflow.</p>
<p>Zed also supports keyboard-driven debugging for users that prefer to keep their hands on the keyboard.
You can step through code, toggle breakpoints, and navigate a debug session without ever touching the mouse.</p>
<p><figure><video src="https://customer-snccc0j9v3kfzkif.cloudflarestream.com/642d476eeaab0423087c9c19aab53828/downloads/default.mp4" width="3280" height="2160" poster="https://zed.dev/img/debugger/keyboard-poster.webp" controls=""></video><figcaption>Navigating through the Debugger surfaces using only the keyboard.</figcaption></figure></p>
<h2 id="a-special-thanks"><a href="#a-special-thanks" aria-label="Copy heading link"><span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="4" x2="20" y1="9" y2="9"></line><line x1="4" x2="20" y1="15" y2="15"></line><line x1="10" x2="8" y1="3" y2="21"></line><line x1="16" x2="14" y1="3" y2="21"></line></svg></span><span>A Special Thanks</span></a></h2>
<p>The debugger started as a community-led project with some impressive stats: <a href="https://github.com/zed-industries/zed/pull/13433/commits">8 months of development, 977 commits, and 25k+ lines of code</a>. The community built the core foundation that made today’s launch possible.</p>
<p>Special thanks to <a href="https://github.com/RemcoSmitsDev">Remco Smits</a> for driving a lot of the heavy lifting on this project—your contributions have been critical to getting us here.</p>
<h2 id="under-the-hood"><a href="#under-the-hood" aria-label="Copy heading link"><span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="4" x2="20" y1="9" y2="9"></line><line x1="4" x2="20" y1="15" y2="15"></line><line x1="10" x2="8" y1="3" y2="21"></line><line x1="16" x2="14" y1="3" y2="21"></line></svg></span><span>Under the Hood</span></a></h2>
<p>Zed's debugger supports debugging a variety of languages through the Debug Adapter Protocol.
But simply implementing the protocol wasn't enough—we needed an architecture that could scale to collaborative debugging, support extensions, and efficiently cache and manage responses from debug adapters.</p>
<p>To achieve this, we built a two-layer architecture: a data layer that communicates directly with the debug adapters, and a UI layer that fetches data from the data layer to render the interface.</p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="rust" data-theme="dark-plus light-plus"><span data-line=""><span>/// All functions are cheap to call, as they grab current state of the debug session and schedule refreshing on a background</span></span>
<span data-line=""><span>/// thread if that state is outdated.</span></span>
<span data-line=""><span>pub</span><span> fn</span><span> modules</span><span>(&amp;</span><span>mut</span><span> self</span><span>, </span><span>cx</span><span>: &amp;</span><span>mut</span><span> Context</span><span>&lt;</span><span>Self</span><span>&gt;) -&gt; &amp;[</span><span>Module</span><span>] {</span></span>
<span data-line=""><span>    /// Kick off a fresh request to a DAP for modules list if we don't have an up-to-date state.</span></span>
<span data-line=""><span>    /// This is a no-op in case we've ran that request already. In case we did not, it kicks off a background task.</span></span>
<span data-line=""><span>    self</span><span>.</span><span>fetch</span><span>(</span></span>
<span data-line=""><span>        /// We cache request results based on it's arguments. `Modules` request does not take any arguments</span></span>
<span data-line=""><span>        dap_command</span><span>::</span><span>ModulesCommand</span><span>,</span></span>
<span data-line=""><span>        /// Callback invoked with the result of a request.</span></span>
<span data-line=""><span>        |</span><span>this</span><span>, </span><span>result</span><span>, </span><span>cx</span><span>| {</span></span>
<span data-line=""><span>            let</span><span> Some</span><span>(</span><span>result</span><span>) = </span><span>result</span><span>.</span><span>log_err</span><span>() </span><span>else</span><span> {</span></span>
<span data-line=""><span>                return</span><span>;</span></span>
<span data-line=""><span>            };</span></span>
<span data-line=""> </span>
<span data-line=""><span>            this</span><span>.modules = </span><span>result</span><span>;</span></span>
<span data-line=""><span>            cx</span><span>.</span><span>emit</span><span>(</span><span>SessionEvent</span><span>::</span><span>Modules</span><span>);</span></span>
<span data-line=""><span>            cx</span><span>.</span><span>notify</span><span>();</span></span>
<span data-line=""><span>        },</span></span>
<span data-line=""><span>        cx</span><span>,</span></span>
<span data-line=""><span>    );</span></span>
<span data-line=""> </span>
<span data-line=""><span>    /// Returns a current list of modules; it might be outdated at the time the new request is underway,</span></span>
<span data-line=""><span>    /// but once it is done, the return value of this function will reflect that.</span></span>
<span data-line=""><span>    &amp;</span><span>self</span><span>.modules</span></span>
<span data-line=""><span>}</span></span></code></pre></div></figure>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="rust" data-theme="dark-plus light-plus"><span data-line=""><span>/// This function is called from the Module list render function in the UI layer whenever the data layer invalidates the module list state.</span></span>
<span data-line=""><span>fn</span><span> schedule_rebuild</span><span>(&amp;</span><span>mut</span><span> self</span><span>, </span><span>cx</span><span>: &amp;</span><span>mut</span><span> Context</span><span>&lt;</span><span>Self</span><span>&gt;) {</span></span>
<span data-line=""><span>    /// Setting the task drops any current work in progress that is out of date</span></span>
<span data-line=""><span>    self</span><span>._rebuild_task = </span><span>Some</span><span>(</span><span>cx</span><span>.</span><span>spawn</span><span>(</span><span>async</span><span> move</span><span> |</span><span>this</span><span>, </span><span>cx</span><span>| {</span></span>
<span data-line=""><span>        this</span><span>.</span><span>update</span><span>(</span><span>cx</span><span>, |</span><span>this</span><span>, </span><span>cx</span><span>| {</span></span>
<span data-line=""><span>            /// The UI layer queries the data layer for modules and clones the data</span></span>
<span data-line=""><span>            let</span><span> modules</span><span> = </span><span>this</span></span>
<span data-line=""><span>                .session</span></span>
<span data-line=""><span>                .</span><span>update</span><span>(</span><span>cx</span><span>, |</span><span>session</span><span>, </span><span>cx</span><span>| </span><span>session</span><span>.</span><span>modules</span><span>(</span><span>cx</span><span>).</span><span>to_owned</span><span>());</span></span>
<span data-line=""><span>            this</span><span>.entries = </span><span>modules</span><span>;</span></span>
<span data-line=""><span>            cx</span><span>.</span><span>notify</span><span>();</span></span>
<span data-line=""><span>        })</span></span>
<span data-line=""><span>        .</span><span>ok</span><span>();</span></span>
<span data-line=""><span>    }));</span></span>
<span data-line=""><span>}</span></span></code></pre></div></figure>
<p>This separation means the UI layer only requests what it needs, allowing the data layer to lazily fetch information and avoid unnecessary requests.
It also makes the data layer solely responsible for maintaining session state, caching responses, and invalidating stale data.
This architecture will make implementing collaborative debugging significantly easier, since the same UI code can be reused across multiplayer sessions—and we only send essential data across the wire, preserving bandwidth.</p>
<p>Supporting every debug adapter out of the box wasn't feasible—there are over <a href="https://microsoft.github.io/debug-adapter-protocol/implementors/adapters/">70 DAP implementations</a>, each with its own quirks.
To solve this, we <a href="https://zed.dev/docs/extensions/debugger-extensions">extended</a> Zed's extension API to support debugger integration.</p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="rust" data-theme="dark-plus light-plus"><span data-line=""><span>    /// Returns the debug adapter binary for the specified adapter name and configuration.</span></span>
<span data-line=""><span>    fn</span><span> get_dap_binary</span><span>(</span></span>
<span data-line=""><span>        &amp;</span><span>mut</span><span> self</span><span>,</span></span>
<span data-line=""><span>        _adapter_name</span><span>: </span><span>String</span><span>,</span></span>
<span data-line=""><span>        _config</span><span>: </span><span>DebugTaskDefinition</span><span>,</span></span>
<span data-line=""><span>        _user_provided_debug_adapter_path</span><span>: </span><span>Option</span><span>&lt;</span><span>String</span><span>&gt;,</span></span>
<span data-line=""><span>        _worktree</span><span>: &amp;</span><span>Worktree</span><span>,</span></span>
<span data-line=""><span>    ) -&gt; </span><span>Result</span><span>&lt;</span><span>DebugAdapterBinary</span><span>, </span><span>String</span><span>&gt; {</span></span>
<span data-line=""><span>        Err</span><span>(</span><span>"`get_dap_binary` not implemented"</span><span>.</span><span>to_string</span><span>())</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""> </span>
<span data-line=""><span>    /// Determines whether the specified adapter configuration should *launch* a new debuggee process</span></span>
<span data-line=""><span>    /// or *attach* to an existing one. This function should not perform any further validation (outside of determining the kind of a request).</span></span>
<span data-line=""><span>    /// This function should return an error when the kind cannot be determined (rather than fall back to a known default).</span></span>
<span data-line=""><span>    fn</span><span> dap_request_kind</span><span>(</span></span>
<span data-line=""><span>        &amp;</span><span>mut</span><span> self</span><span>,</span></span>
<span data-line=""><span>        _adapter_name</span><span>: </span><span>String</span><span>,</span></span>
<span data-line=""><span>        _config</span><span>: </span><span>serde_json</span><span>::</span><span>Value</span><span>,</span></span>
<span data-line=""><span>    ) -&gt; </span><span>Result</span><span>&lt;</span><span>StartDebuggingRequestArgumentsRequest</span><span>, </span><span>String</span><span>&gt; {</span></span>
<span data-line=""><span>        Err</span><span>(</span><span>"`dap_request_kind` not implemented"</span><span>.</span><span>to_string</span><span>())</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""><span>    /// Converts a high-level definition of a debug scenario (originating in a new session UI) to a "low-level" configuration suitable for a particular adapter.</span></span>
<span data-line=""><span>    ///</span></span>
<span data-line=""><span>    /// In layman's terms: given a program, list of arguments, current working directory and environment variables,</span></span>
<span data-line=""><span>    /// create a configuration that can be used to start a debug session.</span></span>
<span data-line=""><span>    fn</span><span> dap_config_to_scenario</span><span>(&amp;</span><span>mut</span><span> self</span><span>, </span><span>_config</span><span>: </span><span>DebugConfig</span><span>) -&gt; </span><span>Result</span><span>&lt;</span><span>DebugScenario</span><span>, </span><span>String</span><span>&gt; {</span></span>
<span data-line=""><span>        Err</span><span>(</span><span>"`dap_config_to_scenario` not implemented"</span><span>.</span><span>to_string</span><span>())</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""> </span>
<span data-line=""><span>    /// Locators are entities that convert a Zed task into a debug scenario.</span></span>
<span data-line=""><span>    ///</span></span>
<span data-line=""><span>    /// They can be provided even by extensions that don't provide a debug adapter.</span></span>
<span data-line=""><span>    /// For all tasks applicable to a given buffer, Zed will query all locators to find one that can turn the task into a debug scenario.</span></span>
<span data-line=""><span>    /// A converted debug scenario can include a build task (it shouldn't contain any configuration in such case); a build task result will later</span></span>
<span data-line=""><span>    /// be resolved with [`Extension::run_dap_locator`].</span></span>
<span data-line=""><span>    ///</span></span>
<span data-line=""><span>    /// To work through a real-world example, take a `cargo run` task and a hypothetical `cargo` locator:</span></span>
<span data-line=""><span>    /// 1. We may need to modify the task; in this case, it is problematic that `cargo run` spawns a binary. We should turn `cargo run` into a debug scenario with</span></span>
<span data-line=""><span>    /// `cargo build` task. This is the decision we make at `dap_locator_create_scenario` scope.</span></span>
<span data-line=""><span>    /// 2. Then, after the build task finishes, we will run `run_dap_locator` of the locator that produced the build task to find the program to be debugged. This function</span></span>
<span data-line=""><span>    /// should give us a debugger-agnostic configuration for launching a debug target (that we end up resolving with [`Extension::dap_config_to_scenario`]). It's almost as if the user</span></span>
<span data-line=""><span>    /// found the artifact path by themselves.</span></span>
<span data-line=""><span>    ///</span></span>
<span data-line=""><span>    /// Note that you're not obliged to use build tasks with locators. Specifically, it is sufficient to provide a debug configuration directly in the return value of</span></span>
<span data-line=""><span>    /// `dap_locator_create_scenario` if you're able to do that. Make sure to not fill out `build` field in that case, as that will prevent Zed from running second phase of resolution in such case.</span></span>
<span data-line=""><span>    /// This might be of particular relevance to interpreted languages.</span></span>
<span data-line=""><span>    fn</span><span> dap_locator_create_scenario</span><span>(</span></span>
<span data-line=""><span>        &amp;</span><span>mut</span><span> self</span><span>,</span></span>
<span data-line=""><span>        _locator_name</span><span>: </span><span>String</span><span>,</span></span>
<span data-line=""><span>        _build_task</span><span>: </span><span>TaskTemplate</span><span>,</span></span>
<span data-line=""><span>        _resolved_label</span><span>: </span><span>String</span><span>,</span></span>
<span data-line=""><span>        _debug_adapter_name</span><span>: </span><span>String</span><span>,</span></span>
<span data-line=""><span>    ) -&gt; </span><span>Option</span><span>&lt;</span><span>DebugScenario</span><span>&gt; {</span></span>
<span data-line=""><span>        None</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""> </span>
<span data-line=""><span>    /// Runs the second phase of locator resolution.</span></span>
<span data-line=""><span>    /// See [`Extension::dap_locator_create_scenario`] for a hefty comment on locators.</span></span>
<span data-line=""><span>    fn</span><span> run_dap_locator</span><span>(</span></span>
<span data-line=""><span>        &amp;</span><span>mut</span><span> self</span><span>,</span></span>
<span data-line=""><span>        _locator_name</span><span>: </span><span>String</span><span>,</span></span>
<span data-line=""><span>        _build_task</span><span>: </span><span>TaskTemplate</span><span>,</span></span>
<span data-line=""><span>    ) -&gt; </span><span>Result</span><span>&lt;</span><span>DebugRequest</span><span>, </span><span>String</span><span>&gt; {</span></span>
<span data-line=""><span>        Err</span><span>(</span><span>"`run_dap_locator` not implemented"</span><span>.</span><span>to_string</span><span>())</span></span>
<span data-line=""><span>    }</span></span></code></pre></div></figure>
<p>Adding DAP support via an extension involves defining a custom schema that integrates with our JSON server, implementing logic for downloading and launching the adapter, processing debug configuration to add sane default values, and integrating with locators for automatic configuration.
This design follows our approach to LSP extensions, giving extension authors full control to bring their own debug adapters to Zed with minimal friction.</p>
<p>We also wanted inline variable values to work out of the box.
Surprisingly, the <a href="https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_inlineValue">inline values request</a> is a part of the <a href="https://microsoft.github.io/language-server-protocol/">Language Server Protocol (LSP)</a> instead of the DAP.
Using the inline values approach would limit Zed to only showing inline values for DAPs which integrate with LSPs, which isn't many.
A naive workaround might be to use regular expressions to match variable names between the source code and debugger values, but that quickly breaks down when dealing with scopes, and comments.
Instead, we turned to <a href="https://tree-sitter.github.io/tree-sitter/">Tree-sitter</a>. After all Zed is built by the creators of Tree-sitter!</p>
<div><figure><img src="https://zed.dev/img/debugger/inline-values.webp" alt="An inline value example."><figcaption>An inline value example.</figcaption></figure></div>
<p>Through Tree-sitter queries, we can accurately identify variables within the current execution scope, and easily support any language through <code>.scm</code> files without relying on an LSP server to be tightly integrated with a debug adapter.
At launch, inline values are supported for Python, Rust, and Go.
More languages will be supported in the coming weeks.</p>
<h2 id="whats-next"><a href="#whats-next" aria-label="Copy heading link"><span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="4" x2="20" y1="9" y2="9"></line><line x1="4" x2="20" y1="15" y2="15"></line><line x1="10" x2="8" y1="3" y2="21"></line><line x1="16" x2="14" y1="3" y2="21"></line></svg></span><span>What's Next</span></a></h2>
<p>When we set out to build the debugger, we wanted to make it seamless to use, out of the way, and in line with Zed's high standard of quality.
Now that we've built a strong foundation that is compatible with any debug adapter, we're ready to explore and implement advanced features such as:</p>
<ul>
<li>New views: While we support all the fundamental views, we're planning on adding more advanced views such as a watch list, memory view, disassembly view, and a stack trace view</li>
<li>Automatic configuration: We're going to add support for more languages and build systems</li>
<li>Polish and more: reach out to us <a href="https://discord.com/invite/qSDQ8VWc7k">on Discord</a> or <a href="https://github.com/zed-industries/zed/issues">on Zed's GitHub repo</a> to let us know!</li>
</ul><hr></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TI to invest $60B to manufacture foundational semiconductors in the U.S. (175 pts)]]></title>
            <link>https://www.ti.com/about-ti/newsroom/news-releases/2025/texas-instruments-plans-to-invest-more-than--60-billion-to-manufacture-billions-of-foundational-semiconductors-in-the-us.html</link>
            <guid>44314759</guid>
            <pubDate>Thu, 19 Jun 2025 01:50:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ti.com/about-ti/newsroom/news-releases/2025/texas-instruments-plans-to-invest-more-than--60-billion-to-manufacture-billions-of-foundational-semiconductors-in-the-us.html">https://www.ti.com/about-ti/newsroom/news-releases/2025/texas-instruments-plans-to-invest-more-than--60-billion-to-manufacture-billions-of-foundational-semiconductors-in-the-us.html</a>, See on <a href="https://news.ycombinator.com/item?id=44314759">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-lid="richText_218e"><p><b>NEWS HIGHLIGHTS:</b></p>
<ul>
<li>More than $60 billion investment includes seven U.S. semiconductor fabs across three manufacturing mega-sites in Texas and Utah supporting more than 60,000 new U.S. jobs</li>
<li>Largest investment in foundational semiconductor manufacturing in&nbsp;U.S.&nbsp;history, building on TI’s almost-100-year legacy</li>
<li>TI’s largest mega-site in Sherman, Texas includes investment of up to $40 billion dollars for four fabs: SM1 and SM2 – already underway – and two additional fabs, SM3 and SM4</li>
<li>Leverages TI’s strengths as a global technology and manufacturing leader to advance critical innovations from vehicles to smartphones to data centers</li>
</ul>
<p><b>DALLAS, </b>June 18, 2025 – Texas Instruments (TI) (Nasdaq: TXN) today announced its plans to invest more than $60 billion across seven U.S. semiconductor fabs, making this the largest investment in foundational semiconductor manufacturing in U.S. history. Working with the Trump administration and building on the company’s nearly 100-year legacy, TI is expanding its U.S. manufacturing capacity to supply the growing need for semiconductors that will advance critical innovations from vehicles to smartphones to data centers. Combined, TI’s new manufacturing mega-sites in Texas and Utah will support more than 60,000 U.S. jobs.</p>
<p>“TI is building dependable, low-cost 300mm capacity at scale to deliver the analog and embedded processing chips that are vital for nearly every type of electronic system,” said Haviv Ilan, president and CEO of Texas Instruments. “Leading U.S. companies such as Apple, Ford, Medtronic, NVIDIA and SpaceX rely on TI’s world-class technology and manufacturing expertise, and we are honored to work alongside them and the U.S. government to unleash what’s next in American innovation.”</p>
<p>“For nearly a century, Texas Instruments has been a bedrock American company driving innovation in technology and manufacturing,” said U.S. Secretary of Commerce, Howard Lutnick. “President Trump has made it a priority to increase semiconductor manufacturing in America – including these foundational semiconductors that go into the electronics that people use every day. Our partnership with TI will support U.S. chip manufacturing for decades to come.”</p>
</div><div data-lid="richText_402c"><p><b>Unleashing what’s next in American innovation</b></p>
<p>Today, TI is the largest foundational semiconductor manufacturer in the U.S., producing analog and embedded processing chips that are critical for smartphones, vehicles, data centers, satellites and nearly every other electronic device. In order to meet the steadily growing demand for these essential chips, TI is building on its legacy of technology leadership and expanding its U.S. manufacturing presence to help its customers pioneer the next wave of technological breakthroughs.</p>
<p><b>Igniting intelligence with Apple</b></p>
<p>“Texas Instruments' American-made chips help bring Apple products to life, and together, we’ll continue to create opportunity, drive innovation, and invest in the future of advanced manufacturing across the U.S.,” said Apple’s CEO Tim Cook.</p>
<p><b>Fueling the future with Ford</b></p>
<p>Ford and TI are working to strengthen American manufacturing, combining Ford’s automotive expertise with TI’s semiconductor technology to help drive innovation and secure a robust, domestic supply chain for the future of mobility. “At Ford, 80% of the vehicles we sell in the U.S. are assembled in the U.S., and we are proud to stand with technology leaders like TI that continue to invest in manufacturing in the U.S.,” said Jim Farley, President and CEO of Ford Motor Company.</p>
<p><b>Connecting patient care with Medtronic</b></p>
<p>Medtronic and TI are partnering to improve lives when it matters most. “At Medtronic, our life-saving medical technologies rely on semiconductors to deliver precision, performance, and innovation at scale,” said Geoff Martha, Medtronic chairman and CEO. “Texas Instruments has been a vital partner – especially during the global chip shortages – helping us maintain supply continuity and accelerate the development of breakthrough therapies. We’re proud to leverage TI’s U.S.-manufactured semiconductors as we work to transform healthcare and improve outcomes for patients around the world.”</p>
<p><b>Advancing AI with NVIDIA</b></p>
<p>NVIDIA is partnering with TI to unleash the next generation of artificial intelligence architectures. “NVIDIA and TI share the goal to revitalize U.S. manufacturing by building more of the infrastructure for AI factories here in the U.S.,” said Jensen Huang, founder and CEO of NVIDIA. “We look forward to continuing our collaboration with TI by developing products for advanced AI infrastructure.”</p>
<p><b>Securing high-speed satellite internet with SpaceX</b></p>
<p>SpaceX is increasingly leveraging TI’s high-speed process technology to connect its Starlink satellite internet service with TI’s latest 300mm SiGe technology manufactured in Sherman, Texas. “Our fundamental mission is to revolutionize global connectivity and eliminate the digital divide. Core to this mission is constantly pushing the boundaries of what is possible,” said Gwynne Shotwell, president and COO of SpaceX. “SpaceX is manufacturing tens of thousands of Starlink kits a day – all right here in the U.S. – and we are making huge investments in PCB manufacturing and silicon packaging to expand even further. TI’s U.S.-made semiconductors are crucial for securing a U.S. supply chain for our products, and their advanced silicon manufacturing capabilities provide the performance and reliability needed to help us meet the growing demand for high-speed internet all around the world.”</p>
</div><div data-lid="richText_8687"><p><b>Backed by the strength of TI’s U.S. manufacturing presence</b></p>
<p>TI is a driving force behind the return and expansion of semiconductor manufacturing in the U.S. The company’s more than $60 billion investment in U.S. manufacturing includes building and ramping seven, large-scale, connected fabs. Combined, these fabs across three manufacturing mega-sites in Texas and Utah will manufacture hundreds of millions of U.S.-made chips daily that will ignite a bold new chapter in American innovation.</p>
<ul>
<li><b>Sherman, Texas: </b>SM1, TI’s first new fab in Sherman will begin initial production this year, just three years after breaking ground. Construction is also complete on the exterior shell of SM2, TI’s second new fab in Sherman. Incremental investment plans include two additional fabs, SM3 and SM4, to support future demand.</li>
<li><b>Richardson, Texas:</b> TI’s second fab in Richardson, RFAB2, continues to ramp to full production and builds on the company’s legacy of introducing the world’s first 300mm analog fab, RFAB1, in 2011.</li>
<li><b>Lehi, Utah: </b>TI is ramping LFAB1, the company’s first 300mm wafer fab in Lehi. Construction is also well underway on LFAB2, TI’s second Lehi fab that will connect to LFAB1.</li>
</ul>
<p><b>Learn more</b></p>
<ul>
<li><a href="https://www.ti.com/about-ti/newsroom/media-resources/press-kits/ti-invests-60-billion-in-us-manufacturing.html">Press kit</a> (includes images, video b-roll and fact sheet)</li>
<li>Additional press kits (<a href="https://www.ti.com/about-ti/newsroom/media-resources/press-kits/sherman-texas-press-kit.html">Sherman, Texas</a>, <a href="https://www.ti.com/about-ti/newsroom/media-resources/press-kits/lehi-utah-press-kit.html">Lehi, Utah</a>, and <a href="https://www.ti.com/about-ti/newsroom/media-resources/press-kits/richardson-texas-press-kit.html">Richardson, Texas</a>)</li>
</ul>
<p><b>About Texas Instruments</b></p>
<p>Texas Instruments Incorporated (Nasdaq: TXN) is a global semiconductor company that designs, manufactures, and sells analog and embedded processing chips for markets such as industrial, automotive, personal electronics, communications equipment and enterprise systems. At our core, we have a passion to create a better world by making electronics more affordable through semiconductors. This passion is alive today as each generation of innovation builds upon the last to make our technology more reliable, more affordable and lower power, making it possible for semiconductors to go into electronics everywhere. Learn more at&nbsp;<a href="https://c212.net/c/link/?t=0&amp;l=en&amp;o=4167832-1&amp;h=2361819976&amp;u=https%3A%2F%2Fc212.net%2Fc%2Flink%2F%3Ft%3D0%26l%3Den%26o%3D2849181-1%26h%3D2359332189%26u%3Dhttp%253A%252F%252Fwww.ti.com%252F%26a%3DTI.com&amp;a=TI.com">TI.com</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Andrej Karpathy: Software in the era of AI [video] (507 pts)]]></title>
            <link>https://www.youtube.com/watch?v=LCEmiRjPEtQ</link>
            <guid>44314423</guid>
            <pubDate>Thu, 19 Jun 2025 00:33:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=LCEmiRjPEtQ">https://www.youtube.com/watch?v=LCEmiRjPEtQ</a>, See on <a href="https://news.ycombinator.com/item?id=44314423">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[MCP Specification – version 2025-06-18 changes (136 pts)]]></title>
            <link>https://modelcontextprotocol.io/specification/2025-06-18/changelog</link>
            <guid>44314289</guid>
            <pubDate>Wed, 18 Jun 2025 23:59:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://modelcontextprotocol.io/specification/2025-06-18/changelog">https://modelcontextprotocol.io/specification/2025-06-18/changelog</a>, See on <a href="https://news.ycombinator.com/item?id=44314289">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><main><span></span><div id="content-container"><div id="content-area">
<p>This document lists changes made to the Model Context Protocol (MCP) specification since
the previous revision, <a href="https://modelcontextprotocol.io/specification/2025-03-26">2025-03-26</a>.</p>
<h2 id="major-changes"><span>Major changes</span></h2>
<ol>
<li>Remove support for JSON-RPC <strong><a href="https://www.jsonrpc.org/specification#batch" target="_blank" rel="noreferrer">batching</a></strong>
(PR <a href="https://github.com/modelcontextprotocol/specification/pull/416" target="_blank" rel="noreferrer">#416</a>)</li>
<li>Add support for <a href="https://modelcontextprotocol.io/specification/2025-06-18/server/tools#structured-content">structured tool output</a>
(PR <a href="https://github.com/modelcontextprotocol/modelcontextprotocol/pull/371" target="_blank" rel="noreferrer">#371</a>)</li>
<li>Classify MCP servers as <a href="https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization#authorization-server-discovery">OAuth Resource Servers</a>,
adding protected resource metadata to discover the corresponding Authorization server.
(PR <a href="https://github.com/modelcontextprotocol/modelcontextprotocol/pull/338" target="_blank" rel="noreferrer">#338</a>)</li>
<li>Require MCP clients to implement Resource Indicators as described in <a href="https://www.rfc-editor.org/rfc/rfc8707.html" target="_blank" rel="noreferrer">RFC 8707</a> to prevent
malicious servers from obtaining access tokens.
(PR <a href="https://github.com/modelcontextprotocol/modelcontextprotocol/pull/734" target="_blank" rel="noreferrer">#734</a>)</li>
<li>Clarify <a href="https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization#security-considerations">security considerations</a> and best practices
in the authorization spec and in a new <a href="https://modelcontextprotocol.io/specification/2025-06-18/basic/security_best_practices">security best practices page</a>.</li>
<li>Add support for <strong><a href="https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation">elicitation</a></strong>, enabling servers to request additional
information from users during interactions.
(PR <a href="https://github.com/modelcontextprotocol/modelcontextprotocol/pull/382" target="_blank" rel="noreferrer">#382</a>)</li>
<li>Add support for <strong><a href="https://modelcontextprotocol.io/specification/2025-06-18/server/tools#resource-links">resource links</a></strong> in
tool call results. (PR <a href="https://github.com/modelcontextprotocol/modelcontextprotocol/pull/603" target="_blank" rel="noreferrer">#603</a>)</li>
<li>Require <a href="https://modelcontextprotocol.io/specification/2025-06-18/basic/transports#protocol-version-header">negotiated protocol version to be specified</a>
via <code>MCP-Protocol-Version</code> header in subsequent requests when using HTTP (PR <a href="https://github.com/modelcontextprotocol/modelcontextprotocol/pull/548" target="_blank" rel="noreferrer">#548</a>).</li>
<li>Change <strong>SHOULD</strong> to <strong>MUST</strong> in <a href="https://modelcontextprotocol.io/specification/2025-06-18/basic/lifecycle#operation">Lifecycle Operation</a></li>
</ol>
<h2 id="other-schema-changes"><span>Other schema changes</span></h2>
<ol>
<li>Add <code>_meta</code> field to additional interface types (PR <a href="https://github.com/modelcontextprotocol/modelcontextprotocol/pull/710" target="_blank" rel="noreferrer">#710</a>),
and specify <a href="https://modelcontextprotocol.io/specification/2025-06-18/basic#meta">proper usage</a>.</li>
<li>Add <code>context</code> field to <code>CompletionRequest</code>, providing for completion requests to include
previously-resolved variables (PR <a href="https://github.com/modelcontextprotocol/modelcontextprotocol/pull/598" target="_blank" rel="noreferrer">#598</a>).</li>
<li>Add <code>title</code> field for human-friendly display names, so that <code>name</code> can be used as a programmatic
identifier (PR <a href="https://github.com/modelcontextprotocol/modelcontextprotocol/pull/663" target="_blank" rel="noreferrer">#663</a>)</li>
</ol>
<h2 id="full-changelog"><span>Full changelog</span></h2>
<p>For a complete list of all changes that have been made since the last protocol revision,
<a href="https://github.com/modelcontextprotocol/specification/compare/2025-03-26...2025-06-18" target="_blank" rel="noreferrer">see GitHub</a>.</p></div><div id="content-side-layout"><ul id="table-of-contents-content"><li data-depth="0"><a href="#major-changes">Major changes</a></li><li data-depth="0"><a href="#other-schema-changes">Other schema changes</a></li><li data-depth="0"><a href="#full-changelog">Full changelog</a></li></ul></div></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Unregistry – "docker push" directly to servers without a registry (423 pts)]]></title>
            <link>https://github.com/psviderski/unregistry</link>
            <guid>44314085</guid>
            <pubDate>Wed, 18 Jun 2025 23:17:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/psviderski/unregistry">https://github.com/psviderski/unregistry</a>, See on <a href="https://news.ycombinator.com/item?id=44314085">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/psviderski/unregistry/blob/main/.github/images/logo-light.svg#gh-light-mode-only"><img src="https://github.com/psviderski/unregistry/raw/main/.github/images/logo-light.svg#gh-light-mode-only" alt="Unregistry logo"></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/psviderski/unregistry/blob/main/.github/images/logo-dark.svg#gh-dark-mode-only"><img src="https://github.com/psviderski/unregistry/raw/main/.github/images/logo-dark.svg#gh-dark-mode-only" alt="Unregistry logo"></a></p><p dir="auto"><strong>▸ Push docker images directly to remote servers without an external registry ◂</strong></p>
  <p dir="auto">
    <a href="https://discord.gg/eR35KQJhPu" rel="nofollow"><img src="https://camo.githubusercontent.com/bce982f6fd064725216dd2c5bbfc5fa12afeaee92ae5d356ea3f5abf78d0ad88/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646973636f72642d3538363546322e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" alt="Join Discord" data-canonical-src="https://img.shields.io/badge/discord-5865F2.svg?style=for-the-badge&amp;logo=discord&amp;logoColor=white"></a>
    <a href="https://x.com/psviderski" rel="nofollow"><img src="https://camo.githubusercontent.com/7ef3b84c3487de0b84ead3bc3ac609c4cbaa26f231976bbdf4e56b855dfaf42d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f666f6c6c6f772d626c61636b3f7374796c653d666f722d7468652d6261646765266c6f676f3d58266c6f676f436f6c6f723d7768696c65" alt="Follow on X" data-canonical-src="https://img.shields.io/badge/follow-black?style=for-the-badge&amp;logo=X&amp;logoColor=while"></a>
  </p>
</div>
<p dir="auto">Unregistry is a lightweight container image registry that stores and serves images directly from your Docker daemon's
storage.</p>
<p dir="auto">The included <code>docker pussh</code> command (extra 's' for SSH) lets you push images straight to remote Docker servers over SSH.
It transfers only the missing layers, making it fast and efficient.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description docker-pussh-demo.mp4">docker-pussh-demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/783910/456396282-9d704b87-8e0d-4c8a-9544-17d4c63bd050.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAyOTY5MDEsIm5iZiI6MTc1MDI5NjYwMSwicGF0aCI6Ii83ODM5MTAvNDU2Mzk2MjgyLTlkNzA0Yjg3LThlMGQtNGM4YS05NTQ0LTE3ZDRjNjNiZDA1MC5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjE5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYxOVQwMTMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04NWQzNjViYWFlNTI3MjA0MTgxOTUyZWZiMDc5NTkxMThmMjM2MjQwNDI3MWU3ZDc1YTg0NGRmMjcyNTFmYWNiJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.kp-WEQJdsYOZa8e8DyU8_JbguC9jt8GXLWSwGsF4o4k" data-canonical-src="https://private-user-images.githubusercontent.com/783910/456396282-9d704b87-8e0d-4c8a-9544-17d4c63bd050.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAyOTY5MDEsIm5iZiI6MTc1MDI5NjYwMSwicGF0aCI6Ii83ODM5MTAvNDU2Mzk2MjgyLTlkNzA0Yjg3LThlMGQtNGM4YS05NTQ0LTE3ZDRjNjNiZDA1MC5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjE5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYxOVQwMTMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04NWQzNjViYWFlNTI3MjA0MTgxOTUyZWZiMDc5NTkxMThmMjM2MjQwNDI3MWU3ZDc1YTg0NGRmMjcyNTFmYWNiJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.kp-WEQJdsYOZa8e8DyU8_JbguC9jt8GXLWSwGsF4o4k" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">The problem</h2><a id="user-content-the-problem" aria-label="Permalink: The problem" href="#the-problem"></a></p>
<p dir="auto">You've built a Docker image locally. Now you need it on your server. Your options suck:</p>
<ul dir="auto">
<li><strong>Docker Hub / GitHub Container Registry</strong> - Your code is now public, or you're paying for private repos</li>
<li><strong>Self-hosted registry</strong> - Another service to maintain, secure, and pay for storage</li>
<li><strong>Save/Load</strong> - <code>docker save | ssh | docker load</code> transfers the entire image, even if 90% already exists on the server</li>
<li><strong>Rebuild remotely</strong> - Wastes time and server resources. Plus now you're debugging why the build fails in production</li>
</ul>
<p dir="auto">You just want to move an image from A to B. Why is this so hard?</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The solution</h2><a id="user-content-the-solution" aria-label="Permalink: The solution" href="#the-solution"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pussh myapp:latest user@server"><pre>docker pussh myapp:latest user@server</pre></div>
<p dir="auto">That's it. Your image is on the remote server. No registry setup, no subscription, no intermediate storage, no
exposed ports. Just a <strong>direct transfer</strong> of the <strong>missing layers</strong> over SSH.</p>
<p dir="auto">Here's what happens under the hood:</p>
<ol dir="auto">
<li>Establishes SSH tunnel to the remote server</li>
<li>Starts a temporary unregistry container</li>
<li>Forwards a random localhost port to the unregistry port over the tunnel</li>
<li><code>docker push</code> to unregistry through the forwarded port, transferring only the layers that don't already exist
remotely. The transferred image is instantly available on the remote Docker daemon</li>
<li>Stops the unregistry container and closes the SSH tunnel</li>
</ol>
<p dir="auto">It's like <code>rsync</code> for Docker images — simple and efficient.</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">Unregistry was created for <a href="https://github.com/psviderski/uncloud">Uncloud</a>, a lightweight tool for deploying
containers across multiple Docker hosts. We needed something simpler than a full registry but more efficient than
save/load.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">macOS/Linux via Homebrew</h3><a id="user-content-macoslinux-via-homebrew" aria-label="Permalink: macOS/Linux via Homebrew" href="#macoslinux-via-homebrew"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install psviderski/tap/docker-pussh"><pre>brew install psviderski/tap/docker-pussh</pre></div>
<p dir="auto">After installation, to use <code>docker-pussh</code> as a Docker CLI plugin (<code>docker pussh</code> command) you need to create a symlink:</p>
<div dir="auto" data-snippet-clipboard-copy-content="mkdir -p ~/.docker/cli-plugins
ln -sf $(brew --prefix)/bin/docker-pussh ~/.docker/cli-plugins/docker-pussh"><pre>mkdir -p <span>~</span>/.docker/cli-plugins
ln -sf <span><span>$(</span>brew --prefix<span>)</span></span>/bin/docker-pussh <span>~</span>/.docker/cli-plugins/docker-pussh</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">macOS/Linux via direct download</h3><a id="user-content-macoslinux-via-direct-download" aria-label="Permalink: macOS/Linux via direct download" href="#macoslinux-via-direct-download"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Download the latest version
curl -sSL https://raw.githubusercontent.com/psviderski/unregistry/main/docker-pussh \
  -o ~/.docker/cli-plugins/docker-pussh

# Make it executable
chmod +x ~/.docker/cli-plugins/docker-pussh"><pre><span><span>#</span> Download the latest version</span>
curl -sSL https://raw.githubusercontent.com/psviderski/unregistry/main/docker-pussh \
  -o <span>~</span>/.docker/cli-plugins/docker-pussh

<span><span>#</span> Make it executable</span>
chmod +x <span>~</span>/.docker/cli-plugins/docker-pussh</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows</h3><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<p dir="auto">Windows is not currently supported, but you can try using <a href="https://docs.docker.com/desktop/features/wsl/" rel="nofollow">WSL 2</a>
with the above Linux instructions.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Verify installation</h3><a id="user-content-verify-installation" aria-label="Permalink: Verify installation" href="#verify-installation"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Push an image to a remote server. Please make sure the SSH user has permissions to run <code>docker</code> commands (user is
<code>root</code> or non-root user is in <code>docker</code> group). If <code>sudo</code> is required, ensure the user can run <code>sudo docker</code> without
a password prompt.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pussh myapp:latest user@server.example.com"><pre>docker pussh myapp:latest user@server.example.com</pre></div>
<p dir="auto">With SSH key authentication if the private key is not added to your SSH agent:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pussh myapp:latest ubuntu@192.168.1.100 -i ~/.ssh/id_rsa"><pre>docker pussh myapp:latest ubuntu@192.168.1.100 -i <span>~</span>/.ssh/id_rsa</pre></div>
<p dir="auto">Using a custom SSH port:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pussh myapp:latest user@server:2222"><pre>docker pussh myapp:latest user@server:2222</pre></div>
<p dir="auto">Push a specific platform for a multi-platform image. The local Docker has to use
<a href="https://docs.docker.com/desktop/features/containerd/" rel="nofollow">containerd image store</a> to support multi-platform images.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pussh myapp:latest user@server --platform linux/amd64"><pre>docker pussh myapp:latest user@server --platform linux/amd64</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Use cases</h2><a id="user-content-use-cases" aria-label="Permalink: Use cases" href="#use-cases"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deploy to production servers</h3><a id="user-content-deploy-to-production-servers" aria-label="Permalink: Deploy to production servers" href="#deploy-to-production-servers"></a></p>
<p dir="auto">Build locally and push directly to your production servers. No middleman.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker build --platform linux/amd64 -t myapp:1.2.3 .
docker pussh myapp:1.2.3 deploy@prod-server
ssh deploy@prod-server docker run -d myapp:1.2.3"><pre>docker build --platform linux/amd64 -t myapp:1.2.3 <span>.</span>
docker pussh myapp:1.2.3 deploy@prod-server
ssh deploy@prod-server docker run -d myapp:1.2.3</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">CI/CD pipelines</h3><a id="user-content-cicd-pipelines" aria-label="Permalink: CI/CD pipelines" href="#cicd-pipelines"></a></p>
<p dir="auto">Skip the registry complexity in your pipelines. Build and push directly to deployment targets.</p>
<div dir="auto" data-snippet-clipboard-copy-content="- name: Build and deploy
  run: |
    docker build -t myapp:${{ github.sha }} .
    docker pussh myapp:${{ github.sha }} deploy@staging-server"><pre>- <span>name</span>: <span>Build and deploy</span>
  <span>run</span>: <span>|</span>
<span>    docker build -t myapp:${{ github.sha }} .</span>
<span>    docker pussh myapp:${{ github.sha }} deploy@staging-server</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Homelab and air-gapped environments</h3><a id="user-content-homelab-and-air-gapped-environments" aria-label="Permalink: Homelab and air-gapped environments" href="#homelab-and-air-gapped-environments"></a></p>
<p dir="auto">Distribute images in isolated networks without exposing them to the internet.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pussh image:latest user@192.168.1.100"><pre>docker pussh image:latest user@192.168.1.100</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">On local machine</h3><a id="user-content-on-local-machine" aria-label="Permalink: On local machine" href="#on-local-machine"></a></p>
<ul dir="auto">
<li>Docker CLI with plugin support (Docker 19.03+)</li>
<li>OpenSSH client</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">On remote server</h3><a id="user-content-on-remote-server" aria-label="Permalink: On remote server" href="#on-remote-server"></a></p>
<ul dir="auto">
<li>Docker is installed and running</li>
<li>SSH user has permissions to run <code>docker</code> commands (user is <code>root</code> or non-root user is in <code>docker</code> group)</li>
<li>If <code>sudo</code> is required, ensure the user can run <code>sudo docker</code> without a password prompt</li>
</ul>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">The remote Docker daemon works best with <a href="https://docs.docker.com/engine/storage/containerd/" rel="nofollow">containerd image store</a>
enabled. This allows unregistry to access images more efficiently.</p>
<p dir="auto">Add the following configuration to <code>/etc/docker/daemon.json</code> on the remote server and restart the <code>docker</code> service:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;features&quot;: {
    &quot;containerd-snapshotter&quot;: true
  }
}"><pre>{
  <span>"features"</span>: {
    <span>"containerd-snapshotter"</span>: <span>true</span>
  }
}</pre></div>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Advanced usage</h2><a id="user-content-advanced-usage" aria-label="Permalink: Advanced usage" href="#advanced-usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running unregistry standalone</h3><a id="user-content-running-unregistry-standalone" aria-label="Permalink: Running unregistry standalone" href="#running-unregistry-standalone"></a></p>
<p dir="auto">Sometimes you want a local registry without the overhead. Unregistry works great for this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run unregistry locally and expose it on port 5000
docker run -d -p 5000:5000 --name unregistry \
  -v /run/containerd/containerd.sock:/run/containerd/containerd.sock \
  ghcr.io/psviderski/unregistry

# Use it like any registry
docker tag myapp:latest localhost:5000/myapp:latest
docker push localhost:5000/myapp:latest"><pre><span><span>#</span> Run unregistry locally and expose it on port 5000</span>
docker run -d -p 5000:5000 --name unregistry \
  -v /run/containerd/containerd.sock:/run/containerd/containerd.sock \
  ghcr.io/psviderski/unregistry

<span><span>#</span> Use it like any registry</span>
docker tag myapp:latest localhost:5000/myapp:latest
docker push localhost:5000/myapp:latest</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Custom SSH options</h3><a id="user-content-custom-ssh-options" aria-label="Permalink: Custom SSH options" href="#custom-ssh-options"></a></p>
<p dir="auto">Need custom SSH settings? Use the standard SSH config file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# ~/.ssh/config
Host prod-server
    HostName server.example.com
    User deploy
    Port 2222
    IdentityFile ~/.ssh/deploy_key

# Now just use
docker pussh myapp:latest prod-server"><pre><span><span>#</span> ~/.ssh/config</span>
Host prod-server
    HostName server.example.com
    User deploy
    Port 2222
    IdentityFile <span>~</span>/.ssh/deploy_key

<span><span>#</span> Now just use</span>
docker pussh myapp:latest prod-server</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Found a bug or have a feature idea? We'd love your help!</p>
<ul dir="auto">
<li>🐛 Found a bug? <a href="https://github.com/psviderski/unregistry/issues">Open an issue</a></li>
<li>💡 Have ideas or need help? <a href="https://discord.gg/eR35KQJhPu" rel="nofollow">Join Uncloud Discord community</a> where we discuss features,
roadmap, implementation details, and help each other out.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inspiration &amp; acknowledgements</h2><a id="user-content-inspiration--acknowledgements" aria-label="Permalink: Inspiration &amp; acknowledgements" href="#inspiration--acknowledgements"></a></p>
<ul dir="auto">
<li><a href="https://github.com/spegel-org/spegel">Spegel</a> - P2P container image registry that inspired me to implement a
registry that uses containerd image store as a backend.</li>
<li><a href="https://github.com/distribution/distribution">Docker Distribution</a> - the bulletproof Docker registry implementation
that unregistry uses as a base.</li>
</ul>

<p>
  Built with ❤️ by <a href="https://github.com/psviderski">Pasha Sviderski</a> who just wanted to deploy his images
</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New US visa rules will force foreign students to unlock social media profiles (350 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2025/jun/18/social-media-student-visa-screening</link>
            <guid>44314054</guid>
            <pubDate>Wed, 18 Jun 2025 23:11:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2025/jun/18/social-media-student-visa-screening">https://www.theguardian.com/us-news/2025/jun/18/social-media-student-visa-screening</a>, See on <a href="https://news.ycombinator.com/item?id=44314054">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Foreign students will be required to unlock their social media profiles to allow US diplomats to review their online activity before receiving educational and exchange visas, the state department has announced. Those who fail to do so will be suspected of hiding that activity from US officials.</p><p>The new guidance, unveiled by the state department on Wednesday, directs US diplomats to conduct an online presence review to look for “any indications of hostility toward the citizens, culture, government, institutions, or founding principles of the United States”.</p><p>A cable separately obtained by Politico also instructs diplomats to flag any “advocacy for, aid or support for foreign terrorists and other threats to US national security” and “support for unlawful antisemitic harassment or violence”.</p><p>The screening for “antisemitic” activity matches similar guidance given at US Citizenship and Immigration Services under the Department of Homeland Security and has been criticised as an effort to crack down on opposition to the conduct of Israel’s war in Gaza.</p><p>The new state department checks are directed at students and other applicants for visas in the F, M and J categories, which refer to academic and vocational education, as well as cultural exchanges.</p><p>“It is an expectation from American citizens that their government will make every effort to make our country safer, and that is exactly what the <a href="https://www.theguardian.com/us-news/trump-administration" data-link-name="in body link" data-component="auto-linked-tag">Trump administration</a> is doing every single day,” said a senior state department official, adding that Marco Rubio was “helping to make America and its universities safer while bringing the state Department into the 21st century”.</p><figure id="2c0c0441-fde7-4b33-9f05-c7264f202232" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:6,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;US issues broad order to consulates to vet student visas over ‘terrorist activity’&quot;,&quot;elementId&quot;:&quot;2c0c0441-fde7-4b33-9f05-c7264f202232&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/us-news/2025/mar/28/student-visa-applications-denials&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>The Trump administration paused the issuance of new education visas late last month as it mulled new social media vetting strategies. The US had also targeted Chinese students for special scrutiny amid a tense negotiation over tariffs and the supply of rare-earth metals and minerals to the United States.</p><p>The state department directive allowed diplomatic posts to resume the scheduling of interviews for educational and exchange visas, but added that consular officers would conduct a “comprehensive and thorough vetting” of all applicants applying for F, M and J visas.</p><p>“To facilitate this vetting, all applicants for F, M and J non-immigrant visas will be asked to adjust the privacy settings on all their social media profiles to ‘public’”, the official said. “The enhanced social media vetting will ensure we are properly screening every single person attempting to visit our country.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fang, the CLI Starter Kit (113 pts)]]></title>
            <link>https://github.com/charmbracelet/fang</link>
            <guid>44313901</guid>
            <pubDate>Wed, 18 Jun 2025 22:40:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/charmbracelet/fang">https://github.com/charmbracelet/fang</a>, See on <a href="https://news.ycombinator.com/item?id=44313901">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Fang</h2><a id="user-content-fang" aria-label="Permalink: Fang" href="#fang"></a></p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/25087/453127263-3f34ea01-3750-4760-beb2-a1b700e110f5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAzMTg1MDMsIm5iZiI6MTc1MDMxODIwMywicGF0aCI6Ii8yNTA4Ny80NTMxMjcyNjMtM2YzNGVhMDEtMzc1MC00NzYwLWJlYjItYTFiNzAwZTExMGY1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA2MTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNjE5VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWRiYjlmMzhlMjYxZDQ3NDE3YjcxZWM0NmQyYzgzN2ZhNjE3NTMzZjRhNDZkMGUyMDliMDBjZTlhNGNmODE4OWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.qNmzqtvKLyQgIWOFwgomip_UNof2pDx-LF94v9UhF8M"><img width="485" alt="Charm Fang" src="https://private-user-images.githubusercontent.com/25087/453127263-3f34ea01-3750-4760-beb2-a1b700e110f5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAzMTg1MDMsIm5iZiI6MTc1MDMxODIwMywicGF0aCI6Ii8yNTA4Ny80NTMxMjcyNjMtM2YzNGVhMDEtMzc1MC00NzYwLWJlYjItYTFiNzAwZTExMGY1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA2MTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNjE5VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWRiYjlmMzhlMjYxZDQ3NDE3YjcxZWM0NmQyYzgzN2ZhNjE3NTMzZjRhNDZkMGUyMDliMDBjZTlhNGNmODE4OWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.qNmzqtvKLyQgIWOFwgomip_UNof2pDx-LF94v9UhF8M"></a>   
</p>
<p dir="auto">
    <a href="https://github.com/charmbracelet/fang/releases"><img src="https://camo.githubusercontent.com/2491a6829151430ac84c5b91f8de86767fdbf15d6f42e35ea9796c0976ca1f6b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f636861726d62726163656c65742f66616e672e737667" alt="Latest Release" data-canonical-src="https://img.shields.io/github/release/charmbracelet/fang.svg"></a>
    <a href="https://pkg.go.dev/github.com/charmbracelet/fang?tab=doc" rel="nofollow"><img src="https://camo.githubusercontent.com/085bf6223c42adfef9036b4df0988091acacbcc7178b35906aa685b6aec2fe19/68747470733a2f2f676f646f632e6f72672f6769746875622e636f6d2f636861726d62726163656c65742f66616e673f7374617475732e737667" alt="GoDoc" data-canonical-src="https://godoc.org/github.com/charmbracelet/fang?status.svg"></a>
    <a href="https://github.com/charmbracelet/fang/actions"><img src="https://github.com/charmbracelet/fang/workflows/build/badge.svg" alt="Build Status"></a>
</p>
<p dir="auto">The CLI starter kit. A small, experimental library for batteries-included <a href="https://github.com/spf13/cobra">Cobra</a> applications.</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/25087/456775205-5c35e1fa-9577-4f81-a879-3ddb4d4a43f0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAzMTg1MDMsIm5iZiI6MTc1MDMxODIwMywicGF0aCI6Ii8yNTA4Ny80NTY3NzUyMDUtNWMzNWUxZmEtOTU3Ny00ZjgxLWE4NzktM2RkYjRkNGE0M2YwLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA2MTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNjE5VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZkYmZhYTVhZTZkZGE4N2QzNTIwYTUwZGI3MTc3NDMyMjY0N2JmZDM1NDMwZjBlZGI2ZDExOTUxN2I4MzQ5OGUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.7PlqhcO-1c4KQCButUYJuiPZ4VZfGu8CAdHDd5_U6W8"><img width="859" alt="The Charm Fang mascot and title treatment" src="https://private-user-images.githubusercontent.com/25087/456775205-5c35e1fa-9577-4f81-a879-3ddb4d4a43f0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAzMTg1MDMsIm5iZiI6MTc1MDMxODIwMywicGF0aCI6Ii8yNTA4Ny80NTY3NzUyMDUtNWMzNWUxZmEtOTU3Ny00ZjgxLWE4NzktM2RkYjRkNGE0M2YwLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA2MTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNjE5VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZkYmZhYTVhZTZkZGE4N2QzNTIwYTUwZGI3MTc3NDMyMjY0N2JmZDM1NDMwZjBlZGI2ZDExOTUxN2I4MzQ5OGUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.7PlqhcO-1c4KQCButUYJuiPZ4VZfGu8CAdHDd5_U6W8"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>Fancy output</strong>: fully styled help and usage pages</li>
<li><strong>Fancy errors</strong>: fully styled errors</li>
<li><strong>Automatic <code>--version</code></strong>: set it to the <a href="https://pkg.go.dev/runtime/debug#BuildInfo" rel="nofollow">build info</a>, or a version of your choice</li>
<li><strong>Manpages</strong>: Adds a hidden <code>man</code> command to generate <em>manpages</em> using
<a href="https://github.com/muesli/mango">mango</a><sup><a href="#user-content-fn-1-0d05681b58ad0244d82fd35e6e8dccb1" id="user-content-fnref-1-0d05681b58ad0244d82fd35e6e8dccb1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup></li>
<li><strong>Completions</strong>: Adds a <code>completion</code> command to generate shell completions</li>
<li><strong>Themeable</strong>: use the built-in theme, or make your own</li>
<li><strong>UX</strong>: Silent <code>usage</code> output (help is not shown after a user error)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">To use it, invoke <code>fang.Execute</code> passing your root <code>*cobra.Command</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="package main

import (
	&quot;os&quot;

	&quot;github.com/charmbracelet/fang&quot;
	&quot;github.com/spf13/cobra&quot;
)

func main() {
	cmd := &amp;cobra.Command{
		Use:   &quot;example&quot;,
		Short: &quot;A simple example program!&quot;,
	}
	if err := fang.Execute(context.TODO(), cmd); err != nil {
		os.Exit(1)
	}
}"><pre><span>package</span> main

<span>import</span> (
	<span>"os"</span>

	<span>"github.com/charmbracelet/fang"</span>
	<span>"github.com/spf13/cobra"</span>
)

<span>func</span> <span>main</span>() {
	<span>cmd</span> <span>:=</span> <span>&amp;</span>cobra.<span>Command</span>{
		<span>Use</span>:   <span>"example"</span>,
		<span>Short</span>: <span>"A simple example program!"</span>,
	}
	<span>if</span> <span>err</span> <span>:=</span> <span>fang</span>.<span>Execute</span>(<span>context</span>.<span>TODO</span>(), <span>cmd</span>); <span>err</span> <span>!=</span> <span>nil</span> {
		<span>os</span>.<span>Exit</span>(<span>1</span>)
	}
}</pre></div>
<p dir="auto">That's all there is to it!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">See <a href="https://github.com/charmbracelet/fang/contribute">contributing</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Feedback</h2><a id="user-content-feedback" aria-label="Permalink: Feedback" href="#feedback"></a></p>
<p dir="auto">We’d love to hear your thoughts on this project. Feel free to drop us a note!</p>
<ul dir="auto">
<li><a href="https://twitter.com/charmcli" rel="nofollow">Twitter</a></li>
<li><a href="https://charm.sh/chat" rel="nofollow">Discord</a></li>
<li><a href="https://mastodon.social/@charmcli" rel="nofollow">The Fediverse</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto"><a href="https://github.com/charmbracelet/gum/raw/main/LICENSE">MIT</a></p>
<hr>
<p dir="auto">Part of <a href="https://charm.sh/" rel="nofollow">Charm</a>.</p>
<p dir="auto"><a href="https://charm.sh/" rel="nofollow"><img alt="The Charm logo" src="https://camo.githubusercontent.com/bdbe361924a6bb7fbc7e80f53ac2a8dd50c1a85ff952a3063c0fc0cba3a09d6d/68747470733a2f2f73747566662e636861726d2e73682f636861726d2d62616467652e6a7067" width="400" data-canonical-src="https://stuff.charm.sh/charm-badge.jpg"></a></p>
<p dir="auto">Charm热爱开源 • Charm loves open source</p>
<section data-footnotes="">
<ol dir="auto">
<li id="user-content-fn-1-0d05681b58ad0244d82fd35e6e8dccb1">
<p dir="auto">Default cobra man pages generates one man page for each command. This is
generally fine for programs with a lot of sub commands, like git, but its an
overkill for smaller programs.
Mango also uses <em>roff</em> directly instead of converting from markdown, so it
should render better looking man pages. <a href="#user-content-fnref-1-0d05681b58ad0244d82fd35e6e8dccb1" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
</ol>
</section>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Missing 11th of the Month (137 pts)]]></title>
            <link>https://drhagen.com/blog/the-missing-11th-of-the-month/</link>
            <guid>44313550</guid>
            <pubDate>Wed, 18 Jun 2025 21:45:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://drhagen.com/blog/the-missing-11th-of-the-month/">https://drhagen.com/blog/the-missing-11th-of-the-month/</a>, See on <a href="https://news.ycombinator.com/item?id=44313550">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div data-md-component="content">
    
    <article>
      
        


  

<figure>
    <img alt="In months other than September, the 11th is mentioned substantially less often than any other date. It's been that way since long before 9/11 and I have no idea why." src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/calendar_of_meaningful_dates_light.png#only-light">
    <img alt="In months other than September, the 11th is mentioned substantially less often than any other date. It's been that way since long before 9/11 and I have no idea why." src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/calendar_of_meaningful_dates_dark.png#only-dark">
    Source <a href="https://xkcd.com/1140/">xkcd</a>. Image licensed under <a href="https://creativecommons.org/licenses/by-nc/2.5/">CC-BY-NC</a>.
</figure>
<p>On November 28th, 2012, Randall Munroe published <a href="https://xkcd.com/1140/">an xkcd&nbsp;comic</a>&nbsp;that was&nbsp;a calendar in which&nbsp;the size of each date was&nbsp;proportional to&nbsp;how often each date is referenced by its ordinal name&nbsp;(e.g. "October 14th") in the <a href="https://books.google.com/ngrams">Google Ngrams database</a> since 2000. Most of the large&nbsp;days are pretty much what you would expect:&nbsp;<a href="https://en.wikipedia.org/wiki/Independence_Day_(United_States)">July 4th</a>, <a href="https://en.wikipedia.org/wiki/Christmas">December 25th</a>, the 1st of every month, the last day of most months, and of course a <a href="https://en.wikipedia.org/wiki/September_11_attacks">September 11th</a> that shoves&nbsp;its neighbors&nbsp;into the margins. There are not many days that seem to be smaller than the typical size. <a href="https://en.wikipedia.org/wiki/Leap_year">February 29th</a> is a tiny speck, for instance. But if&nbsp;you stare at the comic&nbsp;long enough, you may get the impression that the 11th of most&nbsp;months is unusually small. The title&nbsp;text of the comic concurs, reading "In months other than September, the 11th is mentioned substantially less often than any other date. It's been that way since long before 9/11 and I have no idea why." After digging into the raw data, I believe I have figured out why.</p>
<!-- more -->

<p>First I confirmed&nbsp;that the <code>11th</code> is actually interesting. There are 31 days and one of them <em>has</em> to be smallest. Maybe the <code>11th</code> isn't an outlier; it's just on the smaller end and our eyes&nbsp;are picking up on a pattern that doesn't exist. To confirm this is real,&nbsp;I compared actual numbers, not text size. The Ngrams database returns the total number times a phrase is mentioned in a given year normalized by the total number of books published that year. The database only goes up to the year 2008, so it is presumably unchanged from when Randall queried it in 2012.</p>
<p>I&nbsp;retrieved the count for each day for the year (<code>January 1st</code>, <code>January 2nd</code> etc.) and took the median over the months for each day (median of <code>January 1st</code>, <code>February 1st</code>, etc.) for&nbsp;each year. This summarizes&nbsp;how often the <code>11th</code> and the other 30 days of the month appear in a given year. Using the median prevents outlier days like <code>July 4th</code> from dragging up the average for its&nbsp;corresponding ordinal&nbsp;(the <code>4th</code>). Only if a ordinal&nbsp;is unusual for at least 6&nbsp;of the 12 months will its median appear unusual.</p>
<p>I took the&nbsp;median for each ordinal over the years 2000-2008. The graph below is a histogram of the 31 medians. The <code>1st</code> of the month stands out far above them all&nbsp;and the <code>15th</code> just barely distinguishes itself from the remainder. Being the first day and the middle day of the month, these two make sense.&nbsp;However, the <code>11th</code> stands out as the lowest by a significant&nbsp;margin (p-value &lt; 0.05), with no immediate explanation.</p>
<figure>
    <img alt="histogram_2000-2008" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/histogram_2000-2008_light.png#only-light">
    <img alt="histogram_2000-2008" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/histogram_2000-2008_dark.png#only-dark">
</figure>
<p>This deficit&nbsp;has been around for a long time. Below is all the ordinals for every year in the data set, 1800-2008. The data is smoothed over eleven&nbsp;years to flatten out the noise. Even at the beginning, the <code>11th</code> is significantly lower than the main group. This mild deficit continues for a few decades and then something weird happens in 1860s; the&nbsp;11th suddenly diverges from its place just below the pack. The gap between the <code>11th</code> and the ordinary ordinals expands rapidly until the <code>11th</code> is about half of what one would expect it to be throughout the first half of the twentieth century. The gap shrinks in the second half of the twentieth century, but still&nbsp;persists at a smaller level&nbsp;until&nbsp;the end.</p>
<figure>
    <img alt="ordinals_1800-2008" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/ordinals_1800-2008_light.png#only-light">
    <img alt="ordinals_1800-2008" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/ordinals_1800-2008_dark.png#only-dark">
</figure>
<p>Astute graph readers will notice that something else weird is going on. There are four other lines that are much lower than they should be. From highest to lowest, they are the <code>2nd</code>, the <code>3rd</code>, the <code>22nd</code>, and the <code>23rd</code>.&nbsp;They were&nbsp;even lower than the <code>11th</code> from 1800 until&nbsp;the 1890s. However, starting around 1900, their gaps started shrinking even as the <code>11th</code> diverged until the gap disappeared completely in the 1930s. There is an interesting story there, but because their effect doesn't persist to the present, I'll continue to focus on the <code>11th</code> and leave the others&nbsp;for a <a href="https://drhagen.com/blog/the-missing-23rd-of-the-month/">future post</a>.</p>
<h2 id="typographical-hijinks">Typographical hijinks</h2>
<p>When I began this study, I was hoping to find a hidden taboo of holding events on the 11th or typographical&nbsp;bias&nbsp;against the&nbsp;shorthand ordinal. Alas, the reason is far is far more mundane: a numeral <code>1</code> looks a lot like a capital <code>I</code> or a lowercase <code>l</code> or a lowercase <code>i</code> in most of the fonts used for printing books. An <code>11</code> also looks like an <code>n</code>, apparently. Google's&nbsp;algorithms made mistakes when reading the <code>11th</code> from a page, interpreting&nbsp;the ordinal&nbsp;as some other word.</p>
<p>We can find some of these mistakes by directly searching for nonsense phrases like <code>March&nbsp;llth</code> or <code>July IIth</code> or <code>May iith</code>. There are nine possible combinations&nbsp;of <code>I</code>, <code>l</code>, and <code>i</code> that a <code>11</code> could be mistaken for. &nbsp;Five of them can actually be found in the database for at least one month: <code>IIth</code>, <code>Ilth</code>, <code>iith</code>, <code>lith</code>, and <code>llth</code>. Also found was <code>1lth</code>, <code>1ith</code>, and <code>l1th</code>, in which only one letter was misread. I collectively refer to these errors as&nbsp;<code>xxth</code>.&nbsp;<a href="https://books.google.com/">Google books</a>&nbsp;queries a newer database than the one on which Ngrams was built, but bona fide examples of the misreads can still be found. <a href="https://books.google.com/books?id=OJo3AAAAMAAJ&amp;dq=%22January%20IIth%22&amp;pg=RA3-PA34#v=onepage&amp;q=%22January%20IIth%22&amp;f=false">Here is something</a> that Google books thinks says <code>January IIth</code>: <img alt="January IIth" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/January_IIth_light.png#only-light"><img alt="January IIth" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/January_IIth_dark.png#only-dark">. <a href="https://books.google.com/books?id=EcJQAQAAIAAJ&amp;dq=%22February%20llth%22&amp;pg=RA1-PA79#v=onepage&amp;q=%22February%20llth%22&amp;f=false">And here is one</a> for <code>February llth</code>: <img alt="February llth" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/February_llth_light.png#only-light"><img alt="February llth" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/February_llth_dark.png#only-dark">. <a href="https://books.google.com/books?id=zYHk_df06QsC&amp;dq=%22March%20lith%22&amp;pg=PA402#v=onepage&amp;q=%22March%20lith%22&amp;f=false">And finally one</a>&nbsp;for <code>March lith</code>: <img alt="March lith" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/March_lith_light.png#only-light"><img alt="March lith" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/March_lith_dark.png#only-dark">. There are hordes of these in the database. You can find other ordinals that were misread as well, but the <code>11th</code> with its slender and ambiguous <code>1</code>s was misread far more often than the others.</p>
<p>I added back in every instance of <code>January IIth</code>, <code>January llth</code>, etc. to <code>January 11th</code> and did the same to the other months. The graph below shows that the <code>11th</code> gets a big&nbsp;boost by adding back&nbsp;the&nbsp;nonsense phrases. Before the 1860s,&nbsp;the difference between the <code>11th</code> and the main group&nbsp;is erased. After the 1860s, about a quarter to a third of the difference is erased.</p>
<figure>
    <img alt="xxth_1800-2008" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/xxth_1800-2008_light.png#only-light">
    <img alt="xxth_1800-2008" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/xxth_1800-2008_dark.png#only-dark">
</figure>
<h2 id="to-the-nth-degree">To the nth degree</h2>
<p>So where did&nbsp;the rest of the missing <code>11th</code> go? Well, starting in the 1860s, the Google algorithm starts to make a rather peculiar error—it misreads <code>11th</code> as <code>nth</code>. <a href="https://books.google.com/books?id=r7QaAAAAYAAJ&amp;dq=%22january%20nth%22&amp;pg=RA1-PA82#v=onepage&amp;q=%22january%20nth%22&amp;f=false">Here is one&nbsp;example from a page full</a> of <code>January nth</code>s: <img alt="January nth" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/January_nth_light.png#only-light"><img alt="January nth" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/January_nth_dark.png#only-dark">. In some years, the number of incorrect reads actually exceeds the number of correct reads. I added <code>January nth</code> to <code>January 11th</code> and did the same for all the months. The graph below shows both the&nbsp;<code>nth</code>&nbsp;and its sum with the <code>11th</code>.&nbsp;There was&nbsp;little impact&nbsp;before the 1860s, but then this error alone accounts for nearly all of the missing <code>11th</code>.</p>
<figure>
    <img alt="nth_1800-2008" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/nth_1800-2008_light.png#only-light">
    <img alt="nth_1800-2008" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/nth_1800-2008_dark.png#only-dark">
</figure>
<h2 id="combined-graph">Combined&nbsp;graph</h2>
<p>When the <code>xxth</code> misreads and&nbsp;<code>nth</code> misreads are both added back into&nbsp;the <code>11th</code>, the gap disappears across the entire timeline and the <code>11th</code>&nbsp;looks like an ordinary day of the year.&nbsp;This suggests that the misreading of the <code>11th</code> as <code>nth</code>, <code>IIth</code>, <code>llth</code>, etc. is sufficient to explain the unusually low incidence of the <code>11th</code> as a day of the month.</p>
<figure>
    <img alt="total_1800-2008" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/total_1800-2008_light.png#only-light">
    <img alt="total_1800-2008" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/total_1800-2008_dark.png#only-dark">
</figure>
<h2 id="typographical-machines">Typographical machines</h2>
<p>While it makes sense that the <code>11th</code> was&nbsp;misread more than others, why is the misread rate not uniform? What happened in the 1860s that caused the dramatic rise in the error rate? I suspect that it has something to do with&nbsp;a special device invented in the 1860s_—_the typewriter. <a href="https://web.archive.org/web/20190706134959/https://chronicle.com/blogs/linguafranca/2012/03/14/old-style-versus-lining-figures/">The earliest typewriters did not have a separate key for the numeral <code>1</code>.</a>&nbsp;Typists were expected to use the lowercase <code>l</code> to represent a <code>1</code>. When the algorithm read <code>October llth</code>, it was far more&nbsp;correct than we have been&nbsp;giving it credit. There are not that many documents in Google books that are typewritten, but&nbsp;this popular new contraption had a powerful effect on the evolution of fonts. The <code>1</code> and <code>l</code> were identical on the increasingly familiar typewriters, and&nbsp;the fonts even of printed materials began to reflect this expectation. Compare the <code>l</code>s and <code>1</code>s in this font <a href="https://books.google.com/books?id=i-lOAAAAYAAJ&amp;dq=%22january%2011th%22%201850&amp;pg=PA115#v=onepage&amp;q&amp;f=false">from&nbsp;1850</a>: <img alt="council_meeting" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/council_meeting_light.png#only-light"><img alt="council_meeting" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/council_meeting_dark.png#only-dark">.&nbsp;There is a clear difference between an <code>l</code> with no serifs on the top and the <code>1</code> with a pronounced serif. Compare that to a font <a href="https://books.google.com/books?id=STtaT_fheaMC&amp;lpg=PA334&amp;dq=%22january%2011th%22%201920&amp;pg=PA334#v=onepage&amp;q&amp;f=false">from 1920</a>: <img alt="hotel_on_sunday" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/hotel_on_sunday_light.png#only-light"><img alt="hotel_on_sunday" src="https://drhagen.com/blog/2015-12-29-the-missing-11th-of-the-month/images/hotel_on_sunday_dark.png#only-dark">.&nbsp;The characters are identical except for the kerning. Even to this day, most fonts represent both the <code>1</code> and the <code>l</code> as tall characters with two serifs on the bottom and one left-facing serif at the top. The only difference is that the serif on the <code>1</code> is slightly more angled than on the <code>l</code>. (In this post, I used a special monospace font to make it easier to tell the difference.)&nbsp;The print quality of more recent books (post 1970s) has reduced the rate of&nbsp;failure, but it still has not gone away entirely, so that the remaining failures were noticeable in the xkcd comic.</p>
<p>The largest open question is why <code>nth</code> was chosen so often. It seems like such a strange error to make. The word <code>nth</code> is a legal word&nbsp;in mathematical and scientific publications, so that should help its chances of getting picked. In most fonts the top of the <code>n</code> is really thin, and is likely invisible in many texts on which they trained the algorithm. But there is a big different in height between <code>1</code> and <code>n</code>, especially in the typewriter era, which is where the errors occur. And the phrase <code>January nth</code> is nonsense so that should have hurt its chances of being selected. Is it possible there&nbsp;was an error in one of the modern training texts that had an <code>11th</code> labeled as <code>nth</code>, thereby confusing the algorithm? The only way to know for sure would be to crack open the source code of Google's text-reading algorithm. This is left as an exercise for the reader.</p>
<h6 id="the-code-used-for-the-analysis-in-this-post-is-available-on-github">The <a href="https://github.com/drhagen/xkcd11th">code used for the analysis in this post</a> is available on&nbsp;Github.</h6>







  
  




  



<!--  Disqus needs `color-scheme: light;` as the base color scheme -->




      
    </article>
  </div>
        
      </main>
      
        
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bento: A Steam Deck in a Keyboard (176 pts)]]></title>
            <link>https://github.com/lunchbox-computer/bento</link>
            <guid>44313379</guid>
            <pubDate>Wed, 18 Jun 2025 21:21:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/lunchbox-computer/bento">https://github.com/lunchbox-computer/bento</a>, See on <a href="https://news.ycombinator.com/item?id=44313379">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Bento</h2><a id="user-content-bento" aria-label="Permalink: Bento" href="#bento"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19628643/455292447-d9bac3c8-aa03-4546-88ae-747c9bedbc12.jpeg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAzMTEzMDEsIm5iZiI6MTc1MDMxMTAwMSwicGF0aCI6Ii8xOTYyODY0My80NTUyOTI0NDctZDliYWMzYzgtYWEwMy00NTQ2LTg4YWUtNzQ3YzliZWRiYzEyLmpwZWc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjE5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYxOVQwNTMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zZjQ0MGY2OTM3NGRlY2FlMjAyZGM2YTRiNmU5N2I2YTM4Yzk3NTc1OTUzZDdiYmVhMDUwNjMwZDY1M2ZlYzA4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.6co6OoyR31ORb0-X9jAkTpHmIrEO2HqaPL4as2Q9mrY"><img src="https://private-user-images.githubusercontent.com/19628643/455292447-d9bac3c8-aa03-4546-88ae-747c9bedbc12.jpeg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAzMTEzMDEsIm5iZiI6MTc1MDMxMTAwMSwicGF0aCI6Ii8xOTYyODY0My80NTUyOTI0NDctZDliYWMzYzgtYWEwMy00NTQ2LTg4YWUtNzQ3YzliZWRiYzEyLmpwZWc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjE5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYxOVQwNTMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zZjQ0MGY2OTM3NGRlY2FlMjAyZGM2YTRiNmU5N2I2YTM4Yzk3NTc1OTUzZDdiYmVhMDUwNjMwZDY1M2ZlYzA4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.6co6OoyR31ORb0-X9jAkTpHmIrEO2HqaPL4as2Q9mrY" alt="A photo of Bento, a computer in a keyboard"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is Bento?</h2><a id="user-content-what-is-bento" aria-label="Permalink: What is Bento?" href="#what-is-bento"></a></p>
<p dir="auto">Bento is a computer. Its name come from it's distinctly bento box look, and it takes inspiration from the Comodore 64, and the many creations on r/cyberdeck.</p>
<p dir="auto">It fit perfectly underneath a keyboard, which acts as a lid! Giving you easy access to the internals, as well as a compartment to store various small peripherals.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19628643/455292895-4a38fb38-fbdb-43cf-b44e-4cf81801ce72.jpeg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAzMTEzMDEsIm5iZiI6MTc1MDMxMTAwMSwicGF0aCI6Ii8xOTYyODY0My80NTUyOTI4OTUtNGEzOGZiMzgtZmJkYi00M2NmLWI0NGUtNGNmODE4MDFjZTcyLmpwZWc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjE5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYxOVQwNTMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1mNDFkN2I1NTE5ZWIyOGYwNjk1YmM1NDRhNTI3OTAwNzE4YWM5MjJkMjljMzRiMDYzMjkyYjk3MmJlNmE3MDY3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.66KoyEW-xLdExMDUGGzvmbxga2HlQiXO1VorsV4rtI8"><img src="https://private-user-images.githubusercontent.com/19628643/455292895-4a38fb38-fbdb-43cf-b44e-4cf81801ce72.jpeg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAzMTEzMDEsIm5iZiI6MTc1MDMxMTAwMSwicGF0aCI6Ii8xOTYyODY0My80NTUyOTI4OTUtNGEzOGZiMzgtZmJkYi00M2NmLWI0NGUtNGNmODE4MDFjZTcyLmpwZWc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjE5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYxOVQwNTMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1mNDFkN2I1NTE5ZWIyOGYwNjk1YmM1NDRhNTI3OTAwNzE4YWM5MjJkMjljMzRiMDYzMjkyYjk3MmJlNmE3MDY3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.66KoyEW-xLdExMDUGGzvmbxga2HlQiXO1VorsV4rtI8" alt="Inside bento"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">There is no display</h2><a id="user-content-there-is-no-display" aria-label="Permalink: There is no display" href="#there-is-no-display"></a></p>
<p dir="auto">This is key. Bento is meant to be used with an external display, particularly spatial displays like the XREAL One’s, but obviously it still works with any external monitor with USB-C.</p>
<p dir="auto">The reason for this is to eliminate redundancy. As I find myself using XREAL more and more, the built in display for my laptop or my steam deck goes unused and is simply added weight. Bento removes all that is non-essential, cutting back on weight and making it even more portable.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What’s inside?</h2><a id="user-content-whats-inside" aria-label="Permalink: What’s inside?" href="#whats-inside"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19628643/455292501-3f0a9b38-8294-42f1-bb01-9b6f2e007e9c.jpeg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAzMTEzMDEsIm5iZiI6MTc1MDMxMTAwMSwicGF0aCI6Ii8xOTYyODY0My80NTUyOTI1MDEtM2YwYTliMzgtODI5NC00MmYxLWJiMDEtOWI2ZjJlMDA3ZTljLmpwZWc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjE5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYxOVQwNTMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wNzgyYjA0OTM4YTUwMWJkMWRjODk5YmNlM2JmMTAzYmY4ZjVmODI2YmUyNTMxMzVhMjMyOGRmY2Y2YmNjMmZkJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.rrVvKlrlVuGO_uEkcUEiT2WRF_FI1dRNvsH4d0H_78w"><img src="https://private-user-images.githubusercontent.com/19628643/455292501-3f0a9b38-8294-42f1-bb01-9b6f2e007e9c.jpeg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAzMTEzMDEsIm5iZiI6MTc1MDMxMTAwMSwicGF0aCI6Ii8xOTYyODY0My80NTUyOTI1MDEtM2YwYTliMzgtODI5NC00MmYxLWJiMDEtOWI2ZjJlMDA3ZTljLmpwZWc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNjE5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDYxOVQwNTMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wNzgyYjA0OTM4YTUwMWJkMWRjODk5YmNlM2JmMTAzYmY4ZjVmODI2YmUyNTMxMzVhMjMyOGRmY2Y2YmNjMmZkJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.rrVvKlrlVuGO_uEkcUEiT2WRF_FI1dRNvsH4d0H_78w" alt="a look under the hood"></a></p>
<p dir="auto">This version is powered by the main board of a Steam Deck OLED. It uses the same cooler and battery as well. I chose this board because it was the thinnest and most powerful available. But it can easily fit other SBCs that are more readily available.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why build this</h2><a id="user-content-why-build-this" aria-label="Permalink: Why build this" href="#why-build-this"></a></p>
<p dir="auto">Primarily out of frustration. The dominant players in XR keep promoting their hardware as “computers”, when really they’re an iPad for your face. The most you can do is browse the web, play games, and consume content. They’re overweight and over constrained.</p>
<p dir="auto">If you want to do any <em>real</em> work, the best you can do is mirror your screen from a <em>real</em> computer. This is one of the most popular use cases outside of gaming.</p>
<p dir="auto">If this is true, then we should lean into it! Create a spatial display and ship a computer explicitly designed for it.</p>
<p dir="auto">This is why I built Bento, a computer designed to be used with spatial displays. I propose to you that it is closer to a “spatial computer” than anything else we’ve seen.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What you'll find in this repo</h2><a id="user-content-what-youll-find-in-this-repo" aria-label="Permalink: What you'll find in this repo" href="#what-youll-find-in-this-repo"></a></p>
<p dir="auto">All the STEP, 3MF, and STL files for bento, as well as a few periperhals like a Magic Trackpad tray.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19628643/455293199-d634d455-5398-438e-89d9-d4f9e7b7ca3c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAzMTEzMDEsIm5iZiI6MTc1MDMxMTAwMSwicGF0aCI6Ii8xOTYyODY0My80NTUyOTMxOTktZDYzNGQ0NTUtNTM5OC00MzhlLTg5ZDktZDRmOWU3YjdjYTNjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA2MTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNjE5VDA1MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJlNGE2NzBhMjBmMzU1MWEzMWZkOTlkOTI2NjM4MzRhNGJlOGFiODk0ZmUyOTA1ZTVlYzUyMjliZmExMjMyYWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Sbc-wHMOYw3kd4X872F9A5MGDsEt4Sy_0ACxYa6H77M"><img src="https://private-user-images.githubusercontent.com/19628643/455293199-d634d455-5398-438e-89d9-d4f9e7b7ca3c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTAzMTEzMDEsIm5iZiI6MTc1MDMxMTAwMSwicGF0aCI6Ii8xOTYyODY0My80NTUyOTMxOTktZDYzNGQ0NTUtNTM5OC00MzhlLTg5ZDktZDRmOWU3YjdjYTNjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA2MTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNjE5VDA1MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJlNGE2NzBhMjBmMzU1MWEzMWZkOTlkOTI2NjM4MzRhNGJlOGFiODk0ZmUyOTA1ZTVlYzUyMjliZmExMjMyYWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Sbc-wHMOYw3kd4X872F9A5MGDsEt4Sy_0ACxYa6H77M" alt="A screenshot of Bento in Shapr3D"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Request for Contribution</h2><a id="user-content-request-for-contribution" aria-label="Permalink: Request for Contribution" href="#request-for-contribution"></a></p>
<p dir="auto">I intend to keep working on this project, but I’m open sourcing it because I can only do so much on my own. I’m currently working on a “pro” version with a switch 2 like mounting system to support modules. But here are some things I could use help with:</p>
<ul dir="auto">
<li><strong>Support for other keyboard</strong>: I picked the Magic Keyboard due to how ubiquitous it is, but there are better keyboards out there.</li>
<li><strong>Raspberry Pi 5 variant</strong>: this is crucial, but I can’t find a <em>good</em> battery solution (i.e. a HAT of some kind.)</li>
<li><strong>A framework variant</strong>:  This may require a larger base keyboard and will likely eliminate storage (physical, not digital.)</li>
<li>Support for other SBCs in general.</li>
<li><strong>Peripherals</strong>: I have some sketches for a gamepad and a mouse that could fit perfectly in the compartment, but I would happily acquiesce to someone else with board design skills.</li>
</ul>
<p dir="auto">If you do decide to create your own variant, all I ask is that you <strong>make a PR back to this repository</strong>, so that other’s might benefit from your work.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Websites Are Tracking You via Browser Fingerprinting (248 pts)]]></title>
            <link>https://engineering.tamu.edu/news/2025/06/websites-are-tracking-you-via-browser-fingerprinting.html</link>
            <guid>44313206</guid>
            <pubDate>Wed, 18 Jun 2025 20:55:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.tamu.edu/news/2025/06/websites-are-tracking-you-via-browser-fingerprinting.html">https://engineering.tamu.edu/news/2025/06/websites-are-tracking-you-via-browser-fingerprinting.html</a>, See on <a href="https://news.ycombinator.com/item?id=44313206">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent" name="Main Page Content" role="main" aria-labelledby="maincontent">
						
					
																																		                	            	     			<div aria-label="Websites Are Tracking You Via Browser Fingerprinting content 0" role="region"><p>Clearing your cookies is not enough to protect your privacy online.&nbsp;</p>
<p><a href="https://dl.acm.org/doi/10.1145/3696410.3714548" rel="noopener" target="_blank">New research</a> led by Texas A&amp;M University found that websites are covertly using browser fingerprinting — a method to uniquely identify a web browser — to track people across browser sessions and sites.</p>
<p>“Fingerprinting has always been a concern in the privacy community, but until now, we had no hard proof that it was actually being used to track users,” said Dr. Nitesh Saxena, cybersecurity researcher, professor of computer science and engineering and associate director of the Global Cyber Research Institute&nbsp;at Texas A&amp;M. “Our work helps close that gap.”</p>
<p>When you visit a website, your browser shares a surprising amount of information, like your screen resolution, time zone, device model and more. When combined, these details create a “fingerprint” that’s often unique to your browser. Unlike cookies — which users can delete or block — fingerprinting is much harder to detect or prevent. Most users have no idea it’s happening, and even privacy-focused browsers struggle to fully block it.</p>
<p>“Think of it as a digital signature you didn’t know you were leaving behind,” explained co-author Zengrui Liu, a former doctoral student in Saxena’s lab. “You may look anonymous, but your device or browser gives you away.”</p>
<p>This research marks a turning point in how computer scientists understand the real-world use of browser fingerprinting by connecting it with the use of ads.</p>
<p>“While prior works have studied browser fingerprinting and its usage on different&nbsp;websites, ours is the first to correlate browser fingerprints and ad behaviors, essentially establishing the relationship between web tracking and fingerprinting,” said co-author Dr. Yinzhi Cao, associate professor of computer science and technical director of the Information Security Institute at Johns Hopkins University.</p></div>
				
			
																																					
	
	
		
	
				
																		
																								
								
						
						
										
						
						
								
								
						
						
						
						
														
		
									        	        		<div>
        			<p>Think of it as a digital signature you didn’t know you were leaving behind. You may look anonymous, but your device or browser gives you away.</p>
        			<p><cite>Zengrui Liu</cite>
        		</p></div>
        	        				
			
																																		                	            	     			<div aria-label="Websites Are Tracking You Via Browser Fingerprinting content 1" role="region"><p>To investigate whether websites are using fingerprinting data to track people, the researchers had to go beyond simply scanning websites for the presence of fingerprinting code. They developed a measurement framework called FPTrace, which assesses fingerprinting-based user tracking by analyzing how ad systems respond to changes in browser fingerprints. This approach is based on the insight that if browser fingerprinting influences tracking, altering fingerprints should affect advertiser bidding — where ad space is sold in real time based on the profile of the person viewing the website — and HTTP records — records of communication between a server and a browser.&nbsp;</p>
<p>“This kind of analysis lets us go beyond the surface,” said co-author Jimmy Dani, Saxena’s doctoral student. “We were able to detect not just the presence of fingerprinting, but whether it was being used to <i>identify</i> and <i>target</i> users — which is much harder to prove.”</p>
<p>The researchers found that tracking occurred even when users cleared or deleted cookies. The results showed notable differences in bid values and a decrease in HTTP records and syncing events when fingerprints were changed, suggesting an impact on targeting and tracking.</p>
<p>Additionally, some of these sites linked fingerprinting behavior to backend bidding processes — meaning fingerprint-based profiles were being used in real time, likely to tailor responses to users or pass along identifiers to third parties.&nbsp;</p>
<p>Perhaps more concerning, the researchers found that even users who explicitly opt out of tracking under privacy laws like Europe’s General Data Protection Regulation (GDPR) and California’s California Consumer Privacy Act (CCPA) may still be silently tracked across the web through browser fingerprinting.</p>
<p>Based on the results of this study, the researchers argue that current privacy tools and policies are not doing enough. They call for stronger defenses in browsers and new regulatory attention on fingerprinting practices. They hope that their FPTrace framework can help regulators audit websites and providers who participate in such activities, especially without user consent.&nbsp;</p>
<p>This research was conducted in collaboration with Johns Hopkins University and presented at the ACM Web Conference (WWW) 2025.</p>
<p>Funding for this research is administered by the Texas A&amp;M Engineering Experiment Station (TEES), the official research agency for Texas A&amp;M Engineering.</p></div>
				
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Game Hacking – Valve Anti-Cheat (VAC) (133 pts)]]></title>
            <link>https://codeneverdies.github.io/posts/gh-2/</link>
            <guid>44311682</guid>
            <pubDate>Wed, 18 Jun 2025 17:19:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codeneverdies.github.io/posts/gh-2/">https://codeneverdies.github.io/posts/gh-2/</a>, See on <a href="https://news.ycombinator.com/item?id=44311682">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><hr><blockquote><h2 id="intro">Intro</h2></blockquote><p>In 2002 Valve created an Anti-Cheat solution called “Valve Anti-Cheat” aka <strong>VAC</strong>.
The first game they implemented VAC into was Counter-Strike. When <strong>VAC</strong> was introduced it only operated in
User Mode (Still does) meaning it runs entirely in user space <sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> and has no kernel component.</p><p>Below is a list of games that use <strong>VAC</strong>.. <sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p><pre><code>Call of Duty: Modern Warfare 2
Call of Duty: Modern Warfare 3
Counter-Strike (video game)
Counter-Strike: Condition Zero
Counter-Strike: Source
Counter-Strike 2
Day of Defeat
Day of Defeat: Source
Deathmatch Classic
Half-Life 2: Deathmatch
Half-Life Deathmatch: Source
Ricochet
Team Fortress
Team Fortress Classic
</code></pre><p>A longer list can be found here <sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.</p><hr><blockquote><h2 id="vac-cident">VAC-cident?</h2></blockquote><p>So.. if you don’t know <strong>VAC</strong> has been around for quite a while, at the time of writing it’ll be 23 years.
Over the time they’ve made some mistakes but who doesn’t? (Taken from wikipedia) <sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> <sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup></p><pre><code>- In July 2010, [snip] Approximately 12,000 owners of Call of Duty: Modern Warfare 2 were 
banned when Steam updated a DLL file on disk after it had been loaded into memory by the 
game, causing a false positive detection. These bans were revoked and those affected received 
a free copy of Left 4 Dead 2 or an extra copy to send as a gift. 

- In October 2023, certain users of AMD graphics cards were banned from Counter-Strike 2 
after AMD added support for their "Anti-Lag+" feature via a driver update, which the game 
flagged as a cheat due to it detouring certain DLL functions. AMD subsequently withdrew the 
driver update and Valve pledged to unban any affected users.
</code></pre><p>This post isn’t created to bash Valve they clean up after their mistakes and listen to their community,
gotta love devs when they do that. I also commend them because getting <strong>VAC</strong> banned isn’t such a slap
on the wrist. Getting <strong>VAC</strong> banned has some stipulations such as:</p><ul><li>Having the <strong>VAC</strong> ban show on your Steam profile</li><li>Being banned from all <strong>“GoldSrc”</strong> games</li><li>Being banned from all <strong>“Source engine”</strong> games (The Counter-Strike serise)</li><li>Not being able to <strong>refund</strong> the game you’re <strong>VAC</strong> banned on</li></ul><p>Knowing what’ll happen if you get <strong>VAC</strong> banned is important to know because regardless if
you’re cheating or not false bans are no good. People in the community took it upon themselves
to reverse engineer the Anti-Cheat and understand what it does (some did it just to cheat).</p><hr><blockquote><h2 id="vac-what">VAC what?</h2></blockquote><p>The previous section brings us to a term you may have heard before, the infamous <strong>“VAC Bypass”</strong>.
Searching online for information about bypassing <strong>VAC</strong> brings many blogs and repos that all seem to do/talk about something
similar and that’s “Dumping the <strong>VAC</strong> modules”. Let me explain, <strong>VAC</strong> is <strong>NOT</strong> just one executable on the system it streams it’s
Anti-Cheat modules (DLLs) from a server, once a module is recieved by some routine in <strong>steamservice.dll</strong> inside <strong>steamservice.exe</strong>
(or <strong>steam.exe</strong> if <strong>ran as admin</strong>) it will be loaded using one of the two methods below. <sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> <sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup> <sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup> <sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup> <sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup> <sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup> <sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup> <sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup></p><ul><li><strong>Reflectively load</strong> the DLL into memory</li><li>Use the WinAPI function <strong>LoadLibrary</strong></li></ul><p>By default the Anti-Cheat modules are reflectively loaded into memory. The goal is to force <strong>LoadLibrary</strong> into being used,
then someone can hook that function and dump the modules (DLLs) to disk, allowing someone to analyse the dumped DLLs and understand
what they’re doing to detect cheating.</p><hr><blockquote><h2 id="dumping-vac-modules-in-the-big-25">Dumping VAC Modules in the big ‘25</h2></blockquote><p>To kick start the journey on dumping the <strong>VAC</strong> modules load <strong>steamservice.dll</strong> into <strong>Binary Ninja</strong>. Once the binary is fully analyzed
go to “Triage Summary”</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-1.png" data-src="/VAC-1.png" data-srcset="/VAC-1.png, /VAC-1.png 1.5x, /VAC-1.png 2x" data-sizes="auto" alt="/VAC-1.png" title="VAC-1" srcset="https://codeneverdies.github.io/VAC-1.png, https://codeneverdies.github.io/VAC-1.png 1.5x, https://codeneverdies.github.io/VAC-1.png 2x"></p></blockquote><p>It is <strong>VERY</strong> important to take note that this is a 32-bit process, so all pointers will be 32-bits in size you’ll see why this is important later.</p><p>Next we’ll search for calls to <strong>LoadLibrary*</strong> I’ll save the reader some time and tell you that we should be looking
for calls to <strong>LoadLibraryW</strong> it will be called in a very important function that we can use to back track.</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-2.png" data-src="/VAC-2.png" data-srcset="/VAC-2.png, /VAC-2.png 1.5x, /VAC-2.png 2x" data-sizes="auto" alt="/VAC-2.png" title="VAC-2" srcset="https://codeneverdies.github.io/VAC-2.png, https://codeneverdies.github.io/VAC-2.png 1.5x, https://codeneverdies.github.io/VAC-2.png 2x"></p></blockquote><p>Following the reference takes us to an interesting function <strong>sub_10086f80</strong></p><blockquote><p><img src="https://codeneverdies.github.io/VAC-3.png" data-src="/VAC-3.png" data-srcset="/VAC-3.png, /VAC-3.png 1.5x, /VAC-3.png 2x" data-sizes="auto" alt="/VAC-3.png" title="VAC-3" srcset="https://codeneverdies.github.io/VAC-3.png, https://codeneverdies.github.io/VAC-3.png 1.5x, https://codeneverdies.github.io/VAC-3.png 2x"></p></blockquote><p>Judging by the return value <strong>HMODULE</strong> and the calls to LoadLibrary* it’s safe to say this function’s job is to load
some kind of module and return a handle to it. Following references to where this function is called leads us to another interesting
function <strong>sub_10086c40</strong>.</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-4.png" data-src="/VAC-4.png" data-srcset="/VAC-4.png, /VAC-4.png 1.5x, /VAC-4.png 2x" data-sizes="auto" alt="/VAC-4.png" title="VAC-4" srcset="https://codeneverdies.github.io/VAC-4.png, https://codeneverdies.github.io/VAC-4.png 1.5x, https://codeneverdies.github.io/VAC-4.png 2x"></p></blockquote><p>The beginning of the <strong>sub_10086c40</strong> function didn’t look too important (at the time) but we should remember that this function also returns a handle to a module.
I looked at the references and it shows that this function is called once in the function <strong>sub_10059040</strong>.</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-17.png" data-src="/VAC-17.png" data-srcset="/VAC-17.png, /VAC-17.png 1.5x, /VAC-17.png 2x" data-sizes="auto" alt="/VAC-17.png" title="VAC-17" srcset="https://codeneverdies.github.io/VAC-17.png, https://codeneverdies.github.io/VAC-17.png 1.5x, https://codeneverdies.github.io/VAC-17.png 2x"></p></blockquote><p>We can see <strong>sub_10086c40</strong> being called if we trace back the <strong>first</strong> argument passed to that function,
we’ll see that it was used by another function <strong>sub_100859d0</strong>. If that function call is successful
execution carries on, so it’s safe to say it’s important. Let’s take a look at this function.</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-18.png" data-src="/VAC-18.png" data-srcset="/VAC-18.png, /VAC-18.png 1.5x, /VAC-18.png 2x" data-sizes="auto" alt="/VAC-18.png" title="VAC-5" srcset="https://codeneverdies.github.io/VAC-18.png, https://codeneverdies.github.io/VAC-18.png 1.5x, https://codeneverdies.github.io/VAC-18.png 2x"></p></blockquote><p>This function makes two WinAPI calls</p><ul><li><p><strong>GetTempPathW</strong>
: Retrieves the path of the directory designated for temporary files.</p></li><li><p><strong>GetTempFileNameW</strong>
: Creates a name for a temporary file. If a unique file name is generated, an empty file is created and the handle to it is released; otherwise, only a file name is generated.</p></li></ul><p>The combination of these calls tells us that we need to be looking for any <code>.TMP</code> files being accessed, the names are usually in this format <code>&lt;uuuu&gt;.TMP</code>.</p><p>Now there’s a path to a DLL floating in memory how does it get used? Look no more.</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-5.png" data-src="/VAC-5.png" data-srcset="/VAC-5.png, /VAC-5.png 1.5x, /VAC-5.png 2x" data-sizes="auto" alt="/VAC-5.png" title="VAC-5" srcset="https://codeneverdies.github.io/VAC-5.png, https://codeneverdies.github.io/VAC-5.png 1.5x, https://codeneverdies.github.io/VAC-5.png 2x"></p></blockquote><pre tabindex="0"><code>100591f7   HMODULE eax_13 = sub_10086c40(edi_1, 0)
100591ff   *(esi + 4) = eax_13
100591ff   
10059204   if (eax_13 != 0)
10059215       int32_t eax_14 = sub_10086c20(eax_13, "_runfunc@20")
1005921d       *(esi + 0xc) = eax_14
</code></pre><p>The path <code>edi_1</code> is used by <strong>sub_10086c40</strong>, this function is used to get a handle to a module <code>eax_13</code> then it’s passing that handle to <strong>sub_10086c20</strong>.
<strong>sub_10086c20</strong> takes two arguments we know the first is a handle to a module the second is from what we can see here a string <code>_runfunc@20</code>, the return value
<code>int32_t</code> looks a little weird but this is a 32-bit process remember ;) so this could be a pointer to something dont ya think? Here’s the function prototype</p><pre tabindex="0"><code>int32_t sub_10086c20(HMODULE arg1, PSTR arg2)
</code></pre><p>Place your bets on it being a GetProcAddress wrapper.. Drum roll please… It is…</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-6.png" data-src="/VAC-6.png" data-srcset="/VAC-6.png, /VAC-6.png 1.5x, /VAC-6.png 2x" data-sizes="auto" alt="/VAC-6.png" title="VAC-6" srcset="https://codeneverdies.github.io/VAC-6.png, https://codeneverdies.github.io/VAC-6.png 1.5x, https://codeneverdies.github.io/VAC-6.png 2x"></p></blockquote><p>So with this bit of information we know <strong>steamservice.dll</strong> recieves the <strong>VAC</strong> modules, it’s using a function <strong>sub_10086c40</strong> which calls
<strong>sub_10086f80</strong> to load the Anti-Cheat module and return a handle, then that handle is passed to <strong>sub_10086c20</strong> to get the address
of a function named <code>_runfunc@20</code>. By default as said earlier the modules are reflectively loaded so this isn’t the regular control flow of
<strong>steamservice.dll</strong>, this can be confirmed if you scroll up a bit in <strong>sub_10059040</strong> you’ll see a flag being checked.</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-7.png" data-src="/VAC-7.png" data-srcset="/VAC-7.png, /VAC-7.png 1.5x, /VAC-7.png 2x" data-sizes="auto" alt="/VAC-7.png" title="VAC-7" srcset="https://codeneverdies.github.io/VAC-7.png, https://codeneverdies.github.io/VAC-7.png 1.5x, https://codeneverdies.github.io/VAC-7.png 2x"></p></blockquote><p><strong>steamservice.dll</strong> will most likely take this path unless we can do something about it</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-8.png" data-src="/VAC-8.png" data-srcset="/VAC-8.png, /VAC-8.png 1.5x, /VAC-8.png 2x" data-sizes="auto" alt="/VAC-8.png" title="VAC-8" srcset="https://codeneverdies.github.io/VAC-8.png, https://codeneverdies.github.io/VAC-8.png 1.5x, https://codeneverdies.github.io/VAC-8.png 2x"></p></blockquote><p>Let’s look at it in assembly</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-9.png" data-src="/VAC-9.png" data-srcset="/VAC-9.png, /VAC-9.png 1.5x, /VAC-9.png 2x" data-sizes="auto" alt="/VAC-9.png" title="VAC-9" srcset="https://codeneverdies.github.io/VAC-9.png, https://codeneverdies.github.io/VAC-9.png 1.5x, https://codeneverdies.github.io/VAC-9.png 2x"></p></blockquote><p>Take a look at <code>je 0x10059127</code> ( <code>0x74 0x47</code> )</p><p><code>0x74</code> is the jump if equal instruction and <code>0x47</code> is how many bytes forward to jump (71) in hex</p><p>What we wan’t to do is change the first instruction at <strong>steamservice.dll</strong> + 0x590DE (0x100590de)</p><ul><li>to <code>jne 0x10059127</code> ( <code>0x75 0x47</code> )</li></ul><p>we’re changing the first byte of this instruction to be <code>0x75</code> which is jump if <strong>NOT</strong>
zero/equal. (Inverting)</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-10.png" data-src="/VAC-10.png" data-srcset="/VAC-10.png, /VAC-10.png 1.5x, /VAC-10.png 2x" data-sizes="auto" alt="/VAC-10.png" title="VAC-10" srcset="https://codeneverdies.github.io/VAC-10.png, https://codeneverdies.github.io/VAC-10.png 1.5x, https://codeneverdies.github.io/VAC-10.png 2x"></p></blockquote><p>Now that we have a potential way of dumping the <strong>VAC</strong> modules let’s test it! first we start steam and launch <strong>x32dbg</strong> as
admin we should remember the offset to our instructions <strong>steamservice.dll</strong> + 0x590DE.</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-11.png" data-src="/VAC-11.png" data-srcset="/VAC-11.png, /VAC-11.png 1.5x, /VAC-11.png 2x" data-sizes="auto" alt="/VAC-11.png" title="VAC-11" srcset="https://codeneverdies.github.io/VAC-11.png, https://codeneverdies.github.io/VAC-11.png 1.5x, https://codeneverdies.github.io/VAC-11.png 2x"></p></blockquote><p>Once <strong>x32dbg</strong> is loaded attach to <strong>steamservice.dll</strong></p><blockquote><p><img src="https://codeneverdies.github.io/VAC-12.png" data-src="/VAC-12.png" data-srcset="/VAC-12.png, /VAC-12.png 1.5x, /VAC-12.png 2x" data-sizes="auto" alt="/VAC-12.png" title="VAC-12" srcset="https://codeneverdies.github.io/VAC-12.png, https://codeneverdies.github.io/VAC-12.png 1.5x, https://codeneverdies.github.io/VAC-12.png 2x"></p></blockquote><p>Press <code>CTRL+G</code> and enter <code>steamservice.dll + 0x590DE</code></p><blockquote><p><img src="https://codeneverdies.github.io/VAC-13.png" data-src="/VAC-13.png" data-srcset="/VAC-13.png, /VAC-13.png 1.5x, /VAC-13.png 2x" data-sizes="auto" alt="/VAC-13.png" title="VAC-13" srcset="https://codeneverdies.github.io/VAC-13.png, https://codeneverdies.github.io/VAC-13.png 1.5x, https://codeneverdies.github.io/VAC-13.png 2x"></p></blockquote><p>Now we’re where we need to patch</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-14.png" data-src="/VAC-14.png" data-srcset="/VAC-14.png, /VAC-14.png 1.5x, /VAC-14.png 2x" data-sizes="auto" alt="/VAC-14.png" title="VAC-14" srcset="https://codeneverdies.github.io/VAC-14.png, https://codeneverdies.github.io/VAC-14.png 1.5x, https://codeneverdies.github.io/VAC-14.png 2x"></p></blockquote><p>Right click on that instruction and click “Assemble” then enter <code>jnz 0x10059127</code> and hit ok</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-15.png" data-src="/VAC-15.png" data-srcset="/VAC-15.png, /VAC-15.png 1.5x, /VAC-15.png 2x" data-sizes="auto" alt="/VAC-15.png" title="VAC-15" srcset="https://codeneverdies.github.io/VAC-15.png, https://codeneverdies.github.io/VAC-15.png 1.5x, https://codeneverdies.github.io/VAC-15.png 2x"></p></blockquote><p>It should be changed</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-16.png" data-src="/VAC-16.png" data-srcset="/VAC-16.png, /VAC-16.png 1.5x, /VAC-16.png 2x" data-sizes="auto" alt="/VAC-16.png" title="VAC-16" srcset="https://codeneverdies.github.io/VAC-16.png, https://codeneverdies.github.io/VAC-16.png 1.5x, https://codeneverdies.github.io/VAC-16.png 2x"></p></blockquote><p>The next step is to open <strong>Procmon</strong> play a game that uses <strong>VAC</strong> (I chose CSGO) and wait for
<strong>steamservice.exe</strong> to access some <code>.TMP</code> files.</p><p>Here are the <strong>Procmon</strong> filters</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-19.png" data-src="/VAC-19.png" data-srcset="/VAC-19.png, /VAC-19.png 1.5x, /VAC-19.png 2x" data-sizes="auto" alt="/VAC-19.png" title="VAC-19" srcset="https://codeneverdies.github.io/VAC-19.png, https://codeneverdies.github.io/VAC-19.png 1.5x, https://codeneverdies.github.io/VAC-19.png 2x"></p></blockquote><p>While loading the game we see our first TMP file <code>C:\Windows\Temp\D54A.tmp</code></p><blockquote><p><img src="https://codeneverdies.github.io/VAC-20.png" data-src="/VAC-20.png" data-srcset="/VAC-20.png, /VAC-20.png 1.5x, /VAC-20.png 2x" data-sizes="auto" alt="/VAC-20.png" title="VAC-20" srcset="https://codeneverdies.github.io/VAC-20.png, https://codeneverdies.github.io/VAC-20.png 1.5x, https://codeneverdies.github.io/VAC-20.png 2x"></p></blockquote><p>Let’s join a public match and see if there are others</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-21.png" data-src="/VAC-21.png" data-srcset="/VAC-21.png, /VAC-21.png 1.5x, /VAC-21.png 2x" data-sizes="auto" alt="/VAC-21.png" title="VAC-21" srcset="https://codeneverdies.github.io/VAC-21.png, https://codeneverdies.github.io/VAC-21.png 1.5x, https://codeneverdies.github.io/VAC-21.png 2x"></p></blockquote><p>And some more..</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-22.png" data-src="/VAC-22.png" data-srcset="/VAC-22.png, /VAC-22.png 1.5x, /VAC-22.png 2x" data-sizes="auto" alt="/VAC-22.png" title="VAC-22" srcset="https://codeneverdies.github.io/VAC-22.png, https://codeneverdies.github.io/VAC-22.png 1.5x, https://codeneverdies.github.io/VAC-22.png 2x"></p></blockquote><p>We can also look at these files in the temp directory..</p><blockquote><p><img src="https://codeneverdies.github.io/VAC-23.png" data-src="/VAC-23.png" data-srcset="/VAC-23.png, /VAC-23.png 1.5x, /VAC-23.png 2x" data-sizes="auto" alt="/VAC-23.png" title="VAC-23" srcset="https://codeneverdies.github.io/VAC-23.png, https://codeneverdies.github.io/VAC-23.png 1.5x, https://codeneverdies.github.io/VAC-23.png 2x"></p></blockquote><p>I copied all of these files to a new directory and loaded <code>D54A.tmp</code> into <strong>PE-bear</strong></p><blockquote><p><img src="https://codeneverdies.github.io/VAC-24.png" data-src="/VAC-24.png" data-srcset="/VAC-24.png, /VAC-24.png 1.5x, /VAC-24.png 2x" data-sizes="auto" alt="/VAC-24.png" title="VAC-24" srcset="https://codeneverdies.github.io/VAC-24.png, https://codeneverdies.github.io/VAC-24.png 1.5x, https://codeneverdies.github.io/VAC-24.png 2x"></p></blockquote><p>We see something familiar <code>_runfunc@20</code> this is the function that was found using <strong>sub_10086c20</strong>.</p><hr><blockquote><h2 id="to-be-continued">To be continued</h2></blockquote><p>In the next post we will be doing analysis on these Anti-Cheat Modules to get a better understanding of what’s going on.
I hope you enjoyed this post and most importantly learned a thing or two. Stay tuned!</p><hr><blockquote><h2 id="references">References</h2></blockquote></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Andrej Karpathy's YC AI SUS talk on the future of the industry (229 pts)]]></title>
            <link>https://www.donnamagi.com/articles/karpathy-yc-talk</link>
            <guid>44311509</guid>
            <pubDate>Wed, 18 Jun 2025 16:56:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.donnamagi.com/articles/karpathy-yc-talk">https://www.donnamagi.com/articles/karpathy-yc-talk</a>, See on <a href="https://news.ycombinator.com/item?id=44311509">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://www.donnamagi.com/">back</a></p><p>This is a transcript of Andrej Karpathy's talk on Software 3.0 on June 17th. I was in the audience, and it was recorded in a noisy environment - set your expectations accordingly.</p><p>YC has said the official video will take a few weeks to release, by which Karpathy himself agrees the talk will be deprecated.<!-- --> <a href="https://x.com/karpathy/status/1935077692258558443" target="_blank" referrerpolicy="origin">https://x.com/karpathy/status/1935077692258558443</a></p><p>and ok wow, this is going viral. let's<!-- --> <a href="https://x.com/DonnaMagi" target="_blank" referrerpolicy="origin">keep in touch</a>?</p><p>I think it's actually an extremely unique and very interesting time to enter the industry right now. And I think fundamentally the reason for that is that software is changing. Again. And I say again because I actually gave this talk already. But the problem is that software keeps changing, so I actually have a lot of material to create new talks. And I think it's changing quite fundamentally. I think broadly speaking, software has not changed much at such a fundamental level for 70 years. And then it's changed, I think, about twice quite rapidly in the last few years. And so there's just a huge amount of work to do, a huge amount of software to write and rewrite.</p><p>So let's take a look at maybe the realm of software. So if we're going to think of this as like a map of software, this is a really cool tool called Map. This is kind of like all the software that's written. These aren't instructions. Computer for clearing out tasks in digital space. So if you zoom in here, these are all different kinds of repositories. And this is all the code that has been written. And a few years ago, I kind of observed that software was kind of changing and there was kind of like a new type of software around. And I called this Software 2.0 at the time. And the idea here was that Software 1.0 is the code you write on the computer. Software 2.0 are basically neural networks. In particular, the weights of the neural network. And you're not writing this code correctly. You're more like tuning the data sets and then you're running an optimizer to create the parameters. And I think at the time, neural networks were kind of seen as just a different kind of classifier, like a decision tree or something like that. And so I think this training was a lot more appropriate.</p><p>And now actually what we have is kind of like an equivalent of GitHub in the realm of Software 2.0. And I think the hugging face is basically an equivalent of GitHub in Software 2.0. And there's also a model atlas and you can visualize all the code written there. In case you're curious, by the way, the giant circle, the point in the middle, these are the parameters of Flux, the image generator. And so any time someone tunes a lower on top of a Flux model, you basically create a GitHub event in this space and create a different kind of image generator. So basically what we have is Software 1.0 is the computer code that programs the computer. Software 2.0 are the weights which program neural networks. And here's an example of AlexNet image recognizer neural network. Now so far, all of the neural networks that we've been familiar with until recently were kind of like fixed-functional computers. Image categories or something like that. And I think what's changed, and I think it's a fundamental change, is that neural networks became programmable with large libraries. And so I see this as quite new, unique. It's a new kind of computer. And in my mind, it's worth giving it the designation of a Software 3.0. And basically your products are now programs that program people all over. And remarkably, these products are written in English. So it's kind of a very interesting programming language.</p><p>So maybe to summarize the business, if you're doing central classification, for example, you can imagine writing some Python to basically do central classification. Or you can train neural networks. Or you can drop a large amount of code. So here I'm just using SoftPrompt, and you can imagine changing it and programming the computer's life. So basically we have Software 1.0, Software 2.0. And I think we're seeing, I mean, you've seen a lot of GitHub code is not just like code anymore. There's a bunch of English interspersed with code. And so I think there's a growing category of that kind of code. So not only is it a new programming paradigm, it's also remarkable to me that it's in our native language of English. And so this blew my mind a few years ago now. I tweeted this, and I think it captured the attention of a lot of people. And one of the things that I currently pinpoint to them is that arguably we're not programming computers in English.</p><p>Now when I was at Tesla, we were working on the Autopilot, and we were trying to get the car to drive. And I sort of showed this slide at the time where you can imagine that the inputs for the car are on the bottom, and they're going through the software stack to produce the steering and acceleration. And I made the observation at the time that there was a ton of C++ code around in the Autopilot, which was the software model development. And that there were some neural nets in there doing the interactions. And I kind of observed that over time as we made the Autopilot better, basically the neural network grew in capability and size. And in addition to that, all this C++ code was being completed. And a lot of the capabilities and functionality that was originally in 1.0 was migrated to 2.0. So as an example, a lot of the stitching up of information across images from the different cameras across time was done by neural network, and we were able to delete a lot of code. And so the software development stack was quite literally made through the software stack of the Autopilot. So I thought this was a brilliant model at the time. And I think we're seeing the same thing again, where basically we have a new kind of software, and it's being through the stack, made through a completely different programming paradigm.</p><p>And I think if you're entering the industry, it's a very good idea to be fluent in all of them. Because they all have slight pros and cons, and you may want to program some functionality in 1.0 or 2.0, or 3.0, or you're going to train in LLM, or you're going to just run from LLM, that shouldn't be any software that's explicit, etc. So we don't have to make these decisions to actually potentially fluidly transition to LLMs.</p><p>So what I want to get into now is, first I want to, in the first part, talk about LLMs, and what I think of this new paradigm in the ecosystem, and what that looks like. What is this new computer? What does it look like? And what does the ecosystem look like? I was struck by this quote from Andrew, actually many years ago now, I think. And I think Andrew is going to speak right after me. But he said that the term AI is probably not his thing. And I do think that it captures something very interesting, in that LLMs certainly feel like they have properties of utilities, right? So, LLM labs, like OpenAI, Gemini, Fungi, etc, they spend time to train the LLMs, and this is kind of equivalent to a built-in AI algorithm, and then there's op-ecs to serve them intelligence over APIs to all of us. And this is done through internet access, where we pay per million tokens or something like that, and we have a lot of demands that are very utility-like demands out of this API. We demand low latency, high uptime, etc.</p><p>In electricity, you would have a transfer switch, so you can transfer your electricity source from, like, grid, solar, or battery, or generator. In LLMs, we have maybe open router, and easily switch between the different types of LLMs that exist. Because the LLMs are software, they don't need more physical space, so it's okay to have, basically, like six electricity providers that you can switch between, right? Because they don't need this directly. And I think what's also really fascinating, and we saw this in the last few days, actually, a lot of the LLMs went down, and people were kind of stuck and unable to work.</p><p>And I think it's kind of fascinating to me that when the state-of-the-art LLMs go down, it's actually kind of like an intelligence brownout in the world. It's kind of like when the voltage is unreliable on the grid, and the planet just gets smaller. The more reliance we have on these models, which already is, like, really dramatic, and I think will continue to grow. But LLMs don't only have case of utilities. I think it's also fair to say that they have some power tools called CAPs. And the reason for this is that the CAPs required for building LLMs is actually quite large. It's not just like building some power station or something like that, right? You're investing a huge amount of money, and I think the technology is growing quite rapidly. So we're in a world where we have, sort of, deep tech trees, research and development, secrets, that are centralizing our shiny LLM labs. But I think the analogy varies a little bit also, because, as I mentioned, this is software. And software is a bit less expensive, but because it is so valuable. And so I think that's just an interesting kind of thing to think about.</p><p>There's many analogies you can make, like a formatted data process, and maybe, for instance, something like a cluster with certain max-ops. And you can think about, when you're using it, you actually use it only through the software, not through the hardware. That's kind of what the CAPs are. But if you're actually also building it on hardware, and you're sharing it using Google, that's kind of like an integral on your platform. So I think there's analogies here that make sense. But actually, I think the analogy that makes the most sense, perhaps, is that, in my mind, LLM's have very strong analogies to operating systems, in that this is not just electricity or power. It's not something that comes automatically to happen. It's a commodity. These are now increasingly complex software ecosystems. So they're not just simple commodities like electricity. And it's kind of interesting to me that the ecosystem is shaping in a very similar kind of way, where you have a few closed-source providers, like Windows and macOS, and then you have an open-source alternative, like Linux. And I think for LLM's as well, we have a few competing closed-source providers. And then maybe the LLM ecosystem is currently maybe a close approximation to something that may grow into something like Linux. Again, I think it's still very early, because these are just simple LLMs. And I'm starting to see that these are going to get a lot more complicated. It's not just about the LLM itself, but it's about the tool use, the full-time analogies, how all that works. And so when I sort of have this realization about that, I try to sketch it out. And it kind of seems to me like LLMs are kind of like a new operating system, right? So the LLM is a new kind of computer. It's kind of like a CPU equivalent. The context windows are kind of like the memory. And then the LLM is orchestrating memory and compute for problem solving using all these capabilities. And so definitely, if you look at it, it looks very much like software. from that perspective.</p><p>A few more analogies, for example, if you want to download an app, say I go to VS Code and I go to Download, you can download VS Code and you can run it on Windows, Linux, or Mac, in the same way as you can take an LLM app, like a cursor, and you can run it on GBT, or Blob, or JPEG streams, right? It's just a drop-in. So it's kind of like similar in that way as well. The more analogies that I can describe to you is that we're kind of like in this 1960s-ish era, where LLM compute is still very expensive for this new kind of a computer, and that forces the LLMs to be centralized in the cloud, and we're all just sort of fake clients that interact with it over the network, and none of us have full utilization of these computers. And therefore, it makes sense to use timesharing, where we're all just, you know, at the mission of the batch, when they're running the computer in the cloud. And this is very much what the computers used to look like. During this time, the operating systems were in the cloud, everything was streamed around them, and they were batching. And so the personal computing revolution hasn't happened yet, because it's just not that common, and it doesn't make sense, but I think some people are trying. And it turns out that Mac minis, for example, are a very good fit for some of the LLMs, because it's all purely batch-run inference, this is all super-memory-bound, so this actually works. And I think these are some early indications that you have personal computing, but this hasn't really happened yet, and it's not clear what this looks like. Maybe some of you guys are interested in talking about what this is, or how it works, or what it should be.</p><p>Maybe one more analogy that I'll mention is, whenever I talk to ChartsQt or some LLM directly in text, I feel like I'm talking to an operating system through the terminal. Like, it's text, it's direct access to the operating system, and I think a GUI hasn't yet really been invented in a general way, like ChartsQt had a GUI, but different than just a text box. Certainly some of the apps that we're going to go into in a bit have GUI, but there's no GUI across all the tasks and devices. There are some ways in which LLMs are different from operating systems in some kind of unique way, and from early computing. And I wrote about this one particular property that strikes me as very different. It's that LLMs, they flip the direction of technology that is usually present in technology. So for example, with electricity, the economy is getting quite expensive, and lots of new transformative technologies have not been around, particularly in government-owned corporations that are the first officers, because it's new and expensive, etc. And only later did they fix it to consumers. But I feel like LLMs are kind of what flipped around. So maybe with early computers, it was all about ballistics and military use, but with LLMs, it's all about up the oil bank or something like that. This is certainly like a lot of minds.</p><p>And so it's really fascinating to me that we have a new magical computer, and it's like helping the oil bank. It's not helping the government to do something really crazy like some military ballistics or some special technology. Indeed, corporations are now getting the lagging, apparently not, of all of us, of all these technologies. So it's just backwards. And I think it informs me in some of the uses of how we want to use this technology, or like where I saw the first apps in the store.</p><p>So in summary so far, LLMs, app LLMs, I think it's accurate language to use. LLMs are complicated operating systems. They're circa 1960s computing, or we do computing already. And they're currently available via time-sharing and distributed like a utility. What is new and unprecedented is that they're not in the hands of a few governments and corporations. They're in the hands of all of us, because we all have a computer, and it's all just software. And Chachapitibos leans down to our computers that collect billions of people like this and play it overnight. And this is insane. And it's kind of insane to me that this is the case. And now it is our time to enter the industry and program these computers. It's crazy.</p><p>So I think this is a better part. Before we program LLMs, we have to kind of like spend some time to think about what these things are. And I especially like to talk about their psychology. So the way I like to think about LLMs is that they're kind of like little spirits. They are stochastic simulations of, and the simulator in this case happens to be an autoregressive transformer. So a transformer is a neural network. And it just kind of like goes from level of focus, it goes chunk, chunk, chunk, chunk, chunk. And there's an almost equal amount of compute for every single chunk. And this simulator, of course, is just, it's basically there's some weights involved and we fit it to all the text that we have on the internet and so on. And you end up with this kind of a simulator. And because it is trained in humans, it's got this emergent psychology that is human-like.</p><p>So the first thing you'll notice is, of course, LLMs have this type of deep knowledge and memory. And they can remember lots of things, a lot more than any single individual can because they've read so many things. It actually kind of reminds me of this movie Rain Man, which I actually really recommend people watch. It's an amazing movie. I love this movie. Dustin Hoffman here is an autistic sponge who has almost perfect memory. So he can read like a notebook and remember all of the names and phone numbers. And I kind of feel like LLMs are kind of like very similar. They can remember shock hashes and lots of different kinds of things, very, very easily. So they certainly have superpowers and stuff in some respect, but they also have a bunch of, I would say, cognitive deficits. So they hallucinate quite a bit and they kind of make up stuff and don't have a very good sort of internal model of self-knowledge, but not sufficient at least. And this has gotten better, but not perfect. They display jagged intelligence. So they're going to be superhuman in some problem-solving ways, and then they're going to make mistakes that basically no human will make, like, you know, it will insist that 9.11 is greater than 9.9, or that there are two bars of strawberry. These are some famous examples. But basically there are rough edges that you can trip on. So that's kind of, I think, also kind of cool. They also kind of suffer from internal brain amnesia. So, and I think alluding to the fact that if you have a coworker, which is your organization, this coworker will, over time, learn your organization and they will understand and gain, like, a huge amount of context on the organization and they go home and they sleep and they consolidate knowledge and they build expertise over time. LLMs don't natively do this. This is not something that has really been solved in R&amp;D with LLMs by them. And so context windows are really kind of like a working memory that you have to sort of program the working memory quite directly because they don't just kind of, like, get borrowed by people. And I think a lot of people get tripped up by analogies in this way. In popular culture, I recommend people watch these two movies, Memento and First Dates. In both of these movies, the protagonists, their ways are mixed, and their context windows get wiped every single morning. And it's really problematic to go to work or have relationships when this happens. And this happens to a lot of us all the time.</p><p>I guess one more thing I would point to is security-related limitations on the use of LLMs. So, for example, LLMs are quite vulnerable. They are susceptible to conflict-injection risks. They might leak your data, etc. And there's many other considerations security-related. So, basically, long story short, you have to simultaneously think through this superhuman thing that has a bunch of cognitive deficits and issues. How do we enhance them? They are extremely likely usable. And so how do we program them? And how do we work around their deficits and toy with their superhuman powers?</p><p>So what I'm going to switch to now is talk about the operators. How do we use these models? And what are some of the biggest operators? This is not a comprehensive list of some of the things that I thought were interesting in this stuff. The first thing I'm kind of excited about is what I would call partial autonomy apps. So, for example, let's work with the example of coding. You can certainly go to chat.gt directly, and you can start copy-pasting code around, and copy-pasting awkward words and stuff around, and then code, and copy-pasting everything around. Why would you do that? Why would you go directly to the operators? It makes a lot more sense to have an app dedicated for this. And so I think many of you use Cursor. I do as well. And Cursor is kind of like the thing you want instead. You don't want to just directly go to the chat.gt. And I think Cursor is a very good example of an early LLM app that has a bunch of properties that I think are useful across all the LLMs. So in particular, you will notice that we have a traditional interface that allows a human to go in and do all the work manually, just as before. But in addition to that, we now have this LLM integration that allows us to go in bigger chunks. And so some of the properties of LLM apps that I think are shared are useful. Number one, the LLMs basically do a ton of good context management. Number two, they orchestrate multiple calls to LLMs. So in the case of Cursor, there's under-the-hood eventing models for all your files, the actual chat models, models that apply ifs to the code, and this whole orchestra is for you. A really big one that I think also may be not fully appreciated in all this is application-specific and the importance of it. Because you don't just want to talk to the operating system directly in text. Text is very hard to read, interpret, understand, and also you don't want to take some of these actions natively in text. So it's much better to just see the bit as like red and green change, and you can see what's the matter of subtracting. It's much easier to just do command Y to accept or command N to reject. You don't have to type it in text. So it really allows the human to audit the work of these fabulous systems and to grow faster. We're going to come back to this in a little bit later as well. And the last kind of feature I want to point out is that there's what I call the autonomous slider. So for example, in Cursor, you can just do calculation. You're mostly in charge. You can select a chunk of code and command K to change a static chunk of code. You can do Command L to change this entire file, or you can do Command I, which just, you know, for better or worse, it packages up a lot of things, makes sure that it orchestrates people all at once. It's got a GUI that allows you to audit some of its work. So, for example, it will cite sources that you can imagine inspecting them, and it's got an autonomy slider. You can either just do a quick search, or you can do research, or you can do deep research and come back to all this later. So this is all just very well-structured, automated, and optimized.</p><p>So I guess my question is, I feel like a lot of software will become partially autonomous. And I'm trying to think through, like, what does that look like? In fact, many of you maintain products and services. How are you going to make your products and services partially autonomous? Can an LLM see everything that a human can see? Can an LLM act in all the ways that a human can act? And can humans supervise and stay in the loop of this activity? Because, again, these are allowable systems that aren't yet perfect. And what does the diff look like on Photoshop? There's a lot of things we don't know. And also, a lot of the traditional software right now has all these switches and all this kind of stuff. It's all designed for people. All this has to change and become accessible to LLMs.</p><p>So one thing I wanted to stress with a lot of these LLM apps that I'm not sure gets as much attention as it should, is LLM. We're now kind of, like, cooperating with the apps. And usually, they are doing a generation, and we assume this sort of verification. It is in our interest to make this move as fast as possible, so we're getting a lot of work. There are two major ways that I think this can be done. Number one, you can speed up verification a lot. And I think GUIs, for example, are extremely important for this, because a GUI utilizes your computer vision GPU in all of our head. Reading text is effortful, and it's not looking at stuff. It's on a headset. It's just a, like, a highway to your brain. So I think GUIs are very useful for auditing systems and visual representations in general. And number two, I would say is, we have to keep the AI in a leash. I think a lot of people are getting way overexcited with AI engines, and it's not useful to me to get the diff of 1,000 lines of code into my repo. Like, I have to, I'm still the bottom line, right? Even though the 1,000 lines come out instantly, I have to make sure that this thing is not introducing bugs. It's just like, and that it's doing the correct thing, right, and that there's no security issues and so on. So I think that, yeah, basically, you have to sort of, like, it's in our interest to make the flow of these two go very, very fast, and we have to somehow keep the AI in a leash because it gets way too overactive. It's kind of like this. This is how I feel when I do AI-assisted coding. If I'm just live coding, everything is nice and great, but if I'm actually trying to get work done, it's not so great to have an overactive engine doing all this kind of stuff. So this slide is not very good, I'm sorry, but I guess I'm trying to develop, like many of you, some ways of utilizing these engines in my career to look at AI-assisted coding, and if I don't work, I'm always scared to get way too big bits. I always go with small incremental chunks. I want to make sure that everything is good. I want to spin this thing very, very fast, and I sort of work on small chunks of single-token thing, and so I think many of you probably have a little bit similar ways to work with algorithms.</p><p>I also saw a number of blog posts that try to develop these best practices for working with algorithms, and here's one that I recently came across that's quite good, and it kind of discusses some techniques, and some of them have to do with how you keep the AI on a leash. And so as an example, if you're on prompting, if your prompt is vague, then the AI might not do exactly what you want it, and in that case, the verification will fail. You're gonna ask for something else. If the verification fails, then you're gonna start spinning. So it makes a lot more sense to spend a bit more time to be more complete in your prompts, which increases the probability of a successful verification, and you can move forward. And so I think a lot of us are gonna end up finding techniques like this.</p><p>I think in my own work as well, I'm very interested in what education looks like, and together with, now that we have an AI, and a lens for what does education look like, and I think a large amount of thought for me goes into how we keep AI on a leash. I don't think it just works to go through trashy ATM, you know, like the aging business. I don't think this works, because the AI is like, it gets lost in the woods. And so for me, this is actually two separate apps, for example, there's an app for a teacher that creates courses, and then there's an app that takes courses and serves them to students. And in both cases, we now have this intermediate artifact of a course that is auditable, we can make sure it's good, we can make sure it's consistent, and the AI is kept on a leash with respect to a certain syllabus, a certain progression of projects, and so on. And so this is one way of keeping AI on a leash that I think has a much higher likelihood of working. And the AI is not getting lost in the woods.</p><p>One more kind of methodology I wanted to sort of allude to is I'm not a most major to partial autonomy, I've kind of worked on this, I think, for five years for Tesla, and this is also a partial autonomy product that shares a lot of features, like for example, right there, the instrument panel has the weight of the product, so it's showing me what the neuron sees, and so on. And then the autonomy slide, where over the course of my tenure there, we did more and more autonomous tasks for Google News. And maybe the story that I wanted to tell very briefly is, actually the first time I drove a self-driving vehicle was in 2013, and I had a friend who works at Rainbow, and he offered to give me a drive around Palo Alto. I took this picture using a moving bus at the time, and many of you are so young that you might not even know what that is, but yeah, this was like all the rage at the time. And we got into this car, and we went for about a 40-minute drive around Palo Alto, and the highways, the streets, and so on, and this drive was perfect. There was zero traffic issues. And this was in 2013, which is now 12 years ago. And it kind of struck me, because at the time when I had this perfect drive, it was a perfect gift, I felt like, wow, self-driving was imminent, because this just worked, this is incredible. But here we are 12 years later, and we are still working on the tunnel. We are still working on the driving engines. And even now, we haven't actually like fully solved the problem. Like, you may see way-bos going around, and they look driverless, but there's still a lot of tool operation, and a lot of human input of what was driving. So we still haven't even like declared success, but I think it's definitely like going to succeed at this, but it just took a long time. And so I think, to me, what I found there is that there's a very large, what I call network-to-product gap that people don't intuitively always understand very well. And look, if you imagine works as like this binary array of a different situation, of like what works and doesn't work, then that is worse than many in products, in like works at all. In the sense that, if anything works, you can make demos. But in many cases, lots of things must work, but very few people are new to the product. And this is especially the case in high-reliability areas. Not all the areas are like this, but for sure in high-reliability cases, it means. And I would say autonomy, of course, because like this thing is going to crash, which would be bad. But I would also say software is like this. If you make a single mistake in software, we know that there could be a code path that's going to break, or it's going to introduce a security issue, or a zero-day, or something like that. Like, look, software is really tricky, I think, in the same way that driving is tricky. And so when I see things like, oh, 2035 is the year of agents, I get very concerned, and I kind of feel like, you know, this is the decade of agents, and this is going to be, by some time, you do this carefully, and this is software. Let's be serious here, okay?</p><p>One more kind of analogy that I always think through is the Iron Man suit. I think this is, I always love Iron Man, I think it's like so correct in a bunch of ways, with respect to technology and how it will play out. And what I love about the Iron Man suit is that it's both an augmentation, and twin-star mechanic driver. And it's also an agent, and in some of the movies, the Iron Man suit is quite autonomous, and you can fly around, climb trees, and all this kind of stuff. And so this is the autonomy sluggish, it can be, we can build augmentations, or we can build agents, and we kind of want to do a bit of both, but at this stage, I would say, working with Palo Alto LMS itself, I would say, you know, it's less Iron Man robots, and more Iron Man suits that you want to build. It's less, like, building flashy demos of autonomous agents, and more, building partial autonomy products. And these products affect who you speak, and we're trying to, and this is done so that the generation verification of the human is very, very fast. But we are not losing the sight of the fact that it is, in principle, possible to automate this work, and there should be an autonomy slider in your product, and you should be thinking about how you can slide that autonomy slider, and make your product sort of more autonomous over time. But this is kind of how, I think there's lots of opportunities in these kinds of products.</p><p>I want to now switch gears a little bit, and talk about one other dimension that I think is very important. Not only is there a new type of programmer language that allows for autonomy in software, but also, as I mentioned, it's programmed in English, which is this natural interface. And suddenly, everyone is a programmer, because everyone speaks natural language, like English. So this is extremely bullish, and very interesting to me, and also completely unprecedented, I would say. It used to be the case that you need to spend five to 10 years studying something to be able to do something that software can do. This is not the case anymore.</p><p>So I think that, by chance, anyone has heard of IBM? Okay. This is the tweet that introduced this, but I'm told that this is now a major meme. A story about this is that, I've been on Twitter for 15 years, or something like that, at this point, and I still have no clue which tweet will become viral, and which tweet this will send over the years. And I thought that this tweet was going to be, I'm going to just have a shower of thoughts, but this became a total meme, and I really just can't tell, but I guess it's working. Or even name something that everyone is calling, but can apply it to the same words. Now they're speaking Yiddish and everything. This is life. Yeah, this is like a major contribution now or something like that, so.</p><p>So Tom Wolfe from Hugging Feet shared this beautiful video that I really love. These are kids vibe coding. And I find that this is such a wholesome video, like, I love this video. Like, how can you look at this video and feel bad about the future? The future is great. I think this will end up being like a gateway drug to software development. I'm not a doer about the future of the generation, and I think, yeah, I love this video. So, I tried vibe coding a little bit as well, because it's so fun. So, vibe coding is so great when you want to build something super duper custom that doesn't appear to exist, and you just want to wing it because it's a Saturday or something like that. So, I built this iOS app, and I built into the, I can't actually program it in Swift, but I was really shocked that I was able to build a super basic app, and I'm not going to explain it, that's really dumb. But, I kind of like, this was just like a day of work, and this was running on my phone like later that day, and I was like, wow, this is amazing. I didn't have to like leave from Swift for like five days or something like that to like get started. I also vibe coded this app with a menu gem, and this is a lot, you can try a menu gem on that. And, I basically have this problem where I show up at a restaurant, I reach for the menu, and I have no idea what any of the things are, and I need pictures. So, this doesn't exist, so I was like, hey, I'm going to vibe code it. So, this is what it looks like. You go to menu gem, that app, and take a picture of the menu, and then menu gem generates the images. And, everyone gets five dollars in credits for free when you sign up, and therefore, this is a major cost center in my life. So, this is a negative revenue app for me right now. I lost a huge amount of money on menu gem. Okay.</p><p>But, the fascinating thing about menu gem for me is that the code, well, the vibe coding part, the code was actually an easy part of vibe coding menu gem. And, most of it actually was when I tried to make it real so that you can actually have authentication, payments, domain name, and personal deployment. This was really hard, and all of this was not code. All of this DevOps stuff was me and the browser clicking stuff. And, this was an extreme spot into another room. So, it was really fascinating that I had the menu gem basically demo working on my laptop in a few hours, and then it took me a week because I was trying to make it do it. And, the reason for this is this was just really annoying. So, for example, if you try to add Google log into your webpage, I know this is very small, but just a huge amount of instructions of this important library telling you how to integrate this, and this is crazy, like it's telling me, go to this URL, click on this dropdown, choose this, go to this, and click on that, and it's like telling me what to do. Like, the computer is telling me the actions I should be taking, like, you do it. What do I do? What the hell? I had to follow all these instructions. This was crazy.</p><p>So, I think the last part of my talk, therefore, focuses on can we just build for agents? I don't want to do this work anymore. Thank you. Okay.</p><p>So, roughly speaking, I think there's a new category of consumer and manipulator of digital information. It used to be just humans through GUIs, or computers, or APIs. And now it's a completely new thing. And agents are computers, but they are human-like. Kind of, right? They're people spirits. There's people spirits on the internet, and they need to interact with our software infrastructure. Can we build for them? It's a new thing. So, as an example, you can have robots.txt in your domain, and you can instruct, or advise, I suppose, web crawlers on how to behave with your website. In the same way, you can have maybe LLMs.txt file, which is just a simple markdown that's telling LLMs what this domain is about. And this is very readable to an LLM. If it had to be said, get the HTML of your webpage and try to parse it. This is very error-prone and difficult, and it will screw it up, and it's not going to work. So, we can just directly speak to the LLM, and it's worth it. 5.1. I see some of the services now are transitioning a lot of their docs to be specifically for LLMs. So, for Cell and Stripe, as an example, are early users here. But there are a few more that I've seen before. And they offer their documentation in Markdown. Markdown is crazy for LLMs to understand. This is great. Maybe one simple example from my experience as well. Maybe someone you know from Google Brown who makes beautiful animation videos. Yeah, I love this library that he wrote, Mavic. And I wanted to make my own. And there's extensive documentation on how to use M. So, I didn't want to actually read through it. So, I copy-pasted the whole thing to an LLM. And I just grabbed what I wanted, and it just worked out of the box. But LLM just byte-coded the animation exactly what I wanted. And I was like, wow, this is amazing. So, if we can make docs legible to LLMs, it's going to unlock a huge amount of objectives. And I think this is one of the core issues that should happen.</p><p>The other thing I wanted to point out is that we do unfortunately have to. It's not just about taking your docs and making them appear in Markdown. That's the easy part. We actually have to change the docs. Because any time your docs say, like, this is bad. And LLM will not be able to agent-maintain this action right now. So, Purcell, for example, is replacing every occurrence of play with an equivalent program that your LLM agent could take on your behalf. And so, I think this is very interesting. And then, of course, there's a model context protocol from them. And this is also another way. It's a protocol that's given directly to agents. It's a new consumer and particular commercial application. So, I'm very bullish on this.</p><p>The other thing I really like is the number of little tools here and there that are helping ingest data in very LLM-friendly formats. So, for example, when I go to a GitHub repo, I can't feed this to an LLM and ask questions about it. Because this is a human interface from GitHub. So, when you just change the URL from GitHub to GitHub Ingest, then this will actually concatenate all the files into a single giant text. And it will create a directory structure. It's ready to copy-paste it into a favorite LLM. And you can use it. Maybe even more of a dramatic example of this is Eclipse, where it's not just the raw content of its files. This is from Devon. But also, they have Devon basically do analysis of the GitHub repo. Devon basically builds up a whole box of pages just for your repo. And you can imagine that this is even more helpful to copy-paste into your LLM. So, I love all the little tools that basically just change the URL and make something accessible to an LLM. So, this is all well and great. And I think there's more to come.</p><p>One little note I wanted to make is that it is absolutely possible that in the future, LLMs will be able to, it's not even future, this is today, they'll be able to go around and they'll be able to put stuff at home. But I still think it's very worth basically eating LLM's pathway and making it easier for them to access all this information. Because this is still fairly expensive, I would say, to do this. And a lot more difficult. And so, I do think that lots of software that are being long-tailed, where it won't let them gap. Because these are not live layers or repositories or traditional infrastructure. And we will need these tools. But I think for everyone else, I think it's very worth it. So, I'm bullish on both.</p><p>So, in summary, what an amazing time to give to the industry. We need to rewrite a ton of code. A ton of code will be written by professionals and by players. These LLM's are kind of like utilities, kind of like labs, but they're kind of, especially, like operating systems. But it's so early. It's like 1960s operating systems. And I think a lot of the analogies cross over. And these LLM's are kind of like these fallible people spirits that we have to learn to work with. And in order to do that properly, we need to adjust our infrastructure. So, when you're building these LLM's, it's practical ways of working effectively with these LLM's and some of the tools that make that possible. And how you can spin this very, very quickly and basically create partial time for products. And then, yeah, a lot of code has to be written for the agents. But, in any case, going back to the Iron Man suit analogy, I think what we'll see over the next decade, roughly, is we're going to take the slider from left to right. Very interesting. It's going to be very interesting to see what that looks like. So, with all of you. Thank you.</p><p>that's all. let's keep in touch?</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The unreasonable effectiveness of fuzzing for porting programs (206 pts)]]></title>
            <link>https://rjp.io/blog/2025-06-17-unreasonable-effectiveness-of-fuzzing</link>
            <guid>44311241</guid>
            <pubDate>Wed, 18 Jun 2025 16:26:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rjp.io/blog/2025-06-17-unreasonable-effectiveness-of-fuzzing">https://rjp.io/blog/2025-06-17-unreasonable-effectiveness-of-fuzzing</a>, See on <a href="https://news.ycombinator.com/item?id=44311241">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        

<section>
<p><em>A simple strategy of having LLMs write fuzz tests and build up a port in topological order seems effective at automating porting from C to Rust.</em></p>
<h2>Agents are starting to produce more and more code</h2>
<p>A week or 2 back, I was reflecting on some code Claude had generated for me and
I had a sort of moment of clarity. "Clarity" might be overstating it; more
like the type of thought you have in the shower or after a few beers. Anyway.</p>
<p>The thought was: LLMs produce more and more code, and they'll eventually be
producing more code than people. We can imagine at some point we're going to
fork from mostly people writing code to mostly computers. What does this mean
for how we're going to treat our libraries and code maintenance in the future?</p>
<p>LLMs make it easier to deal with API issues or inconsistencies, they're dealing
with it, not us. Will this continue, leaving us with a <a href="https://en.wikipedia.org/wiki/The_Princess_and_the_Pea">The
Princess and the Pea</a> situation where we deal with piles of leaking
abstractions? Or will we use LLMs to radically change our APIs when needed - will
things get cleaner and better and faster as we go?</p>
<p>LLMs open up the door to performing radical updates that we'd
never really consider in the past. We can port our libraries from one language
to another. We can change our APIs to fix issues, and give downstream users an
LLM prompt to migrate over to the new version automatically, instead of
rewriting their code themselves. We can make massive internal refactorings.
These are types of tasks that in the past, <em>rightly</em>, a senior engineer would
reject in a project until its the last possibly option. Breaking customers
almost never pays off, and its hard to justify refactoring on a "maintenance
mode" project.</p>
<p>But if its more about finding the right prompt and letting an
LLM do the work, maybe that changes our decision process.</p>
<h2>Maintaining big important libraries is no fun</h2>
<p>I used to work on
<a href="https://www.tensorflow.org/">TensorFlow</a>. (TensorFlow is a system to develop
gradient based models, like PyTorch. It's not used so much anymore outside of
Google.) Now TensorFlow had some design flaws in the core language, and a
botched version 1 -&gt; version 2 migration didn't help matters too much. But as a
maintainer, the biggest issue was the enormous technical debt. It suffered from
a sort of "career advancement syndrome" <label for="sn-sad-9e298f0f"></label><span>I sadly missed this period and only was around for the aftermath where we had to deal with the cruft. </span>: TensorFlow was popular and you got credit for
contributing to it, so there was a huge incentive to add some feature as quickly
as you could and then get away.</p>
<p>As a result of a few years of this style of development, a huge surface of
Python code had been cobbled together on top of the C++ core. The complexity
only spiraled over time: engineers came into the project and needed to get
something done. Increasingly the easiest thing to do was to add some Python global or
context manager, shove something in it, then grab it later. Do this over and
over and eventually it becomes impossibly to figure out what's happening. It
also ended up being incredibly slow. It would take 10s of minutes to build some
graphs (TensorFlow's equivalent of a program).</p>
<p>As a result of this over time TensorFlow became harder and harder to maintain,
much less evolve <label for="sn-drag-2cb313ba"></label><span>It was also hard to keep good engineers working on the project. If
you're a good engineer, other teams want you and you could choose to work on
less crufty, more exciting projects like <a href="https://docs.jax.dev/">Jax</a> or
<a href="https://rjp.io/blog/openxla.org">XLA</a>, or you know, Ads. </span>. One idea we discussed internally to try to fix this was
porting the Python code, bit by bit, into a more comprehensible C++ layer, and
then re-exporting it via PyBind. At first you'd still call everything through
the Python interface, but as you ported code over, you could start to use the
internal C++ interfaces more and more, getting faster and more stable as you
went. This would incrementally improve the performance and consistency of the
system, or that was the hope.</p>
<p>But it would have been an enormous effort to do this all. Each time you tried to
port a module you'd reveal subtle errors or assumptions that you'd need to debug
and revisit, update users etc. Given our staffing, we couldn't really justify
the time versus the real work of making sure Ads didn't break.</p>
<p>Even though TensorFlow was moving to maintenance mode, if we could have made
this type of change it would have improved the maintainer and users' experience
for the years we need to keep the lights on. But because the cleanup was thorny
and slow, we just couldn't justify the cost.</p>
<p>This is by no means limited to TensorFlow. If you're working on something like
<a href="https://github.com/libexpat/libexpat">libexpat</a> its hard to justify
refactoring your code or rewriting it in Rust, no matter how potentially useful
it would be, when you've got this at the top of your page:</p>
<p><img alt="libexpat staffing warning" src="https://rjp.io/blog/libexpat-warning.png"></p>
<p>But what if we could make this type of refactoring much, much cheaper?</p>
<h2>Infinite patience versus really hard problems</h2>
<p>My experience with coding agents like <a href="https://github.com/anthropics/claude-code">Claude
Code</a> and <a href="https://aider.chat/">Aider</a>
makes me think this could be different today. I've found that agents are really
good when you set them up against a concrete obstacle: "I changed this API, now
fix everything until the tests pass". They can show real ingenuity in finding
solutions to subtle problems. I've had the LLM diagnose subtle bugs in a
program, write test cases, write minimizers, and probe for solutions, where my
contribution to this conversation largely consisted of "keep going until the
test passes". Not always, of course. Good luck getting an agent to produce
anything other than React &amp; Tailwind on your new project, for instance.</p>
<p>My hunch is that the stronger the test case, the more concrete the objective the
LLM has to fulfill, the less likely it is to try to sabotage the problem. e.g. "I made
an empty test and it passed", or to misinterpret the request "you asked for a
Ruby on Rails project, so here's NextJS, like you asked for".</p>
<p>You're putting the agent between a rock and a hard place, as it were, and this
reduces the easy solutions it could use, and thus it works harder to find a real
solution.</p>
<p>In general its really hard to define test cases, a priori, that are specific
enough to constrain the LLM to do exactly what you want. But what about when
we're changing an existing project? What if we could constrain the output before
and after the changes to be the same? This could be as simple as running all of
our tests for our project, if we're confident they're very thorough. For most
complex changes, it might be hard to keep both versions aligned.</p>
<p>What happens if we take a very specific and important type of refactoring:
moving from an unsafe language like C to a safe language like Rust? "I did something with memory I shouldn't have" are
still in the <a href="https://www.cvedetails.com/vulnerabilities-by-types.php">top 3 for CVEs</a><label for="sn-cve-f8b1dde5"></label><span>No longer #1, not because the number of memory CVEs is going down, but because XSS vulnerabilities have grown so quickly 🤦. </span>. This is exactly the
type of thing we might <em>like</em> to do, but hard to justify.  Can we make porting more of a prompt engineering problem instead of a coding
one? Let's find out...</p>
<h2>Going from C to Rust</h2>
<p>Porting C to Rust with LLMs isn't a brand new idea:
<a href="https://github.com/immunant/c2rust">c2rust</a> can produce a mechanical
translation of C code to Rust, though the result is intentionally "C in Rust
syntax", with the idea that a person uses it as a starting point to then Rustify
the project.</p>
<p>More recently researchers have started throwing LLMs at the problem.
<a href="https://arxiv.org/pdf/2412.14234">Syzygy</a> is the only paper I found which produced a complete working
implementation of a library, the other papers were more like "we threw this at
the wall and it compiled". Syzygy leverages a bunch of machinery to mechanize
parts of the porting process:</p>
<p><img alt="syzygy" src="https://rjp.io/blog/syzygy.png"></p>
<p>What's cool is their approach <em>works</em>: they get a compression program out of it, and its <a href="https://github.com/syzygy-project/Syzygy_Zopfli/blob/main/rust_code/src/main.rs">all safe Rust</a>. That's a non-trivial achievement! That said, it runs 3x slower than the C version, and its not
<em>identical</em>: how much this detail matters depends on the users of your API and how much
you care about <a href="https://en.wikipedia.org/wiki/Hyrum%27s_Law">Hyrum's Law</a>.</p>
<p>Syzygy puts a lot of work into developing test cases for symbols as they port,
but because the tests are against inferred properties of the C program, instead
of directly comparing against the C behavior, you can end up with subtle changes
in behavior. This is hard to avoid with their approach: if the interfaces
differ, it becomes hard to perform a direct comparison between your C and Rust
programs.</p>
<p>I wanted to test if we could do something radically simpler and more direct: <em>what if we just
randomly compared the C and Rust output as we ported</em>? Would that be sufficient
to port an entire library? Or would our tests be ineffectual and we'd get stuck
halfway through. In effect we'd be performing a specific type of fuzz or
<a href="https://en.wikipedia.org/wiki/Property_testing">property testing</a> to our
program as we ported it.</p>
<h2>Property testing</h2>
<p>Property testing has an interesting supposition: we can avoid the drudge work of
writing tests by having the computer build out the test cases for us, and just
check if a property holds for our system.  Trivia item: it is a fundamental law
of all property testing frameworks that they start with one of 2 examples:</p>
<ul>
<li>reverse is its own inverse, so <code>x = reverse(reverse(x))</code>.</li>
<li>the elements of a sorted list are in order</li>
</ul>
<p>Don't believe me? Check it out:</p>
<ul>
<li><a href="https://github.com/BurntSushi/quickcheck">https://github.com/BurntSushi/quickcheck</a></li>
<li><a href="https://github.com/HypothesisWorks/hypothesis">https://github.com/HypothesisWorks/hypothesis</a></li>
<li><a href="https://hackage-content.haskell.org/package/QuickCheck-2.16.0.0/docs/Test-QuickCheck.html">Haskell Quickcheck</a></li>
<li><a href="https://github.com/emil-e/rapidcheck">https://github.com/emil-e/rapidcheck</a></li>
</ul>
<p>These examples are common for a reason. They're the perfect fit for property
testing: you just feed random lists in and check your condition holds. A common
critique of property testing is that it doesn't really extend beyond these
examples. Either its too hard to generate meaningful inputs, or
properties to test, and so we're better off writing individual test
cases. My experience is that there's some narrow cases where property testing is
great, but I still end up writing plenty of individual tests.</p>
<p>Sometimes determining the property that holds is hard: I can
test individual cases and validate them, but I don't know how to generalize.  Or
generating interesting inputs might be hard. If I want to test a C parser,
sampling random bit strings isn't going to help me very much.  Entire projects
like <a href="https://github.com/csmith-project/csmith">Csmith</a> are dedicated to fuzzing C compilers.</p>
<p>But in our case we've solved at least the property test is solved for us: we
have the <em>perfect</em> output property to test: for a given input X, does our Rust
library produce the exact same output as our C library? If we can test this over
an interesting part of our input space, then we can be confident we've preserved
our behavior.</p>
<p>Let's see how this works out.</p>
<h2>Porting C to Rust, attempt 0</h2>
<p>The logs of my first attempt are fortunately lost to the howling void. I spent
an hour or 2 with Claude Code, following this policy:</p>
<ul>
<li>Port symbol X</li>
<li>Write a test for symbol X to make sure it works</li>
</ul>
<p>I wasn't comparing directly against the C version, instead relying on the unit tests
to validate the behavior was preserved. This attempt did not go well. As modules
were ported over, and we started to use symbols from previous ports, we exposed
more and more latent bugs. Eventually, you're in a state where the top-level
<code>ZopfliCompress</code> doesn't work, and you're stuck debugging the whole program to
figure out what happened.</p>
<p>I realized this wasn't going to scale. Even if I got something working, I was
looking at a lot of work and I couldn't explain how to replicate what I did with
another project.</p>
<h2>Porting C to Rust, attempt 1</h2>
<p>My <del>first</del>second attempt was <del>truly</del> slightly less crude:</p>
<ul>
<li>For each C module, ask Claude Code to write a Rust version.</li>
<li>Then write a fuzz test for that module</li>
</ul>
<p>The results of this experiment are on
<a href="https://github.com/rjpower/zopfli/tree/master/zopfli-rs/">Github</a>.</p>
<p>This time, it worked: other than writing the <a href="https://github.com/rjpower/zopfli/blob/master/port/RUST_PORTING.md">porting guidelines</a> and cajoling the model, I didn't do much work.  I didn't do any debugging by
hand or have to intercede to understand what was happening. In the case a test found a discrepancy, I would have the model write a <a href="https://github.com/rjpower/zopfli/blob/master/port/bugs/20241219_deflate_tree_encoding_discrepancy.md">bug report</a>
and then iterate until it solved the problem.</p>
<p>But again this wasn't automated and it was hardly reproducible. I had to babysit
the process quite a bit<label for="sn-babysit-06e936e9"></label><span>Mostly telling it to keep going. I probably could have written <code>while true; echo 'keep going until the test passes' | claude</code> and gotten most of the way there. </span>, and while the library seemed to work, the
results were <em>subtly different</em> than the original Zopfli C implementation. And
because of the ad-hoc approach, I didn't have a good idea of what had changed.</p>
<p>Still this was promising. So I was incentivized to try again, with a bit more rigor and automation.</p>
<h2>Porting C to Rust, "for real"</h2>
<p>Our final process would be more rigorous. We want to port a small portion of our
program at a time, and automate as much of the porting as possibly. Instead of
calling into an agent and babysitting it, we'd need to hand-roll our own
machinery to call into an LLM API and perform the necessary edits in a fully
automated fashion. So our overall process would be:</p>
<ul>
<li>
<p>Sort symbols in topological order based on the static call graph. If function F calls function G, then first port function G, then function F, etc. (Cycles need to be ported together, but we didn't have any for this library). This ensures that when we are porting a new symbol into Rust, we can call into our child symbols immediately; we don't have to implement anything new or call into C. "Symbol" here means a struct, enum or function.</p>
</li>
<li>
<p>For a given public C declaration (in a header file), create an FFI entry for it and a Rust implementation with the exact same signature. We'll do this by giving the LLM the C header &amp; source and asking for a translation.</p>
</li>
</ul>
<pre><code>// ffi.rs
pub fn AddDynamicTree(
    ll_lengths: *const ::std::os::raw::c_uint,
    d_lengths: *const ::std::os::raw::c_uint,
    bp: *mut ::std::os::raw::c_uchar,
    out: *mut *mut ::std::os::raw::c_uchar,
    outsize: *mut usize,
);

// deflate.rs
pub unsafe fn AddDynamicTree(
    ll_lengths: *const c_uint,
    d_lengths: *const c_uint,
    bp: *mut c_uchar,
    out: *mut *mut c_uchar,
    outsize: *mut usize,
) {
    ...
}
</code></pre>
<ul>
<li>Have the LLM write a fuzz test which samples over possibly inputs and compares the C and Rust implementations, asserting if there is a discrepancy <a href="https://github.com/rjpower/portkit/blob/1d5a2c21adbfc71cb0906fd16a0cf7252be5dcd5/zopfli-port/rust/fuzz/fuzz_targets/fuzz_AddDynamicTree.rs">example</a>. No attempt was made to guide the fuzz test inputs other than instructing the LLM to avoid generating invalid structures (e.g. null pointers or numbers out of an expected range).</li>
</ul>
<pre><code>#[derive(Debug, arbitrary::Arbitrary)]
struct FuzzInput {
    ll_lengths: Vec&lt;u32&gt;,
    d_lengths: Vec&lt;u32&gt;,
}

fuzz_target!(|input: FuzzInput| {
    ...
    // setup inputs
    let c_result = ffi::AddDynamicTree(...)
    let rust_result = rust::AddDynamicTree(...)

    assert_eq!(c_result, rust_result, "C and Rust diverged.");
</code></pre>
<ul>
<li>Run the fuzz test and fix until everything compiles and the fuzz test passes.</li>
</ul>
<p>We repeat this for each symbol in the program until we hit the top-level main().</p>
<p>Prior to porting I modified the C source slightly to make a few static functions
extern so that we could create an FFI to them. This would allow the LLM to port
smaller chunks at a time (otherwise there were some modules where the LLM would
have to port 1000 lines of code at once because only the main symbol was
visible). I also copied some <code>#define</code> constants by hand because my C traversal
hadn't detected them.</p>
<p>Originally I tried to break up the task of porting a symbol. We'd separately
define the FFI to C, a stub implementation of the Rust function, define a fuzz
test, then build the implementation. The idea was that this would let us verify
each of these via a separate call and we'd be more robust. Ultimately this
proved more confusing to the agents than helpful.</p>
<p>In the end, I simply prompted the LLM to do all the steps for porting a symbol
at once, reprompting it if the fuzz test didn't exist or pass.</p>
<p>And in the end... it worked? The result is a <a href="https://github.com/rjpower/portkit/tree/main/zopfli-port">Rust implementation of
Zopfli</a> that gives
<em>identical</em> results on every input I've tried to the C version. This is
different from the Syzygy results, where they ended up with a program that
compresses correctly, but not identically <label for="sn-better-46a42f45"></label><span>This isn't a claim my approach is <em>better</em> than Syzygy, just different. </span> to the C version.  Because
we locked the Rust and C versions to use the same API at each step, the
resulting program isn't very "rusty", but its a complete translation.</p>
<p>About 90% of symbols were auto-ported using gemini-2.5-pro-06-05 and a crappy
"agent" library I wrote up for this task. The remaining 10% I switched over to
running in a Claude Code, as Gemini seemed to struggle with patch formats and
imports. The only work I did manually was to adjust a few places where the LLM
was calling the FFI code from Rust and clean up some warning messages.</p>
<h2>But why does this work at all?</h2>
<p>To clarify, the surprising result is not that fuzzing would detect discrepancies
at the top-level of our library. Target specific fuzzers work great for this task:
<a href="https://github.com/csmith-project/csmith">CSmith</a> and
<a href="https://jepsen.io/">Jepsen</a> find all sorts of weird interesting bugs.
What's surprising is that with only one exception<label for="sn-claude-5296e104"></label><span>Gemini introduces a weird typo into a program, which was detected a few symbols downstream by Claude and repaired. You can see <a href="https://rjp.io/blog/claude-rust-port-conversation">the session log here</a>. </span>, the LLM translation + naive
fuzz test correctly validated the symbol behavior <em>for every symbol</em>. This meant
that we didn't run into any issues where after porting A, B ... Q, suddenly we
detect a subtle bug in R.</p>
<p>This meant we didn't have to "backtrack", or inspect the rest of the code base
as we ported symbols: each symbol ported in isolation, we tested it (implicitly
testing the dependent symbols as well), and we moved on. If this didn't work,
we'd have to inspect the whole program and debug it, over and over, as we built
it up.</p>
<p>If we think about it, this shouldn't have worked as well as it did. Fuzzing
shouldn't hold for all functions. Imagine I'm converting something like a manual
floating-point multiplier:</p>
<pre><code>mul(a_bits: &amp;[byte], b_bits: &amp;[byte], c_bits: mut &amp;[byte]):
</code></pre>
<p>If I fuzz this by providing random bytes, am I likely to detect a subtle issue
with underflow, or detect the <a href="https://en.wikipedia.org/wiki/Pentium_F00F_bug">Pentium FOOF bug</a>?  Or imagine a C parser:
fuzzing random bytes wouldn't trigger much of the parser. Without a lot of
probes, it seems hard to test these function spaces!</p>
<p>My hunch is that this works due to a combination of factors:</p>
<ul>
<li>I chose a simple library to work with. Zopfli isn't trivial, but you aren't juggling a lot of state, for example.</li>
<li>Fuzzers are good at probing the input space. If you <a href="https://llvm.org/docs/LibFuzzer.html#tracing-cmp-instructions">compile with the right arguments</a> the fuzzer will try to choose inputs which trigger different branches. You can even give good examples (a corpus) to start the fuzzer off. I didn't do this for my experiments, but its easy to imagine an LLM generating a decent starting corpus.</li>
<li>Most code doesn't express subtle logic paths. If I test if a million inputs are correctly sorted, I've probably implemented the sorter correctly.</li>
<li>Complex logic, when it exists, is broken up across functions. Our individual tests make it easier to ensure the combined calls work.</li>
<li>The LLM produces correct code most of the time! The fuzz test is just there to validate we did the right thing.</li>
</ul>
<h2>Caveats, or don't try this at home</h2>
<p>While the overall system seems to work, its far from ready to drop in to a new project.</p>
<p><em>The resulting Rust code is very "C-like"</em></p>
<p>By construction, we use the same unsafe C interface for each symbol we port.
This is different from Syzygy, which generates a safe Rust version of the
program in one shot.  This was convenient but not strictly required: I could
have tweaked the prompts and clearly indicate public/private interfaces, letting
the LLM use more idiomatic Rust for internal functions. Retaining the same
interfaces simplified writing the fuzz tests and let the LLM call into the
original C code when debugging.</p>
<p>That said, because our end result has end-to-end fuzz tests and tests for every
symbol, its now much easier to "rustify" the code with confidence. This is a
great task for future work.</p>
<p><em>The automation isn't complete</em></p>
<p>I tweaked the agent framework and prompts as I went to get better results out of Gemini. I needed to have Claude come in at the end to finish the last few symbols, and I fixed a few typos here and there. With better prompting and a better agent framework, this intervention would be reduced, but likely not go away entirely.</p>
<p><em>Zopfli is easy</em></p>
<p>Zopfli is a simple library with many functions which are input/output oriented.
It's not clear how this would work if you introduced more stateful interfaces.</p>
<h2>Conclusions and future work</h2>
<p>Whew. This ended up taking a few days and being more than the initial trivial
experiment I intended. That said, I think there's some interesting insights:</p>
<ul>
<li><em>Porting via LLMs is surprisingly cost effective and only getting cheaper</em>. Even with my multiple rounds of experimentation and tweaking agents etc, the total cost for this experiment was ~$50, or about $0.01 a line. (For comparison, Syzygy cost ~$1500 using the O3 model). I suspect this could be brought down another 10x by walking up a "complexity tree": first try porting symbols mechanically (constants, structs, enums), then trying out a cheap model before falling back to more expensive models. For instance, Gemini Flash 2.5 is capable of producing correct Rust code for some non-trivial problems.</li>
<li><em>Full automation is hard</em>. The chat based interface of LLMs/agents can lead us to believe that they are more capable than they really are. We don't often realize how much we're guiding the direction of the LLMs until we try to go completely hands off.</li>
<li><em>Full automation isn't necessary</em>. Imagine a system where you have agents port as many symbols as they possibly can, in parallel. If a symbol fails to port, you mark it "tainted" and move on. You then continue until you can't port anything else. In such a system, a human could be brought in to fix issues as they emerge, but you could still rely on the agents to handle &gt;90% of the work of porting in an asynchronous manner.</li>
</ul>
<p>If you're interested in the (truly terrible) code I used for the porting, you
can find it on <a href="https://github.com/rjpower/portkit/">Github</a>. I'm not sure if I'll
continue down this route further myself, but let me know if you're interested in
this space or trying to port a project and I'm happy to chat more!</p>


</section>

    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing documentation for AI: best practices (183 pts)]]></title>
            <link>https://docs.kapa.ai/improving/writing-best-practices</link>
            <guid>44311217</guid>
            <pubDate>Wed, 18 Jun 2025 16:23:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.kapa.ai/improving/writing-best-practices">https://docs.kapa.ai/improving/writing-best-practices</a>, See on <a href="https://news.ycombinator.com/item?id=44311217">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Retrieval-Augmented Generation (RAG) systems like Kapa rely on your
documentation to provide accurate, helpful information. When documentation
serves both humans and machines well, it creates a self-reinforcing loop of
content quality: clear documentation improves AI answers, and those answers
help surface gaps that further improve the docs.</p>
<p>This guide provides best practices for creating documentation that works
effectively for both human readers and AI/LLM consumption in RAG systems. Many
best practices benefit both simultaneously, often in complementary ways.</p>
<h2 id="why-documentation-quality-matters">Why documentation quality matters<a href="#why-documentation-quality-matters" aria-label="Direct link to Why documentation quality matters" title="Direct link to Why documentation quality matters">​</a></h2>
<p>Documentation quality has always been important for helping users understand
and use your product effectively. And it becomes even more important when AI
systems use that same content to answer user questions. Poor documentation
doesn't just frustrate human readers, it directly degrades the quality of AI
responses, creating a compounding problem where bad content leads to bad
answers.</p>
<p>Understanding how AI systems process and use your documentation reveals why
content quality is non-negotiable for good AI performance.</p>
<h3 id="how-ai-systems-process-your-documentation">How AI systems process your documentation<a href="#how-ai-systems-process-your-documentation" aria-label="Direct link to How AI systems process your documentation" title="Direct link to How AI systems process your documentation">​</a></h3>
<p>Kapa works by finding relevant pieces of your content and using them to
construct answers. The process involves three main components:</p>
<ul>
<li><strong>Retriever</strong>: Searches through your knowledge sources to find content that
matches the user's question</li>
<li><strong>Vector database</strong>: Stores your content in a searchable format that enables
fast and accurate retrieval</li>
<li><strong>Generator</strong>: A Large Language Model (LLM) that uses the retrieved content
to create helpful responses</li>
</ul>
<p>Information flows through a specific process once you connect knowledge sources
to Kapa:</p>
<ol>
<li><strong>Ingestion</strong>: Content is divided into chunks (smaller, focused sections)
and stored in the vector database</li>
<li><strong>Query processing</strong>: When users ask questions, the system converts their
question into a searchable format</li>
<li><strong>Retrieval</strong>: The system finds the most relevant chunks from your
documentation</li>
<li><strong>Answer generation</strong>: The LLM uses these chunks as context to generate a
response</li>
</ol>
<p>In the steps that an AI takes to consume your content, there are some writing
and structural patterns worth highlighting that can negatively impact how well
your content is understood:</p>
<ul>
<li><strong>AI systems work with chunks</strong>: They process documentation as discrete,
independent pieces rather than reading it as a continuous narrative</li>
<li><strong>They rely on content matching</strong>: They find information by comparing user
questions with your content, not by following logical document structure</li>
<li><strong>They lose implicit connections</strong>: Relationships between sections may not be
preserved unless explicitly stated</li>
<li><strong>They cannot infer unstated information</strong>: Unlike humans who can make
reasonable assumptions, AI systems can only work with explicitly documented
information</li>
</ul>
<p>Documentation optimized for AI systems should ideally be explicit,
self-contained, and contextually complete. The more a chunk can stand alone
while maintaining clear relationships to related content, the better it can be
understood by the AI. The more explicit and less ambiguous the information is,
the better the retrieval accuracy is and the better equipped the AI becomes at
answering questions confidently.</p>
<p>While AI does work remarkably well with unstructured content, it's also true
that information written and structured for with retrieval in mind can greatly
improve the quality of an "Ask AI" interface to your knowledge sources.</p>
<h3 id="why-chunking-is-necessary">Why chunking is necessary<a href="#why-chunking-is-necessary" aria-label="Direct link to Why chunking is necessary" title="Direct link to Why chunking is necessary">​</a></h3>
<p>Ideally, chunking would not be necessary, and the AI could continuously keep
your entire knowledge base in context, all the time. Unfortunately, this is
impractical. Not only due to token limits but also because LLMs perform
significantly better when provided with optimized, focused contexts. A large or
overly broad context increases the likelihood that the model overlooks or
misinterprets critical information, resulting in reduced accuracy and less
coherent outputs.</p>
<p>Dividing documents into smaller, semantically coherent chunks enables retrieval
systems to present the most relevant content to the LLM. This targeted approach
significantly improves model comprehension, retrieval precision, and overall
response quality.</p>
<h2 id="quick-tips-to-optimize-your-content">Quick tips to optimize your content<a href="#quick-tips-to-optimize-your-content" aria-label="Direct link to Quick tips to optimize your content" title="Direct link to Quick tips to optimize your content">​</a></h2>
<p>Optimizing content for AI is similar in principle to optimizing content for
accessibility and screen readers: the clearer, more structured, and more
machine-readable your content is, the better it performs. Just as clear
semantic structure helps accessibility tools parse content effectively, a clear
structure significantly improves AI accuracy. This section outlines some
actionable, practical improvements you can apply today to make your docs more
machine-readable.</p>
<p>Prioritizing these adjustments sets a strong foundation for addressing more
nuanced content challenges, as discussed in the section <a href="#content-design-challenges-for-ai">Content design
challenges for AI</a>.</p>
<h3 id="1-use-standardized-semantic-html">1. Use standardized semantic HTML<a href="#1-use-standardized-semantic-html" aria-label="Direct link to 1. Use standardized semantic HTML" title="Direct link to 1. Use standardized semantic HTML">​</a></h3>
<p>For website sources, ensure correct and semantic use of HTML elements like
headings (<code>&lt;h1&gt;</code>, <code>&lt;h2&gt;</code>), lists (<code>&lt;ul&gt;</code>, <code>&lt;ol&gt;</code>), and tables (<code>&lt;table&gt;</code>).
Semantic HTML ensures clear document structure, improving how accurately
content is chunked and retrieved.</p>
<div><p>Example</p><div><pre tabindex="0"><code><span><span>&lt;</span><span>h2</span><span>&gt;</span><span>How to enable webhooks</span><span>&lt;/</span><span>h2</span><span>&gt;</span><span></span><br></span><span><span></span><span>&lt;</span><span>ol</span><span>&gt;</span><span></span><br></span><span><span>  </span><span>&lt;</span><span>li</span><span>&gt;</span><span>Log in to your CloudSync dashboard.</span><span>&lt;/</span><span>li</span><span>&gt;</span><span></span><br></span><span><span>  </span><span>&lt;</span><span>li</span><span>&gt;</span><span>Navigate to Settings </span><span>&amp;gt;</span><span> Webhooks.</span><span>&lt;/</span><span>li</span><span>&gt;</span><span></span><br></span><span><span>  </span><span>&lt;</span><span>li</span><span>&gt;</span><span>Toggle webhooks to "Enabled".</span><span>&lt;/</span><span>li</span><span>&gt;</span><span></span><br></span><span><span></span><span>&lt;/</span><span>ol</span><span>&gt;</span><br></span></code></pre></div></div>
<p>More importantly, <strong>avoid incorrect use</strong> of elements. An incorrectly placed
<code>&lt;h2&gt;</code> element, for example, can have dire consequences for how a machine
parses your content.</p>
<h3 id="2-avoid-pdfs-prefer-html-or-markdown">2. Avoid PDFs, prefer HTML or Markdown<a href="#2-avoid-pdfs-prefer-html-or-markdown" aria-label="Direct link to 2. Avoid PDFs, prefer HTML or Markdown" title="Direct link to 2. Avoid PDFs, prefer HTML or Markdown">​</a></h3>
<p>PDF documents often have complex visual layouts that make machine parsing
difficult. Migrating content from PDFs to HTML or Markdown drastically improves
text extraction and retrieval quality.</p>
<h3 id="3-create-crawler-friendly-content">3. Create crawler-friendly content<a href="#3-create-crawler-friendly-content" aria-label="Direct link to 3. Create crawler-friendly content" title="Direct link to 3. Create crawler-friendly content">​</a></h3>
<p>Simplify page structures by reducing or eliminating custom UI elements,
JavaScript-driven dynamic content, and complex animations. Clear, predictable
HTML structure facilitates easier indexing and parsing.</p>
<p>Replace complex JavaScript widgets with plain-text alternatives or simple
interactive elements.</p>
<h3 id="4-ensure-semantic-clarity">4. Ensure semantic clarity<a href="#4-ensure-semantic-clarity" aria-label="Direct link to 4. Ensure semantic clarity" title="Direct link to 4. Ensure semantic clarity">​</a></h3>
<p>Use descriptive headings and meaningful URLs reflecting the content hierarchy.
Semantic clarity helps the AI correctly infer content relationships, greatly
enhancing retrieval accuracy.</p>
<div><p>Example of a meaningful URL</p><div><pre tabindex="0"><code><span><span>✅ Good: /docs/cloudsync/setup-webhooks</span><br></span><span><span>❌ Poor: /docs/page12345</span><br></span></code></pre></div></div>
<h3 id="5-provide-text-equivalents-for-visuals">5. Provide text equivalents for visuals<a href="#5-provide-text-equivalents-for-visuals" aria-label="Direct link to 5. Provide text equivalents for visuals" title="Direct link to 5. Provide text equivalents for visuals">​</a></h3>
<p>Always include clear text descriptions for critical visual information such as
diagrams, charts, and screenshots. This ensures crucial details remain
accessible to machines and screen readers alike.</p>
<div><p>Example</p><div><pre tabindex="0"><code><span><span>!</span><span>[</span><span>System architecture diagram</span><span>](</span><span>architecture.png</span><span>)</span><span></span><br></span><span><span></span><br></span><span><span></span><span>**</span><span>Figure 1:</span><span>**</span><span> Diagram illustrating the CloudSync integration workflow,</span><br></span><span><span>detailing authentication, data upload, and confirmation steps.</span><br></span></code></pre></div></div>
<h3 id="6-keep-layouts-simple">6. Keep layouts simple<a href="#6-keep-layouts-simple" aria-label="Direct link to 6. Keep layouts simple" title="Direct link to 6. Keep layouts simple">​</a></h3>
<p>Avoid layouts where meaning is derived heavily from visual positioning or
formatting. Layout is lost during conversion, and any meaning it was designed
to convey with it. Content structured simply with clear headings, lists, and
paragraphs translates effectively into plain text.</p>
<h2 id="content-design-challenges-for-ai">Content design challenges for AI<a href="#content-design-challenges-for-ai" aria-label="Direct link to Content design challenges for AI" title="Direct link to Content design challenges for AI">​</a></h2>
<p>This section takes a closer look at common content design anti-patterns that
can create challenges for AI systems. These challenges often arise from how
information is organized, contextualized, or assumed rather than how it's
formatted. Each example highlights a specific problem pattern, why it causes
issues for AI, and how to rewrite or restructure your content to avoid it.</p>
<h3 id="contextual-dependencies">Contextual dependencies<a href="#contextual-dependencies" aria-label="Direct link to Contextual dependencies" title="Direct link to Contextual dependencies">​</a></h3>
<p><strong>The problem:</strong> Documentation that scatters key details and definitions across
multiple sections or paragraphs creates problems when content is divided into
chunks. When critical information is separated from its context, individual
chunks can become ambiguous or incomplete.</p>
<p>Understanding how chunking works in practice reveals why proximity matters.
Kapa attempts to preserve document structure by keeping sections intact when
possible, but practical constraints often force splits:</p>
<ul>
<li>Sections that are too long get divided at paragraph or sentence boundaries</li>
<li>Sections that are too short get combined with neighboring content</li>
<li>Chunk sizes must be balanced for optimal retrieval performance</li>
</ul>
<p>Since chunk boundaries can't be perfectly predicted, the closer related
information appears in your source content, the more likely it stays together
after chunking. This proximity principle becomes critical for maintaining
meaning.</p>
<p>Consider this (simplified) problematic example:</p>
<div><pre tabindex="0"><code><span><span>Authentication tokens expire after 24 hours by default.</span><br></span><span><span></span><br></span><span><span>The system provides several configuration options for different environments.</span><br></span><span><span></span><br></span><span><span>When implementing the login flow, ensure you handle this appropriately.</span><br></span></code></pre></div>
<p>When this content gets chunked, the middle sentence about configuration options
might cause the chunking algorithm to separate the token expiration detail from
the implementation guidance. The resulting chunk containing "When implementing
the login flow, ensure you handle this appropriately" loses crucial context
about what "this" refers to and the specific 24-hour timeframe.</p>
<p><strong>The remedy:</strong> Keep related information together within close proximity. When
introducing a concept that has important constraints or context, include those
details in the same paragraph or immediately adjacent paragraphs.</p>
<div><pre tabindex="0"><code><span><span>Authentication tokens expire after 24 hours by default. When implementing the</span><br></span><span><span>login flow, ensure you handle token expiration by refreshing tokens before the</span><br></span><span><span>24-hour limit or implementing proper error handling for expired token</span><br></span><span><span>responses.</span><br></span><span><span></span><br></span><span><span>The system provides several configuration options for different environments,</span><br></span><span><span>including custom token expiration periods.</span><br></span></code></pre></div>
<p>By keeping the constraint (24-hour expiration) close to its implementation
guidance, they're much more likely to remain in the same chunk, regardless of
where the boundaries fall.</p>
<p>Look for sections that become unclear when read in isolation, especially where
section headings are generic and multi-step processes that reference context
from earlier paragraphs.</p>
<h3 id="semantic-discoverability-gaps">Semantic discoverability gaps<a href="#semantic-discoverability-gaps" aria-label="Direct link to Semantic discoverability gaps" title="Direct link to Semantic discoverability gaps">​</a></h3>
<p><strong>The problem:</strong> Kapa finds information based on semantic similarity between
queries and content. If important terms or concepts aren't present in a chunk,
that chunk won't be retrieved for relevant queries, even if it contains exactly
the information needed.</p>
<div><pre tabindex="0"><code><span><span>##</span><span> Configure timeouts</span><span></span><br></span><span><span></span><br></span><span><span>Configure custom timeout settings and retry logic for improved reliability in</span><br></span><span><span>production environments. Access these options through the admin panel.</span><br></span></code></pre></div>
<p>If a user asks "How do I configure <strong>CloudSync</strong> timeouts?", this chunk might
not be retrieved because "CloudSync" doesn't appear in the text.</p>
<p><strong>The remedy:</strong> Establish consistent terminology for your product's unique
concepts and use them systematically. Include specific product or feature names
when documenting functionality.</p>
<div><pre tabindex="0"><code><span><span>##</span><span> Configure CloudSync timeouts</span><span></span><br></span><span><span></span><br></span><span><span>Configure custom CloudSync timeout settings and retry logic for improved</span><br></span><span><span>reliability in production environments. Access these options through the</span><br></span><span><span>CloudSync admin panel.</span><br></span></code></pre></div>
<p>Your product's unique terminology won't be well-represented in the model's
training data. Explicit, consistent usage helps establish what content relates
to which product features.</p>
<p><strong>A note of balance:</strong> This doesn't mean you should repeat the product name in
every sentence or heading. Kapa also uses document structure, URLs, and parent
headings to infer context. The important thing is that for any given chunk,
there’s a clear and consistent signal that connects it to your product or
feature. See <a href="#hierarchical-information-architecture">Hierarchical information architecture</a>
for how structural metadata supports this.</p>
<h3 id="implicit-knowledge-assumptions">Implicit knowledge assumptions<a href="#implicit-knowledge-assumptions" aria-label="Direct link to Implicit knowledge assumptions" title="Direct link to Implicit knowledge assumptions">​</a></h3>
<p><strong>The problem:</strong> Kapa operates on a simple principle: if information isn't
explicitly documented, it doesn't exist in the system's knowledge base. Unlike
human readers who can draw on external knowledge or make reasonable inferences,
Kapa only works with the information provided.</p>
<p>When documentation assumes user knowledge, these become dangerous gaps.
Well-designed RAG systems should choose uncertainty over inaccuracy, but this
only works when documentation explicitly addresses the topics users ask about.</p>
<p><strong>The remedy:</strong> Include prerequisite steps within procedural content rather
than assuming prior setup. When referencing external tools or concepts, provide
brief context or links to detailed explanations.</p>
<div><p>Before</p><div><pre tabindex="0"><code><span><span>##</span><span> Setting up webhooks</span><span></span><br></span><span><span></span><br></span><span><span>Configure your endpoint URL in the dashboard and test the connection.</span><br></span></code></pre></div></div>
<div><p>After</p><div><pre tabindex="0"><code><span><span>##</span><span> Setting up CloudSync webhooks</span><span></span><br></span><span><span></span><br></span><span><span>Before configuring webhooks, ensure you have:</span><br></span><span><span></span><br></span><span><span></span><span>-</span><span> A publicly accessible HTTPS endpoint</span><br></span><span><span></span><span>-</span><span> Valid SSL certificate</span><br></span><span><span></span><span>-</span><span> CloudSync API credentials</span><br></span><span><span></span><br></span><span><span>Configure your endpoint URL in the CloudSync dashboard under Settings &gt;</span><br></span><span><span>Integrations, then use the "Test connection" button to verify setup.</span><br></span></code></pre></div></div>
<p>Look for instructions that assume familiarity with tools or interfaces, or
reference "standard" configurations without explanation.</p>
<h3 id="visual-information-dependencies">Visual information dependencies<a href="#visual-information-dependencies" aria-label="Direct link to Visual information dependencies" title="Direct link to Visual information dependencies">​</a></h3>
<p><strong>The problem:</strong> Critical information embedded in images, diagrams, and videos
create problems for the ingestion processes that parse your documentation. When
key information appears only in visual elements, users may receive incomplete
answers.</p>
<div><p>Example: Information that completely depends on a graphical element</p><div><pre tabindex="0"><code><span><span>See the diagram below for the complete API workflow:</span><br></span><span><span></span><span>!</span><span>[</span><span>Complex flowchart showing 8-step process</span><span>](</span><span>workflow.png</span><span>)</span><span></span><br></span><span><span></span><br></span><span><span>Follow these steps to implement the integration.</span><br></span></code></pre></div></div>
<p>Instructions that depend on visual elements become inaccessible to automated
systems, making the instruction meaningless.</p>
<p><strong>The remedy:</strong> Provide text-based alternatives that capture the essential
information. Represent workflow diagrams as numbered step lists while keeping
visual elements as supplements.</p>
<div><pre tabindex="0"><code><span><span>##</span><span> CloudSync API workflow</span><span></span><br></span><span><span></span><br></span><span><span>The CloudSync integration follows this workflow:</span><br></span><span><span></span><br></span><span><span></span><span>1.</span><span> </span><span>**</span><span>Authentication</span><span>**</span><span>: Send API credentials to </span><span>`/auth/token`</span><span> endpoint</span><br></span><span><span></span><span>2.</span><span> </span><span>**</span><span>Validation</span><span>**</span><span>: System validates credentials and returns access token</span><br></span><span><span></span><span>3.</span><span> </span><span>**</span><span>Data preparation</span><span>**</span><span>: Format your data according to CloudSync schema</span><br></span><span><span></span><span>4.</span><span> </span><span>**</span><span>Upload request</span><span>**</span><span>: POST data to </span><span>`/sync/upload`</span><span> with access token</span><br></span><span><span></span><span>5.</span><span> </span><span>**</span><span>Processing</span><span>**</span><span>: CloudSync validates and processes the data</span><br></span><span><span></span><span>6.</span><span> </span><span>**</span><span>Status check</span><span>**</span><span>: Poll </span><span>`/sync/status/{job_id}`</span><span> for processing updates</span><br></span><span><span></span><span>7.</span><span> </span><span>**</span><span>Completion</span><span>**</span><span>: Receive confirmation when sync completes</span><br></span><span><span></span><span>8.</span><span> </span><span>**</span><span>Error handling</span><span>**</span><span>: Handle any validation or processing errors</span><br></span><span><span></span><br></span><span><span></span><span>!</span><span>[</span><span>API workflow diagram</span><span>](</span><span>workflow.png</span><span>)</span><span></span><br></span><span><span></span><span>_</span><span>Visual representation of the workflow steps above</span><span>_</span><br></span></code></pre></div>
<h3 id="layout-dependent-information">Layout-dependent information<a href="#layout-dependent-information" aria-label="Direct link to Layout-dependent information" title="Direct link to Layout-dependent information">​</a></h3>
<p><strong>The problem:</strong> Information that depends on visual layout, positioning, or
table structure often loses meaning when processed as text by machines. While
humans can interpret visual relationships and grouped content, AI systems
struggle to maintain these connections.</p>
<p>Complex or poorly structured comparison tables with merged headers and visual
groupings become ambiguous when converted to plain text:</p>
<table><thead><tr><th>Pricing</th><th></th><th></th></tr></thead><tbody><tr><td><strong>Basic Plan</strong></td><td><strong>Standard Plan</strong></td><td><strong>Enterprise Plan</strong></td></tr><tr><td>5 users</td><td>25 users</td><td>Unlimited users</td></tr><tr><td>1GB storage</td><td>10GB storage</td><td>Unlimited storage</td></tr><tr><td>Email support</td><td>Phone support</td><td>24/7 dedicated support</td></tr><tr><td><strong>API Limits</strong></td><td></td><td></td></tr><tr><td>100 requests/hour</td><td>1,000 requests/hour</td><td>No rate limit</td></tr><tr><td>Basic endpoints only</td><td>All endpoints</td><td>All endpoints + webhooks</td></tr></tbody></table>
<p><strong>The remedy:</strong> If a tabular representation is preferable, ensure that the
headers and rows are semantically correct. However, tabular representation is
not always appropriate or necessary. You may also consider alternatives that
preserve relationships in text form. Use structured lists or repeated context
that maintains the connections. For example:</p>
<div><pre tabindex="0"><code><span><span>##</span><span> CloudSync pricing plans</span><span></span><br></span><span><span></span><br></span><span><span></span><span>###</span><span> Basic Plan</span><span></span><br></span><span><span></span><br></span><span><span></span><span>-</span><span> 5 users</span><br></span><span><span></span><span>-</span><span> 1GB storage</span><br></span><span><span></span><span>-</span><span> Email support</span><br></span><span><span></span><span>-</span><span> API limits: 100 requests/hour, basic endpoints only</span><br></span><span><span></span><br></span><span><span></span><span>###</span><span> Standard Plan</span><span></span><br></span><span><span></span><br></span><span><span></span><span>-</span><span> 25 users</span><br></span><span><span></span><span>-</span><span> 10GB storage</span><br></span><span><span></span><span>-</span><span> Phone support</span><br></span><span><span></span><span>-</span><span> API limits: 1,000 requests/hour, all endpoints</span><br></span><span><span></span><br></span><span><span></span><span>###</span><span> Enterprise Plan</span><span></span><br></span><span><span></span><br></span><span><span></span><span>-</span><span> Unlimited users</span><br></span><span><span></span><span>-</span><span> Unlimited storage</span><br></span><span><span></span><span>-</span><span> 24/7 dedicated support</span><br></span><span><span></span><span>-</span><span> API limits: No rate limit, all endpoints plus webhooks</span><br></span></code></pre></div>
<p>Keep simple reference tables where each row is self-contained, but supplement
or replace complex tables where relationships between cells convey important
meaning.</p>
<h2 id="content-organization">Content organization<a href="#content-organization" aria-label="Direct link to Content organization" title="Direct link to Content organization">​</a></h2>
<p>The following techniques help create content that can be effectively retrieved,
without sacrificing readability.</p>
<h3 id="hierarchical-information-architecture">Hierarchical information architecture<a href="#hierarchical-information-architecture" aria-label="Direct link to Hierarchical information architecture" title="Direct link to Hierarchical information architecture">​</a></h3>
<p>When your content gets ingested into Kapa, preprocessing steps extract metadata
that helps preserve context and boost retrieval accuracy. One of the most
valuable pieces of data extracted is the hierarchical position of each document
or section.</p>
<p>This hierarchy includes multiple layers of context: URL paths, document titles,
and headings. These elements work together to build contextual understanding
for content chunks after they're separated from their original location.</p>
<p>Design your content hierarchy so that each section carries sufficient context
to be understood independently, while maintaining clear relationships to parent
and sibling content.</p>
<p>When planning content structure, consider how users would find any given
section without search. Ensure each section includes enough context to be
understood independently:</p>
<ul>
<li><strong>Product family</strong>: Which product or service area</li>
<li><strong>Product name</strong>: Specific product or feature name</li>
<li><strong>Version information</strong>: When applicable</li>
<li><strong>Component specificity</strong>: Subfeatures or modules</li>
<li><strong>Functional context</strong>: What the user is trying to accomplish</li>
</ul>
<p>This hierarchical clarity helps AI systems understand relationships between
concepts and provides richer context when retrieving information for user
queries.</p>
<h3 id="self-contained-sections">Self-contained sections<a href="#self-contained-sections" aria-label="Direct link to Self-contained sections" title="Direct link to Self-contained sections">​</a></h3>
<p>Documentation sections that depend on readers following a linear path or
remembering details from previous sections become problematic when processed as
independent chunks. Sections are retrieved based on relevance and document
order is not preserved, so sections should ideally make sense when encountered
in isolation.</p>
<p>Compare these two approaches to the same information:</p>
<div><p>Context-dependent</p><div><pre tabindex="0"><code><span><span>##</span><span> Updating webhook URLs</span><span></span><br></span><span><span></span><br></span><span><span>Now change the endpoint to your new URL and save the configuration.</span><br></span></code></pre></div></div>
<div><p>Self-contained</p><div><pre tabindex="0"><code><span><span>##</span><span> Updating webhook URLs</span><span></span><br></span><span><span></span><br></span><span><span>To update webhook endpoints in CloudSync:</span><br></span><span><span></span><br></span><span><span></span><span>1.</span><span> Navigate to Settings &gt; Webhooks in your CloudSync dashboard</span><br></span><span><span></span><span>2.</span><span> Select the webhook you want to modify</span><br></span><span><span></span><span>3.</span><span> Change the endpoint URL to your new address, and click Save</span><br></span></code></pre></div></div>
<p>The self-contained version works when retrieved as an isolated chunk because it
includes the essential context: what system (CloudSync), where to find the
setting (Settings &gt; Webhooks), and complete steps. The context-dependent
version assumes the reader knows what "endpoint" refers to and where they are
in the interface.</p>
<p>Front-load essential context and include complete information within each
section boundary. This doesn't mean repeating everything everywhere, but
ensuring sections remain actionable when encountered independently.</p>
<p>Consider starting each section with brief context about its scope and
prerequisites, using descriptive headings that indicate what the section
accomplishes, and including essential background information without assuming
prior reading. Look for sections that reference "as mentioned above," "now that
you've," or "with everything configured" as signals that context needs to be
made explicit.</p>
<h3 id="error-context-with-solutions">Error context with solutions<a href="#error-context-with-solutions" aria-label="Direct link to Error context with solutions" title="Direct link to Error context with solutions">​</a></h3>
<p>Troubleshooting documentation deserves special attention because users often
search by copying exact error messages they encounter. When your documentation
includes the specific error text alongside solutions, it creates direct matches
between user queries and helpful content.</p>
<p>When documenting troubleshooting steps, quote exact error messages and describe
observable symptoms alongside solutions.</p>
<div><p>Generic troubleshooting</p><div><pre tabindex="0"><code><span><span>##</span><span> Connection problems</span><span></span><br></span><span><span></span><br></span><span><span>If the connection fails, check your network settings and firewall configuration.</span><br></span></code></pre></div></div>
<div><p>Specific troubleshooting</p><div><pre tabindex="0"><code><span><span>##</span><span> CloudSync connection problems</span><span></span><br></span><span><span></span><br></span><span><span></span><span>###</span><span> Error: "Connection timeout after 30 seconds"</span><span></span><br></span><span><span></span><br></span><span><span>This error occurs when CloudSync cannot reach the…</span><br></span><span><span></span><br></span><span><span></span><span>###</span><span> Error: "Authentication failed (401)"</span><span></span><br></span><span><span></span><br></span><span><span>This indicates invalid or expired credentials…</span><br></span></code></pre></div></div>
<p>Including exact error text ensures users can find help when searching with the
specific messages they're seeing. To identify which error messages to
prioritize in your documentation, review the <a href="https://docs.kapa.ai/analytics/common-questions">Common Questions</a>
analytics in the Kapa platform. This shows you the actual error messages and
problems users are asking about most frequently.</p>
<h2 id="conclusion">Conclusion<a href="#conclusion" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Creating documentation that serves both human readers and AI effectively
centers on a fundamental principle: explicit, self-contained content that
maintains clear relationships between concepts. Eliminating contextual
dependencies, ensuring discoverability, filling knowledge gaps, and providing
text alternatives for visual content help mitigate inherent limitations in how
machines consume your docs.</p>
<p>Documentation that works for AI is, at its core, just great documentation:
clear, structured, explicit, and user-focused. The better your docs serve your
users, the better your AI serves them, too.</p>
<p>Review and analyze user conversations, particularly conversations with
uncertain or downvoted answers. Start with immediate fixes to frequently asked
questions, then gradually restructure scattered information into coherent,
complete sections. The goal is documentation where every section stands alone
while maintaining logical connections to related concepts.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My iPhone 8 Refuses to Die: Now It's a Solar-Powered Vision OCR Server (320 pts)]]></title>
            <link>https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/</link>
            <guid>44310944</guid>
            <pubDate>Wed, 18 Jun 2025 15:49:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/">https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/</a>, See on <a href="https://news.ycombinator.com/item?id=44310944">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>After running for over a year, my solar-powered setup has processed <strong>83,418 OCR requests</strong> and <strong>48GB of images</strong> using nothing but Apple’s Vision framework and renewable energy. Most people toss their old iPhones in a drawer when they upgrade. Me? I turned mine into a server that saves me money while running completely off-grid.</p>
<p><em>Note: The OCR processing serves a separate side project unrelated to this blog.</em></p>
<p>Could I have just run this on my Mac like a normal person? Absolutely. But where’s the fun in that?</p>
<p><img loading="lazy" src="https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/iphone-8-vision-ocr-server-solar.jpg" alt="iPhone 8 OCR Server Setup">
</p>
<h2 id="tldr---technical-summary">TL;DR - Technical Summary</h2>
<p><strong>The Setup:</strong></p>
<ul>
<li>iPhone 8 running SwiftUI app with Apple Vision OCR</li>
<li>EcoFlow River 2 Pro (768Wh) + 220W solar panel</li>
<li>Mini PC handling web services and API routing</li>
<li>Tailscale network connecting everything</li>
</ul>
<p><strong>Performance After 1+ Year:</strong></p>
<ul>
<li>83,418 total OCR requests processed</li>
<li>48GB of image data handled</li>
<li>1000+ requests on busy days</li>
<li>76% battery health after continuous operation</li>
<li>$84-120 CAD annual electricity savings</li>
</ul>
<p><strong>Key Learnings:</strong></p>
<ul>
<li>Apple Vision framework rivals cloud services for accuracy</li>
<li>Old hardware is surprisingly reliable for server workloads</li>
<li>Solar power works well with proper battery management</li>
<li>Local processing beats cloud services for privacy and cost at scale</li>
</ul>
<h2 id="why-would-you-even-do-this">Why Would You Even Do This?</h2>
<h3 id="the-logical-approach">The Logical Approach</h3>
<p>I have an image-heavy personal project that chews through hundreds of images daily, categorizing them automatically. Any reasonable person would run the OCR processing on their Mac - Apple’s Vision framework works great on macOS.</p>
<h3 id="the-me-approach">The “Me” Approach</h3>
<p>But I’m not reasonable. I see a perfectly good iPhone 8 and think: <em>“You know what this needs? A second career as a solar-powered image processing servant."</em> My EcoFlow River 2 Pro was sitting idle between camping trips, so switching my existing OCR server to solar felt like the natural evolution.</p>
<h3 id="the-unexpected-benefits">The Unexpected Benefits</h3>
<ul>
<li><strong>Real-time dashboard</strong> on my window sill while bird watching</li>
<li><strong>Grid independence</strong> for personal projects</li>
<li><strong>Actual cost savings</strong> that add up over time</li>
<li><strong>Amazing conversation starter</strong> when people visit</li>
</ul>
<p>The financial benefits are modest but real. Looking at my actual power consumption data: 37.4 kWh in May ($7.21) and 45.8 kWh in April ($8.82). Over a year, that’s meaningful savings.</p>
<p><em>“Is it practical? Debatable. Is it cool? Absolutely."</em></p>
<h2 id="what-exactly-is-this-thing">What Exactly Is This Thing?</h2>
<p>Here’s my delightfully over-engineered setup:</p>
<ol>
<li><strong>Mini PC</strong> running my web server, image processing service, Plex server, and various other services</li>
<li><strong>iPhone 8</strong> perched on my window sill, running a SwiftUI app that serves as both OCR processor and real-time dashboard</li>
<li><strong>EcoFlow power station</strong> keeping both devices running off-grid</li>
<li><strong>Tailscale network</strong> connecting everything seamlessly</li>
</ol>
<p>The workflow is beautifully simple: My image processing service sends images to the phone for OCR processing using Apple’s Vision framework. The phone processes the text, sends it back, and updates its dashboard with processing stats. All while I watch birds outside my window and feel smug about my setup.</p>
<h2 id="the-hardware-setup-solar-meets-computing">The Hardware Setup: Solar Meets Computing</h2>
<h3 id="power-station-choice-and-reality-check">Power Station Choice and Reality Check</h3>
<p>I didn’t buy the EcoFlow River 2 Pro specifically for this project. I bought it because I convinced myself I was going to become one of those outdoorsy people who camps and needs portable power. Well, turns out I’m still more of an “indoor cat with outdoor aspirations” kind of person. But my impulse purchase isn’t gathering dust!</p>
<p><img loading="lazy" src="https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/ecoflow-river-pro-2.jpg" alt="EcoFlow River 2 Pro portable power station">
</p>
<p><em>Pro tip: When researching portable power stations, <a href="https://gearscouts.com/">GearScouts.com</a> is an excellent price comparison site that could save you some time.</em></p>
<h3 id="power-consumption-and-solar-performance">Power Consumption and Solar Performance</h3>
<table>
<thead>
<tr>
<th>Component</th>
<th>Idle Power</th>
<th>Processing Load</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>iPhone 8 OCR Server</td>
<td>0.5-1W</td>
<td>2-5W</td>
<td>Surprisingly efficient</td>
</tr>
<tr>
<td>Mini PC (multiple services)</td>
<td>15W</td>
<td>25-30W</td>
<td>Includes Plex, Archive Warriors</td>
</tr>
<tr>
<td><strong>Total Daily Consumption</strong></td>
<td><strong>~1.2kWh</strong></td>
<td><strong>Variable</strong></td>
<td>Based on actual TP-Link data</td>
</tr>
</tbody>
</table>
<p><strong>Solar Performance by Season:</strong></p>
<ul>
<li><strong>Summer</strong>: 150-220W peak input, infinite runtime with battery charging</li>
<li><strong>Fall/Spring</strong>: 20-60W average, hybrid solar/battery operation</li>
<li><strong>Winter</strong>: 5-20W if lucky, mostly battery power (15-20 hours runtime)</li>
</ul>
<p>The River 2 Pro’s 768Wh capacity provides excellent buffer for Canada’s unpredictable weather. Its battery management system deserves credit - it’s not just dumping power into devices; it’s managing charging curves properly.</p>
<h2 id="building-the-ios-ocr-server-app">Building the iOS OCR Server App</h2>
<p>Creating a server on iOS sounds complicated, but Apple’s done most of the heavy lifting. The real challenge was making it run continuously without iOS deciding my app wasn’t important enough to keep running.</p>
<p><img loading="lazy" src="https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/iphone-8-ocr-server.jpeg" alt="iPhone 8 on window sill serving as OCR server">
</p>
<h3 id="apples-vision-framework-the-unsung-hero">Apple’s Vision Framework: The Unsung Hero</h3>
<p>Apple’s Vision framework is genuinely impressive and criminally underused. While everyone obsesses over ChatGPT and cloud-based OCR services, Apple quietly shipped a local OCR solution that’s fast, accurate, and runs entirely on-device.</p>
<p>Here’s the core processing code:</p>
<div><pre tabindex="0"><code data-lang="swift"><span>import</span> <span>Vision</span>
<span>import</span> <span>UIKit</span>

<span>func</span> <span>processImage</span>(<span>_</span> image: UIImage, completion: @escaping (String?) -&gt; Void) {
    <span>guard</span> <span>let</span> cgImage = image.cgImage <span>else</span> {
        completion(<span>nil</span>)
        <span>return</span>
    }

    <span>let</span> request = VNRecognizeTextRequest { request, error <span>in</span>
        <span>guard</span> <span>let</span> observations = request.results <span>as</span>? [VNRecognizedTextObservation] <span>else</span> {
            completion(<span>nil</span>)
            <span>return</span>
        }

        <span>let</span> recognizedText = observations.compactMap { observation <span>in</span>
            observation.topCandidates(<span>1</span>).first?.string
        }.joined(separator: <span>"</span><span>\n</span><span>"</span>)

        completion(recognizedText)
    }

    request.recognitionLevel = .accurate
    request.usesLanguageCorrection = <span>true</span>

    <span>let</span> handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
    <span>try</span>? handler.perform([request])
}
</code></pre></div><p>The accuracy rivals some cloud services I’ve tested, and it’s processing everything locally. No API calls, no usage limits, no privacy concerns.</p>
<h3 id="swiftui-dashboard-and-analytics">SwiftUI Dashboard and Analytics</h3>
<p>The dashboard was the fun part - something that would look cool on my window sill and provide real-time stats:</p>
<div><pre tabindex="0"><code data-lang="swift"><span>struct</span> <span>DashboardView</span>: View {
    @StateObject <span>private</span> <span>var</span> server = OCRServer()
    @State <span>private</span> <span>var</span> stats = ProcessingStats()

    <span>var</span> body: some View {
        VStack(spacing: <span>20</span>) {
            Text(<span>"OCR Server Status"</span>)
                .font(.title)
                .fontWeight(.bold)

            HStack {
                StatCard(title: <span>"Requests Today"</span>, value: <span>"</span><span>\(</span>stats.requestsToday<span>)</span><span>"</span>)
                StatCard(title: <span>"Total Processed"</span>, value: <span>"</span><span>\(</span>stats.totalProcessed<span>)</span><span>"</span>)
            }

            HStack {
                StatCard(title: <span>"Avg Processing Time"</span>, value: <span>"</span><span>\(</span>stats.avgProcessingTime<span>)</span><span>ms"</span>)
                StatCard(title: <span>"Success Rate"</span>, value: <span>"</span><span>\(</span>stats.successRate<span>)</span><span>%"</span>)
            }

            BatteryView(percentage: UIDevice.current.batteryLevel)

            Text(<span>"Server running on port 8080"</span>)
                .font(.caption)
                .foregroundColor(.secondary)
        }
        .padding()
    }
}
</code></pre></div><p>I integrated Google Analytics 4 because I’m a data nerd. The dashboard shows 139,917 total users with 17,643 this month, 6:28 average session duration, and 11 currently active users. It’s like having a tiny data center dashboard on your window sill.</p>
<h2 id="the-solar-power-challenge">The Solar Power Challenge</h2>
<p>Running electronics on solar power sounds simple until you face Canada’s weather reality. We get everything from blazing summer days (all 3 of them) to months of overcast skies that make solar panels about as useful as a chocolate teapot.</p>
<h3 id="seasonal-strategy-and-battery-management">Seasonal Strategy and Battery Management</h3>
<p>I’ve developed a weather-dependent approach:</p>
<ul>
<li><strong>Summer</strong>: Solar handles everything plus charges other devices</li>
<li><strong>Fall/Spring</strong>: Hybrid solar/battery with careful monitoring</li>
<li><strong>Winter</strong>: Mostly battery power with occasional solar boosts</li>
</ul>
<p>The phone’s battery health held up reasonably well. After over a year of constant operation, it’s at 76% capacity. The power station’s battery management deserves credit here.</p>
<p><img loading="lazy" src="https://terminalbytes.com/iphone-8-solar-powered-vision-ocr-server/iphone-8-battery-health-ocr-server.jpeg" alt="iPhone 8 Battery Health After OCR Server Usage">
</p>
<p>One unexpected discovery: the phone performs OCR faster when slightly warm (but not hot). Cold Canadian mornings mean slower processing times - something I never would have noticed with wall power.</p>
<h2 id="cost-analysis-solar-vs-grid-power">Cost Analysis: Solar vs Grid Power</h2>
<h3 id="investment-and-operating-costs">Investment and Operating Costs</h3>
<p><strong>Initial Investment:</strong></p>
<ul>
<li>EcoFlow River 2 Pro: $599 CAD (bought for camping anyway)</li>
<li>220W Solar Panel: $180 CAD</li>
<li>Cables, mounting hardware: ~$50 CAD</li>
<li><strong>Additional solar investment</strong>: ~$230 CAD</li>
</ul>
<p><strong>Monthly Savings:</strong>
Based on actual EcoFlow data showing 37.4-45.8 kWh monthly consumption, I’m saving approximately $84-120 CAD annually. The payback period is about 2-3 years.</p>
<h3 id="comparison-with-cloud-ocr-services">Comparison with Cloud OCR Services</h3>
<p>Cloud OCR services typically charge $1.00-1.50 per 1,000 requests. With over 83,000 requests processed, cloud services would have cost me $83-125 CAD, plus privacy concerns about sending images to external servers.</p>
<p>My solar setup? Zero per-request costs and complete privacy.</p>
<h2 id="what-i-learned-after-a-year">What I Learned After a Year</h2>
<h3 id="reliability-surprises">Reliability Surprises</h3>
<p><strong>The hardware is incredibly reliable.</strong> This phone has been running continuously for over a year, and it just keeps going. Performance hasn’t degraded noticeably despite the constant workload.</p>
<p><strong>iOS background processing works better than expected</strong> - once you figure out the right approach. The key is using background app refresh properly and keeping the HTTP server active with regular requests.</p>
<p><strong>Apple’s Vision framework improves over time.</strong> Text recognition that used to fail now works perfectly, especially with handwritten text and unusual fonts.</p>
<h3 id="common-problems-and-solutions">Common Problems and Solutions</h3>
<p><strong>Solar Power Intermittency:</strong> I configured the power station to prioritize the phone (lower power draw) and gracefully shut down the Mini PC when battery gets low. The phone can handle basic OCR requests solo for several hours if needed.</p>
<p><strong>Heat Management:</strong> Direct sunlight plus continuous processing equals thermal throttling. I added shade, improved airflow, and implemented smart processing that reduces requests when the phone reports high temperature.</p>
<p><strong>iOS Background Limitations:</strong> iOS really doesn’t want apps running forever. I use background app refresh, minimal location services, and keep the HTTP server responding to requests. It’s a delicate balance between staying alive and preserving battery.</p>
<h2 id="why-this-actually-matters">Why This Actually Matters</h2>
<p>Beyond the obvious coolness factor, this project demonstrates several important principles:</p>
<p><strong>Privacy First:</strong> Every image stays on my devices. No cloud uploads, no third-party access. In an era where everything gets sent to someone else’s computer, truly local processing feels revolutionary.</p>
<p><strong>Energy Independence:</strong> While the savings aren’t life-changing, the principle matters. This proves meaningful computing workloads can run entirely on renewable energy, even in challenging climates.</p>
<p><strong>E-Waste Reduction:</strong> That phone was destined for a drawer. Now it’s a productive member of my tech ecosystem. How many old devices could be repurposed instead of becoming electronic waste?</p>
<p><strong>Local-First Computing:</strong> Not everything needs to be in the cloud. Sometimes the best solution is sitting right in front of you, powered by the sun, processing your data locally and privately.</p>
<p>The setup has become my go-to demonstration for visitors interested in renewable energy or local computing. Plus, I genuinely love glancing at my window sill and seeing real-time processing stats while watching birds at my feeders.</p>
<h2 id="resources-and-next-steps">Resources and Next Steps</h2>
<p>If you’re interested in building something similar:</p>
<h3 id="hardware">Hardware</h3>
<ul>
<li><a href="https://us.ecoflow.com/products/river-2-pro-portable-power-station">EcoFlow River 2 Pro</a> - The power station I use</li>
<li><a href="https://www.renogy.com/100-watt-12-volt-monocrystalline-solar-panel/">Renogy 100W Solar Panel</a> - Similar to my setup</li>
<li>Any iPhone 8 or newer with iOS 13+ for Vision framework support</li>
</ul>
<h3 id="software-resources">Software Resources</h3>
<ul>
<li><a href="https://developer.apple.com/documentation/vision">Apple Vision Framework Documentation</a> - Official OCR implementation docs</li>
<li><a href="https://developer.apple.com/documentation/backgroundtasks">Background App Refresh Guide</a> - Keeping iOS apps alive</li>
<li><a href="https://github.com/Building42/Telegraph">SwiftUI HTTP Server Examples</a> - HTTP server implementation</li>
</ul>
<h3 id="power-management-tools">Power Management Tools</h3>
<ul>
<li><a href="https://www.kasasmart.com/">TP-Link Kasa Smart Plugs</a> - For monitoring actual power consumption</li>
<li>EcoFlow app - Built-in monitoring for the River 2 Pro</li>
<li><a href="https://gearscouts.com/">GearScouts.com</a> - Price comparison for power stations and outdoor gear</li>
</ul>
<p><em>This article was last updated while watching my solar-powered setup process its 83,418th OCR request, powered entirely by Canadian sunshine.</em></p>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built a tensor library from scratch in C++/CUDA (110 pts)]]></title>
            <link>https://github.com/nirw4nna/dsc</link>
            <guid>44310678</guid>
            <pubDate>Wed, 18 Jun 2025 15:20:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/nirw4nna/dsc">https://github.com/nirw4nna/dsc</a>, See on <a href="https://news.ycombinator.com/item?id=44310678">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/nirw4nna/dsc/blob/main/docs/logo.png"><img src="https://github.com/nirw4nna/dsc/raw/main/docs/logo.png" alt="Logo" width="200"></a></p><p dir="auto"><h3 tabindex="-1" dir="auto">
DSC
</h3><a id="user-content-dsc" aria-label="Permalink: 
DSC
" href="#dsc"></a></p>
<p dir="auto"><a href="https://opensource.org/licenses/BSD-3-Clause" rel="nofollow"><img src="https://camo.githubusercontent.com/2c9d35079d2be27b8d22ec33d2e0e45f76f0521dd7faff71cb99461112781759/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4253445f332d2d436c617573652d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-BSD_3--Clause-blue.svg"></a>
<a href="https://github.com/nirw4nna/dsc/actions/workflows/tests.yml"><img src="https://github.com/nirw4nna/dsc/actions/workflows/tests.yml/badge.svg" alt="Unit Tests"></a></p>
</div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">About</h2><a id="user-content-about" aria-label="Permalink: About" href="#about"></a></p>
<p dir="auto">DSC is a PyTorch-compatible tensor library and inference framework for machine learning models.
It features a C-compatible low-level API that is wrapped in a modern Python API very similar to NumPy / PyTorch but
with some nice usability improvements.</p>
<p dir="auto">Some key features of DSC include:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Intuitive API</strong>: DSC Python API closely resembles NumPy / PyTorch.</p>
</li>
<li>
<p dir="auto"><strong>Built-in neural networks support</strong>: DSC comes with <code>nn.Module</code> built-in. Porting a model from PyTorch to DSC
is trivial (check out the <a href="https://github.com/nirw4nna/dsc/tree/main/examples/models">examples</a>).</p>
</li>
<li>
<p dir="auto"><strong>Multiple backends</strong>: DSC supports both <strong>CPU</strong> and <strong>CUDA</strong> with other backends being worked on.
Programs written using DSC can seamlessly switch between backends by simply adding a <code>dsc.set_default_device('...')</code>
instruction, no changes needed.</p>
</li>
<li>
<p dir="auto"><strong>Minimal external dependencies</strong>: DSC doesn't require external libraries to be efficient.
On CPU the core operations are written from scratch in portable C++, this makes code written using DSC extremely portable.</p>
</li>
<li>
<p dir="auto"><strong>No runtime allocations</strong>: DSC has it's own custom memory allocator, memory is pre-allocated
only once so no extra calls to <code>malloc()</code> or <code>free()</code> are required. It's also possible
to switch to a linear allocator to remove the (minimal) overhead introduced by a general purpose allocator.</p>
</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick start" href="#quick-start"></a></p>
<p dir="auto">Getting started with DSC is very simple. The only requirements are:</p>
<ul dir="auto">
<li>A compiler with good support for C++20</li>
<li>GNU Make for building</li>
</ul>
<p dir="auto">On a Linux-based system these can be obtained with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt update
sudo apt install build-essential"><pre>sudo apt update
sudo apt install build-essential</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">The recommended way to install DSC is from source:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:nirw4nna/dsc.git
cd dsc/
python3 -m venv venv
source venv/bin/activate
python3 -m pip install -e ."><pre>git clone git@github.com:nirw4nna/dsc.git
<span>cd</span> dsc/
python3 -m venv venv
<span>source</span> venv/bin/activate
python3 -m pip install -e <span>.</span></pre></div>
<p dir="auto">To build the C++ library:</p>
<div dir="auto" data-snippet-clipboard-copy-content="make clean; make shared DSC_FAST=1"><pre>make clean<span>;</span> make shared DSC_FAST=1</pre></div>
<p dir="auto">This will compile DSC without any debug information, you can specify different options
to enable/disable specific features:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Option</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>DSC_LOG_LEVEL</td>
<td>Configure the logging level (values: [0-3] with 0 meaning everything on)</td>
</tr>
<tr>
<td>DSC_FAST</td>
<td>Turn off logging (level=2) and compile with the highest optimisation level</td>
</tr>
<tr>
<td>DSC_CUDA</td>
<td>Enable CUDA backend</td>
</tr>
<tr>
<td>DSC_MAX_OBJS</td>
<td>Max number of DSC tensors that can be used at the same time (<strong>default=1K</strong>)</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">To verify that everything worked out as expected try a simple operation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -c &quot;import dsc; x = dsc.arange(10); print(x)&quot;"><pre>python3 -c <span><span>"</span>import dsc; x = dsc.arange(10); print(x)<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">CUDA backend</h3><a id="user-content-cuda-backend" aria-label="Permalink: CUDA backend" href="#cuda-backend"></a></p>
<p dir="auto">This provides GPU acceleration on NVIDIA GPUs. To get started make sure to have the <a href="https://developer.nvidia.com/cuda-toolkit" rel="nofollow">CUDA Toolkit</a>
installed.</p>
<p dir="auto">To build the C++ library with CUDA enabled:</p>
<div dir="auto" data-snippet-clipboard-copy-content="make clean; make shared DSC_FAST=1 DSC_CUDA=1"><pre>make clean<span>;</span> make shared DSC_FAST=1 DSC_CUDA=1</pre></div>
<p dir="auto"><strong>Note:</strong> if you see errors when compiling with CUDA support make sure that the CUDA installation path is <code>/usr/local/cuda</code>
(default on Ubuntu).
If that is not the case you have to manually update the Makefile or set the <code>CUDA</code> environment variable before calling make.</p>
<p dir="auto">To verify that the CUDA backend is working try a simple GPU operation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -c &quot;import dsc; x = dsc.arange(10, device='cuda'); print(x)&quot;"><pre>python3 -c <span><span>"</span>import dsc; x = dsc.arange(10, device='cuda'); print(x)<span>"</span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running tests</h2><a id="user-content-running-tests" aria-label="Permalink: Running tests" href="#running-tests"></a></p>
<p dir="auto">DSC uses <code>pytest</code> to run unit tests against NumPy which is the reference for correctness.</p>
<p dir="auto">To run all the tests simple do:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd python/tests/
pytest -s test_ops.py --no-header --no-summary -q"><pre><span>cd</span> python/tests/
pytest -s test_ops.py --no-header --no-summary -q</pre></div>
<p dir="auto"><strong>Note:</strong> there are quite a few tests so to run them it's better to compile DSC with <code>DSC_FAST=1</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">BSD-3-Clause</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Framework Laptop 12 review (245 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/</link>
            <guid>44310583</guid>
            <pubDate>Wed, 18 Jun 2025 15:09:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/">https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/</a>, See on <a href="https://news.ycombinator.com/item?id=44310583">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="app">
    <p><a href="#main">
  Skip to content
</a></p>



<main id="main">
            <article data-id="2101033">
  
  <header>
  <div>
    <div>
      <p><span>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 40"><defs><clipPath id="section-gadgets_svg__a"><path fill="none" d="M0 0h40v40H0z"></path></clipPath><clipPath id="section-gadgets_svg__b"><path fill="none" d="M0 0h40v40H0z"></path></clipPath></defs><g clip-path="url(#section-gadgets_svg__a)"><g fill="currentColor" clip-path="url(#section-gadgets_svg__b)"><path d="M38 22c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2V4h-4V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2S8 .9 8 2v2H4v4H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v4h4v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h4v-4h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6zm-6 10H8V8h24z"></path><path d="M24.7 17.3 20 12h-7.1c-.6 0-1 .4-1 1s.4 1 1 1h6.3l4.1 4.7L20 22h8v-8z"></path><path d="m15.2 22.7 4.7 5.3H27c.6 0 1-.4 1-1s-.4-1-1-1h-6.3l-4.1-4.7 3.3-3.3h-8v8z"></path></g></g></svg>
  </span>
  <span>
    how much would you pay for personality?
  </span>
</p>
    </div>

    

    <p>
      A sturdy, thoughtful, cute design that just can't compete in its price range.
    </p>

    

    <div>
            <p><a data-pswp-width="2560" data-pswp-height="1440" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1440x810.jpeg 1440w" data-cropped="false" href="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg" target="_blank">
              <img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg" alt="" loading="eager" decoding="async" fetchpriority="high" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3150-1440x810.jpeg 1440w" sizes="(max-width: 2560px) 100vw, 2560px">
            </a></p><div id="caption-2101680">
    
    <p>
      Framework's Laptop 12 has a lot of personality, but also a lot of shortcomings.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
          </div>

    <div>
    
    <p>
      Framework's Laptop 12 has a lot of personality, but also a lot of shortcomings.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>"What's this purple laptop? It's cool."</p>
<p>Over a decade-plus of doing gadget reviews and review-adjacent things, my wife (and, lately, my 5-year-old) have mostly stopped commenting on the ever-shifting selection of laptops I have in my bag or lying around the house at any given time. Maybe she can't tell them apart, or maybe she just figures there isn't that much to say about whatever black or silver metal slab I'm carrying around. Either way, they practically never elicit any kind of response, unless there are just too many of them sitting out in too many places.</p>
<p>But she&nbsp;<em>did</em> ask about the Framework Laptop 12, the third and latest major design in Framework's slowly expanding lineup of modular, repairable, upgradeable laptops. With its five two-toned color options and sturdy plastic exterior, it's definitely more approachable and friendly-looking than the Laptop 13 or Laptop 16, both metal slabs with a somewhat less-finished and prototype-y look to them. But it retains the features that a certain kind of PC geek likes about Framework's other laptops—user-customizable and swappable ports, an easy-to-open design, first-class Linux support, and the promise of future upgrades that improve its performance and other specs.</p>
<h2>Look and feel</h2>
<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3126-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Laptop 12 stacked atop the Laptop 13.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Plastic gets a bad rap, and there are indeed many subpar plastic gadgets out there. When done poorly, plastic can look and feel cheap, resulting in less durable devices that show more wear over time.</p>
<p>But well-done plastic can still feel solid and high-quality, in addition to being easier to make in different colors. Framework says the Laptop 12's chassis is a combination of ABS plastic and <a href="https://en.wikipedia.org/wiki/Thermoplastic_polyurethane">TPU plastic</a> (a more flexible, rubberized material), molded over a metal inner structure. The result is something that can probably actually take the shock of a drop or a fall better than many aluminum-and-glass laptops without feeling overly cheap or chintzy.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>The five two-tone color options—the boring, businesslike black and gray, plus purple-and-gray lavender, pink-and-baby-blue bubblegum, and the green sage options—are the most fun thing about it, and the lavender and bubblegum colors are particularly eye-catching.</p>

<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3135-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Keyboard and trackpad. Only the lavender and gray laptops get a color-matched trackpad; the keyboard and deck are always different shades of gray.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Matching other components to the exterior of the system can be a bit of a crapshoot, though. The screwdriver and spudger that Framework provides for upgrading and repairing all of its systems <em>does</em> match the color of the laptop, and the two-tone styluses for the touchscreens will also match the laptops when they're made available for purchase in the coming months.</p>
<p>The lavender option is the only one that can also be configured with a color-matched lavender trackpad—the only other trackpad option is gray, and the keyboard deck and the keyboard itself are all gray no matter what color laptop you pick. This is presumably meant to limit the number of different trackpad options that Framework has to manufacture and stock, but it is too bad that the laptop's keyboard and palm rest aren't as colorful as the rest of it.</p>
<p>The Laptop 12 also uses Framework's still-unique Expansion Card system for customizing the built-in ports. These are all 10 Gbps USB 3.2 Gen 2 ports rather than the Thunderbolt ports on the Intel versions of the Laptop 13, but all four support the same speeds, all four support charging, and all four support display output, so you really can put whatever port you want wherever you want it.</p>
<p>A downside of the Laptop 12 is that, as of this writing, only the USB-C Expansion Modules are available in color-matched versions. If you want USB-A, HDMI, DisplayPort, or any other kind of port on your system, you'll get the silver modules that were designed to match the finish on the Framework Laptops 13 and 16, so you'll have to put up with at least one mismatched port on your otherwise adorable system.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          

<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3116-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Only the USB-C Expansion Cards are available in lavender, which can make for goofy-looking mismatches. But I do prefer the Framework 16-style retention switches to the Framework Laptop 13's retention buttons, which you need to hold down as you pull out the Expansion Card.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Once you get past the adorable design, the Expansion Modules, and the sturdy construction, the system's downsides start to become more apparent. The 12.2-inch, 1920×1200 touchscreen gets plenty bright and has a respectable contrast ratio (440 nits and 1,775:1 in our testing, respectively). But it's surrounded by thick black bezels on all sides, particularly on the bottom—it does seem that either a larger screen or a slightly smaller laptop design would be possible if so much space weren't wasted by these thick borders.</p>
<p>The display has good viewing angles but a distinctly mediocre color gamut, covering around 60 percent of the SRGB color space (compared to the high 90s for the Laptop 13 and most midrange to high-end IPS screens in other laptops). This is low enough that most colors appear slightly muted and washed out—reds most noticeably, though greens aren't much better. You definitely don't need a colorimeter to see the difference here.</p>
<p>Framework's color-matched stylus isn't ready yet, but you won't need to wait for one if you want to use a pen with this touchscreen. Both the Universal Stylus Initiative (USI) 2.0 and Microsoft Pen Protocol (MPP) 2.0 specs are supported, so the Surface Pen, a bunch of Lenovo styluses, and any number of inexpensive third-party Amazon styluses will all work just fine. That said, the screen can only support one of those stylus specs at a time—MPP is on by default, and you can swap between them in the BIOS settings.</p>
<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3142-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The webcam and mic have locks to disable them so that the OS can't see or use them.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>The keyboard feels mostly fine, with good key spacing and a nice amount of travel. I noticed that I was occasionally missing letters the first couple of days I used the laptop—I was pressing the keys, but they intermittently didn't register. That got better as I adjusted to the system. The trackpad is also unremarkable in a good way. Finger tracking and multi-touch gestures all worked as intended.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>But the keyboard lacks a backlight, and it doesn't have the fingerprint sensor you get with the Laptop 13. With no fingerprint sensor and no IR webcam, there are no biometric authentication options available for use with Windows Hello, so you'll either need a PIN or a password to unlock your laptop every time you want to use it. Either omission would be sort of annoying in a laptop in this price range (we complained about the lack of keyboard backlight in <a href="https://arstechnica.com/gadgets/2022/07/review-microsofts-surface-laptop-go-2-has-a-lot-of-problems-but-i-like-it-anyway/">the $700 Surface Laptop Go 2</a> a few years ago), but to be missing&nbsp;<em>both</em> is particularly frustrating in a modern system that costs this much.</p>
<h2>Repairs and upgrades</h2>



<p>We've been inside the Framework Laptop 13 enough times that we don't do deep dives into its insides anymore, but as a new (and, in some ways, more refined) design, the Laptop 12 warrants a closer look this time around.</p>
<p>Framework's pack-in Torx screwdriver is still the only tool you need to work on the Laptop 12. Undo the eight captive screws on the bottom of the laptop, and you'll be able to lift away the entire keyboard and trackpad area to expose all of the other internal components, including the RAM, SSD, battery, and the motherboard itself.</p>
<p>The motherboard is quite a bit smaller than the Framework Laptop 13 board, and the two are definitely&nbsp;<em>not</em> interchangeable. Framework has never said otherwise, but it's worth highlighting that these are two totally separate models that will have their own distinct components and upgrade paths—that goes for parts like the speakers and battery, too.</p>

<figure>
    <p><img width="2560" height="1441" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-2048x1153.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-980x552.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3171-1440x811.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Laptop 12 motherboard on top, Laptop 13 motherboard on bottom.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>As a result of that reduction in board space, the Laptop 12 can only fit a single DDR5 RAM slot, which reduces memory bandwidth and limits your RAM capacity to 48GB. It also uses shorter M.2 2230 SSDs, like the Surface lineup or the Steam Deck. Unlike a few years ago, these SSDs are now readily available at retail, and it's also easy to buy warranty-less ones on eBay or elsewhere that have been pulled from OEM systems. But they're still a bit more expensive than the more common M.2 2280 size, and you have fewer options overall.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>Framework has already published <a href="https://guides.frame.work/Guide/Framework+Laptop+12+(13th+Gen+Intel%C2%AE+Core%E2%84%A2)+DIY+Edition+Quick+Start+Guide/429?_gl=1*1dc9fwi*sg_ga4w_production_ga*NTEzNDU2MDg1LjE3NTAxODA0Mzg.*sg_ga4w_production_ga_PYG8X65YJJ*czE3NTAxODA0MzckbzEkZzEkdDE3NTAxODI3NDckajYwJGwwJGgw">a guide on setting up the DIY Edition of the laptop</a> and&nbsp;<a href="https://guides.frame.work/c/Framework_Laptop_12">a few repair guides</a> for common components. Guides for replacing bigger or more co parts, like the display or the webcam, are still listed as "coming soon."</p>
<h2>Performance and battery life</h2>
<p>I could politely describe the Laptop 12's 2.5-year-old 13th-gen Intel Core processor as "mature." This generation of Intel chips <em>has&nbsp;</em>stuck around for a lot longer than usual, to the point that Intel recently acknowledged that it has been dealing with shortages. They're appealing to PC companies because they still offer decent everyday performance for basic computing without the additional costs imposed by things like on-package memory or having some or all of the chip manufactured outside of Intel's own factories.</p>
<p>The upside of a slightly older processor is a more stable computing experience, in both Windows and Linux, since the companies and communities involved have had more time to add support and work out bugs; I had none of the sleep-and-wake issues or occasional video driver crashes I had while testing the Ryzen AI 300 version of the Framework Laptop 13.</p>


<p>The downside, of course, is that performance is pretty unexciting. These low-power U-series 12th- and 13th-gen Intel chips remain capable when it comes to day-to-day computing, but they fall far behind the likes of Intel and AMD's newer chips, Qualcomm's Snapdragon chips from the Microsoft Surface and other Copilot+ PCs, or the Apple M4 in the MacBook Air.</p>


<p>And while none of these chips are really intended for gaming laptops, the Laptop 12 isn't even a great fit for that kind of casual Steam Deck-y 3D gaming that most Framework Laptop 13 models can handle. Technically, this is the same basic Intel Iris Xe GPU that the first few generations of Framework Laptop 13 used, which is not exciting as integrated GPUs go but is at least still minimally capable. But because the Laptop 12 only has a single RAM slot instead of two, memory bandwidth is halved, which makes the GPU identify itself as "Intel UHD Graphics" to the device manager and drags down performance accordingly. (This is something these GPUs have always done, but they usually ship in systems that either have two RAM slots or soldered-down memory, so it usually doesn't come up.)</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>Framework has tuned these chips to consume the same amount of power in both the "Balanced" and "Best Performance" power modes in Windows, with a 15 W sustained power limit and a 40 W limit for shorter, bursty workloads. This keeps the laptop feeling nice and responsive for day-to-day use and helps keep a lid on power usage for battery life reasons, but it also limits its performance for extended CPU-intensive workloads like our Handbrake video encoding test.</p>

<p>The Laptop 12 takes a&nbsp;<em>lot</em> longer to accomplish these tasks than some other laptops we've tested with similar chips, either because of the lower memory bandwidth or because Best Performance mode doesn't let the chip consume a bunch of extra power. I'm not inclined to complain too much about this because it's not the kind of thing you really buy an ultraportable laptop to do, but as with light gaming, it's worth noting that the Laptop 12 doesn't hit that same "usable for these workloads in a pinch" balance that the Laptop 13 does.</p>
<figure>
    <p><img width="2048" height="1536" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008.png" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008.png 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-640x480.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-1024x768.png 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-768x576.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-1536x1152.png 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-980x735.png 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/Framework-Laptop-12.008-1440x1080.png 1440w" sizes="auto, (max-width: 2048px) 100vw, 2048px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Laptop 12's battery life is decent relative to most Laptop 13s.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>The Core i5 version of the Laptop 12 lasted around 10 hours in the PCMark Modern Office battery life test, which isn't stunning but is a step up from what the fully specced versions of the Framework Laptop 13 can offer. It will be just fine for a long flight or a full day of work or school. Our Framework reviews often complain about battery life, but I don't think it will be an issue here for most users.</p>
<h2>About that price</h2>
<p>In some ways, the Laptop 12 is trying to be a fundamentally&nbsp;<em>different</em> laptop from the Laptop 13. For all the Laptop 13's upgrades over the years, it has never had a touchscreen option, stylus support, or a convertible hinge.</p>
<p>But in most of the ways that count, the Laptop 12 is meant to be an "entry-level, lower-cost laptop," which is how Framework CEO Nirav Patel <a href="https://www.youtube.com/watch?v=Ejl-7X74tgc&amp;t=171s">has positioned it</a> in the company's announcement blog posts and videos. It features a slightly smaller, lower-resolution, less colorful screen with a lower refresh rate; a non-backlit keyboard; and considerably weaker processors. It also lacks both a fingerprint reader and a face-scanning webcam for Windows Hello.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>The issue is that these cost-cutting compromises come at a price that's a bit outside of what you'd expect of a "budget" laptop.</p>
<p>The DIY Edition of the Laptop 12 we're evaluating here—a version that ships with the Windows license and all the components you need but which you assemble yourself—will run you at least $1,176, depending on the Expansion Modules you choose for your ports. That includes 16GB of GDDR5 RAM and a 1TB M.2 2230 SSD, plus the Core i5-1334U processor option (2 P-cores, 8 E-cores). If you stepped down to a 500GB SSD instead, that's still $1,116. A pre-built edition—only available in black, but with identical specifications—would run you $1,049.</p>

<figure>
    <p><img width="2560" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3155-1440x810.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Laptop 13 compared to the Laptop 12. The Laptop 12 is missing quite a few quality-of-life things and has worse performance, but it isn't all that much cheaper.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>This puts the Framework Laptop 12 in the same general price range as Apple's MacBook Air, Microsoft's 13-inch Surface Laptop, and even many editions of the Framework Laptop 13. And the Laptop 12 is charming, but its day-to-day user experience falls well short of any of those devices.</p>
<p>You can make it cheaper! Say you go for the Core i3-1315U version (two P-cores, four E-cores) instead, and you buy your own 16GB stick of DDR5 RAM (roughly $50 instead of $80) and 1TB SSD ($70 or $80 for <a href="https://www.newegg.com/silicon-power-1tb/p/0D9-0021-00171?Item=9SIBDGPK454247">a decent one</a>, instead of $159). Say you have plenty of USB-C chargers at home so you don't need to pay $55 for Framework's version, and say you run Linux or ChromeOS, or you already have a Windows 11 product key, or you've brought your own Windows 11 key from one of those gray-market key selling sites (as little as $10).</p>
<p>Now we're talking about a PC that's a little under $700, which is closer to "reasonable" for a brand-new touchscreen PC. But the laptop's old CPU and poky performance also mean it's competing with a wide swath of refurbished, used, and closeout-priced older PCs from other manufacturers.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>In December, for example, I bought an SSD-less <a href="https://www.lenovo.com/us/en/p/laptops/thinkpad/thinkpadl/thinkpad-l13-yoga-gen-3-13-inch-intel/len101t0032?orgRef=https%253A%252F%252Fwww.google.com%252F&amp;srsltid=AfmBOoqjpN9S8iIG7xfG4vGA-Dv7fPWAoBj6sd6Y9oDZXN_KBVmVbRiT">Lenovo ThinkPad L13 Yoga Gen 3</a> from eBay for around $300, with around a year left on its warranty. After I'd added an SSD and reinstalled Windows—no additional cost because it had a valid Windows license already—I ended up with a PC with the same screen resolution and similar specs but with a better-quality display with smaller bezels that made the screen larger without making the laptop larger; a faster GPU configuration; a backlit keyboard; and a fingerprint reader.</p>
<p>I know it's not possible for everyone to just go out and buy a laptop like this. The boring black outline of a midrange ThinkPad is also the polar opposite of the Framework Laptop 12, but it's an example of what the tech-savvy buyer can find in the secondhand market if you're trying to find a cost-effective alternative to what Framework is offering here.</p>

<h2>A good laptop, but not a good value</h2>
<figure>
    <p><img width="2560" height="1441" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132.jpeg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132.jpeg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-2048x1153.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-980x552.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/06/IMG_3132-1440x811.jpeg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      The Framework Laptop 12.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>There are plenty of factors beyond Framework’s control that contribute to the Laptop 12’s price, starting with on-again-off-again global trade wars and the uncertainty that comes with them. There's also Framework’s status as a niche independent PC company rather than a high-volume behemoth. When you ship the number of computers that Apple does, it’s almost certainly easier to make a $999 laptop that is both premium and profitable.</p>
<p>But whatever the reason, I can’t escape the feeling that the Laptop 12 was meant to be cheaper than it has ended up being. The result is a computer with many of the compromises of an entry-level system, but without a matching entry-level price tag. It’s hard to put a price on some of the less-tangible benefits of a Framework laptop, like ease of repairs and the promise of future upgrades, but my gut feeling is that the Framework Laptop 13 falls on the “right” side of that line, and the Laptop 12 doesn't.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>I am charmed by the Laptop 12. It's cute and functional, and it stands out among high-end aluminum slabs. It adds some subtle refinement to elements of the original Framework Laptop 13 design, including some things I hope end up making it into some future iteration of its design—softer corners, more color options, and an easier-to-install keyboard and trackpad. And it's far from a <em>bad</em> performer for day-to-day desktop use; it's just that the old, poky processor limits its capabilities compared to other PCs that don't cost that much more than it does.</p>
<p>I probably wouldn't recommend this over the Laptop 13 for anyone interested in what Framework is doing, unless a touchscreen is a make-or-break feature, and even then, I'd encourage people to take a good, long look at Microsoft, Lenovo, Dell, or HP's convertible offerings first. But I hope that Framework does what it's done for the Laptop 13 over the last four or so years: introduce updated components, iterate on different elements of the design, and gradually bring the price down into a more reasonable range through refurbished and factory-second parts. As a $1,000-ish computer, this leaves a lot to be desired. But as the foundation for a new Framework platform, it has enough promise to be interesting.</p>
<h3>The good</h3>
<ul>
<li>Eye-catching, colorful, friendly design that stands out among metal slabs.</li>
<li>Simple to build, repair, and upgrade.</li>
<li>Dual-plastic design over a metal frame is good for durability.</li>
<li>First convertible touchscreen in the Framework laptop.</li>
<li>Customizable ports.</li>
<li>Decent performance for everyday computing.</li>
<li>Respectable battery life.</li>
</ul>
<h3>The bad</h3>
<ul>
<li>Old, slow chip isn't really suitable for light gaming or heavy productivity work that the larger Framework Laptop 13 can do.</li>
<li>Pre-built laptop only comes in boring black.</li>
<li>Mediocre colors and large bezels spoil the screen.</li>
<li>Keyboard sometimes felt like it was missing keystrokes until I had adjusted to compensate.</li>
</ul>
<h3>The ugly</h3>
<ul>
<li>It's just too expensive for what it is. It looks and feels like a lower-cost laptop, but without a dramatically lower price than the nicer, faster Framework 13.</li>
</ul>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/andrew_cunningham/"><img src="https://cdn.arstechnica.net/wp-content/uploads/2016/05/a.cunningham-45-1.jpg" alt="Photo of Andrew Cunningham"></a></p>
  </div>

  <div>
    

    <p>
      Andrew is a Senior Technology Reporter at Ars Technica, with a focus on consumer tech including computer hardware and in-depth reviews of operating systems like Windows and macOS. Andrew lives in Philadelphia and co-hosts a weekly book podcast called <a href="https://overduepodcast.com/">Overdue</a>.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/gadgets/2025/06/framework-laptop-12-review-im-excited-to-see-what-the-2nd-generation-looks-like/#comments" title="30 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    30 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/tech-policy/2025/06/trump-org-launches-47-month-wireless-service-teases-odd-499-phone/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2211177567-768x432.jpg" alt="Listing image for first story in Most Read: Mocked Trump Mobile yanks coverage map that ignored Trump renaming Gulf of Mexico" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>
  </main>





  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Homomorphically Encrypting CRDTs (232 pts)]]></title>
            <link>https://jakelazaroff.com/words/homomorphically-encrypted-crdts/</link>
            <guid>44309520</guid>
            <pubDate>Wed, 18 Jun 2025 12:59:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jakelazaroff.com/words/homomorphically-encrypted-crdts/">https://jakelazaroff.com/words/homomorphically-encrypted-crdts/</a>, See on <a href="https://news.ycombinator.com/item?id=44309520">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-content="" data-astro-cid-onuac4el=""> 



<p>Here’s a problem with local-first software.</p>
<p>You want to work on a document together with a friend who lives far away from you.
That sounds like local-first’s bread and butter: store the document as a CRDT, then use some sort of sync server to merge updates and relay them between you and your friend.</p>
<p>But there’s a catch: the contents of that document are secret.
So secret, in fact, that <em>you don’t even want the app developer to know what they are</em>.</p>
<p>One way to solve this is end-to-end encryption.
You and your friend agree on a secret key, known only to each other.
You each use that key to encrypt your changes before sending them, decrypt them upon receipt, and no one in the middle is able to listen in.
Because the document is a CRDT, you can each still get the latest document without the sync server merging the updates.</p>
<p>That is indeed a solution, and modern browser APIs make it fairly simple to implement a basic version of it. <a href="https://plus.excalidraw.com/blog/end-to-end-encryption" data-astro-cid-bi7aps5f="">Excalidraw’s writeup of their implementation</a><a data-tooltip="" href="https://plus.excalidraw.com/blog/end-to-end-encryption" data-astro-cid-bi7aps5f=""> <img src="https://excalidraw.nyc3.cdn.digitaloceanspaces.com/lp-cms/media/Excalidraw%20blog%20-%20End-to-End%20Encryption%20in%20the%20Browser-1.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">Excalidraw Blog | End-to-End Encryption in the Browser</span> <span data-astro-cid-bi7aps5f="">Excalidraw introduces browser-based end-to-end encryption using Web Cryptography APIs for secure, private drawing storage.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://plus.excalidraw.com/favicon.svg" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">plus.excalidraw.com/blog/end-to-end-encryption</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> is only about 750 words — including code samples!<sup><a href="#user-content-fn-e2ee" id="user-content-fnref-e2ee" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">1</a></sup></p>
<p>Unfortunately, we’ve introduced a new problem.</p>
<p>You and your friend live far away from each other, so you tend to work while they’re sleeping and vice versa.
That was fine when the sync server could merge your changes and send you the latest document when you opened it.</p>
<p>Now, however, the server can no longer understand the changes you send.
If you want to see your friend’s latest changes, you’ll need to both be online at the same time.</p>
<p>Enter <strong>homomorphic encryption</strong>: a special form of encryption that allows a computer to <em>run programs on encrypted data without decrypting it</em>.
Using a homomorphically encrypted CRDT, a sync server could merge your friend’s and your changes into one document without ever knowing what the document contains.<sup><a href="#user-content-fn-otherways" id="user-content-fnref-otherways" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">2</a></sup></p>
<p>In this article, we’ll explore how homomorphic encryption works and build a homomorphically encrypted last write wins register CRDT.
We’ll also learn about some fundamental limitations of homomorphic encryption, and how they affect local-first software specifically.</p>
<p>I try to assume as little knowledge as possible about both encryption and CRDTs.
If you want to brush up before continuing on, my <a href="https://jakelazaroff.com/words/an-interactive-intro-to-crdts/" data-astro-cid-bi7aps5f="">Interactive Intro to CRDTs</a><a data-tooltip="" href="https://jakelazaroff.com/words/an-interactive-intro-to-crdts/" data-astro-cid-bi7aps5f=""> <img src="https://jakelazaroff.com/og/an-interactive-intro-to-crdts.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">An Interactive Intro to CRDTs | jakelazaroff.com</span> <span data-astro-cid-bi7aps5f="">CRDTs don't have to be all academic papers and math jargon. Learn what CRDTs are and how they work through interactive visualizations and code samples.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://jakelazaroff.com/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">jakelazaroff.com/words/an-interactive-intro-to-crdts/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> and Jeremy Kun’s <a href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">A High-Level Technical Overview of Fully Homomorphic Encryption</a><a data-tooltip="" href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">A High-Level Technical Overview of Fully Homomorphic Encryption</span> <span data-astro-cid-bi7aps5f="">About two years ago, I switched teams at Google to focus on fully homomorphic encryption (abbreviated FHE, or sometimes HE). Since then I’ve got to work on a lot of interesting projects, learning along the way about post-quantum cryptography, compiler design, and the ins and outs of fully homomorphic encryption.
If you’ve heard about FHE and you’re a software person, you’ve probably heard two things: it lets you run programs directly on encrypted data without ever decrypting it; and it’s still too slow to be useful for anything.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://www.jeremykun.com/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">www.jeremykun.com/2024/05/04/fhe-overview/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> are good places to start.</p>
<p>(Obligatory disclaimer: I am not a cryptographer!
While I’m reasonably confident that my code and advice here is generally sound, cryptography is a field in which subtle bugs and exploits can look fine to the untrained eye.
Before using anything here in an environment you’d describe with the word “production”, consult someone who works on this professionally.)</p>
<h2 id="homomorphic-hello-world">Homomorphic Hello World</h2>
<p>First, let’s look at a small code sample that uses homomorphic encryption.</p>
<p>Writing the encryption code itself from scratch would take much more code than can fit in this article.
Instead, we’ll use <a href="https://github.com/zama-ai/tfhe-rs" data-astro-cid-bi7aps5f="">THFE-rs</a><a data-tooltip="" href="https://github.com/zama-ai/tfhe-rs" data-astro-cid-bi7aps5f=""> <img src="https://opengraph.githubassets.com/0076171220c59d2546a04eccf3e34abc6618c1b595c0711a3ea1f2f2d96312ad/zama-ai/tfhe-rs" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">GitHub - zama-ai/tfhe-rs: TFHE-rs: A Pure Rust implementation of the TFHE Scheme for Boolean and Integer Arithmetics Over Encrypted Data.</span> <span data-astro-cid-bi7aps5f="">TFHE-rs: A Pure Rust implementation of the TFHE Scheme for Boolean and Integer Arithmetics Over Encrypted Data. - zama-ai/tfhe-rs</span> <span data-astro-cid-bi7aps5f=""> <img src="https://github.githubassets.com/favicons/favicon.svg" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">github.com/zama-ai/tfhe-rs</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>, a homomorphic encryption library written in Rust.</p>
<p>The flow goes something like this:</p>
<ol>
<li>A client generates a key pair consisting of a client key and a server key.</li>
<li>The client encrypts their data using the client key and sends both the encrypted data and server key to the server.</li>
<li>The server uses the server key to perform some computation on the encrypted data and sends the result back to the client.</li>
<li>The client decrypts the result with the client key.</li>
</ol>
<p>Here’s what this looks like in code.
We’ll take two numbers — <code>clear_a</code> and <code>clear_b</code> — and add them together.
Rather than actually sending anything over a network, we’ll just use a function called <code>server_compute</code> to play the part of the server.</p>
<pre data-language="rust"><code is:raw=""><span>use</span> <span>tfhe<span>::</span>prelude<span>::</span></span><span>*</span><span>;</span>
<span>use</span> <span>tfhe<span>::</span></span><span>{</span>generate_keys<span>,</span> set_server_key<span>,</span> <span>ConfigBuilder</span><span>,</span> <span>FheUint32</span><span>,</span> <span>ServerKey</span><span>}</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> config <span>=</span> <span>ConfigBuilder</span><span>::</span><span>default</span><span>(</span><span>)</span><span>.</span><span>build</span><span>(</span><span>)</span><span>;</span>

    <span>// generate client and server keys</span>
    <span>let</span> <span>(</span>client_key<span>,</span> server_key<span>)</span> <span>=</span> <span>generate_keys</span><span>(</span>config<span>)</span><span>;</span>

    <span>// generate plaintext</span>
    <span>let</span> clear_a<span>:</span> <span>u32</span> <span>=</span> <span>27</span><span>;</span>
    <span>let</span> clear_b<span>:</span> <span>u32</span> <span>=</span> <span>128</span><span>;</span>

    <span>// encrypt plaintext and "send to server"</span>
    <span>let</span> result <span>=</span> <span>server_compute</span><span>(</span>
        server_key<span>,</span>
        <span>FheUint32</span><span>::</span><span>encrypt</span><span>(</span>clear_a<span>,</span> <span>&amp;</span>client_key<span>)</span><span>,</span>
        <span>FheUint32</span><span>::</span><span>encrypt</span><span>(</span>clear_b<span>,</span> <span>&amp;</span>client_key<span>)</span><span>,</span>
    <span>)</span><span>;</span>

    <span>// decrypt the result</span>
    <span>let</span> decrypted_result<span>:</span> <span>u32</span> <span>=</span> result<span>.</span><span>decrypt</span><span>(</span><span>&amp;</span>client_key<span>)</span><span>;</span>

    <span>// assert that the result is what we expect</span>
    <span>assert_eq!</span><span>(</span>decrypted_result<span>,</span> clear_a <span>+</span> clear_b<span>)</span><span>;</span>
<span>}</span>

<span>fn</span> <span>server_compute</span><span>(</span>key<span>:</span> <span>ServerKey</span><span>,</span> cipher_a<span>:</span> <span>FheUint32</span><span>,</span> cipher_b<span>:</span> <span>FheUint32</span><span>)</span> <span>-&gt;</span> <span>FheUint32</span> <span>{</span>
    <span>set_server_key</span><span>(</span>key<span>)</span><span>;</span>
    <span>return</span> cipher_a <span>+</span> cipher_b<span>;</span>
<span>}</span>
</code></pre>
<p>Get the keys, encrypt two numbers, add their ciphertexts together, decrypt the result.
Not too bad, right?</p>
<p>The simplicity is deceptive!
Rust supports operator overloading, so when we run <code>cipher_a + cipher_b</code> and both of the operands are <code>FheUint32</code>, what’s <em>really</em> happening is that TFHE-rs runs a bunch of cryptography code.</p>
<p>Before we build our homomorphically encrypted CRDT, let’s peek at what TFHE-rs is doing under the hood.</p>
<h2 id="under-the-hood">Under the Hood</h2>
<p>To start, what does it even mean to “run programs on encrypted data”?</p>
<p>In short, it means you can use encrypted data in certain math operations, and when you decrypt the data you get the result you would have gotten if you had performed the same operations with the plaintext data.
That requires an encryption scheme in which at least one of the following is true (I’ll use the notation <code>E(a)</code> to indicate the encrypted version of the plaintext <code>a</code>):</p>
<ul>
<li><code>E(a) + E(b) = E(a + b)</code>: adding the encrypted values of the plaintext numbers <code>a</code> and <code>b</code> results in the encrypted sum of the plaintext sum <code>a + b</code>.</li>
<li><code>E(a) × E(b) = E(a × b)</code>: multiplying the encrypted values of the plaintext numbers <code>a</code> and <code>b</code> results in the encrypted product of the plaintext product <code>a × b</code>.</li>
</ul>
<p>What this means is that <strong>if you add or multiply two homomorphically encrypted values, then decrypt them, <em>you get the respective sum or product of the original plaintext values</em></strong>.</p>
<p>Here’s an extremely simple example that you should absolutely never use anywhere.
First, let’s pick a number as a key.
We “encrypt” numbers by multiplying them by the key, and “decrypt” numbers by dividing them.</p>
<p>Let’s say our key is 7 and our “plaintext” numbers are 5 and 6.
We can multiply each number by our key 6 to get “encrypted” numbers of 35 and 42.
Even if someone has access to our encrypted numbers, they can’t figure out what our original plaintext numbers were without the key.</p>
<p>What they <em>can</em> do is add the encrypted numbers together.
If they give us back the sum, 77, we can divide it by our key 7 to get 11 — <em>the same result we’d get by directly adding our original numbers</em>.
Try it out by changing the numbers in the playground below:</p>
<homomorphic-addition-demo></homomorphic-addition-demo>
<p>Because it satisfies the first criterion — <code>E(a) + E(b) = E(a + b)</code> — we can say that our toy encryption scheme is homomorphic over addition.
Encryption that supports only one operation is called <em>partially homomorphic encryption</em>.
All in all, there are four different levels:</p>
<ul>
<li><strong>Partially homomorphic encryption</strong> allows only one of the two operations: <em>either</em> addition <em>or</em> multiplication, but not both.</li>
<li><strong>Somewhat homomorphic encryption</strong> and <strong>leveled homomorphic encryption</strong> allow both operations, but limit the amount of times they can be used.</li>
<li><strong>Fully homomorphic encryption</strong> allows an unlimited amount of both operations.</li>
</ul>
<p>Partially homomorphic encryption is relatively easy to implement, but has limited uses.
The word “relatively” is doing some heavy lifting here — you or I probably couldn’t come up with a partially homomorphic encryption scheme — but it’s simple enough that there are algorithms such as RSA that are accidentally homomorphic over one operation.</p>
<p>Supporting <em>more than one</em> operation is significantly more useful, but each calculation adds “noise” to the result.
Too much noise makes it impossible to decrypt.
There are two broad strategies for reducing noise: limiting the number or “depth” of operations (<em>somewhat</em> and <em>leveled</em> homomorphic encryption), and “bootstrapping”, which reduces the level of noise mid-computation (<em>fully</em> homomorphic encryption).</p>
<p>Why does it matter whether we can perform <em>both</em> addition and multiplication?</p>
<p>When we talk about doing math on encrypted data, we’re really talking about the underlying bits: the 1s and 0s that make it up.
To add and multiply the bits, we use the logical operations “exclusive or” (XOR) and “binary and” (AND), respectively.</p>
<p>Click on the switches in the playground below to toggle between 1 and 0.
You can see that the AND output is the product of its two inputs, and the XOR output is roughly the sum of its two inputs.<sup><a href="#user-content-fn-addition" id="user-content-fnref-addition" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">3</a></sup></p>
<logic-gates height="180"><switch-gate id="one-and-left" x="40" y="10"></switch-gate><switch-gate id="one-and-right" x="85" y="10"></switch-gate><and-gate id="one-and" x="55" y="70" left="#one-and-left" right="#one-and-right"></and-gate><output-gate x="63" y="140" center="#one-and"></output-gate><switch-gate id="one-xor-left" x="190" y="10"></switch-gate><switch-gate id="one-xor-right" x="235" y="10"></switch-gate><xor-gate id="one-xor" x="205" y="70" left="#one-xor-left" right="#one-xor-right"></xor-gate><output-gate x="213" y="140" center="#one-xor"></output-gate></logic-gates>
<p>This is called a <em>Boolean circuit</em> — essentially, a function that takes 1s and 0s as input and returns 1s and 0s as output.
In this context, the logical operations are called <em>logic gates</em>.</p>
<p>We can create new logic gates by combining ones we have.
Here’s how to create “inclusive or” (OR) and inverter (NOT) operations using only XOR and AND.</p>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 310"><rect x="1" y="71" width="143" height="230" stroke="currentColor" fill="none" rx="4" ry="4"></rect><foreignObject x="0" y="0" width="150" height="300"></foreignObject><rect x="156" y="71" width="143" height="230" stroke="currentColor" fill="none" rx="4" ry="4"></rect><foreignObject x="150" y="0" width="150" height="300"></foreignObject></svg>
<p>Once we’ve built a gate, we can then use it to build <em>yet other</em> gates.
Here’s how to make an “exclusive nor” (XNOR) using XOR and our newly-constructed NOT gate:</p>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 300 260"><rect x="61" y="1" width="143" height="250" stroke="currentColor" fill="none" rx="4" ry="4"></rect><foreignObject x="0" y="0" width="210" height="300"></foreignObject></svg>
<p>It turns out that combining just XOR and AND like this is enough to perform <em>any computation</em>!
All other logical operations can be created by combining only XOR and AND, which means that adding and multiplying the encrypted data is sufficient to simulate arbitrary Boolean logic.</p>
<p>Here’s a circuit that implements the “greater than” operator on two-bit numbers (between 0 and 3).
Using only AND, XOR and the other gates we’ve built with them, it returns 1 if the first number is greater than the second, and 0 otherwise.</p>
<p>Type in the square boxes at the top to enter the input numbers.
The two rounded boxes below each square input box are the <em>binary representation</em> of that number.</p>
<p>Don’t forget this circuit — it’ll come in handy later!</p>
<logic-gates height="365"><text-gate id="four-left" x="60"></text-gate><text-gate id="four-right" x="160"></text-gate><value-gate x="60" y="30" id="four-left-ms" center="#four-left" value-prop="leftValue"></value-gate><value-gate x="98" y="30" id="four-left-ls" center="#four-left" value-prop="rightValue"></value-gate><value-gate x="160" y="30" id="four-right-ms" center="#four-right" value-prop="leftValue"></value-gate><value-gate x="198" y="30" id="four-right-ls" center="#four-right" value-prop="rightValue"></value-gate><xnor-gate id="four-msbeq-1" x="25" y="110" left="#four-left-ms" right="#four-right-ms"></xnor-gate><not-gate id="four-msbgt-1" x="105" y="90" center="#four-right-ms"></not-gate><and-gate id="four-msbgt-2" x="105" y="150" left="#four-left-ms" right="#four-msbgt-1"></and-gate><not-gate id="four-lsbgt-1" x="200" y="90" center="#four-right-ls"></not-gate><and-gate id="four-lsbgt-2" x="200" y="150" left="#four-left-ls" right="#four-lsbgt-1"></and-gate><and-gate id="four-gt-1" x="140" y="220" left="#four-msbeq-1" right="#four-lsbgt-2"></and-gate><or-gate id="four-gt-2" x="112" y="280" left="#four-msbgt-2" right="#four-gt-1"></or-gate><output-gate x="120" y="330" center="#four-gt-2"></output-gate></logic-gates>
<p>In these examples, we’ve been looking at circuits that use plaintext 1s and 0s as their inputs and outputs.
With homomorphic encryption, the circuits operate on <em>encrypted data</em>.
Performing an AND on two encrypted bits returns another encrypted bit — and we can’t find out what it is unless we have the key.</p>
<p>So that’s how homomorphic encryption works in a nutshell.
You express your program as a Boolean circuit, and then simulate the circuit using the encrypted data as input.
The output of the circuit will be the encrypted result, which the client can then decrypt.</p>
<p>Crucially, <em>none of this reveals any sort of relationship between the plaintext values</em>.
For example, even if <code>E(a) + E(b)</code> were positive, <code>E(a + b)</code> might be negative.
Adding and multiplying ciphertext corresponds to the same operations on the underlying plaintext, but there’s no correlation between any of the ciphertext results and the underlying plaintext results — you need to decrypt the result to figure out what happened.</p>
<h2 id="a-fully-homomorphic-crdt">A Fully Homomorphic CRDT</h2>
<p>Now that we have a high level understanding of homomorphic encryption, let’s build a homomorphically-encrypted last write wins register.</p>
<p>A last write wins register holds a single value and two additional bits of metadata: a “clock” that gets incremented by one whenever the value is set, and an ID indicating the peer who last wrote to it.
Like all CRDTs, it also has a merge function that describes how it should be combined with another of the same type.</p>
<p>The last write wins register merge algorithm works like this:</p>
<ul>
<li>If the received clock is less than the local clock, the register doesn’t change its state.</li>
<li>If the received clock is greater than the local clock, the register overwrites its local value with the received value. It also stores the received clock and peer ID.</li>
<li>Ties are broken by comparing the local peer ID to the peer ID in the received state.</li>
</ul>
<p>Here’s a playground in which you can see how this algorithm works:</p>
<lwwregister-demo></lwwregister-demo>
<p>Try playing around with the latency and the network toggle.
See how updates are accepted only if the sending peer’s clock is higher than the receiving peer’s clock. If the clocks are tied, the update from the right peer will win out, since the peer ID <code>bob</code> is lexicographically greater than <code>alice</code>.</p>
<p>Okay, let’s look at some code.
First, here’s what an <em>unencrypted</em> last write wins register might look like in Rust:</p>
<pre data-language="rust"><code is:raw=""><span>const</span> <span>DATA_SIZE</span><span>:</span> <span>usize</span> <span>=</span> <span>16</span><span>;</span>

<span>pub</span> <span>struct</span> <span>Register</span> <span>{</span>
    <span>pub</span> peer<span>:</span> <span>u64</span><span>,</span>
    <span>pub</span> clock<span>:</span> <span>u64</span><span>,</span>
    <span>pub</span> value<span>:</span> <span>[</span><span>u8</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>,</span>
<span>}</span>

<span>impl</span> <span>Register</span> <span>{</span>
    <span>pub</span> <span>fn</span> <span>new</span><span>(</span>peer<span>:</span> <span>u64</span><span>)</span> <span>-&gt;</span> <span>Register</span> <span>{</span>
        <span>Register</span> <span>{</span>
            peer<span>,</span>
            clock<span>:</span> <span>0</span><span>,</span>
            value<span>:</span> <span>[</span><span>0</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>,</span>
        <span>}</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>set</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> peer<span>:</span> <span>u64</span><span>,</span> value<span>:</span> <span>[</span><span>u8</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>)</span> <span>{</span>
        <span>self</span><span>.</span>peer <span>=</span> peer<span>;</span>
        <span>self</span><span>.</span>clock <span>+=</span> <span>1</span><span>;</span>
        <span>self</span><span>.</span>value <span>=</span> value<span>;</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>set_string</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> peer<span>:</span> <span>u64</span><span>,</span> value<span>:</span> <span>&amp;</span><span>str</span><span>)</span> <span>{</span>
        <span>let</span> bytes <span>=</span> value<span>.</span><span>as_bytes</span><span>(</span><span>)</span><span>;</span>
        <span>let</span> len <span>=</span> bytes<span>.</span><span>len</span><span>(</span><span>)</span><span>.</span><span>min</span><span>(</span><span>DATA_SIZE</span><span>)</span><span>;</span>

        <span>let</span> <span>mut</span> data <span>=</span> <span>[</span><span>0</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>;</span>
        data<span>[</span><span>..</span>len<span>]</span><span>.</span><span>copy_from_slice</span><span>(</span><span>&amp;</span>bytes<span>[</span><span>..</span>len<span>]</span><span>)</span><span>;</span>

        <span>self</span><span>.</span><span>set</span><span>(</span>id<span>,</span> data<span>)</span><span>;</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>&amp;</span><span>Register</span><span>)</span> <span>{</span>
        <span>if</span> <span>self</span><span>.</span>clock <span>&gt;</span> other<span>.</span>clock <span>{</span>
            <span>return</span><span>;</span>
        <span>}</span><span>;</span>

        <span>if</span> <span>self</span><span>.</span>clock <span>==</span> other<span>.</span>clock <span>&amp;&amp;</span> <span>self</span><span>.</span>peer <span>&gt;</span> other<span>.</span>peer <span>{</span>
            <span>return</span><span>;</span>
        <span>}</span>

        <span>self</span><span>.</span>peer <span>=</span> other<span>.</span>peer<span>;</span>
        <span>self</span><span>.</span>clock <span>=</span> other<span>.</span>clock<span>;</span>
        <span>self</span><span>.</span>value <span>=</span> other<span>.</span>value<span>;</span>
    <span>}</span>
<span>}</span>
</code></pre>
<p>It has a peer ID, a clock and a value.
To merge with another register, it just takes the peer ID, clock and value from the register with the higher clock.
In case of a tie, it uses the peer ID as a tiebreaker.</p>
<p>Because Rust is a low-level language, we need separate functions to convert types such as strings into the raw bytes to store as the value.
We also store the value in an array with a statically-known size — although as we’ll see, that’s less of a Rust limitation than it is a fundamental constraint of homomorphic encryption.</p>
<p>Here’s the skeleton of an <code>EncryptedRegister</code> struct:</p>
<pre data-language="rust"><code is:raw=""><span>use</span> <span>core<span>::</span></span>array<span>;</span>
<span>use</span> <span>tfhe<span>::</span>prelude<span>::</span></span><span>*</span><span>;</span>
<span>use</span> <span>tfhe<span>::</span></span><span>{</span><span>ClientKey</span><span>,</span> <span>FheUint64</span><span>,</span> <span>FheUint8</span><span>}</span><span>;</span>

<span>use</span> <span>crate</span><span>::</span><span>Register</span><span>;</span>

<span>const</span> <span>DATA_SIZE</span><span>:</span> <span>usize</span> <span>=</span> <span>16</span><span>;</span>

<span>pub</span> <span>struct</span> <span>EncryptedRegister</span> <span>{</span>
    peer<span>:</span> <span>FheUint64</span><span>,</span>
    clock<span>:</span> <span>FheUint64</span><span>,</span>
    value<span>:</span> <span>[</span><span>FheUint8</span><span>;</span> <span>DATA_SIZE</span><span>]</span><span>,</span>
<span>}</span>

<span>impl</span> <span>EncryptedRegister</span> <span>{</span>
    <span>pub</span> <span>fn</span> <span>encrypt</span><span>(</span>clear<span>:</span> <span>&amp;</span><span>Register</span><span>,</span> key<span>:</span> <span>&amp;</span><span>ClientKey</span><span>)</span> <span>-&gt;</span> <span>EncryptedRegister</span> <span>{</span>
        <span>EncryptedRegister</span> <span>{</span>
            peer<span>:</span> <span>FheUint64</span><span>::</span><span>encrypt</span><span>(</span>clear<span>.</span>peer<span>,</span> key<span>)</span><span>,</span>
            clock<span>:</span> <span>FheUint64</span><span>::</span><span>encrypt</span><span>(</span>clear<span>.</span>clock<span>,</span> key<span>)</span><span>,</span>
            value<span>:</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> <span>FheUint8</span><span>::</span><span>encrypt</span><span>(</span>clear<span>.</span>value<span>[</span>i<span>]</span><span>,</span> key<span>)</span><span>)</span><span>,</span>
        <span>}</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> key<span>:</span> <span>&amp;</span><span>ClientKey</span><span>)</span> <span>-&gt;</span> <span>Register</span> <span>{</span>
        <span>Register</span> <span>{</span>
            peer<span>:</span> <span>FheUint64</span><span>::</span><span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>peer<span>,</span> key<span>)</span><span>,</span>
            clock<span>:</span> <span>FheUint64</span><span>::</span><span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>clock<span>,</span> key<span>)</span><span>,</span>
            value<span>:</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> <span>FheUint8</span><span>::</span><span>decrypt</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>value<span>[</span>i<span>]</span><span>,</span> key<span>)</span><span>)</span><span>,</span>
        <span>}</span>
    <span>}</span>

    <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>EncryptedRegister</span><span>)</span> <span>{</span>
        <span>// ...</span>
    <span>}</span>
<span>}</span>
</code></pre>
<p>Pretty similar to the unencrypted <code>Register</code> struct!
<code>FheUint64</code> has replaced <code>u64</code>, and <code>value</code> is now an array of <code>FheUint8</code> rather than <code>u8</code>.
These are TFHE-rs types that encrypt the corresponding Rust types.
But other than that, the struct is the same.</p>
<p>The implementation has two new methods:</p>
<ul>
<li><code>encrypt</code>, which takes a normal <code>Register</code> and a client key, encrypts all the fields and returns an <code>EncryptedRegister</code>.</li>
<li><code>decrypt</code>, which takes a client key, decrypts all the fields and returns a normal <code>Register</code>.</li>
</ul>
<p>We’ve also omitted the <code>set</code> and <code>set_string</code> methods.
Since <code>EncryptedRegister</code> runs on the server, the value will never be set manually.
The only thing it needs to do is merge an incoming register with the register it has in memory.</p>
<p>Okay, so what does the <code>merge</code> method look like?</p>
<p>As we saw before, TFHE-rs overloads operators like <code>+</code> to make working with encrypted values more convenient.
For operators that don’t support overloading such as <code>&lt;</code>, TFHE-rs has methods like <code>gt</code>.</p>
<p>Given that, you might think we could write the <code>merge</code> method like this:</p>
<pre data-language="rust"><code is:raw=""><span>impl</span> <span>EncryptedRegister</span> <span>{</span>
  <span>// ...</span>

  <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>EncryptedRegister</span><span>)</span> <span>{</span>
    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>{</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>&amp;&amp;</span> <span>self</span><span>.</span>id<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>id<span>)</span> <span>{</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>self</span><span>.</span>id <span>=</span> other<span>.</span>id<span>;</span>
    <span>self</span><span>.</span>clock <span>=</span> other<span>.</span>clock<span>;</span>
    <span>self</span><span>.</span>value <span>=</span> other<span>.</span>value<span>;</span>
  <span>}</span>
<span>}</span>
</code></pre>
<p>This will <em>definitely not work</em>!</p>
<p>Remember that we can’t retrieve any information by operating on the encrypted data — <em>including information about the results of intermediate steps</em>.</p>
<p>To more clearly show the problem with this strategy, we can add some logging:</p>
<pre data-language="rust"><code is:raw=""><span>impl</span> <span>EncryptedRegister</span> <span>{</span>
  <span>// ...</span>

  <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>EncryptedRegister</span><span>)</span> <span>{</span>
    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>{</span>
      <span>println!</span><span>(</span><span>"local clock is greater than other clock!"</span><span>)</span><span>;</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>if</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span> <span>&amp;&amp;</span> <span>self</span><span>.</span>peer<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>peer<span>)</span> <span>{</span>
      <span>println!</span><span>(</span><span>"clocks are equal but local peer is greater than other peer!"</span><span>)</span><span>;</span>
      <span>return</span><span>;</span>
    <span>}</span>

    <span>println!</span><span>(</span><span>"overwriting local data with remote data!"</span><span>)</span><span>;</span>
    <span>self</span><span>.</span>peer <span>=</span> other<span>.</span>peer<span>;</span>
    <span>self</span><span>.</span>clock <span>=</span> other<span>.</span>clock<span>;</span>
    <span>self</span><span>.</span>value <span>=</span> other<span>.</span>value<span>;</span>
  <span>}</span>
<span>}</span>
</code></pre>
<p>Although we still couldn’t decrypt the encrypted data, this (fake) implementation would reveal the result of the merge!
We’d know which branches our code took, and therefore learn which decrypted clock was higher and which encrypted data was written to the register.</p>
<p>Instead, our merge function must <em>eagerly</em> evaluate all branches in our code.
It also means that all loops must run for a statically-known number of iterations.
More generally, <strong>our code must always execute as though operating on the worst case input</strong>, because altering behavior based on the input would leak information about it.</p>
<p>Here’s the <em>real</em> code for our merge function:</p>
<pre data-language="rust"><code is:raw="">  <span>pub</span> <span>fn</span> <span>merge</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> other<span>:</span> <span>&amp;</span><span>EncryptedRegister</span><span>)</span> <span>{</span>
    <span>let</span> higher_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>

    <span>let</span> equal_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
    <span>let</span> higher_peer <span>=</span> <span>self</span><span>.</span>peer<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>

    <span>let</span> keep_self <span>=</span> higher_clock <span>|</span> <span>(</span>equal_clock <span>&amp;</span> higher_peer<span>)</span><span>;</span>

    <span>self</span><span>.</span>peer <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>peer<span>,</span> <span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>
    <span>self</span><span>.</span>clock <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>clock<span>,</span> <span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
    <span>self</span><span>.</span>value <span>=</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>value<span>[</span>i<span>]</span><span>,</span> <span>&amp;</span>other<span>.</span>value<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
  <span>}</span>
</code></pre>
<p>Superficially, it looks fairly similar, but there are a couple of important differences.
Let’s take it line by line.</p>
<p>First, we determine whether the local clock is higher than the other clock:</p>
<pre data-language="rust"><code is:raw=""><span>let</span> higher_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
</code></pre>
<p>If we think back to the logic gates, we can imagine what’s going on under the hood here, right?
We built this exact circuit!
Ours only operated on two-bit numbers, but the idea was the same: accept two numbers and return a 0 or 1 indicating whether the first number is higher than the second.</p>
<p>(In our circuit, the result was a <em>plaintext</em> 0 or 1 — but remember, homomorphic encryption operates with <em>encrypted</em> values!
The <code>gt</code> method actually returns an <code>FheBool</code>: an <em>encrypted bool</em> which indicates whether the local clock is higher than the other one.)</p>
<p>If we had the client key, we could decrypt that variable and find out its true value.
We can’t do that, but we can still <em>combine it with other encrypted values</em> to write our merge algorithm.</p>
<p>Here are the conditions to break a tie between the clocks:</p>
<pre data-language="rust"><code is:raw=""><span>let</span> equal_clock <span>=</span> <span>self</span><span>.</span>clock<span>.</span><span>eq</span><span>(</span><span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
<span>let</span> higher_peer <span>=</span> <span>self</span><span>.</span>peer<span>.</span><span>gt</span><span>(</span><span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>
</code></pre>
<p>Two more <code>FheBool</code>s indicating whether the clocks are equal and if the local peer ID is higher.</p>
<p>Next, we combine them:</p>
<pre data-language="rust"><code is:raw=""><span>let</span> keep_self <span>=</span> higher_clock <span>|</span> <span>(</span>equal_clock <span>&amp;</span> higher_peer<span>)</span><span>;</span>
</code></pre>
<p>This combines all those <code>FheBool</code>s to determine whether to keep the local data or overwrite it with the merged data.<sup><a href="#user-content-fn-astute" id="user-content-fnref-astute" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">4</a></sup></p>
<p>Those <code>|</code> and <code>&amp;</code> operators are bitwise AND and bitwise OR, which work exactly like the AND and OR logic gates we made earlier.
They’re similar to the logical AND and OR we’re used to — <code>&amp;&amp;</code> and <code>||</code>, but with one big difference: bitwise operators are <em>eager</em>.
Whereas logical AND and OR might skip the second expression depending on the first, bitwise operators will <em>always</em> evaluate both sides.</p>
<p>Now that we’ve determined the register values to keep — even if we can’t tell which ones — we need to write the data to the register.
Here’s the secret sauce:</p>
<pre data-language="rust"><code is:raw=""><span>self</span><span>.</span>peer <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>peer<span>,</span> <span>&amp;</span>other<span>.</span>peer<span>)</span><span>;</span>
<span>self</span><span>.</span>clock <span>=</span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>clock<span>,</span> <span>&amp;</span>other<span>.</span>clock<span>)</span><span>;</span>
<span>self</span><span>.</span>value <span>=</span> <span>array<span>::</span></span><span>from_fn</span><span>(</span><span><span>|</span>i<span>|</span></span> keep_self<span>.</span><span>select</span><span>(</span><span>&amp;</span><span>self</span><span>.</span>value<span>[</span>i<span>]</span><span>,</span> <span>&amp;</span>other<span>.</span>value<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</code></pre>
<p>Rather than <code>if</code> or <code>match</code> expressions, we use <code>FheBool</code>’s <code>select</code> method.
It returns the first argument if the underlying <code>FheBool</code> value is <code>true</code>, or the second argument if the underlying value is <code>false</code>.</p>
<p>This is important: <em>the return value is different from both arguments</em>.
While decrypting the return value would reveal the same plaintext as one of the arguments, in ciphertext all three are distinct.
This means that we can’t tell which values we’ve set on the register by the end of the merge.</p>
<p>When the merge is done, every piece of ciphertext has changed — the peer ID, the clock and the register value.
The plaintext values might have updated (or might not have!) but there’s no way to tell by looking at the ciphertext.</p>
<p>Problem solved, right?
We can now have the server merge our CRDT without knowing what it contains?
Weeeellllllll…</p>
<h2 id="fundamental-limitations">Fundamental Limitations</h2>
<p>Homomorphic encryption has constraints that sharply limit its effectiveness with regard to local-first software.</p>
<p>For starters: encryption keys.
In both the simple adding example and the last-write wins register, we generated a key that would be passed to the server.
That only needs to happen once, but the difference between the size of our key and the size of our data can be surprising.</p>
<p>Our register took up only 32 bytes of data — 8 bytes each for the peer and clock, and 16 bytes for the value.
Meanwhile, TFHE-rs generated a <em>123 megabyte</em> server key.
We can compress the key down to about 27 megabytes, but still: that’s almost 850,000 times more key than data!</p>
<p>The payload here is particularly small, but a disparity of that size isn’t unheard of.
In his <a href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">overview of fully homomorphic encryption</a><a data-tooltip="" href="https://www.jeremykun.com/2024/05/04/fhe-overview/" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">A High-Level Technical Overview of Fully Homomorphic Encryption</span> <span data-astro-cid-bi7aps5f="">About two years ago, I switched teams at Google to focus on fully homomorphic encryption (abbreviated FHE, or sometimes HE). Since then I’ve got to work on a lot of interesting projects, learning along the way about post-quantum cryptography, compiler design, and the ins and outs of fully homomorphic encryption.
If you’ve heard about FHE and you’re a software person, you’ve probably heard two things: it lets you run programs directly on encrypted data without ever decrypting it; and it’s still too slow to be useful for anything.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://www.jeremykun.com/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">www.jeremykun.com/2024/05/04/fhe-overview/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>, Jeremy Kun cites examples in which ciphertexts of dozens or hundreds of <em>kilobytes</em> require keys on the order of <em>gigabytes</em>.</p>
<p>Runtime performance is also — to put it lightly — lacking.
I benchmarked the unencrypted and encrypted versions of the last write wins register on an M4 MacBook Pro.
The unencrypted one averaged a merge time of 0.52 nanoseconds.</p>
<p>The encrypted one?
<em>1.06 seconds</em>.
That’s not a typo: the homomorphically encrypted merge is <em>two billion times slower</em>.<sup><a href="#user-content-fn-gpu" id="user-content-fnref-gpu" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">5</a></sup></p>
<p>Not great!</p>
<p>That’s not all.
We said before that our code must execute as though operating on the worst case input.
Even if the performance issues improve by many orders of magnitude, the “worst case” requirement will still impose constraints on the CRDT algorithm itself.</p>
<p>Consider a fully homomorphically encrypted last-write wins <em>map</em> CRDT.
Most maps store keys sparsely, so the map only grows in size as keys are added.</p>
<p>Here’s a playground that simulates encrypting a sparse map.<sup><a href="#user-content-fn-pretend" id="user-content-fnref-pretend" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">6</a></sup>
When you modify the plaintext map on the left, the encrypted map on the right updates.
Can you see a security issue?</p>
<encrypted-map-demo></encrypted-map-demo>
<p>Imagine you only had access to the map on the right.
You could still see data being added and removed!
Furthermore, this map lazily encrypts only the data that changes, which would allow you to see exactly which key changed (if any).</p>
<p>A homomorphically encrypted map CRDT couldn’t do that.
Since it must assume a worst-case input, it must store the keys <em>densely</em>: limiting the size to a fixed number of keys and reserving all the space up front.
Merging two identical maps would be exactly as computationally intensive as merging two maps in which <em>every</em> key was updated.<sup><a href="#user-content-fn-op" id="user-content-fnref-op" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">7</a></sup></p>
<p>The playground below simulates a homomorphically encrypted map.
While you can add and remove keys to the plaintext map on the left, the encrypted map on the right behaves as though every key is filled.
And no matter how you modify the plaintext map, <em>everything</em> in the encrypted map changes:</p>
<encrypted-map-demo dense="true" keys="4"></encrypted-map-demo>
<p>From the outside, there’s no way to tell what changed in the map: we see the exact same number of keys, and every value has changed.
To calculate the new map, the server must go through and merge <em>every single key</em>.
After that, it needs to transfer the full map to each peer — because remember, as far as it knows, the entire map is different.</p>
<p>These are fundamental limitations of homomorphic encryption!
The requirement that homomorphically encrypted code performs as though operating on the worst-case input dramatically increases both the space and time required to update.</p>
<h2 id="parting-thoughts">Parting Thoughts</h2>
<p>I started this article thinking that local-first software and homomorphic encryption would be natural bedfellows.</p>
<p>But honestly, I came away… a little less enamored.
The fundamental limitations of homomorphic encryption mean that it will always operate under a set of worst-case assumptions.
Homomorphically encrypted CRDTs aren’t intractable, but they are severely limited by these intrinsic constraints.</p>
<p>So the question remains: how can we secure local-first apps without severely degrading usability?</p>
<p>Luckily, I’m not the only one thinking about this problem!</p>

<p>CRDTs are a relatively young technology — the paper formalizing them was published in 2011 — so there’s still a lot of unexplored solution space.
We may not have solved this problem yet, but I’m confident that we’re closing in on it!</p>

<section data-footnotes="">
<ol>
<li id="user-content-fn-e2ee">
<p>More comprehensive solutions might try to implement things like <a href="https://en.wikipedia.org/wiki/Forward_secrecy" data-astro-cid-bi7aps5f="">forward secrecy</a><a data-tooltip="" href="https://en.wikipedia.org/wiki/Forward_secrecy" data-astro-cid-bi7aps5f=""> <img src="https://upload.wikimedia.org/wikipedia/commons/e/e0/KDF_chain.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">Forward secrecy - Wikipedia</span>  <span data-astro-cid-bi7aps5f=""> <img src="https://en.wikipedia.org/static/favicon/wikipedia.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">en.wikipedia.org/wiki/Forward_secrecy</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>, which is absolutely not easy.
But the basic version is still better than nothing. <a href="#user-content-fnref-e2ee" data-footnote-backref="" aria-label="Back to reference 1" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-otherways">
<p>Homomorphic encryption isn’t the only way to solve this problem.
You might instead just ignore it, and have the server store every version of your encrypted document — wasteful, but it’d work!
You could also use a CRDT implementation that only requires <em>changes</em> to be sent to the server, rather than the full document. <a href="#user-content-fnref-otherways" data-footnote-backref="" aria-label="Back to reference 2" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-addition">
<p>You might notice that when both XOR inputs are 1, the result is 0.
You might also remember from math class that the result of 1 + 1 is, uh, not 0.
So how can XOR represent addition?</p>
<p>Remember that we’re operating on binary numbers — all we have is 0 and 1!
Adding to 1 in binary is like adding to 9 in decimal: since we’re out of digits, we instead roll that place back to 0 and <em>carry the 1 to the next place</em>, giving us 10.
The XOR gate represents the <em>sum digit</em> of that Boolean addition.
To fully represent the result, <a href="https://en.wikipedia.org/wiki/XOR_gate#Addition" data-astro-cid-bi7aps5f="">we’d need to use the AND gate as well to represent the <em>carry digit</em></a><a data-tooltip="" href="https://en.wikipedia.org/wiki/XOR_gate#Addition" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">XOR gate - Wikipedia</span>  <span data-astro-cid-bi7aps5f=""> <img src="https://en.wikipedia.org/static/favicon/wikipedia.ico#Addition" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">en.wikipedia.org/wiki/XOR_gate#Addition</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a>. <a href="#user-content-fnref-addition" data-footnote-backref="" aria-label="Back to reference 3" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-astute">
<p><em>Truly</em> eagle-eyed readers will notice that this is <em>also</em> the same circuit we used to determine whether one two-bit number was greater than another.
At a high level, the logic there was “most significant bit is greater” OR (“most significant bits are equal” AND “least significant bit is greater”).
Here, the logic is “clock is greater” OR (“clocks are equal” AND “peer ID is greater”). <a href="#user-content-fnref-astute" data-footnote-backref="" aria-label="Back to reference 4" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-gpu">
<p>Granted, I was only able to run it on the CPU — TFHE-rs only supports GPU acceleration on Linux — but even if I could run it on the GPU, <a href="https://docs.zama.ai/tfhe-rs/get-started/summary" data-astro-cid-bi7aps5f="">TFHE-rs’s benchmarks</a><a data-tooltip="" href="https://docs.zama.ai/tfhe-rs/get-started/summary" data-astro-cid-bi7aps5f="">  <span data-astro-cid-bi7aps5f="">TFHE-rs</span>  <span data-astro-cid-bi7aps5f=""> <img src="https://docs.zama.ai/~gitbook/image?url=https%3A%2F%2F572209210-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fcollections%252FprREL84Xd1lx94uAslRx%252Ficon%252F3R1LaM67E4BE3WhJZF5p%252FLogo%2520-%2520Square.png%3Falt%3Dmedia%26token%3Db39ed5d3-5537-4c62-9389-5b23f830072b&amp;width=48&amp;height=48&amp;sign=c9d8fc58&amp;sv=2" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">docs.zama.ai/tfhe-rs/get-started/summary</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> indicate that it would only speed things up by a factor of 3–5.
Even at the high end of that range, the encrypted merge would <em>still</em> be 400 million times slower than the unencrypted one. <a href="#user-content-fnref-gpu" data-footnote-backref="" aria-label="Back to reference 5" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-pretend">
<p>When I say “simulates encrypting”, I mean “displays a bunch of random hex digits”.
Please humor me! <a href="#user-content-fnref-pretend" data-footnote-backref="" aria-label="Back to reference 6" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-op">
<p>Note that this all assumes a <em>state-based</em> CRDT.
An <em>operation-based</em> CRDT — where the important operation is appending to a log of events rather than merging — might have a totally different set of tradeoffs. <a href="#user-content-fnref-op" data-footnote-backref="" aria-label="Back to reference 7" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-meri">
<p>Meri Leeworthy published a great <a href="https://meri.garden/a-deep-dive-explainer-on-beekem-protocol/" data-astro-cid-bi7aps5f="">deep-dive explainer on KeyHive’s key encapsulation mechanism</a><a data-tooltip="" href="https://meri.garden/a-deep-dive-explainer-on-beekem-protocol/" data-astro-cid-bi7aps5f=""> <img src="https://static.meri.garden/mesh-gradient.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">A deep-dive explainer on Ink and Switch's BeeKEM protocol</span> <span data-astro-cid-bi7aps5f="">I'm a programmer, designer, writer and artist. I try to make tools for community autonomy, creativity, and resistance.</span> <span data-astro-cid-bi7aps5f=""> <img src="https://meri.garden/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""> <span data-astro-cid-bi7aps5f="">meri.garden/a-deep-dive-explainer-on-beekem-protocol/</span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"> <use href="/icons.svg#share"></use> </svg> </span> </a> which is absolutely worth a read! <a href="#user-content-fnref-meri" data-footnote-backref="" aria-label="Back to reference 8" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
</ol>
</section> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Workout.cool – Open-source fitness coaching platform (678 pts)]]></title>
            <link>https://github.com/Snouzy/workout-cool</link>
            <guid>44309320</guid>
            <pubDate>Wed, 18 Jun 2025 12:33:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Snouzy/workout-cool">https://github.com/Snouzy/workout-cool</a>, See on <a href="https://news.ycombinator.com/item?id=44309320">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#about">About</a></li>
<li><a href="#-project-origin--motivation">Project Origin &amp; Motivation</a></li>
<li><a href="#quick-start">Quick Start</a></li>
<li><a href="#exercise-database-import">Exercise Database Import</a></li>
<li><a href="#project-architecture">Project Architecture</a></li>
<li><a href="#roadmap">Roadmap</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#deployment">Deployment</a></li>
<li><a href="#resources">Resources</a></li>
<li><a href="#license">License</a></li>
<li><a href="#-sponsor-this-project">Sponsor This Project</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributors</h2><a id="user-content-contributors" aria-label="Permalink: Contributors" href="#contributors"></a></p>
<a href="https://github.com/Snouzy/workout-cool/graphs/contributors">
  <img src="https://camo.githubusercontent.com/7c851db85ae8783e22cf0dabfd6e6cce46c6c360fed141a0f287fa87d478c4f7/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d536e6f757a792f776f726b6f75742d636f6f6c" data-canonical-src="https://contrib.rocks/image?repo=Snouzy/workout-cool">
</a>
<p dir="auto"><h2 tabindex="-1" dir="auto">About</h2><a id="user-content-about" aria-label="Permalink: About" href="#about"></a></p>
<p dir="auto">A comprehensive fitness coaching platform that allows create workout plans for you, track progress, and access a vast exercise database with
detailed instructions and video demonstrations.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🎯 Project Origin &amp; Motivation</h2><a id="user-content--project-origin--motivation" aria-label="Permalink: 🎯 Project Origin &amp; Motivation" href="#-project-origin--motivation"></a></p>
<p dir="auto">This project was born from a personal mission to revive and improve upon a previous fitness platform. As the <strong>primary contributor</strong> to the
original <a href="https://github.com/workout-lol/workout-lol">workout.lol</a> project, I witnessed its journey and abandonment. 🥹</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Story Behind <strong><em>workout.cool</em></strong></h3><a id="user-content-the-story-behind-workoutcool" aria-label="Permalink: The Story Behind workout.cool" href="#the-story-behind-workoutcool"></a></p>
<ul dir="auto">
<li>🏗️ <strong>Original Contributor</strong>: I was the main contributor to workout.lol</li>
<li>💼 <strong>Business Challenges</strong>: The original project faced major hurdles with exercise video partnerships (no reliable video provider) could
be established</li>
<li>💰 <strong>Project Sale</strong>: Due to these partnership issues, the project was sold to another party</li>
<li>📉 <strong>Abandonment</strong>: The new owner quickly realized that <strong>exercise video licensing costs were prohibitively expensive</strong>, began to be sick
and abandoned the entire project</li>
<li>🔄 <strong>Revival Attempts</strong>: For the past <strong>9 months</strong>, I've been trying to reconnect with the new stakeholder</li>
<li>📧 <strong>Radio Silence</strong>: Despite multiple (15) attempts, there has been no response</li>
<li>🚀 <strong>New Beginning</strong>: Rather than let this valuable work disappear, I decided to create a fresh, modern implementation</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why <strong><em>workout.cool</em></strong> Exists</h3><a id="user-content-why-workoutcool-exists" aria-label="Permalink: Why workout.cool Exists" href="#why-workoutcool-exists"></a></p>
<p dir="auto"><strong>Someone had to step up.</strong></p>
<p dir="auto">The opensource fitness community deserves better than broken promises and abandoned platforms.</p>
<p dir="auto">I'm not building this for profit.</p>
<p dir="auto">This isn't just a revival : it's an evolution. <strong>workout.cool</strong> represents everything the original project could have been, with the
reliability, modern approach, and <strong>maintenance</strong> that the fitness open source community deserves.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">👥 From the Community, For the Community</h2><a id="user-content--from-the-community-for-the-community" aria-label="Permalink: 👥 From the Community, For the Community" href="#-from-the-community-for-the-community"></a></p>
<p dir="auto"><strong>I'm not just a developer : I'm a user who refused to let our community down.</strong></p>
<p dir="auto">I experienced firsthand the frustration of watching a beloved tool slowly disappear. Like many of you, I had workouts saved, progress
tracked, and a routine built around the platform.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">My Mission: Rescue &amp; Revive.</h3><a id="user-content-my-mission-rescue--revive" aria-label="Permalink: My Mission: Rescue &amp; Revive." href="#my-mission-rescue--revive"></a></p>
<p dir="auto"><em>If you were part of the original workout.lol community, welcome back! If you're new here, welcome to the future of fitness platform
management.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites</h3><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li>Node.js 18+</li>
<li>Either:
<ul dir="auto">
<li>Docker</li>
<li>OR PostgreSQL external database</li>
</ul>
</li>
<li>pnpm (recommended) or npm</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Clone the repository</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Snouzy/workout-cool.git
cd workout-cool"><pre>git clone https://github.com/Snouzy/workout-cool.git
<span>cd</span> workout-cool</pre></div>
</li>
<li>
<p dir="auto"><strong>Install dependencies</strong></p>

</li>
<li>
<p dir="auto"><strong>Set up environment variables</strong></p>

<p dir="auto">Fill in your database URL and other required environment variables:</p>
<div dir="auto" data-snippet-clipboard-copy-content="DATABASE_URL=&quot;postgresql://username:password@localhost:5432/workout_cool&quot;
BETTER_AUTH_SECRET=&quot;your-secret-key&quot;
# ... other variables"><pre><span>DATABASE_URL</span><span>=</span><span><span>"</span>postgresql://username:password@localhost:5432/workout_cool<span>"</span></span>
<span>BETTER_AUTH_SECRET</span><span>=</span><span><span>"</span>your-secret-key<span>"</span></span>
<span><span>#</span> ... other variables</span></pre></div>
</li>
<li>
<p dir="auto"><strong>Set up the database</strong></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Option 1: Using Docker</h4><a id="user-content-option-1-using-docker" aria-label="Permalink: Option 1: Using Docker" href="#option-1-using-docker"></a></p>
<p dir="auto">The project provides a convenient <code>make</code> command that handles everything:</p>

<p dir="auto">This single command will:</p>
<ul dir="auto">
<li>Start the PostgreSQL database using Docker</li>
<li>Run database migrations</li>
<li>Start the development server</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Option 2: Manual PostgreSQL Setup</h4><a id="user-content-option-2-manual-postgresql-setup" aria-label="Permalink: Option 2: Manual PostgreSQL Setup" href="#option-2-manual-postgresql-setup"></a></p>
<p dir="auto">If you prefer to use your own PostgreSQL installation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run migrations
npx prisma migrate deploy
npx prisma generate

# Start the development server
pnpm dev"><pre><span><span>#</span> Run migrations</span>
npx prisma migrate deploy
npx prisma generate

<span><span>#</span> Start the development server</span>
pnpm dev</pre></div>
</li>
<li>
<p dir="auto"><strong>Open your browser</strong> Navigate to <a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a></p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Exercise Database Import</h2><a id="user-content-exercise-database-import" aria-label="Permalink: Exercise Database Import" href="#exercise-database-import"></a></p>
<p dir="auto">The project includes a comprehensive exercise database. To import a sample of exercises:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites for Import</h3><a id="user-content-prerequisites-for-import" aria-label="Permalink: Prerequisites for Import" href="#prerequisites-for-import"></a></p>
<ol dir="auto">
<li><strong>Prepare your CSV file</strong></li>
</ol>
<p dir="auto">Your CSV should have these columns:</p>
<div data-snippet-clipboard-copy-content="id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value"><pre><code>id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value
</code></pre></div>
<p dir="auto">You can use the provided example.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Import Commands</h3><a id="user-content-import-commands" aria-label="Permalink: Import Commands" href="#import-commands"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Import exercises from a CSV file
pnpm run import:exercises-full /path/to/your/exercises.csv

# Example with the provided sample data
pnpm run import:exercises-full ./data/sample-exercises.csv"><pre><span><span>#</span> Import exercises from a CSV file</span>
pnpm run import:exercises-full /path/to/your/exercises.csv

<span><span>#</span> Example with the provided sample data</span>
pnpm run import:exercises-full ./data/sample-exercises.csv</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">CSV Format Example</h3><a id="user-content-csv-format-example" aria-label="Permalink: CSV Format Example" href="#csv-format-example"></a></p>
<div data-snippet-clipboard-copy-content="id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value
157,&quot;Fentes arrières à la barre&quot;,&quot;Barbell Reverse Lunges&quot;,&quot;<p>Stand upright...</p>&quot;,&quot;<p>Stand upright...</p>&quot;,https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,TYPE,STRENGTH
157,&quot;Fentes arrières à la barre&quot;,&quot;Barbell Reverse Lunges&quot;,&quot;<p>Stand upright...</p>&quot;,&quot;<p>Stand upright...</p>&quot;,https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,PRIMARY_MUSCLE,QUADRICEPS"><pre lang="csv"><code>id,name,name_en,description,description_en,full_video_url,full_video_image_url,introduction,introduction_en,slug,slug_en,attribute_name,attribute_value
157,"Fentes arrières à la barre","Barbell Reverse Lunges","&lt;p&gt;Stand upright...&lt;/p&gt;","&lt;p&gt;Stand upright...&lt;/p&gt;",https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,TYPE,STRENGTH
157,"Fentes arrières à la barre","Barbell Reverse Lunges","&lt;p&gt;Stand upright...&lt;/p&gt;","&lt;p&gt;Stand upright...&lt;/p&gt;",https://youtube.com/...,https://img.youtube.com/...,slug-fr,slug-en,PRIMARY_MUSCLE,QUADRICEPS
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Available Attribute Types</h3><a id="user-content-available-attribute-types" aria-label="Permalink: Available Attribute Types" href="#available-attribute-types"></a></p>
<ul dir="auto">
<li><strong>TYPE</strong>: <code>STRENGTH</code>, <code>CARDIO</code>, <code>PLYOMETRICS</code>, <code>STRETCHING</code>, etc.</li>
<li><strong>PRIMARY_MUSCLE</strong>: <code>QUADRICEPS</code>, <code>CHEST</code>, <code>BACK</code>, <code>SHOULDERS</code>, etc.</li>
<li><strong>SECONDARY_MUSCLE</strong>: Secondary muscle groups targeted</li>
<li><strong>EQUIPMENT</strong>: <code>BARBELL</code>, <code>DUMBBELL</code>, <code>BODYWEIGHT</code>, <code>MACHINE</code>, etc.</li>
<li><strong>MECHANICS_TYPE</strong>: <code>COMPOUND</code>, <code>ISOLATION</code></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Project Architecture</h2><a id="user-content-project-architecture" aria-label="Permalink: Project Architecture" href="#project-architecture"></a></p>
<p dir="auto">This project follows <strong>Feature-Sliced Design (FSD)</strong> principles with Next.js App Router:</p>
<div data-snippet-clipboard-copy-content="src/
├── app/ # Next.js pages, routes and layouts
├── processes/ # Business flows (multi-feature)
├── widgets/ # Composable UI with logic (Sidebar, Header)
├── features/ # Business units (auth, exercise-management)
├── entities/ # Domain entities (user, exercise, workout)
├── shared/ # Shared code (UI, lib, config, types)
└── styles/ # Global CSS, themes"><pre><code>src/
├── app/ # Next.js pages, routes and layouts
├── processes/ # Business flows (multi-feature)
├── widgets/ # Composable UI with logic (Sidebar, Header)
├── features/ # Business units (auth, exercise-management)
├── entities/ # Domain entities (user, exercise, workout)
├── shared/ # Shared code (UI, lib, config, types)
└── styles/ # Global CSS, themes
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Architecture Principles</h3><a id="user-content-architecture-principles" aria-label="Permalink: Architecture Principles" href="#architecture-principles"></a></p>
<ul dir="auto">
<li><strong>Feature-driven</strong>: Each feature is independent and reusable</li>
<li><strong>Clear domain isolation</strong>: <code>shared</code> → <code>entities</code> → <code>features</code> → <code>widgets</code> → <code>app</code></li>
<li><strong>Consistency</strong>: Between business logic, UI, and data layers</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example Feature Structure</h3><a id="user-content-example-feature-structure" aria-label="Permalink: Example Feature Structure" href="#example-feature-structure"></a></p>
<div data-snippet-clipboard-copy-content="features/
└── exercise-management/
├── ui/ # UI components (ExerciseForm, ExerciseCard)
├── model/ # Hooks, state management (useExercises)
├── lib/ # Utilities (exercise-helpers)
└── api/ # Server actions or API calls"><pre><code>features/
└── exercise-management/
├── ui/ # UI components (ExerciseForm, ExerciseCard)
├── model/ # Hooks, state management (useExercises)
├── lib/ # Utilities (exercise-helpers)
└── api/ # Server actions or API calls
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<p dir="auto">Here are the next steps and goals for Workout.cool:</p>
<ul>
<li> 🏋️‍♂️ Add new exercises and videos</li>
<li> 📱 Mobile app (React Native)</li>
<li> 🏆 Badges and gamification system</li>
<li> 📊 Advanced progress statistics</li>
<li> 🤝 Integration with wearables (watches, trackers)</li>
<li> 🌍 Multilingual support</li>
<li> 🔒 OAuth authentication (Google, Apple, etc.)</li>
<li> 💬 Built-in community forum</li>
</ul>
<p dir="auto">Feel free to suggest your ideas via <a href="https://github.com/Snouzy/workout-cool/issues">issues</a>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We welcome contributions! Please see our <a href="https://github.com/Snouzy/workout-cool/blob/main/CONTRIBUTING.md">Contributing Guide</a> for details.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Development Workflow</h3><a id="user-content-development-workflow" aria-label="Permalink: Development Workflow" href="#development-workflow"></a></p>
<ol dir="auto">
<li>Fork the repository</li>
<li>Create a feature branch (<code>git checkout -b feature/amazing-feature</code>)</li>
<li>Make your changes</li>
<li>Commit your changes (<code>git commit -m 'feat: add amazing feature'</code>)</li>
<li>Push to the branch (<code>git push origin feature/amazing-feature</code>)</li>
<li>Open a Pull Request</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Code Style</h3><a id="user-content-code-style" aria-label="Permalink: Code Style" href="#code-style"></a></p>
<ul dir="auto">
<li>Follow TypeScript best practices</li>
<li>Use Feature-Sliced Design architecture</li>
<li>Write meaningful commit messages</li>
<li>Add tests for new features</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deployment</h2><a id="user-content-deployment" aria-label="Permalink: Deployment" href="#deployment"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using Docker (Not ready yet : todo)</h3><a id="user-content-using-docker-not-ready-yet--todo" aria-label="Permalink: Using Docker (Not ready yet : todo)" href="#using-docker-not-ready-yet--todo"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build the Docker image
docker build -t workout-cool .

# Run the container
docker run -p 3000:3000 workout-cool"><pre><span><span>#</span> Build the Docker image</span>
docker build -t workout-cool <span>.</span>

<span><span>#</span> Run the container</span>
docker run -p 3000:3000 workout-cool</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Manual Deployment</h3><a id="user-content-manual-deployment" aria-label="Permalink: Manual Deployment" href="#manual-deployment"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build the application
pnpm build

# Run database migrations
export DATABASE_URL=&quot;your-production-db-url&quot;
npx prisma migrate deploy

# Start the production server
pnpm start"><pre><span><span>#</span> Build the application</span>
pnpm build

<span><span>#</span> Run database migrations</span>
<span>export</span> DATABASE_URL=<span><span>"</span>your-production-db-url<span>"</span></span>
npx prisma migrate deploy

<span><span>#</span> Start the production server</span>
pnpm start</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Resources</h2><a id="user-content-resources" aria-label="Permalink: Resources" href="#resources"></a></p>
<ul dir="auto">
<li><a href="https://feature-sliced.design/" rel="nofollow">Feature-Sliced Design</a></li>
<li><a href="https://nextjs.org/docs" rel="nofollow">Next.js Documentation</a></li>
<li><a href="https://www.prisma.io/docs/" rel="nofollow">Prisma Documentation</a></li>
<li><a href="https://github.com/better-auth/better-auth">Better Auth</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License. See the <a href="https://github.com/Snouzy/workout-cool/blob/main/LICENSE">LICENSE</a> file for details.</p>
<p dir="auto"><a href="https://github.com/Snouzy/workout-cool/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/28f4d479bf0a9b033b3a3b95ab2adc343da448a025b01aefdc0fbc7f0e169eb8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d677265656e2e737667" alt="MIT License" data-canonical-src="https://img.shields.io/badge/License-MIT-green.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🤝 Join the Rescue Mission</h2><a id="user-content--join-the-rescue-mission" aria-label="Permalink: 🤝 Join the Rescue Mission" href="#-join-the-rescue-mission"></a></p>
<p dir="auto"><strong>This is about rebuilding what we lost, together.</strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How You Can Help</h3><a id="user-content-how-you-can-help" aria-label="Permalink: How You Can Help" href="#how-you-can-help"></a></p>
<ul dir="auto">
<li>🌟 <strong>Star this repo</strong> to show the world our community is alive and thriving</li>
<li>🐛 <strong>Report issues</strong> you find. I'm listening to every single one</li>
<li>💡 <strong>Share your feature requests</strong> finally, someone who will actually implement them !</li>
<li>🔄 <strong>Spread the word</strong> to fellow fitness enthusiasts who lost hope</li>
<li>🤝 <strong>Contribute code</strong> if you're a developer : let's build this together</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">💖 Sponsor This Project</h2><a id="user-content--sponsor-this-project" aria-label="Permalink: 💖 Sponsor This Project" href="#-sponsor-this-project"></a></p>
<p dir="auto">Appear in the README and on the website as supporter by donating:</p>
<p><a href="https://ko-fi.com/workoutcool" rel="nofollow">
    <img src="https://camo.githubusercontent.com/70e2ef5e0263b261f9a2a314bb1d6919d1d43292eed117fe8fc766a68c7d96ea/68747470733a2f2f6b6f2d66692e636f6d2f696d672f676974687562627574746f6e5f736d2e737667" alt="Sponsor on Ko-fi" data-canonical-src="https://ko-fi.com/img/githubbutton_sm.svg">
  </a>
  &nbsp;&nbsp;&nbsp;
  
  
</p>
<p dir="auto">
  <em>If you believe in open-source fitness tools and want to help this project thrive,<br>
  consider buying me a coffee ☕ or sponsoring the continued development.</em>
</p>
<p dir="auto">
  Your support helps cover hosting costs, exercise database updates, and continuous improvement.<br>
  Thank you for keeping <strong>workout.cool</strong> alive and evolving 💪
</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is There a Half-Life for the Success Rates of AI Agents? (221 pts)]]></title>
            <link>https://www.tobyord.com/writing/half-life</link>
            <guid>44308711</guid>
            <pubDate>Wed, 18 Jun 2025 10:53:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tobyord.com/writing/half-life">https://www.tobyord.com/writing/half-life</a>, See on <a href="https://news.ycombinator.com/item?id=44308711">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
          <div data-collection-id="605a132f7a36283cfa0c4af9" data-edit-main-image="Banner">
            <p><h2>Writing</h2></p>
            
          </div>
            <section data-content-field="main-content" data-collection-id="605a132f7a36283cfa0c4af9" data-edit-main-image="Banner">  
  <article id="post-681b2734c79dc21300b23894" data-item-id="681b2734c79dc21300b23894">
    
    <!--POST TILE-->

    

    

    <!--MAIN CONTENT-->

    <div data-layout-label="Post Body" data-type="item" data-updated-on="1746610169593" id="item-681b2734c79dc21300b23894"><div data-block-type="2" id="block-84469d78d64003385a03">
  <p>Building on the recent empirical work of Kwa et al. (2025), I show that within their suite of research-engineering tasks the performance of AI agents on longer-duration tasks can be explained by an extremely simple mathematical model — a constant rate of failing during each minute a human would take to do the task. This implies an exponentially declining success rate with the length of the task and that each agent could be characterised by its own half-life. This empirical regularity allows us to estimate the success rate for an agent at different task lengths. And the fact that this model is a good fit for the data is suggestive of the underlying causes of failure on longer tasks — that they involve increasingly large sets of subtasks where failing any one fails the task. Whether this model applies more generally on other suites of tasks is unknown and an important subject for further work.</p><p><strong>METR’s results on the length of tasks agents can reliably complete</strong></p><p>A recent paper by <a href="https://doi.org/10.48550/arXiv.2503.14499"><span>Kwa et al. (2025)</span></a> from the research organisation <a href="https://metr.org/" target="_blank">METR</a> has found an exponential trend in the duration of the tasks that frontier AI agents can solve: every 7 months, the length of task they can solve doubles.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_8461">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png" data-image-dimensions="1926x1070" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png" width="1926" height="1070" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/3ac5cc96-1407-48a0-80e4-ab8a226e3d6c/Figure+1.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_8778">

<p>These headline results are based on&nbsp; a test suite of 170 software engineering, cybersecurity, general reasoning, and ML tasks that they assembled to be indicative of the kinds of tasks that could help AI agents assist in AI research. These tasks are assembled from three different benchmarks that take different amounts of time for humans to achieve:</p>




















  
  



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_11913">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png" data-image-dimensions="1820x1028" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png" width="1820" height="1028" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/0eed5814-5ddb-49a9-9ed1-2052bcc794f4/Figure+2.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_12229">
  <p>In general, ability to perform a task drops off as its duration increases, so they use the AI agent’s performance on tasks of different lengths to estimate the task-length at which the model would have a 50% success rate. They then showed that this length has been doubling every 7 months as the capabilities of frontier agents improve. The task-lengths are measured by how long it took humans to solve the same tasks.</p><p>They used 50% success rate as their chief performance threshold because it is the easiest level to robustly estimate. They are well aware that for many tasks, the required success rate for useful work may be much higher — such as 80%, 99%, or 99.9999%. They do measure the 80% success rate and find their mean estimate to have a doubling time of 213 days, compared to 212 days of the 50% rate. These are close to identical within their margin of error (±40 days for the 50% rate), so they conclude that the particular threshold doesn’t seem to have much effect on their headline result about the rate of improvement.</p><p>But there is quite a gap between the 50% success rate time-horizon and the 80% success rate time horizon. For the best model (Claude 3.7 Sonnet) it could achieve a 50% success rate on tasks up to 59 minutes <em>vs</em> only 15 minutes if an 80% success rate was required. If those results generalise to the other models, then we could also see it like this: the task length for an 80% success rate is 1/4 the task length for a 50% success rate. Or in terms of improvement: what is doable with a 50% success rate now is doable with an 80% success rate in 14 months’ time (= 2 doubling times).</p><p>The idea of measuring improvement in AI capabilities over time via time horizons at a chosen success rate is novel and interesting. AI forecasting is often hamstrung by the lack of a good measure for the y-axis of performance over time. We can track progress within a particular benchmark, but these are often solved in a couple of years, and we lack a good measure of underlying capability that can span multiple benchmarks. METR’s measure allows comparisons between very different kinds of tasks in a common currency (time it takes a human) and shows a strikingly clear trend line — suggesting it is measuring something real.</p><p>But there are grounds for criticism too. In particular, there is room to wonder how much these results generalise outside of this kind of task suite. We know that there are some tasks humans can do very quickly that AIs can’t solve (e.g. some simple spatial reasoning or intuitive physics tasks) and others that would take humans an extremely long time, but AIs can do quickly (e.g. rote mathematics). So a simple measure of ‘time it would take a human’ cannot explain all AI capability improvements. Kwa et al. are&nbsp; aware of this and even list several other ways that this task-suite may be non-representative of real-world performance including:</p><ul data-rte-list="default"><li><p>All tasks were automatically scorable (a domain where RL works best)</p></li><li><p>No interaction with other agents</p></li><li><p>Lax resource constraints</p></li></ul><p>For the purposes of this essay, we will take the data for what it is (performance on a particular task suite that may or may not generalise further) and explore underlying mechanisms that could explain it.</p><h3><strong>Explaining these results via a constant hazard rate</strong></h3><p>These results call out for some explanation of what is going on. For example, exactly how does the time horizon shrink as the required success probability is increased? And what does it <em>mean</em> for an agent to be able to perform an 8-hour task, but not a 16-hour task. Isn’t a 16-hour task just one 8-hour task after another?</p><p><a href="https://en.wikipedia.org/wiki/Survival_analysis" target="_blank">Survival analysis</a> is the field of understanding how the probability of something failing increases as a function of time. It tracks the survival probability at a time <em>S</em>(<em>t</em>) — that is, the chance it still hasn’t failed by that point. The simplest model in survival analysis is a constant hazard rate. This means that the chance of something failing in the next step (conditional on making it that far) is constant. A constant hazard rate leads to an exponentially declining survival curve. This behaviour is well-known from phenomena like radioactive decay, where there is a constant chance of decay at any moment, leading to an exponentially declining chance of the isotope’s survival over time, which is often measured by a half-life.</p><p>If AI agent success-rates drop off with task length in this manner, then the 50% success rate time-horizon for each agent from Kwa et al. is precisely the <em>half-life</em> of that agent. As with the half-life of a radioisotope, this isn’t just the median lifespan, it is the median remaining lifespan starting at any time — something that is only possible for an exponential survival curve. Unlike for particles, this AI agent half-life would be measured not in clock time, but in how long it takes a human to complete the task.</p><p>One rationale for this constant hazard rate model for AI agents is that tasks require getting past a series of steps each of which could end your attempt, with the longer the duration of the task, the more such steps. More precisely, if tasks could be broken down into a long sequence of equal-length subtasks with a constant (and independent) chance of failure, such that to succeed in the whole task, the agent needs to succeed in <em>all</em> subtasks, then that would create an exponential survival curve. I.e. when Pr(<em>Task</em>) = Pr(<em>Subtask</em>$_1$ &amp; <em>Substask</em>$_2$ &amp; … &amp; <em>Subtask</em>$_N$).</p><p>But we don’t have to assume a perfect breakdown of the task into equal-length-equal-difficulty subtasks in order to get an exponential distribution (and corresponding constant hazard rate). We can also think of the constant hazard rate model as being agnostic as to how the task is broken down, just saying that the chance of succeeding in a subtask of duration <em>t</em> is always equal to the chance of succeeding in each of a set of smaller subtasks whose combined duration is also <em>t</em>. So on this model, it doesn’t matter what level of granularity you assess the task at — whether you see it as one 60-minute task, six 10-minute tasks, or a 20-minute task plus forty 1-minute tasks — the chance of succeeding is set by the total time it would take a human to complete it.</p><p>This constant hazard rate model would predict that the time horizon for an 80% success rate is about ⅓ of the time horizon for a 50% success rate. This is because the chance of surviving three periods with an 80% success rate = (0.8)$^3$ = 0.512 ≈ 50%. More precisely, the time horizon for a success probability of <em>p</em> would be ln(<em>p</em>)/ln(<em>q</em>) times as long as one with success probability <em>q</em>. So an 80% time-horizon would be ln(0.8)/ln(0.5) = 0.322 times as long as the 50% time-horizon. Kwa et al. estimate the 80% time-horizon for Claude 3.7 Sonnet to be 0.25 as long, which is close to this theoretical estimate and within the margin of error given the noisiness of the results. The following chart shows how these numbers relate to the exponential survival curve (where <em>T</em>$_{50}$ is the 50% time-horizon etc.).</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_24919">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png" data-image-dimensions="2427x1731" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png" width="2427" height="1731" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/298e3bea-1b1d-49f8-a665-8832aea191af/Figure+3.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_25236">
  <p>Here are some useful comparisons for how the predicted time horizons over which an agent could get very high success rates compare to the measured time horizon for a 50% success rate:</p><p><em>&nbsp; T</em>$_{80}$ ≈ 1/3&nbsp;<em>T</em>$_{50}$<br>&nbsp;&nbsp;<em>T</em>$_{90}$ ≈ 1/7&nbsp;<em>T</em>$_{50}$<br>&nbsp;&nbsp;<em>T</em>$_{99}$ ≈ 1/70 <em>T</em>$_{50}$<br>&nbsp;&nbsp;<em>T</em>$_{99.9}$ ≈ 1/700 <em>T</em>$_{50}$<em><br>&nbsp; [and each additional ‘nine’ of reliability beyond this divides the time horizon by 10]</em></p><p>We can also use this model to calculate how long we’d expect it to take between the 50% success rate time horizon reaching a given length and a high success rate time horizon reaching that some length (on the assumption of the 7-month doubling time and the constant hazard rate model): </p><p><em>&nbsp; T</em>$_{80}$ reaches any particular length about 1 year after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>T</em>$_{90}$ reaches any particular length about 2 years after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>T</em>$_{99}$ reaches any particular length about 4 years after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>T</em>$_{99.9}$ reaches any particular length about 6 years after <em>T</em>$_{50}$ does<br>&nbsp;&nbsp;<em>[each additional ‘nine’ of reliability requires 2 more years…]</em></p><p>Kwa et al. also attempt to fit a relationship between the success probability and time-horizon (see the figure below, which I’ve adapted from their paper). They plot the time horizon on a log scale and note that this reveals a sigmoid-shaped decay curve of success rate (the coloured bars). They show that this is reasonably well fit by a logistic function (the black curves). The paper doesn’t compare this to how well alternative functions would fit the data.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_26796">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png" data-image-dimensions="2503x1718" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png" width="2503" height="1718" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/d641d697-240b-4c0f-8676-c649e477d235/Figure+4.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_27113">
  <p>The data is also well-fit by an exponential function, which also looks like a sigmoid when plotted on this logarithmic x-axis. I’ve added this to the above figure (from Kwa et al.) in the form of the dotted blue curves.&nbsp; It fits the data better for some models (such as the top left) and worse on others. It fits the data roughly as well overall, while being substantially more likely <em>a priori</em> — exponential decay is <em>the </em>simplest survival curve and has only one free parameter instead of two. Moreover, while logistic functions are quite simple and natural, the black curve here is not really a logistic distribution, but the more complex log-logistic distribution which merely looks like a logistic distribution when plotted on a logarithmic x-axis.</p><p>The paper also plots the survival curve for <em>human</em> performance over increasing time periods:</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1746614245989_28357">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png" data-image="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png" data-image-dimensions="2148x760" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png" width="2148" height="760" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/9c39bdb4-ea1d-441d-ae07-d13e80c36369/Figure+5.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1746614245989_28674">
  <p>Intriguingly, this human survival curve seems to be noticeably better than a constant hazard rate (i.e. the chance of succeeding over long timescales drops off more slowly for humans than the constant hazard rate predicts). For example, on this graph, humans had about a 50% success rate at the 1.5-hour mark, which suggests 25% at 3 hours, 12.5% at 6 hours and 6.25% at 12 hours if the hazard rate is constant. However, the humans were still above 20% success rate at that point.&nbsp;</p><p>This could indicate a different scaling behaviour of success rate with time horizon for humans compared to AI agents, which would be well worth investigating and may suggest important underlying mechanisms (e.g. that the humans were better at correcting earlier failed subtasks). If human performance scales differently with task length than AI agent performance, that would be an important result, suggesting that there is a notable inefficiency in the current AI paradigm. This warrants further research.&nbsp;</p><p>However, there are other potential explanations. For instance, it could also be an artefact of this graph being an aggregate of humans with different ability levels, since even if all individual humans have a constant hazard rate (and so each have an exponential survival curve) a mixture of different humans would be a weighted sum of exponentials with different time constants and that distribution decays slower than an exponential (see <a href="https://arxiv.org/abs/2308.09045"><span>Ord (2023)</span></a> for details).</p><p>The AI agents don’t suffer from mixing different capability levels into the same statistics because there is a separate data series for each agent. However, there is a similar effect that could still be present in the measurement of AI agents’ performance over increasingly long tasks. It is plausible that some tasks are inherently easier than others per unit time, corresponding to different hazard rates. If so, then the survival curve over the whole task suite would be averaging different exponential decay curves together. This produces an aggregate decay curve with thicker tails than an exponential (corresponding to a declining effective hazard rate). Again, see <a href="https://arxiv.org/abs/2308.09045"><span>Ord (2023)</span></a> for details. Dealing with this effect is important as it could still be the case that the mechanism of constant hazard rate (and what it implies about the agents’ behaviour) holds for every task, even if it isn’t visible in the aggregate of these tasks.</p><h3><strong>Upshots of the constant hazard rate model</strong></h3><p>If the constant hazard rate model is sufficient to explain the drop-off in success rates on a task suite, there are several interesting upshots:</p><ul data-rte-list="default"><li><p>It allows us to make predictions for the time-horizons at other success rates, such as 90% and 99%.</p></li></ul><ul data-rte-list="default"><li><p>It allows us to make predictions for how the success rate improves over time for a fixed task-length.</p></li></ul><ul data-rte-list="default"><li><p>It provides simple rules of thumb for predicting success probabilities (e.g. that if you double the task duration, you square the success probability).</p></li></ul><ul data-rte-list="default"><li><p>It suggests that AI agent performance (at least on this task suite) can be characterised by a half-life.</p></li></ul><ul data-rte-list="default"><li><p>It provides indirect evidence that what really is going on under the hood is that tasks are made up of many sequential subtasks and the chance of succeeding at the whole requires succeeding at every individual component. Moreover, this suggests that the current AI agents are not very good at recovering from earlier mistakes.</p></li></ul><ul data-rte-list="default"><li><p>Because the exponential distribution is the unique memoryless distribution, another way of seeing it is that the chance of failing at the next moment is independent of how far you’ve come — just like how the chance of a radioisotope decaying in the next minute is independent on how many minutes it has survived so far. This would be a surprising and interesting property for reasoning agents.</p></li></ul><ul data-rte-list="default"><li><p>Deviations from exponential decay for certain models may provide evidence that their hazard rate is increasing (or decreasing) with time, which might provide hints as to what they are doing wrong (or right).</p></li></ul><ul data-rte-list="default"><li><p>It can explain the same 7-month doubling time for the 50% time-horizon and 80% time-horizon: a 7-month halving-time for the underlying hazard rate would produce the 7-month doubling-time of all such time horizons.</p></li></ul><ul data-rte-list="default"><li><p>It also helps conceptually explain what the time horizons could even mean. For example, if it can complete a day’s work, why can’t it just do that twice to produce two days of work? On the constant hazard rate model, the issue is that if has a 50% chance of succeeding on Monday’s work, then it only has a 25% chance of succeeding in both Monday’s and Tuesday’s work, which is too low to count as reliably achieving the 2-day task (and similarly for any higher reliability threshold).</p></li></ul><p>Note that I am not claiming AI agents have a precisely constant rate of failure per minute of time it would take a human to complete the task. The claim is instead that something like this appears to be roughly true or stochastically true. All other models imply that there is some systematic change in the hazard rate over time, and my suggestion (pending more information about the precise fits of different models) is that the data doesn’t warrant such assumptions.</p><p>If systematic deviations from exponential decay are found, such as the hazard rate increasing (or decreasing) with time, this might provide useful hints as to what the agents are doing wrong (or right). i.e. the constant hazard rate model can also work as a theoretical baseline from which to measure empirical deviations.</p><p>Also, note that <em>whatever</em> the survival curve is, if you lower the hazard rate by a factor of <em>k</em> at all times, the time horizons for all success rate levels also increase by a factor of <em>k</em>  (because the entire survival curve is  being stretched horizontally by a factor of <em>k</em>). Moreover, this is bidirectional — the only way to increase the time horizons at all success-rate levels by the same factor is to lower hazard rate by that factor. So to the extent that the METR evidence suggests the time horizons at every success rate are doubling at the same rate, it is also suggesting that the agent improvements are taking the form of reducing the hazard rate across all times by the same factor (i.e.. halving it every 7 months). This is intriguing and would be true whether or not there is a constant hazard rate for each model..</p><h3><strong>Further work</strong></h3><p>So far these results and analysis are merely suggestive of a constant hazard rate. Ideally one would conduct a formal statistical analysis on how well an exponential decay curve fits METRs data compared to the log-logistic they use.&nbsp;It would also be good to run robust statistical comparisons of the human decay curves versus the AI agent decay curves to see if there are systematic differences (e.g. different half-lives or different shapes that aren’t just an artefact of the experimental setup). And of course it is also important to know how much any of this generalises to other suites of tasks.</p><h3><strong>References</strong></h3><p>Thomas Kwa et al., <a href="https://arxiv.org/abs/2503.14499"><span>Measuring AI Ability to Complete Long Tasks</span></a>, arXiv:2503.14499 [cs.AI], 2025.</p><p>Toby Ord, <a href="https://arxiv.org/abs/2308.09045"><span>The Lindy Effect</span></a>. arXiv:2308.09045 [physics.soc-ph], 2023.</p>
</div></div>

    <!--BLOG INJECTION-->

    <!-- MathJax: Reset equation counter after every blog entry -->
$\setCounter{0}$    

    <!--CATEGORIES-->

    

  </article>
</section>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Terpstra Keyboard (219 pts)]]></title>
            <link>http://terpstrakeyboard.com/web-app/keys.htm</link>
            <guid>44308558</guid>
            <pubDate>Wed, 18 Jun 2025 10:31:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://terpstrakeyboard.com/web-app/keys.htm">http://terpstrakeyboard.com/web-app/keys.htm</a>, See on <a href="https://news.ycombinator.com/item?id=44308558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="landing-page">
		<div>
		  
		  <p><img alt="" src="http://terpstrakeyboard.com/web-app/1x1.png">
          </p>

			<form id="settingsForm">
				<div>
					<div>
						<p><label>Tuning\Layout Quick Links</label>
                          

                          <label>Fundamental (Hz)</label>
                          
                       </p>

						<p><label>Right Facing Steps</label>
                          

                          <label>Up/Right Facing Steps</label>
                          
						</p>
                    </div>

					<div>
						<p><label>Hex Size (pixels)</label>
                          

                          <label>Rotation (degrees)</label>
                          
						</p>

						<p><label>Instrument</label>
                          

                          <label>
                              
                              <span>Enumerate Scale</span>
                          </label>

                          <label>
                              
                              <span>Use Spectrum Colors</span>
                          </label>

                          <label>
                              
                              <span>Blank Keys (No labels)</span>
                          </label>
						</p>
					</div>
				</div>

                <p><img alt="" src="http://terpstrakeyboard.com/web-app/1x1.png">
                </p>

				<div>
					<p><label>Scale (<a href="http://www.huygens-fokker.org/scala/scl_format.html" target="new">Scala format</a>)</label>
						
                  </p>

                  <div>
                    <p><label id="numberLabel">Steps To Equivalence Interval</label>
                      

                      
                      
					</p>

                    <p>
                        

                        <label id="note_colorsLabel">Color Layout</label>
                        
                    </p>
                  </div>
				</div>
				<br>
				
            </form>
        </div>

		<p>
          Designed by <a href="http://siementerpstra.com/" target="new">Siemen Terpstra</a> in the late ’80’s. WebApp developed by <a href="http://jamesfenn.com/" target="new">James Fenn</a> with additions and modifications by <a href="http://brandlew.com/" target="new">Brandon Lewis</a>, <a href="http://whatmusicreallyis.com/" title="What Music Really İs" target="new">Bo Constantinsen</a> and <a href="https://sites.google.com/site/wangchengu/" target="new">Chengu Wang</a>. Credits to Scott Thompson and <a href="http://ozanyarman.com/" target="new">Dr Ozan Yarman</a> for contributing samples. Current version 1.5.2 (Jan. 2015 — May 2019), released as Free/Libre and Open Source Software under <a href="https://www.gnu.org/licenses/gpl-3.0.en.html" target="new">GPL-3.0</a>. Download, fork, and get your name down here by fixing issues and implementing features via <a href="https://github.com/wcgbg/terpstrakeyboard/" target="new">GitHub</a>!
		</p>
	</div></div>]]></description>
        </item>
    </channel>
</rss>