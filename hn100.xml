<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 12 Sep 2023 05:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[YouTube-dl fork with additional features and fixes (165 pts)]]></title>
            <link>https://github.com/yt-dlp/yt-dlp</link>
            <guid>37474066</guid>
            <pubDate>Mon, 11 Sep 2023 21:37:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/yt-dlp/yt-dlp">https://github.com/yt-dlp/yt-dlp</a>, See on <a href="https://news.ycombinator.com/item?id=37474066">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">


<p dir="auto">yt-dlp is a <a href="https://github.com/ytdl-org/youtube-dl">youtube-dl</a> fork based on the now inactive <a href="https://github.com/blackjack4494/yt-dlc">youtube-dlc</a>. The main focus of this project is adding new features and patches while also keeping up to date with the original project</p>


<ul dir="auto">
<li><a href="#new-features">NEW FEATURES</a>
<ul dir="auto">
<li><a href="#differences-in-default-behavior">Differences in default behavior</a></li>
</ul>
</li>
<li><a href="#installation">INSTALLATION</a>
<ul dir="auto">
<li><a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation">Detailed instructions</a></li>
<li><a href="#update">Update</a></li>
<li><a href="#release-files">Release Files</a></li>
<li><a href="#dependencies">Dependencies</a></li>
<li><a href="#compile">Compile</a></li>
</ul>
</li>
<li><a href="#usage-and-options">USAGE AND OPTIONS</a>
<ul dir="auto">
<li><a href="#general-options">General Options</a></li>
<li><a href="#network-options">Network Options</a></li>
<li><a href="#geo-restriction">Geo-restriction</a></li>
<li><a href="#video-selection">Video Selection</a></li>
<li><a href="#download-options">Download Options</a></li>
<li><a href="#filesystem-options">Filesystem Options</a></li>
<li><a href="#thumbnail-options">Thumbnail Options</a></li>
<li><a href="#internet-shortcut-options">Internet Shortcut Options</a></li>
<li><a href="#verbosity-and-simulation-options">Verbosity and Simulation Options</a></li>
<li><a href="#workarounds">Workarounds</a></li>
<li><a href="#video-format-options">Video Format Options</a></li>
<li><a href="#subtitle-options">Subtitle Options</a></li>
<li><a href="#authentication-options">Authentication Options</a></li>
<li><a href="#post-processing-options">Post-processing Options</a></li>
<li><a href="#sponsorblock-options">SponsorBlock Options</a></li>
<li><a href="#extractor-options">Extractor Options</a></li>
</ul>
</li>
<li><a href="#configuration">CONFIGURATION</a>
<ul dir="auto">
<li><a href="#configuration-file-encoding">Configuration file encoding</a></li>
<li><a href="#authentication-with-netrc">Authentication with netrc</a></li>
<li><a href="#notes-about-environment-variables">Notes about environment variables</a></li>
</ul>
</li>
<li><a href="#output-template">OUTPUT TEMPLATE</a>
<ul dir="auto">
<li><a href="#output-template-examples">Output template examples</a></li>
</ul>
</li>
<li><a href="#format-selection">FORMAT SELECTION</a>
<ul dir="auto">
<li><a href="#filtering-formats">Filtering Formats</a></li>
<li><a href="#sorting-formats">Sorting Formats</a></li>
<li><a href="#format-selection-examples">Format Selection examples</a></li>
</ul>
</li>
<li><a href="#modifying-metadata">MODIFYING METADATA</a>
<ul dir="auto">
<li><a href="#modifying-metadata-examples">Modifying metadata examples</a></li>
</ul>
</li>
<li><a href="#extractor-arguments">EXTRACTOR ARGUMENTS</a></li>
<li><a href="#plugins">PLUGINS</a>
<ul dir="auto">
<li><a href="#installing-plugins">Installing Plugins</a></li>
<li><a href="#developing-plugins">Developing Plugins</a></li>
</ul>
</li>
<li><a href="#embedding-yt-dlp">EMBEDDING YT-DLP</a>
<ul dir="auto">
<li><a href="#embedding-examples">Embedding examples</a></li>
</ul>
</li>
<li><a href="#deprecated-options">DEPRECATED OPTIONS</a></li>
<li><a href="https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#contributing-to-yt-dlp">CONTRIBUTING</a>
<ul dir="auto">
<li><a href="https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#opening-an-issue">Opening an Issue</a></li>
<li><a href="https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions">Developer Instructions</a></li>
</ul>
</li>
<li><a href="https://github.com/yt-dlp/yt-dlp/wiki">WIKI</a>
<ul dir="auto">
<li><a href="https://github.com/yt-dlp/yt-dlp/wiki/FAQ">FAQ</a></li>
</ul>
</li>
</ul>

<h2 tabindex="-1" dir="auto">NEW FEATURES</h2>
<ul dir="auto">
<li>
<p dir="auto">Forked from <a href="https://github.com/blackjack4494/yt-dlc/commit/f9401f2a91987068139c5f757b12fc711d4c0cee"><strong>yt-dlc@f9401f2</strong></a> and merged with <a href="https://github.com/ytdl-org/youtube-dl/commit/07af47960f3bb262ead02490ce65c8c45c01741e"><strong>youtube-dl@42f2d4</strong></a> (<a href="https://github.com/yt-dlp/yt-dlp/issues/21" data-hovercard-type="issue" data-hovercard-url="/yt-dlp/yt-dlp/issues/21/hovercard">exceptions</a>)</p>
</li>
<li>
<p dir="auto"><strong><a href="#sponsorblock-options">SponsorBlock Integration</a></strong>: You can mark/remove sponsor sections in YouTube videos by utilizing the <a href="https://sponsor.ajay.app/" rel="nofollow">SponsorBlock</a> API</p>
</li>
<li>
<p dir="auto"><strong><a href="#sorting-formats">Format Sorting</a></strong>: The default format sorting options have been changed so that higher resolution and better codecs will be now preferred instead of simply using larger bitrate. Furthermore, you can now specify the sort order using <code>-S</code>. This allows for much easier format selection than what is possible by simply using <code>--format</code> (<a href="#format-selection-examples">examples</a>)</p>
</li>
<li>
<p dir="auto"><strong>Merged with animelover1984/youtube-dl</strong>: You get most of the features and improvements from <a href="https://github.com/animelover1984/youtube-dl">animelover1984/youtube-dl</a> including <code>--write-comments</code>, <code>BiliBiliSearch</code>, <code>BilibiliChannel</code>, Embedding thumbnail in mp4/ogg/opus, playlist infojson etc. Note that NicoNico livestreams are not available. See <a href="https://github.com/yt-dlp/yt-dlp/pull/31" data-hovercard-type="pull_request" data-hovercard-url="/yt-dlp/yt-dlp/pull/31/hovercard">#31</a> for details.</p>
</li>
<li>
<p dir="auto"><strong>YouTube improvements</strong>:</p>
<ul dir="auto">
<li>Supports Clips, Stories (<code>ytstories:&lt;channel UCID&gt;</code>), Search (including filters)<strong>*</strong>, YouTube Music Search, Channel-specific search, Search prefixes (<code>ytsearch:</code>, <code>ytsearchdate:</code>)<strong>*</strong>, Mixes, and Feeds (<code>:ytfav</code>, <code>:ytwatchlater</code>, <code>:ytsubs</code>, <code>:ythistory</code>, <code>:ytrec</code>, <code>:ytnotif</code>)</li>
<li>Fix for <a href="https://github.com/ytdl-org/youtube-dl/issues/29326" data-hovercard-type="issue" data-hovercard-url="/ytdl-org/youtube-dl/issues/29326/hovercard">n-sig based throttling</a> <strong>*</strong></li>
<li>Supports some (but not all) age-gated content without cookies</li>
<li>Download livestreams from the start using <code>--live-from-start</code> (<em>experimental</em>)</li>
<li><code>255kbps</code> audio is extracted (if available) from YouTube Music when premium cookies are given</li>
<li>Channel URLs download all uploads of the channel, including shorts and live</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Cookies from browser</strong>: Cookies can be automatically extracted from all major web browsers using <code>--cookies-from-browser BROWSER[+KEYRING][:PROFILE][::CONTAINER]</code></p>
</li>
<li>
<p dir="auto"><strong>Download time range</strong>: Videos can be downloaded partially based on either timestamps or chapters using <code>--download-sections</code></p>
</li>
<li>
<p dir="auto"><strong>Split video by chapters</strong>: Videos can be split into multiple files based on chapters using <code>--split-chapters</code></p>
</li>
<li>
<p dir="auto"><strong>Multi-threaded fragment downloads</strong>: Download multiple fragments of m3u8/mpd videos in parallel. Use <code>--concurrent-fragments</code> (<code>-N</code>) option to set the number of threads used</p>
</li>
<li>
<p dir="auto"><strong>Aria2c with HLS/DASH</strong>: You can use <code>aria2c</code> as the external downloader for DASH(mpd) and HLS(m3u8) formats</p>
</li>
<li>
<p dir="auto"><strong>New and fixed extractors</strong>: Many new extractors have been added and a lot of existing ones have been fixed. See the <a href="https://github.com/yt-dlp/yt-dlp/blob/master/Changelog.md">changelog</a> or the <a href="https://github.com/yt-dlp/yt-dlp/blob/master/supportedsites.md">list of supported sites</a></p>
</li>
<li>
<p dir="auto"><strong>New MSOs</strong>: Philo, Spectrum, SlingTV, Cablevision, RCN etc.</p>
</li>
<li>
<p dir="auto"><strong>Subtitle extraction from manifests</strong>: Subtitles can be extracted from streaming media manifests. See <a href="https://github.com/yt-dlp/yt-dlp/commit/be6202f12b97858b9d716e608394b51065d0419f">commit/be6202f</a> for details</p>
</li>
<li>
<p dir="auto"><strong>Multiple paths and output templates</strong>: You can give different <a href="#output-template">output templates</a> and download paths for different types of files. You can also set a temporary path where intermediary files are downloaded to using <code>--paths</code> (<code>-P</code>)</p>
</li>
<li>
<p dir="auto"><strong>Portable Configuration</strong>: Configuration files are automatically loaded from the home and root directories. See <a href="#configuration">CONFIGURATION</a> for details</p>
</li>
<li>
<p dir="auto"><strong>Output template improvements</strong>: Output templates can now have date-time formatting, numeric offsets, object traversal etc. See <a href="#output-template">output template</a> for details. Even more advanced operations can also be done with the help of <code>--parse-metadata</code> and <code>--replace-in-metadata</code></p>
</li>
<li>
<p dir="auto"><strong>Other new options</strong>: Many new options have been added such as <code>--alias</code>, <code>--print</code>, <code>--concat-playlist</code>, <code>--wait-for-video</code>, <code>--retry-sleep</code>, <code>--sleep-requests</code>, <code>--convert-thumbnails</code>, <code>--force-download-archive</code>, <code>--force-overwrites</code>, <code>--break-match-filter</code> etc</p>
</li>
<li>
<p dir="auto"><strong>Improvements</strong>: Regex and other operators in <code>--format</code>/<code>--match-filter</code>, multiple <code>--postprocessor-args</code> and <code>--downloader-args</code>, faster archive checking, more <a href="#format-selection">format selection options</a>, merge multi-video/audio, multiple <code>--config-locations</code>, <code>--exec</code> at different stages, etc</p>
</li>
<li>
<p dir="auto"><strong>Plugins</strong>: Extractors and PostProcessors can be loaded from an external file. See <a href="#plugins">plugins</a> for details</p>
</li>
<li>
<p dir="auto"><strong>Self updater</strong>: The releases can be updated using <code>yt-dlp -U</code>, and downgraded using <code>--update-to</code> if required</p>
</li>
<li>
<p dir="auto"><strong>Nightly builds</strong>: <a href="#update-channels">Automated nightly builds</a> can be used with <code>--update-to nightly</code></p>
</li>
</ul>
<p dir="auto">See <a href="https://github.com/yt-dlp/yt-dlp/blob/master/Changelog.md">changelog</a> or <a href="https://github.com/yt-dlp/yt-dlp/commits">commits</a> for the full list of changes</p>
<p dir="auto">Features marked with a <strong>*</strong> have been back-ported to youtube-dl</p>
<h3 tabindex="-1" dir="auto">Differences in default behavior</h3>
<p dir="auto">Some of yt-dlp's default options are different from that of youtube-dl and youtube-dlc:</p>
<ul dir="auto">
<li>yt-dlp supports only <a href="##" title="Windows 7">Python 3.7+</a>, and <em>may</em> remove support for more versions as they <a href="https://devguide.python.org/versions/#python-release-cycle" rel="nofollow">become EOL</a>; while <a href="https://github.com/ytdl-org/youtube-dl/issues/30568#issue-1118238743" data-hovercard-type="issue" data-hovercard-url="/ytdl-org/youtube-dl/issues/30568/hovercard">youtube-dl still supports Python 2.6+ and 3.2+</a></li>
<li>The options <code>--auto-number</code> (<code>-A</code>), <code>--title</code> (<code>-t</code>) and <code>--literal</code> (<code>-l</code>), no longer work. See <a href="#Removed">removed options</a> for details</li>
<li><code>avconv</code> is not supported as an alternative to <code>ffmpeg</code></li>
<li>yt-dlp stores config files in slightly different locations to youtube-dl. See <a href="#configuration">CONFIGURATION</a> for a list of correct locations</li>
<li>The default <a href="#output-template">output template</a> is <code>%(title)s [%(id)s].%(ext)s</code>. There is no real reason for this change. This was changed before yt-dlp was ever made public and now there are no plans to change it back to <code>%(title)s-%(id)s.%(ext)s</code>. Instead, you may use <code>--compat-options filename</code></li>
<li>The default <a href="#sorting-formats">format sorting</a> is different from youtube-dl and prefers higher resolution and better codecs rather than higher bitrates. You can use the <code>--format-sort</code> option to change this to any order you prefer, or use <code>--compat-options format-sort</code> to use youtube-dl's sorting order</li>
<li>The default format selector is <code>bv*+ba/b</code>. This means that if a combined video + audio format that is better than the best video-only format is found, the former will be preferred. Use <code>-f bv+ba/b</code> or <code>--compat-options format-spec</code> to revert this</li>
<li>Unlike youtube-dlc, yt-dlp does not allow merging multiple audio/video streams into one file by default (since this conflicts with the use of <code>-f bv*+ba</code>). If needed, this feature must be enabled using <code>--audio-multistreams</code> and <code>--video-multistreams</code>. You can also use <code>--compat-options multistreams</code> to enable both</li>
<li><code>--no-abort-on-error</code> is enabled by default. Use <code>--abort-on-error</code> or <code>--compat-options abort-on-error</code> to abort on errors instead</li>
<li>When writing metadata files such as thumbnails, description or infojson, the same information (if available) is also written for playlists. Use <code>--no-write-playlist-metafiles</code> or <code>--compat-options no-playlist-metafiles</code> to not write these files</li>
<li><code>--add-metadata</code> attaches the <code>infojson</code> to <code>mkv</code> files in addition to writing the metadata when used with <code>--write-info-json</code>. Use <code>--no-embed-info-json</code> or <code>--compat-options no-attach-info-json</code> to revert this</li>
<li>Some metadata are embedded into different fields when using <code>--add-metadata</code> as compared to youtube-dl. Most notably, <code>comment</code> field contains the <code>webpage_url</code> and <code>synopsis</code> contains the <code>description</code>. You can <a href="#modifying-metadata">use <code>--parse-metadata</code></a> to modify this to your liking or use <code>--compat-options embed-metadata</code> to revert this</li>
<li><code>playlist_index</code> behaves differently when used with options like <code>--playlist-reverse</code> and <code>--playlist-items</code>. See <a href="https://github.com/yt-dlp/yt-dlp/issues/302" data-hovercard-type="pull_request" data-hovercard-url="/yt-dlp/yt-dlp/pull/302/hovercard">#302</a> for details. You can use <code>--compat-options playlist-index</code> if you want to keep the earlier behavior</li>
<li>The output of <code>-F</code> is listed in a new format. Use <code>--compat-options list-formats</code> to revert this</li>
<li>Live chats (if available) are considered as subtitles. Use <code>--sub-langs all,-live_chat</code> to download all subtitles except live chat. You can also use <code>--compat-options no-live-chat</code> to prevent any live chat/danmaku from downloading</li>
<li>YouTube channel URLs download all uploads of the channel. To download only the videos in a specific tab, pass the tab's URL. If the channel does not show the requested tab, an error will be raised. Also, <code>/live</code> URLs raise an error if there are no live videos instead of silently downloading the entire channel. You may use <code>--compat-options no-youtube-channel-redirect</code> to revert all these redirections</li>
<li>Unavailable videos are also listed for YouTube playlists. Use <code>--compat-options no-youtube-unavailable-videos</code> to remove this</li>
<li>The upload dates extracted from YouTube are in UTC <a href="https://github.com/yt-dlp/yt-dlp/blob/89e4d86171c7b7c997c77d4714542e0383bf0db0/yt_dlp/extractor/youtube.py#L3898-L3900">when available</a>. Use <code>--compat-options no-youtube-prefer-utc-upload-date</code> to prefer the non-UTC upload date.</li>
<li>If <code>ffmpeg</code> is used as the downloader, the downloading and merging of formats happen in a single step when possible. Use <code>--compat-options no-direct-merge</code> to revert this</li>
<li>Thumbnail embedding in <code>mp4</code> is done with mutagen if possible. Use <code>--compat-options embed-thumbnail-atomicparsley</code> to force the use of AtomicParsley instead</li>
<li>Some internal metadata such as filenames are removed by default from the infojson. Use <code>--no-clean-infojson</code> or <code>--compat-options no-clean-infojson</code> to revert this</li>
<li>When <code>--embed-subs</code> and <code>--write-subs</code> are used together, the subtitles are written to disk and also embedded in the media file. You can use just <code>--embed-subs</code> to embed the subs and automatically delete the separate file. See <a href="https://github.com/yt-dlp/yt-dlp/issues/630#issuecomment-893659460" data-hovercard-type="issue" data-hovercard-url="/yt-dlp/yt-dlp/issues/630/hovercard">#630 (comment)</a> for more info. <code>--compat-options no-keep-subs</code> can be used to revert this</li>
<li><code>certifi</code> will be used for SSL root certificates, if installed. If you want to use system certificates (e.g. self-signed), use <code>--compat-options no-certifi</code></li>
<li>yt-dlp's sanitization of invalid characters in filenames is different/smarter than in youtube-dl. You can use <code>--compat-options filename-sanitization</code> to revert to youtube-dl's behavior</li>
<li>yt-dlp tries to parse the external downloader outputs into the standard progress output if possible (Currently implemented: <a href="https://github.com/yt-dlp/yt-dlp/issues/5931" data-hovercard-type="issue" data-hovercard-url="/yt-dlp/yt-dlp/issues/5931/hovercard"><del>aria2c</del></a>). You can use <code>--compat-options no-external-downloader-progress</code> to get the downloader output as-is</li>
<li>yt-dlp versions between 2021.09.01 and 2023.01.02 applies <code>--match-filter</code> to nested playlists. This was an unintentional side-effect of <a href="https://github.com/yt-dlp/yt-dlp/commit/8f18aca8717bb0dd49054555af8d386e5eda3a88">8f18ac</a> and is fixed in <a href="https://github.com/yt-dlp/yt-dlp/commit/d7b460d0e5fc710950582baed2e3fc616ed98a80">d7b460</a>. Use <code>--compat-options playlist-match-filter</code> to revert this</li>
</ul>
<p dir="auto">For ease of use, a few more compat options are available:</p>
<ul dir="auto">
<li><code>--compat-options all</code>: Use all compat options (Do NOT use)</li>
<li><code>--compat-options youtube-dl</code>: Same as <code>--compat-options all,-multistreams,-playlist-match-filter</code></li>
<li><code>--compat-options youtube-dlc</code>: Same as <code>--compat-options all,-no-live-chat,-no-youtube-channel-redirect,-playlist-match-filter</code></li>
<li><code>--compat-options 2021</code>: Same as <code>--compat-options 2022,no-certifi,filename-sanitization,no-youtube-prefer-utc-upload-date</code></li>
<li><code>--compat-options 2022</code>: Same as <code>--compat-options playlist-match-filter,no-external-downloader-progress</code>. Use this to enable all future compat options</li>
</ul>
<h2 tabindex="-1" dir="auto">INSTALLATION</h2>

<p dir="auto"><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe"><img src="https://camo.githubusercontent.com/5e7d03f7f5cc1dc4cd6797a5ede9af299143001f2fc89a7386b87f3d4828c5d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d57696e646f77735f7836342d626c75652e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d77696e646f7773" alt="Windows" data-canonical-src="https://img.shields.io/badge/-Windows_x64-blue.svg?style=for-the-badge&amp;logo=windows"></a>
<a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp"><img src="https://camo.githubusercontent.com/5461aa20146a9fe60de1cc47ac0de9a070a05e73dec54a829d1cf21d85324974/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4c696e75782f4253442d7265642e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d6c696e7578" alt="Unix" data-canonical-src="https://img.shields.io/badge/-Linux/BSD-red.svg?style=for-the-badge&amp;logo=linux"></a>
<a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos"><img src="https://camo.githubusercontent.com/65d1ed3107ea8b6ab3c6b06766904cbc6fdd9e7fd961929f81349e38e3229767/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4d61634f532d6c69676874626c75652e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d6170706c65" alt="MacOS" data-canonical-src="https://img.shields.io/badge/-MacOS-lightblue.svg?style=for-the-badge&amp;logo=apple"></a>
<a href="https://pypi.org/project/yt-dlp" rel="nofollow"><img src="https://camo.githubusercontent.com/8360967fa65c453c411c016c2cfab78837823a995bd4a7510df6fa80f15085f6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d507950692d626c75652e7376673f6c6f676f3d70797069266c6162656c436f6c6f723d353535353535267374796c653d666f722d7468652d6261646765" alt="PyPi" data-canonical-src="https://img.shields.io/badge/-PyPi-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge"></a>
<a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz"><img src="https://camo.githubusercontent.com/d1a58041d43ca05b59ad01cc301b4282ffa507f12e87a7a1bb51d6e82080c312/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d536f757263655f7461722d677265656e2e7376673f7374796c653d666f722d7468652d6261646765" alt="Source Tarball" data-canonical-src="https://img.shields.io/badge/-Source_tar-green.svg?style=for-the-badge"></a>
<a href="#release-files"><img src="https://camo.githubusercontent.com/b38bcbc7dbeb210434768a9f20c9fcebf7d25fe6a3438334d910aae7ee277008/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4f746865722d677265792e7376673f7374796c653d666f722d7468652d6261646765" alt="Other variants" data-canonical-src="https://img.shields.io/badge/-Other-grey.svg?style=for-the-badge"></a>
<a href="https://github.com/yt-dlp/yt-dlp/releases"><img src="https://camo.githubusercontent.com/139754d1ce29070f1f33c96733a649a3fb009d6f2cf91edad2e86560d53d70ee/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d416c6c5f56657273696f6e732d6c69676874677265792e7376673f7374796c653d666f722d7468652d6261646765" alt="All versions" data-canonical-src="https://img.shields.io/badge/-All_Versions-lightgrey.svg?style=for-the-badge"></a></p>

<p dir="auto">You can install yt-dlp using <a href="#release-files">the binaries</a>, <a href="https://pypi.org/project/yt-dlp" rel="nofollow">pip</a> or one using a third-party package manager. See <a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation">the wiki</a> for detailed instructions</p>
<h2 tabindex="-1" dir="auto">UPDATE</h2>
<p dir="auto">You can use <code>yt-dlp -U</code> to update if you are using the <a href="#release-files">release binaries</a></p>
<p dir="auto">If you <a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation#with-pip">installed with pip</a>, simply re-run the same command that was used to install the program</p>
<p dir="auto">For other third-party package managers, see <a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation#third-party-package-managers">the wiki</a> or refer their documentation</p>
<a id="user-content-update-channels">
</a><p dir="auto">There are currently two release channels for binaries, <code>stable</code> and <code>nightly</code>.
<code>stable</code> is the default channel, and many of its changes have been tested by users of the nightly channel.
The <code>nightly</code> channel has releases built after each push to the master branch, and will have the most recent fixes and additions, but also have more risk of regressions. They are available in <a href="https://github.com/yt-dlp/yt-dlp-nightly-builds/releases">their own repo</a>.</p>
<p dir="auto">When using <code>--update</code>/<code>-U</code>, a release binary will only update to its current channel.
<code>--update-to CHANNEL</code> can be used to switch to a different channel when a newer version is available. <code>--update-to [CHANNEL@]TAG</code> can also be used to upgrade or downgrade to specific tags from a channel.</p>
<p dir="auto">You may also use <code>--update-to &lt;repository&gt;</code> (<code>&lt;owner&gt;/&lt;repository&gt;</code>) to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories.</p>
<p dir="auto">Example usage:</p>
<ul dir="auto">
<li><code>yt-dlp --update-to nightly</code> change to <code>nightly</code> channel and update to its latest release</li>
<li><code>yt-dlp --update-to stable@2023.02.17</code> upgrade/downgrade to release to <code>stable</code> channel tag <code>2023.02.17</code></li>
<li><code>yt-dlp --update-to 2023.01.06</code> upgrade/downgrade to tag <code>2023.01.06</code> if it exists on the current channel</li>
<li><code>yt-dlp --update-to example/yt-dlp@2023.03.01</code> upgrade/downgrade to the release from the <code>example/yt-dlp</code> repository, tag <code>2023.03.01</code></li>
</ul>

<h2 tabindex="-1" dir="auto">RELEASE FILES</h2>
<h4 tabindex="-1" dir="auto">Recommended</h4>
<table>
<thead>
<tr>
<th>File</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp">yt-dlp</a></td>
<td>Platform-independent <a href="https://docs.python.org/3/library/zipimport.html" rel="nofollow">zipimport</a> binary. Needs Python (recommended for <strong>Linux/BSD</strong>)</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe">yt-dlp.exe</a></td>
<td>Windows (Win7 SP1+) standalone x64 binary (recommended for <strong>Windows</strong>)</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos">yt-dlp_macos</a></td>
<td>Universal MacOS (10.15+) standalone executable (recommended for <strong>MacOS</strong>)</td>
</tr>
</tbody>
</table>
<h4 tabindex="-1" dir="auto">Alternatives</h4>
<table>
<thead>
<tr>
<th>File</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_x86.exe">yt-dlp_x86.exe</a></td>
<td>Windows (Vista SP2+) standalone x86 (32-bit) binary</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_min.exe">yt-dlp_min.exe</a></td>
<td>Windows (Win7 SP1+) standalone x64 binary built with <code>py2exe</code><br> (<a href="#standalone-py2exe-builds-windows">Not recommended</a>)</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux">yt-dlp_linux</a></td>
<td>Linux standalone x64 binary</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux.zip">yt-dlp_linux.zip</a></td>
<td>Unpackaged Linux executable (no auto-update)</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_armv7l">yt-dlp_linux_armv7l</a></td>
<td>Linux standalone armv7l (32-bit) binary</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64">yt-dlp_linux_aarch64</a></td>
<td>Linux standalone aarch64 (64-bit) binary</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win.zip">yt-dlp_win.zip</a></td>
<td>Unpackaged Windows executable (no auto-update)</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos.zip">yt-dlp_macos.zip</a></td>
<td>Unpackaged MacOS (10.15+) executable (no auto-update)</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos_legacy">yt-dlp_macos_legacy</a></td>
<td>MacOS (10.9+) standalone x64 executable</td>
</tr>
</tbody>
</table>
<h4 tabindex="-1" dir="auto">Misc</h4>
<table>
<thead>
<tr>
<th>File</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz">yt-dlp.tar.gz</a></td>
<td>Source tarball</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS">SHA2-512SUMS</a></td>
<td>GNU-style SHA512 sums</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS.sig">SHA2-512SUMS.sig</a></td>
<td>GPG signature file for SHA512 sums</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS">SHA2-256SUMS</a></td>
<td>GNU-style SHA256 sums</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS.sig">SHA2-256SUMS.sig</a></td>
<td>GPG signature file for SHA256 sums</td>
</tr>
</tbody>
</table>
<p dir="auto">The public key that can be used to verify the GPG signatures is <a href="https://github.com/yt-dlp/yt-dlp/blob/master/public.key">available here</a>
Example usage:</p>
<div data-snippet-clipboard-copy-content="curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS"><pre><code>curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS
</code></pre></div>

<p dir="auto"><strong>Note</strong>: The manpages, shell completion (autocomplete) files etc. are available inside the <a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz">source tarball</a></p>
<h2 tabindex="-1" dir="auto">DEPENDENCIES</h2>
<p dir="auto">Python versions 3.7+ (CPython and PyPy) are supported. Other versions and implementations may or may not work correctly.</p>

<p dir="auto">While all the other dependencies are optional, <code>ffmpeg</code> and <code>ffprobe</code> are highly recommended</p>
<h3 tabindex="-1" dir="auto">Strongly recommended</h3>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://www.ffmpeg.org/" rel="nofollow"><strong>ffmpeg</strong> and <strong>ffprobe</strong></a> - Required for <a href="#format-selection">merging separate video and audio files</a> as well as for various <a href="#post-processing-options">post-processing</a> tasks. License <a href="https://www.ffmpeg.org/legal.html" rel="nofollow">depends on the build</a></p>
<p dir="auto">There are bugs in ffmpeg that causes various issues when used alongside yt-dlp. Since ffmpeg is such an important dependency, we provide <a href="https://github.com/yt-dlp/FFmpeg-Builds#ffmpeg-static-auto-builds">custom builds</a> with patches for some of these issues at <a href="https://github.com/yt-dlp/FFmpeg-Builds">yt-dlp/FFmpeg-Builds</a>. See <a href="https://github.com/yt-dlp/FFmpeg-Builds#patches-applied">the readme</a> for details on the specific issues solved by these builds</p>
<p dir="auto"><strong>Important</strong>: What you need is ffmpeg <em>binary</em>, <strong>NOT</strong> <a href="https://pypi.org/project/ffmpeg" rel="nofollow">the python package of the same name</a></p>
</li>
</ul>
<h3 tabindex="-1" dir="auto">Networking</h3>
<ul dir="auto">
<li><a href="https://github.com/certifi/python-certifi"><strong>certifi</strong></a>* - Provides Mozilla's root certificate bundle. Licensed under <a href="https://github.com/certifi/python-certifi/blob/master/LICENSE">MPLv2</a></li>
<li><a href="https://github.com/google/brotli"><strong>brotli</strong></a>* or <a href="https://github.com/python-hyper/brotlicffi"><strong>brotlicffi</strong></a> - <a href="https://en.wikipedia.org/wiki/Brotli" rel="nofollow">Brotli</a> content encoding support. Both licensed under MIT <sup><a href="https://github.com/google/brotli/blob/master/LICENSE">1</a> <a href="https://github.com/python-hyper/brotlicffi/blob/master/LICENSE">2</a> </sup></li>
<li><a href="https://github.com/aaugustin/websockets"><strong>websockets</strong></a>* - For downloading over websocket. Licensed under <a href="https://github.com/aaugustin/websockets/blob/main/LICENSE">BSD-3-Clause</a></li>
</ul>
<h3 tabindex="-1" dir="auto">Metadata</h3>
<ul dir="auto">
<li><a href="https://github.com/quodlibet/mutagen"><strong>mutagen</strong></a>* - For <code>--embed-thumbnail</code> in certain formats. Licensed under <a href="https://github.com/quodlibet/mutagen/blob/master/COPYING">GPLv2+</a></li>
<li><a href="https://github.com/wez/atomicparsley"><strong>AtomicParsley</strong></a> - For <code>--embed-thumbnail</code> in <code>mp4</code>/<code>m4a</code> files when <code>mutagen</code>/<code>ffmpeg</code> cannot. Licensed under <a href="https://github.com/wez/atomicparsley/blob/master/COPYING">GPLv2+</a></li>
<li><a href="https://github.com/xattr/xattr"><strong>xattr</strong></a>, <a href="https://github.com/iustin/pyxattr"><strong>pyxattr</strong></a> or <a href="http://savannah.nongnu.org/projects/attr" rel="nofollow"><strong>setfattr</strong></a> - For writing xattr metadata (<code>--xattr</code>) on <strong>Linux</strong>. Licensed under <a href="https://github.com/xattr/xattr/blob/master/LICENSE.txt">MIT</a>, <a href="https://github.com/iustin/pyxattr/blob/master/COPYING">LGPL2.1</a> and <a href="http://git.savannah.nongnu.org/cgit/attr.git/tree/doc/COPYING" rel="nofollow">GPLv2+</a> respectively</li>
</ul>
<h3 tabindex="-1" dir="auto">Misc</h3>
<ul dir="auto">
<li><a href="https://github.com/Legrandin/pycryptodome"><strong>pycryptodomex</strong></a>* - For decrypting AES-128 HLS streams and various other data. Licensed under <a href="https://github.com/Legrandin/pycryptodome/blob/master/LICENSE.rst">BSD-2-Clause</a></li>
<li><a href="https://github.com/ariya/phantomjs"><strong>phantomjs</strong></a> - Used in extractors where javascript needs to be run. Licensed under <a href="https://github.com/ariya/phantomjs/blob/master/LICENSE.BSD">BSD-3-Clause</a></li>
<li><a href="https://github.com/mitya57/secretstorage"><strong>secretstorage</strong></a> - For <code>--cookies-from-browser</code> to access the <strong>Gnome</strong> keyring while decrypting cookies of <strong>Chromium</strong>-based browsers on <strong>Linux</strong>. Licensed under <a href="https://github.com/mitya57/secretstorage/blob/master/LICENSE">BSD-3-Clause</a></li>
<li>Any external downloader that you want to use with <code>--downloader</code></li>
</ul>
<h3 tabindex="-1" dir="auto">Deprecated</h3>
<ul dir="auto">
<li><a href="https://www.libav.org/" rel="nofollow"><strong>avconv</strong> and <strong>avprobe</strong></a> - Now <strong>deprecated</strong> alternative to ffmpeg. License <a href="https://libav.org/legal" rel="nofollow">depends on the build</a></li>
<li><a href="https://github.com/faissaloo/SponSkrub"><strong>sponskrub</strong></a> - For using the now <strong>deprecated</strong> <a href="#sponskrub-options">sponskrub options</a>. Licensed under <a href="https://github.com/faissaloo/SponSkrub/blob/master/LICENCE.md">GPLv3+</a></li>
<li><a href="http://rtmpdump.mplayerhq.hu/" rel="nofollow"><strong>rtmpdump</strong></a> - For downloading <code>rtmp</code> streams. ffmpeg can be used instead with <code>--downloader ffmpeg</code>. Licensed under <a href="http://rtmpdump.mplayerhq.hu/" rel="nofollow">GPLv2+</a></li>
<li><a href="http://mplayerhq.hu/design7/info.html" rel="nofollow"><strong>mplayer</strong></a> or <a href="https://mpv.io/" rel="nofollow"><strong>mpv</strong></a> - For downloading <code>rstp</code>/<code>mms</code> streams. ffmpeg can be used instead with <code>--downloader ffmpeg</code>. Licensed under <a href="https://github.com/mpv-player/mpv/blob/master/Copyright">GPLv2+</a></li>
</ul>
<p dir="auto">To use or redistribute the dependencies, you must agree to their respective licensing terms.</p>
<p dir="auto">The standalone release binaries are built with the Python interpreter and the packages marked with <strong>*</strong> included.</p>
<p dir="auto">If you do not have the necessary dependencies for a task you are attempting, yt-dlp will warn you. All the currently available dependencies are visible at the top of the <code>--verbose</code> output</p>
<h2 tabindex="-1" dir="auto">COMPILE</h2>
<h3 tabindex="-1" dir="auto">Standalone PyInstaller Builds</h3>
<p dir="auto">To build the standalone executable, you must have Python and <code>pyinstaller</code> (plus any of yt-dlp's <a href="#dependencies">optional dependencies</a> if needed). Once you have all the necessary dependencies installed, simply run <code>pyinst.py</code>. The executable will be built for the same architecture (x86/ARM, 32/64 bit) as the Python used.</p>
<div data-snippet-clipboard-copy-content="python3 -m pip install -U pyinstaller -r requirements.txt
python3 devscripts/make_lazy_extractors.py
python3 pyinst.py"><pre><code>python3 -m pip install -U pyinstaller -r requirements.txt
python3 devscripts/make_lazy_extractors.py
python3 pyinst.py
</code></pre></div>
<p dir="auto">On some systems, you may need to use <code>py</code> or <code>python</code> instead of <code>python3</code>.</p>
<p dir="auto"><code>pyinst.py</code> accepts any arguments that can be passed to <code>pyinstaller</code>, such as <code>--onefile/-F</code> or <code>--onedir/-D</code>, which is further <a href="https://pyinstaller.org/en/stable/usage.html#what-to-generate" rel="nofollow">documented here</a>.</p>
<p dir="auto"><strong>Note</strong>: Pyinstaller versions below 4.4 <a href="https://github.com/pyinstaller/pyinstaller#requirements-and-tested-platforms">do not support</a> Python installed from the Windows store without using a virtual environment.</p>
<p dir="auto"><strong>Important</strong>: Running <code>pyinstaller</code> directly <strong>without</strong> using <code>pyinst.py</code> is <strong>not</strong> officially supported. This may or may not work correctly.</p>
<h3 tabindex="-1" dir="auto">Platform-independent Binary (UNIX)</h3>
<p dir="auto">You will need the build tools <code>python</code> (3.7+), <code>zip</code>, <code>make</code> (GNU), <code>pandoc</code>* and <code>pytest</code>*.</p>
<p dir="auto">After installing these, simply run <code>make</code>.</p>
<p dir="auto">You can also run <code>make yt-dlp</code> instead to compile only the binary without updating any of the additional files. (The build tools marked with <strong>*</strong> are not needed for this)</p>
<h3 tabindex="-1" dir="auto">Standalone Py2Exe Builds (Windows)</h3>
<p dir="auto">While we provide the option to build with <a href="https://www.py2exe.org/" rel="nofollow">py2exe</a>, it is recommended to build <a href="#standalone-pyinstaller-builds">using PyInstaller</a> instead since the py2exe builds <strong>cannot contain <code>pycryptodomex</code>/<code>certifi</code> and needs VC++14</strong> on the target computer to run.</p>
<p dir="auto">If you wish to build it anyway, install Python and py2exe, and then simply run <code>setup.py py2exe</code></p>
<div data-snippet-clipboard-copy-content="py -m pip install -U py2exe -r requirements.txt
py devscripts/make_lazy_extractors.py
py setup.py py2exe"><pre><code>py -m pip install -U py2exe -r requirements.txt
py devscripts/make_lazy_extractors.py
py setup.py py2exe
</code></pre></div>
<h3 tabindex="-1" dir="auto">Related scripts</h3>
<ul dir="auto">
<li><strong><code>devscripts/update-version.py</code></strong> - Update the version number based on current date.</li>
<li><strong><code>devscripts/set-variant.py</code></strong> - Set the build variant of the executable.</li>
<li><strong><code>devscripts/make_changelog.py</code></strong> - Create a markdown changelog using short commit messages and update <code>CONTRIBUTORS</code> file.</li>
<li><strong><code>devscripts/make_lazy_extractors.py</code></strong> - Create lazy extractors. Running this before building the binaries (any variant) will improve their startup performance. Set the environment variable <code>YTDLP_NO_LAZY_EXTRACTORS=1</code> if you wish to forcefully disable lazy extractor loading.</li>
</ul>
<p dir="auto">Note: See their <code>--help</code> for more info.</p>
<h3 tabindex="-1" dir="auto">Forking the project</h3>
<p dir="auto">If you fork the project on GitHub, you can run your fork's <a href="https://github.com/yt-dlp/yt-dlp/blob/master/.github/workflows/build.yml">build workflow</a> to automatically build the selected version(s) as artifacts. Alternatively, you can run the <a href="https://github.com/yt-dlp/yt-dlp/blob/master/.github/workflows/release.yml">release workflow</a> or enable the <a href="https://github.com/yt-dlp/yt-dlp/blob/master/.github/workflows/release-nightly.yml">nightly workflow</a> to create full (pre-)releases.</p>
<h2 tabindex="-1" dir="auto">USAGE AND OPTIONS</h2>

<div data-snippet-clipboard-copy-content="yt-dlp [OPTIONS] [--] URL [URL...]"><pre><code>yt-dlp [OPTIONS] [--] URL [URL...]
</code></pre></div>
<p dir="auto"><code>Ctrl+F</code> is your friend :D</p>


<h2 tabindex="-1" dir="auto">General Options:</h2>
<div data-snippet-clipboard-copy-content="-h, --help                      Print this help text and exit
--version                       Print program version and exit
-U, --update                    Update this program to the latest version
--no-update                     Do not check for updates (default)
--update-to [CHANNEL]@[TAG]     Upgrade/downgrade to a specific version.
                                CHANNEL can be a repository as well. CHANNEL
                                and TAG default to &quot;stable&quot; and &quot;latest&quot;
                                respectively if omitted; See &quot;UPDATE&quot; for
                                details. Supported channels: stable, nightly
-i, --ignore-errors             Ignore download and postprocessing errors.
                                The download will be considered successful
                                even if the postprocessing fails
--no-abort-on-error             Continue with next video on download errors;
                                e.g. to skip unavailable videos in a
                                playlist (default)
--abort-on-error                Abort downloading of further videos if an
                                error occurs (Alias: --no-ignore-errors)
--dump-user-agent               Display the current user-agent and exit
--list-extractors               List all supported extractors and exit
--extractor-descriptions        Output descriptions of all supported
                                extractors and exit
--use-extractors NAMES          Extractor names to use separated by commas.
                                You can also use regexes, &quot;all&quot;, &quot;default&quot;
                                and &quot;end&quot; (end URL matching); e.g. --ies
                                &quot;holodex.*,end,youtube&quot;. Prefix the name
                                with a &quot;-&quot; to exclude it, e.g. --ies
                                default,-generic. Use --list-extractors for
                                a list of extractor names. (Alias: --ies)
--default-search PREFIX         Use this prefix for unqualified URLs. E.g.
                                &quot;gvsearch2:python&quot; downloads two videos from
                                google videos for the search term &quot;python&quot;.
                                Use the value &quot;auto&quot; to let yt-dlp guess
                                (&quot;auto_warning&quot; to emit a warning when
                                guessing). &quot;error&quot; just throws an error. The
                                default value &quot;fixup_error&quot; repairs broken
                                URLs, but emits an error if this is not
                                possible instead of searching
--ignore-config                 Don't load any more configuration files
                                except those given by --config-locations.
                                For backward compatibility, if this option
                                is found inside the system configuration
                                file, the user configuration is not loaded.
                                (Alias: --no-config)
--no-config-locations           Do not load any custom configuration files
                                (default). When given inside a configuration
                                file, ignore all previous --config-locations
                                defined in the current file
--config-locations PATH         Location of the main configuration file;
                                either the path to the config or its
                                containing directory (&quot;-&quot; for stdin). Can be
                                used multiple times and inside other
                                configuration files
--flat-playlist                 Do not extract the videos of a playlist,
                                only list them
--no-flat-playlist              Fully extract the videos of a playlist
                                (default)
--live-from-start               Download livestreams from the start.
                                Currently only supported for YouTube
                                (Experimental)
--no-live-from-start            Download livestreams from the current time
                                (default)
--wait-for-video MIN[-MAX]      Wait for scheduled streams to become
                                available. Pass the minimum number of
                                seconds (or range) to wait between retries
--no-wait-for-video             Do not wait for scheduled streams (default)
--mark-watched                  Mark videos watched (even with --simulate)
--no-mark-watched               Do not mark videos watched (default)
--color [STREAM:]POLICY         Whether to emit color codes in output,
                                optionally prefixed by the STREAM (stdout or
                                stderr) to apply the setting to. Can be one
                                of &quot;always&quot;, &quot;auto&quot; (default), &quot;never&quot;, or
                                &quot;no_color&quot; (use non color terminal
                                sequences). Can be used multiple times
--compat-options OPTS           Options that can help keep compatibility
                                with youtube-dl or youtube-dlc
                                configurations by reverting some of the
                                changes made in yt-dlp. See &quot;Differences in
                                default behavior&quot; for details
--alias ALIASES OPTIONS         Create aliases for an option string. Unless
                                an alias starts with a dash &quot;-&quot;, it is
                                prefixed with &quot;--&quot;. Arguments are parsed
                                according to the Python string formatting
                                mini-language. E.g. --alias get-audio,-X
                                &quot;-S=aext:{0},abr -x --audio-format {0}&quot;
                                creates options &quot;--get-audio&quot; and &quot;-X&quot; that
                                takes an argument (ARG0) and expands to
                                &quot;-S=aext:ARG0,abr -x --audio-format ARG0&quot;.
                                All defined aliases are listed in the --help
                                output. Alias options can trigger more
                                aliases; so be careful to avoid defining
                                recursive options. As a safety measure, each
                                alias may be triggered a maximum of 100
                                times. This option can be used multiple times"><pre><code>-h, --help                      Print this help text and exit
--version                       Print program version and exit
-U, --update                    Update this program to the latest version
--no-update                     Do not check for updates (default)
--update-to [CHANNEL]@[TAG]     Upgrade/downgrade to a specific version.
                                CHANNEL can be a repository as well. CHANNEL
                                and TAG default to "stable" and "latest"
                                respectively if omitted; See "UPDATE" for
                                details. Supported channels: stable, nightly
-i, --ignore-errors             Ignore download and postprocessing errors.
                                The download will be considered successful
                                even if the postprocessing fails
--no-abort-on-error             Continue with next video on download errors;
                                e.g. to skip unavailable videos in a
                                playlist (default)
--abort-on-error                Abort downloading of further videos if an
                                error occurs (Alias: --no-ignore-errors)
--dump-user-agent               Display the current user-agent and exit
--list-extractors               List all supported extractors and exit
--extractor-descriptions        Output descriptions of all supported
                                extractors and exit
--use-extractors NAMES          Extractor names to use separated by commas.
                                You can also use regexes, "all", "default"
                                and "end" (end URL matching); e.g. --ies
                                "holodex.*,end,youtube". Prefix the name
                                with a "-" to exclude it, e.g. --ies
                                default,-generic. Use --list-extractors for
                                a list of extractor names. (Alias: --ies)
--default-search PREFIX         Use this prefix for unqualified URLs. E.g.
                                "gvsearch2:python" downloads two videos from
                                google videos for the search term "python".
                                Use the value "auto" to let yt-dlp guess
                                ("auto_warning" to emit a warning when
                                guessing). "error" just throws an error. The
                                default value "fixup_error" repairs broken
                                URLs, but emits an error if this is not
                                possible instead of searching
--ignore-config                 Don't load any more configuration files
                                except those given by --config-locations.
                                For backward compatibility, if this option
                                is found inside the system configuration
                                file, the user configuration is not loaded.
                                (Alias: --no-config)
--no-config-locations           Do not load any custom configuration files
                                (default). When given inside a configuration
                                file, ignore all previous --config-locations
                                defined in the current file
--config-locations PATH         Location of the main configuration file;
                                either the path to the config or its
                                containing directory ("-" for stdin). Can be
                                used multiple times and inside other
                                configuration files
--flat-playlist                 Do not extract the videos of a playlist,
                                only list them
--no-flat-playlist              Fully extract the videos of a playlist
                                (default)
--live-from-start               Download livestreams from the start.
                                Currently only supported for YouTube
                                (Experimental)
--no-live-from-start            Download livestreams from the current time
                                (default)
--wait-for-video MIN[-MAX]      Wait for scheduled streams to become
                                available. Pass the minimum number of
                                seconds (or range) to wait between retries
--no-wait-for-video             Do not wait for scheduled streams (default)
--mark-watched                  Mark videos watched (even with --simulate)
--no-mark-watched               Do not mark videos watched (default)
--color [STREAM:]POLICY         Whether to emit color codes in output,
                                optionally prefixed by the STREAM (stdout or
                                stderr) to apply the setting to. Can be one
                                of "always", "auto" (default), "never", or
                                "no_color" (use non color terminal
                                sequences). Can be used multiple times
--compat-options OPTS           Options that can help keep compatibility
                                with youtube-dl or youtube-dlc
                                configurations by reverting some of the
                                changes made in yt-dlp. See "Differences in
                                default behavior" for details
--alias ALIASES OPTIONS         Create aliases for an option string. Unless
                                an alias starts with a dash "-", it is
                                prefixed with "--". Arguments are parsed
                                according to the Python string formatting
                                mini-language. E.g. --alias get-audio,-X
                                "-S=aext:{0},abr -x --audio-format {0}"
                                creates options "--get-audio" and "-X" that
                                takes an argument (ARG0) and expands to
                                "-S=aext:ARG0,abr -x --audio-format ARG0".
                                All defined aliases are listed in the --help
                                output. Alias options can trigger more
                                aliases; so be careful to avoid defining
                                recursive options. As a safety measure, each
                                alias may be triggered a maximum of 100
                                times. This option can be used multiple times
</code></pre></div>
<h2 tabindex="-1" dir="auto">Network Options:</h2>
<div data-snippet-clipboard-copy-content="--proxy URL                     Use the specified HTTP/HTTPS/SOCKS proxy. To
                                enable SOCKS proxy, specify a proper scheme,
                                e.g. socks5://user:pass@127.0.0.1:1080/.
                                Pass in an empty string (--proxy &quot;&quot;) for
                                direct connection
--socket-timeout SECONDS        Time to wait before giving up, in seconds
--source-address IP             Client-side IP address to bind to
-4, --force-ipv4                Make all connections via IPv4
-6, --force-ipv6                Make all connections via IPv6
--enable-file-urls              Enable file:// URLs. This is disabled by
                                default for security reasons."><pre><code>--proxy URL                     Use the specified HTTP/HTTPS/SOCKS proxy. To
                                enable SOCKS proxy, specify a proper scheme,
                                e.g. socks5://user:pass@127.0.0.1:1080/.
                                Pass in an empty string (--proxy "") for
                                direct connection
--socket-timeout SECONDS        Time to wait before giving up, in seconds
--source-address IP             Client-side IP address to bind to
-4, --force-ipv4                Make all connections via IPv4
-6, --force-ipv6                Make all connections via IPv6
--enable-file-urls              Enable file:// URLs. This is disabled by
                                default for security reasons.
</code></pre></div>
<h2 tabindex="-1" dir="auto">Geo-restriction:</h2>
<div data-snippet-clipboard-copy-content="--geo-verification-proxy URL    Use this proxy to verify the IP address for
                                some geo-restricted sites. The default proxy
                                specified by --proxy (or none, if the option
                                is not present) is used for the actual
                                downloading
--xff VALUE                     How to fake X-Forwarded-For HTTP header to
                                try bypassing geographic restriction. One of
                                &quot;default&quot; (only when known to be useful),
                                &quot;never&quot;, an IP block in CIDR notation, or a
                                two-letter ISO 3166-2 country code"><pre><code>--geo-verification-proxy URL    Use this proxy to verify the IP address for
                                some geo-restricted sites. The default proxy
                                specified by --proxy (or none, if the option
                                is not present) is used for the actual
                                downloading
--xff VALUE                     How to fake X-Forwarded-For HTTP header to
                                try bypassing geographic restriction. One of
                                "default" (only when known to be useful),
                                "never", an IP block in CIDR notation, or a
                                two-letter ISO 3166-2 country code
</code></pre></div>
<h2 tabindex="-1" dir="auto">Video Selection:</h2>
<div data-snippet-clipboard-copy-content="-I, --playlist-items ITEM_SPEC  Comma separated playlist_index of the items
                                to download. You can specify a range using
                                &quot;[START]:[STOP][:STEP]&quot;. For backward
                                compatibility, START-STOP is also supported.
                                Use negative indices to count from the right
                                and negative STEP to download in reverse
                                order. E.g. &quot;-I 1:3,7,-5::2&quot; used on a
                                playlist of size 15 will download the items
                                at index 1,2,3,7,11,13,15
--min-filesize SIZE             Abort download if filesize is smaller than
                                SIZE, e.g. 50k or 44.6M
--max-filesize SIZE             Abort download if filesize is larger than
                                SIZE, e.g. 50k or 44.6M
--date DATE                     Download only videos uploaded on this date.
                                The date can be &quot;YYYYMMDD&quot; or in the format 
                                [now|today|yesterday][-N[day|week|month|year]].
                                E.g. &quot;--date today-2weeks&quot; downloads only
                                videos uploaded on the same day two weeks ago
--datebefore DATE               Download only videos uploaded on or before
                                this date. The date formats accepted is the
                                same as --date
--dateafter DATE                Download only videos uploaded on or after
                                this date. The date formats accepted is the
                                same as --date
--match-filters FILTER          Generic video filter. Any &quot;OUTPUT TEMPLATE&quot;
                                field can be compared with a number or a
                                string using the operators defined in
                                &quot;Filtering Formats&quot;. You can also simply
                                specify a field to match if the field is
                                present, use &quot;!field&quot; to check if the field
                                is not present, and &quot;&amp;&quot; to check multiple
                                conditions. Use a &quot;\&quot; to escape &quot;&amp;&quot; or
                                quotes if needed. If used multiple times,
                                the filter matches if atleast one of the
                                conditions are met. E.g. --match-filter
                                !is_live --match-filter &quot;like_count>?100 &amp;
                                description~='(?i)\bcats \&amp; dogs\b'&quot; matches
                                only videos that are not live OR those that
                                have a like count more than 100 (or the like
                                field is not available) and also has a
                                description that contains the phrase &quot;cats &amp;
                                dogs&quot; (caseless). Use &quot;--match-filter -&quot; to
                                interactively ask whether to download each
                                video
--no-match-filters              Do not use any --match-filter (default)
--break-match-filters FILTER    Same as &quot;--match-filters&quot; but stops the
                                download process when a video is rejected
--no-break-match-filters        Do not use any --break-match-filters (default)
--no-playlist                   Download only the video, if the URL refers
                                to a video and a playlist
--yes-playlist                  Download the playlist, if the URL refers to
                                a video and a playlist
--age-limit YEARS               Download only videos suitable for the given
                                age
--download-archive FILE         Download only videos not listed in the
                                archive file. Record the IDs of all
                                downloaded videos in it
--no-download-archive           Do not use archive file (default)
--max-downloads NUMBER          Abort after downloading NUMBER files
--break-on-existing             Stop the download process when encountering
                                a file that is in the archive
--break-per-input               Alters --max-downloads, --break-on-existing,
                                --break-match-filter, and autonumber to
                                reset per input URL
--no-break-per-input            --break-on-existing and similar options
                                terminates the entire download queue
--skip-playlist-after-errors N  Number of allowed failures until the rest of
                                the playlist is skipped"><pre><code>-I, --playlist-items ITEM_SPEC  Comma separated playlist_index of the items
                                to download. You can specify a range using
                                "[START]:[STOP][:STEP]". For backward
                                compatibility, START-STOP is also supported.
                                Use negative indices to count from the right
                                and negative STEP to download in reverse
                                order. E.g. "-I 1:3,7,-5::2" used on a
                                playlist of size 15 will download the items
                                at index 1,2,3,7,11,13,15
--min-filesize SIZE             Abort download if filesize is smaller than
                                SIZE, e.g. 50k or 44.6M
--max-filesize SIZE             Abort download if filesize is larger than
                                SIZE, e.g. 50k or 44.6M
--date DATE                     Download only videos uploaded on this date.
                                The date can be "YYYYMMDD" or in the format 
                                [now|today|yesterday][-N[day|week|month|year]].
                                E.g. "--date today-2weeks" downloads only
                                videos uploaded on the same day two weeks ago
--datebefore DATE               Download only videos uploaded on or before
                                this date. The date formats accepted is the
                                same as --date
--dateafter DATE                Download only videos uploaded on or after
                                this date. The date formats accepted is the
                                same as --date
--match-filters FILTER          Generic video filter. Any "OUTPUT TEMPLATE"
                                field can be compared with a number or a
                                string using the operators defined in
                                "Filtering Formats". You can also simply
                                specify a field to match if the field is
                                present, use "!field" to check if the field
                                is not present, and "&amp;" to check multiple
                                conditions. Use a "\" to escape "&amp;" or
                                quotes if needed. If used multiple times,
                                the filter matches if atleast one of the
                                conditions are met. E.g. --match-filter
                                !is_live --match-filter "like_count&gt;?100 &amp;
                                description~='(?i)\bcats \&amp; dogs\b'" matches
                                only videos that are not live OR those that
                                have a like count more than 100 (or the like
                                field is not available) and also has a
                                description that contains the phrase "cats &amp;
                                dogs" (caseless). Use "--match-filter -" to
                                interactively ask whether to download each
                                video
--no-match-filters              Do not use any --match-filter (default)
--break-match-filters FILTER    Same as "--match-filters" but stops the
                                download process when a video is rejected
--no-break-match-filters        Do not use any --break-match-filters (default)
--no-playlist                   Download only the video, if the URL refers
                                to a video and a playlist
--yes-playlist                  Download the playlist, if the URL refers to
                                a video and a playlist
--age-limit YEARS               Download only videos suitable for the given
                                age
--download-archive FILE         Download only videos not listed in the
                                archive file. Record the IDs of all
                                downloaded videos in it
--no-download-archive           Do not use archive file (default)
--max-downloads NUMBER          Abort after downloading NUMBER files
--break-on-existing             Stop the download process when encountering
                                a file that is in the archive
--break-per-input               Alters --max-downloads, --break-on-existing,
                                --break-match-filter, and autonumber to
                                reset per input URL
--no-break-per-input            --break-on-existing and similar options
                                terminates the entire download queue
--skip-playlist-after-errors N  Number of allowed failures until the rest of
                                the playlist is skipped
</code></pre></div>
<h2 tabindex="-1" dir="auto">Download Options:</h2>
<div data-snippet-clipboard-copy-content="-N, --concurrent-fragments N    Number of fragments of a dash/hlsnative
                                video that should be downloaded concurrently
                                (default is 1)
-r, --limit-rate RATE           Maximum download rate in bytes per second,
                                e.g. 50K or 4.2M
--throttled-rate RATE           Minimum download rate in bytes per second
                                below which throttling is assumed and the
                                video data is re-extracted, e.g. 100K
-R, --retries RETRIES           Number of retries (default is 10), or
                                &quot;infinite&quot;
--file-access-retries RETRIES   Number of times to retry on file access
                                error (default is 3), or &quot;infinite&quot;
--fragment-retries RETRIES      Number of retries for a fragment (default is
                                10), or &quot;infinite&quot; (DASH, hlsnative and ISM)
--retry-sleep [TYPE:]EXPR       Time to sleep between retries in seconds
                                (optionally) prefixed by the type of retry
                                (http (default), fragment, file_access,
                                extractor) to apply the sleep to. EXPR can
                                be a number, linear=START[:END[:STEP=1]] or
                                exp=START[:END[:BASE=2]]. This option can be
                                used multiple times to set the sleep for the
                                different retry types, e.g. --retry-sleep
                                linear=1::2 --retry-sleep fragment:exp=1:20
--skip-unavailable-fragments    Skip unavailable fragments for DASH,
                                hlsnative and ISM downloads (default)
                                (Alias: --no-abort-on-unavailable-fragments)
--abort-on-unavailable-fragments
                                Abort download if a fragment is unavailable
                                (Alias: --no-skip-unavailable-fragments)
--keep-fragments                Keep downloaded fragments on disk after
                                downloading is finished
--no-keep-fragments             Delete downloaded fragments after
                                downloading is finished (default)
--buffer-size SIZE              Size of download buffer, e.g. 1024 or 16K
                                (default is 1024)
--resize-buffer                 The buffer size is automatically resized
                                from an initial value of --buffer-size
                                (default)
--no-resize-buffer              Do not automatically adjust the buffer size
--http-chunk-size SIZE          Size of a chunk for chunk-based HTTP
                                downloading, e.g. 10485760 or 10M (default
                                is disabled). May be useful for bypassing
                                bandwidth throttling imposed by a webserver
                                (experimental)
--playlist-random               Download playlist videos in random order
--lazy-playlist                 Process entries in the playlist as they are
                                received. This disables n_entries,
                                --playlist-random and --playlist-reverse
--no-lazy-playlist              Process videos in the playlist only after
                                the entire playlist is parsed (default)
--xattr-set-filesize            Set file xattribute ytdl.filesize with
                                expected file size
--hls-use-mpegts                Use the mpegts container for HLS videos;
                                allowing some players to play the video
                                while downloading, and reducing the chance
                                of file corruption if download is
                                interrupted. This is enabled by default for
                                live streams
--no-hls-use-mpegts             Do not use the mpegts container for HLS
                                videos. This is default when not downloading
                                live streams
--download-sections REGEX       Download only chapters that match the
                                regular expression. A &quot;*&quot; prefix denotes
                                time-range instead of chapter. Negative
                                timestamps are calculated from the end.
                                &quot;*from-url&quot; can be used to download between
                                the &quot;start_time&quot; and &quot;end_time&quot; extracted
                                from the URL. Needs ffmpeg. This option can
                                be used multiple times to download multiple
                                sections, e.g. --download-sections
                                &quot;*10:15-inf&quot; --download-sections &quot;intro&quot;
--downloader [PROTO:]NAME       Name or path of the external downloader to
                                use (optionally) prefixed by the protocols
                                (http, ftp, m3u8, dash, rstp, rtmp, mms) to
                                use it for. Currently supports native,
                                aria2c, avconv, axel, curl, ffmpeg, httpie,
                                wget. You can use this option multiple times
                                to set different downloaders for different
                                protocols. E.g. --downloader aria2c
                                --downloader &quot;dash,m3u8:native&quot; will use
                                aria2c for http/ftp downloads, and the
                                native downloader for dash/m3u8 downloads
                                (Alias: --external-downloader)
--downloader-args NAME:ARGS     Give these arguments to the external
                                downloader. Specify the downloader name and
                                the arguments separated by a colon &quot;:&quot;. For
                                ffmpeg, arguments can be passed to different
                                positions using the same syntax as
                                --postprocessor-args. You can use this
                                option multiple times to give different
                                arguments to different downloaders (Alias:
                                --external-downloader-args)"><pre><code>-N, --concurrent-fragments N    Number of fragments of a dash/hlsnative
                                video that should be downloaded concurrently
                                (default is 1)
-r, --limit-rate RATE           Maximum download rate in bytes per second,
                                e.g. 50K or 4.2M
--throttled-rate RATE           Minimum download rate in bytes per second
                                below which throttling is assumed and the
                                video data is re-extracted, e.g. 100K
-R, --retries RETRIES           Number of retries (default is 10), or
                                "infinite"
--file-access-retries RETRIES   Number of times to retry on file access
                                error (default is 3), or "infinite"
--fragment-retries RETRIES      Number of retries for a fragment (default is
                                10), or "infinite" (DASH, hlsnative and ISM)
--retry-sleep [TYPE:]EXPR       Time to sleep between retries in seconds
                                (optionally) prefixed by the type of retry
                                (http (default), fragment, file_access,
                                extractor) to apply the sleep to. EXPR can
                                be a number, linear=START[:END[:STEP=1]] or
                                exp=START[:END[:BASE=2]]. This option can be
                                used multiple times to set the sleep for the
                                different retry types, e.g. --retry-sleep
                                linear=1::2 --retry-sleep fragment:exp=1:20
--skip-unavailable-fragments    Skip unavailable fragments for DASH,
                                hlsnative and ISM downloads (default)
                                (Alias: --no-abort-on-unavailable-fragments)
--abort-on-unavailable-fragments
                                Abort download if a fragment is unavailable
                                (Alias: --no-skip-unavailable-fragments)
--keep-fragments                Keep downloaded fragments on disk after
                                downloading is finished
--no-keep-fragments             Delete downloaded fragments after
                                downloading is finished (default)
--buffer-size SIZE              Size of download buffer, e.g. 1024 or 16K
                                (default is 1024)
--resize-buffer                 The buffer size is automatically resized
                                from an initial value of --buffer-size
                                (default)
--no-resize-buffer              Do not automatically adjust the buffer size
--http-chunk-size SIZE          Size of a chunk for chunk-based HTTP
                                downloading, e.g. 10485760 or 10M (default
                                is disabled). May be useful for bypassing
                                bandwidth throttling imposed by a webserver
                                (experimental)
--playlist-random               Download playlist videos in random order
--lazy-playlist                 Process entries in the playlist as they are
                                received. This disables n_entries,
                                --playlist-random and --playlist-reverse
--no-lazy-playlist              Process videos in the playlist only after
                                the entire playlist is parsed (default)
--xattr-set-filesize            Set file xattribute ytdl.filesize with
                                expected file size
--hls-use-mpegts                Use the mpegts container for HLS videos;
                                allowing some players to play the video
                                while downloading, and reducing the chance
                                of file corruption if download is
                                interrupted. This is enabled by default for
                                live streams
--no-hls-use-mpegts             Do not use the mpegts container for HLS
                                videos. This is default when not downloading
                                live streams
--download-sections REGEX       Download only chapters that match the
                                regular expression. A "*" prefix denotes
                                time-range instead of chapter. Negative
                                timestamps are calculated from the end.
                                "*from-url" can be used to download between
                                the "start_time" and "end_time" extracted
                                from the URL. Needs ffmpeg. This option can
                                be used multiple times to download multiple
                                sections, e.g. --download-sections
                                "*10:15-inf" --download-sections "intro"
--downloader [PROTO:]NAME       Name or path of the external downloader to
                                use (optionally) prefixed by the protocols
                                (http, ftp, m3u8, dash, rstp, rtmp, mms) to
                                use it for. Currently supports native,
                                aria2c, avconv, axel, curl, ffmpeg, httpie,
                                wget. You can use this option multiple times
                                to set different downloaders for different
                                protocols. E.g. --downloader aria2c
                                --downloader "dash,m3u8:native" will use
                                aria2c for http/ftp downloads, and the
                                native downloader for dash/m3u8 downloads
                                (Alias: --external-downloader)
--downloader-args NAME:ARGS     Give these arguments to the external
                                downloader. Specify the downloader name and
                                the arguments separated by a colon ":". For
                                ffmpeg, arguments can be passed to different
                                positions using the same syntax as
                                --postprocessor-args. You can use this
                                option multiple times to give different
                                arguments to different downloaders (Alias:
                                --external-downloader-args)
</code></pre></div>
<h2 tabindex="-1" dir="auto">Filesystem Options:</h2>
<div data-snippet-clipboard-copy-content="-a, --batch-file FILE           File containing URLs to download (&quot;-&quot; for
                                stdin), one URL per line. Lines starting
                                with &quot;#&quot;, &quot;;&quot; or &quot;]&quot; are considered as
                                comments and ignored
--no-batch-file                 Do not read URLs from batch file (default)
-P, --paths [TYPES:]PATH        The paths where the files should be
                                downloaded. Specify the type of file and the
                                path separated by a colon &quot;:&quot;. All the same
                                TYPES as --output are supported.
                                Additionally, you can also provide &quot;home&quot;
                                (default) and &quot;temp&quot; paths. All intermediary
                                files are first downloaded to the temp path
                                and then the final files are moved over to
                                the home path after download is finished.
                                This option is ignored if --output is an
                                absolute path
-o, --output [TYPES:]TEMPLATE   Output filename template; see &quot;OUTPUT
                                TEMPLATE&quot; for details
--output-na-placeholder TEXT    Placeholder for unavailable fields in
                                &quot;OUTPUT TEMPLATE&quot; (default: &quot;NA&quot;)
--restrict-filenames            Restrict filenames to only ASCII characters,
                                and avoid &quot;&amp;&quot; and spaces in filenames
--no-restrict-filenames         Allow Unicode characters, &quot;&amp;&quot; and spaces in
                                filenames (default)
--windows-filenames             Force filenames to be Windows-compatible
--no-windows-filenames          Make filenames Windows-compatible only if
                                using Windows (default)
--trim-filenames LENGTH         Limit the filename length (excluding
                                extension) to the specified number of
                                characters
-w, --no-overwrites             Do not overwrite any files
--force-overwrites              Overwrite all video and metadata files. This
                                option includes --no-continue
--no-force-overwrites           Do not overwrite the video, but overwrite
                                related files (default)
-c, --continue                  Resume partially downloaded files/fragments
                                (default)
--no-continue                   Do not resume partially downloaded
                                fragments. If the file is not fragmented,
                                restart download of the entire file
--part                          Use .part files instead of writing directly
                                into output file (default)
--no-part                       Do not use .part files - write directly into
                                output file
--mtime                         Use the Last-modified header to set the file
                                modification time (default)
--no-mtime                      Do not use the Last-modified header to set
                                the file modification time
--write-description             Write video description to a .description file
--no-write-description          Do not write video description (default)
--write-info-json               Write video metadata to a .info.json file
                                (this may contain personal information)
--no-write-info-json            Do not write video metadata (default)
--write-playlist-metafiles      Write playlist metadata in addition to the
                                video metadata when using --write-info-json,
                                --write-description etc. (default)
--no-write-playlist-metafiles   Do not write playlist metadata when using
                                --write-info-json, --write-description etc.
--clean-info-json               Remove some internal metadata such as
                                filenames from the infojson (default)
--no-clean-info-json            Write all fields to the infojson
--write-comments                Retrieve video comments to be placed in the
                                infojson. The comments are fetched even
                                without this option if the extraction is
                                known to be quick (Alias: --get-comments)
--no-write-comments             Do not retrieve video comments unless the
                                extraction is known to be quick (Alias:
                                --no-get-comments)
--load-info-json FILE           JSON file containing the video information
                                (created with the &quot;--write-info-json&quot; option)
--cookies FILE                  Netscape formatted file to read cookies from
                                and dump cookie jar in
--no-cookies                    Do not read/dump cookies from/to file
                                (default)
--cookies-from-browser BROWSER[+KEYRING][:PROFILE][::CONTAINER]
                                The name of the browser to load cookies
                                from. Currently supported browsers are:
                                brave, chrome, chromium, edge, firefox,
                                opera, safari, vivaldi. Optionally, the
                                KEYRING used for decrypting Chromium cookies
                                on Linux, the name/path of the PROFILE to
                                load cookies from, and the CONTAINER name
                                (if Firefox) (&quot;none&quot; for no container) can
                                be given with their respective seperators.
                                By default, all containers of the most
                                recently accessed profile are used.
                                Currently supported keyrings are: basictext,
                                gnomekeyring, kwallet, kwallet5, kwallet6
--no-cookies-from-browser       Do not load cookies from browser (default)
--cache-dir DIR                 Location in the filesystem where yt-dlp can
                                store some downloaded information (such as
                                client ids and signatures) permanently. By
                                default ${XDG_CACHE_HOME}/yt-dlp
--no-cache-dir                  Disable filesystem caching
--rm-cache-dir                  Delete all filesystem cache files"><pre><code>-a, --batch-file FILE           File containing URLs to download ("-" for
                                stdin), one URL per line. Lines starting
                                with "#", ";" or "]" are considered as
                                comments and ignored
--no-batch-file                 Do not read URLs from batch file (default)
-P, --paths [TYPES:]PATH        The paths where the files should be
                                downloaded. Specify the type of file and the
                                path separated by a colon ":". All the same
                                TYPES as --output are supported.
                                Additionally, you can also provide "home"
                                (default) and "temp" paths. All intermediary
                                files are first downloaded to the temp path
                                and then the final files are moved over to
                                the home path after download is finished.
                                This option is ignored if --output is an
                                absolute path
-o, --output [TYPES:]TEMPLATE   Output filename template; see "OUTPUT
                                TEMPLATE" for details
--output-na-placeholder TEXT    Placeholder for unavailable fields in
                                "OUTPUT TEMPLATE" (default: "NA")
--restrict-filenames            Restrict filenames to only ASCII characters,
                                and avoid "&amp;" and spaces in filenames
--no-restrict-filenames         Allow Unicode characters, "&amp;" and spaces in
                                filenames (default)
--windows-filenames             Force filenames to be Windows-compatible
--no-windows-filenames          Make filenames Windows-compatible only if
                                using Windows (default)
--trim-filenames LENGTH         Limit the filename length (excluding
                                extension) to the specified number of
                                characters
-w, --no-overwrites             Do not overwrite any files
--force-overwrites              Overwrite all video and metadata files. This
                                option includes --no-continue
--no-force-overwrites           Do not overwrite the video, but overwrite
                                related files (default)
-c, --continue                  Resume partially downloaded files/fragments
                                (default)
--no-continue                   Do not resume partially downloaded
                                fragments. If the file is not fragmented,
                                restart download of the entire file
--part                          Use .part files instead of writing directly
                                into output file (default)
--no-part                       Do not use .part files - write directly into
                                output file
--mtime                         Use the Last-modified header to set the file
                                modification time (default)
--no-mtime                      Do not use the Last-modified header to set
                                the file modification time
--write-description             Write video description to a .description file
--no-write-description          Do not write video description (default)
--write-info-json               Write video metadata to a .info.json file
                                (this may contain personal information)
--no-write-info-json            Do not write video metadata (default)
--write-playlist-metafiles      Write playlist metadata in addition to the
                                video metadata when using --write-info-json,
                                --write-description etc. (default)
--no-write-playlist-metafiles   Do not write playlist metadata when using
                                --write-info-json, --write-description etc.
--clean-info-json               Remove some internal metadata such as
                                filenames from the infojson (default)
--no-clean-info-json            Write all fields to the infojson
--write-comments                Retrieve video comments to be placed in the
                                infojson. The comments are fetched even
                                without this option if the extraction is
                                known to be quick (Alias: --get-comments)
--no-write-comments             Do not retrieve video comments unless the
                                extraction is known to be quick (Alias:
                                --no-get-comments)
--load-info-json FILE           JSON file containing the video information
                                (created with the "--write-info-json" option)
--cookies FILE                  Netscape formatted file to read cookies from
                                and dump cookie jar in
--no-cookies                    Do not read/dump cookies from/to file
                                (default)
--cookies-from-browser BROWSER[+KEYRING][:PROFILE][::CONTAINER]
                                The name of the browser to load cookies
                                from. Currently supported browsers are:
                                brave, chrome, chromium, edge, firefox,
                                opera, safari, vivaldi. Optionally, the
                                KEYRING used for decrypting Chromium cookies
                                on Linux, the name/path of the PROFILE to
                                load cookies from, and the CONTAINER name
                                (if Firefox) ("none" for no container) can
                                be given with their respective seperators.
                                By default, all containers of the most
                                recently accessed profile are used.
                                Currently supported keyrings are: basictext,
                                gnomekeyring, kwallet, kwallet5, kwallet6
--no-cookies-from-browser       Do not load cookies from browser (default)
--cache-dir DIR                 Location in the filesystem where yt-dlp can
                                store some downloaded information (such as
                                client ids and signatures) permanently. By
                                default ${XDG_CACHE_HOME}/yt-dlp
--no-cache-dir                  Disable filesystem caching
--rm-cache-dir                  Delete all filesystem cache files
</code></pre></div>
<h2 tabindex="-1" dir="auto">Thumbnail Options:</h2>
<div data-snippet-clipboard-copy-content="--write-thumbnail               Write thumbnail image to disk
--no-write-thumbnail            Do not write thumbnail image to disk (default)
--write-all-thumbnails          Write all thumbnail image formats to disk
--list-thumbnails               List available thumbnails of each video.
                                Simulate unless --no-simulate is used"><pre><code>--write-thumbnail               Write thumbnail image to disk
--no-write-thumbnail            Do not write thumbnail image to disk (default)
--write-all-thumbnails          Write all thumbnail image formats to disk
--list-thumbnails               List available thumbnails of each video.
                                Simulate unless --no-simulate is used
</code></pre></div>
<h2 tabindex="-1" dir="auto">Internet Shortcut Options:</h2>
<div data-snippet-clipboard-copy-content="--write-link                    Write an internet shortcut file, depending
                                on the current platform (.url, .webloc or
                                .desktop). The URL may be cached by the OS
--write-url-link                Write a .url Windows internet shortcut. The
                                OS caches the URL based on the file path
--write-webloc-link             Write a .webloc macOS internet shortcut
--write-desktop-link            Write a .desktop Linux internet shortcut"><pre><code>--write-link                    Write an internet shortcut file, depending
                                on the current platform (.url, .webloc or
                                .desktop). The URL may be cached by the OS
--write-url-link                Write a .url Windows internet shortcut. The
                                OS caches the URL based on the file path
--write-webloc-link             Write a .webloc macOS internet shortcut
--write-desktop-link            Write a .desktop Linux internet shortcut
</code></pre></div>
<h2 tabindex="-1" dir="auto">Verbosity and Simulation Options:</h2>
<div data-snippet-clipboard-copy-content="-q, --quiet                     Activate quiet mode. If used with --verbose,
                                print the log to stderr
--no-quiet                      Deactivate quiet mode. (Default)
--no-warnings                   Ignore warnings
-s, --simulate                  Do not download the video and do not write
                                anything to disk
--no-simulate                   Download the video even if printing/listing
                                options are used
--ignore-no-formats-error       Ignore &quot;No video formats&quot; error. Useful for
                                extracting metadata even if the videos are
                                not actually available for download
                                (experimental)
--no-ignore-no-formats-error    Throw error when no downloadable video
                                formats are found (default)
--skip-download                 Do not download the video but write all
                                related files (Alias: --no-download)
-O, --print [WHEN:]TEMPLATE     Field name or output template to print to
                                screen, optionally prefixed with when to
                                print it, separated by a &quot;:&quot;. Supported
                                values of &quot;WHEN&quot; are the same as that of
                                --use-postprocessor (default: video).
                                Implies --quiet. Implies --simulate unless
                                --no-simulate or later stages of WHEN are
                                used. This option can be used multiple times
--print-to-file [WHEN:]TEMPLATE FILE
                                Append given template to the file. The
                                values of WHEN and TEMPLATE are same as that
                                of --print. FILE uses the same syntax as the
                                output template. This option can be used
                                multiple times
-j, --dump-json                 Quiet, but print JSON information for each
                                video. Simulate unless --no-simulate is
                                used. See &quot;OUTPUT TEMPLATE&quot; for a
                                description of available keys
-J, --dump-single-json          Quiet, but print JSON information for each
                                url or infojson passed. Simulate unless
                                --no-simulate is used. If the URL refers to
                                a playlist, the whole playlist information
                                is dumped in a single line
--force-write-archive           Force download archive entries to be written
                                as far as no errors occur, even if -s or
                                another simulation option is used (Alias:
                                --force-download-archive)
--newline                       Output progress bar as new lines
--no-progress                   Do not print progress bar
--progress                      Show progress bar, even if in quiet mode
--console-title                 Display progress in console titlebar
--progress-template [TYPES:]TEMPLATE
                                Template for progress outputs, optionally
                                prefixed with one of &quot;download:&quot; (default),
                                &quot;download-title:&quot; (the console title),
                                &quot;postprocess:&quot;,  or &quot;postprocess-title:&quot;.
                                The video's fields are accessible under the
                                &quot;info&quot; key and the progress attributes are
                                accessible under &quot;progress&quot; key. E.g.
                                --console-title --progress-template
                                &quot;download-title:%(info.id)s-%(progress.eta)s&quot;
-v, --verbose                   Print various debugging information
--dump-pages                    Print downloaded pages encoded using base64
                                to debug problems (very verbose)
--write-pages                   Write downloaded intermediary pages to files
                                in the current directory to debug problems
--print-traffic                 Display sent and read HTTP traffic"><pre><code>-q, --quiet                     Activate quiet mode. If used with --verbose,
                                print the log to stderr
--no-quiet                      Deactivate quiet mode. (Default)
--no-warnings                   Ignore warnings
-s, --simulate                  Do not download the video and do not write
                                anything to disk
--no-simulate                   Download the video even if printing/listing
                                options are used
--ignore-no-formats-error       Ignore "No video formats" error. Useful for
                                extracting metadata even if the videos are
                                not actually available for download
                                (experimental)
--no-ignore-no-formats-error    Throw error when no downloadable video
                                formats are found (default)
--skip-download                 Do not download the video but write all
                                related files (Alias: --no-download)
-O, --print [WHEN:]TEMPLATE     Field name or output template to print to
                                screen, optionally prefixed with when to
                                print it, separated by a ":". Supported
                                values of "WHEN" are the same as that of
                                --use-postprocessor (default: video).
                                Implies --quiet. Implies --simulate unless
                                --no-simulate or later stages of WHEN are
                                used. This option can be used multiple times
--print-to-file [WHEN:]TEMPLATE FILE
                                Append given template to the file. The
                                values of WHEN and TEMPLATE are same as that
                                of --print. FILE uses the same syntax as the
                                output template. This option can be used
                                multiple times
-j, --dump-json                 Quiet, but print JSON information for each
                                video. Simulate unless --no-simulate is
                                used. See "OUTPUT TEMPLATE" for a
                                description of available keys
-J, --dump-single-json          Quiet, but print JSON information for each
                                url or infojson passed. Simulate unless
                                --no-simulate is used. If the URL refers to
                                a playlist, the whole playlist information
                                is dumped in a single line
--force-write-archive           Force download archive entries to be written
                                as far as no errors occur, even if -s or
                                another simulation option is used (Alias:
                                --force-download-archive)
--newline                       Output progress bar as new lines
--no-progress                   Do not print progress bar
--progress                      Show progress bar, even if in quiet mode
--console-title                 Display progress in console titlebar
--progress-template [TYPES:]TEMPLATE
                                Template for progress outputs, optionally
                                prefixed with one of "download:" (default),
                                "download-title:" (the console title),
                                "postprocess:",  or "postprocess-title:".
                                The video's fields are accessible under the
                                "info" key and the progress attributes are
                                accessible under "progress" key. E.g.
                                --console-title --progress-template
                                "download-title:%(info.id)s-%(progress.eta)s"
-v, --verbose                   Print various debugging information
--dump-pages                    Print downloaded pages encoded using base64
                                to debug problems (very verbose)
--write-pages                   Write downloaded intermediary pages to files
                                in the current directory to debug problems
--print-traffic                 Display sent and read HTTP traffic
</code></pre></div>
<h2 tabindex="-1" dir="auto">Workarounds:</h2>
<div data-snippet-clipboard-copy-content="--encoding ENCODING             Force the specified encoding (experimental)
--legacy-server-connect         Explicitly allow HTTPS connection to servers
                                that do not support RFC 5746 secure
                                renegotiation
--no-check-certificates         Suppress HTTPS certificate validation
--prefer-insecure               Use an unencrypted connection to retrieve
                                information about the video (Currently
                                supported only for YouTube)
--add-headers FIELD:VALUE       Specify a custom HTTP header and its value,
                                separated by a colon &quot;:&quot;. You can use this
                                option multiple times
--bidi-workaround               Work around terminals that lack
                                bidirectional text support. Requires bidiv
                                or fribidi executable in PATH
--sleep-requests SECONDS        Number of seconds to sleep between requests
                                during data extraction
--sleep-interval SECONDS        Number of seconds to sleep before each
                                download. This is the minimum time to sleep
                                when used along with --max-sleep-interval
                                (Alias: --min-sleep-interval)
--max-sleep-interval SECONDS    Maximum number of seconds to sleep. Can only
                                be used along with --min-sleep-interval
--sleep-subtitles SECONDS       Number of seconds to sleep before each
                                subtitle download"><pre><code>--encoding ENCODING             Force the specified encoding (experimental)
--legacy-server-connect         Explicitly allow HTTPS connection to servers
                                that do not support RFC 5746 secure
                                renegotiation
--no-check-certificates         Suppress HTTPS certificate validation
--prefer-insecure               Use an unencrypted connection to retrieve
                                information about the video (Currently
                                supported only for YouTube)
--add-headers FIELD:VALUE       Specify a custom HTTP header and its value,
                                separated by a colon ":". You can use this
                                option multiple times
--bidi-workaround               Work around terminals that lack
                                bidirectional text support. Requires bidiv
                                or fribidi executable in PATH
--sleep-requests SECONDS        Number of seconds to sleep between requests
                                during data extraction
--sleep-interval SECONDS        Number of seconds to sleep before each
                                download. This is the minimum time to sleep
                                when used along with --max-sleep-interval
                                (Alias: --min-sleep-interval)
--max-sleep-interval SECONDS    Maximum number of seconds to sleep. Can only
                                be used along with --min-sleep-interval
--sleep-subtitles SECONDS       Number of seconds to sleep before each
                                subtitle download
</code></pre></div>
<h2 tabindex="-1" dir="auto">Video Format Options:</h2>
<div data-snippet-clipboard-copy-content="-f, --format FORMAT             Video format code, see &quot;FORMAT SELECTION&quot;
                                for more details
-S, --format-sort SORTORDER     Sort the formats by the fields given, see
                                &quot;Sorting Formats&quot; for more details
--format-sort-force             Force user specified sort order to have
                                precedence over all fields, see &quot;Sorting
                                Formats&quot; for more details (Alias: --S-force)
--no-format-sort-force          Some fields have precedence over the user
                                specified sort order (default)
--video-multistreams            Allow multiple video streams to be merged
                                into a single file
--no-video-multistreams         Only one video stream is downloaded for each
                                output file (default)
--audio-multistreams            Allow multiple audio streams to be merged
                                into a single file
--no-audio-multistreams         Only one audio stream is downloaded for each
                                output file (default)
--prefer-free-formats           Prefer video formats with free containers
                                over non-free ones of same quality. Use with
                                &quot;-S ext&quot; to strictly prefer free containers
                                irrespective of quality
--no-prefer-free-formats        Don't give any special preference to free
                                containers (default)
--check-formats                 Make sure formats are selected only from
                                those that are actually downloadable
--check-all-formats             Check all formats for whether they are
                                actually downloadable
--no-check-formats              Do not check that the formats are actually
                                downloadable
-F, --list-formats              List available formats of each video.
                                Simulate unless --no-simulate is used
--merge-output-format FORMAT    Containers that may be used when merging
                                formats, separated by &quot;/&quot;, e.g. &quot;mp4/mkv&quot;.
                                Ignored if no merge is required. (currently
                                supported: avi, flv, mkv, mov, mp4, webm)"><pre><code>-f, --format FORMAT             Video format code, see "FORMAT SELECTION"
                                for more details
-S, --format-sort SORTORDER     Sort the formats by the fields given, see
                                "Sorting Formats" for more details
--format-sort-force             Force user specified sort order to have
                                precedence over all fields, see "Sorting
                                Formats" for more details (Alias: --S-force)
--no-format-sort-force          Some fields have precedence over the user
                                specified sort order (default)
--video-multistreams            Allow multiple video streams to be merged
                                into a single file
--no-video-multistreams         Only one video stream is downloaded for each
                                output file (default)
--audio-multistreams            Allow multiple audio streams to be merged
                                into a single file
--no-audio-multistreams         Only one audio stream is downloaded for each
                                output file (default)
--prefer-free-formats           Prefer video formats with free containers
                                over non-free ones of same quality. Use with
                                "-S ext" to strictly prefer free containers
                                irrespective of quality
--no-prefer-free-formats        Don't give any special preference to free
                                containers (default)
--check-formats                 Make sure formats are selected only from
                                those that are actually downloadable
--check-all-formats             Check all formats for whether they are
                                actually downloadable
--no-check-formats              Do not check that the formats are actually
                                downloadable
-F, --list-formats              List available formats of each video.
                                Simulate unless --no-simulate is used
--merge-output-format FORMAT    Containers that may be used when merging
                                formats, separated by "/", e.g. "mp4/mkv".
                                Ignored if no merge is required. (currently
                                supported: avi, flv, mkv, mov, mp4, webm)
</code></pre></div>
<h2 tabindex="-1" dir="auto">Subtitle Options:</h2>
<div data-snippet-clipboard-copy-content="--write-subs                    Write subtitle file
--no-write-subs                 Do not write subtitle file (default)
--write-auto-subs               Write automatically generated subtitle file
                                (Alias: --write-automatic-subs)
--no-write-auto-subs            Do not write auto-generated subtitles
                                (default) (Alias: --no-write-automatic-subs)
--list-subs                     List available subtitles of each video.
                                Simulate unless --no-simulate is used
--sub-format FORMAT             Subtitle format; accepts formats preference,
                                e.g. &quot;srt&quot; or &quot;ass/srt/best&quot;
--sub-langs LANGS               Languages of the subtitles to download (can
                                be regex) or &quot;all&quot; separated by commas, e.g.
                                --sub-langs &quot;en.*,ja&quot;. You can prefix the
                                language code with a &quot;-&quot; to exclude it from
                                the requested languages, e.g. --sub-langs
                                all,-live_chat. Use --list-subs for a list
                                of available language tags"><pre><code>--write-subs                    Write subtitle file
--no-write-subs                 Do not write subtitle file (default)
--write-auto-subs               Write automatically generated subtitle file
                                (Alias: --write-automatic-subs)
--no-write-auto-subs            Do not write auto-generated subtitles
                                (default) (Alias: --no-write-automatic-subs)
--list-subs                     List available subtitles of each video.
                                Simulate unless --no-simulate is used
--sub-format FORMAT             Subtitle format; accepts formats preference,
                                e.g. "srt" or "ass/srt/best"
--sub-langs LANGS               Languages of the subtitles to download (can
                                be regex) or "all" separated by commas, e.g.
                                --sub-langs "en.*,ja". You can prefix the
                                language code with a "-" to exclude it from
                                the requested languages, e.g. --sub-langs
                                all,-live_chat. Use --list-subs for a list
                                of available language tags
</code></pre></div>
<h2 tabindex="-1" dir="auto">Authentication Options:</h2>
<div data-snippet-clipboard-copy-content="-u, --username USERNAME         Login with this account ID
-p, --password PASSWORD         Account password. If this option is left
                                out, yt-dlp will ask interactively
-2, --twofactor TWOFACTOR       Two-factor authentication code
-n, --netrc                     Use .netrc authentication data
--netrc-location PATH           Location of .netrc authentication data;
                                either the path or its containing directory.
                                Defaults to ~/.netrc
--netrc-cmd NETRC_CMD           Command to execute to get the credentials
                                for an extractor.
--video-password PASSWORD       Video password (vimeo, youku)
--ap-mso MSO                    Adobe Pass multiple-system operator (TV
                                provider) identifier, use --ap-list-mso for
                                a list of available MSOs
--ap-username USERNAME          Multiple-system operator account login
--ap-password PASSWORD          Multiple-system operator account password.
                                If this option is left out, yt-dlp will ask
                                interactively
--ap-list-mso                   List all supported multiple-system operators
--client-certificate CERTFILE   Path to client certificate file in PEM
                                format. May include the private key
--client-certificate-key KEYFILE
                                Path to private key file for client
                                certificate
--client-certificate-password PASSWORD
                                Password for client certificate private key,
                                if encrypted. If not provided, and the key
                                is encrypted, yt-dlp will ask interactively"><pre><code>-u, --username USERNAME         Login with this account ID
-p, --password PASSWORD         Account password. If this option is left
                                out, yt-dlp will ask interactively
-2, --twofactor TWOFACTOR       Two-factor authentication code
-n, --netrc                     Use .netrc authentication data
--netrc-location PATH           Location of .netrc authentication data;
                                either the path or its containing directory.
                                Defaults to ~/.netrc
--netrc-cmd NETRC_CMD           Command to execute to get the credentials
                                for an extractor.
--video-password PASSWORD       Video password (vimeo, youku)
--ap-mso MSO                    Adobe Pass multiple-system operator (TV
                                provider) identifier, use --ap-list-mso for
                                a list of available MSOs
--ap-username USERNAME          Multiple-system operator account login
--ap-password PASSWORD          Multiple-system operator account password.
                                If this option is left out, yt-dlp will ask
                                interactively
--ap-list-mso                   List all supported multiple-system operators
--client-certificate CERTFILE   Path to client certificate file in PEM
                                format. May include the private key
--client-certificate-key KEYFILE
                                Path to private key file for client
                                certificate
--client-certificate-password PASSWORD
                                Password for client certificate private key,
                                if encrypted. If not provided, and the key
                                is encrypted, yt-dlp will ask interactively
</code></pre></div>
<h2 tabindex="-1" dir="auto">Post-Processing Options:</h2>
<div data-snippet-clipboard-copy-content="-x, --extract-audio             Convert video files to audio-only files
                                (requires ffmpeg and ffprobe)
--audio-format FORMAT           Format to convert the audio to when -x is
                                used. (currently supported: best (default),
                                aac, alac, flac, m4a, mp3, opus, vorbis,
                                wav). You can specify multiple rules using
                                similar syntax as --remux-video
--audio-quality QUALITY         Specify ffmpeg audio quality to use when
                                converting the audio with -x. Insert a value
                                between 0 (best) and 10 (worst) for VBR or a
                                specific bitrate like 128K (default 5)
--remux-video FORMAT            Remux the video into another container if
                                necessary (currently supported: avi, flv,
                                gif, mkv, mov, mp4, webm, aac, aiff, alac,
                                flac, m4a, mka, mp3, ogg, opus, vorbis,
                                wav). If target container does not support
                                the video/audio codec, remuxing will fail.
                                You can specify multiple rules; e.g.
                                &quot;aac>m4a/mov>mp4/mkv&quot; will remux aac to m4a,
                                mov to mp4 and anything else to mkv
--recode-video FORMAT           Re-encode the video into another format if
                                necessary. The syntax and supported formats
                                are the same as --remux-video
--postprocessor-args NAME:ARGS  Give these arguments to the postprocessors.
                                Specify the postprocessor/executable name
                                and the arguments separated by a colon &quot;:&quot;
                                to give the argument to the specified
                                postprocessor/executable. Supported PP are:
                                Merger, ModifyChapters, SplitChapters,
                                ExtractAudio, VideoRemuxer, VideoConvertor,
                                Metadata, EmbedSubtitle, EmbedThumbnail,
                                SubtitlesConvertor, ThumbnailsConvertor,
                                FixupStretched, FixupM4a, FixupM3u8,
                                FixupTimestamp and FixupDuration. The
                                supported executables are: AtomicParsley,
                                FFmpeg and FFprobe. You can also specify
                                &quot;PP+EXE:ARGS&quot; to give the arguments to the
                                specified executable only when being used by
                                the specified postprocessor. Additionally,
                                for ffmpeg/ffprobe, &quot;_i&quot;/&quot;_o&quot; can be
                                appended to the prefix optionally followed
                                by a number to pass the argument before the
                                specified input/output file, e.g. --ppa
                                &quot;Merger+ffmpeg_i1:-v quiet&quot;. You can use
                                this option multiple times to give different
                                arguments to different postprocessors.
                                (Alias: --ppa)
-k, --keep-video                Keep the intermediate video file on disk
                                after post-processing
--no-keep-video                 Delete the intermediate video file after
                                post-processing (default)
--post-overwrites               Overwrite post-processed files (default)
--no-post-overwrites            Do not overwrite post-processed files
--embed-subs                    Embed subtitles in the video (only for mp4,
                                webm and mkv videos)
--no-embed-subs                 Do not embed subtitles (default)
--embed-thumbnail               Embed thumbnail in the video as cover art
--no-embed-thumbnail            Do not embed thumbnail (default)
--embed-metadata                Embed metadata to the video file. Also
                                embeds chapters/infojson if present unless
                                --no-embed-chapters/--no-embed-info-json are
                                used (Alias: --add-metadata)
--no-embed-metadata             Do not add metadata to file (default)
                                (Alias: --no-add-metadata)
--embed-chapters                Add chapter markers to the video file
                                (Alias: --add-chapters)
--no-embed-chapters             Do not add chapter markers (default) (Alias:
                                --no-add-chapters)
--embed-info-json               Embed the infojson as an attachment to
                                mkv/mka video files
--no-embed-info-json            Do not embed the infojson as an attachment
                                to the video file
--parse-metadata [WHEN:]FROM:TO
                                Parse additional metadata like title/artist
                                from other fields; see &quot;MODIFYING METADATA&quot;
                                for details. Supported values of &quot;WHEN&quot; are
                                the same as that of --use-postprocessor
                                (default: pre_process)
--replace-in-metadata [WHEN:]FIELDS REGEX REPLACE
                                Replace text in a metadata field using the
                                given regex. This option can be used
                                multiple times. Supported values of &quot;WHEN&quot;
                                are the same as that of --use-postprocessor
                                (default: pre_process)
--xattrs                        Write metadata to the video file's xattrs
                                (using dublin core and xdg standards)
--concat-playlist POLICY        Concatenate videos in a playlist. One of
                                &quot;never&quot;, &quot;always&quot;, or &quot;multi_video&quot;
                                (default; only when the videos form a single
                                show). All the video files must have same
                                codecs and number of streams to be
                                concatable. The &quot;pl_video:&quot; prefix can be
                                used with &quot;--paths&quot; and &quot;--output&quot; to set
                                the output filename for the concatenated
                                files. See &quot;OUTPUT TEMPLATE&quot; for details
--fixup POLICY                  Automatically correct known faults of the
                                file. One of never (do nothing), warn (only
                                emit a warning), detect_or_warn (the
                                default; fix file if we can, warn
                                otherwise), force (try fixing even if file
                                already exists)
--ffmpeg-location PATH          Location of the ffmpeg binary; either the
                                path to the binary or its containing directory
--exec [WHEN:]CMD               Execute a command, optionally prefixed with
                                when to execute it, separated by a &quot;:&quot;.
                                Supported values of &quot;WHEN&quot; are the same as
                                that of --use-postprocessor (default:
                                after_move). Same syntax as the output
                                template can be used to pass any field as
                                arguments to the command. If no fields are
                                passed, %(filepath,_filename|)q is appended
                                to the end of the command. This option can
                                be used multiple times
--no-exec                       Remove any previously defined --exec
--convert-subs FORMAT           Convert the subtitles to another format
                                (currently supported: ass, lrc, srt, vtt)
                                (Alias: --convert-subtitles)
--convert-thumbnails FORMAT     Convert the thumbnails to another format
                                (currently supported: jpg, png, webp). You
                                can specify multiple rules using similar
                                syntax as --remux-video
--split-chapters                Split video into multiple files based on
                                internal chapters. The &quot;chapter:&quot; prefix can
                                be used with &quot;--paths&quot; and &quot;--output&quot; to set
                                the output filename for the split files. See
                                &quot;OUTPUT TEMPLATE&quot; for details
--no-split-chapters             Do not split video based on chapters (default)
--remove-chapters REGEX         Remove chapters whose title matches the
                                given regular expression. The syntax is the
                                same as --download-sections. This option can
                                be used multiple times
--no-remove-chapters            Do not remove any chapters from the file
                                (default)
--force-keyframes-at-cuts       Force keyframes at cuts when
                                downloading/splitting/removing sections.
                                This is slow due to needing a re-encode, but
                                the resulting video may have fewer artifacts
                                around the cuts
--no-force-keyframes-at-cuts    Do not force keyframes around the chapters
                                when cutting/splitting (default)
--use-postprocessor NAME[:ARGS]
                                The (case sensitive) name of plugin
                                postprocessors to be enabled, and
                                (optionally) arguments to be passed to it,
                                separated by a colon &quot;:&quot;. ARGS are a
                                semicolon &quot;;&quot; delimited list of NAME=VALUE.
                                The &quot;when&quot; argument determines when the
                                postprocessor is invoked. It can be one of
                                &quot;pre_process&quot; (after video extraction),
                                &quot;after_filter&quot; (after video passes filter),
                                &quot;video&quot; (after --format; before
                                --print/--output), &quot;before_dl&quot; (before each
                                video download), &quot;post_process&quot; (after each
                                video download; default), &quot;after_move&quot;
                                (after moving video file to it's final
                                locations), &quot;after_video&quot; (after downloading
                                and processing all formats of a video), or
                                &quot;playlist&quot; (at end of playlist). This option
                                can be used multiple times to add different
                                postprocessors"><pre><code>-x, --extract-audio             Convert video files to audio-only files
                                (requires ffmpeg and ffprobe)
--audio-format FORMAT           Format to convert the audio to when -x is
                                used. (currently supported: best (default),
                                aac, alac, flac, m4a, mp3, opus, vorbis,
                                wav). You can specify multiple rules using
                                similar syntax as --remux-video
--audio-quality QUALITY         Specify ffmpeg audio quality to use when
                                converting the audio with -x. Insert a value
                                between 0 (best) and 10 (worst) for VBR or a
                                specific bitrate like 128K (default 5)
--remux-video FORMAT            Remux the video into another container if
                                necessary (currently supported: avi, flv,
                                gif, mkv, mov, mp4, webm, aac, aiff, alac,
                                flac, m4a, mka, mp3, ogg, opus, vorbis,
                                wav). If target container does not support
                                the video/audio codec, remuxing will fail.
                                You can specify multiple rules; e.g.
                                "aac&gt;m4a/mov&gt;mp4/mkv" will remux aac to m4a,
                                mov to mp4 and anything else to mkv
--recode-video FORMAT           Re-encode the video into another format if
                                necessary. The syntax and supported formats
                                are the same as --remux-video
--postprocessor-args NAME:ARGS  Give these arguments to the postprocessors.
                                Specify the postprocessor/executable name
                                and the arguments separated by a colon ":"
                                to give the argument to the specified
                                postprocessor/executable. Supported PP are:
                                Merger, ModifyChapters, SplitChapters,
                                ExtractAudio, VideoRemuxer, VideoConvertor,
                                Metadata, EmbedSubtitle, EmbedThumbnail,
                                SubtitlesConvertor, ThumbnailsConvertor,
                                FixupStretched, FixupM4a, FixupM3u8,
                                FixupTimestamp and FixupDuration. The
                                supported executables are: AtomicParsley,
                                FFmpeg and FFprobe. You can also specify
                                "PP+EXE:ARGS" to give the arguments to the
                                specified executable only when being used by
                                the specified postprocessor. Additionally,
                                for ffmpeg/ffprobe, "_i"/"_o" can be
                                appended to the prefix optionally followed
                                by a number to pass the argument before the
                                specified input/output file, e.g. --ppa
                                "Merger+ffmpeg_i1:-v quiet". You can use
                                this option multiple times to give different
                                arguments to different postprocessors.
                                (Alias: --ppa)
-k, --keep-video                Keep the intermediate video file on disk
                                after post-processing
--no-keep-video                 Delete the intermediate video file after
                                post-processing (default)
--post-overwrites               Overwrite post-processed files (default)
--no-post-overwrites            Do not overwrite post-processed files
--embed-subs                    Embed subtitles in the video (only for mp4,
                                webm and mkv videos)
--no-embed-subs                 Do not embed subtitles (default)
--embed-thumbnail               Embed thumbnail in the video as cover art
--no-embed-thumbnail            Do not embed thumbnail (default)
--embed-metadata                Embed metadata to the video file. Also
                                embeds chapters/infojson if present unless
                                --no-embed-chapters/--no-embed-info-json are
                                used (Alias: --add-metadata)
--no-embed-metadata             Do not add metadata to file (default)
                                (Alias: --no-add-metadata)
--embed-chapters                Add chapter markers to the video file
                                (Alias: --add-chapters)
--no-embed-chapters             Do not add chapter markers (default) (Alias:
                                --no-add-chapters)
--embed-info-json               Embed the infojson as an attachment to
                                mkv/mka video files
--no-embed-info-json            Do not embed the infojson as an attachment
                                to the video file
--parse-metadata [WHEN:]FROM:TO
                                Parse additional metadata like title/artist
                                from other fields; see "MODIFYING METADATA"
                                for details. Supported values of "WHEN" are
                                the same as that of --use-postprocessor
                                (default: pre_process)
--replace-in-metadata [WHEN:]FIELDS REGEX REPLACE
                                Replace text in a metadata field using the
                                given regex. This option can be used
                                multiple times. Supported values of "WHEN"
                                are the same as that of --use-postprocessor
                                (default: pre_process)
--xattrs                        Write metadata to the video file's xattrs
                                (using dublin core and xdg standards)
--concat-playlist POLICY        Concatenate videos in a playlist. One of
                                "never", "always", or "multi_video"
                                (default; only when the videos form a single
                                show). All the video files must have same
                                codecs and number of streams to be
                                concatable. The "pl_video:" prefix can be
                                used with "--paths" and "--output" to set
                                the output filename for the concatenated
                                files. See "OUTPUT TEMPLATE" for details
--fixup POLICY                  Automatically correct known faults of the
                                file. One of never (do nothing), warn (only
                                emit a warning), detect_or_warn (the
                                default; fix file if we can, warn
                                otherwise), force (try fixing even if file
                                already exists)
--ffmpeg-location PATH          Location of the ffmpeg binary; either the
                                path to the binary or its containing directory
--exec [WHEN:]CMD               Execute a command, optionally prefixed with
                                when to execute it, separated by a ":".
                                Supported values of "WHEN" are the same as
                                that of --use-postprocessor (default:
                                after_move). Same syntax as the output
                                template can be used to pass any field as
                                arguments to the command. If no fields are
                                passed, %(filepath,_filename|)q is appended
                                to the end of the command. This option can
                                be used multiple times
--no-exec                       Remove any previously defined --exec
--convert-subs FORMAT           Convert the subtitles to another format
                                (currently supported: ass, lrc, srt, vtt)
                                (Alias: --convert-subtitles)
--convert-thumbnails FORMAT     Convert the thumbnails to another format
                                (currently supported: jpg, png, webp). You
                                can specify multiple rules using similar
                                syntax as --remux-video
--split-chapters                Split video into multiple files based on
                                internal chapters. The "chapter:" prefix can
                                be used with "--paths" and "--output" to set
                                the output filename for the split files. See
                                "OUTPUT TEMPLATE" for details
--no-split-chapters             Do not split video based on chapters (default)
--remove-chapters REGEX         Remove chapters whose title matches the
                                given regular expression. The syntax is the
                                same as --download-sections. This option can
                                be used multiple times
--no-remove-chapters            Do not remove any chapters from the file
                                (default)
--force-keyframes-at-cuts       Force keyframes at cuts when
                                downloading/splitting/removing sections.
                                This is slow due to needing a re-encode, but
                                the resulting video may have fewer artifacts
                                around the cuts
--no-force-keyframes-at-cuts    Do not force keyframes around the chapters
                                when cutting/splitting (default)
--use-postprocessor NAME[:ARGS]
                                The (case sensitive) name of plugin
                                postprocessors to be enabled, and
                                (optionally) arguments to be passed to it,
                                separated by a colon ":". ARGS are a
                                semicolon ";" delimited list of NAME=VALUE.
                                The "when" argument determines when the
                                postprocessor is invoked. It can be one of
                                "pre_process" (after video extraction),
                                "after_filter" (after video passes filter),
                                "video" (after --format; before
                                --print/--output), "before_dl" (before each
                                video download), "post_process" (after each
                                video download; default), "after_move"
                                (after moving video file to it's final
                                locations), "after_video" (after downloading
                                and processing all formats of a video), or
                                "playlist" (at end of playlist). This option
                                can be used multiple times to add different
                                postprocessors
</code></pre></div>
<h2 tabindex="-1" dir="auto">SponsorBlock Options:</h2>
<p dir="auto">Make chapter entries for, or remove various segments (sponsor,
introductions, etc.) from downloaded YouTube videos using the
<a href="https://sponsor.ajay.app/" rel="nofollow">SponsorBlock API</a></p>
<div data-snippet-clipboard-copy-content="--sponsorblock-mark CATS        SponsorBlock categories to create chapters
                                for, separated by commas. Available
                                categories are sponsor, intro, outro,
                                selfpromo, preview, filler, interaction,
                                music_offtopic, poi_highlight, chapter, all
                                and default (=all). You can prefix the
                                category with a &quot;-&quot; to exclude it. See [1]
                                for description of the categories. E.g.
                                --sponsorblock-mark all,-preview
                                [1] https://wiki.sponsor.ajay.app/w/Segment_Categories
--sponsorblock-remove CATS      SponsorBlock categories to be removed from
                                the video file, separated by commas. If a
                                category is present in both mark and remove,
                                remove takes precedence. The syntax and
                                available categories are the same as for
                                --sponsorblock-mark except that &quot;default&quot;
                                refers to &quot;all,-filler&quot; and poi_highlight,
                                chapter are not available
--sponsorblock-chapter-title TEMPLATE
                                An output template for the title of the
                                SponsorBlock chapters created by
                                --sponsorblock-mark. The only available
                                fields are start_time, end_time, category,
                                categories, name, category_names. Defaults
                                to &quot;[SponsorBlock]: %(category_names)l&quot;
--no-sponsorblock               Disable both --sponsorblock-mark and
                                --sponsorblock-remove
--sponsorblock-api URL          SponsorBlock API location, defaults to
                                https://sponsor.ajay.app"><pre><code>--sponsorblock-mark CATS        SponsorBlock categories to create chapters
                                for, separated by commas. Available
                                categories are sponsor, intro, outro,
                                selfpromo, preview, filler, interaction,
                                music_offtopic, poi_highlight, chapter, all
                                and default (=all). You can prefix the
                                category with a "-" to exclude it. See [1]
                                for description of the categories. E.g.
                                --sponsorblock-mark all,-preview
                                [1] https://wiki.sponsor.ajay.app/w/Segment_Categories
--sponsorblock-remove CATS      SponsorBlock categories to be removed from
                                the video file, separated by commas. If a
                                category is present in both mark and remove,
                                remove takes precedence. The syntax and
                                available categories are the same as for
                                --sponsorblock-mark except that "default"
                                refers to "all,-filler" and poi_highlight,
                                chapter are not available
--sponsorblock-chapter-title TEMPLATE
                                An output template for the title of the
                                SponsorBlock chapters created by
                                --sponsorblock-mark. The only available
                                fields are start_time, end_time, category,
                                categories, name, category_names. Defaults
                                to "[SponsorBlock]: %(category_names)l"
--no-sponsorblock               Disable both --sponsorblock-mark and
                                --sponsorblock-remove
--sponsorblock-api URL          SponsorBlock API location, defaults to
                                https://sponsor.ajay.app
</code></pre></div>
<h2 tabindex="-1" dir="auto">Extractor Options:</h2>
<div data-snippet-clipboard-copy-content="--extractor-retries RETRIES     Number of retries for known extractor errors
                                (default is 3), or &quot;infinite&quot;
--allow-dynamic-mpd             Process dynamic DASH manifests (default)
                                (Alias: --no-ignore-dynamic-mpd)
--ignore-dynamic-mpd            Do not process dynamic DASH manifests
                                (Alias: --no-allow-dynamic-mpd)
--hls-split-discontinuity       Split HLS playlists to different formats at
                                discontinuities such as ad breaks
--no-hls-split-discontinuity    Do not split HLS playlists to different
                                formats at discontinuities such as ad breaks
                                (default)
--extractor-args IE_KEY:ARGS    Pass ARGS arguments to the IE_KEY extractor.
                                See &quot;EXTRACTOR ARGUMENTS&quot; for details. You
                                can use this option multiple times to give
                                arguments for different extractors"><pre><code>--extractor-retries RETRIES     Number of retries for known extractor errors
                                (default is 3), or "infinite"
--allow-dynamic-mpd             Process dynamic DASH manifests (default)
                                (Alias: --no-ignore-dynamic-mpd)
--ignore-dynamic-mpd            Do not process dynamic DASH manifests
                                (Alias: --no-allow-dynamic-mpd)
--hls-split-discontinuity       Split HLS playlists to different formats at
                                discontinuities such as ad breaks
--no-hls-split-discontinuity    Do not split HLS playlists to different
                                formats at discontinuities such as ad breaks
                                (default)
--extractor-args IE_KEY:ARGS    Pass ARGS arguments to the IE_KEY extractor.
                                See "EXTRACTOR ARGUMENTS" for details. You
                                can use this option multiple times to give
                                arguments for different extractors
</code></pre></div>
<h2 tabindex="-1" dir="auto">CONFIGURATION</h2>
<p dir="auto">You can configure yt-dlp by placing any supported command line option to a configuration file. The configuration is loaded from the following locations:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Main Configuration</strong>:</p>
<ul dir="auto">
<li>The file given by <code>--config-location</code></li>
</ul>
</li>
<li>
<p dir="auto"><strong>Portable Configuration</strong>: (Recommended for portable installations)</p>
<ul dir="auto">
<li>If using a binary, <code>yt-dlp.conf</code> in the same directory as the binary</li>
<li>If running from source-code, <code>yt-dlp.conf</code> in the parent directory of <code>yt_dlp</code></li>
</ul>
</li>
<li>
<p dir="auto"><strong>Home Configuration</strong>:</p>
<ul dir="auto">
<li><code>yt-dlp.conf</code> in the home path given by <code>-P</code></li>
<li>If <code>-P</code> is not given, the current directory is searched</li>
</ul>
</li>
<li>
<p dir="auto"><strong>User Configuration</strong>:</p>
<ul dir="auto">
<li><code>${XDG_CONFIG_HOME}/yt-dlp.conf</code></li>
<li><code>${XDG_CONFIG_HOME}/yt-dlp/config</code> (recommended on Linux/macOS)</li>
<li><code>${XDG_CONFIG_HOME}/yt-dlp/config.txt</code></li>
<li><code>${APPDATA}/yt-dlp.conf</code></li>
<li><code>${APPDATA}/yt-dlp/config</code> (recommended on Windows)</li>
<li><code>${APPDATA}/yt-dlp/config.txt</code></li>
<li><code>~/yt-dlp.conf</code></li>
<li><code>~/yt-dlp.conf.txt</code></li>
<li><code>~/.yt-dlp/config</code></li>
<li><code>~/.yt-dlp/config.txt</code></li>
</ul>
<p dir="auto">See also: <a href="#notes-about-environment-variables">Notes about environment variables</a></p>
</li>
<li>
<p dir="auto"><strong>System Configuration</strong>:</p>
<ul dir="auto">
<li><code>/etc/yt-dlp.conf</code></li>
<li><code>/etc/yt-dlp/config</code></li>
<li><code>/etc/yt-dlp/config.txt</code></li>
</ul>
</li>
</ol>
<p dir="auto">E.g. with the following configuration file yt-dlp will always extract the audio, not copy the mtime, use a proxy and save all videos under <code>YouTube</code> directory in your home directory:</p>
<div data-snippet-clipboard-copy-content="# Lines starting with # are comments

# Always extract audio
-x

# Do not copy the mtime
--no-mtime

# Use this proxy
--proxy 127.0.0.1:3128

# Save all videos under YouTube directory in your home directory
-o ~/YouTube/%(title)s.%(ext)s"><pre><code># Lines starting with # are comments

# Always extract audio
-x

# Do not copy the mtime
--no-mtime

# Use this proxy
--proxy 127.0.0.1:3128

# Save all videos under YouTube directory in your home directory
-o ~/YouTube/%(title)s.%(ext)s
</code></pre></div>
<p dir="auto"><strong>Note</strong>: Options in configuration file are just the same options aka switches used in regular command line calls; thus there <strong>must be no whitespace</strong> after <code>-</code> or <code>--</code>, e.g. <code>-o</code> or <code>--proxy</code> but not <code>- o</code> or <code>-- proxy</code>. They must also be quoted when necessary as-if it were a UNIX shell.</p>
<p dir="auto">You can use <code>--ignore-config</code> if you want to disable all configuration files for a particular yt-dlp run. If <code>--ignore-config</code> is found inside any configuration file, no further configuration will be loaded. For example, having the option in the portable configuration file prevents loading of home, user, and system configurations. Additionally, (for backward compatibility) if <code>--ignore-config</code> is found inside the system configuration file, the user configuration is not loaded.</p>
<h3 tabindex="-1" dir="auto">Configuration file encoding</h3>
<p dir="auto">The configuration files are decoded according to the UTF BOM if present, and in the encoding from system locale otherwise.</p>
<p dir="auto">If you want your file to be decoded differently, add <code># coding: ENCODING</code> to the beginning of the file (e.g. <code># coding: shift-jis</code>). There must be no characters before that, even spaces or BOM.</p>
<h3 tabindex="-1" dir="auto">Authentication with netrc</h3>
<p dir="auto">You may also want to configure automatic credentials storage for extractors that support authentication (by providing login and password with <code>--username</code> and <code>--password</code>) in order not to pass credentials as command line arguments on every yt-dlp execution and prevent tracking plain text passwords in the shell command history. You can achieve this using a <a href="https://stackoverflow.com/tags/.netrc/info" rel="nofollow"><code>.netrc</code> file</a> on a per-extractor basis. For that you will need to create a <code>.netrc</code> file in <code>--netrc-location</code> and restrict permissions to read/write by only you:</p>
<div data-snippet-clipboard-copy-content="touch ${HOME}/.netrc
chmod a-rwx,u+rw ${HOME}/.netrc"><pre><code>touch ${HOME}/.netrc
chmod a-rwx,u+rw ${HOME}/.netrc
</code></pre></div>
<p dir="auto">After that you can add credentials for an extractor in the following format, where <em>extractor</em> is the name of the extractor in lowercase:</p>
<div data-snippet-clipboard-copy-content="machine <extractor> login <username> password <password>"><pre><code>machine &lt;extractor&gt; login &lt;username&gt; password &lt;password&gt;
</code></pre></div>
<p dir="auto">E.g.</p>
<div data-snippet-clipboard-copy-content="machine youtube login myaccount@gmail.com password my_youtube_password
machine twitch login my_twitch_account_name password my_twitch_password"><pre><code>machine youtube login myaccount@gmail.com password my_youtube_password
machine twitch login my_twitch_account_name password my_twitch_password
</code></pre></div>
<p dir="auto">To activate authentication with the <code>.netrc</code> file you should pass <code>--netrc</code> to yt-dlp or place it in the <a href="#configuration">configuration file</a>.</p>
<p dir="auto">The default location of the .netrc file is <code>~</code> (see below).</p>
<p dir="auto">As an alternative to using the <code>.netrc</code> file, which has the disadvantage of keeping your passwords in a plain text file, you can configure a custom shell command to provide the credentials for an extractor. This is done by providing the <code>--netrc-cmd</code> parameter, it shall output the credentials in the netrc format and return <code>0</code> on success, other values will be treated as an error. <code>{}</code> in the command will be replaced by the name of the extractor to make it possible to select the credentials for the right extractor.</p>
<p dir="auto">E.g. To use an encrypted <code>.netrc</code> file stored as <code>.authinfo.gpg</code></p>
<div data-snippet-clipboard-copy-content="yt-dlp --netrc-cmd 'gpg --decrypt ~/.authinfo.gpg' https://www.youtube.com/watch?v=BaW_jenozKc"><pre><code>yt-dlp --netrc-cmd 'gpg --decrypt ~/.authinfo.gpg' https://www.youtube.com/watch?v=BaW_jenozKc
</code></pre></div>
<h3 tabindex="-1" dir="auto">Notes about environment variables</h3>
<ul dir="auto">
<li>Environment variables are normally specified as <code>${VARIABLE}</code>/<code>$VARIABLE</code> on UNIX and <code>%VARIABLE%</code> on Windows; but is always shown as <code>${VARIABLE}</code> in this documentation</li>
<li>yt-dlp also allow using UNIX-style variables on Windows for path-like options; e.g. <code>--output</code>, <code>--config-location</code></li>
<li>If unset, <code>${XDG_CONFIG_HOME}</code> defaults to <code>~/.config</code> and <code>${XDG_CACHE_HOME}</code> to <code>~/.cache</code></li>
<li>On Windows, <code>~</code> points to <code>${HOME}</code> if present; or, <code>${USERPROFILE}</code> or <code>${HOMEDRIVE}${HOMEPATH}</code> otherwise</li>
<li>On Windows, <code>${USERPROFILE}</code> generally points to <code>C:\Users\&lt;user name&gt;</code> and <code>${APPDATA}</code> to <code>${USERPROFILE}\AppData\Roaming</code></li>
</ul>
<h2 tabindex="-1" dir="auto">OUTPUT TEMPLATE</h2>
<p dir="auto">The <code>-o</code> option is used to indicate a template for the output file names while <code>-P</code> option is used to specify the path each type of file should be saved to.</p>

<p dir="auto"><strong>tl;dr:</strong> <a href="#output-template-examples">navigate me to examples</a>.</p>

<p dir="auto">The simplest usage of <code>-o</code> is not to set any template arguments when downloading a single file, like in <code>yt-dlp -o funny_video.flv "https://some/video"</code> (hard-coding file extension like this is <em>not</em> recommended and could break some post-processing).</p>
<p dir="auto">It may however also contain special sequences that will be replaced when downloading each video. The special sequences may be formatted according to <a href="https://docs.python.org/3/library/stdtypes.html#printf-style-string-formatting" rel="nofollow">Python string formatting operations</a>, e.g. <code>%(NAME)s</code> or <code>%(NAME)05d</code>. To clarify, that is a percent symbol followed by a name in parentheses, followed by formatting operations.</p>
<p dir="auto">The field names themselves (the part inside the parenthesis) can also have some special formatting:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Object traversal</strong>: The dictionaries and lists available in metadata can be traversed by using a dot <code>.</code> separator; e.g. <code>%(tags.0)s</code>, <code>%(subtitles.en.-1.ext)s</code>. You can do Python slicing with colon <code>:</code>; E.g. <code>%(id.3:7:-1)s</code>, <code>%(formats.:.format_id)s</code>. Curly braces <code>{}</code> can be used to build dictionaries with only specific keys; e.g. <code>%(formats.:.{format_id,height})#j</code>. An empty field name <code>%()s</code> refers to the entire infodict; e.g. <code>%(.{id,title})s</code>. Note that all the fields that become available using this method are not listed below. Use <code>-j</code> to see such fields</p>
</li>
<li>
<p dir="auto"><strong>Addition</strong>: Addition and subtraction of numeric fields can be done using <code>+</code> and <code>-</code> respectively. E.g. <code>%(playlist_index+10)03d</code>, <code>%(n_entries+1-playlist_index)d</code></p>
</li>
<li>
<p dir="auto"><strong>Date/time Formatting</strong>: Date/time fields can be formatted according to <a href="https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes" rel="nofollow">strftime formatting</a> by specifying it separated from the field name using a <code>&gt;</code>. E.g. <code>%(duration&gt;%H-%M-%S)s</code>, <code>%(upload_date&gt;%Y-%m-%d)s</code>, <code>%(epoch-3600&gt;%H-%M-%S)s</code></p>
</li>
<li>
<p dir="auto"><strong>Alternatives</strong>: Alternate fields can be specified separated with a <code>,</code>. E.g. <code>%(release_date&gt;%Y,upload_date&gt;%Y|Unknown)s</code></p>
</li>
<li>
<p dir="auto"><strong>Replacement</strong>: A replacement value can be specified using a <code>&amp;</code> separator according to the <a href="https://docs.python.org/3/library/string.html#format-specification-mini-language" rel="nofollow"><code>str.format</code> mini-language</a>. If the field is <em>not</em> empty, this replacement value will be used instead of the actual field content. This is done after alternate fields are considered; thus the replacement is used if <em>any</em> of the alternative fields is <em>not</em> empty. E.g. <code>%(chapters&amp;has chapters|no chapters)s</code>, <code>%(title&amp;TITLE={:&gt;20}|NO TITLE)s</code></p>
</li>
<li>
<p dir="auto"><strong>Default</strong>: A literal default value can be specified for when the field is empty using a <code>|</code> separator. This overrides <code>--output-na-placeholder</code>. E.g. <code>%(uploader|Unknown)s</code></p>
</li>
<li>
<p dir="auto"><strong>More Conversions</strong>: In addition to the normal format types <code>diouxXeEfFgGcrs</code>, yt-dlp additionally supports converting to <code>B</code> = <strong>B</strong>ytes, <code>j</code> = <strong>j</strong>son (flag <code>#</code> for pretty-printing, <code>+</code> for Unicode), <code>h</code> = HTML escaping, <code>l</code> = a comma separated <strong>l</strong>ist (flag <code>#</code> for <code>\n</code> newline-separated), <code>q</code> = a string <strong>q</strong>uoted for the terminal (flag <code>#</code> to split a list into different arguments), <code>D</code> = add <strong>D</strong>ecimal suffixes (e.g. 10M) (flag <code>#</code> to use 1024 as factor), and <code>S</code> = <strong>S</strong>anitize as filename (flag <code>#</code> for restricted)</p>
</li>
<li>
<p dir="auto"><strong>Unicode normalization</strong>: The format type <code>U</code> can be used for NFC <a href="https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize" rel="nofollow">Unicode normalization</a>. The alternate form flag (<code>#</code>) changes the normalization to NFD and the conversion flag <code>+</code> can be used for NFKC/NFKD compatibility equivalence normalization. E.g. <code>%(title)+.100U</code> is NFKC</p>
</li>
</ol>
<p dir="auto">To summarize, the general syntax for a field is:</p>
<div data-snippet-clipboard-copy-content="%(name[.keys][addition][>strf][,alternate][&amp;replacement][|default])[flags][width][.precision][length]type"><pre><code>%(name[.keys][addition][&gt;strf][,alternate][&amp;replacement][|default])[flags][width][.precision][length]type
</code></pre></div>
<p dir="auto">Additionally, you can set different output templates for the various metadata files separately from the general output template by specifying the type of file followed by the template separated by a colon <code>:</code>. The different file types supported are <code>subtitle</code>, <code>thumbnail</code>, <code>description</code>, <code>annotation</code> (deprecated), <code>infojson</code>, <code>link</code>, <code>pl_thumbnail</code>, <code>pl_description</code>, <code>pl_infojson</code>, <code>chapter</code>, <code>pl_video</code>. E.g. <code>-o "%(title)s.%(ext)s" -o "thumbnail:%(title)s\%(title)s.%(ext)s"</code>  will put the thumbnails in a folder with the same name as the video. If any of the templates is empty, that type of file will not be written. E.g. <code>--write-thumbnail -o "thumbnail:"</code> will write thumbnails only for playlists and not for video.</p>
<a id="user-content-outtmpl-postprocess-note">
<p dir="auto"><strong>Note</strong>: Due to post-processing (i.e. merging etc.), the actual output filename might differ. Use <code>--print after_move:filepath</code> to get the name after all post-processing is complete.</p>
<p dir="auto">The available fields are:</p>
<ul dir="auto">
<li><code>id</code> (string): Video identifier</li>
<li><code>title</code> (string): Video title</li>
<li><code>fulltitle</code> (string): Video title ignoring live timestamp and generic title</li>
<li><code>ext</code> (string): Video filename extension</li>
<li><code>alt_title</code> (string): A secondary title of the video</li>
<li><code>description</code> (string): The description of the video</li>
<li><code>display_id</code> (string): An alternative identifier for the video</li>
<li><code>uploader</code> (string): Full name of the video uploader</li>
<li><code>license</code> (string): License name the video is licensed under</li>
<li><code>creator</code> (string): The creator of the video</li>
<li><code>timestamp</code> (numeric): UNIX timestamp of the moment the video became available</li>
<li><code>upload_date</code> (string): Video upload date in UTC (YYYYMMDD)</li>
<li><code>release_timestamp</code> (numeric): UNIX timestamp of the moment the video was released</li>
<li><code>release_date</code> (string): The date (YYYYMMDD) when the video was released in UTC</li>
<li><code>modified_timestamp</code> (numeric): UNIX timestamp of the moment the video was last modified</li>
<li><code>modified_date</code> (string): The date (YYYYMMDD) when the video was last modified in UTC</li>
<li><code>uploader_id</code> (string): Nickname or id of the video uploader</li>
<li><code>channel</code> (string): Full name of the channel the video is uploaded on</li>
<li><code>channel_id</code> (string): Id of the channel</li>
<li><code>channel_follower_count</code> (numeric): Number of followers of the channel</li>
<li><code>channel_is_verified</code> (boolean): Whether the channel is verified on the platform</li>
<li><code>location</code> (string): Physical location where the video was filmed</li>
<li><code>duration</code> (numeric): Length of the video in seconds</li>
<li><code>duration_string</code> (string): Length of the video (HH:mm:ss)</li>
<li><code>view_count</code> (numeric): How many users have watched the video on the platform</li>
<li><code>concurrent_view_count</code> (numeric): How many users are currently watching the video on the platform.</li>
<li><code>like_count</code> (numeric): Number of positive ratings of the video</li>
<li><code>dislike_count</code> (numeric): Number of negative ratings of the video</li>
<li><code>repost_count</code> (numeric): Number of reposts of the video</li>
<li><code>average_rating</code> (numeric): Average rating give by users, the scale used depends on the webpage</li>
<li><code>comment_count</code> (numeric): Number of comments on the video (For some extractors, comments are only downloaded at the end, and so this field cannot be used)</li>
<li><code>age_limit</code> (numeric): Age restriction for the video (years)</li>
<li><code>live_status</code> (string): One of "not_live", "is_live", "is_upcoming", "was_live", "post_live" (was live, but VOD is not yet processed)</li>
<li><code>is_live</code> (boolean): Whether this video is a live stream or a fixed-length video</li>
<li><code>was_live</code> (boolean): Whether this video was originally a live stream</li>
<li><code>playable_in_embed</code> (string): Whether this video is allowed to play in embedded players on other sites</li>
<li><code>availability</code> (string): Whether the video is "private", "premium_only", "subscriber_only", "needs_auth", "unlisted" or "public"</li>
<li><code>start_time</code> (numeric): Time in seconds where the reproduction should start, as specified in the URL</li>
<li><code>end_time</code> (numeric): Time in seconds where the reproduction should end, as specified in the URL</li>
<li><code>extractor</code> (string): Name of the extractor</li>
<li><code>extractor_key</code> (string): Key name of the extractor</li>
<li><code>epoch</code> (numeric): Unix epoch of when the information extraction was completed</li>
<li><code>autonumber</code> (numeric): Number that will be increased with each download, starting at <code>--autonumber-start</code>, padded with leading zeros to 5 digits</li>
<li><code>video_autonumber</code> (numeric): Number that will be increased with each video</li>
<li><code>n_entries</code> (numeric): Total number of extracted items in the playlist</li>
<li><code>playlist_id</code> (string): Identifier of the playlist that contains the video</li>
<li><code>playlist_title</code> (string): Name of the playlist that contains the video</li>
<li><code>playlist</code> (string): <code>playlist_id</code> or <code>playlist_title</code></li>
<li><code>playlist_count</code> (numeric): Total number of items in the playlist. May not be known if entire playlist is not extracted</li>
<li><code>playlist_index</code> (numeric): Index of the video in the playlist padded with leading zeros according the final index</li>
<li><code>playlist_autonumber</code> (numeric): Position of the video in the playlist download queue padded with leading zeros according to the total length of the playlist</li>
<li><code>playlist_uploader</code> (string): Full name of the playlist uploader</li>
<li><code>playlist_uploader_id</code> (string): Nickname or id of the playlist uploader</li>
<li><code>webpage_url</code> (string): A URL to the video webpage which if given to yt-dlp should allow to get the same result again</li>
<li><code>webpage_url_basename</code> (string): The basename of the webpage URL</li>
<li><code>webpage_url_domain</code> (string): The domain of the webpage URL</li>
<li><code>original_url</code> (string): The URL given by the user (or same as <code>webpage_url</code> for playlist entries)</li>
</ul>
</a><p dir="auto">All the fields in <a href="#filtering-formats">Filtering Formats</a> can also be used</p>
<p dir="auto">Available for the video that belongs to some logical chapter or section:</p>
<ul dir="auto">
<li><code>chapter</code> (string): Name or title of the chapter the video belongs to</li>
<li><code>chapter_number</code> (numeric): Number of the chapter the video belongs to</li>
<li><code>chapter_id</code> (string): Id of the chapter the video belongs to</li>
</ul>
<p dir="auto">Available for the video that is an episode of some series or programme:</p>
<ul dir="auto">
<li><code>series</code> (string): Title of the series or programme the video episode belongs to</li>
<li><code>season</code> (string): Title of the season the video episode belongs to</li>
<li><code>season_number</code> (numeric): Number of the season the video episode belongs to</li>
<li><code>season_id</code> (string): Id of the season the video episode belongs to</li>
<li><code>episode</code> (string): Title of the video episode</li>
<li><code>episode_number</code> (numeric): Number of the video episode within a season</li>
<li><code>episode_id</code> (string): Id of the video episode</li>
</ul>
<p dir="auto">Available for the media that is a track or a part of a music album:</p>
<ul dir="auto">
<li><code>track</code> (string): Title of the track</li>
<li><code>track_number</code> (numeric): Number of the track within an album or a disc</li>
<li><code>track_id</code> (string): Id of the track</li>
<li><code>artist</code> (string): Artist(s) of the track</li>
<li><code>genre</code> (string): Genre(s) of the track</li>
<li><code>album</code> (string): Title of the album the track belongs to</li>
<li><code>album_type</code> (string): Type of the album</li>
<li><code>album_artist</code> (string): List of all artists appeared on the album</li>
<li><code>disc_number</code> (numeric): Number of the disc or other physical medium the track belongs to</li>
<li><code>release_year</code> (numeric): Year (YYYY) when the album was released</li>
</ul>
<p dir="auto">Available only when using <code>--download-sections</code> and for <code>chapter:</code> prefix when using <code>--split-chapters</code> for videos with internal chapters:</p>
<ul dir="auto">
<li><code>section_title</code> (string): Title of the chapter</li>
<li><code>section_number</code> (numeric): Number of the chapter within the file</li>
<li><code>section_start</code> (numeric): Start time of the chapter in seconds</li>
<li><code>section_end</code> (numeric): End time of the chapter in seconds</li>
</ul>
<p dir="auto">Available only when used in <code>--print</code>:</p>
<ul dir="auto">
<li><code>urls</code> (string): The URLs of all requested formats, one in each line</li>
<li><code>filename</code> (string): Name of the video file. Note that the <a href="#outtmpl-postprocess-note">actual filename may differ</a></li>
<li><code>formats_table</code> (table): The video format table as printed by <code>--list-formats</code></li>
<li><code>thumbnails_table</code> (table): The thumbnail format table as printed by <code>--list-thumbnails</code></li>
<li><code>subtitles_table</code> (table): The subtitle format table as printed by <code>--list-subs</code></li>
<li><code>automatic_captions_table</code> (table): The automatic subtitle format table as printed by <code>--list-subs</code></li>
</ul>
<p dir="auto">Available only after the video is downloaded (<code>post_process</code>/<code>after_move</code>):</p>
<ul dir="auto">
<li><code>filepath</code>: Actual path of downloaded video file</li>
</ul>
<p dir="auto">Available only in <code>--sponsorblock-chapter-title</code>:</p>
<ul dir="auto">
<li><code>start_time</code> (numeric): Start time of the chapter in seconds</li>
<li><code>end_time</code> (numeric): End time of the chapter in seconds</li>
<li><code>categories</code> (list): The <a href="https://wiki.sponsor.ajay.app/w/Types#Category" rel="nofollow">SponsorBlock categories</a> the chapter belongs to</li>
<li><code>category</code> (string): The smallest SponsorBlock category the chapter belongs to</li>
<li><code>category_names</code> (list): Friendly names of the categories</li>
<li><code>name</code> (string): Friendly name of the smallest category</li>
<li><code>type</code> (string): The <a href="https://wiki.sponsor.ajay.app/w/Types#Action_Type" rel="nofollow">SponsorBlock action type</a> of the chapter</li>
</ul>
<p dir="auto">Each aforementioned sequence when referenced in an output template will be replaced by the actual value corresponding to the sequence name. E.g. for <code>-o %(title)s-%(id)s.%(ext)s</code> and an mp4 video with title <code>yt-dlp test video</code> and id <code>BaW_jenozKc</code>, this will result in a <code>yt-dlp test video-BaW_jenozKc.mp4</code> file created in the current directory.</p>
<p dir="auto"><strong>Note</strong>: Some of the sequences are not guaranteed to be present since they depend on the metadata obtained by a particular extractor. Such sequences will be replaced with placeholder value provided with <code>--output-na-placeholder</code> (<code>NA</code> by default).</p>
<p dir="auto"><strong>Tip</strong>: Look at the <code>-j</code> output to identify which fields are available for the particular URL</p>
<p dir="auto">For numeric sequences you can use <a href="https://docs.python.org/3/library/stdtypes.html#printf-style-string-formatting" rel="nofollow">numeric related formatting</a>; e.g. <code>%(view_count)05d</code> will result in a string with view count padded with zeros up to 5 characters, like in <code>00042</code>.</p>
<p dir="auto">Output templates can also contain arbitrary hierarchical path, e.g. <code>-o "%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s"</code> which will result in downloading each video in a directory corresponding to this path template. Any missing directory will be automatically created for you.</p>
<p dir="auto">To use percent literals in an output template use <code>%%</code>. To output to stdout use <code>-o -</code>.</p>
<p dir="auto">The current default template is <code>%(title)s [%(id)s].%(ext)s</code>.</p>
<p dir="auto">In some cases, you don't want special characters such as , spaces, or &amp;, such as when transferring the downloaded filename to a Windows system or the filename through an 8bit-unsafe channel. In these cases, add the <code>--restrict-filenames</code> flag to get a shorter title.</p>
<h4 tabindex="-1" dir="auto">Output template examples</h4>
<div dir="auto" data-snippet-clipboard-copy-content="$ yt-dlp --print filename -o &quot;test video.%(ext)s&quot; BaW_jenozKc
test video.webm    # Literal name with correct extension

$ yt-dlp --print filename -o &quot;%(title)s.%(ext)s&quot; BaW_jenozKc
youtube-dl test video ''_.webm    # All kinds of weird characters

$ yt-dlp --print filename -o &quot;%(title)s.%(ext)s&quot; BaW_jenozKc --restrict-filenames
youtube-dl_test_video_.webm    # Restricted file name

# Download YouTube playlist videos in separate directory indexed by video order in a playlist
$ yt-dlp -o &quot;%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s&quot; &quot;https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re&quot;

# Download YouTube playlist videos in separate directories according to their uploaded year
$ yt-dlp -o &quot;%(upload_date>%Y)s/%(title)s.%(ext)s&quot; &quot;https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re&quot;

# Prefix playlist index with &quot; - &quot; separator, but only if it is available
$ yt-dlp -o &quot;%(playlist_index&amp;{} - |)s%(title)s.%(ext)s&quot; BaW_jenozKc &quot;https://www.youtube.com/user/TheLinuxFoundation/playlists&quot;

# Download all playlists of YouTube channel/user keeping each playlist in separate directory:
$ yt-dlp -o &quot;%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s&quot; &quot;https://www.youtube.com/user/TheLinuxFoundation/playlists&quot;

# Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home
$ yt-dlp -u user -p password -P &quot;~/MyVideos&quot; -o &quot;%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s&quot; &quot;https://www.udemy.com/java-tutorial&quot;

# Download entire series season keeping each series and each season in separate directory under C:/MyVideos
$ yt-dlp -P &quot;C:/MyVideos&quot; -o &quot;%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s&quot; &quot;https://videomore.ru/kino_v_detalayah/5_sezon/367617&quot;

# Download video as &quot;C:\MyVideos\uploader\title.ext&quot;, subtitles as &quot;C:\MyVideos\subs\uploader\title.ext&quot;
# and put all temporary files in &quot;C:\MyVideos\tmp&quot;
$ yt-dlp -P &quot;C:/MyVideos&quot; -P &quot;temp:tmp&quot; -P &quot;subtitle:subs&quot; -o &quot;%(uploader)s/%(title)s.%(ext)s&quot; BaW_jenoz --write-subs

# Download video as &quot;C:\MyVideos\uploader\title.ext&quot; and subtitles as &quot;C:\MyVideos\uploader\subs\title.ext&quot;
$ yt-dlp -P &quot;C:/MyVideos&quot; -o &quot;%(uploader)s/%(title)s.%(ext)s&quot; -o &quot;subtitle:%(uploader)s/subs/%(title)s.%(ext)s&quot; BaW_jenozKc --write-subs

# Stream the video being downloaded to stdout
$ yt-dlp -o - BaW_jenozKc"><pre>$ yt-dlp --print filename -o <span><span>"</span>test video.%(ext)s<span>"</span></span> BaW_jenozKc
<span>test</span> video.webm    <span><span>#</span> Literal name with correct extension</span>

$ yt-dlp --print filename -o <span><span>"</span>%(title)s.%(ext)s<span>"</span></span> BaW_jenozKc
youtube-dl <span>test</span> video <span><span>'</span><span>'</span></span>_.webm    <span><span>#</span> All kinds of weird characters</span>

$ yt-dlp --print filename -o <span><span>"</span>%(title)s.%(ext)s<span>"</span></span> BaW_jenozKc --restrict-filenames
youtube-dl_test_video_.webm    <span><span>#</span> Restricted file name</span>

<span><span>#</span> Download YouTube playlist videos in separate directory indexed by video order in a playlist</span>
$ yt-dlp -o <span><span>"</span>%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s<span>"</span></span> <span><span>"</span>https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re<span>"</span></span>

<span><span>#</span> Download YouTube playlist videos in separate directories according to their uploaded year</span>
$ yt-dlp -o <span><span>"</span>%(upload_date&gt;%Y)s/%(title)s.%(ext)s<span>"</span></span> <span><span>"</span>https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re<span>"</span></span>

<span><span>#</span> Prefix playlist index with " - " separator, but only if it is available</span>
$ yt-dlp -o <span><span>"</span>%(playlist_index&amp;{} - |)s%(title)s.%(ext)s<span>"</span></span> BaW_jenozKc <span><span>"</span>https://www.youtube.com/user/TheLinuxFoundation/playlists<span>"</span></span>

<span><span>#</span> Download all playlists of YouTube channel/user keeping each playlist in separate directory:</span>
$ yt-dlp -o <span><span>"</span>%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s<span>"</span></span> <span><span>"</span>https://www.youtube.com/user/TheLinuxFoundation/playlists<span>"</span></span>

<span><span>#</span> Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home</span>
$ yt-dlp -u user -p password -P <span><span>"</span>~/MyVideos<span>"</span></span> -o <span><span>"</span>%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s<span>"</span></span> <span><span>"</span>https://www.udemy.com/java-tutorial<span>"</span></span>

<span><span>#</span> Download entire series season keeping each series and each season in separate directory under C:/MyVideos</span>
$ yt-dlp -P <span><span>"</span>C:/MyVideos<span>"</span></span> -o <span><span>"</span>%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s<span>"</span></span> <span><span>"</span>https://videomore.ru/kino_v_detalayah/5_sezon/367617<span>"</span></span>

<span><span>#</span> Download video as "C:\MyVideos\uploader\title.ext", subtitles as "C:\MyVideos\subs\uploader\title.ext"</span>
<span><span>#</span> and put all temporary files in "C:\MyVideos\tmp"</span>
$ yt-dlp -P <span><span>"</span>C:/MyVideos<span>"</span></span> -P <span><span>"</span>temp:tmp<span>"</span></span> -P <span><span>"</span>subtitle:subs<span>"</span></span> -o <span><span>"</span>%(uploader)s/%(title)s.%(ext)s<span>"</span></span> BaW_jenoz --write-subs

<span><span>#</span> Download video as "C:\MyVideos\uploader\title.ext" and subtitles as "C:\MyVideos\uploader\subs\title.ext"</span>
$ yt-dlp -P <span><span>"</span>C:/MyVideos<span>"</span></span> -o <span><span>"</span>%(uploader)s/%(title)s.%(ext)s<span>"</span></span> -o <span><span>"</span>subtitle:%(uploader)s/subs/%(title)s.%(ext)s<span>"</span></span> BaW_jenozKc --write-subs

<span><span>#</span> Stream the video being downloaded to stdout</span>
$ yt-dlp -o - BaW_jenozKc</pre></div>
<h2 tabindex="-1" dir="auto">FORMAT SELECTION</h2>
<p dir="auto">By default, yt-dlp tries to download the best available quality if you <strong>don't</strong> pass any options.
This is generally equivalent to using <code>-f bestvideo*+bestaudio/best</code>. However, if multiple audiostreams is enabled (<code>--audio-multistreams</code>), the default format changes to <code>-f bestvideo+bestaudio/best</code>. Similarly, if ffmpeg is unavailable, or if you use yt-dlp to stream to <code>stdout</code> (<code>-o -</code>), the default becomes <code>-f best/bestvideo+bestaudio</code>.</p>
<p dir="auto"><strong>Deprecation warning</strong>: Latest versions of yt-dlp can stream multiple formats to the stdout simultaneously using ffmpeg. So, in future versions, the default for this will be set to <code>-f bv*+ba/b</code> similar to normal downloads. If you want to preserve the <code>-f b/bv+ba</code> setting, it is recommended to explicitly specify it in the configuration options.</p>
<p dir="auto">The general syntax for format selection is <code>-f FORMAT</code> (or <code>--format FORMAT</code>) where <code>FORMAT</code> is a <em>selector expression</em>, i.e. an expression that describes format or formats you would like to download.</p>

<p dir="auto"><strong>tl;dr:</strong> <a href="#format-selection-examples">navigate me to examples</a>.</p>

<p dir="auto">The simplest case is requesting a specific format; e.g. with <code>-f 22</code> you can download the format with format code equal to 22. You can get the list of available format codes for particular video using <code>--list-formats</code> or <code>-F</code>. Note that these format codes are extractor specific.</p>
<p dir="auto">You can also use a file extension (currently <code>3gp</code>, <code>aac</code>, <code>flv</code>, <code>m4a</code>, <code>mp3</code>, <code>mp4</code>, <code>ogg</code>, <code>wav</code>, <code>webm</code> are supported) to download the best quality format of a particular file extension served as a single file, e.g. <code>-f webm</code> will download the best quality format with the <code>webm</code> extension served as a single file.</p>
<p dir="auto">You can use <code>-f -</code> to interactively provide the format selector <em>for each video</em></p>
<p dir="auto">You can also use special names to select particular edge case formats:</p>
<ul dir="auto">
<li><code>all</code>: Select <strong>all formats</strong> separately</li>
<li><code>mergeall</code>: Select and <strong>merge all formats</strong> (Must be used with <code>--audio-multistreams</code>, <code>--video-multistreams</code> or both)</li>
<li><code>b*</code>, <code>best*</code>: Select the best quality format that <strong>contains either</strong> a video or an audio or both (ie; <code>vcodec!=none or acodec!=none</code>)</li>
<li><code>b</code>, <code>best</code>: Select the best quality format that <strong>contains both</strong> video and audio. Equivalent to <code>best*[vcodec!=none][acodec!=none]</code></li>
<li><code>bv</code>, <code>bestvideo</code>: Select the best quality <strong>video-only</strong> format. Equivalent to <code>best*[acodec=none]</code></li>
<li><code>bv*</code>, <code>bestvideo*</code>: Select the best quality format that <strong>contains video</strong>. It may also contain audio. Equivalent to <code>best*[vcodec!=none]</code></li>
<li><code>ba</code>, <code>bestaudio</code>: Select the best quality <strong>audio-only</strong> format. Equivalent to <code>best*[vcodec=none]</code></li>
<li><code>ba*</code>, <code>bestaudio*</code>: Select the best quality format that <strong>contains audio</strong>. It may also contain video. Equivalent to <code>best*[acodec!=none]</code> (<a href="https://github.com/yt-dlp/yt-dlp/issues/979#issuecomment-919629354" data-hovercard-type="issue" data-hovercard-url="/yt-dlp/yt-dlp/issues/979/hovercard">Do not use!</a>)</li>
<li><code>w*</code>, <code>worst*</code>: Select the worst quality format that contains either a video or an audio</li>
<li><code>w</code>, <code>worst</code>: Select the worst quality format that contains both video and audio. Equivalent to <code>worst*[vcodec!=none][acodec!=none]</code></li>
<li><code>wv</code>, <code>worstvideo</code>: Select the worst quality video-only format. Equivalent to <code>worst*[acodec=none]</code></li>
<li><code>wv*</code>, <code>worstvideo*</code>: Select the worst quality format that contains video. It may also contain audio. Equivalent to <code>worst*[vcodec!=none]</code></li>
<li><code>wa</code>, <code>worstaudio</code>: Select the worst quality audio-only format. Equivalent to <code>worst*[vcodec=none]</code></li>
<li><code>wa*</code>, <code>worstaudio*</code>: Select the worst quality format that contains audio. It may also contain video. Equivalent to <code>worst*[acodec!=none]</code></li>
</ul>
<p dir="auto">For example, to download the worst quality video-only format you can use <code>-f worstvideo</code>. It is however recommended not to use <code>worst</code> and related options. When your format selector is <code>worst</code>, the format which is worst in all respects is selected. Most of the time, what you actually want is the video with the smallest filesize instead. So it is generally better to use <code>-S +size</code> or more rigorously, <code>-S +size,+br,+res,+fps</code> instead of <code>-f worst</code>. See <a href="#sorting-formats">Sorting Formats</a> for more details.</p>
<p dir="auto">You can select the n'th best format of a type by using <code>best&lt;type&gt;.&lt;n&gt;</code>. For example, <code>best.2</code> will select the 2nd best combined format. Similarly, <code>bv*.3</code> will select the 3rd best format that contains a video stream.</p>
<p dir="auto">If you want to download multiple videos, and they don't have the same formats available, you can specify the order of preference using slashes. Note that formats on the left hand side are preferred; e.g. <code>-f 22/17/18</code> will download format 22 if it's available, otherwise it will download format 17 if it's available, otherwise it will download format 18 if it's available, otherwise it will complain that no suitable formats are available for download.</p>
<p dir="auto">If you want to download several formats of the same video use a comma as a separator, e.g. <code>-f 22,17,18</code> will download all these three formats, of course if they are available. Or a more sophisticated example combined with the precedence feature: <code>-f 136/137/mp4/bestvideo,140/m4a/bestaudio</code>.</p>
<p dir="auto">You can merge the video and audio of multiple formats into a single file using <code>-f &lt;format1&gt;+&lt;format2&gt;+...</code> (requires ffmpeg installed); e.g. <code>-f bestvideo+bestaudio</code> will download the best video-only format, the best audio-only format and mux them together with ffmpeg.</p>
<p dir="auto"><strong>Deprecation warning</strong>: Since the <em>below</em> described behavior is complex and counter-intuitive, this will be removed and multistreams will be enabled by default in the future. A new operator will be instead added to limit formats to single audio/video</p>
<p dir="auto">Unless <code>--video-multistreams</code> is used, all formats with a video stream except the first one are ignored. Similarly, unless <code>--audio-multistreams</code> is used, all formats with an audio stream except the first one are ignored. E.g. <code>-f bestvideo+best+bestaudio --video-multistreams --audio-multistreams</code> will download and merge all 3 given formats. The resulting file will have 2 video streams and 2 audio streams. But <code>-f bestvideo+best+bestaudio --no-video-multistreams</code> will download and merge only <code>bestvideo</code> and <code>bestaudio</code>. <code>best</code> is ignored since another format containing a video stream (<code>bestvideo</code>) has already been selected. The order of the formats is therefore important. <code>-f best+bestaudio --no-audio-multistreams</code> will download only <code>best</code> while <code>-f bestaudio+best --no-audio-multistreams</code> will ignore <code>best</code> and download only <code>bestaudio</code>.</p>
<h2 tabindex="-1" dir="auto">Filtering Formats</h2>
<p dir="auto">You can also filter the video formats by putting a condition in brackets, as in <code>-f "best[height=720]"</code> (or <code>-f "[filesize&gt;10M]"</code> since filters without a selector are interpreted as <code>best</code>).</p>
<p dir="auto">The following numeric meta fields can be used with comparisons <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>=</code> (equals), <code>!=</code> (not equals):</p>
<ul dir="auto">
<li><code>filesize</code>: The number of bytes, if known in advance</li>
<li><code>filesize_approx</code>: An estimate for the number of bytes</li>
<li><code>width</code>: Width of the video, if known</li>
<li><code>height</code>: Height of the video, if known</li>
<li><code>aspect_ratio</code>: Aspect ratio of the video, if known</li>
<li><code>tbr</code>: Average bitrate of audio and video in KBit/s</li>
<li><code>abr</code>: Average audio bitrate in KBit/s</li>
<li><code>vbr</code>: Average video bitrate in KBit/s</li>
<li><code>asr</code>: Audio sampling rate in Hertz</li>
<li><code>fps</code>: Frame rate</li>
<li><code>audio_channels</code>: The number of audio channels</li>
<li><code>stretched_ratio</code>: <code>width:height</code> of the video's pixels, if not square</li>
</ul>
<p dir="auto">Also filtering work for comparisons <code>=</code> (equals), <code>^=</code> (starts with), <code>$=</code> (ends with), <code>*=</code> (contains), <code>~=</code> (matches regex) and following string meta fields:</p>
<ul dir="auto">
<li><code>url</code>: Video URL</li>
<li><code>ext</code>: File extension</li>
<li><code>acodec</code>: Name of the audio codec in use</li>
<li><code>vcodec</code>: Name of the video codec in use</li>
<li><code>container</code>: Name of the container format</li>
<li><code>protocol</code>: The protocol that will be used for the actual download, lower-case (<code>http</code>, <code>https</code>, <code>rtsp</code>, <code>rtmp</code>, <code>rtmpe</code>, <code>mms</code>, <code>f4m</code>, <code>ism</code>, <code>http_dash_segments</code>, <code>m3u8</code>, or <code>m3u8_native</code>)</li>
<li><code>language</code>: Language code</li>
<li><code>dynamic_range</code>: The dynamic range of the video</li>
<li><code>format_id</code>: A short description of the format</li>
<li><code>format</code>: A human-readable description of the format</li>
<li><code>format_note</code>: Additional info about the format</li>
<li><code>resolution</code>: Textual description of width and height</li>
</ul>
<p dir="auto">Any string comparison may be prefixed with negation <code>!</code> in order to produce an opposite comparison, e.g. <code>!*=</code> (does not contain). The comparand of a string comparison needs to be quoted with either double or single quotes if it contains spaces or special characters other than <code>._-</code>.</p>
<p dir="auto"><strong>Note</strong>: None of the aforementioned meta fields are guaranteed to be present since this solely depends on the metadata obtained by particular extractor, i.e. the metadata offered by the website. Any other field made available by the extractor can also be used for filtering.</p>
<p dir="auto">Formats for which the value is not known are excluded unless you put a question mark (<code>?</code>) after the operator. You can combine format filters, so <code>-f "bv[height&lt;=?720][tbr&gt;500]"</code> selects up to 720p videos (or videos where the height is not known) with a bitrate of at least 500 KBit/s. You can also use the filters with <code>all</code> to download all formats that satisfy the filter, e.g. <code>-f "all[vcodec=none]"</code> selects all audio-only formats.</p>
<p dir="auto">Format selectors can also be grouped using parentheses; e.g. <code>-f "(mp4,webm)[height&lt;480]"</code> will download the best pre-merged mp4 and webm formats with a height lower than 480.</p>
<h2 tabindex="-1" dir="auto">Sorting Formats</h2>
<p dir="auto">You can change the criteria for being considered the <code>best</code> by using <code>-S</code> (<code>--format-sort</code>). The general format for this is <code>--format-sort field1,field2...</code>.</p>
<p dir="auto">The available fields are:</p>
<ul dir="auto">
<li><code>hasvid</code>: Gives priority to formats that have a video stream</li>
<li><code>hasaud</code>: Gives priority to formats that have an audio stream</li>
<li><code>ie_pref</code>: The format preference</li>
<li><code>lang</code>: The language preference</li>
<li><code>quality</code>: The quality of the format</li>
<li><code>source</code>: The preference of the source</li>
<li><code>proto</code>: Protocol used for download (<code>https</code>/<code>ftps</code> &gt; <code>http</code>/<code>ftp</code> &gt; <code>m3u8_native</code>/<code>m3u8</code> &gt; <code>http_dash_segments</code>&gt; <code>websocket_frag</code> &gt; <code>mms</code>/<code>rtsp</code> &gt; <code>f4f</code>/<code>f4m</code>)</li>
<li><code>vcodec</code>: Video Codec (<code>av01</code> &gt; <code>vp9.2</code> &gt; <code>vp9</code> &gt; <code>h265</code> &gt; <code>h264</code> &gt; <code>vp8</code> &gt; <code>h263</code> &gt; <code>theora</code> &gt; other)</li>
<li><code>acodec</code>: Audio Codec (<code>flac</code>/<code>alac</code> &gt; <code>wav</code>/<code>aiff</code> &gt; <code>opus</code> &gt; <code>vorbis</code> &gt; <code>aac</code> &gt; <code>mp4a</code> &gt; <code>mp3</code> &gt; <code>ac4</code> &gt; <code>eac3</code> &gt; <code>ac3</code> &gt; <code>dts</code> &gt; other)</li>
<li><code>codec</code>: Equivalent to <code>vcodec,acodec</code></li>
<li><code>vext</code>: Video Extension (<code>mp4</code> &gt; <code>mov</code> &gt; <code>webm</code> &gt; <code>flv</code> &gt; other). If <code>--prefer-free-formats</code> is used, <code>webm</code> is preferred.</li>
<li><code>aext</code>: Audio Extension (<code>m4a</code> &gt; <code>aac</code> &gt; <code>mp3</code> &gt; <code>ogg</code> &gt; <code>opus</code> &gt; <code>webm</code> &gt; other). If <code>--prefer-free-formats</code> is used, the order changes to <code>ogg</code> &gt; <code>opus</code> &gt; <code>webm</code> &gt; <code>mp3</code> &gt; <code>m4a</code> &gt; <code>aac</code></li>
<li><code>ext</code>: Equivalent to <code>vext,aext</code></li>
<li><code>filesize</code>: Exact filesize, if known in advance</li>
<li><code>fs_approx</code>: Approximate filesize</li>
<li><code>size</code>: Exact filesize if available, otherwise approximate filesize</li>
<li><code>height</code>: Height of video</li>
<li><code>width</code>: Width of video</li>
<li><code>res</code>: Video resolution, calculated as the smallest dimension.</li>
<li><code>fps</code>: Framerate of video</li>
<li><code>hdr</code>: The dynamic range of the video (<code>DV</code> &gt; <code>HDR12</code> &gt; <code>HDR10+</code> &gt; <code>HDR10</code> &gt; <code>HLG</code> &gt; <code>SDR</code>)</li>
<li><code>channels</code>: The number of audio channels</li>
<li><code>tbr</code>: Total average bitrate in KBit/s</li>
<li><code>vbr</code>: Average video bitrate in KBit/s</li>
<li><code>abr</code>: Average audio bitrate in KBit/s</li>
<li><code>br</code>: Average bitrate in KBit/s, <code>tbr</code>/<code>vbr</code>/<code>abr</code></li>
<li><code>asr</code>: Audio sample rate in Hz</li>
</ul>
<p dir="auto"><strong>Deprecation warning</strong>: Many of these fields have (currently undocumented) aliases, that may be removed in a future version. It is recommended to use only the documented field names.</p>
<p dir="auto">All fields, unless specified otherwise, are sorted in descending order. To reverse this, prefix the field with a <code>+</code>. E.g. <code>+res</code> prefers format with the smallest resolution. Additionally, you can suffix a preferred value for the fields, separated by a <code>:</code>. E.g. <code>res:720</code> prefers larger videos, but no larger than 720p and the smallest video if there are no videos less than 720p. For <code>codec</code> and <code>ext</code>, you can provide two preferred values, the first for video and the second for audio. E.g. <code>+codec:avc:m4a</code> (equivalent to <code>+vcodec:avc,+acodec:m4a</code>) sets the video codec preference to <code>h264</code> &gt; <code>h265</code> &gt; <code>vp9</code> &gt; <code>vp9.2</code> &gt; <code>av01</code> &gt; <code>vp8</code> &gt; <code>h263</code> &gt; <code>theora</code> and audio codec preference to <code>mp4a</code> &gt; <code>aac</code> &gt; <code>vorbis</code> &gt; <code>opus</code> &gt; <code>mp3</code> &gt; <code>ac3</code> &gt; <code>dts</code>. You can also make the sorting prefer the nearest values to the provided by using <code>~</code> as the delimiter. E.g. <code>filesize~1G</code> prefers the format with filesize closest to 1 GiB.</p>
<p dir="auto">The fields <code>hasvid</code> and <code>ie_pref</code> are always given highest priority in sorting, irrespective of the user-defined order. This behaviour can be changed by using <code>--format-sort-force</code>. Apart from these, the default order used is: <code>lang,quality,res,fps,hdr:12,vcodec:vp9.2,channels,acodec,size,br,asr,proto,ext,hasaud,source,id</code>. The extractors may override this default order, but they cannot override the user-provided order.</p>
<p dir="auto">Note that the default has <code>vcodec:vp9.2</code>; i.e. <code>av1</code> is not preferred. Similarly, the default for hdr is <code>hdr:12</code>; i.e. dolby vision is not preferred. These choices are made since DV and AV1 formats are not yet fully compatible with most devices. This may be changed in the future as more devices become capable of smoothly playing back these formats.</p>
<p dir="auto">If your format selector is <code>worst</code>, the last item is selected after sorting. This means it will select the format that is worst in all respects. Most of the time, what you actually want is the video with the smallest filesize instead. So it is generally better to use <code>-f best -S +size,+br,+res,+fps</code>.</p>
<p dir="auto"><strong>Tip</strong>: You can use the <code>-v -F</code> to see how the formats have been sorted (worst to best).</p>
<h2 tabindex="-1" dir="auto">Format Selection examples</h2>
<div dir="auto" data-snippet-clipboard-copy-content="# Download and merge the best video-only format and the best audio-only format,
# or download the best combined format if video-only format is not available
$ yt-dlp -f &quot;bv+ba/b&quot;

# Download best format that contains video,
# and if it doesn't already have an audio stream, merge it with best audio-only format
$ yt-dlp -f &quot;bv*+ba/b&quot;

# Same as above
$ yt-dlp

# Download the best video-only format and the best audio-only format without merging them
# For this case, an output template should be used since
# by default, bestvideo and bestaudio will have the same file name.
$ yt-dlp -f &quot;bv,ba&quot; -o &quot;%(title)s.f%(format_id)s.%(ext)s&quot;

# Download and merge the best format that has a video stream,
# and all audio-only formats into one file
$ yt-dlp -f &quot;bv*+mergeall[vcodec=none]&quot; --audio-multistreams

# Download and merge the best format that has a video stream,
# and the best 2 audio-only formats into one file
$ yt-dlp -f &quot;bv*+ba+ba.2&quot; --audio-multistreams


# The following examples show the old method (without -S) of format selection
# and how to use -S to achieve a similar but (generally) better result

# Download the worst video available (old method)
$ yt-dlp -f &quot;wv*+wa/w&quot;

# Download the best video available but with the smallest resolution
$ yt-dlp -S &quot;+res&quot;

# Download the smallest video available
$ yt-dlp -S &quot;+size,+br&quot;



# Download the best mp4 video available, or the best video if no mp4 available
$ yt-dlp -f &quot;bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4] / bv*+ba/b&quot;

# Download the best video with the best extension
# (For video, mp4 > mov > webm > flv. For audio, m4a > aac > mp3 ...)
$ yt-dlp -S &quot;ext&quot;



# Download the best video available but no better than 480p,
# or the worst video if there is no video under 480p
$ yt-dlp -f &quot;bv*[height<=480]+ba/b[height<=480] / wv*+ba/w&quot;

# Download the best video available with the largest height but no better than 480p,
# or the best video with the smallest resolution if there is no video under 480p
$ yt-dlp -S &quot;height:480&quot;

# Download the best video available with the largest resolution but no better than 480p,
# or the best video with the smallest resolution if there is no video under 480p
# Resolution is determined by using the smallest dimension.
# So this works correctly for vertical videos as well
$ yt-dlp -S &quot;res:480&quot;



# Download the best video (that also has audio) but no bigger than 50 MB,
# or the worst video (that also has audio) if there is no video under 50 MB
$ yt-dlp -f &quot;b[filesize<50M] / w&quot;

# Download largest video (that also has audio) but no bigger than 50 MB,
# or the smallest video (that also has audio) if there is no video under 50 MB
$ yt-dlp -f &quot;b&quot; -S &quot;filesize:50M&quot;

# Download best video (that also has audio) that is closest in size to 50 MB
$ yt-dlp -f &quot;b&quot; -S &quot;filesize~50M&quot;



# Download best video available via direct link over HTTP/HTTPS protocol,
# or the best video available via any protocol if there is no such video
$ yt-dlp -f &quot;(bv*+ba/b)[protocol^=http][protocol!*=dash] / (bv*+ba/b)&quot;

# Download best video available via the best protocol
# (https/ftps > http/ftp > m3u8_native > m3u8 > http_dash_segments ...)
$ yt-dlp -S &quot;proto&quot;



# Download the best video with either h264 or h265 codec,
# or the best video if there is no such video
$ yt-dlp -f &quot;(bv*[vcodec~='^((he|a)vc|h26[45])']+ba) / (bv*+ba/b)&quot;

# Download the best video with best codec no better than h264,
# or the best video with worst codec if there is no such video
$ yt-dlp -S &quot;codec:h264&quot;

# Download the best video with worst codec no worse than h264,
# or the best video with best codec if there is no such video
$ yt-dlp -S &quot;+codec:h264&quot;



# More complex examples

# Download the best video no better than 720p preferring framerate greater than 30,
# or the worst video (still preferring framerate greater than 30) if there is no such video
$ yt-dlp -f &quot;((bv*[fps>30]/bv*)[height<=720]/(wv*[fps>30]/wv*)) + ba / (b[fps>30]/b)[height<=720]/(w[fps>30]/w)&quot;

# Download the video with the largest resolution no better than 720p,
# or the video with the smallest resolution available if there is no such video,
# preferring larger framerate for formats with the same resolution
$ yt-dlp -S &quot;res:720,fps&quot;



# Download the video with smallest resolution no worse than 480p,
# or the video with the largest resolution available if there is no such video,
# preferring better codec and then larger total bitrate for the same resolution
$ yt-dlp -S &quot;+res:480,codec,br&quot;"><pre><span><span>#</span> Download and merge the best video-only format and the best audio-only format,</span>
<span><span>#</span> or download the best combined format if video-only format is not available</span>
$ yt-dlp -f <span><span>"</span>bv+ba/b<span>"</span></span>

<span><span>#</span> Download best format that contains video,</span>
<span><span>#</span> and if it doesn't already have an audio stream, merge it with best audio-only format</span>
$ yt-dlp -f <span><span>"</span>bv*+ba/b<span>"</span></span>

<span><span>#</span> Same as above</span>
$ yt-dlp

<span><span>#</span> Download the best video-only format and the best audio-only format without merging them</span>
<span><span>#</span> For this case, an output template should be used since</span>
<span><span>#</span> by default, bestvideo and bestaudio will have the same file name.</span>
$ yt-dlp -f <span><span>"</span>bv,ba<span>"</span></span> -o <span><span>"</span>%(title)s.f%(format_id)s.%(ext)s<span>"</span></span>

<span><span>#</span> Download and merge the best format that has a video stream,</span>
<span><span>#</span> and all audio-only formats into one file</span>
$ yt-dlp -f <span><span>"</span>bv*+mergeall[vcodec=none]<span>"</span></span> --audio-multistreams

<span><span>#</span> Download and merge the best format that has a video stream,</span>
<span><span>#</span> and the best 2 audio-only formats into one file</span>
$ yt-dlp -f <span><span>"</span>bv*+ba+ba.2<span>"</span></span> --audio-multistreams


<span><span>#</span> The following examples show the old method (without -S) of format selection</span>
<span><span>#</span> and how to use -S to achieve a similar but (generally) better result</span>

<span><span>#</span> Download the worst video available (old method)</span>
$ yt-dlp -f <span><span>"</span>wv*+wa/w<span>"</span></span>

<span><span>#</span> Download the best video available but with the smallest resolution</span>
$ yt-dlp -S <span><span>"</span>+res<span>"</span></span>

<span><span>#</span> Download the smallest video available</span>
$ yt-dlp -S <span><span>"</span>+size,+br<span>"</span></span>



<span><span>#</span> Download the best mp4 video available, or the best video if no mp4 available</span>
$ yt-dlp -f <span><span>"</span>bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4] / bv*+ba/b<span>"</span></span>

<span><span>#</span> Download the best video with the best extension</span>
<span><span>#</span> (For video, mp4 &gt; mov &gt; webm &gt; flv. For audio, m4a &gt; aac &gt; mp3 ...)</span>
$ yt-dlp -S <span><span>"</span>ext<span>"</span></span>



<span><span>#</span> Download the best video available but no better than 480p,</span>
<span><span>#</span> or the worst video if there is no video under 480p</span>
$ yt-dlp -f <span><span>"</span>bv*[height&lt;=480]+ba/b[height&lt;=480] / wv*+ba/w<span>"</span></span>

<span><span>#</span> Download the best video available with the largest height but no better than 480p,</span>
<span><span>#</span> or the best video with the smallest resolution if there is no video under 480p</span>
$ yt-dlp -S <span><span>"</span>height:480<span>"</span></span>

<span><span>#</span> Download the best video available with the largest resolution but no better than 480p,</span>
<span><span>#</span> or the best video with the smallest resolution if there is no video under 480p</span>
<span><span>#</span> Resolution is determined by using the smallest dimension.</span>
<span><span>#</span> So this works correctly for vertical videos as well</span>
$ yt-dlp -S <span><span>"</span>res:480<span>"</span></span>



<span><span>#</span> Download the best video (that also has audio) but no bigger than 50 MB,</span>
<span><span>#</span> or the worst video (that also has audio) if there is no video under 50 MB</span>
$ yt-dlp -f <span><span>"</span>b[filesize&lt;50M] / w<span>"</span></span>

<span><span>#</span> Download largest video (that also has audio) but no bigger than 50 MB,</span>
<span><span>#</span> or the smallest video (that also has audio) if there is no video under 50 MB</span>
$ yt-dlp -f <span><span>"</span>b<span>"</span></span> -S <span><span>"</span>filesize:50M<span>"</span></span>

<span><span>#</span> Download best video (that also has audio) that is closest in size to 50 MB</span>
$ yt-dlp -f <span><span>"</span>b<span>"</span></span> -S <span><span>"</span>filesize~50M<span>"</span></span>



<span><span>#</span> Download best video available via direct link over HTTP/HTTPS protocol,</span>
<span><span>#</span> or the best video available via any protocol if there is no such video</span>
$ yt-dlp -f <span><span>"</span>(bv*+ba/b)[protocol^=http][protocol!*=dash] / (bv*+ba/b)<span>"</span></span>

<span><span>#</span> Download best video available via the best protocol</span>
<span><span>#</span> (https/ftps &gt; http/ftp &gt; m3u8_native &gt; m3u8 &gt; http_dash_segments ...)</span>
$ yt-dlp -S <span><span>"</span>proto<span>"</span></span>



<span><span>#</span> Download the best video with either h264 or h265 codec,</span>
<span><span>#</span> or the best video if there is no such video</span>
$ yt-dlp -f <span><span>"</span>(bv*[vcodec~='^((he|a)vc|h26[45])']+ba) / (bv*+ba/b)<span>"</span></span>

<span><span>#</span> Download the best video with best codec no better than h264,</span>
<span><span>#</span> or the best video with worst codec if there is no such video</span>
$ yt-dlp -S <span><span>"</span>codec:h264<span>"</span></span>

<span><span>#</span> Download the best video with worst codec no worse than h264,</span>
<span><span>#</span> or the best video with best codec if there is no such video</span>
$ yt-dlp -S <span><span>"</span>+codec:h264<span>"</span></span>



<span><span>#</span> More complex examples</span>

<span><span>#</span> Download the best video no better than 720p preferring framerate greater than 30,</span>
<span><span>#</span> or the worst video (still preferring framerate greater than 30) if there is no such video</span>
$ yt-dlp -f <span><span>"</span>((bv*[fps&gt;30]/bv*)[height&lt;=720]/(wv*[fps&gt;30]/wv*)) + ba / (b[fps&gt;30]/b)[height&lt;=720]/(w[fps&gt;30]/w)<span>"</span></span>

<span><span>#</span> Download the video with the largest resolution no better than 720p,</span>
<span><span>#</span> or the video with the smallest resolution available if there is no such video,</span>
<span><span>#</span> preferring larger framerate for formats with the same resolution</span>
$ yt-dlp -S <span><span>"</span>res:720,fps<span>"</span></span>



<span><span>#</span> Download the video with smallest resolution no worse than 480p,</span>
<span><span>#</span> or the video with the largest resolution available if there is no such video,</span>
<span><span>#</span> preferring better codec and then larger total bitrate for the same resolution</span>
$ yt-dlp -S <span><span>"</span>+res:480,codec,br<span>"</span></span></pre></div>
<h2 tabindex="-1" dir="auto">MODIFYING METADATA</h2>
<p dir="auto">The metadata obtained by the extractors can be modified by using <code>--parse-metadata</code> and <code>--replace-in-metadata</code></p>
<p dir="auto"><code>--replace-in-metadata FIELDS REGEX REPLACE</code> is used to replace text in any metadata field using <a href="https://docs.python.org/3/library/re.html#regular-expression-syntax" rel="nofollow">python regular expression</a>. <a href="https://docs.python.org/3/library/re.html?highlight=backreferences#re.sub" rel="nofollow">Backreferences</a> can be used in the replace string for advanced use.</p>
<p dir="auto">The general syntax of <code>--parse-metadata FROM:TO</code> is to give the name of a field or an <a href="#output-template">output template</a> to extract data from, and the format to interpret it as, separated by a colon <code>:</code>. Either a <a href="https://docs.python.org/3/library/re.html#regular-expression-syntax" rel="nofollow">python regular expression</a> with named capture groups, a single field name, or a similar syntax to the <a href="#output-template">output template</a> (only <code>%(field)s</code> formatting is supported) can be used for <code>TO</code>. The option can be used multiple times to parse and modify various fields.</p>
<p dir="auto">Note that these options preserve their relative order, allowing replacements to be made in parsed fields and viceversa. Also, any field thus created can be used in the <a href="#output-template">output template</a> and will also affect the media file's metadata added when using <code>--embed-metadata</code>.</p>
<p dir="auto">This option also has a few special uses:</p>
<ul dir="auto">
<li>
<p dir="auto">You can download an additional URL based on the metadata of the currently downloaded video. To do this, set the field <code>additional_urls</code> to the URL that you want to download. E.g. <code>--parse-metadata "description:(?P&lt;additional_urls&gt;https?://www\.vimeo\.com/\d+)"</code> will download the first vimeo video found in the description</p>
</li>
<li>
<p dir="auto">You can use this to change the metadata that is embedded in the media file. To do this, set the value of the corresponding field with a <code>meta_</code> prefix. For example, any value you set to <code>meta_description</code> field will be added to the <code>description</code> field in the file - you can use this to set a different "description" and "synopsis". To modify the metadata of individual streams, use the <code>meta&lt;n&gt;_</code> prefix (e.g. <code>meta1_language</code>). Any value set to the <code>meta_</code> field will overwrite all default values.</p>
</li>
</ul>
<p dir="auto"><strong>Note</strong>: Metadata modification happens before format selection, post-extraction and other post-processing operations. Some fields may be added or changed during these steps, overriding your changes.</p>
<p dir="auto">For reference, these are the fields yt-dlp adds by default to the file metadata:</p>
<table>
<thead>
<tr>
<th>Metadata fields</th>
<th>From</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>title</code></td>
<td><code>track</code> or <code>title</code></td>
</tr>
<tr>
<td><code>date</code></td>
<td><code>upload_date</code></td>
</tr>
<tr>
<td><code>description</code>,  <code>synopsis</code></td>
<td><code>description</code></td>
</tr>
<tr>
<td><code>purl</code>, <code>comment</code></td>
<td><code>webpage_url</code></td>
</tr>
<tr>
<td><code>track</code></td>
<td><code>track_number</code></td>
</tr>
<tr>
<td><code>artist</code></td>
<td><code>artist</code>, <code>creator</code>, <code>uploader</code> or <code>uploader_id</code></td>
</tr>
<tr>
<td><code>genre</code></td>
<td><code>genre</code></td>
</tr>
<tr>
<td><code>album</code></td>
<td><code>album</code></td>
</tr>
<tr>
<td><code>album_artist</code></td>
<td><code>album_artist</code></td>
</tr>
<tr>
<td><code>disc</code></td>
<td><code>disc_number</code></td>
</tr>
<tr>
<td><code>show</code></td>
<td><code>series</code></td>
</tr>
<tr>
<td><code>season_number</code></td>
<td><code>season_number</code></td>
</tr>
<tr>
<td><code>episode_id</code></td>
<td><code>episode</code> or <code>episode_id</code></td>
</tr>
<tr>
<td><code>episode_sort</code></td>
<td><code>episode_number</code></td>
</tr>
<tr>
<td><code>language</code> of each stream</td>
<td>the format's <code>language</code></td>
</tr>
</tbody>
</table>
<p dir="auto"><strong>Note</strong>: The file format may not support some of these fields</p>
<h2 tabindex="-1" dir="auto">Modifying metadata examples</h2>
<div dir="auto" data-snippet-clipboard-copy-content="# Interpret the title as &quot;Artist - Title&quot;
$ yt-dlp --parse-metadata &quot;title:%(artist)s - %(title)s&quot;

# Regex example
$ yt-dlp --parse-metadata &quot;description:Artist - (?P<artist>.+)&quot;

# Set title as &quot;Series name S01E05&quot;
$ yt-dlp --parse-metadata &quot;%(series)s S%(season_number)02dE%(episode_number)02d:%(title)s&quot;

# Prioritize uploader as the &quot;artist&quot; field in video metadata
$ yt-dlp --parse-metadata &quot;%(uploader|)s:%(meta_artist)s&quot; --embed-metadata

# Set &quot;comment&quot; field in video metadata using description instead of webpage_url,
# handling multiple lines correctly
$ yt-dlp --parse-metadata &quot;description:(?s)(?P<meta_comment>.+)&quot; --embed-metadata

# Do not set any &quot;synopsis&quot; in the video metadata
$ yt-dlp --parse-metadata &quot;:(?P<meta_synopsis>)&quot;

# Remove &quot;formats&quot; field from the infojson by setting it to an empty string
$ yt-dlp --parse-metadata &quot;video::(?P<formats>)&quot; --write-info-json

# Replace all spaces and &quot;_&quot; in title and uploader with a `-`
$ yt-dlp --replace-in-metadata &quot;title,uploader&quot; &quot;[ _]&quot; &quot;-&quot;
"><pre><span><span>#</span> Interpret the title as "Artist - Title"</span>
$ yt-dlp --parse-metadata <span><span>"</span>title:%(artist)s - %(title)s<span>"</span></span>

<span><span>#</span> Regex example</span>
$ yt-dlp --parse-metadata <span><span>"</span>description:Artist - (?P&lt;artist&gt;.+)<span>"</span></span>

<span><span>#</span> Set title as "Series name S01E05"</span>
$ yt-dlp --parse-metadata <span><span>"</span>%(series)s S%(season_number)02dE%(episode_number)02d:%(title)s<span>"</span></span>

<span><span>#</span> Prioritize uploader as the "artist" field in video metadata</span>
$ yt-dlp --parse-metadata <span><span>"</span>%(uploader|)s:%(meta_artist)s<span>"</span></span> --embed-metadata

<span><span>#</span> Set "comment" field in video metadata using description instead of webpage_url,</span>
<span><span>#</span> handling multiple lines correctly</span>
$ yt-dlp --parse-metadata <span><span>"</span>description:(?s)(?P&lt;meta_comment&gt;.+)<span>"</span></span> --embed-metadata

<span><span>#</span> Do not set any "synopsis" in the video metadata</span>
$ yt-dlp --parse-metadata <span><span>"</span>:(?P&lt;meta_synopsis&gt;)<span>"</span></span>

<span><span>#</span> Remove "formats" field from the infojson by setting it to an empty string</span>
$ yt-dlp --parse-metadata <span><span>"</span>video::(?P&lt;formats&gt;)<span>"</span></span> --write-info-json

<span><span>#</span> Replace all spaces and "_" in title and uploader with a `-`</span>
$ yt-dlp --replace-in-metadata <span><span>"</span>title,uploader<span>"</span></span> <span><span>"</span>[ _]<span>"</span></span> <span><span>"</span>-<span>"</span></span>
</pre></div>
<h2 tabindex="-1" dir="auto">EXTRACTOR ARGUMENTS</h2>
<p dir="auto">Some extractors accept additional arguments which can be passed using <code>--extractor-args KEY:ARGS</code>. <code>ARGS</code> is a <code>;</code> (semicolon) separated string of <code>ARG=VAL1,VAL2</code>. E.g. <code>--extractor-args "youtube:player-client=android_embedded,web;include_live_dash" --extractor-args "funimation:version=uncut"</code></p>
<p dir="auto">Note: In CLI, <code>ARG</code> can use <code>-</code> instead of <code>_</code>; e.g. <code>youtube:player-client"</code> becomes <code>youtube:player_client"</code></p>
<p dir="auto">The following extractors use this feature:</p>
<h4 tabindex="-1" dir="auto">youtube</h4>
<ul dir="auto">
<li><code>lang</code>: Prefer translated metadata (<code>title</code>, <code>description</code> etc) of this language code (case-sensitive). By default, the video primary language metadata is preferred, with a fallback to <code>en</code> translated. See <a href="https://github.com/yt-dlp/yt-dlp/blob/c26f9b991a0681fd3ea548d535919cec1fbbd430/yt_dlp/extractor/youtube.py#L381-L390">youtube.py</a> for list of supported content language codes</li>
<li><code>skip</code>: One or more of <code>hls</code>, <code>dash</code> or <code>translated_subs</code> to skip extraction of the m3u8 manifests, dash manifests and <a href="https://github.com/yt-dlp/yt-dlp/issues/4090#issuecomment-1158102032" data-hovercard-type="issue" data-hovercard-url="/yt-dlp/yt-dlp/issues/4090/hovercard">auto-translated subtitles</a> respectively</li>
<li><code>player_client</code>: Clients to extract video data from. The main clients are <code>web</code>, <code>android</code> and <code>ios</code> with variants <code>_music</code>, <code>_embedded</code>, <code>_embedscreen</code>, <code>_creator</code> (e.g. <code>web_embedded</code>); and <code>mweb</code> and <code>tv_embedded</code> (agegate bypass) with no variants. By default, <code>ios,android,web</code> is used, but <code>tv_embedded</code> and <code>creator</code> variants are added as required for age-gated videos. Similarly, the music variants are added for <code>music.youtube.com</code> urls. You can use <code>all</code> to use all the clients, and <code>default</code> for the default clients.</li>
<li><code>player_skip</code>: Skip some network requests that are generally needed for robust extraction. One or more of <code>configs</code> (skip client configs), <code>webpage</code> (skip initial webpage), <code>js</code> (skip js player). While these options can help reduce the number of requests needed or avoid some rate-limiting, they could cause some issues. See <a href="https://github.com/yt-dlp/yt-dlp/pull/860" data-hovercard-type="pull_request" data-hovercard-url="/yt-dlp/yt-dlp/pull/860/hovercard">#860</a> for more details</li>
<li><code>player_params</code>: YouTube player parameters to use for player requests. Will overwrite any default ones set by yt-dlp.</li>
<li><code>comment_sort</code>: <code>top</code> or <code>new</code> (default) - choose comment sorting mode (on YouTube's side)</li>
<li><code>max_comments</code>: Limit the amount of comments to gather. Comma-separated list of integers representing <code>max-comments,max-parents,max-replies,max-replies-per-thread</code>. Default is <code>all,all,all,all</code>
<ul dir="auto">
<li>E.g. <code>all,all,1000,10</code> will get a maximum of 1000 replies total, with up to 10 replies per thread. <code>1000,all,100</code> will get a maximum of 1000 comments, with a maximum of 100 replies total</li>
</ul>
</li>
<li><code>formats</code>: Change the types of formats to return. <code>dashy</code> (convert HTTP to DASH), <code>duplicate</code> (identical content but different URLs or protocol; includes <code>dashy</code>), <code>incomplete</code> (cannot be downloaded completely - live dash and post-live m3u8)</li>
<li><code>innertube_host</code>: Innertube API host to use for all API requests; e.g. <code>studio.youtube.com</code>, <code>youtubei.googleapis.com</code>. Note that cookies exported from one subdomain will not work on others</li>
<li><code>innertube_key</code>: Innertube API key to use for all API requests</li>
</ul>
<h4 tabindex="-1" dir="auto">youtubetab (YouTube playlists, channels, feeds, etc.)</h4>
<ul dir="auto">
<li><code>skip</code>: One or more of <code>webpage</code> (skip initial webpage download), <code>authcheck</code> (allow the download of playlists requiring authentication when no initial webpage is downloaded. This may cause unwanted behavior, see <a href="https://github.com/yt-dlp/yt-dlp/pull/1122" data-hovercard-type="pull_request" data-hovercard-url="/yt-dlp/yt-dlp/pull/1122/hovercard">#1122</a> for more details)</li>
<li><code>approximate_date</code>: Extract approximate <code>upload_date</code> and <code>timestamp</code> in flat-playlist. This may cause date-based filters to be slightly off</li>
</ul>
<h4 tabindex="-1" dir="auto">generic</h4>
<ul dir="auto">
<li><code>fragment_query</code>: Passthrough any query in mpd/m3u8 manifest URLs to their fragments if no value is provided, or else apply the query string given as <code>fragment_query=VALUE</code>. Does not apply to ffmpeg</li>
<li><code>variant_query</code>: Passthrough the master m3u8 URL query to its variant playlist URLs if no value is provided, or else apply the query string given as <code>variant_query=VALUE</code></li>
<li><code>hls_key</code>: An HLS AES-128 key URI <em>or</em> key (as hex), and optionally the IV (as hex), in the form of <code>(URI|KEY)[,IV]</code>; e.g. <code>generic:hls_key=ABCDEF1234567980,0xFEDCBA0987654321</code>. Passing any of these values will force usage of the native HLS downloader and override the corresponding values found in the m3u8 playlist</li>
<li><code>is_live</code>: Bypass live HLS detection and manually set <code>live_status</code> - a value of <code>false</code> will set <code>not_live</code>, any other value (or no value) will set <code>is_live</code></li>
</ul>
<h4 tabindex="-1" dir="auto">funimation</h4>
<ul dir="auto">
<li><code>language</code>: Audio languages to extract, e.g. <code>funimation:language=english,japanese</code></li>
<li><code>version</code>: The video version to extract - <code>uncut</code> or <code>simulcast</code></li>
</ul>
<h4 tabindex="-1" dir="auto">crunchyrollbeta (Crunchyroll)</h4>
<ul dir="auto">
<li><code>format</code>: Which stream type(s) to extract (default: <code>adaptive_hls</code>). Potentially useful values include <code>adaptive_hls</code>, <code>adaptive_dash</code>, <code>vo_adaptive_hls</code>, <code>vo_adaptive_dash</code>, <code>download_hls</code>, <code>download_dash</code>, <code>multitrack_adaptive_hls_v2</code></li>
<li><code>hardsub</code>: Preference order for which hardsub versions to extract, or <code>all</code> (default: <code>None</code> = no hardsubs), e.g. <code>crunchyrollbeta:hardsub=en-US,None</code></li>
</ul>
<h4 tabindex="-1" dir="auto">vikichannel</h4>
<ul dir="auto">
<li><code>video_types</code>: Types of videos to download - one or more of <code>episodes</code>, <code>movies</code>, <code>clips</code>, <code>trailers</code></li>
</ul>
<h4 tabindex="-1" dir="auto">niconico</h4>
<ul dir="auto">
<li><code>segment_duration</code>: Segment duration in milliseconds for HLS-DMC formats. Use it at your own risk since this feature <strong>may result in your account termination.</strong></li>
</ul>
<h4 tabindex="-1" dir="auto">youtubewebarchive</h4>
<ul dir="auto">
<li><code>check_all</code>: Try to check more at the cost of more requests. One or more of <code>thumbnails</code>, <code>captures</code></li>
</ul>
<h4 tabindex="-1" dir="auto">gamejolt</h4>
<ul dir="auto">
<li><code>comment_sort</code>: <code>hot</code> (default), <code>you</code> (cookies needed), <code>top</code>, <code>new</code> - choose comment sorting mode (on GameJolt's side)</li>
</ul>
<h4 tabindex="-1" dir="auto">hotstar</h4>
<ul dir="auto">
<li><code>res</code>: resolution to ignore - one or more of <code>sd</code>, <code>hd</code>, <code>fhd</code></li>
<li><code>vcodec</code>: vcodec to ignore - one or more of <code>h264</code>, <code>h265</code>, <code>dvh265</code></li>
<li><code>dr</code>: dynamic range to ignore - one or more of <code>sdr</code>, <code>hdr10</code>, <code>dv</code></li>
</ul>
<h4 tabindex="-1" dir="auto">tiktok</h4>
<ul dir="auto">
<li><code>api_hostname</code>: Hostname to use for mobile API requests, e.g. <code>api-h2.tiktokv.com</code></li>
<li><code>app_version</code>: App version to call mobile APIs with - should be set along with <code>manifest_app_version</code>, e.g. <code>20.2.1</code></li>
<li><code>manifest_app_version</code>: Numeric app version to call mobile APIs with, e.g. <code>221</code></li>
</ul>
<h4 tabindex="-1" dir="auto">rokfinchannel</h4>
<ul dir="auto">
<li><code>tab</code>: Which tab to download - one of <code>new</code>, <code>top</code>, <code>videos</code>, <code>podcasts</code>, <code>streams</code>, <code>stacks</code></li>
</ul>
<h4 tabindex="-1" dir="auto">twitter</h4>
<ul dir="auto">
<li><code>api</code>: Select one of <code>graphql</code> (default), <code>legacy</code> or <code>syndication</code> as the API for tweet extraction. Has no effect if logged in</li>
</ul>
<h4 tabindex="-1" dir="auto">stacommu, wrestleuniverse</h4>
<ul dir="auto">
<li><code>device_id</code>: UUID value assigned by the website and used to enforce device limits for paid livestream content. Can be found in browser local storage</li>
</ul>
<h4 tabindex="-1" dir="auto">twitch</h4>
<ul dir="auto">
<li><code>client_id</code>: Client ID value to be sent with GraphQL requests, e.g. <code>twitch:client_id=kimne78kx3ncx6brgo4mv6wki5h1ko</code></li>
</ul>
<h4 tabindex="-1" dir="auto">nhkradirulive (NHK  LIVE)</h4>
<ul dir="auto">
<li><code>area</code>: Which regional variation to extract. Valid areas are: <code>sapporo</code>, <code>sendai</code>, <code>tokyo</code>, <code>nagoya</code>, <code>osaka</code>, <code>hiroshima</code>, <code>matsuyama</code>, <code>fukuoka</code>. Defaults to <code>tokyo</code></li>
</ul>
<p dir="auto"><strong>Note</strong>: These options may be changed/removed in the future without concern for backward compatibility</p>

<h2 tabindex="-1" dir="auto">PLUGINS</h2>
<p dir="auto">Note that <strong>all</strong> plugins are imported even if not invoked, and that <strong>there are no checks</strong> performed on plugin code. <strong>Use plugins at your own risk and only if you trust the code!</strong></p>
<p dir="auto">Plugins can be of <code>&lt;type&gt;</code>s <code>extractor</code> or <code>postprocessor</code>.</p>
<ul dir="auto">
<li>Extractor plugins do not need to be enabled from the CLI and are automatically invoked when the input URL is suitable for it.</li>
<li>Extractor plugins take priority over builtin extractors.</li>
<li>Postprocessor plugins can be invoked using <code>--use-postprocessor NAME</code>.</li>
</ul>
<p dir="auto">Plugins are loaded from the namespace packages <code>yt_dlp_plugins.extractor</code> and <code>yt_dlp_plugins.postprocessor</code>.</p>
<p dir="auto">In other words, the file structure on the disk looks something like:</p>
<div data-snippet-clipboard-copy-content="    yt_dlp_plugins/
        extractor/
            myplugin.py
        postprocessor/
            myplugin.py"><pre><code>    yt_dlp_plugins/
        extractor/
            myplugin.py
        postprocessor/
            myplugin.py
</code></pre></div>
<p dir="auto">yt-dlp looks for these <code>yt_dlp_plugins</code> namespace folders in many locations (see below) and loads in plugins from <strong>all</strong> of them.</p>
<p dir="auto">See the <a href="https://github.com/yt-dlp/yt-dlp/wiki/Plugins">wiki for some known plugins</a></p>
<h2 tabindex="-1" dir="auto">Installing Plugins</h2>
<p dir="auto">Plugins can be installed using various methods and locations.</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Configuration directories</strong>:
Plugin packages (containing a <code>yt_dlp_plugins</code> namespace folder) can be dropped into the following standard <a href="#configuration">configuration locations</a>:</p>
<ul dir="auto">
<li><strong>User Plugins</strong>
<ul dir="auto">
<li><code>${XDG_CONFIG_HOME}/yt-dlp/plugins/&lt;package name&gt;/yt_dlp_plugins/</code> (recommended on Linux/macOS)</li>
<li><code>${XDG_CONFIG_HOME}/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
<li><code>${APPDATA}/yt-dlp/plugins/&lt;package name&gt;/yt_dlp_plugins/</code> (recommended on Windows)</li>
<li><code>${APPDATA}/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
<li><code>~/.yt-dlp/plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
<li><code>~/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
</ul>
</li>
<li><strong>System Plugins</strong>
<ul dir="auto">
<li><code>/etc/yt-dlp/plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
<li><code>/etc/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Executable location</strong>: Plugin packages can similarly be installed in a <code>yt-dlp-plugins</code> directory under the executable location (recommended for portable installations):</p>
<ul dir="auto">
<li>Binary: where <code>&lt;root-dir&gt;/yt-dlp.exe</code>, <code>&lt;root-dir&gt;/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
<li>Source: where <code>&lt;root-dir&gt;/yt_dlp/__main__.py</code>, <code>&lt;root-dir&gt;/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
</ul>
</li>
<li>
<p dir="auto"><strong>pip and other locations in <code>PYTHONPATH</code></strong></p>
<ul dir="auto">
<li>Plugin packages can be installed and managed using <code>pip</code>. See <a href="https://github.com/yt-dlp/yt-dlp-sample-plugins">yt-dlp-sample-plugins</a> for an example.
<ul dir="auto">
<li>Note: plugin files between plugin packages installed with pip must have unique filenames.</li>
</ul>
</li>
<li>Any path in <code>PYTHONPATH</code> is searched in for the <code>yt_dlp_plugins</code> namespace folder.
<ul dir="auto">
<li>Note: This does not apply for Pyinstaller/py2exe builds.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p dir="auto"><code>.zip</code>, <code>.egg</code> and <code>.whl</code> archives containing a <code>yt_dlp_plugins</code> namespace folder in their root are also supported as plugin packages.</p>
<ul dir="auto">
<li>e.g. <code>${XDG_CONFIG_HOME}/yt-dlp/plugins/mypluginpkg.zip</code> where <code>mypluginpkg.zip</code> contains <code>yt_dlp_plugins/&lt;type&gt;/myplugin.py</code></li>
</ul>
<p dir="auto">Run yt-dlp with <code>--verbose</code> to check if the plugin has been loaded.</p>
<h2 tabindex="-1" dir="auto">Developing Plugins</h2>
<p dir="auto">See the <a href="https://github.com/yt-dlp/yt-dlp-sample-plugins">yt-dlp-sample-plugins</a> repo for a template plugin package and the <a href="https://github.com/yt-dlp/yt-dlp/wiki/Plugin-Development">Plugin Development</a> section of the wiki for a plugin development guide.</p>
<p dir="auto">All public classes with a name ending in <code>IE</code>/<code>PP</code> are imported from each file for extractors and postprocessors repectively. This respects underscore prefix (e.g. <code>_MyBasePluginIE</code> is private) and <code>__all__</code>. Modules can similarly be excluded by prefixing the module name with an underscore (e.g. <code>_myplugin.py</code>).</p>
<p dir="auto">To replace an existing extractor with a subclass of one, set the <code>plugin_name</code> class keyword argument (e.g. <code>class MyPluginIE(ABuiltInIE, plugin_name='myplugin')</code> will replace <code>ABuiltInIE</code> with <code>MyPluginIE</code>). Since the extractor replaces the parent, you should exclude the subclass extractor from being imported separately by making it private using one of the methods described above.</p>
<p dir="auto">If you are a plugin author, add <a href="https://github.com/topics/yt-dlp-plugins">yt-dlp-plugins</a> as a topic to your repository for discoverability.</p>
<p dir="auto">See the <a href="https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions">Developer Instructions</a> on how to write and test an extractor.</p>
<h2 tabindex="-1" dir="auto">EMBEDDING YT-DLP</h2>
<p dir="auto">yt-dlp makes the best effort to be a good command-line program, and thus should be callable from any programming language.</p>
<p dir="auto">Your program should avoid parsing the normal stdout since they may change in future versions. Instead they should use options such as <code>-J</code>, <code>--print</code>, <code>--progress-template</code>, <code>--exec</code> etc to create console output that you can reliably reproduce and parse.</p>
<p dir="auto">From a Python program, you can embed yt-dlp in a more powerful fashion, like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from yt_dlp import YoutubeDL

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']
with YoutubeDL() as ydl:
    ydl.download(URLS)"><pre><span>from</span> <span>yt_dlp</span> <span>import</span> <span>YoutubeDL</span>

<span>URLS</span> <span>=</span> [<span>'https://www.youtube.com/watch?v=BaW_jenozKc'</span>]
<span>with</span> <span>YoutubeDL</span>() <span>as</span> <span>ydl</span>:
    <span>ydl</span>.<span>download</span>(<span>URLS</span>)</pre></div>
<p dir="auto">Most likely, you'll want to use various options. For a list of options available, have a look at <a href="https://github.com/yt-dlp/yt-dlp/blob/master/yt_dlp/YoutubeDL.py#L183"><code>yt_dlp/YoutubeDL.py</code></a> or <code>help(yt_dlp.YoutubeDL)</code> in a Python shell. If you are already familiar with the CLI, you can use <a href="https://github.com/yt-dlp/yt-dlp/blob/master/devscripts/cli_to_api.py"><code>devscripts/cli_to_api.py</code></a> to translate any CLI switches to <code>YoutubeDL</code> params.</p>
<p dir="auto"><strong>Tip</strong>: If you are porting your code from youtube-dl to yt-dlp, one important point to look out for is that we do not guarantee the return value of <code>YoutubeDL.extract_info</code> to be json serializable, or even be a dictionary. It will be dictionary-like, but if you want to ensure it is a serializable dictionary, pass it through <code>YoutubeDL.sanitize_info</code> as shown in the <a href="#extracting-information">example below</a></p>
<h2 tabindex="-1" dir="auto">Embedding examples</h2>
<h4 tabindex="-1" dir="auto">Extracting information</h4>
<div dir="auto" data-snippet-clipboard-copy-content="import json
import yt_dlp

URL = 'https://www.youtube.com/watch?v=BaW_jenozKc'

#  See help(yt_dlp.YoutubeDL) for a list of available options and public functions
ydl_opts = {}
with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    info = ydl.extract_info(URL, download=False)

    #  ydl.sanitize_info makes the info json-serializable
    print(json.dumps(ydl.sanitize_info(info)))"><pre><span>import</span> <span>json</span>
<span>import</span> <span>yt_dlp</span>

<span>URL</span> <span>=</span> <span>'https://www.youtube.com/watch?v=BaW_jenozKc'</span>

<span>#  See help(yt_dlp.YoutubeDL) for a list of available options and public functions</span>
<span>ydl_opts</span> <span>=</span> {}
<span>with</span> <span>yt_dlp</span>.<span>YoutubeDL</span>(<span>ydl_opts</span>) <span>as</span> <span>ydl</span>:
    <span>info</span> <span>=</span> <span>ydl</span>.<span>extract_info</span>(<span>URL</span>, <span>download</span><span>=</span><span>False</span>)

    <span>#  ydl.sanitize_info makes the info json-serializable</span>
    <span>print</span>(<span>json</span>.<span>dumps</span>(<span>ydl</span>.<span>sanitize_info</span>(<span>info</span>)))</pre></div>
<h4 tabindex="-1" dir="auto">Download using an info-json</h4>
<div dir="auto" data-snippet-clipboard-copy-content="import yt_dlp

INFO_FILE = 'path/to/video.info.json'

with yt_dlp.YoutubeDL() as ydl:
    error_code = ydl.download_with_info_file(INFO_FILE)

print('Some videos failed to download' if error_code
      else 'All videos successfully downloaded')"><pre><span>import</span> <span>yt_dlp</span>

<span>INFO_FILE</span> <span>=</span> <span>'path/to/video.info.json'</span>

<span>with</span> <span>yt_dlp</span>.<span>YoutubeDL</span>() <span>as</span> <span>ydl</span>:
    <span>error_code</span> <span>=</span> <span>ydl</span>.<span>download_with_info_file</span>(<span>INFO_FILE</span>)

<span>print</span>(<span>'Some videos failed to download'</span> <span>if</span> <span>error_code</span>
      <span>else</span> <span>'All videos successfully downloaded'</span>)</pre></div>
<h4 tabindex="-1" dir="auto">Extract audio</h4>
<div dir="auto" data-snippet-clipboard-copy-content="import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

ydl_opts = {
    'format': 'm4a/bestaudio/best',
    #  See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments
    'postprocessors': [{  # Extract audio using ffmpeg
        'key': 'FFmpegExtractAudio',
        'preferredcodec': 'm4a',
    }]
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    error_code = ydl.download(URLS)"><pre><span>import</span> <span>yt_dlp</span>

<span>URLS</span> <span>=</span> [<span>'https://www.youtube.com/watch?v=BaW_jenozKc'</span>]

<span>ydl_opts</span> <span>=</span> {
    <span>'format'</span>: <span>'m4a/bestaudio/best'</span>,
    <span>#  See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments</span>
    <span>'postprocessors'</span>: [{  <span># Extract audio using ffmpeg</span>
        <span>'key'</span>: <span>'FFmpegExtractAudio'</span>,
        <span>'preferredcodec'</span>: <span>'m4a'</span>,
    }]
}

<span>with</span> <span>yt_dlp</span>.<span>YoutubeDL</span>(<span>ydl_opts</span>) <span>as</span> <span>ydl</span>:
    <span>error_code</span> <span>=</span> <span>ydl</span>.<span>download</span>(<span>URLS</span>)</pre></div>
<h4 tabindex="-1" dir="auto">Filter videos</h4>
<div dir="auto" data-snippet-clipboard-copy-content="import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

def longer_than_a_minute(info, *, incomplete):
    &quot;&quot;&quot;Download only videos longer than a minute (or with unknown duration)&quot;&quot;&quot;
    duration = info.get('duration')
    if duration and duration < 60:
        return 'The video is too short'

ydl_opts = {
    'match_filter': longer_than_a_minute,
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    error_code = ydl.download(URLS)"><pre><span>import</span> <span>yt_dlp</span>

<span>URLS</span> <span>=</span> [<span>'https://www.youtube.com/watch?v=BaW_jenozKc'</span>]

<span>def</span> <span>longer_than_a_minute</span>(<span>info</span>, <span>*</span>, <span>incomplete</span>):
    <span>"""Download only videos longer than a minute (or with unknown duration)"""</span>
    <span>duration</span> <span>=</span> <span>info</span>.<span>get</span>(<span>'duration'</span>)
    <span>if</span> <span>duration</span> <span>and</span> <span>duration</span> <span>&lt;</span> <span>60</span>:
        <span>return</span> <span>'The video is too short'</span>

<span>ydl_opts</span> <span>=</span> {
    <span>'match_filter'</span>: <span>longer_than_a_minute</span>,
}

<span>with</span> <span>yt_dlp</span>.<span>YoutubeDL</span>(<span>ydl_opts</span>) <span>as</span> <span>ydl</span>:
    <span>error_code</span> <span>=</span> <span>ydl</span>.<span>download</span>(<span>URLS</span>)</pre></div>
<h4 tabindex="-1" dir="auto">Adding logger and progress hook</h4>
<div dir="auto" data-snippet-clipboard-copy-content="import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

class MyLogger:
    def debug(self, msg):
        # For compatibility with youtube-dl, both debug and info are passed into debug
        # You can distinguish them by the prefix '[debug] '
        if msg.startswith('[debug] '):
            pass
        else:
            self.info(msg)

    def info(self, msg):
        pass

    def warning(self, msg):
        pass

    def error(self, msg):
        print(msg)


#  See &quot;progress_hooks&quot; in help(yt_dlp.YoutubeDL)
def my_hook(d):
    if d['status'] == 'finished':
        print('Done downloading, now post-processing ...')


ydl_opts = {
    'logger': MyLogger(),
    'progress_hooks': [my_hook],
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    ydl.download(URLS)"><pre><span>import</span> <span>yt_dlp</span>

<span>URLS</span> <span>=</span> [<span>'https://www.youtube.com/watch?v=BaW_jenozKc'</span>]

<span>class</span> <span>MyLogger</span>:
    <span>def</span> <span>debug</span>(<span>self</span>, <span>msg</span>):
        <span># For compatibility with youtube-dl, both debug and info are passed into debug</span>
        <span># You can distinguish them by the prefix '[debug] '</span>
        <span>if</span> <span>msg</span>.<span>startswith</span>(<span>'[debug] '</span>):
            <span>pass</span>
        <span>else</span>:
            <span>self</span>.<span>info</span>(<span>msg</span>)

    <span>def</span> <span>info</span>(<span>self</span>, <span>msg</span>):
        <span>pass</span>

    <span>def</span> <span>warning</span>(<span>self</span>, <span>msg</span>):
        <span>pass</span>

    <span>def</span> <span>error</span>(<span>self</span>, <span>msg</span>):
        <span>print</span>(<span>msg</span>)


<span>#  See "progress_hooks" in help(yt_dlp.YoutubeDL)</span>
<span>def</span> <span>my_hook</span>(<span>d</span>):
    <span>if</span> <span>d</span>[<span>'status'</span>] <span>==</span> <span>'finished'</span>:
        <span>print</span>(<span>'Done downloading, now post-processing ...'</span>)


<span>ydl_opts</span> <span>=</span> {
    <span>'logger'</span>: <span>MyLogger</span>(),
    <span>'progress_hooks'</span>: [<span>my_hook</span>],
}

<span>with</span> <span>yt_dlp</span>.<span>YoutubeDL</span>(<span>ydl_opts</span>) <span>as</span> <span>ydl</span>:
    <span>ydl</span>.<span>download</span>(<span>URLS</span>)</pre></div>
<h4 tabindex="-1" dir="auto">Add a custom PostProcessor</h4>
<div dir="auto" data-snippet-clipboard-copy-content="import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

#  See help(yt_dlp.postprocessor.PostProcessor)
class MyCustomPP(yt_dlp.postprocessor.PostProcessor):
    def run(self, info):
        self.to_screen('Doing stuff')
        return [], info


with yt_dlp.YoutubeDL() as ydl:
    #  &quot;when&quot; can take any value in yt_dlp.utils.POSTPROCESS_WHEN
    ydl.add_post_processor(MyCustomPP(), when='pre_process')
    ydl.download(URLS)"><pre><span>import</span> <span>yt_dlp</span>

<span>URLS</span> <span>=</span> [<span>'https://www.youtube.com/watch?v=BaW_jenozKc'</span>]

<span>#  See help(yt_dlp.postprocessor.PostProcessor)</span>
<span>class</span> <span>MyCustomPP</span>(<span>yt_dlp</span>.<span>postprocessor</span>.<span>PostProcessor</span>):
    <span>def</span> <span>run</span>(<span>self</span>, <span>info</span>):
        <span>self</span>.<span>to_screen</span>(<span>'Doing stuff'</span>)
        <span>return</span> [], <span>info</span>


<span>with</span> <span>yt_dlp</span>.<span>YoutubeDL</span>() <span>as</span> <span>ydl</span>:
    <span>#  "when" can take any value in yt_dlp.utils.POSTPROCESS_WHEN</span>
    <span>ydl</span>.<span>add_post_processor</span>(<span>MyCustomPP</span>(), <span>when</span><span>=</span><span>'pre_process'</span>)
    <span>ydl</span>.<span>download</span>(<span>URLS</span>)</pre></div>
<h4 tabindex="-1" dir="auto">Use a custom format selector</h4>
<div dir="auto" data-snippet-clipboard-copy-content="import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

def format_selector(ctx):
    &quot;&quot;&quot; Select the best video and the best audio that won't result in an mkv.
    NOTE: This is just an example and does not handle all cases &quot;&quot;&quot;

    # formats are already sorted worst to best
    formats = ctx.get('formats')[::-1]

    # acodec='none' means there is no audio
    best_video = next(f for f in formats
                      if f['vcodec'] != 'none' and f['acodec'] == 'none')

    # find compatible audio extension
    audio_ext = {'mp4': 'm4a', 'webm': 'webm'}[best_video['ext']]
    # vcodec='none' means there is no video
    best_audio = next(f for f in formats if (
        f['acodec'] != 'none' and f['vcodec'] == 'none' and f['ext'] == audio_ext))

    # These are the minimum required fields for a merged format
    yield {
        'format_id': f'{best_video[&quot;format_id&quot;]}+{best_audio[&quot;format_id&quot;]}',
        'ext': best_video['ext'],
        'requested_formats': [best_video, best_audio],
        # Must be + separated list of protocols
        'protocol': f'{best_video[&quot;protocol&quot;]}+{best_audio[&quot;protocol&quot;]}'
    }


ydl_opts = {
    'format': format_selector,
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    ydl.download(URLS)"><pre><span>import</span> <span>yt_dlp</span>

<span>URLS</span> <span>=</span> [<span>'https://www.youtube.com/watch?v=BaW_jenozKc'</span>]

<span>def</span> <span>format_selector</span>(<span>ctx</span>):
    <span>""" Select the best video and the best audio that won't result in an mkv.</span>
<span>    NOTE: This is just an example and does not handle all cases """</span>

    <span># formats are already sorted worst to best</span>
    <span>formats</span> <span>=</span> <span>ctx</span>.<span>get</span>(<span>'formats'</span>)[::<span>-</span><span>1</span>]

    <span># acodec='none' means there is no audio</span>
    <span>best_video</span> <span>=</span> <span>next</span>(<span>f</span> <span>for</span> <span>f</span> <span>in</span> <span>formats</span>
                      <span>if</span> <span>f</span>[<span>'vcodec'</span>] <span>!=</span> <span>'none'</span> <span>and</span> <span>f</span>[<span>'acodec'</span>] <span>==</span> <span>'none'</span>)

    <span># find compatible audio extension</span>
    <span>audio_ext</span> <span>=</span> {<span>'mp4'</span>: <span>'m4a'</span>, <span>'webm'</span>: <span>'webm'</span>}[<span>best_video</span>[<span>'ext'</span>]]
    <span># vcodec='none' means there is no video</span>
    <span>best_audio</span> <span>=</span> <span>next</span>(<span>f</span> <span>for</span> <span>f</span> <span>in</span> <span>formats</span> <span>if</span> (
        <span>f</span>[<span>'acodec'</span>] <span>!=</span> <span>'none'</span> <span>and</span> <span>f</span>[<span>'vcodec'</span>] <span>==</span> <span>'none'</span> <span>and</span> <span>f</span>[<span>'ext'</span>] <span>==</span> <span>audio_ext</span>))

    <span># These are the minimum required fields for a merged format</span>
    <span>yield</span> {
        <span>'format_id'</span>: <span>f'<span><span>{</span><span>best_video</span>[<span>"format_id"</span>]<span>}</span></span>+<span><span>{</span><span>best_audio</span>[<span>"format_id"</span>]<span>}</span></span>'</span>,
        <span>'ext'</span>: <span>best_video</span>[<span>'ext'</span>],
        <span>'requested_formats'</span>: [<span>best_video</span>, <span>best_audio</span>],
        <span># Must be + separated list of protocols</span>
        <span>'protocol'</span>: <span>f'<span><span>{</span><span>best_video</span>[<span>"protocol"</span>]<span>}</span></span>+<span><span>{</span><span>best_audio</span>[<span>"protocol"</span>]<span>}</span></span>'</span>
    }


<span>ydl_opts</span> <span>=</span> {
    <span>'format'</span>: <span>format_selector</span>,
}

<span>with</span> <span>yt_dlp</span>.<span>YoutubeDL</span>(<span>ydl_opts</span>) <span>as</span> <span>ydl</span>:
    <span>ydl</span>.<span>download</span>(<span>URLS</span>)</pre></div>

<h2 tabindex="-1" dir="auto">DEPRECATED OPTIONS</h2>
<p dir="auto">These are all the deprecated options and the current alternative to achieve the same effect</p>
<h4 tabindex="-1" dir="auto">Almost redundant options</h4>
<p dir="auto">While these options are almost the same as their new counterparts, there are some differences that prevents them being redundant</p>
<div data-snippet-clipboard-copy-content="-j, --dump-json                  --print &quot;%()j&quot;
-F, --list-formats               --print formats_table
--list-thumbnails                --print thumbnails_table --print playlist:thumbnails_table
--list-subs                      --print automatic_captions_table --print subtitles_table"><pre><code>-j, --dump-json                  --print "%()j"
-F, --list-formats               --print formats_table
--list-thumbnails                --print thumbnails_table --print playlist:thumbnails_table
--list-subs                      --print automatic_captions_table --print subtitles_table
</code></pre></div>
<h4 tabindex="-1" dir="auto">Redundant options</h4>
<p dir="auto">While these options are redundant, they are still expected to be used due to their ease of use</p>
<div data-snippet-clipboard-copy-content="--get-description                --print description
--get-duration                   --print duration_string
--get-filename                   --print filename
--get-format                     --print format
--get-id                         --print id
--get-thumbnail                  --print thumbnail
-e, --get-title                  --print title
-g, --get-url                    --print urls
--match-title REGEX              --match-filter &quot;title ~= (?i)REGEX&quot;
--reject-title REGEX             --match-filter &quot;title !~= (?i)REGEX&quot;
--min-views COUNT                --match-filter &quot;view_count >=? COUNT&quot;
--max-views COUNT                --match-filter &quot;view_count <=? COUNT&quot;
--break-on-reject                Use --break-match-filter
--user-agent UA                  --add-header &quot;User-Agent:UA&quot;
--referer URL                    --add-header &quot;Referer:URL&quot;
--playlist-start NUMBER          -I NUMBER:
--playlist-end NUMBER            -I :NUMBER
--playlist-reverse               -I ::-1
--no-playlist-reverse            Default
--no-colors                      --color no_color"><pre><code>--get-description                --print description
--get-duration                   --print duration_string
--get-filename                   --print filename
--get-format                     --print format
--get-id                         --print id
--get-thumbnail                  --print thumbnail
-e, --get-title                  --print title
-g, --get-url                    --print urls
--match-title REGEX              --match-filter "title ~= (?i)REGEX"
--reject-title REGEX             --match-filter "title !~= (?i)REGEX"
--min-views COUNT                --match-filter "view_count &gt;=? COUNT"
--max-views COUNT                --match-filter "view_count &lt;=? COUNT"
--break-on-reject                Use --break-match-filter
--user-agent UA                  --add-header "User-Agent:UA"
--referer URL                    --add-header "Referer:URL"
--playlist-start NUMBER          -I NUMBER:
--playlist-end NUMBER            -I :NUMBER
--playlist-reverse               -I ::-1
--no-playlist-reverse            Default
--no-colors                      --color no_color
</code></pre></div>
<h4 tabindex="-1" dir="auto">Not recommended</h4>
<p dir="auto">While these options still work, their use is not recommended since there are other alternatives to achieve the same</p>
<div data-snippet-clipboard-copy-content="--force-generic-extractor        --ies generic,default
--exec-before-download CMD       --exec &quot;before_dl:CMD&quot;
--no-exec-before-download        --no-exec
--all-formats                    -f all
--all-subs                       --sub-langs all --write-subs
--print-json                     -j --no-simulate
--autonumber-size NUMBER         Use string formatting, e.g. %(autonumber)03d
--autonumber-start NUMBER        Use internal field formatting like %(autonumber+NUMBER)s
--id                             -o &quot;%(id)s.%(ext)s&quot;
--metadata-from-title FORMAT     --parse-metadata &quot;%(title)s:FORMAT&quot;
--hls-prefer-native              --downloader &quot;m3u8:native&quot;
--hls-prefer-ffmpeg              --downloader &quot;m3u8:ffmpeg&quot;
--list-formats-old               --compat-options list-formats (Alias: --no-list-formats-as-table)
--list-formats-as-table          --compat-options -list-formats [Default] (Alias: --no-list-formats-old)
--youtube-skip-dash-manifest     --extractor-args &quot;youtube:skip=dash&quot; (Alias: --no-youtube-include-dash-manifest)
--youtube-skip-hls-manifest      --extractor-args &quot;youtube:skip=hls&quot; (Alias: --no-youtube-include-hls-manifest)
--youtube-include-dash-manifest  Default (Alias: --no-youtube-skip-dash-manifest)
--youtube-include-hls-manifest   Default (Alias: --no-youtube-skip-hls-manifest)
--geo-bypass                     --xff &quot;default&quot;
--no-geo-bypass                  --xff &quot;never&quot;
--geo-bypass-country CODE        --xff CODE
--geo-bypass-ip-block IP_BLOCK   --xff IP_BLOCK"><pre><code>--force-generic-extractor        --ies generic,default
--exec-before-download CMD       --exec "before_dl:CMD"
--no-exec-before-download        --no-exec
--all-formats                    -f all
--all-subs                       --sub-langs all --write-subs
--print-json                     -j --no-simulate
--autonumber-size NUMBER         Use string formatting, e.g. %(autonumber)03d
--autonumber-start NUMBER        Use internal field formatting like %(autonumber+NUMBER)s
--id                             -o "%(id)s.%(ext)s"
--metadata-from-title FORMAT     --parse-metadata "%(title)s:FORMAT"
--hls-prefer-native              --downloader "m3u8:native"
--hls-prefer-ffmpeg              --downloader "m3u8:ffmpeg"
--list-formats-old               --compat-options list-formats (Alias: --no-list-formats-as-table)
--list-formats-as-table          --compat-options -list-formats [Default] (Alias: --no-list-formats-old)
--youtube-skip-dash-manifest     --extractor-args "youtube:skip=dash" (Alias: --no-youtube-include-dash-manifest)
--youtube-skip-hls-manifest      --extractor-args "youtube:skip=hls" (Alias: --no-youtube-include-hls-manifest)
--youtube-include-dash-manifest  Default (Alias: --no-youtube-skip-dash-manifest)
--youtube-include-hls-manifest   Default (Alias: --no-youtube-skip-hls-manifest)
--geo-bypass                     --xff "default"
--no-geo-bypass                  --xff "never"
--geo-bypass-country CODE        --xff CODE
--geo-bypass-ip-block IP_BLOCK   --xff IP_BLOCK
</code></pre></div>
<h4 tabindex="-1" dir="auto">Developer options</h4>
<p dir="auto">These options are not intended to be used by the end-user</p>
<div data-snippet-clipboard-copy-content="--test                           Download only part of video for testing extractors
--load-pages                     Load pages dumped by --write-pages
--youtube-print-sig-code         For testing youtube signatures
--allow-unplayable-formats       List unplayable formats also
--no-allow-unplayable-formats    Default"><pre><code>--test                           Download only part of video for testing extractors
--load-pages                     Load pages dumped by --write-pages
--youtube-print-sig-code         For testing youtube signatures
--allow-unplayable-formats       List unplayable formats also
--no-allow-unplayable-formats    Default
</code></pre></div>
<h4 tabindex="-1" dir="auto">Old aliases</h4>
<p dir="auto">These are aliases that are no longer documented for various reasons</p>
<div data-snippet-clipboard-copy-content="--avconv-location                --ffmpeg-location
--clean-infojson                 --clean-info-json
--cn-verification-proxy URL      --geo-verification-proxy URL
--dump-headers                   --print-traffic
--dump-intermediate-pages        --dump-pages
--force-write-download-archive   --force-write-archive
--load-info                      --load-info-json
--no-clean-infojson              --no-clean-info-json
--no-split-tracks                --no-split-chapters
--no-write-srt                   --no-write-subs
--prefer-unsecure                --prefer-insecure
--rate-limit RATE                --limit-rate RATE
--split-tracks                   --split-chapters
--srt-lang LANGS                 --sub-langs LANGS
--trim-file-names LENGTH         --trim-filenames LENGTH
--write-srt                      --write-subs
--yes-overwrites                 --force-overwrites"><pre><code>--avconv-location                --ffmpeg-location
--clean-infojson                 --clean-info-json
--cn-verification-proxy URL      --geo-verification-proxy URL
--dump-headers                   --print-traffic
--dump-intermediate-pages        --dump-pages
--force-write-download-archive   --force-write-archive
--load-info                      --load-info-json
--no-clean-infojson              --no-clean-info-json
--no-split-tracks                --no-split-chapters
--no-write-srt                   --no-write-subs
--prefer-unsecure                --prefer-insecure
--rate-limit RATE                --limit-rate RATE
--split-tracks                   --split-chapters
--srt-lang LANGS                 --sub-langs LANGS
--trim-file-names LENGTH         --trim-filenames LENGTH
--write-srt                      --write-subs
--yes-overwrites                 --force-overwrites
</code></pre></div>
<h4 tabindex="-1" dir="auto">Sponskrub Options</h4>
<p dir="auto">Support for <a href="https://github.com/faissaloo/SponSkrub">SponSkrub</a> has been deprecated in favor of the <code>--sponsorblock</code> options</p>
<div data-snippet-clipboard-copy-content="--sponskrub                      --sponsorblock-mark all
--no-sponskrub                   --no-sponsorblock
--sponskrub-cut                  --sponsorblock-remove all
--no-sponskrub-cut               --sponsorblock-remove -all
--sponskrub-force                Not applicable
--no-sponskrub-force             Not applicable
--sponskrub-location             Not applicable
--sponskrub-args                 Not applicable"><pre><code>--sponskrub                      --sponsorblock-mark all
--no-sponskrub                   --no-sponsorblock
--sponskrub-cut                  --sponsorblock-remove all
--no-sponskrub-cut               --sponsorblock-remove -all
--sponskrub-force                Not applicable
--no-sponskrub-force             Not applicable
--sponskrub-location             Not applicable
--sponskrub-args                 Not applicable
</code></pre></div>
<h4 tabindex="-1" dir="auto">No longer supported</h4>
<p dir="auto">These options may no longer work as intended</p>
<div data-snippet-clipboard-copy-content="--prefer-avconv                  avconv is not officially supported by yt-dlp (Alias: --no-prefer-ffmpeg)
--prefer-ffmpeg                  Default (Alias: --no-prefer-avconv)
-C, --call-home                  Not implemented
--no-call-home                   Default
--include-ads                    No longer supported
--no-include-ads                 Default
--write-annotations              No supported site has annotations now
--no-write-annotations           Default
--compat-options seperate-video-versions  No longer needed"><pre><code>--prefer-avconv                  avconv is not officially supported by yt-dlp (Alias: --no-prefer-ffmpeg)
--prefer-ffmpeg                  Default (Alias: --no-prefer-avconv)
-C, --call-home                  Not implemented
--no-call-home                   Default
--include-ads                    No longer supported
--no-include-ads                 Default
--write-annotations              No supported site has annotations now
--no-write-annotations           Default
--compat-options seperate-video-versions  No longer needed
</code></pre></div>
<h4 tabindex="-1" dir="auto">Removed</h4>
<p dir="auto">These options were deprecated since 2014 and have now been entirely removed</p>
<div data-snippet-clipboard-copy-content="-A, --auto-number                -o &quot;%(autonumber)s-%(id)s.%(ext)s&quot;
-t, -l, --title, --literal       -o &quot;%(title)s-%(id)s.%(ext)s&quot;"><pre><code>-A, --auto-number                -o "%(autonumber)s-%(id)s.%(ext)s"
-t, -l, --title, --literal       -o "%(title)s-%(id)s.%(ext)s"
</code></pre></div>
<h2 tabindex="-1" dir="auto">CONTRIBUTING</h2>
<p dir="auto">See <a href="https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#contributing-to-yt-dlp">CONTRIBUTING.md</a> for instructions on <a href="https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#opening-an-issue">Opening an Issue</a> and <a href="https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions">Contributing code to the project</a></p>
<h2 tabindex="-1" dir="auto">WIKI</h2>
<p dir="auto">See the <a href="https://github.com/yt-dlp/yt-dlp/wiki">Wiki</a> for more information</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft to kill off third-party printer drivers in Windows (122 pts)]]></title>
            <link>https://www.theregister.com/2023/09/11/go_native_or_go_home/</link>
            <guid>37473628</guid>
            <pubDate>Mon, 11 Sep 2023 21:00:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/09/11/go_native_or_go_home/">https://www.theregister.com/2023/09/11/go_native_or_go_home/</a>, See on <a href="https://news.ycombinator.com/item?id=37473628">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Microsoft has made it clear: it will ax third-party printer drivers in Windows.</p>
<p>The death rattle will be lengthy, as the timeline for the end of servicing stretches into 2027  although Microsoft noted that the dates will be subject to change. There is, after all, always that important customer with a strange old printer lacking Mopria support.</p>
<p><a target="_blank" rel="nofollow" href="https://mopria.org/mopria-alliance">Mopria</a> is part of the Windows' teams justification for removing support. Founded in 2013 by Canon, HP, Samsung and Xerox, the <a target="_blank" rel="nofollow" href="https://mopria.org/">Mopria Alliance</a>'s mission is to provide universal standards for printing and scanning. Epson, Lexmark, Adobe and Microsoft have also joined the gang since then.</p>

    

<p>Since Windows 10 21H2, Microsoft has baked Mopria support into the flagship operating system, with support for devices connected via the network or USB, thanks to the Microsoft IPP Class driver. Microsoft said: "This removes the need for print device manufacturers to provide their own installers, drivers, utilities, and so on."</p>

        


        

<p>The software giant also said that customization can be performed via Print Support Apps from the Windows Store. It added: "This framework improves reliability and performance by moving customization from the Win32 framework to the UWP software development framework."</p>
<p>While some wags have dubbed the framework the <a target="_blank" href="https://www.theregister.com/2021/10/26/microsofts_uwp_unwanted_windows_platform/">"Unwanted Windows Platform"</a>, it's always good to see legacy tech being retired in favor of something with a bright future ahead of it.</p>

        

<p>Microsoft's timeline for the end of servicing will be staged. The next milestone will occur in 2025 when no new printer drivers will be published to Windows Update  although existing drivers can still be updated. In 2026, driver ranking will be tweaked to bring the IPP inbox class driver to the top, and by 2027  except for security-related fixes  no printer driver updates will be allowed.</p>
<ul>

<li><a href="https://www.theregister.com/2023/07/11/microsoft_patch_tuesday/">Miscreants exploit five Microsoft bugs as Windows giant addresses 130 flaws</a></li>

<li><a href="https://www.theregister.com/2022/12/05/opinion_column_printing/">Killing trees with lasers isn't cool, says Epson. So why are inkjets any better?</a></li>

<li><a href="https://www.theregister.com/2022/03/04/on_call/">Saving a loved one from a document disaster</a></li>

<li><a href="https://www.theregister.com/2021/10/18/windows_printing/">Microsoft admits to yet more printing problems in Windows as back-at-the-office folks asked for admin credentials</a></li>
</ul>
<p>To be clear, the end of servicing applies to drivers provided via Windows Update. Manufacturers will, according to Microsoft, "need to provide customers with an alternative means to download and install those printer drivers." Legacy v3 and v4 Windows printer drivers are facing the end of servicing ax.</p>
<p>Microsoft added that multi-function devices  print, scan and fax  will work via the inbox drivers.</p>
<p>Printing and Windows have <a target="_blank" href="https://www.theregister.com/2022/07/26/windows_10_printer_bork/">long been uneasy bedfellows</a>. While Microsoft hopes the end-of-servicing will take away some legacy driver headaches, there are plenty of other components within the Windows printing subsystem that can <a target="_blank" href="https://www.theregister.com/2021/10/18/windows_printing/">occasionally topple</a> when poked the wrong way by a patch. </p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[uBlock-Origin  1.52.0 (192 pts)]]></title>
            <link>https://github.com/gorhill/uBlock/releases/tag/1.52.0</link>
            <guid>37472994</guid>
            <pubDate>Mon, 11 Sep 2023 20:13:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/gorhill/uBlock/releases/tag/1.52.0">https://github.com/gorhill/uBlock/releases/tag/1.52.0</a>, See on <a href="https://news.ycombinator.com/item?id=37472994">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:gorhill/uBlock" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="x8C6R9GJSwffziApgP8Nx1Hb5B4IUBkEuexf3bwqIz2-7Feg9FkhJtAX9lTquRZnH9DjGw5zPjEi5JI5gpqigg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="gorhill/uBlock" data-current-org="" data-current-owner="gorhill" data-logged-in="false">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Freleases%2Fshow&amp;source=header-repo&amp;source_repo=gorhill%2FuBlock" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/gorhill/uBlock/releases/tag/1.52.0&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="2f3c5dde8d1530c1097543858fcd6094e612f73a18c39160cb2954947a81a077" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/releases/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Blood pressure should be measured lying down: study (206 pts)]]></title>
            <link>https://newsroom.heart.org/news/high-blood-pressure-while-lying-down-linked-to-higher-risk-of-heart-health-complications</link>
            <guid>37471354</guid>
            <pubDate>Mon, 11 Sep 2023 18:20:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsroom.heart.org/news/high-blood-pressure-while-lying-down-linked-to-higher-risk-of-heart-health-complications">https://newsroom.heart.org/news/high-blood-pressure-while-lying-down-linked-to-higher-risk-of-heart-health-complications</a>, See on <a href="https://news.ycombinator.com/item?id=37471354">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body-container">
          <p>Research Highlights:</p>

<ul>
	<li>An analysis of data from a long-running study of more than 11,000 adults from four diverse communities in the United States has found that adults who had high blood pressure while both seated upright and lying supine (flat on their backs) had a higher risk of heart disease, stroke, heart failure or premature death compared to adults without high blood pressure while upright and supine.&nbsp;</li>
	<li>Adults who had high blood pressure while lying supine but not while seated upright had similar elevated risks of heart attack, stroke, heart failure or premature death as adults who had high blood pressure in both supine and upright positions.</li>
	<li>The increased risk of heart disease, stroke, heart failure or premature death did not differ by the type of blood pressure medication used among participants.</li>
</ul>

<p><strong>Embargoed until 6:30a.m. CT/7:30 a.m. ET Thursday, Sept. 7, 2023</strong></p>

<p>BOSTON, Sept. 7, 2023  People who had high blood pressure while lying flat on their backs had a higher risk of heart attack, stroke, heart failure or premature death, according to new research to be presented at the American Heart Associations <a href="https://professional.heart.org/en/meetings/hypertension" rel="" target="_blank" title="">Hypertension Scientific Sessions 2023</a>, to be held Sept. 7-10, 2023, in Boston. The meeting is the premier scientific exchange focused on recent advances in basic and clinical research on high blood pressure and its relationship to cardiac and kidney disease, stroke, obesity and genetics.</p>

<p>The autonomic nervous system regulates blood pressure in different body positions; however, gravity may cause blood to pool when seated or upright, and the body is sometimes unable to properly regulate blood pressure during lying, seated and standing positions, the authors noted.</p>

<p>If blood pressure is only measured while people are seated upright, cardiovascular disease risk may be missed if not measured also while they are lying supine on their backs, said lead study author Duc M. Giao, a researcher and a 4<sup>th</sup>-year M.D. student at Harvard Medical School in Boston.</p>

<p>To examine body position, blood pressure and heart health risk, the researchers examined health data for 11,369 adults from the longitudinal Atherosclerosis Risk in Communities (ARIC) study. The data on supine and seated blood pressure was gathered during the enrollment period, ARIC visit 1, which took place between 19871989. Participants had their blood pressure taken while briefly lying down at a clinic. The average age of participants at that time was 54 years old; 56% of the group self-identified as female; and 25% of participants self-identified as Black race. Participants in this analysis were followed for an average of 25 to 28 years, up through ARIC visit 5, which includes health data collected from 2011-2013.</p>

<p>The researchers findings included:</p>

<ul>
	<li>16% percent of participants who did not have high blood pressure  defined in this study as having top and bottom blood pressure measures greater than or equal to 130/80 mm Hg  while seated had high blood pressure while lying supine (flat on their backs), compared to 74% of those with seated high blood pressure who also had supine high blood pressure.</li>
	<li>In comparison to participants who did not have high blood pressure while seated and supine, participants who had high blood pressure while seated and supine had a 1.6 times higher risk of developing coronary heart disease; a 1.83 times higher risk of developing heart failure; a 1.86 times higher risk of stroke; a 1.43 times higher risk of overall premature death; and a 2.18 times higher risk of dying from coronary heart disease</li>
	<li>Participants who had high blood pressure while supine but not while seated had similar elevated risks as participants who had high blood pressure while both seated and supine.</li>
	<li>Differences in blood pressure medication use did not affect these elevated risks in either group.</li>
</ul>

<p>Our findings suggest people with known risk factors for heart disease and stroke may benefit from having their blood pressure checked while lying flat on their backs, Giao said.</p>

<p>Efforts to manage blood pressure during daily life may help lower blood pressure while sleeping. Future research should compare supine blood pressure measurements in the clinic with overnight measurements.</p>

<p>The studys limitations included that it focused on adults who were middle-aged at the time of enrollment, meaning the results might not be as generalizable to older populations, Giao said.</p>

<p><strong>Note: Giao presents <em>Seated And Supine Blood Pressure And Risk Of Cardiovascular Disease And Mortality From The Atherosclerosis Risk In Communities Study </em>at 2:15 p.m. ET on Saturday, Sept. 9, 2023, Presentation #071; Abstract #452</strong></p>

<p>Background:</p>

<ul>
	<li>The Atherosclerosis Risk in Communities (ARIC) study is an ongoing, community-based cohort of 15,792 adults in the United States enrolled from 1987-1989 to investigate the causes for atherosclerotic disease (plaque or fatty buildup in the arteries). ARIC study participants were ages 4565 years at the start of the study and from rural areas in the U.S. (Forsyth County, North Carolina, and Washington County, Maryland) and urban areas: Minneapolis and Jackson, Mississippi. The research and data from the ARIC clinical visits  including hospital record abstraction, ECG tracings, and physician and coroner questionnaires, as well as death certificate data  have led to discoveries and guidelines surrounding atherosclerosis, heart disease, kidney disease, diabetes, stroke and cognitive decline.</li>
	<li>The <a href="https://www.ahajournals.org/doi/full/10.1161/HYP.0000000000000065" target="_blank">2017 ACC/AHA Guideline for the Prevention, Detection, Evaluation, and Management of High Blood Pressure in Adults</a> classifies hypertension as having top and bottom numbers greater than or equal to 130/80 mm Hg, which was the definition of hypertension used in this study.</li>
</ul>

<p>Co-authors and their disclosures are listed in the abstract. The study was funded by the National Institutes of Health.</p>

<p>Statements and conclusions of studies that are presented at the American Heart Associations scientific meetings are solely those of the study authors and do not necessarily reflect the Associations policy or position. The Association makes no representation or guarantee as to their accuracy or reliability. The Association receives funding primarily from individuals; foundations and corporations (including pharmaceutical, device manufacturers and other companies) also make donations and fund specific Association programs and events. The Association has strict policies to prevent these relationships from influencing the science content. Revenues from pharmaceutical and biotech companies, device manufacturers and health insurance providers and the Associations overall financial information are available <a href="https://www.heart.org/en/about-us/aha-financial-information">here</a>.</p>

<p><strong>Additional Resources:</strong></p>

<ul>
	<li>Available multimedia is on right column of release link&nbsp;<a href="https://newsroom.heart.org/news/high-blood-pressure-while-lying-down-linked-to-higher-risk-of-heart-health-complications?preview=3a007402d06b4cd7cbc3c53acb93b5f5" rel="" target="_blank" title="">https://newsroom.heart.org/news/high-blood-pressure-while-lying-down-linked-to-higher-risk-of-heart-health-complications?preview=3a007402d06b4cd7cbc3c53acb93b5f5</a></li>
	<li><a href="https://www.abstractsonline.com/pp8/?_ga=2.68132181.553845914.1693780001-1860925560.1690312372#!/10947" rel="" target="_blank" title="">Program abstracts online at embargo</a></li>
	<li>AHA news release: <a href="https://newsroom.heart.org/news/if-blood-pressure-rises-upon-standing-so-may-risk-for-heart-attack" target="_blank">If blood pressure rises upon standing, so may risk for heart attack</a> (March 2022)</li>
	<li>AHA&nbsp; news release: <a href="https://newsroom.heart.org/news/blood-pressure-rising-at-night-linked-to-doubling-risk-of-death-in-adults-with-diabetes" target="_blank">Blood pressure rising at night linked to doubling risk of death in adults with diabetes</a> (Sept. 2021)</li>
	<li>AHA news release: <a href="https://newsroom.heart.org/news/abnormal-blood-pressure-levels-while-sleeping-increase-risk-of-heart-disease-stroke" target="_blank">Abnormal blood pressure levels while sleeping increase risk of heart disease</a> (November 2020)</li>
	<li>Follow AHA/ASA news on X (formerly known as Twitter) <a href="https://twitter.com/HeartNews" target="_blank">@HeartNews</a> #Hypertension23</li>
</ul>

<p><strong>###</strong></p>

<p><strong>About the American Heart Association </strong></p>

<p>The American Heart Association is a relentless force for a world of longer, healthier lives. We are dedicated to ensuring equitable health in all communities. Through collaboration with numerous organizations, and powered by millions of volunteers, we fund innovative research, advocate for the publics health and share lifesaving resources. The Dallas-based organization has been a leading source of health information for nearly a century. Connect with us on <a href="http://www.heart.org/en" target="_blank">heart.org</a>, <a href="http://facebook.com/AmericanHeart" target="_blank">Facebook</a>, <a href="https://twitter.com/American_Heart" rel="" target="_blank" title="">X</a>&nbsp;or by calling 1-800-AHA-USA1.</p>

<p><strong>For Media Inquiries and AHA Expert Perspective: </strong></p>

<p>AHA Communications &amp; Media Relations&nbsp;in Dallas: 214-706-1173; <a href="mailto:ahacommunications@heart.org">ahacommunications@heart.org</a></p>

<p>John Arnst: 214-706-1060; <a href="mailto:John.Arnst@heart.org">John.Arnst@heart.org</a></p>

<p>For Public Inquiries: 1-800-AHA-USA1 (242-8721)</p>

<p><a href="https://www.heart.org/en" target="_blank">heart.org</a> and <a href="https://www.stroke.org/en" target="_blank">stroke.org</a></p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In Germany, 27 are in 'preventive detention' b/c they might do climate protests (246 pts)]]></title>
            <link>https://mastodon.energy/@Sustainable2050/111039159882536261</link>
            <guid>37471048</guid>
            <pubDate>Mon, 11 Sep 2023 18:00:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.energy/@Sustainable2050/111039159882536261">https://mastodon.energy/@Sustainable2050/111039159882536261</a>, See on <a href="https://news.ycombinator.com/item?id=37471048">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[MGM is down, cybersecurity attack ongoing (169 pts)]]></title>
            <link>https://www.casino.org/news/mgm-resorts-suffers-cybersecurity-attack-system-outage-reported/</link>
            <guid>37470801</guid>
            <pubDate>Mon, 11 Sep 2023 17:44:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.casino.org/news/mgm-resorts-suffers-cybersecurity-attack-system-outage-reported/">https://www.casino.org/news/mgm-resorts-suffers-cybersecurity-attack-system-outage-reported/</a>, See on <a href="https://news.ycombinator.com/item?id=37470801">Hacker News</a></p>
Couldn't get https://www.casino.org/news/mgm-resorts-suffers-cybersecurity-attack-system-outage-reported/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Real-Time 3D Gaussian Splatting in WebGL (218 pts)]]></title>
            <link>https://antimatter15.com/splat/</link>
            <guid>37470611</guid>
            <pubDate>Mon, 11 Sep 2023 17:31:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antimatter15.com/splat/">https://antimatter15.com/splat/</a>, See on <a href="https://news.ycombinator.com/item?id=37470611">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="info">
			<h3>WebGL 3D Gaussian Splat Viewer</h3>
			<p>Use mouse or arrow keys to navigate.</p>
			<p><small>
				By <a href="https://twitter.com/antimatter15">Kevin Kwok</a>.
				Code on
				<a href="https://github.com/antimatter15/splat">Github</a>.
			</small>
		</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Why did Visual Basic die? (308 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37470318</link>
            <guid>37470318</guid>
            <pubDate>Mon, 11 Sep 2023 17:12:56 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37470318">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="37471507"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471507" href="https://news.ycombinator.com/vote?id=37471507&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Visual Basic (both of them) still exist, but their use has dropped dramatically through some big changes:<p>* "Visual .NET" (aka "Visual Fred" <a href="http://catb.org/jargon/html/V/Visual-Fred.html" rel="nofollow noreferrer">http://catb.org/jargon/html/V/Visual-Fred.html</a> ) was released by Microsoft. This was an incompatible language confusingly <i>also</i> called Visual Basic. I don't think Microsoft realized how angry this made developers and businesses, who were being asked to spend hundreds of billions of dollars (USD) to rewrite code just to keep the same functionality. Before that time, many thought that Visual Basic's wide use gave it a kind of "herd immunity". I don't have numbers with me, but I remember that years later that a study found that some were sticking to the original Visual Basic (even though it was no longer supported), a few had moved to Visual .NET, and many other had abandoned Visual Basic entirely (some to C#, others beyond). In short, the Visual Basic community was split into multiple communities, and anyone using Visual Basic would have to worry about either lack of support or yet another harmful change.</p><p>* The rise of the web and of platforms other than Windows (including Android, iOS, MacOS, Linux). Visual Basic is fine when you send files via sneakernet to another Windows user. Now people want to access through their web browser, smartphone, etc. If you have a website, anything can access it (as long as they have the permissions), and you don't have to worry about synchronizing data changes the way you do if people make changes on their local device. Most of the simple "fill in a form" kinds of applications that Visual Basic was used for are more sensibly web applications (server side or client side).</p><p>Visual Basic is still used. And yes, I think there could be better tools for developing software. But as best as I recall, that's how we ended up here.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472277"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472277" href="https://news.ycombinator.com/vote?id=37472277&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Visual Basic is one of the best arguments for open source and community ownership in the history of computing, IMO. Microsoft's decision to tank it was hugely painful for companies that had made major investments in it -- no company should make that kind of investment in a proprietary platform that can be killed off by a single company and not forked and maintained by others.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472386"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472386" href="https://news.ycombinator.com/vote?id=37472386&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>As someone who was writing Visual Basic.NET back when it came out, there was no upside to it over writing in C#. VB's original sweet spot was for writing small scripts and apps in Windows, and it was the only language available. When the .NET line came out, you could do the same things in whichever language you wanted. When new tasks came in, I started defaulting to C# for that reason. I don't think anyone actually prefers VB syntax. C# has a pretty robust community around it now.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37472754"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472754" href="https://news.ycombinator.com/vote?id=37472754&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt; When new tasks came in, I started defaulting to C# for that reason. I don't think anyone actually prefers VB syntax. C# has a pretty robust community around it now.<p>Which was your experience because you knew C. VB appealed to people who were not programmers (or not very good ones like me). Microsoft effectively tossed an easy to learn procedural language in the trash and said "go learn all these advanced CS concepts or stop writing stuff" to which most hardcore VB users chose the latter.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472059"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472059" href="https://news.ycombinator.com/vote?id=37472059&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Lol, my first programming gig as a teenager was performing a VB6 -&gt; VB.NET "upgrade" of a 200K sloc legacy desktop application, which obviously ended up being a total rewrite. Everything in my career since then has seemed easy in comparison.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472298"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472298" href="https://news.ycombinator.com/vote?id=37472298&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I was a hobby VB developer at this time and I abandoned it shortly after VB.net without really realizing why. I also abandoned Windows completely shortly thereafter.<p>The main thing that killed me was the size of the files that you had to distribute when you used VB.net. No one had the .net runtime early on and it was absolutely massive. I was paying for outgoing data by the gigabyte back then and our connections were much slower.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472597"><td></td></tr>
            <tr id="37472322"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472322" href="https://news.ycombinator.com/vote?id=37472322&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Yeah, as someone who worked somewhere that had a VB6 project: VB.net was only at all useful as a stop gap between VB and C# and a barely useful one at that.<p>The language as it stands is fine and interop with .NET means it is a decent choice to use, outside of C# just having a bigger user base from a "how easy can I hire devs" standpoint.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472377"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472377" href="https://news.ycombinator.com/vote?id=37472377&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Great points. And in a tongue in cheek way, having reactive components you can attach handlers to fetch / redraw you UI .. is still there, it's just labelled vue or react ;)</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472779"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472779" href="https://news.ycombinator.com/vote?id=37472779&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span><i>Yet I can't help wondering what problems it had that caused them to abandon it?</i><p>Tech comes and goes, theres nothing to vb specifically. As a language its pretty limited, tedious and quirky.</p><p><i>Moreover, why hasn't someone come out with a solid replacement?</i></p><p>Because webdev at its core is a community of stubborn smart guys who <i>love</i> the complexity and hate dull business code. They will present absurd arguments like I can do this and that, as if it couldnt be packed into a vb component and drag-dropped onto a form from a palette without accompanying 1kloc boilerplate and pages of configuration documentation with no sane defaults. VB GUI model may be obsolete, gray and non-responsive, but no one prevents from building responsive interfaces wysiwyg way. My peer web designer does it without bothering with html/css much and it works for her for decades. All the tech is there, its just nobodys collective interest to combine it into a business RAD instead of an intermediate haskell-level mindfuck starter kit. You cant burn hundreds of millions doing actual work on a platform that everyone could start using solo in just a few days and deliver a working solution next week, even if raw and clumsy as it usually goes with non-pros.</p><p>Id like to get a better and less bitter explanation, but there is none.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472327"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472327" href="https://news.ycombinator.com/vote?id=37472327&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>The reason is even bigger than just VB.<p>MS at the time just decide to <i>fully</i> kill the "enthusiast" developer and the "single/truly small" team developer. This is mostly covered under the "RAD" umbrella.</p><p>It kills VB, FoxPro, and now more evidently, Access (more like let it slowly die).</p><p>.NET + Visual Studio + Sql Server are <i>not</i> a substitute in this market. Them are for "professional developer"/"a small cog in a big machine". The worst part is that this move somehow kill the other tools in this space (because somehow others follow suit or whatever) and without somebody leading the charge to see how adapt this tool for the web. MS not getting the Web, Borland doing Hara-kiri and others getting annihilated by "free" open source and all that not helps.</p><p>Ironically, this market have rebound in the myriad of tools like "low code, notebooks, etc" that fill (badly!) the gap.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472473"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472473" href="https://news.ycombinator.com/vote?id=37472473&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Serious question: what is a good alternative to Access? The database design tools and basic forms were incredibly easy to use, and there were very good tutorials for everything else. LibreOffice Base is different and not even close in comprehensiveness, and there seems to be nothing replacing it that isn't a super expensive SaaS.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37472653"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472653" href="https://news.ycombinator.com/vote?id=37472653&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>SQLite. It's not close in terms of ease-of-use of the GUI administration and forms, but as a single-server database solution to embed into a line-of-business app it's fantastic.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37472752"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472752" href="https://news.ycombinator.com/vote?id=37472752&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>The GUI and forms is what made Access, though. Back in the day I actually made a system with Access that connected to a SQL Server database on the backend rather than Access's file-based engine.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472728"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472728" href="https://news.ycombinator.com/vote?id=37472728&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Airtable is the closest I have found for ease of use - users that dont know SQL can put together basic queries and views quickly.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472748"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472748" href="https://news.ycombinator.com/vote?id=37472748&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Equally serious question: what is the use case for Access?<p>It's been installed on every corp workstation I've had and it's never been useful.  In my experience either Excel can do it or you need a real programming language/database.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472621"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472621" href="https://news.ycombinator.com/vote?id=37472621&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>The parents point is that there is no alternative, because everyone left the market chasing Microsoft.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472594"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472594" href="https://news.ycombinator.com/vote?id=37472594&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>For a long time I would have said FileMaker, but considering the strategic moves of Claris in the last few years probably not anymore</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472615"><td></td></tr>
                        <tr id="37470603"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470603" href="https://news.ycombinator.com/vote?id=37470603&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Does VBA for Excel count? Because if it does then VBA for Excel has reached the"nuclear resistant cockroach" level in finance.<p>You wouldn't believe what sort of processes in very big banks/financial institutions are built using 10 year old VBA macros. In fact, VBA consulting for finance is a very juicy cottage industry at least in Europe to this very day.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37470877"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470877" href="https://news.ycombinator.com/vote?id=37470877&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Excel is literally 2D programming. Us mortal developers who can only put lines below one another are incapable of comprehending it, so we only get to ask the wise finance people how their enigma works.<p>On a serious note, I dread excel. If your PC is set to german, excel will translate the VBA keywords to german. But if you want to type them, you have to do that in english and then have excel translate them.</p><p>I don't want to accept that crap like this is the standard.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472516"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472516" href="https://news.ycombinator.com/vote?id=37472516&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>The 2D aspect is the part of Excel I don't understand.  Why does it have to be a grid?<p>It's great for laying out things meant to print, and making invoices and stuff... But why didn't we have code files and proper fixed layout DB-style tables as "pages" that can go in a workbook?</p><p>Maybe keeping everything as 2D as possible is a necessary compromise for the spatial thinkers out there, and they just wouldn't want it if it was full of boring linear stuff.</p><p>I love the reactivity and the concept that anywhere you put a value, you can put an =expression. But the 2D stuff seems like it's for the people who always have a sense of where things are in space.</p><p>They've done a good job of convincing people that it's not programming and they can do it, I can't really complain, because if Excel didn't exist we might all still have to use paper on a regular basis, or completely unstructured text files.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471538"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471538" href="https://news.ycombinator.com/vote?id=37471538&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt; Us mortal developers who can only put lines below one another<p>At least my spaghetti code goes into one direction only...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472328"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472328" href="https://news.ycombinator.com/vote?id=37472328&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>"2D programming" is normally called array programming, and it's common in scientific computing, ML, and finance. It does require a different mindset, kind of similar to SQL but not exactly. See APL, K, q, NumPy, matlab, Julia, and friends for languages that embrace this paradigm</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472138"><td></td></tr>
            <tr id="37472216"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472216" href="https://news.ycombinator.com/vote?id=37472216&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt; But if you want to type them, you have to do that in english and then have excel translate them.<p>Huh? At least in the versions I've used I've always needed to type the commands in German.</p><p>There's even an online German - English Excel dictionary...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37471107"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471107" href="https://news.ycombinator.com/vote?id=37471107&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Healthcare and insurance too! I transform into some glorious magical elf when I volunteer to do the VBA tasks nobody else understands or can lower themselves to do.<p>You can make Excel do some real wacky stuff. I have a spreadsheet that actually calls out to exec() to run a curl POST on commandline and consume REST API endpoints, parse the results, and update the spreadsheet -- why on earth?? because the API was ready but the web app was delayed. I was the fix. :D
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471164"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471164" href="https://news.ycombinator.com/vote?id=37471164&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Pretty sure you can access a REST API from VBA without resorting to exec()</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37471214"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37471214" href="https://news.ycombinator.com/vote?id=37471214&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>It only works with toy examples and then stops working. For reasons unknown, as it should work.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37471459"><td></td></tr>
                              <tr id="37472447"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472447" href="https://news.ycombinator.com/vote?id=37472447&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Pre-face: I write a lot of VBA<p>VBA is kind of the result of people only - ONLY - wanting to use Excel for everything. I work with those people. They have mastered excel, but have little to zero interest in learning anything else, and would rather see the world be built around excel.</p><p>So you (like me) get tasked with building applications and forms in VBA.</p><p>I was STOKED when MS announced Python for excel, but alas, turned out to not be what I (and many other) wanted.</p><p>What's the medicine? Dunno, hire analysts that are more open to using other tools . Don't get me wrong, I love using excel for many tasks - but damnit, it's not the only tool.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470889"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470889" href="https://news.ycombinator.com/vote?id=37470889&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I know of a restaurant franchisee with 170+ locations that uses a home grown ERP system built in VBA on top of Access by an accountant about 20 years ago.<p>I once had to update it to optimize (minimize) front-line staff working hours so the company didn't have to pay health insurance for those employees. A real nightmare of a task in more ways than one!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471254"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471254" href="https://news.ycombinator.com/vote?id=37471254&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>My first programming job in the 90s while I was still in college was building systems exactly like you describe (and they were as poorly built as you think hah). I worked for a small IT programming/consulting shop. We did small jobs like this in town in addition to installing networks, IVRs, etc... while working on larger software to sell (which is an entirely different/crazy story that involved burning CD demos and using a hand 'stomper' to label and then mail them out).</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472418"><td></td></tr>
                  <tr id="37470909"><td></td></tr>
                <tr id="37471364"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471364" href="https://news.ycombinator.com/vote?id=37471364&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I can confirm this: I know several mechanical engineers that do mission critical-type systems (think "power plants"), and they routinely use Excel VBA for calculations.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37471506"><td></td></tr>
                        <tr id="37472385"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472385" href="https://news.ycombinator.com/vote?id=37472385&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Before VBA for Excel, I wrote numerous macros, including ATAN2(x,y) before there was an ATAN2.<p>Afterwards, forget it. Maybe one.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470767"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470767" href="https://news.ycombinator.com/vote?id=37470767&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>There is a book called: Professional Excel Development. If you want to get into it. You could probably use that book to build an OS in Excel. I'm not joking.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37471265"><td></td></tr>
                  <tr id="37472520"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472520" href="https://news.ycombinator.com/vote?id=37472520&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I used it as a coding layman in 15-buck-an-hour "admin" (clerk/secretary) roles to automate some processes that my predecessors had done by hand. Mostly just copying entries from a spreadsheet to company Excel/Powerpoint templates and printing them without killing myself copy/pasting or going into the save/print dialog 50 times. It did its work as something any old schmuck could harness to save themselves from carpal tunnel.<p>To that point, I imagine that there are a LOT of admin jobs (or, at least, a lot of tasks) that could be almost completely automated away. It's probably not even a capability issue, but one of job security on the employee side and a lack of *waves hands vaguely* on the employer side.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470953"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470953" href="https://news.ycombinator.com/vote?id=37470953&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>So true!!! :D :D :D<p>If Excel stops working, financial institutions around the world would collapse.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37470812"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470812" href="https://news.ycombinator.com/vote?id=37470812&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I think for a lot of purposes, the internet kind of took over.<p>VB was great if you needed to do something limited to a single machine.</p><p>These days, we want data to be available across machines which requires using a network, and the default network is the internet.</p><p>If I'm going to be using the internet anyway, I can knock up something in HTML + JS + firebase/whatever data store, and have an application that works on any platform, and is accessible from anywhere in the world. You <i>might</i> need slightly more technical knowledge, but not so much that you can't have a simple CRUD app running in a day or so of work.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472722"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472722" href="https://news.ycombinator.com/vote?id=37472722&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span><i>You might need slightly more technical knowledge, but not so much that you can't have a simple CRUD app running in a day or so of work.</i><p>Well thats quite a professional bubble you live in. Web dev is truly a frog in a boiling water.</p><p>If my VB/Delphi/Access/PIC buddy who made various apps and hardware back in the day asked me for a platform and I advised him to use what HN praises as simple, then pretty sure hell never contact me with it again.</p><p>I mean, yeah, <i>CRUD</i> is not hard to do by tutorial. But he will laugh at my CRUD explanation, because he never ever thought about <i>implementing</i> input &lt;&gt; data channels. Its akin to positioning heads above a cylinder to fetch a database record.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470920"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470920" href="https://news.ycombinator.com/vote?id=37470920&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I can write HTML/CSS/JS and a few back-end languages like PHP in my sleep but there's no way I could hand-code a web app as fast as I could a desktop app in 1997 using VB.<p>I would LOVE it if I could, though.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472214"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472214" href="https://news.ycombinator.com/vote?id=37472214&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Have you tried <a href="https://anvil.works/" rel="nofollow noreferrer">https://anvil.works</a>? (I'm a founder!)<p>It's quite explicitly VB-esque (only using Python), and having a single paradigm rather than stitching together several different programs speeds things up even for those who can write HTML/JS/CSS in their sleep. (And of course, you can drop out to JS/CSS/HTML if you want.) Overall I think the development speed is comparable to VB6.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472470"><td></td></tr>
                  <tr id="37472543"><td></td></tr>
            <tr id="37472104"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472104" href="https://news.ycombinator.com/vote?id=37472104&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Same. Im very good at cranking out Tailwind + React UIs, but nothing touches the productivity I had with VB6 or even early C# and Winforms.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37471885"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471885" href="https://news.ycombinator.com/vote?id=37471885&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I've never written VB, but I can bang out a React UI in jsxstyle like nobody's business.<p>You've piqued my curiosity.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472159"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472159" href="https://news.ycombinator.com/vote?id=37472159&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Development in VB6 was basically dragging and dropping controls onto a window or dialog box, setting properties on them, and then filling in snippets of code to tie things together.<p>And "control" here means anything from simple labels and buttons up to database connections and embedded COM objects.</p><p>Literally anybody could bang out a simple Windows .exe like nobody's business.</p><p>It would have been really cool if Microsoft included it in the OS like they used to do with QBasic.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472012"><td></td></tr>
                        <tr id="37471167"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471167" href="https://news.ycombinator.com/vote?id=37471167&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Yep, this is the right answer. I was there, 23 years ago programming VB6, making apps that were wrapped up in InstallShield, burnt to a CD, and then mailed to our customers. We did do some web things with Apache and some C++ apps interfaced via CGI. But it seemed that overnight Java and Servlets came about and made CGI-bin obsolete and slow as molasses. Add in JSPs and Struts and you had fully functional web apps that were fast and scalable.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472419"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472419" href="https://news.ycombinator.com/vote?id=37472419&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Does no-one remember VBScript and Active Server Pages (ASP). It was Visual basic as a server-side language, positioned to challenge PHP.<p>VB made it to the web :-)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472759"><td></td></tr>
                  <tr id="37472229"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472229" href="https://news.ycombinator.com/vote?id=37472229&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Yup, that was also my take. I used to write 'on box' and you communicated between components using Com+, and when you switched to http, there was no reason to stick with just VB6.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37470944"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470944" href="https://news.ycombinator.com/vote?id=37470944&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt; VB was great if you needed to do something limited to a single machine.<p>VB6 used to work with oracle across network.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472555"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472555" href="https://news.ycombinator.com/vote?id=37472555&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Same here, we did a VB6 project that allowed the customer to chat with their remote mortgage advisor using a webcam, around '98.<p>The client was supposed to go to a local branch of the bank and then connect to the banks HQ.</p><p>I have also wondered why the software industry with the arrival of Internet went away from all these excellent tools. Not just VB6, but remember all the 4GL and model driven development tools. All gone and never really replaced.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471108"><td></td></tr>
                <tr id="37472129"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472129" href="https://news.ycombinator.com/vote?id=37472129&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>More common, in my experience, was the 2 tier app where a (very) fat client directly talked to the db, and all the businesses and data access logic was intermixed with UI event handlers and some stored procedures.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37471386"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471386" href="https://news.ycombinator.com/vote?id=37471386&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Don't forget that VB.NET was a thing.  I worked on a web-based client management system that used VB.NET on the back-end.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37472245"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472245" href="https://news.ycombinator.com/vote?id=37472245&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>It still is a thing. VB.net is a CLR language just like C#. You can use a tool to translate from one to the other and back.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37471291"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471291" href="https://news.ycombinator.com/vote?id=37471291&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I think the answer is: because Microsoft let it. I'm a big fan of modern .NET, but my biggest complaint is that Microsoft views, and always has, the CLR as the C# Language Runtime and not the Common Language Runtime.<p>For example, see the relationship between F# and C#. The CLR is constantly getting features that are only to support features in C#, leaving F# in a position where they either don't get the feature, can't add the feature, or begrudgingly add the feature to keep up compatibility with C#, which is something it does take seriously. But this has the effect of "dirtying up" the F# language by either adding features that don't really belong in the language or keeping features out.</p><p>The other thing is that C# consistently adds features to itself that are inspired by F#, since F# already implements these features on the CLR, thus showing their viability. So what happens is that C# continually approaches a more bloated language with a subset of it being a poor copy of F#. But then F# gets dragged along towards having a small subset of C# in it for compatibility purposes. So it's simultaneously making both languages worse.</p><p>Even the iron languages project that lead to IronPython, IronRuby, etc. was a bit of a Trojan horse to test out and exercise the CLR and .NET with no intention of ever providing long-term support for those projects. The DLR, which was implemented to support those, appears to be just maintained by a skeleton crew of people invested in it, probably by those interested in keeping IronPython up and running.</p><p>I do not understand why Microsoft takes this approach. It is myopic, shows a misunderstanding of their own technology in the CLR, and ultimately turns C# into another C++, leave dead languages and projects in the wake.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472399"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472399" href="https://news.ycombinator.com/vote?id=37472399&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>To be fair VB.net originally had dynamic capabilities that C# lacked so at least it possessed features that made its existence justifiable.<p>However the extreme changes required to go from VB6 -&gt; VB.net made it all kind of moot, might as well rewrite.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472295"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472295" href="https://news.ycombinator.com/vote?id=37472295&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Interestingly enough there are a small handful of things VB.NET and C++/CLI can do which C# can't, which enable the former languages to have better interoperability with COM interfaces.<p>That was with .NET Framework and I'm not sure what the story is today with .NET Core aka .NET 5+.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472371"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472371" href="https://news.ycombinator.com/vote?id=37472371&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>They eventually added optional names parameters which made the interop problem much better for C# compared to VB.<p>C# wasn't as good still but you no longer had to put earlier params at their default value (which is what made it unusable).</p><p>C++/CLI will probably remain king of interop though being a Frankenstein combination of the .NET runtime and C++.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472493"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472493" href="https://news.ycombinator.com/vote?id=37472493&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>The other headache was indexed properties, but I don't think that's changed.<p>C++/CLI is no man's land and I tried my best to avoid it; and so far, I've managed; sometimes after-the-fact.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37471934"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471934" href="https://news.ycombinator.com/vote?id=37471934&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>The F# situation is not comparable to VB.NET.<p>VB.NET was just C# semantics with a VB like syntax. From the beginning it didn't serve any purpose except to make VB Fans feel slightly more at home.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472734"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472734" href="https://news.ycombinator.com/vote?id=37472734&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Related but off-topic:  Nothing I see today compares to the productivity that I saw with Lotus Notes, Dbase 3 and 4, Paradox, Microsoft Access, and a few other things from that era.  There are some really good SAAS offerings that target this space, but none of them seem as dominant as I would expect.  There was a time that if you wanted a simple application that could be covered with 3-4 tables and 5-6 views, any of the things I mentioned above could handle it.  You could explain the business problem and hand a developer a book on any of the above technologies and expect to have a working product a month later.  Today, this isn't really true.  It does seem we've gone backwards a bit...</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37470852"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470852" href="https://news.ycombinator.com/vote?id=37470852&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I used VB all the way from 1.0 to 6.0.<p>And when VB.NET came out came out in 2002, that was exactly when all the types of GUI-database projects VB6 was used for professionally, started being built in PHP/MySQL/HTML/CSS instead. The switch would have happened anyways, but the fact that VB.NET wasn't backwards-compatible made it really easy to switch since you were going to have to learn/build something new anyway -- otherwise there probably would have been a somewhat longer transition period. Microsoft really shot themselves in the foot (but the web benefited).</p><p>And then on the hobbyist/personal side, that's also basically when casual developers switched from building fun Windows apps to building fun websites.</p><p>So I'd mark it up entirely to web programming replacing it on both sides.</p><p>As for what a replacement might look like, Google had created App Maker (2016-2020) that got replaced by AppSheet (2020-present), which is the closest I've found for the drag-and-drop GUI/database aspect of VB6. But those have been very much geared towards business development, not kids learning programming. Maybe some parents here can chime in on what their kids are learning to program in?</p><p>[1] <a href="https://en.wikipedia.org/wiki/Google_App_Maker" rel="nofollow noreferrer">https://en.wikipedia.org/wiki/Google_App_Maker</a></p><p>[2] <a href="https://en.wikipedia.org/wiki/AppSheet" rel="nofollow noreferrer">https://en.wikipedia.org/wiki/AppSheet</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471053"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471053" href="https://news.ycombinator.com/vote?id=37471053&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Good explanation. Hobbiest switched. Because IIS and Asp.net run very well with VB.NET. but who could afford that compared to PHP.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37471209"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471209" href="https://news.ycombinator.com/vote?id=37471209&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>The bigger pain point, from memory, was having an ODBC-compliant driver so the OS
 could actually talk to your DB. That basically meant MSSQL if you wanted to co-exist happily with early dotnet.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37470764"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470764" href="https://news.ycombinator.com/vote?id=37470764&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Microsoft accidentally killed it moving to .NET and the increasingly stupid GUI libraries they offered up.  Silverlight, 32 bit native, winforms etc and etc.<p>Web development meanwhile went bonkers and VB was a poor cousin to C# very suddenly.</p><p>Its a shame, VB was not for purists but it was very productive.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471028"><td></td></tr>
                <tr id="37471366"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471366" href="https://news.ycombinator.com/vote?id=37471366&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I don't think they did. They said increasingly so. They're just getting at Microsoft continually re-inventing the wheel and then abandoning that wheel when it comes to their GUI libraries. There's WinForms -&gt; WPF -&gt; UDP -&gt; WinUI, Xamarin Forms -&gt; .NET MAUI, and then the evolution of Avalonia and Uno as third-parties trying to step in. And all of those options still exist! You literally have a minimum of 8 GUI options, at least on Windows. There are of course more by external parties.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37471978"><td></td></tr>
                        <tr id="37471790"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471790" href="https://news.ycombinator.com/vote?id=37471790&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Agreed, .NET (well, C# specifically) was my go to for anything with a GUI until they tried moving on from Winforms/WPF and the way forward seemed to become a large undefined mess.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37471424"><td></td></tr>
                  <tr id="37472685"><td></td></tr>
            <tr id="37472598"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472598" href="https://news.ycombinator.com/vote?id=37472598&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Visual Basic was mostly used for LOB applications.  That stuff mostly migrated to the web.  When businesses decided to move those applications to the web, they had a choice between C# and VB.NET (if they stuck to the MS stack).  C# ultimately won.  Most of the old VB developers that are still around have converted over to being C# developers now.<p>When everything was a desktop app then the choice was C++ or VB and there were a lot of situations where VB "won".  Today C# does everything VB used to do but better on the desktop and non-performance sensitive applications are increasingly using Electron anyway.</p><p>Performance sensitive desktop app = C++
Windows focused non-performace sensitive desktop app = C#
Cross platform non-performace sensitive desktop app = JS/TS in Electron
Web app = anything but VB
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472350"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472350" href="https://news.ycombinator.com/vote?id=37472350&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>The FOSS community could do SO much better if they wanted to, with reactive web tech, typescript instead of basic, a project file format that's easy to work with in Git, Android support, etc.<p>Despite all this talk about no-code, it seems like all we have now is like, a CMS that lets you embed Google maps, but if you want to do anything more you have to use code.</p><p>Maybe it's just that end users usually don't need to build anything anymore, there's almost always an professionally made app for everything.</p><p>I've tried many times to "Build the app you want to see in the world" and almost every time the result is I decide that living with and working around the imperfections of what's out there is more practical than building and maintaining anything by myself in hopes of the the very small chance people notice and it becomes A Thing.</p><p>Perhaps better dev tools like VB aimed at one off software like that could change the equation, and if the tools existed we'd all find uses for them?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472445"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472445" href="https://news.ycombinator.com/vote?id=37472445&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt;  with reactive web tech, typescript<p>I very much doubt. Actually reactive frameworks such as react and languages that transpile in other languages such as trypescript are the reason they couldn't. These overcomplicated, seemingly well architected, tools are in fact just a fractal of poor design.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472591"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472591" href="https://news.ycombinator.com/vote?id=37472591&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Not just VB, but also things like Borland Delphi.<p>25 years ago, you could visually compose a UI using standardized components, including advanced concepts like a layout manager. You could do data-binding visually by navigating a linked database. You can write logic/events just by double-clicking a button and the event is created. Here you'd write your code which would typically be pretty easy because all contextual objects are readily available.</p><p>Sure enough, I understand that the above development model also has its limitations and doesn't serve all common modern needs. But still, it's pretty pathetic what we ended up with. Our tool chains are much more complicated and we program at a lower abstraction level whilst requiring a laundry list of skills.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471664"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471664" href="https://news.ycombinator.com/vote?id=37471664&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>The web happened. VB was great for builing stand alone desktop applications, possibly with database integration. But they never managed to deliver a similar slick and self-contained solution for building web-based solutions. It didnt help that the migration from vb6 to vb.net was too painful, but the root cause was the demand for business desktop apps disappeared.<p>The infamous WebForms API was supposed to bring the same gui builder paradigm to the web, but the developer experience was not as great.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471277"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471277" href="https://news.ycombinator.com/vote?id=37471277&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I started my career with VB/VBA/Access, but got burned by one of their non-backward compatible upgrades (I don't remember which versions) that derailed an important project.  This was around the time that Java was what the cool kids were using, and I was very amenable to trying open source after that experience, and I never looked back.<p>It was a great experience though, especially for a self-taught beginner long before code academies and YouTube.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471948"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471948" href="https://news.ycombinator.com/vote?id=37471948&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>People want more these days. With VB you could write a CRUD application that saved data to a database and ran some reports. But now people want the database to pull in multiple feeds from other businesses via API, people want it to be available on the web, it needs to have a self service component so the customer can also log in, at the back end the database is connected to several other systems with daily feeds coming in and out. Shit is just much more complicated these days.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37472586"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472586" href="https://news.ycombinator.com/vote?id=37472586&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>VB6 could do all these things and more, in the 90's I built just the right small parts in C++ to glue things together, and we could do multi-processing, resilient multi-server setups, and much much more. 
Serving HTTP for some always-limiting browser was an obvious easy sidenote, ie. what your mantra "available on the web" wraps in silver, but leveraging the full power of the desktop and OS was what power users actually wanted, and would still want now if the apps were there.
I think some people at Microsoft realized VB had the potential to canibalize a lot of what they wanted as their proprietary backyard, and they killed the baby before it would grow into more of a mass movement. 
Now they succeeded, many that would have been educated into owning their computers now mindlessly click on "Yes" and are lost in the adds panopticon, but then MS had to non-compete some of the pie off to Google and co...</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472566"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472566" href="https://news.ycombinator.com/vote?id=37472566&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Microsoft effectively killed VB because they decided everyone should be using .NET. While they created VB .NET in an attempt to make the transition palatable to VB programmers, this was a second-class citizen of the CLR, and more importantly too different from VB, more akin to an inferior C# with VB-like syntax than actual VB. It was widely decried by VB developers [0], and nick-named "Visual Fred" due to really being a different language than VB. Microsoft ignored that, and also didn't bring a VB-like experience to .NET (maybe with the exception of WinForms, I'm not too familiar). It didn't help that mainstream software development started drifting to the web, and later to mobile apps.<p>[0] <a href="https://classicvb.net/vfred/breaks.asp" rel="nofollow noreferrer">https://classicvb.net/vfred/breaks.asp</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470632"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470632" href="https://news.ycombinator.com/vote?id=37470632&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Are you talking about VB the language, or Visual Studio the IDE with its awesome GUI builder for WinForms (and not so awesome ones for the subsequent Windows GUI layers, forget what they're called).<p>VB is still around but desktop apps in general (and thus VS's GUI builder) largely gave way to web technologies invented outside Microsoft. The dev experience is definitely worse though. Visual Studio was sooooo nice and integrated.</p><p>I don't think this is really VB or NET's fault, Microsoft just kinda missed (or failed the fight against) the web transition. They were busy trying to make it coexist with Windows with seamless downloads like ClickOnce but ultimately simple web pages won out for their reach and ease of use, then mobile app stores came along, and now desktop apps are petty much dead except for niches.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471390"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471390" href="https://news.ycombinator.com/vote?id=37471390&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I remember, in some sense, Google (and also a group of anti Microsoft developers) encouraged web development as a strategy to break the Windows software monopoly.<p>So it's not that Microsoft missed it, it is that a big shift happened and it was specifically targeted against them. There's nothing they could have done, except to embrace it, and they did. Visual Basic Script existed and was very popular for a while. IE4 ruled the web.</p><p>The shift to web based technologies is a Google and an open source win. And it is also an inferior experience. That was the price we paid =)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471522"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471522" href="https://news.ycombinator.com/vote?id=37471522&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Even in the IE4 days, Microsoft was still very much "we'll only do as much web as necessary, but we can't lose our Windows fort... how can we sabotage this effort?". They were in full-on EEE mode[1] then and wanted to subvert, not join, the web. JScript, ActiveX, ClickOnce, IE's idiosyncracies, etc. were all Microsoft's own efforts to get around the webification of everything. They lost, not just because of Google but also Netscape, Mozilla/Phoenix, eBay, Craigslist, Amazon, Match, MapQuest, etc. Nobody wanted to build desktop apps anymore once they could effortlessly reach everyone via the web without having to be a Microsoft vassal.<p>Even ASP and IIS etc. insisted on having its own stack -- superior in some ways, but way less compatible and more expensive. Again their own doing. Free/cheap won out, I guess :)</p><p>[1] <a href="https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguish#Examples_by_Microsoft" rel="nofollow noreferrer">https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguis...</a></p><p>(edit: how the heck do you actually properly make hyperlinks on HN? I always struggle with this)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37470935"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470935" href="https://news.ycombinator.com/vote?id=37470935&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt; web pages won out for their reach and ease of use,<p>Your definition of "ease of use" is amazing. /s
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471078"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471078" href="https://news.ycombinator.com/vote?id=37471078&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Heh, point taken. But I'd say overall it's easier for grandma to go to Gmail to check her email than have to figure out what this "Netscrape Communicator" is, install it, set up her "pop three" from her "eyesp" and then deal with a million viruses.<p>Yeah, the web ain't perfect, but it did win... ads got worse, though =/
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37470741"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470741" href="https://news.ycombinator.com/vote?id=37470741&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Visual Basic 6 was very productive - yes.  But it was considered by many to be a toy language (it didn't support class inheritance for one).<p>VB.NET - at least initial versions - was about productive as C#, thus there was no incentive to use VB.</p><p>The thing that made VB6 super productive was it's form designer.  The .NET successor - WinForms designer - wasn't nearly as fast and capable (to this day, really).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472196"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472196" href="https://news.ycombinator.com/vote?id=37472196&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>From my perspective, Microsoft killed it.  It did not die on its own.  The upgrade path from VB6 to VB.NET was basically unusable.  You either stayed on VB6 or you rewrote the application.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37470684"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470684" href="https://news.ycombinator.com/vote?id=37470684&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt;&gt; I have yet to find a tool that can allow me to be as productive in so short a time as Visual Basic.<p>For me: Delphi or Lazarus
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472358"><td></td></tr>
                <tr id="37472474"><td></td></tr>
                  <tr id="37470688"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470688" href="https://news.ycombinator.com/vote?id=37470688&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>VB died because of .NET and VB.NET. The syntaxes were similar, but VB.NET was much closer to a "real programming language" in feeling and complexity than VB was, and that's not what anyone who used VB actually wanted.<p>Microsoft was more interested in developing .NET and C# to battle Java, and less interested in developing and promoting VB, their own successful and original product.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                      <tr id="37472465"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472465" href="https://news.ycombinator.com/vote?id=37472465&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>What made VB6 super-productive and quick to get up and running also made it terrible for long-term maintenance, because it hid too much of the details.<p>Then there were language warts like set versus let (strong vs weak pointers), for example, which was way above the paygrade of the average VB6 coders; and having to rely on the Win32 API anyway in order to start doing actual work and work around all of its limitaitons.</p><p>One thing VB6 did do right was forcing interface-based inheritance: Composition over class inheritance was seen as a weakness back then but it was proven as the right concept.</p><p>This is just my opinion!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471131"><td></td></tr>
            <tr id="37471134"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471134" href="https://news.ycombinator.com/vote?id=37471134&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span><i>&gt;nothing I have found compares to that development experience today. I would go so far as to say we've gone backwards in a big way.</i><p>I did Visual Basic 3.0 through VB 6.0 corporate development for a few years back in the 1990s.  The closest equivalent today for desktop apps <i>that still has Microsoft's focus on future innovation</i> is C# with Windows Forms.  (I downplay the "obvious" comparison of VB.NET to VB 6.0 because Microsoft already said they will "stop evolving" Visual Basic .NET -- so that's a technology dead end and will fall further and further behind the latest C# as the years go by.)</p><p>I personally don't experience that C#/Winforms has gone backwards from VB 6.0.  Workflow feels much the same as VB6:  Drag some GUI components like text boxes and buttons onto a form, code the controls' event handlers, build the exe.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471178"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471178" href="https://news.ycombinator.com/vote?id=37471178&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I've only done a small amount of VB programming, but from what I remember of it, WinForms + C# is lightyears ahead of VB.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37470665"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470665" href="https://news.ycombinator.com/vote?id=37470665&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Its simple: Corporate internal app development (mostly CRUD stuff, as youd expect) was the bread-and-butter of VB, and it moved wholesale to HTML &amp; JavaScript.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472149"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472149" href="https://news.ycombinator.com/vote?id=37472149&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I recently wondered:<p>Why is there no VB-Like tool for building Electron - Apps using eg. React-Widgets &amp; Javascript as the scripting - language?</p><p>I'm not married to Electron vs. another similar, possibly more modern / less ressource hungry alternative; or another Frontend Framework.</p><p>But it seems to be it should be possible to do a VB-Style thing using these kinds of tools, Drag+Drop, and Javascript, most of the components should be available already...</p><p>And it could be a fun environment for prototyping, having fun, creating really bad games &amp; greeting card apps again like in the 90s/2000s etc etc.</p><p>It could be a fun learning environment for newbies while a powerful GUI builder for multiple platforms for experts..</p><p>Why isn't there such a thing? Would somebody please build this? :D
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470873"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470873" href="https://news.ycombinator.com/vote?id=37470873&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>VB (and BASIC in general) has a legacy of being a procedural language. In the late 90s and early 2000s, there was a massive push to leverage object oriented programming and design. The births of Java in the 90s and then C# in the 2000s were clear catalysts to abandon procedural programming.<p>Of course in .NET VB has nearly all of the OO capabilities that C# has, but I think most developers just decided to go all-in on object oriented programming and learn C# and graduate from their procedural past.</p><p>A lot of colleges taught Java and moving to it or C# in the workplace was a much more natural process.</p><p>There are BASIC alternatives and they are fairly strong offerings, but I think OO and functional programming are the standard today.</p><p>That means python, Rust, Golang, Ruby, C#, and Java are the mainstream languages.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37470891"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470891" href="https://news.ycombinator.com/vote?id=37470891&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Honestly, I was never a fan of Basic but I still don't understand why there isn't a GUI app builder as productive as the VB6-era tools were for any language.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37470969"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37470969" href="https://news.ycombinator.com/vote?id=37470969&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Devs dont design user interfaces anymore. There are separate disciplines for user experience, graphic design, front-end development, API development, and data storage choices.<p>Theres no reason to have a drag and drop UI builder anymore.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472436"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472436" href="https://news.ycombinator.com/vote?id=37472436&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>not everything is a team effort, and the desire espoused all over this thread for an equivalency clearly demonstrates that there is a 'market' there.<p>when some small developer is making a silly one-off app for a mom&amp;pop local store to facilitate a one-off kind of task they aren't interested in handing off work and splitting meager profits. not every company has the whole "front-end/back-end/devops/ux/design/management" paradigm going on.</p><p>the reality is that microsoft , a fairly litigious group of people, abandoned a concept for their own reasons; and the rest of the market doesn't exactly know where they can step in that minefield of offering equivalent features to a piece of software that is still on life support by a very very large/valuable/litigious company.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="37470421"><td></td></tr>
                <tr id="37470493"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470493" href="https://news.ycombinator.com/vote?id=37470493&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I think thats the sense in which it died in that if you were going to switch to .NET you would probably also switch to C#.  The original VB had A GUI builder better than almost anything (even today!) but the UI builders in todays visual studio support C# as well if not better than VB.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37470752"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37470752" href="https://news.ycombinator.com/vote?id=37470752&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>You bring up a good point, which is that I remember most the GUI builder.  Basic isn't really that great a language, all things considered.  But being able to quickly design and deploy GUI apps with VB was better than anything I've encountered since.<p>There are things like WebFlow and FlutterFlow but those tools feel clunky by comparison.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37470811"><td></td></tr>
                <tr id="37470924"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37470924" href="https://news.ycombinator.com/vote?id=37470924&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Netbeans java gui builder is the only builder Ive seen come close to the vb gui builder. In fact I was a little disappointed moving from net beans to a real Java ide since their gui builders dont exist or arent as good. But vb as a language is a lot better than Java for getting to a productive state for a novice.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37470710"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37470710" href="https://news.ycombinator.com/vote?id=37470710&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>And everything in that IDE was fast and super responsive. Today's IDE are more complex than ever and yet they lack in functionality that was present in VB6.0</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37470868"><td></td></tr>
                  <tr id="37470663"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37470663" href="https://news.ycombinator.com/vote?id=37470663&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I once had a job offer from a big company that developed exclusively in VB.NET, after transitioning away from VB6. Both for desktop and web. So it's still out there.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37470985"><td></td></tr>
            <tr id="37470848"><td></td></tr>
                <tr id="37470992"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37470992" href="https://news.ycombinator.com/vote?id=37470992&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Well Basic evolved a lot. Comparing 80s Basic on whatever hardware you imagine (think goto + Line Numbers) to 90s Microsoft VisualBasic (think event Hndlers for UIs) is a similar jump than VisualBasic to VisualBasic.NET (writing OO code).<p>VB.NET is the same but so much more evolved. Also the runtime below was switched (surely for the better).</p><p>However, like outlined in other comments: VB.NET is a OOP language which handles procedural/functional code as a subset while VB of the 90s was purely procedural.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37471812"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471812" href="https://news.ycombinator.com/vote?id=37471812&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>With VB.NET, VB basically turned into C# with different syntax. I think a lot of people in that space might as well have been writing C#, so a lot of them switched. Then separately, perhaps almost simultaneously, the Microsoft ecosystem lost relevance.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37470442"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470442" href="https://news.ycombinator.com/vote?id=37470442&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>IMO, it didn't really make a smooth transition to the web. Webforms was the attempt but it was a leaky abstraction (iirc, it kept state for UI components but not variables so things got... weird) and kind of worked against the paradigms at the time (everything was a POST).</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37470795"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470795" href="https://news.ycombinator.com/vote?id=37470795&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Yeah exactly, VB dying is sort of the CONVERSE of programmers adopting platforms, not languages - <a href="https://old.reddit.com/r/ProgrammingLanguages/comments/sk6w1c/programming_languages_as_biological_strategies/hvkxp4n/" rel="nofollow noreferrer">https://old.reddit.com/r/ProgrammingLanguages/comments/sk6w1...</a><p>- Why is JS popular?  Not because it's the best language, but because it's attached to the browser, and people want to deploy apps to browsers (Figma, etc.)</p><p>- Why is shell the 8th most popular language on Github, and the 6th fastest growing? [1]  Not because it's the best language, but because it's attached to the Unix kernel (specifically Linux, which has a ton of features).  Software in containers and virtual machines must talk to kernels.</p><p>So then the converse is</p><p>- Why is VB no longer popular?  Not because it's a worse language than it used to be (though maybe that's true), but the platform that it supported isn't as popular.</p><p>Like others said, it's probably popular for Excel and app automation, etc.  But today more apps are targeting web and mobile, not Windows desktop.</p><p>- Same answer with  Objective C and Swift -- people are using them to write apps for a platform.  And Kotlin/Android, etc.</p><p>[1] <a href="https://octoverse.github.com/2022/top-programming-languages" rel="nofollow noreferrer">https://octoverse.github.com/2022/top-programming-languages</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472264"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472264" href="https://news.ycombinator.com/vote?id=37472264&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I used VB 3.0 thru VB 6.0 and I echo the sentiment: those were fun and productive times.  Really solid integrated development environment centered around the desktop UI / form design / UI controls like buttons, textboxes, combo boxes, etc.<p>I also agree that the web changed everything and that is the major reason for the shift away.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472501"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472501" href="https://news.ycombinator.com/vote?id=37472501&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Popularity of a language is like a popularity of a soda - it's never about the inherent quality or features, but always about the company that stands behind it.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472369"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472369" href="https://news.ycombinator.com/vote?id=37472369&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I just recently wanted to spin up a simple CRUD UI over a simple DB schema and also thought of Visual Basic for the first time.  It seems so well suited to something like building operational tooling for the endless parade of internal APIs.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472093"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472093" href="https://news.ycombinator.com/vote?id=37472093&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt; despite all the advances in technology since then, nothing I have found compares to that development experience today<p>You are going to have to qualify this a lot more, because it is absolutely not true. Coding VB was...fine. Language features were primitive, even compared to what was available in the mainstream back then. The tooling was fully proprietary and expensive. Languages and IDEs today are 1000x better in every way.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472260"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472260" href="https://news.ycombinator.com/vote?id=37472260&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>It's the Python of yesteryear. Nothing about the language was great but the ecosystem was amazing.<p>CRUD app creation was almost completely point and click, VBA was easy and you could take data straight from a document and process it in complex ways, the database integrations were great and easy to use.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472224"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472224" href="https://news.ycombinator.com/vote?id=37472224&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Honestly it's because Microsoft sucked at marketing. There were so many confusing name choices and unclear development guidelines that people simply stopped using it. Even now developing for windows is a pain and that's because there is like 5 different ways to build an app there.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37471824"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471824" href="https://news.ycombinator.com/vote?id=37471824&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>You're probably speaking to VB plus Visual Studio RAD (Rapid Application Development aka "drag and drop" GUI programming) and you more or less still have that today with VB.NET and Windows Forms, with minor differences.<p>What killed the older VB6 and it's APIs, whose name I can't recall even though I developed for it in the 1990s, was .net coming along.</p><p>Be warned, it might just be easier to learn C# as it's more well-supported/documented, and VB.net seems like effectively a language dialect of C#.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472421"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472421" href="https://news.ycombinator.com/vote?id=37472421&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Because you could only use it if you paid for it. If they had released it for free, as in beer, lots of people would have made wi does apps with it.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37470966"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470966" href="https://news.ycombinator.com/vote?id=37470966&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>vb didn't die, it evolved into vb.net, but the market place has shifted to C# for the most part.<p>What has died, is the maturity of microsofts tool set, they keep changing their concept/design/platform.</p><p>silverlight/wpf/uwp/winui2|3/etc..</p><p>vb was around for nearly 2 decades and had a very mature tool set, everything since then hasn't gotten nearly that sort of life span or dedication to tool sets.</p><p>Developing in visual studio now, is more like web dev in the 2000s, I can't tell you how often you have to go to the xaml and make correction or adjustments that the UI just can't get right, or just goes bonkers and can't render the UI at all until something is fixed.</p><p>It is really sad, because the power of those old drag and drop builders that just worked meant that prototyping and mocking up applications was much much faster.</p><p>now standing up a UI based project takes ages, I'll usually do a console application now, and are dumping results to a API or console.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471206"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471206" href="https://news.ycombinator.com/vote?id=37471206&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>People always worry that Microsoft is silverlighting (that is a verb now) MAUI. I think that would be the end of VisualStudio. The word Visual had a meaning. When they would give up MAUI in favor of React Native (like Office) or Blazor (like the popular opinion), why the hack someone would buy a VS license. And when the think they could again commercialize .NET itself, then .NET would be dead. Modern Java, Flutter and TypeScript would easily swallow their market shares. MAUI, Blazor and .NET are an awesome set of products if they would just put some more concentration in MAUI.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472396"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472396" href="https://news.ycombinator.com/vote?id=37472396&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>The development paradigm was great. I wish three were something similar for Python, having to switch to TCL/TK for UI stuff is extremely annoying.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37470692"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470692" href="https://news.ycombinator.com/vote?id=37470692&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I wonder the same thing about Apple's HyperCard. I used HyperCard a ton at school and side-projects. Visual Basic was a firmer and better step-up from that imho, less so as VBScript when it came to browsers.<p>As per other comments VBA is alive and well and I did many consulting gigs using that in Ireland in the late 1990s. I hope, in the name of all that is holy, that code isn't running still.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37470763"><td></td></tr>
            <tr id="37470719"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470719" href="https://news.ycombinator.com/vote?id=37470719&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I wonder about HyperCard as well.  I never used it but, from everything I've ever heard about it, it was also an amazing developer experience.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37470776"><td></td></tr>
                        <tr id="37470872"><td></td></tr>
            <tr id="37472092"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472092" href="https://news.ycombinator.com/vote?id=37472092&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I started my career with FoxPro for DOS, dabbled a bit with Visual FoxPro and then I found Visual Basic. Spent a lot of fun years with it before moving to ASP and web development in general. Not exactly sure what was the reason for its demise. I guess .NET killed it.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472720"><td></td></tr>
            <tr id="37470777"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470777" href="https://news.ycombinator.com/vote?id=37470777&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>There's still no better GUI toolkit out there than VB6 that I have used. It was amazing.<p>Problem is a lot of apps that would have been traditional LOB apps written in VB/C# have moved to the web so demand isn't there's clear advantages to C# as a language over VB.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37470858"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470858" href="https://news.ycombinator.com/vote?id=37470858&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Netbeans Java Swing GUI builder is far better. That's without factoring in the enormous third party component landscape that is available.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37471325"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471325" href="https://news.ycombinator.com/vote?id=37471325&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Oh man, I remember building GUI apps with it in college.  My professor only let me use it on the condition that I could fully explain what every component was doing.  I could.  I took the generated swing code and added a <i>massive</i> amount of comments, but even having to do that I was still an order of magnitude faster than every other CS student without.  A lot of my peers were super jealous that I had so much free time, but they weren't willing to invest time in their tools (gdb, perl, regex, sql, etc).  It was quite the force multiplier.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37472378"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472378" href="https://news.ycombinator.com/vote?id=37472378&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I was a VB6 developer in the 1990s, and I actually wrote a pretty well known product with it (I bet most people didn't know it was VB6 underneath!).<p>When I got my first job in software engineering, it was when .Net was in beta.  Since I only knew VB, I chose to go with VB.Net and Windows Forms.</p><p>Well, that was quite a jarring introduction to actual object oriented programming!</p><p>This is the point when a lot of people surrendered.  The change was too much, there was no path forward for VB, and Microsoft wanted everyone on .Net.</p><p>I eventually switched to C# and we had a hybrid application for a while before it was all ported to C#.  It actually went on to become industry leading software in its space.</p><p>20+ years later, that doesn't seem to have been a bad choice.  Windows Forms is very simple, with a powerful drag-and-drop designer, double-click to hook up events, it's all very similar.  Yes, you have to understand object oriented programming much more than you would have under VB, but that's as close as you are going to get from Microsoft on the desktop.  They try to hide it as best they can.</p><p>I <i>still</i> use Windows Forms if I need to put together something that "just works" on Windows as a desktop app quickly.  It's unmatched for that.</p><p>There are newer technologies, such as WPF, WinUI3, Avalonia and others for .Net but they come with a complexity that would be above your typical use-case for VB6 back in the day.  The VB6 user who just wanted to get stuff done probably doesn't want to have to understand how to implement the MVVM pattern just to get a UI.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470978"><td></td></tr>
            <tr id="37471197"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471197" href="https://news.ycombinator.com/vote?id=37471197&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>The web is cross-platform and easier to deploy. The (non-?)existence of RAD tools for it vs VB pales in comparison to that advantage.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37470940"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470940" href="https://news.ycombinator.com/vote?id=37470940&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>We had plenty of amazing paradigms/development environments/holistic experiences which we've regressed from:<p>- LISP environments</p><p>- Smalltalk environments</p><p>- Symbolics genera</p><p>- Mesa and Cedar</p><p>- Apple's Newton</p><p>Besides things like Oberon...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471828"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471828" href="https://news.ycombinator.com/vote?id=37471828&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>New version of VB was VB.net.<p>People still used VB.net primarily as they were familiar with VB.</p><p>But the new projects gradually took over C#.net, as it sounded more cool.</p><p>VB.net died after a while.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471717"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471717" href="https://news.ycombinator.com/vote?id=37471717&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>How about some love for Quick BASIC? That was my big Christmas present once upon a time when the dinosaurs still roamed the earth.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37472167"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472167" href="https://news.ycombinator.com/vote?id=37472167&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Oh you spoiled little children with your integrated development environment and in place editing. Real programmers used GW-Basic with consecutive line numbers.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472287"><td></td></tr>
            <tr id="37470729"><td></td></tr>
                <tr id="37470922"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470922" href="https://news.ycombinator.com/vote?id=37470922&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>How many incompatible languages called VB-something did Microsoft create? There's at least the classic Visual Basic, VBA, VBScript, and VB.NET.<p>Although to be fair, the distinction between the language and libraries has never been very clear in BASIC variants.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37471884"><td></td></tr>
                <tr id="37472155"><td></td></tr>
                  <tr id="37471150"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471150" href="https://news.ycombinator.com/vote?id=37471150&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>While I was never a big fan of VB, I agree that the developer experience has been getting worse for most languages since the early 1990s.<p>Do you have a top 3 list of things you miss?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471958"><td></td></tr>
            <tr id="37471439"><td></td></tr>
            <tr id="37471938"><td></td></tr>
            <tr id="37470717"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470717" href="https://news.ycombinator.com/vote?id=37470717&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>My personal reason, which is probably unique:<p>VB 1.0 was extremely buggy, making it unusable for my use cases.</p><p>VB 1.1, which fixed the bugs I was encountering, was a paid upgrade.</p><p>I switched to Linux and never looked back.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470644"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470644" href="https://news.ycombinator.com/vote?id=37470644&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I got the feeling that a lot of the corp/small business apps that used to be developed in VB and deployed to PCs have been replaced by Web-based apps.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37471070"><td></td></tr>
                <tr id="37471317"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471317" href="https://news.ycombinator.com/vote?id=37471317&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>VB was first released in 1991. If you're certain that you remember it from the late 80's, then you may be thinking of QBASIC. I first learned to write code in QBASIC running on MS-DOS.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37471083"><td></td></tr>
            <tr id="37470859"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470859" href="https://news.ycombinator.com/vote?id=37470859&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>DotNet is meant to be the replacement.  The big problem though is that MS keeps failing to make a GUI framework that is quick-and-easy as VB forms were.  Winforms still exists, and it's only a <i>bit</i> clumsier than VB forms, but it's very old and not modern.  The more modern .NET gui-frameworks are much less user-friendly.<p>Linguistically, I think the successor to VB is Powershell.  It's the same mashup of inconsistent flags that let you swap between "this is a serious language" and "I'm smashing crap together" with tons of unexpected weird behavior, but instead of being a quick-and-dirty GUI app maker, it's a Shell.  Hardcore focus on being easy and productive but unforgivably warty.</p><p>As for VB itself, VB.Net just didn't offer much value distinct from C#, so most people who were coding in VB switched to C#.</p><p>So if you're an old longbearded MS LOB programmer who started before .NET, and you're still working in Microsoft LOB shops, you're probably doing similar stuff but with C#.  But realistically, you've probably also switched to Web.</p><p>And the lack of the VB-level ease-of-use in <i>web</i> technologies is a whole other story.  All the hoary mess of using a document-engine for a cross-platform application server makes it pretty untameable.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470656"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470656" href="https://news.ycombinator.com/vote?id=37470656&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>VB6 morphed into VB.Net but then the web took over and anything that could be accomplished as a desktop app was replaced by a web page.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472097"><td></td></tr>
            <tr id="37470372"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470372" href="https://news.ycombinator.com/vote?id=37470372&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>There's still Visual Basic .NET, is it much different productivity wise? I assume the language has changed enough from classic VB.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37470403"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470403" href="https://news.ycombinator.com/vote?id=37470403&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I honestly haven't used it because I thought they had retired it altogether.  The last time I did use it was about 20 years ago and found that they had changed it enough that I wasn't nearly as productive with it as, say, VB6.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37470474"><td></td></tr>
                        <tr id="37472294"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472294" href="https://news.ycombinator.com/vote?id=37472294&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt;"nothing I have found compares to that development experience today. I would go so far as to say we've gone backwards in a big way"<p>&gt;"I have yet to find a tool that can allow me to be as productive in so short a time as Visual Basic"</p><p>I have and do program in many languages. From my perspective - for type the of applications usually done in VB Delphi / Lazarus would run circles around it. Both productivity and performance wise. It is also possible to do things one simply can not accomplish in VB.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471211"><td></td></tr>
                <tr id="37472193"><td></td></tr>
                  <tr id="37472114"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Calculate the difference and intersection of any two regexes (278 pts)]]></title>
            <link>http://phylactery.org/antimirov/</link>
            <guid>37470285</guid>
            <pubDate>Mon, 11 Sep 2023 17:10:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://phylactery.org/antimirov/">http://phylactery.org/antimirov/</a>, See on <a href="https://news.ycombinator.com/item?id=37470285">Hacker News</a></p>
<div id="readability-page-1" class="page">
<pre><h2>enter some regular expressions!</h2>
<b></b> := 
<b></b> := 
<hr>
<b>   ~</b> = <span id="not-alpha"></span>
<b>   ~</b> = <span id="not-beta"></span>
<b> &lt; </b> = <span id="alpha-lt-beta">false</span>
<b> = </b> = <span id="alpha-eq-beta">true</span>
<b> &gt; </b> = <span id="alpha-gt-beta">false</span>
<b> &amp; </b> = <span id="alpha-and-beta">.*</span>
<b> ^ </b> = <span id="alpha-xor-beta"></span>
<b> - </b> = <span id="alpha-minus-beta"></span>
<hr>
<b>s</b> := 

<b>s  </b> = <span id="str-in-alpha">true</span>
<b>s  </b> = <span id="str-in-beta">true</span>
<hr>
<b>||</b> = <span id="alpha-card">0</span>
<b>||</b> = <span id="beta-card">0</span>
<hr>
<b>dfa()</b> has <span id="alpha-dfa">0</span> states

<b>dfa()</b> has <span id="beta-dfa">0</span> states

<hr>
<b>regex syntax</b>

  <b>.</b>         match any single character
  <b>xy</b>        concatenation: match <b>x</b> and then <b>y</b>
  <b>x|y</b>       alternation: match <b>x</b> or <b>y</b>
  <b>x*</b>        kleene star: match <b>x</b> zero-or-more times
  <b>(xyz)</b>     grouping: treat <b>xyz</b> as a single item (e.g. <b>(xyz)*</b>)
  <b>()</b>        an empty regex matches the empty string
  <b>x+</b>        kleene plus: match <b>x</b> one-or-more times (equivalent to <b>xx*</b>)
  <b>x?</b>        optional: optionally match <b>x</b> (equivalent to <b>(x|)</b>)
  <b>x{n}</b>      exponentiation: concatenate <b>x</b> to itself <b>n</b> times
  <b>x{m,n}</b>    repetition: concatenate <b>x</b> to itself between <b>m</b> and <b>n</b> times
  <b>[a-z0-9]</b>  grouping: match any single character in the group
  <b>[^a-z0-9]</b> negative grouping: match any single character <b>not</b> in the group
  <b>\c</b>        escaping: match the special character <b>c</b>
  <b>\u001a</b>    unicode escaping: match the corresponding UTF-16 character
  a, b, c   all other characters match themselves

<b>unsupported features</b>

  - anchors (e.g. <b>^</b>, <b>$</b>), <i>although <b>^</b> and <b>$</b> must still be escaped!</i>
  - zero-width assertions (e.g. <b>(?=...)</b>, <b>(?&lt;=...)</b>)
  - back references (e.g. <b>\1</b>, <b>\2</b>)
  - subgroup extraction
  - searching or partial matching
  - other flags that change behavior (e.g. case-insensitivity)

see <a href="https://github.com/non/antimirov">https://github.com/non/antimirov</a> for more information

by <a href="http://plastic-idolatry.com/erik/">eirkr sheim</a> (@d6 on <a href="https://twitter.com/d6">twitter</a> and <a href="https://mastodon.social/@d6">mastodon</a>)

    </pre>

    
    
    

    

    

    <!-- <script type="text/javascript" src="antimirov-web-fastopt.js"></script> -->

  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[WiFi can read through walls (206 pts)]]></title>
            <link>https://news.ucsb.edu/2023/021198/wifi-can-read-through-walls</link>
            <guid>37469920</guid>
            <pubDate>Mon, 11 Sep 2023 16:45:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.ucsb.edu/2023/021198/wifi-can-read-through-walls">https://news.ucsb.edu/2023/021198/wifi-can-read-through-walls</a>, See on <a href="https://news.ycombinator.com/item?id=37469920">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-history-node-id="21198">
      

    
                    <div><p><span><span><span><span lang="EN" xml:lang="EN"><span><span>Researchers in UC Santa Barbara professor Yasamin Mostofis lab have proposed a new foundation that can enable high-quality imaging of still objects with only WiFi signals. Their method uses the Geometrical Theory of Diffraction and the corresponding Keller cones to trace edges of the objects. The technique has also enabled, for the first time, imaging, or reading, the English alphabet through walls with WiFi, a task deemed too difficult for WiFi due to the complex details of the letters.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span></span></span></span></p>
<p><span><span><span><span lang="EN" xml:lang="EN"><span><span>For more details on this technology, check their video at <span><a href="https://www.youtube.com/watch?v=pvqL3gqGDeM">https://www.youtube.com/watch?v=pvqL3gqGDeM</a></span></span></span></span></span></span></span></p>
<p><span><span><span><span lang="EN" xml:lang="EN"><span><span><span>Imaging still scenery with WiFi is considerably challenging due to the lack of motion, said Mostofi, a professor of electrical and computer engineering. We have then taken a completely different approach to tackle this challenging problem by focusing on tracing the edges of the objects instead.&nbsp; The proposed methodology and experimental results appeared in the Proceedings of the 2023 IEEE </span></span></span></span><span lang="EN" xml:lang="EN"><span><span><span>National Conference on Radar</span></span></span></span><span lang="EN" xml:lang="EN"><span><span><span> (RadarConf) on June 21, 2023.</span></span></span></span></span></span></span></p>
</div>

                  

                  <div><p><span><span><span><span lang="EN" xml:lang="EN"><span><span>This innovation builds on previous work in the Mostofi Lab, which since 2009 has pioneered sensing with everyday radio frequency signals such as WiFi for several different applications, including crowd analytics, person identification, smart health and smart spaces. </span></span></span></span></span></span></p>
<p><span><span><span><span lang="EN" xml:lang="EN"><span><span>When a given wave is incident on an edge point, a cone of outgoing rays emerges according to the Kellers Geometrical Theory of Diffraction (GTD), referred to as a Keller cone, Mostofi explained. The researchers note that this interaction is not limited to visibly sharp edges but applies to a broader set of surfaces with a small enough curvature.</span></span></span></span></span></span></p>
<p><span lang="EN" xml:lang="EN"><span><span>Depending on the edge orientation, the cone then leaves different footprints (i.e., conic sections) on a given receiver grid. We then develop a mathematical framework that uses these conic footprints as signatures to infer the orientation of the edges, thus creating an edge map of the scene,&nbsp; Mostofi continued. </span></span></span></p>
</div>

                  <div><p><span><span><span><span lang="EN" xml:lang="EN"><span><span>More specifically, the team proposed a Keller cone-based imaging projection kernel. This kernel is implicitly a function of the edge orientations, a relationship that is then exploited to infer the existence/orientation of the edges via hypothesis testing over a small set of possible edge orientations. In other words, if existence of an edge is determined, the edge orientation that best matches the resulting Keller cone-based signature is chosen for a given point that they are interested in imaging. </span></span></span></span></span></span></p>
<p><span><span><span><span lang="EN" xml:lang="EN"><span><span>Edges of real-life objects have local dependencies, said Anurag Pallaprolu, the lead Ph.D. student on the project. Thus, once we find the high-confidence edge points via the proposed imaging kernel, we then propagate their information to the rest of the points using Bayesian information propagation. This step can further help improve the image, since some of the edges may be in a blind region, or <span>can be </span>overpowered by other edges that are closer to the transmitters. Finally, once an image is formed, the researchers can further improve the image by using image completion tools from the area of vision.</span></span></span></span></span></span></p>
<p><span><span><span><span lang="EN" xml:lang="EN"><span><span><span>It is worth noting that traditional imaging techniques result in poor imaging quality when deployed with commodity WiFi transceivers, added Pallaprolu, as the surfaces can appear near-specular at lower frequencies, thus not leaving enough signature on the receiver grid. &nbsp;</span></span></span></span></span></span></span></p>
<p><span><span><span><span lang="EN" xml:lang="EN"><span><span><span>The researchers have also extensively studied the impact of several different parameters, such as curvature of a surface, edge orientation, distance to the receiver grid, and transmitter location on the Keller cones and their proposed edge-based imaging system, thereby developing a foundation for a methodical imaging system design. </span></span></span></span></span></span></span></p>
</div>

                  <div><p><span><span><span><span lang="EN" xml:lang="EN"><span><span><span>In the teams experiments, three off-the-shelf WiFi transmitters send wireless waves in the area. WiFi receivers are then mounted on an unmanned vehicle that emulates a WiFi receiver grid as it moves.&nbsp; The receiver measures the received signal power which it then uses for imaging, based on the proposed methodology. </span></span></span></span></span></span></span></p>
<p><span lang="EN" xml:lang="EN"><span><span><span>The researchers have extensively tested this technology with several experiments in three different areas, including through-wall scenarios. In one example application, they developed a WiFi Reader to showcase the capabilities of the proposed pipeline. </span></span></span></span></p>
</div>

                  <div><p><span><span><span><span lang="EN" xml:lang="EN"><span><span><span>This application is particularly informative as the English alphabet presents complex details which can be used to test the performance of the imaging system. Along this line, the group has shown how they can successfully image several alphabet-shaped objects. In addition to imaging, they can further classify the letters. Finally, they have shown how their approach enables WiFi to image and read through walls by imaging the details and further reading the letters of the word BELIEVE through walls. They have furthermore imaged a number of other objects as well, showing that they can capture details previously not possible with WiFi. </span></span></span></span></span></span></span></p>
<p><span><span><span><span lang="EN" xml:lang="EN"><span><span>Overall, the proposed approach can open up new directions for RF imaging.</span></span></span></span></span></span></p>
</div>

                  

                  

      
  




            
      
            
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The meeting of the minds that launched AI (142 pts)]]></title>
            <link>https://spectrum.ieee.org/dartmouth-ai-workshop</link>
            <guid>37469849</guid>
            <pubDate>Mon, 11 Sep 2023 16:40:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/dartmouth-ai-workshop">https://spectrum.ieee.org/dartmouth-ai-workshop</a>, See on <a href="https://news.ycombinator.com/item?id=37469849">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-elid="2659955531" data-post-url="https://spectrum.ieee.org/dartmouth-ai-workshop" data-authors="Grace Solomonoff" data-headline="The Meeting of the Minds That Launched AI" data-page-title="The Meeting of the Minds That Launched AI - IEEE Spectrum"><p>The <a href="http://jmc.stanford.edu/articles/dartmouth/dartmouth.pdf" rel="noopener noreferrer" target="_blank">Dartmouth Summer Research Project on Artificial Intelligence</a>, held from 18 June through 17 August of 1956, is widely considered the event that kicked off AI as a research discipline. Organized by <a href="https://amturing.acm.org/award_winners/mccarthy_1118322.cfm" target="_blank">John McCarthy</a>, <a href="https://web.media.mit.edu/~minsky/" target="_blank">Marvin Minsky</a>, <a href="https://spectrum.ieee.org/claude-shannon-tinkerer-prankster-and-father-of-information-theory" target="_blank">Claude Shannon</a>, and <a href="https://en.wikipedia.org/wiki/Nathaniel_Rochester_(computer_scientist)" target="_blank">Nathaniel Rochester</a>, it brought together a few dozen of the leading thinkers in AI, computer science, and information theory to map out future paths for investigation.</p><p>A group photo [shown above] captured seven of the main participants. When the photo was <a href="https://spectrum.ieee.org/history-of-ai" target="_self">reprinted</a> in Eliza Stricklands October 2021 article The Turbulent Past and Uncertain Future of Artificial Intelligence in <em><a href="https://spectrum.ieee.org/">IEEE Spectrum</a></em>, the caption identified six people, plus one unknown. So who was this unknown person?</p><h2>Who is in the photo?</h2><p>Six of the people in the photo are easy to identify. In the back row, from left to right, we see <a href="https://www.nytimes.com/2008/12/04/us/04selfridge.html" rel="noopener noreferrer" target="_blank">Oliver Selfridge</a>, Nathaniel Rochester, Marvin Minsky, and John McCarthy. Sitting in front on the left is <a href="http://raysolomonoff.com/index.html" rel="noopener noreferrer" target="_blank">Ray Solomonoff</a>, and on the right, Claude Shannon. All six contributed to AI, computer science, or related fields in the decades following the Dartmouth workshop.</p><p><img alt="Close up of a black and white photo of seven smiling men, sitting on a lawn." data-rm-shortcode-id="4c395bd647a036dd5677d8158e7eeb05" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/close-up-of-a-black-and-white-photo-of-seven-smiling-men-sitting-on-a-lawn.jpg?id=33603729&amp;width=980" height="2250" id="6557b" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/close-up-of-a-black-and-white-photo-of-seven-smiling-men-sitting-on-a-lawn.jpg?id=33603729&amp;width=980" width="3000"><small placeholder="Add Photo Caption...">In the back row from left to right are Oliver Selfridge, Nathaniel Rochester, Marvin Minsky, and John McCarthy. In front on the left is Ray Solomonoff; on the right, Claude Shannon. The identity of the person between Solomonoff and Shannon remained a mystery for some time.</small><small placeholder="Add Photo Credit...">The Minsky Family</small></p><p>Between Solomonoff and Shannon is the unknown person. Over the years, some people <a href="https://pemey.medium.com/whos-that-kid-laughing-with-high-socks-in-the-middle-of-summer-7801a34feeef" rel="noopener noreferrer" target="_blank">suggested</a> that this was <a href="https://en.wikipedia.org/wiki/Trenchard_More" target="_blank">Trenchard More</a>, another AI expert who attended the workshop.</p><p>I first ran across the Dartmouth group photo in 2018, when I was gathering material for <a href="https://raysolomonoff.com/" rel="noopener noreferrer" target="_blank">Rays memorial website</a>. Ray and I had met in 1969, and we got married in 1989; he <a href="https://www.nytimes.com/2010/01/10/science/10solomonoff.html" rel="noopener noreferrer" target="_blank">passed away</a> in late 2009. Over the years, I had attended a number of his talks, and I had met many of Rays peers and colleagues in AI, so I was curious about the photo.</p><p>I thought, Gee, that guy in the middle doesnt look like my memory of Trenchard. So I called up Trenchards son Paul More. He assured me that the unknown person was not his father.</p><p>More recently, I discovered a letter among Rays papers. On 8 November 1956, Nat Rochester sent a short note and a copy of the photo to some colleagues: Enclosed is a print of the photograph I took of the <a href="https://spectrum.ieee.org/topic/artificial-intelligence/">Artificial Intelligence</a> group. He sent his note to McCarthy, Minsky, Selfridge, Shannon, Solomonoffand Peter Milner.</p><p data-rm-resized-container="25%"><img alt="A typed letter with the photograph of the six men,  is addressed to six names from Nathaniel Rochester." data-rm-shortcode-id="29a714f8f61d2228ac93efd838df9fb6" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-typed-letter-with-the-photograph-of-the-six-men-is-addressed-to-six-names-from-nathaniel-rochester.jpg?id=33603732&amp;width=980" height="1375" id="1b532" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-typed-letter-with-the-photograph-of-the-six-men-is-addressed-to-six-names-from-nathaniel-rochester.jpg?id=33603732&amp;width=980" width="1338"><small placeholder="Add Photo Caption...">Several months after the workshop, Nathaniel Rochester sent a copy of the photo, along with this note, to six people.</small><small placeholder="Add Photo Credit...">Grace Solomonoff</small></p><p>So the unknown person must be Milner! This makes perfect sense. <a href="https://www.mcgill.ca/psychology/article/remembering-peter-m-milner-1919-2018" target="_blank">Milner</a> was working on neuropsychology at <a href="https://www.mcgill.ca/" target="_blank">McGill University</a>, in Montreal, although he had trained as an electrical engineer. Hes not generally lumped in with the other AI pioneers because his research interests diverged from theirs. Even at Dartmouth, he felt he was in over his head, as he wrote in his 1999 autobiography: I was invited to a meeting of computer scientists and information theorists at Dartmouth College. Most of the time I had no idea what they were talking about.</p><p>In his fascinating autobiography, Milner writes about his work in radar development during World War II, and his switch after the war from nuclear-reactor design to psychology. His doctoral thesis in 1954, <a href="https://escholarship.mcgill.ca/concern/theses/zc77sv01t" target="_blank">Effects of Intracranial Stimulation on Rat Behaviour</a>, examined the effects of electrical stimulation on certain rat neurons, which became widely and enthusiastically known as pleasure centers.</p><p>This work led to one of Milners most famous papers, <a href="https://psycnet.apa.org/record/1959-00249-001" target="_blank">The Cell Assembly: Mark II</a>, in 1957. The paper describes how, when a neuron in the brain fires, it excites similar connected neurons (especially those already aroused by sensory input) and randomly excites other cortical neurons. Cells may form assemblies and connect with other assemblies. But the neurons dont seem to exhibit the same snowballing behavior of atoms that leads to an exponential explosion. How neurons might inhibit this effect were among his ideas that led to new insights at the workshop.</p><p>Milners work contributed to the early development of artificial neural networks, and its why he was included in the Dartmouth meeting. There was considerable interest among AI researchers in studying the brain and neurons in order to reproduce its functions and intelligence.</p><p>But as Strickland notes in her October 2021 <em>Spectrum</em> article, a division was already forming in AI research. One side focused on replicating the brain, while the other was more interested in what the mind might do to directly solve problems. Scientists interested in this latter approach were also represented at Dartmouth and later championed the rise of symbolic logic, using heuristic and algorithmic processes, which Ill discuss in a bit.</p><h2>Where Was the Photo Taken?</h2><p>Rochesters photo from 1956 shows the left-hand side of Dartmouth Hall in the background. In 2006 Dartmouth convened a conference, <a href="https://en.wikipedia.org/wiki/AI@50" target="_blank">AI@50</a>, to celebrate the 50th anniversary of the AI gathering and to discuss AIs present and future. Trenchard More, the person most often misidentified as the unknown person in Nats photo, met with the organizers, <a href="https://en.wikipedia.org/wiki/James_H._Moor" rel="noopener noreferrer" target="_blank">James Moor</a> and Carey Heckman, as well as Wendy Conquest, who was working on a movie about AI for the conference. None of the AI@50 organizers knew exactly where the 1956 meeting had taken place.</p><p>More led them across the lawn and to the left-hand side door of Dartmouth Hall. He showed them the rooms that were used, which in turn triggered an old memory. During the 1956 meeting, as More recalled in a <a href="http://raysolomonoff.com/dartmouth/misc/Trenchardinterview2011.pdf" rel="noopener noreferrer" target="_blank">2011 interview</a>, Selfridge, and Minsky, and McCarthy, and Ray Solomonoff, and I gathered around a dictionary on a stand to look up the word <em>heuristic</em>, because we thought that might be a useful word. On that 2006 tour of Dartmouth Hall, he was delighted to find that the dictionary was still there.</p><p>The word <em>heuristic</em> was invoked all through the summer of 1956. Instead of trying to analyze the brain to develop machine intelligence, some participants focused on the operational steps needed to solve a given problem, making particular use of heuristic methods to quickly identify the steps.</p><p>Early in the summer, for instance, Herb Simon and Allen Newell gave a talk on a program they had written, the <a href="https://ieeexplore.ieee.org/document/1056797" target="_blank">logic theory machine</a>. The program relied on early ideas of symbolic logic, with algorithmic steps and heuristic guidance in list form. They later won the 1975 Turing Award for these ideas. Think of heuristics as intuitive guides. The logic theory machine used such guides to initiate the algorithmic stepsthat is, the set of instructions to actually carry out the problem solving.<br></p><h2>Who Wasnt in the Photo</h2><p>There was one person who was at the Dartmouth Workshop from time to time but was never included in any of the lists of attendees: Gloria Minsky, Marvins wife.</p><p>But Gloria was definitely a presence that summer. Marvin, Ray, and John McCarthy were the only three participants to stay for the entire eight-week workshop. Everyone else came and went as their schedules allowed. At the time, Gloria was a pediatrics fellow at Childrens Hospital in Boston, but whenever she could, she would drive up to Dartmouth, stay in Marvins apartment, and visit with whoever was at the workshop.</p><p>Several years earlier, in the spring of 1952, Gloria had been doing her residency in pathology at New Yorks Bellevue Hospital, when she began dating Marvin. Marvin was a Ph.D. student at Princeton, as was McCarthy, and the two were invited to Bell Labs for the summer to work under Claude Shannon. In July, just four months after their first meeting, Gloria and Marvin got married. Although Marvin was working nonstop for Shannon, Shannon insisted he and Gloria take a honeymoon in New Mexico.</p><p data-rm-resized-container="25%"><img alt="A letter from John McCarthy to Ray Solomonoff on Dartmouth College stationery." data-rm-shortcode-id="85ad1d73a223bb5735e7b0fe2c2fc67d" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-letter-from-john-mccarthy-to-ray-solomonoff-on-dartmouth-college-stationery.jpg?id=33603735&amp;width=980" height="1001" id="4a2b5" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-letter-from-john-mccarthy-to-ray-solomonoff-on-dartmouth-college-stationery.jpg?id=33603735&amp;width=980" width="620"><small placeholder="Add Photo Caption...">In March 1956, John McCarthy, one of the Dartmouth AI workshops organizers, invited Ray Solomonoff to the summer workshop in Hanover, N.H.</small><small placeholder="Add Photo Credit...">Grace Solomonoff</small></p><p>Four years later, McCarthy, Shannon, and Minsky, along with Nat Rochester, organized the <a href="http://jmc.stanford.edu/articles/dartmouth/dartmouth.pdf" target="_blank">Dartmouth workshop</a>. Gloria remembered a conversation between her husband and Ray, in which Marvin expressed a thought that later became one of his hallmarks: You need to see something in more than one way to understand it. In Minskys 2007 book <a href="https://www.simonandschuster.com/books/The-Emotion-Machine/Marvin-Minsky/9780743276641" rel="noopener noreferrer" target="_blank"><em>The Emotion Machine</em></a>, he looked at how emotions, intuitions, and feelings create different descriptions and provide different ways of looking at things. He tended to favor symbolic logic and deductive methods in AI, which he called good old-fashioned AI.</p><p>Ray, meanwhile, was focused on probabilitiesthe likelihood of something happening and predictions of how it might evolve. He later developed algorithmic probability, an early version of algorithmic information theory, in which each different description of something leads with a probabilistic likelihood (some more likely, some less likely) of a given outcome in the future. Probabilistic methods eventually became the underpinnings of machine learning.</p><p>These days, as chatbots enter the limelight, and compression methods are used more in AI, the value of understanding things in many ways and using probabilistic predictions will only grow in importance. That is, logic and probability methods are uniting. These in turn are being aided by new work on neural nets as well as symbolic logic. And so the photo that Nat Rochester took not only captured a moment in time for AI. It also offered a glimpse into how AI would develop.</p><p><em>The author thanks Gloria Minsky, Margaret Minsky,</em><em>Nicholas Rochester, Julie Sussman, Gerald Jay Sussman, and Paul More for their help and patience.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: Firefox addon to quarantine a tab to use offline with private data (125 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37469321</link>
            <guid>37469321</guid>
            <pubDate>Mon, 11 Sep 2023 16:06:17 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37469321">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="37472816"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472816" href="https://news.ycombinator.com/vote?id=37472816&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>We have our first bug bounty!<p>Thank you "dz2742" for finding out [1] existing connections including websockets are not terminated and has won 100 USD! This is exactly the type of exploit I was hoping to catch.</p><p>Now I have to figure out how to fix that :) And also think about refilling the bug bounty pool without becoming very poor very soon.</p><p><a href="https://github.com/matusfaro/quarantab/issues/2">https://github.com/matusfaro/quarantab/issues/2</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37469787"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37469787" href="https://news.ycombinator.com/vote?id=37469787&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><br><div>
                  <p><span>Cool idea!  I don't really picture myself using this, but I think this add-on is a great example of how great a browser Firefox is.  I'd be the first to critique Mozilla, and there are definitely things about Firefox I don't like (ex. Pocket, telemetry on by default), but overall I think it's an amazing product in that it allows for multiple levels of isolation (profiles, containers, private mode) and a level of control over them that Chromium either doesn't do as cleanly or doesn't do at all.  As an aside, the only thing I think Chromium does better is the debugging experience; I don't truly understand why Firefox thinks it shouldn't support debugging Node.js like Chromium does.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37469951"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37469951" href="https://news.ycombinator.com/vote?id=37469951&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>&gt; it allows for multiple levels of isolation<p>Yes! Chrome has a visually similar functionality to Firefox Containers hidden away behind a feature flag [1] at the moment. BUT under the hood it's simply just tab grouping with no isolation. I presume isolation is against Google's interests so we will never see this kind of feature.</p><p>As for Firefox's API, the Contextual Identities API [2] that allows you to create/delete containers is amazing and easy to work with as a dev. And it works out-of-the-box, it doesn't need the companion addon Multi-Account Containers (MAC) [3] which really should've been part of Firefox in my opinion.</p><p>1. chrome://flags/#tab-groups-save</p><p>2. <a href="https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/API/contextualIdentities" rel="nofollow noreferrer">https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/Web...</a></p><p>3. <a href="https://addons.mozilla.org/en-US/firefox/addon/multi-account-containers/" rel="nofollow noreferrer">https://addons.mozilla.org/en-US/firefox/addon/multi-account...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471275"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471275" href="https://news.ycombinator.com/vote?id=37471275&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>Firefox's containers are useless for privacy, given other enhancements of Firefox (e.g., Total Cookie Protection). And as far as "isolation", privacy or security are concerned, Chrome's profiles are actually superior due to ability to have different extensions and history per profile. Chrome's extensions in general still have superior security (e.g., activate on click or only for certain websites), so sometimes different profiles aren't even needed.<p>Chrome's Profiles are also remembered when you "install an app" (SSB/PWA), so you could have "apps" started in their own profiles.</p><p>Firefox's containers are only useful if you want multiple logins to the same service in the same browser window. But I never found that usecase to be very compelling.</p><p>Firefox's containers are an often lauded feature, and I don't understand why, given the integration issues or general awkwardness. It's probably a reminiscence of the "Facebook container" extension, which was a bandaid until better site isolation was implemented.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472905"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472905" href="https://news.ycombinator.com/vote?id=37472905&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><br><div>
                  <p><span>A counter anecdote is that I have the exact opposite use case. I don't share my computer with other users, so I've never needed something like profiles. Firefox containers are great for keeping different sites, especially those notorious for tracking (e.g. Amazon, Google, LinkedIn) completely isolated from each other or from general browsing. Plus, the extension that allows for creating temporary containers is great for one-off visits to e-commerce sites without needing to switch to a new private/incognito window. I'm not sure I've ever wanted my extensions isolated by container/profile, that seems like it would hinder productivity. Same for history. It's great having all my history commingled, especially if I want to find something from 30 tabs ago.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37471780"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37471780" href="https://news.ycombinator.com/vote?id=37471780&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>&gt; Chrome's profiles are actually superior due to ability to have different extensions and history per profile<p>Interesting attack vector I haven't thought about which could leak information out of a network-locked Firefox Container. It would be under an assumption you have either:</p><p>1. A malicious extension installed (you have a much worse problem in this case)</p><p>2. A side-effect of an existing extension that leaks information to the outside world. (e.g. translate a part of a page, lookup a word in a dictionary, pre-fetch some images...)</p><p>&gt; Firefox's containers are only useful if you want multiple logins</p><p>I think there are valid use cases for both Containers and Profiles. You can go down the list to have more and more isolation as needed:</p><p>- Grouping tabs to stay organized, no isolation</p><p>- Firefox containers, same browser window, shared history &amp; extensions</p><p>- Chrome profiles, almost complete isolation within same browser (different processes)</p><p>- Separate browser instances</p><p>- Separate devices
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471798"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37471798" href="https://news.ycombinator.com/vote?id=37471798&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><br><div>
                  <p><span>Firefox has profiles too.  Containers are for use within a profile.  You keep saying that containers aren't useful but you don't elucidate on <i>how</i> they are useless for privacy or <i>what</i> integration issues exist.  I don't know how to interpret 'general awkwardness.'  Can you fill in some details?</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37471987"><td></td></tr>
                <tr id="37472249"><td></td></tr>
                              <tr id="37473191"><td></td></tr>
                <tr id="37473550"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37473550" href="https://news.ycombinator.com/vote?id=37473550&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>This extension is still very cool.<p>+1 to Cyberchef, its awesome. If you really have qualms about the URL its trivial to re-host / serve it to yourself offline.</p><p>My favorite part is whole recipe feature (Cyberchef builds a URL with the configured processors you use to process data).</p><p>I find myself using that a ton to share XPath / JPAth expressions type work with sample data to others by sharing that URL.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37474291"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37474291" href="https://news.ycombinator.com/vote?id=37474291&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><br><div>
                  <p><span>I know this won't land well, and certainly it's a good option, but there's a terrific and hilarious irony in someone saying "I don't really trust the third parties with my non public data" And you're like, yeah use the one tool that's built and maintained by a literal spy agency.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37469420"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37469420" href="https://news.ycombinator.com/vote?id=37469420&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>While I applaud your effort and thinking of privacy issues, I will continue to do these in a terminal and Python REPL for all the reasons you bring up.<p>It would certainly be nice to get something ala F-droid for free software extensions like yours (which guarantees source code matches built package IIRC), as a response to your question 3.</p><p>I am sure one can create an alternative extensions store in FF and change some config in about:config to use it, though it's likely non-trivial.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37469649"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37469649" href="https://news.ycombinator.com/vote?id=37469649&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>Agreed, and to be honest, this extension is more for myself as I would be extremely skeptical if someone else made it especially with the permissions it requires.<p>It would probably be more successful as a feature added to an existing trusted extension such as Temporary Containers.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470673"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470673" href="https://news.ycombinator.com/vote?id=37470673&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>&gt; I will continue to do these in a terminal and Python REPL for all the reasons you bring up.<p>Do you have a way to prevent terminal utilities from accessing the network?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471080"><td></td></tr>
                <tr id="37472108"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472108" href="https://news.ycombinator.com/vote?id=37472108&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>Interesting options, wasn't aware of those.<p>The only minor counter-argument would be laziness as a security threat: the more difficult you make the process, the more likely the user will skip seemingly useless steps, thus compromising security.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="37470379"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470379" href="https://news.ycombinator.com/vote?id=37470379&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>&gt; Submit my name and birthdate to estimate my date of death<p>Totally off topic, but curious how this works? Nationality and life expectancy? Sex at birth? Assassins for hire?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37474316"><td></td></tr>
            <tr id="37470934"><td></td></tr>
                <tr id="37471496"><td></td></tr>
                <tr id="37471606"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37471606" href="https://news.ycombinator.com/vote?id=37471606&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>Oh my god, this is scary. I don't <i>want</i> to live until the 2060s, lol. It terrifies me to think about how the world will be then...<p>But anyway, thanks for the link!</p><p>For the curious, this is their methodology:</p><p>&gt; Remaining life expectancy at specific age (in days) was obtained by interpolating (spline) the 5 yearly/duration age-specific period life expectancies.</p><p>&gt; Population.io uses official demographic data produced by the United Nations and published in the World Population Prospects
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="37470504"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470504" href="https://news.ycombinator.com/vote?id=37470504&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>Cool add on! Thanks for this. It's a use case I've often thought about, for the purposes you mention. I wish there was a built in permission to disable AJAX after page load. Bad for ads, I guess.<p>2. Exploit idea (not trying for the bounty, just thinking aloud). I wonder if a website could play background music (or a video) with stenographically encoded data, then another tab could listen to it with microphone permissions on and decode it that way. I'm thinking like a fake video conferencing site, or malicious telephony how-to doc that deals with API calls and such and links to a fake password hasher that then plays the audio for the first tab to hear. Convoluted, I know, just an idea.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472030"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472030" href="https://news.ycombinator.com/vote?id=37472030&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>&gt; built in permission to disable AJAX after page load<p>Interesting, but consider this is a cat-and-mouse game. If you are the only one using this trick it may work for you, but I assume would be easy to overcome. (e.g. keep the page loading forever or until ads are loaded. Have the ads be J-free after page load, ...)</p><p>&gt; website could play background music ... another tab could listen</p><p>You would need mic access from the other tab, but yes. If you send it over high enough frequency you wouldn't even hear it. You would just have a visual feedback that the tab is playing music.</p><p>On a side-note, I recall there was some kind of hardware device pairing (maybe Chromecast?) that used data over voice to establish that you are physically near the other device.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472845"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472845" href="https://news.ycombinator.com/vote?id=37472845&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>&gt; On a side-note, I recall there was some kind of hardware device pairing (maybe Chromecast?) that used data over voice to establish that you are physically near the other device.<p>Yeah, that's pretty common in home smart devices. Looks like Google patented one version and Sonos has their implementation too. In my experience it works better than Bluetooth, especially in (2.4 GHz) noisy environments
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472896"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472896" href="https://news.ycombinator.com/vote?id=37472896&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>Funny that you say Sonos.<p>I also remember there was a data-over-voice library called "chirp.io" which now redirects to Sonos homepage. Now I know why they acquired them :)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472964"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37472964" href="https://news.ycombinator.com/vote?id=37472964&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>I wonder if it's also part of the patent battle they got in with Google over smart speaker stuff.<p>Side rant: It's so sad, to this day Google Assistant works terribly on my Sonos system, and it's a major reason I'm reluctant to further buy into their ecosystem. And Sonos's own assistant doesn't even support Spotify, last I checked. Their whole UX is... not great. I really wanted to work there and maybe try to fix some of the issues I experience as a user, but they rejected me. Alas.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                    <tr id="37472771"><td></td></tr>
                <tr id="37472870"><td></td></tr>
                  <tr id="37472086"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472086" href="https://news.ycombinator.com/vote?id=37472086&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>that's a nice idea<p>the same way you can silence the sound output of a tab you should have as simple and reliable a tool to stop communication to either the network, os or both.</p><p>i'd love a tool to see which tabs are talking with each other also
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37473003"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37473003" href="https://news.ycombinator.com/vote?id=37473003&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>&gt; love a tool to see which tabs are talking with each other also<p>Cool idea but probably not that useful and difficult to accomplish. There are many ways to communicate that could be grouped into:</p><p>1. tab -&gt; tab (same domain)</p><p>2. tab -&gt; tab (different domain)</p><p>3. tab -&gt; server -&gt; tab</p><p>For #1, there are so many ways to transfer information it would be hard to detect and differentiate whether it's communication or just happens to be using the same resource. (e.g. one sets a cookie or local storage and the other one reads it)</p><p>For #3, it would be impossible to detect. Especially if detection is an issue, both tabs could be communicating with unrelated servers which talk with each other.</p><p>For #2, it would be the only interesting one as there is limited options (e.g. Broadcast Channel), but at the same time I assume rarely used in practice. And if detection is an issue, they would switch to #3 to avoid it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37473044"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UK air traffic control meltdown (144 pts)]]></title>
            <link>https://jameshaydon.github.io/nats-fail/</link>
            <guid>37468600</guid>
            <pubDate>Mon, 11 Sep 2023 15:19:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jameshaydon.github.io/nats-fail/">https://jameshaydon.github.io/nats-fail/</a>, See on <a href="https://news.ycombinator.com/item?id=37468600">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
      <p>Comments on <a href="https://www.reddit.com/r/programming/comments/16fhmuq/a_deep_dive_into_the_bug_that_caused_the_uk_air/?utm_source=share&amp;utm_medium=web2x&amp;context=3">reddit</a></p>
<p>On 28 August 2023 <em>NATS</em>, the UK's air traffic control operator, suffered a
<strong>major</strong> technical incident. The BBC reports that more than <a href="https://www.bbc.com/news/uk-66685349">2000 flights were
cancelled</a> and the cost has been estimated
at over <em>100 million</em> GBP. The incident probably affected hundreds of thousands
of people.</p>
<p>The press initially reported the cause was a faulty flight plan: <em>UK air traffic
control: inquiry into whether French error caused failure</em> (The Times) and in
typical Mail Online reporting style: <em>"Did blunder by French airline spark air
traffic control issues? Officials probe if a single badly filed travel plan
caused UK's entire flight-control system to collapse in worst outage for a
decade - with 1,000 flights cancelled and chaos set to last DAYS"</em>.</p>
<p>So what happened? These are notes on my reading of the incident report:</p>
<blockquote>
<p>NATS Major Incident Preliminary Report<br>
Flight Plan Reception Suite Automated (FPRSA-R) Sub-system Incident 28th August 2023 <br>
<a href="https://publicapps.caa.co.uk/docs/33/NERL%20Major%20Incident%20Investigation%20Preliminary%20Report.pdf">pdf</a>.</p>
</blockquote>
<p><em>NATS</em> is a "public-private" company in the UK that is responsible for all of
the UK's air traffic control:</p>
<blockquote>
<p>Air Traffic Control (ATC) is the provision and operation of a safe system for
controlling and monitoring aircraft.
[..] <br>
aircraft [..] are required to file a flight plan.
[..] <br>
ATC ensures that aircraft are safely separated laterally and vertically.</p>
</blockquote>
<h2 id="what-went-wrong">What went wrong</h2>
<blockquote>
<p>The start of the sequence of events leading to the incident can be tracked
back to the point at which a flight plan was entered into the flight planning
system.</p>
<p>[Airlines] submit the plan into Eurocontrols Integrated Initial Flight Plan
Processing System (IFPS).
[..]</p>
<p>If the submitted flight plan is accepted by IFPS, i.e. it is compliant with
IFPS defined parameters [...] this is sufficient for a flight to depart with
local ATC approval. The flight plan will be sent from IFPS to all relevant
ANSPs who need to manage the flight.
[..]</p>
<p>Within the NATS En-route operations at Swanwick Centre, the data is passed to
FPRSA-R. The FPRSA-R sub-system exists to convert the data received from IFPS
(in a format known as ATS Data Exchange Presentation, ADEXP) into a format
that is compatible with the UK National Airspace System (NAS). NAS is the
flight data processing system which contains all of the relevant airspace and
routings.
[..]</p>
<p>FPRSA-R has a primary and backup system monitored both by dedicated Control
and Monitoring (C&amp;M) systems and also an aggregated central C&amp;M system.
Further resilience is provided by NAS storing 4 hours of previously filed
flight data to allow the operation to continue in the event of the loss of
automatic processing of flight data.
[..]</p>
<p>In addition to the technical resilience provided by backup systems, and the 4
hours of stored flight data, there is operational contingency available to
allow safe service to continue. This is provided through the ability to input
flight data manually, directly into NAS using a manual input system.</p>
</blockquote>
<p>To summarise:</p>
<ul>
<li>Flight plans are first submitted to a European-wide authority <em>IFPS</em>.</li>
<li>If a plan is accepted, the flight is cleared for takeoff.</li>
<li><em>NATS</em> requires the flight plan be transferred to them at least 4 hours before
the aircraft is due to enter UK airspace. This is supposed to give NATS a
4-hour window to be able to fix any problems in processing flight plans.</li>
<li>It seems that there is also probably some process which <em>delays</em> flight plans
until <em>close</em> to the deadline (see below). This might be to avoid congesting
the system with flight plans too early, or lots of plans that may later
change. Still, this results in flight plans being received by NATS sometimes
<em>hours</em> after the flight has taken off.</li>
</ul>
<blockquote>
<p>The NATS ATC System was operating normally.
[..]</p>
<p>[On] 28 August the airline submitted an ICAO4444 compliant flight plan into
Eurocontrols flight planning distribution system, IFPS.</p>
</blockquote>
<p>ICAO stands for <a href="https://en.wikipedia.org/wiki/International_Civil_Aviation_Organization" title="wikipedia">International Civil Aviation
Organization</a>, a United Nations agency.
An ICAO4444 flight plan looks like this:</p>
<pre><code><span>(FPL-TTT123-IS
</span><span>-C550/L-SDE1E2GHIJ3J5RWZ/SB1D1
</span><span>-KPWM1225
</span><span>-N0440F310 SSOXS5 SSOXS DCT BUZRD
</span><span>DCT SEY DCT HTO J174 ORF J121
</span><span>CHS EESNT LUNNI1
</span><span>-KJAX0214 KMCO
</span><span>-PBN/A1L1B1C1D1O1T1 NAV/Z1 GBAS
</span><span>DAT/1FANS2PDC SUR/260B RSP180
</span><span>DOF/220501 REG/N123A SEL/BPAM
</span><span>CODE/A05ED7)
</span></code></pre>
<p>Such messages are in a format that is meant to be read by machines, but also by
humans if necessary. The format is spec'd over many many pages of PDF, but is
roughly:</p>
<pre><code><span>( FPL-ACID-Flt Rules Flight Type
</span><span>- AC Type/Wake Cat-
</span><span>Equip.&amp;Capability
</span><span>- Departure EOBT
</span><span>- Speed Altitude [sp] Route
</span><span>- Destination ETE [sp]
</span><span>Alternate(s)
</span><span>- Other Information )
</span></code></pre>
<p>The route part (in this example: <code>N0440F310 SSOXS5 SSOXS DCT BUZRD DCT SEY DCT HTO J174 ORF J121 CHS EESNT LUNNI1</code>) encodes an overall speed (here <code>N0440</code>
meaning <code>440 knots</code>), an overall altitude (here <code>F310</code> which means "Flight
Level 310" which means <code>310  100 ft</code> (can also be in <code>km</code>)), and a sequence of
waypoints (referenced by name) separated by a description of how to get from the
previous waypoint to the next one, usually by referencing a "known route" by
name.</p>
<blockquote>
<p>The flight plan was accepted by IFPS
[..] <br>
the aircraft was cleared to depart at 04:00.
[..]</p>
<p>At 08:32 the flight plan was received by NATS FPRSA-R sub-system from
Eurocontrols IFPS system. This is consistent with the 4 hour rule mentioned
above. The purpose of the FPRSA-R software is to extract the UK portion of the
flight plan [..]</p>
<p>The flight plans delivered to FPRSA-R by IFPS are converted from [..] ICAO4444
to [..] ADEXP. ADEXP is a European-wide flight plan specification that
includes, amongst other data, additional geographical waypoints within the
European region specific to the route of a flight. For flights transiting
through UK airspace, rather than landing in the UK, this will include
additional waypoints outside of UK airspace required for its onward journey.
Following this conversion the ADEXP version of a flight plan includes, amongst
other aspects, the original ICAO4444 flight plan plus an additional list of
waypoints and other data.</p>
</blockquote>
<p>ADEXP looks like this:</p>
<pre><code><span>-TITLE IFPL
</span><span>-BEGIN ADDR
</span><span>  -FAC LIIRZEZX
</span><span>  [...]
</span><span>  -FAC LYZZEBXX
</span><span>-END ADDR
</span><span>-ADEP EDDF
</span><span>-ADES LGTS
</span><span>-ARCID KIM1
</span><span>-ARCTYP B738
</span><span>-CEQPT SDGRWY
</span><span>-EOBD 170729
</span><span>-EOBT 0715
</span><span>-FILTIM 280832
</span><span>-IFPLID AT00441635
</span><span>-ORIGIN -NETWORKTYPE SITA -FAC FRAOXLH
</span><span>-SEQPT C
</span><span>-WKTRC M
</span><span>-PBN B2
</span><span>-REG DABHM
</span><span>-SEL KMGJ
</span><span>-SRC FPL
</span><span>-TTLEET 0210
</span><span>-RFL F330
</span><span>-SPEED N0417
</span><span>-FLTRUL I
</span><span>-FLTTYP S
</span><span>-ROUTE N0417F330 ANEKI8L ANEKI Y163 NATOR UN850 TRA UP131 RESIA Q333
</span><span>BABAG UN606 PEVAL DCT PETAK UL607 PINDO UM603 EDASI
</span><span>-ALTRNT1 LBSF
</span><span>-BEGIN RTEPTS
</span><span>  -PT -PTID EDDF -FL F004 -ETO 170729073000
</span><span>  -PT -PTID RID -FL F100 -ETO 170729073404
</span><span>  -PT -PTID ANEKI -FL F210 -ETO 170729073856
</span><span>  -PT -PTID NEKLO -FL F214 -ETO 170729073911
</span><span>  -PT -PTID BADLI -FL F248 -ETO 170729074118
</span><span>  -PT -PTID PABLA -FL F279 -ETO 170729074348
</span><span>  -PT -PTID HERBI -FL F308 -ETO 170729074624
</span><span>  -PT -PTID NATOR -FL F330 -ETO 170729074911
</span><span>  -PT -PTID TITIX -FL F330 -ETO 170729075154
</span><span>  -PT -PTID TRA -FL F330 -ETO 170729075323
</span><span>  -PT -PTID ARGAX -FL F330 -ETO 170729080055
</span><span>  -PT -PTID RESIA -FL F330 -ETO 170729080731
</span><span>  -PT -PTID UNTAD -FL F330 -ETO 170729081243
</span><span>  -PT -PTID DIKEM -FL F330 -ETO 170729081627
</span><span>  -PT -PTID ROKIB -FL F330 -ETO 170729081824
</span><span>  -PT -PTID BABAG -FL F330 -ETO 170729082816
</span><span>  -PT -PTID PEVAL -FL F330 -ETO 170729082916
</span><span>  -PT -PTID PETAK -FL F330 -ETO 170729091754
</span><span>  -PT -PTID PINDO -FL F330 -ETO 170729093322
</span><span>  -PT -PTID EDASI -FL F165 -ETO 170729094347
</span><span>  -PT -PTID LGTS -FL F000 -ETO 170729095713
</span><span>-END RTEPTS
</span><span>-SID ANEKI8L
</span><span>-ATSRT Y163 ANEKI NATOR
</span><span>-ATSRT UN850 NATOR TRA
</span><span>-ATSRT UP131 TRA RESIA
</span><span>-ATSRT Q333 RESIA BABAG
</span><span>-ATSRT UN606 BABAG PEVAL
</span><span>-DCT PEVAL PETAK
</span><span>-ATSRT UL607 PETAK PINDO
</span><span>n -ATSRT UM603 PINDO EDASI
</span></code></pre>
<p>You can read about ADEXP in the <a href="https://www.eurocontrol.int/sites/default/files/2023-06/eurocontrol-released-specification-adexp-3-4.pdf" title="pdf">official spec</a>.
Some notable fields (page 48):</p>
<table><thead><tr><th>Adexp Primary Field</th><th>Kind</th><th>Syntax</th><th>Semantic</th></tr></thead><tbody>
<tr><td>route</td><td>b</td><td><code>'-' "ROUTE" {LIM_CHAR}</code></td><td>Complete ICAO Field 15 information containing speed, RFL and route (conforming to the syntax given in Ref. [3]).</td></tr>
<tr><td>rtepts</td><td>c</td><td><code>'-' "BEGIN" "RTEPTS" { pt I ad / vec} '-' "END" "RTEPTS"</code></td><td>List of route points. May also contain an aerodrome identifier.</td></tr>
</tbody></table>
<p>In the example, we have the ICAO route:</p>
<pre><code><span>-ROUTE N0417F330 ANEKI8L ANEKI Y163 NATOR UN850 TRA UP131 RESIA Q333 BABAG UN606 PEVAL DCT PETAK UL607 PINDO UM603 EDASI
</span></code></pre>
<p>(9 waypoints, 11 if you add the start and end waypoints)</p>
<p>Visually, routes look like:
<img src="https://jameshaydon.github.io/nats-fail/flight_plan.png" alt="some route" title="A flight plan route"></p>
<p>(You can play around with flight plans at
<a href="https://flightplandatabase.com/">flightplandatabase.com</a>, a website for people
who like playing with flight simulators)</p>
<p>We can indent the "route" parts between the waypoints in the ICAO plan to make
things clearer:</p>
<pre><code><span>N0417F330
</span><span>  ANEKI8L 
</span><span>  ANEKI 
</span><span>    Y163
</span><span>  NATOR
</span><span>    UN850
</span><span>  TRA
</span><span>    UP131
</span><span>  RESIA
</span><span>    Q333
</span><span>  BABAG
</span><span>    UN606
</span><span>  PEVAL
</span><span>    DCT
</span><span>  PETAK
</span><span>    UL607
</span><span>  PINDO
</span><span>    UM603
</span><span>  EDASI
</span></code></pre>
<p>E.g. <code>ANEKI Y163 NATOR</code> means "go from waypoint <code>ANEKI</code> to waypoint <code>NATOR</code> via
the route <code>Y163</code>". <code>DCT</code> means "direct".</p>
<p>The <code>ADEXP</code> format has more waypoints, along with more precision about altitude and estimated time at each waypoint:</p>
<pre><code><span>-BEGIN RTEPTS
</span><span>-PT -PTID EDDF  -FL F004 -ETO 170729073000
</span><span>-PT -PTID RID   -FL F100 -ETO 170729073404
</span><span>-PT -PTID ANEKI -FL F210 -ETO 170729073856
</span><span>-PT -PTID NEKLO -FL F214 -ETO 170729073911
</span><span>-PT -PTID BADLI -FL F248 -ETO 170729074118
</span><span>-PT -PTID PABLA -FL F279 -ETO 170729074348
</span><span>-PT -PTID HERBI -FL F308 -ETO 170729074624
</span><span>-PT -PTID NATOR -FL F330 -ETO 170729074911
</span><span>-PT -PTID TITIX -FL F330 -ETO 170729075154
</span><span>-PT -PTID TRA   -FL F330 -ETO 170729075323
</span><span>-PT -PTID ARGAX -FL F330 -ETO 170729080055
</span><span>-PT -PTID RESIA -FL F330 -ETO 170729080731
</span><span>-PT -PTID UNTAD -FL F330 -ETO 170729081243
</span><span>-PT -PTID DIKEM -FL F330 -ETO 170729081627
</span><span>-PT -PTID ROKIB -FL F330 -ETO 170729081824
</span><span>-PT -PTID BABAG -FL F330 -ETO 170729082816
</span><span>-PT -PTID PEVAL -FL F330 -ETO 170729082916
</span><span>-PT -PTID PETAK -FL F330 -ETO 170729091754
</span><span>-PT -PTID PINDO -FL F330 -ETO 170729093322
</span><span>-PT -PTID EDASI -FL F165 -ETO 170729094347
</span><span>-PT -PTID LGTS  -FL F000 -ETO 170729095713
</span><span>-END RTEPTS
</span></code></pre>
<p>(21 waypoints)</p>
<p>We can mark which of the ADEXP waypoints have a corresponding waypoint in the ICAO plan (with a <code>+</code>) and which are implicit (with a <code>|</code>): </p>
<pre><code><span>EDDF   |
</span><span>RID    |
</span><span>ANEKI  + 
</span><span>NEKLO  |
</span><span>BADLI  |
</span><span>PABLA  |
</span><span>HERBI  |
</span><span>NATOR  +
</span><span>TITIX  |
</span><span>TRA    +
</span><span>ARGAX  |
</span><span>RESIA  +
</span><span>UNTAD  |
</span><span>DIKEM  |
</span><span>ROKIB  |
</span><span>BABAG  +
</span><span>PEVAL  |
</span><span>PETAK  +
</span><span>PINDO  +
</span><span>EDASI  +
</span><span>LGTS   |
</span></code></pre>
<p>Note that the ICAO waypoints do not contain the start and end, since in the
original ICAO format these are specified in other fields (so it would waste
space to list them again in this list).</p>
<blockquote>
<p>The ADEXP waypoints plan included two waypoints along its route that were
geographically distinct but which have the same designator.</p>
</blockquote>
<p>This means there were two lines like:</p>
<pre><code><span>-PT -PTID RESIA -FL F330 -ETO 170729080731
</span></code></pre>
<p>that had the same <code>PTID</code> string like <code>"RESIA"</code>.</p>
<blockquote>
<p>Although there has been work by ICAO and other bodies to eradicate non-unique
waypoint names there are duplicates around the world. In order to avoid
confusion latest standards state that such identical designators should be
geographically widely spaced. In this specific event, both of the waypoints
were located outside of the UK, one towards the beginning of the route and one
towards the end; approximately 4000 nautical miles apart.</p>
</blockquote>
<p>4000 nautical miles is 7408km. Here is an arc of that length on the globe:
<img src="https://jameshaydon.github.io/nats-fail/4000-nautical-miles.png" alt="4000 nautical miles on a globe"></p>
<blockquote>
<p>Once the ADEXP file had been received, the FPRSA-R software commenced
searching for the UK airspace entry point in the waypoint information per the
ADEXP flight plan, commencing at the first line of that waypoint data. FPRSA-R
was able to specifically identify the character string as it appeared in the
ADEXP flight plan text.</p>
</blockquote>
<p>The programming style is very imperative. Furthermore, the description sounds
like the procedure is working directly on the textual representation of the
flight plan, rather than a data structure parsed from the text file. This would
be quite worrying, but it might also just be how it is explained.</p>
<blockquote>
<p>Having correctly identified the entry point, the software moved on to search
for the exit point from UK airspace in the waypoint data.</p>
<p>Having completed those steps,</p>
</blockquote>
<p>This part of the code identified <code>entry</code> and <code>exit</code> waypoints to UK airspace in
the list of <code>ADEXP</code> waypoints.</p>
<blockquote>
<p>FPRSA-R then searches the ICAO4444 section of
the ADEXP file.</p>
</blockquote>
<p>It seems at this point, having identified the entry and exit points from the
list of ADEXP waypoints, it will try to extract the UK portion of the flight plan from the ICAO route.</p>
<blockquote>
<p>It initially searches from the beginning of that data, to find
the identified UK airspace entry point. This was successfully found. Next, it
searches backwards, from the end of that section, to find the UK airspace exit
point. This did not appear in that section of the flight plan so the search
was unsuccessful. As there is no requirement for a flight plan to contain an
exit waypoint from a Flight Information Region (FIR) or a countrys airspace,
the software is designed to cope with this scenario.</p>
<p>Therefore, where there is no UK exit point explicitly included, the software
logic utilises the waypoints as detailed in the ADEXP file to search for the
next nearest point beyond the UK exit point. This was also not present.</p>
<p>The software therefore moved on to the next waypoint.</p>
</blockquote>
<p>OK, so I think this is what is going on, the situation looked something like
this:</p>
<pre><code><span>           4       2        8         5              1           9
</span><span>ICAO:  F------Q--------T--------O-----------P---------------Y--------U
</span><span>
</span><span>ADEXP: F   S  Q    C   T   A    O  E  X     P   W   B   Q   Y        U
</span><span>                       UK  UK   UK UK UK    UK  UK
</span></code></pre>
<p>Here the ICAO route has waypoints (represented by capital letters) separated by
known routes (numbers). On the bottom we have the ADEXP waypoints. The ADEXP
waypoints that are located in the UK airspace are marked with <code>UK</code>.</p>
<ul>
<li>The software has identified:
<ul>
<li><code>entry</code>: waypoint <code>T</code></li>
<li><code>exit</code>: waypoint <code>W</code>
in the ADEXP waypoints.</li>
</ul>
</li>
<li>The software finds waypoint <code>T</code> in the ICAO flight plan.</li>
<li>The software <em>does not</em> find waypoint <code>W</code> in the ICAO flight plan.</li>
<li>The software therefore takes the next waypoint in the ADEXP list, so <code>B</code>, and
tries to find it too, and also does not find it.</li>
<li>So it does this again, taking waypoint <code>Q</code>, and it <em>does</em> find it, but at the
<em>start</em> of the ICAO flight plan, before the plane even enters the UK.</li>
</ul>
<blockquote>
<p>This search was successful as a duplicate identifier appeared in the flight
plan.</p>
</blockquote>
<p>What should the software have done? Well, <code>Q</code> is clearly <em>not</em> the waypoint we
are searching for, we are searching for waypoint <code>Y</code>, since <code>[T, O, P, Y]</code> is
the smallest segment of the ICAO plan that contains all the UK waypoints.</p>
<p>It's important to note here that the original algorithm is buggy; it is perfectly
possible to unambiguously extract the UK portion of this example flight plan;
see <a href="https://jameshaydon.github.io/nats-fail/#how-to-code-this-properly">below</a>. And this is likely the case for the
flight plan that caused the meltdown too.</p>
<blockquote>
<p>Having found an entry and exit point, with the latter being the duplicate and
therefore geographically incorrect, the software could not extract a valid UK
portion of flight plan between these two points. This is the root cause of the
incident. We can therefore rule out any cyber related contribution to this
incident.</p>
</blockquote>
<p>It sounds like the exception was raised in a later portion of the code, which
converts the plan to an internal format for <em>NAS</em>. This part failed because the
identified entry/exit waypoints didn't even specify a valid segment of the ICAO
route.</p>
<blockquote>
<p>Safety critical software systems are designed to always fail safely. This
means that in the event they cannot proceed in a demonstrably safe manner,
they will move into a state that requires manual intervention.</p>
</blockquote>
<p>We are left wondering if, had the misidentified waypoint been in a more
plausible geographic location, the code might not have thrown an exception and
passed along wrong data to ATCOs.</p>
<blockquote>
<p>In this case the software within the FPRSA-R subsystem was unable to establish
a reasonable course of action that would preserve safety and so raised a
critical exception. A critical exception is, broadly speaking, an exception of
last resort after exploring all other handling options. Critical exceptions
can be raised as a result of software logic or hardware faults, but
essentially mark the point at which the affected system cannot continue.</p>
</blockquote>
<p>It sounds like the software was written thinking this exception would never
occur.</p>
<blockquote>
<p>Clearly a better way to handle this specific logic error would be for FPRSA-R
to identify and remove the message and avoid a critical exception. However,
since flight data is safety critical information that is passed to ATCOs the
system must be sure it is correct and could not do so in this case. It
therefore stopped operating, avoiding any opportunity for incorrect data being
passed to a controller. The change to the software will now remove the need
for a critical exception to be raised in these specific circumstances.</p>
<p>Having raised a critical exception the FPRSA-R primary system wrote a log file
into the system log. It then correctly placed itself into maintenance mode and
the C&amp;M system identified that the primary system was no longer available. In
the event of a failure of a primary system the backup system is designed to
take over processing seamlessly. In this instance the backup system took over
processing flight plan messages. As is common in complex real-time systems the
backup system software is located on separate hardware with separate power and
data feeds.</p>
<p>Therefore, on taking over the duties of the primary server, the backup system
applied the same logic to the flight plan with the same result. It
subsequently raised its own critical exception, writing a log file into the
system log and placed itself into maintenance mode.</p>
<p>At this point with both the primary and backup FPRSA-R sub-systems having
failed safely the FPRSA-R was no longer able to automatically process flight
plans. It required restoration to normal service through manual intervention.
The entire process described above, from the point of receipt of the ADEXP
message to both the primary and backup sub-systems moving into maintenance
mode, took less than 20 seconds. 08:32 therefore marks the point at which the
automatic processing of flight plans ceased and the 4 hour buffer to manual
flight plan input commenced. The steps taken to restore the FPRSA-R sub-system
are described in section 5 of this report.</p>
</blockquote>
<p>Then support teams tried to fix things, but unfortunately it took longer than
the 4 hours they had:</p>
<blockquote>
<p>The 1st Line support team were alerted to the incident through the C&amp;M systems
that directly monitor operational systems as well as through direct feedback
from the Operational teams using the FPRSA-R sub-system at the time. The
initial response for the team followed standard recovery processes using the
centralised C&amp;M systems to restart the sub-system. Following multiple attempts
to restore the service, which were unsuccessful, the 2nd Line engineering team
was mobilised and supported the on-site engineers remotely via video link.</p>
</blockquote>
<p><img src="https://jameshaydon.github.io/nats-fail/off-and-on-again.jpg" alt="have you tried turning it off and on again?"></p>
<blockquote>
<p>The on-call teams working remotely with the on-site engineering teams followed
a staged analysis, involving increasingly detailed procedures to attempt to
resolve the issue, none of which were successful. As per standard escalation
procedures, 2nd Line engineers were engaged to provide further access to
advanced diagnostics and logging capabilities.</p>
</blockquote>
<p>It doesn't say how long it took, but the manufacturer of the <code>FPRSA-R</code> system was
eventually called:</p>
<blockquote>
<p>Additional support was then requested from the Technical Design team and
sub-system manufacturer as 1st and 2nd Line support had been unable to restore
the service or identify the precise root cause, which was unusual. The
manufacturer was able to offer further expertise including analysis of
lower-level software logs which led to identification of the likely flight
plan that had caused the software exception. Through understanding which
flight plan had caused the incident the manufacturer was able to provide the
precise sequence of actions necessary to recover the system in a controlled
and safe manner.</p>
</blockquote>
<p>The system was eventually restored, but unfortunately the knock-on effects by
that point were already disastrous.</p>
<p>The manufacturer is an Austrian company, <a href="https://en.wikipedia.org/wiki/Frequentis">Frequentis
AG</a>:</p>
<blockquote>
<p>An FPRSA sub-system has existed in NATS for many years and in 2018 the
previous FPRSA sub- system was replaced with new hardware and software
manufactured by Frequentis AG, one of the leading global ATC System providers.
The manufacturers ATC products are operating in approximately 150 countries
and they hold a world-leading position in aeronautical information management
(AIM) and message handling systems.</p>
</blockquote>
<p>The "Nobody ever gets fired for hiring Accenture" defence.</p>
<p>We can find a few job ads related to air traffic control systems at Frequentis
AG on their <a href="https://jobs.frequentis.com/careers/SearchJobs/air?listFilterMode=1">careers
page</a>
Programming languages used: <code>Ada</code>, <code>C++</code>, <code>Java</code>, <code>Python</code>, with <code>Java</code> being
the most common. The code above sounds like it could have been written in any of
these languages, but Ada would at least be safer than the others in other ways.</p>
<h2 id="thoughts">Thoughts</h2>
<p>Things that went wrong:</p>
<ol>
<li>The software that processes flight plans (<code>FPRSA-R</code>) was written in a buggy
way.</li>
<li>The software and system are not properly tested.</li>
<li>The <code>FPRSA-R</code> system has bad <a href="https://en.wikipedia.org/wiki/Failure_mode_and_effects_analysis" title="wikipedia">failure modes</a></li>
</ol>
<h3 id="the-software-was-buggy">The software was buggy</h3>
<p>The software was incapable of extracting the UK portion of the ICAO flight plan,
even though the flight plan was apparently valid (at least according to IFPS).</p>
<ul>
<li>
<p>The procedure was very fiddly and failed for a silly reason.</p>
</li>
<li>
<p>Waypoint markers are not globally unique, but this is a known issue, so NATS
should make sure their systems are robust enough to handle it. <em>All other air
traffic control authorities have to deal with this</em>. NATS says the following
about this in the report:</p>
<blockquote>
<p>Although there has been work by ICAO and other bodies to eradicate
non-unique waypoint names there are duplicates around the world. In order to
avoid confusion latest standards state that such identical designators
should be geographically widely spaced. In this specific event, both of the
waypoints were located outside of the UK, one towards the beginning of the
route and one towards the end; approximately 4000 nautical miles apart.</p>
</blockquote>
<p>When waypoints with the same name are widely spaced, this makes flight plans
unambiguous, because successive waypoints in a flight plan cannot be too far
apart. They also mention possible actions they will take:</p>
<blockquote>
<p>The feasibility of working through the UK state with ICAO to remove the
small number of duplicate waypoint names in the ICAO administered global
dataset that relate to this incident.</p>
</blockquote>
<p>Waypoint names are clearly chosen to be short and snappy. Here's a sequence
from some flight plan I found: <code>KOMAL</code>, <code>ATRAK</code>, <code>SORES</code>, <code>SAKTA</code>, <code>ALMIK</code>,
<code>IGORO</code>, <code>ATMED</code>, etc. It's clear that the system has been designed so these
names can be communicated quickly, e.g. over radio, and that pilots and
air traffic controllers can become familiar with those on the routes they
usually fly. Changing the name of a waypoint can be a scary operation.
Uniqueness is obviously desirable, but it has to be balanced against other
considerations. Including this suggestion in the initial report feels like
NATS is trying to shift the blame onto ICAO.</p>
<p>Furthermore, I don't see why a flight plan can't include the same <em>geographic</em>
waypoint several times; for example for leisure flights or military exercises.
Taking off and landing at the same airport is definitely a thing (called a
"round-robin flight plan"). It doesn't sound like the <code>FPRSA-R</code> algorithm
would be very robust to that.</p>
</li>
</ul>
<p>NATS officials are trying to spin this as:</p>
<blockquote>
<p>An air traffic meltdown in Britain was caused by a "one in 15 million" event,
the boss of traffic control provider NATS said, as initial findings showed how
a single flight plan with two identically labelled markers caused the chaos.</p>
<p>"This was a one in 15 million chance. We've processed 15 million flight plans
with this system up until this point and never seen this before," NATS CEO
Martin Rolfe told the BBC, as airlines stepped up calls for compensation for
the breakdown. <a href="https://www.reuters.com/world/uk/uk-aviation-regulator-review-air-traffic-control-failure-2023-09-06/">Reuters</a></p>
</blockquote>
<p>The system was put in place in 2018, so what Martin Rolfe is saying here is that
this sort of thing only had a chance of occurring "once every 5 years", which is
apparently an acceptable frequency for having a complete air traffic control
meltdown.</p>
<h3 id="the-system-was-poorly-tested">The system was poorly tested</h3>
<p><a href="https://en.wikipedia.org/wiki/Fuzzing">fuzzing</a>, for example, may have
prevented this. By bombarding such a system with randomly generated flight
plans, you can see if any of them cause bad failure modes: a crashed system
where one doesn't know immediately what went wrong. By inspecting which sorts of
flight plans cause problems, it would become apparent that those with duplicate
waypoint identifiers in the ADEXP portion cannot be processed properly.</p>
<h3 id="the-fprsa-r-system-has-bad-failure-modes">The <code>FPRSA-R</code> system has bad failure modes</h3>
<p>All systems can malfunction, so the important thing is that they malfunction <em>in
a good way</em> and that those responsible are <em>prepared</em> for malfunctions.</p>
<p>A single flight plan caused a problem, and the entire <code>FPRSA-R</code> system crashed,
which means no flight plans are being processed at all. If there is a problem
with a single flight plan, it should be moved to a separate slower queue, for
manual processing by humans. NATS acknowledges this in their "actions already
undertaken or in progress":</p>
<blockquote>
<p>The addition of specific message filters into the data flow between IFPS and
FPRSA-R to filter out any flight plans that fit the conditions that caused the
incident.</p>
</blockquote>
<p>When <code>FPRSA-R</code> it did crash, it did so in an obscure way. This is a system
which <em>processes flight plans</em>, yet the relevant flight plan was only found in
"lower-level software logs". If there is an error processing a flight plan,
which brings down the whole system, a notification (including the flight plan)
should immediately be sent to some monitoring team.</p>
<p>NATS was also not prepared for an <code>FPRSA-R</code> system failure. The 1st
and 2nd Line support engineers were not able to locate, or did not think to
check, the low-level log files. This has been fixed:</p>
<blockquote>
<p>An operating instruction has been put in place to allow prompt recovery of the
FPRSA-R sub-system if the same circumstances recur. Each of the technical
operators have been trained to implement the new process. With enhanced
monitoring in place, additional engineering expertise will also be present to
oversee the activity.</p>
</blockquote>
<h3 id="possible-lack-of-formal-verification">Possible lack of formal verification</h3>
<p>As reddit user <code>DontWannaMissAFling</code> <a href="https://www.reddit.com/r/programming/comments/16fhmuq/comment/k02o6n8/?utm_source=share&amp;utm_medium=web2x&amp;context=3">points out</a>:</p>
<blockquote>
<p>But what's wild to me is that something as safety critical as air traffic
control apparently isn't using proven techniques like formal verification,
model checking to eliminate these classes of bugs entirely.</p>
<p>Like as an industry we use TLA+ to stop AWS from having downtime or Xboxes
segfaulting, but not to keep planes in the air?</p>
</blockquote>
<p>I agree that it certainly doesn't sound like any formal verification was used in
this case (for this system), and the report doesn't mention anything. Using
formal verification would certainly have helped here, I might explore this in
subsequent posts.</p>
<p>But it's possible formal verification was used, but faulty code still made its
way into production: end-2-end formal verification for large systems is still in
its infancy. We'll have to wait for the result of the enquiry to know more.</p>
<h2 id="humans-were-kept-safe-at-all-times">Humans were kept safe at all times</h2>
<p>I'd like to note (as does NATS in the report) that despite all the problems
highlighted above, planes in the air over the UK were still safe at all times.
They were being monitored by experienced ATCOs, which monitor planes by their
known flight plan, radio, radar and vision. The consequence of all this was not
that any human lives were put in danger, it's simply that far fewer flights
could take off in the first place, or had to be diverted away from UK airspace.
NATS did the right thing (reducing the number of flights), and kept everybody
safe.</p>
<h2 id="how-to-code-this-properly">How to code this properly</h2>
<p>So, how can we avoid this bug?</p>
<p>Let's recap the problem. There are two sequences of waypoints:</p>
<ul>
<li><code>ADEXP</code>: the full list of waypoints.</li>
<li><code>ICAO</code>: a subsequence of the ADEXP waypoints.</li>
</ul>
<p>Because the ICAO plan doesn't need to include the waypoints at which it
enters/exits an air traffic control region, extracting the segment of the ICAO
flight plan corresponding to the UK portion of the flight is not entirely
trivial. Of course, if we take the entire ICAO flight plan, it already contains
the UK portion, but what we really want is the <em>smallest</em> such segment. It's
interesting to note here that a flight could possibly enter UK airspace, and
then exit it again, and enter it again. We'll ignore this, that is, we will just
find a single contiguous segment that contains all UK portions of the flight,
since this is what the original code seemed to do.</p>
<p>I'm unsure why this task attempts to do this only using the ADEXP data, rather
than consulting a database about how waypoints and flight segments intersect UK
airspace. It seems strange, but let's move on.</p>
<p>Note that it is impossible to achieve this task with the ICAO flight plan alone
(and no knowledge of routes), even if you know for each waypoint if it is in the
UK or not. Indeed you could even be in a situation where <em>none</em> of the waypoints
in the ICAO route are in the UK, for example when the flight plan clips a small
portion of the UK between two of the ICAO waypoints.</p>
<p>So this is why the ADEXP waypoint list is used, and the assumption here, I
assume, is that the ADEXP list contains <em>all</em> the waypoints, and that
furthermore, waypoint granularity is such that if <em>adjacent</em> waypoints both
don't intersect UK airspace, then the segment between them doesn't either.</p>
<p>The mistake of the faulty algorithm described above is to try to work on both
the ICAO data and the ADEXP data as they are, maintaining pointers into each of
them, updating them with vague and wrong invariants in the background of the
programmer's mind. This is a recipe for bugs. Instead, the first thing to do is
to reconcile the data and then carefully extract the UK portion from that.</p>
<p>So we create a data structure for a plan:</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>-- A flight plan, with segments between points 'p' via routes 'r'.
</span><span>data </span><span>Plan</span><span> p r
</span><span>  = </span><span>End</span><span> p
</span><span>  | </span><span>Leg</span><span> p r (</span><span>Plan</span><span> p r)
</span></code></pre>
<p>(This is <a href="https://www.haskell.org/">Haskell</a> code, but the ideas apply to most languages.)</p>
<p>This says that a <code>Plan p r</code> has either arrived at its destination <code>End p</code>, or
consists of a segment starting from <code>p</code>, via <code>r</code>, and the <code>rest</code> of the plan:
<code>Leg p r rest</code>.</p>
<p>We can now define all the sorts of flight plan data we will deal with:</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>type </span><span>ICAO</span><span>     p r = </span><span>Plan</span><span> p r         </span><span>-- points and routes, no intermediate waypoints
</span><span>type </span><span>ADEXP</span><span>    p   = </span><span>Plan</span><span> p [p]       </span><span>-- points and intermediate waypoints, no route data
</span><span>type </span><span>Combined</span><span> p r = </span><span>Plan</span><span> p (</span><span>Via</span><span> p r) </span><span>-- all the data combined
</span><span>
</span><span>data </span><span>Via</span><span> p r = </span><span>Via
</span><span>  { route   :: r,
</span><span>    </span><span>through </span><span>::</span><span> [</span><span>p</span><span>]
</span><span>  }
</span><span>  </span><span>deriving</span><span> stock (</span><span>Show</span><span>)
</span></code></pre>
<p>Here <code>Combined</code> is our reconciled flight plan, it combined all the information
form ICAO and ADEXP. We can project a <code>Plan</code> back down to <code>ICAO</code> or <code>ADEXP</code>:</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>projectICAO </span><span>:: Combined </span><span>p r </span><span>-&gt; ICAO </span><span>p r
</span><span>projectICAO = mapRoutes (.route)
</span><span>
</span><span>projectADEXP </span><span>:: Combined </span><span>p r </span><span>-&gt; ADEXP </span><span>p
</span><span>projectADEXP = mapRoutes (.through)
</span><span>
</span><span>mapRoutes </span><span>::</span><span> (</span><span>r </span><span>-&gt; </span><span>r</span><span>') </span><span>-&gt; Plan </span><span>p r </span><span>-&gt; Plan </span><span>p r</span><span>'
</span><span>mapRoutes _ (</span><span>End</span><span> p) = </span><span>End</span><span> p
</span><span>mapRoutes f (</span><span>Leg</span><span> p r rest) = </span><span>Leg</span><span> p (f r) (mapRoutes f rest)
</span></code></pre>
<p>We'll assume we have already parsed the data into the data structures above.
This is just a matter of reading the spec carefully and turning it into code,
and hopefully something the <code>FPRSA-R</code> did correctly, though as noted previously
it might be working on the text version directly.</p>
<p>Now we write our reconciliation function. For ICAO and ADEXP to reconcile, the
start and end points must match. When reconciling a leg of a flight plan, a
certain amount of waypoints can be skipped in the ICAO plan, and the rest of
them must reconcile with the rest of the flight plan:</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>reconcile </span><span>::</span><span> (</span><span>Eq </span><span>p</span><span>) </span><span>=&gt; ICAO </span><span>p r </span><span>-&gt;</span><span> [</span><span>p</span><span>] </span><span>-&gt;</span><span> [</span><span>Combined </span><span>p r</span><span>]
</span><span>reconcile (</span><span>End</span><span> p) [p']             | p == p' = pure (</span><span>End</span><span> p)
</span><span>reconcile (</span><span>Leg</span><span> p r rest) (p' : ps) | p == p' = </span><span>do
</span><span>  (through, restAdexp) &lt;- splits ps
</span><span>  recoRest &lt;- reconcile rest restAdexp
</span><span>  pure (</span><span>Leg</span><span> p </span><span>Via</span><span> {route = r, through} recoRest)
</span><span>reconcile _ _ = </span><span>[]
</span><span>
</span><span>-- | All the ways to snap a list in two.
</span><span>splits </span><span>::</span><span> [</span><span>a</span><span>] </span><span>-&gt;</span><span> [([</span><span>a</span><span>], [</span><span>a</span><span>])]
</span><span>splits </span><span>[] </span><span>= [(</span><span>[]</span><span>, </span><span>[]</span><span>)]
</span><span>splits xs@(x : rest) = (</span><span>[]</span><span>, xs) : (first (x :) &lt;$&gt; splits rest)
</span></code></pre>
<p>Note that the function produces <em>all</em> the possible reconciliations. This is
because reconciliations are not necessarily unique because waypoints can appear
more than once. By calculating all the possible reconciliations, we'll know if
the data is ambiguous, and flag those flight plans for manual processing.</p>
<p>Next, we extract the UK portion of the flight plan. This is done in 3 steps:</p>
<ol>
<li>Remove all legs at the start which don't cross into UK airspace.</li>
<li>Traverse the legs which are in UK airspace.</li>
<li>Once the rest of the flight plan is never again in the UK, cut it short.</li>
</ol>
<p>Each function calls the next step in sequence. Note that we return a
<code>NonUkPlan</code> error when the system reaches the end of the plan without having
found a UK part. By having a compiler which checks that pattern-matches are
covering, the possible failures arise naturally while coding.</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>-- Extract the UK part of the flight.
</span><span>ukSegment </span><span>::</span><span> (</span><span>p </span><span>-&gt; Bool</span><span>) </span><span>-&gt; Combined </span><span>p r </span><span>-&gt; Either Err</span><span> (</span><span>Combined </span><span>p r</span><span>)
</span><span>ukSegment uk (</span><span>End</span><span> p)
</span><span>  | nonUkPlan uk (</span><span>End</span><span> p) = </span><span>Left NonUkPlan
</span><span>  | otherwise = pure (</span><span>End</span><span> p)
</span><span>ukSegment uk plan@(</span><span>Leg</span><span> _ _ rest) =
</span><span>  </span><span>if</span><span> nonUkLeg uk plan
</span><span>    </span><span>then</span><span> ukSegment uk rest
</span><span>    </span><span>else</span><span> pure (flyUK uk plan)
</span><span>
</span><span>-- Fly the UK part of the flight.
</span><span>flyUK </span><span>::</span><span> (</span><span>p </span><span>-&gt; Bool</span><span>) </span><span>-&gt; Combined </span><span>p r </span><span>-&gt; Combined </span><span>p r
</span><span>flyUK _ (</span><span>End</span><span> end) = </span><span>End</span><span> end
</span><span>flyUK uk (</span><span>Leg</span><span> p v rest)
</span><span>  | nonUkPlan uk rest = </span><span>Leg</span><span> p v (afterUK rest)
</span><span>  | otherwise = </span><span>Leg</span><span> p v (flyUK uk rest)
</span><span>
</span><span>-- Skip the rest of the flight.
</span><span>afterUK </span><span>:: Combined </span><span>p r </span><span>-&gt; Combined </span><span>p r
</span><span>afterUK plan = </span><span>End</span><span> (start plan)
</span></code></pre>
<p>These use some small functions:</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>-- The next leg of the journey doesn't fly through the UK.
</span><span>nonUkLeg </span><span>::</span><span> (</span><span>p </span><span>-&gt; Bool</span><span>) </span><span>-&gt; Combined </span><span>p r </span><span>-&gt; Bool
</span><span>nonUkLeg uk (</span><span>End</span><span> p) = not (uk p)
</span><span>nonUkLeg uk (</span><span>Leg</span><span> p v _) = not (uk p) &amp;&amp; not (any uk v.through)
</span><span>
</span><span>-- The whole plan isn't in the UK.
</span><span>nonUkPlan </span><span>::</span><span> (</span><span>a </span><span>-&gt; Bool</span><span>) </span><span>-&gt; Combined </span><span>a r </span><span>-&gt; Bool
</span><span>nonUkPlan uk plan = all (nonUkLeg uk) (legs plan)
</span><span>
</span><span>legs </span><span>:: Plan </span><span>p r </span><span>-&gt;</span><span> [</span><span>Plan </span><span>p r</span><span>]
</span><span>legs (</span><span>End</span><span> p) = [</span><span>End</span><span> p]
</span><span>legs plan@(</span><span>Leg</span><span> _ _ rest) = plan : legs rest
</span><span>
</span><span>start </span><span>:: Plan </span><span>p r </span><span>-&gt; </span><span>p
</span><span>start (</span><span>End</span><span> p) = p
</span><span>start (</span><span>Leg</span><span> p _ _) = p
</span></code></pre>
<p>Putting it all together, we get:</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>ukPartOfICAO </span><span>::</span><span> (</span><span>Eq </span><span>p</span><span>) </span><span>=&gt;</span><span> (</span><span>p </span><span>-&gt; Bool</span><span>) </span><span>-&gt; ICAO </span><span>p r </span><span>-&gt;</span><span> [</span><span>p</span><span>] </span><span>-&gt; Either Err</span><span> (</span><span>ICAO </span><span>p r</span><span>)
</span><span>ukPartOfICAO uk icao adexp = </span><span>case</span><span> reconcile icao adexp </span><span>of
</span><span>  [plan] -&gt; projectICAO &lt;$&gt; ukSegment uk plan
</span><span>  </span><span>[]     </span><span>-&gt; </span><span>Left CannotReconcileIcaoAdexp
</span><span>  _      -&gt; </span><span>Left AmbiguousReconciliationsOfIcaoAdexp
</span></code></pre>
<p>We collected the following errors while coding:</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>data </span><span>Err
</span><span>  = </span><span>NonUkPlan
</span><span>  | </span><span>CannotReconcileIcaoAdexp
</span><span>  | </span><span>AmbiguousReconciliationsOfIcaoAdexp
</span></code></pre>
<p>Let's test it with our example:</p>
<pre><code><span>           4       2        8         5              1           9
</span><span>ICAO:  F------Q--------T--------O-----------P---------------Y--------U
</span><span>
</span><span>ADEXP: F   S  Q    C   T   A    O  E  X     P   W   B   Q   Y        U
</span><span>                       UK  UK   UK UK UK    UK  UK
</span></code></pre>
<pre data-lang="haskell"><code data-lang="haskell"><span>inUK = (</span><span>`</span><span>elem</span><span>`</span><span> ["</span><span>T</span><span>", "</span><span>A</span><span>", "</span><span>O</span><span>", "</span><span>E</span><span>", "</span><span>X</span><span>", "</span><span>P</span><span>", "</span><span>W</span><span>"])
</span><span>icao = ("</span><span>F</span><span>", </span><span>4</span><span>) ~&gt; ("</span><span>Q</span><span>", </span><span>2</span><span>) ~&gt; ("</span><span>T</span><span>", </span><span>8</span><span>) ~&gt; ("</span><span>O</span><span>", </span><span>5</span><span>) ~&gt; ("</span><span>P</span><span>", </span><span>1</span><span>) ~&gt; ("</span><span>Y</span><span>", </span><span>9</span><span>) ~&gt; </span><span>End </span><span>"</span><span>U</span><span>"
</span><span>adexp = ["</span><span>F</span><span>", "</span><span>S</span><span>", "</span><span>Q</span><span>", "</span><span>C</span><span>", "</span><span>T</span><span>", "</span><span>A</span><span>", "</span><span>O</span><span>", "</span><span>E</span><span>", "</span><span>X</span><span>", "</span><span>P</span><span>", "</span><span>W</span><span>", "</span><span>B</span><span>", "</span><span>Q</span><span>", "</span><span>Y</span><span>", "</span><span>U</span><span>"]
</span><span>
</span><span>infixr </span><span>6 </span><span>~&gt;
</span><span>(~&gt;) (p, r) = </span><span>Leg</span><span> p r
</span></code></pre>
<p>And try this at the REPL:</p>
<pre><code><span>&gt; ukPortionOfICAO inUK icao adexp
</span><span>Right (Leg "T" 8 (Leg "O" 5 (Leg "P" 1 (End "Y"))))
</span></code></pre>
<p>We can see that this is the correct result:</p>
<pre><code><span>                                 UK portion of ICAO
</span><span>                       
</span><span>           4       2        8         5              1           9
</span><span>ICAO:  F------Q--------T--------O-----------P---------------Y--------U
</span><span>
</span><span>ADEXP: F   S  Q    C   T   A    O  E  X     P   W   B   Q   Y        U
</span><span>                       UK  UK   UK UK UK    UK  UK
</span></code></pre>
<p>The waypoint <code>Q</code> is a duplicate in the ADEXP list, but the system still returns
the correct portion of the ICAO flight path. Crisis averted! The fact that there
is a duplicate identifier in this case is immaterial, the ICAO and ADEXP data
still reconcile unambiguously, and the correct sub-route is well-defined.</p>
<p>How large can flight plans get? Well here is a flight plan from London to Sydney
that contains a total of 158 waypoints, and about a third of them appear in the
ICAO route:
<img src="https://jameshaydon.github.io/nats-fail/london-sydney.png" alt="London to Sydney flight plan"> 
<code>ukPortionOfICAO</code> returns practically instantly for such a flight plan.</p>
<p>Comments on <a href="https://www.reddit.com/r/programming/comments/16fhmuq/a_deep_dive_into_the_bug_that_caused_the_uk_air/?utm_source=share&amp;utm_medium=web2x&amp;context=3">reddit</a></p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Webb Discovers Methane, Carbon Dioxide in Atmosphere of K2-18B (210 pts)]]></title>
            <link>https://www.nasa.gov/goddard/2023/webb-discovers-methane-carbon-dioxide-in-atmosphere-of-k2-18b/</link>
            <guid>37468342</guid>
            <pubDate>Mon, 11 Sep 2023 15:04:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nasa.gov/goddard/2023/webb-discovers-methane-carbon-dioxide-in-atmosphere-of-k2-18b/">https://www.nasa.gov/goddard/2023/webb-discovers-methane-carbon-dioxide-in-atmosphere-of-k2-18b/</a>, See on <a href="https://news.ycombinator.com/item?id=37468342">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Develop with Cocoa for Apple devices without using Objective-C (124 pts)]]></title>
            <link>https://felixk15.github.io/posts/c_ocoa/</link>
            <guid>37468031</guid>
            <pubDate>Mon, 11 Sep 2023 14:44:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://felixk15.github.io/posts/c_ocoa/">https://felixk15.github.io/posts/c_ocoa/</a>, See on <a href="https://news.ycombinator.com/item?id=37468031">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h2 id="summary">
<span>Summary</span><a href="#summary"><i></i></a>
</h2>
<p>In this post Ill go into detail about how, during my contracting work for <a href="https://www.shimmerindustries.com/" target="_blank" rel="noopener noreferrer">Shimmer Industries</a> (a company founded by <a href="https://twitter.com/EskilSteenberg" target="_blank" rel="noopener noreferrer">Eskil Steenberg</a> which is focused on developing a real-time lighting designing software), I worked on a piece of software which enabled us to use the MacOS and iOS APIs using a custom C API without having to use ObjC.</p>
<p>This technology (which we decided to call <strong>c-ocoa</strong>) was used to implement support for OSX &amp; iOS for Eskils platform abstraction library <a href="https://gamepipeline.org/betray.html" target="_blank" rel="noopener noreferrer">betray</a>. Betray is whats driving the tools of Shimmer Industries. This allowed us to easily ship existing applications to mobile with only minimal code changes. <a href="https://felixk15.github.io/assets/img/posts/c_ocoa/betray_abstraction.png"><img data-src="/assets/img/posts/c_ocoa/betray_abstraction.png" alt="Betray abstraction layers" data-proofer-ignore="" src="https://felixk15.github.io/assets/img/posts/c_ocoa/betray_abstraction.png"></a> <em>Abstraction layers of betray using c_ocoa for calling native APIs</em></p>
<p>Eskil was generous enough to allow me to write about how this problem was solved. Additionally, we decided to make the <a href="#sourcecode">final project completely open-source!</a>.</p>
<p>The result of the endevaour is effectively a C-API which you can use to interact with the Cocoa API. Since there are C bindings for virtually all programming languages out there, you could even use the generated API with</p>
<ul>
<li><a href="https://www.lua.org/pil/26.html" target="_blank" rel="noopener noreferrer">Lua</a></li>
<li><a href="https://docs.python.org/3/extending/extending.html" target="_blank" rel="noopener noreferrer">Python</a></li>
<li><a href="https://blog.appsignal.com/2018/10/30/ruby-magic-building-a-ruby-c-extension-from-scratch.html" target="_blank" rel="noopener noreferrer">Ruby</a></li>
<li>or even <a href="https://medium.com/jspoint/a-simple-guide-to-load-c-c-code-into-node-js-javascript-applications-3fcccf54fd32" target="_blank" rel="noopener noreferrer">Javascript</a>
</li>
</ul>
<p>Yes, thats right. Cocoa from the web, yall!</p>
<p>In the end we have an application that shares 95% of its code between platforms and has an identical look and feel. <a href="https://felixk15.github.io/assets/img/posts/c_ocoa/zenith_on_devices.png"><img data-src="/assets/img/posts/c_ocoa/zenith_on_devices.png" alt="Zenith on different devices" data-proofer-ignore="" src="https://felixk15.github.io/assets/img/posts/c_ocoa/zenith_on_devices.png"></a> <em>Application running on Android, Windows 11 and iOS</em></p>
<p>Since this is a code generator, another nice bonus is that you can immediately re-generate the C-API when a new ObjC API becomes available. Theres no waiting for the maintainer of a language bindings library to update the code, just re-run the generator and voila, you have the latest API.</p>
<p>Watched the latest WWDC where a new library got presented and want to try it out in your C-only program? Just re-generate and start tinkering!</p>
<h2 id="why-would-you-do-this-just-use-objective-c11">
<span>Why would you do this? Just use Objective-C!!!11</span><a href="#why-would-you-do-this-just-use-objective-c11"><i></i></a>
</h2>
<p>Let me start this post by saying that yes, we couldve used ObjC to be able to use the MacOS and iOS APIs from within the C codebase that was already in place. We decided against it though, because we wanted to have a pure C codebase without any of the weird ObjC code in there. We didnt want maintainers (who are all mostly familiar with C) to first learn how to parse ObjC and also thought that this would be a cool thing that might also interesting other developers. It was certainly an interesting experience for me since outside some courses in University, Ive never touched an OSX or iOS development and was also unfamiliar with ObjC and XCode as an IDE.</p>
<h2 id="introducing-the-objective-c-runtime">
<span>Introducing the Objective-C runtime</span><a href="#introducing-the-objective-c-runtime"><i></i></a>
</h2>
<p>Eskil actually gave me the first hint by pointing me towards the ObjC runtime.</p>
<p>Quoting the official documentation it states that <a href="https://developer.apple.com/documentation/objectivec/objective-c_runtime" target="_blank" rel="noopener noreferrer">The Objective-C runtime is a runtime library that provides support for the dynamic properties of the Objective-C language</a>. This sounds interesting, but slighty vague, so after taking a closer look at the documentation and the API itself, to see if theres stuff in there that could be of use for solving this problem, I was delighted to see that theres stuff like give me a list of all classes and methods and, most importantly call the function with a given name on this object. Bingo, this is exactly what I was looking for and seems to be a good foundation to build a C API on. Even better: The ObjC runtime itself is even a pure C API!</p>
<h2 id="the-plan">
<span>The plan</span><a href="#the-plan"><i></i></a>
</h2>
<p>The overall plan is to have a C API which, under the hood, uses the ObjC runtime to call functions that are normally only accessible when programming in ObjC.</p>
<p>In praxis this would mean that ObjC code like this:</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
</pre></td>
<td><pre><span>char</span><span>*</span> <span>getClipboardString</span><span>()</span>
<span>{</span>
  <span>UIPasteboard</span> <span>pasteBoard</span> <span>=</span> <span>[</span><span>UIPasteboard</span> <span>generalPasteboard</span><span>];</span>
  <span>NSString</span> <span>pasteBoardContent</span> <span>=</span> <span>[</span><span>pasteBoard</span> <span>string</span><span>];</span>

  <span>NSUInteger</span> <span>length</span> <span>=</span> <span>[</span><span>pasteBoardContent</span> <span>lengthOfBytesUsingEncoding</span><span>:</span><span>NSUTF8StringEncoding</span><span>];</span>
  <span>char</span><span>*</span> <span>pString</span> <span>=</span> <span>(</span><span>char</span><span>*</span><span>)</span><span>malloc</span><span>(</span><span>length</span><span>);</span>

  <span>[</span><span>pasteBoardContent</span> <span>getCString</span><span>:</span><span>pString</span> <span>maxLength</span><span>:</span><span>length</span> <span>encoding</span><span>:</span><span>NSUTF8StringEncoding</span><span>];</span>
  <span>return</span> <span>pString</span><span>;</span>
<span>}</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<p>could be written like this in C:</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
</pre></td>
<td><pre><span>char</span><span>*</span> <span>getClipboardString</span><span>()</span>
<span>{</span>
  <span>uipasteboard_t</span> <span>pasteBoard</span> <span>=</span> <span>uipasteboard_generalPasteboard</span><span>();</span>
  <span>nsstring_t</span> <span>pasteBoardContent</span> <span>=</span> <span>uipasteboard_string</span><span>(</span> <span>pasteBoard</span> <span>);</span>

  <span>unsigned</span> <span>long</span> <span>length</span> <span>=</span> <span>nsstring_lengthOfBytesUsingEncoding</span><span>(</span> <span>pasteBoardContent</span><span>,</span> <span>NSUTF8StringEncoding</span> <span>);</span>
  <span>char</span> <span>*</span><span>cStr</span> <span>=</span> <span>(</span><span>char</span> <span>*</span><span>)</span><span>malloc</span><span>(</span><span>sizeof</span><span>(</span><span>char</span><span>)</span> <span>*</span> <span>(</span><span>uint</span><span>)</span><span>length</span><span>);</span>
  <span>nsstring_getCString</span><span>(</span><span>pasteBoardContent</span><span>,</span> <span>cStr</span><span>,</span> <span>length</span><span>,</span> <span>NSUTF8StringEncoding</span><span>);</span>

  <span>return</span> <span>cStr</span><span>;</span>
<span>}</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<p>It was clear pretty early on that wed need some kind of external code-generation tool that would use the ObjC runtime to parse the ObjC APIs and use the parsed information to generate the C API.</p>
<h2 id="the-code-generator">
<span>The code generator</span><a href="#the-code-generator"><i></i></a>
</h2>
<p>Lets get right into it and talk about the code generator. From a birds-eye-view the code generator is a command-line tool (written in C) which uses the ObjC runtime API to query various data from ObjC APIs and uses this queried data to generate multiple .c/.h files which can be used in a project to interface with these ObjC APIs using normal C functions.</p>
<p>For each ObjC class that you want to create a C API for, the tool basically does these things in order: <a href="https://felixk15.github.io/assets/img/posts/c_ocoa/code_generator_workflow.png"><img data-src="/assets/img/posts/c_ocoa/code_generator_workflow.png" alt="Code Generator Workflow" data-proofer-ignore="" src="https://felixk15.github.io/assets/img/posts/c_ocoa/code_generator_workflow.png"></a> <em>Code Generator Workflow</em></p>
<h3 id="query-objc-class">
<span>Query ObjC Class</span><a href="#query-objc-class"><i></i></a>
</h3>
<p>The first step is to query metadata about the class that were interested in turning into a C API. The code generator works by providing it with one or more ObjC class name(s) (optionally including wildcards). For each class the generator generates a pair of .c/.h files.</p>
<p>This is done by first querying for <em>all</em> (see notes about what <em>all</em> actually means) available classes using <a href="https://developer.apple.com/documentation/objectivec/1418579-objc_getclasslist?language=objc" target="_blank" rel="noopener noreferrer"><code>objc_getClassList()</code></a>. The query results will be returned using an opaque type <code>Class</code> which can be used together with <code>class_</code> prefixed functions to query more metadata about a specific class. So the generator first caches all available classes (Ill later talk about how classes are made visible for the ObjC runtime API) and then compares the name of each class (by using <a href="https://developer.apple.com/documentation/objectivec/1418635-class_getname?language=objc" target="_blank" rel="noopener noreferrer"><code>class_getName()</code></a>) against the input of the user.</p>
<p>Once one ore more matching classes have been found, we move to the next step.</p>
<blockquote><p><em>Note</em>: <em>All</em> available classes depends on what ObjC frameworks are linked at the time the code generator is run. This also means that you cant generate code for iOS only classes when running the code generator on OSX since you cant link the iOS frameworks to the OSX app. For generating code for iOS classes, you have to run the code generator on an iOS simulator.</p></blockquote>
<h3 id="flattening-objc-class-hierarchy">
<span>Flattening ObjC Class Hierarchy</span><a href="#flattening-objc-class-hierarchy"><i></i></a>
</h3>
<p>ObjC is an object oriented language, C is not. So we need some way to map ObjCs object orientness to C functions.</p>
<p>For this problem I decided to flatten the class hierarchy of the class that you want to export to C. This is done by using the <a href="https://developer.apple.com/documentation/objectivec/1418498-class_getsuperclass?language=objc" target="_blank" rel="noopener noreferrer"><code>class_getSuperClass()</code></a> function from the ObjC Runtime API - this also returns an object of type <code>Class</code> - same as <code>objc_getClassList()</code> before.</p>
<p>For each class found during the Query ObjC Class step, the program needs to recursively move up the class hierarchy, This is an image illustrating the class hierarchy for the class <a href="https://developer.apple.com/documentation/foundation/nsmutablestring?language=objc" target="_blank" rel="noopener noreferrer"><code>NSMutableString</code></a>: <a href="https://felixk15.github.io/assets/img/posts/c_ocoa/class_hierarchy.png"><img data-src="/assets/img/posts/c_ocoa/class_hierarchy.png" alt="Class Hierarchy of NSMutableString" data-proofer-ignore="" src="https://felixk15.github.io/assets/img/posts/c_ocoa/class_hierarchy.png"></a> <em>Class Hierarchy of NSMutableString (The arrow point upwards the class hierarchy)</em></p>
<p>Unlike C++, ObjC does <em>not</em> support multiple inheritance (thank god!), so this process is rather straight-forward.</p>
<p>By using <code>class_getSuperClass()</code>, we can recursively go up the class hierarchy and query the methods of each individual class. Querying the methods of a class is done using the <a href="https://developer.apple.com/documentation/objectivec/1418490-class_copymethodlist?language=objc" target="_blank" rel="noopener noreferrer"><code>class_copyMethodList()</code></a> API - this returns an array of the opaque type <code>Method</code> which can be used to query metadata about a specific method.</p>
<p>The result of this step is an array of methods from the complete class hierarchy. Spoiler: Since well later call the methods using just their name, we dont have to worry about methods that have been overriden at this point.</p>
<blockquote><p><em>Note</em>: This works for only for <em>non-static</em> methods. To also get the <em>static</em> methods of a class you have to get whats called the <code>meta-class</code>. This is done by using the <a href="https://developer.apple.com/documentation/objectivec/1418629-object_getclass?language=objc" target="_blank" rel="noopener noreferrer"><code>object_getClass()</code></a> function with a <code>Class</code> object as its argument. This returns another object of type <code>Class</code>. This object can be used together with <code>class_copyMethodList()</code> to get the list of static methods.</p></blockquote>
<p>The result of this step are 2 arrays:</p>
<ol>
<li>Array of all <em>static</em> methods</li>
<li>Array of all <em>non-static</em> methods</li>
</ol>
<h3 id="resolving-objc-runtime-types">
<span>Resolving ObjC Runtime Types</span><a href="#resolving-objc-runtime-types"><i></i></a>
</h3>
<p>Before talking about how the metadata of methods is queried, I have to talk about how types are encoded in the ObjC runtime.</p>
<p>When querying argument and/or return types using the ObjC runtime you only get back a string. Naively I though that this is just the name of the return type (eg: <code>int</code>, <code>float</code> or <code>NSString</code>) but things are a bit more complicated than that. Basically we have to differentiate between base types (<code>int</code>, <code>char</code>, <code>float</code> etc), user defined POD(plain-old-data) types (your typical structs), references (pointers) and ids (more on that later). For each of these types, the type name from the ObjC runtime has to be parsed differently.</p>
<h4 id="base-types">
<span>Base Types</span><a href="#base-types"><i></i></a>
</h4>
<p>Lets start with the most simple - base types. Instead of fully qualified type names (like <code>int</code>), the ObjC runtime returns a string with 1 element. Fortunately, theres a 1:1 mapping between these ObjC base types and C base types. The mapping listed below is from the <a href="https://opensource.apple.com/source/objc4/objc4-709/runtime/runtime.h.auto.html" target="_blank" rel="noopener noreferrer"><code>objc/runtime.h</code></a> file</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td>
<td><pre><span>#define _C_CHR      'c'
#define _C_UCHR     'C'
#define _C_SHT      's'
#define _C_USHT     'S'
#define _C_INT      'i'
#define _C_UINT     'I'
#define _C_LNG      'l'
#define _C_ULNG     'L'
#define _C_LNG_LNG  'q'
#define _C_ULNG_LNG 'Q'
#define _C_FLT      'f'
#define _C_DBL      'd'
#define _C_BFLD     'b'
#define _C_BOOL     'B'
#define _C_VOID     'v'
</span></pre></td>
</tr></tbody></table></code></p>
</div>
<h4 id="user-defined-structs">
<span>User defined structs</span><a href="#user-defined-structs"><i></i></a>
</h4>
<p>The next type, user defined POD types (struct XY) are a bit more complex. The ObjC runtime returns the type name and the complete layout of the custom type. To drive home what I mean by this, think of a struct like this:</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
</pre></td>
<td><pre><span>struct</span> <span>AwesomeType</span>
<span>{</span>
  <span>char</span> <span>a</span><span>;</span>
  <span>int</span> <span>b</span><span>;</span>
  <span>float</span> <span>c</span><span>;</span>
<span>};</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<p>This type would be encoded by the ObjC runtime like this: <code>{AwesomeType=cif}</code> With the info on how base types are encoded, the part after the equal sign can be parsed as a list of base types. So this is basically <code>AwesomeType = char int float</code>.</p>
<p>This extends to structs within structs as well. So this struct:</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
12
</pre></td>
<td><pre><span>struct</span> <span>Foo</span>
<span>{</span>
  <span>int</span> <span>a</span><span>;</span>
  <span>unsigned</span> <span>int</span> <span>b</span><span>;</span>
  <span>float</span> <span>c</span><span>;</span>
<span>};</span>

<span>struct</span> <span>Boo</span>
<span>{</span>
  <span>double</span> <span>a</span><span>;</span>
  <span>Foo</span> <span>foo</span><span>;</span>
<span>};</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<p>Would result in this type name for <code>struct Boo</code>: <code>{Boo=d{Foo=iIf}}</code>.</p>
<blockquote><p><em>Note</em>: The first version of the code generator actually re-created all structs that where used as return and argument types but as you might already have guessed, you loose the member name of each struct member. The current version assumes that you use the actual ObjC struct (which are the same as C structs).</p></blockquote>
<h4 id="references">
<span>References</span><a href="#references"><i></i></a>
</h4>
<p>Following this, we have references which are marked with a <code>^</code> preceeding the encoded type name. This is thankfully quite easy to incorporate into the existing type parsing. Care should be taken with opaque types however since theyll be returned like this: <code>{OpaqueType=}</code> (For reference: Opaque types are types where the caller doesnt know the type layout).</p>
<h4 id="id">
<span>ID</span><a href="#id"><i></i></a>
</h4>
<p>Finally, we have id - this is actually a <a href="https://developer.apple.com/documentation/objectivec/id" target="_blank" rel="noopener noreferrer">type from the objc runtime API</a>. This is a reference to an ObjC object, unfortunately for this we dont get any more type information. Looking at this from C, we basically only know that this is a <code>void*</code> and not what type.</p>
<h3 id="collect-objc-method-metadata">
<span>Collect ObjC Method Metadata</span><a href="#collect-objc-method-metadata"><i></i></a>
</h3>
<p>Now that it is established how the types are encoded, we can continue with querying metadata about each individual method.</p>
<p>After the class hierarchy has been flattened, we have generated a list of all methods of the complete class hierarchy. In this step we use this list to query information about each individual method. Things that were interested in include the following:</p>
<ol>
<li>Method Name</li>
<li>Return Type</li>
<li>Number of Parameters</li>
<li>Type of Parameters</li>
</ol>
<p>Since we now operate on objects of type <code>Method</code>, we can use the ObjC runtime functions prefixed with <code>method_</code>. The first function that is being used is <a href="https://developer.apple.com/documentation/objectivec/1418758-method_getname?language=objc" target="_blank" rel="noopener noreferrer"><code>method_getName()</code></a> - this is similar to <code>class_getName()</code> and returns a string.</p>
<p>The next function that we can use is <a href="https://developer.apple.com/documentation/objectivec/1418591-method_getreturntype?language=objc" target="_blank" rel="noopener noreferrer"><code>method_getReturnType()</code></a> this returns an ObjC runtime encoded type string (that we know how to parse thanks to <a href="#resolving-objc-runtime-types">the previous chapter</a>). To get the parameter list, <a href="https://developer.apple.com/documentation/objectivec/1418968-method_getnumberofarguments?language=objc" target="_blank" rel="noopener noreferrer"><code>method_getNumberOfArguments()</code></a> together with <a href="https://developer.apple.com/documentation/objectivec/1418607-method_getargumenttype?language=objc" target="_blank" rel="noopener noreferrer"><code>method_getArgumentType()</code></a> can be used. The returned type is also encoded as an ObjC runtime type.</p>
<blockquote><p><em>Note</em>: Unfortunately the name of the arguments can not be queried - until this is solved, the arguments follow a <code>arg0</code>, <code>arg1</code>, <code>arg2</code>, etc naming-scheme.</p></blockquote>
<p>With these information available, we have a complete function signature and now only need to find out how we can do the actual function call.</p>
<h3 id="calling-objc-methods-using-the-objc-runtime">
<span>Calling ObjC Methods Using The ObjC Runtime</span><a href="#calling-objc-methods-using-the-objc-runtime"><i></i></a>
</h3>
<p>For calling ObjC methods various variations of <code>objc_msgSend</code> can be used. I say various variations because there are multiple version depending on what kind of return value you expect (this only applies when targeting i386 hosts).</p>
<ol>
<li>
<a href="https://developer.apple.com/documentation/objectivec/1456712-objc_msgsend" target="_blank" rel="noopener noreferrer"><code>objc_msgSend</code></a> for function that return types &lt;= 16 bytes.</li>
<li>
<a href="https://developer.apple.com/documentation/objectivec/1456697-objc_msgsend_fpret" target="_blank" rel="noopener noreferrer"><code>objc_msgSend_fpret</code></a> for float return types.</li>
<li>
<a href="https://developer.apple.com/documentation/objectivec/1456730-objc_msgsend_stret" target="_blank" rel="noopener noreferrer"><code>objc_msgSend_stret</code></a> for functions that return types &gt; 16 bytes.</li>
</ol>
<p>Calculating the size of the return type can be added as part of the pass where the return type is parsed. During code generation, the size and type of the return value can then be used to select the correct <code>objc_msgSend</code> function.</p>
<p>If you look at the definition of any of these function, youll see that the first 2 arguments are always <code>ID self</code> and <code>SEL selector</code>. The first argument <code>ID self</code> is a pointer to the object which we want to call this method on (or, in case of static methods, the meta-class). The <code>SEL selector</code> argument is the name of the method at runtime. This can be retrieved by calling <a href="https://developer.apple.com/documentation/objectivec/1418557-sel_registername?language=objc" target="_blank" rel="noopener noreferrer"><code>sel_registerName()</code></a> with the method name as argument (as given by <code>method_getName()</code>).</p>
<blockquote><p><em>Note</em>: The return value of <code>sel_registerName()</code> is cached internally.</p></blockquote>
<h3 id="generate-c-source-code">
<span>Generate C Source Code</span><a href="#generate-c-source-code"><i></i></a>
</h3>
<p>Were now perfectly set-up to start generating the C source code, which will become the basis of our API.</p>
<p>If you remember from <a href="#flattening-objc-class-hierarchy">the previous chapter</a> we are left with 2 arrays which contain elements of type <code>Method</code> after the class hierarchy has been flattened. One array for static and one array for non-static methods. These arrays are now used to create matching C functions for each individual method.</p>
<p>We do this by using the method meta-data that weve collected <a href="#collect-objc-method-metadata">earlier</a>. The result of this step will be matching .h/.c files for each ObjC class that should be exported.</p>
<p>The first step when writing the C source files is always the file prefix. For .h files the prefix is a header guard and a single typedef for syntactic sugar (example for <code>NSString</code>):</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
</pre></td>
<td><pre><span>#ifndef C_OCOA_NSSTRING_HEADER
#define C_OCOA_NSSTRING_HEADER
</span><span>typedef</span> <span>nsstring_t</span> <span>void</span><span>*</span><span>;</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<p>(for .h files theres also a postfix needed to add the <code>#endif</code> for the header guard).</p>
<p>For .c files this is a couple of defines that change what variation of <code>objc_msgSend</code> is being called based on the target ABI (since this could theoretically be x86 or ARM). To make the generated C code work on both architectures, these defines are added at the beginning of every .c file:</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
</pre></td>
<td><pre><span>#ifdef __arm64__
#define abi_objc_msgSend_stret objc_msgSend
#else
#define abi_objc_msgSend_stret objc_msgSend_stret
#endif
#ifdef __i386__
#define abi_objc_msgSend_fpret objc_msgSend_fpret
#else
#define abi_objc_msgSend_fpret objc_msgSend
#endif
</span></pre></td>
</tr></tbody></table></code></p>
</div>
<blockquote><p><em>Note</em>: this means that instead of <code>objc_msgSend_stret</code> and <code>objc_msgSend_fpret</code> we need to use <code>abi_objc_msgSend_stret</code> &amp; <code>abi_objc_msgSend_fpret</code> respectively.</p></blockquote>
<p>For each C function we first call <code>sel_registerName()</code> to get the correct selector for the method that we want to call. After that we have to make the call to <code>objc_msgSend()</code> to perform the actual function call.</p>
<blockquote><p><em>Note</em>: Since all the <code>objc_msgSend</code> function are typless, they need to be cast to the correct function-ptr type to be called with the correct arguments. This will be done using a synctactic-sugar helper-macro for better readability and debugability.</p></blockquote>
<h4 id="example">
<span>Example</span><a href="#example"><i></i></a>
</h4>
<p>To give a concrete example lets focus on the source code generation of the <code>NSString</code> class with one method, <a href="https://developer.apple.com/documentation/foundation/nsstring/1415702-getcstring?language=objc" target="_blank" rel="noopener noreferrer"><code>getCString()</code></a></p>
<p>The function declaration in Objective-C for that function looks like this:</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
</pre></td>
<td><pre><span>@interface</span> <span>NSString</span> <span>:</span> <span>NSSObject</span>

<span>-</span> <span>(</span><span>BOOL</span><span>)</span><span>getCString</span><span>:(</span><span>char</span> <span>*</span><span>)</span><span>buffer</span> 
        <span>maxLength</span><span>:(</span><span>NSUInteger</span><span>)</span><span>maxBufferCount</span> 
        <span>encoding</span><span>:(</span><span>NSStringEncoding</span><span>)</span><span>encoding</span><span>;</span>

<span>@end</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
</pre></td>
<td><pre><span>// Usage-Code:</span>
<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>**</span> <span>argv</span><span>)</span>
<span>{</span>
  <span>NSString</span><span>*</span> <span>string</span> <span>=</span> <span>[[</span><span>NSString</span> <span>alloc</span><span>]</span> <span>init</span><span>];</span>
  <span>//Fill string with data...</span>
  <span>//..</span>
  <span>char</span> <span>buffer</span><span>[</span><span>512</span><span>];</span>
  <span>[</span><span>string</span> <span>getCstring</span><span>:</span><span>buffer</span> <span>maxLength</span><span>:</span><span>512</span> <span>encoding</span><span>:</span><span>NSUTF8StringEncoding</span><span>]</span>
<span>}</span>

</pre></td>
</tr></tbody></table></code></p>
</div>
<p>Using the information provided earlier, the generated c_ocoa .h/.c file(s) would look like this (including usage-code):</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
</pre></td>
<td><pre><span>// nsstring.h</span>
<span>#ifndef C_OCOA_NSSTRING_HEADER
#define C_OCOA_NSSTRING_HEADER
</span><span>typedef</span> <span>nsstring_t</span> <span>void</span><span>*</span><span>;</span>

<span>bool</span> <span>nsstring_getCString</span><span>(</span> <span>nsstring_t</span> <span>object</span><span>,</span> <span>char</span><span>*</span> <span>arg0</span><span>,</span> <span>unsigned</span> <span>long</span> <span>long</span> <span>arg1</span><span>,</span> <span>unsigned</span> <span>long</span> <span>long</span> <span>arg2</span> <span>);</span>

<span>#endif
</span></pre></td>
</tr></tbody></table></code></p>
</div>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
</pre></td>
<td><pre><span>// nsstring.c</span>
<span>#include</span> <span>"nsstring.h"</span><span>
</span>
<span>bool</span> <span>nsstring_getCString</span><span>(</span> <span>nsstring_t</span> <span>object</span><span>,</span> <span>char</span><span>*</span> <span>arg0</span><span>,</span> <span>unsigned</span> <span>long</span> <span>long</span> <span>arg1</span><span>,</span> <span>unsigned</span> <span>long</span> <span>long</span> <span>arg2</span> <span>)</span>
<span>{</span>
	<span>SEL</span> <span>methodSelector</span> <span>=</span> <span>sel_registerName</span><span>(</span> <span>"getCString:maxLength:encoding:"</span> <span>);</span>

	<span>#define nsstring_getCString_call( obj, selector, arg0, arg1, arg2 ) ((bool (*)( id, SEL, char*, unsigned long long, unsigned long long ))objc_msgSend) ( obj, selector, arg0, arg1, arg2 )
</span>	<span>return</span> <span>nsstring_getCString_call</span><span>(</span> <span>(</span><span>id</span><span>)</span><span>object</span><span>,</span> <span>methodSelector</span><span>,</span> <span>arg0</span><span>,</span> <span>arg1</span><span>,</span> <span>arg2</span> <span>);</span>
	<span>#undef nsstring_getCString_call
</span><span>}</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
</pre></td>
<td><pre><span>// Usage-Code:</span>
<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>**</span> <span>argv</span><span>)</span>
<span>{</span>
  <span>nsstring_t</span> <span>string</span> <span>=</span> <span>nsstring_alloc</span><span>(</span> <span>nsstring_init</span><span>()</span> <span>);</span>
  <span>//Fill string with data...</span>
  <span>//..</span>
  <span>char</span> <span>buffer</span><span>[</span><span>512</span><span>];</span>
  <span>nsstring_getCString</span><span>(</span><span>string</span><span>,</span> <span>buffer</span><span>,</span> <span>512</span><span>,</span> <span>NSUTF8StringEncoding</span><span>);</span>
<span>}</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<h2 id="conclusion">
<span>Conclusion</span><a href="#conclusion"><i></i></a>
</h2>
<p>Generating a pure C API to interface with the OSX/iOS system APIs was a very interesting problem for me personally since it gave me an excuse to work with ObjC and get to know some internals of the language. This was definitely a big undertaking and, to be honest, I expected it to fail at any moment because of some problem(s) that we didnt foresee. But fortunately everything worked out pretty nicely and it was definitely a scary feeling letting the generator work through the <em>entire</em> class hierarchy of big frameworks like <strong>AppKit</strong>, <strong>Foundation</strong> or <strong>GLKit</strong>. That being said we did hit a couple of problems that were actively working on, namely parameter naming and adding auto-generated documentation to the various functions. But all in all the result is pretty cool and we were able to ship an iOS application using the technology.</p>
<h2 id="sourcecode">
<span>Sourcecode</span><a href="#sourcecode"><i></i></a>
</h2>
<p>The entire source code of the code generator has been made open source and can be <a href="https://github.com/FelixK15/c_ocoa" target="_blank" rel="noopener noreferrer">accessed on github</a>. Follow the README in the repository to build the generator locally.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[X sues Calif. to avoid revealing how it makes controversial content decisions (176 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/09/x-sues-calif-to-avoid-revealing-how-it-makes-controversial-content-decisions/</link>
            <guid>37467607</guid>
            <pubDate>Mon, 11 Sep 2023 14:15:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/09/x-sues-calif-to-avoid-revealing-how-it-makes-controversial-content-decisions/">https://arstechnica.com/tech-policy/2023/09/x-sues-calif-to-avoid-revealing-how-it-makes-controversial-content-decisions/</a>, See on <a href="https://news.ycombinator.com/item?id=37467607">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      "Rebranding censorship"    
</h4>
            
            <h2 itemprop="description">X decried law's "draconian financial penalties," up to $15K per violation per day.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/09/GettyImages-1563274899-800x532.jpg" alt="X sues Calif. to avoid revealing how it makes controversial content decisions">
      <figcaption></figcaption>  </figure>

  




<!-- cache hit 35:single/related:c4dbb0a9ea6a9aff2f1f0298bc009002 --><!-- empty -->
<p>Today, Elon Musk's X Corp. <a href="https://cdn.arstechnica.net/wp-content/uploads/2023/09/X-Corp-v-Bonta-9-8-2023-Complaint.pdf">sued</a> to block California's content moderation law, AB 587. In its complaint, filed in a US district court in California, X Corp. is seeking a preliminary and permanent injunction stopping California Attorney General Robert Bonta from enforcing the law.</p>
<p>AB 587 passed in September 2022, requiring social media platforms to submit a "terms of service report" semi-annually to California's attorney general, providing "a detailed description of content moderation practices used" and "information about whether, and if so how, the social media company defines and moderates" hate speech or racism, extremism or radicalization, disinformation or misinformation, harassment, and foreign political interference. Under the law, social media platforms must also provide information and statistics on any content moderation actions taken in those categories.</p>
<p>In X's complaint, the company accused California of trying to dictate X's terms of service and compel "controversial disclosures about how X Corp. moderates content on its platform."</p>
<p>The law stipulated that all platforms were required to start collecting data for their first terms of service report covering content moderation during the third quarter of 2023 and submit those reports to Bonta by January 1, 2024.</p>
<p>Platforms could be found violating the law for failing to post terms of service about content moderation, missing a deadline to submit a terms of service report, or materially omitting or misrepresenting information about content moderation. Any platform violating the law risks fineswhich X described as "draconian financial penalties"up to $15,000 per violation per day.</p>
<p>In its complaint, X Corp. argued that AB 587 violates the First Amendment by compelling "companies like X Corp. to engage in speech against their will" and "impermissibly" interfering "with the constitutionally protected editorial judgments of companies." X Corp. said that if the court did not block the law, California could pressure companies "to remove, demonetize, or deprioritize constitutionally protected speech that the state deems undesirable or harmful."</p>
<p>"The State of California touts AB 587 as a mere 'transparency measure' under which certain social media companies must make their content moderation policies and statistics publicly&nbsp;available," X's complaint said. But, X alleged, the state's "true intent" is "to pressure social media platforms to 'eliminate' certain constitutionally protected content viewed by the state as problematic."</p>                                            
                                                        
<p>X Corp. alleged that AB 587 violates other laws, including the Dormant Commerce Clause"failing to restrict its extensive reporting requirements to information about Californians"and Section 230 of the Communications Decency Actwhich grants platforms immunity from liability for any action voluntarily taken in good faith to restrict access to or availability of material that the provider or user considers to be obscene, lewd, lascivious, filthy, excessively violent, harassing, or otherwise objectionable, whether or not such material is constitutionally protected.</p>
<p>"Because AB 587 imposes liability on such actions if they are taken without the required disclosures, AB 587 is preempted by the broad immunity afforded by Section 230," X's complaint said.</p>
<p>Ars could not immediately reach X for comment. Bonta's office said: While we have not yet been served with the complaint, we will review it and respond in court.</p>
<p>The author of AB 587, California assemblymember Jesse Gabriel, released a statement saying that the law "is a pure transparency measure that simply requires companies to be upfront about if and how they are moderating content. It in no way requires any specific content moderation policieswhich is why it passed with strong, bipartisan support. If Twitter has nothing to hide, then they should have no objection to this bill.</p>
<p>But tech groups and policy experts echoed X's concerns over AB 587.</p>
<p>Adam Kovacevich, the CEO of the tech industry policy coalition Chamber of Progress, said that "requiring companies to give their content moderation playbook to scammers and conspiracists is a bad idea."</p>
<p>Even if you don't like anything about Elon Musks leadership of X, its clear that requiring tech platforms to publish a detailed blueprint of how to work around content moderators will have negative consequences for users online," Kovacevich said. "Letting platforms set their own editorial standards also leaves consumers with more choices about what kind of platforms they spend time on.</p>
<p>Netchoice, a group representing tech companies and trade associations, has called AB 587 "the Golden States new online censorship law." In a statement about X Corp.'s lawsuit, Netchoice said that the law would force companies to submit "intrusive" and "often impossible to comply with" disclosures "about constitutionally protected editorial decisions." Netchoice's director of litigation, Chris Marchese, said that the court should enjoin AB 587 to protect free speech online.</p>
<p>The First Amendment prohibits the government from regulating lawful speechdirectly or indirectly," Marchese said. "States cannot avoid this prohibition by rebranding censorship as transparency requirements."</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidias AI supremacy is only temporary (162 pts)]]></title>
            <link>https://petewarden.com/2023/09/10/why-nvidias-ai-supremacy-is-only-temporary/</link>
            <guid>37467585</guid>
            <pubDate>Mon, 11 Sep 2023 14:13:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://petewarden.com/2023/09/10/why-nvidias-ai-supremacy-is-only-temporary/">https://petewarden.com/2023/09/10/why-nvidias-ai-supremacy-is-only-temporary/</a>, See on <a href="https://news.ycombinator.com/item?id=37467585">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						
<figure><a href="https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png"><img data-attachment-id="7833" data-permalink="https://petewarden.com/2023/09/10/why-nvidias-ai-supremacy-is-only-temporary/dallc2b7e-2023-09-09-18-43-21-computer-chips-running-in-a-foot-race/" data-orig-file="https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png" data-orig-size="1024,1024" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race" data-image-description="" data-image-caption="" data-medium-file="https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png?w=300" data-large-file="https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png?w=550" src="https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png?w=1024" alt="" srcset="https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png 1024w, https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png?w=150 150w, https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png?w=300 300w, https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>Nvidia is an amazing company that has executed a contrarian vision for decades, and has rightly become one of the most valuable corporations on the planet thanks to its central role in the AI revolution. I want to explain why I believe its top spot in machine learning is far from secure over the next few years. To do that, Im going to talk about some of the drivers behind Nvidias current dominance, and then how they will change in the future.</p>



<h2><span>Currently</span></h2>



<p>Heres why I think Nvidia is winning so hard right now.</p>



<p><strong>#1  Almost Nobody is Running Large ML Apps</strong></p>



<p>Outside of a few large tech companies, very few corporations have advanced to actually running large scale AI models in production. Theyre still figuring out how to get started with these new capabilities, so the main costs are around dataset collection, hardware for training, and salaries for model authors. This means that machine learning is focused on training, not inference.</p>



<p><strong>#2  All Nvidia Alternatives Suck</strong></p>



<p>If youre a developer creating or using ML models, using an Nvidia GPU is a lot easier and less time consuming than an AMD OpenCL card, Google TPU, a Cerebras system, or any other hardware. The software stack is much more mature, there are many more examples, documentation, and other resources, finding engineers experienced with Nvidia is much easier, and integration with all of the major frameworks is better. There is no realistic way for a competitor to beat the platform effect Nvidia has built. It makes sense for the current market to be winner-takes-all, and theyre the winner, full stop.</p>



<p><strong>#3  Researchers have the Purchasing Power</strong></p>



<p>Its incredibly hard to hire ML researchers, anyone with experience has their pick of job offers right now. That means they need to be kept happy, and one of the things they demand is use of the Nvidia platform. Its what they know, theyre productive with it, picking up an alternative would take time and not result in skills the job market values, whereas working on models with the tools theyre comfortable with does. Because researchers are so expensive to hire and retain, their preferences are given a very high priority when purchasing hardware.</p>



<p><strong>#4  Training Latency Rules</strong></p>



<p>As a rule of thumb models need to be trainable from scratch in about a week. Ive seen this hold true since the early days of AlexNet, because if the iteration cycle gets any longer its very hard to do the empirical testing and prototyping thats still essential to reach your accuracy goals. As hardware gets faster, people build bigger models up until the point that the training once again takes roughly the same amount of time, and reap the benefits through higher-quality models rather than reduced total training time. This makes buying the latest Nvidia GPUs very attractive, since your existing code will mostly just work, but faster. In theory theres an opportunity here for competitors to win with lower latency, but the inevitably poor state of their software stack (CUDA has had decades of investment) means its mostly an illusion.</p>



<h2><strong>Whats going to change?</strong></h2>



<p>So, hopefully Ive made a convincing case that there are strong structural reasons behind Nvidias success. Heres how I see those conditions changing over the next few years.</p>



<p><strong>#1  Inference will Dominate, not Training</strong></p>



<p>Somebody years ago told me Training costs scale with the number of researchers, inference costs scale with the number of users. What I took away from this is that theres some point in the future where the amount of compute any company is using for running models on user requests will exceed the cycles theyre spending on training. Even if the cost of a single training run is massive and running inference is cheap, there are so many potential users in the world with so many different applications that the accumulated total of those inferences will exceed the training total. There are only ever going to be so many researchers.</p>



<p>What this means for hardware is that priorities will shift towards reducing inference costs. A lot of ML researchers see inference as a subset of training, but this is wrong in some fundamental ways. Its often very hard to assemble a sizable batch of inputs during inference, because that process trades off latency against throughput, and latency is almost always key in user-facing applications. Small or single-input batches change the workload dramatically, and call for very different optimization approaches. There are also a lot of things (like the weights) that remain constant during inference, and so can benefit from pre-processing techniques like weight compression or constant folding.</p>



<p><strong>#2  CPUs are Competitive for Inference </strong></p>



<p>I didnt even list CPUs in the Nvidia alternatives above because theyre still laughably slow for training. The main desktop CPUs (x86, Arm, and maybe RISC-V soon) have the benefit of many decades of toolchain investment. They have an even more mature set of development tools and community than Nvidia. They can also be much cheaper per arithmetic op than any GPU.</p>



<p>Old-timers will remember the early days of the internet when most of the cost of setting up a dot-com was millions of dollars for a bunch of high-end web server hardware from someone like Sun. This was because they were the only realistic platform that could serve web pages reliably and with low-latency. They had the fastest hardware money could buy, and that was important when entire sites needed to fit on a single machine. Suns market share was rapidly eaten by the introduction of software that could distribute the work across a large number of individually much less capable machines, commodity x86 boxes that were far cheaper.</p>



<p>Training is currently very hard to distribute in a similar way. The workloads make it possible to split work across a few GPUs that are tightly interconnected, but the pattern of continuous updates makes reducing latency by sharding across low-end CPUs unrealistic. This is not true for inference though. The model weights are fixed and can easily be duplicated across a lot of machines at initialization time, so no communication is needed. This makes an army of commodity PCs very appealing for applications relying on ML inference.</p>



<p><strong>#3  Deployment Engineers gain Power</strong></p>



<p>As inference costs begin to dominate training, there will be a lot of pressure to reduce those costs. Researchers will no longer be the highest priority, so their preferences will carry less weight. They will be asked to do things that are less personally exciting in order to streamline production. There are also going to be a lot more people capable of training models coming into the workforce over the next few years, as the skills involved become more widely understood. This all means researchers corporate power will shrink and the needs of the deployment team will be given higher priority.</p>



<p><strong>#4  Application Costs Rule</strong></p>



<p>When inference dominates the overall AI budget, the hardware and workload requirements are very different. Researchers value the ability to quickly experiment, so they need flexibility to prototype new ideas. Applications usually change their models comparatively infrequently, and may use the same fundamental architecture for years, once the researchers have come up with something that meets their needs. We may almost be heading towards a world where model authors use a specialized tool, like Matlab is for mathematical algorithms, and then hand over the results to deployment engineers who will manually convert the results into something more efficient for an application. This will make sense because any cost savings will be multiplied over a long period of time if the model architecture remains constant (even if the weights change).</p>



<h2>What does this Mean for the Future?</h2>



<p>If you believe my four predictions above, then its hard to escape the conclusion that Nvidias share of the overall AI market is going to drop. That market is going to grow massively so I wouldnt be surprised if they continue to grow in absolute unit numbers, but I cant see how their current margins will be sustainable.</p>



<p>I expect the winners of this shift will be traditional CPU platforms like x86 and Arm. Inference will need to be tightly integrated into traditional business logic to run end user applications, so its difficult to see how even hardware specialized for inference can live across a bus, with the latency involved. Instead I expect CPUs to gain much more tightly integrated machine learning support, first as co-processors and eventually as specialized instructions, like the evolution of floating point support.</p>



<p>On a personal level, these beliefs drive my own research and startup focus. The impact of improving inference is going to be so high over the next few years, and it still feels neglected compared to training. There are signs that this is changing though. Communities like <a href="https://www.reddit.com/r/LocalLLaMA/">r/LocalLlama</a> are mostly focused on improving inference, the success of <a href="https://github.com/ggerganov/ggml">GGML</a> shows how much of an appetite there is for inference-focused frameworks, and the spread of a few general-purpose models increases the payoff of inference optimizations. One reason Im so obsessed with the edge is that its the closest environment to the army of commodity PCs that I think will run most cloud AI in the future. Even back in 2013 I originally wrote <a href="https://github.com/jetpacapp/DeepBeliefSDK">the Jetpac SDK</a> to accelerate computer vision on a cluster of 100 m1.small AWS servers, since that was cheaper and faster than a GPU instance for running inference across millions of images. It was only afterwards that I realized what a good fit it was for mobile devices.</p>



<p>Id love to hear your thoughts on whether inference is going to be as important as Im predicting! Let me know in the comments if you think Im onto something, or if I should be stocking up on Nvidia stock.</p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[9/11 in Realtime (210 pts)]]></title>
            <link>https://911realtime.org:443/</link>
            <guid>37467077</guid>
            <pubDate>Mon, 11 Sep 2023 13:38:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://911realtime.org:443/">https://911realtime.org:443/</a>, See on <a href="https://news.ycombinator.com/item?id=37467077">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <li> <h2>9/11 Realtime</h2> <p><b>Thanks and Open Source Notices</b></p> <h2>Thank you to our backers:</h2> <ul> <li>Will Harris</li> <li>Chris Wooster</li> <li>Robinson Collado</li> <li>Richard Harms</li> <li>Matt MG Herron</li> <li>Adil Majid</li> <li>Alana Malone</li> <li>Kori Stephens</li> <li>Marina Harper</li> <li>James Wendel</li> <li>Jason Smith</li> <li>Adam Garst</li> <li>Andrew Poirier</li> <li>Ty Satrang</li> </ul> <h2>Special thanks to <a href="http://hivelocity.net/">Hivelocity</a></h2> <p><b>Based on Platinum by Robbie Byrd</b></p> <p>A UI framework using native CSS/JS replications of the Mac OS 8.1 interface components. The project is named after the interface theme that came with MacOS 8 and 9, Platinum.</p> <p><a href="https://github.com/robbiebyrd/platinum" target="_blank">Platinum on Github</a></p> <p><b>Based on memento.js by Vijith Assar</b></p> <p><a href="https://github.com/vijithassar/memento" target="_blank">memento.js on Github</a></p> <p> Based on <b><a href="https://github.com/npjg/classic.css" target="_blank">New Dawn</a></b> by <b><a href="https://github.com/npjg" target="_blank">Nathanael Gentry</a></b>. </p><p>Copyright (c) 2019 Nathanael Gentry</p>  <p> Based on <b><a href="https://github.com/ticky/classic-scrollbars" target="_blank">Scrollbars of the Classic Mac OS</a></b> by <b><a href="https://github.com/ticky" target="_blank">Jessica Stokes (@ticky)</a></b>. </p> <hr> <p><b>New Dawn</b> and <b>Platinum</b> License</p> <div><p> MIT License </p><p>  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: </p></div> <div><p> The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. </p><p>  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. </p></div> <p>A huge thanks to Apple, Inc., who maintains the copyright on the Apple Icon, background patterns, interface sounds and interface components.</p>  </li> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unix Domain Sockets vs Loopback TCP Sockets (2014) (136 pts)]]></title>
            <link>https://nicisdigital.wordpress.com/2014/03/03/unix-domain-sockets-vs-loopback-tcp-sockets/</link>
            <guid>37466475</guid>
            <pubDate>Mon, 11 Sep 2023 12:51:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nicisdigital.wordpress.com/2014/03/03/unix-domain-sockets-vs-loopback-tcp-sockets/">https://nicisdigital.wordpress.com/2014/03/03/unix-domain-sockets-vs-loopback-tcp-sockets/</a>, See on <a href="https://news.ycombinator.com/item?id=37466475">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
       <p>Two communicating processes on a single machine have a few options. They can use regular TCP sockets, UDP sockets, unix domain sockets, or shared memory. A recent project I was working on used Node.js with two communicating processes on the same machine. I wanted to know how to reduce the CPU utilization of the machine, so I ran a few experiments to compare the efficiency between unix domain sockets and TCP sockets using the loopback interface. This post covers my experiments and test results.</p>
<p>First off, is a disclaimer. This test is not exhaustive. Both client and server are written in Node.js and can only be as efficient as the Node.js runtime.</p>
<p>All code in this post is available at:&nbsp;<a href="http://github.com/nicmcd/uds_vs_tcp">github.com/nicmcd/uds_vs_tcp</a></p>
<h2>Server Application</h2>
<p>I created a simple Node.js server application that could be connected to via TCP socket or Unix domain socket. It simply echos all received messages. Here is the code:</p>
<pre title="">var assert = require('assert');
assert(process.argv.length == 4, 'node server.js &lt;tcp port&gt; &lt;domain socket path&gt;');

var net = require('net');

var tcpPort = parseInt(process.argv[2]);
assert(!isNaN(tcpPort), 'bad TCP port');
console.log('TCP port: ' + tcpPort);

var udsPath = process.argv[3];
console.log('UDS path: ' + udsPath);

function createServer(name, portPath) {
    var server = net.createServer(function(socket) {
        console.log(name + ' server connected');
        socket.on('end', function() {
            console.log(name + ' server disconnected');
        });
        socket.write('start sending now!');
        socket.pipe(socket);
    });
    server.listen(portPath, function() {
        console.log(name + ' server listening on ' + portPath);
    });
}

var tcpServer = createServer('TCP', tcpPort);
var udsServer = createServer('UDS', udsPath);
</pre>
<h2>Client Application</h2>
<p><span>The client application complements the server application. It connects to the server via TCP or Unix domain sockets. It sends a bunch of randomly generated packets and measures the time it takes to finish. When complete, it prints the time and exits. Here is the code:</span></p>
<pre title="">var assert = require('assert');
assert(process.argv.length == 5, 'node client.js &lt;port or path&gt; &lt;packet size&gt; &lt;packet count&gt;');

var net = require('net');
var crypto = require('crypto');

if (isNaN(parseInt(process.argv[2])) == false)
    var options = {port: parseInt(process.argv[2])};
else
    var options = {path: process.argv[2]};
console.log('options: ' + JSON.stringify(options));

var packetSize = parseInt(process.argv[3]);
assert(!isNaN(packetSize), 'bad packet size');
console.log('packet size: ' + packetSize);

var packetCount = parseInt(process.argv[4]);
assert(!isNaN(packetCount), 'bad packet count');
console.log('packet count: ' + packetCount);

var client = net.connect(options, function() {
    console.log('client connected');
});

var printedFirst = false;
var packet = crypto.randomBytes(packetSize).toString('base64').substring(0,packetSize);
var currPacketCount = 0;
var startTime;
var endTime;
var delta;
client.on('data', function(data) {
    if (printedFirst == false) {
        console.log('client received: ' + data);
        printedFirst = true;
    }
    else {
        currPacketCount += 1;
        if (data.length != packetSize)
            console.log('weird packet size: ' + data.length);
        //console.log('client received a packet: ' + currPacketCount);
    }

    if (currPacketCount &lt; packetCount) {
        if (currPacketCount == 0) {
            startTime = process.hrtime();
        }
        client.write(packet);
    } else {
        client.end();
        endTime = process.hrtime(startTime);
        delta = (endTime[0] * 1e9 + endTime[1]) / 1e6;
        console.log('millis: ' + delta);
    }
});
</pre>
<h2>Running a Single Test</h2>
<p>First start the server application with:</p>
<pre title="">node server.js 5555 /tmp/uds
</pre>
<p>This starts the server using TCP port 5555 and Unix domain socket /tmp/uds.</p>
<p>Now we can run the client application to get some statistics. Lets first try the TCP socket. Run the client with:</p>
<pre title="">
node client.js 5555 1000 100000

</pre>
<p>This runs the client application using TCP port 5555 and sends 100,000 packets all sized 1000 bytes. This tooks 8006 milliseconds on my machine. We can now try running with the Unix domain socket with:</p>
<pre title="">
node client.js /tmp/uds 1000 100000

</pre>
<p>This runs the client the same as before except it uses the /tmp/uds Unix domain socket instead of the TCP socket. On my machine this took 3570 milliseconds to run. These two runs show that for 1k byte packets, Unix domain sockets are about 2-3x more efficient than TCP sockets.<br>
At this point you might be completely convinced that Unix domain sockets are better and youll use them whenever you can. Thats too easy. Lets run the client application a whole bunch of times and graph the results.<br>
I recently posted about a <a title="taskrun  An easy-to-use python package for running tasks with dependencies and process&nbsp;management" href="https://nicisdigital.wordpress.com/2013/12/13/taskrun/">python package</a> I created for running many tasks and aggregating the data. I thought this socket comparison would make a good example.</p>
<h2>Running the Full Test</h2>
<p>As mentioned, running the full test uses the Taskrun Python package (available at <a title="Taskrun Python Package" href="http://github.com/nicmcd/taskrun">github.com/nicmcd/taskrun</a>). The script I quickly hacked together to run the client application and parse the results is as follows:</p>
<pre title="">
import taskrun
import os

POWER = 15
RUNS = 10
PACKETS_PER_RUN = 100000

manager = taskrun.Task.Manager(
    numProcs = 1,
    showCommands = True,
    runTasks = True,
    showProgress = True)

DIR = "sims"
mkdir = manager.task_new('dir', 'rm -rI ' + DIR + '; mkdir ' + DIR)

def makeName(stype, size, run):
    return stype + '_size' + str(size) + '_run' + str(run)

def makeCommand(port_or_path, size, name):
    return 'node client.js ' + port_or_path + ' ' + str(size) + ' ' + str(PACKETS_PER_RUN) + \
        ' | grep millis | awk \'{printf "%s, ", $2}\' &gt; ' + os.path.join(DIR, name)

barrier1 = manager.task_new('barrier1', 'sleep 0')
for exp in range(0, POWER):
    size = pow(2, exp)
    for run in range(0, RUNS):
        # Unix domain socket test
        name = makeName('uds', size, run)
        task = manager.task_new(name, makeCommand('/tmp/uds', size, name))
        task.dependency_is(mkdir)
        barrier1.dependency_is(task)

        # TCP socket test
        name = makeName('tcp', size, run)
        task = manager.task_new(name, makeCommand('5555', size, name))
        task.dependency_is(mkdir)
        barrier1.dependency_is(task)

# create CSV header
filename = os.path.join(DIR, 'uds_vs_tcp.csv')
header = 'NAME, '
for run in range(0, RUNS):
    header += 'RUN ' + str(run) + ', '
hdr_task = manager.task_new('CSV header', 'echo \'' + header + '\' &gt; ' + filename)
hdr_task.dependency_is(barrier1)

# UDS to CSV
cmd = ''
for exp in range(0,POWER):
    size = pow(2, exp)
    cmd += 'echo -n \'UDS Size ' + str(size) + ', \' &gt;&gt; ' + filename + '; '
    for run in range(0, RUNS):
        name = makeName('uds', size, run)
        cmd += 'cat ' + os.path.join(DIR, name) + ' &gt;&gt; ' + filename + '; '
    cmd += 'echo \'\' &gt;&gt; ' + filename + '; '
uds_task = manager.task_new('UDS to CSV', cmd)
uds_task.dependency_is(hdr_task)

# TCP to CSV
cmd = ''
for exp in range(0,POWER):
    size = pow(2, exp)
    cmd += 'echo -n \'TCP Size ' + str(size) + ', \' &gt;&gt; ' + filename + '; '
    for run in range(0, RUNS):
        name = makeName('tcp', size, run)
        cmd += 'cat ' + os.path.join(DIR, name) + ' &gt;&gt; ' + filename + '; '
    cmd += 'echo \'\' &gt;&gt; ' + filename + '; '
tcp_task = manager.task_new('TCP to CSV', cmd)
tcp_task.dependency_is(uds_task)

manager.run_request_is()

</pre>
<p>Admittedly, this isnt the prettiest code to look at, but it gets the job done. For both Unix domain socket and TCP socket, it runs the client application for all packet sizes that are a power of 2 from 1 to 16384. Each setup is run 10 times. Each test result is written to its own file. After all the tests have been run, the taskrun script creates a CSV file using all the test results. The CSV file can then be imported into a spreadsheet application for analysis.</p>
<h2>Results</h2>
<p>I ran this on an&nbsp;<a href="http://ark.intel.com/products/75789/">Intel E5-2620 v2</a>&nbsp;processor with 16GB of RAM. I imported the CSV into Excel, averaged the 10 results of each setup, then graphed the results. This first graph shows the execution time compared to packet size on a logarithmic graph.</p>
<p><a href="https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png"><img data-attachment-id="604" data-permalink="https://nicisdigital.wordpress.com/2014/03/03/unix-domain-sockets-vs-loopback-tcp-sockets/exe-time-vs-pkt-size/" data-orig-file="https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png" data-orig-size="966,611" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="Execution Time vs. Packet Size" data-image-description="" data-image-caption="" data-medium-file="https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png?w=300" data-large-file="https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png?w=510" alt="Execution Time vs. Packet Size" src="https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png?w=510&amp;h=322" width="510" height="322" srcset="https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png?w=510 510w, https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png?w=150 150w, https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png?w=300 300w, https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png?w=768 768w, https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png 966w" sizes="(max-width: 510px) 100vw, 510px"></a></p>
<p>The results shown here are fairly predicable. The Unix domain sockets are always more efficient and the efficiency benefit is in the 2-3x range. After noticing some weird ups and down in the graph, I decided to generate a graph with the execution times normalized to the TCP execution time.</p>
<p><a href="https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png"><img data-attachment-id="605" data-permalink="https://nicisdigital.wordpress.com/2014/03/03/unix-domain-sockets-vs-loopback-tcp-sockets/rel-exe-time-vs-pkt-size/" data-orig-file="https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png" data-orig-size="964,562" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="Relative Execution Time vs Packet Size" data-image-description="" data-image-caption="" data-medium-file="https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png?w=300" data-large-file="https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png?w=510" alt="Relative Execution Time vs Packet Size" src="https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png?w=510&amp;h=297" width="510" height="297" srcset="https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png?w=510 510w, https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png?w=150 150w, https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png?w=300 300w, https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png?w=768 768w, https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png 964w" sizes="(max-width: 510px) 100vw, 510px"></a></p>
<p>Im not exactly sure why the efficiency of Unix domain sockets varies as it does compared to TCP sockets, but it is always better. This is simply because Unix domain sockets dont traverse the operating systems network stack. The kernel simply copies the data from the clients application into the file buffer in the servers application.</p>

	              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intuitively Understanding Harris Corner Detector (166 pts)]]></title>
            <link>https://comsci.blog/posts/intuitive-harris</link>
            <guid>37466302</guid>
            <pubDate>Mon, 11 Sep 2023 12:35:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://comsci.blog/posts/intuitive-harris">https://comsci.blog/posts/intuitive-harris</a>, See on <a href="https://news.ycombinator.com/item?id=37466302">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>If you ever tried to learn how the Harris corner detection algorithm works, you might have
noticed that the process is not intuitive at all. First, you start with an energy function, approximate it
using Taylor approximation, get a matrix from that, then find the eigenvalues of that matrix, etc.
But when you come to the final implementation, it is rather simple and seems easier.
If you are like me, this is not intuitive at all. But today I will present you a much easier way to understand
how the Harris corner detection algorithm works.</p>

<p>Lets start with understanding what is a corner. We can simply think of it as a connection of edges. For two edges
to be able to connect, they sure need to be not parallel, so looking at a corner, we should see that
edges moving in different directions (they would be parallel if they moved in the same directions):</p>

<p><img src="https://comsci.blog/assets/intuitive-harris-0.png" alt="" width="400"></p>

<p><em>Figure source: <a href="https://www.cse.psu.edu/~rtc12/CSE486/lecture06.pdf" target="_blank">https://www.cse.psu.edu/~rtc12/CSE486/lecture06.pdf</a></em></p>

<p>So it is obvious that the gradients of the image I<sub>x</sub> and I<sub>y</sub> will both be active in the corner
region. We know that adding I<sub>x</sub><sup>2</sup> and I<sub>y</sub><sup>2</sup> shows the regions with change
in x <strong><em>or</em></strong> y directions (which is the basis of the all edge detection algorithms). So one thing that comes to mind is multiplying the I<sub>x</sub><sup>2</sup>
and I<sub>y</sub><sup>2</sup> so that we will only see regions on the image that have a change in both x <strong><em>and</em></strong> y
directions at the same time, just like corners!</p>

<p>Lets start implementing this. First, lets find a pretty basic image that will have lots of corners inside it.</p>

<div><pre><code><span>import</span> <span>cv2</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>

<span># wget https://logowik.com/content/uploads/images/bbc-america9038.jpg -O assets/bbc.jpg
</span>
<span>img</span> <span>=</span> <span>cv2</span><span>.</span><span>imread</span><span>(</span><span>"assets/bbc.jpg"</span><span>,</span> <span>cv2</span><span>.</span><span>IMREAD_GRAYSCALE</span><span>)</span>
<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>img</span><span>,</span> <span>cmap</span><span>=</span><span>"gray"</span><span>)</span>
</code></pre></div>

<p><img src="https://comsci.blog/assets/intuitive-harris_1_1.png" alt="png"></p>

<p>Now we can start finding the gradient of this image using the Sobel operator:</p>

<div><pre><code><span>Ix</span> <span>=</span> <span>cv2</span><span>.</span><span>Sobel</span><span>(</span><span>img</span><span>,</span> <span>ddepth</span><span>=</span><span>cv2</span><span>.</span><span>CV_32F</span><span>,</span> <span>dx</span><span>=</span><span>1</span><span>,</span> <span>dy</span><span>=</span><span>0</span><span>)</span>
<span>Iy</span> <span>=</span> <span>cv2</span><span>.</span><span>Sobel</span><span>(</span><span>img</span><span>,</span> <span>ddepth</span><span>=</span><span>cv2</span><span>.</span><span>CV_32F</span><span>,</span> <span>dx</span><span>=</span><span>0</span><span>,</span> <span>dy</span><span>=</span><span>1</span><span>)</span>
</code></pre></div>

<p>Okay, we are there now. Lets plot the I<sub>x</sub><sup>2</sup>I<sub>y</sub><sup>2</sup>, we are expecting it to give us regions with both x and y directions:</p>

<div><pre><code><span>plt</span><span>.</span><span>imshow</span><span>(</span><span>Ix</span><span>**</span><span>2</span> <span>*</span> <span>Iy</span><span>**</span><span>2</span><span>,</span> <span>cmap</span><span>=</span><span>'gray'</span><span>)</span>
</code></pre></div>

<p><img src="https://comsci.blog/assets/intuitive-harris_5_1.png" alt="png"></p>

<p>As you can see, we are kind of not successful, because this shows us the both corners and edges that move along in
both x and y directions. But we need to get rid of the edges.</p>

<p>If you carefully look at this resulting image, you will notice that corners are either isolated like the top left
corner of the B logo, or they are at the end of these edges. Maybe we cant get rid of the edges directly,
but if somehow we can remove the corners from I<sub>x</sub><sup>2</sup>I<sub>y</sub><sup>2</sup>, we can subtract it from the original I<sub>x</sub><sup>2</sup>I<sub>y</sub><sup>2</sup> and
get only the corners. Actually, we can get rid of the corners. Since the corners are isolated in this image,
applying a Gaussian blur will decrease the intensities of the corners a lot!</p>

<p>Lets see this:</p>

<div><pre><code><span>corners_suppressed</span> <span>=</span> <span>cv2</span><span>.</span><span>GaussianBlur</span><span>(</span><span>Ix</span><span>**</span><span>2</span> <span>*</span> <span>Iy</span><span>**</span><span>2</span><span>,</span> <span>ksize</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>),</span> <span>sigmaX</span><span>=</span><span>1</span><span>)</span>
<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>corners_suppressed</span><span>,</span> <span>cmap</span><span>=</span><span>'gray'</span><span>)</span>
</code></pre></div>

<p><img src="https://comsci.blog/assets/intuitive-harris_7_1.png" alt="png"></p>

<p>We can even do a better job of removing the corners by applying the blur before squaring. Because square will increase
the intensity of isolated corners, making it less affected by the blur. So we can instead do:</p>

<div><pre><code><span>corners_suppressed</span> <span>=</span> <span>cv2</span><span>.</span><span>GaussianBlur</span><span>(</span><span>Ix</span><span>*</span> <span>Iy</span><span>,</span> <span>ksize</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>),</span> <span>sigmaX</span><span>=</span><span>1</span><span>)</span> <span>**</span> <span>2</span>
<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>corners_suppressed</span><span>,</span> <span>cmap</span><span>=</span><span>'gray'</span><span>)</span>
</code></pre></div>

<p><img src="https://comsci.blog/assets/intuitive-harris_9_1.png" alt="png"></p>

<p>Now that we have the corners mostly suppressed image, we can try subtracting this from the I<sub>x</sub><sup>2</sup>I<sub>y</sub><sup>2</sup> and get only the corners. Lets try it:</p>

<div><pre><code><span>plt</span><span>.</span><span>imshow</span><span>(</span><span>Ix</span><span>**</span><span>2</span> <span>*</span> <span>Iy</span><span>**</span><span>2</span> <span>-</span> <span>corners_suppressed</span><span>,</span> <span>cmap</span><span>=</span><span>'gray'</span><span>)</span>
</code></pre></div>

<p><img src="https://comsci.blog/assets/intuitive-harris_11_1.png" alt="png"></p>

<p>That doesnt seem to work, but the reason is clear. Edges of the I<sub>x</sub><sup>2</sup>I<sub>y</sub><sup>2</sup>
have different intensity than <code>corners_suppressed</code>, since <code>corners_suppressed</code> has been blurred.
We want them to have the same intensity in edges so that they cancel the edges when they are subtracted.</p>

<p>We can make the edges of I<sub>x</sub><sup>2</sup>I<sub>y</sub><sup>2</sup> similar intensity to edges of <code>corners_suppressed</code>
by applying Gaussian blur to I<sub>x</sub><sup>2</sup> and I<sub>y</sub><sup>2</sup> seperately before multiplying them.
We will apply the blur to squared gradients to make sure the corners are less affected by the blur.</p>

<div><pre><code><span>Ix_squared</span> <span>=</span> <span>cv2</span><span>.</span><span>GaussianBlur</span><span>(</span><span>Ix</span><span>**</span><span>2</span><span>,</span> <span>ksize</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>),</span> <span>sigmaX</span><span>=</span><span>1</span><span>)</span>
<span>Iy_squared</span> <span>=</span> <span>cv2</span><span>.</span><span>GaussianBlur</span><span>(</span><span>Iy</span><span>**</span><span>2</span><span>,</span> <span>ksize</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>),</span> <span>sigmaX</span><span>=</span><span>1</span><span>)</span>

<span>corners</span> <span>=</span> <span>Ix_squared</span> <span>*</span> <span>Iy_squared</span> <span>-</span> <span>corners_suppressed</span>
<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>corners</span><span>,</span> <span>cmap</span><span>=</span><span>'gray'</span><span>)</span>
</code></pre></div>
<p><img src="https://comsci.blog/assets/intuitive-harris_13_1.png" alt="png"></p>

<p>Yes! We successfully get the corners of the image. Now if we look at the data inside the <code>corners</code> matrix, you will notice that
corners have extremely large values and other parts have smaller values. Lets threshold it:</p>

<div><pre><code><span>corners</span><span>[</span><span>corners</span> <span>&lt;</span> <span>corners</span><span>.</span><span>max</span><span>()</span> <span>/</span> <span>5</span><span>]</span> <span>=</span> <span>0</span>
<span>corners</span><span>[</span><span>corners</span> <span>!=</span> <span>0</span><span>]</span> <span>=</span> <span>255</span>

<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>corners</span><span>,</span> <span>cmap</span><span>=</span><span>"gray"</span><span>)</span>
</code></pre></div>

<p><img src="https://comsci.blog/assets/intuitive-harris_15_1.png" alt="png"></p>

<p>Okay, lets plot these points as circles in our image:</p>

<div><pre><code><span>new_img</span> <span>=</span> <span>cv2</span><span>.</span><span>cvtColor</span><span>(</span><span>img</span><span>,</span> <span>cv2</span><span>.</span><span>COLOR_GRAY2RGB</span><span>)</span>

<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>img</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]):</span>
    <span>for</span> <span>j</span> <span>in</span> <span>range</span><span>(</span><span>img</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]):</span>
        <span>if</span> <span>corners</span><span>[</span><span>i</span><span>][</span><span>j</span><span>]</span> <span>==</span> <span>255</span><span>:</span>
            <span>cv2</span><span>.</span><span>circle</span><span>(</span><span>new_img</span><span>,</span> <span>(</span><span>j</span><span>,</span> <span>i</span><span>),</span> <span>radius</span><span>=</span><span>2</span><span>,</span> <span>color</span><span>=</span><span>(</span><span>255</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>),</span> <span>thickness</span><span>=-</span><span>1</span><span>)</span>

<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>new_img</span><span>,</span> <span>cmap</span><span>=</span><span>"gray"</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</code></pre></div>

<p><img src="https://comsci.blog/assets/intuitive-harris_17_0.png" alt="png"></p>

<p>And with this, we have implemented the Harris corner detection algorithm and we havent talked about things like
fitting ellipses, Taylor series approximation, or any of that stuff. This implementation is equivalent to the
other implementations of this algorithm.</p>

<p>Here is the full code:</p>
<div><pre><code><span>import</span> <span>cv2</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>

<span># wget https://logowik.com/content/uploads/images/bbc-america9038.jpg -O assets/bbc.jpg
</span>
<span>img</span> <span>=</span> <span>cv2</span><span>.</span><span>imread</span><span>(</span><span>"assets/bbc.jpg"</span><span>,</span> <span>cv2</span><span>.</span><span>IMREAD_GRAYSCALE</span><span>)</span>

<span>Ix</span> <span>=</span> <span>cv2</span><span>.</span><span>Sobel</span><span>(</span><span>img</span><span>,</span> <span>ddepth</span><span>=</span><span>cv2</span><span>.</span><span>CV_32F</span><span>,</span> <span>dx</span><span>=</span><span>1</span><span>,</span> <span>dy</span><span>=</span><span>0</span><span>)</span>
<span>Iy</span> <span>=</span> <span>cv2</span><span>.</span><span>Sobel</span><span>(</span><span>img</span><span>,</span> <span>ddepth</span><span>=</span><span>cv2</span><span>.</span><span>CV_32F</span><span>,</span> <span>dx</span><span>=</span><span>0</span><span>,</span> <span>dy</span><span>=</span><span>1</span><span>)</span>

<span>corners_suppressed</span> <span>=</span> <span>cv2</span><span>.</span><span>GaussianBlur</span><span>(</span><span>Ix</span><span>*</span> <span>Iy</span><span>,</span> <span>ksize</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>),</span> <span>sigmaX</span><span>=</span><span>1</span><span>)</span> <span>**</span> <span>2</span>
<span>Ix_squared</span> <span>=</span> <span>cv2</span><span>.</span><span>GaussianBlur</span><span>(</span><span>Ix</span><span>**</span><span>2</span><span>,</span> <span>ksize</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>),</span> <span>sigmaX</span><span>=</span><span>1</span><span>)</span>
<span>Iy_squared</span> <span>=</span> <span>cv2</span><span>.</span><span>GaussianBlur</span><span>(</span><span>Iy</span><span>**</span><span>2</span><span>,</span> <span>ksize</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>),</span> <span>sigmaX</span><span>=</span><span>1</span><span>)</span>

<span>corners</span> <span>=</span> <span>Ix_squared</span> <span>*</span> <span>Iy_squared</span> <span>-</span> <span>corners_suppressed</span>
<span>corners</span><span>[</span><span>corners</span> <span>&lt;</span> <span>corners</span><span>.</span><span>max</span><span>()</span> <span>/</span> <span>5</span><span>]</span> <span>=</span> <span>0</span>
<span>corners</span><span>[</span><span>corners</span> <span>!=</span> <span>0</span><span>]</span> <span>=</span> <span>255</span>

<span>new_img</span> <span>=</span> <span>cv2</span><span>.</span><span>cvtColor</span><span>(</span><span>img</span><span>,</span> <span>cv2</span><span>.</span><span>COLOR_GRAY2RGB</span><span>)</span>

<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>img</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]):</span>
    <span>for</span> <span>j</span> <span>in</span> <span>range</span><span>(</span><span>img</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]):</span>
        <span>if</span> <span>corners</span><span>[</span><span>i</span><span>][</span><span>j</span><span>]</span> <span>==</span> <span>255</span><span>:</span>
            <span>cv2</span><span>.</span><span>circle</span><span>(</span><span>new_img</span><span>,</span> <span>(</span><span>j</span><span>,</span> <span>i</span><span>),</span> <span>radius</span><span>=</span><span>2</span><span>,</span> <span>color</span><span>=</span><span>(</span><span>255</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>),</span> <span>thickness</span><span>=-</span><span>1</span><span>)</span>

<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>new_img</span><span>,</span> <span>cmap</span><span>=</span><span>"gray"</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</code></pre></div>
<p>Hopefully, you now understand how this algorithm works and enjoy the process.</p>

  </div>
</article>



      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beyond OpenAPI (144 pts)]]></title>
            <link>https://antonz.org/interactive-api-tutorials/</link>
            <guid>37466207</guid>
            <pubDate>Mon, 11 Sep 2023 12:26:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antonz.org/interactive-api-tutorials/">https://antonz.org/interactive-api-tutorials/</a>, See on <a href="https://news.ycombinator.com/item?id=37466207">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><header></header><p>Not all documentation is created equal. According to the popular classification, there are four document types: tutorials, how-to guides, technical references, and explanations.</p><div><p><img alt="Four types of documentation" src="https://antonz.org/interactive-api-tutorials/documentation.png"></p></div><p>OpenAPI, the de facto standard for documenting APIs, is a decent reference-style documentation (and client code generator, of course). But it can't serve as a good how-to or tutorial.</p><p>In this article, I will introduce a concise and readable way to write interactive tutorials and how-tos for any HTTP API (REST, RPC, or other style). And for that (surprise, surprise), we will rely on the HTTP protocol itself.</p><h2 id="a-crash-course-in-http-messages">A crash course in HTTP messages</h2><p>HTTP/1.x is a plain-text protocol that describes the communication between the client and the server. The client sends messages like this:</p><pre tabindex="0"><code>POST /anything/chat HTTP/1.1
host: httpbingo.org
content-type: application/json
user-agent: curl/7.87.0

{
    "message": "Hello!"
}
</code></pre><p>And receives messages like this in response:</p><pre tabindex="0"><code>HTTP/1.1 200 OK
date: Mon, 28 Aug 2023 07:51:49 GMT
content-type: application/json

{
    "message": "Hi!"
}
</code></pre><blockquote><p>HTTP/2, the successor to HTTP/1.1, is a binary protocol. However, all tools (such as the browser devtools or curl) display HTTP/2 messages in plain text (just like HTTP/1.1), so we can safely ignore this fact for our purposes.</p></blockquote><div><figure><img alt="HTTP request and response" src="https://antonz.org/interactive-api-tutorials/http-messages.png"><figcaption>It's easy to read HTTP requests and responses once you get used to it.</figcaption></figure></div><p><strong>HTTP request</strong> consists of three main sections:</p><ol><li>Request line:</li></ol><pre tabindex="0"><code>POST /anything/chat HTTP/1.1
</code></pre><ul><li>The <em>method</em> (<code>POST</code>) defines the operation the client wants to perform.</li><li>The <em>path</em> (<code>/anything/chat</code>) is the URL of the requested resource (without the protocol, domain and port).</li><li>The <em>version</em> (<code>HTTP/1.1</code>) indicates the version of the HTTP protocol.</li></ul><ol start="2"><li>Request headers:</li></ol><pre tabindex="0"><code>host: httpbingo.org
content-type: application/json
user-agent: curl/7.87.0
</code></pre><p>Each header is a key-value pair that tells the server some useful information about the request. In our case it's the hostname of the server (<code>httpbingo.org</code>), the type of the content (<code>application/json</code>) and the client's self-identification (<code>user-agent</code>).</p><ol start="3"><li>Request body:</li></ol><pre tabindex="0"><code>{
    "message": "Hello!"
}
</code></pre><p>The actual data that the client sends to the server.</p><p>The HTTP protocol is stateless, so any state must be contained within the request itself, either in the headers or in the body.</p><p><strong>HTTP response</strong> also consists of three main sections:</p><ol><li>Status line:</li></ol><pre tabindex="0"><code>HTTP/1.1 200 OK
</code></pre><ul><li>The <em>version</em> (<code>HTTP/1.1</code>) indicates the version of the HTTP protocol.</li><li>The <em>status code</em> (<code>200</code>) tells whether the request was successful or not, and why (there are many status codes for <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Status">different situations</a>).</li><li>The <em>status message</em> is a human-readable description of the status code. HTTP/2 does not have it.</li></ul><ol start="2"><li>Response headers:</li></ol><pre tabindex="0"><code>date: Mon, 28 Aug 2023 07:51:49 GMT
content-type: application/json
</code></pre><p>Similar to request headers, these provide useful information about the response to the client.</p><ol start="3"><li>Response body:</li></ol><pre tabindex="0"><code>{
    "message": "Hi!"
}
</code></pre><p>The actual data that the server sends to the client.</p><p>There is much more to the HTTP protocol, but this basic knowledge is enough to cover most of API use cases. So let's move on.</p><h2 id="using-http-to-document-api-usage">Using HTTP to document API usage</h2><p>We are going to take an HTTP request:</p><pre tabindex="0"><code>POST /anything/chat HTTP/1.1
host: httpbingo.org
content-type: application/json
user-agent: curl/7.87.0

{
    "message": "Hello!"
}
</code></pre><p>And modify it just a little bit:</p><ul><li>include the full URL in the request line instead of the path;</li><li>remove the protocol version.</li></ul><pre tabindex="0"><code>POST http://httpbingo.org/anything/chat
content-type: application/json

{
    "message": "Hello!"
}
</code></pre><p>This format is perfect for API usage examples. It's concise and readable, yet formal enough to be executed programmatically (directly from the documentation, as we'll see shortly).</p><h2 id="writing-an-interactive-api-guide">Writing an interactive API guide</h2><p>Instead of telling you how to write an interactive API tutorial, I'm going to show you one. We'll use <a href="https://docs.github.com/en/rest/gists/gists">Gists API</a> as an example. It's a compact and useful GitHub service for storing code snippets (called "gists").</p><div><figure><img alt="GitHub Gists" src="https://antonz.org/interactive-api-tutorials/gists.png"><figcaption>Gists are quite handy when a full-blown Git repository is too much.</figcaption></figure></div><p>Even if you are not a GitHub user, you still have access to the Gists API.</p><h3 id="reading-gists">Reading gists</h3><p>Let's take a look at the <strong>public gists</strong> of my pal Redowan (user <code>rednafi</code>). The response can be quite chatty, so we'll only select the 3 most recent (<code>per_page = 3</code>):</p><pre tabindex="0"><code>GET https://api.github.com/users/rednafi/gists?per_page=3
accept: application/json
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><p>A family of non-standard <code>x-ratelimit</code> headers tell us how GitHub <strong>limits</strong> our requests:</p><ul><li>There is a total number of <code>x-ratelimit-limit</code> requests available per hour.</li><li>We've already used <code>x-ratelimit-used</code> requests.</li><li>So there are <code>x-ratelimit-remaining</code> requests left.</li></ul><p>We need to keep an eye on these to make sure we don't exceed the quota.</p><p>We can use a combination of <code>page</code> and <code>per_page</code> query parameters to select a <strong>slice of gists</strong>. For example, here are gists 10-15:</p><pre tabindex="0"><code>GET https://api.github.com/users/rednafi/gists?page=3&amp;per_page=5
accept: application/json
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><p>Note that GitHub provides navigation links in the <code>link</code> header:</p><pre tabindex="0"><code>link:
    &lt;https://api.github.com/user/30027932/gists?page=2&amp;per_page=5&gt;; rel="prev",
    &lt;https://api.github.com/user/30027932/gists?page=4&amp;per_page=5&gt;; rel="next",
    &lt;https://api.github.com/user/30027932/gists?page=7&amp;per_page=5&gt;; rel="last",
    &lt;https://api.github.com/user/30027932/gists?page=1&amp;per_page=5&gt;; rel="first"
</code></pre><p>That's thoughtful of them!</p><p>Okay, now let's take a look at the <strong>specific gist</strong> with id <code>88242fd822603290255877e396664ba5</code> (this one is mine; let's not bother Redowan anymore):</p><pre tabindex="0"><code>GET https://api.github.com/gists/88242fd822603290255877e396664ba5
accept: application/json
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><p>We can see that there is a <code>greet.py</code> file written in the Python <code>language</code> with a certain <code>content</code>:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>Greeter</span>:
</span></span><span><span>    <span>def</span> <span>__init__</span>(<span>self</span>, <span>greeting</span>):
</span></span><span><span>        <span>self</span><span>.</span><span>greeting</span> <span>=</span> <span>greeting</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>greet</span>(<span>self</span>, <span>who</span>):
</span></span><span><span>        <span>print</span>(<span>f</span><span>"</span><span>{</span><span>self</span><span>.</span><span>greeting</span><span>}</span><span>, </span><span>{</span><span>who</span><span>}</span><span>!"</span>)
</span></span><span><span>
</span></span><span><span><span>gr</span> <span>=</span> <span>Greeter</span>(<span>"Hello"</span>)
</span></span><span><span><span>gr</span><span>.</span><span>greet</span>(<span>"world"</span>)
</span></span></code></pre></div><codapi-snippet sandbox="python" editor="basic"></codapi-snippet><p><em>(yep, you can also create interactive Python examples!)</em></p><p>Interestingly, the gist has a <code>history</code>. It appears that every time you edit a gist, GitHub creates a new version, while also keeping previous versions.</p><p>Let's get the <strong>earliest revision</strong>, which has a <code>version</code> = <code>4c10d27cfb163d654745f1d72f2c7ce14225b83b</code> (a bit long, I know):</p><pre tabindex="0"><code>GET https://api.github.com/gists/88242fd822603290255877e396664ba5/4c10d27cfb163d654745f1d72f2c7ce14225b83b
accept: application/json
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><p>The code in the gist was much simpler back then:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>msg</span> <span>=</span> <span>"Hello, world!"</span>
</span></span><span><span><span>print</span>(<span>msg</span>)
</span></span></code></pre></div><codapi-snippet sandbox="python" editor="basic"></codapi-snippet><h3 id="modifying-gists">Modifying gists</h3><p>Okay, so we know how to list gists for a user, how to get a specific gist, and even how to get a specific revision. Now let's <strong>create a new gist</strong>!</p><pre tabindex="0"><code>POST https://api.github.com/gists
content-type: application/json
accept: application/json

{
    "description": "Greetings in Markdown",
    "public": true,
    "files":{
        "README.md":{
            "content":"Hello, world!"
        }
    }
}
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><p>What's that? We have a <code>401 Unauthorized</code> error. The response body explains: "requires authentication" and even provides a link to the documentation (oh, I just love GitHub APIs).</p><p>Understandably, GitHub does not allow anonymous users to create new gists. We have to authenticate with an API token.</p><blockquote><p>If you want the following examples to work, enter your API token in the field below. You can create one with a 'gist' scope in the <a href="https://github.com/settings/tokens">GitHub settings</a>.</p><p>After you enter the token below, it will be stored locally in the browser and will not be sent anywhere (except to the GitHub API when you click the Run button).</p></blockquote><p>Let's try again, this time with an <code>authorization</code> header.</p><p>Note the <code>public</code> parameter. The service supports "secret" gists (<code>public = false</code>), but it's the "security by obscurity" type of secrecy. Secret gists do not show up in the "GET gists" API method, but they are still accessible by id, even by anonymous users.</p><pre tabindex="0"><code data-lang="authenticated">POST https://api.github.com/gists
content-type: application/json
accept: application/json
authorization: bearer {token}

{
    "description": "Greetings in Markdown",
    "public": true,
    "files":{
        "README.md":{
            "content":"Hello, world!"
        }
    }
}
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><details><summary>I don't have a token, just show me the results</summary><pre><code>HTTP/1.1 201 
cache-control: private, max-age=60, s-maxage=60
content-length: 3758
content-type: application/json; charset=utf-8
etag: "819f6b4f728843abcb50ad63da200a4c110245585b3eb1c0f59a5ebe86c8ecf5"
location: https://api.github.com/gists/b17474320a629af38255c0a6efbc72b9
x-accepted-oauth-scopes: 
x-github-media-type: github.v3
x-github-request-id: E8B5:8EDA:511F73:51AC33:64EE0266
x-oauth-scopes: gist
x-ratelimit-limit: 5000
x-ratelimit-remaining: 4997
x-ratelimit-reset: 1693323114
x-ratelimit-resource: core
x-ratelimit-used: 3

{
  "url": "https://api.github.com/gists/b17474320a629af38255c0a6efbc72b9",
  "forks_url": "https://api.github.com/gists/b17474320a629af38255c0a6efbc72b9/forks",
  "commits_url": "https://api.github.com/gists/b17474320a629af38255c0a6efbc72b9/commits",
  "id": "b17474320a629af38255c0a6efbc72b9",
  "node_id": "G_kwDOACz0htoAIGIxNzQ3NDMyMGE2MjlhZjM4MjU1YzBhNmVmYmM3MmI5",
  "git_pull_url": "https://gist.github.com/b17474320a629af38255c0a6efbc72b9.git",
  "git_push_url": "https://gist.github.com/b17474320a629af38255c0a6efbc72b9.git",
  "html_url": "https://gist.github.com/nalgeon/b17474320a629af38255c0a6efbc72b9",
  "files": {
    "README.md": {
      "filename": "README.md",
      "type": "text/markdown",
      "language": "Markdown",
      "raw_url": "https://gist.githubusercontent.com/nalgeon/b17474320a629af38255c0a6efbc72b9/raw/5dd01c177f5d7d1be5346a5bc18a569a7410c2ef/README.md",
      "size": 13,
      "truncated": false,
      "content": "Hello, world!"
    }
  },
  ...
}</code></pre></details><p>HTTP status <code>201 Created</code> means that a new gist has been created as a result of our request.</p><p>Okay, now we can <strong>update a gist</strong> using its <code>id</code> (don't forget to replace the <code>{gist_id}</code> in the request line with the actual <code>id</code> value):</p><pre tabindex="0"><code data-lang="authenticated">PATCH https://api.github.com/gists/{gist_id}
content-type: application/json
accept: application/json
authorization: bearer {token}

{
    "description": "Greetings in Markdown",
    "public": true,
    "files":{
        "README.md":{
            "content":"Hola, mundo!"
        }
    }
}
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><details><summary>I don't have a token, just show me the results</summary><pre><code>HTTP/1.1 200 
cache-control: private, max-age=60, s-maxage=60
content-type: application/json; charset=utf-8
etag: W/"989eaec7cdb50ba6441e77ea2defba257b98a535f26c2ba6062f152ceffb2d77"
x-accepted-oauth-scopes: 
x-github-media-type: github.v3
x-github-request-id: E8B5:8EDA:5188AA:52163F:64EE027F
x-oauth-scopes: gist
x-ratelimit-limit: 100
x-ratelimit-remaining: 98
x-ratelimit-reset: 1693323129
x-ratelimit-resource: gist_update
x-ratelimit-used: 2

{
  "url": "https://api.github.com/gists/b17474320a629af38255c0a6efbc72b9",
  "forks_url": "https://api.github.com/gists/b17474320a629af38255c0a6efbc72b9/forks",
  "commits_url": "https://api.github.com/gists/b17474320a629af38255c0a6efbc72b9/commits",
  "id": "b17474320a629af38255c0a6efbc72b9",
  "node_id": "G_kwDOACz0htoAIGIxNzQ3NDMyMGE2MjlhZjM4MjU1YzBhNmVmYmM3MmI5",
  "git_pull_url": "https://gist.github.com/b17474320a629af38255c0a6efbc72b9.git",
  "git_push_url": "https://gist.github.com/b17474320a629af38255c0a6efbc72b9.git",
  "html_url": "https://gist.github.com/nalgeon/b17474320a629af38255c0a6efbc72b9",
  "files": {
    "README.md": {
      "filename": "README.md",
      "type": "text/markdown",
      "language": "Markdown",
      "raw_url": "https://gist.githubusercontent.com/nalgeon/b17474320a629af38255c0a6efbc72b9/raw/95975f3d0bac707ce4355dfc4a7955310d212fac/README.md",
      "size": 14,
      "truncated": false,
      "content": "Hola, mundo!"
    }
  },
  ...
}</code></pre></details><p>It now greets us in Spanish </p><p>Very good. Finally, let's <strong>delete a gist</strong>:</p><pre tabindex="0"><code data-lang="authenticated">DELETE https://api.github.com/gists/{gist_id}
accept: application/json
authorization: bearer {token}
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><details><summary>I don't have a token, just show me the results</summary><pre><code>HTTP/1.1 204 
x-accepted-oauth-scopes: 
x-github-media-type: github.v3
x-github-request-id: E8B5:8EDA:51E584:5273CC:64EE027F
x-oauth-scopes: gist
x-ratelimit-limit: 5000
x-ratelimit-remaining: 4996
x-ratelimit-reset: 1693323114
x-ratelimit-resource: core
x-ratelimit-used: 4</code></pre></details><p>HTTP status <code>204 No Content</code> means we deleted the gist, so GitHub has nothing more to tell us about it. It's a little sad to see it go, but we can always make another one, right?</p><p>The Gists API has other useful features, but they are beyond the scope of this tutorial. Here are the functions we've covered:</p><ul><li>List user gists.</li><li>Get a specific gist, or a specific revision of a gist.</li><li>Create a new gist.</li><li>Update an existing gist.</li><li>Delete a gist.</li></ul><p>Now try managing your gists! You can always use this article as a playground.</p><h2 id="implementation">Implementation</h2><p>To run the API examples as we did in the previous section, you'll need a bit of JavaScript that does the following:</p><ol><li>Parses the HTTP request example.</li><li>Calls the API.</li><li>Displays the result.</li></ol><div><figure><img alt="Fetch API playground" src="https://antonz.org/interactive-api-tutorials/playground.png"><figcaption>It's always nice when a playground doesn't need a server.</figcaption></figure></div><p>Since we've limited ourselves to a small subset of HTTP request capabilities, parsing is fairly easy:</p><div><pre tabindex="0"><code data-lang="js"><span><span><span>// parse parses the request specification.
</span></span></span><span><span><span></span><span>function</span> <span>parse</span>(<span>text</span>) {
</span></span><span><span>    <span>const</span> <span>lines</span> <span>=</span> <span>text</span>.<span>split</span>(<span>"\n"</span>);
</span></span><span><span>    <span>let</span> <span>lineIdx</span> <span>=</span> <span>0</span>;
</span></span><span><span>
</span></span><span><span>    <span>// parse method and URL
</span></span></span><span><span><span></span>    <span>const</span> <span>methodUrl</span> <span>=</span> <span>lines</span>[<span>0</span>].<span>split</span>(<span>" "</span>).<span>filter</span>((<span>s</span>) =&gt; <span>s</span>);
</span></span><span><span>    <span>const</span> [<span>method</span>, <span>url</span>] <span>=</span>
</span></span><span><span>        <span>methodUrl</span>.<span>length</span> <span>&gt;=</span> <span>2</span> <span>?</span> <span>methodUrl</span> <span>:</span> [<span>"GET"</span>, <span>methodUrl</span>[<span>0</span>]];
</span></span><span><span>    <span>lineIdx</span> <span>+=</span> <span>1</span>;
</span></span><span><span>
</span></span><span><span>    <span>// parse headers
</span></span></span><span><span><span></span>    <span>const</span> <span>headers</span> <span>=</span> {};
</span></span><span><span>    <span>for</span> (<span>let</span> <span>i</span> <span>=</span> <span>lineIdx</span>; <span>i</span> <span>&lt;</span> <span>lines</span>.<span>length</span>; <span>i</span><span>++</span>) {
</span></span><span><span>        <span>const</span> <span>line</span> <span>=</span> <span>lines</span>[<span>i</span>].<span>trim</span>();
</span></span><span><span>        <span>if</span> (<span>line</span> <span>===</span> <span>""</span>) {
</span></span><span><span>            <span>break</span>;
</span></span><span><span>        }
</span></span><span><span>        <span>const</span> [<span>headerName</span>, <span>headerValue</span>] <span>=</span> <span>line</span>.<span>split</span>(<span>":"</span>);
</span></span><span><span>        <span>headers</span>[<span>headerName</span>.<span>trim</span>()] <span>=</span> <span>headerValue</span>.<span>trim</span>();
</span></span><span><span>        <span>lineIdx</span> <span>+=</span> <span>1</span>;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>// parse body
</span></span></span><span><span><span></span>    <span>const</span> <span>body</span> <span>=</span> <span>lines</span>.<span>slice</span>(<span>lineIdx</span> <span>+</span> <span>1</span>).<span>join</span>(<span>"\n"</span>);
</span></span><span><span>
</span></span><span><span>    <span>return</span> { <span>method</span>, <span>url</span>, <span>headers</span>, <span>body</span> };
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>const</span> <span>spec</span> <span>=</span> <span>parse</span>(<span>`GET https://httpbingo.org/uuid`</span>);
</span></span><span><span><span>console</span>.<span>log</span>(<span>JSON</span>.<span>stringify</span>(<span>spec</span>, <span>null</span>, <span>2</span>));
</span></span></code></pre></div><codapi-snippet sandbox="javascript" editor="basic"></codapi-snippet><p>Calling the API and displaying the results is trivial  just use the Fetch API and display the result as plain text:</p><div><pre tabindex="0"><code data-lang="js"><span><span><span>// execCode sends an HTTP request according to the spec
</span></span></span><span><span><span>// and returns the response as text with status, headers and body.
</span></span></span><span><span><span></span><span>async</span> <span>function</span> <span>execCode</span>(<span>spec</span>) {
</span></span><span><span>    <span>const</span> <span>resp</span> <span>=</span> <span>await</span> <span>sendRequest</span>(<span>spec</span>);
</span></span><span><span>    <span>const</span> <span>text</span> <span>=</span> <span>await</span> <span>responseText</span>(<span>resp</span>);
</span></span><span><span>    <span>return</span> <span>text</span>;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// sendRequest sends an HTTP request according to the spec.
</span></span></span><span><span><span></span><span>async</span> <span>function</span> <span>sendRequest</span>(<span>spec</span>) {
</span></span><span><span>    <span>const</span> <span>options</span> <span>=</span> {
</span></span><span><span>        <span>method</span><span>:</span> <span>spec</span>.<span>method</span>,
</span></span><span><span>        <span>headers</span><span>:</span> <span>spec</span>.<span>headers</span>,
</span></span><span><span>        <span>body</span><span>:</span> <span>spec</span>.<span>body</span> <span>||</span> <span>undefined</span>,
</span></span><span><span>    };
</span></span><span><span>    <span>return</span> <span>await</span> <span>fetch</span>(<span>spec</span>.<span>url</span>, <span>options</span>);
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// responseText returns the response as text
</span></span></span><span><span><span>// with status, headers and body.
</span></span></span><span><span><span></span><span>async</span> <span>function</span> <span>responseText</span>(<span>resp</span>) {
</span></span><span><span>    <span>const</span> <span>version</span> <span>=</span> <span>"HTTP/1.1"</span>;
</span></span><span><span>    <span>const</span> <span>text</span> <span>=</span> <span>await</span> <span>resp</span>.<span>text</span>();
</span></span><span><span>    <span>const</span> <span>messages</span> <span>=</span> [<span>`</span><span>${</span><span>version</span><span>}</span><span> </span><span>${</span><span>resp</span>.<span>status</span><span>}</span><span> </span><span>${</span><span>resp</span>.<span>statusText</span><span>}</span><span>`</span>];
</span></span><span><span>    <span>for</span> (<span>const</span> <span>hdr</span> <span>of</span> <span>resp</span>.<span>headers</span>.<span>entries</span>()) {
</span></span><span><span>        <span>messages</span>.<span>push</span>(<span>`</span><span>${</span><span>hdr</span>[<span>0</span>]<span>}</span><span>: </span><span>${</span><span>hdr</span>[<span>1</span>]<span>}</span><span>`</span>);
</span></span><span><span>    }
</span></span><span><span>    <span>if</span> (<span>text</span>) {
</span></span><span><span>        <span>messages</span>.<span>push</span>(<span>""</span>, <span>text</span>);
</span></span><span><span>    }
</span></span><span><span>    <span>return</span> <span>messages</span>.<span>join</span>(<span>"\n"</span>);
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>const</span> <span>spec</span> <span>=</span> {
</span></span><span><span>    <span>method</span><span>:</span> <span>"GET"</span>,
</span></span><span><span>    <span>url</span><span>:</span> <span>"https://httpbingo.org/uuid"</span>,
</span></span><span><span>};
</span></span><span><span>
</span></span><span><span><span>const</span> <span>text</span> <span>=</span> <span>await</span> <span>execCode</span>(<span>spec</span>);
</span></span><span><span><span>console</span>.<span>log</span>(<span>text</span>);
</span></span></code></pre></div><codapi-snippet sandbox="javascript" editor="basic"></codapi-snippet><p>Fetch API works in the browser, so there is no intermediate server involved. The only nuance is that the documentation must either be on the same domain as the API itself, or the API must allow cross-domain requests. But even if that's not the case, you can always proxy the requests  it's not too much work.</p><p>If you want an out-of-the-box solution, I've written a simple library that supports both JavaScript and Fetch API playgrounds:</p><p><a href="https://github.com/nalgeon/codapi-js"><strong><code>codapi-js</code></strong></a></p><p>Ideally, I'd like most documentation to be interactive. Not just API guides, but everything from algorithms (like <a href="https://samwho.dev/hashing/">hashing</a>) to programming languages (like <a href="https://antonz.org/go-1-21-builtins/">Go</a> or <a href="https://antonz.org/trying-odin/">Odin</a>) to databases (like <a href="https://antonz.org/sql-compare-neighbors/">SQLite</a>), frameworks and tools, and even individual packages.</p><p>And (shameless plug here!) I'm building a platform that allows just that  easily embeddable code playgrounds for documentation, online education, and fun. Check it out if you are interested:</p><p><a href="https://codapi.org/"><strong><code>codapi</code></strong></a></p><p>And please try to write an interactive guide the next time you develop an API!</p>

<p><em><a href="https://antonz.org/subscribe/"><i></i>&nbsp;<strong>Subscribe</strong></a>
to keep up with new posts.</em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A LLM+OLAP Solution (103 pts)]]></title>
            <link>https://doris.apache.org/zh-CN/blog/Tencent-LLM/</link>
            <guid>37466182</guid>
            <pubDate>Mon, 11 Sep 2023 12:23:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://doris.apache.org/zh-CN/blog/Tencent-LLM/">https://doris.apache.org/zh-CN/blog/Tencent-LLM/</a>, See on <a href="https://news.ycombinator.com/item?id=37466182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container" itemprop="articleBody"><p>Six months ago, I wrote about <a href="https://doris.apache.org/blog/Tencent%20Music/" target="_blank" rel="noopener noreferrer">why we replaced ClickHouse with Apache Doris as an OLAP engine</a> for our data management system. Back then, we were struggling with the auto-generation of SQL statements. As days pass, we have made progresses big enough to be references for you (I think), so here I am again. </p><p>We have adopted Large Language Models (LLM) to empower our Doris-based OLAP services.</p><h2 id="llm--olap">LLM + OLAP<a href="#llm--olap" aria-label="LLM + OLAP" title="LLM + OLAP"></a></h2><p>Our incentive was to save our internal staff from the steep learning curve of SQL writing. Thus, we used LLM as an intermediate. It transforms natural language questions into SQL statements and sends the SQLs to the OLAP engine for execution.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_1-6672112c0d09d75171d8ed9a749ff196.png" width="1280" height="253"></p><p>Like every AI-related experience, we came across some friction:</p><ol><li>LLM does not understand data jargons, like "fields", "rows", "columns" and "tables". Instead, they can perfectly translate business terms like "corporate income" and "DAU", which are basically what the fields/rows/columns are about. That means it can work well only if the analysts use the exact right word to refer to the metric they need when typing their questions.</li><li>The LLM we are using is slow in inference. It takes over 10 seconds to respond. As it charges fees by token, cost-effectiveness becomes a problem.</li><li>Although the LLM is trained on a large collection of public datasets, it is under-informed of niche knowledge. In our case, the LLM is super unfamiliar with indie songs, so even if the songs are included in our database, the LLM will not able to identify them properly. </li><li>Sometimes our input questions require adequate and latest legal, political, financial, and regulatory information, which is hard to be included in a training dataset or knowledge base. We need to connect the LLM to wider info bases in order to perform more diversified tasks.</li></ol><p>We knock these problems down one by one.</p><h3 id="1-a-semantic-layer">1. A semantic layer<a href="#1-a-semantic-layer" aria-label="1. A semantic layer" title="1. A semantic layer"></a></h3><p>For problem No.1, we introduce a semantic layer between the LLM and the OLAP engine. This layer translates business terms into the corresponding data fields. It can identify data filtering conditions from the various natural language wordings, relate them to the metrics involved, and then generate SQL statements. </p><p>Besides that, the semantic layer can optimize the computation logic. When analysts input a question that involves a complicated query, let's say, a multi-table join, the semantic layer can split that into multiple single-table queries to reduce semantic distortion.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_2-bb2fdaed64ef15214c0542204dd45832.png" width="1280" height="289"></p><h3 id="2-llm-parsing-rules">2. LLM parsing rules<a href="#2-llm-parsing-rules" aria-label="2. LLM parsing rules" title="2. LLM parsing rules"></a></h3><p>To increase cost-effectiveness in using LLM, we evaluate the computation complexity of all scenarios, such as metric computation, detailed record retrieval, and user segmentation. Then, we create rules and dedicate the LLM-parsing step to only complicated tasks. That means for the simple computation tasks, it will skip the parsing. </p><p>For example, when an analyst inputs "tell me the earnings of the major musical platforms", the LLM identifies that this question only entails several metrics or dimensions, so it will not further parse it but send it straight for SQL generation and execution. This can largely shorten query response time and reduce API expenses. </p><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_3-3ab023081e1acb069d34a4ce24aef010.png" width="1280" height="406"></p><h3 id="3-schema-mapper-and-external-knowledge-base">3. Schema Mapper and external knowledge base<a href="#3-schema-mapper-and-external-knowledge-base" aria-label="3. Schema Mapper and external knowledge base" title="3. Schema Mapper and external knowledge base"></a></h3><p>To empower the LLM with niche knowledge, we added a Schema Mapper upstream from the LLM. The Schema Mapper maps the input question to an external knowledge base, and then the LLM will do parsing.</p><p>We are constantly testing and optimizing the Schema Mapper. We categorize and rate content in the external knowledge base, and do various levels of mapping (full-text mapping and fuzzy mapping) to enable better semantic parsing.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_4-261ee680cf77335b25f32e41d7a4924b.png" width="2001" height="647"></p><h3 id="4-plugins">4. Plugins<a href="#4-plugins" aria-label="4. Plugins" title="4. Plugins"></a></h3><p>We used plugins to connect the LLM to more fields of information, and we have different integration methods for different types of plugins:</p><ul><li><strong>Embedding local files</strong>: This is especially useful when we need to "teach" the LLM the latest regulatory policies, which are often text files. Firstly, the system vectorizes the local text file, executes semantic searches to find matching or similar terms in the local file, extracts the relevant contents and puts them into the LLM parsing window to generate output. </li><li><strong>Third-party plugins</strong>: The marketplace is full of third-party plugins that are designed for all kinds of sectors. With them, the LLM is able to deal with wide-ranging topics. Each plugin has its own prompts and calling function. Once the input question hits a prompt, the relevant plugin will be called.</li></ul><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_5-70a170e771dd9eadcc1488b94d892478.png" width="2001" height="645"></p><p>After we are done with above four optimizations, the SuperSonic framework comes into being.</p><h2 id="the-supersonic-framework">The SuperSonic framework<a href="#the-supersonic-framework" aria-label="The SuperSonic framework" title="The SuperSonic framework"></a></h2><p>Now let me walk you through this <a href="https://github.com/tencentmusic/supersonic" target="_blank" rel="noopener noreferrer">framework</a>:</p><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_6-cbbbb25041c807376b2b9d14609e82c8.png" width="1280" height="1117"></p><ul><li>An analyst inputs a question.</li><li>The Schema Mapper maps the question to an external knowledge base.</li><li>If there are matching fields in the external knowledge base, the question will not be parsed by the LLM. Instead, a metric computation formula will trigger the OLAP engine to start querying. If there is no matching field, the question will enter the LLM.</li><li>Based on the pre-defined rules, the LLM rates the complexity level of the question. If it is a simple query, it will go directly to the OLAP engine; if it is a complicated query, it will be semantically parsed and converted to a DSL statement.</li><li>At the Semantic Layer, the DSL statement will be split based on its query scenario. For example, if it is a multi-table join query, this layer will generate multiple single-table query SQL statements.</li><li>If the question involves external knowledge, the LLM will call a third-party plugin.</li></ul><p><strong>Example</strong></p><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_7-c20b3cc2b0b00b32bc2825c1d62b1d5d.png" width="2001" height="1126"></p><p>To answer whether a certain song can be performed on variety shows, the system retrieves the OLAP data warehouse for details about the song, and presents it with results from the Commercial Use Query third-party plugin.</p><h2 id="olap-architecture">OLAP Architecture<a href="#olap-architecture" aria-label="OLAP Architecture" title="OLAP Architecture"></a></h2><p>As for the OLAP part of this framework, after several rounds of architectural evolution, this is what our current OLAP pipeline looks like. </p><p>Raw data is sorted into tags and metrics, which are custom-defined by the analysts. The tags and metrics are under unified management in order to avoid inconsistent definitions. Then, they are combined into various tagsets and metricsets for various queries. </p><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_8-6d517a787c782510bf3869176730ce3a.png" width="1709" height="1119"></p><p>We have drawn two main takeaways for you from our architectural optimization experience.</p><p><strong>1. Streamline the links</strong></p><p>Before we adopted Apache Doris, we used to have ClickHouse to accelerate the computation of tags and metrics, and Elasticsearch to process dimensional data. That's two analytic engines and requires us to adapt the query statements to both of them. It was high-maintenance.</p><p>Thus, we replaced ClickHouse with Apache Doris, and utilized the <a href="https://doris.apache.org/docs/dev/lakehouse/multi-catalog/es" target="_blank" rel="noopener noreferrer">Elasticsearch Catalog</a> functionality to connect Elasticsearch data to Doris. In this way, we make Doris our unified query gateway. </p><p><strong>2. Split the flat tables</strong></p><p>In early versions of our OLAP architecture, we used to put data into flat tables, which made things tricky. For one thing, flat tables absorbed all the writing latency from upstreams, and that added up to considerable loss in data realtimeliness. For another, 50% of data in a flat table was dimensional data, which was rarely updated. With every new flat table came some bulky dimensional data that consumed lots of storage space. </p><p>Therefore, we split the flat tables into metric tables and dimension tables. As they are updated in different paces, we put them into different data models.</p><ul><li><strong>Metric tables</strong>: We arrange metric data in the Aggregate Key model of Apache Doris, which means new data will be merged with the old data by way of SUM, MAX, MIN, etc.</li><li><strong>Dimension tables</strong>: These tables are in the Unique Key model of Apache Doris, which means new data record will replace the old. This can greatly increase performance in our query scenarios.</li></ul><p>You might ask, does this cause trouble in queries, since most queries require data from both types of tables? Don't worry, we address that with the Rollup feature of Doris. On the basis of the base tables, we can select the dimensions we need to create Rollup views, which will automatically execute <code>GROUP BY</code>. This relieves us of the need to define tags for each Rollup view and largely speed up queries.</p><h2 id="other-tricks">Other Tricks<a href="#other-tricks" aria-label="Other Tricks" title="Other Tricks"></a></h2><p>In our experience with Apache Doris, we also find some other functionalities handy, so I list them here for you, too:</p><p><strong>1. Materialized View</strong></p><p>A Materialized View is a pre-computed dataset. It is a way to accelerate queries when you frequently need to access data of certain dimensions. In these scenarios, we define derived tags and metrics based on the original ones. For example, we create a derived metric by combining Metric 1, Metric 2, and Metric 3: <code>sum(m1+m2+m3)</code>. Then, we can create a Materialized View for it. According to the Doris release schedule, version 2.1 will support multi-table Materialized Views, and we look forward to that.</p><p><strong>2. Flink-Doris-Connector</strong></p><p>This is for Exactly-Once guarantee in data ingestion. The Flink-Doris-Connector implements a checkpoint mechanism and two-stage commit, and allows for auto data synchronization from relational databases to Doris.</p><p><strong>3. Compaction</strong></p><p>When the number of aggregation tasks or data volume becomes overwhelming for Flink, there might be huge latency in data compaction. We solve that with Vertical Compaction and Segment Compaction. Vertical Compaction supports loading of only part of the columns, so it can reduce storage consumption when compacting flat tables. Segment Compaction can avoid generating too much segments during data writing, and allows for compaction while writing simultaneously.   </p><h2 id="whats-next">What's Next<a href="#whats-next" aria-label="What's Next" title="What's Next"></a></h2><p>With an aim to reduce costs and increase service availability, we plan to test the newly released Storage-Compute Separation and Cross-Cluster Replication of Doris, and we embrace any ideas and inputs about the SuperSonic framework and the Apache Doris project.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Networking for introverts (141 pts)]]></title>
            <link>https://www.economist.com/business/2023/09/07/networking-for-introverts-a-how-to-guide</link>
            <guid>37466147</guid>
            <pubDate>Mon, 11 Sep 2023 12:20:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/business/2023/09/07/networking-for-introverts-a-how-to-guide">https://www.economist.com/business/2023/09/07/networking-for-introverts-a-how-to-guide</a>, See on <a href="https://news.ycombinator.com/item?id=37466147">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><h2>Making the business of meeting strangers marginally less awful</h2></section><div><div data-body-id="cp2"><div><figure><div><figcaption>Listen to this story.</figcaption> <p><span>Enjoy more audio and podcasts on<!-- --> <a id="audio-ios-cta" href="https://economist-app.onelink.me/d2eC/bed1b25" target="_blank" rel="noreferrer">iOS</a> <!-- -->or<!-- --> <a id="audio-android-cta" href="https://economist-app.onelink.me/d2eC/7f3c199" target="_blank" rel="noreferrer">Android</a>.</span></p></div><audio controls="" id="audio-player" preload="none" src="https://www.economist.com/media-assets/audio/060%20Business%20-%20Bartleby%20copy-452e13ecc2cba8a5207cbc25654ef43a.mp3" title="Networking for introverts: a how-to guide " controlslist="nodownload"><p>Your browser does not support the &lt;audio&gt; element.</p></audio></figure></div><p data-component="paragraph"><span data-caps="initial">C</span><small>orporate life</small> throws up some stressful moments. Bringing bad news to your boss; facing an interview panel; making a big presentation. But few things are worse than networking if you are an introvert.</p><p data-component="paragraph">You arrive at an event to find that everyone there apparently knows each other already. And then you look more closely and spot the fellow-sufferers. They are the people who are actually reading the conference blurb. They look at email on their phones with greater intensity than ever happens at the office. They endlessly circulate the room, like bits of plastic in the ocean waiting to be snagged on something. They take a seat in the main hall while the sound engineers are still testing the microphones.</p><p data-component="paragraph">Fortunately, there is advice out there on how to break the ice with strangers. Unfortunately, its abysmal. One sage counsels making contact in queues, because it is easier to talk to the person in front of you and behind you. You are meant to ambush people on the escalator, in the toilets and in the queue to get your name tag. In the line for coffee, open the door to jobs and sales by saying six incomprehensible words: Juicing up for the big keynote?</p><p data-component="paragraph">On it goes. Dont be afraid to laugh, because nothing drains the tension from a room like someone who cannot stop chuckling. Bring personal information into the conversation, lest people think you are at a conference on treasury-management software only for commercial gain. Use the other persons name twice, to appear truly engaged. And take notes on conversations afterwards so you can follow up with them.</p><p data-component="paragraph">Add these ingredients together, and you have the recipe for success:</p><p data-component="paragraph">Juicing up for the big keynote?</p><p data-component="paragraph">What?</p><p data-component="paragraph">Juicing up for the big keynote?</p><p data-component="paragraph">I dont know what that means.</p><p data-component="paragraph">[Scan name badge] Keith, is it?</p><p data-component="paragraph">Er, yes.</p><p data-component="paragraph">[Laughing] Im having a baby, Keith.</p><p data-component="paragraph">Keith?</p><p data-component="paragraph">[Take out notepad]</p><p data-component="paragraph">If this is how to network, no wonder people go to the main hall early.</p><p data-component="paragraph">Making contacts on a site like LinkedIn is a lot less stressful. There is no eye contact, after all, and the rules of the road are agreed. And all those connection requests do appear to help with careers. A paper published last year by Karthik Rajkumar of LinkedIn and co-authors from academia found empirical evidence for the insight that underpins all kinds of networkingthat, because they bring you new information, more infrequent and distant relationships (or weak ties) are more useful than close contacts.</p><p data-component="paragraph">The researchers randomly changed the People You May Know recommendations algorithm that LinkedIn shows its users, so that the prevalence of weaker and stronger connections varied among people on the site. The experiment showed that weaker ties (where a pair of users had only one mutual friend, say) were more likely to lead to job applications and job moves than those where people had 25 mutual friends or more.</p><p data-component="paragraph">This sounds like nirvana for introverts: start spamming everyone with connection requests, close the office door and wait for job offers. But it is not that easy. Even weak ties need tending. Even online, interacting with people is easier if you find it energising; a survey-based study of LinkedIn, by Joanna Davis of Augustana College and her co-authors, found that extroversion was a predictor of networking ability.</p><p data-component="paragraph">There isnt a genuinely painless way for introverts to network. Still, methods to do it exist that are wiser than standing in a queue and hoping the guy who doesnt know how to get coffee out of the machine is your ticket to career success.</p><p data-component="paragraph">The real secret is to save your energy for the people who are most likely to be interesting to you. In the online realm, for instance, Dr Rajkumars study does not find that the weaker the tie, the better. The sweet spot in networking on LinkedIn is someone with moderately weak ties to you: connecting with a person with ten mutual friends markedly increases the probability of changing jobs compared with someone with just one shared friend.</p><p data-component="paragraph">In other words, networking pays off if you can identify people who can bring you new information but are close enough to your world that this information is useful. In the offline world, a tool like Chat<small>GPT</small> should make it easier to find useful prospects in a list of event attendees. But you still need to overcome all your instincts and approach them.<span></span></p><p data-component="paragraph"><b>Read more from Bartleby, our columnist on management and work:<br></b><i><a href="https://www.economist.com/business/2023/08/31/the-best-bosses-know-how-to-subtract-work">The best bosses know how to subtract work</a> (Aug 31st)<br></i><i><a href="https://www.economist.com/business/2023/08/24/the-benefits-of-a-good-workplace-mentoring-scheme-are-undeniable">How to get the most out of mentoring</a> (Aug 24th)<br></i><i><a href="https://www.economist.com/business/article66888-prod.ece" target="_blank">A retiring consultants advice on consultants</a> (Jul 17th)</i></p><p data-component="paragraph"><i>Also: How the Bartleby column <a href="https://www.economist.com/column-names">got its name</a></i></p></div><p>This article appeared in the Business section of the print edition under the headline "Stranger things"</p><div data-tracking-id="content-well-chapter-list"><h2><a href="https://www.economist.com/business/">Business</a> <span>September 9th 2023</span></h2><ul><li><a href="https://www.economist.com/business/2023/09/03/meet-ernie-chinas-answer-to-chatgpt"><span>Meet Ernie, Chinas answer to ChatGPT</span></a></li><li><a href="https://www.economist.com/business/2023/09/07/german-builders-are-on-the-brink-of-collapse"><span>German builders are on the brink of collapse</span></a></li><li><a href="https://www.economist.com/business/2023/09/07/tiktok-is-wading-into-south-east-asias-e-commerce-wars"><span>TikTok is wading into South-East Asias e-commerce wars</span></a></li><li><a href="https://www.economist.com/business/2023/09/07/a-strike-at-chevron-shows-a-reinvigorated-union-movement"><span>A strike at Chevron shows a reinvigorated union movement</span></a></li><li><a href="https://www.economist.com/business/2023/09/07/meet-the-worlds-most-enduring-product"><span>Meet the worlds most enduring product</span></a></li><li><a href="https://www.economist.com/business/2023/09/07/networking-for-introverts-a-how-to-guide"><span>Networking for introverts: a how-to guide</span></a></li><li><a href="https://www.economist.com/business/2023/09/04/americas-bosses-just-wont-quit-that-could-spell-trouble"><span>Americas bosses just wont quit. That could spell trouble</span></a></li></ul></div><div orientation="vertical" data-test-id="vertical"><div orientation="vertical"><figure><img alt="The new Middle East: The promise and the perils" loading="lazy" width="1280" height="1684" decoding="async" data-nimg="1" sizes="300px" srcset="https://www.economist.com/img/b/16/21/90/media-assets/image/20230909_DE_EU.jpg 16w, https://www.economist.com/img/b/32/42/90/media-assets/image/20230909_DE_EU.jpg 32w, https://www.economist.com/img/b/48/63/90/media-assets/image/20230909_DE_EU.jpg 48w, https://www.economist.com/img/b/64/84/90/media-assets/image/20230909_DE_EU.jpg 64w, https://www.economist.com/img/b/96/126/90/media-assets/image/20230909_DE_EU.jpg 96w, https://www.economist.com/img/b/128/168/90/media-assets/image/20230909_DE_EU.jpg 128w, https://www.economist.com/img/b/256/336/90/media-assets/image/20230909_DE_EU.jpg 256w, https://www.economist.com/img/b/360/473/90/media-assets/image/20230909_DE_EU.jpg 360w, https://www.economist.com/img/b/384/505/90/media-assets/image/20230909_DE_EU.jpg 384w, https://www.economist.com/img/b/480/631/90/media-assets/image/20230909_DE_EU.jpg 480w, https://www.economist.com/img/b/600/789/90/media-assets/image/20230909_DE_EU.jpg 600w, https://www.economist.com/img/b/834/1097/90/media-assets/image/20230909_DE_EU.jpg 834w, https://www.economist.com/img/b/960/1263/90/media-assets/image/20230909_DE_EU.jpg 960w, https://www.economist.com/img/b/1096/1441/90/media-assets/image/20230909_DE_EU.jpg 1096w, https://www.economist.com/img/b/1280/1684/90/media-assets/image/20230909_DE_EU.jpg 1280w, https://www.economist.com/img/b/1424/1873/90/media-assets/image/20230909_DE_EU.jpg 1424w" src="https://www.economist.com/img/b/1424/1873/90/media-assets/image/20230909_DE_EU.jpg"></figure></div><div orientation="vertical"><h3 orientation="vertical">From the September 9th 2023 edition</h3><p orientation="vertical">Discover stories from this section and more in the list of contents </p><a href="https://www.economist.com/printedition/2023-09-09" data-analytics="sidebar:weekly_edition"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1zm.142 4.5l-1.008 1.062c3.33 3.276 4.194 4.14 4.608 4.5-1.602-.018-3.168-.018-10.242-.018v1.584c7.074 0 8.73 0 10.242-.018-.432.36-1.314 1.206-4.608 4.536l1.008 1.044 6.354-6.354L12.142 5.5z" fill="#2E45B8" fill-rule="nonzero"></path></g></svg><span>Explore the edition</span></a></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Project Gutenberg Open Audiobook Collection (283 pts)]]></title>
            <link>https://marhamilresearch4.blob.core.windows.net/gutenberg-public/Website/index.html</link>
            <guid>37466027</guid>
            <pubDate>Mon, 11 Sep 2023 12:06:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://marhamilresearch4.blob.core.windows.net/gutenberg-public/Website/index.html">https://marhamilresearch4.blob.core.windows.net/gutenberg-public/Website/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=37466027">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="particles">
        
        <p><strong>Thousands of free and open audiobooks powered by Microsoft AI</strong></p>
      </div><div id="Paper">
    <h2>Paper</h2>
    <div>
      <p>
        <h2>For more technical information on the code used to generate these audiobooks please see our IEEE Big Data paper: <a href="https://arxiv.org/abs/2009.08044">Large Scale Intelligent Microservices</a><br></h2>
        
      </p>
      <p>@article{hamilton2020large,<br> &nbsp;title={Large-Scale Intelligent Microservices},<br> &nbsp;author={Hamilton, Mark and Gonsalves, Nick and Lee, Christina <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and Raman, Anand and Walsh, Brendan and Prasad, Siddhartha<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and Banda, Dalitso and Zhang, Lucy and Zhang, Lei and Freeman, William T},<br> &nbsp;journal={arXiv preprint arXiv:2009.08044},<br> &nbsp;year={2020}}</p>
    </div>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The right to data ownership is the only way to take on Big Tech (221 pts)]]></title>
            <link>https://www.telegraph.co.uk/business/2023/09/11/right-data-ownership-big-tech-google-meta-competition/</link>
            <guid>37465972</guid>
            <pubDate>Mon, 11 Sep 2023 12:02:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.telegraph.co.uk/business/2023/09/11/right-data-ownership-big-tech-google-meta-competition/">https://www.telegraph.co.uk/business/2023/09/11/right-data-ownership-big-tech-google-meta-competition/</a>, See on <a href="https://news.ycombinator.com/item?id=37465972">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-test="article-body-text">
  
  

	

	
	

	
	



  
  
    <p>Today, giant technology companies are more powerful than any nation state. Their whims set the political agenda.&nbsp;</p><p>Ian Bremmer calls this technopolarity, describing how digital power defines politics and reshapes the world. For example, consider their enthusiasm for AI regulation: laws which could in theory be written and enforced by them.&nbsp;</p><p>This does not seem healthy for either the economy or our democracy  so can anything on Earth stop Big Tech?</p><p>One argument is that we should not worry. As sure as eggs are eggs, we are told, a period of market dominance will be followed by hubris, in a self-correcting cycle.&nbsp;</p><p>Just look at Microsoft, they say. Twenty-five years ago this month, the software company surpassed General Electric to become the most valuable on the planet.&nbsp;</p><p>The same month, a tiny new company with an odd name was formally incorporated. It called itself Google. By 2012, it had overtaken Microsofts market capitalisation, although Microsoft has subsequently caught up.&nbsp;</p><p>This week, <a href="https://www.telegraph.co.uk/business/2023/09/10/google-monopoly-lawsuit-antitrust-trial-doj-kent-walker/" target="_blank" rel="noopener noreferrer">the biggest antitrust trial</a> since the American government took on Microsoft begins  and this time it is Google who is in the dock. So dont worry about monopolies: it seems the system takes care of itself. Everything is for the best in all possible worlds, as Voltaires Dr Pangloss assured us.</p>
  
</div><div data-test="article-body-text">
  
  

	

	
	

	
	



  
  
    <p>How neat this is  perhaps too neat. For a start, monopoly profits ought to see private capital flooding into startups and would-be rivals, to grab their market share.&nbsp;</p><p>Startups like Neeva, for example, the search engine I wrote about last year, founded by top ex-Google executives and engineers. Its search results were so good, compared to Googles, <a href="https://www.telegraph.co.uk/technology/2022/05/30/seen-google-free-future-like-breathing-clean-air/" target="_blank" rel="noopener noreferrer">they were like a breath of fresh&nbsp;air</a>. More fool me, though.</p><p>Neeva announced it was closing down for good in May, unable to make a business out of its superior product. In fact, there has not been a major new platform to challenge the incumbents for well over a decade. Capital keeps finding other things to fund, even some very silly things, like lab grown meat.</p><p>But advocates of strong competition law also have a problem. Very often, <a href="https://www.telegraph.co.uk/news/2023/06/15/the-eu-might-just-break-the-internet/" target="_blank" rel="noopener noreferrer">the authorities are all bark</a>, and no bite  and the European Union is one of the worst offenders in this regard.&nbsp;</p><p>The top Silicon Valley lawyer behind the Microsoft antitrust case, Gary Reback conceded as much when he also advised us not to worry. The mere act of competition scrutiny benefits the market: the trial is the remedy, he has said.</p><p>But very often the competition watchdogs intervention seems to have no impact at all, and has only made the dominant player stronger. Google has run rings around competition authorities by appearing to take it on the chin, then offering up a less onerous behavioural remedy.&nbsp;</p><p>This is accepted and life carries on much as before. Structural remedies do not necessarily improve things much either. When in 2000, a Microsoft breakup was privately shopped around its computer rivals, it seemed nobody wanted to take any of the chopped up pieces of the company.</p><p>So if doing nothing is not an option and doing something is ineffective, what else is left to do?&nbsp;</p>
  
</div><div data-test="article-body-text">
  
  

	

	
	

	
	



  
  
    <p>Something needs to change, for just as advocates of the theory of network effects predicted, online markets are winner-takes all markets. Big Tech only ever seems to get bigger.</p><p>But do not despair  the answer may be a very old one.</p><p>Today, Google is no more about web search or maps than those American candy shops on Oxford Street are in the business of selling sweets. That is what we see when we walk past, but it is not really what they do.&nbsp;</p><p>Google and Meta, Facebooks parent company, are giant personal data processing companies. But because of a peculiar loophole, they get that data for free  it doesnt belong to anyone.&nbsp;</p><p>If only we were allowed to assert ownership of our data, in the form of a strong property right, we could start to do some interesting things with it.&nbsp;</p><p>We could demand its destruction, because it would be ours, giving us much stronger privacy protection than we enjoy today. We would also be able to trade it and do the one thing we cannot currently  help determine its value.&nbsp;</p><p>Our decisions would help set the price for this data. This is not a popular idea with everyone. Academics and the digital NGOs, a familiar looking blob, hate the prospect, in part because it leaves them with a diminished political role, if any at all.</p><p>The computer scientist Jaron Lanier, the best-known advocate of the idea of stronger property rights, says some people are horrified by the idea of capitalism online, but this would be a more honest capitalism. The familiar free arrangement has been a disaster. He calls it Data Dignity.</p><p>I am not suggesting property rights are a panacea, or a replacement for careful and enlightened competition enforcement by nation states. But in the spirit of experimentation, should we not try the one thing we have not actually tried online yet  capitalism?</p>
  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Removing Garbage Collection from the Rust Language (2013) (163 pts)]]></title>
            <link>http://pcwalton.github.io/_posts/2013-06-02-removing-garbage-collection-from-the-rust-language.html</link>
            <guid>37465185</guid>
            <pubDate>Mon, 11 Sep 2023 10:23:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://pcwalton.github.io/_posts/2013-06-02-removing-garbage-collection-from-the-rust-language.html">http://pcwalton.github.io/_posts/2013-06-02-removing-garbage-collection-from-the-rust-language.html</a>, See on <a href="https://news.ycombinator.com/item?id=37465185">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>I've been floating ways to simplify the memory management story in Rust around the core team lately. Memory management is a contentious topic, since we've worked hard to get to the current state of things, and with the push toward stability lately, there is a (quite reasonable!) resistance to any changes at this state. Still, I think the current memory management story in Rust is worth revisiting, as the current state of things may cause us problems down the line. Working with Dave Herman and Niko Matsakis, I've formulated a fairly concrete proposal at this point. The basic idea is to <em>remove garbage collection from the core language and relegate it to the standard library</em>, with a minimal set of language hooks in place to allow for flexible, pluggable automatic memory management.</p>
<p>This post is designed to explain the "why", not the "how"I'm leaving the concrete details of the proposed system to a future blog post or mailing list discussion. Rather, this explains the issue that I see with the current system. I think that the garbage collection story as it stands in Rust is not quite ideal, for three reasons: <em>familiarity</em>, <em>simplicity</em>, and <em>flexibility</em>. I'll cover each in turn.</p>
<h2>Familiarity</h2>
<p>One of the most common questions almost every Rust beginner asks is "when do I use managed pointers, and when do I use owned pointers?" Or, more simply, "what are all these <code>~</code> and <code>@</code> symbols everywhere?" Having worked on Rust for many years now, I've seen several reasons for the difficulty. Chief among them are:</p>
<ol>
<li>
<p><em>The difference between the stack and the heap is a difficult concept to grasp for many programmers used to languages like Java that don't make such a distinction.</em> This is, unfortunately, a fundamental difficulty of working in a systems language. There's little that can be done about this without taking control of allocation out of the hands of the programmer. Doing that, however, would compromise the goals of the languagein low-level, performance-critical programming, being able to precisely control whether allocations occur on the stack or on the heap is crucial.</p>
</li>
<li>
<p><em>The sigils make the code unfamiliar before the concepts are learned.</em> Unlike the rest of the punctuation in Rust, <code>~</code> and <code>@</code> are not part of the standard repertoire of punctuation in C-like languages, and as a result the language can seem intimidating. One of the benefits of keywords is that they are self-documenting in a way that punctuation is not. This could be fixed by switching to keywords, which I prefer for this reason; however, syntactic beauty is in the eye of the beholder and so I won't lose sleep over this not changing if the community prefers the current syntax.</p>
</li>
<li>
<p><em>There are two heaps, not just one, so beginners are confused as to which one to allocate into.</em> This is a result of the "minimize sharing by default" philosophy of the concurrency system. However, the concurrency system has been part of the <em>library</em> rather than the language for several years now, so this seems somewhat out of place.</p>
</li>
<li>
<p><em>Programmers don't know which to use, since some operations are available with <code>~</code> and some operations are available with <code>@</code></em>. Actually, we were confused on this point for a long time as wellit wasn't clear whether <code>~</code> or <code>@</code> would become dominant. We debated for a long time which to present first, <code>~</code> or <code>@</code>. However, as the language and community evolved, and coding standards became more settled, a clear winner emerged: the owning pointer <code>~</code>. In practice, the rule has been that <em>programmers should use <code>~</code> to allocate in all circumstances except when they have no way of knowing precisely when the object in question should be freed.</em></p>
</li>
</ol>
<p>Point (4), to me, is the most critical. The rule that emerged<code>~</code> over <code>@</code>should not be surprising, in retrospect, as it is how systems software has been developed for decades. The key insight that was missing is that <em>the owning pointer <code>~</code> is just the Rust equivalent of <code>malloc</code> and <code>free</code>.</em> For many, probably most C programs, <code>malloc</code> and <code>free</code> are just fine (assuming you use them correctly, of course); each heap allocation is allocated in just one place and destroyed in just one place. Only when the lifetimes of objects become very complex do C and C++ programmers resort to manual reference counting to determine when an object should be freed (and many, perhaps most, C programs never get there). <em>This</em> is the role that has emerged for <code>@</code> in Rust programs: <code>@</code> is a replacement for manual reference counting in C programs. The <code>kobject</code> system in the Linux kernel, the <code>GObject</code> system in <code>glib</code>, and so forth, are the C equivalents of <code>@</code> in Rust.</p>
<p>The key point here is that these are very specialized use cases in C, and <code>@</code> has been relegated to a similarly marginal role in idiomatic Rust code. We thought for a while that many Rust programs would use <code>@</code> extensively and that it would ease the learning curve for those not used to destructor-based memory management and references. This has not, however, been the case in practice. In reality, since the libraries all use owning pointers (<code>~</code>), Rust programmers have to learn them quickly anyhow. And once Rust programmers learn how to use <code>~</code> effectively, they quickly find <code>@</code> relegated to a marginal role, if it's used at all. <code>~</code> has so many advantages: deterministic allocation and destruction, interaction with the standard library, freedom from GC marking pauses, simpler semantics, appendability where vectors and strings are concerned, and sendability across tasks.</p>
<p>I think we're better off teaching <code>~</code> as the go-to solution for most programs and relegating <code>@</code> to a specialized role. <code>@</code> has its use cases, to be sure; large, event-driven C++ programs use reference counting for a reason. But those use cases are specialized. Beginners should not be asking "should I use <code>~</code> or <code>@</code>?" The answer is almost always <code>~</code>.</p>
<p>In this regard relegating <code>@</code> to a library is just the natural conclusion of this approach. I feel that what beginners should be taught is that <code>~</code> is the way to allocate in Rust, and letting an <code>~</code> owning pointer go out of scope is the way you free in Rust. This is what we should be teaching in the <em>language</em> tutorial. As beginners become more comfortable with this and explore the libraries, they will learn about ways to achieve more dynamic memory management: tracing garbage collection with the <code>Gc</code> type, reference counting with the <code>Rc</code> type, and thread-safe reference counting with the <code>Arc</code> type. But by building only <code>~</code> into the language, we can reduce confusion by, in effect, making the language more opinionated.</p>
<h2>Simplicity</h2>
<p>Although Rust didn't start out that way, one of the most interesting applications of Rust has been very low-level programming, even down to the level of kernels. The interest in this application of Rust was something of a surprise to us, but in hindsight it makes perfect sense. Low-level control over memory management isn't something that most applications software, especially on the server side, wants; most of that software has migrated over to languages like Java, Ruby, and JavaScript that trade control and performance for convenience by making memory management automatically, and dynamically, managed by the runtime. The remaining class of software, most of which is written in C and C++, is software that must manage memory manually in order to achieve some combination of performance, simplicity, and/or the ability to self-host. The prospect of using a new language for <em>this</em> class of software, which includes OS kernels, game engines, and browser engines among others, is what is fueling the growth of the nascent Rust community.</p>
<p>It might be possible to create a language that presents only a simple, fully automatic memory management system at first, and which surfaces the machinery of safe manual memory management* only when the programmer requires it for maximum performance. This would ease the learning curve, as programmers would be able to write many, perhaps most programs without ever learning how to manage memory at all. However, at this point I don't think that this language exists yet, and in particular I don't think Rust is that language. There are basically two problems here: (1) <code>~</code> owning pointers are everywhere in Rust, from the standard library to the built-in macros, making learning about them a necessity from the get-go; and (2) it is basically impossible to program Rust without at least a cursory understanding of references (a.k.a. <code>&amp;</code> pointers) and their lifetime semantics; even <code>vec::each()</code> uses references.</p>
<p>Despite the fact that this might seem like a negative result, I actually think it's quite positive for the project. It helps to define the project's scope. I don't think automatic memory management in Rust is ever going to be as convenient as memory management in, say, Ruby or Java, and that's OK! <em>The same level of control that adds cognitive overhead to memory management in Rust compared to other languages also makes Rust able to go where few other industry languages have.</em> This space, I think, is where Rust can really shine.</p>
<p>In short, I think that Rust as a <em>language</em> should focus on roughly the same application domain as C++ does.</p>
<p>Important to this effort is to have as small of a runtime as possible, just as C++ does, leaving higher-level abstractions to libraries. And, in fact, we are almost there already. The only runtime support that compiled Rust programs require are a small set of "language items", which are magic functions or traits written <em>in Rust</em> that are known to the compiler. Looking at the set of language items, and disqualifying legacy items that will be removed soon such as <code>annihilate</code> and <code>log_type</code>, there are just a few categories:</p>
<ol>
<li>
<p>Operator traits, like <code>Add</code> and <code>Sub</code>. These are analogous to <code>operator+</code>, <code>operator-</code>, and so forth in C++.</p>
</li>
<li>
<p>Memory primitives, like <code>str_eq</code>. These are somewhat legacy at this point and probably could be converted to LLVM intrinsics like <code>memcmp</code> without much trouble, especially after dynamically sized types happens. In any case, in most C++ compilers <code>memcmp</code> and friends are builtins.</p>
</li>
<li>
<p>Failure: <code>fail</code> and <code>fail_bounds_check</code>. This is analogous to <code>throw</code> in C++, although a Rust program that doesn't want to use stack unwinding might want to use <code>abort</code> instead (which would be like <code>-fno-exceptions</code>) or do something more elaborate like the Linux kernel's "oops" functionality.</p>
</li>
<li>
<p>Allocation primitives <code>malloc</code> and <code>free</code>. These have direct C++ equivalents: <code>operator new</code> and <code>operator delete</code>.</p>
</li>
<li>
<p>Garbage collection primitives.</p>
</li>
</ol>
<p>Of these, the only language items that don't have direct C++ equivalents are the garbage collection primitives. If those were eliminated, then Rust as a language would be every bit as freestanding as C++ is. In terms of suitability for kernel and embedded development, Rust would be on truly equal footing.</p>
<p>In summary: (1) all Rust programmers have to know how <code>~</code> and <code>&amp;</code> work, despite the presence of <code>@</code>; (2) the only additional runtime primitives that Rust exposes and C++ doesn't are those related to <code>@</code>.</p>
<h2>Flexibility</h2>
<p>When it comes to memory management, there are obviously many different strategies: stack allocation, heap allocation with <code>malloc</code> and <code>free</code>, arena allocation, and garbage collection. What's less well known is that even among garbage collection, there are many different approaches, each with advantages and disadvantages. There's thread-local GC, thread-safe GC, incremental GC, generational GC, reference counting, thread-safe reference counting, deferred reference counting, ulterior reference countingthe list goes on and on. (For a good survey of automatic memory management techniques and how they relate to one another, check out <a href="http://www.cs.virginia.edu/~cs415/reading/bacon-garbage.pdf">"A Unified Theory of Garbage Collection" by Bacon et al.</a>) A program that wants to maximize performance among some axis (latency versus throughput) and remain safe with objects with complex lifetimes may have reasons to choose one or the other.</p>
<p>Specifically, there's the perennial debate between reference counting and tracing garbage collection. Many applications are better with tracing GC because of the increased throughput it provides and straightforward handling of cycles, and many applications are better with reference counting because of implementation simplicity, cache behavior, mostly-incremental operation, and promptness of deallocation. It makes sense for applications to be able to choose between the two. Even more important is the tradeoff between thread-safe and thread-local garbage collection: concurrent garbage collection is practically always more expensive than thread-local garbage collection, so it makes sense for programs to restrict concurrent GC (including atomic reference counting) to be used only when needed.</p>
<p>Integrating multiple tracing garbage collectors or cycle collectors into one system is a hard problem, and I don't think Rust is going to really solve it. However, integrating reference counting into a garbage collected system is straightforward, as long as cycles are not created (and in Rust we can forbid the creation of such cycles through clever use of the type system). In practice this seems to work well: we typically use thread-local tracing GC for data with complex lifetimes within one task, and we use thread-safe reference counting for data that must be shared between tasks.</p>
<p>Equally important is the ability to integrate with <em>external</em> garbage collection systems (usually reference counted ones). This is a problem that is often overlooked, but is terribly important for client software such as mobile apps and browser engines. On Windows, apps must integrate with the reference-counted COM system in order to use DirectX and other APIs. On the Mac and on iOS, apps have to integrate with Objective-C and the closely-related Core Foundation, also reference-counted systems. On Linux, GNOME apps have to integrate with GObject, again a reference-counted system. On Android, apps have to integrate with the garbage-collected Dalvik subsystem via the JNI. All of this requires that the memory management system in the language be deeply flexible.</p>
<p>Because of this, I'm suspect of blessing any particular form of automatic memory management in the core language. In Rust, the <code>@</code> type is not only blessed with special syntax, but is eligible for borrowing and other operations in a way that user-defined types aren't. Although Rust provides the facilities needed to build practically all the other forms of garbage collection, as well as those needed to integrate with external GC systems in a safe way, the resulting smart pointers feel second-class compared to <code>@</code>. A systems language designed to work in a diverse set of environments should have the flexibility to create memory management abstractions that feel first-class.</p>
<h2>Conclusion</h2>
<p>For these three reasonsfamiliarity, simplicity, and flexibilityI'd like to propose removing <code>@</code> pointers from the language and replacing them with a small set of hooks allowing the same functionality to be implemented as a library and on user-defined types. We would ship tracing GC as part of the standard library and make it just as powerful and convenient as it is today (except for the <code>@</code> syntax). We'd gain a flexible set of abstractions, make the language easier to learn, and make Rust into a truly freestanding language environment.</p>
<p>* Note that the <em>safe</em> qualifier here disqualifies manually-built free lists in garbage-collected languages, as these manually-built free lists provide no protection against errors like double "frees", leaks, and danging pointers. (They're significantly worse than true manual memory management anyhow; the GC still has to trace through objects in arenas at mark time, copy the objects within out into the tenured generation when they survive a minor collection, write barrier the objects, and so forth.)</p>
<p> Note that I don't mean you shouldn't write Web frameworks and Web sites in Rust: in fact, I think Rust would be a fantastic language for many classes of Web server software, especially that which must scale to the highest loads and squeeze every ounce of performance out of the servers on the racks.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The SHA256 for this sentence begins with: one, eight, two, a, seven, c and nine. (157 pts)]]></title>
            <link>https://twitter.com/lauriewired/status/1700982575291142594</link>
            <guid>37465086</guid>
            <pubDate>Mon, 11 Sep 2023 10:08:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/lauriewired/status/1700982575291142594">https://twitter.com/lauriewired/status/1700982575291142594</a>, See on <a href="https://news.ycombinator.com/item?id=37465086">Hacker News</a></p>
Couldn't get https://twitter.com/lauriewired/status/1700982575291142594: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[30 years of The X-Files (103 pts)]]></title>
            <link>https://arstechnica.com/culture/2023/09/the-truth-is-out-there-celebrate-30-years-of-the-x-files-with-our-30-favorite-episodes/</link>
            <guid>37464576</guid>
            <pubDate>Mon, 11 Sep 2023 08:51:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/culture/2023/09/the-truth-is-out-there-celebrate-30-years-of-the-x-files-with-our-30-favorite-episodes/">https://arstechnica.com/culture/2023/09/the-truth-is-out-there-celebrate-30-years-of-the-x-files-with-our-30-favorite-episodes/</a>, See on <a href="https://news.ycombinator.com/item?id=37464576">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/pilot-800x535.jpg" alt="Mulder sitting at his desk, Scully sitting on top of it, with " i="" want="" to="" believe="" poster="" in="" the="">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/08/pilot.jpg" data-height="802" data-width="1200">Enlarge</a> <span>/</span> FBI agents Fox Mulder (David Duchovny) and Dana Scully (Gillian Anderson) were the heart and soul of <em>The X-Files</em>.</p><p>20th Century Fox</p></figcaption>  </figure>

  




<!-- cache hit 60:single/related:c9f3407209fffb29899b23a2484a42b2 --><!-- empty -->
<p>In September 1993, fictional FBI Special Agents <a href="https://en.wikipedia.org/wiki/Fox_Mulder">Fox Mulder</a> (David Duchovny) and <a href="https://en.wikipedia.org/wiki/Dana_Scully">Dana Scully</a> (Gillian Anderson) made their broadcast TV debut on <a href="https://en.wikipedia.org/wiki/The_X-Files"><em>The X-Files</em></a> and went on to investigate alien abductions and all manner of strange phenomena for nine full seasons and <a href="https://en.wikipedia.org/wiki/The_X-Files_(film)">two</a> feature <a href="https://en.wikipedia.org/wiki/The_X-Files:_I_Want_to_Believe">films</a>, followed by two additional limited-run seasons in 2016 and 2018. This hugely popular and influential series celebrates its 30th anniversary this month, giving us a prime opportunity to pay homage to our favorite episodes and characters.</p>
<p><strong>(Spoilers for <em>The X-Files</em> below.)</strong></p>
    <div>
            <p>(Ars Technica may earn compensation for sales from links on this post through <a href="https://arstechnica.com/affiliate-link-policy/">affiliate programs</a>.)</p>
        </div>

<p><em>The X-Files</em> was created by <a href="https://en.wikipedia.org/wiki/Chris_Carter_(screenwriter)">Chris Carter</a>, who was a fan of the 1970s horror series <a href="https://en.wikipedia.org/wiki/Kolchak:_The_Night_Stalker"><em>Kolchak: The Night Stalker</em></a>, featuring a wire service reporter (Darren McGavin) investigating mysterious crimes with a supernatural or science fiction element. Other cited influences included <em>The Twilight Zone, Night Gallery, Twin Peaks</em> (in which Duchovny played a transgender DEA agent), and Jonathan Demme's 1988 Oscar-winning film <em>The Silence of the Lambs</em>.</p>
<p>Carter liked the idea of a TV series featuring FBI agents investigating the paranormal. He deliberately made Mulder (nicknamed "Spooky") the true believer and Scully the science-based skeptica gender swap to counter broad cultural stereotypes. Carter described the pair as a dichotomy, representing his desire to believe in something versus an inability to believethe age-old tension between skepticism and faith.</p>
<p>As the characters developed over subsequent seasons, we saw them internalize that tension, with Mulder sometimes getting discouraged and questioning his longing to believe and Scully being forced to confront how her science sometimes conflicted with her devout Catholic faith. They each had deep personal journeys as well; both lost family members, for instance, and Mulder's obsession with alien abductions was fueled by the disappearance of his sister Samantha when he was a kid. And while Carter was adamant early on that this would be a purely platonic relationship<em>&nbsp;la</em> Emma Peel and John Steed in <em>The Avengers</em> British TV seriesthat changed as the friendship between Mulder and Scully deepened, with increasingly romantic overtones. But the series never openly acknowledged the two having sex until season 11's "<a href="https://en.wikipedia.org/wiki/Plus_One_(The_X-Files)">Plus One</a>."</p>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/08/lonegunmen1.jpg" data-height="798" data-width="1200" alt="The Lone Gunmen were introduced in the S1 episode &quot;E.B.E.&quot; and soon became fan favorites."><img alt="The Lone Gunmen were introduced in the S1 episode &quot;E.B.E.&quot; and soon became fan favorites." src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/lonegunmen1-640x426.jpg" width="640" height="426" srcset="https://cdn.arstechnica.net/wp-content/uploads/2023/08/lonegunmen1.jpg 2x"></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/08/lonegunmen1.jpg" data-height="798" data-width="1200">Enlarge</a> <span>/</span> The Lone Gunmen were introduced in the S1 episode "E.B.E." and soon became fan favorites.</p><p>20th Century Fox</p></figcaption></figure>
<p><em>The X-Files</em> quickly blossomed from a cult series into a bona fide pop culture phenomenon throughout its first seven seasons, racking up a lot of Emmy and Golden Globe awards. Scully is often credited with encouraging young women to pursue careers in medicine, science, or the FBI, a phenomenon dubbed the "<a href="https://en.wikipedia.org/wiki/Dana_Scully#%22The_Scully_Effect%22">Scully effect</a>."</p>                                            
                                                        
<p>While it started out dealing with UFOs and alien abduction, Carter and his writing team realized early on that it would be difficult to sustain that momentum over multiple seasons. So there were essentially two kinds of episodes: those advancing the over-arching "<a href="https://en.wikipedia.org/wiki/Mythology_of_The_X-Files">mytharc</a>" of the series canonoften featuring appearances by William B. Davis as the iconic <a href="https://en.wikipedia.org/wiki/Cigarette_Smoking_Man">Cigarette Smoking Man</a> (CSM)and standalone "Monster of the Week" (MOW) episodes unrelated to the series mythology. From horror to humor, shadowy conspiracies to arcane folklore, the series offered something for everyone, a key factor in its broad popular appeal.</p>
<h2>The Seasons That Shall Not Be Named</h2>
<p>For the first five seasons, <em>The X-Files</em> filmed in Vancouver, British Columbia, but it moved to Los Angeles starting with the sixth season so that Duchovny could be closer to his then-wife Tea Leoni. When Duchovny's contract expired after the seventh season, he effectively quit the series, though he returned occasionally as Mulder during S8 and S9or, as I like to call them, "The Seasons That Shall Not Be Named."</p>
<p>This was unquestionably the low point of the series, especially with Anderson also winding down her involvement. Carter apparently convinced himself that he could simply find new leadsin this case, Robert Patrick as <a href="https://en.wikipedia.org/wiki/John_Doggett">John Doggett</a> and Annabeth Gish as <a href="https://en.wikipedia.org/wiki/Monica_Reyes">Monica Reyes</a>and the show would run indefinitely. But there is simply no <em>X-Files</em> without Mulder and Scully. Add in the deaths of fan favorites <a href="https://en.wikipedia.org/wiki/The_Lone_Gunmen">the Lone Gunmen</a> in "<a href="https://en.wikipedia.org/wiki/Jump_the_Shark_(The_X-Files)">Jump the Shark</a>" (S9), and no wonder ratings steeply declined.</p>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/08/doggettandreyes.jpg" data-height="800" data-width="1200" alt="The series introduced Monica Reyes (Annabeth Gish) and John Doggett (Robert Patrick) as new leads for the eighth and ninth seasons. But there is no <em>X-Files</em> without Mulder and Scully, and ratings sharply declined."><img alt="The series introduced Monica Reyes (Annabeth Gish) and John Doggett (Robert Patrick) as new leads for the eighth and ninth seasons. But there is no <em>X-Files</em> without Mulder and Scully, and ratings sharply declined." src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/doggettandreyes-640x427.jpg" width="640" height="427" srcset="https://cdn.arstechnica.net/wp-content/uploads/2023/08/doggettandreyes.jpg 2x"></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/08/doggettandreyes.jpg" data-height="800" data-width="1200">Enlarge</a> <span>/</span> The series introduced Monica Reyes (Annabeth Gish) and John Doggett (Robert Patrick) as new leads for the eighth and ninth seasons. But there is no <em>X-Files</em> without Mulder and Scully, and ratings sharply declined.</p><p>20th Century Fox</p></figcaption></figure>
<p>The show's original run ended with that ninth season. But <em>The X-Files</em> lived on via DVD and (more recently) streaming platforms, and its hardcore fan base remained fiercely loyal. The 2008 film <a href="https://en.wikipedia.org/wiki/The_X-Files:_I_Want_to_Believe"><em>The X-Files: I Want to Believe</em></a> (with a standalone MOW plot) received mixed reviews and didn't exactly light up the box office, but it grossed $68 million against its $30 million budget. That was enough to spark rumors of a possible third film; both Duchovny and Anderson expressed a willingness to co-star.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Chrome just rolled out a new way to track you and serve ads (477 pts)]]></title>
            <link>https://theconversation.com/google-chrome-just-rolled-out-a-new-way-to-track-you-and-serve-ads-heres-what-you-need-to-know-213150</link>
            <guid>37464574</guid>
            <pubDate>Mon, 11 Sep 2023 08:50:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theconversation.com/google-chrome-just-rolled-out-a-new-way-to-track-you-and-serve-ads-heres-what-you-need-to-know-213150">https://theconversation.com/google-chrome-just-rolled-out-a-new-way-to-track-you-and-serve-ads-heres-what-you-need-to-know-213150</a>, See on <a href="https://news.ycombinator.com/item?id=37464574">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>Late last week, Google announced something called the Privacy Sandbox has been rolled out <a href="https://privacysandbox.com/intl/en_us/news/privacy-sandbox-for-the-web-reaches-general-availability">to a majority of Chrome users</a>, and will reach 100% of users in the coming months. But what is it, exactly? </p>

<p>The new suite of features represents a fundamental shift in how Chrome will track user data for the benefit of advertisers. Instead of third-party cookies, Chrome can now tap directly into your browsing history to gather information on advertising topics (more on that later).</p>

<p>In development since 2019, this change has attracted <a href="https://gizmodo.com/google-privacy-sandbox-now-on-every-chrome-browser-1850812404">a great deal of controversy</a>, as some commentators have deemed it <a href="https://arstechnica.com/gadgets/2023/09/googles-widely-opposed-ad-platform-the-privacy-sandbox-launches-in-chrome/">invasive in terms of privacy</a>.</p>

<p>Understanding how it works  and whether you want to opt in or out  is important, since Chrome remains the most widely used browser in the world, with a 63% market share <a href="https://www.statista.com/statistics/268254/market-share-of-internet-browsers-worldwide-since-2009/">as of May 2023</a> (Safari is in second place with 13%).</p>

<h2>Wait, what is a cookie?</h2>

<p>In 1994, computer engineer Lou Montulli at Netscape revolutionised the way we browsed the internet with his <a href="https://montulli.blogspot.com/2013/05/the-reasoning-behind-web-cookies.html">invention of the cookie</a>. For the first time, web pages could remember our passwords, preferences, language settings and even shopping carts.</p>

<p>This method was supposed to be a private exchange of information just between a user and a website  whats known as a first-party cookie. But within two years, advertisers worked out how to hack cookies <a href="https://qz.com/2000350/the-inventor-of-the-digital-cookie-has-some-regrets">to track users</a>. These are third-party cookies.</p>

<p>You can think of a first-party cookie like a shop assistant who listens to your preferences and is happy to hold your bags or clothes while you make your selection  but only while you are inside their store.</p>

<p>A third-party cookie is like a bug from an old spy movie. It listens to everything in your room, but only shares the info with its allies. The spy can place this cookie on other peoples sites, to record what you visit and what data you enter. If youve ever wondered how Facebook has served you an ad about something related to a news story you just read, chances are its because you have third-party cookies enabled.</p>

<p>Unregulated online tracking and surveillance via cookies were the default until 2018, when the European Unions <a href="https://gdpr.eu/what-is-gdpr/">General Data Protection Regulations</a> (GDPR) and the <a href="https://oag.ca.gov/privacy/ccpa">California Consumer Privacy Act</a> (CCPA) were introduced. If you have noticed more pop-ups notifying you of cookies and asking for your informed consent, you have the GDPR and CCPA to thank.</p>

<hr>
<p>
  <em>
    <strong>
      Read more:
      <a href="https://theconversation.com/cookies-i-looked-at-50-well-known-websites-and-most-are-gathering-our-data-illegally-176203">Cookies: I looked at 50 well-known websites and most are gathering our data illegally</a>
    </strong>
  </em>
</p>
<hr>


<p>The <a href="https://clearcode.cc/blog/third-party-cookies-demise/#safari-and-firefox-turn-off-support-for-third-party-cookies">first browsers</a> to turn off support for third-party cookies were Apples Safari in 2017 and Mozillas Firefox in 2019.</p>

<p>But Google is also a major online advertising company, with ads <a href="https://www.doofinder.com/en/statistics/google-revenue-breakdown">making up 57.8% of Googles revenue</a> as of 2023. They <a href="https://www.forbes.com/sites/theyec/2022/09/12/the-slow-death-of-third-party-cookies">have been slowest off the mark</a> in turning off third-party cookies in Chrome. With the introduction of the Privacy Sandbox, they now hope to start turning cookies off sometime in 2024.</p>

<h2>How is the Privacy Sandbox different from cookies?</h2>

<p>The details on how the Privacy Sandbox collection of features works <a href="https://developer.chrome.com/en/blog/shipping-privacy-sandbox/#whats-shipping">are rather technical</a>. But here are a few of the most important aspects.</p>

<p>Instead of using third-party cookies to serve you ads across the internet, Chrome will provide something called advertising Topics. These are high-level summaries of your browsing behaviour, tracked locally (such as in your browsing history), that companies can access on request to serve you ads on particular subjects.</p>

<p>Additionally, there are features such as <a href="https://developer.chrome.com/docs/privacy-sandbox/protected-audience/">Protected Audience</a> that can serve you ads for remarketing (for example, Chrome tracked you visiting a listing for a toaster, so now you will get ads for toasters elsewhere), and <a href="https://developer.chrome.com/docs/privacy-sandbox/attribution-reporting/">Attribution Reporting</a>, that gathers data on ad clicks.</p>

<p>In short, instead of third-party cookies doing the spying, the features these cookies enable will be available directly within Chrome.</p>



<h2>Is user tracking necessarily bad?</h2>

<p>While Google pitches the Privacy Sandbox as something that will improve user privacy, <a href="https://movementforanopenweb.com/googles-privacy-sandbox-a-closer-look-at-claims-and-contradictions/">not everyone agrees</a>.</p>

<p>If these features are switched on, Google  one of the worlds biggest advertising companies  is essentially able to listen to you everywhere on the web.</p>

<p>Tracking technology can arguably benefit us as well. For example, it could be helpful if an online store reminds you every three months you need a new toothbrush, or that this time last year you bought a birthday card for your mum.</p>

<p>Offloading cognitive effort, such as reminders like these, is a great way automation can assist humanity. When used in situations where pinpoint accuracy is required, it can make our lives easier and more pleasant.</p>

<p>However, if you are not comfortable with surveillance, the alternative to third-party cookies may not necessarily be the new Privacy Sandbox in Chrome.</p>

<p>The alternative is to completely disable tracking altogether.</p>

<h2>What can you do?</h2>

<p>If you dont want your online activities to be tracked for advertising purposes, there are a few straightforward choices.</p>

<p>By far the most private browsers are specialist non-tracking browsers that prioritise no tracking, such as <a href="https://duckduckgo.com/">DuckDuckGo</a> and <a href="https://brave.com/">Brave</a>. But if you dont want to get that nerdy, Safari and Firefox already have third-party cookies blocked by default.</p>

<figure>
            <a href="https://images.theconversation.com/files/547410/original/file-20230911-19-1eiqz9.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="A screenshot of a Chrome settings page listing Ad topics, Site-suggested ads and Ad measurement" data-src="https://images.theconversation.com/files/547410/original/file-20230911-19-1eiqz9.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/547410/original/file-20230911-19-1eiqz9.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=324&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/547410/original/file-20230911-19-1eiqz9.jpg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=324&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/547410/original/file-20230911-19-1eiqz9.jpg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=324&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/547410/original/file-20230911-19-1eiqz9.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=407&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/547410/original/file-20230911-19-1eiqz9.jpg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=407&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/547410/original/file-20230911-19-1eiqz9.jpg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=407&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/547410/original/file-20230911-19-1eiqz9.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p></a>
            <figcaption>
              <span>The tools found in Google Chrome are nestled under Settings - Ads privacy. You can toggle each section on or off individually, and click on them to look at more details.</span>
              <span><span>Screenshot via The Conversation</span></span>
            </figcaption>
          </figure>

<p>If you dont mind some useful targeted advertising, you can leave the Chrome Privacy Sandbox settings on.</p>

<p>If you want to adjust these settings or switch them off, click the three dots in the upper-right corner and go to <em>Settings &gt; Privacy and Security &gt; Ad privacy</em>. Its unclear if toggling these features off will stop Chrome from collecting these data altogether, or if it just wont share the data with advertisers. You can find out more details about each feature on <a href="https://support.google.com/chrome/answer/13355898">the Google Chrome Help page</a>.</p>

<p>Lastly, its good to remember nothing truly comes for free. Software costs money to develop. If youre not paying towards that, then its likely you  or your data  are the product. We need to revolutionise how we think about our own data and what value it truly holds.</p>

<hr>
<p>
  <em>
    <strong>
      Read more:
      <a href="https://theconversation.com/the-ugly-truth-tech-companies-are-tracking-and-misusing-our-data-and-theres-little-we-can-do-127444">The ugly truth: tech companies are tracking and misusing our data, and there's little we can do</a>
    </strong>
  </em>
</p>
<hr>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta deletes Al Jazeera presenters profile after show criticising Israel (162 pts)]]></title>
            <link>https://www.aljazeera.com/news/2023/9/10/meta-deletes-al-jazeera-presenters-profile-after-show-criticising-israel</link>
            <guid>37464482</guid>
            <pubDate>Mon, 11 Sep 2023 08:36:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.aljazeera.com/news/2023/9/10/meta-deletes-al-jazeera-presenters-profile-after-show-criticising-israel">https://www.aljazeera.com/news/2023/9/10/meta-deletes-al-jazeera-presenters-profile-after-show-criticising-israel</a>, See on <a href="https://news.ycombinator.com/item?id=37464482">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><em>Tip of the Iceberg episode investigated how Facebook targets Palestinian content related to Israel.</em></p></div><div aria-live="polite" aria-atomic="true"><p>Al Jazeera Arabic presenter Tamer Almisshal has had his Facebook profile deleted by Meta 24 hours after&nbsp;the programme Tip of the Iceberg aired an investigation into Metas censorship of Palestinian content titled <a href="https://www.youtube.com/watch?v=cnj0rmnsAdI" target="_blank">The Locked Space</a>.</p>
<p>The programmes investigation, which aired on Friday, included admissions by Eric Barbing, former head of Israels cybersecurity apparatus, about his organisations effort to track Palestinian content according to criteria that included liking a photo of a Palestinian killed by Israeli forces.</p>
<p>Then the agency would approach Facebook and argue that the content should be taken down.</p>
<p>According to Barbing, Facebook usually complies with the requests and Israels security apparatus follows up cases, including bringing court cases if need be.</p>
<p>The investigation followed up on Barbings admissions by interviewing a number of human and digital rights experts who agreed that there was a distinct imbalance in how Palestinian content is restricted.</p>
<p>The programme also interviewed Julie Owono, a member of Facebooks oversight board, who admitted there is a discrepancy in how rules are interpreted and applied to Palestinian content and added that recommendations had been sent to Facebook to correct this.</p>
<p>Al Jazeera has asked Facebook about why Almisshals profile was shut down with no prior warning or explanation. It had not received a response by the time of publication.</p>
<h2 id="targeting-a-journalist">Targeting a journalist</h2>
<p>Almisshal said the profile that was deleted is his personal page, set up by him in 2006 and verified. He had at least 700,000 followers on it.</p>
<p>After the huge success of the episode, I discovered that my personal Facebook profile had been deleted with no explanations given, he told Al Jazeera. It really does seem like some kind of revenge for the programme. We havent received any response from Facebook yet.</p>
<p>The programmes team had set out to investigate how wide the gap was between how Palestinian and Israeli posts and material are treated by Facebook.</p>
<p>To do that, it set up an experiment in which it built two different pages, one with a pro-Palestinian perspective and the other a pro-Israeli one, and ran trials on them. The team concluded that there was indeed a big discrepancy in how much scrutiny there is and how rules are applied to posts on either page.</p>
<blockquote data-width="550" data-dnt="true">
<p lang="en" dir="ltr">A day after Tamer Almisshal, a Palestinian <a href="https://twitter.com/hashtag/journalist?src=hash&amp;ref_src=twsrc%5Etfw">#journalist</a> with <a href="https://twitter.com/hashtag/Jazeera?src=hash&amp;ref_src=twsrc%5Etfw">#Jazeera</a>, presented shocking evidence of Meta's censorship of Palestinian content on its platforms during his "Tip of the Iceberg" TV show, Facebook has taken down his page.<a href="https://twitter.com/hashtag/IsraeliCrimes?src=hash&amp;ref_src=twsrc%5Etfw">#IsraeliCrimes</a> <a href="https://twitter.com/hashtag/Palestine?src=hash&amp;ref_src=twsrc%5Etfw">#Palestine</a> <a href="https://t.co/RGQm8sUuzQ">pic.twitter.com/RGQm8sUuzQ</a></p>
<p> Hadeel Abo Aita (@aita_hadeel) <a href="https://twitter.com/aita_hadeel/status/1700858011084857400?ref_src=twsrc%5Etfw">September 10, 2023</a></p></blockquote>

<p>It is not clear why Facebook would choose to delete an individuals page in response to a programme.</p>
<p>There was no explanation, no warning, Almisshal said. There had been no issues with any of the content on my page before. No message saying I had violated any rules.</p>
<p>Almisshal stands by his programme.</p>
<p>Last March, Facebook restricted my account, and it has happened other times, but usually the situation is resolved, he said. This was a journalistically sound project, and we communicated with Meta for it, giving them the opportunity to speak during the investigation.</p>
<p>But to target a journalist individually instead  I would never have expected that.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A group of open source Android apps without ads and unnecessary permissions (354 pts)]]></title>
            <link>https://www.simplemobiletools.com</link>
            <guid>37463662</guid>
            <pubDate>Mon, 11 Sep 2023 06:33:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.simplemobiletools.com">https://www.simplemobiletools.com</a>, See on <a href="https://news.ycombinator.com/item?id=37463662">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <div>
                <p>
                    <h2>Lets stay in touch!</h2>
                </p>

                <div>
                    <form action="https://www.simplemobiletools.com/newsletter" method="POST" novalidate="">
                        
                        <div>
                            <p>

                            <label>
                                <span>Your email address</span>
                            </label></p><p>Fill in the email address field with a valid email.</p>
                        </div>

                        
                    </form>
                </div>
            </div>

            <p><span>Replacing your Android apps one by one since 2016.</span>

                <span>Copyright  2023, All Rights Reserved.</span></p>
            
        </div></div>]]></description>
        </item>
    </channel>
</rss>