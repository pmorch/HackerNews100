<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 06 Nov 2024 06:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[What if your computer beeps each time it sends data to Google? (180 pts)]]></title>
            <link>https://berthub.eu/articles/posts/tracker-beeper/</link>
            <guid>42057036</guid>
            <pubDate>Wed, 06 Nov 2024 03:21:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://berthub.eu/articles/posts/tracker-beeper/">https://berthub.eu/articles/posts/tracker-beeper/</a>, See on <a href="https://news.ycombinator.com/item?id=42057036">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><nav id="main-menu" aria-label="Main Menu">
  
</nav>

      

      <main id="content">


<article>
  

  
  

  <div>
  <p>A week ago, I finally got round to implementing an idea I’d been toying with for years: what if your computer made a little bit of noise every time it sent data to Google?</p>
<p>From studying logs, I’d long known just how many sites send all your visits and clicks to (at least) Google, but a log that you have to manually create first and then analyze is not very dramatic. You need to work on it and finally you think “well yeah that is a lot”.</p>
<center>
<video controls="" width="90%">
    <source src="https://berthub.eu/articles/trackerbeeper.mp4" type="video/mp4">
    Sorry, your browser doesn't support embedded videos.
</video>
</center>
<p>The video above beeps only on Google, and it shows how the <a href="https://werkenvoornederland.nl/">official Dutch government jobs site</a> (which also advertises for the intelligence and security services) sends your every click to Google - despite never asking for your permission to do so. It also reports to Google if you clicked the button “apply for this job”, or even “call us for information”. Nice.</p>
<p>I announced the tool in a tweet:</p>
<p><img loading="lazy" src="https://berthub.eu/articles/beeper-tweet.png"></p>
<p>And within a week, the video received a million views. This spurred me on to add support for Facebook and dozens of the other trackers that infest our sites. Behold the noise when you visit some well known news sites:</p>
<center>
<video controls="" width="90%">
    <source src="https://berthub.eu/articles/trackerbeeper-wow.mp4" type="video/mp4">
    Sorry, your browser doesn't support embedded videos.
</video>
</center>

<ul>
<li><a href="https://www.rtlnieuws.nl/tech/artikel/5329774/google-tracking-klikker-googerteller-bert-hubert-privacy-online">RTLNieuws.nl</a></li>
<li><a href="https://9to5google.com/2022/08/22/app-beeps-send-data-google/">9to5Google</a></li>
<li><a href="https://www.it-daily.net/shortnews/google-teller-browser-plugin-macht-ein-geraeusch-wenn-google-daten-erhaelt">it-daily.net</a></li>
<li><a href="https://stadt-bremerhaven.de/googerteller-app-piept-jedes-mal-wenn-der-rechner-daten-an-google-uebertraegt/">Stadt Bremerhaven</a></li>
<li><a href="https://tarnkappe.info/artikel/datenschutz/googerteller-dem-datenkraken-auf-der-spur-254630.html">Tarnkappe.info</a></li>
</ul>
<h2 id="status-of-the-software">Status of the software</h2>
<p>For now, <a href="https://github.com/berthubert/googerteller">it is still pretty rough stuff</a>, suitable only for Linux, OSX and BSD users comfortable entering command lines. The goals are:</p>
<ol>
<li>Continue development on Linux until the necessary features are implemented and stable</li>
<li>Perhaps simultaneously make an Apple / OSX version available that runs with a single click</li>
<li>Create a Windows version</li>
<li>Perhaps perhaps try to implement something similar on iOS and Android, which will not be easy: phones prefer to snitch on you in full privacy</li>
</ol>
<h2 id="live-demo-installation">Live demo installation</h2>
<p>I would also <strong>love</strong> to turn this into a live demo for use on phones and tablets. The idea would then be to have a low power WiFi network. There’s a big QR code (on a poster or a big screen). If you scan that, your phone asks you if you want to join the demo WiFi.</p>
<p>And when you do and use your phone, big speakers make the tracker noises. For extra points, make one speaker per tracker, so a huge Google speaker, one for Facebook and dozens of smaller ones.</p>
<p>Especially Android phones leak information 24/7 so this should be a pretty convincing demo.</p>
<p>If anyone wants to help make this happen, let me know. All it requires is a Raspberry Pi and another phone to deliver internet connectivity.</p>
<h2 id="further-goals">Further Goals</h2>
<ul>
<li>Support all popular trackers</li>
<li>Configurable which ones you want to hear about</li>
<li>With configurable sounds (also in stereo, so “google” in the middle, “Facebook” on the right speaker)</li>
</ul>

</div>

  



</article>

<nav>
  
</nav>




      </main>

      

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mozilla is eliminating its advocacy division (152 pts)]]></title>
            <link>https://www.theverge.com/2024/11/5/24289124/mozilla-foundation-layoffs-advocacy-global-programs</link>
            <guid>42055979</guid>
            <pubDate>Tue, 05 Nov 2024 23:04:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/11/5/24289124/mozilla-foundation-layoffs-advocacy-global-programs">https://www.theverge.com/2024/11/5/24289124/mozilla-foundation-layoffs-advocacy-global-programs</a>, See on <a href="https://news.ycombinator.com/item?id=42055979">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The Mozilla Foundation laid off 30 percent of its workforce and completely eliminated its advocacy and global programs divisions, <a href="https://techcrunch.com/2024/11/05/mozilla-foundation-lays-off-30-staff-drops-advocacy-division/"><em>TechCrunch </em>reports</a>.&nbsp;</p><p>While Mozilla is best known for its Firefox web browser, the Mozilla Foundation — the parent of the Mozilla Corporation — describes itself as standing up “for the health of the internet.” With its advocacy and global programs divisions gone, its impact may be lessened going forward.</p><p>“Fighting for a free and open internet will always be core to our mission, and advocacy continues to be a critical tool in that work. We’re revisiting how we pursue that work, not stopping it,” Brandon Borrman, the Mozilla Foundation’s communications chief, said in an email to <em>The Verge. </em>Borrman declined to confirm exactly how many people were laid off, but said it was about “30% of the current team.”</p><p>This is Mozilla’s <a href="https://www.theverge.com/2024/2/13/24072184/mozilla-is-laying-off-around-60-workers-and-scaling-back-its-mastodon-instance">second round of layoffs this year</a>. In February, the Mozilla Corporation laid off around 60 workers said it would be making a “strategic correction” that would involve involve cutting back its work on a Mastodon instance. Mozilla shut down its virtual 3D platform and refocused its efforts on Firefox and AI. The Mozilla Foundation had around 120 employees before this more recent round of layoffs, according to <em>TechCrunch</em>.</p><p>In an email sent to all employees on October 30th, Nabhia Syed, the foundation’s executive director, said that the advocacy and global programs divisions “are no longer part of our structure.”</p><p>“Navigating this topsy-turvy, distracting time requires laser focus — and sometimes saying goodbye to the excellent work that has gotten us this far because it won’t get us to the next peak,” wrote Syed, who previously worked as the chief executive of <em>The Markup</em>, an investigative news site. “Lofty goals demand hard choices.”&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unix Programmer's Manual Third Edition [pdf] (1973) (102 pts)]]></title>
            <link>https://dspinellis.github.io/unix-v3man/v3man.pdf</link>
            <guid>42055644</guid>
            <pubDate>Tue, 05 Nov 2024 22:12:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dspinellis.github.io/unix-v3man/v3man.pdf">https://dspinellis.github.io/unix-v3man/v3man.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=42055644">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Tracking down a mysterious skateboarder from 1979 (165 pts)]]></title>
            <link>https://www.ncrabbithole.com/p/tony-hawk-fayetteville-nc-girl-skateboarder-1979</link>
            <guid>42055558</guid>
            <pubDate>Tue, 05 Nov 2024 22:00:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ncrabbithole.com/p/tony-hawk-fayetteville-nc-girl-skateboarder-1979">https://www.ncrabbithole.com/p/tony-hawk-fayetteville-nc-girl-skateboarder-1979</a>, See on <a href="https://news.ycombinator.com/item?id=42055558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Here’s a really cool picture:</p><div data-attrs="{&quot;instagram_id&quot;:&quot;DB42tOlymvY&quot;,&quot;title&quot;:&quot;A post shared by @tonyhawk&quot;,&quot;author_name&quot;:&quot;tonyhawk&quot;,&quot;thumbnail_url&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/__ss-rehost__IG-meta-DB42tOlymvY.jpg&quot;,&quot;timestamp&quot;:null,&quot;belowTheFold&quot;:false}" data-component-name="InstagramToDOM"><p><a href="https://instagram.com/p/DB42tOlymvY" target="_blank" rel=""><img src="https://substackcdn.com/image/fetch/w_640,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F__ss-rehost__IG-meta-DB42tOlymvY.jpg"></a></p></div><p><span>A former colleague from my TV days, Michelle Chernicoff, sent me this picture of a young girl skateboarding underneath an umbrella on a rainy day. She saw it after Tony Hawk </span><a href="https://www.instagram.com/p/DB42tOlymvY/" rel="">posted it to his Instagram page</a><span> on Saturday. According to his caption, Tony </span><em>really</em><span> wanted to know who it was:</span></p><blockquote><p>New fav mystery skater unlocked: from Fayetteville Observer (NC), 1973. Style, grace, confidence, and… goofy footed, in the rain!! I hope she’s still around.</p></blockquote><p><span>Folks, we love two things around here: </span><a href="https://www.ncrabbithole.com/p/tony-hawk-went-to-some-coffee-shops" rel="">Tony Hawk</a><span> and </span><a href="https://www.ncrabbithole.com/p/steven-spielberg-indiana-jones-nc-highway-patrol" rel="">decades-old mysteries</a><span>. </span></p><p><span>There were two initial clues: The date and the newspaper. I plugged the picture into Google Image Search to see if I could find the original post. Back on September 21, blackarchives.co had </span><a href="https://www.instagram.com/blackarchives.co/p/DALpuCIur9i/?img_index=1" rel="">posted a collection of images</a><span> taken by </span><em>Fayetteville Observer</em><span> photographers between 1973 and 1987. The image of the skateboarder in the rain was the first one in the carousel. Tony Hawk himself commented the same day: “New fav skater unlocked.”</span></p><p><span>Next, I did some searches on the </span><em>Observer</em><span>’s website. The image showed up in </span><a href="https://www.fayobserver.com/picture-gallery/news/2019/06/21/from-the-archives-fayetteville-observer/69320732007/" rel="">a monthly roundup of old ‘70s photos</a><span> that staff photographer </span><a href="https://www.instagram.com/aacraft/" rel="">Andrew Craft</a><span> had pulled from the paper’s negative archive in 2019 (“Anytime I come across a skateboarding image I always scan it,” he said later in an Instagram comment). This photo also included a date: January 20, 1979 (not 1973, as Hawk’s post said). But that was it. No location. No name. No back story.</span></p><p>So I turned to the people you turn to when times get tough: Librarians.</p><p><a href="https://x.com/TarHeelFoodways" rel="">John O’Connor</a><span> is the manager of the Robinson-Spangler Carolina Room at the Charlotte Mecklenburg Library. He’s also a </span><em>Rabbit Hole</em><span> reader, and he’s helped me try to find archival material in the past. I sent him a message on Sunday morning to see if his library had </span><em>Fayetteville Observer</em><span> on microfilm. No, he said, but he’d call the Cumberland County Public Library as soon as they opened in the afternoon. There, library associate Matt Morgan got the film out, found the January 21, 1979 edition of the </span><em>Fayetteville Observer</em><span>, and sent over what he found:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png" width="1048" height="1254" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1254,&quot;width&quot;:1048,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1852844,&quot;alt&quot;:&quot;scan of 1979 newspaper&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="scan of 1979 newspaper" title="scan of 1979 newspaper" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>There, on page 1B, was the image and the caption that had the rest of the details:</p><blockquote><p>Shaundra Shane didn’t want to get wet, but the idea of spending a Saturday without her skateboard was to much to handle. So she got her umbrella and wheeled down Dinsmore Drive.</p></blockquote><p><span>From there, I tracked down the 10-year-old girl in the picture. She’s 56 now, still lives in Fayetteville, and her name is Shaunda, not Shaundra (the caption misspelled it). I reached out Sunday night, and we talked after she got off of work on Monday afternoon. She remembers the day the picture was taken. An </span><em>Observer</em><span> photographer, the late Steve Aldridge, lived across the street from her on Dinsmore Drive. “Mr. Steve saw me out there riding my skateboard in the rain,” she says. He asked her if she could get permission from her grandparents to take a picture. He also asked if she could come back with a prop. “He wanted me to have an umbrella, so I got an umbrella,” she says. “He took that picture and that was it.”</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png" width="1456" height="920" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9b72356c-16c8-453b-948f-77042873849b_2076x1312.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:920,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4286930,&quot;alt&quot;:&quot;Google map street view of Dinsmore Drive in 2023&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Google map street view of Dinsmore Drive in 2023" title="Google map street view of Dinsmore Drive in 2023" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>A 2023 view from the area where the original picture was taken on Dinsmore Drive in Fayetteville (via Google Street View)</figcaption></figure></div><p>Shaunda lived with her grandparents in a neighborhood that she loved. Family and friends were all close by. She’d been a skater for about a year, ever since her uncle brought her a little pink board with metal wheels. Her friends were more into roller skating back then, but going to the rink was kind of boring, she thought. It was limiting. Skateboarding allowed her to explore. “I was free,” Shaunda says. “You’re outside. There’s the wind. There’s just, like, a freeness, you know?”</p><p>Shaunda was the only member of her group that skateboarded, and she kept doing it for years. She got faster. Started learning tricks. But she stopped skateboarding after she got her first boyfriend in middle school. As a teenager, she started spending more time with him and less time on the board. Plus, she didn’t need to skateboard to get around—he had a car.</p><p>After that, skateboarding and the image of her doing it faded from her life. “It was just a picture in the paper,” she says. At some point, a car backed over her little pink board.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.ncrabbithole.com/p/tony-hawk-fayetteville-nc-girl-skateboarder-1979?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.ncrabbithole.com/p/tony-hawk-fayetteville-nc-girl-skateboarder-1979?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p>Shaunda went on living. She moved to a new place in Fayetteville. She got married and divorced. She had three kids. She started working as a patient access specialist at Cape Fear Valley Health.</p><p>Then, back in September, she got a call from a woman who still lived in her old neighborhood. She told Shaunda that she had the old picture of her skateboarding in the rain and gave it to her. A week later, the same picture started to show up on Facebook. Shaunda called up her friend, who swore that she didn’t put the image online. “I said, well, did your mom do it? And she was like, girl, my mom don’t know how to post nothing on Facebook,” she says, laughing.</p><p>It was just a coincidence. Someone online had rediscovered the picture at the same time as Shaunda’s friend. And for a while, that was it. Shaunda made it her profile picture. A few friends tagged her when they saw the image online. They made comments. They liked her ponytails. They dropped fire emojis.</p><p>Then Tony Hawk posted the picture. And then I called.</p><p><span>“It's amazing,” Shaunda says. “I kind of have butterflies, you know? I definitely do know who Tony Hawk is. I have watched Tony Hawk.” The </span><em>Fayetteville Observer</em><span> got in touch, and they’re having her re-create the photo, 45 years later (UPDATE: </span><a href="https://www.fayobserver.com/story/lifestyle/2024/11/04/who-is-the-mystery-skater-shared-by-tony-hawk-shares-fayetteville-observer-photo/76047957007/?taid=672975236877af0001c06d8e" rel="">Here’s the picture</a><span>). A local skate shop has been in touch. Her phone is blowing up.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic" width="400" height="533.2417582417582" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:400,&quot;bytes&quot;:1193953,&quot;alt&quot;:&quot;Shaunda Shane, modern day pic&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Shaunda Shane, modern day pic" title="Shaunda Shane, modern day pic" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Shaunda Shane, today.</figcaption></figure></div><p><span>Also, she had to learn what it meant to ride “goofy footed.” It’s when you ride a board with your right foot forward instead of your left. “Oh,” Shaunda says. “Well, I </span><em>am</em><span> left-handed.”</span></p><p>Shaunda said she hasn’t quite been able to re-create the freedom she felt on a board all those years ago, although riding a motorcycle comes pretty close. In any event, she doesn’t really have the time to skateboard now anyway. “I mean, literally all I do is work,” she jokes. “Trying to pay these bills.”</p><p>Her son skateboards, though. And so, months ago, she got back up on a board for the first time in years. Just for a little bit. She’s a little afraid of falling, but the picture really brings back memories of a free-ranging childhood in a nice little neighborhood. “I probably would be skateboarding now,” she says, “if I really thought I could.”</p><p><strong>MANY UPDATES (11/5/24, 9:06 a.m.):</strong><span> First up, Tony Hawk has seen this article and </span><a href="https://x.com/deftlyinane/status/1853651377840374245" rel="">name-dropped Shaunda in an Instagram story</a><span>, and Shaunda is aware that Tony Hawk has name-dropped her, but she hasn’t read this story yet because she was on her way to work. We’re all leading busy lives, people!</span></p><div data-attrs="{&quot;instagram_id&quot;:&quot;DB_l10pPUn5&quot;,&quot;title&quot;:&quot;A post shared by @tonyhawk&quot;,&quot;author_name&quot;:&quot;tonyhawk&quot;,&quot;thumbnail_url&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/__ss-rehost__IG-meta-DB_l10pPUn5.jpg&quot;,&quot;timestamp&quot;:null,&quot;belowTheFold&quot;:true}" data-component-name="InstagramToDOM"><p><a href="https://instagram.com/p/DB_l10pPUn5" target="_blank" rel=""><img src="https://substackcdn.com/image/fetch/w_640,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F__ss-rehost__IG-meta-DB_l10pPUn5.jpg" loading="lazy"></a></p></div><p><span>Also, the </span><em>Fayetteville Observer</em><span> posted its version of this story last night, which included the new picture of Shaunda riding a skateboard with an umbrella (</span><a href="https://www.fayobserver.com/story/lifestyle/2024/11/04/who-is-the-mystery-skater-shared-by-tony-hawk-shares-fayetteville-observer-photo/76047957007/?taid=672975236877af0001c06d8e" rel="">you can go to their website to see it</a><span>). The phenomenally talented </span><a href="https://www.instagram.com/aacraft" rel="">Andrew Craft</a><span> took that photo. He’s also the one who scanned the original picture of Shaunda from 1979 and posted it online, and really, there’s no story here without him. He also </span><a href="https://www.fayobserver.com/picture-gallery/news/local/2024/11/04/from-the-archives-fayetteville-nc-skateboarders-in-the-70s-and-80s/76054303007/?taid=67296e172eec1a0001fac113" rel="">posted even </a><em><a href="https://www.fayobserver.com/picture-gallery/news/local/2024/11/04/from-the-archives-fayetteville-nc-skateboarders-in-the-70s-and-80s/76054303007/?taid=67296e172eec1a0001fac113" rel="">more </a></em><a href="https://www.fayobserver.com/picture-gallery/news/local/2024/11/04/from-the-archives-fayetteville-nc-skateboarders-in-the-70s-and-80s/76054303007/?taid=67296e172eec1a0001fac113" rel="">historical photos of skateboarders</a><span> around Fayetteville, including another archival shot of Shaunda skateboarding with two friends. Check it out.</span></p><p><span>Lastly, a correction: I originally said that riding goofy foot meant riding with your left foot forward. It’s actually the opposite, which I would have known had I been paying attention TO THE PICTURE THAT I BASED THIS ENTIRE STORY ON. Anyhow, I regret the error. My penance shall be listening to the </span><a href="https://tonyhawkgames.fandom.com/wiki/Tony_Hawk%27s_Pro_Skater_2_Soundtrack" rel="">Tony Hawk’s Pro Skater 2 soundtrack</a><span> for the rest of the day (which I was gonna do anyway).</span></p><p data-attrs="{&quot;url&quot;:&quot;https://www.ncrabbithole.com/p/tony-hawk-fayetteville-nc-girl-skateboarder-1979/comments&quot;,&quot;text&quot;:&quot;Leave a comment&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.ncrabbithole.com/p/tony-hawk-fayetteville-nc-girl-skateboarder-1979/comments" rel=""><span>Leave a comment</span></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mozilla Foundation lays off 30% staff, drops advocacy division (162 pts)]]></title>
            <link>https://techcrunch.com/2024/11/05/mozilla-foundation-lays-off-30-staff-drops-advocacy-division/</link>
            <guid>42054867</guid>
            <pubDate>Tue, 05 Nov 2024 20:29:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/11/05/mozilla-foundation-lays-off-30-staff-drops-advocacy-division/">https://techcrunch.com/2024/11/05/mozilla-foundation-lays-off-30-staff-drops-advocacy-division/</a>, See on <a href="https://news.ycombinator.com/item?id=42054867">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">The Mozilla Foundation, the nonprofit arm of the Firefox browser maker Mozilla, has laid off 30% of its employees as the organization says it faces a “relentless onslaught of change.”</p>

<p>When reached by TechCrunch, Mozilla Foundation’s communications chief Brandon Borrman confirmed the layoffs in an email.</p>







<p>“The Mozilla Foundation is reorganizing teams to increase agility and impact as we accelerate our work to ensure a more open and equitable technical future for us all. That unfortunately means ending some of the work we have historically pursued and eliminating associated roles to bring more focus going forward,” read the statement shared with TechCrunch.</p>

<p>According to its annual tax filings, the Mozilla Foundation <a href="https://projects.propublica.org/nonprofits/organizations/200097189" target="_blank" rel="noreferrer noopener nofollow">reported having 60 employees</a> during the 2022 tax year. The number of employees at the time of the layoffs was closer to 120 people, according to a person with knowledge. When asked by TechCrunch, Mozilla’s spokesperson did not dispute the figure.</p>

<p>This is the second layoff at Mozilla this year, the first <a href="https://techcrunch.com/2024/02/13/mozilla-downsizes-as-it-refocuses-on-firefox-and-ai-read-the-memo/">affecting dozens of employees</a> who work on the side of the organization that builds the popular Firefox browser.</p>

<p>Mozilla is <a href="https://www.mozilla.org/en-US/about/governance/organizations/" target="_blank" rel="noreferrer noopener nofollow">made up of several organizations</a>, one of which is the Mozilla Corporation, which develops Firefox and other technologies, and another is its nonprofit and tax-exempt Foundation, which oversees Mozilla’s corporate governance structure and sets the browser maker’s policies.</p>

<p>Much of Mozilla’s work focused on advocating for privacy, inclusion, and decentralization of technologies, and “to create safer, more transparent online experiences for everyone,” which ultimately benefit the browser maker and its users.</p>

<p>Announcing the layoffs in an email to all employees on October 30, the Mozilla Foundation’s executive director Nabiha Syed confirmed that two of the foundation’s major divisions — <a href="https://blog.mozilla.org/en/mozilla/mozilla-welcomes-ashley-boyd-vp-of-advocacy/" target="_blank" rel="noreferrer noopener nofollow">advocacy</a> and <a href="https://foundation.mozilla.org/en/blog/welcoming-j-bob-alotta-mozilla-foundations-new-vp-global-programs/" target="_blank" rel="noreferrer noopener nofollow">global programs</a> — are “no longer a part of our structure.” </p>

<p>After publication, Borrman told TechCrunch that “advocacy is still a central tenet of Mozilla Foundation’s work and will be embedded in all the other functional areas,” without providing specifics.</p>

<p>The move, according to Syed, is in part to produce a “unified, powerful narrative from the Foundation,” including revamping the foundation’s strategic communications.</p>







<p>“Our mission at Mozilla is more high-stakes than ever,” wrote Syed in an email to staff, a copy of which was shared with TechCrunch. “We find ourselves in a relentless onslaught of change in the technology (and broader) world, and the idea of putting people before profit feels increasingly radical.”&nbsp;</p>

<p>“Navigating this topsy-turvy, distracting time requires laser focus — and sometimes saying goodbye to the excellent work that has gotten us this far because it won’t get us to the next peak. Lofty goals demand hard choices,” wrote Syed.</p>

<p>Syed, who joined <a href="https://foundation.mozilla.org/en/blog/mozilla-foundation-welcomes-nabiha-syed-as-executive-director/" target="_blank" rel="noreferrer noopener nofollow">the Mozilla Foundation in February</a>, previously served as chief executive at data journalism and investigative news site The Markup.</p>

<p><em>Updated with comment from Mozilla.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Traceroute Isn't Real (109 pts)]]></title>
            <link>https://gekk.info/articles/traceroute.htm</link>
            <guid>42054835</guid>
            <pubDate>Tue, 05 Nov 2024 20:22:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gekk.info/articles/traceroute.htm">https://gekk.info/articles/traceroute.htm</a>, See on <a href="https://news.ycombinator.com/item?id=42054835">Hacker News</a></p>
<div id="readability-page-1" class="page">
<p><strong><a href="https://gekk.info/">gekk.info</a> « <a href="https://gekk.info/articles/index.html">articles</a></strong></p>




<h2>Traceroute isn't real, or: Whoops! Everyone Was Wrong Forever</h2>



<p>There is no such thing as traceroute.</p>
<p>I used to deliver network training at work. It was freeform, I was given wide latitude to design it as I saw fit, so I focused on things that I had seen people struggling with - clearly explaining VLANs in a less abstract manner than most literature, for instance, as well as actually explaining how QoS queuing works, which very few people understand properly.</p>
<p>One of the "chapters" in my presentation was about traceroute, and it more or less said <em>"Don't use it, because you don't know how, and almost nobody you'll talk to does either, so try your best to ignore them."</em> This is not just my opinion, it's backed up by people much more experienced than me. For a good summary I highly recommend <a href="https://www.slideshare.net/RichardSteenbergen/a-practical-guide-to-correctly-troubleshooting-with-traceroute" target="_blank" rel="nofollow noopener noreferrer">this presentation</a>.</p>
<p>But as good as that deck is, I always felt it left out a crucial piece of information: Traceroute, <em>as far as the industry is concerned</em>, does not exist.</p>
<p>Look it up. There is no RFC. There are no ports for traceroute, no rules in firewalls to accommodate it, no best practices for network operators. Why is that?</p>
<h3>Traceroute has no history</h3>
<p>First off: Yes, there <em>is</em> a traceroute RFC. It's <a href="https://datatracker.ietf.org/doc/html/rfc1393" target="_blank" rel="nofollow noopener noreferrer">RFC1393</a>, it's 31 years old, and to my knowledge nothing supports it. The RFCs are jam-packed with brilliant ideas nobody implemented. This is one of them. The traceroute we have is completely unrelated to this.</p>
<p>Unsurprisingly however, it's a good description of how a traceroute protocol <em>should</em> work. You send a packet to a given destination, with a special flag set, and any machine it passes through observes the flag and says "oh, this packet is meant to be traced," so it generates an ICMP Traceroute response and sends it back to the originating host.</p>
<p>The host, then, sends a single packet and receives a flood of responses describing the path that packet took, definitively. Great! Or, I mean, it would be, if anything supported it. And if it was 1993.</p>
<p>As the linked presentation explains, traceroute simply no longer works in the modern world, at least not "as designed" - and it no longer <em>can</em> work that way, for several reasons not the least that networks have been abstracted in ways it did not anticipate.</p>
<p>There are now things like MPLS, which operate by encapsulating IP - in other words, putting a bag over a packets head, throwing it in the back of a van, driving it across town and letting it loose so it has no idea how far it's traveled. Without getting much further into how that works: It is completely impossible for it to satisfy the expectations of traceroute.</p>
<p>This "tool" works purely at layer 3, so it's impossible for it to adapt to the sort of "layer 12-dimensional-chess" type shenanigan that MPLS does - and there are other problems, but they're all getting ahead of reality, since traceroute never even worked correctly as intended, and there's no reason it would.</p>
<p>Traceroute, you see, is "clever," which is an engineering term that means "fragile." When programmers discover something "clever," any ability they may have had to assess its sustainability or purpose-fit often goes out the window, because it's far more important to embrace the "cleverness" than to solve a problem reliably.</p>
<p>The RFC process is likely not perfect - it's basically an enormous committee system, so, that's troubling - but it does at least constitute a review and consensus process. Had someone written out a spec for traceroute, and then vendors had agreed to implement it, that would be one thing. But that is not what happened.</p>
<h3>Traceroute is a filthy hack</h3>
<p>From the traceroute man page (1987):</p>
<pre>Implemented by Van Jacobson from a suggestion by Steve Deering.
Debugged by a cast of thousands with particularly cogent
suggestions or fixes from C. Philip Wood, Tim Seaver and Ken Adelman.</pre>
<p>I can't find any proper history of the tool, but my <em>impression</em> and my <em>assumption</em> is that it is simply a behavior that someone <em>noticed was possible</em>. Engineers did not get together and design a system for this; some people just realized that it was <em>a side effect of other network behavior not intended to accomplish this goal.</em></p>
<p>In other words, it's an exploit, and that is really the best way to describe both how it works, and why it's a bad idea.</p>
<hr>
<p><strong>Here's how traceroute works:</strong></p>
<p>When you send a packet to a destination, it often has to go through multiple routers, or "hops."</p>
<p>To prevent packets from cycling indefinitely in a network due to routing loops (router A points to router B which points to router A...) they include a Time-To-Live field, which is set to a reasonably high value when a packet is created, and each machine that the packet passes through decrements that field by one.</p>
<p>When the field hits zero, the packet gets thrown away. <em>As a courtesy,</em> the router that's dropping the packet has the <em>option</em> to generate a new packet, using the ICMP protocol, with the subtype <em>"TTL Exceeded,"</em> and send it back to the originating machine, to let it know there's something wrong with the network path.</p>
<p>These clever fellows in 1987 realized that by manipulating the TTL value, you can choose which router will send that ICMP message.</p>
<p>Send a packet with the TTL set to 1. The first router you hit will decrement it to zero. The packet is now "dead", so it drops the packet, and sends back TTL Exceeded. That response will originate from the router's own IP address - congratulations, you now have the IP of the first hop.</p>
<p>Now send another with the TTL set to 2. The first router will decrement it to 1 and pass it, and the <em>second</em> one will decrement to zero and drop it. Now you have <em>its</em> IP address.</p>
<p>Repeat, increasing TTL each time, until the final hop responds. You now have your complete path.</p>
<hr>
<p>This is indeed quite clever, but don't lose sight of what is going on here. TTL Exceeded is simply <em>not meant for this.</em> It is a message meant to diagnose a specific, unrelated kind of network malfunction. It's not intended for tracing paths, and for reasons I'll explain, it's also exactly the kind of feature that may exist in a lab, and in the first few experimental networks, but gets abandoned as soon as money enters the picture.</p>
<h3>DJ Shadow - Why Hip Hop Sucks In 96 (It's The Money)</h3>
<p><em>TTL Exceeded</em> is not a "feature."</p>
<p>Features are things that enable functionality. It doesn't do that.</p>
<p>Features are things that affect end-user experience. It doesn't do that.</p>
<p>TTL Exceeded is purely informational. It's useful to exactly one person: a network engineer. It would be absolutely untenable to report this sort of error to an end user, since there's positively nothing they can do about it, so no application will ever do this.</p>
<p>Not only were these messages not intended for end users, they weren't even intended for network operators as we know them now.</p>
<p>In 1987, virtually every network admin could get an email address for the admin of pretty much any other network, worldwide, with a couple phone calls or a whois lookup. That meant it was practical to troubleshoot <em>other peoples networks</em>, which are often where these errors are seen. Nowadays? Forget about it. Hah. Wow. <em>No way.</em></p>
<p>If you get a TTL Exceeded while trying to reach another host through the <em>internet</em>, there is a zero percent probability that you can get traction on your problem unless you are a Fortune 500 - and even then it will be tough. At least half the companies that are likely to be involved simply do not provide any form of support for problems involving less than millions of hosts.</p>
<p>It is, generally speaking, not possible to call AT&amp;T and say "Hey, when I try to ping one of your subscribers in California from a Level3 circuit in New York, I'm hitting a routing loop." I have worked for an ISP with direct peering with those networks and that simply never worked. We got incompetent, consumer-grade support techs and the issue went nowhere, if we even had a contact at all.</p>
<p>It's even harder to call the exchange partners, the network providers that may sit in between AT&amp;T and Level3 in this equation. Nobody will even tell you who they are, and if they did, <em>there is simply nobody to call.</em> Those phone numbers don't exist unless you're a network engineer at one of their direct partners who is calling to report that a fiber port is down.</p>
<p>No, AT&amp;T is not going to push your complaint up the line to XO. Haha. No.</p>
<p>Problems like this are fairly rare these days, which makes it even less likely that anyone will be on hand to work on them. Most of the time, IME, they get resolved through Brownian Troubleshooting: large scale network maintenance happens for unrelated reasons and <em>incidentally</em> fixes the problem.</p>
<p>So traceroute, on an internet scale, has been useless for ages. You think you see a routing problem? So what? There's absolutely nothing you can do about it.</p>
<p>With that information, go ahead and ask yourself if you think anyone, at any network hardware company, has given a shit about implementing TTL Exceeded since the 90s. The answer is obvious: No. Without a doubt, this is not on anyone's priority list.</p>
<p>If you're at Juniper, nobody is clamoring for this. You do not have ISPs threatening to switch to Cisco (lmao!) just because you didn't implement TTL Exceeded correctly, because they aren't using it. The kind of problems ISPs care about are "we lost the entire US northeast" or "we can't reach Comcast, at all." The NOCs involved at that point may use traceroute, but they will get by without it. Nobody is going to make a C-level escalation with Juniper over it.</p>
<p>So, as a network hardware vendor, with a certain budget and a whole galaxy of internet standards to implement, are you going to put time into this? Absolutely not.</p>
<p>Academics, perhaps. People doing research and experiments at universities 35 years ago might have stuck to the specs religiously, but there is <em>no financial reason whatsoever to implement this correctly.</em></p>
<p>But then, we're ahead of ourselves again. Because what is a "correct" implementation?</p>
<h3>Nothing involving a router is "correct", but</h3>
<p>RFC 792, <em>"INTERNET CONTROL MESSAGE PROTOCOL",</em> explains how to implement TTL Exceeded (which I believe is technically called "Time Exceeded"):</p>
<pre>   Description

      If the gateway processing a datagram finds the time to live field
      is zero it must discard the datagram.  The gateway <em>may</em> also notify
      the source host via the time exceeded message.

      If a host reassembling a fragmented datagram cannot complete the
      reassembly due to missing fragments within its time limit it
      discards the datagram, and it <em>may</em> send a time exceeded message.</pre>
<p>Anyone who knows how to read an RFC understands the crushing solemnity of MAY.</p>
<p>Something that you MAY do is something that WILL NOT be done when it counts. MAY, in RFC terminology, means exactly what the dictionary says it should: The implementer can do it if they want.</p>
<p>That means that it is "standards-compliant" to create a router that has absolutely no implementation of ICMP TTL Exceeded. "May" can mean "never." It is completely up to the vendor.</p>
<p>And wouldn't you know it: vendors <em>do</em>, in fact, choose to <em>never</em> send these messages, and for good reason: It's hard!</p>
<p>The linked presentation (seriously! read it! please! you will benefit!) addresses why this is, and in short it comes down to the fact that routers are basically supercomputers. Consider that a core router at an ISP is potentially handling <em>billions</em> of packets per second. Running all that through a conventional CPU is absurd, and nobody has done this in decades.</p>
<p>Instead, routers contain custom, purpose-built hardware - called the <em>data plane</em> - consisting of dedicated silicon with the sole ability to look at <em>the parts of a packet that matter for routing purposes</em> and ask a couple very simple questions, e.g. <em>"Do I have a way to get this to its next hop?"</em> and <em>"does it still have time to live?"</em></p>
<p>There are probably other details, but you get my point - it's <em>highly</em> optimized.</p>
<p>99.99% of the packets that pass through such a device simply come in one port, get glanced at, and are then hurled out of another port so the silicon can get on to the next packet. During all of this, the actual <em>computer</em> part of the router, the thing that can make complex decisions, is idle.</p>
<p>Yes, routers do contain general-purpose computers; they're pitiful little things. From the linked presentation (read it!!):</p>
<pre>A 320-640+ Gbps router may only have a 600MHz CPU
ICMP generation is <em>NOT</em> a priority for the router.
</pre>
<p>Yeah. The CPU is... Not Fast.</p>
<p>As I implied, this is how supercomputers often work: you have a <em>massive</em> array of extremely fast processors, that can only solve certain, very well defined kinds of problems, and then off to the side you have some horrible little Core i3 Ideapad Yoga whose sole job is to feed program and data into the thing and then pull the string on its back.</p>
<p>With a supercomputer, feeding it invalid data will simply crash the process and you'll have to start all over. That's not an option with networks, since you can't control the incoming data, so routers need a way to handle exception scenarios. That's where the computer - known as the <em>control plane</em> - comes in.</p>
<p>In addition to feeding configs to the data plane, the control plane CPU is responsible for handling unexpected situations. If an interface goes down, the data plane simply starts dropping packets (if it has no other paths.) It takes no other actions; it's the control plane's job to notice this event and do something about it, e.g. sending SNMP traps so someone in a NOC can investigate.</p>
<p>I don't know how many <em>TTLs</em> get <em>Exceeded</em> these days, but even if a router sees tons of them every day, there's nothing it can do to fix the problem, and sending TTL Exceeded is a <em>MAY</em>, not a <em>MUST</em>, so no vendor is going to spend an extra $100,000 to design circuitry to generate and return those responses. That means that any packet that runs out of TTL will have to get forwarded to the control plane, which will decide if and when to send a response.</p>
<p>It goes without saying that the control plane is a very busy little bee. It's bad enough that it has to handle all the "exceptions", which are going to be <em>plentiful</em> in a carrier network with millions of hosts passing through it, but it <em>also</em> has to handle any actual self-destined traffic.</p>
<p>In addition to the millions of hosts that an internet router has to arbitrate between, it also has its own IP addresses, which people rudely try to interact with all day long. When you ping a router, you're making that poor little 600MHz ARM chip find time to deal with your traffic, <em>not</em> the terabit-per-second monster that it's married to. Same goes for SNMP queries, regular config backups, and other forms of management access.</p>
<p>Other than traceroute, <em>TTL Exceeded</em> serves very little purpose in the modern world, and with traceroutes being such a tiny percentage of traffic, it is perfectly reasonable for network admins to not care if it works or not. When you put all this together, it becomes apparent that most network providers are never going to spend a second thinking about this.</p>
<p>You can easily confirm this is true. Run a traceroute... anywhere. Yahoo dot com. You will see nodes that never respond, 9 times out of 10.</p>
<h3>The Worst Diagnostics In The World</h3>
<p>I cannot even guess how many times I have seen network techs see one hop not respond and say "well it looks like hop 5 is down, so that's your problem," even though hop 6 is responding.</p>
<p>It is impossible for me to imagine how they think the internet works, but they're playing against a stacked deck, because traceroute is just <em>the worst diagnostic tool imaginable.</em></p>
<p>A good tool gives you a go, a no-go, or information. That is, it tells you something is working, or broken, or provides data you can interpret.</p>
<p>Traceroute does provide a single "go" outcome: If you see a trace get <em>all the way through to the last node</em>, well, okay, that's a success. The path is probably fine.</p>
<p>However, it also only provides a single "no-go" outcome, and it's not the one people think. Lack of response from hosts is not a failure. The <em>sole</em> failure you can identify reliably from traceroute is a network loop. If you see the same pair of nodes respond over and over, then you have a loop.</p>
<p>...and that information is almost completely useless, because this is the exact problem that TTL Exceeded is meant to diagnose, so you can just use it as intended. Just ping the target, and you'll see a TTL Exceeded response from one of the two routers that is looping the packet, identifying the failure point. Admittedly, traceroute does tell you <em>both</em> of those names, which is convenient.</p>
<p>Inadvertent routing loops are incredibly rare however, and 99% of the ones that I have seen were actually caused by network interfaces being down, and would have been discovered and resolved through ordinary, thorough network review.</p>
<p>As far as information? Well, read the presentation. The information provided by traceroute is limited, objectively incorrect and misleading in many cases, and fiendishly hard to interpret.</p>
<p>Lack of response from a node means nothing.</p>
<p>Even if <em>all</em> the nodes past a certain point aren't responding, <em>that also means nothing.</em></p>
<p>If the nodes have high latency, <em>that also means nothing.</em></p>
<p>If they respond on some probes and not others, <em>that also means nothing.</em></p>
<p><em>Nothing you see in a traceroute means anything, because it is all accidental.</em></p>
<p>You are sending a packet through a network that <em>did not plan for it</em>. Nobody has taken steps to ensure your traceroute should succeed. There are no "Traceroute" checkboxes or statements in router configs. There's a really spicy reason for that, too: Traceroute does not even meet the most minimal definitions of a network protocol.</p>
<p>It's not a special kind of ICMP message, or a UDP or TCP packet that uses a defined port. You cannot "permit traceroute" in a firewall, because it has no standard characteristics. A lot of people think traceroute sends pings - this is an option, but never the default behavior AFAIK.</p>
<p>By default, traceroute simply sends <em>a gibberish UDP packet on a random pair of ephemeral ports.</em> The entire point is to be thrown away before a host even gets a <em>chance</em> to consume it, so the contents are irrelevant.</p>
<p>That means that if you were <em>trying</em> to prepare a network to handle traceroute, you wouldn't be able to. From a network perspective, traceroute does not exist.</p>
<p>It's simply an exploit, a trick someone discovered, so it's to be expected that it has no defined qualities. It's just random junk being thrown at a host, hoping that everything along the paths responds in a way that they are <em>explicitly not required to.</em> Is it any surprise that the resulting signal to noise ratio is awful?</p>
<h3>So What Does All This Mean</h3>
<p>It means that you can't run a traceroute unless you <em>know what you expect to see.</em></p>
<p>When you're tracing inside a network that you control - such as a large enterprise WAN, multiple sites connected with VPNs, or an ISP that you work for - you can guess what each hop will look like, or at least look at the results and suss out whether they looks like they "should."</p>
<p>If you trace from, say, a server at one business location to one at another, you might see your local prem router, then a network edge router, a few core routers, another edge and then another prem router.</p>
<p>From this you can guess, pretty reliably, that you made it all the way to the destination, but either had trouble reaching the specific host (investigate the local router/firewall) or that the host is ACLed or doesn't send ICMP responses (do packet captures on the host.)</p>
<p>If you're tracing through a network you don't control, you have <em>no idea</em> how it's supposed to work. If you're a seasoned network tech who's seen some shit (and, ideally, worked on provider-scale networks) then you can run a traceroute over an unknown network and <em>maybe, possibly,</em> suss out something, but there are no guidelines, it's pure gut feeling: <em>does this look right?</em></p>
<p>If you aren't that experienced however, you should avoid it, because you are not immune to propaganda. When you see high latency, hops not responding or whatever, that information will stick in your head. Despite your best efforts, it will affect the course of your troubleshooting even though you would not be able to say, if asked, what those results <em>meant</em> and what should be <em>done with them.</em></p>
<p>As a diagnostician, you should ask yourself one question before performing any test: <em>"What would I do if the outcome was x? And what if it was y?"</em></p>
<p>Can you fill in x and y? Can you answer either question? If not - why run the test?</p>
<p>And if you <em>do</em> run the traceroute anyway, <em>god forbid</em> you mention it to someone else. Do not write down the results unless you think you actually know what they mean, because no matter how offhandedly you do it, whoever comes across it is <em>guaranteed</em> to see it as a lifeline.</p>
<p>Network techs are mostly incompetent. It is a sad truth, and it's not their fault. People get pressed into jobs that they are told are far less complicated than they actually are. It has been my experience that <em>easily</em> 75% of people working networking jobs are operating in a state of absolute terror, trying to keep their head above water with problems they don't really understand at all.</p>
<p>If you say "hop five isn't responding," congratulations - you just identified "the cause of the problem" as far as all those folks are concerned, and there's no way to get that piss out of the pool.</p>
<p>Whoever you said it in front of is going to refuse, <em>aggressively</em>, to do a lick of additional troubleshooting until "hop five" starts responding. If that's clearly a node that nobody on the conf call or email thread has access to, then everyone's going to throw up their hands and say "Well I Guess We Just Have To Hope It Starts Working." I have seen this countless times.</p>
<p>It happens because, fundamentally, troubleshooting networks <em>sucks.</em></p>
<p>If you don't have total control of the entire path, end to end, with admin access <em>and</em> expertise on every node along the way, there is no way to get a complete picture of what's going on. That kind of access is extremely rare; you're probably a high-ranking network architect if you have it; and <em>even with all that access</em> there are still plenty of cases where you simply cannot see what's wrong, because it's happening either too fast, or in a place that's impossible to inspect.</p>
<p>As a result, networking is full of superstition. People casting spells, executing words of power, trying to read tea leaves and declaring that the end times are coming, not because the hard info isn't available, but because it's incredibly difficult to obtain and interpret.</p>
<h3>The Thanksgiving Uncle Problem</h3>
<p>Read the presentation. It does a better job than this messy post at illustrating the problem. Even if you don't understand networking, by the time you're done, you will be convinced that this is too complicated for <em>most people</em>, full stop. There are just too many unknowns.</p>
<p>You will hate me for making you read this. You will regret it, because you will now be the only person in every conversation who understands these things, and the knowledge is damning. You will have to sit, silently, as everyone around you makes egregious errors in diagnostics that lead them down completely incorrect paths. This is the <em>Thanksgiving Uncle Problem.</em></p>
<p>That's the situation where you, a gay leftist, go to Thanksgiving dinner with the family, and a shitty uncle sits across from you and begins telling lies about society, about people of color, about gay marriage, and so on. If you're self-destructive, you engage him. It will not go well.</p>
<p>The reason for this is that, in order for him to accept <em>anything</em> you say, he needs to accept that many of his foundational beliefs about the world are wrong. Ideas like "the police protect us" and "children need a mom and a dad" have been part of his worldview for so long that he has, without question, made <em>millions</em> of decisions based on these assumptions.</p>
<p>In order for him to discard them, he has to admit that he has been making a fool of himself, doing incredibly wrong and often <em>harmful</em> things, for his entire life. That is too much guilt to handle, and he - and most people - will do anything possible to avoid accepting it. Certainly, this is not a door he's willing to open when he's on his fourth mimosa and doing his best not to think about the goddamn insurance adjuster job he has to go back to on Monday.</p>
<p>So you will read this slideshow, and then you will sit on conference calls thinking, "My god. They are all wrong. And they've always been wrong. And <em>I can't help them,</em> because they will fight me tooth and claw to continue being wrong."</p>
<p>I have no advice on how to deal with this, but it's better to be correct than to be comfortable.</p>
<p><small>Footnote: Ironically, it seems very possible to me that the systems that most consistently enable this are the cheapest routers on the market. Every single home "gateway" ever sold runs Linux, where the ICMP implementation is a core kernel feature, not a user provided daemon.</small></p><small>
<p>I would not be surprised at all if the Linux kernel devs actually <em>have</em> made sure that TTL Exceeded is implemented and enabled by default - and since most Linux-based routers do everything in pure software, there is no data/control plane split to worry about, so sending an Exceeded is more or less "free."</p>
</small><p><small>This would only make a difference for traceroute if Linux was used for anything other than the cheapest endpoint routers, but it's still very funny.</small></p>
<hr>
<h2>Addendum #1</h2>
<p>I reviewed that slide deck again and learned that I conflated a couple concepts.</p>
<p>Yes, the control plane <em>may</em> be responsible for handling exceptions, including ICMP generation, but it is apparently more likely (at least, at the scale of equipment that I am discussing) that the data plane has a <em>fast path</em> and <em>slow path</em>, both located in the data plane, and the slowpath is responsible for handling this work. The control plane, in such a device, <em>only</em> handles data destined to the router's own IP.</p>
<p>However, the slowpath is (per Richard Steenbergen, the author of that presentation; we will trust his research is valid) <em>still</em> a general-purpose CPU instead of custom silicon, so functionally, the point I was making is still valid: There is a very slow computer handling these packets.</p>
<p>Steenbergen uses this fact to make the point that, because these slow-path CPUs <em>are</em> so slow, they are usually rate-limited. Yes, this means that <em>some number of TTL Exceeded messages will simply be thrown away, even if they are enabled.</em></p>
<p>The example given is that a handful of users running MTR (do not get me started on this bastard program) can actually hit this rate limit. This is an outstanding example because I have seen something similar in practice.</p>
<p>Consider what that would look like, and how common it would be: If you have a NOC full of people who think they know what they're doing, but don't, that only enhances the probability that everyone is trying to troubleshoot on their own instead of doing a screenshare and coordinating their efforts - thus, you have six guys running MTR to the same IP.</p>
<p>If they hit that rate limit, what do they see? Nodes suddenly not responding! Randomly, in fact - sometimes responding, sometimes not! That means it's not just a hop that doesn't respond to traceroutes, but <em>packet loss!</em> Wow! We found the problem!</p>
<p>Of course, if four of them hit Ctrl+C, the PL would mysteriously vanish. Huh! Weird! Well, it must be an intermittent issue in <em>squints at resolved hostname</em> Hurricane Electric's network. I'm <em>sure</em> they have a flapping port they haven't noticed (lol.) Just send them a trouble ticket!</p>
<p>By the time this useless waste of effort has resolved (e.g. HE has received, acked, investigated, and declared the ticket No Trouble Found and rejected it) the problem has probably gone away due to unrelated network weather effects. The NOC guys all tell each other that HE was lying about their broken network, slap each other on the back for being smarter than the other bastards, and go out for beers.</p>
<p>How do I know this? Because I've been part of it!</p>
<p>My employer used to have an unholy number of customer sites terminated with little Linux shitboxes - you know the sort, they used to be common as dirt. Tiny Soekris-esque SBCs in folded sheet metal boxes with 12V power supplies, running horrible little SoCs and a copy of Busybox from before the fall of Rome. We had reasons for it that I won't go into.</p>
<p>These things were underpowered to put it mildly. They could route maybe 30 mbps, and if you turned on any firewall functionality that dropped to 10. This was at a time when a <em>tremendous</em> number of customers were on connections no faster than 5 mbps, so, this wasn't a huge problem. We got rid of them all when bandwidths skyrocketed.</p>
<p>But what used to happen is that you'd have three or four people looking at one of these things at once, and you'd start seeing packet loss. And there you go, the customer has a bad connection. Kick it to the ISP and close the ticket, right?</p>
<p>I can't count how many times this happened, but I do remember after about four years of doing this, I had come up with a method for getting more accurate latency stats: just ping -i .1. Absolutely <em>hammer</em> the thing with pings while you have the customer test their usual business processes, and it'll be easier to see latency spikes if something is eating up too much bandwidth.</p>
<p>What I discovered is that running two of these in parallel would produce exactly 50% packet loss, with total reliability. I then tested and found that if I just fired up three or four <em>normal</em> pings, at the default interval, it would do the same thing. 30% or 40% packet loss.</p>
<p>There is no telling how many issues we prolonged because everyone was running their own pings simultaneously and the kernel was getting overloaded and throwing some of them out. This is a snapshot of every network support center, everywhere. It is a bad scene.</p>

	<p><a href="https://gekk.info/articles/index.html">List of Articles</a></p>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Companies Are Ditching the Cloud: The Rise of Cloud Repatriation (109 pts)]]></title>
            <link>https://thenewstack.io/why-companies-are-ditching-the-cloud-the-rise-of-cloud-repatriation/</link>
            <guid>42054813</guid>
            <pubDate>Tue, 05 Nov 2024 20:19:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thenewstack.io/why-companies-are-ditching-the-cloud-the-rise-of-cloud-repatriation/">https://thenewstack.io/why-companies-are-ditching-the-cloud-the-rise-of-cloud-repatriation/</a>, See on <a href="https://news.ycombinator.com/item?id=42054813">Hacker News</a></p>
Couldn't get https://thenewstack.io/why-companies-are-ditching-the-cloud-the-rise-of-cloud-repatriation/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Fisker EVs Hired an IT Spy Who Funneled Millions to N. Korea's Missile Program (131 pts)]]></title>
            <link>https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi</link>
            <guid>42054791</guid>
            <pubDate>Tue, 05 Nov 2024 20:16:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi">https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi</a>, See on <a href="https://news.ycombinator.com/item?id=42054791">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
    <strong><span>Follow us today...</span></strong>
    <a href="https://www.facebook.com/sharer.php?u=https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi" target="_blank">
      <img src="https://www.torquenews.com/profiles/torquenews/facebook.png" loading="lazy" width="64" height="64" alt="Facebook icon">
    </a>&nbsp;
    <a href="https://x.com/share?text=Fisker+EVs+Hired+An+IT+Spy+Who+Funneled+Millions+To+North+Korea%E2%80%99s+Missile+Program%2C+CEO+Henrik+Fisker%2C+Says+%E2%80%9CThe+Case+Is+With+the+FBI%E2%80%9D%20-%C2%A0via%C2%A0@torquenewsauto&amp;url=https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi">
      <img src="https://www.torquenews.com/profiles/torquenews/x.png" loading="lazy" width="64" height="64" alt="X icon">
    </a>&nbsp;
    <a href="https://t.me/teslaev" target="_blank" title="Join us on Telegram!">
      <img src="https://www.torquenews.com/profiles/torquenews/telegram.png" loading="lazy" width="64" height="64" alt="Telegram icon">
    </a>&nbsp;
    <a href="https://www.reddit.com/submit?url=https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi&amp;title=Fisker+EVs+Hired+An+IT+Spy+Who+Funneled+Millions+To+North+Korea%E2%80%99s+Missile+Program%2C+CEO+Henrik+Fisker%2C+Says+%E2%80%9CThe+Case+Is+With+the+FBI%E2%80%9D" target="_blank" title="Join us on Reddit!">
      <img src="https://www.torquenews.com/profiles/torquenews/reddit.png" loading="lazy" width="64" height="64" alt="Reddit icon">
    </a>
  </p><div><p>Welcome to the shocking world of automotive espionage.&nbsp;</p>
<p>It appears North Korea is targeting U.S. automakers. A report from <a href="https://www.autonews.com/manufacturing/an-fisker-hired-north-korean-spy/" target="_blank">Automotive News</a> (by subscription) reveals that <a href="https://www.torquenews.com/fisker" target="_blank">Fisker Inc.</a>, the <a href="https://www.fiskerinc.com/" target="_blank">Ocean SUV electric car</a> manufacturer located in Manhattan Beach, California, hired an IT worker who was a spy for the North Korean government to steal money for its missile program.&nbsp;</p>
<blockquote><p>
AN says, "A remote information technology employee hired by&nbsp;Fisker Inc.&nbsp;turned out to be a spy for the North Korean government."
</p></blockquote>
<p>North Korea targeted Fisker and other automakers.</p>
<p>The report from the Danish magazine The Engineer said Fisker was among numerous U.S. automobile companies targeted by a money laundering scheme that funneled more than a staggering $6 million to North Korea's ballistic missile program.</p>
<blockquote><p>
AN says, "The story, based on documents filed by the U.S. Department of Justice, says Fisker hired a remote IT worker named Kou Thao in October 2021. But Thao's purported address in Arizona belonged to a woman named Christina Chapman, who set up laptop computers that the North Koreans accessed through Russia and China."
</p></blockquote>
<p>Fisker filed for bankruptcy shortly after firing the spy.&nbsp;</p>
<p>Fisker terminated Thao in September 2023 after the Justice Department notified the electric vehicle maker that it was being scammed. The financial strain caused by the espionage activities could have led to <a href="https://www.reuters.com/business/autos-transportation/ev-startup-fisker-files-bankruptcy-2024-06-18/" target="_blank">Fisker&nbsp;filing for bankruptcy</a>&nbsp;nine months later, in June 2024.</p>
<p>The report says Fisker wasn't the only automaker targeted by the spy. The Justice Department's April indictment of Chapman identifies one of her co-conspirators as "Frank C.," a contractor who worked for "a Fortune 500 iconic American automotive manufacturer located in Detroit, Michigan," starting in April 2022.&nbsp;</p>
<p>The Justice Department document doesn't name the company. Two American Fortune 500 automakers are in Detroit, Michigan: General Motors and Ford Motor Company.&nbsp;</p>
<p>The <a href="https://www.wsj.com/tech/north-korean-spies-are-infiltrating-u-s-companies-through-it-jobs-e45a1be8" target="_blank">Wall Street Journal</a> (by subscription) report says that North Korean spies are infiltrating U.S. companies through IT jobs, and companies unknowingly hire North Koreans for hundreds of low-level jobs, "giving Pyongyang access to cash and IP."</p>
<p>The FBI is aware of the problem.</p>
<p>The FBI warned companies about North Korea's highly tailored, difficult-to-detect social engineering campaigns against employees of decentralized finance ("DeFi"), cryptocurrency, and similar businesses to deploy malware and steal company cryptocurrency.</p>
<p>The FBI issued this public service announcement on September 3, 2024.</p>
<p><em>The Democratic People's Republic of Korea ("DPRK" aka North Korea) is conducting highly tailored, difficult-to-detect social engineering campaigns against employees of decentralized finance ("DeFi"), cryptocurrency, and similar businesses to deploy malware and steal company cryptocurrency.</em></p>
<p><em>North Korean social engineering schemes are not just complex, but also elaborate, often compromising victims with sophisticated technical acumen. Given the scale and persistence of this malicious activity, even those well-versed in cybersecurity practices can be vulnerable to North Korea's determination to compromise networks connected to cryptocurrency assets.</em></p>
<p><em>North Korean malicious cyber actors researched various targets connected to cryptocurrency exchange-traded funds (ETFs) over the last several months. This research included pre-operational preparations suggesting North Korean actors may attempt malicious cyber activities against companies associated with cryptocurrency ETFs or other cryptocurrency-related financial products.</em></p>
<p><em>For companies active in or associated with the cryptocurrency sector, the FBI emphasizes North Korea employs sophisticated tactics to steal cryptocurrency funds. It is a persistent threat to organizations with access to large quantities of cryptocurrency-related assets or products.</em></p>
<p>Regarding it being a target of a North Korean spy, CEO Henrik Fisker told The Engineer that the case "is with the FBI" and declined to comment further.</p>
<p>I am&nbsp;<a href="https://www.torquenews.com/users/denis-flierl" target="_blank">Denis Flierl</a>, a Senior Torque News Writer since 2012. I’ve invested over 13 years in the automotive industry in a consulting role, working with every major car brand. I am an experienced Rocky Mountain Automotive Press member. You'll find my expert Subaru analysis <a href="https://www.torquenews.com/subaru" target="_blank">here</a>. Follow me on my X&nbsp;<a href="https://twitter.com/SubaruReport" target="_blank"><strong>SubaruReport</strong></a><strong>,&nbsp;</strong><a href="https://twitter.com/AllSubaru" target="_blank"><strong>All&nbsp;Subaru</strong></a><strong>, </strong><a href="https://twitter.com/WRX_STI_News" target="_blank"><strong>WRXSTI</strong></a>, <a href="https://twitter.com/DenisFlierl" target="_blank"><strong>@DenisFlierl</strong></a>,&nbsp;<a href="https://www.facebook.com/SubaruReport" target="_blank">Facebook</a>, and&nbsp;<a href="https://www.instagram.com/subaru.report/" target="_blank">Instagram</a>.</p>
<p>Photo credit: Denis Flierl via Fisker Inc.</p>
</div><p>
    <strong><span>Follow us today...</span></strong>
    <a href="https://www.facebook.com/sharer.php?u=https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi" target="_blank">
      <img src="https://www.torquenews.com/profiles/torquenews/facebook.png" loading="lazy" width="64" height="64" alt="Facebook icon">
    </a>&nbsp;
    <a href="https://x.com/share?text=Fisker+EVs+Hired+An+IT+Spy+Who+Funneled+Millions+To+North+Korea%E2%80%99s+Missile+Program%2C+CEO+Henrik+Fisker%2C+Says+%E2%80%9CThe+Case+Is+With+the+FBI%E2%80%9D%20-%C2%A0via%C2%A0@torquenewsauto&amp;url=https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi">
      <img src="https://www.torquenews.com/profiles/torquenews/x.png" loading="lazy" width="64" height="64" alt="X icon">
    </a>&nbsp;
    <a href="https://t.me/teslaev" target="_blank" title="Join us on Telegram!">
      <img src="https://www.torquenews.com/profiles/torquenews/telegram.png" loading="lazy" width="64" height="64" alt="Telegram icon">
    </a>&nbsp;
    <a href="https://www.reddit.com/submit?url=https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi&amp;title=Fisker+EVs+Hired+An+IT+Spy+Who+Funneled+Millions+To+North+Korea%E2%80%99s+Missile+Program%2C+CEO+Henrik+Fisker%2C+Says+%E2%80%9CThe+Case+Is+With+the+FBI%E2%80%9D" target="_blank" title="Join us on Reddit!">
      <img src="https://www.torquenews.com/profiles/torquenews/reddit.png" loading="lazy" width="64" height="64" alt="Reddit icon">
    </a>
  </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. chip revival plan chooses sites (112 pts)]]></title>
            <link>https://spectrum.ieee.org/nstc</link>
            <guid>42054779</guid>
            <pubDate>Tue, 05 Nov 2024 20:14:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/nstc">https://spectrum.ieee.org/nstc</a>, See on <a href="https://news.ycombinator.com/item?id=42054779">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="U.S. Chip Revival Plan Chooses Sites"><p>Last week the organization tasked with running the the <a href="https://www.semiconductors.org/chips-rd-programs/" target="_blank">biggest chunk</a> of <a href="https://spectrum.ieee.org/tag/chips-act" target="_blank">U.S. CHIPS Act’s</a> US $13 billion R&amp;D program made some significant strides: The <a href="https://www.nist.gov/chips/research-development-programs/national-semiconductor-technology-center" target="_blank">National Semiconductor Technology Center (NSTC)</a>  released a strategic plan and selected the sites of two of three planned facilities and released a new strategic plan. The locations of the two sites—a <a href="https://www.nist.gov/news-events/news/2024/11/biden-harris-administration-announces-sunnyvale-ca-expected-location-second" target="_blank"><u>“design and collaboration” center</u></a> in Sunnyvale, Calif., and a lab devoted to advancing the <a href="https://www.nist.gov/news-events/news/2024/10/biden-harris-administration-announces-ny-creates-albany-nanotech-complex" target="_blank"><u>leading edge of chipmaking</u></a>, in Albany, N.Y.—build on an existing ecosystem at each location, experts say. The location of the third planned center—a chip prototyping and packaging site that could be especially <a href="https://spectrum.ieee.org/power-electronics" target="_self"><u>critical for speeding semiconductor startups</u></a>—is still a matter of speculation.</p><p>“The NSTC represents a once-in-a-generation opportunity for the U.S. to accelerate the pace of innovation in semiconductor technology,” <a href="https://natcast.org/leadership" target="_blank"><u>Deirdre Hanford</u></a>, CEO of Natcast, the nonprofit that runs the NSTC centers, said in a <a href="https://natcast.org/biden-harris-announces-second-chips-flagship-facility" rel="noopener noreferrer" target="_blank"><u>statement</u></a>. According to the strategic plan, which covers 2025 to 2027, the NSTC is meant to accomplish three goals: extend U.S. technology leadership, reduce the time and cost to prototype, and build and sustain a semiconductor workforce development ecosystem. The three centers are meant to do a mix of all three. </p><h2>New York gets extreme ultraviolet lithography</h2><p>NSTC plans to direct $825 million into the Albany project. The site will be dedicated to extreme ultraviolet lithography, a technology that’s essential to making the most advanced logic chips. The <a href="https://ny-creates.org/ny-creates-campus-complex/" rel="noopener noreferrer" target="_blank"><u>Albany Nanotech Complex</u></a>, which has already seen more than $25 billion in investments from the state and industry partners over two decades, will form the heart of the future NSTC center. It already has an<a href="https://spectrum.ieee.org/tag/euv" target="_self"><u> EUV lithography</u></a> machine on site and has begun an expansion to install a next-generation version, called <a href="https://spectrum.ieee.org/high-na-euv" target="_self"><u>high-NA EUV</u></a>, which promises to produce even finer chip features. Working with a tool recently installed in Europe, <a href="https://spectrum.ieee.org/tag/ibm">IBM</a>, a long-time tenant of the Albany research facility, reported record yields of <a href="https://research.ibm.com/blog/new-euv-patterning-yield-benchmarks" rel="noopener noreferrer" target="_blank"><u>copper interconnects built every 21 nanometers</u></a>, a pitch several nanometers tighter than possible with ordinary <a href="https://spectrum.ieee.org/high-na-euv">EUV</a>. </p><p>“It’s fulfilling to see that this ecosystem can be taken to the national and global level through CHIPS Act funding,” said <a href="https://research.ibm.com/people/mukesh-khare" rel="noopener noreferrer" target="_blank"><u>Mukesh Khare</u></a>, general manager of IBM’s <a href="https://spectrum.ieee.org/topic/semiconductors/">semiconductors</a> division, speaking from the future site of the NSTC EUV center. “It’s the right time, and we have all the ingredients.”</p><p>While only a few companies are capable of manufacturing cutting edge logic using EUV, the impact of the NSTC center will be much broader, Khare argues. It will extend down as far as early-stage startups with ideas or materials for improving the chipmaking process “An EUV R&amp;D center doesn’t mean just one machine,” says Khare. “It needs so many machines around it… It’s a very large ecosystem.”</p><h2>Silicon Valley lands the design center</h2><p>The design center is tasked with conducting advanced research in <a href="https://spectrum.ieee.org/tag/chip-design" target="_self"><u>chip design</u></a>, electronic design automation (EDA), chip and system architectures, and<a href="https://spectrum.ieee.org/tag/security" target="_self"><u> hardware security</u></a>. It will also host the NSTC’s design enablement gateway—a program that provides NSTC members with a secure, cloud-based access to design tools, reference processes and designs, and shared data sets, with the goal of reducing the time and cost of design. Additionally, it will house workforce development, member convening, and administration functions. </p><p>Situating the design center in Silicon Valley, with its concentration of research universities, venture capital, and workforce, seems like the obvious choice to many experts. “I can’t think of a better place,” says Patrick Soheili, co-founder of interconnect technology startup Eliyan, which is based in Santa Clara, Calif.</p><p><a href="https://www.linkedin.com/in/abhijeet-chakraborty-1a21931/" rel="noopener noreferrer" target="_blank"><u>Abhijeet Chakraborty</u></a>, vice president of engineering in the technology and product group at Silicon Valley-based <a href="https://www.synopsys.com/" rel="noopener noreferrer" target="_blank"><u>Synopsys</u></a>, a leading maker of EDA software, sees Silicon Valley’s expansive tech ecosystem as one of its main advantages in landing the NSTC’s design center. The region concentrates companies and researchers involved in the whole spectrum of the industry from semiconductor process technology to cloud software.</p><p>Access to such a broad range of industries is increasingly important for <a href="https://spectrum.ieee.org/tag/chip-design">chip design</a> startups, he says. “To design a chip or component these days you need to go from concept to design to validation in an environment that takes care of the entire stack,” he says. It’s prohibitively expensive for a startup to do that alone, so one of Chakraborty’s hopes for the design center is that it will help startups access the design kits and other data needed to operate in this new environment.</p><h2>Packaging and prototyping still to come</h2><p>A third promised center for prototyping and packaging is still to come. “The big question is where does the packaging and prototyping go?” says Mark Granahan, cofounder and CEO of Pennsylvania-based <a href="https://spectrum.ieee.org/power-electronics" target="_blank">power semiconductor startup Ideal Semiconductor</a>. “To me that’s a great opportunity.” He points out that because there is so little packaging technology infrastructure in the United States, any ambitious state or region should have a shot at hosting such a center. One of the original intentions of the act, after all, was to expand the number of regions of the country that are involved in the semiconductor industry.</p><p>But that hasn’t stopped some already tech-heavy regions from wanting it. “Oregon offers the strongest ecosystem for such a facility,” a spokesperson for <a href="https://www.intel.com/content/www/us/en/foundry/overview.html" rel="noopener noreferrer" target="_blank"><u>Intel</u></a>, whose technology development is done there. “The state is uniquely positioned to contribute to the success of the NSTC and help drive technological advancements in the U.S. semiconductor industry.”</p><p>As NSTC makes progress, Granahan’s concern is that bureaucracy will expand with it and slow efforts to boost the U.S. chip industry. Already the layers of control are multiplying. The <a href="https://www.nist.gov/chips" rel="noopener noreferrer" target="_blank"><u>Chips Office</u></a> at the National Institute of Standards and Technology executes the Act. The NSTC is administered by the nonprofit <a href="https://natcast.org/" rel="noopener noreferrer" target="_blank"><u>Natcast</u></a>, which directs the EUV center, which is in a facility run by another nonprofit, <a href="https://ny-creates.org/" rel="noopener noreferrer" target="_blank"><u>NY CREATES</u></a>. “We want these things to be agile and make local decisions.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First time ever, AMD outsells Intel in the datacenter space (355 pts)]]></title>
            <link>https://www.tomshardware.com/pc-components/cpus/for-the-first-time-ever-amd-outsells-intel-in-the-datacenter-space</link>
            <guid>42054449</guid>
            <pubDate>Tue, 05 Nov 2024 19:27:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/pc-components/cpus/for-the-first-time-ever-amd-outsells-intel-in-the-datacenter-space">https://www.tomshardware.com/pc-components/cpus/for-the-first-time-ever-amd-outsells-intel-in-the-datacenter-space</a>, See on <a href="https://news.ycombinator.com/item?id=42054449">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body">
<p>For well more than two decades, Intel has been the undisputed leader in the market for datacenter CPUs. Intel's Xeon processors powered the vast majority of servers, whereas AMD's processors commanded a single-digit market share just some seven or eight years ago. However, the situation has changed drastically. While Intel's Xeon CPUs still power the majority of servers, the most expensive machines now use AMD's EPYC processors. This is why AMD's datacenter business unit now outsells Intel's datacenter and AI business group, as observed by&nbsp;<a data-analytics-id="inline-link" href="https://x.com/SKundojjala/status/1853041284157682063" data-url="https://x.com/SKundojjala/status/1853041284157682063" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">SemiAnalysis</a>.</p><p>Indeed, AMD's datacenter segment revenue reached&nbsp;<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amd-rakes-in-cash-with-best-quarterly-revenue-ever-amid-datacenter-business-rise-but-gaming-business-craters" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/amd-rakes-in-cash-with-best-quarterly-revenue-ever-amid-datacenter-business-rise-but-gaming-business-craters">$3.549 billion</a>&nbsp;in the third quarter, whereas Intel's datacenter and AI group's earnings were&nbsp;<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intel-lost-usd16-6-billion-in-q3-reports-usd13-3-billion-in-revenue" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/intel-lost-usd16-6-billion-in-q3-reports-usd13-3-billion-in-revenue">$3.3 billion</a>&nbsp;in Q3 2024. Just two years ago, Intel's DCAI group earned $5 billion - $6 billion per quarter. But as AMD's EPYC processors have gained competitive advantages over Intel's Xeon CPUs, Intel has had to sell its server chips at significant discounts, which has reduced the company's revenue and profit margins.</p><p>It is noteworthy that Intel's flagship&nbsp;<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intels-latest-flagship-128-core-xeon-cpu-costs-usd17-800-granite-rapids-sets-a-new-high-watermark" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/intels-latest-flagship-128-core-xeon-cpu-costs-usd17-800-granite-rapids-sets-a-new-high-watermark">128-core Xeon 6980P 'Granite Rapids' processor costs $17,800</a>, making it the company's most expensive standard CPU ever. By contrast, AMD's most expensive 96-core EPYC 6979P processor costs $11,805. If demand for Intel's Xeon 6900-series processors remains high and the company can supply these CPUs in decent volumes, then Intel's datacenter revenue will likely get back on track and surpass AMD's datacenter sales. However, Intel still has to ramp up production of its Granite Rapids products.&nbsp;</p><p>While both Intel and AMD now earn around $3-3.5 billion per quarter selling datacenter CPUs, Nvidia earns much more from its datacenter GPUs and networking chips, which are required to make AI processors work in concert in datacenters. In fact, sales of Nvidia's networking products totaled&nbsp;<a data-analytics-id="inline-link" href="https://s201.q4cdn.com/141608511/files/doc_financials/2025/Q225/Q2FY25-CFO-Commentary.pdf" data-url="https://s201.q4cdn.com/141608511/files/doc_financials/2025/Q225/Q2FY25-CFO-Commentary.pdf" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">$3.668 billion</a>&nbsp;in the company's second quarter of fiscal 2025. Meanwhile, compute GPU sales reached $22.604 billion in Q2 FY2025, which far surpasses the combined sales of Intel and AMD datacenter hardware. Altogether, Nvidia sold nearly $42 billion worth of AI and HPC GPUs in the first half of this year, and it is likely that the company will sell even more datacenter processors in the second half.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-gRFTK4rNSThvy54UVmstUM"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tencent Hunyuan-Large (128 pts)]]></title>
            <link>https://github.com/Tencent/Tencent-Hunyuan-Large</link>
            <guid>42054186</guid>
            <pubDate>Tue, 05 Nov 2024 18:52:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Tencent/Tencent-Hunyuan-Large">https://github.com/Tencent/Tencent-Hunyuan-Large</a>, See on <a href="https://news.ycombinator.com/item?id=42054186">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
    <a href="https://github.com/Tencent/Tencent-Hunyuan-Large/blob/main/README_CN.md">中文</a>&nbsp; ｜ English
</p>
<p dir="auto">
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4f7f1dcd99f1e24cb8787dc695f3fb708e2f9799fe3559366a3d58139e74491a/68747470733a2f2f647363616368652e74656e63656e742d636c6f75642e636e2f75706c6f61642f75706c6f616465722f68756e7975616e2d363462343138666430353263303333623232386530346263373762626334623534666437663562632e706e67"><img src="https://camo.githubusercontent.com/4f7f1dcd99f1e24cb8787dc695f3fb708e2f9799fe3559366a3d58139e74491a/68747470733a2f2f647363616368652e74656e63656e742d636c6f75642e636e2f75706c6f61642f75706c6f616465722f68756e7975616e2d363462343138666430353263303333623232386530346263373762626334623534666437663562632e706e67" width="400" data-canonical-src="https://dscache.tencent-cloud.cn/upload/uploader/hunyuan-64b418fd052c033b228e04bc77bbc4b54fd7f5bc.png"></a> <br>
</p>
<p dir="auto">
    🫣&nbsp;<a href="https://huggingface.co/tencent/Tencent-Hunyuan-Large" rel="nofollow"><b>Hugging Face</b></a>&nbsp;&nbsp; |  &nbsp;&nbsp;🖥️&nbsp;&nbsp;<a href="https://llm.hunyuan.tencent.com/" rel="nofollow"><b>official website</b></a>&nbsp;&nbsp;｜&nbsp;&nbsp;🕖&nbsp;&nbsp; <a href="https://cloud.tencent.com/product/hunyuan" rel="nofollow"><b>HunyuanAPI</b></a>
</p><p dir="auto">
    <a href="https://arxiv.org/abs/2411.02265" rel="nofollow"><b>Technical Report</b></a>&nbsp;&nbsp;｜&nbsp;&nbsp; <a href="https://huggingface.co/spaces/tencent/Hunyuan-Large" rel="nofollow"><b>Demo</b></a>&nbsp;&nbsp;&nbsp;｜&nbsp;&nbsp; <a href="https://cloud.tencent.com/document/product/851/112032" rel="nofollow"><b>Tencent Cloud TI</b></a>&nbsp;&nbsp;&nbsp;</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Model Introduction</h2><a id="user-content-model-introduction" aria-label="Permalink: Model Introduction" href="#model-introduction"></a></p>
<p dir="auto">With the rapid development of artificial intelligence technology, large language models (LLMs) have made significant progress in fields such as natural language processing, computer vision, and scientific tasks. However, as the scale of these models increases, optimizing resource consumption while maintaining high performance has become a key challenge. To address this challenge, we have explored Mixture of Experts (MoE) models. The currently unveiled Hunyuan-Large (Hunyuan-MoE-A52B) model is the largest open-source Transformer-based MoE model in the industry, featuring a total of 389 billion parameters and 52 billion active parameters. This is currently the largest open-source Transformer-based MoE model in the industry, featuring a total of 389 billion parameters and 52 billion active parameters.</p>
<p dir="auto">By open-sourcing the Hunyuan-Large model and revealing related technical details, we hope to inspire more researchers with innovative ideas and collectively advance the progress and application of AI technology. We welcome you to join our open-source community to explore and optimize future AI models together!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Introduction to Technical Advantages</h3><a id="user-content-introduction-to-technical-advantages" aria-label="Permalink: Introduction to Technical Advantages" href="#introduction-to-technical-advantages"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Model</h4><a id="user-content-model" aria-label="Permalink: Model" href="#model"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>High-Quality Synthetic Data</strong>: By enhancing training with synthetic data, Hunyuan-Large can learn richer representations, handle long-context inputs, and generalize better to unseen data.</p>
</li>
<li>
<p dir="auto"><strong>KV Cache Compression</strong>: Utilizes Grouped Query Attention (GQA) and Cross-Layer Attention (CLA) strategies to significantly reduce memory usage and computational overhead of KV caches, improving inference throughput.</p>
</li>
<li>
<p dir="auto"><strong>Expert-Specific Learning Rate Scaling</strong>: Sets different learning rates for different experts to ensure each sub-model effectively learns from the data and contributes to overall performance.</p>
</li>
<li>
<p dir="auto"><strong>Long-Context Processing Capability</strong>: The pre-trained model supports text sequences up to 256K, and the Instruct model supports up to 128K, significantly enhancing the ability to handle long-context tasks.</p>
</li>
<li>
<p dir="auto"><strong>Extensive Benchmarking</strong>: Conducts extensive experiments across various languages and tasks to validate the practical effectiveness and safety of Hunyuan-Large.</p>
</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">inference Framework</h4><a id="user-content-inference-framework" aria-label="Permalink: inference Framework" href="#inference-framework"></a></p>
<ul dir="auto">
<li>This open-source release offers two inference backend options tailored for the Hunyuan-Large model: the popular vLLM-backend and the TRT-LLM-backend. Both solutions include optimizations for enhanced performance. For instance, the introduction of a new CLA structure significantly reduces GPU memory usage, achieving a 50% savings in the KV-Cache portion, which ensures efficient handling of long text scenarios. Additionally, by employing FP8 quantization, we achieve a 50% reduction in memory usage compared to traditional FP16/BF16 quantization, while maintaining precision and resulting in a 70% increase in throughput. Meanwhile, by leveraging the efficient operators at the core of TRT-LLM, the performance of the TRT-LLM solution surpasses that of vLLM by over 30%. The TRT-LLM solution is widely used in Tencent's Hunyuan project. In this release, we are initially open-sourcing the vLLM solution, with plans to release the TRT-LLM solution in the near future.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Training Framework</h4><a id="user-content-training-framework" aria-label="Permalink: Training Framework" href="#training-framework"></a></p>
<ul dir="auto">
<li>The Hunyuan-Large open-source model is fully compatible with the Hugging Face format, enabling researchers and developers to perform model fine-tuning using the hf-deepspeed framework. Additionally, we support training acceleration through the use of flash attention. To further assist in the adoption process, we have made the corresponding training scripts and model implementations publicly available to the community through this release, facilitating subsequent model training and fine-tuning operations based on these resources.</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Related News</h2><a id="user-content-related-news" aria-label="Permalink: Related News" href="#related-news"></a></p>
<ul dir="auto">
<li>2024.11.5 <a href="https://cloud.tencent.com/product/ti" rel="nofollow">TI Platform</a> has integrated Hunyuan-Large model already, you can easily train and deploy it in just a few steps. Visit <a href="https://console.cloud.tencent.com/tione/v2/aimarket/detail/hunyuan_series?PublicAlgoGroupId=hunyuan-large-chat&amp;detailTab=demo" rel="nofollow">Chat with Hunyuan-Large</a> to experience real-time conversations with the model, and explore <a href="https://cloud.tencent.com/document/product/851/112032" rel="nofollow">Hunyuan-Large Best Practice on TI</a> to create your own customized Hunyuan-Large model.</li>
<li>2024.11.5 We have open-sourced <strong>Hunyuan-A52B-Pretrain</strong>, <strong>Hunyuan-A52B-Instruct</strong>, and <strong>Hunyuan-A52B-Instruct-FP8</strong> on Hugging Face. We also released a technical report and a training and inference operations manual, providing detailed information on the model's capabilities and the procedures for training and inference.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmark Evaluation</h2><a id="user-content-benchmark-evaluation" aria-label="Permalink: Benchmark Evaluation" href="#benchmark-evaluation"></a></p>
<p dir="auto"><strong>Hunyuan-Large pre-trained model</strong> achieves the best overall performance compared to both Dense and MoE based
competitors having similar activated parameter sizes.  For aggregated benchmarks such as MMLU, MMLU-Pro, and CMMLU,
Hunyuan-Large consistently achieves the best performance, confirming its comprehensive abilities on aggregated tasks.
Hunyuan-Large also shows superior performance in commonsense understanding and reasoning, and classical NLP tasks
such as QA and reading comprehension tasks (e.g., CommonsenseQA, PIQA and TriviaQA).<br>
For the mathematics capability, Hunyuan-Large outperforms all baselines in math datasets of GSM8K and MATH,
and also gains the best results on CMATH in Chinese.We also observe that Hunyuan-Large achieves the overall
best performance in all Chinese tasks (e.g., CMMLU, C-Eval).</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>LLama3.1-405B</th>
<th>LLama3.1-70B</th>
<th>Mixtral-8x22B</th>
<th>DeepSeek-V2</th>
<th>Hunyuan-Large</th>
</tr>
</thead>
<tbody>
<tr>
<td>MMLU</td>
<td>85.2</td>
<td>79.3</td>
<td>77.8</td>
<td>78.5</td>
<td><strong>88.4</strong></td>
</tr>
<tr>
<td>MMLU-Pro</td>
<td><strong>61.6</strong></td>
<td>53.8</td>
<td>49.5</td>
<td>-</td>
<td>60.2</td>
</tr>
<tr>
<td>BBH</td>
<td>85.9</td>
<td>81.6</td>
<td>78.9</td>
<td>78.9</td>
<td><strong>86.3</strong></td>
</tr>
<tr>
<td>HellaSwag</td>
<td>-</td>
<td>-</td>
<td><strong>88.7</strong></td>
<td>87.8</td>
<td>86.8</td>
</tr>
<tr>
<td>CommonsenseQA</td>
<td>85.8</td>
<td>84.1</td>
<td>82.4</td>
<td>-</td>
<td><strong>92.9</strong></td>
</tr>
<tr>
<td>WinoGrande</td>
<td>86.7</td>
<td>85.3</td>
<td>85.0</td>
<td>84.9</td>
<td><strong>88.7</strong></td>
</tr>
<tr>
<td>PIQA</td>
<td>-</td>
<td>-</td>
<td>83.6</td>
<td>83.7</td>
<td><strong>88.3</strong></td>
</tr>
<tr>
<td>NaturalQuestions</td>
<td>-</td>
<td>-</td>
<td>39.6</td>
<td>38.7</td>
<td><strong>52.8</strong></td>
</tr>
<tr>
<td>DROP</td>
<td>84.8</td>
<td>79.6</td>
<td>80.4</td>
<td>80.1</td>
<td><strong>88.9</strong></td>
</tr>
<tr>
<td>ARC-C</td>
<td><strong>96.1</strong></td>
<td>92.9</td>
<td>91.2</td>
<td>92.4</td>
<td>95.0</td>
</tr>
<tr>
<td>TriviaQA</td>
<td>-</td>
<td>-</td>
<td>82.1</td>
<td>79.9</td>
<td><strong>89.2</strong></td>
</tr>
<tr>
<td>CMMLU</td>
<td>-</td>
<td>-</td>
<td>60.0</td>
<td>84.0</td>
<td><strong>90.2</strong></td>
</tr>
<tr>
<td>C-Eval</td>
<td>-</td>
<td>-</td>
<td>59.6</td>
<td>81.7</td>
<td><strong>91.9</strong></td>
</tr>
<tr>
<td>C3</td>
<td>-</td>
<td>-</td>
<td>71.4</td>
<td>77.4</td>
<td><strong>82.3</strong></td>
</tr>
<tr>
<td>GSM8K</td>
<td>89.0</td>
<td>83.7</td>
<td>83.7</td>
<td>79.2</td>
<td><strong>92.8</strong></td>
</tr>
<tr>
<td>MATH</td>
<td>53.8</td>
<td>41.4</td>
<td>42.5</td>
<td>43.6</td>
<td><strong>69.8</strong></td>
</tr>
<tr>
<td>CMATH</td>
<td>-</td>
<td>-</td>
<td>72.3</td>
<td>78.7</td>
<td><strong>91.3</strong></td>
</tr>
<tr>
<td>HumanEval</td>
<td>61.0</td>
<td>58.5</td>
<td>53.1</td>
<td>48.8</td>
<td><strong>71.4</strong></td>
</tr>
<tr>
<td>MBPP</td>
<td><strong>73.4</strong></td>
<td>68.6</td>
<td>64.2</td>
<td>66.6</td>
<td>72.6</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>Hunyuan-Large-Instruct</strong> achieves consistent improvements on most types of tasks compared to LLMs having similar
activated parameters, indicating the effectiveness of our post-training.    Delving into the model performance
in different categories of benchmarks, we find that our instruct model achieves the best performance on MMLU and MATH dataset.<br>
Notably, on the MMLU dataset, our model demonstrates a significant improvement, outperforming the LLama3.1-405B model by 2.6%.<br>
This enhancement is not just marginal but indicative of the Hunyuan-Large-Instruct’s superior understanding and reasoning
capabilities across a wide array of language understanding tasks. The model’s prowess is further underscored in its performance
on the MATH dataset, where it surpasses the LLama3.1-405B by a notable margin of 3.6%.<br>
Remarkably, this leap in accuracy is achieved with only 52 billion activated parameters, underscoring the efficiency of our model.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>LLama3.1 405B Inst.</th>
<th>LLama3.1 70B Inst.</th>
<th>Mixtral 8x22B Inst.</th>
<th>DeepSeekV2.5 Chat</th>
<th>Hunyuan-Large Inst.</th>
</tr>
</thead>
<tbody>
<tr>
<td>MMLU</td>
<td>87.3</td>
<td>83.6</td>
<td>77.8</td>
<td>80.4</td>
<td><strong>89.9</strong></td>
</tr>
<tr>
<td>CMMLU</td>
<td>-</td>
<td>-</td>
<td>61.0</td>
<td>-</td>
<td><strong>90.4</strong></td>
</tr>
<tr>
<td>C-Eval</td>
<td>-</td>
<td>-</td>
<td>60.0</td>
<td>-</td>
<td><strong>88.6</strong></td>
</tr>
<tr>
<td>BBH</td>
<td>-</td>
<td>-</td>
<td>78.4</td>
<td>84.3</td>
<td><strong>89.5</strong></td>
</tr>
<tr>
<td>HellaSwag</td>
<td>-</td>
<td>-</td>
<td>86.0</td>
<td><strong>90.3</strong></td>
<td>88.5</td>
</tr>
<tr>
<td>ARC-C</td>
<td><strong>96.9</strong></td>
<td>94.8</td>
<td>90.0</td>
<td>-</td>
<td>94.6</td>
</tr>
<tr>
<td>GPQA_diamond</td>
<td><strong>51.1</strong></td>
<td>46.7</td>
<td>-</td>
<td>-</td>
<td>42.4</td>
</tr>
<tr>
<td>MATH</td>
<td>73.8</td>
<td>68.0</td>
<td>49.8</td>
<td>74.7</td>
<td><strong>77.4</strong></td>
</tr>
<tr>
<td>HumanEval</td>
<td>89.0</td>
<td>80.5</td>
<td>75.0</td>
<td>89.0</td>
<td><strong>90.0</strong></td>
</tr>
<tr>
<td>AlignBench</td>
<td>6.0</td>
<td>5.9</td>
<td>6.2</td>
<td>8.0</td>
<td><strong>8.3</strong></td>
</tr>
<tr>
<td>MT-Bench</td>
<td>9.1</td>
<td>8.8</td>
<td>8.1</td>
<td>9.0</td>
<td><strong>9.4</strong></td>
</tr>
<tr>
<td>IFEval strict-prompt</td>
<td><strong>86.0</strong></td>
<td>83.6</td>
<td>71.2</td>
<td>-</td>
<td>85.0</td>
</tr>
<tr>
<td>Arena-Hard</td>
<td>69.3</td>
<td>55.7</td>
<td>-</td>
<td>76.2</td>
<td><strong>81.8</strong></td>
</tr>
<tr>
<td>AlpacaEval-2.0</td>
<td>39.3</td>
<td>34.3</td>
<td>30.9</td>
<td>50.5</td>
<td><strong>51.8</strong></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto"><h2 tabindex="-1" dir="auto">Data</h2><a id="user-content-data" aria-label="Permalink: Data" href="#data"></a></p>
<p dir="auto">Hunyuan-Large provides model training-related processes. In this section, you can process the training data format for use in model training.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Training Data Format and Processing</h3><a id="user-content-training-data-format-and-processing" aria-label="Permalink: Training Data Format and Processing" href="#training-data-format-and-processing"></a></p>
<p dir="auto">Training data is processed into a messages format as follows. The default system prompt for training and inference is "You are a helpful assistant." Below are examples of single-turn and multi-turn data:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Single-Turn Data Format

messages = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;海水为什么是咸的&quot; },
    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;海水是咸的主要是因为其中含有许多溶解在水中的盐类和矿物质。这些盐类和矿物质来自于地球表面的岩石和土壤中的化学物质，随着时间的推移，它们被带到了海洋中。当海水蒸发时，水分蒸发掉了，但盐类和矿物质仍然留在水中，导致海水变得更加咸味。因此，海水的咸度是由其中的盐类和矿物质的含量决定的。&quot;}
]

# Multi-Turn Data Format
messages = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;知道重庆森林这部电影吗？&quot; },
    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;知道呀，是一部由王家卫导演的片子。&quot; },
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot; 而主演里更是有王菲，一上映便受到追捧。&quot; },
    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;所以此片获得了第14届香港电影金像奖最佳影片奖。&quot;}
]

from tokenizers import ByteLevelBPETokenizer
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;./models&quot;, use_fast=False, trust_remote_code=True)

ids = tokenizer.apply_chat_template(messages)
"><pre><span># Single-Turn Data Format</span>

<span>messages</span> <span>=</span> [
    {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>"You are a helpful assistant."</span>},
    {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"海水为什么是咸的"</span> },
    {<span>"role"</span>: <span>"assistant"</span>, <span>"content"</span>: <span>"海水是咸的主要是因为其中含有许多溶解在水中的盐类和矿物质。这些盐类和矿物质来自于地球表面的岩石和土壤中的化学物质，随着时间的推移，它们被带到了海洋中。当海水蒸发时，水分蒸发掉了，但盐类和矿物质仍然留在水中，导致海水变得更加咸味。因此，海水的咸度是由其中的盐类和矿物质的含量决定的。"</span>}
]

<span># Multi-Turn Data Format</span>
<span>messages</span> <span>=</span> [
    {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>"You are a helpful assistant."</span>},
    {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"知道重庆森林这部电影吗？"</span> },
    {<span>"role"</span>: <span>"assistant"</span>, <span>"content"</span>: <span>"知道呀，是一部由王家卫导演的片子。"</span> },
    {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>" 而主演里更是有王菲，一上映便受到追捧。"</span> },
    {<span>"role"</span>: <span>"assistant"</span>, <span>"content"</span>: <span>"所以此片获得了第14届香港电影金像奖最佳影片奖。"</span>}
]

<span>from</span> <span>tokenizers</span> <span>import</span> <span>ByteLevelBPETokenizer</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>AutoTokenizer</span>

<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>"./models"</span>, <span>use_fast</span><span>=</span><span>False</span>, <span>trust_remote_code</span><span>=</span><span>True</span>)

<span>ids</span> <span>=</span> <span>tokenizer</span>.<span>apply_chat_template</span>(<span>messages</span>)</pre></div>
<p dir="auto">For more usage references, see the <code>./models/test.py</code> file.</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto">You can quickly get started by referring to the content in the <a href="https://github.com/Tencent/Tencent-Hunyuan-Large/blob/main/examples/README.md">Quick Start Guide</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Model Training</h2><a id="user-content-model-training" aria-label="Permalink: Model Training" href="#model-training"></a></p>
<p dir="auto">To simplify the Training process, HunyuanLLM provides a pre-built Docker image:</p>
<p dir="auto"><a href="https://hub.docker.com/repository/docker/hunyuaninfer/hunyuan-large/general" rel="nofollow">hunyuaninfer/hunyuan-large</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware Requirements</h3><a id="user-content-hardware-requirements" aria-label="Permalink: Hardware Requirements" href="#hardware-requirements"></a></p>
<p dir="auto">Tested on H20, without enabling <code>make_moe_param_leaf_module</code> and using <code>zero3+offload</code>, with a <code>max_seq_length</code> of 2048, full fine-tuning requires at least 32 GPUs, and LoRA fine-tuning requires at least 8 GPUs.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Training Performance</h3><a id="user-content-training-performance" aria-label="Permalink: Training Performance" href="#training-performance"></a></p>
<p dir="auto">With the minimum configuration (8 GPUs for LoRA fine-tuning), <code>per_device_train_batch_size</code> is set to 1, and <code>gradient_accumulation_steps</code> is set to 1, resulting in approximately 35 seconds per iteration.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Launch Method</h3><a id="user-content-launch-method" aria-label="Permalink: Launch Method" href="#launch-method"></a></p>
<p dir="auto">Refer to: <a href="https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/trainer" rel="nofollow">HuggingFace Transformers Trainer</a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Single-Machine Training</h4><a id="user-content-single-machine-training" aria-label="Permalink: Single-Machine Training" href="#single-machine-training"></a></p>
<p dir="auto">In the <code>train</code> directory, execute:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt
bash train.sh"><pre>pip install -r requirements.txt
bash train.sh</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Multi-Machine Training</h4><a id="user-content-multi-machine-training" aria-label="Permalink: Multi-Machine Training" href="#multi-machine-training"></a></p>
<p dir="auto">To start training on multiple machines, follow the steps below and ensure that all machines are within the same cluster.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Configure Passwordless SSH Login Between Machines</h5><a id="user-content-configure-passwordless-ssh-login-between-machines" aria-label="Permalink: Configure Passwordless SSH Login Between Machines" href="#configure-passwordless-ssh-login-between-machines"></a></p>
<p dir="auto">The following steps use two machines as an example, with their IPs represented as <code>${ip1}</code> and <code>${ip2}</code>. These operations are performed within a Docker container.</p>
<p dir="auto">First, configure passwordless SSH between containers on each machine.</p>
<div dir="auto" data-snippet-clipboard-copy-content="ssh-keygen			# Generate id_rsa and id_rsa.pub for passwordless login
ssh-keygen -t rsa -A    # Generate /etc/ssh/ssh_host_rsa_key and ssh_host_ecdsa_key for starting 'SSH listen' later
/usr/sbin/sshd -p 36005 -o ListenAddress=0.0.0.0        # Start SSH listen
echo &quot;Port 36005&quot; > ~/.ssh/config   # Change SSH connection port to 36005
passwd root    # Set root password to avoid alerts from monitoring platforms"><pre>ssh-keygen			<span><span>#</span> Generate id_rsa and id_rsa.pub for passwordless login</span>
ssh-keygen -t rsa -A    <span><span>#</span> Generate /etc/ssh/ssh_host_rsa_key and ssh_host_ecdsa_key for starting 'SSH listen' later</span>
/usr/sbin/sshd -p 36005 -o ListenAddress=0.0.0.0        <span><span>#</span> Start SSH listen</span>
<span>echo</span> <span><span>"</span>Port 36005<span>"</span></span> <span>&gt;</span> <span>~</span>/.ssh/config   <span><span>#</span> Change SSH connection port to 36005</span>
passwd root    <span><span>#</span> Set root password to avoid alerts from monitoring platforms</span></pre></div>
<p dir="auto">Note: The <code>36005</code> here is an example. You can choose any port, but ensure that the port is <strong>open</strong> and <strong>not occupied by other processes</strong>.</p>
<p dir="auto">Next, within the container on each machine, execute:</p>

<p dir="auto"><strong>Copy the output SSH public key and paste it into the <code>~/.ssh/authorized_keys</code> file, with one public key per line. This must be done on every machine.</strong> Ultimately, the <code>~/.ssh/authorized_keys</code> file on each machine should be identical and contain the public keys of all machines.</p>
<p dir="auto">It's important to note that during multi-node training, the code executed on each node must be consistent. It is recommended to mount a shared network drive. If mounting a shared drive is not possible, you need to manually copy the dataset, scripts, and code to the same directory on all machines.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Start Multi-Machine Training</h5><a id="user-content-start-multi-machine-training" aria-label="Permalink: Start Multi-Machine Training" href="#start-multi-machine-training"></a></p>
<p dir="auto">Once the preparation steps are completed and dependencies are confirmed to be installed (if not, execute <code>pip install -r requirements.txt</code> to install), you can add the following configuration at the beginning of <code>train.sh</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export HOST_GPU_NUM=8
# Current machine IP
export LOCAL_IP=${ip1}
# Multi-node machine IPs, separated by commas
export NODE_IP_LIST=&quot;${ip1}:8,${ip2}:8&quot;
# Number of machine nodes
export NODES=2
export NODE_NUM=$((${NODES} * ${HOST_GPU_NUM}))"><pre><span>export</span> HOST_GPU_NUM=8
<span><span>#</span> Current machine IP</span>
<span>export</span> LOCAL_IP=<span>${ip1}</span>
<span><span>#</span> Multi-node machine IPs, separated by commas</span>
<span>export</span> NODE_IP_LIST=<span><span>"</span><span>${ip1}</span>:8,<span>${ip2}</span>:8<span>"</span></span>
<span><span>#</span> Number of machine nodes</span>
<span>export</span> NODES=2
<span>export</span> NODE_NUM=<span><span>$((</span><span>${NODES}</span> <span>*</span> <span>${HOST_GPU_NUM}</span><span>))</span></span></pre></div>
<p dir="auto">Note: Replace <code>${ip1}</code> and <code>${ip2}</code> with the actual IP addresses!</p>
<p dir="auto">Then, on the machine with <code>${ip1}</code>, execute <code>bash train.sh</code> in the <code>train/</code> directory. Note that on the first run, you might see the following output:</p>
<div data-snippet-clipboard-copy-content="The authenticity of host '[ip]:36005 ([ip]:36005)' can't be established.
ECDSA key fingerprint is xxxxxx.
ECDSA key fingerprint is MD5:xxxxxx.
Are you sure you want to continue connecting (yes/no)?"><pre lang="ssh"><code>The authenticity of host '[ip]:36005 ([ip]:36005)' can't be established.
ECDSA key fingerprint is xxxxxx.
ECDSA key fingerprint is MD5:xxxxxx.
Are you sure you want to continue connecting (yes/no)?
</code></pre></div>
<p dir="auto">At this point, type <code>yes</code> to continue.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Key Parameters</h5><a id="user-content-key-parameters" aria-label="Permalink: Key Parameters" href="#key-parameters"></a></p>
<p dir="auto">The key parameters in the script are as follows:</p>
<ul dir="auto">
<li><code>--deepspeed</code>: This parameter should point to a DeepSpeed configuration file. The <code>train</code> folder provides three default DeepSpeed configuration files: <code>ds_zero2_no_offload.json</code>, <code>ds_zero3_no_offload.json</code>, <code>ds_zero3_offload.json</code>. The required GPU memory decreases in this order.</li>
<li><code>--model_name_or_path</code>: The path to the HF pre-trained model. Ensure this path contains the <code>modeling_hunyuan.py</code> and <code>configuration_hunyuan.py</code> files; otherwise, it cannot be loaded.</li>
<li><code>--tokenizer_name_or_path</code>: The path to the tokenizer folder. Ensure this path contains the <code>tokenization_hy.py</code> file; otherwise, it cannot be loaded.</li>
<li><code>--train_data_file</code>: The path to the training file, which should be a JSONL file.</li>
<li><code>--output_dir</code>: The output directory where logs, tensorboard files, and model weights will be stored.</li>
<li><code>--per_device_train_batch_size</code>: The batch size per GPU.</li>
<li><code>--gradient_accumulation_steps</code>: The number of gradient accumulation steps. The global batch size is <code>per_device_train_batch_size * gradient_accumulation_steps * dp_size</code>.</li>
<li><code>--max_steps</code>: The total number of training steps.</li>
<li><code>--save_steps</code>: The number of steps between saving checkpoints.</li>
<li><code>--use_lora</code>: Whether to use LoRA for training. This also accepts <code>--lora_rank</code>, <code>--lora_alpha</code>, and <code>--lora_dropout</code> parameters. LoRA is applied by default to the 'q_proj', 'k_proj', 'v_proj', 'o_proj' parameters. If you need to change this, modify it in the code. Note: <strong>When using LoRA for training, only the LoRA weights are saved, not the base model weights</strong>. If you need to merge LoRA weights, see the "LoRA Weight Merging" section below.</li>
<li><code>--make_moe_param_leaf_module</code>: When using zero3 and MoE training, treat the MoE module as a leaf module, meaning its parameters are not split by zero3. This option is expected to significantly increase memory usage.</li>
<li><code>--gradient_checkpointing</code>: Enable gradient checkpointing.</li>
<li><code>--train_attention_params_only</code>: Whether to train only the attention parameters.</li>
<li><code>--learning_rate</code>: The maximum learning rate during training.</li>
<li><code>--min_lr</code>: The minimum learning rate during training.</li>
<li><code>--use_flash_attn</code>: 开启 flash-attention 进行训练加速</li>
</ul>
<p dir="auto"><strong>Note:</strong></p>
<ul dir="auto">
<li>If you want to continue training from a previously saved checkpoint instead of loading pre-trained weights, specify <code>--resume_from_checkpoint</code> with the path to the checkpoint from the previous training. Do not specify <code>--model_name_or_path</code>, as this will only load the weights and not the training state.</li>
<li>When continuing training from a checkpoint, there might be slight deviations in loss due to randomness introduced by some non-deterministic algorithms, which is considered normal. Refer to: <a href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#randomness" rel="nofollow">HuggingFace Transformers Trainer Randomness</a></li>
<li>When <code>--model_name_or_path</code> is specified, all model-related parameters will be ignored.</li>
<li>Samples within a batch will be padded to align with the longest sample in the batch, with each sample having a maximum length of <code>max_seq_length</code>. Any excess will be truncated.</li>
<li>If you encounter warnings about bias weights not being loaded, you can ignore them, as biases are not used in Hunyuan-Large.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">What to Do If Out of Memory?</h4><a id="user-content-what-to-do-if-out-of-memory" aria-label="Permalink: What to Do If Out of Memory?" href="#what-to-do-if-out-of-memory"></a></p>
<p dir="auto">Refer to: <a href="https://www.deepspeed.ai/docs/config-json/" rel="nofollow">DeepSpeed Configuration</a></p>
<p dir="auto">You can try modifying the DeepSpeed configuration by removing the auto attribute from these parameters and reducing their values:</p>
<ul dir="auto">
<li><code>stage3_param_persistence_threshold</code></li>
<li><code>stage3_prefetch_bucket_size</code></li>
<li><code>stage3_max_reuse_distance</code></li>
<li><code>stage3_max_reuse_distance</code></li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Merging LoRA Models</h4><a id="user-content-merging-lora-models" aria-label="Permalink: Merging LoRA Models" href="#merging-lora-models"></a></p>
<p dir="auto">The saved LoRA weights cannot be merged into the zero3 model during training because, with zero3 enabled, model weights are split across different data parallel ranks. If you want to merge LoRA weights into the base model, you can do so offline to obtain the merged weight file. Execute <code>merge_lora_weight.sh</code> to merge the LoRA weights with the base model weights. The parameters include:</p>
<ul dir="auto">
<li><code>--base_model_path</code>: Directory of the base model weights</li>
<li><code>--adapter_model_path</code>: Directory of the LoRA weights</li>
<li><code>--output_path</code>: Directory to save the merged weights</li>
<li><code>--save_dtype</code>: Data format for storing the merged weights, available options include: fp16, bf16, fp32</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Inference and Deployment</h2><a id="user-content-inference-and-deployment" aria-label="Permalink: Inference and Deployment" href="#inference-and-deployment"></a></p>
<p dir="auto">HunyuanLLM uses TRT-LLM and vLLM for deployment. We are open sourcing the vLLM deployment (see Reasoning with vLLM), and the TRT-LLM deployment (see Reasoning with TRT-LLM) will be available in the near future.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using TRT-LLM for Inference</h2><a id="user-content-using-trt-llm-for-inference" aria-label="Permalink: Using TRT-LLM for Inference" href="#using-trt-llm-for-inference"></a></p>
<p dir="auto">To be opened</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using vLLM for Inference</h2><a id="user-content-using-vllm-for-inference" aria-label="Permalink: Using vLLM for Inference" href="#using-vllm-for-inference"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Docker:</h3><a id="user-content-docker" aria-label="Permalink: Docker:" href="#docker"></a></p>
<p dir="auto">To simplify the deployment process, HunyuanLLM provides a pre-built Docker image:</p>
<p dir="auto"><a href="https://hub.docker.com/repository/docker/hunyuaninfer/hunyuan-large/general" rel="nofollow">hunyuaninfer/hunyuan-large</a>. You only need to download the model files and start the Docker container using the code below to begin model inference.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run --name hunyuanLLM_infer -itd --privileged --user root --net=host --ipc=host --gpus=8 hunyuaninfer/hunyuan-large:infer-open-source"><pre>docker run --name hunyuanLLM_infer -itd --privileged --user root --net=host --ipc=host --gpus=8 hunyuaninfer/hunyuan-large:infer-open-source</pre></div>
<p dir="auto">Note: Docker container privilege management. The above code uses privileged mode (<code>--privileged</code>) to start the Docker container, which grants the container higher privileges, increasing the risk of data leakage and cluster security threats. It is recommended to avoid using privileged mode unless necessary to reduce security risks. For scenarios where privileged mode is required, conduct a thorough security assessment and implement appropriate security monitoring and hardening measures.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configure Passwordless SSH Login Between Machines</h3><a id="user-content-configure-passwordless-ssh-login-between-machines-1" aria-label="Permalink: Configure Passwordless SSH Login Between Machines" href="#configure-passwordless-ssh-login-between-machines-1"></a></p>
<p dir="auto">The following steps use two machines as an example, with their IPs represented as <code>${ip1}</code> and <code>${ip2}</code>. These operations are performed within a Docker container.</p>
<p dir="auto">First, run <code>passwd</code> on both machines to set a password, for example: <code>Tmp123,./</code></p>
<p dir="auto">Copy <code>inference/login_ssh.py</code> into the container and execute the following command, ensuring the IP and password are correctly entered.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 login_ssh.py --ips ${ip1},${ip2} --port 36000 --password=Tmp123,./"><pre>python3 login_ssh.py --ips <span>${ip1}</span>,<span>${ip2}</span> --port 36000 --password=Tmp123,./</pre></div>
<p dir="auto"><strong>Note 📢: Before starting, be sure to verify multi-machine communication using VLLM's debugging script: <a href="https://docs.vllm.ai/en/latest/getting_started/debugging.html" rel="nofollow">https://docs.vllm.ai/en/latest/getting_started/debugging.html</a></strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">BF16 Deployment</h3><a id="user-content-bf16-deployment" aria-label="Permalink: BF16 Deployment" href="#bf16-deployment"></a></p>
<p dir="auto">BF16 requires 16 H800 or H20 GPUs for deployment. After verifying that multi-machine communication is correct, execute the following steps:</p>
<p dir="auto">Before running the commands, set the following environment variables:</p>
<div dir="auto" data-snippet-clipboard-copy-content="${LOCAL_IP}: The IP corresponding to bond1 on the current machine
${MODEL_PATH}: Path to the Hunyuan LLM model"><pre><span>${LOCAL_IP}</span>: The IP corresponding to bond1 on the current machine
<span>${MODEL_PATH}</span>: Path to the Hunyuan LLM model</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Step 1: Start Ray</h4><a id="user-content-step-1-start-ray" aria-label="Permalink: Step 1: Start Ray" href="#step-1-start-ray"></a></p>
<p dir="auto">Ray is an open-source library for parallel and distributed Python. In this section, we use Ray to achieve multi-machine communication.</p>
<p dir="auto">Ray Component Configuration Hardening: The default configuration of Ray components does not enable authentication mechanisms for service ports (e.g., 6379, 8265), posing risks of unauthorized access and command execution. It is recommended to deploy Ray components only in trusted internal network environments or ensure strict access control list (ACL) policies are implemented for these ports to prevent unauthorized network access.</p>
<p dir="auto">First, start Ray on each node (either in the background or by keeping the terminal running):</p>
<p dir="auto">On the head node:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export VLLM_HOST_IP=${LOCAL_IP}
export NCCL_SOCKET_IFNAME=bond1
export GLOO_SOCKET_IFNAME=bond1
ray start --block --head --node-ip-address=${LOCAL_IP} --port=6379"><pre><span>export</span> VLLM_HOST_IP=<span>${LOCAL_IP}</span>
<span>export</span> NCCL_SOCKET_IFNAME=bond1
<span>export</span> GLOO_SOCKET_IFNAME=bond1
ray start --block --head --node-ip-address=<span>${LOCAL_IP}</span> --port=6379</pre></div>
<p dir="auto">On all worker nodes:</p>
<p dir="auto">Note: Replace <code>{HEAD NODE $LOCAL_IP}</code> with the actual <code>${LOCAL_IP}</code> of the head node.</p>
<div dir="auto" data-snippet-clipboard-copy-content="export VLLM_HOST_IP=${LOCAL_IP}
export NCCL_SOCKET_IFNAME=bond1
export GLOO_SOCKET_IFNAME=bond1
ray start --block --address={HEAD NODE $LOCAL_IP}:6379 --node-ip-address=${LOCAL_IP}"><pre><span>export</span> VLLM_HOST_IP=<span>${LOCAL_IP}</span>
<span>export</span> NCCL_SOCKET_IFNAME=bond1
<span>export</span> GLOO_SOCKET_IFNAME=bond1
ray start --block --address={HEAD NODE <span>$LOCAL_IP</span>}:6379 --node-ip-address=<span>${LOCAL_IP}</span></pre></div>
<p dir="auto">If Ray fails to start, execute <code>ray stop</code> and then run the above commands again.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Step 2: Execute Inference</h4><a id="user-content-step-2-execute-inference" aria-label="Permalink: Step 2: Execute Inference" href="#step-2-execute-inference"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Method 1: Command Line Inference</h4><a id="user-content-method-1-command-line-inference" aria-label="Permalink: Method 1: Command Line Inference" href="#method-1-command-line-inference"></a></p>
<p dir="auto">Below is a code snippet demonstrating how to quickly request the chat model using <code>vLLM</code>:</p>
<p dir="auto">Note: vLLM Component Remote Code Execution Protection. In the code below, if the <code>trust-remote-code</code> configuration option of the vLLM component is enabled, it will allow loading and executing code from remote model repositories, which may lead to the execution of malicious code. Unless explicitly required by business needs, it is recommended to keep this configuration option disabled to reduce potential security threats.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import os
from vllm import LLM, SamplingParams

model_path=os.environ.get('MODEL_PATH')

llm = LLM(model=model_path,
        tokenizer=model_path,
        trust_remote_code=True,
        max_model_len=10240,
        dtype='bfloat16',
        tensor_parallel_size=16,
        pipeline_parallel_size=1,
        disable_log_stats=False,
        gpu_memory_utilization=0.98,
        disable_custom_all_reduce=True,
        #distributed_executor_backend='ray',
        enforce_eager=True,
        max_num_seqs=8,
        use_v2_block_manager=True,
        quantization=None)

prompts = [&quot;海水为什么是咸的&quot;]

sampling_params = SamplingParams(
    temperature=0.7, top_p=0.6, max_tokens=200, top_k=20, repetition_penalty=1.05)

outputs = llm.generate(prompts, sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f&quot;Prompt: {prompt!r}, Generated text: {generated_text!r}&quot;)"><pre><span>import</span> <span>os</span>
<span>from</span> <span>vllm</span> <span>import</span> <span>LLM</span>, <span>SamplingParams</span>

<span>model_path</span><span>=</span><span>os</span>.<span>environ</span>.<span>get</span>(<span>'MODEL_PATH'</span>)

<span>llm</span> <span>=</span> <span>LLM</span>(<span>model</span><span>=</span><span>model_path</span>,
        <span>tokenizer</span><span>=</span><span>model_path</span>,
        <span>trust_remote_code</span><span>=</span><span>True</span>,
        <span>max_model_len</span><span>=</span><span>10240</span>,
        <span>dtype</span><span>=</span><span>'bfloat16'</span>,
        <span>tensor_parallel_size</span><span>=</span><span>16</span>,
        <span>pipeline_parallel_size</span><span>=</span><span>1</span>,
        <span>disable_log_stats</span><span>=</span><span>False</span>,
        <span>gpu_memory_utilization</span><span>=</span><span>0.98</span>,
        <span>disable_custom_all_reduce</span><span>=</span><span>True</span>,
        <span>#distributed_executor_backend='ray',</span>
        <span>enforce_eager</span><span>=</span><span>True</span>,
        <span>max_num_seqs</span><span>=</span><span>8</span>,
        <span>use_v2_block_manager</span><span>=</span><span>True</span>,
        <span>quantization</span><span>=</span><span>None</span>)

<span>prompts</span> <span>=</span> [<span>"海水为什么是咸的"</span>]

<span>sampling_params</span> <span>=</span> <span>SamplingParams</span>(
    <span>temperature</span><span>=</span><span>0.7</span>, <span>top_p</span><span>=</span><span>0.6</span>, <span>max_tokens</span><span>=</span><span>200</span>, <span>top_k</span><span>=</span><span>20</span>, <span>repetition_penalty</span><span>=</span><span>1.05</span>)

<span>outputs</span> <span>=</span> <span>llm</span>.<span>generate</span>(<span>prompts</span>, <span>sampling_params</span>)

<span># Print the outputs.</span>
<span>for</span> <span>output</span> <span>in</span> <span>outputs</span>:
    <span>prompt</span> <span>=</span> <span>output</span>.<span>prompt</span>
    <span>generated_text</span> <span>=</span> <span>output</span>.<span>outputs</span>[<span>0</span>].<span>text</span>
    <span>print</span>(<span>f"Prompt: <span><span>{</span><span>prompt</span>!r<span>}</span></span>, Generated text: <span><span>{</span><span>generated_text</span>!r<span>}</span></span>"</span>)</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Method 2: Service-Based Inference</h4><a id="user-content-method-2-service-based-inference" aria-label="Permalink: Method 2: Service-Based Inference" href="#method-2-service-based-inference"></a></p>
<p dir="auto">Below we demonstrate how to deploy the model using <code>vLLM</code> in a service-based manner and make requests.</p>
<p dir="auto">Run the following on the head node:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export VLLM_HOST_IP=${LOCAL_IP}
export NCCL_SOCKET_IFNAME=bond1
export GLOO_SOCKET_IFNAME=bond1"><pre><span>export</span> VLLM_HOST_IP=<span>${LOCAL_IP}</span>
<span>export</span> NCCL_SOCKET_IFNAME=bond1
<span>export</span> GLOO_SOCKET_IFNAME=bond1</pre></div>
<p dir="auto">Next, start the service by running:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd inference
sh run_server.sh"><pre><span>cd</span> inference
sh run_server.sh</pre></div>
<p dir="auto"><em>Tips</em>: Troubleshooting, if you encounter the following error:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ray.exceptions.RaySystemError: System error: No module named 'transformers_modules' traceback: Traceback (most recent call last):
ModuleNotFoundError: No module named 'transformers_modules'"><pre><span>ray</span>.<span>exceptions</span>.<span>RaySystemError</span>: <span>System</span> <span>error</span>: <span>No</span> <span>module</span> <span>named</span> <span>'transformers_modules'</span> <span>traceback</span>: <span>Traceback</span> (<span>most</span> <span>recent</span> <span>call</span> <span>last</span>):
<span>ModuleNotFoundError</span>: <span>No</span> <span>module</span> <span>named</span> <span>'transformers_modules'</span></pre></div>
<p dir="auto">Copy the <code>~/.cache/huggingface/modules/</code> directory from the head node to the corresponding path on all worker nodes.</p>
<p dir="auto">After successfully running <code>run_server.sh</code>, execute the request script:</p>

<p dir="auto">Be sure to modify <code>${LOCAL_IP}</code> and <code>${MODEL_PATH}</code> in <code>openapi.sh</code> to values match the corresponding service.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quantized Model Deployment:</h3><a id="user-content-quantized-model-deployment" aria-label="Permalink: Quantized Model Deployment:" href="#quantized-model-deployment"></a></p>
<p dir="auto">This section describes the process of deploying a quantized model using vLLM.</p>
<p dir="auto">Image: The deployment image is the same as for BF16.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Int8 Quantized Model Deployment:</h4><a id="user-content-int8-quantized-model-deployment" aria-label="Permalink: Int8 Quantized Model Deployment:" href="#int8-quantized-model-deployment"></a></p>
<p dir="auto">To deploy the Int8-weight-only version of the Hunyuan-L model, simply set the environment variables in <code>run_server_int8.sh</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="${MODEL_PATH}: Path to the BF16 model
${LOCAL_IP}: The IP corresponding to bond1 on the current machine"><pre><span>${MODEL_PATH}</span>: Path to the BF16 model
<span>${LOCAL_IP}</span>: The IP corresponding to bond1 on the current machine</pre></div>
<p dir="auto">Then, start the Int8 service by running:</p>

<p dir="auto">After successfully running <code>run_server_int8.sh</code>, execute the request script:</p>

<p dir="auto"><h4 tabindex="-1" dir="auto">FP8 Quantized Model Deployment:</h4><a id="user-content-fp8-quantized-model-deployment" aria-label="Permalink: FP8 Quantized Model Deployment:" href="#fp8-quantized-model-deployment"></a></p>
<p dir="auto">To deploy the W8A8C8 version of the Hunyuan-L model, simply set the environment variables in <code>run_server_fp8.sh</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="${MODEL_PATH}: Path to the FP8 model
${LOCAL_IP}: The IP corresponding to bond1 on the current machine"><pre><span>${MODEL_PATH}</span>: Path to the FP8 model
<span>${LOCAL_IP}</span>: The IP corresponding to bond1 on the current machine</pre></div>
<p dir="auto">Then, start the FP8 service by running:</p>

<p dir="auto">After successfully running <code>run_server_fp8.sh</code>, execute the request script:</p>

<p dir="auto"><h4 tabindex="-1" dir="auto">FP8 BENCHMARK</h4><a id="user-content-fp8-benchmark" aria-label="Permalink: FP8 BENCHMARK" href="#fp8-benchmark"></a></p>
<p dir="auto">This part introduces the Benchmark of Hunyuan Large Instruct FP8 quantitative model.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Dataset</th>
<th>BF16</th>
<th>W8A8C8-FP8</th>
</tr>
</thead>
<tbody>
<tr>
<td>ARC-C</td>
<td>94.6</td>
<td>94.2</td>
</tr>
<tr>
<td>C-Eval</td>
<td>88.6</td>
<td>89.2</td>
</tr>
<tr>
<td>CMMLU</td>
<td>90.4</td>
<td>89.8</td>
</tr>
<tr>
<td>MMLU</td>
<td>89.9</td>
<td>88.9</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Inference Performance</h3><a id="user-content-inference-performance" aria-label="Permalink: Inference Performance" href="#inference-performance"></a></p>
<p dir="auto">This section presents the efficiency test results of deploying various models (original and quantized) using vLLM, including inference speed (tokens/s) under different batch sizes.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Inference Framework</th>
<th>Model</th>
<th>Number of GPUs (H20)</th>
<th>input_length</th>
<th>batch=1</th>
<th>batch=4</th>
</tr>
</thead>
<tbody>
<tr>
<td>vLLM</td>
<td>Hunyuan-Large</td>
<td>16</td>
<td>2048</td>
<td>20.2</td>
<td>75.5</td>
</tr>
<tr>
<td>vLLM</td>
<td>Hunyuan-Large(int8 weight only)</td>
<td>8</td>
<td>2048</td>
<td>19.3</td>
<td>73.6</td>
</tr>
<tr>
<td>vLLM</td>
<td>Hunyuan-Large(W8A8C8-FP8)</td>
<td>8</td>
<td>2048</td>
<td>19.8</td>
<td>74.9</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tokenizer</h2><a id="user-content-tokenizer" aria-label="Permalink: Tokenizer" href="#tokenizer"></a></p>
<p dir="auto">The tokenizer used in the HunYuan-Large model balances compression rate and effectiveness, ensuring that embeddings are sufficiently trained. The vocabulary includes 100K tokens integrated from tiktoken. Additionally, we trained an extra 29K Chinese tokens using a large amount of high-quality Chinese training data to enhance the model's Chinese capabilities and the tokenizer's compression rate. Combined, our new tokenizer improves the compression rate compared to the LLaMA3 tokenizer, increasing from 2.78 characters/token to 3.13 characters/token.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hunyuan API</h2><a id="user-content-hunyuan-api" aria-label="Permalink: Hunyuan API" href="#hunyuan-api"></a></p>
<p dir="auto">You can experience our Hunyuan-Large model on Tencent Cloud. For details, please visit: <a href="https://cloud.tencent.com/document/product/1729/97730" rel="nofollow">https://cloud.tencent.com/document/product/1729/97730</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Interactive Demo Web</h2><a id="user-content-interactive-demo-web" aria-label="Permalink: Interactive Demo Web" href="#interactive-demo-web"></a></p>
<p dir="auto">The Hunyuan-Large web demo is now open. Visit <a href="https://huggingface.co/spaces/tencent/Hunyuan-Large" rel="nofollow">https://huggingface.co/spaces/tencent/Hunyuan-Large</a> to easily experience our model.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Training/Inference on TI</h2><a id="user-content-traininginference-on-ti" aria-label="Permalink: Training/Inference on TI" href="#traininginference-on-ti"></a></p>
<p dir="auto">Tencent Cloud's <a href="https://cloud.tencent.com/product/ti" rel="nofollow">TI Platform</a> is a comprehensive machine learning platform tailored for AI engineers. With the Hunyuan-Large model already integrated, you can easily train and deploy it in just a few steps. Visit <a href="https://console.cloud.tencent.com/tione/v2/aimarket/detail/hunyuan_series?PublicAlgoGroupId=hunyuan-large-chat&amp;detailTab=demo" rel="nofollow">Chat with Hunyuan-Large</a> to experience real-time conversations with the model, and explore <a href="https://cloud.tencent.com/document/product/851/112032" rel="nofollow">Hunyuan-Large Best Practice on TI</a> to create your own customized Hunyuan-Large model.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you find our work helpful, feel free to give us a cite.</p>
<div data-snippet-clipboard-copy-content="@misc{sun2024hunyuanlargeopensourcemoemodel,
      title={Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent}, 
      author={Xingwu Sun and Yanfeng Chen and Yiqing Huang and Ruobing Xie and Jiaqi Zhu and Kai Zhang and Shuaipeng Li and Zhen Yang and Jonny Han and Xiaobo Shu and Jiahao Bu and Zhongzhi Chen and Xuemeng Huang and Fengzong Lian and Saiyong Yang and Jianfeng Yan and Yuyuan Zeng and Xiaoqin Ren and Chao Yu and Lulu Wu and Yue Mao and Tao Yang and Suncong Zheng and Kan Wu and Dian Jiao and Jinbao Xue and Xipeng Zhang and Decheng Wu and Kai Liu and Dengpeng Wu and Guanghui Xu and Shaohua Chen and Shuang Chen and Xiao Feng and Yigeng Hong and Junqiang Zheng and Chengcheng Xu and Zongwei Li and Xiong Kuang and Jianglu Hu and Yiqi Chen and Yuchi Deng and Guiyang Li and Ao Liu and Chenchen Zhang and Shihui Hu and Zilong Zhao and Zifan Wu and Yao Ding and Weichao Wang and Han Liu and Roberts Wang and Hao Fei and Peijie She and Ze Zhao and Xun Cao and Hai Wang and Fusheng Xiang and Mengyuan Huang and Zhiyuan Xiong and Bin Hu and Xuebin Hou and Lei Jiang and Jiajia Wu and Yaping Deng and Yi Shen and Qian Wang and Weijie Liu and Jie Liu and Meng Chen and Liang Dong and Weiwen Jia and Hu Chen and Feifei Liu and Rui Yuan and Huilin Xu and Zhenxiang Yan and Tengfei Cao and Zhichao Hu and Xinhua Feng and Dong Du and Tinghao She and Yangyu Tao and Feng Zhang and Jianchen Zhu and Chengzhong Xu and Xirui Li and Chong Zha and Wen Ouyang and Yinben Xia and Xiang Li and Zekun He and Rongpeng Chen and Jiawei Song and Ruibin Chen and Fan Jiang and Chongqing Zhao and Bo Wang and Hao Gong and Rong Gan and Winston Hu and Zhanhui Kang and Yong Yang and Yuhong Liu and Di Wang and Jie Jiang},
      year={2024},
      eprint={2411.02265},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.02265}, 
}"><pre><code>@misc{sun2024hunyuanlargeopensourcemoemodel,
      title={Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent}, 
      author={Xingwu Sun and Yanfeng Chen and Yiqing Huang and Ruobing Xie and Jiaqi Zhu and Kai Zhang and Shuaipeng Li and Zhen Yang and Jonny Han and Xiaobo Shu and Jiahao Bu and Zhongzhi Chen and Xuemeng Huang and Fengzong Lian and Saiyong Yang and Jianfeng Yan and Yuyuan Zeng and Xiaoqin Ren and Chao Yu and Lulu Wu and Yue Mao and Tao Yang and Suncong Zheng and Kan Wu and Dian Jiao and Jinbao Xue and Xipeng Zhang and Decheng Wu and Kai Liu and Dengpeng Wu and Guanghui Xu and Shaohua Chen and Shuang Chen and Xiao Feng and Yigeng Hong and Junqiang Zheng and Chengcheng Xu and Zongwei Li and Xiong Kuang and Jianglu Hu and Yiqi Chen and Yuchi Deng and Guiyang Li and Ao Liu and Chenchen Zhang and Shihui Hu and Zilong Zhao and Zifan Wu and Yao Ding and Weichao Wang and Han Liu and Roberts Wang and Hao Fei and Peijie She and Ze Zhao and Xun Cao and Hai Wang and Fusheng Xiang and Mengyuan Huang and Zhiyuan Xiong and Bin Hu and Xuebin Hou and Lei Jiang and Jiajia Wu and Yaping Deng and Yi Shen and Qian Wang and Weijie Liu and Jie Liu and Meng Chen and Liang Dong and Weiwen Jia and Hu Chen and Feifei Liu and Rui Yuan and Huilin Xu and Zhenxiang Yan and Tengfei Cao and Zhichao Hu and Xinhua Feng and Dong Du and Tinghao She and Yangyu Tao and Feng Zhang and Jianchen Zhu and Chengzhong Xu and Xirui Li and Chong Zha and Wen Ouyang and Yinben Xia and Xiang Li and Zekun He and Rongpeng Chen and Jiawei Song and Ruibin Chen and Fan Jiang and Chongqing Zhao and Bo Wang and Hao Gong and Rong Gan and Winston Hu and Zhanhui Kang and Yong Yang and Yuhong Liu and Di Wang and Jie Jiang},
      year={2024},
      eprint={2411.02265},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.02265}, 
}
</code></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Contact Us</h2><a id="user-content-contact-us" aria-label="Permalink: Contact Us" href="#contact-us"></a></p>
<p dir="auto">If you would like to leave a message for our R&amp;D and product teams, Welcome to contact our open-source team . You can also contact us via email (<a href="mailto:hunyuan_opensource@tencent.com">hunyuan_opensource@tencent.com</a>).</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Whirlwind – Async concurrent hashmap for Rust (116 pts)]]></title>
            <link>https://github.com/fortress-build/whirlwind</link>
            <guid>42053747</guid>
            <pubDate>Tue, 05 Nov 2024 18:02:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/fortress-build/whirlwind">https://github.com/fortress-build/whirlwind</a>, See on <a href="https://news.ycombinator.com/item?id=42053747">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">🌀 Whirlwind</h2><a id="user-content--whirlwind" aria-label="Permalink: 🌀 Whirlwind" href="#-whirlwind"></a></p>
<p dir="auto"><a href="https://github.com/yourusername/shardmap/actions"><img src="https://camo.githubusercontent.com/3d4342abdadc99b07e57b7ca52055dc301b3653c031e4c2aea260634adc71ad9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f666f7274726573732d6275696c642f776869726c77696e642f727573742e796d6c3f6272616e63683d6d61696e" alt="Build Status" data-canonical-src="https://img.shields.io/github/actions/workflow/status/fortress-build/whirlwind/rust.yml?branch=main"></a>
<a href="https://crates.io/crates/whirlwind" rel="nofollow"><img src="https://camo.githubusercontent.com/969ebd9c3027f52c2839134225f7037dc1fb82a0203a8d8e9cae1d66df8f0f32/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f776869726c77696e64" alt="Crates.io" data-canonical-src="https://img.shields.io/crates/v/whirlwind"></a>
<a href="https://docs.rs/whirlwind" rel="nofollow"><img src="https://camo.githubusercontent.com/76ace1e89bfc0634727a66633ac4a07f80da861497a48251bcb0fa59e266c7b7/68747470733a2f2f646f63732e72732f776869726c77696e642f62616467652e737667" alt="Docs.rs" data-canonical-src="https://docs.rs/whirlwind/badge.svg"></a>
<a href="https://github.com/fortress-build/whirlwind/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/940e860108f5b2c58bdd7f1551e99c79b31000843e59249a5e33ca94dcbbd173/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f6c2f776869726c77696e64" alt="License" data-canonical-src="https://img.shields.io/crates/l/whirlwind"></a></p>
<p dir="auto">An asynchronous, sharded <code>HashMap</code> for high-performance concurrent data access
in Rust.</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">This crate is in development, and breaking changes may be made up until a 1.0 release.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">📖 Table of Contents</h2><a id="user-content--table-of-contents" aria-label="Permalink: 📖 Table of Contents" href="#-table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#-features">Features</a></li>
<li><a href="#-installation">Installation</a></li>
<li><a href="#-usage">Usage</a></li>
<li><a href="#-examples">Examples</a></li>
<li><a href="#-benchmarks">Benchmark</a></li>
<li><a href="#-contributing">Contributing</a></li>
<li><a href="#license">License</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">✨ Features</h2><a id="user-content--features" aria-label="Permalink: ✨ Features" href="#-features"></a></p>
<ul dir="auto">
<li><strong>Async Ready</strong>: Seamless integration with Rust's <code>async</code>/<code>await</code> syntax.</li>
<li><strong>High Performance</strong>: Sharding minimizes lock contention in concurrent environments.</li>
<li><strong>Thread-safe</strong>: Safe for use across multiple threads without fear of data races.</li>
<li><strong>Familiar API</strong>: Intuitive <code>HashMap</code>-like interface for ease of adoption.</li>
<li><strong>Customizable Shards</strong>: Configure the number of shards to optimize for your workload.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">📦 Installation</h2><a id="user-content--installation" aria-label="Permalink: 📦 Installation" href="#-installation"></a></p>
<p dir="auto">Add <code>whirlwind</code> to your <code>Cargo.toml</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[dependencies]
whirlwind = &quot;0.1.1&quot;"><pre>[<span>dependencies</span>]
<span>whirlwind</span> = <span><span>"</span>0.1.1<span>"</span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔧 Usage</h2><a id="user-content--usage" aria-label="Permalink: 🔧 Usage" href="#-usage"></a></p>
<p dir="auto">Here's a quick example to get you started:</p>
<div dir="auto" data-snippet-clipboard-copy-content="use whirlwind::ShardMap;

#[tokio::main]
async fn main() {
    let map = ShardMap::new();

    map.insert(&quot;apple&quot;, 3).await;
    map.insert(&quot;banana&quot;, 5).await;

    if let Some(quantity) = map.get(&amp;&quot;apple&quot;).await {
        println!(&quot;We have {} apples!&quot;, quantity);
    }

    map.remove(&amp;&quot;banana&quot;).await;
}"><pre><span>use</span> whirlwind<span>::</span><span>ShardMap</span><span>;</span>

<span>#<span>[</span>tokio<span>::</span>main<span>]</span></span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> map = <span>ShardMap</span><span>::</span><span>new</span><span>(</span><span>)</span><span>;</span>

    map<span>.</span><span>insert</span><span>(</span><span>"apple"</span><span>,</span> <span>3</span><span>)</span><span>.</span><span>await</span><span>;</span>
    map<span>.</span><span>insert</span><span>(</span><span>"banana"</span><span>,</span> <span>5</span><span>)</span><span>.</span><span>await</span><span>;</span>

    <span>if</span> <span>let</span> <span>Some</span><span>(</span>quantity<span>)</span> = map<span>.</span><span>get</span><span>(</span><span>&amp;</span><span>"apple"</span><span>)</span><span>.</span><span>await</span> <span>{</span>
        <span>println</span><span>!</span><span>(</span><span>"We have {} apples!"</span>, quantity<span>)</span><span>;</span>
    <span>}</span>

    map<span>.</span><span>remove</span><span>(</span><span>&amp;</span><span>"banana"</span><span>)</span><span>.</span><span>await</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">📚 Examples</h2><a id="user-content--examples" aria-label="Permalink: 📚 Examples" href="#-examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Concurrent Inserts</h3><a id="user-content-concurrent-inserts" aria-label="Permalink: Concurrent Inserts" href="#concurrent-inserts"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="use whirlwind::ShardMap;
use tokio::task::JoinSet;

#[tokio::main]
async fn main() {
    let map = ShardMap::new();
    let tasks: JoinSet<_> = (0..1000).map(|i| {
        let map = map.clone();
        tokio::spawn(async move {
            map.insert(i, i * 2).await;
        })
    }).collect();

    tasks.join_all().await.ok();

    assert_eq!(map.len().await, 1000);
}"><pre><span>use</span> whirlwind<span>::</span><span>ShardMap</span><span>;</span>
<span>use</span> tokio<span>::</span>task<span>::</span><span>JoinSet</span><span>;</span>

<span>#<span>[</span>tokio<span>::</span>main<span>]</span></span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> map = <span>ShardMap</span><span>::</span><span>new</span><span>(</span><span>)</span><span>;</span>
    <span>let</span> tasks<span>:</span> <span>JoinSet</span><span>&lt;</span><span>_</span><span>&gt;</span> = <span>(</span><span>0</span>..<span>1000</span><span>)</span><span>.</span><span>map</span><span>(</span>|i| <span>{</span>
        <span>let</span> map = map<span>.</span><span>clone</span><span>(</span><span>)</span><span>;</span>
        tokio<span>::</span><span>spawn</span><span>(</span><span>async</span> <span>move</span> <span>{</span>
            map<span>.</span><span>insert</span><span>(</span>i<span>,</span> i <span>*</span> <span>2</span><span>)</span><span>.</span><span>await</span><span>;</span>
        <span>}</span><span>)</span>
    <span>}</span><span>)</span><span>.</span><span>collect</span><span>(</span><span>)</span><span>;</span>

    tasks<span>.</span><span>join_all</span><span>(</span><span>)</span><span>.</span><span>await</span><span>.</span><span>ok</span><span>(</span><span>)</span><span>;</span>

    <span>assert_eq</span><span>!</span><span>(</span>map.len<span>(</span><span>)</span>.<span>await</span>, <span>1000</span><span>)</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Custom Shard Count</h3><a id="user-content-custom-shard-count" aria-label="Permalink: Custom Shard Count" href="#custom-shard-count"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="use whirlwind::ShardMap;

#[tokio::main]
async fn main() {
    let map = ShardMap::with_shards(64); // Initialize with 64 shards
    // Use the map as needed
}"><pre><span>use</span> whirlwind<span>::</span><span>ShardMap</span><span>;</span>

<span>#<span>[</span>tokio<span>::</span>main<span>]</span></span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> map = <span>ShardMap</span><span>::</span><span>with_shards</span><span>(</span><span>64</span><span>)</span><span>;</span> <span>// Initialize with 64 shards</span>
    <span>// Use the map as needed</span>
<span>}</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">📊 Benchmarks</h2><a id="user-content--benchmarks" aria-label="Permalink: 📊 Benchmarks" href="#-benchmarks"></a></p>
<p dir="auto">Benchmarks were run in a asyncified version of <a href="https://github.com/xacrimon/conc-map-bench">this benchmark</a>. You can
find it <a href="https://github.com/willothy/conc-map-bench">here</a>. Since the benchmarks use <a href="https://github.com/jonhoo/bustle"><code>jonhoo/bustle</code></a>,
an asyncified fork of that library (<a href="https://github.com/willothy/bustle">here</a>) is required.</p>
<p dir="auto">Machine: Apple M3 Max (2023 16-inch MacBook Pro, 36GB RAM)</p>
<p dir="auto">OS: macOS 15.0</p>
<p dir="auto">See the <code>results/</code> directory.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Read Heavy (std hasher)</h3><a id="user-content-read-heavy-std-hasher" aria-label="Permalink: Read Heavy (std hasher)" href="#read-heavy-std-hasher"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/ReadHeavy.std.throughput.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/ReadHeavy.std.throughput.svg" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/ReadHeavy.std.latency.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/ReadHeavy.std.latency.svg" alt=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exchange (std hasher)</h3><a id="user-content-exchange-std-hasher" aria-label="Permalink: Exchange (std hasher)" href="#exchange-std-hasher"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/Exchange.std.throughput.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/Exchange.std.throughput.svg" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/Exchange.std.latency.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/Exchange.std.latency.svg" alt=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rapid Grow (std hasher)</h3><a id="user-content-rapid-grow-std-hasher" aria-label="Permalink: Rapid Grow (std hasher)" href="#rapid-grow-std-hasher"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/RapidGrow.std.throughput.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/RapidGrow.std.throughput.svg" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/RapidGrow.std.latency.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/RapidGrow.std.latency.svg" alt=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Read Heavy (ahash)</h3><a id="user-content-read-heavy-ahash" aria-label="Permalink: Read Heavy (ahash)" href="#read-heavy-ahash"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/ReadHeavy.ahash.throughput.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/ReadHeavy.ahash.throughput.svg" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/ReadHeavy.ahash.latency.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/ReadHeavy.ahash.latency.svg" alt=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exchange (ahash)</h3><a id="user-content-exchange-ahash" aria-label="Permalink: Exchange (ahash)" href="#exchange-ahash"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/Exchange.ahash.throughput.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/Exchange.ahash.throughput.svg" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/Exchange.ahash.latency.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/Exchange.ahash.latency.svg" alt=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rapid Grow (ahash)</h3><a id="user-content-rapid-grow-ahash" aria-label="Permalink: Rapid Grow (ahash)" href="#rapid-grow-ahash"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/RapidGrow.ahash.throughput.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/RapidGrow.ahash.throughput.svg" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/RapidGrow.ahash.latency.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/RapidGrow.ahash.latency.svg" alt=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">🤝 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 🤝 Contributing" href="#-contributing"></a></p>
<p dir="auto">Contributions are welcome! Please follow these steps:</p>
<ol dir="auto">
<li>Fork the repository.</li>
<li>Create a new branch: <code>git checkout -b feature/your-feature</code>.</li>
<li>Commit your changes: <code>git commit -am 'Add your feature'</code>.</li>
<li>Push to the branch: <code>git push origin feature/your-feature</code>.</li>
<li>Open a pull request.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running Tests</h3><a id="user-content-running-tests" aria-label="Permalink: Running Tests" href="#running-tests"></a></p>
<p dir="auto">Ensure all tests pass before submitting a PR:</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Code Style</h3><a id="user-content-code-style" aria-label="Permalink: Code Style" href="#code-style"></a></p>
<p dir="auto">We use <code>rustfmt</code> for code formatting:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Copyright 2024 Will Hopkins</p>
<p dir="auto">Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at</p>
<p dir="auto"><a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">http://www.apache.org/licenses/LICENSE-2.0</a></p>
<p dir="auto">Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>
<hr>
<p dir="auto">Made with 💖 and Rust.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I wrote an open-source browser alternative for Computer Use for any LLM (142 pts)]]></title>
            <link>https://github.com/gregpr07/browser-use</link>
            <guid>42052432</guid>
            <pubDate>Tue, 05 Nov 2024 15:51:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/gregpr07/browser-use">https://github.com/gregpr07/browser-use</a>, See on <a href="https://news.ycombinator.com/item?id=42052432">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">🌐 Browser-Use</h2><a id="user-content--browser-use" aria-label="Permalink: 🌐 Browser-Use" href="#-browser-use"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Open-Source Web Automation with LLMs</h3><a id="user-content-open-source-web-automation-with-llms" aria-label="Permalink: Open-Source Web Automation with LLMs" href="#open-source-web-automation-with-llms"></a></p>

<p dir="auto"><a href="https://github.com/gregpr07/browser-use/stargazers"><img src="https://camo.githubusercontent.com/0f19403bd69bc9dd8ab794fa4cce28d24300bfd37881d117c0b68e001bc86418/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f67726567707230372f62726f777365722d7573653f7374796c653d736f6369616c" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/gregpr07/browser-use?style=social"></a>
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a>
<a href="https://www.python.org/downloads/" rel="nofollow"><img src="https://camo.githubusercontent.com/1e5852941fcfe768cdba62e1ef6b1db0d9c87c4f9017432c39ad06853f6d4df9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e31312b2d626c75652e737667" alt="Python 3.11+" data-canonical-src="https://img.shields.io/badge/python-3.11+-blue.svg"></a></p>
<p dir="auto"><em>Let LLMs interact with websites naturally</em></p>
<p dir="auto"><a href="#-key-features">Key Features</a> •
<a href="#-live-demos">Live Demos</a> •
<a href="#-quick-start">Quick Start</a> •
<a href="#-examples">Examples</a> •
<a href="#-supported-models">Models</a></p>
</div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🎥 Live Demos</h2><a id="user-content--live-demos" aria-label="Permalink: 🎥 Live Demos" href="#-live-demos"></a></p>
<p dir="auto">Watch Browser-Use tackle real-world tasks:</p>
<div dir="auto">
  <div dir="auto">
    <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/gregpr07/browser-use/blob/main/static/kayak.gif"><img src="https://github.com/gregpr07/browser-use/raw/main/static/kayak.gif" alt="Kayak flight search demo" data-animated-image=""></a></p><p dir="auto"><i>Prompt: Go to kayak.com and find a one-way flight from Zürich to San Francisco on 12 January 2025.</i></p>
  </div>
  <div dir="auto">
    <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/gregpr07/browser-use/blob/main/static/photos.gif"><img src="https://github.com/gregpr07/browser-use/raw/main/static/photos.gif" alt="Photos search demo" data-animated-image=""></a></p><p dir="auto"><i>Prompt: Opening new tabs and searching for images for these people: Albert Einstein, Oprah Winfrey, Steve Jobs. Then ask me for further instructions.</i></p>
  </div>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Key Features</h2><a id="user-content--key-features" aria-label="Permalink: 🚀 Key Features" href="#-key-features"></a></p>
<ul dir="auto">
<li>🤖 <strong>Universal LLM Support</strong> - Works with any Language Model</li>
<li>🎯 <strong>Smart Element Detection</strong> - Automatically finds interactive elements</li>
<li>📑 <strong>Multi-Tab Management</strong> - Seamless handling of browser tabs</li>
<li>🔍 <strong>XPath Extraction</strong> - No more manual DevTools inspection</li>
<li>👁️ <strong>Vision Model Support</strong> - Process visual page information</li>
<li>🛠️ <strong>Customizable Actions</strong> - Add your own browser interactions</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">💻 Quick Start</h2><a id="user-content--quick-start" aria-label="Permalink: 💻 Quick Start" href="#-quick-start"></a></p>
<p dir="auto">Create a virtual environment and install the dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# I recommend using uv
pip install -r requirements.txt"><pre><span><span>#</span> I recommend using uv</span>
pip install -r requirements.txt</pre></div>
<p dir="auto">Add your API keys to the <code>.env</code> file.</p>

<p dir="auto">You can use any LLM model that is supported by LangChain by adding correct environment variables. Head over to the <a href="https://python.langchain.com/docs/integrations/chat/" rel="nofollow">langchain models</a> page to see all available models.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📝 Examples</h2><a id="user-content--examples" aria-label="Permalink: 📝 Examples" href="#-examples"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from src import Agent
from langchain_openai import ChatOpenAI

# Initialize browser agent
agent = Agent(
	task='Find cheapest flight from London to Kyrgyzstan and return the url.',
	llm=ChatOpenAI(model='gpt-4o'),
)

# Let it work its magic
await agent.run()"><pre><span>from</span> <span>src</span> <span>import</span> <span>Agent</span>
<span>from</span> <span>langchain_openai</span> <span>import</span> <span>ChatOpenAI</span>

<span># Initialize browser agent</span>
<span>agent</span> <span>=</span> <span>Agent</span>(
	<span>task</span><span>=</span><span>'Find cheapest flight from London to Kyrgyzstan and return the url.'</span>,
	<span>llm</span><span>=</span><span>ChatOpenAI</span>(<span>model</span><span>=</span><span>'gpt-4o'</span>),
)

<span># Let it work its magic</span>
<span>await</span> <span>agent</span>.<span>run</span>()</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Chain of Agents</h3><a id="user-content-chain-of-agents" aria-label="Permalink: Chain of Agents" href="#chain-of-agents"></a></p>
<p dir="auto">You can persist the browser across multiple agents and chain them together.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from langchain_anthropic import ChatAnthropic
from src import Agent, Controller

# Persist the browser state across agents
controller = Controller()

# Initialize browser agent
agent1 = Agent(
	task='Open 5 VCs websites in the New York area.',
	llm=ChatAnthropic(model_name='claude-3-sonnet', timeout=25, stop=None, temperature=0.3),
	controller=controller,
)
agent2 = Agent(
	task='Give me the names of the founders of the companies in all tabs.',
	llm=ChatAnthropic(model_name='claude-3-sonnet', timeout=25, stop=None, temperature=0.3),
	controller=controller,
)

# Let it work its magic
await agent1.run()
founders, history = await agent2.run()

print(founders)"><pre><span>from</span> <span>langchain_anthropic</span> <span>import</span> <span>ChatAnthropic</span>
<span>from</span> <span>src</span> <span>import</span> <span>Agent</span>, <span>Controller</span>

<span># Persist the browser state across agents</span>
<span>controller</span> <span>=</span> <span>Controller</span>()

<span># Initialize browser agent</span>
<span>agent1</span> <span>=</span> <span>Agent</span>(
	<span>task</span><span>=</span><span>'Open 5 VCs websites in the New York area.'</span>,
	<span>llm</span><span>=</span><span>ChatAnthropic</span>(<span>model_name</span><span>=</span><span>'claude-3-sonnet'</span>, <span>timeout</span><span>=</span><span>25</span>, <span>stop</span><span>=</span><span>None</span>, <span>temperature</span><span>=</span><span>0.3</span>),
	<span>controller</span><span>=</span><span>controller</span>,
)
<span>agent2</span> <span>=</span> <span>Agent</span>(
	<span>task</span><span>=</span><span>'Give me the names of the founders of the companies in all tabs.'</span>,
	<span>llm</span><span>=</span><span>ChatAnthropic</span>(<span>model_name</span><span>=</span><span>'claude-3-sonnet'</span>, <span>timeout</span><span>=</span><span>25</span>, <span>stop</span><span>=</span><span>None</span>, <span>temperature</span><span>=</span><span>0.3</span>),
	<span>controller</span><span>=</span><span>controller</span>,
)

<span># Let it work its magic</span>
<span>await</span> <span>agent1</span>.<span>run</span>()
<span>founders</span>, <span>history</span> <span>=</span> <span>await</span> <span>agent2</span>.<span>run</span>()

<span>print</span>(<span>founders</span>)</pre></div>
<p dir="auto">You can use the <code>history</code> to run the agents again deterministically.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Simple Run</h2><a id="user-content-simple-run" aria-label="Permalink: Simple Run" href="#simple-run"></a></p>
<p dir="auto">You can run any of the examples using the command line interface:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python examples/try.py &quot;Your query here&quot; --provider [openai|anthropic]"><pre>python examples/try.py <span><span>"</span>Your query here<span>"</span></span> --provider [openai<span>|</span>anthropic]</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Anthropic</h3><a id="user-content-anthropic" aria-label="Permalink: Anthropic" href="#anthropic"></a></p>
<p dir="auto">You need to add <code>ANTHROPIC_API_KEY</code> to your environment variables. Example usage:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python examples/try.py &quot;Find cheapest flight from London to Paris&quot; --provider anthropic"><pre>python examples/try.py <span><span>"</span>Find cheapest flight from London to Paris<span>"</span></span> --provider anthropic</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">OpenAI</h3><a id="user-content-openai" aria-label="Permalink: OpenAI" href="#openai"></a></p>
<p dir="auto">You need to add <code>OPENAI_API_KEY</code> to your environment variables. Example usage:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python examples/try.py &quot;Search for top AI companies&quot; --provider openai"><pre>python examples/try.py <span><span>"</span>Search for top AI companies<span>"</span></span> --provider openai</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🤖 Supported Models</h2><a id="user-content--supported-models" aria-label="Permalink: 🤖 Supported Models" href="#-supported-models"></a></p>
<p dir="auto">All LangChain chat models are supported.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tested</h3><a id="user-content-tested" aria-label="Permalink: Tested" href="#tested"></a></p>
<ul dir="auto">
<li>GPT-4o</li>
<li>GPT-4o Mini</li>
<li>Claude 3.5 Sonnet</li>
<li>LLama 3.1 405B</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🤝 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 🤝 Contributing" href="#-contributing"></a></p>
<p dir="auto">Contributions are welcome! Also feel free to open issues for any bugs or feature requests.</p>
<hr>
<p><b>Star ⭐ this repo if you find it useful!</b><br>
  Made with ❤️ by the Browser-Use team
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Future Roadmap</h2><a id="user-content-future-roadmap" aria-label="Permalink: Future Roadmap" href="#future-roadmap"></a></p>
<ul>
<li> Save agent actions and execute them deterministically (for QA testing etc)</li>
<li> Pydantic forced output</li>
<li> Third party SERP API for faster Google Search results</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hacking 700M Electronic Arts Accounts (699 pts)]]></title>
            <link>https://battleda.sh/blog/ea-account-takeover</link>
            <guid>42052143</guid>
            <pubDate>Tue, 05 Nov 2024 15:18:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://battleda.sh/blog/ea-account-takeover">https://battleda.sh/blog/ea-account-takeover</a>, See on <a href="https://news.ycombinator.com/item?id=42052143">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[New documentary reveals that 21,000 laborers have died working Saudi Vision 2030 (242 pts)]]></title>
            <link>https://www.archpaper.com/2024/10/documentary-reveals-21000-workers-killed-saudi-vision-2030-neom/</link>
            <guid>42052105</guid>
            <pubDate>Tue, 05 Nov 2024 15:14:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.archpaper.com/2024/10/documentary-reveals-21000-workers-killed-saudi-vision-2030-neom/">https://www.archpaper.com/2024/10/documentary-reveals-21000-workers-killed-saudi-vision-2030-neom/</a>, See on <a href="https://news.ycombinator.com/item?id=42052105">Hacker News</a></p>
Couldn't get https://www.archpaper.com/2024/10/documentary-reveals-21000-workers-killed-saudi-vision-2030-neom/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Hacker Fab (360 pts)]]></title>
            <link>https://docs.hackerfab.org/hacker-fab-space</link>
            <guid>42051968</guid>
            <pubDate>Tue, 05 Nov 2024 14:59:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.hackerfab.org/hacker-fab-space">https://docs.hackerfab.org/hacker-fab-space</a>, See on <a href="https://news.ycombinator.com/item?id=42051968">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><!--$?--><template id="B:0"></template><!--/$--><p>Last updated <time data-visual-test="transparent" datetime="2024-11-05T18:11:11.169Z" title="11/5/2024, 6:11:11 PM">0 minutes ago</time></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Netflix Europe offices raided in tax fraud probe (319 pts)]]></title>
            <link>https://www.bbc.co.uk/news/articles/cwy1vze09wwo</link>
            <guid>42051643</guid>
            <pubDate>Tue, 05 Nov 2024 14:20:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.co.uk/news/articles/cwy1vze09wwo">https://www.bbc.co.uk/news/articles/cwy1vze09wwo</a>, See on <a href="https://news.ycombinator.com/item?id=42051643">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="text-block"><p><b>Offices of streaming giant Netflix in Paris and Amsterdam have been raided by the French and Dutch authorities as part of an investigation into tax fraud, French judicial sources say.</b></p><p>Officials from the two countries have been co-operating on the case since the investigation was opened in November 2022.</p><p>Netflix has not as yet made any specific comment on the raids, but insists it complies with tax laws wherever it operates.</p><p>The Amsterdam office is the headquarters of the company's operations in Europe, the Middle East and Africa.</p></div><div data-component="text-block"><p>The French investigation is being carried out by the National Financial Prosecutor's office (PNF), a special unit used for investigations into high-profile white-collar crime.</p><p>It relates to suspicions of "covering up serious tax fraud and off-the-books work", according to the PNF.</p><p>The company is also under investigation for tax filings for 2019, 2020 and 2021.</p><p>The French sources said authorities in the Netherlands were conducting simultaneous searches, and that co-operation between the two countries had been going on for "many months".</p><p>Last year, French media outlet La Lettre reported that until 2021, Netflix in France minimised its tax payments by declaring its turnover generated in France to the Netherlands.</p><p>After it abandoned this arrangement, La Lettre said, its annual declared turnover in France  jumped from €47.1m ($51.3m; £39.6m) in 2020 to €1.2bn in 2021.</p><p>However, the outlet says investigators are trying to determine whether Netflix continued to attempt to minimise its profits after 2021.</p><p>Netflix arrived in France more than 10 years ago, opening its Paris office in 2020. It has some 10 million subscribers in the country, according to AFP news agency.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Failure Analysis of the Arecibo 305 Meter Telescope Collapse (206 pts)]]></title>
            <link>https://nap.nationalacademies.org/read/26982/chapter/1</link>
            <guid>42051368</guid>
            <pubDate>Tue, 05 Nov 2024 13:37:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nap.nationalacademies.org/read/26982/chapter/1">https://nap.nationalacademies.org/read/26982/chapter/1</a>, See on <a href="https://news.ycombinator.com/item?id=42051368">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="openbook-ocr">
          <p>Below is the uncorrected machine-read text of this chapter, intended to provide our own search engines and external engines with highly rich, chapter-representative searchable text of each book. Because it is UNCORRECTED material, please consider the following text as a useful but insufficient proxy for the authoritative book pages.</p>
                      <p>Failure Analysis of the
Arecibo Observatory 305-Meter
Telescope Collapse




Committee on Analysis of Causes of Failure
and Collapse of the 305-Meter Telescope at
the Arecibo Observatory

Board on Infrastructure and the Constructed
Environment

Division on Engineering and Physical Sciences




                                                 Consensus Study Report
            PREPUBLICATION COPYâSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>NATIONAL ACADEMIES PRESS 500 Fifth Street, NW Washington, DC 20001

This activity was supported by Grant CMMI- 2135084 from the National Science Foundation to the National
Academy of Sciences. Any opinions, findings, conclusions, or recommendations expressed in this publication do
not necessarily reflect the views of any organization or agency that provided support for the project.

International Standard Book Number-13: 978-0-309-XXXXX-X
International Standard Book Number-10: 0-309-XXXXX-X
Digital Object Identifier: https://doi.org/10.17226/26982

Cover: Photo courtesy of the Arecibo Observatory, a facility of the National Science Foundation.

This publication is available from the National Academies Press, 500 Fifth Street, NW, Keck 360, Washington,
DC 20001; (800) 624-6242 or (202) 334-3313; http://www.nap.edu.

Copyright 2024 by the National Academy of Sciences. National Academies of Sciences, Engineering, and
Medicine and National Academies Press and the graphical logos for each are all trademarks of the National
Academy of Sciences. All rights reserved.

Printed in the United States of America.

Suggested citation: National Academies of Sciences, Engineering, and Medicine. 2024. Failure Analysis of the
Arecibo Observatory 305-Meter Telescope Collapse. Washington, DC: The National Academies Press. https://
doi.org/10.17226/26982.




      PREPUBLICATION COPYâSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>The National Academy of Sciences was established in 1863 by an Act of Congress, signed by President
Lincoln, as a private, nongovernmental institution to advise the nation on issues related to science and
technology. Members are elected by their peers for outstanding contributions to research. Dr. Marcia McNutt is
president.

The National Academy of Engineering was established in 1964 under the charter of the National Academy
of Sciences to bring the practices of engineering to advising the nation. Members are elected by their peers for
extraordinary contributions to engineering. Dr. John L. Anderson is president.

The National Academy of Medicine (formerly the Institute of Medicine) was established in 1970 under the
charter of the National Academy of Sciences to advise the nation on medical and health issues. Members are
elected by their peers for distinguished contributions to medicine and health. Dr. Victor J. Dzau is president.

The three Academies work together as the National Academies of Sciences, Engineering, and Medicine
to provide independent, objective analysis and advice to the nation and conduct other activities to solve complex
problems and inform public policy decisions. The National Academies also encourage education and research,
recognize outstanding contributions to knowledge, and increase public understanding in matters of science,
engineering, and medicine.

Learn more about the National Academies of Sciences, Engineering, and Medicine at
www.nationalacademies.org.




      PREPUBLICATION COPYâSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>Consensus Study Reports published by the National Academies of Sciences, Engineering, and
Medicine document the evidence-based consensus on the studyâs statement of task by an authoring committee of
experts. Reports typically include findings, conclusions, and recommendations based on information gathered by
the committee and the committeeâs deliberations. Each report has been subjected to a rigorous and independent
peer-review process and it represents the position of the National Academies on the statement of task.

Proceedings published by the National Academies of Sciences, Engineering, and Medicine chronicle the
presentations and discussions at a workshop, symposium, or other event convened by the National Academies.
The statements and opinions contained in proceedings are those of the participants and are not endorsed by
other participants, the planning committee, or the National Academies.

Rapid Expert Consultations published by the National Academies of Sciences, Engineering, and Medicine are
authored by subject-matter experts on narrowly focused topics that can be supported by a body of evidence. The
discussions contained in rapid expert consultations are considered those of the authors and do not contain policy
recommendations. Rapid expert consultations are reviewed by the institution before release.

For information about other products and activities of the National Academies, please visit
www.nationalacademies.org/about/whatwedo.




      PREPUBLICATION COPYâSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>               COMMITTEE ON ANALYSIS OF CAUSES OF FAILURE AND COLLAPSE OF
                  THE 305-METER TELESCOPE AT THE ARECIBO OBSERVATORY


ROGER L. McCARTHY (NAE), McCarthy Engineering, Chair
RAMÃN L. CARRASQUILLO,1 Carrasquillo Associates
DIANNE CHONG (NAE), Boeing Engineering, Operations &amp; Technology (retired)
ROBERT B. GILBERT (NAE), The University of Texas at Austin
W. ALLEN MARR, JR. (NAE), Geocomp, Inc.
JOHN R. SCULLY, University of Virginia
SAWTEEN SEE, See Robertson Structural Engineers
HABIB TABATABAI, University of WisconsinâMilwaukee


Study Staff
CAMERON OSKVIG, Board Director, Study Director
JAYDA WADE, Research Associate (until July 31, 2023)
JOSEPH PALMER, SR., Program Assistant
RADAKA LIGHTFOOT, Financial Business Partner (until March 20, 2023)
DONAVAN THOMAS, Financial Business Partner (from March 20, 2023)




 1 Deceased   on February 2, 2024.

                                                v



      PREPUBLICATION COPYâSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>           BOARD ON INFRASTRUCTURE AND THE CONSTRUCTED ENVIRONMENT


JESUS M. DE LA GARZA, Clemson University, Chair
BURCU AKINCI, Carnegie Mellon University
STEPHEN AYERS, Ayers Group
BURCIN BECERIK-GERBER, University of Southern California
LEAH BROOKS, The George Washington University
MIKHAIL V. CHESTER, Arizona State University
JAMES (JACK) DEMPSEY, Asset Management Partnership, LLC
LEONARDO DUENAS-OSORIO, Rice University
DEVIN K. HARRIS, University of Virginia
DAVID J. HAUN, Haun Consulting, Inc.
CHRISTOPHER J. MOSSEY, Fermi National Accelerator Laboratory
ANDREW PERSILY, National Institute of Standards and Technology
ROBERT B. RAINES, Atkins Nuclear Secured
JAMES RISPOLI, North Carolina State University
DOROTHY ROBYN, Boston University
SHOSHANNA D. SAXE, University of Toronto


Staff
CAMERON OSKVIG, Board Director
JAMES MYSKA, Senior Program Officer
BRITTANY SEGUNDO, Program Officer
JOSEPH PALMER, SR., Senior Program Assistant
DONAVAN THOMAS, Finance Business Partner




                                               vi



        PREPUBLICATION COPYâSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>                                                   Dedication




     This report is dedicated to committee member Dr. RamÃ³n L. Carrasquillo, who unexpectedly passed away
before this reportâs release. His pragmatic and insightful contributions strengthened the report. In addition to his
extensive engineering and materials science expertise, his deep connection to Puerto Rico helped the commit-
tee develop a nuanced understanding of the community and culture surrounding the Arecibo Observatory. He is
remembered by the committee as a thoughtful, warm, and generous colleague.




NOTE: Image courtesy of Carrasquillo Associates.


                                                        vii



       PREPUBLICATION COPYâSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>PREPUBLICATION COPYâSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>                                               Reviewers




     This Consensus Study Report was reviewed in draft form by individuals chosen for their diverse perspec-
tives and technical expertise. The purpose of this independent review is to provide candid and critical comments
that will assist the National Academies of Sciences, Engineering, and Medicine in making each published report
as sound as possible and to ensure that it meets the institutional standards for quality, objectivity, evidence, and
responsiveness to the study charge. The review comments and draft manuscript remain confidential to protect the
integrity of the deliberative process.
     We thank the following individuals for their review of this report:

    DONALD CAMPBELL, Cornell University
    GREGORY G. DEIERLEIN (NAE), Stanford University
    LENNARD FISK (NAS), University of Michigan
    DAVID GOODYEAR (NAE), Independent Consultant
    MARTHA HAYNES (NAS), Cornell University
    LT. COL. (RET.) CLARENCE (BART) KEMPER, Kemper Engineering Services, LLC
    MATTHYS LEVY (NAE), Thornton Tomasetti
    MOHAMMAD MODARRES, University of Maryland
    JANINE PARDEE, Independent Consultant
    RANDALL POSTON (NAE), Pivot Engineers

     Although the reviewers listed above provided many constructive comments and suggestions, they were not
asked to endorse the conclusions or recommendations of this report nor did they see the final draft before its
release. The review of this report was overseen by WILLIAM F. BAKER, Skidmore Owings and Merrill, LLP,
and STEVE BATTEL (NAE), Battel Engineering. They were responsible for making certain that an independent
examination of this report was carried out in accordance with the standards of the National Academies and that all
review comments were carefully considered. Responsibility for the final content rests entirely with the authoring
committee and the National Academies.




                                                         ix



      PREPUBLICATION COPYâSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>PREPUBLICATION COPYâSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>                                            Contents




PREFACE                                                                      xiii

SUMMARY                                                                        1

1   INTRODUCTION                                                               7
    History of the Arecibo Telescope, 7
    Arecibo Telescope Cable System, 12
    Statement of Task, 16

2   THE COLLAPSE: WHAT HAPPENED                                               18
    Arecibo Telescope Failure Sequence, 18
    Hurricane Maria Hits Arecibo Telescope, 18
    Post-Maria Arecibo Telescope Inspections, 22
    The Hurricane Maria Aftermath, 26
    Bureaucratic Delays in Funding Arecibo Telescope Hurricane Repairs, 28
    Sequence of Cable Failure Events, 30

3   ANALYSIS                                                                  35
    Cable Socket Zinc Creep Failure, 35
    Cable End Sockets, 48
    Wire Breaks, 49
    Earthquake, 55
    Wind Speed Consideration in the Arecibo Telescopeâs Design, 56
    Governing Cable Design Standards, 56
    Arecibo Telescope Cable Load, 58
    Risk Considerations, 61
    Structural Robustness, 63
    Monitoring, 65



                                                    xi



     PREPUBLICATION COPYâSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>xii                                                                            CONTENTS



4     ARECIBO TELESCOPEâS MANAGEMENT AND OVERSIGHT                                   70

5     OTHER LESSONS LEARNED                                                          74
      State of Knowledge, 74
      Continued Research, 75

BIBLIOGRAPHY                                                                         76

APPENDIXES

A     Committee Member Biographical Information                                      81
B     Information-Gathering Activities                                               84
C     Arecibo Telescope Cable Failure Mechanisms Considered by the Committee         86
D     Arecibo Telescope Design Issues Considered by the Committee                    94
E     Acronyms and Abbreviations                                                     96




       PREPUBLICATION COPYâSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>                                                  Preface




     It has been my privilege to chair this committee of distinguished subject-matter experts in its investigation
and final probable cause determination of one of the most publicized and baffling failures of the modern era. It
became clear shortly after the Arecibo Telescopeâs collapse that the zinc used to anchor the steel supporting cable
wires into their sockets had allowed the failed cables to slip out of their sockets, known as spelter sockets. The
sockets slowly lost their grip on a critical number of the cable wires via slow zinc âcreep,â a process where the
zinc deformed slowly at a load below half the socketâs nominal strength. Although the committee agrees with
the conclusions from other forensic reports regarding zinc creep at the connection being the failure mechanism,
the baffling question was, âWhy was there excessive zinc creep at such loading?â Such a failure had never been
reported previously in over a century of widespread zinc spelter socket successful use.
     Fortunately, the committee had the benefit of the detailed analysis and well-documented reports from NASA;
Wiss, Janney, Elstner Associates, Inc.; and Thornton Tomasetti, Inc., without which it could not have completed its
task. Building on their work, the committee presents a clear and plausible explanation of why the telescopeâs sockets
failed when no such sockets have ever been reported to have failed before. Unfortunately, there was not enough
data available to prove our explanation. It is simply the most plausible hypothesis based on the data we do have.
     Without the depth and breadth of expertise on the committee, its task would remain uncompleted. I think I
speak for everyone on the committee when I say that none of us could have done this alone. I want to thank my
colleagues for their unwavering dedication to the task. Their professionalism and competence made my job an
enjoyable one.

                                                                                     Roger L. McCarthy, Chair
                                                        Committee on Analysis of Causes of Failure and Collapse
                                                          of the 305-Meter Telescope at the Arecibo Observatory




                                                        xiii



      PREPUBLICATION COPYâSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>PREPUBLICATION COPYâSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[State of Python 3.13 performance: Free-threading (135 pts)]]></title>
            <link>https://codspeed.io/blog/state-of-python-3-13-performance-free-threading</link>
            <guid>42051197</guid>
            <pubDate>Tue, 05 Nov 2024 13:06:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codspeed.io/blog/state-of-python-3-13-performance-free-threading">https://codspeed.io/blog/state-of-python-3-13-performance-free-threading</a>, See on <a href="https://news.ycombinator.com/item?id=42051197">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>CPython 3.13 was released two weeks ago and this release is the most
performance-oriented in some time. After a quick read of the release notes, a
few things stand out for the impact they can have on the performance:</p>
<ul>
<li>CPython can now run in <strong>free-threaded mode</strong>, with the global interpreter
lock (GIL) disabled</li>
<li>a brand new <strong>just-in-time</strong> (JIT) compiler has been added</li>
<li>CPython now bundles the <code>mimalloc</code> allocator out of the box</li>
</ul>
<p>Let's focus on the free-threaded mode in this article to see how to leverage
this change and how it can impact the performance of Python applications by
measuring performance with CodSpeed.</p>
<blockquote>
<p>⏭️ The JIT and <code>mimalloc</code> performance will be covered in the next post. Stay
tuned!</p>
</blockquote>
<h2 id="free-threaded-cpython"><a href="#free-threaded-cpython">Free-threaded CPython<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h2>
<p>Free-threading is an experimental feature in Python 3.13 that allows CPython to
run without the Global Interpreter Lock (GIL). The GIL is a mutex preventing
multiple threads from executing Python bytecode simultaneously. This design
choice has simplified CPython's memory management and made the C API easier to
work with. However, it has also been one of the most significant barriers to
utilizing modern multi-core processors effectively.</p>
<h4 id="the-multiprocessing-workaround"><a href="#the-multiprocessing-workaround">The Multiprocessing Workaround<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h4>
<p>The traditional solution has been to use the <code>multiprocessing</code> module, which
spawns separate Python processes instead of threads and while this approach
works, it comes with significant limitations:</p>
<ol>
<li>
<p><strong>Memory Overhead</strong>: Each process requires its own Python interpreter
instance and memory space. For data-intensive applications, this can quickly
become a bottleneck.</p>
</li>
<li>
<p><strong>Communication Cost</strong>: Processes can't share memory directly. Data must be
serialized and deserialized when passed between processes, which adds
overhead and complexity.</p>
</li>
<li>
<p><strong>Startup Time</strong>: Creating new processes is significantly slower than
creating threads, making it impractical for tasks that require frequent
spawning of workers.</p>
</li>
</ol>

<p>To illustrate these limitations, let's consider implementing PageRank, the
algorithm that powered Google's early search engine. PageRank is an ideal
example because it:</p>
<ol>
<li>Is compute-intensive (matrix operations)</li>
<li>Works with large datasets (the web graph)</li>
<li>Can benefit significantly from parallelization</li>
</ol>
<p>A naive multithreaded implementation in Python 3.12 or earlier would be
bottlenecked by the GIL during matrix operations, while a multiprocessing
version would struggle with:</p>
<ul>
<li>The memory overhead of copying the graph to each process</li>
<li>The cost of transferring partial results between processes</li>
<li>The complexity of managing shared state</li>
</ul>
<p>Before we proceed, it's important to clarify that our focus here isn't on the
specifics of the PageRank algorithm itself but rather on the parallelization so
we won't go into the details of the algorithm itself here.</p>
<p>Let's look at how we would implement this with those different concurrency
models.</p>
<h3 id="textbook-implementation-single-threaded-"><a href="#textbook-implementation-single-threaded-">Textbook Implementation (Single-Threaded)<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h3>
<pre tabindex="0"><code><span><span>def</span><span> pagerank_single</span><span>(</span><span>matrix</span><span>: np.ndarray, </span><span>num_iterations</span><span>: </span><span>int</span><span>) -&gt; np.ndarray:</span></span>
<span><span>    """Single-threaded PageRank implementation"""</span></span>
<span><span>    size </span><span>=</span><span> matrix.shape[</span><span>0</span><span>]</span></span>
<span><span>    # Initialize scores</span></span>
<span><span>    scores </span><span>=</span><span> np.</span><span>ones</span><span>(size) </span><span>/</span><span> size</span></span>
<span></span>
<span><span>    for</span><span> _ </span><span>in</span><span> range</span><span>(num_iterations):</span></span>
<span><span>        new_scores </span><span>=</span><span> np.</span><span>zeros</span><span>(size)</span></span>
<span><span>        for</span><span> i </span><span>in</span><span> range</span><span>(size):</span></span>
<span><span>            # Get nodes that point to current node</span></span>
<span><span>            incoming </span><span>=</span><span> np.</span><span>where</span><span>(matrix[:, i])[</span><span>0</span><span>]</span></span>
<span><span>            for</span><span> j </span><span>in</span><span> incoming:</span></span>
<span><span>                # Add score contribution from incoming node</span></span>
<span><span>                new_scores[i] </span><span>+=</span><span> scores[j] </span><span>/</span><span> np.</span><span>sum</span><span>(matrix[j]) </span></span>
<span><span>        # Apply damping factor</span></span>
<span><span>        scores </span><span>=</span><span> (</span><span>1</span><span> -</span><span> DAMPING</span><span>) </span><span>/</span><span> size </span><span>+</span><span> DAMPING</span><span> *</span><span> new_scores </span></span>
<span></span>
<span><span>    return</span><span> scores</span></span></code></pre>
<p>The highlighted lines show the two most computationally intensive parts of the
algorithm. The first computes the score contribution from incoming nodes, while
the second applies the damping factor, incorporating the new scores into the
final result.</p>
<p>Parallelizing the first part will be the most beneficial and easy to implement
since we can divide the range and thus efficiently use multiple threads to
compute the <code>new_scores</code> array.</p>
<h3 id="multithreaded-implementation"><a href="#multithreaded-implementation">Multithreaded Implementation<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h3>
<p>For the multithreaded implementation, we'll start by dividing the matrix into
multiple chunks:</p>
<pre tabindex="0"><code><span><span>chunk_size </span><span>=</span><span> size </span><span>//</span><span> num_threads</span></span>
<span><span>chunks </span><span>=</span><span> [(i, </span><span>min</span><span>(i </span><span>+</span><span> chunk_size, size)) </span><span>for</span><span> i </span><span>in</span><span> range</span><span>(</span><span>0</span><span>, size, chunk_size)]</span></span></code></pre>
<p>Each thread will then work on a different chunk of the matrix, updating the new
scores:</p>
<pre tabindex="0"><code><span><span>def</span><span> _thread_worker</span><span>(</span></span>
<span><span>    matrix</span><span>: np.ndarray,</span></span>
<span><span>    scores</span><span>: np.ndarray,</span></span>
<span><span>    new_scores</span><span>: np.ndarray,</span></span>
<span><span>    start_idx</span><span>: </span><span>int</span><span>,</span></span>
<span><span>    end_idx</span><span>: </span><span>int</span><span>,</span></span>
<span><span>    lock</span><span>: threading.Lock,</span></span>
<span><span>):</span></span>
<span><span>    size </span><span>=</span><span> matrix.shape[</span><span>0</span><span>]</span></span>
<span><span>    local_scores </span><span>=</span><span> np.</span><span>zeros</span><span>(size)</span></span>
<span></span>
<span><span>    for</span><span> i </span><span>in</span><span> range</span><span>(start_idx, end_idx):</span></span>
<span><span>        incoming </span><span>=</span><span> np.</span><span>where</span><span>(matrix[:, i])[</span><span>0</span><span>]</span></span>
<span><span>        for</span><span> j </span><span>in</span><span> incoming:</span></span>
<span><span>            local_scores[i] </span><span>+=</span><span> scores[j] </span><span>/</span><span> np.</span><span>sum</span><span>(matrix[j])</span></span>
<span></span>
<span><span>    with</span><span> lock: </span></span>
<span><span>        new_scores </span><span>+=</span><span> local_scores </span></span></code></pre>
<p>It's important to note that updating the <code>new_scores</code> array is done behind a
lock to prevent race conditions. This could potentially become a bottleneck if
the lock is held for too long, but in practice, parallelizing the first part of
the algorithm should provide a significant speedup already.</p>
<p>Finally, we'll feed the chunks to each of the threads:</p>
<pre tabindex="0"><code><span><span>new_scores </span><span>=</span><span> np.</span><span>zeros</span><span>(size)</span></span>
<span><span>lock </span><span>=</span><span> threading.</span><span>Lock</span><span>() </span></span>
<span><span>with</span><span> concurrent.futures.</span><span>ThreadPoolExecutor</span><span>(</span><span>max_workers</span><span>=</span><span>num_threads) </span><span>as</span><span> executor: </span></span>
<span><span>    # Process chunks in parallel</span></span>
<span><span>    futures </span><span>=</span><span> executor.</span><span>map</span><span>( </span></span>
<span><span>        lambda</span><span> args</span><span>: </span><span>_thread_worker</span><span>(*args), </span><span># starmap isn't available on ThreadPoolExecutor</span></span>
<span><span>        [ </span></span>
<span><span>            (matrix, scores, new_scores, start_idx, end_idx, lock) </span></span>
<span><span>            for</span><span> start_idx, end_idx </span><span>in</span><span> chunks </span></span>
<span><span>        ], </span></span>
<span><span>    ) </span></span>
<span><span>new_scores </span><span>=</span><span> (</span><span>1</span><span> -</span><span> DAMPING</span><span>) </span><span>/</span><span> size </span><span>+</span><span> DAMPING</span><span> *</span><span> new_scores</span></span>
<span><span>scores </span><span>=</span><span> new_scores</span></span></code></pre>
<h3 id="multiprocessing-implementation"><a href="#multiprocessing-implementation">Multiprocessing Implementation<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h3>
<p>Essentially, the <code>multiprocessing</code> implementation is extremely similar to the
<code>threading</code> one, let's focus on the differences:</p>
<ul>
<li>
<p>Each worker, will now return the <code>local_scores</code> array instead of updating a
shared <code>new_scores</code> array since processes can't share memory directly. The
local scores will then be aggregated in the main process:</p>
<pre tabindex="0"><code><span><span># Combine results</span></span>
<span><span>new_scores </span><span>=</span><span> sum</span><span>(chunk_results)</span></span></code></pre>
<p>While this should be faster than the threading version, it still incurs the
overhead of the inter-process communication which can become very significant
for large datasets.</p>
</li>
<li>
<p>Instead of using a <code>ThreadPoolExecutor</code>, it will use a <code>multiprocessing.Pool</code>.
The APIs are very similar, but <code>multiprocessing.Pool</code> will spawn a pool of
processes instead of threads:</p>
<pre tabindex="0"><code><span><span>with</span><span> multiprocessing.</span><span>Pool</span><span>(</span><span>processes</span><span>=</span><span>num_processes) </span><span>as</span><span> pool: </span></span>
<span><span>    # Process chunks in parallel</span></span>
<span><span>    chunk_results </span><span>=</span><span> pool.</span><span>starmap</span><span>(_process_chunk, chunks) </span></span>
<span><span>    # Combine results</span></span>
<span><span>    new_scores </span><span>=</span><span> sum</span><span>(chunk_results)</span></span>
<span><span>    new_scores </span><span>=</span><span> (</span><span>1</span><span> -</span><span> DAMPING</span><span>) </span><span>/</span><span> size </span><span>+</span><span> DAMPING</span><span> *</span><span> new_scores</span></span>
<span><span>    scores </span><span>=</span><span> new_scores</span></span></code></pre>
</li>
</ul>
<h2 id="measuring-performance"><a href="#measuring-performance">Measuring Performance<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h2>
<p>In order to measure the actual performance changes, let's build a performance
test. First things first, we need to generate some testing data:</p>
<pre tabindex="0"><code><span><span>def</span><span> create_test_graph</span><span>(</span><span>size</span><span>: </span><span>int</span><span>) -&gt; np.ndarray:</span></span>
<span><span>    # Fixed seed</span></span>
<span><span>    np.random.</span><span>seed</span><span>(</span><span>0</span><span>) </span></span>
<span><span>    # Create random adjacency matrix with ~5 outgoing edges per node</span></span>
<span><span>    matrix </span><span>=</span><span> np.random.</span><span>choice</span><span>([</span><span>0</span><span>, </span><span>1</span><span>], </span><span>size</span><span>=</span><span>(size, size), </span><span>p</span><span>=</span><span>[</span><span>1</span><span> -</span><span> 5</span><span>/</span><span>size, </span><span>5</span><span>/</span><span>size])</span></span>
<span><span>    # Find nodes with no outgoing edges</span></span>
<span><span>    zero_outdegree </span><span>=</span><span> ~</span><span>matrix.</span><span>any</span><span>(</span><span>axis</span><span>=</span><span>1</span><span>)</span></span>
<span><span>    zero_indices </span><span>=</span><span> np.</span><span>where</span><span>(zero_outdegree)[</span><span>0</span><span>]</span></span>
<span><span>    # For each node with no outgoing edges, add a random edge</span></span>
<span><span>    if</span><span> len</span><span>(zero_indices) </span><span>&gt;</span><span> 0</span><span>:</span></span>
<span><span>        random_targets </span><span>=</span><span> np.random.</span><span>randint</span><span>(</span><span>0</span><span>, size, </span><span>size</span><span>=len</span><span>(zero_indices))</span></span>
<span><span>        matrix[zero_indices, random_targets] </span><span>=</span><span> 1</span></span>
<span></span>
<span><span>    return</span><span> matrix</span></span></code></pre>
<p>As highlighted, we're using a fixed seed to ensure reproducibility from one run
to the next. This is important when comparing the performance of different
implementations. Here we're building some fake connections between pages to
build a realistic graph but the mathematical operations would be exactly the
same with an empty matrix as long as the size is the same.</p>
<p>Next, we'll use
<a target="blank" href="https://github.com/CodSpeedHQ/pytest-codspeed"><code>pytest-codspeed</code></a>, a <code>pytest</code>
plugin to measure the performance of the different implementations with various
parameter and with multiple builds/versions of CPython.</p>
<p>First let's define the benchmark cases:</p>
<pre tabindex="0"><code><span><span>@pytest</span><span>.</span><span>mark</span><span>.</span><span>parametrize</span><span>(</span></span>
<span><span>    "pagerank"</span><span>,</span></span>
<span><span>    [</span></span>
<span><span>        pagerank_single</span><span>,</span></span>
<span><span>        partial</span><span>(</span><span>pagerank_multiprocess</span><span>,</span><span> num_processes</span><span>=</span><span>8</span><span>),</span></span>
<span><span>        partial</span><span>(</span><span>pagerank_multithread</span><span>,</span><span> num_threads</span><span>=</span><span>8</span><span>),</span></span>
<span><span>    ],</span></span>
<span><span>    ids</span><span>=</span><span>[</span><span>"single"</span><span>,</span><span> "8-processes"</span><span>,</span><span> "8-threads"</span><span>],</span></span>
<span><span>)</span></span>
<span><span>@pytest</span><span>.</span><span>mark</span><span>.</span><span>parametrize</span><span>(</span></span>
<span><span>    "graph"</span><span>,</span></span>
<span><span>    [</span></span>
<span><span>        create_test_graph</span><span>(</span><span>100</span><span>),</span></span>
<span><span>        create_test_graph</span><span>(</span><span>1000</span><span>),</span></span>
<span><span>        create_test_graph</span><span>(</span><span>2000</span><span>),</span></span>
<span><span>    ],</span></span>
<span><span>    ids</span><span>=</span><span>[</span><span>"XS"</span><span>,</span><span> "L"</span><span>,</span><span> "XL"</span><span>],</span></span>
<span><span>)</span></span>
<span><span>def</span><span> test_pagerank</span><span>(</span></span>
<span><span>    benchmark</span><span>: BenchmarkFixture,</span></span>
<span><span>    pagerank</span><span>: PagerankFunc,</span></span>
<span><span>    graph</span><span>: np.ndarray,</span></span>
<span><span>):</span></span>
<span><span>    benchmark</span><span>(pagerank, graph, </span><span>num_iterations</span><span>=</span><span>10</span><span>)</span></span></code></pre>
<p>Here we're testing the 3 implementations with 3 different graph sizes. The
<code>benchmark</code> fixture is provided by <code>pytest-codspeed</code> and will measure the
execution time of the <code>pagerank</code> function with the given args.</p>
<p>Then, let's write a GitHub Actions workflow to measure the performance with
various builds of CPython on CodSpeed's infrastructure:</p>
<pre tabindex="0"><code><span><span>on</span><span>:</span></span>
<span><span>  push</span><span>:</span></span>
<span><span>jobs</span><span>:</span></span>
<span><span>  codspeed</span><span>:</span></span>
<span><span>    runs-on</span><span>: </span><span>codspeed-macro</span><span> # requests a CodSpeed Macro runner for the jobs</span></span>
<span><span>    strategy</span><span>:</span></span>
<span><span>      matrix</span><span>:</span></span>
<span><span>        python-version</span><span>: [</span><span>"3.12"</span><span>, </span><span>"3.13"</span><span>]</span></span>
<span><span>        include</span><span>:</span></span>
<span><span>          - { </span><span>python-version</span><span>: </span><span>"3.13t"</span><span>, </span><span>gil</span><span>: </span><span>"1"</span><span> } </span></span>
<span><span>          - { </span><span>python-version</span><span>: </span><span>"3.13t"</span><span>, </span><span>gil</span><span>: </span><span>"0"</span><span> } </span></span>
<span><span>    env</span><span>:</span></span>
<span><span>      UV_PYTHON</span><span>: </span><span>${{ matrix.python-version }}</span></span>
<span><span>    steps</span><span>:</span></span>
<span><span>      - </span><span>uses</span><span>: </span><span>actions/checkout@v4</span></span>
<span><span>      - </span><span>name</span><span>: </span><span>Install uv</span></span>
<span><span>        uses</span><span>: </span><span>astral-sh/setup-uv@v3</span></span>
<span><span>      - </span><span>name</span><span>: </span><span>Install CPython &amp; dependencies</span></span>
<span><span>        run</span><span>: </span><span>uv sync --all-extras</span></span>
<span><span>      - </span><span>name</span><span>: </span><span>Run benchmarks</span></span>
<span><span>        uses</span><span>: </span><span>CodSpeedHQ/action@v3</span></span>
<span><span>        env</span><span>:</span></span>
<span><span>          PYTHON_GIL</span><span>: </span><span>${{ matrix.gil }}</span></span>
<span><span>        with</span><span>:</span></span>
<span><span>          run</span><span>: </span><span>uv run pytest --codspeed --codspeed-max-time 10 -vs src/tests.py</span></span></code></pre>
<p>Here we're running the benchmarks with Python 3.12, 3.13, and 3.13 with free
threading support (<code>3.13t</code>), both with and without the GIL. Running 3.13 both
with and without free-threading support will allow us to see its impact on the
performance even when the GIL is enabled.</p>
<p>The python builds used are pulled by <code>uv</code> directly from
<a target="blank" href="https://github.com/indygreg/python-build-standalone">python-build-standalone</a>
built with <code>GCC 6.3.0 20170516</code></p>
<p>The jobs will run on
<a target="blank" href="https://docs.codspeed.io/instruments/walltime/">CodSpeed Macro</a> runners, which
are ARM64 bare-metal instances with 16 cores and 32GB of RAM, dedicated to each
job.</p>
<h2 id="results"><a href="#results">Results<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h2>
<figure><div><p>Measured by</p></div><figcaption><p>Time (in s) for the L graph size, <strong>less is better</strong></p></figcaption></figure>
<!-- -->
<ul>
<li>Without enabling new build options both <b>3.12</b> and
<b>3.13</b> perform very similarly.
We also clearly see the limitation of the <code>multiprocessing</code> implementation being even
slower than the single-threaded one due to the overhead of the inter-process communication.</li>
</ul>
<!-- -->
<ul>
<li>As expected the <code>threading</code> based implementation is the fastest when running
<b>3.13t with no GIL</b> and the GIL is effectively not limiting the
parallel execution of the threads anymore.</li>
</ul>
<!-- -->
<ul>
<li>Still, when running with the free threaded build both <b>with</b> and
<b>without</b> the GIL, we see a significant
slowdown for all other implementations. This is mostly because the free-threaded build
requires the specializing adaptive interpreter to be disabled, thus clearly decreasing
the performance of the other implementations. This overhead should be reduced in the
3.14 release where the specializing adaptive interpreter will be thread-safe and thus
will be re-enabled. At that point, migrating to the free-threaded build should be a
no-brainer for a lot of parallel applications and it will be interesting to measure
the performance changes.</li>
</ul>
<p>For all other graph sizes, the results are very similar and the conclusions are
the same. From this measurement, we can see that the new free-threaded build of
CPython 3.13 can have a significant impact on the performance of parallel
applications, bringing a very relevant alternative to <code>multiprocessing</code>. Still
it's experimental and not yet ready for production use because of the overall
slowdown it introduces, but it's a very promising step in the right direction!</p>
<h4 id="note"><a href="#note">Note<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h4>
<p>This benchmark doesn't include subinterpreters, which is another way to run
Python code parallelly without the GIL introduced in Python 3.12. Subintepreters
proved to be slower than other approaches in most cases, mostly because where
data sharing and inter-worker communication has not been fully solved yet. But
when it's there, it might definitely be a nice alternative to <code>multiprocessing</code>.</p>
<h2 id="resources"><a href="#resources">Resources<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h2>
<ul>
<li>
<p><a target="blank" href="https://github.com/CodSpeedHQ/python-parallel-pagerank/">Repository with the full code</a></p>
</li>
<li>
<p><a target="blank" href="https://docs.python.org/3.13/whatsnew/3.13.html">What's New In Python 3.13?</a></p>
</li>
<li>
<p><a target="blank" href="https://docs.python.org/3/howto/free-threading-python.html">Python experimental support for free threading</a></p>
</li>
<li>
<p><a target="blank" href="https://py-free-threading.github.io/">py-free-threading</a>: a centralized
collection of documentation and trackers around compatibility with
free-threaded CPython</p>
</li>
<li>
<p><a target="blank" href="https://peps.python.org/pep-0659/">PEP 659 - Specializing Adaptive Interpreter</a></p>
</li>
<li>
<p><a target="blank" href="https://docs.google.com/document/d/1hsV25JSwDb08c6-aHrI_YBHyl_uQKlys0ODihSx_aSw">Docs on having the Specializing Adaptive Interpreter without the GIL</a></p>
</li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: rallyup – Lightweight Wake-on-LAN Scheduler (126 pts)]]></title>
            <link>https://github.com/darwindarak/rallyup</link>
            <guid>42050862</guid>
            <pubDate>Tue, 05 Nov 2024 12:04:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/darwindarak/rallyup">https://github.com/darwindarak/rallyup</a>, See on <a href="https://news.ycombinator.com/item?id=42050862">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto"><code>rallyup</code></h2><a id="user-content-rallyup" aria-label="Permalink: rallyup" href="#rallyup"></a></p>
<p dir="auto"><code>rallyup</code> is a lightweight Wake-On-LAN (WOL) scheduler and dependency manager designed for small businesses and homelabs. It ensures that infrastructure services like firewalls, storage, and hypervisors are brought online in the correct order, particularly after events like power outages.</p>
<p dir="auto">A typical setup involves configuring most of the infrastructure for WOL but not for Wake-On-Power, and setting <code>rallyup</code> to run on startup on a low-power device like a Raspberry Pi. When you need to bring the entire environment online, simply power on the device running <code>rallyup</code>, and the rest of the infrastructure will automatically follow in the correct order.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/darwindarak/rallyup/actions/workflows/tests.yml/badge.svg"><img src="https://github.com/darwindarak/rallyup/actions/workflows/tests.yml/badge.svg" alt="Tests"></a>
<a href="https://crates.io/crates/rallyup" rel="nofollow"><img src="https://camo.githubusercontent.com/d06930b3b5f4b41abbd2ec7ab3bc8e39ba0efc33a314dd6961304cfc0d37d75c/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f72616c6c7975702e737667" alt="Crates.io" data-canonical-src="https://img.shields.io/crates/v/rallyup.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul>
<li> <em>VLAN Support</em>: Send WOL packets to devices across different VLANs.</li>
<li> <em>YAML Configuration</em>: Easily define server boot sequences, dependencies, and status checks.</li>
<li> <em>Service Status Checks</em>: Verify that a service is up using built-in status checks (HTTP health checks, NFS, SMB, custom shell commands).
<ul>
<li> HTTP</li>
<li> Open port</li>
<li> Shell</li>
<li> NFS (might just use open port check)</li>
<li> SMB (might just use open port check)</li>
</ul>
</li>
<li> <em>Plugin-Friendly</em>: Users can write their own custom status check plugins.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">The dependencies between servers, along with the methods for validating that they are online, are defined in a YAML configuration file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Servers Configuration</h2><a id="user-content-servers-configuration" aria-label="Permalink: Servers Configuration" href="#servers-configuration"></a></p>
<p dir="auto"><strong>Fields</strong>:</p>
<ul dir="auto">
<li><strong>name</strong>: The name of the server, used for identification when defining dependencies between servers</li>
<li><strong>mac</strong>: The MAC address of the server we want to wake up</li>
<li><strong>interface</strong>: The network interface to use when sending the WOL packet</li>
<li><strong>vlan</strong>: The VLAN ID (optional) that the server is on</li>
<li><strong>depends</strong>: A list of other server names that this server depends on</li>
<li><strong>check</strong>: A list of health checks that must pass before this server is considered fully online</li>
</ul>
<p dir="auto"><strong>Example</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="- name: &quot;firewall&quot;
  mac: &quot;00:11:22:33:44:55&quot;
  interface: &quot;eth0&quot;
  vlan: 100
  depends:
    - &quot;storage&quot;
  check: [... see below]"><pre>- <span>name</span>: <span><span>"</span>firewall<span>"</span></span>
  <span>mac</span>: <span><span>"</span>00:11:22:33:44:55<span>"</span></span>
  <span>interface</span>: <span><span>"</span>eth0<span>"</span></span>
  <span>vlan</span>: <span>100</span>
  <span>depends</span>:
    - <span><span>"</span>storage<span>"</span></span>
  <span>check</span>: <span>[... see below]</span></pre></div>
<ul dir="auto">
<li></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Health Check Configurations</h2><a id="user-content-health-check-configurations" aria-label="Permalink: Health Check Configurations" href="#health-check-configurations"></a></p>
<p dir="auto">Each server can have multiple health checks to ensure the server is fully online before the next device starts up.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Common Fields</h3><a id="user-content-common-fields" aria-label="Permalink: Common Fields" href="#common-fields"></a></p>
<ul dir="auto">
<li><strong>retry</strong>: The interval, defined in human readable string (e.g. 1s, 1 minute, etc.) to wait between retrying this health check</li>
<li><strong>timeout</strong>: The timeout interval after which the check, and subsequently the entire boot sequence, will fail</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Built-in Health Checks</h3><a id="user-content-built-in-health-checks" aria-label="Permalink: Built-in Health Checks" href="#built-in-health-checks"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">HTTP Health Checks</h4><a id="user-content-http-health-checks" aria-label="Permalink: HTTP Health Checks" href="#http-health-checks"></a></p>
<p dir="auto">The HTTP health check verifies whether a specified endpoint responds as expected.</p>
<p dir="auto"><strong>Fields</strong></p>
<ul dir="auto">
<li><strong>type</strong>: should be <code>http</code> for an HTTP health check.</li>
<li><strong>url</strong>:  The URL to perform the HTTP health check against</li>
<li><strong>status</strong>: Expected HTTP status code</li>
<li><strong>regex</strong>: Regex to match in the response body</li>
</ul>
<blockquote>
<p dir="auto">Note: You must provide either <code>status</code> or <code>regex</code>, or both.</p>
</blockquote>
<p dir="auto"><strong>Example</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="- type: http
  url: &quot;http://192.168.1.1/health&quot;
  status: 200
  retry: 5s
  timeout: 30s"><pre>- <span>type</span>: <span>http</span>
  <span>url</span>: <span><span>"</span>http://192.168.1.1/health<span>"</span></span>
  <span>status</span>: <span>200</span>
  <span>retry</span>: <span>5s</span>
  <span>timeout</span>: <span>30s</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Port Health Check</h4><a id="user-content-port-health-check" aria-label="Permalink: Port Health Check" href="#port-health-check"></a></p>
<p dir="auto">The port health check verifies whether a specified TCP port on a server is open and accessible.
This is really a stand-in for verifying NFS and SMB ports until I can figure out how to check if those services are online.</p>
<p dir="auto"><strong>Fields</strong></p>
<ul dir="auto">
<li><strong>type</strong>: should be <code>port</code> for a port health check</li>
<li><strong>ip</strong>: the IP address to check</li>
<li><strong>port</strong>: the port number to check</li>
</ul>
<p dir="auto"><strong>Example</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="- type: port
  ip: &quot;192.168.1.1&quot;
  port: 22
  retry: &quot;10s&quot;
  timeout: &quot;1m&quot;"><pre>- <span>type</span>: <span>port</span>
  <span>ip</span>: <span><span>"</span>192.168.1.1<span>"</span></span>
  <span>port</span>: <span>22</span>
  <span>retry</span>: <span><span>"</span>10s<span>"</span></span>
  <span>timeout</span>: <span><span>"</span>1m<span>"</span></span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Shell Health Checks</h4><a id="user-content-shell-health-checks" aria-label="Permalink: Shell Health Checks" href="#shell-health-checks"></a></p>
<p dir="auto">The shell health check executes a shell command checks the result.
This is to provide the option of user-defined health checks.</p>
<p dir="auto"><strong>Fields</strong></p>
<ul dir="auto">
<li><strong>type</strong>: should be <code>shell</code> for a shell health check.</li>
<li><strong>command</strong>:  he shell command to execute</li>
<li><strong>status</strong>: Expected exit code</li>
<li><strong>regex</strong>: Regex to match in the standard output</li>
</ul>
<blockquote>
<p dir="auto">Note: You must provide either <code>status</code> or <code>regex</code>, or both.</p>
</blockquote>
<p dir="auto"><strong>Example</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="- type: shell
  command: ping -c 1 192.168.1.1
  status: 0
  retry: 5s
  timeout: 20s"><pre>- <span>type</span>: <span>shell</span>
  <span>command</span>: <span>ping -c 1 192.168.1.1</span>
  <span>status</span>: <span>0</span>
  <span>retry</span>: <span>5s</span>
  <span>timeout</span>: <span>20s</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Full Example</h3><a id="user-content-full-example" aria-label="Permalink: Full Example" href="#full-example"></a></p>
<blockquote>
<p dir="auto">TODO:</p>
<ul>
<li> Need to test in the lab and post the actual sample</li>
</ul>
</blockquote>
<div dir="auto" data-snippet-clipboard-copy-content="- name: &quot;Firewall&quot;
  mac: &quot;00:1A:2B:3C:4D:5E&quot;
  interface: eth0
  vlan: 10
  depends: []
  check:
    - type: http
      url: &quot;http://192.168.1.1/health&quot;
      status: 200
      regex: 'ok'

- name: &quot;Storage Server 1&quot;
  mac: &quot;00:1A:2B:3C:4D:5F&quot;
  interface: eth0
  vlan: 100
  depends:
    - &quot;Firewall&quot;
  check:
    - type: port
      ip: 192.168.100.101
      port: 2049
      timeout: 5 minutes

- name: &quot;Storage Server 2&quot;
  mac: &quot;00:1A:2B:3C:4D:5G&quot;
  vlan: 100
  depends:
    - &quot;Firewall&quot;
  check:
    - type: port
      ip: 192.168.100.102
      port: 445
      retry: 5s

- name: &quot;VM Host&quot;
  mac: &quot;00:1A:2B:3C:4D:60&quot;
  vlan: 200
  depends:
    - &quot;Storage Server 1&quot;
    - &quot;Storage Server 2&quot;
  check:
    - type: command
      command: &quot;ping -c 192.168.200.10&quot;
      status: 0"><pre>- <span>name</span>: <span><span>"</span>Firewall<span>"</span></span>
  <span>mac</span>: <span><span>"</span>00:1A:2B:3C:4D:5E<span>"</span></span>
  <span>interface</span>: <span>eth0</span>
  <span>vlan</span>: <span>10</span>
  <span>depends</span>: <span>[]</span>
  <span>check</span>:
    - <span>type</span>: <span>http</span>
      <span>url</span>: <span><span>"</span>http://192.168.1.1/health<span>"</span></span>
      <span>status</span>: <span>200</span>
      <span>regex</span>: <span><span>'</span>ok<span>'</span></span>

- <span>name</span>: <span><span>"</span>Storage Server 1<span>"</span></span>
  <span>mac</span>: <span><span>"</span>00:1A:2B:3C:4D:5F<span>"</span></span>
  <span>interface</span>: <span>eth0</span>
  <span>vlan</span>: <span>100</span>
  <span>depends</span>:
    - <span><span>"</span>Firewall<span>"</span></span>
  <span>check</span>:
    - <span>type</span>: <span>port</span>
      <span>ip</span>: <span>192.168.100.101</span>
      <span>port</span>: <span>2049</span>
      <span>timeout</span>: <span>5 minutes</span>

- <span>name</span>: <span><span>"</span>Storage Server 2<span>"</span></span>
  <span>mac</span>: <span><span>"</span>00:1A:2B:3C:4D:5G<span>"</span></span>
  <span>vlan</span>: <span>100</span>
  <span>depends</span>:
    - <span><span>"</span>Firewall<span>"</span></span>
  <span>check</span>:
    - <span>type</span>: <span>port</span>
      <span>ip</span>: <span>192.168.100.102</span>
      <span>port</span>: <span>445</span>
      <span>retry</span>: <span>5s</span>

- <span>name</span>: <span><span>"</span>VM Host<span>"</span></span>
  <span>mac</span>: <span><span>"</span>00:1A:2B:3C:4D:60<span>"</span></span>
  <span>vlan</span>: <span>200</span>
  <span>depends</span>:
    - <span><span>"</span>Storage Server 1<span>"</span></span>
    - <span><span>"</span>Storage Server 2<span>"</span></span>
  <span>check</span>:
    - <span>type</span>: <span>command</span>
      <span>command</span>: <span><span>"</span>ping -c 192.168.200.10<span>"</span></span>
      <span>status</span>: <span>0</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under either of the following licenses, at your option:</p>
<ul dir="auto">
<li><a href="https://github.com/darwindarak/rallyup/blob/master/LICENSE-MIT">MIT License</a></li>
<li><a href="https://github.com/darwindarak/rallyup/blob/master/LICENSE-APACHE">Apache License 2.0</a></li>
</ul>
<p dir="auto">You may choose to use this project under the terms of either license.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: IMDb SQL Best Movie Finder (125 pts)]]></title>
            <link>https://www.imdb-sql.com/</link>
            <guid>42049936</guid>
            <pubDate>Tue, 05 Nov 2024 09:16:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.imdb-sql.com/">https://www.imdb-sql.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42049936">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Study Reveals Blood Sugar Control is a Key Factor in Slowing Brain Aging (141 pts)]]></title>
            <link>https://www.bgu.ac.il/en/news-and-articles/blood-sugar-control-is-key-factor-in-slowing-brain-aging/</link>
            <guid>42049418</guid>
            <pubDate>Tue, 05 Nov 2024 07:38:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bgu.ac.il/en/news-and-articles/blood-sugar-control-is-key-factor-in-slowing-brain-aging/">https://www.bgu.ac.il/en/news-and-articles/blood-sugar-control-is-key-factor-in-slowing-brain-aging/</a>, See on <a href="https://news.ycombinator.com/item?id=42049418">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Age-related brain atrophy, the gradual loss of neurons and shrinkage of brain tissue, is a natural part of aging, which can lead to cognitive decline and other neurological issues. While so far aging cannot be prevented, recent research from an 18-month dietary intervention offers hope that lifestyle and dietary changes can slow brain aging. A new international study, led by Ben-Gurion University of the Negev as part of the DIRECT PLUS Brain MRI trial, has brought to light how blood sugar control can significantly impact brain health.</p><p>Brain age, as evaluated by MRI measurements of the hippocampus and lateral ventricles, reflects the biological aging of the brain, which can differ from a person's chronological age. Chronological age is the number of years lived, while brain age indicates the brain's actual health. Typically, as we age, the hippocampus shrinks and the lateral ventricles expand, serving as markers of brain aging. Some individuals have a brain age younger or older than their chronological age. A younger brain age suggests better cognitive health, while an older brain age may indicate accelerated aging and increased risk of cognitive decline.</p><p>The study, which was published recently in <a rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0002916524007457" target="_blank"><em>The American Journal of Clinical Nutrition 2024</em></a>, was conducted by an international team of brain and nutrition experts, including researchers from Ben-Gurion University, Harvard University, Leipzig University, and more. The research was primarily carried out by PhD student Dafna Pachter and overseen by Prof. Iris Shai, along with several international collaborators.</p><p>A previous study published two years ago (<a rel="noopener" href="https://pubmed.ncbi.nlm.nih.gov/35021194/" target="_blank"><em>American Journal of Clinical Nutrition 2022 </em></a><em>)</em>, reported that Mediterranean (MED) and green-MED diets significantly attenuated age-related brain atrophy by ∼50% within 18 months.</p><p>In the current study, the researchers aimed to understand the mechanism by which the slowing of brain atrophy occurs.</p><p>The study found that a decline in HbA1c, and key markers of long-term blood sugar levels, are associated with significant positive changes in specific brain regions commonly affected by age-related atrophy. Brain MRI results showed that lower HbA1c levels corresponded to greater deviations in the thalamus, caudate nucleus, and cerebellum – areas crucial for cognitive function, motor control, and sensory processing. The study suggests that improved blood sugar control could be one of the most important factors in slowing down age-related brain changes.</p><p><strong>The Green Mediterranean Diet Shows Promise</strong></p><p>Earlier research has highlighted the benefits of the Green Mediterranean (Green-Med) diet, including better blood sugar control. The Green-Med diet is rich in polyphenols from plant-based sources like Mankai (a high-protein aquatic plant) and green tea, while being low in red and processed meats. The current study further strengthens this connection by suggesting that the Green-Med diet may not only support metabolic health but also exert protective effects on brain structure and function.</p><p><strong>DIRECT PLUS Trial – One of the Largest Brain MRI intervention Studies in the World</strong></p><p>The DIRECT PLUS trial, one of the longest and largest brain MRI studies conducted to date, involved approximately 300 participants who were divided into three dietary groups. Whole-brain MRI measurements were taken before and after the 18-month trial to track changes in brain health. The researchers used Hippocampal Occupancy (HOC), as a proxy for brain age which predicts future risk of dementia. HOC typically decreases with age. Interestingly, some participants exhibited a brain age either younger or older than their chronological age.</p><p>Using NeuroQuant, an FDA-authorized fully automated tool, the research team quantified and segmented the brain MRI-derived data. The study aimed to examine whether improved glycemic control and specific dietary components could slow down brain aging. The results indicated that participants who managed to improve their blood sugar levels and achieve normal glucose status experienced a more pronounced attenuation of brain aging. Notably, those who consumed higher amounts of green tea and Mankai duckweed shakes demonstrated the most significant improvements in both blood sugar levels and brain health.</p><figure><img src="https://www.bgu.ac.il/media/raeewflp/iris_shai_ip.jpg" alt="" data-caption="Prof. Iris Shai | Photo: Dani Machlis"><figcaption>Prof. Iris Shai | Photo: Dani Machlis</figcaption></figure><p><strong>Glycemic Control and Polyphenols: The Key to a</strong><strong> Younger Brain Age?</strong></p><p>The study’s lead researcher, Prof. Iris Shai, from Ben-Gurion University, an adjunct professor at Harvard University, and an Honorary Professor at Leipzig University, explains, “Maintaining low blood sugar levels, even within the normal range, shows promise for preserving a younger brain, especially when combined with a healthy diet and regular physical activity. Specifically, polyphenols found in plant-based foods may cross the blood-brain barrier and help reduce brain inflammation, which is crucial for memory”.</p><figure><img src="https://www.bgu.ac.il/media/jkiddbja/dafna-pachter-ip.jpg" alt="" data-caption="Dafna Pachter | Photo: Courtesy"><figcaption>Dafna Pachter | Photo: Courtesy</figcaption></figure><p>Dafna Pachter, a PhD student and the first author of the paper, adds, "This trial offers a safe approach to potentially slow down our brain aging—by adopting the components of a green-Mediterranean diet."</p><p><strong>A Pathway to Reducing Age-Related Cognitive Decline</strong></p><p>This study is one of the first large-scale trials to directly link dietary changes, particularly those associated with the Green-Med diet, to improved glycemic control and slower brain aging. While further research is needed to fully understand the mechanisms at play, these results suggest a potential avenue for reducing the risk of age-related cognitive decline through relatively simple dietary adjustments.</p><p>The DIRECT PLUS trial was funded by grants from the German Research Foundation (DFG), Israel Ministry of Health, Israel Ministry of Science and Technology, and the California Walnuts Commission. None of the funding providers were involved in any stage of the design, conduct, or analysis of the study, nor did they have access to the study results before publication.</p><p>The researchers: Dafna&nbsp;Pachter,&nbsp;Alon&nbsp;Kaplan,&nbsp;Gal&nbsp;Tsaban,&nbsp;Hila&nbsp;Zelicha,&nbsp;Anat Yaskolka&nbsp;Meir,&nbsp;Ehud&nbsp;Rinott,&nbsp;Gidon&nbsp;Levakov,&nbsp;Moti&nbsp;Salti,&nbsp;Yoram&nbsp;Yovell,&nbsp;Sebastian&nbsp;Huhn,&nbsp;Frauke&nbsp;Beyer,&nbsp;Veronica&nbsp;Witte,&nbsp;Peter&nbsp;Kovacs,&nbsp;Martin&nbsp;von Bergen,&nbsp;Uta&nbsp;Ceglarek,&nbsp;Matthias&nbsp;Blüher,&nbsp;Michael&nbsp;Stumvoll,&nbsp;Frank B.&nbsp;Hu,&nbsp;Meir J.&nbsp;Stampfer,&nbsp;Alon&nbsp;Friedman,&nbsp;Ilan&nbsp;Shelef,&nbsp;Galia&nbsp;Avidan,&nbsp;and Iris&nbsp;Shai.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Programmer in Berlin: Culture (112 pts)]]></title>
            <link>https://wickedchicken.github.io/post/programmer-in-berlin-culture/</link>
            <guid>42049180</guid>
            <pubDate>Tue, 05 Nov 2024 06:52:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wickedchicken.github.io/post/programmer-in-berlin-culture/">https://wickedchicken.github.io/post/programmer-in-berlin-culture/</a>, See on <a href="https://news.ycombinator.com/item?id=42049180">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    
    <div> <p>This is part 4 of a <a href="https://wickedchicken.github.io/post/programmer-in-berlin/">5-part series</a> detailing what I wish I had known as an
American programmer moving to Berlin. This page details cultural differences and things I wasn’t aware of until I stumbled on them.</p>
<h2 id="politics">Politics</h2>
<p>One thing about Germany, and Europe in general, is that it’s relatively left-wing when
compared to the US. This is a
place where universal healthcare is so commonly accepted that no party – not even the
super-racist party! – is talking about removing it. The aforementioned super-racist
party has effectively the same political platform as the mainstream
Republican party in the US (minus the healthcare thing). Politics in Europe certainly has
its own problems, but at least in Germany there is a flourishing multi-party system that
allows for people to have some kind of choice when voting. There is even a fun website
called the “<a href="https://www.wahl-o-mat.de/"><em>Wahl-o-Mat</em></a>” (“Vote-o-Matic”) that tells you which party to vote
for after
answering a series of questions. There’s also none of the Electoral College
silliness, which I won’t get into here.</p>
<p>I think one weird aspect of living in Europe, or at least Germany, is that all the stuff
that US politicians told me were “too expensive” apparently aren’t!</p>
<ul>
<li>
<p>Gas prices are double or even triple what they are in the US. It’s ok, the economy isn’t
collapsing.</p>
</li>
<li>
<p>Universal healthcare with an individual mandate apparently works. It’s ok, the economy
isn’t collapsing.</p>
</li>
<li>
<p>The trains, despite all the ineptness of Deutsche Bahn, are generally pretty good! For
many short-haul flights, such as Berlin to Munich, it’s actually quicker to go by
train than fly. It’s ok, the economy isn’t collapsing.</p>
</li>
<li>
<p>In Germany you get significant paid maternity and paternity leave<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.
It’s ok, the economy isn’t collapsing.</p>
</li>
</ul>
<p>Similarly, some cultural things that are deemed a “slippery slope” in
the US aren’t:</p>
<ul>
<li>
<p>Promoting Nazism is banned. It’s not a slippery slope, and the country hasn’t devolved into
authoritarian groupthink. You just can’t publicly support Nazism.</p>
</li>
<li>
<p>There is no right to firearms in the way there is in the US. You don’t really have
school shootings out here, and society seems to be ok? Nobody seems to want to band
together to make a militia. Nobody in Germany is like, “we need more guns to prevent
government overreach.”</p>
</li>
</ul>
<p>I’m not saying it’s perfect, and there’s plenty of criticism to be given to German
politicians and the EU as a whole, but I personally think it’s a more fair way of
governance.</p>
<h2 id="the-eu">The EU</h2>
<p>What is ironic about the general left-leaningness of Europe is that the EU’s structure
is similar to
what the “states rights!” people want – a coalition of states with relatively strong
political power and identities, working inside a relatively weak federal framework to
handle cooperation and disagreement. There is no EU military and only partially-dominant
EU money, but there is a
common justice framework, freedom of movement and a single market. To my limited
understanding, the EU is like if the US had no federal military but every state had its
own strong military, and Oklahoma spoke a different language and had different money than
everyone else<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>.</p>
<p>I am not qualified to give a good assessment of the EU as a political organization, other
than I am very glad they gave me a visa and I wish they would streamline visa application
and acceptance for more people. What I can comment on, however, is my impressions of
Europeans’ relationship with the EU itself.</p>
<p>I would say that most Americans identify as “American” in some way, while most Europeans
identify with a country inside the EU. A friend once commented that they didn’t “feel
European” (as opposed to the country they were from) and didn’t know what “typical
European stuff” would be (top on the list, as enumerated by Americans: cheese, bread, and
trains). I think this speaks to a relationship with the EU somewhere between how Americans
view the US government and how the Americans view the UN. Nobody says “I’m from the UN” or
“I’m from one of the United Nations” when asked where they’re from, and Americans view the
UN as an organization to be navigated and negotiated with as opposed to something they are
inherently
part of. The EU certainly has more influence over Europeans’ daily lives than the UN does,
but it’s not part of the daily conversation as much as the US government is to Americans.</p>
<p>Digging into that relationship a bit deeper, there is also a bit more skepticism of the EU
than I expected when moving here. Brexit made this pretty clear from the right-wing side,
but some on the left view the EU as some kind of banker’s club where Germany and France
get to tell smaller countries what to do. You can see this in the “<a href="https://en.wiktionary.org/wiki/Lexit">Lexit</a>” (crowd,
people
who wanted the UK to leave the EU so it could enact more socialist policies without being
hindered by the EU’s neoliberal slant. Another example is the
<a href="https://en.wikipedia.org/wiki/Greek_government-debt_crisis#Germany's_role_in_Greece">animosity countries had toward Germany after the eurozone crisis</a> – their
bailouts came attached with harsh
austerity requirements that were criticized as exploitative.</p>
<p>I personally think the EU is cool and wish it would remain a strong uniting force, but I
also can see how its incredible bureaucracy could be streamlined.</p>
<h2 id="crime">Crime</h2>
<p>I can’t speak to general crime rates, but in general Europe <em>feels</em> safer. The relative
lack of guns is refreshing, and (at least in Berlin) the crime centers around break-ins
and bike theft. I recall being in Copenhagen once, before I moved to Europe, and seeing a
woman walking by herself at 3 am. My first thought was concern for her safety, followed by
incredible shame in myself and my country – why did I think this was dangerous in the
first place? Of course, bad things happen all the time and I don’t want to claim Europe is
some kind of crime-free paradise, but it is striking how different it is compared to the
US.</p>
<h2 id="units">Units</h2>
<p>One small adjustment will be getting used to the metric system for everything. I recommend
not trying to do any conversions in your head and just see/feel the units themselves, but
occasionally having a reference point is useful.</p>
<p>Temperature: 0°C is water’s freezing point, 100°C is its boiling point. 37°C is a nominal
human body temperature. 16°C is light sweater weather, 20-22°C is a nice indoor
temperature depending on your preference, anything over 25°C is unpleasantly warm.</p>
<p>Weight: An average supermarket pineapple weighs roughly 1 kilogram! If you take away only
one fact from this article, let it be this one.</p>
<p>Distance: Meters are just yards, so that’s really easy. <em>Centimeters</em>, on the other hand,
are quite hard and I still don’t have a good feel for them. A kitchen planner once
laughed at me when I tried to mark 10 centimeters with my fingers. It’s a little
more than the
width of a credit card I guess? Around 4 inches? I also don’t have a good feel for
kilometers and can’t give many tips, sorry. A nice thing about miles is that people
roughly drive 60 miles per
hour over long distances, meaning a distance in miles roughly corresponds to how many minutes it will take to
drive there. Luckily people roughly drive 100km/h over long distances, meaning you can divide any distance
in kilometers by 100 and that’s roughly the number of hours it’ll take to drive there.</p>
<p>Volume: A cup is roughly 240mL, or 1/4L if you’re in a pinch. A gallon is roughly 4
liters. A liter is about half of a two-liter bottle<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>, or roughly one quart.</p>
<p>A fun fact: one mL of water weighs one gram, which means one liter weighs one
kilogram. Pretty cool!</p>
<h2 id="washing">Washing</h2>
<p>Washing machines in Europe electrically heat their water, which means there is far more
precise temperature control than water taken from the building’s hot water supply. Instead
of “cold,” “warm” and “hot” you have 30°C, 40°C, and 60°C. Luckily clothing sold in Europe
tells you what temperature to wash it at on the tag.</p>
<p>Dryers are relatively uncommon in Germany, and most people hang their wash on a line to dry.
You can do this inside or outside, if the weather allows. Since fully soaked clothes would
take forever to dry, most people have front-loading washers which spin the water out
at the end of a cycle. The higher the RPM, the more water is taken out, but higher RPMs
also add more wear to your clothes. At 1400rpm, cotton and synthetic clothes usually dry
overnight, if not sooner. At 800rpm, it can take a day for things to dry.</p>
<p>I first got annoyed that dryers weren’t common, but I quickly adjusted and stopped caring.
Dryers use immense energy and wear down your clothes, and the added convenience isn’t
worth it unless you’re washing a LOT of clothes (which explains why it’s mostly families
who buy dryers). If you do get a dryer, get a heat pump dryer – they use roughly a third
of the energy of the standard dryers you find in the US (but you do have to empty the
water out afterward).</p>
<p>I disagree with this <a href="https://slate.com/culture/2011/04/on-the-superiority-of-american-domestic-appliances.html">Slate article</a>’s general premise and feel that
European appliances are generally of equal or better quality, but definitely appreciate the
hatred of inscrutable symbols on the appliances here. Devices from Bosch-Siemens are
especially bad in this regard:
I have a washing machine
that has <em>Pflegeleicht</em> (“easy care” or “permanent press”) and <em>Pflegeleicht Plus</em> modes.
The
manual has no explanation of what <em>Pflegeleicht Plus</em> means<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>, and I
only found out via a
Youtube comment on a video showing it in operation<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>. On the other
hand, the manual for my dryer has an entire section telling me what clothes I should dry
on low heat, but doesn’t tell me how to actually do that – I’ve been assuming this entire
time that it’s also <em>Pflegeleicht</em>, but that’s never actually explained. Sometimes you’ll
have a symbol pop up on the display that doesn’t appear in the manual at all, and you have
to scour the internet to figure it out what it means.</p>
<h2 id="weather">Weather</h2>
<p>People in Berlin like to complain about the perpetual gray weather and they’re completely
right. Seasonal affective disorder is very real, and it might be a good idea to look into
Vitamin D and light therapy lamps. One note: a lot of people buy cool-colored light bulbs
or a SAD lamp and use it like a regular light. To get the real effects of these
lamps, you need to get a real one and use it very close to your face for a long time! I
went with the complete nerd option: <a href="https://myluminette.com/">Luminette</a> lamp glasses, which I use
right as I wake up. You look like a <em>total idiot</em> wearing them, but I like them a
lot. I don’t have much beyond anecdotal evidence to show that they work, but it’s worth
looking at the instructions of any SAD lamp you might buy to see if you can follow its
recommended regiment. As an added bonus, they’re good for late sleepers who need to wake
up early and can help you
adjust to new time zones. There are other companies that make similar devices if you’re
interested. Take winters in Berlin seriously!</p>
<p>The real solution is to book a trip to Spain in February, but that isn’t an option for
everyone.</p>
<h2 id="avoiding-the-expat-bubble">Avoiding the “expat bubble”</h2>
<p>Moving to a different country raises a lot of questions about integration, maintaining
your identity, and what community you decide to cultivate. One thing that can be difficult
is keeping ties to your previous community without locking yourself into it. Delving into
the language is one way to prevent this. I think it’s also worthwhile to try and rely on
“expat services” sparingly. They’re great when starting out, but over time they reinforce
the lock-in you’ll have. Language goes a long way here, but it can also help to expand
your social network to people from the country you’re in.</p>
<p>Something you may also want to be aware of is what I call the  “expat tax,” where
companies specialize
in providing English-speaking services or advice but charge more for the service. It can
be hard to discern if you’re getting specialized advice tailored to your needs, or getting
fleeced. Most of the time it’s not a big deal, but occasionally it can end up being
quite costly. I know of a mortgage broker specializing in English-speaking clients who, at
least in one case, gave a worse deal than a German-speaking mortgage broker. Admittedly,
several factors could have caused that, but a slightly worse deal on a mortgage can add
up to a big amount in the long run. Don’t feel too bad if you rely on such services, or
expat groups or whatever – I’m just saying that accepting a reasonable amount of
discomfort at the beginning can yield benefits in the long run.</p>
<p>It’s also worth getting involved on a civic level where possible. Reading local news can
help as is watching/reading <em>Tagesschau</em>, the publicly funded German news organization.
I find it also helps to go on
tours and learn about the history of the place you’re in, not just because it’s
interesting but also to help better relate to the people around you.</p>
<h2 id="customer-service">Customer service</h2>
<p>A lot of people moving to Germany (and especially Berlin) experience shock when
interacting with the famous German concept of customer service. While usually written off
as “rude,” I think there’s a deeper cultural norm powering this. An <a href="https://text.npr.org/482443233">NPR story</a> on opening
the first McDonald’s in Russia sheds light on what is going on here:</p>
<blockquote>
<p>SPIEGEL: At that time in Russia, the relationship between customers and service providers
like cashiers or waiters was basically the opposite of the relationship set up in America.</p>
<p>CHEKALIN: Waiters were kind of above us. They were not serving us. We were kind of, like,
bothering them.</p>
<p>SPIEGEL: In Russia, the role of the customer was to please the waiter, not the other way
around. To understand this, you need to remember that in the ’60s, ’70s and ’80s, Russia
was a state-run economy plagued by severe shortages. There were shortages of bread, of
meat, toilet paper. And so anyone who had access to stuff - to goods or food - had a
tremendous amount of power.</p>
</blockquote>
<p>Ultimately, customer service in Germany (especially eastern Germany) follows the rule that
<em>the person doesn’t really have to help you</em> and <em>you are being served at their behest.</em>
I’m not saying that this is pleasant or right, but it does explain a lot of what’s going
on. It also explains why “is there a manager I can talk to” isn’t really a move you can
pull. In the States it’s usually a dick move anyway but people do it assuming that the
customer service person will be afraid of repercussions. In Germany it doesn’t work very
well because there are few repercussions, and you aren’t respecting their authority.</p>
<p>A side-effect of a highly bureaucratic society is that exceptions can be extensively
documented and handled if the bureaucrat wants to do so. This means that front-line
customer service people often have the ability to break through the system a bit, assuming
they want to and that they know they can do it. It sucks that there is a lot of personal
discretion involved, but sometimes repeatedly mentioning or reminding someone that they
have the power to document an exception can go a long way.</p>
<h2 id="tipping">Tipping</h2>
<p>There are probably huge articles online explaining how to handle the much more relaxed
tipping rules in Germany. The nice thing to note is that there is no
<a href="https://en.wikipedia.org/wiki/Tipped_wage">tipped wage</a> system like in the US and waitstaff must be paid at least
minimum wage regardless of tips. This means they’re not as dependent on tips to get by,
and conversely tipping isn’t as compulsory as it is in the States. The card payment
terminals which always prompt for a tip are a very recent introduction and make everyone
uncomfortable.</p>
<h2 id="whatsapp">WhatsApp</h2>
<p>Nearly everyone in Europe uses WhatsApp, to the point that you might be lightly socially
ostracized if you don’t. Telekom, which <em>runs its own SMS communication network</em>,
allows you to contact their support via WhatsApp because it is so dominant. This has been
a great boon to Facebook and its group of aggressive PMs who want to make sure all your
contact data is harvested.</p>
<p>When I first moved here, you could use WhatsApp without giving access to your contact
list: you could type in a number and directly chat with a person. Then an update came and
the app refused to run unless you gave access. After a while it worked again
without having to give access (I conspiratorially think that Apple
forced them via app store policy), but a strange bug would occur: you could type
in a number directly and try to text them, but they “weren’t on WhatsApp.” Give the app
full access to all your contacts, and suddenly you could text that person. “Weird.”</p>
<p>I am perennially pissed off about this and I hope everyone involved feels bad about
themselves.</p>
<p>Update: <a href="https://www.nytimes.com/2024/10/02/technology/apple-social-apps-contacts-change.html?unlocked_article_code=1.UE4.fNpw.42cXdoBmoRc-&amp;smid=url-share">hahahahahahahahaha</a>.</p>
<h2 id="buying-stuff-on-sundays">Buying stuff on Sundays</h2>
<p>One thing you might get frustrated about is the lack of supermarkets that are open on
Sundays. This is a cultural thing that is supposedly not based on religion, but come on,
of course it is. There are a few gray areas: in Berlin there are some corner stores which are
open on Sundays and
some which aren’t. I’ve heard there is an informal agreement between corner stores who want to
stay open, and they simply don’t report each other so the compliance office doesn’t
investigate. In any case, there are some businesses which are allowed to legally operate on Sundays:
supermarkets can stay open as long as they’re in a train station and are targeted towards
travelers, and florists can sell cut flowers but not flowers in a pot. For everything else,
you have to hope your local corner store bends the rules a bit.</p>
<p>I don’t have any specific guidelines on how to get used to it, other than to adopt the
northern European tradition of <em>preparing for winter</em>. In the old days, you had to make
sure you stored enough potatoes in your cellar to last the winter, otherwise your family
would starve. Germans have somehow kept this mentality into modern times, and you can
sense the panic when someone realizes they don’t have enough groceries to last into
Sunday. Plan ahead, make sure you always have enough potatoes on hand, and be thankful for
what I can imagine is the only advantage of this system: extremely quiet Sunday streets.</p>
<h2 id="smoking">Smoking</h2>
<p>One thing I found unusual was how many people regularly smoked in Europe. Due to a variety
of factors, smoking dropped precipitously in my generation in the US (only to have vaping
take hold in the subsequent generation). This didn’t seem to translate over to here, and
people of all ages smoke in a variety of contexts. I don’t think I’ll forget the time
where I was eating lunch at the office and a coworker was having a conversation with
me, maintaining eye contact, while simultaneously rolling a cigarette. You don’t see that
often in the US!</p>
<p>Supposedly smoking has been legislated out of most public spaces in most of Germany, but
the Berlin nightlife industry fought hard against the regulations when they were being
crafted. Many bars in Berlin are smoking bars, and some restaurants even have a designated
smoking section walled off from the rest of the tables. I guess that’s one of the few
downsides of universal healthcare.</p>
<h2 id="testde--verbraucherzentrale">test.de / Verbraucherzentrale</h2>
<p>If you’re looking for a Consumer Reports or Wirecutter-like site in Germany, I can
recommend <a href="https://www.test.de/">test.de</a> or the
<a href="https://www.verbraucherzentrale.de/"><em>Verbraucherzentrale</em> (“consumer center”)</a>. In addition to product
reviews, they also
have good explanations and overviews of common insurances, guides for planning major life
events, etc. You do, however, have to have a decent German reading level to take advantage
of it.</p>
<h2 id="writing-letters--buying-a-printer">Writing letters / Buying a printer</h2>
<p>This is slowly changing, but Germany is absolutely a paper-based society. Until very
recently, the only guaranteed easy way to cancel subscriptions (gym memberships,
newspapers,
etc) was to mail a letter to terminate the contract. While you might be able to get away
with printing stuff at work or at a local copy shop, it can often be worth it to get a
home printer. I have a cheap black and white laser printer which has performed beautifully
and hasn’t needed a lot of toner. You might also want to look into getting a scanner to
scan and send copies of signed documents, although there are many apps that use your
phone’s camera to do this. It will make your life way easier if you can scan multiple
pages and combine them into a single PDF, reducing image quality to an acceptable file
size along the way. The iOS Files app and MacOS Finder/Preview can do this natively, but
the process can be annoying and can result in extremely high-resolution files that may be
difficult to transfer. Adobe has a scanning app that requires you to upload to their
cloud-based service. Microsoft Lens seems to be a good sweet spot: local storage, can
easily make PDFs of a normal size, and free.</p>
<h3 id="printing-envelopes">Printing Envelopes</h3>
<p>It’s not immediately obvious, but most printers have variable-width paper trays and will
happily print on DL envelopes (<em>DIN-Lang</em>, the standard envelope size for folded A4
paper). This is
great for people with bad handwriting, but it
is also super nice when combined with prepaid postage.</p>
<p>Google Docs doesn’t have envelope sizes natively, but you can install extensions to do
this. I’ve been using Apple Pages to make letters. The key is to select “Envelope,” and
make sure that the page size is set to ‘Envelope DL’. I’m sure Microsoft Word has a
similar thing.</p>
<p>If you’ve bought postage online, just drop it into the word processor and print the
address, return address, and postage all in one go.</p>
<p>I always kept loading the envelopes into the printer the wrong way, so I ended up
writing a little reminder on a label and sticking it near the printer tray so I
wouldn’t mess up again.</p>
<h2 id="subscriptions">Subscriptions</h2>
<p>Germany really locks you into yearlong subscriptions, so watch out. Gym memberships, cell
phone contracts, newspaper subscriptions, etc often have a minimum subscription amount
(usually a year or two years) and make it difficult to cancel. The government has passed
a law to require
simple cancellation online, but if that doesn’t work you’re often best off writing a
letter stating you’d like to cancel. There are many templates for this you can find
on the internet, but note that if you cancel in the middle of the initial period you’ll
likely have to pay until the period ends. Thankfully the law has now changed and you
should be able to cancel on a monthly basis after the initial period.</p>
<h2 id="dating">Dating</h2>
<p>In general, dating works similar to how it does in the US, with a few exceptions. One is
that dating apps can have a different connotation than what you’re used to. Tinder in the
US morphed from a hook-up app to a general dating service, but Germany was behind a few
years on that. For years I avoided Bumble as “a thing only Americans used,” only to
discover
that a much broader section of the Berlin population used it. Just be open to differences
from what you’re used to.</p>
<p>One thing that is a bit weird is everyone having profiles in English. I’ve heard of
stories where two Germans with English profiles figure out they’re German and switch
mid-chat. I think this mainly speaks to Berlin having a lot of non-German speakers (and
certainly a lot of non-German speaking tourists), so English is sometimes easier to start
off with. All I’m saying is don’t worry too much about your own profile being in English,
and don’t read too much into it if someone texts you in English.</p>
<p>Pro-tip: mention your test.de subscription in your profile and watch the matches come in
like crazy.</p>
<h2 id="following-the-rules">Following “The Rules”</h2>
<p>A particularly cheesy joke I once heard goes “do you know why crime is so low in Germany?
Because it’s illegal.” The extent to which rules and signs are just <em>followed</em> here can be
startling to someone accustomed to a more relaxed culture. It’s like the concept of a
“victimless crime” doesn’t exist here: even if you jaywalk at night when there are no cars
in sight, you’re still breaking the rules and that is, in itself, an offense. There are
two effects here.</p>
<p>One is that everyone trusts that everyone else follows the rules. When I was learning to
drive here, my driving instructor chastised me for looking left too much when turning
right at a T intersection. I was worried about people coming from the left who couldn’t
see me or wouldn’t let me turn. My teacher’s response: due to the “right before left”
rule (explained later in the “Driving” section), they would have to stop for me and I
should just trust that. Apparently it’s not
enough to follow the rules, you’re also expected to trust that everyone else is as well.</p>
<p>Two is that “the rules” can take precedence over communication and relationships. Noise
complaints from neighbors are a good example here. If I hear someone making a lot of noise
in an apartment, I use that as a starting point for a conversation with them. Maybe they
didn’t know the noise carried that much, or maybe we can work out an agreement where most
of the noise can happen when I’m not home. Unfortunately, what often happens is some
variant of “is it before 7pm? Then I’m allowed to do whatever I want!” It’s also limiting
for the people who are used to that environment – maybe something is against the rules,
but the neighbors don’t care. Instead of asking, they simply state that it’s against the
rules and automatically assume rejection – again, there is no “victimless crime.”</p>
<p>This is usually one of the more negative aspects of German culture that people cite, and
I can’t say I disagree. I don’t have much advice on how to navigate it except to notice
what gets people really freaked out and try to avoid those offenses. These usually
fall under:</p>
<ul>
<li>Noise during designated quiet hours</li>
<li>Improperly disposing of trash (including not folding boxes in the recycling bin)</li>
<li>Blocking cars (blocking pedestrians or bicycles doesn’t receive the same outrage)</li>
<li>Not lining up properly (I also get mad at this)</li>
<li>Not staying far enough to the right in a bike lane, even if it’s really narrow and no normal person would want to pass you.</li>
</ul>
<h2 id="real-shit-they-dont-want-you-to-know-about">Real shit “they” don’t want you to know about</h2>
<h3 id="worker-rep-on-the-board-_betriebsrat_">Worker rep on the board (<em>Betriebsrat</em>)</h3>
<p>A <a href="https://de.wikipedia.org/wiki/Betriebsrat"><em>Betriebsrat</em></a> is an elected “works council” made up of employees that must be consulted by
management on certain issues. Once a company gets large enough it must accommodate a works
council by law, so some see it as a way toward organized labor in a company.
A <em>Betriebsrat</em> doesn’t have the same power as a union, so it’s worth looking into what they
can and can’t do.</p>
<h3 id="_bildungsurlaub_"><em>Bildungsurlaub</em></h3>
<p>In most German states, you have the right to a “<em>Bildungsurlaub</em>.” This is paid vacation
you’re
allowed to take in conjunction with some kind of job or political training. It differs by
state, but in Berlin you get 5 working days each year which can also roll over,
allowing you take 10 days every two years.</p>
<p>You do have to pay for the class itself and any transportation or lodging.
There are almost always German classes available for this, and you’re
allowed to go anywhere there is a course available. I know a German who went to Vancouver
for two weeks to improve his English, and several of my English-speaking colleagues have
spent time in different parts of Germany improving their German. Of course, there’s
nothing preventing you from taking a German class in your city of residence, but sometimes
it’s nice to have a change of scenery.</p>
<h3 id="properly-opening-a-ritter-sport">Properly opening a Ritter Sport</h3>
<p>I opened Ritter Sports for years without knowing that you’re supposed to open them by
<a href="https://www.youtube.com/watch?v=0SaFuKPX03U">“snapping” them at the seam</a>. This opens them in a way such that the seam can be
resealed, to save the rest for later.</p>
<h2 id="transport">Transport</h2>
<h3 id="public-transit">Public Transit</h3>
<p>Of all the lifestyle upgrades I can think of, universal healthcare and pervasive public
transit are the two huge benefits of moving to Europe. Sure, there are plenty of cars
on the streets, but it’s far easier to take a bus, tram, subway, or commuter rail to your
destination than it is in virtually all of the US. It’s not perfect, but in general the
BVG (the Berlin public transit operator) will generally get you to where you need to go,
at a relatively quick pace. Not all areas of the city have a good connection, though.</p>
<p>However be careful about the ticket system: you have to buy your ticket <em>before</em> you get
on the train. Plainclothes ticket checkers (who are not fondly looked upon)
will spring into action once the doors close, and I’ve known people who have gotten fines
for not buying their tickets quick enough. If this happens, make sure to ask for a receipt
– ticket checkers have been known to scare tourists into paying cash on the spot and
pocketing the money.</p>
<p>Some, but not all, employers offer a discounted monthly transit ticket called a
<em>Jobticket</em> which is probably
worth it if offered. See the <em>Deutschlandticket</em> section for more details.</p>
<p>Google and Apple maps generally have decent transit directions. The BVG and VBB (the
transit operator for the greater Berlin and Brandenburg area) apps also have pretty good
routing. Transit and Citymapper are useful here, but aren’t often used.</p>
<p>One peculiar annoyance of the transit system here is that it can often be difficult to
know what station you’re at. The S-Bahn is particularly egregious here, as the signage
inside the train is often broken and the signage at stations is unlit and very difficult
to see (even more so at night, when the glare on
the windows makes it impossible to look outside). The newer S-Bahn trains make it even
worse – when the doors are about to close and the beeping starts, the screens in the
train switch to
what train it is instead of the current station. This makes it impossible to know where
you are without looking outside. For people like me who read articles on the train and
only react
to the beeps, it can be very stressful to look up and not know if you’ve missed your
stop or not. Oftentimes, the quickest way to tell which S-Bahn stop you’re at is to pull
out your phone and see where you are on Maps.</p>
<h3 id="bicycle">Bicycle</h3>
<p>Berlin’s cycling infrastructure is getting better over time<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup>. In addition
to all the pop-up
bike lanes that appeared during the pandemic (which are being converted to permanent bike
lanes over time), the city is building a bike autobahn (bikeobahn?) to connect major parts
of the city. Cycling in Berlin is semi-accepted. It’s not as nice as Amsterdam or
Copenhagen, but it’s not as bad as anywhere in the US.</p>
<p>Certain areas of the city have relatively good infrastructure (Prenzlauer Berg, in
particular), while others don’t (Neukölln, Potsdamer Platz). When the infrastructure isn’t
good, cyclists are often crammed in with aggressive cars and the result isn’t pretty. I
don’t have good advice here other than to avoid the areas where possible. When it’s not
possible, I urge you to heed the words of a bike shop owner when I asked him how to turn
left on one of Berlin’s major thoroughfares: “do you want to assert your rights or do you
want to stay alive?” Getting a ticket for riding on the sidewalk (slowly, don’t be a jerk
and blow past pedestrians) isn’t so bad compared to dying, nor is it worth it to rush or
bend traffic laws on the road.</p>
<p>Speaking of traffic laws, you may want to read up on the <em>rechts vor links</em>
(“right before left”) rule which described in the driving section. It will
help explain why cars act strangely at 4-way intersections.</p>
<p>Regardless of how well built the cycling infrastructure in your neighborhood is, you will
always
encounter cycling’s second natural enemy behind cars: cobblestones (<em>Kopfsteinpflaster</em>).
These are purpose-built to ruin your day as a cyclist<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup>. In addition to causing
extreme vibration while riding, enough to kick you off your bike if you’re going at anything
resembling a normal speed,
they can be spaced just wide enough to catch your wheel and send you flying. I’d like to
give a special shoutout to the bike lanes which are interrupted by driveways that use
cobblestones <em>just for the bike lane</em> – a special treat for everyone who wants to get
flipped off of their bike. For some reason they seem to be specially chosen and laid to
maximize the chances of catching your wheel.</p>
<p>You can fight cobblestones with a two-pronged approach: avoidance and acceptance. To avoid
them, I recommend the Beeline app. You should probably buy a <a href="https://beeline.co/">Beeline</a> anyway
since it’s a
great device, but I think you can use the app without purchasing the device and it works
as a great cycling navigation aid throughout Berlin. While it doesn’t know about
cobblestones specifically, it does let users rate streets and the routing takes that into
account. For those who want more detailed information, the “Berlin by Bike” website and
app use exact street data to minimize cobblestones as much as possible. Unfortunately I
find both interfaces effectively unusable, so I never end up using these. Google Maps can
route for cyclists but it has no concept of cobblestones and minimal understanding of
bike lanes, so it <a href="https://www.golem.de/news/fahrradtour-als-google-maps-mich-mehrfach-fast-umgebracht-haette-2209-168001.html">often puts cyclists in unpleasant or outright dangerous situations</a>. To
accept cobblestones, you can get a bike with wide tires or underinflate your tires to
absorb the impact (but not too much, or you’ll get a pinch flat). Getting springy grip
tape or wearing cycling gloves can help your wrists too.</p>
<p>Bike theft is definitely a concern around here, but not as much as I recall in SF. I
always followed the advice to bring two U-locks, one for the front wheel and one for the
back wheel/frame. Virtually nobody here seems to do that, and the foldy “D-locks” are
quite common around here. While practical, they offer less security. I’d say the general
consensus is to have a less valuable bike + less effective locking instead of valuable
bike + more effective locking. Another thing to be aware of: some of the bike racks are
too wide or too weirdly designed to be able to lock your rear wheel and frame together
with one U-lock. It might be worthwhile to invest in locking wheel skewers instead of
quick releases so you can care a little less when only one lock fits.</p>
<p>For some reason, despite Berlin’s reputation as a laid-back city (nobody is really in a
rush here), everyone is super aggressive in cars and on bikes. In addition, you also have
people who buzz past you at
especially dangerous times (intersections when the light just turns green, right when a
huge truck is passing centimeters away instead of just waiting a few seconds, the insanely
narrow bike “lane” on the bridge at Warschauer Straße), and otherwise general weird
behavior.</p>
<p>I don’t want to seem too negative here: cycling in Berlin is a great way to get around,
it’s free once you buy the bike and is great for your health. In addition to daily travel,
cycling out to Brandenburg for a long weekend can be a great experience. In summer, it’s
very popular to bike out to one of Berlin or Brandenburg’s many lakes and chill out for
the day. <a href="https://www.komoot.com/">Komoot</a> is a nice way to plan out one of these trips if you don’t know
anyone who knows the way. Enjoy it!</p>
<p>Finally, while you may not be able to vote, you can help your city become a more
bike-friendly place. I’d recommend joining your local ADFC chapter (that would be
<a href="https://berlin.adfc.de/">ADFC-Berlin</a> for the Berliners), who petition for better bike lanes, better
protection of
cyclists, and overall encourage a more bike-friendly city. In Berlin they also organize
the ADFC <em>Sternfahrt</em> (“rally”), for those who want a tamer Critical Mass experience.</p>
<h3 id="regional-and-long-distance-trains">Regional and Long-distance Trains</h3>
<p>Germans love to complain about the Deutsche Bahn, and there’s plenty to complain about –
the general opinion is that it’s a corrupt organization that was affected by the same
privatization spree that created the Deutsche Post. Recent
<a href="https://www.zugfinder.net/en/stationboard-Berlin">dismal on-time performance</a> has
not helped this impression, and trying to understand exactly <em>why</em> DB has such structural
problems is a hobby in and of itself.</p>
<p>However, overall <em>there are trains</em> and <em>the trains actually run</em>. You can reasonably
traverse the country by train, and if you don’t mind paying a bit extra for a high-speed
train you can get from any big city to another in maybe 7 hours tops.</p>
<p>Deutsche Bahn has some confusing pricing for the uninitiated, so I’ll try to explain it
below.</p>
<h4 id="second-class-_flexpreis_">Second-class <em>Flexpreis</em></h4>
<p>The basic ticket is the second-class <em>Flexpreis</em> (“flexible-ticket price”) ticket. This kind
of ticket works like a
public-transit ticket: just like a subway or bus ticket, with a <em>Flexpreis</em> ticket you can
take any train that day to your destination. Also like a subway or bus ticket, there is no
seat reservation: it’s a free-for-all and you may get lucky or not depending on demand.
This also means that DB can’t fundamentally do capacity control for second class. They
try to predict demand and tell you on the website, but there’s nothing stopping 200 people
showing up to ride the same train and filling up all possible space in the car.</p>
<p>The flexibility makes this kind of ticket most useful for commuters who don’t know the
exact time they’ll take a train and don’t mind if they don’t get a seat, since it won’t be
that long of a ride anyway. Reserved seats can be purchased afterward if you settle on a
particular train and think it might be busy.</p>
<h4 id="second-class-_sparpreis_">Second-class <em>Sparpreis</em></h4>
<p>The second-class <em>Sparpreis</em> (“savings price”) ticket is exactly like the <em>Flexpreis</em> ticket
except you have to pick a specific train to ride on. This changes the ticket from a
public-transit style ticket (“take any train going that direction”) to something more like
flying: you have a specific train to catch and can’t substitute it with another one. The
upside is that <em>Sparpreis</em> tickets are usually much cheaper, depending on how busy the train
is and how far out in advance you buy it. A funny consequence of this pricing model is
that the <em>Sparpreis</em> starts cheap and rises to eventually match the cost of the_Flexpreis_
ticket – at which point you can just buy a <em>Flexpreis</em> ticket, since that lets you ride on
any train that day anyway.</p>
<p>Like <em>Flexpreis</em> tickets, second-class <em>Sparpreis</em> tickets also do not come with a seat
reservation. Unlike <em>Flexpreis</em> tickets, most people buying <em>Sparpreis</em> tickets are buying for
some kind of long-distance vacation or business trip. For some routes and times of day
it’s not worth reserving a seat since it will almost certainly be empty, but if you’re
wrong you’ll be standing in the aisle for 5 hours straight. Another annoyance is that the
seat you pick may be reserved for part of your journey, meaning you’ll have to shuffle
around and find another one. Finally, there are the “<em>ggf. freigeben</em>” seats.
<em>Ggf. freigeben</em> (German for “lol, you thought this would be easy?”) means that the
seat may or may not be reserved, and they don’t know due to technical reasons. If you sit
there, you’ll have to wait the entire ride to see if someone actually did reserve it or not.</p>
<p>After getting burned a few times, I’ve just committed to paying the extra few Euro to
always get a reserved seat. If you plan to save the cash, just be prepared to stand in the
aisle/vestibule or shift seats a lot.</p>
<h4 id="first-class">First-class</h4>
<p>First-class on a DB train isn’t like first-class on a plane: you get slightly more
legroom, and occasionally someone will take your food or drink
order so you don’t have to go to the dining car yourself. It used to be a better deal on
emptier trains since you used to get a free seat reservation included, which often made
a first-class ticket only a few more Euro than a second-class ticket with seat
reservation. Sadly seat reservations
<a href="https://web.archive.org/web/20230613062354/https://www.bahn.de/info/kleiner-fahrplanwechsel-juni-23">are no longer included with first-class tickets anymore</a>.</p>
<h4 id="bahncard">BahnCard</h4>
<p>If you plan to do any regular traveling with DB, I highly recommend getting a BahnCard. A BahnCard
25 can rapidly pay for itself after a few trips. These are prepaid discount cards that
come in six major flavors:</p>
<ul>
<li>First- and second- class BahnCard 25</li>
<li>First- and second- class BahnCard 50</li>
<li>First- and second- class BahnCard 100</li>
</ul>
<p>The BahnCard 25 is what applies to most people – you get 25% off of the various kinds of
<em>Sparpreise</em>, sometimes including train travel outside of Germany. The first-class BahnCard
also gives you the 25% benefit on second-class tickets, but not the other way around, so
it only makes sense to have one and not both.</p>
<p>The BahnCard 50 makes the most sense for commuters who need the flexibility of a <em>Flexpreis</em>
ticket, it gives you 50% off of <em>Flexpreise</em> as well as 25% off of <em>Sparpreise</em>.</p>
<p>The BahnCard 100 is for high-rollers. It lets you ride <em>any</em> train on the system, although
you only get a certain number of free seat reservations to go along with it. I only know
of people who get BahnCard 100s through their work, similar to having a company car.</p>
<h4 id="49-euro-ticket--_deutschlandticket_">49 Euro Ticket / <em>Deutschlandticket</em></h4>
<p>This is a relatively new offering that was originally introduced in 2022 during the
outbreak of the Russian invasion of Ukraine to reduce German’s reliance on fossil fuels.
Its success led to the current version with a higher price but more sustainable funding
from the government. The <em>Deutschlandticket</em> allows you to ride any regional, non-high-speed
train, as well as public transit in any city, in all of Germany for €49 a month. This
means you can ride from your apartment to the train station in Berlin using Berlin public
transit, a regional train from Berlin to Hamburg, and Hamburg public transit to your
hotel, all on the same ticket, which costs €49 a month. This is not only a great deal, but
an impressive achievement that they managed to unify the transit networks of every city in Germany. As you
can imagine, many people now opt for the <em>Deutschlandticket</em> instead of single-city subscriptions. You buy it through your local transit
agency, and there are even <em>Jobticket</em> variants you get through your employer that cost a little less.</p>
<p>A small caveat: the <em>Deutschlandticket</em> counts as a second-class ticket on regional trains,
and can’t be combined with a first-class “upgrade” ticket. If you want to ride first-class
on a regional train, you need to pay the full price. Additionally, there are no refunds if
a train gets canceled or delayed. If you are planning on using the <em>Deutschlandticket</em> to
catch a <em>Sparpreis</em> express train somewhere else, it’s probably better to buy a ticket for the whole
journey so you can be rerouted or reimbursed if something goes wrong.</p>
<h4 id="booking">Booking</h4>
<p>Booking trains within Germany is pretty easy – just go to the DB website. Booking trains
to neighboring countries through DB is also generally pretty good. Unfortunately, there is
no unified European train search and booking system, and the accuracy and bookability of
train search falls off steeply as you go further away from Germany. <a href="https://www.omio.com/">Omio</a> provides a
Europe-wide train search in theory, but it’s not as good as Google Flights, Kayak, etc.
Seems like a great opportunity for a scrappy startup to build something!</p>
<p>Sometimes you might get cheaper prices or different conditions if you book trips through
the Austrian rail line (<em>ÖBB</em>) instead of DB.</p>
<p>For longer-distance trips, <a href="https://www.rome2rio.com/">Rome2Rio</a> can provide some good connections that you might not
otherwise think about. There is also <a href="https://www.blablacar.com/">BlaBlaCar</a> which helps you organize long-distance
carpools, but I haven’t used it.</p>
<h3 id="driving">Driving</h3>
<p>I recently got my German driver’s license after converting it from a California one.
Unfortunately California doesn’t offer an exchange with Germany, so I had to go through
almost the whole driving school process here. Exchanging my license meant I needed fewer
required driving hours, didn’t have to do any “special” driving hours (like night driving
or Autobahn driving), and it meant that I could skip any theory classes as long as I
tested ok. Note that the exchanged license only has to be valid when you make the initial
application – if your license expires between the application and when you pass your
exams, everything should still be ok as long as the application itself is valid. If you’re
thinking about exchanging your license, do it while you still can.</p>
<p>An exchanged California license, assuming you don’t need too many practice driving hours,
will cost you roughly €1500 - €2000. That’s the school fees, the fees to use their
“learning platform” (an app to go through all the theoretical exam questions), some
administrative fees they charge whenever they talk to the license office or sign you up for a test,
and the practice driving hours. To help you estimate, I needed about 11 practice hours
in total, at a rate of about €80/hr. The schools aggressively hide their fees online, but
usually if you ask or sign up they have a published fee sheet somewhere.</p>
<p>I went to the license office to directly apply for the exchange, but I think that was really
the wrong order. I had to supply a driving school and test location, so I think the proper
order would have been to sign up with a driving school first. From what I understand, most
if not all schools will handle all the license office paperwork themselves (with an implied fee, of
course).</p>
<p>I had to take the theoretical and practical exam, as if I had never driven before. The
practical was relatively straightforward, but I recommend reading up on Reddit to avoid
common pitfalls. I found practicing for the theoretical exam way harder. The wording of
all the questions is quite tricky, so even if your German is passable it might be better
to take the exam in English, even though the translations aren’t great.</p>
<p>Finally, there are two things I’d want to stress when learning to drive: right-before-left
and where the speed limit signs are.</p>
<h4 id="right-before-left">Right before left</h4>
<p>This is a rule that, as far as I can tell, only exists in Germany and the Netherlands. In the US, there is the
concept of a 4-way stop sign at an intersection. Germany doesn’t have that. At a 4-way
intersection without a traffic light, the <em>rechts vor links</em> (“right before left”) rule applies: you have to
yield to everyone to the right of you. Before you go “oh yeah, that makes sense” I almost
assure you that you don’t understand it. There is no cycling of cars as people take turns,
literally everyone to the right of you goes ahead while you wait, and only when the right
is clear do you go (the people left of you are waiting on you). Imagine that every
intersection had a default sign that said “yield to all cars on the right.”</p>
<p>This rule applies basically everywhere there aren’t traffic signs, not just at 4-way
stops. If there is a road to the
right of you, you have to slow down to see if anyone is coming and yield if anyone is
there. The only exceptions are if it’s a “road with a curb” (meaning a driveway), a
lesser class of road (a dirt road when you’re on a paved one), or if you have a street
sign telling you have right of way or are on a “priority road” (the most common being a
yellow diamond inside a white diamond). This means that in a parking lot, if someone is
arriving to the right of you, you have to yield.</p>
<p>It’s complicated enough driving through the city trying to figure out if the road to the
right of you is a real road or a driveway, but there’s another curveball thrown into the
mix: the sign that tells you that the priority road curves right or left. Usually in these
scenarios I stop my car and write a Prolog program to solve the logic puzzle of
what to do, especially if you want to turn left into oncoming traffic. One nice
shortcut: if you’re on a priority road that curves left, you always have right-of-way no
matter what you’re doing.</p>
<h4 id="where-the-speed-limit-signs-are">Where the speed limit signs are</h4>
<p>This seems trivial, but I found it very frustrating while driving around town. In Germany,
the speed limit signs are posted directly <em>at</em> the intersection, and not after.
Additionally, there is a default speed limit if you <em>don’t</em> see any sign (50km/h within
cities, 100km/h outside). Since I’m usually looking at pedestrians and other cars when
turning at an intersection, I kept missing traffic signs when I was first learning to
drive here. While you’re walking on the street (meaning, in a non-stressful situation),
keep an eye out for them to see where they should be.</p>
<p>Be sure to note the change in default speed limit I mentioned above. Make a mental note
that <em>every</em> time you see a “welcome to X city” sign, you slow down to 50km/h.</p>
<p>Finally, speed limit signs on some roads can be on the <em>left</em> side of the road as well
as on the right – these can be more difficult to see on wider streets, so try to
keep an eye out for them.</p>
<h2 id="medical">Medical</h2>
<p>The medical system in Germany is good, regardless of whether you have public or private
health insurance. In general it helps to know German to navigate the system, but a few
tips can make it a bit easier.</p>
<h3 id="emergencies">Emergencies</h3>
<p>If you have a medical emergency, dial 112 and get an ambulance – make sure to state the
street address first, so if the call gets cut off they still know where to go. The number
to call the police in emergencies is 110. I struggled for a while to remember which was
which – 110 or 112 – until I saw a cheesy poster for a fire brigade’s recreational
rowing team: “We give 112 percent!” It took me a second, like “oh god, 110% isn’t good
enough?” followed by “oohhhhhh.”</p>
<h3 id="getting-an-appointment">Getting an appointment</h3>
<p>With public insurance, it can be hard to find an appointment for your doctor. Some of them
only make appointments by phone, but a growing number schedule patients online via
companies like Doctolib,
Jameda, or Samedi. For specialists it may not be a big deal, but I really recommend
finding a <em>Hausarzt</em> (“primary care physician”) that reliably schedules appointments
online. Some health insurances
offer appointment-making services (Techniker Krankenkassse, for example, has an English
language appointment
hotline), but I don’t have much experience with that.</p>
<p>One bright spot is that you often don’t have to make an appointment to refill a
prescription – usually you can call the doctor’s office, ask for a refill, and you can
pick the prescription up when convenient. Sometimes they’ll even mail it to you!</p>
<h3 id="krankschriebung">Krankschriebung</h3>
<p>With most non-freelancer jobs, if you get sick you’re allowed to take off work for a few
days, but on the third day you’re required to go to a <em>Hausarzt</em> and get
<em>krankgeschrieben</em> (“written off sick”). That process
itself is kind of annoying – most practices won’t give them out over the phone, so you
usually have to go in person while having a cold, just to get a piece of paper saying you
have a cold. At least it’s better than being forced to work while sick. There is
no concept of a fixed amount of “sick days” here and Germans are appalled at the concept.</p>
<h3 id="_überweisungen_"><em>Überweisungen</em></h3>
<p>In the German healthcare system, the <em>Hausarzt</em> is the
gatekeeper for basically all your medical needs. Unless it’s an emergency, you usually
have to see the <em>Hausarzt</em> first before seeing any kind of specialist. The magical
document you need to see a specialist is the <em>Überweisung</em> (“referral”). This is why I
recommended in the previous section that you should really try to find a <em>Hausarzt</em> that’s
easy to book appointments with.</p>
<p>The setup can get a little frustrating at times, since you’re just making an appointment
to get permission to make a different appointment with someone else. It’s also a little
weird because, in theory, you can go to any <em>Hausarzt</em> to do this, not just your main one
(although you may get funny looks if you show up to a new practice with an unusual
request).</p>
<h3 id="rezepte">Rezepte</h3>
<p>A more familiar concept is that most medicine requires a <em>Rezept</em> (“prescription”)
from a doctor in order to pick it up at an <em>Apotheke</em> (“pharmacy”). Despite all the talk of
digitized patient records and “<em>E-Rezepte</em>,” Germany is still a very paper-based society
and you’re likely going to have to bring a stupid piece of paper with you every time you
need to pick up medication. Occasionally you can ask the doctor’s office to send it
directly to the <em>Apotheke</em> if it’s one nearby or you have their fax number handy.</p>
<h3 id="apotheke--over-the-counter">Apotheke / Over the counter</h3>
<p>What’s weird about the <em>Apotheken</em> is that they’re weird amalgams of prescription
medication depots, advisors for over-the-counter medication, and beauty product stores.
You can’t just <em>buy</em> NyQuil here, you have to go up to a person, do a little song and
dance, and hope that you’ve convinced them enough that you get the product that you want.
This goes a little smoother when you <em>don’t</em> know what product you want, as the
pharmacists are usually pretty well trained to help you find what you need. A word of
caution, however – Germans are very medication-averse, so don’t be surprised to get
“herbal” treatments when you expected the real stuff. Sometimes you can explicitly ask for
a medically proven active ingredient, but sometimes you may walk away empty-handed.</p>
<p>I am still astonished that homeopathic “treatments” are not only offered in pharmacies
here, but sometimes <em>actively recommended</em> by pharmacists and doctors! It may sound
like a perverse application of <a href="https://en.wikipedia.org/wiki/Godwin%27s_law">Godwin’s law</a>, but you can literally
<a href="https://quackwatch.org/hx/nazis/">blame the Nazis</a> for this one.</p>
<h3 id="mental-health">Mental Health</h3>
<p>One big failure in the German health system is how mental health is handled. Finding a
German-speaking therapist or psychiatrist is difficult enough, let alone an
English-speaking one.
Supposedly the problem is that the calculation for the number of practitioners
approved in an area by statutory insurance was set in the 90s and was never updated.
If you have statutory insurance and don’t want to wait a year to see a practitioner
(this is not an exaggeration), you’ll often have to just pay the full price out-of-pocket.</p>
<p>Private insurance has a much better track record in this regard, but that means mental
health services are in some ways reserved for the rich.</p>
<p>There is a process to get statutory
insurance to reimburse you for a private practitioner if you can prove that everybody in
your area is full. It’s a complicated process, but some practitioners might be able to
help you navigate it. You will have to pay out-of-pocket to get started, though.</p>
<p><em>The Berliner</em> has a good <a href="https://www.the-berliner.com/berlin/how-to-get-therapy-in-berlin-germany-mental-health/">overview of the whole situation</a> and how to
deal with it.</p>
<h2 id="food">Food</h2>
<p>The food system in Europe is in some ways pretty awesome, and in other ways quite
limiting.</p>
<h3 id="bread">Bread</h3>
<p>European bread is on another level. For my entire life, “bread” lasted for weeks in the
cupboard. It turns out, that’s what makes Wonderbread <em>wonder</em> bread – it has
preservatives in it to make it last far longer. Normal bread only keeps for a few days!</p>
<p>The demand for fresh bread has created a whole supply chain and distribution network
for fresh baked goods that easily outclasses that in the US. Gas stations and train
stations don’t have shrink-wrapped cheese danishes with year-long shelf lives. They have
pastries baked daily, and often freshly made little sandwiches with meat and cheese.
Cities themselves are dotted with bakeries, and often you’ll be within walking distance
of several of them.</p>
<p>That’s not to say that there are variations in quality – you can definitely taste the
difference between an industrially baked croissant and one made locally at the bakery
itself – but the average quality is much higher and the variance much lower.</p>
<p>What you gain in quality you pay for in choice – often you’ll only find the same
variation of the same few pastries or bread types. You’ll occasionally find a fun thing,
but a step into an American supermarket will remind you of the shocking emphasis they have
on having 10 different brands of peanut butter.</p>
<h3 id="cheese--dairy">Cheese / Dairy</h3>
<p>Like bread, cheese is one of Europe’s core competencies. The average supermarket will have
an intimidating cheese selection, both from pasteurized and unpasteurized milks of
various mammals.</p>
<p>If you’re a cheese eater it’s definitely worth trying out some random ones, being open to
the fact that some of them taste or smell downright terrible. I can’t stomach camembert or
brunost but there are a lot of good cheeses I’ve discovered while I’ve been here. Note
that unpasteurized cheese can come with some health risks, but if you’re not
immunocompromised or pregnant it’s usually ok.</p>
<p>What may be a bit unsettling is seeing unrefrigerated milk and eggs. Eggs are not
washed in Europe like they are in the States, which leaves on a protective coating that
lets them last at room temperature much longer. Refrigerating them just extends this
longer, and a close read of the label will show two expiration dates depending on if
they’re refrigerated or not. Similarly, ultrapasteurized milk is milk that stays good for
months at room temperature, but needs to be refrigerated once open.</p>
<h3 id="meat">Meat</h3>
<p>Meat in Europe is generally handled similarly to meat in the US, with a big exception:
pork is better handled here, and (with proper preparation) can be eaten undercooked or
even raw. I’ve eaten <em>Mett</em> (“pork tartare”) and lived to tell the tale, but it took some courage to do it.
It’s common to have cured but not cooked bacon for breakfast. Everyone keeps saying it’s
safe, but it takes a lot to undo an entire lifetime of avoiding uncooked pork.</p>
<h3 id="coffee">Coffee</h3>
<p>Coffee, as in the States, serves a dual purpose as “concentration fuel” and gourmet
experience. However, the latter is emphasized far more than the former. This means that
filter coffee from a carafe is readily available at gas stations and other places where
you’re just trying to get from point A to B, but quite rare to get at a restaurant.
Sometimes you’ll get lucky and a <em>Milchkaffee</em> (“milk coffee”) will be filter coffee with cream, but often
the only way to get anything approaching filter coffee is to admit defeat and order an
americano. I’ve adapted by ordering cappuccinos all the time.</p>
<p>The concept of unlimited filter coffee is foreign here, to the point that when a (sadly
now closed) American diner in Berlin offered it, it was a point of conversation for both
Americans and non-Americans alike. Living abroad means excitedly saying things like “they
had the coffee carafes, like from Denny’s, and the waiter just walks around filling up
your mug!”</p>
<h3 id="alcohol">Alcohol</h3>
<p>Yes, you really can drink beer on the street! The European attitude toward alcohol is far
more relaxed, and in my opinion more healthy. Because it’s everywhere and not strictly
controlled and locked down, people binge drink less. That’s not to say that people don’t
binge drink (how else could you explain the existence of Schlager music?) but it’s far
more acceptable to socially drink in small quantities. I think the stronger public transit
options and weaker Puritan influences drive the shocking (to Americans) attitude toward alcohol here.</p>
<p>Fun fact: the drinking age is 21 in the US because the National Highway Board found
that drunk driving accidents fall off sharply after 21 years of age, and <a href="https://en.wikipedia.org/wiki/National_Minimum_Drinking_Age_Act">therefore states
don’t get federal highway funding</a> if their legal drinking age is under 21. Sucks if you don’t drive, I guess.</p>
<h4 id="wine">Wine</h4>
<p>I’m not a knowledgeable wine drinker, but wine is both cheaper and of higher quality in
Europe than what I’ve seen in the US. Enjoy!</p>
<h2 id="navigation">Navigation</h2>
<p>From here you can go on to the <a href="https://wickedchicken.github.io/post/programmer-in-berlin-language/">language</a> section or back to the
<a href="https://wickedchicken.github.io/post/programmer-in-berlin-finances/">finances</a> section.</p>

 </div>
    


  
  



</article></div>]]></description>
        </item>
    </channel>
</rss>