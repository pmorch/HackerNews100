<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 27 Sep 2023 11:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[I got robbed of my first kernel contribution (147 pts)]]></title>
            <link>https://ariel-miculas.github.io/How-I-got-robbed-of-my-first-kernel-contribution/</link>
            <guid>37671991</guid>
            <pubDate>Wed, 27 Sep 2023 08:58:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ariel-miculas.github.io/How-I-got-robbed-of-my-first-kernel-contribution/">https://ariel-miculas.github.io/How-I-got-robbed-of-my-first-kernel-contribution/</a>, See on <a href="https://news.ycombinator.com/item?id=37671991">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <h3 id="context">Context</h3>
<p>Around a year and a half ago, I’ve asked my former company for some time to
work on an issue that was impacting the debugging capabilities in our project:
gdbserver couldn’t debug multithreaded applications running on a PowerPC32
architecture.  The connection to the gdbserver was broken and it couldn’t
control the debug session anymore. Multiple people have already investigated
this problem and I had a good starting point, but we still weren’t sure in
which software component the issue lied: it could have been the toolchain, the
gdbserver, the Linux kernel or the custom patches we applied on top of the
kernel tree. We were quite far away from finding the root cause.</p>

<h3 id="investigating-the-issue">Investigating the issue</h3>
<p>After diving into the existing analysis for this issue and channeling my
google-fu, I’ve had my first breakthrough: an <a href="https://lore.kernel.org/linuxppc-dev/dc38afe9-6b78-f3f5-666b-986939e40fc6@keymile.com/">email
thread</a>
which not only described the same symptoms as our issue, but also pointed to
the <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v6.6-rc3&amp;id=0c8c0f03e3a292e031596484275c14cf39c0ab7a">exact
commit</a>
which introduced it. The patch that introduced the bug moved the definition of
<code>thread_struct thread</code> from the middle of the <code>task_struct</code> to the end, a
seeminlgy innocuous change.</p>

<p>After debugging the issue, this is what Holger Brunck
<a href="https://lore.kernel.org/linuxppc-dev/e5cbd015-eeb5-31b5-0829-14cc8500dc6d@keymile.com/">observed</a></p>
<blockquote>
  <p>What I see is that gdbserver sends for each thread a SIGSTOP to the kernel and
waits for a response. The kernel does receive all the signals but only respond
to some of them in the error case. Which then matches with my “ps” output as I
see that some threads are not in the state pthread_stop and then the gdbserver
gets suspended.</p>
</blockquote>

<p>The low-level issue was that after interacting with gdbserver, some threads
were in the wrong process state and gdbserver couldn’t control them anymore.</p>

<p>I’ve spent 3-4 days reading commit descriptions related to the PowerPC
architecture and the changes around <code>task_struct</code>, trying to figure out whether
this issue was solved in subsequent kernel versions (spoiler: it was not).
I’ve moved <code>thread_struct thread</code> around to determine when the issue reproduced
and used <a href="https://linux.die.net/man/1/pahole">pahole</a> to inspect
<code>task_struct</code>’s layout. I’ve used
<a href="https://www.kernel.org/doc/html/v5.0/trace/ftrace.html">ftrace</a> to figure out
when the threads of the debugged process were scheduled and that’s how I
realized this could be a memory corruption issue: the threads that were stuck
were only scheduled once, unlike the other ones. I’ve originally dismissed that
this could be a memory corruption issue because in the <a href="https://lore.kernel.org/linuxppc-dev/b78d9e5d-fc2e-3676-a47e-ed5ca7a836e6@keymile.com/">original
thread</a>
it was mentioned that:</p>
<blockquote>
  <p>the content of the buffer is always zero and does not change. So at least no
one is writing non-zero to the buffer.</p>
</blockquote>

<p>That’s what I get for not verifying that the structure isn’t overwritten with
zero bytes (always validate your assumptions).</p>

<p>I remembered that the x86 architecture has <a href="https://en.wikipedia.org/wiki/X86_debug_register">debug
registers</a> that could be used
to trigger data write breakpoints. In fact, this is how I solved a bug back in
my earlier days as a software engineer. Sure enough, PowerPC also implements a
similar capability with the help of the <a href="https://stackoverflow.com/a/327540">DABR register</a>.</p>

<p>I’ve investigated how I could use hardware breakpoints on Linux and I ended up
implementing a linux kernel module based on this <a href="https://stackoverflow.com/a/19755213">excellent stackoverflow
answer</a>. This allowed me to place a
hardware breakpoint on the <a href="https://elixir.bootlin.com/linux/v6.5.5/source/include/linux/sched.h#L746">__state
field</a>
to figure out who on earth writes to it.</p>

<h3 id="finding-the-bug">Finding the bug</h3>
<p>And that’s how I found the issue: my custom kernel module showed the stack
traces from the places where the <code>__state</code> field of <code>task_struct</code> was being
written to.  I’ve noticed an outlier which revealed a buffer overflow in
<code>ptrace_put_fpr</code> (used by the POKEUSER API). This led to important fields from
<code>task_struct</code> getting overwritten, such as <code>__state</code>, which stores the state of
the process and it’s also used by the kernel to keep track of which processes
are stopped by the debugger.</p>

<p>The cause of this overflow? Taking an index meant to be used with an array of
32-bit elements and indexing an array of 64-bit elements. There were 64 indexes
that addressed the FPR, so the total addressable memory was 64 * 8 = 512
bytes. But there were only 32 entries in the fp_state.fpr array, which means
that the available memory was only 32 * 8 = 256 bytes. That allowed the user
(aka gdbserver) to write up to 256 bytes past the end of the array.
<img src="https://ariel-miculas.github.io/images/fpr-overflow.png" alt="fpr-overflow"></p>

<h3 id="sending-the-patch-upstream">Sending the patch upstream</h3>
<p>I’ve sent a patch to the Linux kernel security team (security@kernel.org)
because I wanted to err on the safe side: a memory corruption issue that could
overwrite the memory of the processes’s states could have security
implications. Unfortunately, this mailing list is private so I cannot link to
the original patch I sent.  Michael Ellerman, the PowerPC maintainer, followed
up and told me he will contact me in private to figure this issue out. I have
actually sent him two patches fixing the issue: the original one that I sent to
the security mailing list and <a href="https://lists.ozlabs.org/pipermail/linuxppc-dev/2022-June/244438.html">another
version</a>
(quite different from the first one) which addressed some suggestions received
in reply to my original submission. And the latter patch was actually based on
existing kernel code, which emulated PowerPC32 operations on PowerPC64 (yeah,
they got the FPR indexing right). Neither of those were accepted by Michael
Ellerman, and instead he implemented his <a href="https://lore.kernel.org/all/20220609133245.573565-1-mpe@ellerman.id.au/">own version of the
fix</a>.
I told him that I would really appreciate if he could accept a patch from me,
so that I could receive credit for fixing this issue and become a kernel
contributor. I was also open to working with him, addressing his feedback and
sending subsequent versions of patches. He said (paraphrasing):</p>
<blockquote>
  <p>Sorry, I like my version better. If you want to be a Linux kernel
contributor, here’s an issue you could fix.</p>
</blockquote>

<p>I found this really perplexing and insulting. Instead of getting recognized for
fixing the issue, he wanted to give me more work to do. My company and I should
have received proper credit for solving this issue, especially considering how
much effort we put into it.</p>

<p>I felt it was really unfair to only get a “Reported-by” tag. Here’s the
<a href="https://docs.kernel.org/process/submitting-patches.html#using-reported-by-tested-by-reviewed-by-suggested-by-and-fixes">purpose of the tag</a>:</p>

<blockquote>
  <p>The Reported-by tag gives credit to people who find bugs and report them and it hopefully inspires them to help us again in the future.</p>
</blockquote>

<p>Well, I certainly didn’t feel inspired to get involved with the kernel
community again. On the contrary, I felt belittled and angry that my work
wasn’t properly recognized.</p>

<h3 id="conclusion">Conclusion</h3>
<p>I spent a lot of time and effort doing root cause analysis, fixing the bug,
testing and validating the fix, getting feedback from other engineers at my
company, adapting the fix to the latest kernel version, and sending two
different patches to Michael Ellerman, the PowerPC maintainer. Instead of
accepting my patch or guiding me towards a better solution, he went ahead and
implemented his own fix, giving me credit only for reporting the issue (which
was <a href="https://lore.kernel.org/linuxppc-dev/dc38afe9-6b78-f3f5-666b-986939e40fc6@keymile.com/">already
reported</a>
six years prior to this).</p>

<p>My first contribution to the kernel was a really frustrating and discouraging
experience, dealing with people who do not think it’s important to get proper
recognition for your work.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Arena allocator tips and tricks (130 pts)]]></title>
            <link>https://nullprogram.com/blog/2023/09/27/</link>
            <guid>37670740</guid>
            <pubDate>Wed, 27 Sep 2023 05:59:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nullprogram.com/blog/2023/09/27/">https://nullprogram.com/blog/2023/09/27/</a>, See on <a href="https://news.ycombinator.com/item?id=37670740">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en">
<article>
  
  <time datetime="2023-09-27">
    September 27, 2023
  </time>
  <p>
    nullprogram.com/blog/2023/09/27/
  </p>

  <p>Over the past year I’ve refined my approach to <a href="https://www.rfleury.com/p/untangling-lifetimes-the-arena-allocator">arena allocation</a>.
With practice, it’s effective, simple, and fast; typically as easy to use
as garbage collection but without the costs. Depending on need, an
allocator can weigh just 7–25 lines of code — perfect when <a href="https://nullprogram.com/blog/2023/02/15/">lacking a
runtime</a>. With the core details of my own technique settled, now is a
good time to document and share lessons learned. This is certainly not the
only way to approach arena allocation, but these are practices I’ve worked
out to simplify programs and reduce mistakes.</p>

<p>An arena is a memory buffer and an offset into that buffer, initially
zero. To allocate an object, grab a pointer at the offset, advance the
offset by the size of the object, and return the pointer. There’s a little
more to it, such as ensuring alignment and availability. We’ll get to
that. Objects are not freed individually. Instead, groups of allocations
are freed at once by restoring the offset to an earlier value. Without
individual lifetimes, you don’t need to write destructors, nor do your
programs need to walk data structures at run time to take them apart. You
also no longer need to worry about memory leaks.</p>

<p>A minority of programs inherently require general purpose allocation, at
least in part, that linear allocation cannot fulfill. This includes, for
example, most programming language runtimes. If you like arenas, avoid
accidentally create such a situation through an over-flexible API that
allows callers to assume you have general purpose allocation underneath.</p>

<p>To get warmed up, here’s my style of arena allocation in action that shows
off multiple features:</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>{</span>
    <span>uint8_t</span>  <span>*</span><span>data</span>
    <span>ptrdiff_t</span> <span>len</span><span>;</span>
<span>}</span> <span>str</span><span>;</span>

<span>typedef</span> <span>struct</span> <span>{</span>
    <span>strlist</span> <span>*</span><span>next</span><span>;</span>
    <span>str</span>      <span>item</span><span>;</span>
<span>}</span> <span>strlist</span><span>;</span>

<span>typedef</span> <span>struct</span> <span>{</span>
    <span>str</span> <span>head</span><span>;</span>
    <span>str</span> <span>tail</span><span>;</span>
<span>}</span> <span>strpair</span><span>;</span>

<span>// Defined elsewhere</span>
<span>void</span>    <span>towidechar</span><span>(</span><span>wchar_t</span> <span>*</span><span>,</span> <span>ptrdiff_t</span><span>,</span> <span>str</span><span>);</span>
<span>str</span>     <span>loadfile</span><span>(</span><span>wchar_t</span> <span>*</span><span>,</span> <span>arena</span> <span>*</span><span>);</span>
<span>strpair</span> <span>cut</span><span>(</span><span>str</span><span>,</span> <span>uint8_t</span><span>);</span>

<span>strlist</span> <span>*</span><span>getlines</span><span>(</span><span>str</span> <span>path</span><span>,</span> <span>arena</span> <span>*</span><span>perm</span><span>,</span> <span>arena</span> <span>scratch</span><span>)</span>
<span>{</span>
    <span>int</span> <span>max_path</span> <span>=</span> <span>1</span><span>&lt;&lt;</span><span>15</span><span>;</span>
    <span>wchar_t</span> <span>*</span><span>wpath</span> <span>=</span> <span>new</span><span>(</span><span>&amp;</span><span>scratch</span><span>,</span> <span>wchar_t</span><span>,</span> <span>max_path</span><span>);</span>
    <span>towidechar</span><span>(</span><span>wpath</span><span>,</span> <span>max_path</span><span>,</span> <span>path</span><span>);</span>

    <span>strpair</span> <span>pair</span> <span>=</span> <span>{</span><span>0</span><span>};</span>
    <span>pair</span><span>.</span><span>tail</span> <span>=</span> <span>loadfile</span><span>(</span><span>wpath</span><span>,</span> <span>perm</span><span>);</span>

    <span>strlist</span> <span>*</span><span>head</span> <span>=</span> <span>0</span><span>;</span>
    <span>strlist</span> <span>**</span><span>tail</span> <span>=</span> <span>&amp;</span><span>head</span><span>;</span>
    <span>while</span> <span>(</span><span>pair</span><span>.</span><span>tail</span><span>.</span><span>len</span><span>)</span> <span>{</span>
        <span>pair</span> <span>=</span> <span>cut</span><span>(</span><span>pair</span><span>.</span><span>tail</span><span>,</span> <span>'\n'</span><span>);</span>
        <span>*</span><span>tail</span> <span>=</span> <span>new</span><span>(</span><span>perm</span><span>,</span> <span>strlist</span><span>,</span> <span>1</span><span>);</span>
        <span>(</span><span>*</span><span>tail</span><span>)</span><span>-&gt;</span><span>item</span> <span>=</span> <span>pair</span><span>.</span><span>head</span><span>;</span>
        <span>tail</span> <span>=</span> <span>&amp;</span><span>(</span><span>*</span><span>tail</span><span>)</span><span>-&gt;</span><span>next</span><span>;</span>
    <span>}</span>
    <span>return</span> <span>head</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Take note of these details, each to be later discussed in detail:</p>

<ul>
  <li>
    <p><code>getlines</code> takes two arenas, “permanent” and “scratch”. The former is
for objects that will be returned to the caller. The latter is for
temporary objects whose lifetime ends when the function returns. They
have stack lifetimes just like local variables.</p>
  </li>
  <li>
    <p>Objects are not explicitly freed. Instead, <strong>all allocations from a
scratch arena are implicitly freed upon return</strong>. This would include
error return paths automatically.</p>
  </li>
  <li>
    <p>The <strong>scratch arena is passed by copy</strong> — i.e. a copy of the “header”
not the <em>memory region</em> itself. Allocating only changes the local copy,
and so cannot survive the return. The semantics are obvious to callers,
so they’re less likely to get mixed up.</p>
  </li>
  <li>
    <p>While <code>wpath</code> could be an automatic local variable, it’s relatively
large for the stack, so it’s allocated out of the scratch arena. A
scratch arena safely permits large, dynamic allocations that would never
be safe on the stack. In other words, <strong>a sane <a href="https://man7.org/linux/man-pages/man3/alloca.3.html"><code>alloca</code></a>!</strong>
Same for variable-length arrays (VLAs). A scratch arena means you’ll
never be tempted to use either of these terrible ideas.</p>
  </li>
  <li>
    <p>The second parameter to <code>new</code> is a type, so it’s obviously a macro. As
you will see momentarily, this is not some complex macro magic, just a
convenience one-liner. There is no implicit cast, and you will get a
compiler diagnostic if the type is incorrect.</p>
  </li>
  <li>
    <p>Despite all the allocation, there is not a single <code>sizeof</code> operator nor
size computation. That’s because <strong>size computations are a major source
of defects.</strong> That job is handled by specialized code.</p>
  </li>
  <li>
    <p><strong>Allocation failures are not communicated by a null return</strong>. Lifting
this burden greatly simplifies programs. Instead such errors are handled
non-locally by the arena.</p>
  </li>
  <li>
    <p>All allocations are <strong>zero-initialized by default</strong>. This makes for
simpler, less error-prone programs. When that’s too expensive, this can
become an opt-out without changing the default.</p>
  </li>
</ul>

<p>See also <a href="https://nullprogram.com/blog/2023/01/18/">u-config</a>.</p>

<h3 id="an-arena-implementation">An arena implementation</h3>

<p>An arena suitable for most cases can be this simple:</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>{</span>
    <span>char</span> <span>*</span><span>beg</span><span>;</span>
    <span>char</span> <span>*</span><span>end</span><span>;</span>
<span>}</span> <span>arena</span><span>;</span>

<span>void</span> <span>*</span><span>alloc</span><span>(</span><span>arena</span> <span>*</span><span>a</span><span>,</span> <span>ptrdiff_t</span> <span>size</span><span>,</span> <span>ptrdiff_t</span> <span>align</span><span>,</span> <span>ptrdiff_t</span> <span>count</span><span>)</span>
<span>{</span>
    <span>ptrdiff_t</span> <span>avail</span> <span>=</span> <span>a</span><span>-&gt;</span><span>end</span> <span>-</span> <span>a</span><span>-&gt;</span><span>beg</span><span>;</span>
    <span>ptrdiff_t</span> <span>padding</span> <span>=</span> <span>-</span><span>(</span><span>uintptr_t</span><span>)</span><span>a</span><span>-&gt;</span><span>beg</span> <span>&amp;</span> <span>(</span><span>align</span> <span>-</span> <span>1</span><span>);</span>
    <span>if</span> <span>(</span><span>count</span> <span>&gt;</span> <span>(</span><span>avail</span> <span>-</span> <span>padding</span><span>)</span><span>/</span><span>size</span><span>)</span> <span>{</span>
        <span>abort</span><span>();</span>  <span>// one possible out-of-memory policy</span>
    <span>}</span>
    <span>ptrdiff_t</span> <span>total</span> <span>=</span> <span>size</span> <span>*</span> <span>count</span><span>;</span>
    <span>char</span> <span>*</span><span>p</span> <span>=</span> <span>a</span><span>-&gt;</span><span>beg</span> <span>+</span> <span>padding</span><span>;</span>
    <span>a</span><span>-&gt;</span><span>beg</span> <span>+=</span> <span>padding</span> <span>+</span> <span>total</span><span>;</span>
    <span>return</span> <span>memset</span><span>(</span><span>p</span><span>,</span> <span>0</span><span>,</span> <span>total</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>Yup, just a pair of pointers! When allocating, all sizes are signed <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1428r0.pdf">just
as they ought to be</a>. Unsigned sizes are another historically
common source of defects, and offer no practical advantages in return.
Case in point exercise for the reader: Change each <code>ptrdiff_t</code> to <code>size_t</code>
in <code>alloc</code>, find the defect that results, then fix it.</p>

<p>The <code>align</code> parameter allows the arena to handle any unusual alignments,
something that’s surprisingly difficult to do with libc. It’s difficult to
appreciate its usefulness until it’s convenient.</p>

<p>The <code>uintptr_t</code> business may look unusual if you’ve never come across it
before. To align <code>beg</code>, we need to compute the number of bytes to advance
the address (<code>padding</code>) until the alignment evenly divides the address.
The modulo with <code>align</code> computes the number of bytes it’s since the last
alignment:</p>



<p>We can’t operate numerically on an address like this, so in the code we
first convert to <code>uintptr_t</code>. Alignment is always a power of two, which
notably excludes zero, so no worrying about division by zero. That also
means we can compute modulo by subtracting one and masking with AND:</p>

<div><pre><code>extra = addr &amp; (align - 1)
</code></pre></div>

<p>However, we want the number of bytes to advance to the next alignment,
which is the inverse:</p>

<div><pre><code>padding = -addr &amp; (align - 1)
</code></pre></div>

<p>Add the <code>uintptr_t</code> cast and you have the code in <code>alloc</code>.</p>

<p>The <code>if</code> tests if there’s enough memory and simultaneously for overflow on
<code>size*count</code>. If either fails, it invokes the out-of-memory policy, which
in this case is <code>abort</code>. I strongly recommend that, at least when testing,
always having <em>something</em> in place to, at minimum, abort when allocation
fails, even when you think it cannot happen. It’s easy to use more memory
than you anticipate, and you want a reliable signal when it happens.</p>

<p>An alternative policy is to <a href="https://nullprogram.com/blog/2023/02/12/">longjmp to a “handler”</a>, which with
GCC and Clang doesn’t even require runtime support. In that case add a
<code>jmp_buf</code> to the arena:</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>{</span>
    <span>char</span>  <span>*</span><span>beg</span><span>;</span>
    <span>char</span>  <span>*</span><span>end</span><span>;</span>
    <span>void</span> <span>**</span><span>jmp_buf</span><span>;</span>
<span>}</span> <span>arena</span><span>;</span>

<span>void</span> <span>*</span><span>alloc</span><span>(...)</span>
<span>{</span>
    <span>// ...</span>
    <span>if</span> <span>(</span><span>count</span> <span>&gt;</span> <span>(</span><span>avail</span> <span>-</span> <span>padding</span><span>)</span><span>/</span><span>size</span><span>)</span> <span>{</span>
        <span>__builtin_longjmp</span><span>(</span><span>a</span><span>-&gt;</span><span>jmp_buf</span><span>,</span> <span>1</span><span>);</span>
    <span>}</span>
    <span>// ...</span>
<span>}</span>

<span>bool</span> <span>example</span><span>(...,</span> <span>arena</span> <span>scratch</span><span>)</span>
<span>{</span>
    <span>void</span> <span>*</span><span>jmp_buf</span><span>[</span><span>5</span><span>];</span>
    <span>if</span> <span>(</span><span>__builtin_setjmp</span><span>(</span><span>jmp_buf</span><span>))</span> <span>{</span>
        <span>return</span> <span>0</span><span>;</span>
    <span>}</span>
    <span>scratch</span><span>.</span><span>jmp_buf</span> <span>=</span> <span>jmp_buf</span><span>;</span>
    <span>// ...</span>
    <span>return</span> <span>1</span><span>;</span>
<span>}</span>
</code></pre></div>

<p><code>example</code> returns failure to the caller if it runs out of memory, without
needing to check individual allocations and, thanks to the implicit free
of scratch arenas, without needing to clean up. If callees receiving the
scratch arena don’t set their own <code>jmp_buf</code>, they’ll return here, too. In
a real program you’d probably wrap the <code>setjmp</code> setup in a macro.</p>

<p>Suppose zeroing is too expensive or unnecessary in some cases. Add a flag
to opt out:</p>

<div><pre><code><span>void</span> <span>*</span><span>alloc</span><span>(...,</span> <span>int</span> <span>flags</span><span>)</span>
<span>{</span>
    <span>// ...</span>
    <span>return</span> <span>flag</span><span>&amp;</span><span>NOZERO</span> <span>?</span> <span>p</span> <span>:</span> <span>memset</span><span>(</span><span>p</span><span>,</span> <span>0</span><span>,</span> <span>total</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>Similarly, perhaps there’s a critical moment where you’re holding a
non-memory resource (lock, file handle), or you don’t want allocation
failure to be fatal. In either case, it important that the out-of-memory
policy isn’t invoked. You could request a “soft” failure with another
flag, and then do the usual null pointer check:</p>

<div><pre><code><span>void</span> <span>*</span><span>alloc</span><span>(...,</span> <span>int</span> <span>flags</span><span>)</span>
<span>{</span>
    <span>// ...</span>
    <span>if</span> <span>(</span><span>count</span> <span>&gt;</span> <span>(</span><span>avail</span> <span>-</span> <span>padding</span><span>)</span><span>/</span><span>size</span><span>)</span> <span>{</span>
        <span>if</span> <span>(</span><span>flags</span> <span>&amp;</span> <span>SOFTFAIL</span><span>)</span> <span>{</span>
            <span>return</span> <span>0</span><span>;</span>
        <span>}</span>
        <span>abort</span><span>();</span>
    <span>}</span>
    <span>// ...</span>
<span>}</span>
</code></pre></div>

<p>Most non-trivial programs will probably at least one of these flags.</p>

<p>In case it wasn’t obvious, allocating an arena is simple:</p>

<div><pre><code><span>arena</span> <span>newarena</span><span>(</span><span>ptrdiff_t</span> <span>cap</span><span>)</span>
<span>{</span>
    <span>arena</span> <span>a</span> <span>=</span> <span>{</span><span>0</span><span>};</span>
    <span>a</span><span>.</span><span>beg</span> <span>=</span> <span>malloc</span><span>(</span><span>cap</span><span>);</span>
    <span>a</span><span>.</span><span>end</span> <span>=</span> <span>a</span><span>.</span><span>beg</span> <span>?</span> <span>a</span><span>.</span><span>beg</span><span>+</span><span>cap</span> <span>:</span> <span>0</span><span>;</span>
    <span>return</span> <span>a</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Or make a direct allocation from the operating system, e.g. <code>mmap</code>,
<code>VirtualAlloc</code>. Typically arena lifetime is the whole program, so you
don’t need to worry about freeing it. (Since you’re using arenas, you can
also turn off any memory leak checkers while you’re at it.)</p>

<p>If you need more arenas then you can always allocate smaller ones out of
the first! In multi-threaded applications, each thread may have at least
its own scratch arena.</p>

<h3 id="the-new-macro">The <code>new</code> macro</h3>

<p>I’ve shown <code>alloc</code>, but few parts of the program should be calling it
directly. Instead they have a macro to automatically handle the details. I
call mine <code>new</code>, though of course if you’re writing C++ you’ll need to
pick another name (<code>make</code>? <code>PushStruct</code>?):</p>

<div><pre><code><span>#define new(a, t, n)  (t *)alloc(a, sizeof(t), _Alignof(t), n)
</span></code></pre></div>

<p>The cast is an extra compile-time check, especially useful for avoiding
mistakes in levels of indirection. It also keeps normal code from directly
using the <code>sizeof</code> operator, which is easy to misuse. If you added a
<code>flags</code> parameter, pass in zero for this common case. Keep in mind that
the goal of this macro is to make common allocation simple and robust.</p>

<p>Often you’ll allocate single objects, and so the count is 1. If you think
that’s ugly, you could make variadic version of <code>new</code> that fills in common
defaults. In fact, that’s partly why I put <code>count</code> last!</p>

<div><pre><code><span>#define new(...)            newx(__VA_ARGS__,new4,new3,new2)(__VA_ARGS__)
#define newx(a,b,c,d,e,...) e
#define new2(a, t)          (t *)alloc(a, sizeof(t), alignof(t), 1, 0)
#define new3(a, t, n)       (t *)alloc(a, sizeof(t), alignof(t), n, 0)
#define new4(a, t, n, f)    (t *)alloc(a, sizeof(t), alignof(t), n, f)
</span></code></pre></div>

<p>Not quite so simple, but it optionally makes for more streamlined code:</p>

<div><pre><code><span>thing</span> <span>*</span><span>t</span>   <span>=</span> <span>new</span><span>(</span><span>perm</span><span>,</span> <span>thing</span><span>);</span>
<span>thing</span> <span>*</span><span>ts</span>  <span>=</span> <span>new</span><span>(</span><span>perm</span><span>,</span> <span>thing</span><span>,</span> <span>1000</span><span>);</span>
<span>char</span>  <span>*</span><span>buf</span> <span>=</span> <span>new</span><span>(</span><span>perm</span><span>,</span> <span>char</span><span>,</span> <span>len</span><span>,</span> <span>NOZERO</span><span>);</span>
</code></pre></div>

<p>Side note: If <code>sizeof</code> should be avoided, what about array lengths? That’s
part of the problem! Hardly ever do you want the <em>size</em> of an array, but
rather the <em>number of elements</em>. That includes <code>char</code> arrays where this
happens to be the same number. So instead, define a <code>countof</code> macro that
uses <code>sizeof</code> to compute the value you actually want. I like to have this
whole collection:</p>

<div><pre><code><span>#define sizeof(x)    (ptrdiff_t)sizeof(x)
#define countof(a)   (sizeof(a) / sizeof(*(a)))
#define lengthof(s)  (countof(s) - 1)
</span></code></pre></div>

<p>Yes, you can convert <code>sizeof</code> into a macro like this! It won’t expand
recursively and bottoms out as an operator. <code>countof</code> also, of course,
produces a less error-prone signed count so users don’t fumble around with
<code>size_t</code>. <code>lengthof</code> statically produces null-terminated string length.</p>

<div><pre><code><span>char</span> <span>msg</span><span>[]</span> <span>=</span> <span>"hello world"</span><span>;</span>
<span>write</span><span>(</span><span>fd</span><span>,</span> <span>msg</span><span>,</span> <span>lengthof</span><span>(</span><span>msg</span><span>));</span>

<span>#define MSG "hello world"
</span><span>write</span><span>(</span><span>fd</span><span>,</span> <span>MSG</span><span>,</span> <span>lengthof</span><span>(</span><span>MSG</span><span>));</span>
</code></pre></div>

<h3 id="enhance-alloc-with-attributes">Enhance <code>alloc</code> with attributes</h3>

<p>At least for GCC and Clang, we can further improve <code>alloc</code> with three
function attributes:</p>

<div><pre><code><span>__attribute</span><span>((</span><span>malloc</span><span>,</span> <span>alloc_size</span><span>(</span><span>2</span><span>,</span> <span>4</span><span>),</span> <span>alloc_align</span><span>(</span><span>3</span><span>)))</span>
<span>void</span> <span>*</span><span>alloc</span><span>(...);</span>
</code></pre></div>

<p><code>malloc</code> indicates that the pointer returned by <code>alloc</code> does not alias any
existing object. Enables some significant optimizations that are otherwise
blocked, most often by breaking potential loop-carried dependencies.</p>

<p><code>alloc_size</code> tracks the allocation size for compile-time diagnostics and
run-time assertions (<a href="https://gcc.gnu.org/onlinedocs/gcc/Object-Size-Checking.html"><code>__builtin_object_size</code></a>). This generally
requires a non-zero optimization level. In other words, you will get a
compiler warnings about some out bounds accesses of arena objects, and
with Undefined Behavior Sanitizer you’ll get run-time bounds checking.
It’s a great <a href="https://nullprogram.com/blog/2019/01/25/">complement to fuzzing</a>.</p>

<p>In theory <code>alloc_align</code> may also allow better code generation, but I’ve
yet to observe a case. Consider it optional and low-priority. I mention it
only for completeness.</p>

<h3 id="arena-size-and-growth">Arena size and growth</h3>

<p>How large an arena should you allocate? The simple answer: As much as is
necessary for the program to successfully complete. Usually the cost of
untouched arena memory is low or even zero. Most programs should probably
have an upper limit, at which point they assume something has gone wrong.
Arenas allow this case to be handled gracefully, simplifying recovery and
paving the way for continued operation.</p>

<p>While a sufficient answer for most cases, it’s unsatisfying. There’s a
common assumption that programs should increase their memory usage as much
as needed and let the operating system respond if it’s too much. However,
if you’ve ever tried this yourself, you probably noticed that mainstream
operating systems don’t handle it well. The typical results are system
instability — thrashing, drivers crashing — possibly necessitating a
reboot.</p>

<p>If you insist on this route, on 64-bit hosts you can reserve a gigantic
virtual address space and gradually commit memory as needed. On Linux that
means leaning on overcommit by allocating the largest arena possible at
startup, which will automatically commit through use. <a href="https://nullprogram.com/blog/2019/12/29/">Use <code>MADV_FREE</code> to
decommit.</a></p>

<p>On Windows, <code>VirtualAlloc</code> handles reserve and commit separately. In
addition to the allocation offset, you need a commit offset. Then expand
the committed region ahead of the allocation offset as it grows. If you
ever manually reset the allocation offset, you could decommit as well, or
at least <code>MEM_RESET</code>. At some point commit may fail, which should then
trigger the out-of-memory policy, but the system is probably in poor shape
by that point — i.e. use an abort policy to release it all quickly.</p>

<h3 id="pointer-laundering-filthy-hack">Pointer laundering (filthy hack)</h3>

<p>While allocations out of an arena don’t require individual error checks,
allocating the arena itself at startup requires error handling. It would
be nice if the arena could be allocated out of <code>.bss</code> and punt that job to
the loader. While you <em>could</em> make a big, global <code>char[]</code> array to back
your arena, it’s technically not permitted (strict aliasing). A “clean”
<code>.bss</code> region could be obtained with a bit of assembly — <a href="https://sourceware.org/binutils/docs/as/Comm.html"><code>.comm</code></a>
plus assembly to get the address into C without involving an array. I
wanted a more portable solution, so I came up with this:</p>

<div><pre><code><span>arena</span> <span>getarena</span><span>(</span><span>void</span><span>)</span>
<span>{</span>
    <span>static</span> <span>char</span> <span>mem</span><span>[</span><span>1</span><span>&lt;&lt;</span><span>28</span><span>];</span>
    <span>arena</span> <span>r</span> <span>=</span> <span>{</span><span>0</span><span>};</span>
    <span>r</span><span>.</span><span>beg</span> <span>=</span> <span>mem</span><span>;</span>
    <span>asm</span> <span>(</span><span>""</span> <span>:</span> <span>"+r"</span><span>(</span><span>r</span><span>.</span><span>beg</span><span>));</span>  <span>// launder the pointer</span>
    <span>r</span><span>.</span><span>end</span> <span>=</span> <span>r</span><span>.</span><span>beg</span> <span>+</span> <span>countof</span><span>(</span><span>mem</span><span>);</span>
    <span>return</span> <span>r</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>The <code>asm</code> accepts a pointer and returns a pointer (<code>"+r"</code>). The compiler
cannot “see” that it’s actually empty, and so returns the same pointer.
The arena will be backed by <code>mem</code>, but by laundering the address through
<code>asm</code>, I’ve disconnected the pointer from its origin. As far the compiler
is concerned, this is some foreign, assembly-provided pointer, not a
pointer into <code>mem</code>. It can’t optimize away <code>mem</code> because it’s been given
to a mysterious assembly black box.</p>

<p>While inappropriate for a real project, I think it’s a neat trick.</p>

<h3 id="arena-friendly-container-data-structures">Arena-friendly container data structures</h3>

<p>In my initial example I used a linked list to stores lines. This data
structure is great with arenas. It only takes a few of lines of code to
implement a linked list on top of an arena, and no “destroy” code is
needed. Simple.</p>

<p>What about <a href="https://nrk.neocities.org/articles/hash-trees-and-tries">arena-backed associative arrays</a>? Or arena-backed
dynamic arrays? I have simple, fast, easy solutions for each, but that’s
the subject for my next article!</p>



  
  <ol></ol>

  

  <nav>
  
    
  
  
  </nav>
</article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[China is flooding Taiwan with disinformation (175 pts)]]></title>
            <link>https://www.economist.com/asia/2023/09/26/china-is-flooding-taiwan-with-disinformation</link>
            <guid>37667874</guid>
            <pubDate>Wed, 27 Sep 2023 00:18:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/asia/2023/09/26/china-is-flooding-taiwan-with-disinformation">https://www.economist.com/asia/2023/09/26/china-is-flooding-taiwan-with-disinformation</a>, See on <a href="https://news.ycombinator.com/item?id=37667874">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p><span><a href="https://www.economist.com/asia/" data-analytics="sidebar:section"><span>Asia</span></a></span><span> | <!-- -->Strait up lies</span></p><h2>With elections looming, China wants Taiwanese voters to think America is their greatest threat</h2></section><div data-body-id="cp2"><p data-component="paragraph"><span data-caps="initial">I</span><small>n July one</small> of Taiwan’s top newspapers, <i>United Daily News</i>, published a story based on supposedly leaked minutes from a secret government meeting. America had asked <a href="https://www.economist.com/china/2023/06/19/when-it-comes-to-a-war-with-taiwan-many-chinese-urge-caution">Taiwan</a> to manufacture biological weapons at a lab run by the island’s defence ministry, the report claimed. Taiwanese and American officials quickly denied it. The allegedly leaked minutes, it transpired, were not written in the usual style of Taiwanese government records. They were filled with official-sounding phrases used in mainland China, but not Taiwan. This was likely Chinese disinformation, Taiwanese officials said. Yet the story spread to Taiwanese talk shows and influencers. Within weeks it had evolved into a wilder claim: Taiwan was going to collect 150,000 samples of Taiwanese blood and hand them over to the Americans, so that they could develop a virus to kill Chinese people.</p><p data-component="paragraph">This sort of disinformation is so widespread in Taiwan that analysts have given it a moniker: <i>yi mei lun</i>, or the “<small>US</small> scepticism” narrative. Its spread is becoming a major worry for Taiwan’s government and civil society in the run-up to a hugely important presidential election next January. Taiwanese voters will in effect be asked to decide whether Taiwan should remain aligned with America in strengthening deterrence against a possible Chinese invasion, or should move towards building ties with China. The opposition Kuomintang has called the vote a choice between “war and peace”, implying that the ruling Democratic Progressive Party’s hostility towards China will provoke it to attack. Chinese state actors have backed that framing, spreading narratives that portray <a href="https://www.economist.com/briefing/2023/03/09/america-and-china-are-preparing-for-a-war-over-taiwan">America</a>, not China, as the island’s biggest threat.  Much of the disinformation is intended to reinforce that false message.</p><p data-component="paragraph">Lo Ping-chen, a cabinet minister who since 2018 has been leading a government task force against disinformation, says it has “severely infiltrated” Taiwan’s society. “We used to think there was more during election season. But it’s now become normalised. It happens every day.” Most Taiwanese voters have little idea of this. A recent survey by Doublethink Lab, a Taiwanese group that studies disinformation, found that less than 20% of respondents believed the false information spread in Taiwan during elections came from abroad. Puma Shen, who heads Doublethink Lab, worries about the one-fifth of voters who are not aligned with any party and could be a decisive bloc. “Even if only 15% of voters are truly affected by Chinese disinformation, it takes only 7% of voters to change the election results,” he says.</p><p data-component="paragraph">A recent study of <small>US</small>-scepticism narratives by the Information Environment Research Centre (<small>IORG</small>), a Taiwanese research group, found that Chinese actors were helping to spread most of them. But more than half appeared to have Taiwanese origins. That suggests China is “piggybacking” on fissures in Taiwanese society, says Chihhao Yu, the report’s author. He suggests many Taiwanese have an “orphan mentality”: they fear abandonment by outsiders because of Taiwan’s experience of losing American diplomatic recognition in the 1970s.</p><p data-component="paragraph">Chinese actors are exploiting those fears, just as Russian disinformation exploited America’s racial and cultural cracks for the benefit of Donald Trump in 2016. Chinese disinformation in Taiwan also echoes Russian propaganda about the war in Ukraine, which claims America is behind the conflict (and is creating bioweapons in Ukrainian labs).</p><p data-component="paragraph">China has developed systematic means to make falsehoods trend in Taiwan, says Chien Yu-yen, a former journalist and author of a book about Chinese influence on Taiwan’s media. She points to a spurious claim that America “wants to blow up” <small>TSMC, </small>a Taiwanese chipmaker. It originated with a misleading video posted on Douyin, the Chinese version of TikTok, which featured an American lawmaker appearing to discuss the possibility. The following morning, a Taiwanese newspaper published a story about the video. Opposition lawmakers and talk-show hosts whipped up outrage. “The journey from China’s Douyin to Taiwan’s mass media, videos, newspapers and television took less than half a day,” says Ms Chien. Chinese state media amplified the narrative, as if merely commenting from the outside on a Taiwanese debate.</p><p data-component="paragraph">Taiwanese officials believe that many of the Taiwanese launching<small> US-</small>scepticism untruths are “local collaborators” taking orders and payments from China. But that is hard to prove, because the suspected Chinese funding is probably funnelled through Taiwanese businesspeople or public-relations firms. Wang Kun-yi, a local commentator who frequently writes <small>US</small>-scepticism narratives for Chinese media and pro-China Taiwanese media, defends his work as a commercial enterprise. All journalists in Taiwan serve the bias of their newspapers’ bosses, says Mr Wang, who has worked for both pro-independence and pro-unification newspapers. “Everyone just treats it as a job,” he says. “It’s a tool to feed yourself.”</p><p data-component="paragraph">Taiwan has laws against foreign infiltration and election influence, but they are limited to cases of proven state-sponsored activity. It has additional laws against spreading wilful falsehood in broadcast media, but they do not cover print or digital outlets. In 2020 the government revoked the licence of <small>CTI </small>News, a pro-China channel, citing repeated failures to verify information. <small>CTI</small> simply moved online.</p><p data-component="paragraph">The case sparked accusations of censorship, which Taiwan wants to avoid. So the government has resorted to more liberal methods of fighting disinformation. It has tried to improve media literacy, provide faster official clarifications and promote fact-checking organisations. But such means cannot match the speed of Chinese propaganda. In August Meta removed a network of more than 7,000 accounts, pages and groups that were spreading Chinese disinformation. But new accounts are easy to set up, a problem that will only accelerate with artificial intelligence, says Mr Lo.</p><p data-component="paragraph">Chinese disinformation has already distorted Taiwan’s public conversation. Will it move votes? Meta has noted that the Chinese disinformation network it removed was “high volume, low reach”, despite having a veneer of engagement designed to make the accounts look more popular than they were. Studies of Russian disinformation in America have found that it has little impact on voter preferences. Despite all the messaging in Chinese and Taiwanese media against the Democratic Progressive Party, its candidate, William Lai, is leading in the polls. And for all the scepticism about America, Taiwanese are even warier of China. A 2022 survey by Academia Sinica, a Taiwanese research institution, found 34% of respondents agreeing that America is a “credible” country. Only 9% said the same of China.</p><p data-component="paragraph">China has itself to blame. It recently surrounded Taiwan with warplanes and warships, even as its ruling Communist Party unveiled an integration plan promising benefits to Taiwanese people living in Fujian, a southern province near the island. Most Taiwanese know where their real threat comes from. But China’s insidious efforts to mislead them are increasing.<span>■</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deconstructing Go Type Parameters (103 pts)]]></title>
            <link>https://go.dev/blog/deconstructing-type-parameters</link>
            <guid>37667731</guid>
            <pubDate>Wed, 27 Sep 2023 00:05:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://go.dev/blog/deconstructing-type-parameters">https://go.dev/blog/deconstructing-type-parameters</a>, See on <a href="https://news.ycombinator.com/item?id=37667731">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-slug="/blog/deconstructing-type-parameters">
    
    <h2><a href="https://go.dev/blog/">The Go Blog</a></h2>
    

    
      
      
      
      <h2 id="slices-package-function-signatures">slices package function signatures</h2>
<p>The <a href="https://pkg.go.dev/slices#Clone" rel="noreferrer" target="_blank"><code>slices.Clone</code></a> function is
pretty simple: it makes a copy of a slice of any type.</p>
<pre><code>func Clone[S ~[]E, E any](s S) S {
    return append(s[:0:0], s...)
}
</code></pre>
<p>This works because appending to a slice with zero capacity will
allocate a new backing array.
The function body winds up being shorter than the function signature,
which is in part because the body is short, but also because the
signature is long.
In this blog post we’ll explain why the signature is written the way
that it is.</p>
<h2 id="simple-clone">Simple Clone</h2>
<p>We’ll start by writing a simple generic <code>Clone</code> function.
This is not the one in the <code>slices</code> package.
We want to take a slice of any element type, and return a new slice.</p>
<pre><code>func Clone1[E any](s []E) []E {
    // body omitted
}
</code></pre>
<p>The generic function <code>Clone1</code> has a single type parameter <code>E</code>.
It takes a single argument <code>s</code> which is a slice of type <code>E</code>, and it
returns a slice of the same type.
This signature is straightforward for anybody familiar with generics
in Go.</p>
<p>However, there is a problem.
Named slice types are not common in Go, but people do use them.</p>
<pre><code>// MySlice is a slice of strings with a special String method.
type MySlice []string

// String returns the printable version of a MySlice value.
func (s MySlice) String() string {
    return strings.Join(s, "+")
}
</code></pre>
<p>Let’s say that we want to make a copy of a <code>MySlice</code> and then get the
printable version, but with the strings in sorted order.</p>
<pre><code>func PrintSorted(ms MySlice) string {
    c := Clone1(ms)
    slices.Sort(c)
    return c.String() // FAILS TO COMPILE
}
</code></pre>
<p>Unfortunately, this doesn’t work.
The compiler reports an error:</p>
<pre><code>c.String undefined (type []string has no field or method String)
</code></pre>
<p>We can see the problem if we manually instantiate <code>Clone1</code> by
replacing the type parameter with the type argument.</p>
<pre><code>func InstantiatedClone1(s []string) []string
</code></pre>
<p>The <a href="https://go.dev/ref/spec#Assignability" rel="noreferrer" target="_blank">Go assignment rules</a> allow
us to pass a value of type <code>MySlice</code> to a parameter of type
<code>[]string</code>, so calling <code>Clone1</code> is fine.
But <code>Clone1</code> will return a value of type <code>[]string</code>, not a value of
type <code>MySlce</code>.
The type <code>[]string</code> doesn’t have a <code>String</code> method, so the compiler
reports an error.</p>
<h2 id="flexible-clone">Flexible Clone</h2>
<p>To fix this problem, we have to write a version of <code>Clone</code> that
returns the same type as its argument.
If we can do that, then when we call <code>Clone</code> with a value of type
<code>MySlice</code>, it will return a result of type <code>MySlice</code>.</p>
<p>We know that it has to look something like this.</p>
<pre><code>func Clone2[S ?](s S) S // INVALID
</code></pre>
<p>This <code>Clone2</code> function returns a value that is the same type as its
argument.</p>
<p>Here I’ve written the constraint as <code>?</code>, but that’s just a
placeholder.
To make this work we need to write a constraint that will let us write
the body of the function.
For <code>Clone1</code> we could just use a constraint of <code>any</code> for the element
type.
For <code>Clone2</code> that won’t work: we want to require that <code>s</code> be a slice
type.</p>
<p>Since we know we want a slice, the constraint of <code>S</code> has to be a
slice.
We don’t care what the slice element type is, so let’s just call it
<code>E</code>, as we did with <code>Clone1</code>.</p>
<pre><code>func Clone3[S []E](s S) S // INVALID
</code></pre>
<p>This is still invalid, because we haven’t declared <code>E</code>.
The type argument for <code>E</code> can be any type, which means it also has to
be a type parameter itself.
Since it can be any type, its constraint is <code>any</code>.</p>
<pre><code>func Clone4[S []E, E any](s S) S
</code></pre>
<p>This is getting close, and at least it will compile, but we’re not
quite there yet.
If we compile this version, we get an error when we call <code>Clone4(ms)</code>.</p>
<pre><code>MySlice does not satisfy []string (possibly missing ~ for []string in []string)
</code></pre>
<p>The compiler is telling us that we can’t use the type argument
<code>MySlice</code> for the type parameter <code>S</code>, because <code>MySlice</code> does not
satisfy the constraint <code>[]E</code>.
That’s because <code>[]E</code> as a constraint only permits a slice type
literal, like <code>[]string</code>.
It doesn’t permit a named type like <code>MySlice</code>.</p>
<h2 id="underlying-type-constraints">Underlying type constraints</h2>
<p>As the error message hints, the answer is to add a <code>~</code>.</p>
<pre><code>func Clone5[S ~[]E, E any](s S) S
</code></pre>
<p>To repeat, writing type parameters and constraints <code>[S []E, E any]</code>
means that the type argument for <code>S</code> can be any unnamed slice type,
but it can’t be a named type defined as a slice literal.
Writing <code>[S ~[]E, E any]</code>, with a <code>~</code>, means that the type argument
for <code>S</code> can be any type whose underlying type is a slice type.</p>
<p>For any named type <code>type T1 T2</code> the underlying type of <code>T1</code> is the
underlying type of <code>T2</code>.
The underlying type of a predeclared type like <code>int</code> or a type literal
like <code>[]string</code> is just the type itself.
For the exact details, <a href="https://go.dev/ref/spec#Underlying_types" rel="noreferrer" target="_blank">see the language
spec</a>.
In our example, the underlying type of <code>MySlice</code> is <code>[]string</code>.</p>
<p>Since the underlying type of <code>MySlice</code> is a slice, we can pass an
argument of type <code>MySlice</code> to <code>Clone5</code>.
As you may have noticed, the signature of <code>Clone5</code> is the same as the
signature of <code>slices.Clone</code>.
We’ve finally gotten to where we want to be.</p>
<p>Before we move on, let’s discuss why the Go syntax requires a <code>~</code>.
It might seem that we would always want to permit passing <code>MySlice</code>,
so why not make that the default?
Or, if we need to support exact matching, why not flip things around,
so that a constraint of <code>[]E</code> permits a named type while a constraint
of, say, <code>=[]E</code> only permits slice type literals?</p>
<p>To explain this, let’s first observe that a type parameter list like
<code>[T ~MySlice]</code> doesn’t make sense.
That’s because <code>MySlice</code> is not the underlying type of any other type.
For instance, if we have a definition like <code>type MySlice2 MySlice</code>,
the underlying type of <code>MySlice2</code> is <code>[]string</code>, not <code>MySlice</code>.
So either <code>[T ~MySlice]</code> would permit no types at all, or it would be
the same as <code>[T MySlice]</code> and only match <code>MySlice</code>.
Either way, <code>[T ~MySlice]</code> isn’t useful.
To avoid this confusion, the language prohibits <code>[T ~MySlice]</code>, and
the compiler produces an error like</p>
<pre><code>invalid use of ~ (underlying type of MySlice is []string)
</code></pre>
<p>If Go didn’t require the tilde, so that <code>[S []E]</code> would match any type
whose underlying type is <code>[]E</code>, then we would have to define the
meaning of <code>[S MySlice]</code>.</p>
<p>We could prohibit <code>[S MySlice]</code>, or we could say that <code>[S MySlice]</code>
only matches <code>MySlice</code>, but either approach runs into trouble with
predeclared types.
A predeclared type, like <code>int</code> is its own underlying type.
We want to permit people to be able to write constraints that accept
any type argument whose underlying type is <code>int</code>.
In the language today, they can do that by writing <code>[T ~int]</code>.
If we don’t require the tilde we would still need a way to say “any
type whose underlying type is <code>int</code>”.
The natural way to say that would be <code>[T int]</code>.
That would mean that <code>[T MySlice]</code> and <code>[T int]</code> would behave
differently, although they look very similar.</p>
<p>We could perhaps say that <code>[S MySlice]</code> matches any type whose
underlying type is the underlying type of <code>MySlice</code>, but that makes
<code>[S MySlice]</code> unnecessary and confusing.</p>
<p>We think it’s better to require the <code>~</code> and be very clear about when
we are matching the underlying type rather than the type itself.</p>
<h2 id="type-inference">Type inference</h2>
<p>Now that we’ve explained the signature of <code>slices.Clone</code>, let’s see
how actually using <code>slices.Clone</code> is simplified by type inference.
Remember, the signature of <code>Clone</code> is</p>
<pre><code>func Clone[S ~[]E, E any](s S) S
</code></pre>
<p>A call of <code>slices.Clone</code> will pass a slice to the parameter <code>s</code>.
Simple type inference will let the compiler infer that the type
argument for the type parameter <code>S</code> is the type of the slice being
passed to <code>Clone</code>.
Type inference is then powerful enough to see that the type argument
for <code>E</code> is the element type of the type argument passed to <code>S</code>.</p>
<p>This means that we can write</p>
<pre><code>    c := Clone(ms)
</code></pre>
<p>without having to write</p>
<pre><code>    c := Clone[MySlice, string](ms)
</code></pre>
<p>If we refer to <code>Clone</code> without calling it, we do have to specify a
type argument for <code>S</code>, as the compiler has nothing it can use to infer
it.
Fortunately, in that case, type inference is able to infer the type
argument for <code>E</code> from the argument for <code>S</code>, and we don’t have to
specify it separately.</p>
<p>That is, we can write</p>
<pre><code>    myClone := Clone[MySlice]
</code></pre>
<p>without having to write</p>
<pre><code>    myClone := Clone[MySlice, string]
</code></pre>
<h2 id="deconstructing-type-parameters">Deconstructing type parameters</h2>
<p>The general technique we’ve used here, in which we define one type
parameter <code>S</code> using another type parameter <code>E</code>, is a way to
deconstruct types in generic function signatures.
By deconstructing a type, we can name, and constrain, all aspects of
the type.</p>
<p>For example, here is the signature for <code>maps.Clone</code>.</p>
<pre><code>func Clone[M ~map[K]V, K comparable, V any](m M) M
</code></pre>
<p>Just as with <code>slices.Clone</code>, we use a type parameter for the type of
the parameter <code>m</code>, and then deconstruct the type using two other type
parameters <code>K</code> and <code>V</code>.</p>
<p>In <code>maps.Clone</code> we constrain <code>K</code> to be comparable, as is required for
a map key type.
We can constrain the component types any way we like.</p>
<pre><code>func WithStrings[S ~[]E, E interface { String() string }](s S) (S, []string)
</code></pre>
<p>This says that the argument of <code>WithStrings</code> must be a slice type for
which the element type has a <code>String</code> method.</p>
<p>Since all Go types can be built up from component types, we can always
use type parameters to deconstruct those types and constrain them as
we like.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Radar Maps Platform (127 pts)]]></title>
            <link>https://radar.com/blog/introducing-radar-maps-platform</link>
            <guid>37667450</guid>
            <pubDate>Tue, 26 Sep 2023 23:38:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://radar.com/blog/introducing-radar-maps-platform">https://radar.com/blog/introducing-radar-maps-platform</a>, See on <a href="https://news.ycombinator.com/item?id=37667450">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next" data-reactroot=""><div><div><ul><li><a href="https://radar.com/product/geofencing">Product</a></li><li><a href="https://radar.com/solutions/retail">Solutions</a></li><li><a href="https://radar.com/documentation">Docs</a></li><li><a href="https://radar.com/pricing">Pricing</a></li><li><a href="https://radar.com/about">Company</a></li></ul><div><p><a href="https://radar.com/product/geofencing"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.132 2a3.648 3.648 0 0 1 3.473 3.641v.156c0 2.152-2.911 5.255-3.216 5.572a.649.649 0 0 1-.425.191.649.649 0 0 1-.424-.19c-.3-.318-3.217-3.42-3.217-5.573V5.64A3.643 3.643 0 0 1 7.797 2h.335ZM6.589 5.64a1.369 1.369 0 1 0 2.738 0 1.369 1.369 0 0 0-2.738 0Zm4.496 4.323a5.5 5.5 0 0 0 .227-.335l.006-.006c.903.455 1.447 1.106 1.447 1.848C12.765 12.91 10.715 14 8 14c-2.714 0-4.765-1.088-4.765-2.53 0-.729.526-1.368 1.399-1.823.066.102.131.204.21.311.166.24.334.46.501.676-.586.263-.92.58-.92.837 0 .532 1.423 1.333 3.569 1.333s3.57-.801 3.57-1.333c0-.263-.353-.592-.963-.855l.058-.076c.139-.181.285-.372.426-.576Z" fill="currentColor"></path></svg>Geofences<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.99 3.23a.75.75 0 0 0 0 1.06L10.698 8 6.99 11.709a.75.75 0 1 0 1.06 1.06l4.198-4.197a.81.81 0 0 0 0-1.145L8.05 3.23a.75.75 0 0 0-1.06 0Z" fill="currentColor"></path></svg><span href="/product/geofencing">Industry-leading accuracy with unlimited geofences, polygon geofences, and more</span></a></p><p><a href="https://radar.com/product/trip-tracking"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.2 2H2.8c-.44 0-.8.365-.8.812v11.376c0 .447.36.812.8.812h.167c.44 0 .8-.365.8-.812v-2.436H13.2c.44 0 .8-.365.8-.812V2.812A.808.808 0 0 0 13.2 2Zm-.967 4.872H9.411v2.866H6.59V6.872H3.767V4.006h2.822v2.866H9.41V4.006h2.822v2.866Z" fill="currentColor"></path></svg>Trips<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.99 3.23a.75.75 0 0 0 0 1.06L10.698 8 6.99 11.709a.75.75 0 1 0 1.06 1.06l4.198-4.197a.81.81 0 0 0 0-1.145L8.05 3.23a.75.75 0 0 0-1.06 0Z" fill="currentColor"></path></svg><span href="/product/trip-tracking">Trip tracking, live ETAs, arrival detection, and routing for pickups and deliveries</span></a></p><p><a href="https://radar.com/product/places"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.425 2.375A.788.788 0 0 0 11.75 2h-7.5c-.3 0-.525.15-.675.375C2 5.375 2 5.525 2 5.75c0 .825.675 1.5 1.5 1.5v6c0 .45.3.75.75.75h7.5c.45 0 .75-.3.75-.75v-6c.825 0 1.5-.675 1.5-1.5 0-.225 0-.375-1.575-3.375ZM9.5 12.5v-3h-3v3H5V7.025c.225.15.45.225.75.225.45 0 .825-.225 1.125-.525.3.3.675.525 1.125.525.45 0 .825-.225 1.125-.525.3.3.675.525 1.125.525.3 0 .525-.075.75-.225V12.5H9.5Z" fill="currentColor"></path></svg>Places<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.99 3.23a.75.75 0 0 0 0 1.06L10.698 8 6.99 11.709a.75.75 0 1 0 1.06 1.06l4.198-4.197a.81.81 0 0 0 0-1.145L8.05 3.23a.75.75 0 0 0-1.06 0Z" fill="currentColor"></path></svg><span href="/product/places">Points-of-interest (POI) dataset with chains and categories to detect visits to millions of places</span></a></p><p><a href="https://radar.com/product/api"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.57 6.57A4.578 4.578 0 0 0 8.21 2h-.42a4.573 4.573 0 0 0-4.36 4.57v.195c0 2.702 3.662 6.597 4.037 6.995.15.157.383.24.533.24.15 0 .383-.083.533-.24.383-.398 4.037-4.293 4.037-6.995V6.57ZM7.993 8.29a1.718 1.718 0 1 1 0-3.437c.953 0 1.718.765 1.718 1.718S8.946 8.29 7.993 8.29Z" fill="currentColor"></path></svg>Maps Platform<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.99 3.23a.75.75 0 0 0 0 1.06L10.698 8 6.99 11.709a.75.75 0 1 0 1.06 1.06l4.198-4.197a.81.81 0 0 0 0-1.145L8.05 3.23a.75.75 0 0 0-1.06 0Z" fill="currentColor"></path></svg><span href="/product/api">The cost-effective, all-in-one Google Maps alternative, with geocoding, search, routing, and maps</span></a></p></div></div><div><ul><li><a href="https://radar.com/login">Log in</a></li><li><a href="https://radar.com/contact">Get a demo</a></li></ul></div></div><div><header><nav><ul><li><a href="https://radar.com/blog">Blog Home</a></li><li><a href="https://radar.com/blog/categories/company">Company</a></li><li><a href="https://radar.com/blog/categories/product">Product</a></li><li><a href="https://radar.com/blog/categories/engineering">Engineering</a></li><li><a href="https://radar.com/blog/categories/industry">Industry</a></li><li><a href="https://radar.com/blog/categories/guides">Guides</a></li></ul></nav></header></div><div><h2>It’s time to build</h2><p>See what Radar’s location and geofencing <br>solutions can do for your business.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Philips Hue ecosystem is collapsing into stupidity (835 pts)]]></title>
            <link>https://rachelbythebay.com/w/2023/09/26/hue/</link>
            <guid>37667266</guid>
            <pubDate>Tue, 26 Sep 2023 23:22:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rachelbythebay.com/w/2023/09/26/hue/">https://rachelbythebay.com/w/2023/09/26/hue/</a>, See on <a href="https://news.ycombinator.com/item?id=37667266">Hacker News</a></p>
Couldn't get https://rachelbythebay.com/w/2023/09/26/hue/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Warren Spector – 40 years and I'm still here (122 pts)]]></title>
            <link>https://www.gamedeveloper.com/blogs/my-40-years-in-the-game-industry</link>
            <guid>37665946</guid>
            <pubDate>Tue, 26 Sep 2023 21:18:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gamedeveloper.com/blogs/my-40-years-in-the-game-industry">https://www.gamedeveloper.com/blogs/my-40-years-in-the-game-industry</a>, See on <a href="https://news.ycombinator.com/item?id=37665946">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>To my amazement, this month - September 2023 - marks my 40th year in game development. I’m not sure how I’ve survived so long in a business that more typically burns people out in... somewhat less, let’s just say. That said, I have some thoughts about survival and an urge to tell the story of how I today find myself one of the most long-lived veterans still actively involved in making games. There may be a few older geezers still working, I guess, but most of you who want to chime in with an arthritic “But I’ve been doing this longer than you” or “You’re a young whipper-snapper next to me” are probably big deals on the business side - C-suite types. If you’ve worked more than 40 years and are still actually working with teams to make games, let me know. We can hang out and talk about our lumbago.</p><p>But now I’m just bragging.</p><p>So, I want to share my story. And I want to talk about how things have changed and about how they haven’t. Don’t expect any deep design or creative leadership insights here. I’m going full selfish on you and talking about myself. (Hey, if Cliffy B and John Romero can write their - fascinating - autobiographies, a blog post from me can't hurt.) Maybe you’ll find what I have to say interesting, maybe not. One of the joys of aging is that you kind of stop caring what other people think, so I’m just going to ramble on. You can join me if you want. Or go read someone else's blog. I won't be offended.</p><h3>My story</h3><p>I discovered Dungeons &amp; Dragons back in 1978, just four years after the game made its debut. I bought the white box edition - the one that required players to make up half the rules, a bit of design genius if there ever was one. I mean, once you'd made up a bunch of rules and made the game your own, who was going to give it up for some other set of (possibly better) rules? Modern TTRPG designers may want to take note.</p><p>Anyway, prior to the D&amp;D revelation, I was an obsessive player of little-cardboard-square boardgames with a writer friend of mine - Walton Simons. (The name may sound familiar to Deus Ex fans.) Some of those games came in ziplock bags from a company called Metagames and later from Steve Jackson Games (important to the story later). Walton - better known as "Bud" invited me to play in a D&amp;D game whose Dungeonmaster was an SF writer named Bruce Sterling, before his first publication and before he became one of the fathers of the cyberpunk movement. I played in that campaign for about ten years and if I tell you how it ended I'll end of crying so you'll have to use your imaginations. Let's just say my fellow party members and I started as lowly troublemakers, the Rat Gang, and ended up political, military and individual powers in the river city of Shang. (This was long before TSR came out with Oriental Adventures. See earlier reference to "writing rules to make the game your own.) I played in a bunch of other campaigns as well, but Bruce’s is the one I’ll always remember.</p><p>My personal favorite game was Chill from Pacesetter games, but eventually I created my own set of rules, for a World War 2 RPG. I ran that campaign - about what would have happened in Poland if Nazi occult beliefs were true - for a while before my life changed completely. (As a note, even that Nazi occult stuff was based-in reality as someone saw it, one of the core tenets of the rest of the tabletop RPG's and every digital game I've worked on since.)</p><p>That was the context and only qualification for getting my first professional job in game development. I was a game nerd. I was also a film nerd, working on my PhD at The University of Texas where, as part of that program, I was teaching film history, theory and criticism when the department asked me to give up those classes and help another grad student learn how to teach one of them. I had no idea how I was going to pay my rent.</p><h2>Steve Jackson Games</h2><p>That was when I got a call out of the blue from a gaming friend who was working at Steve Jackson Games asking if I was interested in an Associate Game Editor at Steve Jackson Games, a small developer of tabletop games in my hometown of Austin, Texas. They were looking for a developer on the game side and an editor for Space Gamer and Fantasy Gamer magazines. That was in 1983. The year I dropped out of the PhD program I was in, just a dissertation short of a Doctorate, to make games. My mom cried for ten years. Given the hijinks at the company that might have been justified.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltc021aec5dfa720b5/6511e33890ea2622f11541e9/Steve_Jackson_Warren_Spector_ET_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltc021aec5dfa720b5/6511e33890ea2622f11541e9/Steve_Jackson_Warren_Spector_ET_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltc021aec5dfa720b5/6511e33890ea2622f11541e9/Steve_Jackson_Warren_Spector_ET_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltc021aec5dfa720b5/6511e33890ea2622f11541e9/Steve_Jackson_Warren_Spector_ET_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="5f84eymj7lbd" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltc021aec5dfa720b5/6511e33890ea2622f11541e9/Steve_Jackson_Warren_Spector_ET_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltc021aec5dfa720b5">
        </picture>
        <figcaption>Steve Jackson, me and a surprise guest editor</figcaption></figure><p>I worked on a lot of games at SJG, but the two things that were most noteworthy were Thing in the Darkness and TOON: The Cartoon Roleplaying (both released in 1984). Thing in the Darkness was a solo adventure by Matthew J. Costello which Steve felt needed editing and development. Steve assigned me to do that, but I had no experience flowcharting and revising choose your own adventures and no idea how to do it. Magazine editing I had nailed - I'd done that editing an entertainment magazine insert in The Daily Texan newspaper. Developing a solo adventure? I was clueless. Steve Jackson stepped in and gave me a lesson I needed and will never forget. The Thing in the Darkness appeared on the cover of Fantasy Gamer #3 and was received well. Success!</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd1bf9962bdced534/6511e36a063da84658eaadaf/Thing_in_the_Darkness_2_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd1bf9962bdced534/6511e36a063da84658eaadaf/Thing_in_the_Darkness_2_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd1bf9962bdced534/6511e36a063da84658eaadaf/Thing_in_the_Darkness_2_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd1bf9962bdced534/6511e36a063da84658eaadaf/Thing_in_the_Darkness_2_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="00hscm9yjl3s" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd1bf9962bdced534/6511e36a063da84658eaadaf/Thing_in_the_Darkness_2_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltd1bf9962bdced534">
        </picture>
        <figcaption>Thing in the Darkness – the first published game I worked on (all credit to Matthew J. Costello for a fine foundation and Steve Jackson for giving me an education in flowcharting!)</figcaption></figure><p>TOON was, to this day, a high point in my career. I've always been a cartoon freak and when I saw TOON on a stack of games submitted for magazine publication, I knew we had something special. It was designed by a guy named Greg Costikyan, a well-known, super-talented tabletop designer. (Weirdly, he and I went to the same high school, though we didn't know each other. There must have been something in the water at that school!)</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1bc07117419b515b/6511c42a6805d46c5e6aea30/TOON_Playtest.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1bc07117419b515b/6511c42a6805d46c5e6aea30/TOON_Playtest.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1bc07117419b515b/6511c42a6805d46c5e6aea30/TOON_Playtest.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1bc07117419b515b/6511c42a6805d46c5e6aea30/TOON_Playtest.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="6tldver2w72v" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1bc07117419b515b/6511c42a6805d46c5e6aea30/TOON_Playtest.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt1bc07117419b515b">
        </picture>
        <figcaption>TOON playtest. That’s Allen Varney on the left and Caroline Chase (soon to be Caroline Spector) on the right. If you’re laughing when you playtest a cartoon RPG you’re doing something right.</figcaption></figure><p>Two things struck me about TOON in its original form - it was way too cool to be relegated to magazine publication and it was written in what was called SPI case format. (The latter was the geekiest, dullest albeit maximally effective way of writing rules.) I went to Steve and asked him to let me develop the TOON rules into something simpler and more appropriate to the subject matter. To his credit he told me to run with it. I enlisted the aid of another designer, Allen Varney, who was a great collaborator and deserves more credit for TOON's success than he gets. The game debuted at GENCON IN 1984 - the same day as West End Games' game, Paranoia. The competition for funniest game was on! Both teams won. I never again heard as much laughter at a GENCON. What I'll say is that TOON is still for sale 40 years after Allen and I worked on it. We did something right!</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2ba62833e56cc1f3/6511c55e1d8b4f70eaa511b8/SJG_Crew.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2ba62833e56cc1f3/6511c55e1d8b4f70eaa511b8/SJG_Crew.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2ba62833e56cc1f3/6511c55e1d8b4f70eaa511b8/SJG_Crew.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2ba62833e56cc1f3/6511c55e1d8b4f70eaa511b8/SJG_Crew.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="lfzw5fqywisz" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2ba62833e56cc1f3/6511c55e1d8b4f70eaa511b8/SJG_Crew.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt2ba62833e56cc1f3">
        </picture>
        <figcaption>The SJG crew in the early-80s. That’s me in the vest. My future wife, Caroline, is second from the right. Frequent collaborator, Allen Varney is far right. Steve Jackson is in the jacket and tie in the second row.</figcaption></figure><p>Three years into my tenure as Steve Jackson Games’ Editor-in-Chief (I got promoted!) I got a call out of the blue from someone at TSR. They made me the Godfather offer - $25,000 a year! – so I moved to Lake Geneva, home of D&amp;D.</p><h2>TSR</h2><p>At the time, D&amp;D was perceived in the game development world as yesterday’s news, so bound up with what Gary Gygax and Dave Arneson had come up with they hadn’t innovated in years. Frankly, I thought it was a design problem, that they needed fresh blood, people who wanted to do new things. What I found was something completely different – the design talent up there was prodigious and, frankly, more ambitious than they were allowed to be by the folks in the C-suite. Looking back, you have to understand the executive viewpoint - I mean, would <em>you</em> mess with D&amp;D, the most successful TTRPG on the planet? Well, I would, but no one would mistake me for a business-first sort of guy.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt4d56a701ded622b5/6511e3b847304e6539afec5d/Warren_Spector_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt4d56a701ded622b5/6511e3b847304e6539afec5d/Warren_Spector_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt4d56a701ded622b5/6511e3b847304e6539afec5d/Warren_Spector_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt4d56a701ded622b5/6511e3b847304e6539afec5d/Warren_Spector_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="soq9lmigxigl" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt4d56a701ded622b5/6511e3b847304e6539afec5d/Warren_Spector_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt4d56a701ded622b5">
        </picture>
        <figcaption>Early in my tenure at TSR. Smug bastard, eh? If you can’t read it, the button says “For this we came all the way from Texas?” It was a reference to the weather, not the company!</figcaption></figure><p>At TSR, I worked as a developer, editor or designer on a bunch of games and modules. I was also Manager of the Game Division for a while, working under a talented designer and all-around great guy, Jim Ward. But that was less interesting and fun than working on games! Notably, I worked with Doug Niles on the Top Secret/S.I. RPG (1987), with Jeff Grubb on the Buck Rogers: Battle for the 25th Century Boardgame (1988), and with Zeb Cook on The Bullwinkle &amp; Rocky Party Roleplaying Game (1988). The first of those was well-received and to this day I think the mechanics were really innovative and still relevant. The Buck Rogers game was definitely not received well! I'll defend it to the end – it was a ton of fun and with greater "toy value" than most games at the time.</p><p>I honestly don’t remember if the Bullwinkle and Rockey game attracted any attention at all, but it was for sure zany to the max – hand puppets, spinners, storytelling cards, fake diplomas from Wossamotta U. We tested with all players of all ages, from 7 to 70, and it really did appeal to kids and adults.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt203f7bdee8c5434a/6511e3fa22e09d9d9968bd55/Moose_and_Squirrel_RPG_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt203f7bdee8c5434a/6511e3fa22e09d9d9968bd55/Moose_and_Squirrel_RPG_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt203f7bdee8c5434a/6511e3fa22e09d9d9968bd55/Moose_and_Squirrel_RPG_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt203f7bdee8c5434a/6511e3fa22e09d9d9968bd55/Moose_and_Squirrel_RPG_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="m0mbls4nx107" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt203f7bdee8c5434a/6511e3fa22e09d9d9968bd55/Moose_and_Squirrel_RPG_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt203f7bdee8c5434a">
        </picture>
        <figcaption>Right up there with TOON for the zaniest game I ever worked on!</figcaption></figure><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1b7705263dccef44/6511e44287e59c5946cca70d/Rocky_Bullwinkle_Game_Swag_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1b7705263dccef44/6511e44287e59c5946cca70d/Rocky_Bullwinkle_Game_Swag_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1b7705263dccef44/6511e44287e59c5946cca70d/Rocky_Bullwinkle_Game_Swag_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1b7705263dccef44/6511e44287e59c5946cca70d/Rocky_Bullwinkle_Game_Swag_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="sz2a88mn9416" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1b7705263dccef44/6511e44287e59c5946cca70d/Rocky_Bullwinkle_Game_Swag_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt1b7705263dccef44">
        </picture>
        <figcaption>And here’s why – Cartoony rules, spineers, hand puppets, and more</figcaption></figure><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt70d4b62b7335d03e/6511c7623741cdc91c1eab8d/GenCon_1988.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt70d4b62b7335d03e/6511c7623741cdc91c1eab8d/GenCon_1988.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt70d4b62b7335d03e/6511c7623741cdc91c1eab8d/GenCon_1988.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt70d4b62b7335d03e/6511c7623741cdc91c1eab8d/GenCon_1988.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="1dxkfv7ks761" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt70d4b62b7335d03e/6511c7623741cdc91c1eab8d/GenCon_1988.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt70d4b62b7335d03e">
        </picture>
        <figcaption>Designers at GenCon 1988 playing with Bullwinkle &amp; Rocky stuff. That’s Mike Pondsmith (yes, the Cyberpunk guy!), Lawrence Schick, me, and Zeb Cook, from left to right.</figcaption></figure><p>These projects were early lessons for me that sales aren't the only success criterion (though it's obviously a hugely important one!). I also got to work on the 2<sup>nd</sup> Edition AD&amp;D Dungeonmasters Guide, which was kind of cool, to say the least, but it shipped after I left.</p><p>And speaking of leaving...</p><p>By late 1988, I had started thinking it might be time to move on. It was cool that I got to write a novel (The Hollow Earth Affair) and a Marvel Superheroes solo book (One Thing After Another) featuring the Thing from the Fantastic Four. It was a thrill getting to type “It’s clobbering’ time!” and years later I even got Stan Lee to autograph a copy. Still, I was feeling like the biggest decision I had to make in my job was whether to use percentile dice or D20s. Let’s just say that wasn’t the most fulfilling thing about game design.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blta66adc7a1a415ed8/6511e48860a66b03009b186b/One_Thing_After_Another_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blta66adc7a1a415ed8/6511e48860a66b03009b186b/One_Thing_After_Another_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blta66adc7a1a415ed8/6511e48860a66b03009b186b/One_Thing_After_Another_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blta66adc7a1a415ed8/6511e48860a66b03009b186b/One_Thing_After_Another_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="zrt97esl01zt" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blta66adc7a1a415ed8/6511e48860a66b03009b186b/One_Thing_After_Another_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blta66adc7a1a415ed8">
        </picture>
        <figcaption>My solo adventure featuring The Thing and the Fantastic Four.</figcaption></figure><figure><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltea1b877abe0c6b4a/6511e4c8321f32271c6aad8a/Warren_Spector_Stan_Lee_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltea1b877abe0c6b4a/6511e4c8321f32271c6aad8a/Warren_Spector_Stan_Lee_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltea1b877abe0c6b4a/6511e4c8321f32271c6aad8a/Warren_Spector_Stan_Lee_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltea1b877abe0c6b4a/6511e4c8321f32271c6aad8a/Warren_Spector_Stan_Lee_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="dy9iog6n75mg" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltea1b877abe0c6b4a/6511e4c8321f32271c6aad8a/Warren_Spector_Stan_Lee_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltea1b877abe0c6b4a">
        </picture>
        <figcaption spellcheck="false">And here's Stan the Man autographing a copy of One Thing After Another. I could barely speak. Stan Lee changed my life!</figcaption></figure><figure><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt04f43007366b9a4b/6511eba147304ef362afec62/Double_Agent_by_Warren_Spector_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt04f43007366b9a4b/6511eba147304ef362afec62/Double_Agent_by_Warren_Spector_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt04f43007366b9a4b/6511eba147304ef362afec62/Double_Agent_by_Warren_Spector_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt04f43007366b9a4b/6511eba147304ef362afec62/Double_Agent_by_Warren_Spector_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="lodz8d4b08kn" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt04f43007366b9a4b/6511eba147304ef362afec62/Double_Agent_by_Warren_Spector_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt04f43007366b9a4b">
        </picture>
        <figcaption spellcheck="false">My one published novel, part of a two-book combo. It’s said that everyone has a novel that should stay in a closet and never get published. This was mine. But it was fun as hell to write.</figcaption></figure><p>At that time, I figured there were two logical career paths – become a Disney Imagineer or get into computer games, which I was playing obsessively. I talked to some folks at Disney on the phone, but that didn’t lead anywhere. Probably for the best, as it turned out. See, I had never worked on a theme park attraction, and I was playing computer games like they were going out of style as well as testing the new AD&amp;D digital efforts. I respected the efforts of the folks making those games – they were doing an excellent job of bringing the D&amp;D rules to computers - but I felt that wasn’t the right way to approach things. The <em>feeling</em> of playing D&amp;D was what was important, not the rules. The games I loved the most were the ones that felt right which, at the time, meant the Ultima games made by Richard Garriott and his teams at Origin. Their motto was "We Create Worlds" and they lived up to it. They were getting closer than anyone else to making me feel like I was playing D&amp;D, a game defined by telling stories <em>with</em> an author (or Dungeonmaster) rather than being <em>told</em> a story by that person.</p><p>It was a low point for me, but I got lucky again and, in 1988, got a call from someone I'd worked with at Steve Jackson Games who was now at Origin. One day he called me up and said Origin was looking for an Associate Producer and asked if I was interested. That was easy! YES! The opportunity to work with Richard Garriott, the guy who made the early Ultima games and led teams to create later ones, was too enticing to say no to. Richard got it. He saw everything I wanted games to be (within the limits of then current technology).</p><p>I got the job.</p><h2>Origin</h2><p>It was 1989 and the first things I was assigned to work on at Origin were Ultima VI, Richard's latest project, and Paul Neurath's Space Rogue.</p><p>Paul probably doesn't even remember this, but he asked me to work on the plot and flowchart missions. I did that. He junked everything I did and redid it all himself. Probably the right call. I was pretty green and even though I thought “I’m going to teach these computer guys what interactivity is all about” I quickly realized I knew nothing! I had to forget a lot of what I had learned in the tabletop world and learn to exercise some new muscles. Space Rogue was part of my design fitness program. It’s worth noting here that Space Rogue (1989) is an under-appreciated game that ought to be better known than it is. It consisted of first-person space combat, mashed up with an RPG, mashed up with a really fun arcade game. That idea of genre mashup appealed to me. A lot. And I've used that as the foundation of many of my later games. I owe that to Paul.</p><p>But at the time, I had more success working with Richard. He and I spent weeks crafting the story, the characters, the quests, and the puzzles for what became Ultima VI (1990). I learned his techniques and his philosophy of what games could and should be. Quite the education. And quite the game. Still one of my favorite Ultimas. Heck, one of my favorite games, even if I did work on it.</p><p>I worked on a bunch of other things at Origin, notably Chris Roberts' magnum opus, Wing Commander (1990). That project was a result of a unique vision, a dedicated team and Chris' unwillingness to compromise. I mean, I figured if I had ten arguments with Chris in a day and won three of them, that was a very good day indeed. (I could talk about Wing Commander all day, but that'd fill a book and I'm not writing a book here!)</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf9e0344eae7c21ba/6511c93255583f1ac56cfba3/Young_Warren_Spector.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf9e0344eae7c21ba/6511c93255583f1ac56cfba3/Young_Warren_Spector.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf9e0344eae7c21ba/6511c93255583f1ac56cfba3/Young_Warren_Spector.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf9e0344eae7c21ba/6511c93255583f1ac56cfba3/Young_Warren_Spector.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="8f3yp5bg66zp" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf9e0344eae7c21ba/6511c93255583f1ac56cfba3/Young_Warren_Spector.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltf9e0344eae7c21ba">
        </picture>
        <figcaption>Testing Wing Commander at home. Work was pretty all-consuming back then!</figcaption></figure><p>I worked on other games as well – Martian Dreams, Serpent Isle, Wings of Glory… a bunch of stuff. . I’m especially proud of Martian Dreams, an Ultima-engine game even if no one remembers it even came out.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd8a1576f8152971f/6511c9aec603d30d4775bca3/Savage_Empire.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd8a1576f8152971f/6511c9aec603d30d4775bca3/Savage_Empire.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd8a1576f8152971f/6511c9aec603d30d4775bca3/Savage_Empire.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd8a1576f8152971f/6511c9aec603d30d4775bca3/Savage_Empire.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="hhjm786um3bi" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd8a1576f8152971f/6511c9aec603d30d4775bca3/Savage_Empire.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltd8a1576f8152971f">
        </picture>
        <figcaption>In Savage Empire I was the evil Dr. Spector. In Martian Dreams I was the no-longer-evil-Dr. Spector, the Avatar’s friend. I swear I didn’t ask to be a character in the games – the teams insisted on it.</figcaption></figure><p><br>One game I worked on was particularly special: Ultima Underworld, made by Blue Sky Productions, founded by Paul Neurath who once again entered my life and pushed me further on the path I'm still on today.</p><p>My first encounter with Ultima Underworld came even before Underworld was Underworld. Paul Neurath showed up with a real-time, fully-textured, first-person tech demo - just a demo! I can't speak for anyone else, but all I could think as I watched it was "The world just changed." I saw the opportunity to let players see a world through their own eyes, to be immersed in it, to believe - at least a little - that it was <em>them </em>in the world, not some squidgy puppet they moved around the screen. I went to my boss immediately and begged him to let me work on the game Paul was proposing, but he gave it to another guy! Luckily for me, that other guy left the company a while later. I begged again and this time, I got it. I got to work on Ultima Underworld (1992).</p><figure><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb63e45bf3115ead6/6511f644d81f5986e112869e/Underworld_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb63e45bf3115ead6/6511f644d81f5986e112869e/Underworld_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb63e45bf3115ead6/6511f644d81f5986e112869e/Underworld_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb63e45bf3115ead6/6511f644d81f5986e112869e/Underworld_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="hsl55a8lzser" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb63e45bf3115ead6/6511f644d81f5986e112869e/Underworld_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltb63e45bf3115ead6">
        </picture>
        <figcaption spellcheck="false">Denis Loubet’s Underworld box art remains one my favorites to this day. It captures perfectly the feeling of exploration and danger embodied by the game.</figcaption></figure><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte1d409294df86968/6511f68187e59c399ecca720/Ultima_Underworld_copy.png?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte1d409294df86968/6511f68187e59c399ecca720/Ultima_Underworld_copy.png?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte1d409294df86968/6511f68187e59c399ecca720/Ultima_Underworld_copy.png?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte1d409294df86968/6511f68187e59c399ecca720/Ultima_Underworld_copy.png?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="tvfs3e2w93j0" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte1d409294df86968/6511f68187e59c399ecca720/Ultima_Underworld_copy.png?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blte1d409294df86968">
        </picture>
        <figcaption>The first ad we did for Ultima Underworld was strange – we had to explain what a first-person, real-time game was. No one back then knew. I’m not sure the ad did its job, but it was certainly... strange.</figcaption></figure><p>Working with Paul was great, of course, but the real revelation was meeting Doug Church. To this day, Doug is one of the smartest, most talented, most creative programmer/designers I've ever met. A true Secret Master of Gaming if there ever was one. He led the charge on the project in a way he's never credited for having done. And what a team. It was a bunch of MIT-types - even the non-MIT grads and non-grads were MIT-types in my mind! I don't believe there was anyone on the project who had ever worked on a game before. They had no idea what was and wasn't possible, which was key to their success. And, man, were they smart! A bunch of them lived together in a house they called "Deco Morono" - House of Ten Dumb Guys. Right... The first time I set foot in the place, I realized I was the dumbest guy in the room. It was awesome. The game, when it was released, was unlike anything anyone had ever seen. It attempted to empower players in ways that were unexpected and powerful, laying the groundwork for Immersive Sims to come.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt75e6f99d8f9e9326/6511f7d255583f51986cfbc1/Shipping_Ultima_Underworld_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt75e6f99d8f9e9326/6511f7d255583f51986cfbc1/Shipping_Ultima_Underworld_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt75e6f99d8f9e9326/6511f7d255583f51986cfbc1/Shipping_Ultima_Underworld_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt75e6f99d8f9e9326/6511f7d255583f51986cfbc1/Shipping_Ultima_Underworld_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="krj29tz2ogxt" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt75e6f99d8f9e9326/6511f7d255583f51986cfbc1/Shipping_Ultima_Underworld_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt75e6f99d8f9e9326">
        </picture>
        <figcaption>The day we shipped Ultima Underworld. That’s Doug Church on the left, Paul Neurath in the middle, and me on the right. (We were launching a toy soldier into the air to celebrate. Don’t ask me why.)</figcaption></figure><p>Speaking of Sims-to-come, after Underworld and Underworld 2, it was on to System Shock (1994). I'm sure everyone has their own story of how that game came to be and my memory may be faulty, but here's what I remember... I was at Origin, bored to death of making games about heroes in chainmail or plate, looking like the Mighty Thor, saving princesses and slaying evil mages. I'd worked with Chris Roberts on the original Wing Commander game and decided I could take the ideas behind Underworld and make a science fiction game, set in the Wing Commander universe. I called it Alien Commander. I wrote up a concept doc and got ready to pitch it. What I didn't know was that at Looking Glass Technologies, Doug Church (there he is again...) was working on an SF game of his own. Knowing Paul, I'm sure he was involved, too, but it was Doug I was talking to. We compared notes and all I could think was, "Um... Yeah... That's better than Alien Commander." Paul was able to assemble yet another amazing team to work on the game. He has a knack for building teams...</p><p>The game was cyberpunk to the max, with an engine more powerful than Underworld, with a ton of physics going on. And the plot was, well, good. That was in large part thanks to the participation of Austin Grossman, with whom I've had the pleasure of working many times since. (If you want to know something of what it was like at LG, check out his novel, YOU.)</p><figure><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte67ea7546d2756c7/6511cc1c23d5752444e92290/System_Shock_Cover.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte67ea7546d2756c7/6511cc1c23d5752444e92290/System_Shock_Cover.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte67ea7546d2756c7/6511cc1c23d5752444e92290/System_Shock_Cover.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte67ea7546d2756c7/6511cc1c23d5752444e92290/System_Shock_Cover.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="ypqu5rcaidyg" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte67ea7546d2756c7/6511cc1c23d5752444e92290/System_Shock_Cover.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blte67ea7546d2756c7">
        </picture>
        <figcaption spellcheck="false">Even the System Shock box spoke of disasters awaiting us in a horrific future of man/machine hybrids and machines run amok.</figcaption></figure><p>The System Shock narrative was communicated in a new way - through videologs and messages from dead folks - everyone was dead on Citadel Station, largely because all existing conversation systems sucked (which they still do). But there was more to the story, as it were. System Shock communicated story through elements in the level - signage, dead body placement, messages scrawled (usually in blood) on the walls. I hesitate to say it, but Shock may have introduced environmental storytelling to gaming.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt33460bff4afe3d75/6511ccce5ea4736398597047/SHODAN.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt33460bff4afe3d75/6511ccce5ea4736398597047/SHODAN.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt33460bff4afe3d75/6511ccce5ea4736398597047/SHODAN.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt33460bff4afe3d75/6511ccce5ea4736398597047/SHODAN.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="utndgy40xjlo" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt33460bff4afe3d75/6511ccce5ea4736398597047/SHODAN.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt33460bff4afe3d75">
        </picture>
        <figcaption>SHODAN, mad AI. Not that anything like that would ever happen...</figcaption></figure><p>In addition, the game featured a mad computer, SHODAN, who's still considered one of the great game villains of all time. The player character had no name - they were just Hacker - so players could create their character based on their own choices and imagination. And D&amp;D-like character stats didn’t matter, either. Shock was all about you, in the world, alone and underpowered to deal with an enemy that was smart and knew exactly what you were up to (or seemed to…) Players had some choices to make about how to play – not just what weapon to use to headshot that zombie. But there was more to come on that score.</p><p>I left Origin and officially joined Looking Glass in 1996. Paul Neurath offered me the opportunity to build my own studio in Austin, Texas. That was too good an opportunity to pass up, so after due consideration, I made the leap.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt408d86234d1898df/6511cd42d81f595a6212867b/Warren_Spector_Red_Shirt.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt408d86234d1898df/6511cd42d81f595a6212867b/Warren_Spector_Red_Shirt.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt408d86234d1898df/6511cd42d81f595a6212867b/Warren_Spector_Red_Shirt.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt408d86234d1898df/6511cd42d81f595a6212867b/Warren_Spector_Red_Shirt.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="8quwsv6fkn8y" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt408d86234d1898df/6511cd42d81f595a6212867b/Warren_Spector_Red_Shirt.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt408d86234d1898df">
        </picture>
        <figcaption>Leaving Origin wasn’t easy, but the opportunity to build a studio from scratch made the difference.</figcaption></figure><p>And then there was Thief (1998). A lot of people give me credit for having “made” that game. Leaving aside that no one person can be said to have made or created any game, let me clarify here and now that I worked on Thief for the middle year of a basically three-year development cycle. I’m proud to have been a part of it and hope I contributed something, but let’s cut out the “created by” stuff. Doug and I guy named Greg Lopiccolo&nbsp;led the Thief team to glory, not me.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt444e792a5db84b52/6511cdbdfac8e1c91ff222a4/Thief.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt444e792a5db84b52/6511cdbdfac8e1c91ff222a4/Thief.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt444e792a5db84b52/6511cdbdfac8e1c91ff222a4/Thief.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt444e792a5db84b52/6511cdbdfac8e1c91ff222a4/Thief.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="m5d24njkns7q" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt444e792a5db84b52/6511cdbdfac8e1c91ff222a4/Thief.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt444e792a5db84b52">
        </picture>
        <figcaption>NOT MY GAME! I WORKED ON IT FOR A WHILE. I’M PROUD OF MY CONTRIBUTION. BUT I DIDN’T “CREATE” IT!</figcaption></figure><p>Okay, that out of the way, there was one critical thing that came out of my involvement in Thief. There was a point where I just found an encounter too tough to sneak past, so I went to some folks in the team and said, “Just let me fight my way past things that are too hard for me.” They looked at me like I was crazy and said, “If we make the player powerful enough to fight, no one will ever sneak.” Of course, they were right for Thief. It’s a jewel of a game, tightly focused and beautifully designed. There’s a reason why it’s still cited as one of the best – maybe <em>the</em> best – stealth games ever made. At the time, though, what went through my head was, “I’ll show them. I’ll make a game where you can decide for yourself whether to fight or sneak past any problem.</p><p>Well, there came a point in 1997 where Boston-based Looking Glass was running short of funds and my Austin, Texas studio was the obvious place to make cuts. I remember telling Paul Neurath to shut us down. I mean, shutting down the mothership didn’t make sense. “I’ll find another deal. We’ll be okay.” That’s what I said. Arrogant? I guess. But I was pretty confident. I had a strong team and knew what game I was going to pitch. That was the point when I dug up a proposal I’d written up in 1995 – a pitch for a game called “Troubleshooter.”</p><h2>Ion Storm</h2><p>Troubleshooter was an attempt to make what I called “the real-world roleplaying game.” The main character was Jake Shooter, ex-CIA operative who the agency came to when they had a case too tough – or too shady – to handle themselves. The gameplay was, well, play Deus Ex and you’ll see. Problem was the real-world part of the plan. See, people know how the real world works – telephones and televisions and cars and everything else are well-understood and people have expectations of how they’ll behave. At the time – even today, I’d argue – we can’t live up to people’s expectations and the last thing you want to do in a game is thwart players expectations. The answer was to move the game into the near future – recognizable but far enough out that players couldn’t say, “That’s not the way a computer works!”</p><div><p>Then there was another consideration. Cool as a noir-like private investigator/spy could have been, it would have been about as silly as another space marine or knight in shining armor. And the kinds of situations the player would find themselves in might be thrilling, but a million other games had the spy/military thing covered. I knew we had to do something different. Cutting to the chase, that “different” looked like it was going to be an RPG variant of Command &amp; Conquer. Talk about cool! I would have done that in a heartbeat if John Romero hadn’t called me and offered me the opportunity of a life-time. <em>Another</em> lucky break! My life’s been full of them.</p><p>John drove down to Austin from Dallas and said “Make the game of your dreams. Biggest budget you’ve ever had. Biggest marketing budget you’ve ever had. And no creative interference.” Who the hell says no to that?! I sure as hell wasn’t about to. So we joined Ion Storm, with Eidos as our publisher. Both were great. Both lived up to every promise John made. I owe John a huge debt. I’ll never be able to repay it, but I hope our friendship and mutual respect is enough.</p></div><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt020ff2b146ea4371/6511ce3bd21fb05e23fdfce1/Ion_Storm_1999.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt020ff2b146ea4371/6511ce3bd21fb05e23fdfce1/Ion_Storm_1999.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt020ff2b146ea4371/6511ce3bd21fb05e23fdfce1/Ion_Storm_1999.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt020ff2b146ea4371/6511ce3bd21fb05e23fdfce1/Ion_Storm_1999.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="pc969xbvg1bh" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt020ff2b146ea4371/6511ce3bd21fb05e23fdfce1/Ion_Storm_1999.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt020ff2b146ea4371">
        </picture>
        <figcaption>What a bunch of reprobates! Tom Hall, John Romero, and me. Ion Storm in 1999.</figcaption></figure><p>But back to the story. We were set, deal-wise, but the remnants of my old Looking Glass team and I still knew our world-building needed work. One of the things I’m committed to is finding things people already care about rather than trying to <em>make</em> them care about something. So the team and I started looking around at what normal humans – not just gamers – were thinking and talking about. What we found was that Y2K was on people’s minds. The rise of AI. Terrorism. Human augmentation. Bioengineering. Unequal distribution of wealth. And there were so many conspiracy theories flying around in the cultural ether you could swat ‘em with a baseball bat with no fear of striking out. Those things would be the heart of our world. (Heck, you could make a game about all that today and it would <em>still</em> work. Which is kind of pathetic…) And, not to get too far ahead of myself, one of our rules was that nothing went into the game unless you could point to the real-world reference for it. We even got blueprints, maps and, of course, photographs of places and tools and so on.</p><p>Anyway, we had the foundation of a world and a clue about what the game was about. I knew I wanted to show those Thief guys that you could make a game that gave players choices about how to solve problems. The game had to be about how clever and creative players were, not how clever and creative we, as developers, were. I needed the right team, folks who bought into the world and the gameplay concept. I found them. I could talk about every person on the team but I’m going to risk alienating people and just mention three – Harvey Smith, the game’s Lead Designer, Chris Norden, the Lead Programmer (and later my Assistant Director) and Sheldon Pacotti, our Lead Narrative Designer. Talk about people who get too little credit!</p><p>Chris was a terrific coder who totally bought into the plan. He was what I call “the guy who told me ‘no.’” I’m kind of a kitchen sink guy and often need to be told to back off. More than anyone else, Chris was never shy about doing that. That said, he had an interesting “tell” when we talked that said “yes.” If I asked for something and Chris said “no,” a lot of the time I knew I’d see it working in a couple of weeks. Heck, sometimes the next day! To say he could make things happen would be an understatement.</p><p>And Harvey. What can I say about the guy? I knew he was special even when he was a tester on System Shock. I remember some great design conversations we had late in the night at Origin! He got the Imm Sim thing even then. I knew I needed him on Deus Ex, took him out to dinner, plied him with guacamole and told him what I wanted to do. Happily, he signed on. There’s a lot more I could say, but I’ll just leave it at this – Harvey was a natural-born design team leader and a critical leader overall whose contributions to Deus Ex were huge and under-appreciated.</p><p>Finally, Sheldon. I wanted him on the team from the minute I read the short stories he submitted when he applied for the narrative job. Terrific writer and super smart. People often talk about how “intelligent” a game Deus Ex is. Lots of people contributed to that – as a team we were committed to it – but Sheldon was The Man when it came to ensuring that our plot, NPCs, dialogue and in-game texts were smarter than a lot of other games. (As a note, I’m really proud of the “smartness” of DX. The name refers to Deus Ex Machina which in literary terms describes a bad narrative approach – look it up. It was also called that because the game was at least in part about the potential and peril of AI, of machines that become sentient – Deus Ex Machina... God from the Machine... get it? Plus, I thought it would be funny if people mispronounced it and had to say “sex.” That last one proves that I’m kind of a nitwit at times! And I will never give a game a name that players can’t pronounce. I’m sure embarrassment cost us some sales!</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2bf4d5bc2e4d0962/6511ceb2e821fe6b8e13800b/Deus_Ex_Team.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2bf4d5bc2e4d0962/6511ceb2e821fe6b8e13800b/Deus_Ex_Team.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2bf4d5bc2e4d0962/6511ceb2e821fe6b8e13800b/Deus_Ex_Team.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2bf4d5bc2e4d0962/6511ceb2e821fe6b8e13800b/Deus_Ex_Team.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="qo9ddsz75wmh" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2bf4d5bc2e4d0962/6511ceb2e821fe6b8e13800b/Deus_Ex_Team.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt2bf4d5bc2e4d0962">
        </picture>
        <figcaption>The incredible Deus Ex team. One of the best. And, yes, it was that small. That’s Chris Norden on the far left, Harvey next to him and Sheldon on the far right in the second row. Wish I could name them all, but this is long enough already!</figcaption></figure><p>Anyway, there’s a lot more to say about Deus Ex, but I’ll leave it at this: When we were getting ready to ship, I put my head down on my desk and thought, “If people compare our combat to Half-Life, we’re dead; if they compare us to Thief’s stealth, we’re dead; if they compare our RPG elements to Bioware’s latest, we’re dead. But if they get that they can <em>decide</em> how to play, to do <em>any</em> of those they want, we might rule the world.” I’ll leave it to others to decide which of those describes the finished product, but I’m pretty proud of the result.</p><p>Deus Ex was released in 2000. People liked it pretty well. As important, we on the team liked it. It was definitely a high point in my career and, I hope, in the careers of the team that made it possible and, well, made it!</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbcb2b25db752a4ae/6511cf1ac8c41b3f68950379/Deus_Ex_Launch_Day.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbcb2b25db752a4ae/6511cf1ac8c41b3f68950379/Deus_Ex_Launch_Day.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbcb2b25db752a4ae/6511cf1ac8c41b3f68950379/Deus_Ex_Launch_Day.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbcb2b25db752a4ae/6511cf1ac8c41b3f68950379/Deus_Ex_Launch_Day.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="0vwybxf9ji3f" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbcb2b25db752a4ae/6511cf1ac8c41b3f68950379/Deus_Ex_Launch_Day.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltbcb2b25db752a4ae">
        </picture>
        <figcaption>Deus Ex launch day. Yeah we were in a thing called a “software store.” Some of you may remember those...</figcaption></figure><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb2f61903e06d545a/6511d0381d8b4f7fb5a511c8/Harvey_Smith.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb2f61903e06d545a/6511d0381d8b4f7fb5a511c8/Harvey_Smith.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb2f61903e06d545a/6511d0381d8b4f7fb5a511c8/Harvey_Smith.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb2f61903e06d545a/6511d0381d8b4f7fb5a511c8/Harvey_Smith.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="l179o0m0c82j" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb2f61903e06d545a/6511d0381d8b4f7fb5a511c8/Harvey_Smith.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltb2f61903e06d545a">
        </picture>
        <figcaption>Harvey at the mic, me playing Superman behind him when Deus Ex won some awards at the Game Developers Conference.</figcaption></figure><p>Oh, one last thing. At the time, it was science fiction and people took it as such. I’ve said this before, but I’m not sure I’d make a game like Deus Ex today. Too many people would see it as a documentary. Did I say “pathetic” before. Yep. I did. Now, the idea that games should empower players and give them agency to tell their own stories? That I will always do. But the world and the narrative? Nope. Too dangerous in a world where the things we said about the world in Deus Ex have become our everyday reality. No. I’m not going there. Plus I have other interests and I’m going to indulge them as long as people will continue to fund me.</p><p>After Deus Ex, Ion Storm Austin did some sequels – Deus Ex: Invisible War (2003) and Thief: Deadly Shadows (2004). There are plenty of stories to tell about those, but the critical thing for me is that I’m not much of a sequel guy and so in 2005 I decided to leave the studio and do a start-up: Junction Point.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte6d7be9942d61750/6511d0b6eff67faa4db49a70/Warren_Spector_Awards_Case.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte6d7be9942d61750/6511d0b6eff67faa4db49a70/Warren_Spector_Awards_Case.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte6d7be9942d61750/6511d0b6eff67faa4db49a70/Warren_Spector_Awards_Case.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte6d7be9942d61750/6511d0b6eff67faa4db49a70/Warren_Spector_Awards_Case.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="2s98beysj1di" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte6d7be9942d61750/6511d0b6eff67faa4db49a70/Warren_Spector_Awards_Case.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blte6d7be9942d61750">
        </picture>
        <figcaption>One of the awards cases from Origin, Looking Glass and Ion Storm days. And yeah, I’m bragging. Wanna make something of it?</figcaption></figure><h2>Junction Point</h2><p>Junction Point got started in 2004 by a small group of Ion Storm folks, who like me, were ready for a new challenge.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf120709c7665b6e0/6511d13b55583f4b146cfbad/Junction_Point.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf120709c7665b6e0/6511d13b55583f4b146cfbad/Junction_Point.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf120709c7665b6e0/6511d13b55583f4b146cfbad/Junction_Point.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf120709c7665b6e0/6511d13b55583f4b146cfbad/Junction_Point.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="ogvvnahcvrmf" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf120709c7665b6e0/6511d13b55583f4b146cfbad/Junction_Point.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltf120709c7665b6e0">
        </picture>
        <figcaption>Junction Point, day one. We started out small, but we had big dreams</figcaption></figure><p>Most important was Art Min, my partner. I'm not sure I would have had the nerve to do a start-up myself - I'm pretty sure I would have failed - without him. We had worked together before at a couple of studios on several games and I knew he was a terrific coder and a natural-born leader.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltdcd163ba08028017/6511d2210c0dc861800c343a/Art_Min_and_Warren_Spector.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltdcd163ba08028017/6511d2210c0dc861800c343a/Art_Min_and_Warren_Spector.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltdcd163ba08028017/6511d2210c0dc861800c343a/Art_Min_and_Warren_Spector.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltdcd163ba08028017/6511d2210c0dc861800c343a/Art_Min_and_Warren_Spector.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="7n6r01x0oezo" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltdcd163ba08028017/6511d2210c0dc861800c343a/Art_Min_and_Warren_Spector.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltdcd163ba08028017">
        </picture>
        <figcaption>Art Min and me. I wouldn’t – probably couldn’t - have made Junction Point a reality without my partner in crime.</figcaption></figure><p>There were a dozen of us and we met at a local Mexican restaurant to plan our new endeavor. But there was one more piece of the puzzle - Seamus Blackley ("father of the Xbox"). He had been a developer of note but at the time we started Junction Point he had left development to be an agent at Creative Artists Agency and I signed on with him to represent me.</p><p>Seamus and the other game agents at CAA had a plan to remake the business model in gaming to be more advantageous to developers and it sounded to me like exactly what should happen. I won't go into the details here - that's not my story to tell - but I'll just say it really would have changed things.</p><p>Back home, the team and I started in on some new IP and came up with three that showed promise. One was an epic fantasy RPG/Immersive Sim called Sleeping Giants, based on a concept my wife, Caroline Spector, and I had come up with for DC Comics. (They passed...). One was a multiplayer action Imm Sim called Necessary Evil, a multiplayer game about augmented soldiers and independent operatives who could collaborate with or oppose each other, serving the military, businesses or mercenary groups. It was all about what happened when augmented military soldiers mustered out. I still think that’s a cool background for a game. Someone ought to make it so I can play it. The third concept stemmed from Seamus’s fertile imagination. He hooked me up with director, John Woo and together he and I developed an IP called Ninja Gold, designed to be a transmedia thing - movie and game simultaneously, with us collaborating on its creation. That was a trip. I loved working with John and learned a lot about the similarities between movies and games but, more important, some critical differences. That's a blog for another time.</p><p>The team got working on Sleeping Giants and Ninja Gold and did some great concepting and prototyping. Maybe I'll post some of the work they did some time. I have some cool docs and cooler videos... Anyway, we had a movie deal for Ninja Gold but no game deal at that time. The movie deal went away when the studio decided on a change of strategic direction. (A term that would come to haunt me later.) We got a game deal for Sleeping Giants. It went away - strategic direction again. We got another deal. (Seamus was good at his job!) That deal went away. I think we got another deal, but I can't really remember. Whether we got a third deal or not, we ended up with no deal. And no way to keep Junction Point alive. Until Valve called.</p><p>They offered us a lifeline in the form of a Half-Life 2 episode. Needless to say, I said yes, and the team got to work on yet another project. There's a whole story about that, but this is getting long enough that I'll just say it was set in Ravenholm and the team came up with a thing called the Magnet Gun. (You can imagine what it did.) The MG was unlike anything in the Half-Life arsenal and I thought it was incredibly cool - powerful, open to creative uses, and at times even funny. It took us a long time to master the Half-Life tech and just as we started building what I thought were very cool levels, Valve pulled the plug on us. The timing on that was pretty frustrating.</p><p>So in 2006, or thereabouts (my memory’s hazy), I was on the road with Seamus again, pitching three adult, action-oriented, original IP, desperate to keep the studio alive and the staff employed. (Indie studios don't have an easy row to hoe!) I pitched a bunch of publishers and potential funding partners, but didn't have much success. Then one day Seamus said, "Let's pitch Disney."</p><p>I thought he was crazy. "They're not going to be interested in any of these concepts," I said. To which he replied, "They've changed. Let's do it."</p><p>Oooo...kaaaay....</p><p>The day came for the Disney pitch. As I talked about the three concepts, the dozen or so people around the conference room table stopped paying attention and started checking their phones and all that. I was going to kill Seamus when I got out of there. But then the strangest thing happened.</p><p>Graham Hopper, who headed up Disney's game division at the time, asked me if I'd be interested in making any licensed games. Here's where you have to understand that I've been a huge Disney fan for... well... forever. I said, "Yeah. Give me The Night Stalker or Ducktales and I'm in." (I had an idea for a monster-of-the-week game that would have been perfect for ABC’s Night Stalker series and I love Carl Barks and Scrooge McDuck.) Graham looked at me and said, "What about Mickey Mouse?"</p><p>It took me three seconds to think, "Is he kidding? The most recognizable icon on player Earth?" I said, "Yes!" I have to admit that, with Mickey as my star, I could reach a mainstream audience with the Imm Sim game approach, but I probably would have done anything to work with Mickey.</p><p>Graham said, "We have a Mickey concept I'd like to show you. You don't have to use any of it. Just tell us what you think." So Luigi Priore, who's been with Disney forever, pulled up a PowerPoint and pitched a concept dreamed up by some interns. Some very, very creative interns! It had a lot of elements that made it all the way into the shipping Epic Mickey game. Graham reiterated that I didn't have to use any of their pitch but I had a quick reply. "Are you kidding? Why wouldn't I use some of those ideas? You have some pretty creative folks at Disney and that concept is really strong. You've given me an acorn and I want to grow it into an oak tree! I'm in!"</p><h2>The Interregnum</h2><p>Seamus negotiated a concept development deal for Epic Mickey. I spent six months in 2006 working with my old TOON design collaborator, Allen Varney, and Alex Duran, a talented programmer, to flesh out the idea. Together we came up with a pitch. (and by “pitch,” I mean a 240-page document and a PowerPoint deck). I went to Disney and they ushered me into a conference room with two doors. I went in one, gave the pitch, and eventually was ushered out the other to discuss things further. I was later told that if I'd been ushered out through the door where I'd entered, that would have meant the pitch was unsuccessful. Luckily, I went out through door number 2 and discussions continued.</p><p>After going out the good door, Graham said they wanted to move ahead. Sadly, over sushi, a biz dev guy told me shortly after that the only way I would get to make Epic Mickey was if Disney acquired Junction Point. Stupidly, I guess, I said no. I wasn't ready to sell (and if I’m being honest, the deal just wasn’t that good). Cut to a year later, 2007, and Disney came back and said they still wanted Junction Point to make the game but they still had to acquire the studio. I thought to myself, "How many times is Disney going to come back to me?" I said yes, Seamus negotiated the acquisition, and the deal was done. Almost.</p><p>The plan was to announce the deal at E3. The date was set. The time. The room booked. On the day, the press was filing into that room. Unfortunately, I hadn't yet signed the deal! There were a couple of points I wasn't happy with. So there I was outside with Graham who was waving the papers at me waiting for me to sign. I had Seamus on the phone talking me off a cliff. Finally, he just said "Do you want to work with these guys and make this game?” “Yes,” I said. “Then sign the deal" he said. I signed, the announcement went off just a little late, and I officially worked for Disney.</p><p>When I went back to the studio, two things happened: First, I called my mother to tell her the news. Her response tickles me to this day. What she said was "It's about time" - not "Congratulations" or "Are you crazy?" Just "It's about time." I told you I was a Disney fan!</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt632ca178a3a426e0/6511d2b21c83997060635038/Warren_As_A_Baby_1956.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt632ca178a3a426e0/6511d2b21c83997060635038/Warren_As_A_Baby_1956.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt632ca178a3a426e0/6511d2b21c83997060635038/Warren_As_A_Baby_1956.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt632ca178a3a426e0/6511d2b21c83997060635038/Warren_As_A_Baby_1956.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="zb6kvah00aoz" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt632ca178a3a426e0/6511d2b21c83997060635038/Warren_As_A_Baby_1956.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt632ca178a3a426e0">
        </picture>
        <figcaption>My mother and me – was there any doubt I’d end up a Disney fan?</figcaption></figure><p>The other thing was I went back to the team built to make adult Imm Sims and said "We're making a Mickey Mouse game." A great level builder and my narrative lead left because they simply didn’t want to make that kind of game. Best decision they could have made, for themselves and the project. If your team isn’t onboard and bought-in, you’re doomed. The rest stuck around because they were Disney fans like me or felt like they could become Disney fans by the end of the project. Which everyone did.</p><p>One other thing happened. Paul Weaver, one of the best Creative Producers and studio execs I've ever worked with signed on. We'd worked together at Ion Storm and with him onboard, I felt like Junction Point couldn't fail. I had that much confidence.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt14e38c91cc763d9b/6511d42249219b782a12f263/Paul_Weaver.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt14e38c91cc763d9b/6511d42249219b782a12f263/Paul_Weaver.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt14e38c91cc763d9b/6511d42249219b782a12f263/Paul_Weaver.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt14e38c91cc763d9b/6511d42249219b782a12f263/Paul_Weaver.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="bjki9160wvsu" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt14e38c91cc763d9b/6511d42249219b782a12f263/Paul_Weaver.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt14e38c91cc763d9b">
        </picture>
        <figcaption>Paul Weaver. Production and business powerhouse with substantial creative chops.</figcaption></figure><p>Paul brought along a senior designer named Chase Jones. I didn’t know Chase at the time, but he turned out to be one of the most talented designers I've ever worked with, another one of those natural leaders I’m lucky enough to stumble into. I put him in charge of the Epic Mickey design Allen, Alex and some incredibly creative interns had concepted. I functioned as Creative Director, with ultimate authority. (I always reserve one more vote than everyone else on my teams, combined).</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt7d8a692724357662/6511d46ef7552e73e17ccdc3/Chase_Jones.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt7d8a692724357662/6511d46ef7552e73e17ccdc3/Chase_Jones.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt7d8a692724357662/6511d46ef7552e73e17ccdc3/Chase_Jones.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt7d8a692724357662/6511d46ef7552e73e17ccdc3/Chase_Jones.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="l6z15ohn51ux" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt7d8a692724357662/6511d46ef7552e73e17ccdc3/Chase_Jones.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt7d8a692724357662">
        </picture>
        <figcaption>Chase Jones. Super-talented designer and team leader. Disney Epic Mickey and the sequel, The Power of Two wouldn’t have happened without him.</figcaption></figure><p>At the same time I was keeping an eye on another project I still don't want to talk about. And running the studio. And dealing with Disney. Fun times!</p><p>The Epic Mickey story is a long one so I'll glide over that. Suffice to say it was one of the highlights of my career. It took over 300 people, but we got it done and shipped in 2010. I loved its mashup of platformer, shooter, and adventure. I loved that it was what I called an Imm Sim Lite. (Core gamers didn’t get that, which pains me to this day, but normal humans did, which was pretty darn satisfying.)</p><p>Among the coolest aspects of my Disney experience was that me team and I got to reintroduce Oswald the Lucky Rabbit to the world. (Oswald was Walt Disney's first cartoon star, lost to him in a 1928 contract dispute with his distributor. And, man, is there a story there – about the contract and about how Disney got the rights to Oswald back. A story for another time.) We got to give Oswald a girlfriend - Ortensia - who everybody at Disney and the world now thinks was part of Oswald's world back in the '20s. She wasn't - we named her... brought her to the screen... in a game... and no one realizes it. I laugh every time I think about that!</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt95379c4063888af1/6511d4bcc5e74d769ea57d6f/Oswald_and_Ortensia.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt95379c4063888af1/6511d4bcc5e74d769ea57d6f/Oswald_and_Ortensia.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt95379c4063888af1/6511d4bcc5e74d769ea57d6f/Oswald_and_Ortensia.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt95379c4063888af1/6511d4bcc5e74d769ea57d6f/Oswald_and_Ortensia.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="9cqdzlqohc2w" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt95379c4063888af1/6511d4bcc5e74d769ea57d6f/Oswald_and_Ortensia.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt95379c4063888af1">
        </picture>
        <figcaption>Oswald the Lucky Rabbit and Ortensia, his girlfriend.</figcaption></figure><p>The first game did well when it was released in 2010, so we did a sequel, with Chase very much in charge, supported by Paul and with some help from me. It took almost 800 people to make Disney Epic Mickey: The Power of Two in 2012, but we introduced a lot of new things, most notably adding multiplayer and Broadway musical-type songs. I've always wanted to make a musical game and this was a baby step in that direction. Someday I'll make a full-on, interactive, Imm Sim musical game that is far more than just beat-matching. I have ideas about how to do it and I will… Soon as someone's crazy enough to fund it.</p><p>After that, we started working on an Epic Donald game with the Oliver Twins over in the UK. They and their team did a great job. Concepting was going well. Concept art was spectacular. A prototype showed promise. My first clue that things were going south at Disney was when I pitched the game for continued concepting and the project didn't get greenlit. Sixteen people in a room voted no when they were told by a Marketing guy that data showed that Donald didn't test well. I was seething mad about that conclusion! I didn’t and don’t trust data and research much. Specifically, I felt like they hadn’t tested in Europe and Scandinavia, where the ducks are incredibly popular. And even if it were true that the ducks weren’t popular in the States, didn’t that make it our job to change that and put Donald in a position to test better?</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt0da5e4e47b4472b1/6511d52a3da3ec55db4396c1/Ducktales.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt0da5e4e47b4472b1/6511d52a3da3ec55db4396c1/Ducktales.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt0da5e4e47b4472b1/6511d52a3da3ec55db4396c1/Ducktales.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt0da5e4e47b4472b1/6511d52a3da3ec55db4396c1/Ducktales.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="56jxxvax3eev" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt0da5e4e47b4472b1/6511d52a3da3ec55db4396c1/Ducktales.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt0da5e4e47b4472b1">
        </picture>
        <figcaption>I got to write a series of Ducktales comics despite the demise of Epic Donald! For a Carl Barks fan that was both intimidating and incredible!</figcaption></figure><p>But the non-greenlighting of Epic Donald sent us back to the creative and business drawing board.</p><p>Paul and I – mostly Paul - worked up dozens of sku plans and budgets, but none of them got approved by the powers that be. (It’s worth pointing out that “the powers that be” no longer included Graham Hopper. He was a guy who actually cared about games not just about business, but he’d been replaced. So kudos to him for having some vision. Whatever the opposite is of kudos go to the folks who replaced him. That’s all I’ll say about that!)</p><p>The rest of the Disney-Junction Point story is all business nonsense I'll maybe talk about some other time. The important thing is that Disney shut down Junction Point in 2013. Two hundred people lost their jobs. And after seven years, I was no longer a cast member and, for the first time ever, I was out of work.</p><h2>The Denius-Sams Gaming Academy</h2><p>I'm embarrassed to admit I spent several months after the shutdown of Junction Point sitting on a couch with a remote control in my hand watching YouTube videos. My wife finally told me I needed to get up and find something to do. I got lucky <em>again</em>. A call came in late 2013 from the head of the College of Communication at the University of Texas at Austin asking if I wanted to create a game development program. I’d taught a Master Class in Game Development in 2007, I think it was. (Videos of all the sessions are still on You Tube and I think some of the content is still relevant today.) Anyway, he told me the College had funding for three years and wanted to set up something innovative – a certificate program that would be free for 25 students a year. Yeah, it was a great deal for students and sounded like a fun change of pace to me, so I agreed to a three-year run as leader of whatever program I could sell the University on.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt527404b81bd4e2cc/6511d67d62943278f1ac48b2/WS.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt527404b81bd4e2cc/6511d67d62943278f1ac48b2/WS.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt527404b81bd4e2cc/6511d67d62943278f1ac48b2/WS.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt527404b81bd4e2cc/6511d67d62943278f1ac48b2/WS.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="12d9zpgc754s" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt527404b81bd4e2cc/6511d67d62943278f1ac48b2/WS.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt527404b81bd4e2cc">
        </picture>
        <figcaption>Me in front of the Texas Tower on the University of Texas at Austin campus.</figcaption></figure><p>First thing I did was research what all the other university and college game development programs were doing. There were, according to the ESA, 400 institutions of higher learning offering game dev courses or programs. I didn’t see any point duplicating them. What I found was that all of them I could find and research were teaching actual development – the making of games – and often in the context of small, indie-style teams working on very small projects. And I found that many, maybe most, student projects never reached completion. Finishing things is one of the hallmarks of professionalism. I’ll blog about that someday. (Before any educators start berating me, that wasn’t universally true, but definitely the predominant strain of games education.)</p><p>I knew I wanted to do something different, so I decided to focus on creative and business <em>leadership</em> as it related to relatively large teams. I wanted my students working to complete two projects during the year – a simple one in the first semester and a more complex one in the second. Each student would have a specialty – just like on a real, studio dev team. There would be leadership lectures in the morning and development work in the afternoon and evening.</p><p>My faculty would consist of two folks who had real industry experience and had worked on a mainstream PC or console games recently enough that their experience was still relevant. Too many game dev programs are taught by people who’ve been out of the professional world for too long or have never worked on a game at all. Given how quickly things change in the world of gaming (see below!) either was, let’s just say suboptimal... I found my staff in two excellent developer/teachers – Joshua Howard and D.S. Cohen.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt8cbbffce3ff6cce1/6511d7258a0039fbf6bd1618/WS_2.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt8cbbffce3ff6cce1/6511d7258a0039fbf6bd1618/WS_2.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt8cbbffce3ff6cce1/6511d7258a0039fbf6bd1618/WS_2.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt8cbbffce3ff6cce1/6511d7258a0039fbf6bd1618/WS_2.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="ss1i8tethl58" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt8cbbffce3ff6cce1/6511d7258a0039fbf6bd1618/WS_2.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt8cbbffce3ff6cce1">
        </picture>
        <figcaption>Joshua Howard, me, and D.S. Cohen, instructors at the Denius-Sams Gaming Academy. Here we are preparing to instruct.</figcaption></figure><p>We interviewed dozens and dozens of student candidates, as rigorously as if we were hiring a team of our own. We quizzed them on their development skills but also on their leadership potential. We wanted the best of the best, and I think we found them. The success our graduates have achieved speaks to the quality and qualities of our ex-students as much as – more than – our training of them. I can’t tell you how proud I am of them. I’m as interested in making people as I am in making games at this point in my career (though I have no plans to stop doing either!) and working with the Academy students was a great way to scratch that itch.</p><p>In any event, I haven’t even gotten to the innovative thing the program offered – the real, hands-on opportunity for each student to take on the creative and business leadership opportunities on the projects. Every two weeks, we created a development duo, one playing what might be called the Creative Director role and the other the Producer role. The staff met with these duos daily to talk about what they were doing well and where they needed to do more work. Everyone got to put into practice the leadership lessons learned in the morning lectures.</p><div><p>You’d have to talk to the students to determine how well the program met their needs and prepared them for the real world, but all four semesters resulted in finished games of high quality. And almost all of the students got real-world jobs and, as I said, went on in most cases to do some pretty impressive things.</p><p>I was proud of what we were doing, but mid-way through my three years building and then running the program, I started to feel the itch to get back into development myself. There were still things I wanted to make. As my commitment neared its end, I started thinking about what might come next. I could stay on and be a teacher for the rest of my life. That would have been cool. But I started entertaining the idea of doing a new start-up. I had ideas for games to develop – my One-Block Roleplaying Game... an Interactive Broadway-style musical... An outdoor, multi-player Immersive Sim...</p></div><p>It was 2016 and Paul Neurath entered my life again.</p><h2>OtherSide</h2><p>It was one of those magical, out-of-the-blue calls - the kind of call that has changed the trajectory of my life so many times. This time Paul told me he was doing a start-up of his own, had the boring business stuff under control, and asking if I wanted to join him as a co-founder.</p><p>Hm. Do a start-up where all the HR and business stuff was already in place? Intriguing. The hook was in the water. Then he baited it.</p><div><p>“I have the rights to make a new Underworld game. And I have the rights to make a new System Shock game. I’m going to make the Underworld game up north. You can make the new Shock game. In Austin” (my hometown).</p><p>It was another of those moments – like the Steve Jackson Games job I probably didn’t deserve,,, like the TSR moment... the Origin moment... the Ion Storm moment... the Mickey Mouse moment... It was yet another “Who says no to that?” moment in what’s been a blessed career.</p></div><p>So Paul and I set about building two offices, one in the Boston area, the other in Austin.</p><p>The story of System Shock 3 is long and convoluted. Basically, though, I built a team. I had a concept that included a feature no one had ever seen in a game before. We got funding. We worked at it for a while. The business side of the project went kaflooey. Suddenly, I was looking for a new concept and a new team to build it.</p><p>I had those three ideas for games to make I mentioned earlier. I picked one - Argos: Riders on the Storm (working title), the one that seemed most obviously the next step for Imm Sims. I could have gone for one of the projects I thought would just be cool, personally. But I’m a relentless advocate for Imm Sims and we’re not done defining that genre. Argos would be a next step.</p><p>We’ve built a core team to concept it with me, and prototype it and plan it. That’s really about all I can say right now. Okay, I <em>will</em> say that the team we’ve built so far is one of the strongest I’ve ever worked with. They have the potential to be a team on par with the best in my career – and I’ve been lucky enough to have worked with some great groups. They constantly come back to me with ideas and approaches to realizing the vision better than I could have imagined (which is exactly what you want from a team!) And the concept is... out there. Even I think it’s a little nuts. But if ... <em>when</em>... we pull it off... Man,..</p><p>In addition to being the Creative Director on Argos, I’m the Chief Creative Officer for OtherSide, which means I get my fingers in every game we put in development. My job as CCO involves two things:</p><p>First, to safeguard the studio’s mission. That means empowering players to experiment with deeply interactive worlds and craft their own unique experiences through their play and playstyle choices. In other words, kind of what I’ve tried to do for forty years. Paul Neurath and I are in lockstep on this. No compromises.</p><div><p>Second, it’s my responsibility to help all of our teams, in any way I can, to build the best version of the games they want to make. Tempting as it is to violate this rule, I have to remind myself that it’s <em>their</em> game, not mine. No compromises there either.</p><p>On Argos, I play a central Creative Director role in determining what the game is going to be. But we’re also making another game, Thick as Thieves, which has its own team, its own leaders, and a somewhat different approach to realizing the company mission. Like Argos, and as every game should, Thick as Thieves has some elements that I think will surprise and please players. There’s a great leadership team in place and the rest of the team is strong. I can’t say anything more yet. Sit tight.</p></div><p>In 2023, OtherSide was acquired by a new company called Aonic. It’s early days, but I’m psyched to be a part of their family and I’m looking forward to seeing where our partnership goes. The future looks bright.</p><h3>Ch...ch...ch...changes</h3><p>So I survived 40 years. What's changed in that time? It really is tempting to go with the cliche “everything.” And there's some truth to that:</p><p>For starters, I have a lot more gray hair. But that’s significant only to me. There are plenty of genuinely important changes to talk about.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbec810f0ec89a16b/6511d78ab7734b57dd78dba6/Ion_Storm_2023.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbec810f0ec89a16b/6511d78ab7734b57dd78dba6/Ion_Storm_2023.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbec810f0ec89a16b/6511d78ab7734b57dd78dba6/Ion_Storm_2023.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbec810f0ec89a16b/6511d78ab7734b57dd78dba6/Ion_Storm_2023.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="asfa50x69sqq" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbec810f0ec89a16b/6511d78ab7734b57dd78dba6/Ion_Storm_2023.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltbec810f0ec89a16b">
        </picture>
        <figcaption>Remember that picture earlier of the three reprobate designers at Ion Storm? Here we are in 2023 at the ceremony where John Romero got his well-deserved Lifetime Achievement Award at GDC. That’s Tom Hall on the left, then John, then me.</figcaption></figure><p>When I started, we had nothing, conceptually, development-wise, technologically or tools. There were no game engines to make life easier. There weren't even any existing genres to borrow from and guide feature development. There were no sound cards. (One major sound card company told us they owed their survival to the first Wing Commander game...) Hell, when I started, we had one color - green - and got way too excited when CGA came along! We were making it up as we went along. which was pretty exciting, let me tell you. That feeling of creating something – an entire medium, not just a single game, was pretty thrilling.</p><p>There was no E3 show. And the game space at CES was silent as a tomb. Until one year, Chris Roberts rented what passed for a big-screen TV and a movie theater sound system to demo Wing Commander. Let’s just say the folks in charge of the show came by to make us turn the volume down! We ignored them. You can blame Chris and Origin for the hearing-damaging volume levels at game conventions!</p><p>Selling 100,000 copies of a game was called "going gold" and when that happened you went and bought yourself a Ferrari.</p><p>There was no digital distribution or online and multiplayer gaming. We sold in baggies or boxes filled with disks. The first time we shipped a game you could install on a 10-megabyte hard drive, we said "No one has a hard drive that big - we'll have to ship the game with a drive in the box. And the first time we saw a CD - a CD mind you - we stared at it like apes seeing the monolith in 2001 and said "no one will ever fill one of these up." (We were wrong about that, by the way.)</p><p>We actually depended on these things called "magazines" for reviews and PR.</p><p>We posted fan letters... on paper... some of them handwritten... on office walls.</p><p>The 3D revolution hadn't even begun. There were no games that featured 3D modeling - some of us spoke at SIGGRAPH about the 3D revolution in games even as we were in the midst of that revolution. And speaking of 3D, remember what I said earlier about the first time I saw Paul Neurath's tech demo of a real-time, fully-textured first-person, 3D engine and thought to myself, "The world just changed."</p><p>Audience expectations have gone way up. When I started, games were something new and sold to a limited audience who just wanted a novel kind of entertainment. They were D&amp;D players and technophiles who’d never seen anything like this new thing called “computer games” or “video games.” They – we really were geeks who reveled in our ability to master a medium normal people didn’t get. Today, the mass audience has jumped on the bandwagon and developers must exploit the capabilities of machines powerful enough to dwarf anything we could have imagined. Graphics and gameplay have to be first-rate. User interfaces that use every key on a keyboard and didn’t even have the option of a mouse have to be user friendly today at a level we didn’t have to achieve. We were pioneers back in the day and people expected hardship. We’re settled city dwellers today and our audience expects comfort, usability and quality in all aspects of their experience.</p><p>There were only two ways to reach an audience – through a publisher with connections with software stores or in ziplock bags sold by small studios or individual developers through the mail. Advertisements in something called “magazines” were a major distribution outlet for us, Today there are so many business models – so many ways to reach an audience and make some money – it’s hard to keep up.</p><p>And I haven't even gotten to the internet, always-on connectivity, a computer in everyone's pocket, free-to-play, microtransactions, designing games to be played forever instead of being satisfactorily completable. And there's this early access stuff that sounds to me like shipping a game before it's ready. Not only did those things not exist - we couldn't even have imagined them.</p><p>Today, anyone who says they’re not a gamer is either an outcast (like gamers used to be) or a liar.</p><p>I could go on, but you get the idea.</p><h3>Same old same old</h3><p>So what hasn't changed?</p><p>Things still change every day, just as they did when I started - new genres, new graphical approaches, new distribution methods, new multiplayer modes, new business models, and so on. That may sound like a contradiction, like a list of things I should have mentioned earlier, in discussing things that have changed. The reason it's here is because there's as much innovation possible today as there ever was. I learn from younger developers every day, and they learn from each other, just like I learned from my peers earlier. That hasn’t changed. Nor has the necessity of being a life-long learner. Yes, it’s a cliche, but it's one I proudly adopt for myself. And you should adopt yourself. My guess is you already have. I know game developers well enough to be comfortable saying that. There's always something new to learn.</p><p>People and teams are as committed, as dedicated and as in love with what they do as they were 40 years ago. I literally cried at work recently because the team I'm working with listened to a high-level vision and found a way to express it that was so much more than I expected. Better. Deeper. I cried out of pride in them and gratitude for the gift they gave me then and give me every day.</p><p>Games are and always will be - or at least have the potential to be - storytelling tools. But not in the way earlier media were. We can empower players to tell their own stories through their play choices. We can turn every player into an author. That was true in the early days and it's even truer today.</p><p>Things to learn, committed developers and games as a unique medium? All true 40 years ago. Still true today.</p><h3>Survival</h3><p>Now that you know my story, how did I survive it? Here's a partial list. Maybe you can relate to some of these, see them in your own life, and hold them close to make <em>your </em>survival more likely.</p><p>I've been lucky enough to be able to fill every role associated with game development that didn't involve engineering, art and sound. I've been on the business side and the creative side. I've worked for publishers and developers. I've been a designer, a game director, a creative director and a chief creative officer. I've worked with internal teams and external teams. If variety is the spice of life, I've had the Thai food of careers. Spicy! I'm sure "never been bored" is part of my equation.</p><p>I’ve been surrounded by people way smarter than I am and I've learned something from them every day. I get a lot of credit for a lot of games, but I’d be nowhere without folks like Steve Jackson, Richard Garriott, Paul Neurath, Doug Church, Harvey Smith, Art Min, John Romero, Seamus Blackley, Paul Weaver, Chase Jones and, of course, my understanding and long-suffering wife who's helped me when the chaos of development got me down. There are too many more to name. These were my friends and partners. All have been my teachers and some of them more than that - mentors. I owe each of them a debt of gratitude I can never repay. Find folks like this and cherish them.</p><p>I've been driven to and been given the opportunity in a variety of forums to be a relentless evangelist for a particular kind of game - the Immersive Simulation. It’s a genre I love and to this day think is critical to our maturation as a medium. Note that there's still plenty of work to be done in that space if any of you want to join me – literally, as team members, or figuratively, as fellow creators and evangelists.</p><p>I get bored easily. That might not sound like a survival trait but think about it. Boredom can spur you on to try new things, to challenge yourself, maybe even to boldly go where no one has gone before. I often joke that I’ve been making the same game over and over again, just a bit better each time. To an extent that’s true. I have no interest in making a game that doesn’t allow each player to create their own unique experience. I’m a dyed-in-the-wool Immersive Sim guy. But look more closely and you’ll see that the content and associated techniques never repeat. I’ve worked on fantasy and SF games, original IP as well as sequels to other people’s work and licenses. I’ve mashed up existing genres to create something new and unique. Look at Deus Ex - part RPG, part Shooter, part Stealth game. Or Disney Epic Mickey - part Adventure game, part Shooter (think about it…), part Platformer. And my new game, well, let’s just say it’s one part something different than anything I’ve worked on before, combined with something else and something else again. (You have no idea what’s coming…). Trust me when I tell you that part of my survival is that I’ve never had to repeat myself.</p><p>People have appreciated the work my teams and I have done and have expressed it loudly and affectionately. I may or may not deserve accolades, but I've been lucky enough to receive them and I'm grateful for all the kind words and well-wishes I've received. Maybe it's just my ego, but what I consider to be my successes have nothing to do with reviews, sales or revenue. Success for me is connection with players (and not in the data-collecting way some of you may be thinking). I've had people send me handmade plush toys based on characters in my games. I've had people send me artwork they were inspired to create. I've had people tell me a game I worked on helped get them through chemotherapy. Autism. Cerebral palsy. I ran into a young woman at Disneyland dressed as Ortensia, in a homemade costume, before the character was a star in the Disney firmament. “I started my company because of your game,” I’ve been told. And “I changed the way I thought about design because of a game you worked on.” Now those are success criteria that have kept me going, even when things got tough.</p><p>I've had the opportunity to help, in a small way, create and define a new art form, an opportunity that comes along only two or three times a century. I’ve always thought games were - or could be - important, artistically and culturally. And I'm just pretentious enough to want to make art and be a minor contributor to a medium that has and will continue to change the world. (As a note, that opportunity still exists - we're not a solved problem like other media. We've barely scratched the surface of what games can and should be.)</p><p>I’ve been exceptionally lucky. Every time I’ve needed, wanted, or benefited from a change, a phone call came my way offering me a new opportunity. I’ve always had new challenges and new things to learn. Hard to get bored when that’s the nature of things. That said, I believe that you make your own luck, to an extent. I read somewhere that in order to get hit by a train you have to stand on the tracks. A silly way to put it, but with a grain of truth to it. What it means to me is that you have to work hard to make sure you’re in a position where luck can come your way. I think I’ve done that. I’ve prepared for change and put myself in positions where people want to call me. So they do.</p><p>I’ve loved my work, even when I've hated it. (You developers know what I mean!). I hope every one of you reading this feels that love every day when you go to the office (those of you who still <em>go </em>to an office...).</p><p>Finally, I’m not qualified to do anything else but make games. Most of you can get jobs somewhere else. I guess I could teach, but basically, I got nothing but this. I've <em>had </em>to survive!</p><h3>Conclusions</h3><p>So what conclusions can I draw from 40 years of making games?</p><p>I couldn’t have survived without my mentors, notably Steve Jackson and Richard Garriott. Steve gave me what I think of as my undergraduate degree in game development. Richard gave me my graduate degree. They were integral to everything I've done and I guess become... As I said before, I’ve had too many teachers and collaborators to name. Whether I worked <em>for</em> them or they worked for me, whether we were peers or partners, I’ve tried to learn something from all of them. It isn't too strong a statement that I’ve been taught more by others than I taught them. Well, at least as much as I hope I've taught them. Mentors, teachers, teammates, friends and family are, in a real and tangible sense, responsible for "my" success. If you don't have people like this in your life, find them.</p><p>What I discovered at Origin, Looking Glass, Ion Storm, Junction Point and most recently OtherSide, is that I'm not the world’s greatest designer. I’m certainly not as good as some of the other designers I've worked with. What I'm best at is coming up with a concept and communicating a core vision clearly and compellingly before finding star performers who make that vision real, in as great a way as I could have expected or, usually, even better.</p><p>I’m also pretty good at identifying people with a strong, clear vision of their own and helping them express that vision better than they could without a few pushes and nudges here and there. One of the most important lessons I’ve learned is that if your game doesn't have a clear vision, and you can't express and "sell" it and if you don't hire people better than you are, you're likely to fail. At best you're on the road to mediocrity. Life is too short to work on games that you know are going to be simply okay.</p><p>Finally, to younger developers (which is basically all of you given my... ahem... experience), to you I say, don't lose sight of a significant part of your job - maybe the most important part of your job. It's up to each and every one of you to make things so amazing and innovative that people forget about guys like me. Seriously. I want you to stun me, to do things I can't even imagine. Don't settle for rehashes of earlier games that differ only by virtue of prettier pictures. Show me things I've never seen or done before. Games are not what I call a "solved problem." Other media, more mature than us, may offer different content and even different development tools and distribution models, but in formal terms, movies and novels, to name just two, haven't changed much in 100 years or more. Editing techniques and narrative structures are well-understood. Games are a mystery, still.</p><p>I'll wrap this up by saying that the coolest thing I've seen in the last 40 years is that we changed the world - not me, not the individual companies I've worked for. But the game business and the medium as a whole and all the intelligent, creative people working in it. We went from games for geeks to games for everyone. Our sales dwarf those in any other medium of expression ever. Ever. Think about that. We’re a powerful cultural force. We've developed an entirely new art form. We’re the only medium in the history of humankind that can turn every consumer into a creator and, astonishingly, we do that through the power of play. Think about <em>that</em>!</p><p>We changed the world, yes, but we’re not done yet. As I said, we’re not a solved problem. Anyone reading this could be the next agent of change, whether you work for a huge company or a small one. There’s no telling where innovation will come from. So get to it! Give me a game unlike any I’ve ever played. Show me what you got! Change the world.<br></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rethinking the Luddites in the Age of A.I (193 pts)]]></title>
            <link>https://www.newyorker.com/books/page-turner/rethinking-the-luddites-in-the-age-of-ai</link>
            <guid>37664682</guid>
            <pubDate>Tue, 26 Sep 2023 19:31:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newyorker.com/books/page-turner/rethinking-the-luddites-in-the-age-of-ai">https://www.newyorker.com/books/page-turner/rethinking-the-luddites-in-the-age-of-ai</a>, See on <a href="https://news.ycombinator.com/item?id=37664682">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="ArticlePageChunks"><p>On December 15, 1811, the London <em>Statesman</em> issued a warning about the state of the stocking industry in Nottingham. Twenty thousand textile workers had lost their jobs because of the incursion of automated machinery. Knitting machines known as lace frames allowed one employee to do the work of many without the skill set usually required. In protest, the beleaguered workers had begun breaking into factories to smash the machines. “Nine Hundred Lace Frames have been broken,” the newspaper reported. In response, the government had garrisoned six regiments of soldiers in the town, in a domestic invasion that became a kind of slow-burning civil war of factory owners, supported by the state, against workers. The article was apocalyptic: “God only knows what will be the end of it; nothing but ruin.”</p><p>The workers destroying the lace frames were the group who called themselves Luddites, after Ned Ludd, a (likely fictional) knitting-frame apprentice near Leicester who was said to have rebelled against his boss by destroying a frame with a hammer. Today, the word “Luddite” is used as an insult to anyone resistant to technological innovation; it suggests ignoramuses, sticks in the mud, obstacles to progress. But a new book by the journalist and author Brian Merchant, titled “<a data-offer-url="https://www.amazon.com/Blood-Machine-Origins-Rebellion-Against/dp/0316487740" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Blood-Machine-Origins-Rebellion-Against/dp/0316487740&quot;}" href="https://www.amazon.com/Blood-Machine-Origins-Rebellion-Against/dp/0316487740" rel="nofollow noopener" target="_blank">Blood in the Machine</a>,” argues that Luddism stood not against technology per se but for the rights of workers above the inequitable profitability of machines. The book is a historical reconsideration of the movement and a gripping narrative of political resistance told in short vignettes.</p><p>The hero of the story is George Mellor, a young laborer from Huddersfield who worked as a so-called cropper, smoothing the raised surface of rough cloth with shears. He observed the increasing automation of the industry, concluded that it was unjust, and decided to join the insurgent Luddite movement. A physically towering figure, he organized his fellow-workers and led attacks on factories. One factory owner who was targeted was William Horsfall, a local cloth entrepreneur. Horsfall threatened to ride his horse through “Luddite blood” in order to keep his profitable factories going, hiring mercenaries and installing cannons to defend his machines. In the background of the story, figures such as the ineffectual Prince George, a sybaritic regent for his infirm father, George III, and Lord Byron, the poet, who voiced his sympathy for the Luddites in Parliament, debate which side to support: owners or workers. Byron exhorted the workers in his poem “Song for the Luddites” to “die fighting, or live free.”</p><p>Merchant ably demonstrates the dire stakes of the Luddites’ plight. The trades that had sustained livelihoods for generations were disappearing, and their families were starving. A Lancashire weaver’s weekly pay dropped from twenty-five shillings in 1800 to fourteen in 1811. The market was being flooded with cheaper, inferior goods such as “cut-ups,” stockings made from two pieces of cloth joined together, rather than knit as one continuous whole. The government repeatedly failed to intervene on behalf of the workers. What option remained was attacking the boss’s capital by disabling the factories. The secretive captains of the Luddite forces took on the pseudonym General Ludd or King Ludd, which they used to write public letters and to sign threats of attacks. The spectre of violence led some factory owners to abandon their plans for automation. They reverted to manual labor or closed up shop completely. For a time, it seemed that the Luddites were making headway in empowering themselves over the machines.</p><p>The book offers plenty of satisfying imagery for the twenty-first-century reader experiencing techlash. Merchant argues that the message of Luddism is just as relevant today, as our lives become increasingly enmeshed with digital platforms, from TikTok to Uber and Instacart, that translate our labor and attention into profit, “overlaying a sort of psychic factory onto its workers’ lives.” (Who hasn’t at times wished to take a hammer to their MacBook?) The Luddites sought revenge against the innovation that was holding them hostage. In Merchant’s telling, they were activists, punks, and masked celebrities standing up for the skilled working class, the successors to Robin Hood, another product of Nottingham. “Luddite” by that measure sounds like a compliment.</p><p>“Blood in the Machine” is being published just as we are facing a new wave of technological automation centering on <a href="https://www.newyorker.com/science/annals-of-artificial-intelligence/can-we-stop-the-singularity">artificial intelligence</a>—which some, including the consulting firm McKinsey, have labelled the “Fourth Industrial Revolution.” Merchant uses anachronistic terms like “startup” and “tech titan” to describe early factories and entrepreneurs, seeking to draw parallels with the present. (The book’s analytical sections are weaker than its narrative ones.) The “labor-saving technology” of today threatens new categories of jobs: customer service is being performed by chatbots; Amazon is selling e-books written by ChatGPT. Designers and illustrators are losing jobs to image generators; translators are being asked to “clean up” transcripts generated by A.I. The profusion of <a href="https://www.newyorker.com/culture/infinite-scroll/my-ai-writing-robot">dubious A.I.-generated content</a> resembles the badly made stockings of the nineteenth century. At the time of the Luddites, many hoped the subpar products would prove unacceptable to consumers or to the government. Instead, social norms adjusted. Both the mass-manufactured products and the regimented jobs that produced them quickly became entrenched.</p><p>The Luddites watched as sprawling factory buildings rose over their rural towns, concentrating labor that had traditionally been performed independently in the home or small workshops. The working conditions in those factories, often staffed by children, were execrable; the horror stories that emerged, of mangled limbs and bodies, eventually helped encourage reform. The victims of automation today are less immediately obvious. ChatGPT users can’t see the low-paid content moderators in countries such as Kenya who undergird <a href="https://www.newyorker.com/news/the-new-yorker-interview/its-not-possible-for-me-to-feel-or-be-creepy-an-interview-with-chatgpt">the program</a>’s output, performing an onerous psychological task that <a data-offer-url="https://crowd.cs.vt.edu/wp-content/uploads/2021/02/CHI21_final__The_Psychological_Well_Being_of_Content_Moderators-2.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://crowd.cs.vt.edu/wp-content/uploads/2021/02/CHI21_final__The_Psychological_Well_Being_of_Content_Moderators-2.pdf&quot;}" href="https://crowd.cs.vt.edu/wp-content/uploads/2021/02/CHI21_final__The_Psychological_Well_Being_of_Content_Moderators-2.pdf" rel="nofollow noopener" target="_blank">studies have shown</a> can induce P.T.S.D. There is no single machine that can be smashed to disable artificial intelligence. If the physical server farms that host A.I. programs were attacked, the software could simply be hosted elsewhere. What’s more, the foundation of A.I. is the raw material that humanity has already labored to produce: reams of text and images that programs process into patterns and then remix into fresh “content.” Unlike the machines of the first Industrial Revolution, A.I. does not necessarily need more input; it can sustain itself. “Jobs are definitely going to go away, full stop,” Sam Altman, the C.E.O. of OpenAI, recently told <a href="https://www.theatlantic.com/magazine/archive/2023/09/sam-altman-openai-chatgpt-gpt-4/674764/"><em>The Atlantic</em></a>.</p><p>The tragedy of the Luddites is not the fact that they failed to stop industrialization so much as the way in which they failed. In the end, Parliament “sided decisively with the entrepreneurs,” as Merchant writes, and frame-breaking was made a capital offense. Dozens of workers were executed for Luddite activities, including, in January of 1813, fourteen in one brutal day. George Mellor, the Luddite captain, was eventually convicted of assassinating Horsfall, the factory owner, and was hanged, at the age of twenty-three. Human rebellion proved inadequate against the pull of technological advancement.</p><p>“Blood in the Machine” suggests that although the forces of mechanization can feel beyond our control, the way society responds to such changes is not. Regulation of the textile industry could have protected the Luddite workers before they resorted to destruction. One proposal suggested a tax on every yard of cloth made by machine. After a pro-worker bill failed to pass in the House of Lords, Gravener Henson, a frame knitter turned advocate and historian, led an association of workers that demanded higher wages and labor protections, though such “combination” was outlawed at the time in the U.K. Eventually, Luddism faded into a more general political movement. By the late nineteenth century, the majority of Nottingham’s lace production had been mechanized. In the era of A.I., we have another opportunity to decide whether automation will create advantages for all, or whether its benefits will flow only to the business owners and investors looking to reduce their payrolls. One 1812 letter from the Luddites described their mission as fighting against “all Machinery hurtful to Commonality.” That remains a strong standard by which to judge technological gains.&nbsp;♦</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Prophet: Automatic Forecasting Procedure (258 pts)]]></title>
            <link>https://github.com/facebook/prophet</link>
            <guid>37663820</guid>
            <pubDate>Tue, 26 Sep 2023 18:35:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/facebook/prophet">https://github.com/facebook/prophet</a>, See on <a href="https://news.ycombinator.com/item?id=37663820">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-prophet-automatic-forecasting-procedure" dir="auto"><a href="#prophet-automatic-forecasting-procedure">Prophet: Automatic Forecasting Procedure</a></h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/facebook/prophet/workflows/Build/badge.svg"><img src="https://github.com/facebook/prophet/workflows/Build/badge.svg" alt="Build"></a></p>
<p dir="auto"><a href="https://pypi.python.org/pypi/prophet" rel="nofollow"><img src="https://camo.githubusercontent.com/ca04c4e8c3096c6322baf8345d9e49654b14eb653f95940ba94ff65ed179f0d6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f70726f706865742e737667" alt="PyPI Version" data-canonical-src="https://img.shields.io/pypi/v/prophet.svg"></a>
<a href="https://pepy.tech/project/prophet" rel="nofollow"><img src="https://camo.githubusercontent.com/06eeeb6f5fd9a30c5a11817aade564762f59b68a00aecd9a39fbc7dc5b229d4b/68747470733a2f2f706570792e746563682f62616467652f70726f706865742f6d6f6e7468" alt="PyPI Downloads Monthly" data-canonical-src="https://pepy.tech/badge/prophet/month"></a>
<a href="https://pepy.tech/project/prophet" rel="nofollow"><img src="https://camo.githubusercontent.com/0ed6f9de39980c2849f2a31ca727fdc2a9f8f8f142a742732c2dd35c9f9c7132/68747470733a2f2f706570792e746563682f62616467652f70726f70686574" alt="PyPI Downloads All" data-canonical-src="https://pepy.tech/badge/prophet"></a></p>
<p dir="auto"><a href="https://cran.r-project.org/package=prophet" rel="nofollow"><img src="https://camo.githubusercontent.com/2d7a3597b2a92021a4f3d29cf68f35888afa2930867842f20782b54899b58aea/68747470733a2f2f7777772e722d706b672e6f72672f6261646765732f76657273696f6e2f70726f70686574" alt="CRAN Version" data-canonical-src="https://www.r-pkg.org/badges/version/prophet"></a>
<a href="https://cran.r-project.org/package=prophet" rel="nofollow"><img src="https://camo.githubusercontent.com/ba49b200f4ee0e11105953d13633da8aa63412acc86560b95ae95422d8c1675f/68747470733a2f2f6372616e6c6f67732e722d706b672e6f72672f6261646765732f70726f706865743f636f6c6f723d627269676874677265656e" alt="CRAN Downloads Monthly" data-canonical-src="https://cranlogs.r-pkg.org/badges/prophet?color=brightgreen"></a>
<a href="https://cranlogs.r-pkg.org/badges/grand-total/prophet" rel="nofollow"><img src="https://camo.githubusercontent.com/af33df4c8be51a774861f6df5d737d8856e1fae0fe35d6799219077e336a59fa/68747470733a2f2f6372616e6c6f67732e722d706b672e6f72672f6261646765732f6772616e642d746f74616c2f70726f706865743f636f6c6f723d627269676874677265656e" alt="CRAN Downloads All" data-canonical-src="https://cranlogs.r-pkg.org/badges/grand-total/prophet?color=brightgreen"></a></p>
<p dir="auto"><a href="https://anaconda.org/conda-forge/prophet/" rel="nofollow"><img src="https://camo.githubusercontent.com/e71d3c39b99e7977f6a02856c23eefbcf3c71806da569fd9ba4450142eff07a1/68747470733a2f2f616e61636f6e64612e6f72672f636f6e64612d666f7267652f70726f706865742f6261646765732f76657273696f6e2e737667" alt="Conda_Version" data-canonical-src="https://anaconda.org/conda-forge/prophet/badges/version.svg"></a></p>
<hr>
<p dir="auto"><strong>2023 Update:</strong> We discuss our plans for the future of Prophet in this blog post: <a href="https://medium.com/@cuongduong_35162/facebook-prophet-in-2023-and-beyond-c5086151c138" rel="nofollow">facebook/prophet in 2023 and beyond</a></p>
<hr>
<p dir="auto">Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.</p>
<p dir="auto">Prophet is <a href="https://code.facebook.com/projects/" rel="nofollow">open source software</a> released by Facebook's <a href="https://research.fb.com/category/data-science/" rel="nofollow">Core Data Science team</a>. It is available for download on <a href="https://cran.r-project.org/package=prophet" rel="nofollow">CRAN</a> and <a href="https://pypi.python.org/pypi/prophet/" rel="nofollow">PyPI</a>.</p>
<h2 tabindex="-1" id="user-content-important-links" dir="auto"><a href="#important-links">Important links</a></h2>
<ul dir="auto">
<li>Homepage: <a href="https://facebook.github.io/prophet/" rel="nofollow">https://facebook.github.io/prophet/</a></li>
<li>HTML documentation: <a href="https://facebook.github.io/prophet/docs/quick_start.html" rel="nofollow">https://facebook.github.io/prophet/docs/quick_start.html</a></li>
<li>Issue tracker: <a href="https://github.com/facebook/prophet/issues">https://github.com/facebook/prophet/issues</a></li>
<li>Source code repository: <a href="https://github.com/facebook/prophet">https://github.com/facebook/prophet</a></li>
<li>Contributing: <a href="https://facebook.github.io/prophet/docs/contributing.html" rel="nofollow">https://facebook.github.io/prophet/docs/contributing.html</a></li>
<li>Prophet R package: <a href="https://cran.r-project.org/package=prophet" rel="nofollow">https://cran.r-project.org/package=prophet</a></li>
<li>Prophet Python package: <a href="https://pypi.python.org/pypi/prophet/" rel="nofollow">https://pypi.python.org/pypi/prophet/</a></li>
<li>Release blogpost: <a href="https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/" rel="nofollow">https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/</a></li>
<li>Prophet paper: Sean J. Taylor, Benjamin Letham (2018) Forecasting at scale. The American Statistician 72(1):37-45 (<a href="https://peerj.com/preprints/3190.pdf" rel="nofollow">https://peerj.com/preprints/3190.pdf</a>).</li>
</ul>
<h2 tabindex="-1" id="user-content-installation-in-r---cran" dir="auto"><a href="#installation-in-r---cran">Installation in R - CRAN</a></h2>
<p dir="auto"><g-emoji alias="warning">⚠️</g-emoji> <strong>The CRAN version of prophet is fairly outdated. To get the latest bug fixes and updated country holiday data, we suggest installing the <a href="#installation-in-r---latest-release">latest release</a>.</strong></p>
<p dir="auto">Prophet is a <a href="https://cran.r-project.org/package=prophet" rel="nofollow">CRAN package</a> so you can use <code>install.packages</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="install.packages('prophet')"><pre>install.packages(<span><span>'</span>prophet<span>'</span></span>)</pre></div>
<p dir="auto">After installation, you can <a href="https://facebook.github.io/prophet/docs/quick_start.html#r-api" rel="nofollow">get started!</a></p>
<h2 tabindex="-1" id="user-content-installation-in-r---latest-release" dir="auto"><a href="#installation-in-r---latest-release">Installation in R - Latest release</a></h2>
<div dir="auto" data-snippet-clipboard-copy-content="install.packages('remotes')
remotes::install_github('facebook/prophet@*release', subdir = 'R')"><pre>install.packages(<span><span>'</span>remotes<span>'</span></span>)
<span>remotes</span><span>::</span>install_github(<span><span>'</span>facebook/prophet@*release<span>'</span></span>, <span>subdir</span> <span>=</span> <span><span>'</span>R<span>'</span></span>)</pre></div>
<h4 tabindex="-1" id="user-content-experimental-backend---cmdstanr" dir="auto"><a href="#experimental-backend---cmdstanr">Experimental backend - cmdstanr</a></h4>
<p dir="auto">You can also choose an experimental alternative stan backend called <code>cmdstanr</code>. Once you've installed <code>prophet</code>,
follow these instructions to use <code>cmdstanr</code> instead of <code>rstan</code> as the backend:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# R
# We recommend running this in a fresh R session or restarting your current session
install.packages(c(&quot;cmdstanr&quot;, &quot;posterior&quot;), repos = c(&quot;https://mc-stan.org/r-packages/&quot;, getOption(&quot;repos&quot;)))

# If you haven't installed cmdstan before, run:
cmdstanr::install_cmdstan()
# Otherwise, you can point cmdstanr to your cmdstan path:
cmdstanr::set_cmdstan_path(path = <your existing cmdstan>)

# Set the R_STAN_BACKEND environment variable
Sys.setenv(R_STAN_BACKEND = &quot;CMDSTANR&quot;)"><pre><span><span>#</span> R</span>
<span><span>#</span> We recommend running this in a fresh R session or restarting your current session</span>
install.packages(c(<span><span>"</span>cmdstanr<span>"</span></span>, <span><span>"</span>posterior<span>"</span></span>), <span>repos</span> <span>=</span> c(<span><span>"</span>https://mc-stan.org/r-packages/<span>"</span></span>, getOption(<span><span>"</span>repos<span>"</span></span>)))

<span><span>#</span> If you haven't installed cmdstan before, run:</span>
<span>cmdstanr</span><span>::</span>install_cmdstan()
<span><span>#</span> Otherwise, you can point cmdstanr to your cmdstan path:</span>
<span>cmdstanr</span><span>::</span>set_cmdstan_path(<span>path</span> <span>=</span> <span>&lt;</span><span>your</span> <span>existing</span> <span>cmdstan</span><span>&gt;</span>)

<span><span>#</span> Set the R_STAN_BACKEND environment variable</span>
Sys.setenv(<span>R_STAN_BACKEND</span> <span>=</span> <span><span>"</span>CMDSTANR<span>"</span></span>)</pre></div>
<h3 tabindex="-1" id="user-content-windows" dir="auto"><a href="#windows">Windows</a></h3>
<p dir="auto">On Windows, R requires a compiler so you'll need to <a href="https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started">follow the instructions</a> provided by <code>rstan</code>. The key step is installing <a href="http://cran.r-project.org/bin/windows/Rtools/" rel="nofollow">Rtools</a> before attempting to install the package.</p>
<p dir="auto">If you have custom Stan compiler settings, install from source rather than the CRAN binary.</p>
<h2 tabindex="-1" id="user-content-installation-in-python---pypi-release" dir="auto"><a href="#installation-in-python---pypi-release">Installation in Python - PyPI release</a></h2>
<p dir="auto">Prophet is on PyPI, so you can use <code>pip</code> to install it.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m pip install prophet"><pre>python -m pip install prophet</pre></div>
<ul dir="auto">
<li>From v0.6 onwards, Python 2 is no longer supported.</li>
<li>As of v1.0, the package name on PyPI is "prophet"; prior to v1.0 it was "fbprophet".</li>
<li>As of v1.1, the minimum supported Python version is 3.7.</li>
</ul>
<p dir="auto">After installation, you can <a href="https://facebook.github.io/prophet/docs/quick_start.html#python-api" rel="nofollow">get started!</a></p>
<h3 tabindex="-1" id="user-content-anaconda" dir="auto"><a href="#anaconda">Anaconda</a></h3>
<p dir="auto">Prophet can also be installed through conda-forge.</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda install -c conda-forge prophet"><pre>conda install -c conda-forge prophet</pre></div>
<h2 tabindex="-1" id="user-content-installation-in-python---development-version" dir="auto"><a href="#installation-in-python---development-version">Installation in Python - Development version</a></h2>
<p dir="auto">To get the latest code changes as they are merged, you can clone this repo and build from source manually. This is <strong>not</strong> guaranteed to be stable.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/facebook/prophet.git
cd prophet/python
python -m pip install -e ."><pre>git clone https://github.com/facebook/prophet.git
<span>cd</span> prophet/python
python -m pip install -e <span>.</span></pre></div>
<p dir="auto">By default, Prophet will use a fixed version of <code>cmdstan</code> (downloading and installing it if necessary) to compile the model executables. If this is undesired and you would like to use your own existing <code>cmdstan</code> installation, you can set the environment variable <code>PROPHET_REPACKAGE_CMDSTAN</code> to <code>False</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export PROPHET_REPACKAGE_CMDSTAN=False; python -m pip install -e ."><pre><span>export</span> PROPHET_REPACKAGE_CMDSTAN=False<span>;</span> python -m pip install -e <span>.</span></pre></div>
<h3 tabindex="-1" id="user-content-linux" dir="auto"><a href="#linux">Linux</a></h3>
<p dir="auto">Make sure compilers (gcc, g++, build-essential) and Python development tools (python-dev, python3-dev) are installed. In Red Hat systems, install the packages gcc64 and gcc64-c++. If you are using a VM, be aware that you will need at least 4GB of memory to install prophet, and at least 2GB of memory to use prophet.</p>
<h3 tabindex="-1" id="user-content-windows-1" dir="auto"><a href="#windows-1">Windows</a></h3>
<p dir="auto">Using <code>cmdstanpy</code> with Windows requires a Unix-compatible C compiler such as mingw-gcc. If cmdstanpy is installed first, one can be installed via the <code>cmdstanpy.install_cxx_toolchain</code> command.</p>
<h2 tabindex="-1" id="user-content-changelog" dir="auto"><a href="#changelog">Changelog</a></h2>
<h3 tabindex="-1" id="user-content-version-114-20230530" dir="auto"><a href="#version-114-20230530">Version 1.1.4 (2023.05.30)</a></h3>
<h4 tabindex="-1" id="user-content-python" dir="auto"><a href="#python">Python</a></h4>
<ul dir="auto">
<li>We now rely solely on <code>holidays</code> package for country holidays.</li>
<li>Upgraded cmdstan version to 2.31.0, enabling Apple M1 support.</li>
<li>Fixed bug with Windows installation caused by long paths.</li>
</ul>
<h4 tabindex="-1" id="user-content-r" dir="auto"><a href="#r">R</a></h4>
<ul dir="auto">
<li>Updated <code>holidays</code> data based on holidays version 0.25.</li>
</ul>
<h3 tabindex="-1" id="user-content-version-112-20230120" dir="auto"><a href="#version-112-20230120">Version 1.1.2 (2023.01.20)</a></h3>
<h4 tabindex="-1" id="user-content-python-1" dir="auto"><a href="#python-1">Python</a></h4>
<ul dir="auto">
<li>Sped up <code>.predict()</code> by up to 10x by removing intermediate DataFrame creations.</li>
<li>Sped up fourier series generation, leading to at least 1.5x speed improvement for <code>train()</code> and <code>predict()</code> pipelines.</li>
<li>Fixed bug in how warm start values were being read.</li>
<li>Wheels are now version-agnostic.</li>
</ul>
<h4 tabindex="-1" id="user-content-r-1" dir="auto"><a href="#r-1">R</a></h4>
<ul dir="auto">
<li>Fixed a bug in <code>construct_holiday_dataframe()</code></li>
<li>Updated <code>holidays</code> data based on holidays version 0.18.</li>
</ul>
<h3 tabindex="-1" id="user-content-version-111-20220908" dir="auto"><a href="#version-111-20220908">Version 1.1.1 (2022.09.08)</a></h3>
<ul dir="auto">
<li>(Python) Improved runtime (3-7x) of uncertainty predictions via vectorization.</li>
<li>Bugfixes relating to Python package versions and R holiday objects.</li>
</ul>
<h3 tabindex="-1" id="user-content-version-11-20220625" dir="auto"><a href="#version-11-20220625">Version 1.1 (2022.06.25)</a></h3>
<ul dir="auto">
<li>Replaced <code>pystan2</code> dependency with <code>cmdstan</code> + <code>cmdstanpy</code>.</li>
<li>Pre-packaged model binaries for Python package, uploaded binary distributions to PyPI.</li>
<li>Improvements in the <code>stan</code> model code, cross-validation metric calculations, holidays.</li>
</ul>
<h3 tabindex="-1" id="user-content-version-10-20210328" dir="auto"><a href="#version-10-20210328">Version 1.0 (2021.03.28)</a></h3>
<ul dir="auto">
<li>Python package name changed from fbprophet to prophet</li>
<li>Fixed R Windows build issues to get latest version back on CRAN</li>
<li>Improvements in serialization, holidays, and R timezone handling</li>
<li>Plotting improvements</li>
</ul>
<h3 tabindex="-1" id="user-content-version-07-20200905" dir="auto"><a href="#version-07-20200905">Version 0.7 (2020.09.05)</a></h3>
<ul dir="auto">
<li>Built-in json serialization</li>
<li>Added "flat" growth option</li>
<li>Bugfixes related to <code>holidays</code> and <code>pandas</code></li>
<li>Plotting improvements</li>
<li>Improvements in cross validation, such as parallelization and directly specifying cutoffs</li>
</ul>
<h3 tabindex="-1" id="user-content-version-06-20200303" dir="auto"><a href="#version-06-20200303">Version 0.6 (2020.03.03)</a></h3>
<ul dir="auto">
<li>Fix bugs related to upstream changes in <code>holidays</code> and <code>pandas</code> packages.</li>
<li>Compile model during first use, not during install (to comply with CRAN policy)</li>
<li><code>cmdstanpy</code> backend now available in Python</li>
<li>Python 2 no longer supported</li>
</ul>
<h3 tabindex="-1" id="user-content-version-05-20190514" dir="auto"><a href="#version-05-20190514">Version 0.5 (2019.05.14)</a></h3>
<ul dir="auto">
<li>Conditional seasonalities</li>
<li>Improved cross validation estimates</li>
<li>Plotly plot in Python</li>
<li>Bugfixes</li>
</ul>
<h3 tabindex="-1" id="user-content-version-04-20181218" dir="auto"><a href="#version-04-20181218">Version 0.4 (2018.12.18)</a></h3>
<ul dir="auto">
<li>Added holidays functionality</li>
<li>Bugfixes</li>
</ul>
<h3 tabindex="-1" id="user-content-version-03-20180601" dir="auto"><a href="#version-03-20180601">Version 0.3 (2018.06.01)</a></h3>
<ul dir="auto">
<li>Multiplicative seasonality</li>
<li>Cross validation error metrics and visualizations</li>
<li>Parameter to set range of potential changepoints</li>
<li>Unified Stan model for both trend types</li>
<li>Improved future trend uncertainty for sub-daily data</li>
<li>Bugfixes</li>
</ul>
<h3 tabindex="-1" id="user-content-version-021-20171108" dir="auto"><a href="#version-021-20171108">Version 0.2.1 (2017.11.08)</a></h3>
<ul dir="auto">
<li>Bugfixes</li>
</ul>
<h3 tabindex="-1" id="user-content-version-02-20170902" dir="auto"><a href="#version-02-20170902">Version 0.2 (2017.09.02)</a></h3>
<ul dir="auto">
<li>Forecasting with sub-daily data</li>
<li>Daily seasonality, and custom seasonalities</li>
<li>Extra regressors</li>
<li>Access to posterior predictive samples</li>
<li>Cross-validation function</li>
<li>Saturating minimums</li>
<li>Bugfixes</li>
</ul>
<h3 tabindex="-1" id="user-content-version-011-20170417" dir="auto"><a href="#version-011-20170417">Version 0.1.1 (2017.04.17)</a></h3>
<ul dir="auto">
<li>Bugfixes</li>
<li>New options for detecting yearly and weekly seasonality (now the default)</li>
</ul>
<h3 tabindex="-1" id="user-content-version-01-20170223" dir="auto"><a href="#version-01-20170223">Version 0.1 (2017.02.23)</a></h3>
<ul dir="auto">
<li>Initial release</li>
</ul>
<h2 tabindex="-1" id="user-content-license" dir="auto"><a href="#license">License</a></h2>
<p dir="auto">Prophet is licensed under the <a href="https://github.com/facebook/prophet/blob/main/LICENSE">MIT license</a>.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU tells Apple to open everything up to its rivals (411 pts)]]></title>
            <link>https://appleinsider.com/articles/23/09/26/eu-tells-apple-to-open-everything-up-to-its-rivals</link>
            <guid>37663725</guid>
            <pubDate>Tue, 26 Sep 2023 18:28:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://appleinsider.com/articles/23/09/26/eu-tells-apple-to-open-everything-up-to-its-rivals">https://appleinsider.com/articles/23/09/26/eu-tells-apple-to-open-everything-up-to-its-rivals</a>, See on <a href="https://news.ycombinator.com/item?id=37663725">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>
                        <a href="https://photos5.appleinsider.com/gallery/51155-101060-european-union-flag-xl.jpg">
              <img src="https://photos5.appleinsider.com/gallery/51155-101060-european-union-flag-xl.jpg" alt="">
            </a>
          </p>

          
          
          
                    <p>European Commissioner Thierry Breton says the Digital Markets Act is just a beginning, and Apple must open up its whole ecosystem to competitors.
</p><p>The EU's Digital Markets Act (DMA) <a href="https://appleinsider.com/articles/22/07/19/european-council-approves-digital-markets-act-rules">was created</a> to target Big Tech firms like Apple, and make a fairer business environment for all comers. The DMA became law in the EU in <a href="https://appleinsider.com/articles/22/11/01/europe-confirms-its-digital-markets-act-will-go-after-apples-app-store">November 2022</a>, and became applicable from May 2023, though it is still in the process of <a href="https://appleinsider.com/articles/23/09/06/apples-imessage-gets-a-reprieve-from-eu-law">being implemented</a>.
</p><p>According to <em>Reuters</em>, Thierry Breton has now <a href="https://www.reuters.com/technology/eus-breton-tells-apple-ceo-open-its-ecosystem-rivals-2023-09-26/">called on Apple</a> to open up its hardware and software ecosystem.
</p><p>"The next job for Apple and other Big Tech, under the DMA is to open up its gates to competitors," he said. "Be it the electronic wallet, browsers or app stores, consumers using an Apple <a href="https://appleinsider.com/inside/iphone" title="iPhone" data-kpt="1">iPhone</a> should be able to benefit from competitive services by a range of providers."
</p><p>Breton said this after meeting with <a href="https://appleinsider.com/inside/tim-cook" title="Tim Cook" data-kpt="1">Tim Cook</a> in Brussels, where the Apple CEO is certain to have reiterated the company's arguments about security and privacy.
</p><p>"EU regulation fosters innovation, without compromising on security and privacy," Breton told <em>Reuters</em>.
</p><p>Apple has not commented. Cook's trip to Brussels was not announced, and he hasn't referred to his meeting with the EU Commissioner. However, he <a href="https://twitter.com/tim_cook/status/1706609678522658865/photo/1">has tweeted</a> about meeting with Apple Store staff in the city.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPU.zip: side channel attack that exposes visual data processed on the GPU (175 pts)]]></title>
            <link>https://www.hertzbleed.com/gpu.zip/</link>
            <guid>37663601</guid>
            <pubDate>Tue, 26 Sep 2023 18:19:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hertzbleed.com/gpu.zip/">https://www.hertzbleed.com/gpu.zip/</a>, See on <a href="https://news.ycombinator.com/item?id=37663601">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>

        <p>GPU.zip is a new type of side channel that exposes visual data processed on the graphics processing unit (GPU).
This channel exploits an optimization that is data dependent, software transparent, and present in nearly all modern GPUs: graphical data compression.
We present the first security-centric analysis of this optimization and demonstrate that it can be abused to leak visual data.
For example, using GPU.zip, a malicious webpage can leak pixels from another webpage in the latest version of Google Chrome, violating the browser security model.</p>
<h2 id="research-paper">Research Paper</h2>
<p>The GPU.zip paper will appear in the 45th IEEE Symposium on Security and Privacy (San Francisco, 20-23 May 2024) with the following title:</p>
<ul>
<li>GPU.zip: On the Side-Channel Implications of Hardware-Based Graphical Data Compression</li>
</ul>
<p>You can download a preprint from <a href="https://www.hertzbleed.com/gpu.zip/GPU-zip.pdf">here</a> and the BibTeX citation from <a href="https://www.hertzbleed.com/gpu.zip/GPU-zip.bib">here</a>.</p>
<p>The paper is the result of a collaboration between the following researchers:</p>
<ul>
<li><a href="https://www.cs.utexas.edu/~yingchen/">Yingchen Wang</a> (University of Texas at Austin)</li>
<li><a href="https://www.cs.cmu.edu/~rpaccagn/">Riccardo Paccagnella</a> (Carnegie Mellon University)</li>
<li><a href="https://www.linkedin.com/in/zhaogangse/en">Zhao Gang</a> (University of Texas at Austin)</li>
<li><a href="https://wrv.github.io/">Willy R. Vasquez</a> (University of Texas at Austin)</li>
<li><a href="https://homes.cs.washington.edu/~dkohlbre/">David Kohlbrenner</a> (University of Washington)</li>
<li><a href="https://www.cs.utexas.edu/~hovav/">Hovav Shacham</a> (University of Texas at Austin)</li>
<li><a href="https://cwfletcher.github.io/">Christopher Fletcher</a> (University of Illinois Urbana-Champaign)</li>
</ul>
<h2 id="questions-and-answers">Questions and Answers</h2>
<h3 id="am-i-affected-by-gpuzip">Am I affected by GPU.zip?</h3>
<p>Likely, yes.
We tested integrated GPUs from AMD, Apple, Arm, Intel, and Qualcomm and one discrete GPU from Nvidia.
We have at least preliminary results to show that all tested GPUs are affected.</p>
<h3 id="i-am-a-website-developer-how-do-i-protect-my-users">I am a website developer. How do I protect my users?</h3>
<p>If your website displays sensitive information about users, you should configure your website to deny being embedded by cross-origin websites.
For more information on how to do this, we refer to <a href="https://web.dev/security-headers/">this web.dev article</a>.</p>
<h3 id="i-am-a-user-should-i-be-worried">I am a user. Should I be worried?</h3>
<p>Under most circumstances, probably not.
Most sensitive websites already deny being embedded by cross-origin websites.
As a result, they are not vulnerable to the pixel stealing attack we mounted using GPU.zip.
However, some websites remain vulnerable.
For example, if a user who is logged into Wikipedia visits a malicious webpage, that webpage can exploit GPU.zip to learn the user’s Wikipedia username (as we demonstrate in Section 5.4 of the <a href="https://www.hertzbleed.com/gpu.zip/GPU-zip.pdf">paper</a>).</p>
<h3 id="what-makes-gpuzip-different-from-prior-compression-side-channels">What makes GPU.zip different from prior compression side channels?</h3>
<p>GPU.zip exploits <em>software-transparent</em> uses of compression.
This is in contrast to prior compression side channels, which leak because of software-visible uses of compression and can be mitigated by disabling compression in software.
For a more detailed explanation, we refer to the <a href="https://www.hertzbleed.com/gpu.zip/GPU-zip.pdf">paper</a>.</p>
<h3 id="what-exactly-is-gpu-graphical-data-compression">What exactly is GPU graphical data compression?</h3>
<p>GPU graphical data compression is a feature of modern GPUs used to save memory bandwidth and improve performance without any software involvement.
Specifically, modern GPUs compress graphical data losslessly even when software does not request any compression.</p>
<p>Interestingly, the algorithms used by GPUs for graphical data compression vary across vendors and microarchitectures.
Check out the <a href="https://www.hertzbleed.com/gpu.zip/GPU-zip.pdf">paper</a> for a reverse engineering of several proprietary compression algorithms used by Intel and AMD.</p>
<h3 id="when-did-you-disclose-gpuzip">When did you disclose GPU.zip?</h3>
<p>We disclosed our findings and proof-of-concept code to GPU vendors (AMD, Apple, Arm, Intel, Nvidia, and Qualcomm) and to Google in March 2023.</p>
<h3 id="do-gpu-vendors-plan-to-patch">Do GPU vendors plan to patch?</h3>
<p>As of September 2023, no GPU vendor has committed to patching.</p>
<h3 id="does-chrome-plan-to-patch">Does Chrome plan to patch?</h3>
<p>As of September 2023, Google is still deciding whether and how to patch.</p>
<h3 id="what-about-other-browsers">What about other browsers?</h3>
<p>Chrome is vulnerable to the pixel stealing attack demonstrated in the paper because it satisfies the following three criteria:</p>
<ol>
<li>It allows cross-origin iframes to be loaded with cookies.</li>
<li>It allows rendering SVG filters on iframes.</li>
<li>It delegates rendering tasks to the GPU.</li>
</ol>
<p>Other browsers, like Firefox and Safari, do not meet all these criteria and are therefore not vulnerable.</p>
<h3 id="can-i-use-the-logo">Can I use the logo?</h3>
<p>Yes. The GPU.zip logo is free to use under a <a href="https://creativecommons.org/publicdomain/zero/1.0/">CC0</a> license.</p>
<ul>
<li>Download logo: <a href="https://www.hertzbleed.com/gpu.zip/images/GPU-zip-logo.svg">SVG</a>, <a href="https://www.hertzbleed.com/gpu.zip/images/GPU-zip-logo.png">PNG</a></li>
<li>Download logo with text: <a href="https://www.hertzbleed.com/gpu.zip/images/GPU-zip-logo-with-text.svg">SVG</a>, <a href="https://www.hertzbleed.com/gpu.zip/images/GPU-zip-logo-with-text.png">PNG</a></li>
</ul>
<h3 id="did-you-release-the-source-code-of-gpuzip">Did you release the source code of GPU.zip?</h3>
<p>Yes, you can find the source code at the link:
<a href="https://github.com/UT-Security/gpu-zip">https://github.com/UT-Security/gpu-zip</a></p>


      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Causality for Machine Learning (2020) (113 pts)]]></title>
            <link>https://ff13.fastforwardlabs.com/</link>
            <guid>37663523</guid>
            <pubDate>Tue, 26 Sep 2023 18:14:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ff13.fastforwardlabs.com/">https://ff13.fastforwardlabs.com/</a>, See on <a href="https://news.ycombinator.com/item?id=37663523">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p><a href="https://www.cloudera.com/products/fast-forward-labs-research.html"><img alt="Cloudera Fast Forward" src="https://ff13.fastforwardlabs.com/figures/cloudera-fast-forward-logo.png"></a>
          </p>
          
          
<p>FF13 · ©2020 Cloudera, Inc. All rights reserved</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-cover-splash.png" alt="Causality for Machine Learning report cover"><figcaption>Causality for Machine Learning report cover</figcaption></figure>
<p><em>This is an applied research report by <a href="https://www.cloudera.com/products/fast-forward-labs-research.html">Cloudera Fast Forward Labs</a>. We write reports about emerging technologies. Accompanying each report are working prototypes that exhibit the capabilities of the algorithm and offer detailed technical advice on its practical application. Read our full report on causality for machine learning below or <a href="https://ff13.fastforwardlabs.com/FF13-Causality_for_Machine_Learning-Cloudera_Fast_Forward.pdf" target="_blank" id="report-pdf-download">download the PDF</a>. Also be sure to check out the complementary prototype, <a href="https://scene.fastforwardlabs.com/">Scene</a>.</em></p>
<div><ul><li><a href="#introduction">Introduction</a></li><li><a href="#background%3A-causal-inference">Background: Causal Inference</a><ul><li><a href="#why-are-we-interested-in-causal-inference%3F">Why are we interested in causal inference?</a></li><li><a href="#the-ladder-of-causation">The ladder of causation</a></li><li><a href="#from-correlation-to-causation">From correlation to causation</a></li><li><a href="#from-prediction-to-intervention">From prediction to intervention</a></li><li><a href="#how-do-we-know-which-graph-to-use%3F">How do we know which graph to use?</a></li><li><a href="#tl%3Bdr">TL;DR</a></li></ul></li><li><a href="#causality-and-invariance">Causality and Invariance</a><ul><li><a href="#the-great-lie-of-machine-learning">The great lie of machine learning</a></li><li><a href="#dangers-of-spurious-correlations">Dangers of spurious correlations</a></li><li><a href="#invariance">Invariance</a></li><li><a href="#invariant-causal-prediction">Invariant Causal Prediction</a></li><li><a href="#invariant-risk-minimization">Invariant Risk Minimization</a></li><li><a href="#how-irm-works">How IRM works</a></li></ul></li><li><a href="#prototype">Prototype</a><ul><li><a href="#the-wildcam-dataset">The Wildcam dataset</a></li><li><a href="#experimental-setup">Experimental setup</a></li><li><a href="#results">Results</a></li><li><a href="#product%3A-scene">Product: Scene</a></li></ul></li><li><a href="#landscape">Landscape</a><ul><li><a href="#use-cases">Use Cases</a></li><li><a href="#tools">Tools</a></li></ul></li><li><a href="#ethics">Ethics</a><ul><li><a href="#causal-graphs-make-assumptions-explicit">Causal graphs make assumptions explicit</a></li><li><a href="#omitting-protected-attributes-is-not-enough">Omitting protected attributes is not enough</a></li><li><a href="#invariance-as-a-route-to-fairness">Invariance as a route to fairness</a></li></ul></li><li><a href="#future">Future</a><ul><li><a href="#comparable-approaches">Comparable approaches</a></li><li><a href="#looking-ahead">Looking ahead</a></li></ul></li><li><a href="#conclusion">Conclusion</a></li></ul></div>
<h2 id="introduction">Introduction</h2>
<p>In recent years, machine learning has made remarkable progress, providing novel capabilities like the creation of sophisticated, computable representations of text and images. These capabilities have enabled new products, such as image searches based on image content, automatic translation between many languages, and even the synthesis of realistic images and voice. Simultaneously, machine learning has seen widespread adoption in the enterprise for classic use cases (for instance, predicting customer churn, loan defaulting, and manufacturing equipment failure).</p>
<p>Where machine learning has been successful, it has been extraordinarily so.</p>
<p>In many cases, that success can be attributed to supervised learning on large volumes of training data (combined with extensive computation). Broadly, supervised learning systems excel at one task: <em>prediction</em>. When the goal is to predict an outcome, and when we have many examples of that outcome arising, as well as the features associated with it, we may turn to supervised learning.</p>
<p>As machine learning has gained popularity, its sphere of influence in business processes has expanded beyond narrow prediction and into decision making. The results of machine learning systems are routinely used to set credit limits, anticipate manufacturing equipment failures, and curate our various news feeds. As individuals and businesses seek to learn from the information provided by such complex and nonlinear systems, more (and better) methods for interpretability have been developed, and this is both healthy and important.</p>
<p>However, there are fundamental limits to reasoning based on prediction alone. For instance, what will happen if a bank increases a customer’s credit limit? Such questions cannot be answered by a correlative model built on previously observed data, because they involve a possible change in the customer’s choices as a reaction to the change in credit limit. In many cases, the outcome of our decision process is an <em>intervention</em> - an action that changes something in the world. As we’ll demonstrate in this report, purely correlative predictive systems are not equipped for reasoning under such interventions, and hence are prone to biases. For data-informed decision making under intervention, we need causality.</p>
<p>Even for purely predictive systems, which is very much the forte of supervised learning, applying some causal thinking brings benefits. Causal relationships are by their definition <em>invariant</em>, meaning they hold true across different circumstances and environments. This is a very desirable property for machine learning systems, where we often predict on data that we have not seen in training; we need these systems to be adaptable and robust.</p>
<p>The intersection of causal inference and machine learning is a rapidly expanding area of research. It is already yielding capabilities that are ready for mainstream adoption - capabilities which can help us build more robust, reliable, and fair machine learning systems.</p>
<p>This report is an introduction to causal reasoning as it pertains to much data science and machine learning work. We introduce causal graphs, with a focus on removing the <em>conceptual</em> barriers to understanding. We then use this understanding to explore recent ideas around <em>invariant prediction</em>, which brings some of the benefits of causal graphs to high dimensional problems. Along with the accompanying prototype, we show how even classic machine learning problems, like image classification, can benefit from the tools of causal inference.</p>
<h2 id="background%3A-causal-inference">Background: Causal Inference</h2>
<p>In this chapter, we discuss the essentials of causal reasoning (particularly in how it differs from supervised learning) and give an informal introduction to structural causal models. Grasping the basic notions of causal modeling allows for a much richer understanding of invariance and generalization, which we discuss in the next chapter, <a href="#causality-and-invariance">Causality and Invariance</a>.</p>
<h3 id="why-are-we-interested-in-causal-inference%3F">Why are we interested in causal inference?</h3>
<p>Imagine a bank that would like to reduce the number of business loans which default. Historical data and sophisticated supervised learning techniques may be able to accurately identify which loans are likely to default, and interpretability techniques may tell us some features that are correlated with (or predictive of) defaulting. However, to reduce the default rate, we must understand what changes to make, which requires understanding not only <em>which</em> loans default, but <em>why</em> the loans default.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-01.png" alt="A bank would like to decide which business loans to grant based on true, causal relationships."><figcaption>A bank would like to decide which business loans to grant based on true, causal relationships.</figcaption></figure>
<p>It may be that we find small loans are more likely to default than larger loans. One might naively assume that the bank ought to stop making small loans. However, perhaps it is really the case that smaller businesses are more likely to fail than large businesses, and <em>also</em> more likely to apply for small loans. In this case, the true causal relationship is between the size of the <em>business</em> and defaulting, and not between the size of the <em>loan</em> and defaulting. If this is so, our policy decisions should be influenced by business size, rather than loan size.</p>
<p>Unfortunately, supervised learning alone cannot tell us which is true. If we include both loan size and business size as features in our model, we will simply find that they are both related to loan defaulting, to some extent. While that insight is true - as they are both statistically related to defaulting - which <em>causes</em> defaulting is a separate question, and the one to which we want the answer.</p>
<p>Causality gives us a framework to reason about such questions, and recent developments at the intersection of causality and machine learning are making the discovery of such causal relationships easier.</p>
<h4 id="the-shortcomings-of-supervised-learning">The shortcomings of supervised learning</h4>
<p>Supervised machine learning has proved enormously successful at some tasks. This is particularyly true in dealing with tasks that require high-dimensional inputs, such as computer vision and natural language processing. There has been truly remarkable progress over the past two decades, and it should be noted that an acknowledgment of supervised learning’s shortcomings does not in any way diminish that progress.</p>
<p>With success have come inflated expectations that autonomous systems be capable of independent decision-making, and even human-like intelligence. Current machine learning approaches are unable to meet those expectations, owing to fundamental limitations of pattern recognition.</p>
<p>One such limitation is <strong>generalizability</strong> (also called <em>robustness</em> or <em>adaptability</em>), that is, the ability to apply a model learned in one context in a new environment. Many current state-of-the-art machine learning approaches assume that the trained model will be applied to data that looks the same as the training data. These models are trained on highly specific tasks, like recognizing dogs in images or identifying fraud in banking transactions. In real life, though, the data on which we predict is often different from the data on which we train, even when the task is the same. For example, training data is often subject to some form of selection bias, and simply collecting more of it does not mitigate that.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-02.png" alt="The real world is often distributed differently than our training data."><figcaption>The real world is often distributed differently than our training data.</figcaption></figure>
<p>Another limitation is <strong>explainability</strong>, that is, machine learning models remain mostly “black boxes” that are unable to explain the reasons behind their predictions or recommendations, thus eroding users’ trust and impeding diagnosis and repair. For example, a deep learning system can be trained to recognize cancer in medical images with high accuracy, provided it is given plenty of images and compute power, but - unlike a real doctor - it cannot explain why or how a particular image suggests disease. Several methods for understanding model predictions have been developed, and while these are necessary and welcome, understanding the interpretation and limitations of their outputs is a science in itself. While model interpretation methods like <a href="https://arxiv.org/abs/1602.04938">LIME</a> and <a href="https://arxiv.org/abs/1705.07874">SHAP</a> are useful, they provide insight only into how the model works, and not into how the world works.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-03.png" alt="Predictions alone are often not useful unless accompanied by an explanation."><figcaption>Predictions alone are often not useful unless accompanied by an explanation.</figcaption></figure>
<p>And finally, the understanding of <strong>cause-and-effect</strong> connections - a key element of human intelligence - is absent from pattern recognition systems. Humans have the ability to answer “what if” kinds of questions. <em>What if I change something? What if I had acted differently?</em> Such interventional, counterfactual, or retrospective questions are the forte of human intelligence. While imbuing machines with this kind of intelligence is still far-fetched, researchers in deep learning are increasingly recognizing the importance of these questions, and using them to inform their research.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-04.png" alt="Humans use counterfactual reasoning all the time. This is enabled by our unconcious understanding of cause and effect."><figcaption>Humans use counterfactual reasoning all the time. This is enabled by our unconcious understanding of cause and effect.</figcaption></figure>
<p>All of this means that supervised machine learning systems must be used cautiously in certain situations - and if we want to mitigate these restrictions effectively, causation is key.</p>
<h4 id="what-does-causality-bring-to-the-table%3F">What does causality bring to the table?</h4>
<p>Causal inference provides us with tools that allow us to answer the question of <em>why</em> something happens. This takes us a step further than traditional statistical or machine learning approaches that are focused on predicting outcomes and concerned with identifying associations.</p>
<p>Causality has long been of interest to humanity on a philosophical level, but it has only been in the latter half of the 20th century (thanks to the work of pioneering methodologists such as Donald Rubin and Judea Pearl), that a mathematical framework for causality has been introduced. In recent years, the boom of machine learning has enhanced the development of causal inference and attracted new researchers to the area.</p>
<p>Identifying causal effects helps us understand a variety of things: for example, user behavior in online systems,<sup><a href="#fn2" id="fnref2">[2]</a></sup> effect of social policies, risk factors of diseases. Questions of cause-and-effect are also critical for the design of data-driven applications. For instance, how do algorithmic recommendations affect our purchasing decisions? How do they affect a student’s learning outcome or a doctor’s efficacy? All of these are hard questions and require thinking about the counterfactual: what would have happened in a world with a different system, policy, or intervention?  Without causal reasoning, correlation-based methods can lead us astray.</p>
<p>That said, learning causality is a challenging problem. There are broadly two situations in which we could find ourselves: in one case, we are able to actively intervene in the system we are modeling and get experimental data; in the other, we have only observational data.</p>
<p>The gold standard in establishing causal effects is a Randomised Controlled Trial (RCT) and this falls under the experimental data category. In an RCT, we try to engineer similar populations using random assignment (as choosing the populations manually could introduce selection effects that destroy our ability to learn causal relations) and apply an intervention to one population and not the other. From this, we measure the causal effect of changing one variable as a simple difference in the quantity of interest between the two populations.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-05.png" alt="Randomised controlled trials are the gold standard in establishing causal effects."><figcaption>Randomised controlled trials are the gold standard in establishing causal effects.</figcaption></figure>
<p>We can use RCTs to establish whether a particular causal relation holds. However, trials are not always physically possible, and even when they are, they are not always ethical (for instance, it would not be ethical to deny a patient a treatment that is reasonably believed to work, or trial a news aggregation algorithm designed to influence a person’s mood without informed consent).<sup><a href="#fn3" id="fnref3">[3]</a></sup> In some cases, we can find naturally occurring experiments. In the worst cases, we’re left trying to infer causality from observational data alone.</p>
<p>In general, this is not possible, and we must at least impose some modeling assumptions. There are several formal frameworks for doing so. For our purpose of building intuition, we’ll introduce Judea Pearl’s <a href="http://bayes.cs.ucla.edu/BOOK-2K/">Structural Causal Model</a> (SCM) framework in this chapter.<sup><a href="#fn4" id="fnref4">[4]</a></sup></p>
<h3 id="the-ladder-of-causation">The ladder of causation</h3>
<p>In <a href="http://bayes.cs.ucla.edu/WHY/">The Book of Why</a>, Judea Pearl, an author of much foundational work in causality, describes three kinds of reasoning we can perform as rungs on a ladder. These rungs describe when we need causality, and what it buys us.<sup><a href="#fn5" id="fnref5">[5]</a></sup></p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-06.png" alt="The ladder of causation, as described in The Book of Why."><figcaption>The ladder of causation, as described in <a href="http://bayes.cs.ucla.edu/WHY/">The Book of Why</a>.</figcaption></figure>
<p>On the <strong>first rung</strong>, we can do <strong>statistical and predictive reasoning</strong>. This covers most (but not all) of what we do in machine learning. We may make very sophisticated forecasts, infer latent variables in complex deep generative models, or cluster data according to subtle relations. All of these things sit on rung one.</p>
<p><em>Example: a bank wishes to predict which of its current business loans are likely to default, so it can make financial forecasts that account for likely losses.</em></p>
<p>The <strong>second rung</strong> is <strong>interventional reasoning</strong>. Interventional reasoning allows us to predict what will happen when a system is changed. This enables us to describe what characteristics are particular to the exact observations we’ve made, and what should be invariant across new circumstances. This kind of reasoning requires a <em>causal</em> model. Intervening is a fundamental operation in causality, and we’ll discuss both interventions and causal models in this chapter.</p>
<p><em>Example: a bank would like to reduce the number of loans which default, and considers changing its policies. Predicting what will happen as a result of this intervention requires that the bank understand the causal relations which affect loan defaulting.</em></p>
<p>The <strong>third rung</strong> is <strong>counterfactual reasoning</strong>. On this rung, we can talk not only about what has happened, but also what would have happened if circumstances were different. Counterfactual reasoning requires a more precisely specified causal model than intervention. This form of reasoning is very powerful, providing a mathematical formulation of computing in alternate worlds where events were different.</p>
<p><em>Example: a bank would like to know what the likely return on a loan would have been, had they offered different terms than they did.</em></p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-07.png" alt="The ladder of causation describes the kind of question we can answer depending on the sophistication of our causal model."><figcaption>The ladder of causation describes the kind of question we can answer depending on the sophistication of our causal model.</figcaption></figure>
<p>By now, we hopefully agree that there is something to causality, and it has much to offer. However, we have yet to really <em>define</em> causality. We must begin with a familiar refrain: correlation is not causation.</p>
<h3 id="from-correlation-to-causation">From correlation to causation</h3>
<h4 id="spurious-correlations">Spurious correlations</h4>
<p>Very many things display correlation. The rooster crows when the sun rises.<sup><a href="#fn6" id="fnref6">[6]</a></sup> The lights turn off when you flick a switch. Global temperatures have risen alarmingly since the 1800s, and meanwhile pirate numbers have dwindled to almost nothing.<sup><a href="#fn7" id="fnref7">[7]</a></sup></p>
<p>These examples show us that while correlation can <em>appear</em> as a result of causation, as in the case of the light switch, correlation certainly does not always <em>imply</em> causation, as in the case of the pirates.</p>
<p>Correlated things are not always related.<sup><a href="#fn8" id="fnref8">[8]</a></sup> It’s possible to find many correlations with no readily imaginable causal interaction. The internet treasure <a href="https://www.tylervigen.com/spurious-correlations">Spurious Correlations</a> collects many amusing examples of this. These spurious correlations most likely arise as a result of small sample size and coincidences that are bound to happen when making many comparisons. We should not be surprised if we find something that has low probability if we try many combinations.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/spurious-correlation.png" alt="Figure source: Spurious Correlations."><figcaption>Figure source: <a href="https://www.tylervigen.com/spurious-correlations">Spurious Correlations</a>.</figcaption></figure>
<p>In real world systems, spurious correlations can be cause for serious ethical concerns. For instance, certain characteristics may be spuriously associated with individuals or minority groups, and these characteristics may be highly predictive. As such, the model weights them as important during a learning task. This can easily embed bias and unfairness into an algorithm based on the spurious correlations in a given dataset.</p>
<h4 id="the-principle-of-common-cause">The Principle of Common Cause</h4>
<p>In a posthumous 1956 book, <a href="https://www.goodreads.com/book/show/848892.The_Direction_of_Time">The Direction of Time</a>, Hans Reichenbach outlined the principle of common cause. He states the principle this way:</p>
<blockquote>
<p>“If an improbable coincidence has occurred, there must exist a common cause.”</p>
</blockquote>
<p>Our understanding of causality has evolved, but this language is remarkably similar to what we use now. Let’s discuss how correlation may arise from causation.</p>
<p>We will do this in the framework of Structural Causal Models (SCMs). An SCM is a directed acyclic graph of relationships between variables. The nodes represent variables, and the edges between them point from cause to effect. The value of each variable depends only on its direct parents in the graph (the other variables which point directly into it) and a noise variable that encapsulates any environmental interactions we are not modeling. We will examine three fundamental causal structures.</p>
<div>
<h5 id="causal-terminology">Causal Terminology</h5>
<p>A <strong>causal graph</strong> is a directed acyclic graph denoting the dependency between variables.</p>
<p>A <strong>structural causal model</strong> carries more information than a causal graph alone. It also specifies the functional form of dependencies between variables.</p>
<p>Remarkably, it’s possible to do much causal reasoning - including a calculation of the size of causal effects - via the graph alone, without specifying a parametric form for the relationships between causes and effects.</p>
</div>
<h5 id="1.-direct-causation">1. Direct causation</h5>
<p>The simplest way in which correlation between two variables arises is when one variable is a direct cause of the other. We say that one thing causes another when a change in the first thing, while holding everything else constant, results in a change in the second. In the business loan defaulting example discussed earlier, we could create a two node graph with one of the nodes being whether or not a business is small (say “small business” with values 0 or 1) and the other node being “default” indicating whether or not the business defaulted on the loan. In this case, we would expect that a small business increases the chances of it defaulting.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-08.png" alt="Direct causation gives rise to statistical dependence between two variables. In this fictional example, the indicator variable for Small businesses has a direct causal effect on the Loan Defalt indicator variable."><figcaption>Direct causation gives rise to statistical dependence between two variables. In this fictional example, the indicator variable for Small businesses has a direct causal effect on the Loan Defalt indicator variable.</figcaption></figure>
<p>This setup is immediately reminiscent of supervised learning, where we have a dataset of features, X, and targets, Y, and want to learn a mapping between them. However, in machine learning, we typically start with all available features and select those that are most informative about the target. When drawing a causal relationship, only those features we believe have an actual causal effect on the target should be included as direct causes. As we will see below, there are other diagrams that can lead to a predictive statistical relationship between X and Y in which neither directly causes the other.</p>
<h5 id="2.-common-cause">2. Common cause</h5>
<p>A common pattern is for a single variable to be the cause of multiple other variables. If a variable, Z, is a direct cause of both X and Y, we say that Z is a common cause and call the structure a “fork.” For example, unemployment could potentially cause both loan default and reduced consumer spend.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-09.png" alt="Two effects appear statistically dependent, but only because of a common cause. If the common cause, Unemployment, is fixed, then Consumer Spend and Loan Default become statistically independent."><figcaption>Two effects appear statistically dependent, but only because of a common cause. If the common cause, Unemployment, is fixed, then Consumer Spend and Loan Default become statistically independent.</figcaption></figure>
<p>Because both consumer spend and loan default depend on unemployment, they will appear correlated. A given value of unemployment will generate some values of consumer spend and loan default, and when unemployment changes, both consumer spend and loan default will change. As such, in the joint distribution of the SCM, the two dependent variables (consumer spend and loan default) will appear statistically related to one another.</p>
<p>However, if we were to <em>condition</em> on unemployment (for instance, by selecting data corresponding to a fixed unemployment rate), we would see that consumer spend and loan default are independent from one another.</p>
<p>The common cause unemployment <em>confounds</em> the relationship between consumer spend and loan default. We are unable to correctly calculate the relationship between consumer spend and loan default without accounting for unemployment (by conditioning). This is especially dangerous if unnoticed.</p>
<p>Unfortunately, confounders can be tricky or impossible to detect from observational data alone. In fact, if we look only at consumer spend and loan default, we could see the same joint distribution as in the case where consumer spend and loan default are directly causally related. As such, we should think of causal graphs as encoding our <em>assumptions</em> about the system we are studying. We return to this point in <a href="#how-do-we-know-which-graph-to-use%3F">How do we know which graph to use?</a></p>
<h5 id="3.-common-effect">3. Common effect</h5>
<p>The opposite common pattern is for one effect to have multiple direct causes. A node that has multiple causal parents is called a “collider” with respect to those nodes.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-10.png" alt="Variables that share a common effect are independent, until we fix the effect. For a given value of Loan Default, there is an induced dependency between the Number of Liens and Credit Score."><figcaption>Variables that share a common effect are independent, until we fix the effect. For a given value of Loan Default, there is an induced dependency between the Number of Liens and Credit Score.</figcaption></figure>
<p>A collider is a node that depends on more than one cause. In this example, loan defaulting depends on both commercial credit score and number of liens (a “lien” refers to the right to keep possession of property belonging to another entity until a debt owed by that entity is discharged), so we call loan default a <em>collider</em>.</p>
<p>Colliders are different to chains of direct causation and forks because the conditioning behaviour works oppositely. Before any conditioning, commercial credit score and number of liens are unconditionally independent. There is no variable with causal arrows going into both commercial credit score and number of liens, and no arrow linking them directly, so we should not expect a statistical dependency. However, if we condition on the collider, we will induce a conditional dependence between commercial credit score and number of liens.</p>
<p>This may seem a bit unintuitive, but we can make sense of it with a little thought experiment. Loan default depends on both commercial credit score and number of liens, so if either of those changes value, the chance of loan default changes. We fix the value of loan default (say, we look only at those loans that did default). Now, if we were to learn anything about the value of commercial credit score, we would know something about the number of liens too; only certain values of number of liens are compatible with the conditioned value of loan defaulting and observed value of commercial credit score. As such, conditioning on a collider induces a spurious correlation between the parent nodes. Conditioning on a collider is exactly selection bias!</p>
<h4 id="structural-causal-models%2C-in-code">Structural Causal Models, in code</h4>
<div>
<p>The small causal graphs shown above are an intuitive way to reason about causality. Remarkably, we can do much causal reasoning (and calculate causal effects) with these graphs, simply by specifying qualitatively which variables causally influence others. In the real world, causal graphs can be large and complex.</p>
<p>Of course, there are other ways to encode the information. Given the graph, we can easily write down an expression for the joint distribution: it’s the product of probability distributions for each node conditioned on its direct causal parents. In the case of a collider structure, <code>x</code> → <code>z</code> ← <code>y</code>, the joint distribution is simply <code>p(x,y,z) = p(x) p(y) p(z|x,y)</code>. The conditional probability <code>p(z|x,y)</code> is exactly what we’re used to estimating in supervised learning!</p>
<p>If we know more about the system, we can move from this causal graph to a full structural causal model. An example SCM compatible with this graph would be:</p>
<pre><code>from numpy.random import randn

def x():
  return -5 + randn()

def y():
  return 5 + randn()

def z(x, y):
  return x + y + randn()

def sample():
  x_ = x()
  y_ = y()
  z_ = z(x_, y_)
  return x_, y_, z_
</code></pre>
<p>Each of the variables has an independent random noise associated with it, arising from factors not modeled by the graph. These distributions need not be identical, but must be independent. Notice that the structure of the graph encodes the dependencies between variables, which we see as the function signatures. The values of <code>x</code> and <code>y</code> are independent, but <code>z</code> depends on both. We can also see clearly that the model defines a generative process for the data, since we can easily sample from the joint distribution by calling the <code>sample</code> function. Doing so repeatedly allows us to chart the joint distribution, and see that <code>x</code> and <code>y</code> are indeed independent; there’s no apparent correlation in the scatter chart.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/scm-observed.png" alt="Left: Histograms of the observational distributions of x, y and z. Right: Scatter plot of the observational joint distribution of x and y. Since x and y are not causally connected except through the collider z, they are completely uncorrelated."><figcaption>Left: Histograms of the observational distributions of x, y and z. Right: Scatter plot of the observational joint distribution of x and y. Since x and y are not causally connected except through the collider z, they are completely uncorrelated.</figcaption></figure>
<p>Now that we have a model in code, we can see a selection bias effect. If we condition the data to only values of <code>z</code> (the collider node) greater than a cutoff (which we can do easily, if inefficiently, by filtering the samples to those where <code>z &gt; 2.5</code>), the previously independent <code>x</code> and <code>y</code> become negatively correlated.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/scm-conditioned.png" alt="Left: We have conditioned on z > 2.5 by filtering the samples (note the change of scale), which changes the x and y distributions; they’re both shifted right. Right: The conditional joint distribution of x and y, with a line showing a linear fit, which illustrates the induced negative correlation."><figcaption>Left: We have conditioned on z &gt; 2.5 by filtering the samples (note the change of scale), which changes the x and y distributions; they’re both shifted right. Right: The conditional joint distribution of x and y, with a line showing a linear fit, which illustrates the induced negative correlation.</figcaption></figure>
</div>
<h3 id="from-prediction-to-intervention">From prediction to intervention</h3>
<p>Now that we have some understanding of what a causal model is, we can get to the heart of causality: the difference between an observation and an <em>intervention</em>.</p>
<p>When we introduced the ladder of causation, we mentioned the notion of <em>intervention</em>, something that changes the system. This is a fundamental operation, and it is important to understand the difference between intervention and observation. It may not at first seem natural to consider intervening as a fundamental action, evoking a similar sense of confusion to when one first encounters priors in Bayesian statistics. Is an intervention subjective? Who gets to define what an intervention is?</p>
<p>Simply, an intervention is a change to the data generating process. Samples from the joint distribution of the variables in the graph may be obtained by simply “running the graph forward.” For each cause, we sample from its noise distribution and propagate that value through the SCM to calculate the resulting effects. To compute an <em>interventional</em> distribution, we force particular causes (on which we are intervening) to some value, and propagate those values through the equations of the SCM. This introduces a distribution different from the observational distribution with which we usually work.</p>
<p>There is sometimes confusion between an interventional distribution and a conditional distribution. A conditional distribution is generated by filtering an observed distribution to meet some criteria. For instance, we might want to know the loan default rate among the businesses to which we have granted a loan at a particular interest rate. This interest rate would itself likely have been determined by some model, and as such, the businesses with that rate will likely share statistical similarities.</p>
<p>The interventional distribution (when we intervene on interest rate) is fundamentally different. It is the distribution of loan defaulting if we <em>fix</em> the interest rate to a particular value, regardless of other features of the business that may warrant a different rate. This corresponds to removing all the inbound arrows to the interest rate in the causal graph; we’re forcing the value, so it no longer depends on its causal parents.</p>
<p>Clearly, not all interventions are physically possible! While we could intervene to set the interest rate, we of course would not be able to make every business a large one.</p>
<h4 id="interventions-in-code">Interventions in code</h4>
<div>
<p>It is easy to make interventions concrete with code. Returning to the collider example, to compute an interventional distribution, we could define a new sampling function where instead of drawing all variables at random, we intervene to set <code>x</code> to a particular value. Because this is an intervention, not simply conditioning (as earlier), we must make the change, then run the data generating process again.</p>
<pre><code>def sample_intervened():
  x_ = -3
  y_ = y()
  z_ = z(x_, y_)
  return x_, y_, z_
</code></pre>
<p>Performing this intervention results in a new distribution for <code>z</code>, which is different from the observational distribution that we saw earlier. Further, the relationship between x and y has changed; the joint distribution is now simply the marginal distribution of <code>y</code>, since <code>x</code> is fixed. This is a strikingly different relationship than when we simply conditioned the observational distribution.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/scm-intervened.png" alt="Left: We have intervened to fix x in the data generating process, which changes z, but not y. Right: When we intervened on x, the joint distribution of x and y became just the marginal distribution of y."><figcaption>Left: We have intervened to fix x in the data generating process, which changes z, but not y. Right: When we intervened on x, the joint distribution of x and y became just the marginal distribution of y.</figcaption></figure>
</div>
<h4 id="interventions-in-customer-churn">Interventions in customer churn</h4>
<p>In our <a href="https://ff06-2020.fastforwardlabs.com/">interpretability report</a>, we present a customer churn modeling use case. Briefly, given 20 features of the customers of a telco - things like tenure, demographic attributes, whether they have phone and internet services, and whether they have tech support - we must model their likelihood of churning within a fixed time. To do this, we turn to a dataset of customers and whether they churned in the time period. This can be modeled as straightforward binary classification, and we can use the resulting output scores as a measure of how likely a customer is to churn.</p>
<p>The model used to calculate the churn score is an ensemble of a linear model, a random forest, and a simple feed forward neural network. With appropriate hyperparameters and training procedure, such an ensemble is capable of good predictive performance. That performance is gained by exploiting subtle correlations in the data.</p>
<p>To understand the predictions made, we apply <a href="https://arxiv.org/abs/1602.04938">LIME</a>. This returns a feature importance at the local level: which features contributed to each individual prediction. To accompany the analysis, we built Refractor, an interface for exploring the feature importances. Examining these is interesting, and highlights the factors that are <em>correlated</em> with a customer being likely to churn. <a href="https://refractor.fastforwardlabs.com/">Refractor</a> suggests which features most affect the churn prediction, and allows an analyst to change customer features and see the resulting churn prediction.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/refractor.gif" alt="The Refractor prototype"><figcaption>The <a href="https://refractor.fastforwardlabs.com/">Refractor</a> prototype</figcaption></figure>
<p>Because we have a model that provides new predictions when we change the features, it is tempting to believe we can infer from this alone how to reduce churn probability. Aside from the fact that often the most important features cannot be changed by intervention (tenure, for instance), this is an incorrect interpretation of what LIME and our model provide. The correct interpretation of the prediction is the probability of churn for someone who <em>naturally</em> occurred in our dataset with those features, or, for instance, what this same customer’s churn probability will look like next year (when tenure will have naturally increased by one year), assuming none of their other features change.</p>
<p>Of course, there are some features that can be changed in reality. For instance:</p>
<ul>
<li>the telco could reduce the monthly fee for a customer, or</li>
<li>try to convince them to change contract type from monthly to yearly (one does not have to think too hard about why this changes the short-term churn probability), or</li>
<li>upgrade the service from DSL to fiber-optic.</li>
</ul>
<p>Which of these interventions would most decrease the probability that the customer churns? We don’t know. Our model alone - for all its excellent predictive accuracy - can’t tell us that, precisely because it is entirely correlative. Even a perfect model, that 100% accurately predicts which customers will churn, cannot tell us that.</p>
<p>With some common sense, we can see that a causal interpretation is not appropriate here. LIME often reports that having a faster fiber-optic broadband connection increases churn probability, relative to slower DSL. It seems unlikely that faster internet has this effect. In reality, LIME is correctly reporting that there is a <em>correlation</em> between having fiber-optic and churning, likely because of some latent factors - perhaps people who prefer faster internet are also intrinsically more willing to switch providers. This distinction of interpretation is crucial.</p>
<p>The model can only tell us what <strong>statistical dependencies</strong> exist in the dataset we trained it on. The training dataset was purely observational - a snapshot of a window of time with observations about those customers in it. If we select “give the customer access to tech support” in the app, the model can tell us that similar customers who also had access to tech support were less likely to churn. Our model only captures information about customers who happened to have some combination of features. It does not capture information about what happens when we <em>change</em> a customer’s features. This is an important distinction.</p>
<p>To know what would happen when we intervene to change a feature, we must compute the interventional distribution (or a point prediction), which can be very different from the observational distribution. In the case of churn, it’s likely the true causal graph is rather complex.</p>
<p>Interpretability techniques such as LIME provide important insights into models, but they are not causal insights. To make good decisions using the output of any interpretability method, we need to combine it with causal knowledge.</p>
<p>Often, this causal knowledge is not formally specified in a graph, and we simply call it “domain knowledge,” or expertise. We have emphasized what the <em>model</em> cannot do, in order to make the technical point clear, but in reality, anyone working with the model would naturally apply their own expertise. The move from that to a causal model requires formally encoding the assumptions we make all the time and verifying that the expected statistical relationships hold in our observed data (and if possible, experimenting). Doing so would give us an understanding of the cause-effect relationships in our system, and the ability to reason quantitatively about the effect of interventions.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-11.png" alt=""></figure>
<p>Constructing a useful causal model of churn is a complex undertaking, requiring both deep domain knowledge and a detailed technical understanding of causal inference.<sup><a href="#fn9" id="fnref9">[9]</a></sup> In <a href="#causality-and-invariance">Causality and Invariance</a>, we will discuss some techniques that are bridging the gap between a full causal model and the supervised learning setup we use in problems like churn prediction.</p>
<h4 id="when-do-we-need-interventions%3F">When do we need interventions?</h4>
<p>When do we need to concern ourselves with intervention and causality? If all we want to do is predict, and to do so with high accuracy (or whatever model performance metric we care about), then we should use everything at our disposal to do so. That means making use of all the variables that may correlate with the outcome we’re trying to predict, and it doesn’t matter that they don’t cause the outcome. Correlation is not causation, but correlation is still predictive,<sup><a href="#fn10" id="fnref10">[10]</a></sup> and supervised learning excels at discovering subtle correlations.</p>
<p>Some situations in which this pure supervised learning approach is useful:</p>
<ul>
<li>We want to predict when a machine in our factory will fail.</li>
<li>We want to forecast next quarter’s sales.</li>
<li>We want to identify named entities in some text.</li>
</ul>
<p>Conversely, if we want to predict the effect of an intervention, we need causal reasoning. For example:</p>
<ul>
<li>We want to know what to change about our machines to reduce the likelihood of failures.</li>
<li>We want to know how we can increase next quarter’s sales.</li>
<li>We want to know whether longer or shorter article headlines generate more clicks.<sup><a href="#fn11" id="fnref11">[11]</a></sup></li>
</ul>
<h3 id="how-do-we-know-which-graph-to-use%3F">How do we know which graph to use?</h3>
<p>Knowing the true causal structure of a problem is immensely powerful. Earlier in this chapter, we discussed three building blocks of causal graphs (direct causation, forks, and colliders) but for real problems, a graph can be arbitrarily complex.</p>
<p>The graph structure allows us to reason qualitatively about what statistical dependencies ought to hold in our data. In the absence of abundant randomized controlled trials or other experiments, qualitative thinking is necessary for causal inference. We must use our domain knowledge to construct a plausible graph to test against the data we have. It is possible to refute a causal graph by considering the statistical independence relations it implies, and matching those against the expected relations from the causal structure. For example, if two variables are connected by a common cause on which we have not conditioned, we should expect a statistical dependence between them.</p>
<div>
<h5 id="causal-discovery">Causal Discovery</h5>
<p>The independence relationships implied by a graph can be used for causal discovery. Causal discovery is the process of attempting to recover causal graphs from observational data. There are many approaches appropriate for different sets of assumptions about the graph. However, since many causal graphs can imply the same joint distribution, the best we should hope for from causal discovery is a set of plausible graphs, which, if we are fortunate, may contain the true graph. In reality, inferring the direction of causation in even a two variable system is not always possible from data alone.<sup><a href="#fn12" id="fnref12">[12]</a></sup></p>
</div>
<p>It is not, in general, possible to <em>prove</em> a causal graph, since different graphs can result in the same observed and even interventional distributions. The difficulty of confirming a causal relationship means that we should always proceed with caution when making causal claims. It is best to think of causal models as giving results <em>conditional on a set of causal assumptions</em>. Two nodes that are not directly connected in the causal graph are assumed to be independent in the data generating process, except insofar as the causal relations described above (or combinations of them) induce a statistical dependence.</p>
<p>The validity of the results depends on the validity of the assumptions. Of course, we face the same situation in all machine learning work - and it is to be expected that stronger, causal claims require stronger assumptions than merely observational claims.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-23.png" alt="Sometimes it can be difficult to establish the causal direction even in very simple graphs."><figcaption>Sometimes it can be difficult to establish the causal direction even in very simple graphs.</figcaption></figure>
<p>One case in which we may be able to write down the true causal graph is when we have ourselves created the system. For instance, a manufacturing line may have a sufficiently deterministic process that makes it possible to write down a precise graph encoding which parts move from which machine to another. If we were to model the production of faulty parts, that graph would be a good basis for the causal graph, since a machine that has not processed a given faulty part is unlikely to be responsible for the fault, and causal graphs encode exactly these independences.</p>
<h3 id="tl%3Bdr">TL;DR</h3>
<p>Causal graphical models present an intuitive and powerful means of reasoning about systems. If an application requires only pure prediction, this reasoning is not necessary, and we may apply supervised learning to exploit subtle correlations between variables and our predicted quantity of interest. However, when a prediction will be used to inform a decision that changes the system, or we want to predict for the system under intervention, we <em>must</em> reason causally  - or else likely draw incorrect conclusions. That said, behind every causal conclusion there is always a causal assumption that cannot be tested or verified by mere observation.</p>
<p>Even without a formal education in causal inference, there are advantages to the qualitative reasoning enabled by causal graphical models. Trying to write down a causal graph forces us to confront our mental model of a system, and helps to highlight potential statistical and interpretational errors. Further, it precisely encodes the independence assumptions we are making. However, these graphs could be complex and high dimensional and require close collaboration between practitioners and domain experts who have substantive knowledge of the problem.</p>
<p>In many domains, problems such as the large numbers of predictors, small sample sizes, and possible presence of unmeasured causes, remain serious impediments to practical applications of causal inference. In such cases, there is often limited background knowledge to reduce the space of alternative causal hypotheses. Even when experimental interventions are possible, performing the many thousands of experiments that would be required to discover causal relationships between thousands or tens of thousands of predictors is often not practical.</p>
<p>Given these challenges, how do we combine causal inference and machine learning? Many of the researched approaches at the intersection of ML and causal inference are motivated by the ability to apply causal inference techniques to high dimensional data, and in domains where specifying causal relationships could be difficult. In the next chapter, we will bridge this gap between structural causal models and supervised machine learning.</p>
<h2 id="causality-and-invariance">Causality and Invariance</h2>
<p>Supervised machine learning is very good at prediction, but there are useful lessons we can take from causal models even for purely predictive problems.</p>
<p>Relative to recent advancements made in the broader field of machine learning, the intersection of machine learning and causal reasoning is still in its infancy. Nonetheless, there are several emerging research directions. Here, we focus on one particularly promising path: the link between causality and invariance. Invariance is a desirable property for many machine learning systems: a model that is invariant is one that performs well in new circumstances, particularly when the underlying data distribution changes. As we will see in this chapter, invariance also provides a route to some causal insights, even when working only with observational data.</p>
<h3 id="the-great-lie-of-machine-learning">The great lie of machine learning</h3>
<p>In supervised learning, we wish to predict something that we don’t know, based on only the information that we do have. Usually, this boils down to learning a mapping between input and output.</p>
<p>To create that map, we require a dataset of input features and output targets; the number of examples required scales with the complexity of the problem. We can then fit the parameters of a learning algorithm to the dataset to minimize some loss function that we choose. For instance, if we are predicting a continuous number, like temperature, we might seek to minimize the mean squared difference between the prediction and the true measurements.</p>
<p>If we are not careful, we will <em>overfit</em> the parameters of the ML algorithm to the dataset we train on. In this context, an overfit model is one that has learned the idiosyncrasies (the spurious correlations!) of our dataset. The result is that when the model is applied to any other dataset (even one with the same data generating process), the model’s performance is poor, because it is relying on superficial features that are no longer present.</p>
<p>To avoid overfitting, we employ various regularization schemes and adjust the capacity of the model to an appropriate level. When we fit the model, we shuffle and split our data, so we may learn the parameters from one portion of the data, and validate the resulting model’s performance on another portion. This gives us confidence that the learned parameters are capturing something about all the data we have, and not merely a portion of it.</p>
<p>Whatever procedure we use (be it cross-validation, forward chaining for time series, or simpler train-test-validation splits), we are relying on a crucial assumption. The assumption is that the data points are <em>independent and identically distributed</em> ( i.i.d.). By <em>independent</em>, we mean that each data point was generated without reference to any of the others, and by <em>identically distributed</em>, we mean that the underlying distributions in the data generating process are the same for all the data points.</p>
<p>Paraphrasing <a href="https://www.youtube.com/watch?v=x1UByHT60mQ&amp;feature=youtu.be&amp;t=37m34s">Zoubin Ghahramani</a>,</p>
<blockquote>
<p>the i.i.d. assumption is the great lie of machine learning.</p>
</blockquote>
<p>Rarely are data truly independent and identically distributed. What are the ramifications of this misassumption for machine learning systems?</p>
<h3 id="dangers-of-spurious-correlations">Dangers of spurious correlations</h3>
<p>When we train a machine learning system with the i.i.d. assumption, we are implicitly assuming an underlying data generating process for that data. This data generating process defines an <em>environment</em>. Different data generating processes will result in different environments, with different underlying distributions of features and targets.</p>
<p>When the environment in which we predict differs from the environment in which our machine learning system was trained, we should expect it to perform poorly. The correlations between features and the target are different - and, as such, the model we created to map from features to target in one environment will output incorrect values of the target for the features in another environment.</p>
<p>Unfortunately, it’s rarely possible to know whether the data generating process for data at predict time (in a deployed ML system, for instance) will be the same as during training time. Even once the system is predicting in the wild, if we do not or cannot collect ground truth labels to match to the input features on which the prediction was based, we may never know.</p>
<p>This problem is not academic. <a href="https://arxiv.org/abs/1807.04975">Recognition in Terra Incognita</a> points this out in humorous fashion (see also <a href="http://people.csail.mit.edu/torralba/publications/datasets_cvpr11.pdf">Unbiased Look at Dataset Bias</a>). Both of these papers highlight that computer vision systems trained for visual recognition of objects, animals, and people can utterly fail to recognise the same objects in different contexts. A cow on the slopes of an alpine field is easily recognised, but a cow on a beach is not noticed at all, or poorly classified as a generic “mammal.”</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/terra-incognita.png" alt="Figure from Recognition in Terra Incognita, where annotations were provided by ClarifAI.com."><figcaption>Figure from <a href="https://arxiv.org/abs/1807.04975">Recognition in Terra Incognita</a>, where annotations were provided by <a href="https://www.clarifai.com/">ClarifAI.com</a>.</figcaption></figure>
<p>These failures should not come as a surprise to us! Supervised machine learning is <em>designed</em> to exploit correlations between features to gain predictive performance, and cows and alpine pastures are highly correlated. Neural networks are a very flexible class of models that encode the invariants of the dataset on which they’re trained. If cows dominantly appear on grass, we should expect this to be learned.</p>
<div>
<h5 id="when-is-a-correlation-spurious%3F">When is a correlation spurious?</h5>
<p>In supervised learning, we learn to use subtle correlations, possibly in high dimensional spaces like natural images, to make predictions. What distinguishes a genuine correlation from a spurious one? The answer depends on the intended use of the resulting model.</p>
<p>If we intend for our algorithm to work in only one environment, with very similar images, then we should use all the correlations at our disposal, including those that are very specific to our environment. However, if - as is almost always the case - we intend the algorithm to be used on new data outside of the training environment, we should consider any correlation that only holds in the training environment to be spurious. A spurious correlation is a correlation that only appears to be true due to a selection effect (such as selecting a training set!).</p>
<p>In <a href="#background%3A-causal-inference">Background: Causal Inference</a>, we saw that correlation can arise from several causal structures. In the strictest interpretation, any correlation that does not arise from direct causation could be considered spurious.</p>
<p>Unfortunately, given only a finite set of training data, it is often not possible to know which correlations are spurious. The methods in this section are intended to address precisely that problem.</p>
</div>
<p>When a machine learning algorithm relies heavily on spurious correlations for predictive performance, its performance will be poor on data from outside the dataset on which it was trained. However, that is not the only problem with spurious correlations.</p>
<p>There is an important and growing emphasis on interpretability in machine learning. A machine learning system should not only make predictions, but also provide a means of inspecting how those predictions were made. If a model is relying on spurious correlations, the feature importances (such as those calculated by <a href="https://arxiv.org/abs/1602.04938">LIME</a> or <a href="https://arxiv.org/abs/1705.07874">SHAP</a>) will be similarly spurious. No one should make decisions based on spurious explanations!</p>
<h3 id="invariance">Invariance</h3>
<p>To be confident of our predictions outside of our training and testing datasets, we need a model that is robust to distributional shifts away from the training set. Such a model would have learned a representation which ignores dataset-specific correlations, and instead relies upon features that affect the target in all environments.</p>
<p>How can we go about creating such a model? We could simply train our model with data from multiple environments, as we often do in machine learning (playing fast and loose with the i.i.d. assumption). However, doing so naively would provide us with a model that can only generalize to the environments it has seen (and interpolations of them, if we use a robust objective).<sup><a href="#fn13" id="fnref13">[13]</a></sup> We wish our model to generalize beyond the limited set of environments we can access for training, and indeed extrapolate to new and unseen (perhaps unforeseen) environments. The property we are looking for - performing optimally in all environments - is called invariance.</p>
<p>The connection between causality and invariance is well established. In fact, causal relationships are - by their nature - invariant. The way many intuitive causal relationships are established is by observing that the relationship holds all the time, in all circumstances.</p>
<p>Consider how physical laws are discovered. They are found by performing a series of experiments in different conditions, and monitoring which relationships hold, and what their functional form is. In the process of discovering nature’s laws, we will perform some tests that do not show the expected result. In cases where a law does not hold, this gives us information to refine the law to something that is invariant across environments.<sup><a href="#fn14" id="fnref14">[14]</a></sup></p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-12.png" alt="We learn causal relationships by observing under different experimental conditions. Causal relationships are those that are invariant across the environments created by these conditions."><figcaption>We learn causal relationships by observing under different experimental conditions. Causal relationships are those that are invariant across the environments created by these conditions.</figcaption></figure>
<p>For example, water boils at 100° Celsius (212° Fahrenheit). We could observe that everywhere, and write a simple causal graph: temperature → water boiling. We have learned a relationship that is invariant across all the environments we have observed.</p>
<p>Then, a new experiment conducted on top of a tall mountain reveals that on the mountain, water boils at a slightly lower temperature. After some more experimentation, we improve our causal model, by realising that in fact, both temperature and pressure affect the boiling point of water, and the true invariant relationship is more complicated.</p>
<p>The mathematics of causality make the notion of invariance and environments precise. Environments are defined by interventions in the causal graph. Each intervention changes the data generating process, such that the correlations between variables in the graph may be different (see <a href="#from-prediction-to-intervention">From prediction to intervention</a>). However, direct causal relationships are invariant relationships: if a node in the causal graph depends only on three variables, and our causal model is correct, it will depend on those three variables, and in the same way, regardless of any interventions. It may be that an intervention restricts the values that the causal variables take, but the relationship itself is not changed. Changing the arguments to a function does not change the function itself.</p>
<h4 id="invariance-and-machine-learning">Invariance and machine learning</h4>
<p>In the machine learning setting, we are mostly concerned with using features to predict a target. As such, we tend to select features for their predictive performance. In contrast, causal graphs are constructed based on domain knowledge and statistical independence relations, and thus encode a much richer dependency structure. However, we are not always interested in the entire causal graph. We may be interested only in the causes of a particular target variable. This puts us closer to familiar machine learning territory.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-14.png" alt="In supervised learning, we often use all available variables (or a subset selected for predictive performance) to predict an outcome. With structural causal models, we encode a much richer dependency structure between variables."><figcaption>In supervised learning, we often use all available variables (or a subset selected for predictive performance) to predict an outcome. With structural causal models, we encode a much richer dependency structure between variables.</figcaption></figure>
<p>We will now examine two approaches to combining causal invariance and machine learning. The first, invariant causal prediction, uses the notion of invariance to infer the direct causes of a variable of interest. This restricted form of causal discovery (working out the structure of a small part of the graph in which we are interested) is appropriate for problems with well defined variables where a structural causal model (or at least causal graph) could be created - in principle, if not in practice.</p>
<p>Not all problems are amenable to SCMs. In the following section, we describe invariant risk minimization, where we forego the causal graph and seek to find a predictor that is invariant across multiple environments. We don’t learn anything about the graph structure from this procedure, but we do get a predictor with greatly improved out-of-distribution generalization.</p>
<h3 id="invariant-causal-prediction">Invariant Causal Prediction</h3>
<p><a href="https://arxiv.org/abs/1501.01332">Invariant causal prediction</a> (ICP) addresses the task of invariant prediction explicitly in the framework of structural causal models.</p>
<p>Often, the quantity we are ultimately concerned with in a causal analysis is the causal effect of an intervention: what is the difference in the target quantity when another variable is changed?<sup><a href="#fn15" id="fnref15">[15]</a></sup> To calculate that, we either need to hold some other variables constant, or else account for the fact that they have changed. If we are only interested in the causes that affect a particular target, we do not need to construct the whole graph, but rather only determine which factors are the true direct causes of the target. Once we know that, we can answer causal questions, like how strongly each variable contributes to the effect, or the causal effect of changing one of the input variables.</p>
<p>The key insight offered by ICP is that because direct causal relationships are invariant, we can use that to determine the causal parents (the direct causes). The set-up is similar to that of machine learning; we have some input features, and we’d like a model of an output target. The difference from supervised learning is that the goal is not “performance at predicting the target variable.” In ICP, we aim to discover the direct causes of a given variable - the variables that point directly into the target in the causal graph.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-15.png" alt="We are not always interested in the full causal graph, and instead only seek to find the direct causes of a given target variable. This brings some of the advantages of a causal model into the supervised learning paradigm."><figcaption>We are not always interested in the full causal graph, and instead only seek to find the direct causes of a given target variable. This brings some of the advantages of a causal model into the supervised learning paradigm.</figcaption></figure>
<p>To use ICP, we take a target variable of interest, and construct a plausible list of the potential direct causes of that variable. Then we must define environments for the problem: each environment is a dataset. In the language of SCMs, each environment corresponds to data observed when a particular intervention somewhere in the graph was active. We can reason about this even without specifying the whole graph, or even which particular intervention was active, as long as we can separate the data into environments. In practice, we often take an observed variable to be the environment variable, when it could plausibly be so.</p>
<p>For instance, perhaps we are predicting sales volume in retail, and want to discern what store features causally impact sales. The target is sales volume, and the potential causes would be features like store size, number of nearby competitors, level of staffing, and so on.</p>
<p>Environments might be different counties (or even countries) - something that is unlikely to impact the sales directly, but which may impact the <em>features</em> that impact the sales. For instance, different places will have different populations, and population density is a possible cause of sales volume. Importantly, the environment cannot be a descendent of the target variable.<sup><a href="#fn16" id="fnref16">[16]</a></sup></p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-16.png" alt="We fit a model in multiple environments, and monitor which features are consistently predictive."><figcaption>We fit a model in multiple environments, and monitor which features are consistently predictive.</figcaption></figure>
<p>To apply ICP, we first consider a subset of features. We then fit a linear (Gaussian) regression from this subset to the target in each environment we have defined. If the model does not change between environments (which can be assessed either via the coefficients or a check on residuals), we have found a set of features that appear to result in an invariant predictor. We iterate over subsets of features combinatorially. Features that appear in a model that is invariant are plausible causes of the target variable. The intersection of these sets of plausible causes (i.e., the features which are predictive in all environments) is then a subset of the true direct causes.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-17.png" alt="The features that are consistently predictive of a target are likely the causal parents in the (unknown!) causal graph."><figcaption>The features that are consistently predictive of a target are likely the causal parents in the (unknown!) causal graph.</figcaption></figure>
<p>In machine learning terms, ICP is essentially a feature selection method, where the features selected are very likely to be the direct causes of the target. The model built atop those features can be interpreted causally: a high coefficient for a feature means that feature has a high causal effect on the target, and changes in those features should result in the predicted change in the target.</p>
<p>Naturally, there are some caveats and assumptions. In particular, we must assume there is no unobserved confounding between the features and the target (recall that a confounder is a common cause of the feature and target). If there are known confounders, we must make some adjustments to account for them, as detailed in the <a href="https://arxiv.org/abs/1501.01332">ICP paper</a>. The authors provide an R package, <a href="https://cran.r-project.org/web/packages/InvariantCausalPrediction/index.html">InvariantCausalPrediction</a>, implementing the methods.</p>
<p>The restriction of using a linear Gaussian model - and that environments be discrete, rather than defined by the value of a continuous variable - are removed by nonlinear ICP.<sup><a href="#fn17" id="fnref17">[17]</a></sup> In the nonlinear case, we replace comparing residuals or coefficients with conditional independence tests.<sup><a href="#fn18" id="fnref18">[18]</a></sup></p>
<h3 id="invariant-risk-minimization">Invariant Risk Minimization</h3>
<p>When using Invariant Causal Prediction, we avoid writing the full structural causal model, or even the full graph of the system we are modeling, but we must still think about it.</p>
<p>For many problems, it’s difficult to even attempt drawing a causal graph. While structural causal models provide a complete framework for causal inference, it is often hard to encode known physical laws (such as Newton’s gravitation, or the ideal gas law) as causal graphs. In familiar machine learning territory, how does one model the causal relationships between individual pixels and a target prediction? This is one of the motivating questions behind the paper <a href="https://arxiv.org/abs/1907.02893">Invariant Risk Minimization</a> (IRM). In place of structured graphs, the authors elevate invariance to the defining feature of causality.</p>
<p>They also make the connection between invariance and causality well:</p>
<blockquote>
<p>“If both Newton’s apple and the planets obey the same equations, chances are that gravitation is a thing.”
– <a href="https://arxiv.org/abs/1907.02893">IRM</a> authors</p>
</blockquote>
<p>Like ICP, IRM uses the idea of training in multiple environments. However, unlike ICP, IRM is not concerned with retrieving the causal parents of the target in a causal graph. Rather, IRM focusses on out-of-distribution generalization: the performance of a predictive model when faced with a new environment. The technique proposed aims to create a data representation, on which a classifier or regressor can perform optimally in all environments. The paper itself describes the IRM principle:</p>
<blockquote>
<p>“To learn invariances across environments, find a data representation such that the optimal classifier on top of that representation matches for all environments.”
– <a href="https://arxiv.org/abs/1907.02893">IRM</a> authors</p>
</blockquote>
<p>Said differently, the idea is that there is a latent causal structure behind the problem we’re learning, and the task is to recover a representation that encodes the part of that structure that affects the target. This is different from selecting features, as in Invariant Causal Prediction. In particular, it provides a bridge from very low level features (such as individual pixels) to a representation encoding high level concepts (such as cows).</p>
<h4 id="the-causal-direction">The causal direction</h4>
<p>The idea of a latent causal system generating observed features is particularly useful as a view of computer vision problems. Computer vision researchers have long studied the generative processes involved in moving from real world objects to pixel representations.<sup><a href="#fn19" id="fnref19">[19]</a></sup> It’s instructive to inspect the causal structure of a dataset of cow pictures.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-13.png" alt="When the features are the causes of the target, we say we are learning in the causal direction. When effects are the features, we are learning in the anti-causal direction."><figcaption>When the features are the causes of the target, we say we are learning in the causal direction. When effects are the features, we are learning in the anti-causal direction.</figcaption></figure>
<p>In nature, cows exist in fields and on beaches, and we have an intuitive understanding that the cow itself and the ground are different things. A neural network trying to predict the presence of a cow in an image could be called an “anti-causal” learning problem, because the direction of causation is the opposite of the direction of prediction. The presence of a cow causes certain pixel patterns, but pixels are the input to the network, and the presence of a cow is the output.</p>
<p>However, a further sophistication can be added: the dataset on which we train a neural network is not learning from nature, but rather from annotations provided by humans. This changes the causal direction: we are now learning the effect from the cause, since those annotations are caused by the pixels of the image. This is the view taken by IRM,<sup><a href="#fn20" id="fnref20">[20]</a></sup> which thus interprets supervised learning from images as being a causal (rather than anti-causal) problem.<sup><a href="#fn21" id="fnref21">[21]</a></sup></p>
<p>Not all supervised learning problems are causal. Anti-causal supervised learning problems arise when the label is not provided based on the features, but by some other mechanism that causes the features. For example, in medical imaging, we could obtain a label without reference to the image itself by observing the case over time (this is not a recommended approach for treatment, of course).</p>
<p>Learning in the causal direction explains some of the success of supervised learning - there is a chance that it can recover invariant representations without modification. Any supervised learning algorithm is learning how to combine features to predict the target. If the learning direction is causal, each input is a potential cause of the output, and it’s possible that the features learned will be the true causes. The modifications that invariant risk minimization makes to the learning procedure improve the chance by specifically promoting invariance.</p>
<h3 id="how-irm-works">How IRM works</h3>
<p>To learn an invariant predictor, we must provide the IRM algorithm with data from multiple environments. As in ICP, these environments take the form of datasets - and, as such, the environments must be discrete. We need not specify the graphical or interventional structure associated with the environments. The motivating example of the IRM paper asks us to consider a machine learning system to distinguish cows from camels, highlighting a similar problem to that which <a href="https://arxiv.org/abs/1807.04975">Recognition in Terra Incognita</a> does: animals being classified based on their environment, rather than on the animal itself. In this case, cows on sand may be misclassified as camels, due to the spurious correlations absorbed by computer vision systems.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-18.png" alt="In the IRM setup, we feed the algorithm data from multiple environments, and we must be explicit about which environment a data point belongs to."><figcaption>In the IRM setup, we feed the algorithm data from multiple environments, and we must be explicit about which environment a data point belongs to.</figcaption></figure>
<p>Simply providing data from multiple environments is not enough. The problem of learning the optimal classifier in multiple environments is a bi-level constrained optimization problem, in which we must simultaneously find the optimal data representation and optimal classifier across multiple separate datasets. IRM reduces the problem to a single optimization loop, with the trick of using a constant classifier and introducing a new penalty term to the loss function.</p>
<pre><code>IRM loss = sum over environments (error + penalty)
</code></pre>
<p>The <code>error</code> is the usual error we would use for the problem at hand - for example, the cross entropy for a classification problem - calculated on each environment. The technical definition of the new <code>penalty</code> term is the squared gradient norm with respect to a constant classifier, but it has an intuitive explanation. While the error measures how well the model is performing in each environment, the penalty measures how much the performance could be improved in each environment with one gradient step.</p>
<p>By including the penalty term in the loss, we punish high gradients (situations in which a large improvement in an environment would be possible with one more epoch of learning). The result is a model with optimal performance in all environments. Without the IRM penalty, a model could minimize the loss by performing extremely well in just one environment, and poorly in others. Adding a term to account for the model having a low gradient (roughly, it has converged) in each environment ensures that the learning is balanced between environments.</p>
<p>To understand the IRM paradigm, we can perform a thought experiment. Imagine we have a dataset of cows and camels, and we’d like to learn to classify them as such. We separate out the dataset by the geolocation of photos - those taken in grassy areas form one environment, and those taken in deserts form another.</p>
<p>As a baseline, we perform regular supervised learning to learn a binary classifier between cows and camels. The learning principle at work in supervised learning is referred to as <em>empirical risk minimization</em>, (ERM); we’re just seeking to minimize the usual cross-entropy loss.<sup><a href="#fn22" id="fnref22">[22]</a></sup> We’ll surely find that we can get excellent predictive performance on these two environments, because we have explicitly provided data from both.</p>
<p>The trouble arises when we want to identify a cow on snow, and find that our classifier did not <em>really</em> learn to identify a cow; it learned to identify grass. The holdout performance of our model in any new environment we haven’t trained on will be poor.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-19.png" alt="If we rely on empirical risk minimization, we learn spurious correlations between animals and their environments."><figcaption>If we rely on empirical risk minimization, we learn spurious correlations between animals and their environments.</figcaption></figure>
<p>With IRM, we perform the training across (at least) two environments, and include the penalty term for each in the loss. We’ll almost certainly find that our performance in the training environments is reduced. However, because we have encouraged the learning of invariant features that transfer across environments, we’re more likely to be able to identify cows on snow. In fact, the very reason our performance in training is reduced is that we’ve not absorbed so many spurious correlations that would hurt prediction in new environments.</p>
<p>It is impossible to guarantee that a model trained with IRM learns <em>no</em> spurious correlations. That depends entirely on the environments provided. If a particular feature is a useful discriminator in all environments, it may well be learned as an invariant feature, even if in reality it is spurious. As such, access to sufficiently diverse environments is paramount for IRM to succeed.</p>
<p>However, we should not be reckless in labeling something as an environment. Both ICP and IRM note that splitting on arbitrary variables in observational data can create diverse environments while destroying the very invariances we wish to learn. While IRM promotes invariance as the primary feature of causality, it pays to hold a structural model in the back of one’s mind, and ask if an environment definition makes sense as something that would alter the data-generating process.</p>
<h4 id="considerations-for-applying-irm">Considerations for applying IRM</h4>
<p>IRM buys us extrapolation powers to new datasets, where independent and identically distributed supervised learning can (at best) interpolate between them. Using IRM to construct models improves their generalization properties by explicitly promoting performance across multiple environments, and leaves us with a new, closer-to-causal representation of the input features. Of course, this representation may not be perfect (IRM is an optimization-based procedure, and we will never know if we have found the true minimum risk across all environments) but it should be a step towards latent causal structure. This means that we can use our model to predict based on true, causal correlations, rather than spurious, environment-specific correlations.</p>
<p>However, there is no panacea, and IRM does come with some challenges.</p>
<p>Often, the dataset that we use in a machine learning project is collected well ahead of time, and may have been collected for an entirely different purpose. Even when a well-labeled dataset that is amenable to the problem exists, it is seldom accompanied by detailed metadata (by which we mean “information about the information”). As such, we often do not have information about the environment in which the data was collected.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-20.png" alt="Most datasets are collected in a variety of environments, and without the metadata necessary to separate them. This presents a challenge for invariance-based approaches."><figcaption>Most datasets are collected in a variety of environments, and without the metadata necessary to separate them. This presents a challenge for invariance-based approaches.</figcaption></figure>
<p>Another challenge is finding data from sufficiently diverse environments. If the environments are similar, IRM will be unlikely to learn features that generalize to environments that are different. This is both a blessing and a curse - on the one hand, we do not need to have perfectly separated environments to benefit from IRM, but on the other hand, we are limited by the diversity of environments. If a feature appears to be a good predictor in all the environments we have, IRM will not be able to distinguish that from a true causal feature. In general, the more environments we have, and the more diverse they are, the better IRM will do at learning an invariant predictor, and the closer we will get to a causal representation.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-21.png" alt="IRM relies on representative data from diverse environments. If we cannot collect enough data from sufficiently diverse environments, we may still learn spurious correlations."><figcaption>IRM relies on representative data from diverse environments. If we cannot collect enough data from sufficiently diverse environments, we may still learn spurious correlations.</figcaption></figure>
<p>No model is perfect, and whether or not one is appropriate to use depends on the objective. IRM is more likely to produce an invariant predictor, with good out-of-distribution performance, than empirical risk minimization (regular supervised learning), but using IRM will come at the expense of predictive performance in the training environment.</p>
<p>It’s entirely possible that for a given application, we may be very sure that the data in the eventual test distribution (“in the wild”) will be distributed in the same way as our training data. Further, we may know that all we want to do with the resulting model is predict, not intervene. If both these things are true, we should stick to supervised learning with empirical risk minimization and exploit all the spurious correlations we can.</p>
<h2 id="prototype">Prototype</h2>
<p>The promise of Invariant Risk Minimization (greatly improved out-of-distribution generalization using a representation that is closer-to-causal) is tempting. The IRM paper performs some experiments that clearly show the method works when applied to an artificial structural causal model. Further, an experiment in which an artificial spurious correlation is injected into the MNIST dataset (by coloring the images) is detailed, and works.</p>
<p>In order to gain a better understanding of the algorithm and investigate further, we wanted to test the same technique in a less artificial scenario: on a natural image dataset.</p>
<h3 id="the-wildcam-dataset">The Wildcam dataset</h3>
<p>The <a href="https://www.kaggle.com/c/iwildcam-2019-fgvc6">iWildCam 2019 dataset</a> (from The iWildCam 2019 Challenge Dataset) consists of wildlife images taken using camera traps. In particular, the dataset contains the Caltech Camera Traps (CCT) dataset, on which we focus. The CCT dataset contains 292,732 images, with each image labeled as containing one of 13 animals, or none. The images are collected from 143 locations, and feature a variety of weather conditions and all times of day. The challenge is to identify the animal present in the image.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/wildcam-coyote-raccoon.png" alt="Left, a coyote in its natural environment. Right, a raccoon in the same location at night. Image credit: The iWildCam 2019 Challenge Dataset, used under the Community Data License Agreement."><figcaption>Left, a coyote in its natural environment. Right, a raccoon in the same location at night. Image credit: The <a href="https://arxiv.org/abs/1907.07617">iWildCam 2019 Challenge Dataset</a>, used under the <a href="https://cdla.io/permissive-1-0/">Community Data License Agreement</a>.</figcaption></figure>
<h3 id="experimental-setup">Experimental setup</h3>
<p>This setup maps naturally to the environmental splits used in IRM. Each camera trap location is a distinct physical environment which is roughly consistent, allowing for seasonal, weather, and day/night patterns. No two environments are the same, though the camera locations are spread around roughly the same geographic region (the American Southwest).</p>
<p>The objects of interest in the dataset are animals, which are basically invariant across environments: a raccoon looks like a raccoon in the mountains and in your backyard (though the particular raccoon may be different). The images are not split evenly between environments, since there is more animal activity in some places than others. Nor are the animal species evenly distributed among cameras. Some cameras will primarily produce images of one species or another, depending on the animals active in the area.</p>
<p>If we were to naively train a model using empirical risk on a subset of cameras, we could well end up learning exactly those class imbalances. If 99% of the images from camera 1 are labeled as deer, then we could have a 99% accurate classifier by learning to recognize the fallen tree that is present only in camera 1, rather than the deer themselves. Clearly such a classifier has not really learned to recognize deer, and would be useless for predicting in another environment.</p>
<p>We want to learn to recognize the animals themselves. The IRM setup seems ideally suited to address this challenge.</p>
<p>To validate the approach, we restricted our experiment to only three cameras and two animal species, which were randomly chosen. Of the three cameras, two were used as training environments, and one as a held-out environment for testing. The task was binary classification: distinguish coyotes from raccoons. We used <a href="https://arxiv.org/abs/1512.03385">ResNet18</a>, a pretrained classifier trained on the much larger ImageNet dataset, as a feature extractor with a final fully connected layer with sigmoid output, which we tuned to the problem at hand.</p>
<p>Each of the environments contained images of both coyotes and racoons. Even this reduced dataset exhibited several challenges typical to real world computer vision: some images were dark, some were blurred, some were labeled as containing an animal when only the foot of the animal was visible, and some featured nothing but a patch of fur covering the lens. We saw some success simply ignoring these problems, but ultimately manually selected only those images clearly showing an identifiable coyote or raccoon.</p>
<h3 id="results">Results</h3>
<p>When tackling any supervised learning problem, it’s a good idea to set up a simple baseline against which to compare performance. In the case of a binary classifier, an appropriate baseline model is to always predict the majority class of the training set. The three environments had a class balance as shown in the table below. The majority class in both train environments is coyote, so our baseline accuracy is the accuracy if we always predict the animal is a coyote, regardless of environment or input image.</p>
<div><table>
<thead>
<tr>
<th></th>
<th>Train environment 1</th>
<th>Train environment 2</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coyotes</td>
<td>582</td>
<td>512</td>
<td>144</td>
</tr>
<tr>
<td>Raccoons</td>
<td>276</td>
<td>241</td>
<td>378</td>
</tr>
<tr>
<td>Baseline accuracy</td>
<td>68%</td>
<td>68%</td>
<td>28%</td>
</tr>
</tbody>
</table></div>
<p>When we treated the problem with empirical risk minimization (minimizing the cross-entropy between classes), we found good performance in the train environments, but very poor performance in the test environment. We report the metrics over 120 epochs of training in the table below. The best test accuracy is achieved at epoch 40, after which ERM (empirical risk minimization) begins to overfit. In the case of IRM (invariant risk minimization), we paid a small price in train set accuracy, but achieved much better test results - again, reporting the highest test accuracy achieved in 120 epochs (at epoch 120).</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/erm-vs-irm-table.png" alt="Table comparing metrics on the combined train set and test set for empirical risk minimization (ERM) and invariant risk minimization (IRM)."><figcaption>Table comparing metrics on the combined train set and test set for empirical risk minimization (ERM) and invariant risk minimization (IRM).</figcaption></figure>
<p>ERM outperforms the baseline in all environments, but not by too much in the new test environment. This can be attributed to the learning of spurious correlations. The network was able to effectively distinguish between raccoons and coyotes in the training environments, but the features it relied upon to do so were not general enough to help prediction much in the test environment.</p>
<p>In contrast, IRM loses a single percentage point of accuracy in the train environments, but performs almost as well in the test environment. The feature representation IRM constructs has translated between different environments effectively, and proves an effective discriminator.</p>
<p>As a practical point, we found that IRM worked best when the additional IRM penalty term was not added to the loss until the point at which ERM had reached its best performance - in this case the 40th training epoch. As such, ERM and IRM had identical training routines and performance until this point. When we introduced the IRM penalty, the IRM procedure continued to learn and gain out-of-distribution generalization capability, whereas ERM began to overfit. By the 120th epoch, IRM had the accuracy reported above, whereas ERM had achieved 91% in the combined training environments, at the cost of reducing its test accuracy by a few percentage points to 33%.</p>
<h4 id="interpretability">Interpretability</h4>
<p>IRM yields impressive results, especially considering how hard it is to learn from these images. It has a clear and significant improvement in when compared to ERM in a new environment. In this section, we examine a few concrete examples of successes and failures of our prototype model and our speculations as to why they may be.</p>
<p>It would be nice to have a better sense of whether IRM has learned invariant features. By that we mean, whether it has learned to spot a raccoon’s long bushy tail or a coyote’s slender head, instead of the terrain or foliage in the image. Understanding which parts of the image contribute towards IRM’s performance is a powerful proposition. The classification task itself is hard: if you closely look at some of the images in the Wildcam dataset, at a first glance it’s even hard for us, humans, to point out where exactly the animal is. An interpretability technique like Local Interpretable Model-agnostic Explanations (<a href="https://arxiv.org/abs/1602.04938">LIME</a>) provides valuable insights into how that classification is working.</p>
<p>LIME is an explanation technique that can be applied to almost any type of classification model — our report <a href="https://ff06-2020.fastforwardlabs.com/">FF06: Interpretability</a> discusses these possibilities — but here we will consider its application to image data. LIME is a way to understand how different parts of an input affect the output of a model. This is accomplished, essentially, by turning the dials of the input and observing the effect on the output.</p>
<p>Let’s first try and understand how LIME works at a high level - including what inputs we need to provide, and what to expect as output - through a sample image in the test set. The image on the left of the figure below is a sample raw image of a coyote with dimensions height=747 and width=1024, as were all images in the dataset.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/coyote-resized.png" alt="Left: a raw Wildcam image. Right: Having been cropped and scaled to the input dimensions required by ResNet18."><figcaption>Left: a raw Wildcam image. Right: Having been cropped and scaled to the input dimensions required by ResNet18.</figcaption></figure>
<p>To use the IRM model, we must first perform some image transformations like resizing, cropping, and normalization - using the same transformations that we did when training the model. The input image then appears as shown on the right of the figure above, a normalized, 224 * 224 image. The transformed image when scored by the IRM model outputs a probability of 98% (0.98) for the coyote class! So yes, our model is pretty confident of its prediction.</p>
<p>Now, let’s see how LIME works on this image. First, LIME constructs a local linear model, and makes a prediction for the image. For the example image, the predicted score is 0.95, pretty close to the IRM model. When trying to explain the prediction, LIME uses interpretable representations. For images, interpretable representations are basically contiguous patches of similar pixels called superpixels. The superpixels for an image are generated by a standard algorithm, QuickShift, in the LIME implementation. The left panel in the figure below shows all of the 34 superpixels generated by LIME for the example image.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/lime-masks.png" alt="LIME masks random combinations of superpixels, generated by QuickShift, to build a local linear model."><figcaption>LIME masks random combinations of superpixels, generated by QuickShift, to build a local linear model.</figcaption></figure>
<p>It then creates versions of the original image by randomly masking different combinations of the superpixels as shown in the middle and right panes of the above figure. Each random set of masked superpixels is one perturbation of the image. The modeler chooses the number of perturbations; in our case, we used 1000 perturbations of the original image. LIME then builds a regression model on all these perturbed images and determines the superpixels that contributed most towards the prediction, based on their weights.</p>
<p>The figure below shows the superpixel explanations (with the rest of the image grayed out) for the top 12 features that contribute towards the prediction of the coyote classification. While there are quite a few features that are mostly spurious covering the foliage or terrain, one of them covers the entire body of the coyote. Looking at these explanations provides an alternative way of assessing the IRM model and can enhance our trust that the model is learning to rely on sensible features.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/irm-top-12.png" alt="The non-grayed-out pixels correspond to the top 12 superpixels that contribute positively to the Coyote classification for the IRM model."><figcaption>The non-grayed-out pixels correspond to the top 12 superpixels that contribute positively to the Coyote classification for the IRM model.</figcaption></figure>
<p>Now when we generate the top 12 LIME explanations for the same image but based on the ERM model, they seem to capture more of the surroundings, rather than any of the coyote’s body parts.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/erm-top-12.png" alt="The non-grayed-out pixels correspond to the top 12 superpixels that contribute positively to the Coyote classification for the ERM model. In this case, they didn’t catch much of the coyote."><figcaption>The non-grayed-out pixels correspond to the top 12 superpixels that contribute positively to the Coyote classification for the ERM model. In this case, they didn’t catch much of the coyote.</figcaption></figure>
<p>And then there are instances where LIME explanations seem to rely on spurious features. For example, in the figure below, the original image is classified as a coyote by the IRM model with a probability of 72% (0.72), whereas the LIME score is close to 0.53. The superpixels contributing towards the classification for both the IRM and ERM models usually cover the terrain or foliage, though some outline the coyote’s body.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/spurious-coyote.png" alt="In this instance, both models seem to be relying on environmental features to predict Coyote."><figcaption>In this instance, both models seem to be relying on environmental features to predict Coyote.</figcaption></figure>
<p>We observe that the explanations make more intuitive sense when the LIME score is close to the model score.</p>
<p>IRM can only learn to be invariant with respect to the invariants that the environments encode. If there are spurious correlations that are the same across environments, IRM will not distinguish them from invariant features.</p>
<p>One feature that appears invariant in this dataset is the day or night cycle. Raccoons appear exclusively at night, and IRM could well learn that night means raccoon, and rely on it heavily. This correlation is spurious; a raccoon is still a raccoon during the day! However, we would need more environments, including images of raccoons in the daytime, to disentangle that.</p>
<p>The representation that IRM extracts from an environment should theoretically be closer to encoding the latent causal structure of the problem than that which ERM extracts. In our scenario, we might expect that IRM learns to focus more on the actual animal in the picture, since the presence of the animal is the cause of a given annotation. The animals change little between environments, whereas environmental features (like foliage) are completely different at different camera trap locations. Thus, the causal features ought to be invariant between environments.</p>
<p>That said, although the IRM results appear promising for some samples, it is hard to confirm that there is an obvious pattern, and this can be attributed to both the model and the interpretability technique. We chose to train only the last layer of ResNet18 to come up with the IRM model. This choice has an inherent drawback: the capacity for feature learning is low. As such, we wouldn’t expect perfect results, since it’s unlikely that the pretrained ResNet representations map perfectly to raccoons and coyotes.<sup><a href="#fn23" id="fnref23">[23]</a></sup></p>
<p>Further, although an explanation of an image provides some reassurance of the quality of the model, it’s probably still insufficient to provide an overall picture of the <em>kind</em> of features a given model is using, aggregated from all the individual explanations. And even though explanations for multiple images are insightful, these have to be judiciously selected. When it comes to text or tabular data, there are ways to determine the global feature importances, because the features in tabular data or vocabulary stay consistent across all the data points. The superpixels of an image cannot be consistent across all the images, which makes it really hard to assess whether the explanations make sense. Developing tools to understand large image datasets is a worthy endeavour!</p>
<h3 id="product%3A-scene">Product: Scene</h3>
<figure><img src="https://ff13.fastforwardlabs.com/figures/scene.png" alt="The Scene prototype"><figcaption>The <a href="https://scene.fastforwardlabs.com/">Scene prototype</a></figcaption></figure>
<p>To accompany this report, we built a prototype called <a href="https://scene.fastforwardlabs.com/">Scene</a> that takes you on a guided tour through the dataset, models, and results of our experiment. With Scene, we really wanted to give people a feel for the images that make up the dataset. Each panel of the tour features 16 images from the dataset, cropped and resized to the same dimensions of the images that the model is trained on. Many of the images featured are randomly sampled from the dataset when we generate the page, while others we specifically selected to use as examples. We hope that the amount and variety of images shown helps people get an intuitive feel for the dataset.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/scene-all.png" alt="View all the images in the dataset on the all page."><figcaption>View all the images in the dataset on the <a href="https://scene.fastforwardlabs.com/all">all</a> page.</figcaption></figure>
<p>If you want to go even deeper, we included an <a href="https://scene.fastforwardlabs.com/all">all</a> page, which shows all 2,133 images in the dataset, along with the predictions and interpretability visualizations for each model. It’s nice to be able to use these visualizations to check intuitions (like which features are important to each model) with your own eyes. Of course, even having access to all the images doesn’t mean you can see “the big picture.” It’s difficult to hold everything you’ve seen in your head as you scroll through. If you’re not careful, you’ll end up generalizing the patterns you’ve seen most recently to the entire dataset. This is the challenge of visualizing the scale of the data that machine learning systems take in. Other techniques, like embeddings (as seen in our <a href="https://activelearner.fastforwardlabs.com/">Active Learner</a> prototype) can help you visualize patterns, but then you lose some of the detail gained by being able to see the images up close. No one technique can give you the whole picture; data visualization requires a variety of techniques.</p>
<p>Generating such a large number of images, complete with text labels and interpretability overlays, was an interesting technical challenge. Originally, we’d planned to have Scene animate transitions between the original image and the interpretability overlays. To do this efficiently in a browser, you generate a “sprite sheet” - a large image that contains all the different animation states you’ll transition through (a technique borrowed from video games). It was while we were generating the sprite sheets that we decided that, rather than transitioning through them one at a time, it would be more effective to show the entire sheet. Having more images visible together made comparisons easier and the scale of the dataset more clear. We ended up using the <a href="https://activelearner.fastforwardlabs.com/">node-canvas</a> package to crop and place the images, overlay the interpretability layers, and apply the labels through a node script. Since we do all the work of generating images locally, we guarantee the user as snappy an experience as possible. Static site generation has seen renewed interest as a web-development strategy, and could be especially useful for large-scale data-visualization.</p>
<h2 id="landscape">Landscape</h2>
<p>Causality spans a broad area of topics, including using causal insights to improve machine learning methods, adapting it for high-dimensional datasets and applying them for better data-driven decision making in real-world contexts. We also discussed in <a href="#causality-and-invariance">Causality and Invariance</a> how the collected data is rarely an accurate reflection of the population, and hence may fail to generalize in different environments or new datasets. Methods based on invariance show promise in addressing out-of-distribution generalization.</p>
<h3 id="use-cases">Use Cases</h3>
<p>As we demonstrated in the <a href="#prototype">Prototype</a> chapter, Invariant Risk Minimization is particularly well suited to image problems in diverse physical environments. However, an environment need not mean only the scenery in an image, and when it does, it need not be fixed to a single value. Here we suggest some applications in and beyond computer vision.</p>
<h4 id="healthcare">Healthcare</h4>
<p>In healthcare, medical images have to be manually annotated by radiologists to identify abnormalities. These annotated images are then used to train and build diagnostic models. Often the devices (like MRI scanners) which generate these medical images exhibit some kind of variation.<sup><a href="#fn24" id="fnref24">[24]</a></sup> That is, due to mechanical configurations, vendor differences, or any number of other reasons, the images that are generated by one MRI scanner could be systematically different from another for the same patient. As such, a diagnostic model that was built on the images generated by an old MRI scanner may perform poorly when tested on the images generated by a new scanner. One way to solve this problem is to have the radiologist annotate the images generated by the new scanner and then retrain the model. But that could be expensive and time-consuming. Plus, this isn’t a permanent solution; every time there’s a new scanner or changes to the configuration, it would be necessary to retrain the existing model with an entirely new set of images.</p>
<p>A diagnostic model based on invariant prediction that treats scanners as environments could be immune to these noisy device variations and change its decisions accordingly. This could save the time and money needed to annotate images from the new scanner.</p>
<h4 id="robotics">Robotics</h4>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-26.png" alt="Autonomous systems trained in the lab or in limited environments will struggle to adapt to the diversity present in the real world."><figcaption>Autonomous systems trained in the lab or in limited environments will struggle to adapt to the diversity present in the real world.</figcaption></figure>
<p>Autonomous systems need to detect and adapt to different environments. These systems rely on sophisticated sensors, cameras, and large amounts of labeled and diverse real-world datasets (which are difficult to acquire). Take, for example, the task of autonomously following a man-made trail that is traversed by hikers or mountain bikers. This is a mostly unsolved task for robotics, but yet an important one for applications like search and rescue.<sup><a href="#fn25" id="fnref25">[25]</a></sup></p>
<p>While many types of robots (such as the quadrupedal robot) can be efficient at locomotion, successfully navigating real-world forest trails is hard. Apart from the mechanics of the problem, perceiving real-world trails is difficult. The appearance of the wilderness area may vary a lot depending on the location, unpaved roads generally have less structure (and tend to blend in with the surrounding grass areas, vegetation, and such), and trails change over time. It would be impossible to have a comprehensive dataset of all trails, in all weather and lighting conditions.</p>
<p>In such cases, a possible solution is to cast the trail perception problem as an image classification task and adopt an invariance based approach that operates directly on the image’s raw pixel values. Successful application could allow for out-of-distribution generalization to new trails, since the features learned are more transferable than environment-specific signals. Naturally, similar ideas are relevant for autonomous vehicles in urban areas.</p>
<h4 id="activity-recognition-systems">Activity recognition systems</h4>
<p>Smart devices (phones, watches, fitness trackers) carry a large array of sensors: accelerometers, gyroscopes, magnetometers, barometers, ambient lights sensors, and many more. Categorizing this data by the activity being performed at the time of recording - such as sitting, standing, or swimming - has allowed for the development of machine learning-based human-activity recognition systems. Correctly predicting a wearer’s activity enables a host of contextual applications, in particular in (but not restricted to) the health and wellness space.<sup><a href="#fn26" id="fnref26">[26]</a></sup></p>
<p>Unfortunately, it is hard to satisfactorily model this data due to the diversity exhibited in the real world. A single individual can perform a given activity slightly differently day-to-day, or the device may be unusually placed, or held or worn in a variety of orientations. Of course, different users are also physically diverse, and devices have intrinsic differences in their sensors and systems. This means that we either need a labeled dataset that captures the activity for each user and device (which is prohibitively expensive) or another way of identifying attributes that generalize better. Methods based on invariance could be particularly useful and well-suited in this scenario, capturing the essence of “sitting,” rather than the particular sensor activations for a particular user sitting on a particular chair.</p>
<h4 id="natural-language-processing">Natural language processing</h4>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-25.png" alt="Environments are everywhere. For instance, different sources of natural language."><figcaption>Environments are everywhere. For instance, different sources of natural language.</figcaption></figure>
<p>Invariant prediction approaches are of course not restricted exclusively to image problems. In natural language processing, texts from different publication platforms are tricky to analyze due to different contexts, vocabularies, and differences between how authors express themselves. For instance, financial news articles use a vocabulary and tone that differs from culture or society articles. The former is likely terse, whereas the latter may have an entertaining or personal tone. Similarly, online product reviews are linguistically different from tweets. Sentiment classification also relies heavily on context; different words are used to express whether someone likes a book versus an electronic gadget.</p>
<p>Two recent papers, <a href="https://arxiv.org/abs/2004.05007">An Empirical Study of Invariant Risk Minimization</a> and <a href="https://arxiv.org/abs/2003.09772">Invariant Rationalization</a>, apply the idea of IRM to a sentiment classification task, and find it improves out-of-distribution generalization. In particular, invariance acts to remove spurious reliance on single words which correlate highly with the target. Like images, text corpora form very high-dimensional datasets (there are many possible words!), making spurious correlations extremely unlikely to be noticed “manually.” As such, invariance based approaches are especially promising here.</p>
<h4 id="recommender-systems">Recommender systems</h4>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-24.png" alt="The data a recommender system collects is inherently biased by the suggestions it makes. We can untangle this bias with causal inference."><figcaption>The data a recommender system collects is inherently biased by the suggestions it makes. We can untangle this bias with causal inference.</figcaption></figure>
<p>Recommendation systems are algorithms designed to present relevant items to users on the web (for example, suggesting which movie to watch, a book to read, or a product to buy). As such, making good recommendations is an important problem: we want to make relevant recommendations for a user based on a record of their historical activities, from which we must infer their preferences.</p>
<p>The training data is either explicit (e.g., a rating a user left on a book) or implicit (e.g., linger time on a webpage or click data). There is a well-known exposure problem in recommender systems: a user simply cannot click on an item with which they have not been presented. Modeling the data without accounting for this is akin to the assumption of independent and identically distributed data, and is false: users do not select items randomly and independently of one another. For instance, a user may choose between two competing movies to watch, rather than selecting whether to watch each independently.</p>
<p>RecSys are a classic application for causality, which allows us to correct for this exposure bias by treating the selection of items to present to a user as an intervention. Applying causal approaches to recommendation naturally improves generalization to new data,<sup><a href="#fn27" id="fnref27">[27]</a></sup> and it seems likely that methods using invariant prediction could enhance this.</p>
<h3 id="tools">Tools</h3>
<p>The invariance-based approaches to causality we have discussed do not require dedicated tooling - ICP and IRM are procedures that could be implemented with general purpose machine learning frameworks.</p>
<p>Nonetheless, the authors of the ICP papers <sup><a href="#fn28" id="fnref28">[28]</a></sup> provide corresponding R packages: <a href="https://cran.r-project.org/web/packages/InvariantCausalPrediction/index.html">InvariantCausalPrediction</a> and <a href="https://cran.r-project.org/web/packages/nonlinearICP/index.html">nonlinearICP</a>. The packages make the techniques easy to use, and include additional utilities, such as dedicated plots for confidence intervals on causal coefficients. We are not aware of a package for IRM, but the authors have provided a <a href="http://github.com/facebookresearch/InvariantRiskMinimization/">code repository</a> which reproduces the paper results.</p>
<p>Below, we list a handful of open source projects that aid in traditional, SCM-based causal inference.</p>
<h4 id="dowhy">DoWhy</h4>
<p>Microsoft Research is developing the <a href="https://microsoft.github.io/dowhy/">DoWhy</a> python library for causal inference, incorporating elements of both causal graphical models and potential outcomes. The library is oriented around pandas DataFrames, and fits easily into a Python data analysis workflow. In particular, DoWhy makes a separation between four stages of causal inference:</p>
<ol>
<li>Modeling - defining a causal graph, or else the assumptions necessary for a potential outcomes approach (the common causes of the treatment and the outcome variable).</li>
<li>Identification - identifying the expression it is necessary to evaluate, in terms of conditional probability distributions.</li>
<li>Estimation - estimating the treatment effect. There are many estimation methods available in DoWhy, including machine learning-based methods from another of Microsoft’s causal libraries: <a href="https://github.com/microsoft/EconML">EconML</a>.</li>
<li>Refutation - assessing the robustness of the conclusion. Given the reliance of causal inference on modeling assumptions, it is especially important to find ways to test our conclusions. DoWhy provides several methods for this, such as introducing a dummy common cause or replacing the treatment with a random placebo.</li>
</ol>
<p>In addition to the above, DoWhy includes a novel algorithm, the “do-sampler.” In much of causal inference, the quantity of interest is a single number - for instance, the difference in the outcome variable when a binary treatment variable is applied (“what is the average causal effect of smoking on cancer incidence?”). The do-sampler extends the pandas DataFrame API directly, and moves beyond calculating causal effects to allow sampling from the full interventional distribution. Having done so, we can then compute arbitrary statistics under this intervention. The do-sampler is new, but provides a very promising direction for further research, and a potential avenue to making causal inference accessible to many more data science practitioners.</p>
<h4 id="causaldiscoverytoolbox">CausalDiscoveryToolbox</h4>
<p>The <a href="https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/index.html">Causal Discovery Toolbox</a> provides implementations of many algorithms designed for causal discovery - attempting to recover the full causal graph from observational data alone. There are many approaches to causal discovery, and the library is relatively comprehensive, including algorithms for pairwise causal discovery (inferring the direction of causation between a pair of variables), graph skeleton creation (creating an undirected graph of potential causal relationships), and full graphical causal model discovery.</p>
<p>Discovery of entire causal graphs does not yet appear mature enough that we can naively trust its conclusions about the causal structure of a problem. This makes sense, given the difficulty of the task! Inferring the whole causal structure from only observational data is about the hardest imaginable problem we could face with data.</p>
<h4 id="causalnex">CausalNex</h4>
<p><a href="https://causalnex.readthedocs.io/en/latest/">CausalNex</a> is a very recently released (at time of writing) toolkit by QuantumBlack to help data scientists do causal reasoning. It provides both a graph structure learning component to help build the causal graph and tools to fit that graph as a Bayesian network.</p>
<p>The structure learning component is an implementation of <a href="https://arxiv.org/abs/1803.01422">DAGs with NOTEARS</a>, an algorithm that casts structure learning as a continuous optimization problem. In its simplest form, it assumes linear relationships between variables (but unlike some causal discovery methods, does not assume Gaussian noise). Further, the algorithm assumes that all variables are observed (i.e., there is data for all variables). Unfortunately, this is rarely the case in causal problems.</p>
<p>Within these limitations, the algorithm is performant, and allows the user to specify hard constraints (such as, “these variables cannot be child nodes,” or “there is no causal relationship between these two variables”). This facilitates directly encoding domain knowledge into the graph, and using the structure learning component as an aid in places where the causal connection is not known.</p>
<h4 id="pyro">Pyro</h4>
<p>Uber’s <a href="http://pyro.ai/">Pyro</a> probabilistic programming library is primarily intended for implementing deep probabilistic models and fitting them with variational inference. However, in addition to tools for conditioning on observed data, the library implements a do operation to force a variable to take a certain distribution. This allows simulating from interventional distributions, provided the structural causal model (including equations) is known. The intersection of probabilistic programming with causal inference is nascent, but promising!</p>
<h2 id="ethics">Ethics</h2>
<p>Machine learning is playing an increasingly critical role in our society. Decisions that were previously exclusively made by humans are more frequently being made algorithmically. These algorithmic systems govern everything from which emails reach our inboxes, to whether we are approved for credit, to whom we have the opportunity to date – and their impact on our experience of the world is growing. Furthermore, our understanding of how these systems work is still lacking. We can neither explain nor correct them when their predictions are unfairly discriminatory or their outputs are reinforcing existing biases. Causal reasoning gives us a framework for thinking about these problems.</p>
<h3 id="causal-graphs-make-assumptions-explicit">Causal graphs make assumptions explicit</h3>
<p>Even without employing the full machinery of causal inference, when one approaches a new problem, it can be informative to try to write down the causal graph. This forces us to confront our assumptions about a system. It also allows someone else to understand our assumptions, and furnishes a precise framework for debate.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-22.png" alt="Writing down a causal graph provides a principled way to specify and discuss causal assumptions."><figcaption>Writing down a causal graph provides a principled way to specify and discuss causal assumptions.</figcaption></figure>
<p>Making our assumptions explicit aids transparency, which is a win. However, it doesn’t protect against bad assumptions. Establishing causal relationships is hard. Unless we are able to perform sufficient experiments to validate our hypotheses, causal reasoning from observational data is subject to untested (sometimes untestable) assumptions.</p>
<p>We should make any causal claim with humility. As ever, we should be careful of dressing up a bad analysis with additional formalism.</p>
<h3 id="omitting-protected-attributes-is-not-enough">Omitting protected attributes is not enough</h3>
<p>It is unethical, and in many places illegal, to discriminate on the basis of a protected attribute, such as age, race, or disability. Avoiding <em>direct</em> discrimination (whereby some individuals with particular protected attributes are treated unfavourably) is comparatively easy. Appropriately, these protected attributes are frequently omitted from machine learning systems. Using a protected attribute as a feature directly is inviting discrimination based on that attribute.</p>
<p>More difficult to detect and avoid is <em>indirect causal discrimination</em>. Many features that are not themselves protected attributes are nonetheless highly predictive of a protected attribute. For instance, geographic location can correlate very highly with race, religion, and age. In denying loans to any individual with a particular zipcode, a bank could be committing indirect, but very real, discrimination against a protected attribute.</p>
<p>Another sub-category of discrimination is <em>indirect spurious discrimination</em>. These are instances when there are no pathways from causal attributes to the outcome. However, as we saw in <a href="#from-correlation-to-causation">From correlation to causation</a>, correlations can arise from numerous causal structures. As such, merely omitting the protected attribute does not omit its effects. A system is not guaranteed to be non-discriminatory on a protected attribute simply because it does not include that attribute directly. More simply, just because a feature does not cause the target does not mean that it will not be predictive of the target. This presents a particular challenge to algorithmic systems that are designed to find subtle correlations, especially since much historical data on which algorithms are trained is subject to selection bias (and other biases).</p>
<p>Since removing protected attributes is not enough, we must evaluate the resulting model for its discrimination and fairness properties. There are many possible measures of fairness, and it is generally impossible to optimize for all of them.<sup><a href="#fn29" id="fnref29">[29]</a></sup></p>
<p>Several recent papers<sup><a href="#fn30" id="fnref30">[30]</a></sup>, for instance) have proposed causality as a route to understanding and defining fairness and discrimination. In particular, if we have a causal graphical model of a system, we can see which paths are impacted by protected attributes, and correctly account for that impact. There have also been contributions in non-parametric structural causal models that allow one to detect and distinguish the three main discriminations - namely, direct, indirect and spurious.<sup><a href="#fn31" id="fnref31">[31]</a></sup></p>
<p>That said, the difficulty lies in constructing the causal graph. A causal graph could, of course, be used to embed all kinds of biases and prejudices, but at least provides a basis for argument.</p>
<h3 id="invariance-as-a-route-to-fairness">Invariance as a route to fairness</h3>
<p>An interesting idea is proposed in the final section of the IRM paper: treating groups over which we want fairness as the environments. When we seek to learn an invariant model (be that by ICP or IRM), we are explicitly trying to learn a model that performs optimally in different environments. We could construct those environments by separating out groups having different values for protected attributes. Then, by learning a model that seeks to perform optimally in each environment, we are explicitly trying to guarantee the best performance for each protected attribute.</p>
<p>Said differently, invariant features are exactly those that are consistent across groups. Consider again a bank granting loans, this time directly to individuals. The bank does not wish to discriminate on the basis of protected attributes. By treating the protected attributes as the groups, they are looking to learn what impacts loan defaulting invariantly across those groups.</p>
<p>The idea of learning an invariant predictor across environments is that the representation used is capturing something true about the generative process of the data. This representation would be, to some degree, <em>disentangled</em>, in the sense that each dimension of the representation (a vector) should correspond to something meaningful. <a href="https://arxiv.org/abs/1905.13662">On the Fairness of Disentangled Representations</a> shows experimentally that disentangled representations improve fairness in downstream uses.</p>
<h2 id="future">Future</h2>
<p>At the outset, causal reasoning provides a conceptual and technical framework for addressing questions about the effect of real or hypothetical actions or <em>interventions</em>. Once we understand what the effect of an action is, we can turn the question around and ask what action plausibly caused an event. This gives us a formal language to talk about cause-and-effect. That said, not every question about cause is easy to answer. Further, it may not be a trivial task to find an answer or even to interpret it. Causal graphs that we discuss in the <a href="#background%3A-causal-inference">Background: Causal Inference</a> chapter provide a convenient way to discuss these notions, and allow us to reason about statistical dependencies in observed data.</p>
<p>Structural causal models take a step further to this intuitive way of reasoning by making formal assumptions about the parametric form of how the variables interact.</p>
<p>However, causal graphs and SCMs become difficult to construct as the number of variables increases. Some systems are hard to model in this way. How do we draw a causal graph for pixels of an image? Or words in text? The problem gets out of hand quickly.</p>
<p>Fortunately, not all problems require the entire causal graph. Often, we are interested only in the causal relations associated with one particular target variable. This is where methods based on invariance (like IRM) step in to allow the model to capture stable features across environments (that is, different data generating processes). This paradigm enables out-of-distribution generalization. As opposed to causal graphs or structural causal models, where the only way to validate assumptions of the variable interactions is through experimentation, IRM allows us to test them on an unseen test set!</p>
<h3 id="comparable-approaches">Comparable approaches</h3>
<p>So, at this point we probably agree that methods based on invariance are promising. How else might we approach out-of-distribution generalization? In general, there are two families of approaches; those that learn to match the feature distributions (or estimate a data representation) and those that employ some kind of optimization technique.</p>
<h4 id="domain-adaptation">Domain adaptation</h4>
<p>Domain adaptation is a special case of transfer learning. In domain adaptation, the model learns a task in a source domain, which has some feature distribution, and we would like it to be able to perform the same task well in a target domain, where the feature distribution is different. Domains play the same role as environments in invariance-based approaches; a source domain is an environment that was trained in, and a target domain is any environment that was not trained in.</p>
<p>Domain adaptation also enforces a kind of invariance - it seeks a representation that is distributed the same across source and target domains (so, across environments).<sup><a href="#fn32" id="fnref32">[32]</a></sup> However, truly invariant, causal features need not follow the same distribution in different environments. A snowy cow will not generate quite the same pixel distribution as a sandy cow, and the causal feature we wish to represent is the cow itself.</p>
<h4 id="robust-learning">Robust learning</h4>
<p>The idea of learning across multiple environments is not novel to invariance-based approaches. <a href="https://www.aaai.org/Library/AAAI/2005/aaai05-112.php">Robust Supervised Learning</a> is a family of techniques that uses the same multi-environment setup as IRM (but much predates it), with a similar goal of enabling or enhancing out-of-distribution generalization. Said differently, the goal is a predictor that is robust to distributional shifts of the inputs.</p>
<p>The difference from the IRM setup we have covered is the loss function. The key idea is to add environment-specific “baseline” terms to the loss, and try to fix these terms such that particularly noisy environments where the loss may be high do not dominate. Then, minimizing the loss should guarantee good performance across all the known environments. Further, a robust predictor will perform well in new environments that are interpolations of those seen in training. This certainly improves out-of-distribution generalization, but does not allow <em>extrapolation</em> outside of what was seen in training, whereas IRM can extrapolate, thanks to relying on an invariant predictor.</p>
<h4 id="meta-learning">Meta-learning</h4>
<p>Approaches like domain adaptation, robust learning, and (in general) transfer learning try to alleviate the problem of out-of-distribution generalization to some extent. Unfortunately, learning invariant features with varying distributions across environments is still challenging. These approaches are good at interpolation, but not extrapolation.</p>
<p>This is where meta-learning approaches like Model Agnostic Meta Learning (MAML)<sup><a href="#fn33" id="fnref33">[33]</a></sup> come into play. The underlying idea for meta-learners generally is to attempt to learn tasks with a small number of labeled examples. Training meta-learners is a two-step process involving a <em>learner</em> and a <em>trainer</em>. The goal of the learner (model) is to quickly learn new tasks from a small amount of new data; hence, it is sometimes called a <em>fast learner</em>. (A task here refers to any supervised machine learning problem - e.g., predicting a class given a small number of examples.) This learner is trained, by the meta-learner, to be able to learn from a large number of different tasks. The meta-learner accomplishes this by repeatedly showing the learner hundreds and thousands of different tasks.</p>
<p>Learning then, happens at two levels. The first level focuses on quick acquisition of knowledge within each task with a few examples. The second level slowly pulls and digests information across all tasks. In case of MAML (which is optimization-based), the learner (or the first level) can achieve an optimal fast learning on a new task with only a small number of gradient steps because the meta-learner provides a good initialization of a model’s parameters. This approach is close to the problem of learning an optimal classifier in multiple environments, and could be explored further to learn invariant features within the data.</p>
<p>Some recent works have made the connection between causality and meta-learning explicitly.<sup><a href="#fn34" id="fnref34">[34]</a></sup></p>
<h3 id="looking-ahead">Looking ahead</h3>
<p>In this section, we discuss future possibilities with causality in general, as well as with methods based on invariance.</p>
<h4 id="causal-reinforcement-learning">Causal Reinforcement Learning</h4>
<p>Reinforcement learning is the study of how an agent can learn to choose actions that maximize its future rewards in an interactive and uncertain environment. These agents rely on plenty of simulations (and sometimes real data) to learn which actions lead to high reward in a particular context. Causality is also about calculating the effect of actions, and allows us to transfer knowledge to new, unfamiliar situations. These two disciplines have evolved independently with little interaction between them until recently. Integrating them is likely to be a fruitful area of research, and may extend the reach of both causality and reinforcement learning.<sup><a href="#fn35" id="fnref35">[35]</a></sup></p>
<p>There is a natural mapping between the concept of intervention in causal inference and actions taken in reinforcement learning. Throughout an episode of reinforcement learning (an episode is formed of one run of the system, for example, a complete game of chess, or go), an agent takes actions. This defines a data generating process for the reward that the agent ultimately cares about; different sequences of actions will generate different rewards. Since the agent can choose its actions, each of them is an intervention in this data generating process. In making this connection, we can leverage the mathematics of causal inference. For instance, we could use counterfactuals, the third level of the <a href="#the-ladder-of-causation">The ladder of causation</a>, to reason about actions not taken. Applying such causal techniques may reduce the state space the agent needs to consider, or help account for confounders.</p>
<p>Methods based on invariance, like IRM, in principle, learn to discover unknown invariances from multiple environments. We could leverage this attribute in reinforcement learning. An episode of RL consists of all the states that fall in between an initial state and a terminal state. Since each episode is independent of another, in IRM terminology they could be viewed as different environments. An agent could then learn robust policies from each of these episodes that leverage the invariant part of behaviour or actions that lead to reward.</p>
<p>While reinforcement learning itself is still in nascent stages when it comes to commercial applications, combining it with causality offers great potential.<sup><a href="#fn36" id="fnref36">[36]</a></sup> But prior to that, we need to address some questions. For example, how do we combine programming abstractions in causal modeling with reinforcement learning to help find the best decisions? What tools and libraries are necessary to enable commercial applications in this space?</p>
<h4 id="irm-and-environments">IRM and environments</h4>
<p>IRM uses the idea of training in multiple environments to achieve out-of-distribution generalization. Unfortunately, few datasets come with existing environment annotations. There are at least two ways we can try to address this problem.</p>
<p>The first is to be mindful of the environment when collecting data, and collect metadata alongside it. This may be easy (for example, collecting the geo-location of photos in settings where this is possible and does not violate a user’s privacy), or extremely hard (requiring much post-collection manual labeling).</p>
<p>Another compelling but untested option is to try combining IRM with some sort of clustering to segment a single dataset into environments.<sup><a href="#fn37" id="fnref37">[37]</a></sup> The question would be how to cluster in such a way that meaningful and diverse environments are defined. Since existing clustering approaches are purely correlative, and - as such - vulnerable to spurious correlations, this could prove challenging.</p>
<p>Studying the impact of environment selection, and how to create or curate datasets with multiple environments would be a valuable contribution to making invariance-based methods more widely applicable. (The authors of <a href="https://deepai.org/publication/an-empirical-study-of-invariant-risk-minimization">An Empirical Study of Invariant Risk Minimization</a> reach the same conclusion.)</p>
<h4 id="causal-reasoning-for-algorithmic-fairness">Causal reasoning for algorithmic fairness</h4>
<p>In the <a href="#ethics">Ethics</a> chapter, we reviewed some notions of fairness in prediction problems and shared how tools of causal reasoning can be leveraged to address fairness. They depart in the traditional way of wholly relying on data-driven approaches and emphasize the need to require additional knowledge of the structure of the world, in the form of a causal model. This additional knowledge is particularly valuable, as it informs us how changes in variables propagate in a system (be it natural, engineered, or social). Explicit causal assumptions remove ambiguity from methods that just depend upon statistical correlations. Avoiding discrimination through causal reasoning is an active area of research. As efforts to aid more transparency and fairness in machine learning systems grow, causal reasoning will continue to gain significant momentum in guiding algorithms towards fairness.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Structural causal models give us a framework for thinking precisely about cause and effect, and encoding our assumptions about data generating processes. Knowing the complete model for a system is immensely powerful, allowing us to reason about how the system will behave when we intervene in the data generating process, and correct for selection biases.</p>
<p>In machine learning, we’re often concerned only with prediction, for which we do not need causal inference. However, even in this scenario, taking a causal approach brings some benefits. Notably, causal relationships are invariant (they do not change between environments) and when we learn predictors based on them, we get greatly improved out-of-distribution generalization.</p>
<p>For many problems, constructing a causal graph is prohibitively hard, and always relies on assumptions. When working with only observational data, these assumptions are especially important, since they cannot be validated through experiments. Fortunately, by relying on the correspondence between causal relationships and invariance, we can still construct the relevant part of the causal graph for some problems using <a href="#invariant-causal-prediction">Invariant Causal Prediction</a>. For high dimensional inputs like image and text, we can use <a href="#invariant-risk-minimization">Invariant Risk Minimization</a> to learn a predictor that greatly enhances our out-of-distribution performance by learning not to rely on dataset-specifc spurious correlations.</p>
<p>Research at the intersection of causality and machine learning is blossoming, with many major ML conferences hosting dedicated workshops on the topic. Invariance-based approaches are an especially promising development and are ripe for industrial application. As algorithmic systems become increasingly prevalent, and their influence on decisions grows, the need for causal reasoning becomes all the more acute. We think it is important that practitioners have an understanding of causality, and hope to see causal approaches gain significant traction in mainstream data science practice. We hope this report has sparked some causal curiosity in you!</p>
<hr>
<section>
<ol>
<li id="fn1"><p>See for instance, recent works by Yoshua Bengio, like <a href="https://arxiv.org/abs/1901.10912">A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms</a>. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>See, for instance, <a href="https://eng.uber.com/causal-inference-at-uber/">Using Causal Inference to Improve the Uber User Experience</a>. <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>Facebook performed <a href="https://www.pnas.org/content/111/24/8788">such an experiment</a> in 2012, and received <a href="https://www.theatlantic.com/technology/archive/2014/06/everything-we-know-about-facebooks-secret-mood-manipulation-experiment/373648/">much criticism</a> as a result. The ethical problem is not so much with the experiment itself, but rather that the subjects had not given informed consent, in violation of basic ethical guidelines for psychological research. <a href="#fnref3">↩︎</a></p>
</li>
<li id="fn4"><p>An alternative popular framework is the Neyman-Reuben causal model, also known as <a href="https://www.cambridge.org/core/books/causal-inference-for-statistics-social-and-biomedical-sciences/71126BE90C58F1A431FE9B2DD07938AB">Potential Outcomes</a>. The frameworks are equivalent in that they can compute the same things, though some causal queries may be easier to reason about in one or the other. <a href="#fnref4">↩︎</a></p>
</li>
<li id="fn5"><p>See also Pearl’s article: <a href="https://cacm.acm.org/magazines/2019/3/234929-the-seven-tools-of-causal-inference-with-reflections-on-machine-learning/fulltext">The Seven Tools of Causal Inference, with Reflections on Machine Learning</a>. <a href="#fnref5">↩︎</a></p>
</li>
<li id="fn6"><p>Some farm-experienced members of the CFF team are keen to point out that roosters crow pretty much <em>all the time</em>. <a href="#fnref6">↩︎</a></p>
</li>
<li id="fn7"><p>See this article in <a href="https://www.forbes.com/sites/erikaandersen/2012/03/23/true-fact-the-lack-of-pirates-is-causing-global-warming/#5cb710453a67">Forbes</a>. <a href="#fnref7">↩︎</a></p>
</li>
<li id="fn8"><p>On a technical note, correlation measures only <em>linear</em> association. For instance, <code>x</code> squared is uncorrelated with <code>x</code>, despite being completely dependent on it. When we say “correlation is not causation,” we really mean “statistical dependence is not causation.” <a href="#fnref8">↩︎</a></p>
</li>
<li id="fn9"><p>Alas, it requires a far more detailed technical knowledge than we can provide in this report. We recommend the textbook <a href="http://bayes.cs.ucla.edu/PRIMER/">Causal Inference in Statistics: A Primer</a> for a succinct introduction to Structural Causal Models. An abbreviated overview, (<a href="https://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf">Causal Inference in Statistics: An Overview</a>) is freely available as a PDF. The textbook <a href="https://mitpress.mit.edu/books/elements-causal-inference">Elements of Causal Inference</a> (available through Open Access) also covers structural causal models, and includes several chapters explicitly drawing connections between causal inference and machine learning. <a href="#fnref9">↩︎</a></p>
</li>
<li id="fn10"><p>We will examine the nuances of this statement in <a href="#causality-and-invariance">Causality and invariance</a>. Correlation is predictive <em>in distribution</em>. <a href="#fnref10">↩︎</a></p>
</li>
<li id="fn11"><p>Adam Kelleher and Amit Sharma have an excellent <a href="https://medium.com/@akelleh/introducing-the-do-sampler-for-causal-inference-a3296ea9e78d">blog post</a> describing this problem, and introducing a new causal sampling technology to make solving it easier. <a href="#fnref11">↩︎</a></p>
</li>
<li id="fn12"><p>See <a href="http://jmlr.org/papers/v17/14-518.html">Distinguishing cause from effect using observational data: methods and benchmarks</a>. <a href="#fnref12">↩︎</a></p>
</li>
<li id="fn13"><p>See <a href="https://www.aaai.org/Library/AAAI/2005/aaai05-112.php">Robust Supervised Learning</a>. <a href="#fnref13">↩︎</a></p>
</li>
<li id="fn14"><p>The scientific process of iterated hypothesis and experimentation can also be applied to constructing a causal model for business purposes. The popular George Edward Box quote is pertinent here: “all models are wrong, but some are useful” (see <a href="https://en.wikipedia.org/wiki/All_models_are_wrong">All models are wrong</a>). <a href="#fnref14">↩︎</a></p>
</li>
<li id="fn15"><p>Judea Pearl’s do-calculus is a set of rules to calculate exactly which variables we must account for - and how - to answer a given causal query in a potentially complicated graph. This is not trivial; often there are unobserved variables in a graph, and we must try to express the query only in terms of those variables for which we have data. <a href="#fnref15">↩︎</a></p>
</li>
<li id="fn16"><p>There is a subtlety here. We said environments were defined by interventions. Naturally, it is impossible to intervene on the country a store is built in once the store is built. This turns out not to matter for the purposes of inferring the direct causal parents of the sales volume, so long as the country is further up the graph, and changing country alters the data generating process. <a href="#fnref16">↩︎</a></p>
</li>
<li id="fn17"><p>(See <a href="https://arxiv.org/abs/1706.08576">Invariant Causal Prediction for Nonlinear Models</a>. <a href="#fnref17">↩︎</a></p>
</li>
<li id="fn18"><p>Nonparametric conditional independence testing is an area of active research, and is generally hard - and made more so by having finite data. The nonlinear ICP paper also introduces the notion of defining sets; sometimes no single set of variables is accepted as the set of causal parents, but there are similar sets differing by only one or two variables that may be related. While the algorithm has failed to find a single consistent model, it is nonetheless conveying useful causal information. <a href="#fnref18">↩︎</a></p>
</li>
<li id="fn19"><p>Longer than you may think! See, for instance, <a href="https://dspace.mit.edu/handle/1721.1/11589">Machine perception of three-dimensional solids</a>, published in 1963. <a href="#fnref19">↩︎</a></p>
</li>
<li id="fn20"><p>The final section of the IRM paper includes a charming socratic dialogue that discusses this distinction, as well as the reason that regular supervised learning is so successful, from an invariance standpoint. <a href="#fnref20">↩︎</a></p>
</li>
<li id="fn21"><p>See <a href="https://arxiv.org/abs/1206.6471">On Causal and Anticausal Learning</a> for a description of the insight considering the causal direction of a problem brings to machine learning. <a href="#fnref21">↩︎</a></p>
</li>
<li id="fn22"><p>Technically, loss is the error on the training set, and risk is the error across the whole data distribution. With finite training data, minimizing the loss on the training set is a proxy for minimizing the risk. <a href="#fnref22">↩︎</a></p>
</li>
<li id="fn23"><p>Imperfect interpretability results notwithstanding, using ResNet as a feature extractor is representative of how CV systems are used in the real world, and the resulting out-of-distribution performance improvements are impressive. <a href="#fnref23">↩︎</a></p>
</li>
<li id="fn24"><p>This example is given in <a href="https://arxiv.org/abs/1812.11806">An introduction to domain adaptation and transfer learning</a>, and an empirical study using transfer learning was reported in <a href="https://ieeexplore.ieee.org/document/6945865">Transfer Learning Improves Supervised Image Segmentation Across Imaging Protocols</a> <a href="#fnref24">↩︎</a></p>
</li>
<li id="fn25"><p><a href="http://rpg.ifi.uzh.ch/docs/RAL16_Giusti.pdf">A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots</a> <a href="#fnref25">↩︎</a></p>
</li>
<li id="fn26"><p><a href="https://ieeexplore.ieee.org/document/8444585">Scaling Human Activity Recognition via Deep Learning-based Domain Adaptation</a> outlines the problem and some applications in this space. <a href="#fnref26">↩︎</a></p>
</li>
<li id="fn27"><p>See <a href="http://www.its.caltech.edu/~fehardt/UAI2016WS/papers/Liang.pdf">Causal Inference for Recommendation</a> and <a href="https://arxiv.org/abs/1808.06581">The Deconfounded Recommender: A Causal Inference Approach to Recommendation</a>. <a href="#fnref27">↩︎</a></p>
</li>
<li id="fn28"><p><a href="https://arxiv.org/abs/1501.01332">Causal inference using invariant prediction: identification and confidence intervals</a> and <a href="https://arxiv.org/abs/1706.08576">Invariant Causal Prediction for Nonlinear Models</a>. <a href="#fnref28">↩︎</a></p>
</li>
<li id="fn29"><p>See <a href="https://arxiv.org/abs/1609.05807">Inherent Trade-Offs in the Fair Determination of Risk Scores</a>. <a href="#fnref29">↩︎</a></p>
</li>
<li id="fn30"><p>See <a href="https://arxiv.org/abs/1805.05859">Causal Reasoning for Algorithmic Fairness</a> and <a href="https://arxiv.org/abs/1706.02744">Avoiding Discrimination through Causal Reasoning</a>. <a href="#fnref30">↩︎</a></p>
</li>
<li id="fn31"><p>See <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16949">Fairness in Decision-Making – The Causal Explanation Formula</a>. <a href="#fnref31">↩︎</a></p>
</li>
<li id="fn32"><p><a href="https://arxiv.org/abs/1505.07818">Domain adversarial training of neural networks</a> <a href="#fnref32">↩︎</a></p>
</li>
<li id="fn33"><p><a href="https://arxiv.org/abs/1703.03400">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a> <a href="#fnref33">↩︎</a></p>
</li>
<li id="fn34"><p>See <a href="https://arxiv.org/abs/1901.10912">A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms</a>. <a href="#fnref34">↩︎</a></p>
</li>
<li id="fn35"><p>There is a nice introduction to causal reinforcement learning in the paper <a href="http://gershmanlab.webfactional.com/pubs/RL_causal.pdf">Reinforcement learning and causal models</a>. The blog post <a href="https://causallu.com/2018/12/31/introduction-to-causalrl/">Introduction to Causal RL</a> contains a shorter description, and also suggests some medical applications. <a href="#fnref35">↩︎</a></p>
</li>
<li id="fn36"><p>We are grateful to David Lopez-Paz (one of the <a href="https://arxiv.org/abs/1907.02893">Invariant Risk Minimization</a> authors) for sharing his thoughts and ideas about possible extensions and applications of IRM with us, including applications to reinforcement learning. <a href="#fnref36">↩︎</a></p>
</li>
<li id="fn37"><p>This idea was also suggested to us by David Lopez-Paz. <a href="#fnref37">↩︎</a></p>
</li>
</ol>
</section>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Unity like game editor running in pure WASM (564 pts)]]></title>
            <link>https://raverie-us.github.io/raverie-engine/</link>
            <guid>37663270</guid>
            <pubDate>Tue, 26 Sep 2023 17:58:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://raverie-us.github.io/raverie-engine/">https://raverie-us.github.io/raverie-engine/</a>, See on <a href="https://news.ycombinator.com/item?id=37663270">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[ROCm is AMD's priority, executive says (269 pts)]]></title>
            <link>https://www.eetimes.com/rocm-is-amds-no-1-priority-exec-says/</link>
            <guid>37663194</guid>
            <pubDate>Tue, 26 Sep 2023 17:54:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eetimes.com/rocm-is-amds-no-1-priority-exec-says/">https://www.eetimes.com/rocm-is-amds-no-1-priority-exec-says/</a>, See on <a href="https://news.ycombinator.com/item?id=37663194">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

					<!--?//php echo do_shortcode('[responsivevoice_button voice="US English Male" buttontext="Listen to Post"]') ?-->
					<!-- EET_Top_Leaderboard -->

<p>SANTA CLARA, CALIF. — “If you think about the product portfolio that AMD has, it’s arguably the broadest in the industry in terms of AI compute,” Vamsi Boppana, senior VP of the AI group at AMD, said in his keynote address at the recent AI Hardware Summit. AMD’s hardware portfolio includes data-center–class CPUs and GPUs, consumer GPUs, FPGAs and the Ryzen 7040, a client CPU with NPU designed for PCs. Software is key to unlocking the performance of these different hardware platforms. But how does AMD compete with its GPU competitors’ strong offerings, given its more diverse hardware?</p>
<p>AMD’s software stacks for each class of product are separate: ROCm (short for Radeon Open Compute platform) targets its Instinct data center GPU lines (and, soon, its Radeon consumer GPUs), Vitis AI targets its FPGAs, and ZenDNN targets its client devices.</p>
<p>How far along is AMD with unifying these stacks?</p>


<p>“We have enormous customer pull coming, and that is dictating quite a bit of our near-term plans,” Boppana told EE Times in an interview after his talk here. “The plane is flying right now, so we cannot disassemble the engine. However, we are absolutely doing things at the foundational level to make more unification happen in our stack.”</p>
		
		<div>
		<div>
				<div>
					<figure>
					<a href="https://www.eetimes.com/briocean-technology-shines-bright-as-it-ranks-8th-on-the-top-asia-pacific-distributor-list/" title="Briocean Technology Shines Bright as It Ranks 8th on the Top Asia Pacific Distributor List&nbsp;"><img data-lazy-fallback="1" loading="lazy" width="62" src="https://www.eetimes.com/wp-content/uploads/Briocean-thumbnail-image.jpg?w=62" alt="Briocean Technology Shines Bright as It Ranks 8th on the Top Asia Pacific Distributor List&nbsp;" data-lazy-src="https://www.eetimes.com/wp-content/uploads/Briocean-thumbnail-image.jpg?w=62&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a>
					</figure>
					
				</div><p>By Briocean Technology&nbsp; 09.26.2023</p>
			</div>
		<div>
				<div>
					<figure>
					<a href="https://www.eetimes.com/unlock-the-benefits-of-endpoint-ai-solutions-for-laptop-computers/" title="Unlock the Benefits of Endpoint AI Solutions for Laptop Computers&nbsp;"><img data-lazy-fallback="1" loading="lazy" width="62" src="https://www.eetimes.com/wp-content/uploads/EE-Times_600x340_.jpg?w=62" alt="Unlock the Benefits of Endpoint AI Solutions for Laptop Computers&nbsp;" data-lazy-src="https://www.eetimes.com/wp-content/uploads/EE-Times_600x340_.jpg?w=62&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a>
					</figure>
					
				</div><p>By Himax Technologies. Ltd.&nbsp; 09.26.2023</p>
			</div>
		<div>
				<div>
					<figure>
					<a href="https://www.eetimes.com/1409388-2/" title="SK hynix’s Evolution in CIS HDR Technology and Future Outlook"><img data-lazy-fallback="1" loading="lazy" width="62" src="https://www.eetimes.com/wp-content/uploads/SK-hynix_CIS-HDR-Technology_Thumbnail.png?w=62" alt="SK hynix’s Evolution in CIS HDR Technology and Future Outlook" data-lazy-src="https://www.eetimes.com/wp-content/uploads/SK-hynix_CIS-HDR-Technology_Thumbnail.png?w=62&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a>
					</figure>
					
				</div><p>By Suram Cha, Technical Leader of Next Gen Biz Team, SK hynix&nbsp; 09.26.2023</p>
			</div></div>
<figure id="attachment_1409408" aria-describedby="caption-attachment-1409408"><a href="https://www.eetimes.com/wp-content/uploads/Kisaco_day2-4767.jpg"><img data-lazy-fallback="1" decoding="async" fetchpriority="high" src="https://www.eetimes.com/wp-content/uploads/Kisaco_day2-4767.jpg?w=640&amp;resize=640%2C427" alt="" width="640" height="427" data-recalc-dims="1" data-lazy-src="https://www.eetimes.com/wp-content/uploads/Kisaco_day2-4767.jpg?w=640&amp;is-pending-load=1#038;resize=640%2C427" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption id="caption-attachment-1409408">AMD’s Vamsi Boppana gives a keynote address at the recent AI Hardware Summit in Santa Clara, Calif. (Source: Kisaco Research &amp; Jeffrey Hosier Photography)</figcaption></figure>
<p>Boppana said that there’s some common infrastructure and tooling underlying all three stacks, including an ongoing effort to make a common quantizer.</p>

<p>“Over time, we want to get to a place where users have one execution provider, and underneath that, you will be able to select [a hardware target],” he said. “In the near term, modules are shared across stacks, and over time, as things like heterogeneous platforms are going to become prevalent, the unified elements start coming through.”</p>
<p>A unified stack would be helpful for heterogeneous systems, Boppana said, especially where partitioning is required. Currently, the Vitis stack handles CPU plus xDNA targets, but he agrees that both automatic and user-driven partitioning will be necessary.</p>
<p>“In that scenario, we need to be able to take a problem statement and cut the graph, such that both parts of the graph get executed on [different parts of the hardware], and they need to inter-operate,” he said.</p>
<h3><strong>‘ROCm has evolved’</strong></h3>
<p>ROCm is less mature than competitors’ GPU software offerings, with <a href="https://www.eetimes.com/nvidia-brings-gpu-acceleration-to-computational-lithography/" target="_blank" rel="noopener">Nvidia</a>’s mature CUDA stack often seen as a big part of the market leader’s competitive advantage.</p>
<p>“Software is a journey,” Boppana said. “Anybody who has written or managed complex pieces of software knows it takes time. The good news is, we have been on the journey…ROCm has evolved.”</p>
<p>AMD has made ROCm the No. 1 priority at the company level in the last year, Boppana said, standing up a new organization that’s brought together assets from all the company’s software contributions.</p>
<p>“We have much larger resources actually working on software, and [AMD CEO Lisa Su] has been very clear that she wants to see significant and continued investments on the software side,” Boppana said. “We have agreed to provide people internally, we have acquired Mipsology, and we are looking to grow talent both organically and inorganically.”</p>
<p>AMD also recently stood up an internal <a href="https://www.eetimes.com/podcasts/demystifying-ai-how-neural-networks-like-transformers-really-work/" target="_blank" rel="noopener">AI models</a> group to increase its experience using its own software stack.</p>
<p>“We want a much tighter feedback loop,” Boppana said.</p>
<h3><strong>Using open source to challenge Nvidia</strong></h3>
<p>AMD has embraced OpenAI’s Triton, an open-source programming language and compiler for GPUs that promises to offer an open-source alternative to Nvidia’s CUDA for developers who want to write high-level code that performs optimally on the hardware.</p>
<p>“There are different personas that are programming [our GPUs],” he said. “[Triton] is a level of abstraction that people are comfortable with. It’s productive. And it gets to hardware in a pretty efficient, cogent fashion. But for other customers, that doesn’t matter; they don’t need to develop new kernels. For them, we can ship libraries. So, it’s just a matter of who wants to use us.”</p>
<p>In contrast to Nvidia’s approach with CUDA, which is mostly proprietary, most of AMD’s ROCm stack is open source.</p>
<p>“We partner with the [AI frameworks] and the people writing the libraries and say, ‘If you have a kernel you want to put together, you can take something that exists from us, but if you find there’s the opportunity for you to optimize source code, [you can]’,” he said. “Then we have so many more people that are willing and able to contribute. So, that’s very important and very powerful for us: We think it’s the right strategic direction for us to take.”</p>
<figure id="attachment_1409409" aria-describedby="caption-attachment-1409409"><a href="https://www.eetimes.com/wp-content/uploads/AMD-Hardware-architectures.jpg"><img data-lazy-fallback="1" decoding="async" src="https://www.eetimes.com/wp-content/uploads/AMD-Hardware-architectures.jpg?w=640&amp;resize=640%2C269" alt="" width="640" height="269" data-recalc-dims="1" data-lazy-src="https://www.eetimes.com/wp-content/uploads/AMD-Hardware-architectures.jpg?w=640&amp;is-pending-load=1#038;resize=640%2C269" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption id="caption-attachment-1409409">AMD has a diverse portfolio of hardware architectures for AI acceleration. But how does the company manage its AI software stack for such diverse architectures? (Source: AMD)</figcaption></figure>
<p>MI300 samples are currently with customers, Boppana said, and both customers and AMD have AI training workloads up and running, with availability coming at the end of this year.</p>
<p>ROCm will be crucial to the success of both the MI300 and <a href="https://www.eetimes.com/can-amds-mi300x-take-on-nvidias-h100/" target="_blank" rel="noopener">MI300X</a>.</p>
<p>“Being candid, we have a few places to grow,” he said. “Allowing the community to contribute [to ROCm] alongside us helps us bridge the gap faster.”</p>
				</div><div>
					<div>
						<p><img width="173" height="164" src="https://www.eetimes.com/wp-content/uploads/Sally-Ward_Foxton_low-res-6-e1678729120589.jpg?fit=173%2C164&amp;is-pending-load=1" alt="" decoding="async" data-lazy-src="https://www.eetimes.com/wp-content/uploads/Sally-Ward_Foxton_low-res-6-e1678729120589.jpg?fit=173%2C164&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">						</p>
					</div>
											<div>
							<p>Sally Ward-Foxton</p>

							<p><em>Sally Ward-Foxton covers AI for EETimes.com and EETimes Europe magazine. Sally has spent the last 18 years writing about the electronics industry from London. She has written for Electronic Design, ECN, Electronic Specifier: Design, Components in Electronics, and many more news publications. She holds a Masters' degree in Electrical and Electronic Engineering from the University of Cambridge.   <br></em></p>
						</div>
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sidechannel pixel-stealing attack works in Chromium on all modern GPUs (186 pts)]]></title>
            <link>https://arstechnica.com/security/2023/09/gpus-from-all-major-suppliers-are-vulnerable-to-new-pixel-stealing-attack/</link>
            <guid>37663159</guid>
            <pubDate>Tue, 26 Sep 2023 17:52:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/security/2023/09/gpus-from-all-major-suppliers-are-vulnerable-to-new-pixel-stealing-attack/">https://arstechnica.com/security/2023/09/gpus-from-all-major-suppliers-are-vulnerable-to-new-pixel-stealing-attack/</a>, See on <a href="https://news.ycombinator.com/item?id=37663159">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      SAME ORIGIN POLICY SHATTERED    —
</h4>
            
            <h2 itemprop="description">A previously unknown compression side channel in GPUs can expose images thought to be private.</h2>
                    </header>
        <section>
            <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/09/pixels-800x540.jpg" alt="GPUs from all major suppliers are vulnerable to new pixel-stealing attack">
      <figcaption></figcaption>  </figure>

  




<!-- cache hit 16:single/related:697e3f1e00d323130cb53bbec0736ce7 --><!-- empty -->
<p>GPUs from all six of the major suppliers are vulnerable to a newly discovered attack that allows malicious websites to read the usernames, passwords, and other sensitive visual data displayed by other websites, researchers have demonstrated in a paper published Tuesday.</p>
<p>The cross-origin attack allows a malicious website from one domain—say, example.com—to effectively read the pixels displayed by a website from example.org, or another different domain. Attackers can then reconstruct them in a way that allows them to view the words or images displayed by the latter site. This leakage violates a critical security principle that forms one of the most fundamental security boundaries safeguarding the Internet. Known as the <a href="https://developer.mozilla.org/en-US/docs/Web/Security/Same-origin_policy">same origin policy</a>, it mandates that content hosted on one website domain be isolated from all other website domains.</p>
<h2>Optimizing bandwidth at a cost</h2>
<p>GPU.zip, as the proof-of-concept attack has been named, starts with a malicious website that places a link to the webpage it wants to read inside of an <a href="https://www.hostinger.com/tutorials/what-is-iframe/">iframe</a>, a common HTML element that allows sites to embed ads, images, or other content hosted on other websites. Normally, the same origin policy prevents either site from inspecting the source code, content, or final visual product of the other. The researchers found that data compression that both internal and discrete GPUs use to improve performance acts as a <a href="https://en.wikipedia.org/wiki/Side-channel_attack">side channel</a> that they can abuse to bypass the restriction and steal pixels one by one.</p>                                            
                                                        
<p>“We found that modern GPUs automatically try to compress this visual data, without any application involvement,” Yingchen Wang, the lead author and a researcher at the University of Texas at Austin, wrote in an email. “This is done to save memory bandwidth and improve performance. Since compressibility is data dependent, this optimization creates a side channel which can be exploited by an attacker to reveal information about the visual data.”</p>
<p>For GPU.zip to work, a malicious page must be loaded into the Chrome or Edge browsers. Under-the-hood differences in the way Firefox and Safari work prevent the attack from succeeding when those browsers process an attack page. Another requirement is that the page linked to in the iframe must not be configured to deny being embedded by cross-origin websites.</p>
<p>The security threats that can result when HTML is embedded in iframes on malicious websites have been well-known for more than a decade. Most websites <a href="https://web.dev/security-headers/">restrict the cross-origin embedding</a> of pages displaying user names, passwords, or other sensitive content through X-Frame-Options or Content-Security-Policy headers. Not all, however, do. One example is Wikipedia, which shows the usernames of people who log in to their accounts. A person who wants to remain anonymous while visiting a site they don’t trust could be outed if it contained an iframe containing a link to <code>https://en.wikipedia.org/wiki/Main_Page</code>.</p>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/09/gpu.zip-pixel-theft.jpg" data-height="607" data-width="1554" alt="Pixel stealing PoC for deanonymizing a user, run with other tabs open playing video. “Ground Truth” is the victim iframe (Wikipedia logged in as “Yingchenw”). “AMD” is the attack result on a Ryzen 7 4800U after 30 minutes, with 97 percent accuracy. “Intel” is the attack result for an i7-8700 after 215 minutes with 98 percent accuracy."><img alt="Pixel stealing PoC for deanonymizing a user, run with other tabs open playing video. “Ground Truth” is the victim iframe (Wikipedia logged in as “Yingchenw”). “AMD” is the attack result on a Ryzen 7 4800U after 30 minutes, with 97 percent accuracy. “Intel” is the attack result for an i7-8700 after 215 minutes with 98 percent accuracy." src="https://cdn.arstechnica.net/wp-content/uploads/2023/09/gpu.zip-pixel-theft-640x250.jpg" width="640" height="250" srcset="https://cdn.arstechnica.net/wp-content/uploads/2023/09/gpu.zip-pixel-theft-1280x500.jpg 2x"></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/09/gpu.zip-pixel-theft.jpg" data-height="607" data-width="1554">Enlarge</a> <span>/</span> Pixel stealing PoC for deanonymizing a user, run with other tabs open playing video. “Ground Truth” is the victim iframe (Wikipedia logged in as “Yingchenw”). “AMD” is the attack result on a Ryzen 7 4800U after 30 minutes, with 97 percent accuracy. “Intel” is the attack result for an i7-8700 after 215 minutes with 98 percent accuracy.</p><p>Wang et al.</p></figcaption></figure>
<p>The researchers showed how GPU.zip allows a malicious website they created for their PoC to steal pixels one by one for a user’s Wikipedia username. The attack works on GPUs provided by Apple, Intel, AMD, Qualcomm, Arm, and Nvidia. On AMD’s Ryzen 7 4800U, GPU.zip took about 30 minutes to render the targeted pixels with 97 percent accuracy. The attack required 215 minutes to reconstruct the pixels when displayed on a system running an Intel i7-8700.</p>                                            
                                                        
<p>All of the GPUs analyzed use proprietary forms of compression to optimize the bandwidth available in the memory data bus of the PC, phone, or other device displaying the targeted content. The compression schemes differ from manufacturer to manufacturer and are undocumented, so the researchers reverse-engineered each one. The insights yielded a method that uses the SVG, or the scalable vector graphics image format, to maximize differences in DRAM traffic between black and white target pixels in the presence of compression. While their paper discusses GPU.zip as it applies to iGPUs, or internal GPUs, the technique applies equally to standalone or discrete GPUs as well.</p>
<p>In their paper, the researchers wrote:</p>
<blockquote><p>We demonstrate that an attacker can exploit the iGPU-based compression channel to perform cross-origin pixel stealing attacks in the browser using SVG filters (the latest version of Google Chrome as of April 2023), even though SVG filters are implemented at constant time. The reason is that the attacker can create highly redundant or highly non-redundant patterns depending on a single secret pixel in the browser. As these patterns are processed by the iGPU, their varying degrees of redundancy cause the lossless compression output to depend on the secret pixel. The data-dependent compression output directly translates to data-dependent DRAM traffic and data-dependent cache occupancy. Consequently, we show that, even under the most passive threat model—where an attacker can only observe coarse-grained redundancy information of a pattern using a coarse-grained timer in the browser and lacks the ability to adaptively select input—individual pixels can be leaked. Our proof-of-concept attack succeeds on a range of devices (including computers, phones) from a variety of hardware vendors with distinct GPU architectures (Intel, AMD, Apple, Nvidia). Surprisingly, our attack also succeeds on discrete GPUs, and we have preliminary results indicating the presence of software-transparent compression on those architectures as well.</p></blockquote>

                                                </div>

            
                            <nav>Page: <span>1 <a href="https://arstechnica.com/security/2023/09/gpus-from-all-major-suppliers-are-vulnerable-to-new-pixel-stealing-attack/2/">2</a> <a href="https://arstechnica.com/security/2023/09/gpus-from-all-major-suppliers-are-vulnerable-to-new-pixel-stealing-attack/2/"><span>Next <span>→</span></span></a></span></nav>
            
        </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Tips for Solopreneur? (188 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37662937</link>
            <guid>37662937</guid>
            <pubDate>Tue, 26 Sep 2023 17:39:51 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37662937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="37662937">
      <td><span></span></td>      <td><center><a id="up_37662937" href="https://news.ycombinator.com/vote?id=37662937&amp;how=up&amp;goto=item%3Fid%3D37662937"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=37662937">Ask HN: Tips for Solopreneur?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_37662937">106 points</span> by <a href="https://news.ycombinator.com/user?id=solo_prono">solo_prono</a> <span title="2023-09-26T17:39:51"><a href="https://news.ycombinator.com/item?id=37662937">3 hours ago</a></span> <span id="unv_37662937"></span> | <a href="https://news.ycombinator.com/hide?id=37662937&amp;goto=item%3Fid%3D37662937">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Tips%20for%20Solopreneur%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=37662937&amp;auth=a4c646cbd0cbd88c190eb0bbc2f21002eeaf249c">favorite</a> | <a href="https://news.ycombinator.com/item?id=37662937">52&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>Yo HN! I have been working on some design tools in my spare time to solve problems I've faced over and over, and I'm thinking about monetizing them.</p><p>I've been to some conferences recently and talked to a lot of people who have these problems as well, and they're keen to try it out. I have collected some emails, been communicating with them a bit and even got beers with one of them recently!</p><p>Here's my list of concerns:</p><p>1. It is just me - is that a red flag? Some people have asked me about my team and I told them it was just me. I got the feeling that it may have turned them off because the conversation kind of ended right there. To be fair, after that I did say that it is just me right now BUTTTTTTTT why that is okay due to my experience and work history. However, yes it is my first time doing a business.</p><p>2. How do I set appropriate milestones for me to reach? Do I think about reaching 100 customers before reaching 5 recurring customers for example?</p><p>3. I'm in a small town in PNW. Does that matter if this will be an online thing anyway? Why or when do people move to big cities like Seattle/SF/NYC/Austin etc.</p><p>4. What are some ways to do marketing? Should I even think about that before I have a few customers who are using my product consistently?</p><p>5. I've been inspired by the Startup School videos. Honestly though I'm not sure about fundraising and all these things, it seems very intimidating to me. What's the difference between those things and starting a company and slowly building it up?</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[macOS 14 Sonoma firewall bug fixed (103 pts)]]></title>
            <link>https://mullvad.net/en/blog/2023/9/22/macos-14-sonoma-firewall-bug-fixed/</link>
            <guid>37662791</guid>
            <pubDate>Tue, 26 Sep 2023 17:29:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mullvad.net/en/blog/2023/9/22/macos-14-sonoma-firewall-bug-fixed/">https://mullvad.net/en/blog/2023/9/22/macos-14-sonoma-firewall-bug-fixed/</a>, See on <a href="https://news.ycombinator.com/item?id=37662791">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>The firewall bug in macOS 14 Sonoma betas and release candidates that we <a href="https://mullvad.net/blog/2023/9/13/bug-in-macos-14-sonoma-prevents-our-app-from-working/">blogged about last week</a> has been fixed by Apple.</p>

<p>Yesterday Apple released macOS 14 Sonoma Release Candidate 2 (23A344). This version no longer exhibits the invalid firewall rule evaluation that we observed in the earlier release candidate and betas (starting from beta 6). This also means that our VPN app now works fine in latest Sonoma.</p>

<h2>Why we were affected</h2>

<p>Our VPN app is what we call a privacy preserving VPN client. This means its main purpose is not just to establish a tunnel and make sure it works, but also to ensure there are no leaks and no ways to de-anonymize the user.</p>

<p>To uphold the privacy preserving aspect, we do not think it is enough to solely rely on the routing table or Apple’s content filter provider API for making sure traffic that is supposed to go in the VPN tunnel actually does. Because doing so leaves numerous potential leaks, for example <a href="https://mullvad.net/blog/2020/11/16/big-no-big-sur-mullvad-disallows-apple-apps-bypass-firewall/">this one that was introduced in Big Sur.</a> At Mullvad we believe in adding as many safety layers as possible. Denying unwanted traffic at the firewall layer is an obvious design choice for us.</p>

<p>The firewall bugs we saw could only be observed if the rules contained the quick option, meaning they terminate firewall rule evaluation early. Without quick, all network traffic will be evaluated by subsequent rules and anchors injected by Apple or other software on the computer. We see this as a potential risk. While it might be possible to write firewall rules for a VPN without quick, we want our rules to be as final as possible, for security.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Exploring Linux command-line space time (212 pts)]]></title>
            <link>https://fabiensanglard.net/st/index.html</link>
            <guid>37662655</guid>
            <pubDate>Tue, 26 Sep 2023 17:21:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fabiensanglard.net/st/index.html">https://fabiensanglard.net/st/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=37662655">Hacker News</a></p>
<div id="readability-page-1" class="page"><br><center>
    
</center><p>
Sep 26, 2023</p>
<p>Exploring Linux command-line space time</p><hr>

<p>I was curious to explore how long a program takes to run, how much memory is used over time, and what processes/threads are spawned. To provide answers I wrote a tool, which I named <code>st</code>. Here are a few things I looked into.</p>

<p>See details at the bottom if you are interested in how the tool works (and why I did not use <code><a href="https://man7.org/linux/man-pages/man1/time.1.html">time(1)</a></code> and <code>strace(1)</code>).</p>


<p>Fill</p><hr><p>An entertaining way to use <code>st</code> is so to predict the outcome of a command and explore the reasons for discrepancies. I started with a simple C program, <a href="https://fabiensanglard.net/st/fill.c">fill.c</a> allocating 1GiB and setting each byte individually.</p>

<!-- HTML generated using hilite.me -->
<pre><span>#include &lt;stdio.h&gt;</span>
<span>#include &lt;stdlib.h&gt;</span>
<span>#include &lt;stdint.h&gt;</span>

<span>void</span> <span>fill</span>(<span>uint8_t</span><span>*</span> addr, <span>size_t</span> amount, <span>char</span> value) {
    <span>for</span> (<span>size_t</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> amount; i<span>++</span>) {
        <span>*</span>(addr <span>+</span> i) <span>=</span> value;
    }
}

<span>int</span> <span>main</span>(<span>int</span> argc, <span>char</span> <span>**</span>argv) {
    <span>size_t</span> s <span>=</span> <span>1</span><span>&lt;&lt;</span><span>30</span>;
    <span>uint8_t</span><span>*</span> buffer <span>=</span> (<span>uint8_t</span><span>*</span>)malloc(s);
    fill(buffer, s, atoi(argv[<span>0</span>]));
    <span>return</span> EXIT_SUCCESS;
}
</pre>



<p>Let's build and run it via <code>st</code>.</p>

<pre><b>$</b> clang -o fill fill.c
<b>$</b> sudo st fill
<span><span>EXEC</span>:</span> [/home/leaf/fill]
Num threads = 1
Num process = 1
Max PSS: 1,073,762,304 bytes
Walltime: 1,229ms - user-space: 1,109ms - kernel-space: 87ms
</pre>

<pre>  1┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                               ██████┃
   ┃                                                                         ████████████┃
   ┃                                                                    █████████████████┃
   ┃                                                               ██████████████████████┃
   ┃                                                         ████████████████████████████┃
   ┃                                                    █████████████████████████████████┃
   ┃                                              ███████████████████████████████████████┃
   ┫                                         ████████████████████████████████████████████┃
   ┃                                   ██████████████████████████████████████████████████┃
   ┃                             ████████████████████████████████████████████████████████┃
   ┃                        █████████████████████████████████████████████████████████████┃
   ┃                  ███████████████████████████████████████████████████████████████████┃
   ┃            █████████████████████████████████████████████████████████████████████████┃
   ┃       ██████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0GB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0s                                                                                    1</pre>

<p>I expected 1GiB to be filled in 10ms but it took a whole second. The issue is that the program is generating many page faults. I remember Firefox sped up their startup time 2x by solving a similar issue<a name="back_1" href="#footnote_1"><sup>[1]</sup></a>.</p>

<p>Also noteworthy, the memory usage did not increase abruptly from 0 to 1GiB with <code>malloc</code>. Instead we see a staircase pattern which shows that physical RAM consumption increased as virtual pages were written to by the <code>fill</code> function.</p>


<p>fillfill</p><hr><p>Another test program, <a href="https://fabiensanglard.net/st/fillfill.c">fillfill.c</a>, to check the tool is properly tracking PSS, processes, and threads.</p>

<!-- HTML generated using hilite.me -->
<pre><span>#include &lt;stdio.h&gt;</span>
<span>#include &lt;stdlib.h&gt;</span>
<span>#include &lt;unistd.h&gt;</span>
<span>#include &lt;sys/wait.h&gt;</span>
<span>#include &lt;stdint.h&gt;</span>

<span>void</span> <span>malloc_and_fill</span>(<span>size_t</span> s) {
  <span>uint8_t</span><span>*</span> buffer <span>=</span> (<span>uint8_t</span><span>*</span>) malloc(s);
  <span>for</span> (<span>size_t</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> s; i<span>++</span>) {
    <span>*</span>(buffer<span>+</span> i) <span>=</span> <span>'F'</span>;
  }
  free(buffer);
}

<span>int</span> <span>main</span>(<span>int</span> argc, <span>char</span> <span>**</span>argv) {
  malloc_and_fill(<span>1L</span> <span>&lt;&lt;</span> <span>30</span>);
  <span>int</span> pid <span>=</span> fork();
  <span>if</span> (pid <span>!=</span> <span>0</span>) {
    malloc_and_fill(<span>1L</span> <span>&lt;&lt;</span> <span>31</span>);   
  } <span>else</span> {
    waitpid(pid, <span>NULL</span>, <span>0</span>);
  }
  <span>return</span> <span>0</span>;
}
</pre>


<p>First allocate 1GiB, free it, then spawn a process which does the same but with 2GiB.</p>

<pre><b>$</b> sudo st ./fillfill
<span>EXEC</span>: [./fillfill]
Num threads = 2
Num process = 2
Max PSS: 2,147,591,168 bytes
Walltime: 3,608ms - user-space: 3,277ms - kernel-space: 327ms</pre>

<pre>  2┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                                ███  ┃
   ┃                                                                            ███████  ┃
   ┃                                                                         ██████████  ┃
   ┃                                                                     ██████████████  ┃
   ┃                                                                 ███████████████████ ┃
   ┃                                                              ██████████████████████ ┃
   ┃                                                          ██████████████████████████ ┃
   ┫                          █                           ██████████████████████████████ ┃
   ┃                      ██████                       █████████████████████████████████ ┃
   ┃                   █████████                   █████████████████████████████████████ ┃
   ┃               █████████████               █████████████████████████████████████████ ┃
   ┃           █████████████████            ████████████████████████████████████████████ ┃
   ┃        ████████████████████        ████████████████████████████████████████████████ ┃
   ┃    ████████████████████████    ████████████████████████████████████████████████████ ┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0GB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0s                                                                                    3</pre>



<p>Fill -O3</p><hr><p>Let's get back to <a href="https://fabiensanglard.net/st/fill.c">fill.c</a> but this time compile it with full optimization (<code>-O3</code>).
  </p>

<pre><b>$</b> clang -o fillo <span>-O3</span> fill.c
<b>$</b> sudo st fillo
<span><span>EXEC</span>:</span>: [ /home/leaf/fillo]
Num threads = 1
Num process = 1
Max PSS: 164,864 bytes
Walltime: 5ms - user-space: 1ms - kernel-space: 0ms
</pre>
<pre>164┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┫█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0KB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                   0</pre>  

<p>Wow. From 1000ms to 5ms. And only using 164 KiB?! According to Binary Ninja<a name="back_2" href="#footnote_2"><sup>[2]</sup></a>, the compiler discarded both <code>malloc</code> and <code>fill</code>. It makes sense since the program did not read what was allocated and written. Flag <code>-O3</code> effectively turned <code><a href="https://fabiensanglard.net/st/fill.c">fill.c</a></code> into an empty <code>main</code> function.</p>

<pre><span>int</span> main(<span>int</span> argc, char <span>**</span>argv) {
    <span>return</span> EXIT_SUCCESS;
}
</pre>


<p><b><u>Sidenote:</u></b> Exploring compiler <code>-O3</code> outputs with Binary Ninja is a source of endless amazement. Loop to memset substitution (<a href="https://fabiensanglard.net/st/fill.c">fill.c</a>, <a href="https://fabiensanglard.net/st/fill.webp">cc</a>, <a href="https://fabiensanglard.net/st/fillo.webp">cc -O3</a>), <code>strlen</code> caching ...even without <code>-O3</code> (<a href="https://fabiensanglard.net/st/strlen.c">strlen.c</a>, <a href="https://cloud.binary.ninja/embed/44fb8393-c731-4cda-b412-5cee57989840">cc</a>, <a href="https://cloud.binary.ninja/embed/6c81b81f-6c3d-4ac2-8297-6c291d7667bb">cc -O3</a>), and <code>printf("%c", 'x')</code> to <code>putchar('x')</code>!) substitution are only a few among many cool optimizations.</p>




<p>clang helloworld.c</p><hr>
<p>I have written extensively about <a href="https://fabiensanglard.net/dc">compiler drivers</a> by the past so it was a good opportunity to double check that <code>clang</code> behaved as expected when compiling <a href="https://fabiensanglard.net/st/hello.c">hello.c</a>.</p>

<pre><span>#include &lt;stdio.h&gt;</span>
<span>int</span> <span>main</span>() {
   printf(<span>"Hello, World!"</span>);
   <span>return</span> <span>0</span>;
}
</pre>


<pre><b>$</b> sudo st clang -o hello hello.c
<span><span>EXEC</span>:</span>: [ clang -o hello hello.c]
<span><span>EXEC</span>:</span>: [/usr/lib/llvm-14/bin/clang -cc1 -triple aarch64-unknown-linux-gnu -emit-obj -mrelax-all --mrelax-relocations -disable-free -clear-ast-before-backend -disable-llvm-verifier -discard-value-names -main-file-name hello.c -mrelocation-model pic -pic-level 2 -pic-is-pie -mframe-pointer=non-leaf -fmath-errno -ffp-contract=on -fno-rounding-math -mconstructor-aliases -funwind-tables=2 -target-cpu generic -target-feature +neon -target-feature +v8a -target-abi aapcs -fallow-half-arguments-and-returns -mllvm -treat-scalable-fixed-error-as-warning -debugger-tuning=gdb -fcoverage-compilation-dir=/home/leaf -resource-dir /usr/lib/llvm-14/lib/clang/14.0.0 -internal-isystem /usr/lib/llvm-14/lib/clang/14.0.0/include -internal-isystem /usr/local/include -internal-isystem /usr/bin/../lib/gcc/aarch64-linux-gnu/11/../../../../aarch64-linux-gnu/include -internal-externc-isystem /usr/include/aarch64-linux-gnu -internal-externc-isystem /include -internal-externc-isystem /usr/include -fdebug-compilation-dir=/home/leaf]
<span><span>EXEC</span>:</span>: [/usr/bin/ld -pie -EL -z relro --hash-style=gnu --build-id --eh-frame-hdr -m aarch64linux -dynamic-linker /lib/ld-linux-aarch64.so.1 -o hello /lib/aarch64-linux-gnu/Scrt1.o /lib/aarch64-linux-gnu/crti.o /usr/bin/../lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/bin/../lib/gcc/aarch64-linux-gnu/11 -L/lib/aarch64-linux-gnu -L/usr/lib/aarch64-linux-gnu -L/usr/lib/llvm-14/bin/../lib -L/lib -L/usr/lib /tmp/hello-df1527.o -lgcc --as-needed -lgcc_s --no-as-needed -lc -lgcc --as-needed -lgcc_s --no-as-needed /usr/bin/../lib/gcc/aarch64-linux-gnu/11/crtendS.o /lib/aarch64-linux-gnu/crtn.o ]
Num threads = 3
Num process = 3
Max PSS: 90,911,744 bytes
Walltime: 71ms - user-space: 24ms - kernel-space: 45ms</pre>

<pre> 63┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                             ████████┃
   ┃                                                                           ██████████┃
   ┃                                                                      ███████████████┃
   ┃                                                                    █████████████████┃
   ┃                                                               ██████████████████████┃
   ┃                                                             ████████████████████████┃
   ┃                                                        █████████████████████████████┃
   ┫                                                 ████████████████████████████████████┃
   ┃                                               ██████████████████████████████████████┃
   ┃                                        █████████████████████████████████████████████┃
   ┃                                   ██████████████████████████████████████████████████┃
   ┃                            █████████████████████████████████████████████████████████┃
   ┃                         ████████████████████████████████████████████████████████████┃
   ┃                  ███████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                  36</pre>

<p>Without surprise, we see two forks. One when the driver invokes the compiler and one to invoke the linker. There is no assembler step since it is built-in <code>clang</code>.




</p><p>gcc helloworld.c</p><hr><p>After clang, let's check <code>gcc</code>.

</p><pre><b>$</b> sudo st gcc -o hello hello.c
<span><span>EXEC</span>:</span>: [ gcc -o hello hello.c]
<span><span>EXEC</span>:</span>: [/usr/lib/gcc/aarch64-linux-gnu/11/cc1 -quiet -imultiarch aarch64-linux-gnu hello.c -quiet -dumpbase hello.c -dumpbase-ext .c -mlittle-endian -mabi=lp64 -fasynchronous-unwind-tables -fstack-protector-strong -Wformat -Wformat-security -fstack-clash-protection -o /tmp/ccmt0cw6.s ]
<span><span>EXEC</span>:</span>: [as -EL -mabi=lp64 -o /tmp/ccSRW1gq.o /tmp/ccmt0cw6.s ]
<span><span>EXEC</span>:</span>: [/usr/lib/gcc/aarch64-linux-gnu/11/collect2 -plugin /usr/lib/gcc/aarch64-linux-gnu/11/liblto_plugin.so -plugin-opt=/usr/lib/gcc/aarch64-linux-gnu/11/lto-wrapper -plugin-opt=-fresolution=/tmp/cceZH2hF.res -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lgcc_s --build-id --eh-frame-hdr --hash-style=gnu --as-needed -dynamic-linker /lib/ld-linux-aarch64.so.1 -X -EL -maarch64linux --fix-cortex-a53-843419 -pie -z now -z relro -o hello /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/Scrt1.o /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/crti.o /usr/lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/lib/gcc/aarch64-linux-gnu/11 -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../../lib -L/lib/aarch64-linux-gnu -L/lib/../lib -L/usr/lib/aarch64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/aarch64-linux-gnu/11/../../.. /tmp/ccSRW1gq.o -lg]
<span><span>EXEC</span>:</span>: [/usr/bin/ld -plugin /usr/lib/gcc/aarch64-linux-gnu/11/liblto_plugin.so -plugin-opt=/usr/lib/gcc/aarch64-linux-gnu/11/lto-wrapper -plugin-opt=-fresolution=/tmp/cceZH2hF.res -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lgcc_s --build-id --eh-frame-hdr --hash-style=gnu --as-needed -dynamic-linker /lib/ld-linux-aarch64.so.1 -X -EL -maarch64linux --fix-cortex-a53-843419 -pie -z now -z relro -o hello /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/Scrt1.o /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/crti.o /usr/lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/lib/gcc/aarch64-linux-gnu/11 -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../../lib -L/lib/aarch64-linux-gnu -L/lib/../lib -L/usr/lib/aarch64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/aarch64-linux-gnu/11/../../.. /tmp/ccSRW1gq.o -lgcc --push-state --as-needed -lg]
Num threads = 5
Num process = 5
Max PSS: 18,597,888 bytes
Walltime: 51ms - user-space: 20ms - kernel-space: 20ms</pre>

<pre> 17┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                          ███████                                    ┃
   ┃                                          ███████                                    ┃
   ┃                                        █████████                                    ┃
   ┃                                        █████████                                    ┃
   ┃                                   ██████████████                                    ┃
   ┃                              ███████████████████                                    ┃
   ┃                         ████████████████████████                                    ┃
   ┫                       ██████████████████████████                            ███████ ┃
   ┃                     ████████████████████████████                          █████████ ┃
   ┃                  ███████████████████████████████                        ███████████ ┃
   ┃                  ███████████████████████████████                   ████████████████ ┃
   ┃              ███████████████████████████████████     █████       ██████████████████ ┃
   ┃           ██████████████████████████████████████  ████████    █████████████████████ ┃
   ┃         ████████████████████████████████████████  ██████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                  36</pre>

<p><code>gcc</code> uses two more steps than <code>clang</code>. One for the self-explanatory assembler <code>as</code> and one for the cryptic <code>collect2</code>. I read the <a href="https://gcc.gnu.org/onlinedocs/gccint/Collect2.html">documentation page</a> but I still don't understand what it does.</p>

<p>Noteworthy, <code>gcc</code> uses four times less memory than <code>clang</code>.</p>

















<p>clang++ hello.cc</p><hr><p>Let's checkout the C++ compiler driver from LLVM suite with <code><a href="https://fabiensanglard.net/st/hello.cc">hello.cc</a></code>.</p>

<pre><span>#include &lt;iostream&gt;</span>

<span>int</span> <span>main</span>() {
    std<span>::</span>cout <span>&lt;&lt;</span> <span>"Hello World!"</span>;
    <span>return</span> <span>0</span>;
}
</pre>


<pre><b>$</b> sudo st clang++ hello.cc
<span>EXEC</span>: [ clang++ /home/leaf/hello.cc]
<span>EXEC</span>: [/usr/lib/llvm-14/bin/clang -cc1 -triple aarch64-unknown-linux-gnu -emit-obj -mrelax-all --mrelax-relocations -disable-free -clear-ast-before-backend -disable-llvm-verifier -discard-value-names -main-file-name hello.cc -mrelocation-model pic -pic-level 2 -pic-is-pie -mframe-pointer=non-leaf -fmath-errno -ffp-contract=on -fno-rounding-math -mconstructor-aliases -funwind-tables=2 -target-cpu generic -target-feature +neon -target-feature +v8a -target-abi aapcs -fallow-half-arguments-and-returns -mllvm -treat-scalable-fixed-error-as-warning -debugger-tuning=gdb -fcoverage-compilation-dir=/home/leaf/repos/st -resource-dir /usr/lib/llvm-14/lib/clang/14.0.0 -internal-isystem /usr/bin/../lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11 -internal-isystem /usr/bin/../lib/gcc/aarch64-linux-gnu/11/../../../../include/aarch64-linux-gnu/c++/11 -internal-isystem /usr/bin/../lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/backward -internal-isystem /usr/lib/llvm-14/lib/clang/14.0.0/include -internal-isystem /u]
<span>EXEC</span>: [/usr/bin/ld -pie -EL -z relro --hash-style=gnu --build-id --eh-frame-hdr -m aarch64linux -dynamic-linker /lib/ld-linux-aarch64.so.1 -o a.out /lib/aarch64-linux-gnu/Scrt1.o /lib/aarch64-linux-gnu/crti.o /usr/bin/../lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/bin/../lib/gcc/aarch64-linux-gnu/11 -L/lib/aarch64-linux-gnu -L/usr/lib/aarch64-linux-gnu -L/usr/lib/llvm-14/bin/../lib -L/lib -L/usr/lib /tmp/hello-32e667.o -lstdc++ -lm -lgcc_s -lgcc -lc -lgcc_s -lgcc /usr/bin/../lib/gcc/aarch64-linux-gnu/11/crtendS.o /lib/aarch64-linux-gnu/crtn.o ]
Num threads = 3
Num process = 3
Max PSS: 127,156,224 bytes
Walltime: 221ms - user-space: 136ms - kernel-space: 69ms</pre>

<pre>127┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                                          █████████████              ┃
   ┃            █                    ██████████████████████████████████████              ┃
   ┃            █           ███████████████████████████████████████████████              ┃
   ┃            █         █████████████████████████████████████████████████      █ ████  ┃
   ┃            █  ████████████████████████████████████████████████████████  ████████████┃
   ┫            ███████████████████████████████████████████████████████████ █████████████┃
   ┃           ██████████████████████████████████████████████████████████████████████████┃
   ┃          ███████████████████████████████████████████████████████████████████████████┃
   ┃        █████████████████████████████████████████████████████████████████████████████┃
   ┃      ███████████████████████████████████████████████████████████████████████████████┃
   ┃    █████████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                 221</pre>

<p>No surprises here either. Compiling <code>C++</code> is much more costly in terms of memory (2x), and takes much longer than <code><a href="https://fabiensanglard.net/st/hello.c">hello.c</a></code> (4x).</p>


<p>g++ hello.cc</p><hr>
<p>Let's checkout the C++ compiler driver from GNU suite, <code>g++</code>.</p>

<pre><b>$</b> sudo st g++ hello.cc
<span>EXEC</span>: [ g++ /home/leaf/hello.cc]
<span>EXEC</span>: [/usr/lib/gcc/aarch64-linux-gnu/11/cc1plus -quiet -imultiarch aarch64-linux-gnu -D_GNU_SOURCE /home/leaf/hello.cc -quiet -dumpdir a- -dumpbase hello.cc -dumpbase-ext .cc -mlittle-endian -mabi=lp64 -fasynchronous-unwind-tables -fstack-protector-strong -Wformat -Wformat-security -fstack-clash-protection -o /tmp/ccBFHbve.s ]
<span>EXEC</span>: [as -EL -mabi=lp64 -o /tmp/cckOP2RY.o /tmp/ccBFHbve.s ]
<span>EXEC</span>: [/usr/lib/gcc/aarch64-linux-gnu/11/collect2 -plugin /usr/lib/gcc/aarch64-linux-gnu/11/liblto_plugin.so -plugin-opt=/usr/lib/gcc/aarch64-linux-gnu/11/lto-wrapper -plugin-opt=-fresolution=/tmp/ccpZysdX.res -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lgcc --build-id --eh-frame-hdr --hash-style=gnu --as-needed -dynamic-linker /lib/ld-linux-aarch64.so.1 -X -EL -maarch64linux --fix-cortex-a53-843419 -pie -z now -z relro /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/Scrt1.o /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/crti.o /usr/lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/lib/gcc/aarch64-linux-gnu/11 -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../../lib -L/lib/aarch64-linux-gnu -L/lib/../lib -L/usr/lib/aarch64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/aarch64-linux-gnu/11/../../.. /tmp/cckOP2RY.o -lstdc++ -lm]
<span>EXEC</span>: [/usr/bin/ld -plugin /usr/lib/gcc/aarch64-linux-gnu/11/liblto_plugin.so -plugin-opt=/usr/lib/gcc/aarch64-linux-gnu/11/lto-wrapper -plugin-opt=-fresolution=/tmp/ccpZysdX.res -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lgcc --build-id --eh-frame-hdr --hash-style=gnu --as-needed -dynamic-linker /lib/ld-linux-aarch64.so.1 -X -EL -maarch64linux --fix-cortex-a53-843419 -pie -z now -z relro /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/Scrt1.o /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/crti.o /usr/lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/lib/gcc/aarch64-linux-gnu/11 -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../../lib -L/lib/aarch64-linux-gnu -L/lib/../lib -L/usr/lib/aarch64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/aarch64-linux-gnu/11/../../.. /tmp/cckOP2RY.o -lstdc++ -lm -lgcc_s -lgcc -lc -lgcc_s -lgc]
Num threads = 5
Num process = 5
Max PSS: 60,565,504 bytes
Walltime: 232ms - user-space: 127ms - kernel-space: 48ms</pre>

<pre> 60┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                 ████                ┃
   ┃                                                             ████████                ┃
   ┃                                                        █████████████                ┃
   ┃                                                    █████████████████                ┃
   ┃                                               ██████████████████████                ┃
   ┃                                            █████████████████████████                ┃
   ┃                                         ████████████████████████████                ┃
   ┫                                      ███████████████████████████████                ┃
   ┃                                 ████████████████████████████████████                ┃
   ┃                            █████████████████████████████████████████                ┃
   ┃                        █████████████████████████████████████████████          █████ ┃
   ┃                    █████████████████████████████████████████████████         ██████ ┃
   ┃                █████████████████████████████████████████████████████       █████████┃
   ┃           ██████████████████████████████████████████████████████████     ███████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                 232</pre>

<p>Alike the C driver, <code>gnu</code>'s version requires twice less RAM than <code>clang++</code>.</p>

<p>rust hello.rs</p><hr><p><code>rustc</code> has a reputation to be slow and memory hungry. Seems appropriate since it ran for nearly 200ms and used 131 MiB of RAM (roughly the same as <code>clang++ hello.cc</code>).</p>

<!-- HTML generated using hilite.me -->
<pre><span>fn</span> main() {
    println<span>!</span>(<span>"Hello World!"</span>);
}
</pre>


<p><code>rustc</code> uses an embedded LLVM backend to generates object files. These are handed to the regular <code>gnu</code> suite with <code>collect2</code> and then the linker <code>ld</code>.

</p><pre><b>$</b> sudo st rustc hello.rs 
<span><span>EXEC</span>:</span>: [rustc hello.rs]
<span><span>EXEC</span>:</span>: [cc /tmp/rustc4fdpuV/symbols.o hello.hello.f76cf86e-cgu.0.rcgu.o hello.hello.f76cf86e-cgu.1.rcgu.o hello.hello.f76cf86e-cgu.2.rcgu.o hello.hello.f76cf86e-cgu.3.rcgu.o hello.hello.f76cf86e-cgu.4.rcgu.o hello.hello.f76cf86e-cgu.5.rcgu.o hello.2pelail77dwjevsg.rcgu.o -Wl,--as-needed -L /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib -Wl,-Bstatic /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/libstd-1d2bb2d795f2ca05.rlib /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/libpanic_unwind-f1c586c276421094.rlib /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/libobject-8060a154fd842f2c.rlib /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/libmemchr-1f7fc15c78d3bcac.rlib /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/libaddr2line-965bde82ccc4b5e8.rlib /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/libgimli-352be989f07a059f.rlib /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/librustc_demangle-4f461a85c762abbb.rlib /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/libstd_detect-814ef12b1b7d93c2.rlib /usr/lib/rustlib/aarch64-unknow]
<span><span>EXEC</span>:</span>: [/usr/lib/gcc/aarch64-linux-gnu/11/collect2 -plugin /usr/lib/gcc/aarch64-linux-gnu/11/liblto_plugin.so -plugin-opt=/usr/lib/gcc/aarch64-linux-gnu/11/lto-wrapper -plugin-opt=-fresolution=/tmp/ccXRoDto.res --build-id --eh-frame-hdr --hash-style=gnu --as-needed -dynamic-linker /lib/ld-linux-aarch64.so.1 -X -EL -maarch64linux --fix-cortex-a53-843419 -pie -z now -z relro -o hello /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/Scrt1.o /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/crti.o /usr/lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/lib/rustlib/aarch64-unknown-linux-gnu/lib -L/usr/lib/rustlib/aarch64-unknown-linux-gnu/lib -L/usr/lib/gcc/aarch64-linux-gnu/11 -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../../lib -L/lib/aarch64-linux-gnu -L/lib/../lib -L/usr/lib/aarch64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/aarch64-linux-gnu/11/../../.. /tmp/rustc4fdpuV/symbols.o hello.hello.f76cf86e-cgu.0.rcgu.o hello.hello.f76cf86e-c]
<span><span>EXEC</span>:</span>: [/usr/bin/ld -plugin /usr/lib/gcc/aarch64-linux-gnu/11/liblto_plugin.so -plugin-opt=/usr/lib/gcc/aarch64-linux-gnu/11/lto-wrapper -plugin-opt=-fresolution=/tmp/ccXRoDto.res --build-id --eh-frame-hdr --hash-style=gnu --as-needed -dynamic-linker /lib/ld-linux-aarch64.so.1 -X -EL -maarch64linux --fix-cortex-a53-843419 -pie -z now -z relro -o hello /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/Scrt1.o /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/crti.o /usr/lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/lib/rustlib/aarch64-unknown-linux-gnu/lib -L/usr/lib/rustlib/aarch64-unknown-linux-gnu/lib -L/usr/lib/gcc/aarch64-linux-gnu/11 -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../../lib -L/lib/aarch64-linux-gnu -L/lib/../lib -L/usr/lib/aarch64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/aarch64-linux-gnu/11/../../.. /tmp/rustc4fdpuV/symbols.o hello.hello.f76cf86e-cgu.0.rcgu.o hello.hello.f76cf86e-cgu.1.rcgu.o hello.hello.f76cf86]
Num threads = 14
Num process = 4
Max PSS: 131,376,128 bytes
Walltime: 197ms - user-space: 134ms - kernel-space: 67ms</pre>


<pre>133┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                  ████████████████████████████████   ┃
   ┃                                     █████████████████████████████████████████████   ┃
   ┃                         █        ████████████████████████████████████████████████   ┃
   ┃                     █████      ██████████████████████████████████████████████████   ┃
   ┃                   ███████   █████████████████████████████████████████████████████   ┃
   ┃                  █████████████████████████████████████████████████████████████████  ┃
   ┃                 ██████████████████████████████████████████████████████████████████  ┃
   ┫                 ██████████████████████████████████████████████████████████████████  ┃
   ┃               ████████████████████████████████████████████████████████████████████  ┃
   ┃              █████████████████████████████████████████████████████████████████████  ┃
   ┃             ████████████████████████████████████████████████████████████████████████┃
   ┃           ██████████████████████████████████████████████████████████████████████████┃
   ┃        █████████████████████████████████████████████████████████████████████████████┃
   ┃     ████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                 220</pre>

<p>Notice that fourteen threads were created. Ten of then are coming from <code>rustc</code>.</p>




<p>javac helloworld.java</p><hr><pre><b>$</b> cat HelloWorld.java 
class HelloWorld {
    public static void main(String[] args) {
        System.out.println("Hello, World!"); 
    }
}
<b>$</b> sudo st javac HelloWorld.java
<span><span>EXEC</span>:</span>: [ javac HelloWorld.java]
Num threads = 23
Num process = 1
Max PSS: 75,673,600 bytes
Walltime: 272ms - user-space: 472ms - kernel-space: 48ms</pre>


<pre> 75┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                          ███████████┃
   ┃                                                               ██████████████████████┃
   ┃                                                       ██████████████████████████████┃
   ┃                                             ████████████████████████████████████████┃
   ┃                                      ███████████████████████████████████████████████┃
   ┃                               ██████████████████████████████████████████████████████┃
   ┃                        █████████████████████████████████████████████████████████████┃
   ┫                 ████████████████████████████████████████████████████████████████████┃
   ┃             ████████████████████████████████████████████████████████████████████████┃
   ┃          ███████████████████████████████████████████████████████████████████████████┃
   ┃       ██████████████████████████████████████████████████████████████████████████████┃
   ┃       ██████████████████████████████████████████████████████████████████████████████┃
   ┃       ██████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                 263</pre>


<p>Since the compiler is written in Java, I was expecting to see a second process, execing something like <code>java -cp javac.jar java.tools.Javac ...</code>. Surprisingly, javac is actually a JNI wrapper which acts as a launcher<a name="back_3" href="#footnote_3"><sup>[3]</sup></a>. It instantiates a VM in-process and there is no fork/exec needed.</p>


<p>Also surprising is the number of threads created. Although the next test shows that it is probably just a thread pool created at startup regardless of what the VM actually does.</p>


<p>java Helloworld</p><hr><pre><b>$</b> sudo st java -cp . HelloWorld
<span>EXEC</span>: [ java -cp /home/leaf HelloWorld]
Hello, World!
Num threads = 19
Num process = 1
Max PSS: 30,813,184 bytes
Walltime: 38ms - user-space: 10ms - kernel-space: 21ms</pre>


<pre> 30┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                   █████████████     ┃
   ┃                                                          ██████████████████████     ┃
   ┃                                                   █████████████████████████████     ┃
   ┃                                      ██████████████████████████████████████████     ┃
   ┃                                      ██████████████████████████████████████████     ┃
   ┃                                   █████████████████████████████████████████████     ┃
   ┃                                   █████████████████████████████████████████████     ┃
   ┫                                 ███████████████████████████████████████████████     ┃
   ┃                             ███████████████████████████████████████████████████     ┃
   ┃                          ███████████████████████████████████████████████████████████┃
   ┃                      ███████████████████████████████████████████████████████████████┃
   ┃                    █████████████████████████████████████████████████████████████████┃
   ┃                 ████████████████████████████████████████████████████████████████████┃
   ┃             ████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                  38</pre>

<p>Same elevated number of thread as <code>javac</code>. Likely, <code>javac</code> only created four threads (23-19=4).</p>

<p>go build helloworld.go</p><hr><p><code>go</code> stood up to its reputation of being fast. It generated an executable in 54ms while using only 15MiB of RAM. In this limited helloworld study, it is both the fastest and the least RAM hungry compiler.</p>

<!-- HTML generated using hilite.me -->
<pre><span>package</span> main
<span>import</span> <span>"fmt"</span>
<span>func</span> main() {
    fmt.Println(<span>"hello world"</span>)
}
</pre>


<pre><b>$</b> sudo st go build helloworld.go 
<span><span>EXEC</span>:</span>: [ go build helloworld.go]
Num threads = 9
Num process = 1
Max PSS: 15,591,424 bytes
Walltime: 54ms - user-space: 25ms - kernel-space: 39ms
</pre>


<pre> 15┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                █████████████████████████████████████┃
   ┃                                      ███████████████████████████████████████████████┃
   ┃                                    █████████████████████████████████████████████████┃
   ┃                                █████████████████████████████████████████████████████┃
   ┃                                █████████████████████████████████████████████████████┃
   ┃                           ██████████████████████████████████████████████████████████┃
   ┃                         ████████████████████████████████████████████████████████████┃
   ┫                         ████████████████████████████████████████████████████████████┃
   ┃                      ███████████████████████████████████████████████████████████████┃
   ┃                      ███████████████████████████████████████████████████████████████┃
   ┃                      ███████████████████████████████████████████████████████████████┃
   ┃                █████████████████████████████████████████████████████████████████████┃
   ┃        █████████████████████████████████████████████████████████████████████████████┃
   ┃      ███████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                  53</pre>



<p>Golang relies heavily on goroutines, I was surprised to see nine threads being used on a four core system. Something to look into someday.</p>






<p>Opening Chromium</p><hr><pre><b>$</b> sudo st chromium --headless https://fabiensanglard.net/index.html
<span>EXEC</span>: [ chromium https://fabiensanglard.net/index.html]
<span>EXEC</span>: [ chromium https://fabiensanglard.net/index.html]
<span>EXEC</span>: [/snap/snapd/20102/usr/lib/snapd/snap-seccomp version-info ]
Num threads = 21
Num process = 2
Max PSS: 32,493,568 bytes
Walltime: 51ms - user-space: 639ms - kernel-space: 565ms
</pre>


<pre> 32┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃           █                                                                         ┃
   ┃           █                                                                         ┃
   ┃           █ ████████████████████████████████████████████████████████████████████████┃
   ┫           ██████████████████████████████████████████████████████████████████████████┃
   ┃          ███████████████████████████████████████████████████████████████████████████┃
   ┃     █    ███████████████████████████████████████████████████████████████████████████┃
   ┃   ████ █████████████████████████████████████████████████████████████████████████████┃
   ┃   ████ █████████████████████████████████████████████████████████████████████████████┃
   ┃   ████ █████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                 280</pre>

<p>The process spawned by Chromium is for the GPU render. It looks like it also executed a command to check the version of <a href="https://en.wikipedia.org/wiki/Seccomp">seccomp</a> before sandboxing the GPU.</p>


<p>curl</p><hr><pre><b>$</b> sudo st curl https://fabiensanglard.net/index.html 
<span>EXEC</span>: [ curl https://fabiensanglard.net/index.html]
Num threads = 2
Num process = 1
Max PSS: 4,878,336 bytes
Walltime: 356ms - user-space: 47ms - kernel-space: 7ms
</pre>


<pre>  4┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                █████████████████████████████████████████████████████┃
   ┃                             ████████████████████████████████████████████████████████┃
   ┃                         ████████████████████████████████████████████████████████████┃
   ┃                       ██████████████████████████████████████████████████████████████┃
   ┃                       ██████████████████████████████████████████████████████████████┃
   ┃                       ██████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┫   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃ ████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                 356</pre>


<p>wget</p><hr><pre><b>$</b> sudo st wget https://fabiensanglard.net/index.html 
<span>EXEC</span>: [ wget https://fabiensanglard.net/index.html]
Num threads = 1
Num process = 1
Max PSS: 2,942,976 bytes
Walltime: 348ms - user-space: 8ms - kernel-space: 8ms
</pre>


<pre>  2┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                        █████████████████████████████████████████████┃
   ┃                      ███████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┫   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃ ████████████████████████████████████████████████████████████████████████████████████┃
   ┃ ████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                 348</pre>

<p><code>wget</code> seems to use less RAM than <code>curl</code>. Also, it uses one thread instead of two.</p>



<p>git make</p><hr>
<p>Let's build a medium size project, <code>git</code>, with <code>make</code>.</p>

<pre><b>$</b> sudo apt-get install dh-autoreconf libcurl4-gnutls-dev libexpat1-dev gettext libz-dev libssl-dev
<b>$</b> sudo apt-get install asciidoc xmlto docbook2x
<b>$</b> gh repo clone git/git
<b>$</b> cd git
<b>$</b> make configure
<b>$</b> ./configure --prefix=/usr
<b>$</b> sudo st make all
...
</pre>

<pre>Num threads = 3,155
Num process = 3,155
Max PSS: 144,361,472 bytes
Walltime: 95,427ms - user-space: 83,485ms - kernel-space: 10,995ms
</pre>


<pre>144┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                   █                                                 ┃
   ┃  █        █                       █                              █                  ┃
   ┫  █        ██       █             ███             █        █      █                  ┃
   ┃  █ █ ███  ███ █    ██ █     ███  ███     █  █    █ █ █ █  ██  █  █ ██  █            ┃
   ┃ ████ ████████ ██ █████████████████████  ███ ██  ██████ ██ █████ █████████   █       ┃
   ┃ ██████████████████████████████████████████████████████████████████████████  ████████┃
   ┃████████████████████████████████████████████████████████████████████████████ ████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0s                                                                                   95</pre>

<p>git make -j8</p><hr><p>Let's observe how parallel compilation, <code>-j4</code>,trades walltime with RAM/cores.</p>

<pre><b>$</b> git clean
<b>$</b> sudo st make <span>-j4</span> all
...
Num threads = 3155
Num process = 3155
Max PSS: 288,531,456 bytes
Walltime: 27,556ms - user-space: 90,311ms - kernel-space: 11,458ms
</pre>


<pre>
288┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃               █                                  █                                  ┃
   ┃               █     █                █           █                                  ┃
   ┃     █       █ █     ██       █    █ ██           █       █      █                   ┃
   ┃    ██ █     ███     ██       █  ██████      █    █ █ █   █      █                   ┃
   ┃   ███ ████ ████     ███      ██ ██████   █  █   ██ █ █  ████  █████  █              ┃
   ┫  ████ ████ █████ █ ███████████████████  ██  █  █████ ██ ██████████████  █           ┃
   ┃  █████████████████████████████████████ ██████  ██████████████████████████   █       ┃
   ┃  ████████████████████████████████████████████████████████████████████████   ██ ███  ┃
   ┃  █████████████████████████████████████████████████████████████████████████  ███████ ┃
   ┃  █████████████████████████████████████████████████████████████████████████ █████████┃
   ┃  █████████████████████████████████████████████████████████████████████████ █████████┃
   ┃ ██████████████████████████████████████████████████████████████████████████ █████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0s                                                                                   27</pre>

<p>Using four cores, dropped runtime nearly linearly (3.5) and doubled RAM usage.</p>

<p>m</p><hr><p>The latest command I wanted to observe was <code>m</code> which builds AOSP.</p>
<pre><b>$</b> cd aosp_24
<b>$</b> cat &gt; source build/envsetup.sh
source build/envsetup.sh
lunch aosp_arm64-eng
m -j8
<b>$</b> chmod +x make.sh
<b>$</b> sudo st make.sh
...
</pre>
<pre>Num threads = 136,305
Num process = 132,891
Max PSS: 6,152,478,720 bytes
Walltime: 700864ms - user-space: 9942532ms - kernel-space: 900950ms
</pre>


<pre>  6┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                       █                                             ┃
   ┃                                      ██                                       █     ┃
   ┃                  █████               ███                     █                █     ┃
   ┃                  ███████         ███████                     █ █   █          ██    ┃
   ┃                  ███████████ █ █ ███████                     █ █████          ██    ┃
   ┫                  ███████████████████████                     ████████         ██    ┃
   ┃                 ████████████████████████          ██         ████████         ██    ┃
   ┃          █ █   █████████████████████████        ██████   █   ██████████   █ █ ███ ██┃
   ┃          █ ██ ██████████████████████████        ██████████ █ ██████████████ ████████┃
   ┃  █   ███ ████████████████████████████████       ████████████████████████████████████┃
   ┃ ████ ███████████████████████████████████████   █████████████████████████████████████┃
   ┃ ████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0GB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0s                                                                                  700</pre>

<p>6 GiB and 11 minutes to build a whole OS with 8 cores. Not bad at all!</p>


<p>st internals</p><hr><p>Before writing the tool, I considered relying on <code><a href="https://man7.org/linux/man-pages/man1/time.1.html">time(1)</a></code> and <code>strace(1)</code>. However I was not completely satisfied with them. If <code>time(1)</code> invoked via <code>/usr/bin/time -v</code> could retrieve child processes stats and show peak RSS, I wanted PSS. Also, I wanted to see PSS over time.</p>

<p>Likewise, <code>strace -f</code> could follow child processes but it had a tremendous performance overhead and did not show the command invoked by <code>clone</code> upon fork/thread creation.</p>

<p>In the end, I wrote my own tool and called it <code>st</code> (for space-time). The data sources are as follows.</p>

<ul>
<li>CPU time/ Kernel time relies on <code><a href="https://linux.die.net/man/2/waitpid">waitpid(2)</a></code> and <code><a href="https://man7.org/linux/man-pages/man2/getrusage.2.html">getrusage(2)</a></code>.</li>

<li>PSS over time relies on sampling <code><a href="https://man7.org/linux/man-pages/man5/proc.5.html">proc(5)</a></code>(<code>/proc/PID/smaps</code>) on a regular interval.</li>

<li>Following process/thread creation is done via <code><a href="https://man7.org/linux/man-pages/man7/netlink.7.html">netlink(7)</a></code>.</li>

</ul>

<p>Working with netlink</p><hr><p>Following processes lifecycle via netlink proved more challenging than expected, mainly because of poor documentation (maybe because netlink needs <code>root</code> anyway?). If I managed to dig out <a href="https://fabiensanglard.net/st/process-events.txt">process-events.txt</a> from a mailing list and found a working sample <a href="https://fabiensanglard.net/st/exec-notify.c">exec-notify.c</a>, there wasn't much more around.</p>

<p>One series of articles did stand out. Natan Yellin's blog<a name="back_4" href="#footnote_4"><sup>[4]</sup></a><a name="back_5" href="#footnote_5"><sup>[5]</sup></a><a name="back_6" href="#footnote_6"><sup>[6]</sup></a><a name="back_7" href="#footnote_7"><sup>[7]</sup></a>. is a gem. Among many excellent insights, he clarified<a name="back_8" href="#footnote_8"><sup>[8]</sup></a> the less than intuitive values provided by netlink for parent-gid and parent-pid which saved me a lot of time.

</p>

<p>Hopefully, <code>st</code>'s(<a href="https://github.com/fabiensanglard/st">source code</a>) will add a little extra clarity to <code>netlink</code>.</p>


<p>References</p><hr><p id="paperbox"><table><tbody><tr><td><a name="footnote_1"></a><a href="#back_1">^</a></td><td> [1]</td><td><a href="https://bugzilla.mozilla.org/show_bug.cgi?id=627591">Firefox Bugzilla: Preload dlls on windows</a></td></tr><tr><td><a name="footnote_2"></a><a href="#back_2">^</a></td><td> [2]</td><td><a href="https://binary.ninja/">Binary Ninja</a></td></tr><tr><td><a name="footnote_3"></a><a href="#back_3">^</a></td><td> [3]</td><td><a href="https://gist.github.com/mauricio/2310831">java-launcher.c</a></td></tr><tr><td><a name="footnote_4"></a><a href="#back_4">^</a></td><td> [4]</td><td><a href="https://natanyellin.com/posts/life-and-death-of-a-linux-process">
Life and Death of a Linux Process</a></td></tr><tr><td><a name="footnote_5"></a><a href="#back_5">^</a></td><td> [5]</td><td><a href="https://natanyellin.com/posts/using-linux-audit-to-track-processes">
Using the Linux Audit API to Track Processes</a></td></tr><tr><td><a name="footnote_6"></a><a href="#back_6">^</a></td><td> [6]</td><td><a href="https://natanyellin.com/posts/buggy-netlink-process-connectors">
When Netlink Process Connectors Don’t Process</a></td></tr><tr><td><a name="footnote_7"></a><a href="#back_7">^</a></td><td> [7]</td><td><a href="https://natanyellin.com/posts/tracking-running-processes-on-linux">
The Difficulties of Tracking Running Processes on Linux</a></td></tr><tr><td><a name="footnote_8"></a><a href="#back_8">^</a></td><td> [8]</td><td><a href="https://natanyellin.com/posts/understanding-netlink-process-connector-output">
Understanding Netlink Process Connector Output</a></td></tr></tbody></table></p> <hr>
 <center>*</center></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vizro – toolkit for creating modular data visualization applications (120 pts)]]></title>
            <link>https://github.com/mckinsey/vizro</link>
            <guid>37662561</guid>
            <pubDate>Tue, 26 Sep 2023 17:15:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mckinsey/vizro">https://github.com/mckinsey/vizro</a>, See on <a href="https://news.ycombinator.com/item?id=37662561">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">
<themed-picture data-catalyst-inline="true"><picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/Vizro_Github_Banner_Dark_Mode.png">
  <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/Vizro_Github_Banner_Light_Mode.png">
  <img alt="Vizro logo" src="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/Vizro_Github_Banner_Dark_Mode.png" width="250">
</picture></themed-picture>
</p>
<p dir="auto"><a href="https://pypi.org/project/vizro/" rel="nofollow"><img src="https://camo.githubusercontent.com/8ca8bc983e08b8a9e2151d9c9ad6f7ee06f67c6dad6b4ea8a587bf6688c163af/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e38253230253743253230332e39253230253743253230332e3130253230253743253230332e31312d626c75652e737667" alt="Python version" data-canonical-src="https://img.shields.io/badge/python-3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11-blue.svg"></a>
<a href="https://badge.fury.io/py/vizro" rel="nofollow"><img src="https://camo.githubusercontent.com/c76ace32362608f82e9626febb37e5364c1936354215f9d5845e67bd54fe5dcf/68747470733a2f2f62616467652e667572792e696f2f70792f76697a726f2e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/vizro.svg"></a>
<a href="https://github.com/mckinsey/vizro/blob/main/LICENSE.md"><img src="https://camo.githubusercontent.com/1698104e976c681143eb0841f9675c6f802bb7aa832afc0c7a4e719b1f3cf955/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865253230322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/license-Apache%202.0-blue.svg"></a>
<a href="https://vizro.readthedocs.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/c1972d2d3ad60b878b4bd23ce0def2ee99b8cb83d50ffe829f6192ffced2b9ec/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f76697a726f2f62616467652f3f76657273696f6e3d737461626c65" alt="Documentation" data-canonical-src="https://readthedocs.org/projects/vizro/badge/?version=stable"></a>
<a href="https://www.bestpractices.dev/projects/7858" rel="nofollow"><img src="https://camo.githubusercontent.com/fd762386fcca5b58ccfa34fc02cf49f120452d78754c770a6813426b881b0813/68747470733a2f2f7777772e626573747072616374696365732e6465762f70726f6a656374732f373835382f6261646765" alt="OpenSSF Best Practices" data-canonical-src="https://www.bestpractices.dev/projects/7858/badge"></a></p>

<hr>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/example_screens.png"><img src="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/example_screens.png" width="700"></a>
</p>
<p dir="auto">

<b>
Visual Intelligence. Beautifully engineered
</b>

</p>
<p dir="auto">

Vizro is a toolkit for creating modular data visualization applications

</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/tech_logos.png"><img src="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/tech_logos.png" width="270"></a>
</p>
<h2 tabindex="-1" id="user-content-what-is-vizro" dir="auto"><a href="#what-is-vizro">What is Vizro?</a></h2>
<p dir="auto">

Rapidly self-serve the assembly of customised dashboards in minutes - without the need for advanced coding or design experience - to create flexible and scalable, Python enabled data visualization applications

</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/code_dashboard.png"><img src="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/code_dashboard.png" width="1300"></a>
</p>
<p dir="auto">Use a few lines of simple configuration to create complex dashboards, which are automatically assembled utilising libraries such as <a href="https://github.com/plotly/plotly.py"><strong>Plotly</strong></a> and <a href="https://github.com/plotly/dash"><strong>Dash</strong></a>, with inbuilt coding and design best practices</p>
<p dir="auto">Define high level categories within the configuration, including:</p>
<ul dir="auto">
<li><strong>components:</strong> create charts, tables, input/output interfaces, and more</li>
<li><strong>controls</strong>: create filters, parameter inputs, and custom action controllers</li>
<li><strong>pages, layouts and navigation</strong>: create multiple pages, with customisable layouts and flexible navigation across them</li>
<li><strong>actions and interactions</strong>: create interactions between charts, and use pre-defined or customised actions (such as exporting)</li>
</ul>
<p dir="auto">Configuration can be written in multiple formats including <strong>Pydantic models</strong>, <strong>JSON</strong>, <strong>YAML</strong> or <strong>Python dictionaries</strong> for added flexibility of implementation</p>
<p dir="auto">Optional high-code extensions allow almost infinite customisation in a modular way, combining the best of low-code and high-code - for flexible and scalable, Python enabled data visualization applications</p>
<br>
<h3 tabindex="-1" id="user-content-key-benefits" dir="auto"><a href="#key-benefits">Key benefits</a></h3>

<p dir="auto">
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/value_prop_icons.png"><img src="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/value_prop_icons.png" width="900"></a>
</p>
<br>
<h3 tabindex="-1" id="user-content-examples" dir="auto"><a href="#examples">Examples</a></h3>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/dashboard_examples.png"><img src="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/dashboard_examples.png" width="1300"></a>
</p>
<h3 tabindex="-1" id="user-content-live-demo" dir="auto"><a href="#live-demo">Live demo</a></h3>
<p dir="auto">
<a href="http://vizro.mckinsey.com/" rel="nofollow">
<img src="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/live_interactive_demo.png" width="525" height="296">  </a>
</p>
<h2 tabindex="-1" id="user-content-installation" dir="auto"><a href="#installation">Installation</a></h2>

<p dir="auto">See the <a href="https://vizro.readthedocs.io/en/latest/pages/user_guides/install/" rel="nofollow">Install guide</a> for more information</p>
<p dir="auto">Please note this repository is a monorepo and the core <code>vizro</code> package can be found in <a href="https://github.com/mckinsey/vizro/tree/main/vizro-core">/vizro-core</a></p>
<h2 tabindex="-1" id="user-content-getting-started" dir="auto"><a href="#getting-started">Getting started</a></h2>
<p dir="auto">See the <a href="https://vizro.readthedocs.io/en/latest/pages/tutorials/first_dashboard/" rel="nofollow">Tutorials</a> for creating your first dashboard</p>
<h2 tabindex="-1" id="user-content-documentation" dir="auto"><a href="#documentation">Documentation</a></h2>
<p dir="auto">See the <a href="https://vizro.readthedocs.io/en/latest/" rel="nofollow">Documentation</a> for more details</p>
<h2 tabindex="-1" id="user-content-community-and-development" dir="auto"><a href="#community-and-development">Community and Development</a></h2>
<p dir="auto">We encourage you to ask and answer technical questions via the <a href="https://github.com/mckinsey/vizro/issues">GitHub Issues</a>. This is also the place where you can submit bug reports or request new features.</p>
<h2 tabindex="-1" id="user-content-contributing" dir="auto"><a href="#contributing">Contributing</a></h2>
<p dir="auto">To learn more about making a contribution,
please see the <a href="https://vizro.readthedocs.io/en/latest/pages/development/contributing/" rel="nofollow">Contributing Guide</a> for more information</p>
<p dir="auto">You can also view current and former <a href="https://vizro.readthedocs.io/en/latest/pages/development/authors/" rel="nofollow">contributors</a></p>
<h2 tabindex="-1" id="user-content-reporting-a-security-vulnerability" dir="auto"><a href="#reporting-a-security-vulnerability">Reporting a Security Vulnerability</a></h2>
<p dir="auto">Please see our <a href="https://github.com/mckinsey/vizro/security/policy">security policy</a></p>
<h2 tabindex="-1" id="user-content-license" dir="auto"><a href="#license">License</a></h2>
<p dir="auto"><code>vizro</code> is distributed under the terms of the <a href="https://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">Apache License 2.0</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[macOS Sonoma is available today (377 pts)]]></title>
            <link>https://www.apple.com/newsroom/2023/09/macos-sonoma-is-available-today/</link>
            <guid>37662510</guid>
            <pubDate>Tue, 26 Sep 2023 17:12:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/newsroom/2023/09/macos-sonoma-is-available-today/">https://www.apple.com/newsroom/2023/09/macos-sonoma-is-available-today/</a>, See on <a href="https://news.ycombinator.com/item?id=37662510">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-US" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
    
        




    
    

</nav>



<main id="main" role="main"> 



<span id="opens-in-new-window">opens in new window</span>

	

<section>
<article data-analytics-activitymap-region-id="article">






    
    
    











    <div>
        

        <div>
                    
                    
                        <span>UPDATE</span>
                    
                    
                        <span>September 26, 2023</span>
                    
                    
                </div>

        <div>
                
                
                
                    <h2>
                        
    
        macOS Sonoma is available today
    

                    </h2>
                
            </div>

        

        
            
    
    
    
    
    

        

    </div>







    
    
    






  
    
    
    
    
      <figure aria-label="Media, macOS Sonoma displayed on MacBook Pro, the 27-inch iMac, and MacBook Air.">
        <div>
             
              
              <div>
                macOS Sonoma makes the Mac experience better than ever — from more ways to personalize with widgets, to big updates to Safari and video conferencing, along with optimized gaming.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2023/09/macos-sonoma-is-available-today/article/Apple-macOS-Sonoma-3up.zip" download="" data-analytics-title="Download image" aria-label="Download media, macOS Sonoma displayed on MacBook Pro, the 27-inch iMac, and MacBook Air."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div><a href="https://www.apple.com/macos/sonoma/" target="_blank">macOS Sonoma</a> is now available as a free software update, bringing a rich set of new features to the Mac that make work and play even more powerful. With macOS Sonoma, desktop widgets unlock a new way to personalize the Mac and get more done, while stunning new screen savers, big updates to video conferencing and Safari, along with optimized gaming make the Mac experience better than ever.<br>

</div>
                 
             
                 <h2>Widgets and Stunning Screen Savers
</h2>
                 
             
                 <div>With macOS Sonoma, widgets can be placed right on the desktop and blend seamlessly with the wallpaper while other windows are open. Widgets also become interactive so users can complete a reminder, play or pause media, access home controls, and perform various tasks directly from the widget on their desktop. And through the magic of Continuity, users can further customize their Mac with widgets from their iPhone.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, A collection of widgets displayed on MacBook Pro with macOS Sonoma.">
        <div>
             
              
              <div>
                Users can personalize their desktop with widgets, and with Continuity, they can even add their iPhone widgets to their Mac desktop.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2023/09/macos-sonoma-is-available-today/article/Apple-macOS-Sonoma-Widgets.zip" download="" data-analytics-title="Download image" aria-label="Download media, A collection of widgets displayed on MacBook Pro with macOS Sonoma."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>A new selection of screen savers in macOS Sonoma features slow-motion videos of beautiful locations around the world, such as the sweeping skyline of Hong Kong, the sandstone buttes of Monument Valley in Arizona, and the rolling hills of Sonoma in Northern California. And after login, the screen savers seamlessly transition to become the desktop wallpaper.
</div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>A new selection of screen savers in macOS Sonoma shows slow-motion videos of some of the most beautiful locations around the world.</div>
        
            <a aria-label="Download video: macOS Sonoma Screen Saver" data-analytics-title="Download video - macOS Sonoma Screen Saver" download="" href="https://www.apple.com/newsroom/videos/apple-macos-sonoma-screen-saver/downloads/Apple-macOS-Sonoma-screen-saver.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <h2>Powerful Productivity for Video Conferencing
</h2>
                 
             
                 <div>macOS Sonoma brings enhanced video conferencing features that enable users to present and share their work more effectively within any video conferencing app. Presenter Overlay displays users in front of the content they are sharing, and Reactions allow users to share how they feel with simple hand gestures that trigger fun, frame-filling 3D effects like balloons, confetti, hearts, and more.
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="video-conferencing">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-cd3c7cce41feeadca525a4198744a00d" href="#gallery-cd3c7cce41feeadca525a4198744a00d" data-ac-gallery-trigger="gallery-cd3c7cce41feeadca525a4198744a00d"><span>A video conference call on MacBook Pro shows a presenter in front of a document they’re showing.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-73d7ac7d6ab2c5b0311ab299cd09aa3e" href="#gallery-73d7ac7d6ab2c5b0311ab299cd09aa3e" data-ac-gallery-trigger="gallery-73d7ac7d6ab2c5b0311ab299cd09aa3e"><span>A Zoom meeting on MacBook Pro shows fireworks behind a presenter.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-cd3c7cce41feeadca525a4198744a00d" aria-labelledby="gallery-dotnav-cd3c7cce41feeadca525a4198744a00d" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:presenter-overlay">
                                
                                <div>
                                    <div>Video conference calls get more engaging with new features like Presenter Overlay, which displays users in front of the content they are sharing.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2023/09/macos-sonoma-is-available-today/article/Apple-macOS-Sonoma-Zoom-Presenter-Overlay.zip" download="" data-analytics-title="Download image" aria-label="Download media, A video conference call on MacBook Pro shows a presenter in front of a document they’re showing."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-73d7ac7d6ab2c5b0311ab299cd09aa3e" aria-labelledby="gallery-dotnav-73d7ac7d6ab2c5b0311ab299cd09aa3e" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:reactions">
                                
                                <div>
                                    <div>Reactions allow users to share how they feel with simple hand gestures that trigger fun, frame-filling 3D effects like balloons, confetti, hearts, and more. </div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2023/09/macos-sonoma-is-available-today/article/Apple-macOS-Sonoma-Zoom-Reactions.zip" download="" data-analytics-title="Download image" aria-label="Download media, A Zoom meeting on MacBook Pro shows fireworks behind a presenter."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2>Enhanced Browsing with Safari
</h2>
                 
             
                 <div>In Safari, profiles keep browsing separate between topics like work, school, and more so users can quickly switch between them. Private Browsing gets even better with added protection against some of the most advanced techniques used to track users — Private Browsing windows become locked when not in use and known trackers are blocked from loading. Safari users can also now add any website to the Dock as they would with an app, complete with a simplified toolbar and notifications for an app-like experience.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, MacBook Pro displays two profiles: one labeled School and the other labeled Home.">
        <div>
             
              
              <div>
                Profiles keep browsing separate between topics like work, school, and more so users can quickly switch between them.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2023/09/macos-sonoma-is-available-today/article/Apple-macOS-Sonoma-profiles.zip" download="" data-analytics-title="Download image" aria-label="Download media, MacBook Pro displays two profiles: one labeled School and the other labeled Home."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2>An Immersive Gaming Experience<br>

</h2>
                 
             
                 <div>With the power of Apple silicon, tens of millions of Macs can run demanding games with great performance, long battery life, and breathtaking visuals. macOS Sonoma improves the gaming experience even further with Game Mode, providing more consistent frame rates and dramatically reducing input and audio latency with wireless game controllers and AirPods.<sup> </sup>Game Mode works with any game, including recent and upcoming Mac games like DEATH STRANDING DIRECTOR’S CUT, Stray, Layers of Fear, and SnowRunner.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, MacBook Pro displays a screen from the video game Stray.">
        <div>
             
              
              <div>
                In macOS Sonoma, Game Mode provides more consistent frame rates and dramatically reduced input and audio latency with wireless game controllers and AirPods.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2023/09/macos-sonoma-is-available-today/article/Apple-macOS-Sonoma-gaming.zip" download="" data-analytics-title="Download image" aria-label="Download media, MacBook Pro displays a screen from the video game Stray."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Additional macOS Sonoma Updates</strong>
</h2>
                 
             
                 <div><ul>
<li><strong>Notes</strong>:<strong> </strong>Users can view PDFs and scans of presentations, assignments, research papers, and more right inside Notes. They can also create links from one note to another to relate ideas and content.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Passwords</strong>:<strong> </strong>A set of passwords can now be shared among a group. Everyone in a group can add and edit passwords to keep them up to date, and since sharing is through iCloud Keychain, it’s end-to-end encrypted. Additionally, the one-time verification codes received in Mail will now autofill in Safari, making it easy to securely log in without leaving the browser.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Messages</strong>: Search filters and swipe to reply enhance everyday messaging, while all-new Live Stickers can be created and synced across macOS, iOS, and iPadOS.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Reminders</strong>:<strong> </strong>Intelligent grocery lists in Reminders streamline weekly trips to the store by organizing lists into sections and arranging them horizontally using a new column view.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Keyboard</strong>:<strong> </strong>Autocorrect receives a comprehensive update with a transformer language model, a state-of-the-art on-device machine learning language model that improves accuracy. A refreshed design makes corrections easier to fix and inline predictions quickly finish sentences. Dictation brings next-level speech recognition and the ability to move fluidly between voice and typing.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Screen Sharing:&nbsp; </strong>A new high performance mode in the Screen Sharing app delivers incredibly responsive remote access over high-bandwidth connections — enabling creative professionals to accomplish their work remotely.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    


     
     
    
    
        <div>macOS Sonoma is a free software update that is available starting today. Some features may not be available in all regions, languages, or on all devices. For more information and a full list of features, visit <a href="https://www.apple.com/macos/sonoma/" target="_blank">apple.com/macos/sonoma</a>.
</div>
 

    
    
    




    
    
        
    


    
    
    



    
    
    






    
















	
	
	
		















	
	

</article>



</section>
</main>


	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why don’t Americans eat mutton? (172 pts)]]></title>
            <link>https://modernfarmer.com/2023/09/digging-in-mutton/</link>
            <guid>37662281</guid>
            <pubDate>Tue, 26 Sep 2023 16:59:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://modernfarmer.com/2023/09/digging-in-mutton/">https://modernfarmer.com/2023/09/digging-in-mutton/</a>, See on <a href="https://news.ycombinator.com/item?id=37662281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							<p><span>“Why can we only get lamb in the US, as opposed to mutton?”</span></p>
<p><span>That’s what Bobbie Kramer, a veterinarian near Portland, Oregon, was wondering when she responded to our recent </span><a href="https://modernfarmer.com/2023/08/dig-in-modern-farmer-investigates/"><span>call for reader questions</span></a><span> about where their food comes from.&nbsp;</span></p>
<p><span>“As a meat eater, I enjoy the flavor and texture of lamb. But I’d love to try mutton. I know that in other parts of the world, lamb and mutton are more economical and popular to raise than cattle,” she writes. “I’ve traveled a fair bit (Australia, New Zealand, Europe and Great Britain) and have friends from parts of the world where small ruminants such as sheep and goats are raised for meat and fiber. My good friend from South Africa tells me how she and her husband miss cooking with mutton, which they find more flavorful and satisfying than lamb. What happens to the mutton-aged sheep here?”</span></p>
<p><span>It’s true that it’s difficult, if not impossible, to find mutton—defined as meat from a sheep over two years old—in American grocery stores. “Mutton is not an accessible protein option in the US,” says Megan Wortman, executive director of the </span><a href="https://americanlamb.com/"><span>American Lamb Board</span></a><span>, an industry group aimed at expanding the market for domestic sheep products. If you’re looking to get your hands on some mutton, “you’d have to go through a specialty butcher shop or directly to a special-order processor,” she says.&nbsp;&nbsp;</span></p>
<p><span>Mutton has less tender flesh and a stronger flavor than lamb, which comes from sheep that are less than a year old. (Meat from sheep aged one to two years is generally called “yearling” in the US, and “hogget” elsewhere around the world.) That stronger flavor lends itself to curries, stews and “value-added” products such as spiced sausages, says Wortman, “so most of our mutton goes into value-added products or into specialty ethnic markets at this point.”&nbsp;</span></p>
<p><span>Some mutton is exported to Mexico, where it’s braised low and slow, </span><i><span>barbacoa</span></i><span>-style. Mutton is also often sold at butcher shops that serve communities that have brought a taste for the meat with them from elsewhere, such as new immigrants from Africa, Central America and the Middle East. (Wortman notes that the majority of US lamb and mutton is halal processed.) And in western Kentucky, a tradition of barbecued mutton still holds, although </span><a href="https://bittersoutherner.com/southern-perspective/2020/southern-mutton"><span>no one is quite sure why</span></a><span>.</span></p>
<p><span>“There are consumer segments that would raise their hand and say ‘yes, I would prefer a stronger flavor,’ but we just don’t market it in mainstream grocery stories,” says Wortman. “There’s definitely a general hesitation that the minute you label it ‘mutton’ the average consumer has negative connotations with that product.”</span></p>
<p><span>So, how did mutton, a widely consumed protein around the world, come to be unmarketable to most Americans?</span></p>
<p><img decoding="async" src="https://modernfarmer.com/wp-content/uploads/2023/09/mutton-560x61.png" alt="" width="496" height="54" srcset="https://modernfarmer.com/wp-content/uploads/2023/09/mutton-560x61.png 560w, https://modernfarmer.com/wp-content/uploads/2023/09/mutton-1200x130.png 1200w, https://modernfarmer.com/wp-content/uploads/2023/09/mutton-768x83.png 768w, https://modernfarmer.com/wp-content/uploads/2023/09/mutton-1920x209.png 1920w" sizes="(max-width: 496px) 100vw, 496px"></p>
<p><span>Sheep were first brought to the southwestern US by Spanish conquistadors in the 16th century, and flocks grew with the influx of European settlers, who utilized sheep locally for their wool and meat. With rising demand for wool in the 19th century, sheep farming became more industrialized, but the primary focus was on the wool, not the meat. Simply put, mutton was a byproduct of wool production.</span></p>
<p><span>Mutton was slaughtered, sold and canned locally, but no large-scale infrastructure arose to source and process sheep meat. “The simplest story is that no commercial meat industry developed around mutton,” says Roger Horowitz, a historian and author of </span><a href="https://www.amazon.com/Putting-Meat-American-Table-Transformation/dp/0801882419/ref=sr_1_1?ie=UTF8&amp;qid=1424184552&amp;sr=8-1&amp;keywords=roger+horowitz"><i><span>Putting Meat on the American Tabl</span></i><span>e</span></a><span>. “It seems to me that it was very rural in character.” He points to a can of roast mutton in his collection, dating from the 1890s, as emblematic of the time: It advertised that its contents were both slaughtered and canned “on the range” in Fort McKavett, Texas.</span></p>
<div id="attachment_150227"><p><img aria-describedby="caption-attachment-150227" decoding="async" src="https://modernfarmer.com/wp-content/uploads/2023/09/service-pnp-cph-3b20000-3b24000-3b24500-3b24562r-443x346.jpg" alt="" width="443" height="346" srcset="https://modernfarmer.com/wp-content/uploads/2023/09/service-pnp-cph-3b20000-3b24000-3b24500-3b24562r-443x346.jpg 443w, https://modernfarmer.com/wp-content/uploads/2023/09/service-pnp-cph-3b20000-3b24000-3b24500-3b24562r.jpg 640w" sizes="(max-width: 443px) 100vw, 443px"></p><p id="caption-attachment-150227">A man shearing a sheep at the San Emigdio Ranch in Kern County, CA in 1890. (Carleton E. Watkins/Library of Congress)</p></div>
<p><span>That’s not to say that mutton wasn’t consumed at the dinner table. Mutton chops were featured in cookbooks and restaurant menus from the late 19th and early 20th century, as the population grew and urbanized and demand for protein rose. Lamb was a seasonal product served at Christmas, and for a time, sheep meat was seen as a food for the upper classes. Even first-class passengers on the RMS Titanic </span><a href="https://titanicfacts.net/titanic-menu/"><span>were served</span></a><span> grilled mutton—for luncheon </span><i><span>and</span></i><span> breakfast.&nbsp;</span></p>
<p><span>Sheep numbers in the US peaked in 1884 at </span><a href="https://www.ers.usda.gov/topics/animal-products/sheep-lamb-mutton/sector-at-a-glance/"><span>51 million head</span></a><span>. But with the advent of synthetic fibers in the 20th century, wool production began to flag, and sheep numbers—and the availability of mutton—declined. (In 2016, there were five million head of sheep in the US.) Lamb consumption began to dwindle, too: Americans consumed five pounds of lamb per person in 1912. Today, that number is </span><a href="https://www.statista.com/statistics/183565/per-capita-consumption-of-lamb-and-mutton-in-the-us-since-2000/#:~:text=The%20timeline%20shows%20the%20per,to%201.4%20pounds%20in%202021."><span>about a pound</span></a><span> per person annually.&nbsp;</span></p>
<p><span>Pork, Horowitz notes, was more convenient. “Everybody had pigs, and pigs are a lot better to raise for meat because they eat anything.” And when it came to grazing animals, cows just made more sense: They provide far more meat per animal, and demand for beef was—and remains—high.</span></p>
<div id="attachment_150230"><p><img aria-describedby="caption-attachment-150230" decoding="async" loading="lazy" src="https://modernfarmer.com/wp-content/uploads/2023/09/5757760150_908cac03fb_c-538x346.jpg" alt="" width="538" height="346" srcset="https://modernfarmer.com/wp-content/uploads/2023/09/5757760150_908cac03fb_c-538x346.jpg 538w, https://modernfarmer.com/wp-content/uploads/2023/09/5757760150_908cac03fb_c-768x494.jpg 768w, https://modernfarmer.com/wp-content/uploads/2023/09/5757760150_908cac03fb_c.jpg 799w" sizes="(max-width: 538px) 100vw, 538px"></p><p id="caption-attachment-150230">This woman does not want to cook mutton. (Photo: Ethan/<a href="https://www.flickr.com/photos/42353480@N02/5757760150/in/photolist-dGe98N-2mykJev-f25vaH-2mX345F-7cV4VG-CXFNZz-722Wdy-7Ux1CR-9LN2uJ-zL5DTP-5LgySb-aEqmY6-qFhpbQ-fai8tT-qXMm61-qFpBAg-2ipSL8D-a5oFCK-8xSPg3-2ipSLe5-fp5Ks-a66kFA-cgoi2G-2ipRBvu-9pLw1j-2ipSLaC-Rgnxxv-R5L1nF-PYXdjG-2ewrbqf-67KRHG-DJMLX-4WeVh8-2okEvet-6rLvyV-cP1bNd-R5M91e-PYXgfb-R5KZgn-6YtygK">Flickr</a>)</p></div>
<p><span>By the end of World War II, mutton had come to symbolize everything that Americans wanted to leave behind. Men returned from the war swearing they’d never eat another bite of mutton after stomaching tinned army rations that included the <a href="https://en.wikipedia.org/wiki/C-ration">notoriously unappetizing</a> “Mutton Stew with Vegetables.” Women were enjoying new appliances that allowed them a modicum of freedom from household chores. Modernity and convenience were all the rage, and mutton, which requires dry aging and long, slow cooking times to become tender, was neither modern nor convenient. If mutton ever really had a heyday, by midcentury, it was over.&nbsp;</span></p>
<p><img decoding="async" src="https://modernfarmer.com/wp-content/uploads/2023/09/mutton-560x61.png" alt="" width="496" height="54" srcset="https://modernfarmer.com/wp-content/uploads/2023/09/mutton-560x61.png 560w, https://modernfarmer.com/wp-content/uploads/2023/09/mutton-1200x130.png 1200w, https://modernfarmer.com/wp-content/uploads/2023/09/mutton-768x83.png 768w, https://modernfarmer.com/wp-content/uploads/2023/09/mutton-1920x209.png 1920w" sizes="(max-width: 496px) 100vw, 496px"></p>
<p><span>“I joke sometimes that I do lamb by day and sheep by night,” says Cody Heimke, who, in addition to managing the </span><a href="https://www.nimanranch.com/"><span>Niman Ranch</span></a><span> lamb program, raises a heritage breed of Shropshire sheep on his property in south central Wisconsin. The flock of about 50 head are raised primarily for breeding, but Shropshires were at one time the most popular sheep in the world, primarily because of the quality of their mutton. “[The] breed of sheep doesn’t really matter when it comes to the flavor of lamb, but it does when it comes to the taste of mutton,” he says.&nbsp;</span></p>
<p><span>Heimke does “a little bit” of direct lamb and mutton sales when he has sheep to harvest, selling middle cuts to a restaurant in Madison, and utilizing the rest for </span><a href="http://www.mapletonmynd.com/"><span>sausages</span></a><span> in varieties such as Bavarian-style, Merguez and spicy Berbere. He acknowledges that there isn’t a lot of demand for mutton. “I got a call this year, somebody looking for mutton, which is rare. I don’t usually get those calls.”</span></p>
<p><span>His advice for would-be mutton eaters? “Find somebody at a local farmers market that’s selling lamb. You really gotta find somebody that’s raising sheep and doing direct marketing, and ask them if they’re doing any mutton.”</span></p>
<p><span>For his part, Heimke says he enjoys mutton in sausage form. Last year, one of his wholesale clients was looking for ground lamb, but he didn’t have any in stock. “I’m like, ‘Well, what about ground mutton?’ And we [sold] one-pounders of ground mutton,” he says. “I tasted that before I sold any of it, and it was as good or better than any ground lamb I’ve ever had.”&nbsp;</span></p>
<p><img decoding="async" src="https://modernfarmer.com/wp-content/uploads/2023/09/mutton-560x61.png" alt="" width="496" height="54" srcset="https://modernfarmer.com/wp-content/uploads/2023/09/mutton-560x61.png 560w, https://modernfarmer.com/wp-content/uploads/2023/09/mutton-1200x130.png 1200w, https://modernfarmer.com/wp-content/uploads/2023/09/mutton-768x83.png 768w, https://modernfarmer.com/wp-content/uploads/2023/09/mutton-1920x209.png 1920w" sizes="(max-width: 496px) 100vw, 496px"></p>
<p><span>Have you ever eaten mutton? Do you want to try mutton—or not? Tell us what you think in the comments below.&nbsp;</span></p>
<p><span>Thanks to Bobbie Kramer for submitting her question for our “Digging In” series. Got a question about where your food comes from? <strong>Let us know what you’d like us to investigate next by filling out <a href="https://docs.google.com/forms/d/e/1FAIpQLSey5-eyUf2Epmgxdvoc57Ei-oVZ40Z6-Mu_EIpWio8ug3v7Gg/viewform?usp=sf_link">this form</a>.</strong></span></p>
						</div><div>
				<p>
					Sign up for your Modern Farmer Weekly Newsletter
				</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Magentic – Use LLMs as simple Python functions (247 pts)]]></title>
            <link>https://github.com/jackmpcollins/magentic</link>
            <guid>37661767</guid>
            <pubDate>Tue, 26 Sep 2023 16:31:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/jackmpcollins/magentic">https://github.com/jackmpcollins/magentic</a>, See on <a href="https://news.ycombinator.com/item?id=37661767">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-magentic" dir="auto"><a href="#magentic">magentic</a></h2>
<p dir="auto">Easily integrate Large Language Models into your Python code. Simply use the <code>@prompt</code> decorator to create functions that return structured output from the LLM. Mix LLM queries and function calling with regular Python code to create complex logic.</p>
<p dir="auto"><code>magentic</code> is</p>
<ul dir="auto">
<li><strong>Compact:</strong> Query LLMs without duplicating boilerplate code.</li>
<li><strong>Atomic:</strong> Prompts are functions that can be individually tested and reasoned about.</li>
<li><strong>Transparent:</strong> Create "chains" using regular Python code. Define all of your own prompts.</li>
<li><strong>Compatible:</strong> Use <code>@prompt</code> functions as normal functions, including with decorators like <code>@lru_cache</code>.</li>
<li><strong>Type Annotated:</strong> Works with linters and IDEs.</li>
</ul>
<p dir="auto">Continue reading for sample usage, or go straight to the <a href="https://github.com/jackmpcollins/magentic/blob/main/examples">examples directory</a>.</p>
<h2 tabindex="-1" id="user-content-installation" dir="auto"><a href="#installation">Installation</a></h2>

<p dir="auto">or using poetry</p>

<p dir="auto">Configure your OpenAI API key by setting the <code>OPENAI_API_KEY</code> environment variable or using <code>openai.api_key = "sk-..."</code>. See the <a href="https://github.com/openai/openai-python#usage">OpenAI Python library documentation</a> for more information.</p>
<h2 tabindex="-1" id="user-content-usage" dir="auto"><a href="#usage">Usage</a></h2>
<p dir="auto">The <code>@prompt</code> decorator allows you to define a template for a Large Language Model (LLM) prompt as a Python function. When this function is called, the arguments are inserted into the template, then this prompt is sent to an LLM which generates the function output.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from magentic import prompt


@prompt('Add more &quot;dude&quot;ness to: {phrase}')
def dudeify(phrase: str) -> str:
    ...  # No function body as this is never executed


dudeify(&quot;Hello, how are you?&quot;)
# &quot;Hey, dude! What's up? How's it going, my man?&quot;"><pre><span>from</span> <span>magentic</span> <span>import</span> <span>prompt</span>


<span>@<span>prompt</span>(<span>'Add more "dude"ness to: {phrase}'</span>)</span>
<span>def</span> <span>dudeify</span>(<span>phrase</span>: <span>str</span>) <span>-&gt;</span> <span>str</span>:
    ...  <span># No function body as this is never executed</span>


<span>dudeify</span>(<span>"Hello, how are you?"</span>)
<span># "Hey, dude! What's up? How's it going, my man?"</span></pre></div>
<p dir="auto">The <code>@prompt</code> decorator will respect the return type annotation of the decorated function. This can be <a href="https://docs.pydantic.dev/latest/usage/types/types/" rel="nofollow">any type supported by pydantic</a> including a <code>pydantic</code> model.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from magentic import prompt
from pydantic import BaseModel


class Superhero(BaseModel):
    name: str
    age: int
    power: str
    enemies: list[str]


@prompt(&quot;Create a Superhero named {name}.&quot;)
def create_superhero(name: str) -> Superhero:
    ...


create_superhero(&quot;Garden Man&quot;)
# Superhero(name='Garden Man', age=30, power='Control over plants', enemies=['Pollution Man', 'Concrete Woman'])"><pre><span>from</span> <span>magentic</span> <span>import</span> <span>prompt</span>
<span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>


<span>class</span> <span>Superhero</span>(<span>BaseModel</span>):
    <span>name</span>: <span>str</span>
    <span>age</span>: <span>int</span>
    <span>power</span>: <span>str</span>
    <span>enemies</span>: <span>list</span>[<span>str</span>]


<span>@<span>prompt</span>(<span>"Create a Superhero named {name}."</span>)</span>
<span>def</span> <span>create_superhero</span>(<span>name</span>: <span>str</span>) <span>-&gt;</span> <span>Superhero</span>:
    ...


<span>create_superhero</span>(<span>"Garden Man"</span>)
<span># Superhero(name='Garden Man', age=30, power='Control over plants', enemies=['Pollution Man', 'Concrete Woman'])</span></pre></div>
<p dir="auto">An LLM can also decide to call functions. In this case the <code>@prompt</code>-decorated function returns a <code>FunctionCall</code> object which can be called to execute the function using the arguments provided by the LLM.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from typing import Literal

from magentic import prompt, FunctionCall


def activate_oven(temperature: int, mode: Literal[&quot;broil&quot;, &quot;bake&quot;, &quot;roast&quot;]) -> str:
    &quot;&quot;&quot;Turn the oven on with the provided settings.&quot;&quot;&quot;
    return f&quot;Preheating to {temperature} F with mode {mode}&quot;


@prompt(
    &quot;Prepare the oven so I can make {food}&quot;,
    functions=[activate_oven],
)
def configure_oven(food: str) -> FunctionCall[str]:
    ...


output = configure_oven(&quot;cookies!&quot;)
# FunctionCall(<function activate_oven at 0x1105a6200>, temperature=350, mode='bake')
output()
# 'Preheating to 350 F with mode bake'"><pre><span>from</span> <span>typing</span> <span>import</span> <span>Literal</span>

<span>from</span> <span>magentic</span> <span>import</span> <span>prompt</span>, <span>FunctionCall</span>


<span>def</span> <span>activate_oven</span>(<span>temperature</span>: <span>int</span>, <span>mode</span>: <span>Literal</span>[<span>"broil"</span>, <span>"bake"</span>, <span>"roast"</span>]) <span>-&gt;</span> <span>str</span>:
    <span>"""Turn the oven on with the provided settings."""</span>
    <span>return</span> <span>f"Preheating to <span><span>{</span><span>temperature</span><span>}</span></span> F with mode <span><span>{</span><span>mode</span><span>}</span></span>"</span>


<span>@<span>prompt</span>(</span>
<span>    <span>"Prepare the oven so I can make {food}"</span>,</span>
<span>    <span>functions</span><span>=</span>[<span>activate_oven</span>],</span>
<span>)</span>
<span>def</span> <span>configure_oven</span>(<span>food</span>: <span>str</span>) <span>-&gt;</span> <span>FunctionCall</span>[<span>str</span>]:
    ...


<span>output</span> <span>=</span> <span>configure_oven</span>(<span>"cookies!"</span>)
<span># FunctionCall(&lt;function activate_oven at 0x1105a6200&gt;, temperature=350, mode='bake')</span>
<span>output</span>()
<span># 'Preheating to 350 F with mode bake'</span></pre></div>
<p dir="auto">Sometimes the LLM requires making one or more function calls to generate a final answer. The <code>@prompt_chain</code> decorator will resolve <code>FunctionCall</code> objects automatically and pass the output back to the LLM to continue until the final answer is reached.</p>
<p dir="auto">In the following example, when <code>describe_weather</code> is called the LLM first calls the <code>get_current_weather</code> function, then uses the result of this to formulate its final answer which gets returned.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from magentic import prompt_chain


def get_current_weather(location, unit=&quot;fahrenheit&quot;):
    &quot;&quot;&quot;Get the current weather in a given location&quot;&quot;&quot;
    # Pretend to query an API
    return {
        &quot;location&quot;: location,
        &quot;temperature&quot;: &quot;72&quot;,
        &quot;unit&quot;: unit,
        &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;],
    }


@prompt_chain(
    &quot;What's the weather like in {city}?&quot;,
    functions=[get_current_weather],
)
def describe_weather(city: str) -> str:
    ...


describe_weather(&quot;Boston&quot;)
# 'The current weather in Boston is 72°F and it is sunny and windy.'"><pre><span>from</span> <span>magentic</span> <span>import</span> <span>prompt_chain</span>


<span>def</span> <span>get_current_weather</span>(<span>location</span>, <span>unit</span><span>=</span><span>"fahrenheit"</span>):
    <span>"""Get the current weather in a given location"""</span>
    <span># Pretend to query an API</span>
    <span>return</span> {
        <span>"location"</span>: <span>location</span>,
        <span>"temperature"</span>: <span>"72"</span>,
        <span>"unit"</span>: <span>unit</span>,
        <span>"forecast"</span>: [<span>"sunny"</span>, <span>"windy"</span>],
    }


<span>@<span>prompt_chain</span>(</span>
<span>    <span>"What's the weather like in {city}?"</span>,</span>
<span>    <span>functions</span><span>=</span>[<span>get_current_weather</span>],</span>
<span>)</span>
<span>def</span> <span>describe_weather</span>(<span>city</span>: <span>str</span>) <span>-&gt;</span> <span>str</span>:
    ...


<span>describe_weather</span>(<span>"Boston"</span>)
<span># 'The current weather in Boston is 72°F and it is sunny and windy.'</span></pre></div>
<p dir="auto">LLM-powered functions created using <code>@prompt</code> and <code>@prompt_chain</code> can be supplied as <code>functions</code> to other <code>@prompt</code>/<code>@prompt_chain</code> decorators, just like regular python functions. This enables increasingly complex LLM-powered functionality, while allowing individual components to be tested and improved in isolation.</p>
<p dir="auto">See the <a href="https://github.com/jackmpcollins/magentic/blob/main/examples">examples directory</a> for more.</p>
<h3 tabindex="-1" id="user-content-streaming" dir="auto"><a href="#streaming">Streaming</a></h3>
<p dir="auto">The <code>StreamedStr</code> (and <code>AsyncStreamedStr</code>) class can be used to stream the output of the LLM. This allows you to process the text while it is being generated, rather than receiving the whole output at once.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from magentic import prompt, StreamedStr


@prompt(&quot;Tell me about {country}&quot;)
def describe_country(country: str) -> StreamedStr:
    ...


# Print the chunks while they are being received
for chunk in describe_country(&quot;Brazil&quot;):
    print(chunk, end=&quot;&quot;)
# 'Brazil, officially known as the Federative Republic of Brazil, is ...'"><pre><span>from</span> <span>magentic</span> <span>import</span> <span>prompt</span>, <span>StreamedStr</span>


<span>@<span>prompt</span>(<span>"Tell me about {country}"</span>)</span>
<span>def</span> <span>describe_country</span>(<span>country</span>: <span>str</span>) <span>-&gt;</span> <span>StreamedStr</span>:
    ...


<span># Print the chunks while they are being received</span>
<span>for</span> <span>chunk</span> <span>in</span> <span>describe_country</span>(<span>"Brazil"</span>):
    <span>print</span>(<span>chunk</span>, <span>end</span><span>=</span><span>""</span>)
<span># 'Brazil, officially known as the Federative Republic of Brazil, is ...'</span></pre></div>
<p dir="auto">Multiple <code>StreamedStr</code> can be created at the same time to stream LLM outputs concurrently. In the below example, generating the description for multiple countries takes approximately the same amount of time as for a single country.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from time import time

countries = [&quot;Australia&quot;, &quot;Brazil&quot;, &quot;Chile&quot;]


# Generate the descriptions one at a time
start_time = time()
for country in countries:
    # Converting `StreamedStr` to `str` blocks until the LLM output is fully generated
    description = str(describe_country(country))
    print(f&quot;{time() - start_time:.2f}s : {country} - {len(description)} chars&quot;)

# 22.72s : Australia - 2130 chars
# 41.63s : Brazil - 1884 chars
# 74.31s : Chile - 2968 chars


# Generate the descriptions concurrently by creating the StreamedStrs at the same time
start_time = time()
streamed_strs = [describe_country(country) for country in countries]
for country, streamed_str in zip(countries, streamed_strs):
    description = str(streamed_str)
    print(f&quot;{time() - start_time:.2f}s : {country} - {len(description)} chars&quot;)

# 22.79s : Australia - 2147 chars
# 23.64s : Brazil - 2202 chars
# 24.67s : Chile - 2186 chars"><pre><span>from</span> <span>time</span> <span>import</span> <span>time</span>

<span>countries</span> <span>=</span> [<span>"Australia"</span>, <span>"Brazil"</span>, <span>"Chile"</span>]


<span># Generate the descriptions one at a time</span>
<span>start_time</span> <span>=</span> <span>time</span>()
<span>for</span> <span>country</span> <span>in</span> <span>countries</span>:
    <span># Converting `StreamedStr` to `str` blocks until the LLM output is fully generated</span>
    <span>description</span> <span>=</span> <span>str</span>(<span>describe_country</span>(<span>country</span>))
    <span>print</span>(<span>f"<span><span>{</span><span>time</span>() <span>-</span> <span>start_time</span>:.2f<span>}</span></span>s : <span><span>{</span><span>country</span><span>}</span></span> - <span><span>{</span><span>len</span>(<span>description</span>)<span>}</span></span> chars"</span>)

<span># 22.72s : Australia - 2130 chars</span>
<span># 41.63s : Brazil - 1884 chars</span>
<span># 74.31s : Chile - 2968 chars</span>


<span># Generate the descriptions concurrently by creating the StreamedStrs at the same time</span>
<span>start_time</span> <span>=</span> <span>time</span>()
<span>streamed_strs</span> <span>=</span> [<span>describe_country</span>(<span>country</span>) <span>for</span> <span>country</span> <span>in</span> <span>countries</span>]
<span>for</span> <span>country</span>, <span>streamed_str</span> <span>in</span> <span>zip</span>(<span>countries</span>, <span>streamed_strs</span>):
    <span>description</span> <span>=</span> <span>str</span>(<span>streamed_str</span>)
    <span>print</span>(<span>f"<span><span>{</span><span>time</span>() <span>-</span> <span>start_time</span>:.2f<span>}</span></span>s : <span><span>{</span><span>country</span><span>}</span></span> - <span><span>{</span><span>len</span>(<span>description</span>)<span>}</span></span> chars"</span>)

<span># 22.79s : Australia - 2147 chars</span>
<span># 23.64s : Brazil - 2202 chars</span>
<span># 24.67s : Chile - 2186 chars</span></pre></div>
<h3 tabindex="-1" id="user-content-object-streaming" dir="auto"><a href="#object-streaming">Object Streaming</a></h3>
<p dir="auto">Structured outputs can also be streamed from the LLM by using the return type annotation <code>Iterable</code> (or <code>AsyncIterable</code>). This allows each item to be processed while the next one is being generated. See the example in <a href="https://github.com/jackmpcollins/magentic/blob/main/examples/quiz">examples/quiz</a> for how this can be used to improve user experience by quickly displaying/using the first item returned.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from collections.abc import Iterable
from time import time

from magentic import prompt
from pydantic import BaseModel


class Superhero(BaseModel):
    name: str
    age: int
    power: str
    enemies: list[str]


@prompt(&quot;Create a Superhero team named {name}.&quot;)
def create_superhero_team(name: str) -> Iterable[Superhero]:
    ...


start_time = time()
for hero in create_superhero_team(&quot;The Food Dudes&quot;):
    print(f&quot;{time() - start_time:.2f}s : {hero}&quot;)

# 2.23s : name='Pizza Man' age=30 power='Can shoot pizza slices from his hands' enemies=['The Hungry Horde', 'The Junk Food Gang']
# 4.03s : name='Captain Carrot' age=35 power='Super strength and agility from eating carrots' enemies=['The Sugar Squad', 'The Greasy Gang']
# 6.05s : name='Ice Cream Girl' age=25 power='Can create ice cream out of thin air' enemies=['The Hot Sauce Squad', 'The Healthy Eaters']"><pre><span>from</span> <span>collections</span>.<span>abc</span> <span>import</span> <span>Iterable</span>
<span>from</span> <span>time</span> <span>import</span> <span>time</span>

<span>from</span> <span>magentic</span> <span>import</span> <span>prompt</span>
<span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>


<span>class</span> <span>Superhero</span>(<span>BaseModel</span>):
    <span>name</span>: <span>str</span>
    <span>age</span>: <span>int</span>
    <span>power</span>: <span>str</span>
    <span>enemies</span>: <span>list</span>[<span>str</span>]


<span>@<span>prompt</span>(<span>"Create a Superhero team named {name}."</span>)</span>
<span>def</span> <span>create_superhero_team</span>(<span>name</span>: <span>str</span>) <span>-&gt;</span> <span>Iterable</span>[<span>Superhero</span>]:
    ...


<span>start_time</span> <span>=</span> <span>time</span>()
<span>for</span> <span>hero</span> <span>in</span> <span>create_superhero_team</span>(<span>"The Food Dudes"</span>):
    <span>print</span>(<span>f"<span><span>{</span><span>time</span>() <span>-</span> <span>start_time</span>:.2f<span>}</span></span>s : <span><span>{</span><span>hero</span><span>}</span></span>"</span>)

<span># 2.23s : name='Pizza Man' age=30 power='Can shoot pizza slices from his hands' enemies=['The Hungry Horde', 'The Junk Food Gang']</span>
<span># 4.03s : name='Captain Carrot' age=35 power='Super strength and agility from eating carrots' enemies=['The Sugar Squad', 'The Greasy Gang']</span>
<span># 6.05s : name='Ice Cream Girl' age=25 power='Can create ice cream out of thin air' enemies=['The Hot Sauce Squad', 'The Healthy Eaters']</span></pre></div>
<h3 tabindex="-1" id="user-content-asyncio" dir="auto"><a href="#asyncio">Asyncio</a></h3>
<p dir="auto">Asynchronous functions / coroutines can be used to concurrently query the LLM. This can greatly increase the overall speed of generation, and also allow other asynchronous code to run while waiting on LLM output. In the below example, the LLM generates a description for each US president while it is waiting on the next one in the list. Measuring the characters generated per second shows that this example achieves a 7x speedup over serial processing.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
from time import time
from typing import AsyncIterable

from magentic import prompt


@prompt(&quot;List ten presidents of the United States&quot;)
async def iter_presidents() -> AsyncIterable[str]:
    ...


@prompt(&quot;Tell me more about {topic}&quot;)
async def tell_me_more_about(topic: str) -> str:
    ...


# For each president listed, generate a description concurrently
start_time = time()
tasks = []
async for president in await iter_presidents():
    # Use asyncio.create_task to schedule the coroutine for execution before awaiting it
    # This way descriptions will start being generated while the list of presidents is still being generated
    task = asyncio.create_task(tell_me_more_about(president))
    tasks.append(task)

descriptions = await asyncio.gather(*tasks)

# Measure the characters per second
total_chars = sum(len(desc) for desc in descriptions)
time_elapsed = time() - start_time
print(total_chars, time_elapsed, total_chars / time_elapsed)
# 24575 28.70 856.07


# Measure the characters per second to describe a single president
start_time = time()
out = await tell_me_more_about(&quot;George Washington&quot;)
time_elapsed = time() - start_time
print(len(out), time_elapsed, len(out) / time_elapsed)
# 2206 18.72 117.78"><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>time</span> <span>import</span> <span>time</span>
<span>from</span> <span>typing</span> <span>import</span> <span>AsyncIterable</span>

<span>from</span> <span>magentic</span> <span>import</span> <span>prompt</span>


<span>@<span>prompt</span>(<span>"List ten presidents of the United States"</span>)</span>
<span>async</span> <span>def</span> <span>iter_presidents</span>() <span>-&gt;</span> <span>AsyncIterable</span>[<span>str</span>]:
    ...


<span>@<span>prompt</span>(<span>"Tell me more about {topic}"</span>)</span>
<span>async</span> <span>def</span> <span>tell_me_more_about</span>(<span>topic</span>: <span>str</span>) <span>-&gt;</span> <span>str</span>:
    ...


<span># For each president listed, generate a description concurrently</span>
<span>start_time</span> <span>=</span> <span>time</span>()
<span>tasks</span> <span>=</span> []
<span>async</span> <span>for</span> <span>president</span> <span>in</span> <span>await</span> <span>iter_presidents</span>():
    <span># Use asyncio.create_task to schedule the coroutine for execution before awaiting it</span>
    <span># This way descriptions will start being generated while the list of presidents is still being generated</span>
    <span>task</span> <span>=</span> <span>asyncio</span>.<span>create_task</span>(<span>tell_me_more_about</span>(<span>president</span>))
    <span>tasks</span>.<span>append</span>(<span>task</span>)

<span>descriptions</span> <span>=</span> <span>await</span> <span>asyncio</span>.<span>gather</span>(<span>*</span><span>tasks</span>)

<span># Measure the characters per second</span>
<span>total_chars</span> <span>=</span> <span>sum</span>(<span>len</span>(<span>desc</span>) <span>for</span> <span>desc</span> <span>in</span> <span>descriptions</span>)
<span>time_elapsed</span> <span>=</span> <span>time</span>() <span>-</span> <span>start_time</span>
<span>print</span>(<span>total_chars</span>, <span>time_elapsed</span>, <span>total_chars</span> <span>/</span> <span>time_elapsed</span>)
<span># 24575 28.70 856.07</span>


<span># Measure the characters per second to describe a single president</span>
<span>start_time</span> <span>=</span> <span>time</span>()
<span>out</span> <span>=</span> <span>await</span> <span>tell_me_more_about</span>(<span>"George Washington"</span>)
<span>time_elapsed</span> <span>=</span> <span>time</span>() <span>-</span> <span>start_time</span>
<span>print</span>(<span>len</span>(<span>out</span>), <span>time_elapsed</span>, <span>len</span>(<span>out</span>) <span>/</span> <span>time_elapsed</span>)
<span># 2206 18.72 117.78</span></pre></div>
<h3 tabindex="-1" id="user-content-additional-features" dir="auto"><a href="#additional-features">Additional Features</a></h3>
<ul dir="auto">
<li>The <code>functions</code> argument to <code>@prompt</code> can contain async/coroutine functions. When the corresponding <code>FunctionCall</code> objects are called the result must be awaited.</li>
<li>The <code>Annotated</code> type annotation can be used to provide descriptions and other metadata for function parameters. See <a href="https://docs.pydantic.dev/latest/usage/validation_decorator/#using-field-to-describe-function-arguments" rel="nofollow">the pydantic documentation on using <code>Field</code> to describe function arguments</a>.</li>
<li>The <code>@prompt</code> and <code>@prompt_chain</code> decorators also accept a <code>model</code> argument. You can pass an instance of <code>OpenaiChatModel</code> (from <code>magentic.chat_model.openai_chat_model</code>) to use GPT4 or configure a different temperature.</li>
</ul>
<h2 tabindex="-1" id="user-content-configuration" dir="auto"><a href="#configuration">Configuration</a></h2>
<p dir="auto">The order of precedence of configuration is</p>
<ol dir="auto">
<li>Arguments passed when initializing an instance in Python</li>
<li>Environment variables</li>
</ol>
<p dir="auto">The following environment variables can be set.</p>
<table>
<thead>
<tr>
<th>Environment Variable</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>MAGENTIC_OPENAI_MODEL</td>
<td>OpenAI model</td>
<td>gpt-4</td>
</tr>
<tr>
<td>MAGENTIC_OPENAI_TEMPERATURE</td>
<td>OpenAI temperature</td>
<td>0.5</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" id="user-content-type-checking" dir="auto"><a href="#type-checking">Type Checking</a></h2>
<p dir="auto">Many type checkers will raise warnings or errors for functions with the <code>@prompt</code> decorator due to the function having no body or return value. There are several ways to deal with these.</p>
<ol dir="auto">
<li>Disable the check globally for the type checker. For example in mypy by disabling error code <code>empty-body</code>.
<div dir="auto" data-snippet-clipboard-copy-content="# pyproject.toml
[tool.mypy]
disable_error_code = [&quot;empty-body&quot;]"><pre><span><span>#</span> pyproject.toml</span>
[<span>tool</span>.<span>mypy</span>]
<span>disable_error_code</span> = [<span><span>"</span>empty-body<span>"</span></span>]</pre></div>
</li>
<li>Make the function body <code>...</code> (this does not satisfy mypy) or <code>raise</code>.
<div dir="auto" data-snippet-clipboard-copy-content="@prompt(&quot;Choose a color&quot;)
def random_color() -> str:
    ..."><pre><span>@<span>prompt</span>(<span>"Choose a color"</span>)</span>
<span>def</span> <span>random_color</span>() <span>-&gt;</span> <span>str</span>:
    ...</pre></div>
</li>
<li>Use comment <code># type: ignore[empty-body]</code> on each function. In this case you can add a docstring instead of <code>...</code>.
<div dir="auto" data-snippet-clipboard-copy-content="@prompt(&quot;Choose a color&quot;)
def random_color() -> str:  # type: ignore[empty-body]
    &quot;&quot;&quot;Returns a random color.&quot;&quot;&quot;"><pre><span>@<span>prompt</span>(<span>"Choose a color"</span>)</span>
<span>def</span> <span>random_color</span>() <span>-&gt;</span> <span>str</span>:  <span># type: ignore[empty-body]</span>
    <span>"""Returns a random color."""</span></pre></div>
</li>
</ol>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ollama for Linux – Run LLMs on Linux with GPU Acceleration (161 pts)]]></title>
            <link>https://github.com/jmorganca/ollama/releases/tag/v0.1.0</link>
            <guid>37661755</guid>
            <pubDate>Tue, 26 Sep 2023 16:29:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/jmorganca/ollama/releases/tag/v0.1.0">https://github.com/jmorganca/ollama/releases/tag/v0.1.0</a>, See on <a href="https://news.ycombinator.com/item?id=37661755">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pjax="true" data-test-selector="body-content" data-view-component="true"><h2>Ollama for Linux</h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/jmorganca/ollama/assets/251292/89f8526e-866a-4e19-a73c-3ff850d45c76"><img src="https://github.com/jmorganca/ollama/assets/251292/89f8526e-866a-4e19-a73c-3ff850d45c76" height="220"></a></p>
<p>Ollama for Linux is now available, with GPU acceleration enabled out-of-the-box for Nvidia GPUs.</p>
<p>💯 Ollama will run on cloud servers with multiple GPUs attached<br>
🤖 Ollama will run on WSL 2 with GPU support<br>
😍 Ollama maximizes the number of GPU layers to load to increase performance without crashing<br>
🤩 Ollama will support CPU only, and small hobby gaming GPUs to super powerful workstation graphics cards like the H100</p>
<h3>Download</h3>
<div data-snippet-clipboard-copy-content="curl https://ollama.ai/install.sh | sh"><pre><code>curl https://ollama.ai/install.sh | sh
</code></pre></div>
<p>Manual <a href="https://github.com/jmorganca/ollama/blob/main/docs/linux.md">install steps</a> are also available.</p>
<h2>Changelog</h2>
<ul>
<li>Ollama will now automatically offload as much of the running model as is supported by your GPU for maximum performance without any crashes</li>
<li>Fix issue where characters would be erased when running <code>ollama run</code></li>
<li>Added a new community project by <a data-hovercard-type="user" data-hovercard-url="/users/TwanLuttik/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/TwanLuttik">@TwanLuttik</a> in <a data-error-text="Failed to load title" data-id="1909265837" data-permission-text="Title is private" data-url="https://github.com/jmorganca/ollama/issues/574" data-hovercard-type="pull_request" data-hovercard-url="/jmorganca/ollama/pull/574/hovercard" href="https://github.com/jmorganca/ollama/pull/574">#574</a></li>
</ul>
<h2>New Contributors</h2>
<ul>
<li><a data-hovercard-type="user" data-hovercard-url="/users/TwanLuttik/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/TwanLuttik">@TwanLuttik</a> made their first contribution in <a data-error-text="Failed to load title" data-id="1909265837" data-permission-text="Title is private" data-url="https://github.com/jmorganca/ollama/issues/574" data-hovercard-type="pull_request" data-hovercard-url="/jmorganca/ollama/pull/574/hovercard" href="https://github.com/jmorganca/ollama/pull/574">#574</a></li>
</ul>
<p><strong>Full Changelog</strong>: <a href="https://github.com/jmorganca/ollama/compare/v0.0.21...v0.1.0"><tt>v0.0.21...v0.1.0</tt></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[9th Circuit rejects TSA claim of impunity for checkpoint staff who rape traveler (240 pts)]]></title>
            <link>https://papersplease.org/wp/2023/06/26/9th-circuit-rejects-tsa-claim-of-impunity-for-checkpoint-staff-who-rape-travelers/</link>
            <guid>37661609</guid>
            <pubDate>Tue, 26 Sep 2023 16:21:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://papersplease.org/wp/2023/06/26/9th-circuit-rejects-tsa-claim-of-impunity-for-checkpoint-staff-who-rape-travelers/">https://papersplease.org/wp/2023/06/26/9th-circuit-rejects-tsa-claim-of-impunity-for-checkpoint-staff-who-rape-travelers/</a>, See on <a href="https://news.ycombinator.com/item?id=37661609">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="primary" role="main">	
			
<article id="post-17978">
			
	
	<p><span>Jun</span>
		<span>26</span>
		<span>2023</span>
	</p>
		
	<div>
		<p>Last December, we <a href="https://papersplease.org/wp/2022/12/06/tsa-argues-for-impunity-for-checkpoint-staff-who-rape-travelers/">attended and reported on oral argument before the 9th Circuit Court of Appeals</a> in a case in which the Transportation Security Administration (TSA) argued that TSA checkpoint staff have absolute immunity from lawsuits for assault, even sexual assault or rape, committed against travelers they are “screening”.</p>
<p>We’re pleased to report that today the 9th Circuit panel of judges <a href="https://cdn.ca9.uscourts.gov/datastore/opinions/2023/06/26/22-15402.pdf">rejected the TSA’s claim of impunity</a>. The three judges found unanimously that the Federal Tort Clams Act (FTCA) allows lawsuits against the TSA for damages caused by checkpoint staff who assault travelers. The 9th Circuit thus joins every other Circuit Court of Appeals (the <a href="https://papersplease.org/wp/2019/08/30/3rd-circuit-finds-tsa-checkpoint-staff-conduct-searches-and-can-be-sued-for-misconduct/">3rd</a>, <a href="https://papersplease.org/wp/2023/04/18/4th-circuit-agrees-that-tsa-checkpoint-staff-are-liable-for-assault/">4th</a>, and <a href="https://papersplease.org/wp/2020/08/31/8th-circuit-finds-tsa-agents-can-be-liable-for-assault/">8th</a>) to have addressed this issue in a published opinion.</p>
<p>The case decided today by the 9th Circuit will now return to the U.S. District Court in Las Vegas for much-belated consideration of the claim against the TSA and its officers. The precedent set by today’s decision will apply <a href="https://www.ca9.uscourts.gov/information/circuit-map/">throughout the 9th Circuit</a>, the largest of the Federal judicial circuits, including all of the states on the West Coast.</p>
<p>Kudos to <a href="http://www.corbettrights.com/">Jonathan Corbett, Esq.</a>, who has represented the plaintiffs in each of these cases.&nbsp; Coals for Christmas to the TSA for continuing to argue for impunity for its staff to one Circuit Court after another, despite the growing weight of precedent against the agency and, perhaps more importantly, the moral repugnance of arguing that any agents of the government should be entitled to assault or rape members of the public with impunity.</p>
			</div>
</article><!-- #post-17978 -->
	<!-- #nav-below -->
	

	<!-- #comments .comments-area -->
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FTC Sues Amazon for Illegally Maintaining Monopoly Power (969 pts)]]></title>
            <link>https://www.ftc.gov/news-events/news/press-releases/2023/09/ftc-sues-amazon-illegally-maintaining-monopoly-power</link>
            <guid>37661446</guid>
            <pubDate>Tue, 26 Sep 2023 16:10:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ftc.gov/news-events/news/press-releases/2023/09/ftc-sues-amazon-illegally-maintaining-monopoly-power">https://www.ftc.gov/news-events/news/press-releases/2023/09/ftc-sues-amazon-illegally-maintaining-monopoly-power</a>, See on <a href="https://news.ycombinator.com/item?id=37661446">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The Federal Trade Commission and 17 state attorneys general today sued Amazon.com, Inc. alleging that the online retail and technology company is a monopolist that uses a set of interlocking anticompetitive and unfair strategies to illegally maintain its monopoly power. The FTC and its state partners say Amazon’s actions allow it to stop rivals and sellers from lowering prices, degrade quality for shoppers, overcharge sellers, stifle innovation, and prevent rivals from fairly competing against Amazon. &nbsp;</p>

<p>The complaint alleges that Amazon violates the law not because it is big, but because it engages in a course of exclusionary conduct that prevents current competitors from growing and new competitors from emerging. By stifling competition on price, product selection, quality, and by preventing its current or future rivals from attracting a critical mass of shoppers and sellers, Amazon ensures that no current or future rival can threaten its dominance. Amazon’s far-reaching schemes impact hundreds of billions of dollars in retail sales every year, touch hundreds of thousands of products sold by businesses big and small and affect over a hundred million shoppers.&nbsp;</p>

<p>“Our complaint lays out how Amazon has used a set of punitive and coercive tactics to unlawfully maintain its monopolies,” said FTC Chair Lina M. Khan. “The complaint sets forth detailed allegations noting how Amazon is now exploiting its monopoly power to enrich itself while raising prices and degrading service for the tens of millions of American families who shop on its platform and the hundreds of thousands of businesses that rely on Amazon to reach them. Today’s lawsuit seeks to hold Amazon to account for these monopolistic practices and restore the lost promise of free and fair competition.”</p>

<p>“We’re bringing this case because Amazon’s illegal conduct has stifled competition across a huge swath of the online economy.&nbsp;Amazon is a monopolist that uses its power to hike prices on American shoppers and charge sky-high fees on hundreds of thousands of online sellers,” said John Newman, Deputy Director of the FTC’s Bureau of Competition.&nbsp;“Seldom in the history of U.S. antitrust law has one case had the potential to do so much good for so many people.”</p>

<p>The FTC and states allege Amazon’s anticompetitive conduct occurs in two markets—the online superstore market that serves shoppers and the market for online marketplace services purchased by sellers. These tactics include:</p>

<ul><li>Anti-discounting measures that punish sellers and deter other online retailers from offering prices lower than Amazon, keeping prices higher for products across the internet.&nbsp;For example, if Amazon discovers that a seller is offering lower-priced goods elsewhere, Amazon can bury discounting sellers so far down in Amazon’s search results that they become effectively invisible.</li>
<li>Conditioning sellers’ ability to obtain “Prime” eligibility for their products—a virtual necessity for doing business on Amazon—on sellers using Amazon’s costly fulfillment service, which has made it substantially more expensive for sellers on Amazon to also offer their products on other platforms. This unlawful coercion has in turn limited competitors’ ability to effectively compete against Amazon.</li>
</ul><p>Amazon’s illegal, exclusionary conduct makes it impossible for competitors to gain a foothold. With its amassed power across both the online superstore market and online marketplace services market, Amazon extracts enormous monopoly rents from everyone within its reach. This includes:</p>

<ul><li>Degrading the customer experience by replacing relevant, organic search results with paid advertisements—and deliberately increasing junk ads that worsen search quality and frustrate both shoppers seeking products and sellers who are promised a return on their advertising purchase.</li>
<li>Biasing Amazon’s search results to preference Amazon’s own products over ones that Amazon knows are of better quality.&nbsp;</li>
<li>Charging costly fees on the hundreds of thousands of sellers that currently have no choice but to rely on Amazon to stay in business. These fees range from a monthly fee sellers must pay for each item sold, to advertising fees that have become virtually necessary for sellers to do business. Combined, all of these fees force many sellers to pay close to 50% of their total revenues to Amazon. These fees harm not only sellers but also shoppers, who pay increased prices for thousands of products sold on or off Amazon. &nbsp;</li>
</ul><p>The FTC, along with its state partners, are seeking a permanent injunction in federal court that would prohibit Amazon from engaging in its unlawful conduct and pry loose Amazon’s monopolistic control to restore competition.<s> </s></p>

<p>Connecticut, Delaware, Maine, Maryland, Massachusetts, Michigan, Minnesota, New Jersey, New Hampshire, New Mexico, Nevada, New York, Oklahoma, Oregon, Pennsylvania, Rhode Island, and Wisconsin joined the Commission’s lawsuit. The Commission vote to authorize staff to file for a permanent injunction and other equitable relief in the U.S. District Court for the Western District of Washington was 3-0.</p>

<p><strong>NOTE:&nbsp;</strong>The Commission issues a complaint when it has “reason to believe” that the law has been or is being violated, and it appears to the Commission that a proceeding is in the public interest.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yet another E-Ink weather display – but with Rust (182 pts)]]></title>
            <link>https://harrystern.net/halldisplay.html</link>
            <guid>37661387</guid>
            <pubDate>Tue, 26 Sep 2023 16:06:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://harrystern.net/halldisplay.html">https://harrystern.net/halldisplay.html</a>, See on <a href="https://news.ycombinator.com/item?id=37661387">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p><span><time datetime="2023-09-24T00:00:00-04:00">Sun 24 September 2023</time></span>
   	  <span> - 12 min </span>
          </p>
        </div><section id="content">
          <p>I made one of those e-ink weather displays that you see on tech blogs and hacker news sometimes. It can display the current weather, the temperature and precipitation forecast for the rest of the week, and my current and upcoming tasks from Todoist. It's powered by a couple NiMH AA batteries and lasts at least a couple months on a single charge, if not more.</p>
<p><img alt="hall display" src="https://harrystern.net/images/halldisplay.png"></p>
<p>The code can be found <a href="https://github.com/boustrophedon/eink-esp-weather-display">on my github</a>.</p>
<h2>Hardware</h2>
<h2>E-ink display</h2>
<p>The screen is a <a href="https://www.waveshare.com/product/7.5inch-e-paper-b.htm">7.5", 800 by 480 pixel, 3-color e-ink display</a> that I purchased from waveshare. It does black, white, and red, which is slightly unusual for e-ink and increases the price and lowers the refresh rate, but looks really nice. I believe it works (skipping over how e-ink works in general) by first pushing the black particles to the front, and then pushing the red particles, which are maybe smaller or lighter or less dense, further in front of the black ones.</p>
<h2>Microcontroller</h2>
<p>I bought a <a href="https://www.waveshare.com/e-paper-esp32-driver-board.htm">pre-assembled ESP32 board</a>, also from waveshare, which contains the FPC connector already assembled and connected to the ESP32 module's SPI pins.</p>
<p>I desoldered the on-board LEDs to increase battery life, since the power LED was always on, and the other led seemed to be slightly receiving/drawing power during sleep. I was able to turn it off in software by configuring an internal pulldown to stay on during sleep but I wasn't sure if that would also draw some current so I just removed it since I wasn't using it.</p>
<h2>Software</h2>
<p>This project consists of two different Rust projects.</p>
<p>There's the code that runs on the esp32 board connected to the display, and then there's also code running on a server that both gathers the data and renders it to a file. Originally I was going to do everything on-device, but since the display is battery-powered I thought it would be more efficient to download a single file and display it. So all the API requests, graph drawing, and text rendering operations happen on the server, and the display board effectively acts as a dumb terminal which just displays the data it receives.</p>
<h2>On-device software</h2>
<h3>ESP32 overview</h3>
<p>For the "firmware" running on the esp32 board, I'm using esp-idf with the standard library (i.e. not no-std) via the esp-rs project.</p>
<p>Setup is a tiny bit complicated, but <a href="https://esp-rs.github.io/book/overview/using-the-standard-library.html">the esp-rs book</a> is a great guide and explains everything fairly clearly. Hopefully one day it will be as simple as "edit your runner in .cargo/config.toml to use espflash and then just <code>cargo run</code>" although I think for RISC-V boards with no-std it might be pretty close already. I added a Justfile that exports the libclang and esp toolchain environment variables internally so that I can <code>just build</code> and <code>just run</code>.</p>
<p>The esp32-std embedded ecosystem is comprised of several different crates, including:
- <a href="https://crates.io/crates/esp-idf-sys">esp-idf-sys</a>, which contains bindgen bindings to the <a href="https://github.com/espressif/esp-idf">esp-idf C API</a>
- <a href="https://crates.io/crates/esp-idf-hal">esp-idf-hal</a>, which contains higher-level, type-safe wrappers and drivers for hardware like GPIO and SPI 
- <a href="https://crates.io/crates/esp-idf-svc">esp-idf-svc</a>, which contains implementations and wrappers for system services like Wifi and storage</p>
<p>Additionally, the above crates use and implement traits from the following embedded-rust ecosystem crates:
- <a href="https://crates.io/crates/embedded-hal">embedded-hal</a>
- <a href="https://crates.io/crates/embedded-svc">embedded-svc</a></p>
<p>and several other setup utility binaries like espup, espflash, and embuild, in addition to a fork of the rust compiler for the Xtensa/ESP32 architecture (there are also RISC-V ESP32 processors which don't require the forked compiler and llvm). </p>
<p>I mostly read example code, the various projects' mentioned above documentation, and the esp-idf documentation to figure out how to put everything together. In particular this repo is fairly extensive: <a href="https://github.com/ivmarkov/rust-esp32-std-demo">https://github.com/ivmarkov/rust-esp32-std-demo</a></p>
<p>It can be somewhat confusing to figure out which types you need from which crates to e.g. turn on wifi, but this isn't unique to esp32 or embedded rust. Libraries will take traits from embedded-hal/svc as impl parameters and it can sometimes be difficult to figure out how to instantiate concrete versions of those types. Typically the sample code is useful in those cases to get things started.</p>
<p>Overall, the code is pretty simple.</p>
<ol>
<li>We "gather" the peripherals we need - spi, the modem for wifi, and the gpio pin used to wake up via button press</li>
<li>Turn on the wifi</li>
<li>Get the display data from the server</li>
<li>Turn off the wifi</li>
<li>Send the data to the display and wait until we expect it's done</li>
<li>Tell the e-ink display to go to sleep</li>
<li>Tell the device to sleep for 90 minutes or until the button is pressed</li>
</ol>
<p>The URL to request the data from and the wifi SSID and password are just constants in a <code>src/config.rs</code> file which isn't checked in to git. This gets baked into the final binary.</p>
<h3>Getting the data</h3>
<p>To request the display data from the server, we use the <code>esp_idf_svc::http</code> and <code>embedded_svc::http</code> modules.</p>
<p>There really isn't too much to the code - it's a pretty standard http request. The only thing of note is that we explicitly check that we're getting the right size file back from the server to fit the display.</p>
<h3>Waveshare e-ink driver</h3>
<p>To send the image data to the display, we use the <a href="https://crates.io/crates/epd-waveshare">epd-waveshare</a> crate, but because the last release was published to crates.io 2 years ago, we have to use the git repo url directly in our Cargo.toml file. My display uses the "epd7in5b_v2" driver. The sticker on the back says v3 but everything seems to work.</p>
<p>Other than that everything just works - you set up the SPI device, pass it to the library and give it the image data. It doesn't seem to wait for the display to actually finish updating, so we wait for about 20 seconds before sending a sleep signal to the display, which lowers its power consumption. Since it's an e-ink display, the image continues to be displayed even during sleep, of course.</p>
<blockquote>
<p>Aside: The memory layout for the display data is just the raw bits bitpacked into bytes. The red pixels are handled by simply having two separate buffers packed next to each other. This is inefficient in terms of space - we could instead do a variable length encoding scheme where say 0 is white, 10 is black and 11 is red. However, this would make it significantly slower to do standard drawing operations on because individual pixel access becomes O(n).</p>
</blockquote>
<h2>Rendering software</h2>
<p>The rendering code was by far the most interesting. Everything is drawn with the <a href="https://crates.io/crates/imageproc">imageproc crate</a>, which uses the <a href="https://crates.io/crates/image">image crate</a> and the <a href="https://crates.io/crates/rusttype">rusttype crate</a> for fonts. Additionally, it uses the same <a href="https://crates.io/crates/epd-waveshare">epd_waveshare</a> crate and also the <a href="https://crates.io/crates/embedded_graphics">embedded_graphics</a> crate to pack the image buffer into the format used by the display driver.</p>
<p>So again, the strategy here is to draw the image into a generic image buffer, and then "render" it into the driver display library's buffer type as if the code were running directly on the device. Then as above the esp32 downloads the file and sends it directly to the display without having to do any extra work.</p>
<h3>Gathering the data</h3>
<p>I'm using <a href="https://www.weather.gov/documentation/services-web-api">weather.gov's free API</a> for the weather data, which requires just a couple HTTP requests to get the current weather and the forecast. To figure out the station and gridpoint parameters for the requests, you'll need to make a couple requests manually to some other endpoints that take GPS coordinates and return the nearest stations and gridpoint.</p>
<p>The todo list data is just a single API request with a filter of -24h to +48h on the due date.</p>
<h3>Drawing and measuring text</h3>
<p>While imageproc has a built-in function for drawing text, I had to write several variations of wrappers to get different alignments. The <code>measure_text</code> function I'm pretty sure was taken from example code somewhere in rusttype or imageproc.</p>
<h3>Drawing the graphs</h3>
<p>Imageproc's line drawing methods are just basic Bresenham and have no thickness parameter, so to get thick lines I just drew them with the start and end points offset vertically by a pixel. Note that this obviously does not work for perfectly straight lines and mathematically you'd want to offset by the vector perpendicular to the original line, but since the graph is mostly horizontal it works fine for this case.</p>
<p>Regarding the precipitation graph, it was a fair amount of trial and error to get it looking the way I imagined it. The code is a little obscure and I'm pretty sure it can be simplified. The idea is that we just split the graph into 6x6 squares, turn on some pixels on the diagonal, and then offset the squares themselves. The last part is the part that I think could be simplified (or at least better explained) and is expressed in the <code>xm</code> variable in the code.</p>
<p>I really love how the thin lines in the precipitation graph really make it look blue even though it's the same black as the rest of the image.</p>
<h3>Dithering and "rendering" to the display buffer</h3>
<p>Since the display can only handle 3 colors but the text is rendered with anti-aliasing (and I couldn't find an easy way to not anti-alias) I ended up using the image crate's dithering functionality with just a manually-tuned cutoff point that looked good with the font and the display. Fortunately e-ink displays are a bit fuzzy due to their construction, so combined with the curvy font I chose it's very hard to see any artifacts without looking very closely.</p>
<p>After the final image is dithered to white/black/red, we use the same exact crate that's used in the firmware, but this time to write the buffer instead ofread it. All we have to do is enumerate over the pixels in the image and call <code>set_pixel</code> inside the buffer, and the crate takes care of doing the bitpacking for us. It could be done more efficiently, but since we're not running on the device it doesn't matter as much.</p>
<h3>Running the rendering software</h3>
<p>Unlike the firmware, the rendering software takes an actual json-formatted config file as a cli parameter, along with the location of the output file.</p>
<p>The rendering binary gets run every hour via a systemd user service and timer file which can be found in the scripts/ directory. In my case, the output file gets put into a directory which nginx is configured to serve static files from.</p>
<h3>Extrasafe</h3>
<p>I used my own <a href="https://github.com/boustrophedon/extrasafe">extrasafe crate</a> in the rendering software on the server. It allows you to restrict your software's syscall usage to a subset of your choosing <a href="https://man7.org/linux/man-pages/man2/seccomp.2.html">via seccomp.</a> We start one thread to make the HTTP requests to all the APIs, then pass that raw JSON data to another thread with even less privileges to do the parsing, and finally pass that back to the main thread, where we then do our final restriction that only allows us to write to the output file. I think ideally we would first spawn the threads, wait until the original thread is restricted, and <em>then</em> run the other threads, but with more than one worker thread in the sequence it becomes difficult to organize everything manually.</p>
<p>I'm currently working on improvements to extrasafe to allow the use of Landlock and also maybe a helper function for the <code>unshare</code> syscall. In particular I'd like to restrict the network thread's filesystem access to only the necessary files for DNS and SSL, rather than all files.</p>
<h2>The Case</h2>
<p>I learned how to use CAD software for this project and it was simpler than I thought it would be.</p>
<p>First, you select a plane to draw on, which can be either a plane along two major axes, or a plane formed by part of your model that you've drawn alreday. Then you can draw 2D shapes on that plane with standard 2D curve tools like lines, bezier curves, conics, etc. Once you've got the parts of your shape in place, you then need to constrain the shape. Here "constrain" means to lock in the position, dimension, and other parameters such as angles, radii, control point position, which you do with operations like "these two points are concurrent", "these two points are symmetric about this line", "these two lines are perpendicular", "the length of this line is 5mm", "the angle between these two lines is 30 degrees". Finally, back in "3D mode", you can use tools to extrude, cut out pockets, revolve, or otherwise 3d-ify the 2d sketches you drew.</p>
<p>Additionally, you can make a spreadsheet or mapping of labeled dimensions and use them in the 2d sketches or extrusions in place of using specific dimensions like "5mm" and the model will automatically be updated when you change the values. Overall it's actually pretty fun, although maybe a bit tedious.</p>
<p>I tried out both freecad and onshape, and freecad seemed a bit too easy to get into a buggy state but I otherwise liked the constraint interface. In particular one tiny thing I liked about freecad was that the sketch turned a very visible green when it's fully constrained. Onshape's blue/black scheme is pretty low contrast and can be hard to see, especially when using flux or redshift. Maybe I just missed an external indicator that said whether the sketch was fully constrained or not. The built-in variables table in onshape was simpler than freecad, where you have to use spreadsheet workbench or a separate plugin, and even then I'm pretty sure you have to type <code>spreadsheet.&lt;name&gt;</code> or <code>dd.dd&lt;name&gt;</code> whenever you want to use a variable inside a sketch. I also didn't see a way in onshape to see a list of constraints like you can see in freecad.</p>
<p>I had the case <a href="https://3d.jlcpcb.com/3d-printing-quote">printed by JLCPCB</a> in white resin via SLA for about $20 - $30 USD shipped and it was both really easy and it came out pretty clean. I had to do a second revision because I messed up with the bezel size (accidentally made it symmetric rather than the bottom having a larger lip) and the second order came out just as good or better than the first. <a href="https://cad.onshape.com/documents/7335050833c9b0394faa498c/w/53cba50a0cfeb1391b6847f2/e/390ae9e3c38a9286a52c2ae4">Here's the oncad project.</a></p>
<p>I couldn't find any cheap 3D printing services in the US to do the print for me - I guess they either get outcompeted on labor costs by China or there isn't demand for them on a hobbyist level because hobbyists can buy their own or use a makerspace's printer?</p>
<p>The case is mounted on the wall with regular Command picture hanging strips - I designed pads on the backside of the case specifically for this purpose.</p>
<h2>Things I would like to improve</h2>
<ul>
<li>Case snap-fit for pcb, maybe for display as well/or just supports<ul>
<li>I'm not really sure what's possible for resin in terms of both printability and flexibility / thin parts</li>
</ul>
</li>
<li>Custom PCB with a lower quiescient current voltage regulator for improved battery life? Plus on-board battery holder.</li>
<li>The weather.gov API is occasionally flakey or provides bad JSON, maybe add a retry option via the systemd unit file</li>
<li>Add some code to note if the image from the server hasn't been updated (either via comparing to an RTC or storing the last successful update time in flash) and then maybe draw a red dot or line in the top corner or somewhere. This would be equivalent to just setting one or a couple bytes to all 1s in the binary, so it wouldn't be very difficult.</li>
<li>Minor issues with graph data</li>
<li>Make daily high/lows red and smaller</li>
<li>Show daily high/lows for current day</li>
<li>Measure current draw accurately</li>
<li>Current ranger / ucurrent gold / borrow one from somewhere?</li>
<li>Better test the renderer</li>
<li>Set up some kind of testing with qemu for the firmware</li>
</ul>
<h2>Conclusions</h2>
<p>It's really easy and fun to get started writing code for microcontrollers in Rust! CAD is also pretty fun.</p>
<p><em>Thanks to Neil Chen and Stan Zhang for reviewing a draft of this post</em></p>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Common infections can spark psychiatric illnesses in children (210 pts)]]></title>
            <link>https://www.economist.com/science-and-technology/2023/09/20/how-common-infections-can-spark-psychiatric-illnesses-in-children</link>
            <guid>37661034</guid>
            <pubDate>Tue, 26 Sep 2023 15:43:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/science-and-technology/2023/09/20/how-common-infections-can-spark-psychiatric-illnesses-in-children">https://www.economist.com/science-and-technology/2023/09/20/how-common-infections-can-spark-psychiatric-illnesses-in-children</a>, See on <a href="https://news.ycombinator.com/item?id=37661034">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><h2>And why many doctors do not realise it</h2></section><div><div data-body-id="cp2"><div><figure><div><figcaption>Listen to this story.</figcaption> <p><span>Enjoy more audio and podcasts on<!-- --> <a id="audio-ios-cta" href="https://economist-app.onelink.me/d2eC/bed1b25" target="_blank" rel="noreferrer">iOS</a> <!-- -->or<!-- --> <a id="audio-android-cta" href="https://economist-app.onelink.me/d2eC/7f3c199" target="_blank" rel="noreferrer">Android</a>.</span></p></div><audio controls="" id="audio-player" preload="none" src="https://www.economist.com/media-assets/audio/072%20Science%20and%20technology%20-%20Children%20and%20mental%20health-7425bb279b144fea9958a5cae581e0bb.mp3" title="How common infections can spark psychiatric illnesses in children" controlslist="nodownload"><p>Your browser does not support the &lt;audio&gt; element.</p></audio></figure></div><p data-component="paragraph"><span data-caps="initial">I</span><small>t was a</small> sunny day in September 2007 when Garrett Pohlman, then seven years old, came home from school. Crying, he warned his mother that radiation was coming out of the house’s electrical sockets. If they went outside, he said, birds would peck them to death. These pronouncements were accompanied by odd facial movements. The boy would stick his tongue out and jerk his arms and legs. The day before, Garrett had been a normal boy. Both the paranoia and the tics had come out of the blue, but they proved to be the start of a horrifying mental decline.</p><p data-component="paragraph">In the end, Garrett was lucky. A hospital scan three months later revealed a bacterial sinus infection. A course of antibiotics cured the infection and brought about a striking improvement in his psychiatric symptoms. Garrett had been suffering from <small>PANDAS</small>, which stands for Paediatric Autoimmune-Neuropsychiatric Disorders Associated with <i>Streptococcus</i>.</p><p data-component="paragraph">Many other children are not so fortunate; some have suffered long-term damage. In plain English, Garrett’s unsettling behaviour was the result of an immune system gone haywire following an infection with group A <i>Streptococcus</i>, a common bacterium. (A similar illness, triggered by other infections, goes by the acronym <small>PANS</small>, for Paediatric Acute-onset Neuropsychiatric Syndrome.)</p><p data-component="paragraph">Yet many doctors have heard of neither <small>PANDAS</small> nor <small>PANS</small>. Some have dismissed them as fictitious diseases. Very few countries issue guidance on their diagnosis or treatment. Diana Pohlman, Garrett’s mother, says she is “exhausted” by years spent campaigning to get doctors to take the illnesses seriously.</p><p data-component="paragraph">That is starting to change. Scientists are beginning to characterise the conditions in detail and determine exactly what is going wrong with the immune systems of sufferers. On September 12th Maria Caulfield, a British health minister,  weighed in, telling legislators that <small>PANDAS</small> and <small>PANS</small> exist and are triggered by infections.</p><p data-component="paragraph">Such efforts are starting from a low base. In a 2020 survey for <small>PANS PANDAS UK</small>, a charity, 95% of parents whose children have <small>PANDAS</small> said their family doctors had not offered the diagnosis, suggesting awareness is low. Things were only a little better among specialists. Around half of paediatricians said they had never heard of the disease. Nearly one in five of the parents surveyed said their paediatrician felt that the diagnosis was controversial.</p><p data-component="paragraph">That ignorance carries costs. In many countries children with <small>PANDAS</small> are presented with an alphabet soup of psychiatric misdiagnoses. These can include attention-deficit hyperactivity disorder, autism and sensory-processing disorders. Children can be given inappropriate drugs like antipsychotics, many of which have unpleasant side-effects and which do nothing to treat the cause of their disease.</p><p data-component="paragraph">In some cases parents have been accused of inventing or inducing their children’s illness. <i>The Economist</i> has spoken to parents who say their children have been committed to mental-health services against their will, or removed from their care altogether. According to testimony provided in Parliament, one doctor told a child that he would not treat “an American illness”. In 2019 several dozen children with <small>PANDAS</small> and<small> PANS</small> were discharged from a British hospital. Their parents were told they had a “functional neurological disorder”—a diagnosis that has evolved from the old (and discredited) idea of hysteria, and which some doctors joke grimly means “finding no diagnosis”.</p><p data-component="paragraph">Exactly why the diagnosis is controversial remains unclear. After all, the idea that the aftermath of an infection can cause psychiatric symptoms is not new. Sydenham’s chorea, in which patients suffer from jerky movements of the face and body, is likewise the result of a streptococcal infection. <i>The Economist</i> contacted a number of psychiatrists and professional bodies for comment. Some did not reply. Others said they were unable to offer any comment. The Royal College of Psychiatrists said it was struggling to find an available spokesperson.</p><h2>The body and the mind</h2><p data-component="paragraph">But as evidence accumulates that <small>PANS</small> and <small>PANDAS</small> are real, attitudes are beginning to shift. Scientists who study the disorder now believe it is caused by an auto-immune reaction, in which the body’s immune system mistakenly attacks brain tissue. After infection with <i>Streptococcus</i>, the theory goes, children begin producing antibodies that cause inflammation in their own brains, which in turn causes the psychiatric symptoms.</p><p data-component="paragraph">In 2018 Christopher Pittenger, a psychiatrist at Yale University, and his colleagues extracted antibodies from the blood of children with <small>PANDAS</small> and introduced them into laboratory mice. They found that the antibodies attacked cholinergic interneurons in particular, a group of cells in parts of the brain associated with tic disorders, which are one of the features of <small>PANDAS</small>. Chandra Menendez, a researcher at the University of Oklahoma Health Sciences Centre, says she has found a “correlation between antibodies that target dopamine receptors <small>D</small>1 and <small>D</small>2 and the <small>PANDAS</small> phenotype”. This sort of work could help develop diagnostic tests.</p><p data-component="paragraph">A paper by Dritan Agalliu, a neurologist at Columbia University, currently under review by a scientific journal, suggests that blocking a particular part of the immune system—a type of lymphocyte called T helper 17 cells—with immune-suppressing drugs reduces damage to the brain, at least in mice. Other work suggests that damage to the blood-brain barrier, a filter designed to protect the brain from potentially harmful substances in the blood, could also be part of the story.</p><p data-component="paragraph">Such findings may have significance beyond a single obscure, debilitating illness. For they fit an intriguing, and growing, body of evidence that other kinds of psychiatric conditions might also result from infections. Dr Pittenger says it is now clear that covid-19 infections can trigger psychosis, fatigue and other neuro-psychiatric symptoms. A misbehaving immune system is thought to be the culprit. The idea that schizophrenia may, at least sometimes, likewise be an auto-immune disorder is also under investigation. (Intriguingly, people with any kind of auto-immune disorder appear to be about 40% more likely to develop psychotic disorders such as schizophrenia.)</p><p data-component="paragraph">How long it takes doctors to pick up on this change of thinking is another matter. Some of the prodding now comes not from patients, but from governments. Robin Millar, a British <small>MP</small> who chairs a parliamentary group on <small>PANDAS</small> and <small>PANS</small>, says Britain’s government is committed to working out how to diagnose and assess the illnesses. It has begun discussions with doctors, and is pondering a research project to work out how prevalent the diseases are. A pan-European patient group called <small>EXPAND</small>, founded in 2018, is also pushing to improve understanding.</p><p data-component="paragraph">Such efforts are sorely needed. As Garrett Pohlman’s case shows, if infections are caught early, treatment can be very effective, avoiding long-term damage. Now 23, Mr Pohlman graduated in 2022 with high honours from the University of California, Berkeley, in chemical engineering, and these days runs his own company. Not every patient has been so lucky. <span>■</span></p><p data-component="paragraph"><i>Correction (September 22nd 2023): An earlier version of this article described the use of anti-inflammatory drugs to reduce damage to the brain. In fact, immune-suppressing drugs were used. Apologies for the error.</i></p><p data-component="paragraph"><i>Curious about the world? To enjoy our mind-expanding science coverage, sign up to&nbsp;<a href="https://www.economist.com/newsletters/simply-science" target="_blank">Simply Science</a>, our weekly subscriber-only newsletter.</i></p></div><p>This article appeared in the Science &amp; technology section of the print edition under the headline "PANDA-monium"</p><div orientation="vertical" data-test-id="vertical"><div orientation="vertical"><figure><img alt="Time for a rethink: Helping Ukraine win a long war" loading="lazy" width="1280" height="1684" decoding="async" data-nimg="1" sizes="300px" srcset="https://www.economist.com/img/b/16/21/90/media-assets/image/20230923_DE_US.jpg 16w, https://www.economist.com/img/b/32/42/90/media-assets/image/20230923_DE_US.jpg 32w, https://www.economist.com/img/b/48/63/90/media-assets/image/20230923_DE_US.jpg 48w, https://www.economist.com/img/b/64/84/90/media-assets/image/20230923_DE_US.jpg 64w, https://www.economist.com/img/b/96/126/90/media-assets/image/20230923_DE_US.jpg 96w, https://www.economist.com/img/b/128/168/90/media-assets/image/20230923_DE_US.jpg 128w, https://www.economist.com/img/b/256/336/90/media-assets/image/20230923_DE_US.jpg 256w, https://www.economist.com/img/b/360/473/90/media-assets/image/20230923_DE_US.jpg 360w, https://www.economist.com/img/b/384/505/90/media-assets/image/20230923_DE_US.jpg 384w, https://www.economist.com/img/b/480/631/90/media-assets/image/20230923_DE_US.jpg 480w, https://www.economist.com/img/b/600/789/90/media-assets/image/20230923_DE_US.jpg 600w, https://www.economist.com/img/b/834/1097/90/media-assets/image/20230923_DE_US.jpg 834w, https://www.economist.com/img/b/960/1263/90/media-assets/image/20230923_DE_US.jpg 960w, https://www.economist.com/img/b/1096/1441/90/media-assets/image/20230923_DE_US.jpg 1096w, https://www.economist.com/img/b/1280/1684/90/media-assets/image/20230923_DE_US.jpg 1280w, https://www.economist.com/img/b/1424/1873/90/media-assets/image/20230923_DE_US.jpg 1424w" src="https://www.economist.com/img/b/1424/1873/90/media-assets/image/20230923_DE_US.jpg"></figure></div><div orientation="vertical"><h3 orientation="vertical">From the September 23rd 2023 edition</h3><p orientation="vertical">Discover stories from this section and more in the list of contents </p><a href="https://www.economist.com/printedition/2023-09-23" data-analytics="sidebar:weekly_edition"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1zm.142 4.5l-1.008 1.062c3.33 3.276 4.194 4.14 4.608 4.5-1.602-.018-3.168-.018-10.242-.018v1.584c7.074 0 8.73 0 10.242-.018-.432.36-1.314 1.206-4.608 4.536l1.008 1.044 6.354-6.354L12.142 5.5z" fill="#2E45B8" fill-rule="nonzero"></path></g></svg><span>Explore the edition</span></a></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unity U-turns on controversial runtime fee and begs forgiveness (122 pts)]]></title>
            <link>https://techcrunch.com/2023/09/22/unity-u-turns-on-controversial-runtime-fee-and-begs-forgiveness/</link>
            <guid>37660327</guid>
            <pubDate>Tue, 26 Sep 2023 14:55:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2023/09/22/unity-u-turns-on-controversial-runtime-fee-and-begs-forgiveness/">https://techcrunch.com/2023/09/22/unity-u-turns-on-controversial-runtime-fee-and-begs-forgiveness/</a>, See on <a href="https://news.ycombinator.com/item?id=37660327">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Unity has done a 180 on a <a href="https://techcrunch.com/2023/09/18/unity-reportedly-backtracking-on-new-fees-after-developers-revolt/">controversial new pricing scheme</a> that users of its cross-platform game engine almost unanimously disparaged. A new pricing policy is still incoming, but it’s far less fraught for independent developers, many of whom threatened to leave the engine and platform behind rather than pay.</p>
<p>The changes were announced only last week, and immediately attracted the ire of nearly everyone in the gaming community, prompting a panicked “clarification” soft-pedaling of the “runtime fee” that would be owed with every install of a game past a certain level of revenue. The plan was intensely unpopular, as apart from the increased costs many would incur under it, it suggested that the people running the show at Unity were completely disconnected from the community.</p>
<p>Less than two weeks from its debut, however, the runtime fee policy has been almost completely reversed and its architects are abasing themselves before their customers. Unity Create head Marc Whitten <a href="https://blog.unity.com/news/open-letter-on-runtime-fee" target="_blank" rel="noopener">posted an apologetic note detailing the changes</a>.</p>
<blockquote><p>I want to start with this: I am sorry.</p>
<p>We should have spoken with more of you and we should have incorporated more of your feedback before announcing our new Runtime Fee policy.</p>
<p>You are what makes Unity great, and we know we need to listen, and work hard to earn your trust. We have heard your concerns, and we are making changes in the policy we announced to address them.</p></blockquote>
<p>Under the revised terms, there is no runtime fee whatever for any game developed with a current version of Unity, only for those developers who adopt the latest version in 2024. The free Unity Personal will stay free, and the revenue ceiling above which users must upgrade to the next tier has been raised to $200,000. And above $1 million, users will be able to choose between a per-user fee or a 2.5% revenue share, whichever is lesser.</p>
<p>Whitten also said he will <a href="https://youtube.com/live/qyLcI5O9iUY" target="_blank" rel="noopener">conduct a live Q&amp;A at 1 PM PDT today</a>, during which some further clarifications may be made.</p>
<p>Overall the changes seem to address most of the issues people had with the new terms, and importantly it is more or less opt-in (or the unavoidable product of success) come 2024, on new projects, rather than taking effect on games that are out now or have been for years.</p>
<p>But the high-handed manner in which Unity attempted to squeeze its customers has unquestionably spooked the community, and while the threatened exodus will likely now be far smaller, they will remain watchful for future shenanigans. What trust Unity had built up was seriously damaged by this ill-conceived foray, and many developers may look more seriously at competitors rather than run the risk of the company altering the deal again.</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Investigation: 78% of carbon offset projects globally are “likely junk“ (288 pts)]]></title>
            <link>https://www.power-technology.com/news/report-majority-carbon-offsets-junk-or-useless/</link>
            <guid>37660256</guid>
            <pubDate>Tue, 26 Sep 2023 14:50:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.power-technology.com/news/report-majority-carbon-offsets-junk-or-useless/">https://www.power-technology.com/news/report-majority-carbon-offsets-junk-or-useless/</a>, See on <a href="https://news.ycombinator.com/item?id=37660256">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    
                            <header>
            

        <div>
            
                                        
                <p>Analysis found that 39, or 78%, of the 50 environmental projects were categorised as “likely junk or worthless” due to one or more “fundamental failing".</p>

				                
                            

        </div>
    </header>
    <div>
                                  
                                                
                        
                                                                    
                                        
                <!--<p class="drop-cap"></p>-->
                                                <div>
                    <figure>
                        <img src="https://www.power-technology.com/wp-content/uploads/sites/21/2023/08/shutterstock_1274894119-scaled.jpg">
                        <figcaption>Reforestation is one of the most common projects traded within the VCM. Credit: Richard Whitcombe via Shutterstock.</figcaption>
                    </figure>
                </div>
                            
<p>The “vast majority” of environmental projects most commonly used within the voluntary carbon market (VCM) to offset greenhouse gas (GHG) emissions seem to have “fundamental failings” and cannot be relied upon to tackle global warming, according to a joint investigation from the <em>Guardian </em>and non-profit climate watchdog Corporate Accountability.</p>



<p>The <a href="https://www.theguardian.com/environment/2023/sep/19/do-carbon-credit-reduce-emissions-greenhouse-gases?CMP=Share_AndroidApp_Other" target="_blank">investigation analysed</a> the top 50 emission offset projects, selected because they have sold the most carbon credits within the global VCM, and found that most of them exaggerate climate benefits and underestimate the potential harm caused by the project’s activity.</p>



<p>The most popular projects traded globally include forestry schemes, hydroelectric dams, solar and wind farms, waste disposal and greener household appliance schemes across 20 countries, most of which have developing economies. The data comes from <a href="https://alliedoffsets.com/" target="_blank">Allied Offsets,</a> the world’s biggest and most comprehensive emissions trading database, which aggregates information about projects traded within the VCM from their inception.</p>



<p>The analysis found that 39, or 78%, of the 50 projects were categorised as “likely junk or worthless” due to one or more “fundamental failing” that undermines its alleged emissions offsetting power.</p>



<p>Eight others, or 16%, look “problematic”. There is evidence to suggest that they may have at least one fundamental failing and could therefore be “junk”.</p>



<p>The effectiveness of the remaining three projects could not be assessed properly or classified definitively, largely due to a lack of available public, independent information. The analysis also found that $1.16bn worth of carbon credits have been traded so far from those projects classified as “likely junk or worthless”.</p>



<p>The criteria for assessing whether a project is likely junk was based on whether there was “compelling evidence” or a high risk that the project could not guarantee additional GHG emission cuts. In some cases, there was evidence to suggest that projects were leaking further, additional emissions or simply shifting emissions elsewhere. In other cases, evidence was found to suggest that a project’s climate benefits had been exaggerated.</p>



<h2 id="h-carbon-market-actively-exacerbating-the-climate-emergency">Carbon market “actively exacerbating the climate emergency”</h2>



<p>“The ramifications of this analysis are huge, as it points to systemic failings of the voluntary market, providing additional evidence that junk carbon credits pervade,” said Anuradha Mittal, director of the Oakland Institute think tank. “We cannot afford to waste any more time on false solutions. The issues are far-reaching and pervasive, extending well beyond specific verifiers. The VCM is actively exacerbating the climate emergency.”</p>



<p>The VCM has already <a href="https://www.power-technology.com/features/carbon-credits-greenwashing-market/?cf-view&amp;cf-closed" target="_blank">come under fire</a> several times this year after a number of investigations exposed serious shortcomings in the market.</p>



<p>In January, a <a href="https://www.theguardian.com/environment/2023/jan/18/revealed-forest-carbon-offsets-biggest-provider-worthless-verra-aoe" target="_blank">joint investigation</a> by the <em>Guardian, </em>German newspaper <em>Die Zeit </em>and online climate reporters at <em>SourceMaterial, </em>revealed that more than 90% of rainforest offsets offered by Verra, the world’s leading carbon credits certifier, were likely to be “phantom credits” and did not represent genuine emissions reductions.</p>



<p>Verra published a <a href="https://verra.org/statement-vcm-stakeholders-latest-guardian-attack/" target="_blank">statement</a> on Thursday in response to the latest investigation by the <em>Guardian </em>and Corporate Accountability, suggesting that the <em>Guardian </em>has gone “dangerously off track when it comes to reporting on the VCM”.</p>



<p>At the end of May, an <a href="https://www.offshore-technology.com/news/chevron-carbon-offsets-junk-or-harmful/?cf-view" target="_blank">earlier investigation</a> by Corporate Accountability found that 93% of the carbon offsets used by oil and gas giant <a href="https://www.globaldata.com/store/report/?cdmsid=1544307&amp;scalar=true&amp;pid=310253&amp;sid=21" target="_blank">Chevron</a> “seem to be worthless” and should be presumed “junk” until proven otherwise. At the time a spokesperson for Chevron said the report was “biased against [the company’s] industry”.</p>
                
            </div>
    
<section>
                <header>
                                            <h2>More Relevant</h2>
                        <!--<a href="https://www.power-technology.com/" class="view-more-link">
                            View More
                        </a>-->
                    </header>
                        
                                            <!--<h2>Related Rankings</h2>
                        <a href="" class="view-more-link">
                            View More
                        </a>
                    </header>-->
                                    
            </section>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple exec Eddy Cue set to testify in Google trial about $19B search deal (128 pts)]]></title>
            <link>https://www.cnbc.com/2023/09/26/apple-exec-eddy-cue-testify-google-trial-about-19-billion-search-deal.html</link>
            <guid>37660120</guid>
            <pubDate>Tue, 26 Sep 2023 14:41:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2023/09/26/apple-exec-eddy-cue-testify-google-trial-about-19-billion-search-deal.html">https://www.cnbc.com/2023/09/26/apple-exec-eddy-cue-testify-google-trial-about-19-billion-search-deal.html</a>, See on <a href="https://news.ycombinator.com/item?id=37660120">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-6" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-6-2"><div id="ArticleBody-InlineImage-105060036" data-test="InlineImage"><p>Eddy Cue, senior vice president of services at Apple Inc.</p><p>David Paul Morris | Bloomberg | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/AAPL/">Apple</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> senior vice president of services Eddy Cue is expected to testify all day Tuesday in federal court where the U.S. Department of Justice is accusing <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-2"><a href="https://www.cnbc.com/quotes/GOOGL/">Google</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> of using licensing agreements to monopolize online search.</p><p>Under scrutiny is a deal in which Google pays Apple billions of dollars to be the default search engine on the iPhone's browser and other settings. Google could pay Apple as much as $19 billion this year, according to an estimate from Bernstein.  </p><p>Cue, who <a href="https://www.cnbc.com/2019/06/10/apple-ex-counsel-bruce-sewell-describes-negotiations-with-google.html">negotiated</a> the deal with Google from Apple's side, is expected to testify that Apple picked the Google search engine as an iPhone default because it was the best product. He's also expected to say that Apple doesn't see a reason to create a new Apple search engine because Google already exists, according to a person familiar with Cue's anticipated testimony.</p><p>Cue will also say that Apple has revenue-sharing agreements with competing search engines Yahoo, Microsoft Bing, DuckDuckGo and Ecosia, and that Apple users can change their default search engines, according to a person familiar with Cue's anticipated testimony.</p><p>The testimony could shed some light on one of the <a href="https://www.cnbc.com/2019/06/10/apple-ex-counsel-bruce-sewell-describes-negotiations-with-google.html">highest-profile deals</a> in the technology industry, which has been shrouded in secrecy for the past decade. The money Google pays to Apple for default placement is one of its biggest costs, and the advertising revenue Apple collects from Google is a major part of Apple's profits. </p><p>Apple reports its payments from Google as advertising revenue, reported in its services business, which totaled $78.1 billion in sales in Apple's fiscal 2022. </p><p>"I think their search engine is the best," Apple CEO Tim Cook <a href="https://www.axios.com/2018/11/19/tim-cook-apple-google-search-engine" target="_blank">said</a> when asked about using Google as the iPhone's default search engine in 2018. </p></div><h2><a id="headline0"></a>Google on trial</h2><div><p>Much of Cue's testimony and related financial documents could remain under seal, which means they won't be released to the public.</p><p>Last week, Apple machine learning executive John Giannandrea testified. Before Apple, he worked at Google on its search engine.</p><p>The D.C. District Court judge, Amit Mehta, has said he wants to be conservative about how many documents are released to the public, and last week's Giannandrea testimony was entirely sealed except for 15 minutes, where Giannandrea <a href="https://www.bloomberg.com/news/articles/2023-09-22/apple-ai-chief-posits-new-private-browser-search-at-google-trial" target="_blank">revealed a new search engine setting</a> on the most recent iPhone operating system.</p><p>The DOJ previously had a page on its website where it would post documents and exhibits from the trial, and it was taken down last week on Google's request.</p><p>The Google trial, expected to last 10 weeks, is the <a href="https://www.cnbc.com/2023/09/11/google-to-face-doj-in-the-first-major-tech-monopoly-trial-in-decades.html">biggest technology monopoly trial since</a> the DOJ took <a href="https://en.wikipedia.org/wiki/United_States_v._Microsoft_Corp." target="_blank">on Microsoft</a> more than 20 years ago. The DOJ alleges Google has violated anti-monopoly law by striking exclusive agreements with mobile phone makers for its Android operating system and browser companies for default placement. The government alleges that the practice creates barriers to entry for competing search engines.</p><p>"This case is about the future of the internet and whether Google's search engine will ever face meaningful competition," the DOJ's lawyer, Kenneth Dintzer, told the court <a href="https://www.cnbc.com/2023/09/12/doj-v-google-what-hapened-the-first-day-of-the-anti-monopoly-trial.html">in opening statements</a>. He alleged that Google has more than 89% of the market for general search.</p><p><a href="https://www.cnbc.com/2023/09/08/heres-how-google-plans-to-fight-the-doj.html">Google said</a> before the trial kicked off earlier this month that it sees licensing agreements as a standard business practice that brings its products to consumers and creates a better experience for users. Google also argues that consumers can easily change default search engines on Android and Apple phones.</p><p>The DOJ is expected to present its case for about four weeks, then a coalition of attorneys general will present their case, followed by Google. Google CEO Sundar Pichai is also expected to testify, the DOJ said.</p><p><em>CNBC's Steve Kovach contributed to this story.</em></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spinneret: A modern Common Lisp HTML generator (102 pts)]]></title>
            <link>https://github.com/ruricolist/spinneret</link>
            <guid>37660025</guid>
            <pubDate>Tue, 26 Sep 2023 14:36:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ruricolist/spinneret">https://github.com/ruricolist/spinneret</a>, See on <a href="https://news.ycombinator.com/item?id=37660025">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><strong>Table of Contents</strong></p>
<ul dir="auto">
<li><a href="#spinneret">Spinneret</a>
<ul dir="auto">
<li><a href="#printing-style">Printing style</a></li>
<li><a href="#inserted-spaces">Inserted spaces</a></li>
<li><a href="#line-wrapping">Line wrapping</a></li>
<li><a href="#syntax">Syntax</a>
<ul dir="auto">
<li><a href="#dynamic-output">Dynamic output</a></li>
<li><a href="#interpreting-trees">Interpreting trees</a></li>
<li><a href="#markdown">Markdown</a></li>
</ul>
</li>
<li><a href="#get-html-path"><code>get-html-path</code></a>
<ul dir="auto">
<li><a href="#html-path"><code>*html-path*</code></a></li>
</ul>
</li>
<li><a href="#deftag"><code>deftag</code></a></li>
<li><a href="#parenscript">Parenscript</a></li>
<li><a href="#validation">Validation</a></li>
</ul>
</li>
</ul>

<h2 tabindex="-1" id="user-content-spinneret" dir="auto"><a href="#spinneret">Spinneret</a></h2>
<p dir="auto">In the crowded space of Common Lisp HTML generators, Spinneret
occupies the following coordinates:</p>
<ul dir="auto">
<li>
<p dir="auto">Modern. Targets HTML5. Does not treat XML and HTML as the same
problem. Assumes you will be serving your documents as UTF-8.</p>
</li>
<li>
<p dir="auto">Composable. Makes it easy to refactor HTML generation into separate
functions and macros.</p>
</li>
<li>
<p dir="auto">Pretty. Treats HTML as a document format, not a serialization.
Output is idiomatic and readable, following the coding style of the
HTML5 specification.</p>
</li>
<li>
<p dir="auto">Aggressive. If something can be interpreted as HTML, then it will
be, meaning that some Lisp forms can't be mixed with HTML syntax. In
the trade-off between 90% convenience and 10% correctness Spinneret
is on the side of convenience.</p>
</li>
<li>
<p dir="auto">Bilingual. Spinneret (after loading <code>spinneret/ps</code>) has the same semantics in Lisp and <a href="https://parenscript.common-lisp.dev/" rel="nofollow">Parenscript</a>.</p>
</li>
</ul>
<p dir="auto">HTML generation with Spinneret looks like this:</p>
<div data-snippet-clipboard-copy-content=" (in-package #:spinneret)

 (defparameter *shopping-list*
   '(&quot;Atmospheric ponds&quot;
     &quot;Electric gumption socks&quot;
     &quot;Mrs. Leland's embyronic television combustion&quot;
     &quot;Savage gymnatic aggressors&quot;
     &quot;Pharmaceutical pianos&quot;
     &quot;Intravenous retribution champions&quot;))

 (defparameter *user-name* &quot;John Q. Lisper&quot;)

 (defparameter *last-login* &quot;12th Never&quot;)

 (defmacro with-page ((&amp;key title) &amp;body body)
   `(with-html
      (:doctype)
      (:html
        (:head
         (:title ,title))
        (:body ,@body))))

 (defun shopping-list ()
   (with-page (:title &quot;Home page&quot;)
     (:header
      (:h1 &quot;Home page&quot;))
     (:section
      (&quot;~A, here is *your* shopping list: &quot; *user-name*)
      (:ol (dolist (item *shopping-list*)
             (:li (1+ (random 10)) item))))
     (:footer (&quot;Last login: ~A&quot; *last-login*))))"><pre><code> (in-package #:spinneret)

 (defparameter *shopping-list*
   '("Atmospheric ponds"
     "Electric gumption socks"
     "Mrs. Leland's embyronic television combustion"
     "Savage gymnatic aggressors"
     "Pharmaceutical pianos"
     "Intravenous retribution champions"))

 (defparameter *user-name* "John Q. Lisper")

 (defparameter *last-login* "12th Never")

 (defmacro with-page ((&amp;key title) &amp;body body)
   `(with-html
      (:doctype)
      (:html
        (:head
         (:title ,title))
        (:body ,@body))))

 (defun shopping-list ()
   (with-page (:title "Home page")
     (:header
      (:h1 "Home page"))
     (:section
      ("~A, here is *your* shopping list: " *user-name*)
      (:ol (dolist (item *shopping-list*)
             (:li (1+ (random 10)) item))))
     (:footer ("Last login: ~A" *last-login*))))
</code></pre></div>
<p dir="auto">Which produces:</p>
<div data-snippet-clipboard-copy-content=" <!DOCTYPE html>
 <html lang=en>
  <head>
   <meta charset=UTF-8>
   <title>Home page</title>
  </head>
  <body>
   <header>
    <h1>Home page</h1>
   </header>
   <section>
    John Q. Lisper, here is <em>your</em> shopping list:
    <ol>
     <li>10 Atmospheric ponds
     <li>6 Electric gumption socks
     <li>4 Mrs. Leland's embyronic television combustion
     <li>9 Savage gymnatic aggressors
     <li>6 Pharmaceutical pianos
     <li>9 Intravenous retribution champions
    </ol>
   </section>
   <footer>
    Last login: 12th Never
   </footer>
  </body>
 </html>"><pre><code> &lt;!DOCTYPE html&gt;
 &lt;html lang=en&gt;
  &lt;head&gt;
   &lt;meta charset=UTF-8&gt;
   &lt;title&gt;Home page&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
   &lt;header&gt;
    &lt;h1&gt;Home page&lt;/h1&gt;
   &lt;/header&gt;
   &lt;section&gt;
    John Q. Lisper, here is &lt;em&gt;your&lt;/em&gt; shopping list:
    &lt;ol&gt;
     &lt;li&gt;10 Atmospheric ponds
     &lt;li&gt;6 Electric gumption socks
     &lt;li&gt;4 Mrs. Leland's embyronic television combustion
     &lt;li&gt;9 Savage gymnatic aggressors
     &lt;li&gt;6 Pharmaceutical pianos
     &lt;li&gt;9 Intravenous retribution champions
    &lt;/ol&gt;
   &lt;/section&gt;
   &lt;footer&gt;
    Last login: 12th Never
   &lt;/footer&gt;
  &lt;/body&gt;
 &lt;/html&gt;
</code></pre></div>
<p dir="auto">(Pretty-printing is pretty fast, but Spinneret obeys <code>*print-pretty*</code>
should you want to turn it off.)</p>
<h3 tabindex="-1" id="user-content-printing-style" dir="auto"><a href="#printing-style">Printing style</a></h3>
<p dir="auto">Spinneret tries hard to produce human-writable output – output that
looks like a human being wrote it. Sometimes, however, you may have
markup to render that there is no human-writable way to render,
because no human being would ever write it.</p>
<p dir="auto">In these cases you can set or bind the <code>*html-style*</code> variable to
control Spinneret’s print style. The default is <code>:human</code>, which means
to attempt to produce human-writable output. It can also be set to
<code>:tree</code>, which simply prints every element as if it were a block
element, and every run of text on a new line.</p>
<div data-snippet-clipboard-copy-content="(let ((*html-style* :human))
  (with-html
    (:div
      (:p &quot;Text &quot; (:a &quot;link text&quot;) &quot; more text&quot;))))
=> <div>
    <p>Text <a>link text</a> more text
   </div>&quot;

(let ((*html-style* :tree))
  (with-html-string
    (:div
      (:p &quot;Text &quot; (:a &quot;link text&quot;) &quot; more text&quot;))))
=> <div>
    <p>
     Text
     <a>
      link text
     </a>
      more text
    </p>
   </div>"><pre><code>(let ((*html-style* :human))
  (with-html
    (:div
      (:p "Text " (:a "link text") " more text"))))
=&gt; &lt;div&gt;
    &lt;p&gt;Text &lt;a&gt;link text&lt;/a&gt; more text
   &lt;/div&gt;"

(let ((*html-style* :tree))
  (with-html-string
    (:div
      (:p "Text " (:a "link text") " more text"))))
=&gt; &lt;div&gt;
    &lt;p&gt;
     Text
     &lt;a&gt;
      link text
     &lt;/a&gt;
      more text
    &lt;/p&gt;
   &lt;/div&gt;
</code></pre></div>
<p dir="auto">With <code>*html-style*</code> bound to <code>:tree</code>, and <code>*print-pretty*</code> bound to
nil, output is verbose but predictable:</p>
<div data-snippet-clipboard-copy-content="(let ((*html-style* :tree)
      (*print-pretty* nil))
  (with-html-string
    (:div
      (:p &quot;Text &quot; (:a &quot;link text&quot;) &quot; more text&quot;))))
=> &quot;<div><p>Text <a>link text</a>  more text</p></div>&quot;"><pre><code>(let ((*html-style* :tree)
      (*print-pretty* nil))
  (with-html-string
    (:div
      (:p "Text " (:a "link text") " more text"))))
=&gt; "&lt;div&gt;&lt;p&gt;Text &lt;a&gt;link text&lt;/a&gt;  more text&lt;/p&gt;&lt;/div&gt;"
</code></pre></div>
<p dir="auto">Notice that binding <code>*html-style*</code> to <code>:tree</code> ensures that all tags are
closed.</p>
<h3 tabindex="-1" id="user-content-inserted-spaces" dir="auto"><a href="#inserted-spaces">Inserted spaces</a></h3>
<p dir="auto">By default, when objects are output to HTML, spaces are inserted betweeen them. This is nearly always the right thing to do, but in some special cases, the spaces may be a problem. They can be turned off by setting the flag <code>*suppress-inserted-spaces*</code> to <code>t</code>.</p>
<h3 tabindex="-1" id="user-content-line-wrapping" dir="auto"><a href="#line-wrapping">Line wrapping</a></h3>
<p dir="auto">When pretty-printing, Spinneret makes the best decisions about line
wrapping that it can, given the information it has about how to get
the print length of various types. But, in the case of user-defined
types, it has no way to tell in advance how long they will be when
printed. If you find Spinneret is making bad line-breaking decisions
with your types, you can help it out by specializing <code>html-length</code>.
For example, if you use PURI, you could help Spinneret pretty-print
PURI URIs by teaching it how to get their length:</p>
<div data-snippet-clipboard-copy-content="(defmethod html-length ((uri puri:uri))
  ;; Doesn't cons.
  (length (puri:render-uri uri nil)))"><pre><code>(defmethod html-length ((uri puri:uri))
  ;; Doesn't cons.
  (length (puri:render-uri uri nil)))
</code></pre></div>
<h2 tabindex="-1" id="user-content-syntax" dir="auto"><a href="#syntax">Syntax</a></h2>
<p dir="auto">The rules for WITH-HTML are these:</p>
<ul dir="auto">
<li>
<p dir="auto">All generated forms write to <code>*html*</code>.</p>
</li>
<li>
<p dir="auto">A keyword in function position is interpreted as a tag name. If the
name is not valid as a tag, it is ignored.</p>
<p dir="auto">Certain keywords are recognized as pseudo-tags and given special
treatment:</p>
<p dir="auto">:RAW :DOCTYPE :!DOCTYPE :CDATA :!-- :COMMENT :HTML :HEAD :H* :TAG</p>
<ul dir="auto">
<li>
<p dir="auto">The pseudotag :RAW can be used to bypass Spinneret’s implicit
escaping for raw output. This allows inserting HTML literals, and
bypasses pretty printing.</p>
<p dir="auto">Note that you need :RAW for inline stylesheets and scripts,
otherwise angle brackets will be escaped as if they were HTML:</p>
<div dir="auto" data-snippet-clipboard-copy-content="(with-html-string (:style &quot;a > p{color: white;}&quot;))
=> &quot;<style>a &amp;gt; p{color: white;}</style>&quot;

(with-html-string (:style (:raw &quot;a > p{color: white;}&quot;)))
=> &quot;<style>a > p{color: white;}</style>&quot;
"><pre>(with-html-string (:style <span><span>"</span>a &gt; p{color: white;}<span>"</span></span>))
=&gt; <span><span>"</span>&lt;style&gt;a &amp;gt; p{color: white;}&lt;/style&gt;<span>"</span></span>

(with-html-string (:style (:raw <span><span>"</span>a &gt; p{color: white;}<span>"</span></span>)))
=&gt; <span><span>"</span>&lt;style&gt;a &gt; p{color: white;}&lt;/style&gt;<span>"</span></span>
</pre></div>
</li>
<li>
<p dir="auto">The pseudotags :!– and :COMMENT insert comments into the output.</p>
</li>
<li>
<p dir="auto">The pseudotag :H* renders as one of :H1 through :H6 depending on
how many :SECTION elements it is dynamically nested inside. At the
top level, :H* is equivalent to :H1. Inside the dynamic extent of
one :SECTION tag, it is equivalent to :H2; inside two section
tags, it is equivalent to :H3; and so forth up to :H6.</p>
</li>
<li>
<p dir="auto">The pseudotag :TAG allows dynamic selection of a tag.</p>
</li>
</ul>
<p dir="auto">The value of the LANG attribute of HTML is controlled by
<code>*html-lang*</code>; the value of the meta charset attribute is controlled
by <code>*html-charset*</code>. These are defaults; passing an explicit
attribute takes precedence.</p>
<p dir="auto">Constant classes and ids can be specified with a selector-like
syntax. E.g.:</p>
<div data-snippet-clipboard-copy-content="  (:div#wrapper (:div.section ...))
  ≡ (:div :id &quot;wrapper&quot; (:div :class &quot;section&quot; ...))"><pre><code>  (:div#wrapper (:div.section ...))
  ≡ (:div :id "wrapper" (:div :class "section" ...))
</code></pre></div>
</li>
<li>
<p dir="auto">Keyword-value pairs following a tag are interpreted as attributes.
HTML syntax may not be used in attribute values. Attributes with nil
values are omitted from the output. Boolean attributes with non-nil
values are minimized.</p>
<p dir="auto">Duplicate attributes are handled like duplicate keyword arguments:
all values are evaluated, but only the leftmost value is used. The
exception is the handling of tokenized attributes, such as :CLASS or
:REL. The class of a tag is the union of all its :CLASS arguments.</p>
<p dir="auto">The argument :DATASET introduces a list of :DATA-FOO arguments:</p>
<div data-snippet-clipboard-copy-content="  (:p :dataset (:duck (dolomphious) :fish 'fizzgigious
                      :spoon &quot;runcible&quot;))
  ≡ (:p :data-duck (dolomphious) :data-fish 'fizzgigious
        :data-spoon &quot;runcible&quot;)"><pre><code>  (:p :dataset (:duck (dolomphious) :fish 'fizzgigious
                      :spoon "runcible"))
  ≡ (:p :data-duck (dolomphious) :data-fish 'fizzgigious
        :data-spoon "runcible")
</code></pre></div>
<p dir="auto">For flexibility, even at the cost of efficiency, the argument :ATTRS
introduces a form to evaluate at run time for a plist of extra
attributes and values.</p>
</li>
<li>
<p dir="auto">Forms after the attributes are treated as arguments. Each non-nil
(primary) value returned by an argument to a tag is written to the
stream by HTML, a generic function on which you can define your own
methods. By default only literal arguments are printed. Literal
arguments are strings, characters, numbers and symbols beside NIL.</p>
</li>
</ul>
<p dir="auto">WITH-HTML-STRING is like WITH-HTML, but intercepts the generated HTML
at run time and returns a string.</p>
<h3 tabindex="-1" id="user-content-dynamic-output" dir="auto"><a href="#dynamic-output">Dynamic output</a></h3>
<p dir="auto">For flexibility, even at the cost of efficiency, the pseudo-attribute
:ATTRS introduces a form to evaluate at run time for a plist of extra
attributes and values.</p>
<div data-snippet-clipboard-copy-content="(:p :attrs (list :id &quot;dynamic!&quot;))
=> <p id=&quot;dynamic!&quot;>"><pre><code>(:p :attrs (list :id "dynamic!"))
=&gt; &lt;p id="dynamic!"&gt;
</code></pre></div>
<p dir="auto">Similarly, the pseudo-tag :TAG allows you to select a tag at run time.</p>
<div data-snippet-clipboard-copy-content="(:tag :name &quot;div&quot;
 (:tag :name &quot;p&quot;
  (:tag :name &quot;span&quot;
    &quot;Hello.&quot;)))
≡ (:div (:p (:span &quot;Hello&quot;)))"><pre><code>(:tag :name "div"
 (:tag :name "p"
  (:tag :name "span"
    "Hello.")))
≡ (:div (:p (:span "Hello")))
</code></pre></div>
<p dir="auto">Note that :TAG only allows you to <em>select</em> a tag, not <em>create</em> one.
The tag must still be one that is known to Spinneret to be valid. (That is, either defined as part of HTML or matching the requirements for a custom element.)</p>
<p dir="auto">For maximum dynamicity, you can combine :TAG and :ATTRS:</p>
<div data-snippet-clipboard-copy-content="(:tag :name &quot;div&quot; :attrs (list :id &quot;dynamic!&quot;))
=> <div id=dynamic!></div>"><pre><code>(:tag :name "div" :attrs (list :id "dynamic!"))
=&gt; &lt;div id=dynamic!&gt;&lt;/div&gt;
</code></pre></div>
<h3 tabindex="-1" id="user-content-interpreting-trees" dir="auto"><a href="#interpreting-trees">Interpreting trees</a></h3>
<p dir="auto">For the <em>ne plus ultra</em> of flexibility, you can interpret trees at runtime using a subset of Spinneret syntax:</p>
<div data-snippet-clipboard-copy-content="(interpret-html-tree `(:div :id &quot;dynamic!&quot;))
=> <div id=dynamic!></div>"><pre><code>(interpret-html-tree `(:div :id "dynamic!"))
=&gt; &lt;div id=dynamic!&gt;&lt;/div&gt;
</code></pre></div>
<p dir="auto">The interpreter is still under development; it supports most but not yet all Spinneret syntax.</p>
<h3 tabindex="-1" id="user-content-markdown" dir="auto"><a href="#markdown">Markdown</a></h3>
<p dir="auto">If the additional system <code>spinneret/cl-markdown</code> is loaded, then a
string in function position is first compiled as Markdown (using
<a href="https://github.com/gwkkwg/cl-markdown">CL-MARKDOWN</a>), then passed to <code>format</code> as a control string and
applied to its arguments.</p>
<p dir="auto">This is useful for inline formatting, like links, where sexps would be
clumsy:</p>
<div data-snippet-clipboard-copy-content="(with-html
 (&quot;Here is some copy, with [a link](~a)&quot; link))

(with-html
  (:span &quot;Here is some copy, with &quot;
    (:a :href link &quot;a link.&quot;)))"><pre><code>(with-html
 ("Here is some copy, with [a link](~a)" link))

(with-html
  (:span "Here is some copy, with "
    (:a :href link "a link.")))
</code></pre></div>
<h2 tabindex="-1" id="user-content-get-html-path" dir="auto"><a href="#get-html-path"><code>get-html-path</code></a></h2>
<p dir="auto">Sometimes it is useful for a piece of HTML-generating code to know
where in the document it appears. You might, for example, want to
define a <code>tabulate</code> function that prints list-of-lists as rows of
cells, but only prints the surrounding <code>&lt;table&gt;&lt;/table&gt;</code> if it is not
already within a table. The function <code>get-html-path</code> returns a list of
open tags, from latest to earliest. Usually it will look something
like</p>
<div data-snippet-clipboard-copy-content="  (get-html-path) ;-> '(:table :section :body :html)"><pre><code>  (get-html-path) ;-&gt; '(:table :section :body :html)
</code></pre></div>
<p dir="auto">Thus `tabulate' could be written</p>
<div data-snippet-clipboard-copy-content=" (defun tabulate (&amp;rest rows)
   (with-html
     (flet ((tabulate ()
              (loop for row in rows do
                (:tr (loop for cell in row do
                  (:td cell))))))
       (if (find :table (get-html-path))
           (tabulate)
           (:table (:tbody (tabulate)))))))"><pre><code> (defun tabulate (&amp;rest rows)
   (with-html
     (flet ((tabulate ()
              (loop for row in rows do
                (:tr (loop for cell in row do
                  (:td cell))))))
       (if (find :table (get-html-path))
           (tabulate)
           (:table (:tbody (tabulate)))))))
</code></pre></div>
<p dir="auto">Note that <code>get-html-path</code> returns a freshly-consed list each time it
is called.</p>
<h3 tabindex="-1" id="user-content-html-path" dir="auto"><a href="#html-path"><code>*html-path*</code></a></h3>
<p dir="auto">The variable underneath <code>get-html-path</code> is <code>*html-path*</code>, and it can
be let-bound to manipulate the nested tags (like <code>:h*</code> and <code>tabulate</code>
from the example above).</p>
<p dir="auto">WARNING: Spinneret binds <code>*html-path*</code> with <a href="http://clhs.lisp.se/Body/d_dynami.htm" rel="nofollow">dynamic
extent</a>. If you need to inspect
the binding, use <code>get-html-path</code> instead to get a value you can safely
store.</p>
<p dir="auto"><code>*html-path*</code> is most useful if the document generated by Spinneret is
split into several functions. Binding <code>*html-path*</code> allows to preserve
the structure of the document there.</p>
<p dir="auto">Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="(defun inner-section ()
  &quot;Binds *HTML-PATH* to replicate the depth the output is used in.&quot;
  (with-html-string
    (let ((*html-path* (append *html-path* '(:section :section))))
      (:h* &quot;Heading three levels deep&quot;))))

(defun outer-section (html)
  &quot;Uses HTML from elsewhere and embed it into a section&quot;
  (with-html-string
    (:section
     (:h* &quot;Heading two levels deep&quot;)
     (:section
      (:raw html)))))

(outer-section (inner-section))
;; <section>
;; <h2>Heading two levels deep</h2>
;; <section><h3>Heading three levels deep</h3>
;; </section>
;;</section>"><pre>(<span>defun</span> <span>inner-section</span> ()
  <span><span>"</span>Binds *HTML-PATH* to replicate the depth the output is used in.<span>"</span></span>
  (with-html-string
    (<span>let</span> ((<span>*html-path*</span> (<span>append</span> <span>*html-path*</span> '(:section :section))))
      (:h* <span><span>"</span>Heading three levels deep<span>"</span></span>))))

(<span>defun</span> <span>outer-section</span> (html)
  <span><span>"</span>Uses HTML from elsewhere and embed it into a section<span>"</span></span>
  (with-html-string
    (:section
     (:h* <span><span>"</span>Heading two levels deep<span>"</span></span>)
     (:section
      (:raw html)))))

(outer-section (inner-section))
<span><span>;</span>; &lt;section&gt;</span>
<span><span>;</span>; &lt;h2&gt;Heading two levels deep&lt;/h2&gt;</span>
<span><span>;</span>; &lt;section&gt;&lt;h3&gt;Heading three levels deep&lt;/h3&gt;</span>
<span><span>;</span>; &lt;/section&gt;</span>
<span><span>;</span>;&lt;/section&gt;</span></pre></div>
<h2 tabindex="-1" id="user-content-deftag" dir="auto"><a href="#deftag"><code>deftag</code></a></h2>
<p dir="auto">The stumbling block for all sexp-based HTML generators is order of
evaluation. It's tempting to write something like this:</p>
<div data-snippet-clipboard-copy-content=" ;; Doesn't work
 (defun field (control)
   (with-html (:p control)))

 (defun input (default &amp;key name label (type &quot;text&quot;))
   (with-html
     (:label :for name label)
     (:input :name name :id name :type type :value default)))"><pre><code> ;; Doesn't work
 (defun field (control)
   (with-html (:p control)))

 (defun input (default &amp;key name label (type "text"))
   (with-html
     (:label :for name label)
     (:input :name name :id name :type type :value default)))
</code></pre></div>
<p dir="auto">But it won't work: in <code>(field (input "Default" :name "why" :label "Reason"))</code>, <code>(input)</code> gets evaluated before <code>(field)</code>, and the HTML
is printed inside-out.</p>
<p dir="auto">Macros do work:</p>
<div data-snippet-clipboard-copy-content=" (defmacro field (control)
   `(with-html (:p ,control)))

 (defmacro input (name label &amp;key (type &quot;text&quot;))
   `(with-html
      (:label :for ,name ,label)
      (:input :name ,name :id ,name :type ,type)))"><pre><code> (defmacro field (control)
   `(with-html (:p ,control)))

 (defmacro input (name label &amp;key (type "text"))
   `(with-html
      (:label :for ,name ,label)
      (:input :name ,name :id ,name :type ,type)))
</code></pre></div>
<p dir="auto">But we can do better than this. Spinneret provides a macro-writing
macro, <code>deftag</code>, which lets you <em>refactor</em> HTML without <em>hiding</em> it.</p>
<div data-snippet-clipboard-copy-content=" (deftag field (control attrs)
  `(:p ,@attrs ,@control))

 (deftag input (default attrs &amp;key name label (type &quot;text&quot;))
   (once-only (name)
     `(progn
        (:label :for ,name ,label)
        (:input :name ,name :id ,name :type ,type
          ,@attrs
          :value (progn ,@default)))))"><pre><code> (deftag field (control attrs)
  `(:p ,@attrs ,@control))

 (deftag input (default attrs &amp;key name label (type "text"))
   (once-only (name)
     `(progn
        (:label :for ,name ,label)
        (:input :name ,name :id ,name :type ,type
          ,@attrs
          :value (progn ,@default)))))
</code></pre></div>
<p dir="auto">A macro defined using <code>deftag</code> takes its arguments just like an HTML
element. Instead of</p>
<div data-snippet-clipboard-copy-content="(input &quot;Default&quot; :name &quot;why&quot; :label &quot;Reason&quot;) ; defmacro"><pre><code>(input "Default" :name "why" :label "Reason") ; defmacro
</code></pre></div>
<p dir="auto">You write</p>
<div data-snippet-clipboard-copy-content="(input :name &quot;why&quot; :label &quot;Reason&quot; &quot;Default&quot;) ; deftag"><pre><code>(input :name "why" :label "Reason" "Default") ; deftag
</code></pre></div>
<p dir="auto">The macro re-arranges the arguments so they can be bound to an
ordinary lambda list, like the one above: the body of the tag is bound
to the first argument, and matching attributes are bound to keywords.
Multiple <code>:class</code> arguments, <code>:dataset</code>, and other shorthands are
handled exactly as in the usual HTML syntax.</p>
<p dir="auto">But the great advantage of <code>deftag</code> is how it handles attributes which
are <em>not</em> bound to keywords. In the definition of <code>input</code> using
<code>deftag</code>, you see that the <code>attrs</code> catch-all argument is spliced into
the call to <code>:input</code>. This means that any unhandled attributes pass
through to the actual input element.</p>
<div data-snippet-clipboard-copy-content="(input :name &quot;why&quot; :label &quot;Reason&quot; :required t :class &quot;special&quot; &quot;Default&quot;)
=> <label for=why>Reason</label>
   <input class=special name=why id=why type=text required value=Default>"><pre><code>(input :name "why" :label "Reason" :required t :class "special" "Default")
=&gt; &lt;label for=why&gt;Reason&lt;/label&gt;
   &lt;input class=special name=why id=why type=text required value=Default&gt;
</code></pre></div>
<p dir="auto">In effect, <code>input</code> <em>extends</em> the <code>:input</code> tag, almost like a subclass.
This is a very idiomatic and expressive way of building abstractions
over HTML.</p>
<p dir="auto">(Spinneret used to provide a more elaborate way of building HTML
abstractions, <code>deftemplate</code>, but <code>deftag</code> is simpler and more useful.)</p>
<h2 tabindex="-1" id="user-content-spinneret-in-parenscript" dir="auto"><a href="#spinneret-in-parenscript">Spinneret in Parenscript</a></h2>
<p dir="auto">To use Spinneret with Parenscript, load the system <code>spinneret/ps</code>.</p>
<p dir="auto">The semantics of Spinneret in Parenscript are almost the same. There
is no <code>with-html-string</code>, and <code>with-html</code> returns a
<code>DocumentFragment</code>.</p>
<p dir="auto">If Markdown support is enabled, strings in function position are still
parsed as Markdown, but supplying arguments triggers an error (since
Parenscript does not have <code>format</code>).</p>
<p dir="auto"><code>get-html-path</code> is not implemented for Parenscript.</p>
<p dir="auto">Neither :ATTRS nor :TAG is available in Parenscript.</p>
<h2 tabindex="-1" id="user-content-parenscript-in-spinneret" dir="auto"><a href="#parenscript-in-spinneret">Parenscript in Spinneret</a></h2>
<p dir="auto">To use Parenscript in Spinneret, remember to wrap the <code>ps</code> macro with <code>:raw</code>, otherwise the generated JavaScript will be escaped.</p>
<div dir="auto" data-snippet-clipboard-copy-content="(with-html-string
  (:script
    (:raw (ps
            (defun greeting ()
              (alert &quot;Hello&quot;))))))
=>
&quot;<script>function greeting() {
    __PS_MV_REG = [];
    return alert('Hello');
};</script>&quot;

(with-html-string
  (:div :onclick (:raw (ps (alert &quot;Hello&quot;)))))
&quot;<div onclick=\&quot;alert('Hello');\&quot;></div>&quot;"><pre>(with-html-string
  (:script
    (:raw (ps
            (<span>defun</span> <span>greeting</span> ()
              (alert <span><span>"</span>Hello<span>"</span></span>))))))
=&gt;
<span><span>"</span>&lt;script&gt;function greeting() {</span>
<span>    __PS_MV_REG = [];</span>
<span>    return alert('Hello');</span>
<span>};&lt;/script&gt;<span>"</span></span>

(with-html-string
  (:div :onclick (:raw (ps (alert <span><span>"</span>Hello<span>"</span></span>)))))
<span><span>"</span>&lt;div onclick=<span>\"</span>alert('Hello');<span>\"</span>&gt;&lt;/div&gt;<span>"</span></span></pre></div>
<h2 tabindex="-1" id="user-content-validation" dir="auto"><a href="#validation">Validation</a></h2>
<p dir="auto">Spinneret does not do document validation, but it does warn, at compile time, about invalid tags and attributes.</p>
<p dir="auto">Although HTML5 does include a mechanism for application-specific
attributes (the <code>data-</code> prefix), some client-side frameworks choose to
employ their own prefixes instead. You can disable validation for a
given prefix by adding it to <code>*unvalidated-attribute-prefixes*</code>.</p>
<div data-snippet-clipboard-copy-content="(pushnew &quot;ng-&quot; *unvalidated-attribute-prefixes* :test #’equal)"><pre><code>(pushnew "ng-" *unvalidated-attribute-prefixes* :test #’equal)
</code></pre></div>
<p dir="auto">You can disable attribute validation altogether by adding the empty
string to the list:</p>
<div data-snippet-clipboard-copy-content=";; Disable attribute validation.
(setf *unvalidated-attribute-prefixes* '(&quot;&quot;))"><pre><code>;; Disable attribute validation.
(setf *unvalidated-attribute-prefixes* '(""))
</code></pre></div>
<p dir="auto">Tags are considered valid if they are defined as part of the HTML standard, or if they match the rules for the name of a <a href="https://html.spec.whatwg.org/multipage/custom-elements.html#valid-custom-element-name" rel="nofollow">custom element</a> – basically, start with an ASCII alphabetic character and include a hyphen. For custom elements, attributes are not validated.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Harvard: Student Use Cases for AI (108 pts)]]></title>
            <link>https://hbsp.harvard.edu/inspiring-minds/student-use-cases-for-ai</link>
            <guid>37659542</guid>
            <pubDate>Tue, 26 Sep 2023 14:07:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hbsp.harvard.edu/inspiring-minds/student-use-cases-for-ai">https://hbsp.harvard.edu/inspiring-minds/student-use-cases-for-ai</a>, See on <a href="https://news.ycombinator.com/item?id=37659542">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Google Podcasts to shut down in 2024 (191 pts)]]></title>
            <link>https://techcrunch.com/2023/09/26/google-podcasts-to-shut-down-in-2024-with-listeners-migrated-to-youtube-music/</link>
            <guid>37659482</guid>
            <pubDate>Tue, 26 Sep 2023 14:04:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2023/09/26/google-podcasts-to-shut-down-in-2024-with-listeners-migrated-to-youtube-music/">https://techcrunch.com/2023/09/26/google-podcasts-to-shut-down-in-2024-with-listeners-migrated-to-youtube-music/</a>, See on <a href="https://news.ycombinator.com/item?id=37659482">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Google announced this morning it will be shutting down its Google Podcasts app later in 2024 as part of its broader transition to move its streaming listeners over to YouTube Music. The company earlier this year announced YouTube Music <a href="https://techcrunch.com/2023/04/27/youtube-music-officially-launches-podcasts-for-listeners-in-the-us/">would begin supporting podcasts in the U.S.</a>, which will expand globally by year-end, and more recently said it was adding the ability for <a href="https://techcrunch.com/2023/08/25/youtube-to-support-rss-uploads-for-podcasters-by-year-end-plus-private-feeds-in-youtube-music/">podcasters to upload their RSS feeds to YouTube</a> also by year-end.</p>
<p>Today, Google says it plans on further increasing its investment in the podcast experience on YouTube Music and making it more of a destination for podcast fans with features focused on discovery, community, and switching between audio podcasts and video. The latter is something rival Spotify has also been working on with its <a href="https://techcrunch.com/2022/11/15/spotifys-video-podcast-publishing-tools-expand-to-creators-worldwide/">rollout of video podcast support</a> to creators worldwide last year along with <a href="https://techcrunch.com/2023/03/08/spotify-is-revamping-its-podcaster-tools-including-anchor-and-is-partnering-with-patreon/">community features like Q&amp;As and polls.</a></p>
<p>However, to make YouTube Music the new home for podcasts, that means moving users away from the current offering, Google Podcasts. The company notes this plan reflects how people are already listening. According to Edison, about 23% of weekly podcast users in the U.S. say YouTube is their most frequently used service versus just 4% for Google Podcasts.</p>
<p>To help users with the transition to YouTube Music, the company will offer Google Podcast users a migration tool and the ability to add podcast RSS feeds to their YouTube Music library, including shows that aren’t currently hosted by YouTube — something <a href="https://techcrunch.com/2023/08/25/youtube-to-support-rss-uploads-for-podcasters-by-year-end-plus-private-feeds-in-youtube-music/">it had announced last month was in the works.</a> These migration tools aren’t yet available but will be worked on in the coming weeks and months before being rolled out to all users.</p>
<p>In addition, Google will also support the option to download an OPML file of their show subscriptions from Google Podcasts, which they can upload to any app that supports importing if they don’t want to move to YouTube Music.</p>
<p>“We know this transition will take time, but these efforts will allow us to build an amazing product and a single destination that rewards creators and artists and provides fans with the best Podcasts experience,” <a href="https://blog.youtube/news-and-events/creating-a-centralized-podcast-destination-on-youtube-music/">a YouTube blog post explained.</a> “For now, nothing is changing and fans will continue to have access to YouTube, YouTube Music, and Google Podcasts. We’re committed to being transparent in communicating future changes with our users and podcasters and will have more to share about this process in the coming months,” it said.</p>
<p>Google has been slowly shifting users away from older products in its effort to make YouTube Music more of a competitor to Spotify, Apple Music, Amazon Music and others. In 2020, YouTube Music offered a similar transition strategy to <a href="https://techcrunch.com/2020/05/12/youtube-music-adds-a-transfer-option-ahead-of-google-play-musics-shutdown-this-year/">move music listeners away from Google Play Music ahead of its shutdown</a> that year. However, podcasts have a more difficult product because so many users turn to YouTube itself to listen to podcasts, not to the dedicated Google Podcasts app. Meanwhile, YouTube Music hasn’t been ready to support podcasts until more recently.</p>
<p>The move will leave only Apple among the major players that hasn’t consolidated music and podcasts into a single destination, as Spotify, Amazon and Pandora all offer both types of audio in their flagship applications.</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Newly discovered deep-sea enzyme breaks down PET plastic (148 pts)]]></title>
            <link>https://phys.org/news/2023-09-newly-deep-sea-enzyme-pet-plastic.amp</link>
            <guid>37659327</guid>
            <pubDate>Tue, 26 Sep 2023 13:52:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phys.org/news/2023-09-newly-deep-sea-enzyme-pet-plastic.amp">https://phys.org/news/2023-09-newly-deep-sea-enzyme-pet-plastic.amp</a>, See on <a href="https://news.ycombinator.com/item?id=37659327">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    

    
    

    <div id="popover">
            <p>
             This article has been reviewed according to Science&nbsp;X's <a href="https://sciencex.com/help/editorial-process/" target="_blank">editorial process</a>
                and <a href="https://sciencex.com/help/editorial-standards/" target="_blank"> policies</a>. 
                <a href="https://sciencex.com/help/editorial-team/" target="_blank">Editors</a> have highlighted the following attributes while ensuring the content's credibility: 
            </p>
            <div>
                                            <p><span></span> fact-checked
                </p>
                                                            <p><span></span> peer-reviewed publication
                </p>
                                                                                                                                                                            <p><span></span> proofread
                </p>
                                          
            </div>
            </div>

    

    <div>
        <p>
                                    by Eva Sittig
                                                
                    , 
                                                            
                                    Christian-Albrechts-Universität zu Kiel
                                        
                            </p>
        
    </div>

    
    
<div>
    <figure itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject" id="i1110920">
        <amp-img on="tap:lbx1110920" role="button" tabindex="0" src="https://scx2.b-cdn.net/gfx/news/2023/newly-discovered-deep.jpg" srcset="https://scx1.b-cdn.net/csz/news/800w/2023/newly-discovered-deep.jpg?f=webp 800w" layout="responsive" width="1280" height="735" alt="Newly discovered deep-sea enzyme breaks down PET plastic">
        </amp-img>
                    <figcaption itemprop="description" on="tap:AMP.setState({expanded: !expanded})" tabindex="0" role="button" [class]="expanded ? 'desc expanded' : 'desc'">The crystal structure of PET46 resembles the crystal structure of the IsPETase and LCC - with unique features. Credit: <i>Communications Chemistry</i> (2023). DOI: 10.1038/s42004-023-00998-z</figcaption>
            </figure>
    <amp-lightbox id="lbx1110920" layout="nodisplay" animate-in="fly-in-bottom">
        <div>
            <p><span>× </span> close
            </p>
            <amp-img on="tap:lbx1110920" role="button" tabindex="0" src="https://scx2.b-cdn.net/gfx/news/hires/2023/newly-discovered-deep.jpg" layout="fill" alt="Newly discovered deep-sea enzyme breaks down PET plastic">
            </amp-img>
            <figcaption on="tap:AMP.setState({collapsed: !collapsed})" role="button" tabindex="0" [class]="!collapsed ? 'expanded' : ''">
                The crystal structure of PET46 resembles the crystal structure of the IsPETase and LCC - with unique features. Credit: <i>Communications Chemistry</i> (2023). DOI: 10.1038/s42004-023-00998-z
            </figcaption>
        </div>
    </amp-lightbox>
</div>
<p>Plastic pollution is increasingly affecting the health of coasts and oceans. One well-known problem is plastic bottles made from polyethylene terephthalate, or PET.</p>

		    <!-- Phys.org - News - AMP -->
		

		        
<p>A new study involving scientists from Professor Ruth Schmitz-Streit's research group at Kiel University has shown for the first time, using microorganisms from the deep sea, that polymers such as PET are continuously degraded by an enzyme. Researchers from the University of Hamburg and the Heinrich-Heine-University Düsseldorf played a major role in the microbiological study.
</p><p>The results fundamentally expand the knowledge of PET-degrading enzymes, the underlying mechanism and the evolutionary understanding of the diversity of putative PET-degrading enzymes throughout the global ocean. The research team <a href="https://www.nature.com/articles/s42004-023-00998-z">published the results in the journal <i>Communications Chemistry</i></a>, where they discuss both biotechnological applications and the high relevance for biogeochemical processes in the ocean and on land.
</p><p>The study highlights a special feature of the PET-degrading enzyme. "In our study, we have discovered a new genetic resource from deep-sea organisms belonging to the archaea," says Professor Ruth Schmitz-Streit, head of the Molecular Biology of Microorganisms working group at the Institute of General Microbiology (IfAM) and member of the research priority area Kiel Marine Science (KMS) at Kiel University. Until now, about 80 different PET-degrading enzymes were known, most of which were found in bacteria or fungi.
</p><p>"Our data contribute to a better understanding of the ecological role of deep-sea archaea and the possible degradation of PET waste in the sea," says the microbiologist.
</p><h2>The new enzyme: PET46</h2>
<p>Using a metagenomic approach, the research team has identified and biochemically described the PET-degrading enzyme PET46 from a non-cultured deep-sea microorganism for the first time. This involved identifying the gene from a <a href="https://phys.org/tags/deep+sea/" rel="tag">deep-sea</a> sample based on similarities to known sequences, synthesizing the corresponding coding gene, producing the protein in the bacterium Escherichia coli and then studying it biochemically and structurally.
</p><p>PET46 has many unusual properties and adds to the scaffold diversity of PET-active enzymes. Structurally, the enzyme differs significantly from those previously discovered. For example, it has the ability to degrade both very long-chain PET molecules, known as polymers, and short-chain PET molecules, known as oligomers, which means that degradation can be continuous.
</p><p>Among other things, PET46 uses a completely different mechanism for substrate binding than previously known PET-degrading enzymes. The researchers describe an unusual 'lid' of 45 amino acids above the enzyme's active center as crucial for binding. In other PET enzymes, aromatic <a href="https://phys.org/tags/amino+acids/" rel="tag">amino acids</a> close to the active site are typical.
</p><h2>Promising biotechnology applications</h2>
<p>At the <a href="https://phys.org/tags/molecular+level/" rel="tag">molecular level</a>, PET46 is very similar to another enzyme, ferulic acid esterase. This degrades the natural polymer lignin in <a href="https://phys.org/tags/plant+cell+walls/" rel="tag">plant cell walls</a> by breaking down lignin polymers to release sugars from woody plant parts. Lignin and PET have many structural similarities, so the PET-degrading enzymes found in nature may be important for composting wood in forest soils, for example.
</p><p>The biochemical properties of PET46 therefore make it a very interesting <a href="https://phys.org/tags/enzyme/" rel="tag">enzyme</a> both for marine and terrestrial plastics and for biotechnology. Compared to the best-characterized PET-degrading enzymes from bacteria and composting plants, PET46 is more efficient at 70° Celsius than these reference enzymes at their respective optimum temperatures.
</p><p>The research was carried out as part of the PLASTISEA project, coordinated by Professor Ute Hentschel Humeida of the GEOMAR Helmholtz Center for Ocean Research in Kiel. First author Dr. Jennifer Chow from the University of Hamburg and first author Dr. Pablo Pérez-Garcia, who works as a research assistant in Schmitz-Streit's group, contributed equally to the study.
		 

            <!-- Phys.org - News - AMP -->
		</p>
		
					<div>
				                    <p>
                        <strong>More information:</strong> 
                        Pablo Perez-Garcia et al, An archaeal lid-containing feruloyl esterase degrades polyethylene terephthalate, <i>Communications Chemistry</i> (2023).  <a data-doi="1" href="https://dx.doi.org/10.1038/s42004-023-00998-z" target="_blank">DOI: 10.1038/s42004-023-00998-z</a>
                    </p>
																
				                    <p>
                        <strong>Journal information:</strong> 
                        							<a href="https://phys.org/journals/communications-chemistry/">Communications Chemistry</a>
                                                                    <a href="https://www.nature.com/commschem/" target="_blank" rel="nofollow">
										<i></i>
									</a>
                                                                 
						                    </p>
							</div>
    											
				
                    <p>
                    Provided by
                            
                                            Christian-Albrechts-Universität zu Kiel
                     
                </p>
        

        
        
	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firefox 118 (360 pts)]]></title>
            <link>https://www.mozilla.org/en-US/firefox/118.0/releasenotes/</link>
            <guid>37658915</guid>
            <pubDate>Tue, 26 Sep 2023 13:22:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mozilla.org/en-US/firefox/118.0/releasenotes/">https://www.mozilla.org/en-US/firefox/118.0/releasenotes/</a>, See on <a href="https://news.ycombinator.com/item?id=37658915">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article id="main-content">

    
    <header>
      <div>
          
            <p>Version 118.0, first offered to Release channel users on September 26, 2023</p>
            
          
        </div></header>
  

    <section>
    
      
        
          <div>
              <ul>
        
        
  <li id="note-789750">
    <p>Automated translation of web content is now available to Firefox users! Unlike cloud-based alternatives, translation is done locally in Firefox, so that the text being translated does not leave your machine.</p>
    
  </li>

        
      
        
        
  <li id="note-789751">
    <p>Web Audio in Firefox now uses the FDLIBM math library on all systems to improve anonymity with <a href="https://support.mozilla.org/kb/firefox-protection-against-fingerprinting">Fingerprint Protection</a>.</p>
    
  </li>

        
      
        
        
  <li id="note-789752">
    <p>The visibility of fonts to websites has been restricted to system fonts and language pack fonts to mitigate font fingerprinting in Private Browsing windows.</p>
    
  </li>

        
      
        
        
  <li id="note-789753">
    <p>Video Effects and background blur are now available to Firefox users on Google Meet! (Note: These effects have also been released retroactively to support Firefox versions back to Firefox 115.)</p>
    
  </li>

        
      
        
        
  <li id="note-789762">
    <p>Firefox Suggest users (US-only at this time) will now be able to see browser add-on suggestions right in the address bar based on their keywords.</p>
    
  </li>

        
              </ul>
            </div>
        
      

      
        
          <div>
              <ul>
        
        
  <li id="note-789761">
    <p>Various <a href="https://www.mozilla.org/en-US/security/advisories/mfsa2023-41/">security</a> fixes.</p>
    
  </li>

        
              </ul>
            </div>
        
      

      

      
        
          <div>
              <ul>
        
        
  <li id="note-789754">
    <p>You can find information about policy updates and enterprise specific bug fixes in the <a href="https://support.mozilla.org/kb/firefox-enterprise-118-release-notes">Firefox for Enterprise 118 Release Notes</a>.</p>
    
  </li>

        
              </ul>
            </div>
        
      

      
      

      
        
        
          
        
      

      
        
          <div>
              <ul>
        
        
  <li id="note-789695">
    <p>10 new CSS math functions are now supported: round, mod, rem, pow, sqrt, hypot, log, exp, abs, sign.</p>
    
  </li>

        
      
        
        
  <li id="note-789744">
    <p><a href="https://github.com/annevk/orb">OpaqueResponseBlocking</a> is now enabled by default.</p>
    
  </li>

        
              </ul>
            </div>
        
      

      

      

      

      
        
          <div id="community">
            <ul>
        
        
  <li id="note-789755">
    <p>With the release of Firefox 118, we are pleased to welcome the developers who contributed their first code change to Firefox in this release, 8 of whom were brand new volunteers! Please join us in thanking each of these diligent and enthusiastic individuals, and take a look at their contributions:</p>
<ul>
<li>Andrew de Rozario: <a href="https://bugzilla.mozilla.org/1791983">1791983</a></li>
<li>jackyzy823: <a href="https://bugzilla.mozilla.org/1847513">1847513</a></li>
<li>Janne Heß: <a href="https://bugzilla.mozilla.org/1460986">1460986</a></li>
<li>Krishna Ravishankar: <a href="https://bugzilla.mozilla.org/1578994">1578994</a></li>
<li>Rayyan: <a href="https://bugzilla.mozilla.org/1844745">1844745</a></li>
<li>Shauryadubey123: <a href="https://bugzilla.mozilla.org/1819352">1819352</a></li>
<li>Simon Thoby: <a href="https://bugzilla.mozilla.org/1832576">1832576</a>, <a href="https://bugzilla.mozilla.org/1847763">1847763</a></li>
<li>wujiahuan: <a href="https://bugzilla.mozilla.org/1782159">1782159</a></li>
</ul>
    
  </li>

        
            </ul>
          </div>
        
      
    
    </section>

    <div>
      <p>
        <h2>Get the most recent version</h2>
        
      </p>
    </div>

    
    <section>
      <a href="https://www.mozilla.org/en-US/firefox/all/#product-desktop-release">All Firefox downloads</a>
    </section>
    

    
    
    
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google is picking ChatGPT responses from Quora as correct answer (364 pts)]]></title>
            <link>https://twitter.com/8teapi/status/1706520893621784780</link>
            <guid>37658319</guid>
            <pubDate>Tue, 26 Sep 2023 12:31:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/8teapi/status/1706520893621784780">https://twitter.com/8teapi/status/1706520893621784780</a>, See on <a href="https://news.ycombinator.com/item?id=37658319">Hacker News</a></p>
Couldn't get https://twitter.com/8teapi/status/1706520893621784780: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Yandex open sourced it's BI tool DataLens (171 pts)]]></title>
            <link>https://github.com/datalens-tech</link>
            <guid>37657772</guid>
            <pubDate>Tue, 26 Sep 2023 11:36:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/datalens-tech">https://github.com/datalens-tech</a>, See on <a href="https://news.ycombinator.com/item?id=37657772">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      


    <div>
      <p><a href="#start-of-content">Skip to content</a>
      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p><header role="banner" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  

  <div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        
      </div>
</header>

      
    </div>

  








    


    
    <include-fragment data-base-src="https://github.com/notifications/beta/shelf"></include-fragment>






  <div data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
      <main>
        











<div itemscope="" itemtype="http://schema.org/Organization">
  


<header>
  <div>
        <p><img itemprop="image" src="https://avatars.githubusercontent.com/u/134070780?s=200&amp;v=4" width="100" height="100" alt="@datalens-tech">
        </p>
      

      

    </div>

  <div>
  <nav data-url="/users/datalens-tech/tab_counts" aria-label="Organization">
    <div>
      <ul>
          <li data-tab-item="org-header-overview-tab">
  <a href="https://github.com/datalens-tech">
    
    Overview
  </a>
</li>

          <li data-tab-item="org-header-repositories-tab">
  <a href="https://github.com/orgs/datalens-tech/repositories">
    
    Repositories
      <span title="Not available" data-view-component="true"></span>
  </a>
</li>

          
          <li data-tab-item="org-header-projects-tab">
  <a href="https://github.com/orgs/datalens-tech/projects" data-hotkey="g b">
    
    Projects
      <span title="Not available" data-view-component="true"></span>
  </a>
</li>

          <li data-tab-item="org-header-packages-tab">
  <a href="https://github.com/orgs/datalens-tech/packages">
    
    Packages
  </a>
</li>

          
          <li data-tab-item="org-header-people-tab">
  <a href="https://github.com/orgs/datalens-tech/people">
    
    People
      <span title="Not available" data-view-component="true"></span>
  </a>
</li>

          
          
          
          
      </ul>

      
    </div>
  </nav>
</div>


    
</header>


  <div data-view-component="true">

  <div data-hpc="" data-view-component="true">
  

  

  

     

    <div>

  <h2>
        Pinned

    <svg style="box-sizing: content-box; color: var(--color-icon-primary);" width="16" height="16" viewBox="0 0 16 16" fill="none" data-view-component="true">
  <circle cx="8" cy="8" r="7" stroke="currentColor" stroke-opacity="0.25" stroke-width="2" vector-effect="non-scaling-stroke" fill="none"></circle>
  <path d="M15 8a7.002 7.002 0 00-7-7" stroke="currentColor" stroke-width="2" stroke-linecap="round" vector-effect="non-scaling-stroke"></path>
</svg>
    <span role="status" aria-live="polite" data-error-text="Something went wrong." data-success-text="Order updated."></span>
  </h2>

      <ol>
      <li>
  <div>
      


      <p>
        A modern, scalable analytics system
      </p>

      <p>
          <a href="https://github.com/datalens-tech/datalens/stargazers">
            <svg aria-label="stars" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z"></path>
</svg>
            244
          </a>
          <a href="https://github.com/datalens-tech/datalens/forks">
            <svg aria-label="forks" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>
            4
          </a>
      </p>
    </div>
</li>

      <li>
  <div>
      


      <p>
        User interface for DataLens
      </p>

      <p>
          <span>
  <span></span>
  <span itemprop="programmingLanguage">TypeScript</span>
</span>

          <a href="https://github.com/datalens-tech/datalens-ui/stargazers">
            <svg aria-label="stars" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z"></path>
</svg>
            43
          </a>
          <a href="https://github.com/datalens-tech/datalens-ui/forks">
            <svg aria-label="forks" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>
            2
          </a>
      </p>
    </div>
</li>

      <li>
  <div>
      


      <p>
        Data processing backend for DataLens
      </p>

      <p>
          <span>
  <span></span>
  <span itemprop="programmingLanguage">Python</span>
</span>

          <a href="https://github.com/datalens-tech/datalens-backend/stargazers">
            <svg aria-label="stars" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z"></path>
</svg>
            40
          </a>
          <a href="https://github.com/datalens-tech/datalens-backend/forks">
            <svg aria-label="forks" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>
            2
          </a>
      </p>
    </div>
</li>

      <li>
  <div>
      


      <p>
        Configuration object storage for DataLens
      </p>

      <p>
          <span>
  <span></span>
  <span itemprop="programmingLanguage">TypeScript</span>
</span>

          <a href="https://github.com/datalens-tech/datalens-us/stargazers">
            <svg aria-label="stars" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z"></path>
</svg>
            18
          </a>
          <a href="https://github.com/datalens-tech/datalens-us/forks">
            <svg aria-label="fork" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>
            1
          </a>
      </p>
    </div>
</li>

</ol>

</div>


    <div id="org-profile-repositories">
      <h3>
        
        Repositories
      </h3>
      <div>
          <!-- '"` --><!-- </textarea></xmp> --><form data-autosearch-results-container="org-repositories" role="search" data-turbo="false" action="/datalens-tech" accept-charset="UTF-8" method="get">
            <div>
                <details id="type-options">
                    <summary aria-haspopup="true" data-view-component="true">    <span>Type</span>
                    <span></span>
</summary>                  <details-menu>
                    <div>
                      <header>
                        <span>Select type</span>
                        
                      </header>
                      <p><label role="menuitemradio" aria-checked="true" tabindex="0">
                            
                            
                            <span data-menu-button-text="">All</span>
                          </label>
                          <label role="menuitemradio" aria-checked="false" tabindex="0">
                            
                            
                            <span data-menu-button-text="">Public</span>
                          </label>
                          <label role="menuitemradio" aria-checked="false" tabindex="0">
                            
                            
                            <span data-menu-button-text="">Sources</span>
                          </label>
                          <label role="menuitemradio" aria-checked="false" tabindex="0">
                            
                            
                            <span data-menu-button-text="">Forks</span>
                          </label>
                          <label role="menuitemradio" aria-checked="false" tabindex="0">
                            
                            
                            <span data-menu-button-text="">Archived</span>
                          </label>
                          <label role="menuitemradio" aria-checked="false" tabindex="0">
                            
                            
                            <span data-menu-button-text="">Mirrors</span>
                          </label>
                          <label role="menuitemradio" aria-checked="false" tabindex="0">
                            
                            
                            <span data-menu-button-text="">Templates</span>
                          </label>
                      </p>
                    </div>
                  </details-menu>
                </details>

                  <details id="language-options">
                      <summary aria-haspopup="true" data-view-component="true">    <span>Language</span>
                      <span></span>
</summary>                    <details-menu>
                      <div>
                        <header>
                          <span>Select language</span>
                          
                        </header>
                        <p><label role="menuitemradio" aria-checked="true" tabindex="0">
                            
                            
                            <span data-menu-button-text="">All</span>
                          </label>
                            <label role="menuitemradio" aria-checked="false" tabindex="0">
                              
                              
                              <span data-menu-button-text="">Python</span>
                            </label>
                            <label role="menuitemradio" aria-checked="false" tabindex="0">
                              
                              
                              <span data-menu-button-text="">TypeScript</span>
                            </label>
                        </p>
                      </div>
                    </details-menu>
                  </details>

                <details id="sort-options">
                    <summary aria-haspopup="true" data-view-component="true">    <span>Sort</span>
                    <span></span>
</summary>                  <details-menu>
                    <div>
                      <header>
                        <span>Select order</span>
                        
                      </header>
                      <p><label role="menuitemradio" aria-checked="true" tabindex="0">
                          
                          
                          <span data-menu-button-text="">Last updated</span>
                        </label>
                        <label role="menuitemradio" aria-checked="false" tabindex="0">
                          
                          
                          <span data-menu-button-text="">Name</span>
                        </label>
                        <label role="menuitemradio" aria-checked="false" tabindex="0">
                          
                          
                          <span data-menu-button-text="">Stars</span>
                        </label>
                      </p>
                    </div>
                  </details-menu>
                </details>
                
              </div>
</form>
        </div>
      <div id="org-repositories" data-delay-results="">
    <p><span data-autosearch-results="">Showing 7 of 7 repositories</span></p><div data-view-component="true">
  
  
    <ul data-view-component="true">
        <li data-view-component="true">          <div data-view-component="true" itemprop="owns" itemtype="http://schema.org/Code" itemscope="itemscope">

        <p><span>
  <span></span>
  <span itemprop="programmingLanguage">TypeScript</span>
</span></p><a href="https://github.com/datalens-tech/datalens-ui/stargazers" data-view-component="true">
          
          43
</a>
        <p><span data-view-component="true">
          Apache-2.0
</span></p><a href="https://github.com/datalens-tech/datalens-ui/forks" data-view-component="true">
          <svg aria-label="fork" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>
          2
</a>
      <a href="https://github.com/datalens-tech/datalens-ui/issues" data-view-component="true">
        
        1
</a>

      <a href="https://github.com/datalens-tech/datalens-ui/pulls" data-view-component="true">
        
        1
</a>
      <p><span data-view-component="true">
          Updated <relative-time datetime="2023-09-26T15:59:53Z">Sep 26, 2023</relative-time>
</span></p></div>
</li>
        <li data-view-component="true">          <div itemprop="owns" itemtype="http://schema.org/Code" itemscope="itemscope" data-view-component="true">
  
    <div data-view-component="true">
        <p><a itemprop="name codeRepository" data-hovercard-type="repository" data-hovercard-url="/datalens-tech/datalens-us/hovercard" href="https://github.com/datalens-tech/datalens-us" data-view-component="true">
          datalens-us
</a>
        <span title="Label: Public" data-view-component="true">
          Public
</span></p><p itemprop="description" data-view-component="true">
            Configuration object storage for DataLens
</p></div>
    <div data-view-component="true">

        <p><span>
  <span></span>
  <span itemprop="programmingLanguage">TypeScript</span>
</span></p><a href="https://github.com/datalens-tech/datalens-us/stargazers" data-view-component="true">
          
          18
</a>
        <p><span data-view-component="true">
          Apache-2.0
</span></p><a href="https://github.com/datalens-tech/datalens-us/forks" data-view-component="true">
          <svg aria-label="fork" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>
          1
</a>
      <a href="https://github.com/datalens-tech/datalens-us/issues" data-view-component="true">
        
        0
</a>

      <a href="https://github.com/datalens-tech/datalens-us/pulls" data-view-component="true">
        
        2
</a>
      <p><span data-view-component="true">
          Updated <relative-time datetime="2023-09-26T08:06:22Z">Sep 26, 2023</relative-time>
</span></p></div></div>
</li>
        <li data-view-component="true">          <div itemprop="owns" itemtype="http://schema.org/Code" itemscope="itemscope" data-view-component="true">
  
    <div data-view-component="true">
        <p><a itemprop="name codeRepository" data-hovercard-type="repository" data-hovercard-url="/datalens-tech/datalens/hovercard" href="https://github.com/datalens-tech/datalens" data-view-component="true">
          datalens
</a>
        <span title="Label: Public" data-view-component="true">
          Public
</span></p><p itemprop="description" data-view-component="true">
            A modern, scalable analytics system
</p></div>
    <div data-view-component="true">


        <a href="https://github.com/datalens-tech/datalens/stargazers" data-view-component="true">
          
          244
</a>
        <p><span data-view-component="true">
          Apache-2.0
</span></p><a href="https://github.com/datalens-tech/datalens/forks" data-view-component="true">
          <svg aria-label="fork" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>
          4
</a>
      <a href="https://github.com/datalens-tech/datalens/issues" data-view-component="true">
        
        9
</a>

      <a href="https://github.com/datalens-tech/datalens/pulls" data-view-component="true">
        
        0
</a>
      <p><span data-view-component="true">
          Updated <relative-time datetime="2023-09-25T14:07:07Z">Sep 25, 2023</relative-time>
</span></p></div></div>
</li>
        <li data-view-component="true">          <div data-view-component="true" itemprop="owns" itemtype="http://schema.org/Code" itemscope="itemscope">

        <p><span>
  <span></span>
  <span itemprop="programmingLanguage">Python</span>
</span></p><a href="https://github.com/datalens-tech/datalens-backend/stargazers" data-view-component="true">
          
          40
</a>
        <p><span data-view-component="true">
          Apache-2.0
</span></p><a href="https://github.com/datalens-tech/datalens-backend/forks" data-view-component="true">
          <svg aria-label="fork" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>
          2
</a>
      <a href="https://github.com/datalens-tech/datalens-backend/issues" data-view-component="true">
        
        0
</a>

      <a href="https://github.com/datalens-tech/datalens-backend/pulls" data-view-component="true">
        
        0
</a>
      <p><span data-view-component="true">
          Updated <relative-time datetime="2023-09-25T13:51:04Z">Sep 25, 2023</relative-time>
</span></p></div>
</li>
        <li data-view-component="true">          <div data-view-component="true" itemprop="owns" itemtype="http://schema.org/Code" itemscope="itemscope">

        <p><span>
  <span></span>
  <span itemprop="programmingLanguage">Python</span>
</span>


        <span data-view-component="true">
          0
</span>
        <span data-view-component="true">
          Apache-2.0
</span></p><a href="https://github.com/datalens-tech/publish-unit-test-result-action/forks" data-view-component="true">
          <svg aria-label="fork" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>
          156
</a>
      <a href="https://github.com/datalens-tech/publish-unit-test-result-action/issues" data-view-component="true">
        
        0
</a>

      <a href="https://github.com/datalens-tech/publish-unit-test-result-action/pulls" data-view-component="true">
        
        0
</a>
      <p><span data-view-component="true">
          Updated <relative-time datetime="2023-08-11T12:45:36Z">Aug 11, 2023</relative-time>
</span></p></div>
</li>
        <li data-view-component="true">          <div data-view-component="true" itemprop="owns" itemtype="http://schema.org/Code" itemscope="itemscope">


        <p><span data-view-component="true">
          0
</span>
        <span data-view-component="true">
          MIT
</span></p><a href="https://github.com/datalens-tech/ansible-github_actions_runner/forks" data-view-component="true">
          <svg aria-label="fork" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>
          53
</a>
      <a href="https://github.com/datalens-tech/ansible-github_actions_runner/issues" data-view-component="true">
        
        0
</a>

      <a href="https://github.com/datalens-tech/ansible-github_actions_runner/pulls" data-view-component="true">
        
        0
</a>
      <p><span data-view-component="true">
          Updated <relative-time datetime="2023-08-09T12:22:47Z">Aug 9, 2023</relative-time>
</span></p></div>
</li>
        <li data-view-component="true">          <div itemprop="owns" itemtype="http://schema.org/Code" itemscope="itemscope" data-view-component="true">
  
    <div data-view-component="true">
        <p><a itemprop="name codeRepository" data-hovercard-type="repository" data-hovercard-url="/datalens-tech/action-local-cache/hovercard" href="https://github.com/datalens-tech/action-local-cache" data-view-component="true">
          action-local-cache
</a>
        <span title="Label: Public" data-view-component="true">
          Public
</span></p><p itemprop="description" data-view-component="true">
            A Github Action to save and restore files across job runs directly in the runner's file system
</p></div>
    <div data-view-component="true">

        <p><span>
  <span></span>
  <span itemprop="programmingLanguage">TypeScript</span>
</span>


        <span data-view-component="true">
          0
</span>
        <span data-view-component="true">
          MIT
</span></p><a href="https://github.com/datalens-tech/action-local-cache/forks" data-view-component="true">
          <svg aria-label="fork" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>
          41
</a>
      <a href="https://github.com/datalens-tech/action-local-cache/issues" data-view-component="true">
        
        0
</a>

      <a href="https://github.com/datalens-tech/action-local-cache/pulls" data-view-component="true">
        
        0
</a>
      <p><span data-view-component="true">
          Updated <relative-time datetime="2023-05-18T08:29:51Z">May 18, 2023</relative-time>
</span></p></div></div>
</li>
</ul>  
</div></div>
  </div>
</div>

    <div data-view-component="true">
            





<div id="org-members">
    <a href="https://github.com/orgs/datalens-tech/people" data-ga-click="Orgs, go to people, location:profile people module; text:People">
      <h4>People</h4>
    </a>
    <p>
      This organization has no public members. You must be a member to see who’s a part of this organization.
    </p>
  </div>

  


    

<include-fragment src="/orgs/datalens-tech/topics/most_used?context=overview" accept="text/fragment+html">
  <div>
      <h4>Most used topics</h4>
      <p>Loading…</p>
    </div>
</include-fragment>


        </div>
</div>
</div>

      </main>
  </div>

          




  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google assigns a CVE for libwebp and gives it a 10.0 score (230 pts)]]></title>
            <link>https://stackdiary.com/heap-buffer-overflow-in-libwebp-cve-2023-5129/</link>
            <guid>37657746</guid>
            <pubDate>Tue, 26 Sep 2023 11:33:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stackdiary.com/heap-buffer-overflow-in-libwebp-cve-2023-5129/">https://stackdiary.com/heap-buffer-overflow-in-libwebp-cve-2023-5129/</a>, See on <a href="https://news.ycombinator.com/item?id=37657746">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
							
<p>In case you missed the news, there's a critical 0day in WebP (a heap buffer overflow in the <a href="https://github.com/webmproject/libwebp" target="_blank" rel="noreferrer noopener">libwepb</a> library) floating about, which was initially issued as <a href="https://stackdiary.com/critical-vulnerability-in-webp-codec-cve-2023-4863/" target="_blank" rel="noreferrer noopener">CVE-2023-4863</a> and assigned <a href="https://chromereleases.googleblog.com/2023/09/stable-channel-update-for-desktop_11.html" target="_blank" rel="noreferrer noopener">specifically to Google Chrome</a>. At the time this happened, I wrote my blog post about it and vehemently tried to make it clear that it wasn't just Chrome that was affected, but any software that uses libwebp to render WebP images.</p>



<p><em>That story exploded. 🤯</em></p>



<p>I've just taken note that Google has issued a separate CVE, which is tracked under <a href="https://www.cve.org/CVERecord?id=CVE-2023-5129" target="_blank" rel="noreferrer noopener nofollow">CVE-2023-5129</a>,</p>



<p>With a specially crafted WebP lossless file, libwebp may write data out of bounds to the heap. The ReadHuffmanCodes() function allocates the HuffmanCode buffer with a size that comes from an array of precomputed sizes: kTableSize. The color_cache_bits value defines which size to use. The kTableSize array only takes into account sizes for 8-bit first-level table lookups but not second-level table lookups. libwebp allows codes that are up to 15-bit (MAX_ALLOWED_CODE_LENGTH). When BuildHuffmanTable() attempts to fill the second-level tables it may write data out-of-bounds. The OOB write to the undersized array happens in ReplicateValue.</p>



<p><em><strong>Important:</strong> If you're a news person or someone who isn't sure - this is not a new bug in libwebp; it's the same bug as previously, but now it has been correctly marked as a bug inside the WebP Codec and not just a "bug inside Google Chrome".</em></p>



<p>And Google is not beating around the bush either; they've straight up given it a 10.0 base score.</p>



<figure><img decoding="async" width="998" height="393" src="https://stackdiary.com/wp-content/uploads/2023/09/CVE-2023-5129-severity.png" alt="CVE-2023-5129 severity" srcset="https://stackdiary.com/wp-content/uploads/2023/09/CVE-2023-5129-severity.png 998w, https://stackdiary.com/wp-content/uploads/2023/09/CVE-2023-5129-severity-300x118.png 300w, https://stackdiary.com/wp-content/uploads/2023/09/CVE-2023-5129-severity-768x302.png 768w" sizes="(max-width: 998px) 100vw, 998px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20998%20393'%3E%3C/svg%3E" data-lazy-srcset="https://stackdiary.com/wp-content/uploads/2023/09/CVE-2023-5129-severity.png 998w, https://stackdiary.com/wp-content/uploads/2023/09/CVE-2023-5129-severity-300x118.png 300w, https://stackdiary.com/wp-content/uploads/2023/09/CVE-2023-5129-severity-768x302.png 768w" data-lazy-src="https://stackdiary.com/wp-content/uploads/2023/09/CVE-2023-5129-severity.png"><figcaption><em>The Impact score is 6.0, and the Exploitability score is 3.9.</em></figcaption></figure>



<p>And it's what they should have done in the first place.</p>



<h2>Who is and isn't affected?</h2>



<p>The versions affected by this bug are <strong>from 0.5.0 before 1.3.2</strong>. The type of software affected is pretty much any software that directly uses the WebP Codec to render images. Just in the last two weeks alone, outside of web browsers (most of which should be patched now) - I have seen Red Hat to Debian to software like Puppeteer and the .NET library for ImageMagick patching it. Honestly, I have no idea of the full scope of this, and it's not that easy to track who is or isn't actively patching it.</p>



<p>Ben Hawkes (former Project Zero manager) also <a href="https://blog.isosceles.com/the-webp-0day/" target="_blank" rel="noreferrer noopener">wrote about this 0day</a>, and he had this to say about it:</p>



<p>The bad news is that Android is still likely affected. Similar to Apple's ImageIO, Android has a facility called the&nbsp;<a href="https://developer.android.com/reference/android/graphics/BitmapFactory?ref=blog.isosceles.com">BitmapFactory</a>&nbsp;that handles image decoding, and of course libwebp is supported. As of today, Android hasn't released a security bulletin that includes a fix for CVE-2023-4863 -- although the fix has been merged into AOSP. To put this in context: if this bug does affect Android, then it could potentially be turned into a remote exploit for apps like Signal and WhatsApp. I'd expect it to be fixed in the October bulletin.</p>



<p>Ben's article also has a Proof of Concept example and other interesting notes; make sure to check it out.</p>



<p>But the real question is, why didn't Google tag it specifically for libwebp in the first place? I mean, it clearly was much broader than just Chrome (and many news editorials failed to pick this up initially), and now they've gone ahead and assigned a separate CVE. </p>



<p>And it makes me wonder if the best thing wouldn't be to merge both CVEs to avoid any further confusion.</p>
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DuckDB 0.9.0 (145 pts)]]></title>
            <link>https://duckdb.org/2023/09/26/announcing-duckdb-090.html</link>
            <guid>37657736</guid>
            <pubDate>Tue, 26 Sep 2023 11:32:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://duckdb.org/2023/09/26/announcing-duckdb-090.html">https://duckdb.org/2023/09/26/announcing-duckdb-090.html</a>, See on <a href="https://news.ycombinator.com/item?id=37657736">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							<p><span>2023-09-26</span><span>Mark Raasveldt and Hannes Mühleisen</span></p>
							
							<p><img src="https://duckdb.org/images/blog/yellow-billed-duck.jpg" alt="Image of the Yellow Billed Duck" width="200px"></p>

<p>The DuckDB team is happy to announce the latest DuckDB release (0.9.0). This release is named Undulata after the <a href="https://en.wikipedia.org/wiki/Yellow-billed_duck">Yellow-billed duck</a> native to Africa.</p>

<p>To install the new version, please visit the <a href="https://duckdb.org/docs/installation/index">installation guide</a>. The full release notes can be found <a href="https://github.com/duckdb/duckdb/releases/tag/v0.9.0">here</a>.</p>

<!--more-->
      <h4 id="whats-new-in-090">
        
        <a href="#whats-new-in-090">What’s new in 0.9.0</a>
        
      </h4>
    

<p>There have been too many changes to discuss them each in detail, but we would like to highlight several particularly exciting features!</p>

<ul>
  <li>Out-Of-Core Hash Aggregate</li>
  <li>Storage Improvements</li>
  <li>Index Improvements</li>
  <li>DuckDB-WASM Extensions</li>
  <li>Extension Auto-Loading</li>
  <li>Improved AWS Support</li>
  <li>Iceberg Support</li>
  <li>Azure Support</li>
  <li>PySpark-Compatible API</li>
</ul>

<p>Below is a summary of those new features with examples, starting with a change in our SQL dialect that is designed to produce more intuitive results by default.</p>
      <h4 id="breaking-sql-changes">
        
        <a href="#breaking-sql-changes">Breaking SQL Changes</a>
        
      </h4>
    

<p><a href="https://github.com/duckdb/duckdb/pull/8942"><strong>Struct Auto-Casting</strong></a>. Previously the names of struct entries were ignored when determining auto-casting rules. As a result, struct field names could be silently renamed. Starting with this release, this will result in an error instead.</p>

<div><pre><code><span>CREATE</span> <span>TABLE</span> <span>structs</span><span>(</span><span>s</span> <span>STRUCT</span><span>(</span><span>i</span> <span>INT</span><span>));</span>
<span>INSERT</span> <span>INTO</span> <span>structs</span> <span>VALUES</span> <span>({</span><span>'k'</span><span>:</span> <span>42</span><span>});</span>
<span>-- Mismatch Type Error: Type STRUCT(k INTEGER) does not match with STRUCT(i INTEGER). Cannot cast STRUCTs with different names</span>
</code></pre></div>

<p>Unnamed structs constructed using the <code>ROW</code> function can still be inserted into struct fields.</p>

<div><pre><code><span>INSERT</span> <span>INTO</span> <span>structs</span> <span>VALUES</span> <span>(</span><span>ROW</span><span>(</span><span>42</span><span>));</span>
</code></pre></div>
      <h4 id="core-system-improvements">
        
        <a href="#core-system-improvements">Core System Improvements</a>
        
      </h4>
    

<p><strong><a href="https://github.com/duckdb/duckdb/pull/7931">Out-Of-Core Hash Aggregates</a></strong> and <strong><a href="https://github.com/duckdb/duckdb/pull/8475">Hash Aggregate Performance Improvements.</a></strong> When working with large data sets, memory management is always a potential pain point. By using a streaming execution engine and buffer manager, DuckDB supports many operations on larger than memory data sets. DuckDB also aims to support queries where <em>intermediate</em> results do not fit into memory by using disk-spilling techniques.</p>

<p>In this release, support for disk-spilling techniques is further extended through the support for out-of-core hash aggregates. Now, hash tables constructed during <code>GROUP BY</code> queries or <code>DISTINCT</code> operations that do not fit in memory due to a large number of unique groups will spill data to disk instead of throwing an out-of-memory exception. Due to the clever use of radix partitioning, performance degradation is gradual, and performance cliffs are avoided. Only the subset of the table that does not fit into memory will be spilled to disk.</p>

<p>The performance of our hash aggregate has also improved in general, especially when there are many groups. For example, we compute the number of unique rows in a data set with 30 million rows and 15 columns by using the following query:</p>

<div><pre><code><span>SELECT</span> <span>COUNT</span><span>(</span><span>*</span><span>)</span> <span>FROM</span> <span>(</span><span>SELECT</span> <span>DISTINCT</span> <span>*</span> <span>FROM</span> <span>tbl</span><span>);</span>
</code></pre></div>
<p>If we keep all the data in memory, the query should use around 6GB. However, we can still complete the query if less memory is available. In the table below, we can see how the runtime is affected by lowering the memory limit:</p>

<table>
  <thead>
    <tr>
      <th>memory limit</th>
      <th>v0.8.1</th>
      <th>v0.9.0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>10.0GB</td>
      <td>8.52s</td>
      <td>2.91s</td>
    </tr>
    <tr>
      <td>9.0GB</td>
      <td>8.52s</td>
      <td>3.45s</td>
    </tr>
    <tr>
      <td>8.0GB</td>
      <td>8.52s</td>
      <td>3.45s</td>
    </tr>
    <tr>
      <td>7.0GB</td>
      <td>8.52s</td>
      <td>3.47s</td>
    </tr>
    <tr>
      <td>6.0GB</td>
      <td>OOM</td>
      <td>3.41s</td>
    </tr>
    <tr>
      <td>5.0GB</td>
      <td>OOM</td>
      <td>3.67s</td>
    </tr>
    <tr>
      <td>4.0GB</td>
      <td>OOM</td>
      <td>3.87s</td>
    </tr>
    <tr>
      <td>3.0GB</td>
      <td>OOM</td>
      <td>4.20s</td>
    </tr>
    <tr>
      <td>2.0GB</td>
      <td>OOM</td>
      <td>4.39s</td>
    </tr>
    <tr>
      <td>1.0GB</td>
      <td>OOM</td>
      <td>4.91s</td>
    </tr>
  </tbody>
</table>

<p><strong><a href="https://github.com/duckdb/duckdb/pull/7644">Compressed Materialization.</a></strong> DuckDB’s streaming execution engine has a low memory footprint, but more memory is required for operations such as grouped aggregation. The memory footprint of these operations can be reduced by compression. DuckDB already uses <a href="https://duckdb.org/2022/10/28/lightweight-compression.html">many compression techniques in its storage format</a>, but many of these techniques are too costly to use during query execution. However, certain lightweight compression techniques are so cheap that the benefit of the reducing memory footprint outweight the cost of (de)compression.</p>

<p>In this release, we add support for compression of strings and integer types right before data goes into the grouped aggregation and sorting operators. By using statistics, both types are compressed to the smallest possible integer type. For example, if we have the following table:</p>

<div><pre><code>┌───────┬─────────┐
│  id   │  name   │
│ int32 │ varchar │
├───────┼─────────┤
│   300 │ alice   │
│   301 │ bob     │
│   302 │ eve     │
│   303 │ mallory │
│   304 │ trent   │
└───────┴─────────┘
</code></pre></div>

<p>The <code>id</code> column uses a 32-bit integer. From our statistics we know that the minimum value is 300, and the maximum value is 304. We can subtract 300 and cast to an 8-bit integer instead, reducing the width from 4 bytes down to 1.</p>

<p>The <code>name</code> column uses our internal string type, which is 16 bytes wide. However, our statistics tell us that the longest string here is only 7 bytes. We can fit this into a 64-bit integer like so:</p>
<div><pre><code>alice   -&gt; alice005
bob     -&gt; bob00003
eve     -&gt; eve00003
mallory -&gt; mallory7
trent   -&gt; trent005
</code></pre></div>

<p>This reduces the width from 16 bytes down to 8. To support sorting of compressed strings, we flip the bytes on big-endian machines so that our comparison operators are still correct:</p>
<div><pre><code>alice005 -&gt; 500ecila
bob00003 -&gt; 30000bob
eve00003 -&gt; 30000eve
mallory7 -&gt; 7yrollam
trent005 -&gt; 500tnert
</code></pre></div>

<p>By reducing the size of query intermediates, we can prevent/reduce spilling data to disk, reducing the need for costly I/O operations, thereby improving query performance.</p>

<p><strong>Window Function Performance Improvements (<a href="https://github.com/duckdb/duckdb/pull/7831">#7831</a>, <a href="https://github.com/duckdb/duckdb/pull/7996">#7996</a>, <a href="https://github.com/duckdb/duckdb/pull/8050">#8050</a>, <a href="https://github.com/duckdb/duckdb/pull/8491">#8491</a>).</strong> This release features many improvements to the performance of Window functions due to improved vectorization of the code, more re-use of partial aggregates and improved parallelism through work stealing of tasks. As a result, performance of <a href="https://github.com/duckdb/duckdb/issues/7809#issuecomment-1679387022">Window functions has improved significantly, particularly in scenarios where there are no or few partitions</a>.</p>

<div><pre><code><span>SELECT</span>
    <span>SUM</span><span>(</span><span>driver_pay</span><span>)</span> <span>OVER</span> <span>(</span>
        <span>ORDER</span> <span>BY</span> <span>dropoff_datetime</span> <span>ASC</span>
        <span>RANGE</span> <span>BETWEEN</span>
        <span>INTERVAL</span> <span>3</span> <span>DAYS</span> <span>PRECEDING</span> <span>AND</span>
        <span>INTERVAL</span> <span>0</span> <span>DAYS</span> <span>FOLLOWING</span>
    <span>)</span>
<span>FROM</span> <span>tripdata</span><span>;</span>
</code></pre></div>

<table>
  <thead>
    <tr>
      <th>Version</th>
      <th>Time (s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>v0.8.0</td>
      <td>33.8</td>
    </tr>
    <tr>
      <td>v0.9.0</td>
      <td>3.8</td>
    </tr>
  </tbody>
</table>
      <h4 id="storage-improvements">
        
        <a href="#storage-improvements">Storage Improvements</a>
        
      </h4>
    

<p><a href="https://github.com/duckdb/duckdb/pull/7794"><strong>Vacuuming of Deleted Row Groups</strong></a>. Starting with this release, when deleting data using <code>DELETE</code> statements, entire row groups that are deleted will be automatically cleaned up. Support is also added to <a href="https://github.com/duckdb/duckdb/pull/7824">truncate the database file on checkpoint</a> which allows the database file to be reduced in size after data is deleted. Note that this only occurs if the deleted row groups are located at the end of the file. The system does not yet move around data in order to reduce the size of the file on disk. Instead, free blocks earlier on in the file are re-used to store later data.</p>

<p><strong>Index Storage Improvements (<a href="https://github.com/duckdb/duckdb/pull/7930">#7930</a>, <a href="https://github.com/duckdb/duckdb/pull/8112">#8112</a>, <a href="https://github.com/duckdb/duckdb/pull/8437">#8437</a>, <a href="https://github.com/duckdb/duckdb/pull/8703">#8703</a>)</strong>. Many improvements have been made to both the in-memory footprint, and the on-disk footprint of ART indexes. In particular for indexes created to maintain <code>PRIMARY KEY</code>, <code>UNIQUE</code> or <code>FOREIGN KEY</code> constraints the storage and in-memory footprint is drastically reduced.</p>

<div><pre><code><span>CREATE</span> <span>TABLE</span> <span>integers</span><span>(</span><span>i</span> <span>INTEGER</span> <span>PRIMARY</span> <span>KEY</span><span>);</span>
<span>INSERT</span> <span>INTO</span> <span>integers</span> <span>FROM</span> <span>range</span><span>(</span><span>10000000</span><span>);</span>
</code></pre></div>

<table>
  <thead>
    <tr>
      <th>Version</th>
      <th>Size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>v0.8.0</td>
      <td>278MB</td>
    </tr>
    <tr>
      <td>v0.9.0</td>
      <td>78MB</td>
    </tr>
  </tbody>
</table>

<p>In addition, due to improvements in the manner in which indexes are stored on disk they can now be written to disk incrementally instead of always requiring a full rewrite. This allows for much quicker checkpointing for tables that have indexes.</p>
      <h4 id="extensions">
        
        <a href="#extensions">Extensions</a>
        
      </h4>
    

<p><a href="https://github.com/duckdb/duckdb/pull/8732"><strong>Extension Auto-Loading</strong></a>. Starting from this release, DuckDB supports automatically installing and loading of trusted extensions. As many workflows rely on core extensions that are not bundled, such as <code>httpfs</code>, many users found themselves having to remember to load the required extensions up front. With this change, the extensions will instead be automatically loaded (and optionally installed) when used in a query.</p>

<p>For example, in Python the following code snippet now works without needing to explicitly load the <code>httpfs</code> or <code>json</code> extensions.</p>

<div><pre><code><span>import</span> <span>duckdb</span>

<span>duckdb</span><span>.</span><span>sql</span><span>(</span><span>"FROM 'https://raw.githubusercontent.com/duckdb/duckdb/main/data/json/example_n.ndjson'"</span><span>)</span>
</code></pre></div>

<p>The set of autoloadable extensions is limited to official extensions distributed by DuckDB Labs, and can be <a href="https://github.com/duckdb/duckdb/blob/8feb03d274892db0e7757cd62c145b18dfa930ec/scripts/generate_extensions_function.py#L298">found here</a>. The behavior can also be disabled using the <code>autoinstall_known_extensions</code> and <code>autoload_known_extensions</code> settings, or through the more general <code>enable_external_access</code> setting. See the <a href="https://duckdb.org/docs/sql/configuration.html">configuration options</a>.</p>

<p><a href="https://github.com/duckdb/duckdb-wasm/pull/1403"><strong>DuckDB-WASM Extensions</strong></a>. This release adds support for loadable extensions to DuckDB-WASM. Previously, any extensions that you wanted to use with the WASM client had to be baked in. With this release, extensions can be loaded dynamically instead. When an extension is loaded, the WASM bundle is downloaded and the functionality of the extension is enabled. Give it a try in our <a href="https://shell.duckdb.org/">WASM shell</a>.</p>

<div><pre><code><span>LOAD</span> <span>inet</span><span>;</span>
<span>SELECT</span> <span>'127.0.0.1'</span><span>::</span><span>INET</span><span>;</span>
</code></pre></div>

<p><a href="https://github.com/duckdblabs/duckdb_aws"><strong>AWS Extension</strong></a>. This release marks the launch of the DuckDB AWS extension. This extension contains AWS related features that rely on the AWS SDK. Currently, the extension contains one function, <code>LOAD_AWS_CREDENTIALS</code>, which uses the AWS <a href="https://docs.aws.amazon.com/sdkref/latest/guide/standardized-credentials.html#credentialProviderChain">Credential Provider Chain</a> to automatically fetch and set credentials:</p>

<div><pre><code><span>CALL</span> <span>load_aws_credentials</span><span>();</span>
<span>SELECT</span> <span>*</span> <span>FROM</span> <span>"s3://some-bucket/that/requires/authentication.parquet"</span><span>;</span>
</code></pre></div>

<p><a href="https://duckdb.org/docs/extensions/aws">See the documentation for more information</a>.</p>

<p><a href="https://github.com/duckdblabs/duckdb_iceberg"><strong>Experimental Iceberg Extension</strong></a>. This release marks the launch of the DuckDB Iceberg extension. This extension adds support for reading tables stored in the <a href="https://iceberg.apache.org/">Iceberg format</a>.</p>

<div><pre><code><span>SELECT</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>FROM</span> <span>iceberg_scan</span><span>(</span><span>'data/iceberg/lineitem_iceberg'</span><span>,</span> <span>ALLOW_MOVED_PATHS</span><span>=</span><span>true</span><span>);</span>
</code></pre></div>

<p><a href="https://duckdb.org/docs/extensions/iceberg">See the documentation for more information</a>.</p>

<p><a href="https://github.com/duckdblabs/duckdb_azure"><strong>Experimental Azure Extension</strong></a>. This release marks the launch of the DuckDB Azure extension. This extension allows for DuckDB to natively read data stored on Azure, in a similar manner to how it can read data stored on S3.</p>

<div><pre><code><span>SET</span> <span>azure_storage_connection_string</span> <span>=</span> <span>'&lt;your_connection_string&gt;'</span><span>;</span>
<span>SELECT</span> <span>*</span> <span>from</span> <span>'azure://&lt;my_container&gt;/*.csv'</span><span>;</span>
</code></pre></div>

<p><a href="https://duckdb.org/docs/extensions/azure">See the documentation for more information</a>.</p>
      <h4 id="clients">
        
        <a href="#clients">Clients</a>
        
      </h4>
    

<p><a href="https://github.com/duckdb/duckdb/pull/8083"><strong>Experimental PySpark API</strong></a>. This release features the addition of an experimental Spark API to the Python client. The API aims to be fully compatible with the PySpark API, allowing you to use the Spark API as you are familiar with but while utilizing the power of DuckDB. All statements are translated to DuckDB’s internal plans using our <a href="https://duckdb.org/docs/archive/0.8.1/api/python/relational_api">relational API</a> and executed using DuckDB’s query engine.</p>

<div><pre><code><span>from</span> <span>duckdb.experimental.spark.sql</span> <span>import</span> <span>SparkSession</span> <span>as</span> <span>session</span>
<span>from</span> <span>duckdb.experimental.spark.sql.functions</span> <span>import</span> <span>lit</span><span>,</span> <span>col</span>
<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>

<span>spark</span> <span>=</span> <span>session</span><span>.</span><span>builder</span><span>.</span><span>getOrCreate</span><span>()</span>

<span>pandas_df</span> <span>=</span> <span>pd</span><span>.</span><span>DataFrame</span><span>({</span>
    <span>'age'</span><span>:</span> <span>[</span><span>34</span><span>,</span> <span>45</span><span>,</span> <span>23</span><span>,</span> <span>56</span><span>],</span>
    <span>'name'</span><span>:</span> <span>[</span><span>'Joan'</span><span>,</span> <span>'Peter'</span><span>,</span> <span>'John'</span><span>,</span> <span>'Bob'</span><span>]</span>
<span>})</span>

<span>df</span> <span>=</span> <span>spark</span><span>.</span><span>createDataFrame</span><span>(</span><span>pandas_df</span><span>)</span>
<span>df</span> <span>=</span> <span>df</span><span>.</span><span>withColumn</span><span>(</span>
    <span>'location'</span><span>,</span> <span>lit</span><span>(</span><span>'Seattle'</span><span>)</span>
<span>)</span>
<span>res</span> <span>=</span> <span>df</span><span>.</span><span>select</span><span>(</span>
    <span>col</span><span>(</span><span>'age'</span><span>),</span>
    <span>col</span><span>(</span><span>'location'</span><span>)</span>
<span>).</span><span>collect</span><span>()</span>

<span>print</span><span>(</span><span>res</span><span>)</span>
<span>#[
#    Row(age=34, location='Seattle'),
#    Row(age=45, location='Seattle'),
#    Row(age=23, location='Seattle'),
#    Row(age=56, location='Seattle')
#]
</span></code></pre></div>

<p>Note that the API is currently experimental and features are still missing. We are very interested in feedback. Please report any functionality that you are missing, either through Discord or on Github.</p>
      <h4 id="final-thoughts">
        
        <a href="#final-thoughts">Final Thoughts</a>
        
      </h4>
    

<p>The full release notes can be <a href="https://github.com/duckdb/duckdb/releases/tag/v0.9.0">found on Github</a>. We would like to thank all of the contributors for their hard work on improving DuckDB.</p>

							<p><a href="https://duckdb.org/news/">back to news archive <span></span></a>
						</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why we bootstrap (108 pts)]]></title>
            <link>https://www.flagsmith.com/blog/why-we-bootstrap</link>
            <guid>37657519</guid>
            <pubDate>Tue, 26 Sep 2023 11:06:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.flagsmith.com/blog/why-we-bootstrap">https://www.flagsmith.com/blog/why-we-bootstrap</a>, See on <a href="https://news.ycombinator.com/item?id=37657519">Hacker News</a></p>
<div id="readability-page-1" class="page"><div fs-richtext-element="rich-text"><p>Venture capital isn’t the necessity it once was for tech startups. Bootstrapped companies are competing with—and outflanking—heavily funded goliaths by finding a niche and applying focus. </p><h2>The bootstrapping superpower</h2><p>The combination of small teams and an intense bootstrapping ethos can be a superpower.</p><p>• Venture capital offers a tonne of competitive advantage and can grow a company fast, but it puts you on a treadmill focused on revenue and fast growth targets.<a href="https://posthog.com/blog/using-vc-carefully"> Raising rounds is intoxicating</a> and it's easy to get stuck in the fundraising loop. </p><p>• Bootstrapped founders get to choose their company's focus. For example, David Heinemeier Hansson <a href="https://www.linkedin.com/pulse/bootstrapped-basecamp-executing-nonsense-strategy-sramana-mitra-1e/">bootstrapped Basecamp</a> by focusing on slow, consistent, profitable growth.&nbsp;</p><p>At <a href="https://www.flagsmith.com/">Flagsmith</a>, bootstrapping lets us build efficiently and focus on the customer. We hit $1.5M ARR earlier this year with a super lean team, and we’re genuinely excited about the next chapters. Here are four reasons we continue to bootstrap.&nbsp;&nbsp;</p><h2>1. Customer focus&nbsp;</h2><p>The VC track puts focus on the company’s next raise. Bootstrapping lets us focus on customers.&nbsp;</p><p>There are pros and cons to both tracks. With external capital growth can rocket, but the path is laid out for you. The growth mindset can become very internally focused as you need to look at:</p><p>• Hiring</p><p>• Increasing LTVs </p><p>• Getting more customers fast </p><p>• Hitting board-driven revenue milestones </p><p>We’re profitable and stable and don't have an absolute mandate to hit an external revenue goal. All of the goals we set are our own. </p><p>We’ve had tough budget constraints, but since we’ve stayed profitable we haven’t had to play pricing games by upselling customers or hiking prices. We’ve created a pricing model and customer support ethos that fits with our business goals.</p><h2>2. We haven't had financial pressure to grow fast</h2><h4>Fewer financial pressures from infrastructure costs etc.</h4><p>Back in the day venture capital was applied to technology businesses because they were enormously expensive to start—and run—so you couldn't do it on your own.&nbsp;</p><p>You had to buy pricey hardware. Software tools were expensive and you needed enterprise licences. You probably also needed Oracle databases, which weren’t cheap. To start a technology company you needed capital.</p><p>That's not the case anymore. You can start a sustainable software business for relatively small amounts. A lot of software tools have trials, startup versions or open source options. </p><p>The guts of the Flagsmith platform were essentially free when we were running the agency, and we scaled it to billions of monthly requests on our own budget. That would have cost a fortune 20 years ago.&nbsp;</p><h4>Building out of an agency&nbsp;</h4><p>We started building Flagsmith alongside running an agency. At first, we were able to grow organically in line with <a href="https://github.com/Flagsmith/flagsmith">open source growth</a>. We didn’t have financial pressures to build quickly, so we didn’t need capital.&nbsp;</p><p>As we grew, we realised VC money comes with some competitive advantage, but it also comes with a tonne of obligations like meetings, targets, reporting, and externally set timelines. That doesn’t fit the way we want to build. Since we’ve hit profitability and sustainability, we’ve chosen to continue sidestepping the VC path in favour of growing efficiently.&nbsp;&nbsp;</p><h2>3. Bootstrapping can be sustainable</h2><p>Bootstrapped companies often get asked about sustainability. Really, though, bootstrapped companies are hardened and have had to make their way from the beginning; they have stability baked in at the start.</p><p>Venture-backed companies’ sustainability is commonly measured by runway (the amount of time they have left before they run out of cash.) Typically this is measured in months!&nbsp;</p><p>It's a ticking clock that can lead to running out of money, selling, or having to raise again. This can be fragile. If you look at a bootstrapped company that’s hit profitability, there is no runway. The plane is climbing into the air.&nbsp;</p><p>Coming back to the hardened mentality, bootstrapping founders of sustainable businesses often appreciate the fact that there are no quick wins. They’ve chosen to grow efficiently and have worked with hard financial constraints. They've had to avoid <a href="https://a16z.com/how-to-ruin-your-company-with-one-bad-process/">messing up budgeting choices by growing too fast</a> and creating cultural/technical drift.</p><p>So while there are a huge number of companies that do well with VC money, ask any VC and you will hear that most of their portfolio will fail. There are a lot of companies that raise $5 million and get published on Crunchbase only to disappear overnight because they aren’t sustainable. (e.g. Just <a href="https://venturebeat.com/entrepreneur/getting-to-50-million-how-to-avoid-the-saas-valley-of-death/">4% of SaaS companies reach $1M ARR</a>.)&nbsp;&nbsp;</p><h2>4. Bootstrapping fits our ethos (and excitement around figuring out what's next)</h2><h4>Company ethos</h4><p>We believe enterprise software is not a winner-takes-all game. With the current software landscape, there's room for multiple players in most software markets.&nbsp;</p><p>External capital can help you grow faster and take more market share. But you can also do really well in a market without that competitive advantage if you find your niche.&nbsp;</p><p>We’ve chosen to build a sustainable open source feature flag software with commercial enterprise licenses and on-premises deployment options. Our code is in the open and tested by developers around the globe. Our choices to open source and bootstrap mirror one another.&nbsp;</p><h4>Personal ethos</h4><p>I don't want Flagsmith’s focus to be on money. Part of the joy of bootstrapping Flagsmith is that we aren’t bound to hit externally set revenue goals within a timeframe. We have the freedom to choose what happens next. </p><p>Flagsmith makes money. It’s sustainable. It has fantastic customers and a thriving open source community. There’s no rigidity or pre-ordained milestones for the next phase of our growth, which lets us be creative and enjoy running the business.&nbsp;</p><p>There’s a lot of joy in knowing that we don’t have the next five years set in stone, and we’re not cycling down a path that's getting narrower and narrower while our ability to fail is going up and up.&nbsp;</p><p>‍</p><figure></figure><p>‍</p></div></div>]]></description>
        </item>
    </channel>
</rss>