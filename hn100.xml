<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 05 Mar 2025 19:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[My Beancount books are 95% automatic after 3 years (2024) (122 pts)]]></title>
            <link>https://fangpenlin.com/posts/2024/12/30/my-beancount-books-are-95-percent-automatic/</link>
            <guid>43268454</guid>
            <pubDate>Wed, 05 Mar 2025 16:08:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fangpenlin.com/posts/2024/12/30/my-beancount-books-are-95-percent-automatic/">https://fangpenlin.com/posts/2024/12/30/my-beancount-books-are-95-percent-automatic/</a>, See on <a href="https://news.ycombinator.com/item?id=43268454">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  

<p>I am a big believer in building products for your needs, eating your own dog food, and finding customers with the same needs.
Therefore, I started building <a href="https://beanhub.io/">BeanHub</a>.
Three years later, my <a href="https://beancount.github.io/docs/index.html">Beancount</a> books are 95% automatic, and I am a very happy user of my product.
It‚Äôs hard to describe; as a computer nerd obsessed with automation, seeing my accounting book updating itself without me touching it in an open format brings me pure joy üòç</p>

<figure><img src="https://fangpenlin.com/images/2024-12-30-my-beancount-books-are-95-percent-automatic/beanhub-git-diff.png" alt="Screenshot of Git history and difference showing Beancount and bank transaction CSV files changes"><figcaption><p>Screenshot of Git history and difference showing Beancount and bank transaction CSV files changes</p></figcaption></figure>

<p>Do you see this git commit of the Costco transaction turning from pending to confirmed?
Yep! It‚Äôs all automatic, yet it‚Äôs just a Beancount file that can be read by any open-source Beancount tool!
The better part is that other people are paying me, which is growing.</p>

<p>Today, I saw the unfortunate news about the shutting down of the beloved accounting software Bench on X.</p>

<figure><img src="https://fangpenlin.com/images/2024-12-30-my-beancount-books-are-95-percent-automatic/bench-shutdown-notice.png" alt="Shutdown notice of Bench accounting software"><figcaption><p>Shutdown notice of Bench accounting software</p></figcaption></figure>

<p>I am surprised, but I am not that surprised.
Looking back now, I am really glad that I picked the rough road to build the product for myself.
It doesn‚Äôt bring me financial freedom (yet), but at least I am a happy user myself, and I already have some paying customers.
It‚Äôs not easy, though; it took me three years to get here, and I have overcome many interesting technology challenges and learned a lot from building it.
Today, I would like to share my journey of building and selling BeanHub as a product.</p>

<!--more-->

<h2 id="background">Background</h2>

<p>Three years ago when, I founded my startup, <a href="https://launchplatform.com/">Launch Platform</a>.
Before I even thought about what products to build, I‚Äôd already faced the same problem every startup founder would face ‚Äì what accounting book software to use?
There are many popular options, such as <a href="https://quickbooks.intuit.com/">QuickBooks</a> or <a href="https://www.xero.com/">Xero</a>.
However, as a software engineer with over two decades of experience, I‚Äôve witnessed the rise of internet startups and understand that there‚Äôs no such thing as forever software.
I know the software will be gone one day, and I will have my critical data stuck in an obsolete format or locked up in their data center.</p>

<p>With that in mind, while it‚Äôs very easy to sign up for any of them, I don‚Äôt want to give in.
Therefore, I searched around and found Beancount.
It‚Äôs open source, so I don‚Äôt need to worry about the format becoming obsolete one day.
However, I discovered that the existing tools don‚Äôt meet all my needs.
I am obsessed with automating everything whenever possible.
I want a smart Beancount-based accounting book that does most things automatically for me.
I also wish to have a user-friendly UI interface so that it‚Äôs possible for my wife to look at the books to ensure that I didn‚Äôt buy <a href="https://fangpenlin.com/posts/2024/12/11/cading-and-3d-printing-like-a-software-engineer-part1/">some new toys</a> without her acknowledgment.
She can also input entries herself without learning the nerdy stuff with my help.</p>

<h2 id="file-over-app">File over app</h2>

<p>Building a database-based accounting software is easy because operating data in the database is easy, and countless accounting book software is already doing that.
I don‚Äôt want to make another one as it defeats the original purpose.</p>

<p>Therefore, setting clear goals and rules up front was very important.
For BeanHub, I keep telling myself that all the operations should only happen to the files instead of tables in the database.
That makes it 10 times harder because you need to parse the text file, update it accordingly, and write it back.
But I am glad I did.
That guarantees all my accounting books are in the same open format.</p>

<p>Interestingly, today, I found the article <a href="https://stephango.com/file-over-app">File over app</a> written by <a href="https://stephango.com/">Steph Ango</a>, the CEO of <a href="https://obsidian.md/">Obsidian</a> on X, mentioning Bench shutting down news.
The author also believes in software operating in an open format.
I can‚Äôt agree more.
We will see more cases like the shutting down of Bench in the future.
People will be more aware of the risk of data lock-in when using software that relies on a closed format.
Therefore, while software with ‚Äúfile over app‚Äù in mind may not look sexy in the short term because it is harder to build, it will win in the long run.</p>

<h2 id="open-source-as-much-as-possible">Open source as much as possible</h2>

<p>While building BeanHub, enormous critical components are missing to achieve the goals.
For example, I need a parser to operate on the text file.
To keep updating the file without messing it up, I needed a formatter.
To import transactions from CSV files, I need a rule-based import engine.
While Beancount and many of its tools are open-source, not all meet my needs.
Therefore, I built <a href="https://github.com/LaunchPlatform/beancount-parser">beancount-parser</a> to parse Beancount syntax with comment awareness and <a href="https://github.com/LaunchPlatform/beancount-black">beancount-black</a> to format the syntax.
I also built <a href="https://github.com/LaunchPlatform/beanhub-import">beanhub-import</a> as the rule-based importing engine.</p>

<figure><img src="https://fangpenlin.com/images/2024-12-30-my-beancount-books-are-95-percent-automatic/beanhub-open-source-list.png" alt="Screenshot of BeanHub open source list"><figcaption><p>Screenshot of BeanHub open source list</p></figcaption></figure>

<p>These are just the tip of the iceberg.
Looking back, to my own surprise, I‚Äôve already open-sourced 15 projects just for building BeanHub in the past three years.
You can find the whole list of our <a href="https://beanhub.io/open-source/">BeanHub open-source projects here</a>.
Why I open-sourced this many projects, you may ask.</p>

<p>First, as I built the product on top of open-source tools, I wanted to give back as much as possible.
Second, as a business, I want to keep the ‚Äúfile over app‚Äù concept valid even after the future shutdown of my site.
The importing rule engine beanhub-import is a great example.
People will write their own import rules, and then their workflow will depend on it.
Therefore, they need to be open-source.
Otherwise, users will lose their ability to import transactions after the end of the software life cycle.
Yet another reason for open source is its free exposure.
Even though some people were not interested in a hosted Beancount service like BeanHub at the very first, they learned the existence of BeanHub through the beancount-black formatter I built.</p>

<p>Sample beanhub-import rule YAML file shows how you can define rules for importing transactions:</p>
<div><pre><code><span>inputs</span><span>:</span>
  <span>-</span> <span>match</span><span>:</span> <span>"</span><span>import-data/mercury/*.csv"</span>
    <span>config</span><span>:</span>
      <span># use `mercury` extractor for extracting transactions from the input file</span>
      <span>extractor</span><span>:</span> <span>mercury</span>
      <span># the default output file to use</span>
      <span>default_file</span><span>:</span> <span>"</span><span>books/{{</span><span> </span><span>date.year</span><span> </span><span>}}.bean"</span>
      <span># postings to prepend for all transactions generated from this input file</span>
      <span>prepend_postings</span><span>:</span>
        <span>-</span> <span>account</span><span>:</span> <span>Assets:Bank:US:Mercury</span>
          <span>amount</span><span>:</span>
            <span>number</span><span>:</span> <span>"</span><span>{{</span><span> </span><span>amount</span><span> </span><span>}}"</span>
            <span>currency</span><span>:</span> <span>"</span><span>{{</span><span> </span><span>currency</span><span> </span><span>|</span><span> </span><span>default('USD',</span><span> </span><span>true)</span><span> </span><span>}}"</span>

  <span>-</span> <span>name</span><span>:</span> <span>Routine Wells Fargo expenses</span>
    <span>common_cond</span><span>:</span>
      <span>extractor</span><span>:</span>
        <span>equals</span><span>:</span> <span>"</span><span>plaid"</span>
      <span>file</span><span>:</span>
        <span>suffix</span><span>:</span> <span>"</span><span>(.+)/Wells</span><span> </span><span>Fargo/(.+).csv"</span>
    <span>match</span><span>:</span>
      <span>-</span> <span>cond</span><span>:</span>
          <span>desc</span><span>:</span> <span>"</span><span>Comcast"</span>
        <span>vars</span><span>:</span>
          <span>account</span><span>:</span> <span>Expenses:Internet:Comcast</span>
          <span>narration</span><span>:</span> <span>"</span><span>Comcast</span><span> </span><span>internet</span><span> </span><span>fee"</span>
      <span>-</span> <span>cond</span><span>:</span>
          <span>desc</span><span>:</span> <span>"</span><span>PG&amp;E"</span>
        <span>vars</span><span>:</span>
          <span>account</span><span>:</span> <span>Expenses:Gas:PGE</span>
          <span>narration</span><span>:</span> <span>"</span><span>PG&amp;E</span><span> </span><span>Gas"</span>
    <span>actions</span><span>:</span>
      <span># generate a transaction into the beancount file</span>
      <span>-</span> <span>file</span><span>:</span> <span>"</span><span>books/{{</span><span> </span><span>date.year</span><span> </span><span>}}.bean"</span>
        <span>txn</span><span>:</span>
          <span>payee</span><span>:</span> <span>"</span><span>{{</span><span> </span><span>payee</span><span> </span><span>|</span><span> </span><span>default(omit,</span><span> </span><span>true)</span><span> </span><span>}}"</span>
          <span>narration</span><span>:</span> <span>"</span><span>{{</span><span> </span><span>narration</span><span> </span><span>|</span><span> </span><span>default(desc,</span><span> </span><span>true)</span><span> </span><span>|</span><span> </span><span>default(bank_desc,</span><span> </span><span>true)</span><span> </span><span>}}"</span>
          <span>postings</span><span>:</span>
            <span>-</span> <span>account</span><span>:</span> <span>"</span><span>{{</span><span> </span><span>account</span><span> </span><span>}}"</span>
              <span>amount</span><span>:</span>
                <span>number</span><span>:</span> <span>"</span><span>{{</span><span> </span><span>-amount</span><span> </span><span>}}"</span>
                <span>currency</span><span>:</span> <span>"</span><span>{{</span><span> </span><span>currency</span><span> </span><span>|</span><span> </span><span>default('USD',</span><span> </span><span>true)</span><span> </span><span>}}"</span>
</code></pre></div>

<p>While it sounds like just let‚Äôs open-source everything, the decision didn‚Äôt come without concerns.
Suppose you have followed the open-source community news closely.
In that case, you see stories of big tech companies like Amazon taking open-source projects such as <a href="https://www.mongodb.com/">MongoDB</a>, <a href="https://www.elastic.co/">Elastic Search</a>, or <a href="https://redis.io/">Redis</a> and then providing competing services.
As a business owner, I cannot ignore the risk of competitors providing the same service with my code.
To open source or not to open source is indeed a question. Based on my experience, here are the questions I often ask myself about whether to open-source a project or not:</p>

<ol>
  <li>Is this project going to be useful to someone else</li>
  <li>Who will benefit from this?</li>
  <li>Is this project going to provide exposure?</li>
  <li>Does it help serve the ‚Äúfile over app‚Äù concept?</li>
  <li>Will my competitors going to take it and compete with me?</li>
</ol>

<p>You must think about these factors and find a balance between them.
In the end, while they are not superstar open-source projects (yet), but based on GitHub stars, some people found those projects useful, and I am very proud of doing it:</p>

<figure><img src="https://fangpenlin.com/images/2024-12-30-my-beancount-books-are-95-percent-automatic/launchplatform-github-repos.png" alt="Screenshot of Launch Platform's company open-source GitHub repositories"><figcaption><p>Screenshot of Launch Platform's company open-source GitHub repositories</p></figcaption></figure>

<p>I hope one day I can open source even more, but because once it‚Äôs open source, you cannot take it back, so I would rather be careful than regret it.
Also, being able to open source more usually requires a proper business to keep your income growing while others cannot take advantage of it.
So, I need to take time and think about it.</p>

<p>But even if it‚Äôs not 100% open source yet, you can run all your workflows with BeanHub using the open-source tools we provide locally.
The only thing missing is the <a href="https://plaid.com/">Plaid</a> API integration for pulling and dumping bank transactions into local CSV files for beanhub-import to consume.
Nothing stops you from signing up with Plaid API and doing the same locally.
The only problem is that some banks, such as Chase, require a security reviewing process for accessing transaction data, and you may not want to go through it as a single user.</p>

<h2 id="built-a-github-from-the-ground-up">Built a GitHub from the ground up</h2>

<p>One of the biggest benefits of operating on a plaintext-based open format is that you can track them easily with Git and have a full history of changes for free.
Therefore, it‚Äôs a no-brainer for me to build BeanHub on top of Git.
I like the <a href="https://www.heroku.com/">Heroku</a> deployment experience.
One can write deployment configurations such as code and push.
The platform will take care of it.
I want to provide a similar experience with BeanHub, allowing users to push the changes to their Git repository, and it will check your books automatically.
When there‚Äôs an update on the server end, such as bank transaction updates, we make a commit with changes so you can pull locally.</p>

<figure><img src="https://fangpenlin.com/images/2024-12-30-my-beancount-books-are-95-percent-automatic/git-push-screenshot.png" alt="Screenshot of BeanHub repository git push console output showing the Beancount balance"><figcaption><p>Screenshot of BeanHub repository git push console output showing the Beancount balance</p></figcaption></figure>

<p>It‚Äôs just a hosted Git repository with some hooks, so it shouldn‚Äôt be hard, right?
No, unfortunately, it‚Äôs pretty hard.
Hosting Git repositories isn‚Äôt that hard if you don‚Äôt need to consider</p>

<ul>
  <li>Scalability</li>
  <li>Durability</li>
  <li>Data integrity</li>
  <li>Forking</li>
  <li>Cost-effectiveness</li>
  <li>Security</li>
</ul>

<p>And if you need to consider all of these, you‚Äôre basically building a GitHub from the ground up.
And yes, I did it for BeanHub by myself.
How it works deserves yet another full-blown blog post.
In fact, I have already written it here, in case you‚Äôre interested: <a href="https://beanhub.io/blog/2024/06/26/how-beanhub-works-part2-layer-based-git-repos/">How BeanHub works, part 2, a large-scale auditable Git repository system based on container layers</a>.
tl;dr, I use containers with overlayfs to capture changes made in each git operation.</p>

<figure><img src="https://fangpenlin.com/images/2024-12-30-my-beancount-books-are-95-percent-automatic/overlayfs.svg" alt="The diagram a viewer seeing the upper folder with lower folder underneath, the viewer sees a merged folder by combing the changes introduced in the upper layer on top of the lower layer"><figcaption><p>The diagram a viewer seeing the upper folder with lower folder underneath, the viewer sees a merged folder by combing the changes introduced in the upper layer on top of the lower layer</p></figcaption></figure>

<p>The tech I built here made hosting many Git repositories with custom automatic hook actions possible, and I can already think of many interesting use cases for it.
I am considering spinning it off as a standalone product empowering git-based ‚Äúfile over app‚Äù software.
If you‚Äôre interested in it, please reach out to me at <a href="https://fangpenlin.com/cdn-cgi/l/email-protection#f89e99969f889d96b894998d969b908894998c9e978a95d69b9795"><span data-cfemail="5d3b3c333a2d38331d313c28333e352d313c293b322f30733e3230">[email&nbsp;protected]</span></a>.
With enough people expressing their interests, I could make it happen. Please let me know.</p>

<h2 id="security-issues">Security issues</h2>

<p>Security is another interesting topic to consider when building software like this, particularly when processing files provided by users.
Even Beancount files may seem innocent and harmless, but you might be surprised to learn that one can easily execute code with carefully created ones.
Here‚Äôs an example:</p>

<p><code>main.bean</code>:</p>

<pre><code>option "insert_pythonpath" "true"
plugin "my_plugins"

2024-04-21 open Assets:Cash
2024-04-21 open Expenses:Food

2024-04-22 * "Dinner"
    Assets:Cash       -20.00 USD
    Expenses:Food      20.00 USD
</code></pre>

<p><code>my_plugins.py</code>:</p>

<div><pre><code><span>__plugins__</span> <span>=</span> <span>[</span><span>"evil"</span><span>]</span>

<span>def</span> <span>evil</span><span>(</span><span>entries</span><span>,</span> <span>options</span><span>):</span>
    <span>print</span><span>(</span><span>'!!ALL YOUR ACCOUNTING BOOKS ARE BELONG TO US!!'</span><span>)</span>
    <span>return</span> <span>entries</span><span>,</span> <span>[]</span>

</code></pre></div>

<p>Fortunately, I built a large-scale data pipeline for my former employer dealing with user-uploaded data with potential zero-day exploits.
To process them securely, I‚Äôve learned to adopt sandboxing technology with containers.
It‚Äôs yet another interesting topic worth its full-blown article.
Guess what? I‚Äôve already written one. You can find it here if you‚Äôre interested: <a href="https://beanhub.io/blog/2024/04/23/how-beanhub-works-part1-sandboxing/">How BeanHub Works part1 contains the danger of processing Beancount data with sandbox</a>.</p>

<figure><img src="https://fangpenlin.com/images/2024-12-30-my-beancount-books-are-95-percent-automatic/beanhub-attacked-with-sandbox.svg" alt="The diagram of an attacker uploads data with malicious code, the server processes it inside a sandbox. The attacker's code tried to access outside of the sandbox but failed."><figcaption><p>The diagram of an attacker uploads data with malicious code, the server processes it inside a sandbox. The attacker's code tried to access outside of the sandbox but failed.</p></figcaption></figure>

<p>Thanks to the container-based sandbox technology, BeanHub dodged a bullet in the recent <a href="https://jinja.palletsprojects.com/en/stable/changes/#version-3-1-5">Jinja2 security bugs</a>, allowing the template environment powered by its SandboxedEnvironment to escape in certain situations. Because all the operations are done inside the sandboxed container, it won‚Äôt impact anything else unless the attacker can break the sandbox.
I guess this is the point of defense in depth ‚Äì you don‚Äôt rely on a single layer of defense.</p>

<h2 id="product-builders-imposter-syndrome">Product builder‚Äôs imposter syndrome</h2>

<p>I have confidence in building products.
But being a salesperson is yet another story.
Very often, I compare my product with big companies‚Äô products, and I always feel there‚Äôs something short in it for me to sell proudly.
I always think, okay, after adding the feature XYZ, it should be good enough, and I should sell it harder by then.
But after adding the new feature, I still felt it was not good enough to sell it at full throttle.</p>

<p>In fact, there is a faction of startup folks who would tell you, let‚Äôs sell the product first and then build software to scale later, such as described in the famous <a href="https://paulgraham.com/ds.html">Do things don‚Äôt scale article</a>.
I think they are right, but knowing it‚Äôs right versus doing it is still different.
I wish I could sell a product before it even exists, but I am more of a builder than a seller.
The gap between confidence levels makes me almost always move toward building first and then selling later.</p>

<p>Another source of my problem comes from the pride of being a builder; I want to push out something that‚Äôs polished.
The fear of pushing out your idea first and others may do it before you also exist.
I am still learning how to overcome these and gear myself toward this approach.
Even with a build-something-first approach, to strike a balance for a builder type of startup founder, I guess a rule of thumb could be ‚Äì if it‚Äôs good enough for your internal use cases, it‚Äôs good enough for others.
Think about it. Building those products takes a whole team of good engineers; you should be proud of what you‚Äôve built with just yourself or a very small team.</p>

<h2 id="you-also-need-to-educate-the-users-and-grow-the-community">You also need to educate the users and grow the community</h2>

<p>While selling the product itself is one challenge, teaching your users how to use it is also another.
Plaintext double-entry accounting is a very niche topic, and not everybody knows what‚Äôs double-entry accounting.
Some of its concepts are also hard to digest, such as why the income would be negative in Beancount.
To help the users understand plaintext double-entry accounting, I also launched <a href="https://academy.beanhub.io/">BeanHub Academy</a>, a tutorial for plaintext double-entry accounting with Beancount.</p>

<figure><img src="https://fangpenlin.com/images/2024-12-30-my-beancount-books-are-95-percent-automatic/beanhub-academy.png" alt="Screenshot of BeanHub Academy website home page"><figcaption><p>Screenshot of BeanHub Academy website home page</p></figcaption></figure>

<p>The tutorial is still working in progress.
Surely, it will attract some potential users if they read it.
It‚Äôs not long enough for me to judge how effective it is, but I believe that if more people understand plaintext-based double-entry accounting in the long run, it will benefit BeanHub.</p>

<p>While we mentioned the fear of potential competition in the open source section, it‚Äôs too early for me to worry about it because there‚Äôs none for now.
When the addressable market is very small, the biggest concern is to grow the community size.
Therefore, I sponsored the <a href="https://plaintextaccounting.org/">plaintextaccounting.org</a> website in exchange for an ad explosion despite the fact that other open-source tools may compete with my own.</p>

<figure><img src="https://fangpenlin.com/images/2024-12-30-my-beancount-books-are-95-percent-automatic/pta-sponsorship.png" alt="Screnshot of Plain Text Accounting website"><figcaption><p>Screnshot of Plain Text Accounting website</p></figcaption></figure>

<p>That ad brings many users to BeanHub; it was a great investment to promote my product and help the community a bit; it‚Äôs a win-win.</p>

<h2 id="finanl-thought">Finanl thought</h2>

<p>I hope you find my article interesting.
As I said many times, I think building software is like a marathon.
While it takes a long time for me to get things done right, once they are done right, the positive impact can last longer.
I also believe more software should embody the ‚Äúfile over app‚Äù idea and let the user control their data.
I hope to see more apps like BeanHub and Obsidian take this approach.</p>

<p>What‚Äôs next, you may ask?</p>

<blockquote>
  <p>What about an LLM-powered automatic book cooking feature?</p>
</blockquote>

<p>That sounds like a great idea‚Ä¶ huh‚Ä¶.You know what? Maybe not üòÖ
Other than that, if you have any feedback about BeanHub, please feel free to reach out to us at <a href="https://fangpenlin.com/cdn-cgi/l/email-protection#f1828481819e8385b19394909f998493df989e"><span data-cfemail="5f2c2a2f2f302d2b1f3d3a3e31372a3d713630">[email&nbsp;protected]</span></a></p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things we've learned about building products (117 pts)]]></title>
            <link>https://newsletter.posthog.com/p/50-things-weve-learned-about-building</link>
            <guid>43267095</guid>
            <pubDate>Wed, 05 Mar 2025 14:44:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsletter.posthog.com/p/50-things-weve-learned-about-building">https://newsletter.posthog.com/p/50-things-weve-learned-about-building</a>, See on <a href="https://news.ycombinator.com/item?id=43267095">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>To celebrate 50k subscribers to Product for Engineers, here are the 50 most important lessons we‚Äôve learned about building successful products:</p><div><li><p><a href="https://newsletter.posthog.com/i/145504867/they-need-to-be-genuinely-small" rel="">Small teams</a><span> (6 people or fewer) can build great products, but they need to be given autonomy to set their own goals, prioritize their roadmap, pick metrics, talk to users, and ship code fast.</span></p></li><li><p><span>The success of a product is a result of the people who work on it. Having a high bar for hiring is critical because </span><a href="https://newsletter.posthog.com/i/149601419/taking-shortcuts-when-hiring" rel="">talent compounds</a><span>. Nothing slows you down more than a bad hire who isn‚Äôt working out.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e6c492e-e5e8-4cae-af5a-c89459313c9e_1798x1032.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e6c492e-e5e8-4cae-af5a-c89459313c9e_1798x1032.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e6c492e-e5e8-4cae-af5a-c89459313c9e_1798x1032.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e6c492e-e5e8-4cae-af5a-c89459313c9e_1798x1032.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e6c492e-e5e8-4cae-af5a-c89459313c9e_1798x1032.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e6c492e-e5e8-4cae-af5a-c89459313c9e_1798x1032.png" width="658" height="377.8076923076923" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5e6c492e-e5e8-4cae-af5a-c89459313c9e_1798x1032.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:836,&quot;width&quot;:1456,&quot;resizeWidth&quot;:658,&quot;bytes&quot;:347653,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e6c492e-e5e8-4cae-af5a-c89459313c9e_1798x1032.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e6c492e-e5e8-4cae-af5a-c89459313c9e_1798x1032.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e6c492e-e5e8-4cae-af5a-c89459313c9e_1798x1032.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e6c492e-e5e8-4cae-af5a-c89459313c9e_1798x1032.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div></li><li><p><span>Building a great product requires </span><a href="https://newsletter.posthog.com/i/149601419/not-trusting-teammates" rel="">trust</a><span>. A lack of trust is often one of the biggest bottlenecks a team has. This is likely a result of hiring someone who isn‚Äôt a high enough level, or failing to give feedback to that person to help them improve.</span></p></li><li><p><span>Trust is also built with </span><a href="https://newsletter.posthog.com/i/143650666/they-value-transparency-and-trust" rel="">transparency</a><span>. Work in public, have discussions in the open, and document what you‚Äôre working on. This gives everyone the context they need, and eliminates the political squabbles that plague many companies.</span></p></li><li><p><span>Rely on </span><a href="https://newsletter.posthog.com/i/138480760/trust-and-feedback-over-process" rel="">trust and feedback</a><span>, not process. This is one of our </span><a href="https://posthog.com/handbook/company/values?utm_source=posthog-newsletter&amp;utm_medium=email" rel="">core values</a><span>. Building and scaling something people want is a nuanced problem, so we let people use their judgement. When they get it wrong, we are direct and give feedback.</span></p></li><li><p><span>Execs should </span><a href="https://newsletter.posthog.com/i/152182724/a-product-engineers-set-their-own-quarterly-goals" rel="">share company goals</a><span>; product teams (engineers) should figure out what to build to achieve them and set their own goals. Both should review that what they‚Äôre building is having an actual impact using metrics and user feedback.</span></p></li><li><p><span>Your product is downstream from </span><a href="https://newsletter.posthog.com/i/140835142/your-entire-strategy-is-downstream-of-your-icp" rel="">your ideal customer profile</a><span> (ICP). They are who you are building for and are the most important factor for helping you decide what to build. An accurate ICP will define not just which customers you target, but </span><em>every</em><span> </span><em>aspect</em><span> of your product and go-to-market strategy.</span></p></li><li><p><span>To find your ICP, </span><a href="https://newsletter.posthog.com/i/140835142/start-with-your-best-guess-and-test-it" rel="">take a guess and then test it</a><span>. Ask questions on signup, compare retention, identify your power users, run NPS surveys. As you gain more intel (and confidence), add detail to it.</span></p></li><li><p><span>Create </span><a href="https://newsletter.posthog.com/i/140404838/agree-on-your-product-principles" rel="">product principles</a><span>. Ours are ‚Äúprovide every tool needed for evaluating success, get in first, and be the source of truth for customer and product data.‚Äù These give you a common language for discussing ideas and making decisions.</span></p></li><li><p><a href="https://newsletter.posthog.com/p/how-we-decide-what-to-build" rel="">Map everything your users could want</a><span>, because you‚Äôll need it to prioritize your roadmap. This ensures you aren‚Äôt missing out on great ideas beyond the horizon.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb2e0e7f-edb7-44f7-a438-4c7bd007eb8e_2150x1320.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb2e0e7f-edb7-44f7-a438-4c7bd007eb8e_2150x1320.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb2e0e7f-edb7-44f7-a438-4c7bd007eb8e_2150x1320.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb2e0e7f-edb7-44f7-a438-4c7bd007eb8e_2150x1320.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb2e0e7f-edb7-44f7-a438-4c7bd007eb8e_2150x1320.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb2e0e7f-edb7-44f7-a438-4c7bd007eb8e_2150x1320.png" width="695" height="426.7376373626374" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/eb2e0e7f-edb7-44f7-a438-4c7bd007eb8e_2150x1320.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:894,&quot;width&quot;:1456,&quot;resizeWidth&quot;:695,&quot;bytes&quot;:235350,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb2e0e7f-edb7-44f7-a438-4c7bd007eb8e_2150x1320.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb2e0e7f-edb7-44f7-a438-4c7bd007eb8e_2150x1320.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb2e0e7f-edb7-44f7-a438-4c7bd007eb8e_2150x1320.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb2e0e7f-edb7-44f7-a438-4c7bd007eb8e_2150x1320.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div></li><li><p><span>Your product is more than just its functionality. It‚Äôs </span><a href="https://newsletter.posthog.com/p/how-not-to-be-boring?utm_source=publication-search" rel="">your brand</a><span> and how your product is experienced by others. How much you invest in each function, the people you hire, what you decide to build, what function leads your company, your customer communication, and your pricing all matter in making you unique.</span></p></li><li><p><span>Your website is the first impression your product is making. A </span><a href="https://newsletter.posthog.com/i/148803758/what-would-stop-me-from-using-this-thing" rel="">boring</a><span>, templated one signals the product and team behind the website isn‚Äôt very strong. Creating a website unique to you and built for your ideal customer profile is a way to prevent this, and encourage users into your product.</span></p></li><li><p><span>Sometimes it‚Äôs </span><a href="https://newsletter.posthog.com/i/142577602/is-your-product-the-problem-or-the-market" rel="">the market‚Äôs problem</a><span>, not your product. For example, when Monzo founder and YC partner </span><a href="https://twitter.com/t_blom" rel="">Tom Blomfield</a><span> was building a bill-splitting service for college friends, he got the advice to stop building and start trying to acquire new users. When he got one new user after four weeks of cold calling, he knew it was time to pivot.</span></p></li><li><p><span>If you‚Äôre going to pivot, </span><a href="https://newsletter.posthog.com/i/142577602/if-youre-going-to-pivot-make-it-big" rel="">make it big</a><span>. Stewart Butterfield pivoted two video game companies into Flickr and Slack. LinkedIn co-founder Reid Hoffman says startup founders can pivot from failure to success, but only if they ‚Äúslash and burn‚Äù the rest of their business. If it looks similar, go further.</span></p></li><li><p><span>As 37Signal‚Äôs </span><a href="https://twitter.com/jasonfried/status/1337095209620946944" rel="">Jason Fried</a><span> says ‚ÄúYou cannot validate an idea. It doesn‚Äôt exist, you have to build the thing. The market will validate it.‚Äù</span></p></li><li><p>Plans are useful, but sticking rigidly to them is not. Good execution should not mean executing a specific plan, but repeatedly doing the most important things. Judge people on what they ship, how often they ship, and the impact of their work, not on whether they ‚Äústuck to the plan.‚Äù</p></li><li><p>Waiting ‚Äúone more week‚Äù to ship something is nearly always a bad idea. This attitude extrapolated out over months = way less momentum. The sooner you get something into the hands of users, the sooner you‚Äôll learn if it‚Äôs something they‚Äôll value, and how to make it better.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cf06c58-be72-4063-8a23-b6247b8c5e13_1200x630.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cf06c58-be72-4063-8a23-b6247b8c5e13_1200x630.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cf06c58-be72-4063-8a23-b6247b8c5e13_1200x630.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cf06c58-be72-4063-8a23-b6247b8c5e13_1200x630.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cf06c58-be72-4063-8a23-b6247b8c5e13_1200x630.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cf06c58-be72-4063-8a23-b6247b8c5e13_1200x630.jpeg" width="695" height="364.875" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7cf06c58-be72-4063-8a23-b6247b8c5e13_1200x630.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:630,&quot;width&quot;:1200,&quot;resizeWidth&quot;:695,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cf06c58-be72-4063-8a23-b6247b8c5e13_1200x630.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cf06c58-be72-4063-8a23-b6247b8c5e13_1200x630.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cf06c58-be72-4063-8a23-b6247b8c5e13_1200x630.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cf06c58-be72-4063-8a23-b6247b8c5e13_1200x630.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div></li><li><p><a href="https://newsletter.posthog.com/i/138653073/reduce-work-in-progress" rel="">Reduce your work in progress</a><span>. PRs should be doable in a day, start your day by responding to others‚Äô review requests, merge whenever, ship behind a feature flag, and test in production. Each of these reduces the amount of context needed to get work done.</span></p></li><li><p><span>Shipping fast is a core part of our product development philosophy, but we realize this has trade offs. Technology procurement, planning meetings, agile rituals, metrics reviews take a back seat. </span><a href="https://newsletter.posthog.com/p/how-we-work-asynchronously" rel="">Being async</a><span> gives us more ability to do this.</span></p></li><li><p><a href="https://newsletter.posthog.com/p/how-we-choose-technologies?utm_source=publication-search" rel="">Adopting new technologies</a><span> into your product should only be done for hair-on-fire problems like excessive costs, scaling challenges, or customer needs. Potential solution technologies can be found by asking what ways your team or other teams have solved that solution themselves.</span></p></li><li><p><a href="https://newsletter.posthog.com/p/the-deadline-doom-loop" rel="">Artificial deadlines</a><span> will not make your team faster. They often result in a doom loop that leads to mountains of tech debt and burn out. Instead, clear the processes that slow teams down, and give small teams the autonomy to ship fast.</span></p></li><li><p><span>Another way to ship faster is to have no design by default. </span><a href="https://newsletter.posthog.com/i/138480760/dont-let-designers-dictate-to-engineers" rel="">Get a design system running</a><span> and then leave engineers to build. When needed, have a design review and polish what‚Äôs already been shipped.</span></p></li><li><p><a href="https://newsletter.posthog.com/p/dont-make-these-classic-feature-flag?utm_source=publication-search" rel="">Feature flags</a><span> enable product engineers to ship changes quickly, test in production, and get feedback from real users. They also mitigate risk by acting as kill switches for rollouts not working as expected.</span></p></li><li><p><span>The best type of communication at PostHog is a </span><a href="https://newsletter.posthog.com/i/152182724/b-radically-transparent-communication" rel="">pull request</a><span>. Compared to messages or issues, they turn feedback into impact immediately. This aligns with our bias for action and helps us create tighter feedback loops.</span></p></li><li><p><a href="https://newsletter.posthog.com/i/140404838/make-ownership-clear" rel="">Make ownership clear</a><span>. This makes deciding what to build much clearer and faster. Ownership-avoidant teams spend too much time planning, brainstorming, meeting, and project managing when they could just be shipping.</span></p></li><li><p><span>Engineers are capable of deciding what to build. They understand the technical constraints, see patterns across features, and can figure out how to solve a problem. They just might have an </span><a href="https://newsletter.posthog.com/p/talk-to-users?open=false#%C2%A7you-have-an-information-bottleneck" rel="">information bottleneck</a><span> when it comes to what users want.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c23846a-f569-4f4f-b6fe-97e806ddc14d_2038x968.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c23846a-f569-4f4f-b6fe-97e806ddc14d_2038x968.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c23846a-f569-4f4f-b6fe-97e806ddc14d_2038x968.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c23846a-f569-4f4f-b6fe-97e806ddc14d_2038x968.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c23846a-f569-4f4f-b6fe-97e806ddc14d_2038x968.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c23846a-f569-4f4f-b6fe-97e806ddc14d_2038x968.png" width="1456" height="692" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9c23846a-f569-4f4f-b6fe-97e806ddc14d_2038x968.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:692,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:108994,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c23846a-f569-4f4f-b6fe-97e806ddc14d_2038x968.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c23846a-f569-4f4f-b6fe-97e806ddc14d_2038x968.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c23846a-f569-4f4f-b6fe-97e806ddc14d_2038x968.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c23846a-f569-4f4f-b6fe-97e806ddc14d_2038x968.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div></li><li><p><span>Instead of controlling the engineers, </span><a href="https://newsletter.posthog.com/i/152182724/product-managers-give-engineers-context" rel="">product managers</a><span> should create context for product teams by analyzing product analytics, conducting user research, doing competitor research, and more.</span></p></li><li><p>Most people aren't Steve Jobs. They don't just "know" what to build from the start or have a grand vision. Instead, they ship things, get them into the hands of users, iterate on feedback, and repeat. The faster you do this, the better your product will be.</p></li><li><p><span>Hire and rely on </span><a href="https://newsletter.posthog.com/p/beyond-the-10x-engineer" rel="">product engineers</a><span>. They have full-stack technical skills needed to build a product along with customer obsession. Yes, this means they need to talk to users, do user interviews, recruit tests for new features, collect feedback, do support, and respond to incidents.</span></p></li><li><p><span>Read </span><a href="https://www.momtestbook.com/" rel="">The Mom Test</a><span>. It will teach you how to talk to potential users about their problems. A key lesson is the two types of user interviews: problem exploration and solution validation. The first guides future product decisions. The second helps improve what you‚Äôre working on right now.</span></p></li><li><p><span>To get the most out of </span><a href="https://newsletter.posthog.com/i/143894211/how-to-prepare-for-a-user-interview" rel="">a user interview</a><span>, know who your users are (</span><a href="https://newsletter.posthog.com/p/defining-our-icp-is-the-most-important" rel="">ICP</a><span>), how they‚Äôre using your product, and what you want to build next. Doing this will ensure that the interview clarifies your focus in next steps, rather than distracting and confusing you.</span></p></li><li><p><span>Out of all the things you could potentially build, support requests are one of the most ‚Äúreal.‚Äù A specific user has a specific problem. When you solve it, you just </span><a href="https://newsletter.posthog.com/i/147529900/constant-improvements" rel="">improved your product</a><span>, other changes can‚Äôt say the same.</span></p></li><li><p><span>Doing support as an engineer </span><a href="https://newsletter.posthog.com/i/147529900/encouraging-full-cycle-ownership" rel="">encourages ownership</a><span> of the full lifecycle of a product from ideation to implementation to ongoing maintenance. Each of these builds on each other by building the context on real customer pain points and the code behind the issues.</span></p></li><li><p><a href="https://newsletter.posthog.com/p/doing-support-makes-you-a-better" rel="">Engineers doing support</a><span> shortens the loop between a problem for users and a fix being shipped. No support people, product managers, and planning processes need to get in the way. As a bonus, users love this.</span></p></li><li><p><span>Using your own product AKA </span><a href="https://newsletter.posthog.com/p/using-your-own-product-is-a-superpower" rel="">dogfooding</a><span> helps you ship faster, intercept problems before they reach users, deeply understand your product, and develop empathy for your users. It </span><a href="https://newsletter.posthog.com/i/146560398/mistakes-to-avoid" rel="">does not replace</a><span> talking to users, getting real world feedback, and tracking real usage metrics, though.</span></p></li><li><p><span>Scratching your own itch, like </span><a href="https://newsletter.posthog.com/i/146560398/you-need-to-scratch-your-own-itch" rel="">our product team did with interview popups</a><span>, can help validate use cases. If you‚Äôre not using your own product, but should be, it‚Äôs a sign something is wrong (and you should do something to change it).</span></p></li><li><p><span>Great product builders are always </span><a href="https://newsletter.posthog.com/i/141291244/always-prototyping-and-experimenting" rel="">prototyping and experimenting</a><span>. This means they need to be comfortable building MVPs and proofs of concept, shipping unpolished work, getting feedback, and pivoting when things aren‚Äôt working. It also means having all the skills to go from zero to built, everything from infrastructure to design.</span></p></li><li><p>Running a successful A/B test requires a good hypothesis explaining what you‚Äôre testing and why you‚Äôre testing it. Include the context of the test, the change, the metric, and the expected results.</p></li><li><p>When running product experiments, know that they might fail and get removed. This means you should set it up to be removed (with a feature flag) and ship the ‚Äúgood enough‚Äù version over the perfectly maintainable and scalable one. You can improve it later, once it succeeds.</p></li><li><p><span>Product experiments can </span><a href="https://newsletter.posthog.com/i/148982770/how-to-start-developing-an-experimentation-mindset" rel="">be a lot ‚Äúdumber‚Äù than you realize</a><span>. For example, instead of building out a full feature, try a fake door test. This is where you add options or buttons with nothing behind them to check that people will click on them.</span></p></li><li><p><span>Failure is not the end of the world for product experiments. At </span><a href="https://hbr.org/2017/09/the-surprising-power-of-online-experiments" rel="">Google</a><span>, 80-90% of experiments "fail." You might think of this as a waste of time, but, at scale, 10% of successes can more than pay for all the failures. For example, an A/B test of how Bing displayed headlines boosted revenue by 12% (more than $100M at the time).</span></p></li><li><p><span>When focusing on growth, </span><a href="https://newsletter.posthog.com/i/148982770/prioritize-like-a-growth-engineer" rel="">think (and prioritize) like a growth engineer</a><span>. Identify a target area, figure out a metric representing that area, create a hypothesis on how to improve it, and implement as small an experiment as possible to test it.</span></p></li><li><p><span>It‚Äôs just about never </span><a href="https://newsletter.posthog.com/i/142334709/its-too-early-for-analytics" rel="">too early for analytics</a><span>. This makes sense for </span><em>pre-launch</em><span> products, but </span><em>launching</em><span> without analytics because ‚Äúit‚Äôs too early‚Äù is a false economy. Once you launch, priorities shift from shipping as fast as possible to shipping the right thing as fast as possible.</span></p></li><li><p>When starting with analytics, start small. Choose a specific product or feature, track its usage with autocapture, visualize this data with trends and retention graphs, and try to ship features that improve those. The ‚Äúmodern data stack industrial complex‚Äù makes this seem a lot more complicated than it actual is.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79b74ce8-5a9a-45ef-ba90-ed2e1a1e4986_754x746.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79b74ce8-5a9a-45ef-ba90-ed2e1a1e4986_754x746.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79b74ce8-5a9a-45ef-ba90-ed2e1a1e4986_754x746.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79b74ce8-5a9a-45ef-ba90-ed2e1a1e4986_754x746.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79b74ce8-5a9a-45ef-ba90-ed2e1a1e4986_754x746.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79b74ce8-5a9a-45ef-ba90-ed2e1a1e4986_754x746.png" width="599" height="592.6445623342175" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/79b74ce8-5a9a-45ef-ba90-ed2e1a1e4986_754x746.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:746,&quot;width&quot;:754,&quot;resizeWidth&quot;:599,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79b74ce8-5a9a-45ef-ba90-ed2e1a1e4986_754x746.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79b74ce8-5a9a-45ef-ba90-ed2e1a1e4986_754x746.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79b74ce8-5a9a-45ef-ba90-ed2e1a1e4986_754x746.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79b74ce8-5a9a-45ef-ba90-ed2e1a1e4986_754x746.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div></li><li><p><span>Don‚Äôt know which metric to start with? We recommend </span><a href="https://newsletter.posthog.com/p/wtf-is-activation-and-why-should" rel="">activation</a><span> as its upstream of other metrics, one that engineers can directly influence, and useful across the organization.</span></p></li><li><p><span>Beyond activation, retention is </span><a href="https://newsletter.posthog.com/i/137830990/define-start-tracking-your-high-value-events" rel="">another critical metric</a><span> for understanding the impact of what you are building. Tracking how this changes week by week can start to give you an idea of if your changes are getting users to stick around.</span></p></li><li><p><span>To </span><a href="https://newsletter.posthog.com/i/137830990/zoom-out-on-your-high-value-events" rel="">measure product-market fit</a><span>, check if user engagement is growing exponentially compared to user growth and that retention flattens (above 0%). After that, check that ICP users retain better than non-ICP ones and that your paying customers are part of that ICP.</span></p></li><li><p>Growth reviews ensure that what teams are building is having an impact on the metrics that matter, like revenue, product analytics, and user feedback. These are a main responsibility of product managers at PostHog.</p></li><li><p><span>If your company builds multiple products, </span><a href="https://newsletter.posthog.com/i/138480760/create-mini-startup-teams" rel="">treat each of them like a mini-startup</a><span>. This means their own product decisions, pricing, revenue, costs, and coordination with customer-facing teams.</span></p></li><li><p><span>Work on a product </span><a href="https://newsletter.posthog.com/i/142577602/not-working-on-something-youre-excited-about-pivot" rel="">you‚Äôre excited about</a><span>. As James, PostHog co-founder, wrote in his </span><a href="https://posthog.com/founders/product-market-fit-game" rel="">guide to finding product-market fit</a><span>: </span></p></li></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MacBook Air M4 (339 pts)]]></title>
            <link>https://www.apple.com/macbook-air/</link>
            <guid>43266537</guid>
            <pubDate>Wed, 05 Mar 2025 14:06:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/macbook-air/">https://www.apple.com/macbook-air/</a>, See on <a href="https://news.ycombinator.com/item?id=43266537">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" role="main" data-page-type="overview">
		






















		<section data-component-list="Hero LocalNavTrigger" data-analytics-section-engagement="name:hero">
			<div data-analytics-activitymap-region-id="ribbon">
						<p>
								Now through April 2, get extra trade-in credit toward a new Mac with Apple&nbsp;Trade&nbsp;In.<span><a href="#footnote-1" aria-label="Footnote * symbol" data-modal-close="">*</a></span>
								<a href="https://www.apple.com/us/shop/goto/buy_mac" aria-label="Shop all Mac" data-analytics-title="shop mac - trade in">Shop Mac</a>
							</p>
					</div>
			
			<div>
					<p>Built for Apple&nbsp;Intelligence.</p>
					
					
					<p><span>Available starting March 12th</span></p>
					<p>MacBook&nbsp;Air is the world‚Äôs most popular laptop for a reason. Actually, for a lot of reasons. It delivers <em>up to 18 hours of battery life.<sup><a href="#footnote-4" aria-label="Footnote 1" data-modal-close="">1</a></sup></em> The <em>M4 chip</em> unlocks a whole new level of performance for work and play. <em>Apple&nbsp;Intelligence</em> is built in to help you get things done effortlessly.<sup><a href="#footnote-5" aria-label="Footnote 2" data-modal-close="">2</a></sup> And it now comes in a stunning Sky Blue color. With the perfectly portable MacBook&nbsp;Air, you‚Äôll be ready to take on just about anything,&nbsp;anywhere.</p>
				</div>
		</section>
		<article aria-label="Design" data-anim-scroll-group="Section - Design" data-component-list="ARToggle">
			<div data-component-list="WordAnim" data-anchor=".section-design" data-reverse="true" data-start="a0t-85vh" data-anim-type="fade" data-duration="0.5">
						<h2 data-component-list="WordAnim" data-anchor=".section-design" data-reverse="true" data-start="a0t-85vh" data-anim-type="slide" data-y="-4px" data-duration="0.5" data-ease="0.1" data-ease-function="easeOutQuad">DESIGN</h2>
						<p data-component-list="WordAnim" data-anchor=".section-design" data-reverse="true" data-start="a0t-85vh" data-anim-type="slide" data-y="10px" data-ease="0.1" data-duration="0.4" data-ease-function="easeOutQuad">Built to<br> go places.</p>
					</div>
			
			<div>
				<p><em>Remarkably light and less than half an inch thin,</em> MacBook&nbsp;Air fits easily into your on-the-go lifestyle ‚Äî and your bag. MacBook&nbsp;Air with M4 is made with over 50&nbsp;percent recycled materials and has a durable recycled aluminum&nbsp;enclosure.</p>
			</div>
			<div>
					<div data-analytics-section-engagement="name:sizes">
						<div>
								<picture id="overview-design-sizes-design-sizes-endframe-2" data-lazy="">
									<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
									<img src="https://www.apple.com/v/macbook-air/t/images/overview/design/sizes/design_sizes_endframe__ckfqlo8f44eq_large.jpg" onload="__lp(event)" alt="13-inch and 15-inch MacBook Air back-to-back, showing size difference">
								</picture>
								
								
							</div>
						<div>
							<h3>Two perfectly portable sizes.</h3>
							<p>The 13‚Äëinch MacBook&nbsp;Air is the ultimate on-the-go laptop, and the 15‚Äëinch model gives you even more space onscreen for&nbsp;multitasking.<sup><a href="#footnote-6" aria-label="Footnote 3" data-modal-close="">3</a></sup></p>
						</div>
						
					</div>
					<div data-analytics-section-engagement="name:finishes gallery">
							<h3>A new color is in the air.</h3>
							<p>Now with Sky&nbsp;Blue, four beautiful colors will have you on cloud nine. And every MacBook&nbsp;Air comes with a color-matched MagSafe charging&nbsp;cable.</p>
						</div>
					<div id="lifestyle-gallery" data-analytics-gallery-id="lifestyle gallery" data-component-list="LifestyleGallery" role="group" aria-label="Lifestyle Gallery" data-analytics-section-engagement="name:lifestyle gallery">
								<div id="lifestyle-gallery-item-1" data-analytics-gallery-item-id="fits easily">
										<p>Incredibly light and thin, MacBook&nbsp;Air <em>fits easily in your backpack.</em></p>
										<div>
												<picture id="overview-design-lifestyle-gallery-design-portability-1-2" data-lazy="">
													<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
													<img src="https://www.apple.com/v/macbook-air/t/images/overview/design/lifestyle-gallery/design_portability_1__gfw34rh367u6_large.jpg" onload="__lp(event)" alt="A student placing MacBook Air into their backpack">
												</picture>
												
												
											</div>
									</div>
								<div id="lifestyle-gallery-item-2" data-analytics-gallery-item-id="work from anywhere">
										<p>With <em>up to 18 hours of battery life,<sup><a href="#footnote-4" aria-label="Footnote 1" data-modal-close="">1</a></sup></em> you can get work done anywhere you go.</p>
										<div>
												<picture id="overview-design-lifestyle-gallery-design-portability-2-2" data-lazy="">
													<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
													<img src="https://www.apple.com/v/macbook-air/t/images/overview/design/lifestyle-gallery/design_portability_2__e1065fp5a0ae_large.jpg" onload="__lp(event)" alt="Person in work environment seated on stool using MacBook Air unplugged on their lap while reaching for box on high shelf with other hand">
												</picture>
												
												
											</div>
									</div>
								<div id="lifestyle-gallery-item-3" data-analytics-gallery-item-id="game mode">
										<p>The impressive display on MacBook&nbsp;Air makes <em>TV, movies, and games look truly&nbsp;striking.</em></p>
										<div>
												<picture id="overview-design-lifestyle-gallery-design-portability-3-2" data-lazy="">
													<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
													<img src="https://www.apple.com/v/macbook-air/t/images/overview/design/lifestyle-gallery/design_portability_3__b06jnvn3tjpy_large.jpg" onload="__lp(event)" alt="Three people on a couch watching full-screen video on MacBook Air">
												</picture>
												
												
											</div>
									</div>
							</div>
				</div>
		</article>
		<article aria-label="Performance" data-anim-scroll-group="Section - Performance">
			<div data-component-list="WordAnim" data-anchor=".section-performance" data-reverse="true" data-start="a0t-85vh" data-anim-type="fade" data-duration="0.5">
						<h2 data-component-list="WordAnim" data-anchor=".section-performance" data-reverse="true" data-start="a0t-85vh" data-anim-type="slide" data-y="-4px" data-duration="0.5" data-ease="0.1" data-ease-function="easeOutQuad">Performance and battery life</h2>
						<p data-component-list="WordAnim" data-anchor=".section-performance" data-reverse="true" data-start="a0t-85vh" data-anim-type="slide" data-y="10px" data-ease="0.1" data-duration="0.4" data-ease-function="easeOutQuad">M4. The chip that zips.</p>
					</div>
			
			<div data-analytics-section-engagement="name:m4 highlights" data-component-list="Modal">
				<div>
					<picture id="overview-performance-performance-mx-1" data-lazy="">
						<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
						<img src="https://www.apple.com/v/macbook-air/t/images/overview/performance/performance_mx__b6imot8vkguu_large.jpg" onload="__lp(event)" alt="Apple M4 Chip">
					</picture>
					
					<hr role="presentation">
					<figure>
						<p><span>Up to</span><span>23<span>x</span></span><span>faster than fastest Intel‚Äëbased MacBook&nbsp;Air<sup><a href="#footnote-7" aria-label="Footnote 4" data-modal-close="">4</a></sup></span></p>
					</figure>
					<figure>
						<p><span>Up to</span><span>2<span>x</span></span><span>faster than MacBook&nbsp;Air&nbsp;(M1)<sup><a href="#footnote-8" aria-label="Footnote 5" data-modal-close="">5</a></sup></span></p>
					</figure>
					<figure>
						<p><span>Up to</span><span>18 hrs</span><span><em>battery life</em><sup><a href="#footnote-4" aria-label="Footnote 1" data-modal-close="">1</a></sup></span></p>
					</figure>
				</div>
				<div>
					<p><em>Multitasker. Multifaster.</em> MacBook&nbsp;Air with M4 brings even more speed and fluidity to everything you do, like working between loads of apps and tabs, editing videos, or playing games like Sid Meier‚Äôs Civilization¬Æ VII. All with a silent, fanless&nbsp;design.</p>
					<p><em>Neural Engine. Blazing fast for AI.</em> Thanks to the faster Neural&nbsp;Engine in the M4 chip, MacBook&nbsp;Air has even more powerful AI capabilities to enhance everything you do. From automatic camera frame centering to AI image upscaling to running the latest large language models, you‚Äôll be more productive and creative than&nbsp;ever.</p>
					<p><em>Live a full battery life.</em> MacBook&nbsp;Air has up to 18 hours of battery life. And it supports fast charge, getting up to 50 percent in just 30 minutes.<sup><a href="#footnote-9" aria-label="Footnote 6" data-modal-close="">6</a></sup> So you can power through anything you‚Äôre working on without worrying about your&nbsp;battery.</p>
				</div>
				
				<div data-component-list="PerformanceGalleryScrollGroup" data-modal-id="modal-performance" data-trigger=".modal-graph-trigger" id="modal-performance" data-modal-close-label="Close" data-analytics-activitymap-region-id="modal content">
						<div>
							<div>
									
									<p>M4. The chip that&nbsp;zips.</p>
									<p>Apple silicon combines a powerful CPU and GPU, memory, and more on a single chip, making <em>all your apps ‚Äî from Microsoft 365 Copilot to Adobe Creative Cloud ‚Äî</em> run faster while requiring less&nbsp;power.</p>
								</div>
							<div>
								<picture id="overview-performance-modal-modal-hero-1" lazy-load="true" data-download-area-keyframe="{&quot;start&quot;: &quot;a0t - 100a1t&quot;, &quot;end&quot;: &quot;a1b - 100vh&quot;, &quot;anchors&quot;:[&quot;.page-overview&quot;, &quot;.section-performance&quot;]}">
									<source srcset="https://www.apple.com/v/macbook-air/t/images/overview/performance/modal/modal_hero__dxdbuechqpea_small.jpg, https://www.apple.com/v/macbook-air/t/images/overview/performance/modal/modal_hero__dxdbuechqpea_small_2x.jpg 2x" media="(max-width:734px)">
									<source srcset="https://www.apple.com/v/macbook-air/t/images/overview/performance/modal/modal_hero__dxdbuechqpea_medium.jpg, https://www.apple.com/v/macbook-air/t/images/overview/performance/modal/modal_hero__dxdbuechqpea_medium_2x.jpg 2x" media="(max-width:1068px)">
									<source srcset="https://www.apple.com/v/macbook-air/t/images/overview/performance/modal/modal_hero__dxdbuechqpea_large.jpg, https://www.apple.com/v/macbook-air/t/images/overview/performance/modal/modal_hero__dxdbuechqpea_large_2x.jpg 2x" media="(min-width:0px)">
									<img src="https://www.apple.com/v/macbook-air/t/images/overview/performance/modal/modal_hero__dxdbuechqpea_large.jpg" onload="__lp(event)" alt="MacBook Air screen showing gaming to demonstrate M4 chip capabilities">
								</picture>
								<div>
									<p>Sid Meier‚Äôs Civilization¬Æ VII</p>
								</div>
							</div>
							<div>
									<p>The M4 chip in MacBook&nbsp;Air makes graphics and gaming look better than ever. With <em>second-generation hardware-accelerated ray tracing,</em> you get more realistic images for truly immersive gaming. And Dynamic&nbsp;Caching drives a huge performance boost for pro apps and games. So everything you do feels less like work and more like&nbsp;play.</p>
								</div>
						</div>
						<div data-analytics-activitymap-region-id="modal galleries">
										<div id="vs-previous-models-gallery" data-gallery-toggle="">
											<h3>M4 compared to previous MacBook&nbsp;Air models.</h3>
											<div id="13-inch-model-m4-compare" role="group" data-analytics-gallery-id="macbook air 13 inch m4 compare gallery" aria-label="Performance gallery for 13-inch and 15-inch MacBook Air with M4 chip" data-component-list="PerformanceGallery" data-kfstart="" aria-hidden="false">
														<div data-analytics-gallery-interaction-type="tabnav">
																<ul role="tablist">
																	<li role="presentation">
																		<a role="tab" href="#13-inch-model-gallery-item-1-ai-image-upscaling" id="13-inch-model-gallery-item-1-trigger" data-ac-gallery-trigger="13-inch-model-gallery-item-1" tabindex="0">AI image upscaling</a>
																	</li>
																	
																	<li role="presentation">
																		<a role="tab" href="#13-inch-model-gallery-item-2-excel-calculations" id="13-inch-model-gallery-item-2-trigger" data-ac-gallery-trigger="13-inch-model-gallery-item-2" tabindex="-1">Excel calculations</a>
																	</li>
																	
																	<li role="presentation">
																		<a role="tab" href="#13-inch-model-gallery-item-3-gaming" id="13-inch-model-gallery-item-3-trigger" data-ac-gallery-trigger="13-inch-model-gallery-item-3" tabindex="-1">Gaming</a>
																	</li>
																	
																	<li role="presentation">
																		<a role="tab" href="#13-inch-model-gallery-item-4-photo-editing" id="13-inch-model-gallery-item-4-trigger" data-ac-gallery-trigger="13-inch-model-gallery-item-4" tabindex="-1">Photo editing</a>
																	</li>
																	
																	<li role="presentation">
																		<a role="tab" href="#13-inch-model-gallery-item-5-video-editing" id="13-inch-model-gallery-item-5-trigger" data-ac-gallery-trigger="13-inch-model-gallery-item-5" tabindex="-1">Video editing</a>
																	</li>
																</ul>
																
															</div>
														<div>
															<div id="13-inch-model-gallery-item-1" data-ac-gallery-item="" data-analytics-gallery-item-id="ai image upscaling" data-index="0">
																<p>Faster AI photo resolution enhancement<sup><a href="#footnote-10" aria-label="Footnote 7" data-modal-close="">7</a></sup></p>
																<div>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with&nbsp;M4</span>
																		</p></div>
																		<figure>
																			<div>
																				<p><span>23.4</span>
																					<span>x</span>
																				</p>
																				</div>
																		</figure>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with&nbsp;M3</span>
																		</p></div>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with&nbsp;M1</span>
																		</p></div>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with quad-core Intel&nbsp;Core&nbsp;i7 (baseline)</span>
																		</p></div>
																	</div>
															</div>
															<div id="13-inch-model-gallery-item-2" data-ac-gallery-item="" data-analytics-gallery-item-id="excel calculations" data-index="1">
																<p>Faster spreadsheet calculation performance<sup><a href="#footnote-11" aria-label="Footnote 8" data-modal-close="">8</a></sup></p>
																<div>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with&nbsp;M4</span>
																		</p></div>
																		<figure>
																			<div>
																				<p><span>4.7</span>
																					<span>x</span>
																				</p>
																				</div>
																		</figure>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with&nbsp;M3</span>
																		</p></div>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with&nbsp;M1</span>
																		</p></div>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with quad-core Intel&nbsp;Core&nbsp;i7 (baseline)</span>
																		</p></div>
																	</div>
															</div>
															<div id="13-inch-model-gallery-item-3" data-ac-gallery-item="" data-analytics-gallery-item-id="gaming" data-index="2">
																<p>Faster gaming performance<sup><a href="#footnote-12" aria-label="Footnote 9" data-modal-close="">9</a></sup></p>
																<div>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with&nbsp;M4</span>
																		</p></div>
																		<figure>
																			<div>
																				<p><span>1.6</span>
																					<span>x</span>
																				</p>
																				</div>
																		</figure>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with&nbsp;M3</span>
																		</p></div>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with&nbsp;M1 (baseline)</span>
																		</p></div>
																	</div>
															</div>
															<div id="13-inch-model-gallery-item-4" data-ac-gallery-item="" data-analytics-gallery-item-id="photo editing" data-index="3">
																<p>Faster image filters and effects performance<sup><a href="#footnote-13" aria-label="Footnote 10" data-modal-close="">10</a></sup></p>
																<div>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with&nbsp;M4</span>
																		</p></div>
																		<figure>
																			<div>
																				<p><span>3.6</span>
																					<span>x</span>
																				</p>
																				</div>
																		</figure>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with&nbsp;M3</span>
																		</p></div>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with&nbsp;M1</span>
																		</p></div>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with quad-core Intel&nbsp;Core&nbsp;i7 (baseline)</span>
																		</p></div>
																	</div>
															</div>
															<div id="13-inch-model-gallery-item-5" data-ac-gallery-item="" data-analytics-gallery-item-id="video editing" data-index="4">
																<p>Faster scene edit detection performance<sup><a href="#footnote-14" aria-label="Footnote 11" data-modal-close="">11</a></sup></p>
																<div>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with&nbsp;M4</span>
																		</p></div>
																		<figure>
																			<div>
																				<p><span>18.4</span>
																					<span>x</span>
																				</p>
																				</div>
																		</figure>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with&nbsp;M3</span>
																		</p></div>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with&nbsp;M1</span>
																		</p></div>
																		<div>
																			
																			<p><span>13‚Äëinch MacBook&nbsp;Air with quad-core Intel&nbsp;Core&nbsp;i7 (baseline)</span>
																		</p></div>
																	</div>
															</div>
														</div>
													</div>
										</div>
										<div>
											<h3>M4 compared to a PC laptop with Intel&nbsp;Core&nbsp;Ultra&nbsp;7.</h3>
											<div id="macbook-air-m4-pc-compare" aria-label="macbook air m4 vs pc i7 compare gallery" data-component-list="PerformanceGallery" role="group" data-kfstart="t - 85vh" data-analytics-gallery-id="macbook air m4 vs pc i7 compare gallery">
												<div data-analytics-gallery-interaction-type="tabnav">
														<ul role="tablist">
															<li role="presentation">
																<a role="tab" href="#compare-gallery-gallery-item-1-ai-image-upscaling" id="compare-gallery-gallery-item-1-trigger" data-ac-gallery-trigger="compare-gallery-gallery-item-1" tabindex="0">AI image upscaling</a>
															</li>
															
															<li role="presentation">
																<a role="tab" href="#compare-gallery-gallery-item-2-code-compiling-on-battery" id="compare-gallery-gallery-item-2-trigger" data-ac-gallery-trigger="compare-gallery-gallery-item-2" tabindex="-1">Code compiling on battery</a>
															</li>
															
															<li role="presentation">
																<a role="tab" href="#compare-gallery-gallery-item-3-photo-editing" id="compare-gallery-gallery-item-3-trigger" data-ac-gallery-trigger="compare-gallery-gallery-item-3" tabindex="-1">Photo editing</a>
															</li>
															
															<li role="presentation">
																<a role="tab" href="#compare-gallery-gallery-item-4-web-browsing" id="compare-gallery-gallery-item-4-trigger" data-ac-gallery-trigger="compare-gallery-gallery-item-4" tabindex="-1">Web browsing</a>
															</li>
															
															<li role="presentation">
																<a role="tab" href="#compare-gallery-gallery-item-5-video-editing" id="compare-gallery-gallery-item-5-trigger" data-ac-gallery-trigger="compare-gallery-gallery-item-5" tabindex="-1">Video editing</a>
															</li>
														</ul>
														
													</div>
												<div>
													<div id="compare-gallery-gallery-item-1" data-ac-gallery-item="" data-analytics-gallery-item-id="ai image upscaling" data-index="0">
														<p>Faster AI photo resolution enhancement<sup><a href="#footnote-20" aria-label="Footnote 17" data-modal-close="">17</a></sup></p>
														<div>
																
																<figure>
																	<div>
																		<p><span>6</span>
																			<span>x</span>
																		</p>
																		</div>
																</figure>
																<div>
																	
																	<p><span>PC laptop with Intel&nbsp;Core&nbsp;Ultra&nbsp;7 (baseline)</span>
																</p></div>
															</div>
													</div>
													<div id="compare-gallery-gallery-item-2" data-ac-gallery-item="" data-analytics-gallery-item-id="code compiling on battery" data-index="1">
														<p>More project builds per charge<sup><a href="#footnote-21" aria-label="Footnote 18" data-modal-close="">18</a></sup></p>
														<div>
																
																<figure>
																	<div>
																		<p><span>2</span>
																			<span>x</span>
																		</p>
																		</div>
																</figure>
																<div>
																	
																	<p><span>PC laptop with Intel&nbsp;Core&nbsp;Ultra&nbsp;7 (baseline)</span>
																</p></div>
															</div>
													</div>
													<div id="compare-gallery-gallery-item-3" data-ac-gallery-item="" data-analytics-gallery-item-id="photo editing" data-index="2">
														<p>Faster photo editing performance<sup><a href="#footnote-22" aria-label="Footnote 19" data-modal-close="">19</a></sup></p>
														<div>
																
																<figure>
																	<div>
																		<p><span>2.2</span>
																			<span>x</span>
																		</p>
																		</div>
																</figure>
																<div>
																	
																	<p><span>PC laptop with Intel&nbsp;Core&nbsp;Ultra&nbsp;7 (baseline)</span>
																</p></div>
															</div>
													</div>
													<div id="compare-gallery-gallery-item-4" data-ac-gallery-item="" data-analytics-gallery-item-id="web browsing" data-index="3">
														<p>Faster web application responsiveness<sup><a href="#footnote-23" aria-label="Footnote 20" data-modal-close="">20</a></sup></p>
														<div>
																
																<figure>
																	<div>
																		<p><span>1.6</span>
																			<span>x</span>
																		</p>
																		</div>
																</figure>
																<div>
																	
																	<p><span>PC laptop with Intel&nbsp;Core&nbsp;Ultra&nbsp;7 (baseline)</span>
																</p></div>
															</div>
													</div>
													<div id="compare-gallery-gallery-item-5" data-ac-gallery-item="" data-analytics-gallery-item-id="video editing" data-index="4">
														<p>Faster scene edit detection performance<sup><a href="#footnote-24" aria-label="Footnote 21" data-modal-close="">21</a></sup></p>
														<div>
																
																<figure>
																	<div>
																		<p><span>1.9</span>
																			<span>x</span>
																		</p>
																		</div>
																</figure>
																<div>
																	
																	<p><span>PC laptop with Intel&nbsp;Core&nbsp;Ultra&nbsp;7 (baseline)</span>
																</p></div>
															</div>
													</div>
												</div>
											</div>
										</div>
									</div>
					</div>
			</div>
		</article>
		<article aria-label="Apple-intelligence" data-anim-scroll-group="Section - Apple-intelligence" data-analytics-section-engagement="name:apple intelligence">
			
			<div>
				<p>
					<h2 data-component-list="WordAnim" data-anchor=".section-apple-intelligence" data-reverse="true" data-start="a0t-85vh" data-anim-type="slide" data-y="10px" data-ease="0.1" data-duration="0.4" data-ease-function="easeOutQuad"><strong>Apple Intelligence.</strong> <br> Do more. Effort&nbsp;less.</h2>
				</p>
				<p>Apple&nbsp;Intelligence is the personal intelligence system that helps you write, express yourself, and get things done effortlessly. With groundbreaking privacy protections, it gives you peace of mind that no one else can access your data ‚Äî not even&nbsp;Apple.<sup><a href="#footnote-5" aria-label="Footnote 2" data-modal-close="">2</a></sup></p>
				
				<div id="apple intelligence gallery" data-analytics-gallery-id="apple intelligence gallery" data-component-list="TabnavGallery">
					<div data-analytics-gallery-interaction-type="tabnav">
							<ul role="tablist" aria-label="Apple Intelligence features">
								<li role="presentation">
									<a href="#apple-intelligence-gallery-item-1" id="apple-intelligence-gallery-item-1-trigger" data-ac-gallery-trigger="apple-intelligence-gallery-item-1" data-analytics-gallery-item-id="writing tools">
				            							<span>Writing Tools</span>
				            						</a>
								</li>
								<li role="presentation">
									<a href="#apple-intelligence-gallery-item-2" id="apple-intelligence-gallery-item-2-trigger" data-ac-gallery-trigger="apple-intelligence-gallery-item-2" data-analytics-gallery-item-id="image playground">
				            							<span>Image Playground</span>
				            						</a>
								</li>
								<li role="presentation">
									<a href="#apple-intelligence-gallery-item-3" id="apple-intelligence-gallery-item-3-trigger" data-ac-gallery-trigger="apple-intelligence-gallery-item-3" data-analytics-gallery-item-id="siri">
				            							<span>Siri&nbsp;</span>
				            						</a>
								</li>
							</ul>
							
						</div>
					<div>
						
						<div id="apple-intelligence-gallery-item-2" data-analytics-gallery-item-id="image playground" data-ac-gallery-item="">
							
							<p>Create fun, original images based on a description, a concept, or even a person from your Photos&nbsp;library.</p>
						</div>
						<div id="apple-intelligence-gallery-item-3" data-analytics-gallery-item-id="siri" data-ac-gallery-item="">
							
							<p>Siri can help you like never before, drawing on your personal context by using the information on your device to help find what you‚Äôre looking for ‚Äî like a presentation file shared with you in an email weeks ago.<sup><a href="#footnote-25" aria-label="Footnote 22" data-modal-close="">22</a></sup> Siri can even tap into ChatGPT to bring answers right to you.&nbsp;No account needed.</p>
						</div>
					</div>
				</div>
				<div>
						<h3>Great&nbsp;powers come with great&nbsp;<strong>privacy.</strong></h3>
						<div>
							<p>Apple&nbsp;Intelligence is designed to protect your privacy at every step. It‚Äôs integrated into the core of your Mac through on-device processing. So it‚Äôs aware of your personal information without collecting your personal information.</p>
							<p>And with groundbreaking Private Cloud Compute, Apple&nbsp;Intelligence can draw on larger server-based models, running on Apple&nbsp;silicon, to handle more complex requests for you while protecting your&nbsp;privacy.</p>
						</div>
					</div>
			</div>
		</article>
		<article aria-label="Mac + iPhone" data-anim-scroll-group="Section - Mac-plus-iphone" data-analytics-section-engagement="name:mac + iphone">
			
			<div data-component-list="TileOverlay">
						<div>
							<picture id="overview-mac-plus-iphone-mac-iphone-1" data-lazy="" data-download-area-keyframe="{&quot;start&quot;: &quot;a0t - 200vh&quot;, &quot;end&quot;: &quot;a0b + 100vh&quot;, &quot;anchors&quot;:[&quot;.section-mac-plus-iphone&quot;]}">
								<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
								<img src="https://www.apple.com/v/macbook-air/t/images/overview/mac-plus-iphone/mac_iphone__n2863l0ne0q6_large.jpg" onload="__lp(event)" alt="Demonstrating iPhone Mirroring app, with iPhone screen shown on MacBook Air screen">
							</picture>
							
						</div>
						<div>
							<p><label tabindex="0" for="tile-overlay-toggle-mac-plus-iphone">
									<span data-anim-keyframe-1="{&quot;start&quot;:&quot;b - 90vh&quot;,&quot;cssClass&quot;:&quot;animate-in&quot;}">
										<span>
											<svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 39 39" style="enable-background:new 0 0 39 39;" xml:space="preserve">
												
												<path d="M19.5,9.8c0.6,0,1,0.4,1,1l0,7.7l7.7,0c0.6,0,1,0.4,1,1c0,0.6-0.4,1-1,1l-7.7,0l0,7.7c0,0.6-0.4,1-1,1
													c-0.6,0-1-0.4-1-1l0-7.7l-7.7,0c-0.6,0-1-0.5-1-1c0-0.6,0.4-1,1-1l7.7,0l0-7.7C18.5,10.2,19,9.8,19.5,9.8L19.5,9.8z"></path>
											</svg>
										</span>
										<span role="button" aria-expanded="false" aria-controls="content-toggle-mac-plus-iphone">
											<span>Learn more about Mac + iPhone</span>
										</span>
									</span>
								</label></p>
						</div>
					</div>
			<div>
				<p><em>If you love iPhone, you‚Äôll love Mac.</em> Mac is designed to be just as easy to learn as iPhone. Whether you‚Äôre using iPhone&nbsp;Mirroring to view and control your iPhone from your Mac,<sup><a href="#footnote-26" aria-label="Footnote 23" data-modal-close="">23</a></sup> copying text on your iPhone to paste on your Mac, or locating devices with Find&nbsp;My ‚Äî when you use Mac with iPhone, they work together like&nbsp;magic.</p>
			</div>
		</article>
		<article aria-label="Switch to Mac" data-anim-scroll-group="Section - Switchers" data-analytics-section-engagement="name:switch gallery">
			<div>
				<p>
					<h2>New to Mac?</h2>
				</p>
			</div>
			
		</article>
		<article aria-label="Macos" data-anim-scroll-group="Section - Macos" data-analytics-section-engagement="name:macos apps">
			
			
			<div>
				<p>With tens of thousands of apps optimized for Apple&nbsp;silicon, <em>all your favorites run lightning fast in macOS,</em> including Microsoft 365 Copilot, Adobe Creative Cloud, and Google Workspace with Gemini. And regular, free macOS software updates keep things running&nbsp;smoothly.</p>
			</div>
		</article>
		<article aria-label="Display" data-anim-scroll-group="Section - Display" data-analytics-section-engagement="name:display">
			<div data-component-list="WordAnim" data-anchor=".section-display" data-reverse="true" data-start="a0t-85vh" data-anim-type="fade" data-duration="0.5">
						<h2 data-component-list="WordAnim" data-anchor=".section-display" data-reverse="true" data-start="a0t-85vh" data-anim-type="slide" data-y="-4px" data-duration="0.5" data-ease="0.1" data-ease-function="easeOutQuad">Display</h2>
						<p data-component-list="WordAnim" data-anchor=".section-display" data-reverse="true" data-start="a0t-85vh" data-anim-type="slide" data-y="10px" data-ease="0.1" data-duration="0.4" data-ease-function="easeOutQuad">Love at every&nbsp;sight.</p>
					</div>
			
			<div>
				<p>The <em>Liquid Retina display</em> on MacBook&nbsp;Air supports <em>1 billion colors</em> and has up to <em>2x the resolution</em> of comparable PC laptops. Photos and videos pop with rich contrast and sharp detail, and text appears supercrisp.</p>
			</div>
		</article>
		<article aria-label="Camera and Audio" data-anim-scroll-group="Section - Camera-audio" data-analytics-section-engagement="name:camera and audio">
			<div>
				<div data-component-list="WordAnim" data-anchor=".section-camera-audio" data-reverse="true" data-start="a0t-85vh" data-anim-type="fade" data-duration="0.5">
						<h2 data-component-list="WordAnim" data-anchor=".section-camera-audio" data-reverse="true" data-start="a0t-85vh" data-anim-type="slide" data-y="-4px" data-duration="0.5" data-ease="0.1" data-ease-function="easeOutQuad">Camera and audio</h2>
						<p data-component-list="WordAnim" data-anchor=".section-camera-audio" data-reverse="true" data-start="a0t-85vh" data-anim-type="slide" data-y="10px" data-ease="0.1" data-duration="0.4" data-ease-function="easeOutQuad">When you&nbsp;move, it moves.</p>
					</div>
				<div id="camera and audio gallery" data-analytics-gallery-id="camera and audio gallery" data-component-list="TabnavGallery">
					<div data-analytics-gallery-interaction-type="tabnav">
							<ul role="tablist" aria-label="Camera and Audio features">
								<li role="presentation">
									<a href="#camera-audio-gallery-item-1" id="camera-audio-gallery-item-1-trigger" data-ac-gallery-trigger="camera-audio-gallery-item-1" data-analytics-gallery-item-id="center stage">
													<span>Center Stage</span>
												</a>
								</li>
								<li role="presentation">
									<a href="#camera-audio-gallery-item-2" id="camera-audio-gallery-item-2-trigger" data-ac-gallery-trigger="camera-audio-gallery-item-2" data-analytics-gallery-item-id="desk view">
													<span>Desk View</span>
												</a>
								</li>
							</ul>
							
						</div>
					<div>
						<div id="camera-audio-gallery-item-1" data-analytics-gallery-item-id="center stage" data-ac-gallery-item="">
							
							<p>Stay in frame during video calls, even as you move around or when more people join your&nbsp;frame.</p>
						</div>
						<div id="camera-audio-gallery-item-2" data-analytics-gallery-item-id="desk view" data-ac-gallery-item="">
							
							<p>Share a top-down view of your workspace while staying onscreen ‚Äî great if you‚Äôre tutoring online or showing off your latest&nbsp;project.</p>
						</div>
					</div>
				</div>
				<div>
					<div>
							<h3>Three-mic array.</h3>
							<p>MacBook&nbsp;Air provides enhanced voice clarity in audio and video calls while minimizing background noise.</p>
						</div>
					<div>
							<h3>Immersive sound system.</h3>
							<p>The speakers on MacBook&nbsp;Air support Spatial&nbsp;Audio along with Dolby Atmos, so you can enjoy a three-dimensional soundstage for music and&nbsp;movies.</p>
						</div>
				</div>
			</div>
		</article>
		<article aria-label="Connectivity" data-anim-scroll-group="Section - Connectivity" data-analytics-section-engagement="name:connectivity">
			<div data-component-list="WordAnim" data-anchor=".section-connectivity" data-reverse="true" data-start="a0t-85vh" data-anim-type="fade" data-duration="0.5">
						<h2 data-component-list="WordAnim" data-anchor=".section-connectivity" data-reverse="true" data-start="a0t-85vh" data-anim-type="slide" data-y="-4px" data-duration="0.5" data-ease="0.1" data-ease-function="easeOutQuad">Ports and Connectivity</h2>
						<p data-component-list="WordAnim" data-anchor=".section-connectivity" data-reverse="true" data-start="a0t-85vh" data-anim-type="slide" data-y="10px" data-ease="0.1" data-duration="0.4" data-ease-function="easeOutQuad">Become well connected.</p>
					</div>
			<div>
						<div data-component-list="HardwarePinAnimation" data-anim-direction="1">
							<div>
								<p><span>MagSafe</span>
								</p>
								<p><span></span>
									<span>Two Thunderbolt <br>4 ports</span>
								</p>
							</div>
							<picture id="overview-connectivity-connectivity-right-1" data-lazy="">
								<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
								<img src="https://www.apple.com/v/macbook-air/t/images/overview/connectivity/connectivity_right__fxge1xn29amq_large.jpg" onload="__lp(event)" alt="Left side of MacBook Air with MagSafe port and two Thunderbolt 4 ports">
							</picture>
							
						</div>
						<div data-component-list="HardwarePinAnimation" data-anim-direction="-1">
							
							<picture id="overview-connectivity-connectivity-left-1" data-lazy="">
								<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
								<img src="https://www.apple.com/v/macbook-air/t/images/overview/connectivity/connectivity_left__b8ntzos2h22u_large.jpg" onload="__lp(event)" alt="Right side of MacBook Air with headphone jack">
							</picture>
							
						</div>
					</div>
			<div>
				<p>The <em>MagSafe charging cable</em> attaches and detaches magnetically, preventing any unintended flights. Two <em>Thunderbolt&nbsp;4 ports</em> let you connect high-speed accessories and charge your Mac. The headphone jack supports high‚Äëimpedance headphones. And Wi‚ÄëFi&nbsp;6E provides up to 2x faster throughput compared with Wi‚ÄëFi&nbsp;6.<sup><a href="#footnote-28" aria-label="Footnote 25" data-modal-close="">25</a></sup></p>
			</div>
			<div>
				<picture id="overview-connectivity-connectivity-displays-1" data-lazy="">
					<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
					<img src="https://www.apple.com/v/macbook-air/t/images/overview/connectivity/connectivity_displays__er91a9b94oeq_large.jpg" onload="__lp(event)" alt="A person in an office environment using open MacBook Air with two external displays and Magic Keyboard, with iPhone in their other hand">
				</picture>
				
				<div>
					<picture id="overview-connectivity-connectivity-icon-1" data-lazy="">
						<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
						<img src="https://www.apple.com/v/macbook-air/t/images/overview/connectivity/connectivity_icon__e7h8wwhbf2aa_large.png" onload="__lp(event)" alt="">
					</picture>
					
					<hr role="presentation">
					<p>Turn MacBook Air into the perfect workspace by <em>connecting up to two external displays.</em> The extra screen space makes working with multiple documents or apps a&nbsp;breeze.</p>
				</div>
			</div>
		</article>
		<article aria-label="Security" data-anim-scroll-group="Section - Security" data-analytics-section-engagement="name:security">
			<div data-component-list="WordAnim" data-anchor=".section-security" data-reverse="true" data-start="a0t-85vh" data-anim-type="fade" data-duration="0.5">
						<h2 data-component-list="WordAnim" data-anchor=".section-security" data-reverse="true" data-start="a0t-85vh" data-anim-type="slide" data-y="-4px" data-duration="0.5" data-ease="0.1" data-ease-function="easeOutQuad">Security</h2>
						<p data-component-list="WordAnim" data-anchor=".section-security" data-reverse="true" data-start="a0t-85vh" data-anim-type="slide" data-y="10px" data-ease="0.1" data-duration="0.4" data-ease-function="easeOutQuad">Hold&nbsp;down the fort <br>with just&nbsp;a&nbsp;finger.</p>
					</div>
			
			<div>
				<p>The backlit Magic&nbsp;Keyboard comes with full-height function keys and <em>Touch&nbsp;ID, giving you a fast, easy, and secure</em> way to unlock your Mac, sign in to accounts, and authorize payments for purchases ‚Äî all at the touch of your&nbsp;finger.</p>
			</div>
		</article>
		<article aria-label="Upgraders" data-anim-scroll-group="Section - Upgraders" data-analytics-section-engagement="name:upgrade">
			<div>
				<h2>There‚Äôs never been a better time to upgrade.</h2>
				<div data-component-list="UpgradersGallery">
						<p><span>Select your current MacBook&nbsp;Air:</span></p>
					</div>
				
			</div>
		</article>
		<div>
					<div data-analytics-section-engagement="name:accessories">
							<div>
								<h2>Accessories</h2>
								<h3>Explore Mac accessories.</h3>
								<p>
									<a href="https://www.apple.com/us/shop/goto/mac/accessories" data-analytics-title="shop accessories" aria-label="Shop Mac accessories">Shop</a>
								</p>
							</div>
							<div>
								<picture id="overview-routers-accessories-1" data-lazy="">
									<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
									<img src="https://www.apple.com/v/macbook-air/t/images/overview/routers/accessories__fbw0ne5ig5iu_large.jpg" onload="__lp(event)" alt="Mac accessories: AirPods, Studio Display, Magic Keyboard, Magic Mouse, Magic Trackpad, MagSafe charge cable in Midnight color">
								</picture>
								
							</div>
						</div>
					
					<div data-analytics-section-engagement="name:environment">
								<picture id="overview-routers-icon-environment-1" data-lazy="">
									<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
									<img src="https://www.apple.com/v/macbook-air/t/images/overview/routers/icon_environment__tc0qvlmjpuqi_large.png" onload="__lp(event)" alt=" ">
								</picture>
								
								<h2>Designed with the <span>earth</span> in mind.</h2>
								<p>Recycled materials. A greener supply chain. Responsible packaging. MacBook&nbsp;Air was engineered to be as light on the environment as it is in your&nbsp;hands.</p>
								<p> <a href="https://www.apple.com/environment/" data-analytics-title="learn more about apple and the environment">
																				<span>Learn more about Apple and the&nbsp;environment</span><span></span>
																			</a>
								</p>
							</div>
					<div data-analytics-section-engagement="name:compare">
								<h2>Which laptop is right for&nbsp;you?</h2>
								
								<div>
									<picture id="overview-routers-compare-mba-13-15-1" data-lazy="">
										<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
										<img src="https://www.apple.com/v/macbook-air/t/images/overview/routers/compare_mba_13_15__caznvrb61zyu_large.png" onload="__lp(event)" alt="">
									</picture>
									
									<picture id="overview-routers-compare-4-swatches-1" data-lazy="">
										<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
										<img src="https://www.apple.com/v/macbook-air/t/images/overview/routers/compare_4_swatches__fne5w29xx56y_large.png" onload="__lp(event)" alt="Available in Sky Blue, Silver, Starlight, and Midnight">
									</picture>
									
									<h3>
										<span>New</span> MacBook&nbsp;Air<br> 13‚Äù&nbsp;and&nbsp;15‚Äù
										<span>M4 chip</span>
									</h3>
									<p data-pricing-hide="macbook-air"><span data-pricing-product="macbook-air" data-product-template="${price.display.from}"></span><span data-acmi="" data-pricing-product="macbook-air" data-product-template=" or ${price.display.perMonth} for ${price.display.months} mo." data-pricing-hide="macbook-air"></span><span data-acmi="" data-pricing-hide="macbook-air"><span><a href="#footnote-2" aria-label="Footnote ** symbol" data-modal-close="">**</a></span></span>
									</p>
									<p>13.6-inch or 15.3-inch Liquid&nbsp;Retina&nbsp;display<sup><a href="#footnote-31" aria-label="Footnote 28" data-modal-close="">28</a></sup></p>
									<p>Apple&nbsp;M4 chip</p>
									<p>Apple&nbsp;Intelligence<sup><a href="#footnote-5" aria-label="Footnote 2" data-modal-close="">2</a></sup></p>
									<p>16GB to 32GB&nbsp;<br>unified memory</p>
									<p>256GB to <br>2TB storage<sup><a href="#footnote-32" aria-label="Footnote 29" data-modal-close="">29</a></sup></p>
									<p>Up to <br>18 hours battery&nbsp;life<sup><a href="#footnote-4" aria-label="Footnote 1" data-modal-close="">1</a></sup></p>
									<p>Touch&nbsp;ID</p>
									
									<picture id="overview-routers-compare-mbp-14-16-1" data-lazy="">
										<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
										<img src="https://www.apple.com/v/macbook-air/t/images/overview/routers/compare_mbp_14_16__f7ovwhzitq6i_large.png" onload="__lp(event)" alt="">
									</picture>
									
									<picture id="overview-routers-compare-mbp-swatches-1" data-lazy="">
										<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
										<img src="https://www.apple.com/v/macbook-air/t/images/overview/routers/compare_mbp_swatches__wbcyasy3ria2_large.png" onload="__lp(event)" alt="Available in Space Black and Silver">
									</picture>
									
									<h3>
										MacBook&nbsp;Pro 14‚Äù&nbsp;and&nbsp;16‚Äù
										<span>M4, M4&nbsp;Pro, <br>or M4&nbsp;Max&nbsp;chip</span>
									</h3>
									<p data-pricing-hide="macbook-pro"><span data-pricing-product="macbook-pro" data-product-template="${price.display.from}"></span><span data-acmi="" data-pricing-product="macbook-pro" data-product-template=" or ${price.display.perMonth} for ${price.display.months} mo." data-pricing-hide="macbook-pro"></span><span data-acmi="" data-pricing-hide="macbook-pro"><span><a href="#footnote-2" aria-label="Footnote ** symbol" data-modal-close="">**</a></span></span>
									</p>
									<p>14.2-inch or 16.2-inch <br>Liquid&nbsp;Retina XDR display<sup><a href="#footnote-31" aria-label="Footnote 28" data-modal-close="">28</a></sup></p>
									<div><p>Apple&nbsp;M4, M4&nbsp;Pro,</p><p> or M4&nbsp;Max chip</p></div>
									<p>Apple&nbsp;Intelligence<sup><a href="#footnote-5" aria-label="Footnote 2" data-modal-close="">2</a></sup></p>
									<p>16GB to 128GB&nbsp;<br>unified memory</p>
									<p>512GB to <br>8TB storage<sup><a href="#footnote-32" aria-label="Footnote 29" data-modal-close="">29</a></sup></p>
									<p>Up to <br>24 hours battery&nbsp;life<sup><a href="#footnote-33" aria-label="Footnote 30" data-modal-close="">30</a></sup></p>
									<p>Touch&nbsp;ID</p>
									
									
								</div>
							</div>
					<div data-analytics-section-engagement="name:trade-in">
							<div>
								<h2>Apple Trade In</h2>
								<h3>Get credit toward a new MacBook&nbsp;Air.</h3>
								<p>Just trade in your eligible computer for credit or recycle it for free. It‚Äôs good for you and the&nbsp;planet.<sup><a href="#footnote-30" aria-label="Footnote 27" data-modal-close="">27</a></sup></p>
								<p> <a href="https://www.apple.com/shop/trade-in" data-analytics-title="learn more about trade in" aria-label="Learn more about Apple Trade In">
																				<span>Learn more</span><span></span>
																			</a>
								</p>
							</div>
							<div>
								<picture id="overview-routers-trade-in-1" data-lazy="">
									<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
									<img src="https://www.apple.com/v/macbook-air/t/images/overview/routers/trade_in__6u9w2o7115uu_large.jpg" onload="__lp(event)" alt="Front view of MacBook&nbsp;Air.">
								</picture>
								
							</div>
						</div>
					<div data-analytics-section-engagement="name:acmi">
							<div>
								<h2>Apple Card</h2>
								<h3>Get 3% Daily Cash back with Apple&nbsp;Card.</h3>
								<p>And pay over time, interest-free when you choose to check out with Apple&nbsp;Card Monthly&nbsp;Installments.<span><a href="#footnote-3" aria-label="Footnote ‚Ä† symbol" data-modal-close="">‚Ä†</a></span></p>
								<p> <a href="https://www.apple.com/apple-card/" data-analytics-title="learn more about apple card" aria-label="Learn more about Apple Card">
																				<span>Learn more</span><span></span>
																			</a>
								</p>
							</div>
							<div>
								<picture id="overview-routers-acmi-1" data-lazy="">
									<source data-empty="" srcset="data:image/gif;base64,R0lGODlhAQABAHAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" media="(min-width:0px)">
									<img src="https://www.apple.com/v/macbook-air/t/images/overview/routers/acmi__exv1gmpbb5m6_large.jpg" onload="__lp(event)" alt="One hand holding an iPhone with financial information on screen and other hand holding an Apple Card">
								</picture>
								
							</div>
						</div>
					
					
				</div>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple unveils new Mac Studio, the most powerful Mac ever, featuring M4 Max (216 pts)]]></title>
            <link>https://www.apple.com/newsroom/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/</link>
            <guid>43266474</guid>
            <pubDate>Wed, 05 Mar 2025 14:00:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/newsroom/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/">https://www.apple.com/newsroom/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/</a>, See on <a href="https://news.ycombinator.com/item?id=43266474">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-US" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
    
        




    
    
    
	
	

</nav>





<main id="main" role="main"> 




<span id="opens-in-new-window">opens in new window</span>
<section>
<article data-analytics-activitymap-region-id="article">






    
    
    









    





    <div>
        
		
        <div>
                    
                    
                        <span>PRESS RELEASE</span>
                    
                    
                        <span>March 5, 2025</span>
                    
                    
                </div>

        <div>
                
                
                
                    <h2>
                        
    
        Apple unveils new Mac&nbsp;Studio, the most powerful Mac ever, featuring M4&nbsp;Max and new M3&nbsp;Ultra
    

                    </h2>
                
            </div>

        <div>
                
                
                    With Thunderbolt 5, up to 512GB of unified memory, and an up to 16TB SSD, all in a compact design, the ultimate pro desktop delivers even more performance
                
            </div>

        
            
    
    
    
    
    

        

    </div>







    
    
    






  
    
    
    
    
      <figure aria-label="Media, A closeup view of the new Mac Studio. ">
        <div>
             
              
              <div>
                The new Mac Studio ‚Äî powered by M4 Max and M3 Ultra for groundbreaking performance and extensive connectivity ‚Äî is the ultimate pro desktop.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/article/Apple-Mac-Studio-front-250305.zip" download="" data-analytics-title="download image - Apple-Mac-Studio-front-250305_big" aria-label="Download media, A closeup view of the new Mac Studio. "></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div><strong><span>CUPERTINO, CALIFORNIA</span> </strong>Apple today announced the new <a href="https://www.apple.com/mac-studio/" target="_blank">Mac Studio</a>, the most powerful Mac ever made, featuring M4 Max and the new M3 Ultra chip. The ultimate pro desktop delivers groundbreaking pro performance, extensive connectivity now with Thunderbolt 5, and new capabilities in its compact and quiet design that can live right on a desk. Mac Studio can tackle the most intense workloads with its powerful CPU, Apple‚Äôs advanced graphics architecture, higher unified memory capacity, ultrafast SSD storage, and a faster and more efficient Neural Engine. It provides a big boost in performance compared to the previous generation, and a massive leap for users coming from older Macs.
</div>
                 
             
                 <div>Mac Studio is a powerhouse for AI, capable of running large language models (LLMs) with over 600 billion parameters entirely in memory, thanks to its advanced GPU and up to 512GB of unified memory with M3 Ultra ‚Äî the most ever in a personal computer. It‚Äôs also built for Apple Intelligence, the personal intelligence system that transforms how users work, communicate, and express themselves, while protecting their privacy. The new Mac Studio is available to pre-order today, with availability beginning March 12.
</div>
                 
             
                 <div>‚ÄúThe new Mac Studio is the most powerful Mac we‚Äôve ever made,‚Äù said John Ternus, Apple‚Äôs senior vice president of Hardware Engineering. ‚ÄúA complete game-changer for pros around the world ‚Äî powering both home and pro studios ‚Äî Mac Studio sits in a class of its own, offering a staggering amount of performance in a compact, quiet design that fits beautifully on your desk. With this new Mac Studio, we‚Äôre delivering even more extreme performance with M4 Max and M3 Ultra, support for half a terabyte of unified memory, up to 16TB of superfast storage, and Thunderbolt 5 connectivity. Mac Studio truly is the ultimate pro desktop.‚Äù
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, Two Apple silicon chips (M4 Max and M3 Ultra) side by side.">
        <div>
             
              
              <div>
                With the immensely powerful M4 Max and new M3&nbsp;Ultra, the new Mac Studio delivers unprecedented power for the most demanding professional workflows.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/article/Apple-Mac-Studio-M4-Max-and-M3-Ultra-chips-250305.zip" download="" data-analytics-title="download image - Apple-Mac-Studio-M4-Max-and-M3-Ultra-chips-250305_big" aria-label="Download media, Two Apple silicon chips (M4 Max and M3 Ultra) side by side."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Mac Studio with M4 Max: A Performance Juggernaut</strong>
</h2>
                 
             
                 <div>The new Mac Studio with M4 Max is the perfect choice for video editors, colorists, developers, engineers, photographers, creative pros, and other users who need to blaze through intensive workflows. It delivers phenomenal single-threaded CPU performance with the world‚Äôs fastest CPU core, along with outstanding multithreaded CPU performance for complex workloads. Featuring an up to 16-core CPU, an up to 40-core GPU, over half a terabyte per second of unified memory bandwidth, and a Neural Engine that is over 3x faster than M1 Max, Mac Studio with M4 Max can run on-device AI models incredibly fast. Mac Studio with M4 Max is up to 3.5x faster than Mac Studio with M1 Max, and is up to 6.1x faster than the most powerful Intel-based 27-inch iMac.<sup>1</sup>
</div>
                 
             
                 <div>The GPU in M4 Max also brings Apple‚Äôs advanced graphics architecture to Mac Studio for the first time, including dynamic caching, hardware-accelerated mesh shading, and a second-generation ray-tracing engine for more seamless content creation and gaming. Mac Studio with M4 Max starts at 36GB of unified memory, with support for up to 128GB, so users can do everything from sorting through thousands of images with speed and precision, to producing complex compositions with hundreds of tracks, plug-ins, and virtual instruments, all played in real time. And with the powerful Media Engine in M4 Max, which features two ProRes accelerators, Mac Studio performance is outstanding for videographers who can effortlessly work with multiple streams of 4K ProRes.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, Mac Studio next to a Studio Display showing an Adobe Substance project onscreen.">
        <div>
             
              
              <div>
                Whether users are rendering 3D animations or editing multicamera projects, Mac Studio rips through complex workflows.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/article/Apple-Mac-Studio-M4-Max-3D-software-250305.zip" download="" data-analytics-title="download image - Apple-Mac-Studio-M4-Max-3D-software-250305_big" aria-label="Download media, Mac Studio next to a Studio Display showing an Adobe Substance project onscreen."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div><strong>Mac Studio with M4 Max enables:</strong><sup>1</sup>
</div>
                 
             
                 <div><ul>
<li>Up to 1.6x faster image processing in Adobe Photoshop when compared to Mac Studio with M1 Max, and up to 2.9x faster when compared to the 27-inch iMac with Core i9.&nbsp;</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Up to 2.1x faster build performance when compiling code in Xcode when compared to Mac Studio with M1 Max, and up to 3.1x faster when compared to the 27-inch iMac with Core i9.&nbsp;</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Up to 1.2x faster ProRes transcode performance in Compressor when compared to Mac Studio with M1 Max, and up to 2.8x faster when compared to the 27-inch iMac with Core i9.&nbsp;</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Up to 1.6x faster video processing performance in Topaz Video AI when compared to Mac Studio with M1 Max, and up to 5x faster when compared to the 27-inch iMac with Core i9.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, Mac Studio next to a Studio Display showing an Adobe Photoshop project onscreen.">
        <div>
             
              
              <div>
                Mac Studio with M4 Max delivers faster filter and function performance in Adobe Photoshop.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/article/Apple-Mac-Studio-M4-Max-Adobe-Photoshop-250305.zip" download="" data-analytics-title="download image - Apple-Mac-Studio-M4-Max-Adobe-Photoshop-250305_big" aria-label="Download media, Mac Studio next to a Studio Display showing an Adobe Photoshop project onscreen."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Mac Studio with M3 Ultra: The Pinnacle of Pro Performance</strong>
</h2>
                 
             
                 <div>Mac Studio with M3 Ultra pushes demanding workflows to a whole new level. It delivers nearly 2x faster performance than M4 Max in workloads that take advantage of high CPU and GPU core counts, and massive amounts of unified memory.<sup>2</sup> Mac Studio with M3 Ultra is up to 2.6x faster than Mac Studio with M1 Ultra, and up to 6.4x faster than the 16-core Intel Xeon W-based Mac Pro.<sup>1</sup> With the new M3 Ultra, Mac Studio features an up to 32-core CPU with 24 performance cores, 50 percent more than any previous Ultra chip and the most CPU cores ever in a Mac. It also offers an up to 80-core GPU, more than any Apple silicon chip; a powerful 32-core Neural Engine for on-device AI and machine learning (ML); and a high-bandwidth memory architecture that delivers over 800GB/s of unified memory bandwidth.
</div>
                 
             
                 <div>Mac Studio with M3 Ultra starts with 96GB of unified memory, which can be configured up to 512GB ‚Äî the most unified memory ever in a personal computer ‚Äî and up to 16TB of ultrafast SSD storage, so content and data can be kept locally. That‚Äôs enough storage for over 12 hours of 8K ProRes video. The advanced graphics architecture brings Dynamic Caching, along with hardware-accelerated mesh shading and ray tracing, so graphics workflows like GPU-based renderers are up to 2.6x faster than Mac Studio with M1 Ultra.
</div>
                 
             
                 <div><strong>Mac Studio with M3 Ultra enables:</strong><sup>1</sup>
</div>
                 
             
                 <div><ul>
<li>Up to 16.9x faster token generation using an LLM with hundreds of billions of parameters in LM Studio when compared to Mac Studio with M1 Ultra, thanks to its massive amounts of unified memory.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Up to 2.6x faster scene rendering performance in Maxon Redshift when compared to Mac Studio with M1 Ultra, and up to 6.4x faster when compared to the 16-core Intel-based Mac Pro with Radeon Pro W5700X.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Up to 1.1x faster basecalling for DNA sequencing in Oxford Nanopore MinKNOW when compared to Mac Studio with M1 Ultra, and up to 21.1x faster when compared to the 16-core Intel-based Mac Pro with Radeon Pro W5700X.&nbsp;</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Up to 1.4x faster 8K video rendering performance in Final Cut Pro when compared to Mac Studio with M1 Ultra, and up to 4x faster when compared to the 16-core Intel-based Mac Pro with Radeon Pro W5700X.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>Mac Studio with M3 Ultra unleashes unprecedented performance for the most extreme workflows. And with up to half a terabyte of unified memory, users can run LLMs with hundreds of billions of parameters entirely in memory.</div>
        
            <a aria-label="Download video: Mac Studio with M3 Ultra" data-analytics-title="Download video - Mac Studio with M3 Ultra" download="" href="https://www.apple.com/newsroom/videos/2025/autoplay/03/apple-mac-studio-m3-ultra-davinci-resolve/downloads/Apple-Mac-Studio-M3-Ultra-DaVinci-Resolve-250305.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Thunderbolt 5 for High-Bandwidth Accessories and Expansion</strong>
</h2>
                 
             
                 <div>The new Mac Studio features Thunderbolt 5 ports that deliver transfer speeds up to 120 Gb/s, up to 3x faster than the prior generation, enabling faster external storage, expansion chassis, and powerful hub solutions. For those who rely on PCIe expansion cards for their workflows, Thunderbolt 5 allows users to connect an external expansion chassis with higher bandwidth and lower latency. And with M3 Ultra, Mac Studio now drives up to eight Pro Display XDRs at the full 6K resolution. Mac Studio also offers a wide array of connectivity within easy reach for pros, including a 10Gb Ethernet port, an HDMI port, an SDXC card slot on the front to conveniently import photos and video, along with built-in Wi-Fi and Bluetooth.
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="mac-studio-design">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-66356a0d081a3a5373bffd3fb26adfc4" href="#gallery-66356a0d081a3a5373bffd3fb26adfc4" data-ac-gallery-trigger="gallery-66356a0d081a3a5373bffd3fb26adfc4"><span>The back view of Mac Studio showing all of its ports.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-faca586b79c8393ffbf174004d939abe" href="#gallery-faca586b79c8393ffbf174004d939abe" data-ac-gallery-trigger="gallery-faca586b79c8393ffbf174004d939abe"><span>A professional working at their desk with multiple displays connected to Mac Studio.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-66356a0d081a3a5373bffd3fb26adfc4" aria-labelledby="gallery-dotnav-66356a0d081a3a5373bffd3fb26adfc4" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:ports">
                                
                                <div>
                                    <div>Mac Studio features Thunderbolt 5 ports with transfer rates up to 120Gb/s ‚Äî 3x faster than Thunderbolt&nbsp;4 ‚Äî and supports built-in Wi-Fi and bluetooth connectivity.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/article/Apple-Mac-Studio-back-250305.zip" download="" data-analytics-title="download image - Apple-Mac-Studio-back-250305_big" aria-label="Download media, The back view of Mac Studio showing all of its ports."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-faca586b79c8393ffbf174004d939abe" aria-labelledby="gallery-dotnav-faca586b79c8393ffbf174004d939abe" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:displays">
                                
                                <div>
                                    <div>With the advanced connectivity of Mac Studio, users can drive external displays, directly offload SDXC cards, connect high-speed peripherals, or use an external chassis for PCIe expansion to build the studio of their dreams.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/article/Apple-Mac-Studio-lifestyle-250305.zip" download="" data-analytics-title="download image - Apple-Mac-Studio-lifestyle-250305_big" aria-label="Download media, A professional working at their desk with multiple displays connected to Mac Studio."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Built for Apple Intelligence</strong>
</h2>
                 
             
                 <div>Mac Studio helps pros push the boundaries of what they can do, and Apple Intelligence elevates those experiences even further. Writing is even more dynamic with Writing Tools, which can help users rewrite, proofread, or summarize ‚Äî whether they are responding to emails or using summarization to draft an abstract in seconds in apps like Scrivener. Pros can minimize unnecessary distractions with Priority Notifications and use live transcription in Notes to record and easily recap important meetings. With new Siri improvements, users can move fluidly between spoken and typed requests to accelerate tasks throughout their day, and Siri can answer thousands of questions about Mac features and settings, with step-by-step instructions like how to combine PDF files in Preview. With access to ChatGPT seamlessly integrated into Writing Tools and Siri, users can tap into ChatGPT‚Äôs expertise,&nbsp;so they can get things done even faster and easier. Users can choose to enable ChatGPT integration, and are in full control of when to use it and what information is shared with ChatGPT. Users can also explore creative new ways to express themselves visually with Image Playground, and drop their original image right into their paper, mood board, or Keynote presentation. Whether users are researching their next project, editing a video, creating new designs, or preparing for their next lecture, these new tools will help pros be even more productive.
</div>
                 
             
                 <div>Designed to protect users‚Äô privacy at every step, Apple Intelligence uses on-device processing, meaning that many of the models that power it run entirely on device. For requests that require access to larger models, Private Cloud Compute extends the privacy and security of Mac into the cloud to unlock even more intelligence. When using Private Cloud Compute, users‚Äô data is never stored or shared with Apple; it is used only to fulfill their request.
</div>
                 
             
                 <h2><strong>macOS Sequoia: An Unrivaled Experience</strong>
</h2>
                 
             
                 <div><a href="https://www.apple.com/macos/macos-sequoia/" target="_blank">macOS Sequoia</a> completes the new Mac Studio experience with a host of exciting features, including iPhone Mirroring, which allows users to wirelessly interact with their iPhone, its apps, and notifications directly from their Mac.<sup>3</sup> Pros can now move files, photos, and videos between iPhone and Mac as easily as they can drag and drop between apps on Mac. Easier window tiling means users can stay organized with a window layout that works best for them. The all-new Passwords app gives convenient access to passwords, passkeys, and other credentials, all stored in one place. And users can apply beautiful built-in backgrounds for video calls, which include a variety of color gradients and system wallpapers, or upload their own photos. Safari, the world‚Äôs fastest browser,<sup>4</sup> now surfaces relevant information on sites in Highlights; summarizes articles in the redesigned Reader; keeps videos front and center in a new Video Viewer; and lets users hide distracting items with Distraction Control. Gaming gets even more immersive with features like Personalized Spatial Audio and improvements to Game Mode, along with a breadth of exciting titles, including Cyberpunk 2077: Ultimate Edition by CD PROJEKT RED, Assassin‚Äôs Creed Shadows, and more.
</div>
                 
             
                 <div>Next month, macOS Sequoia 15.4 will make it easier than ever to set up the new Mac Studio with iPhone.<sup>5</sup> By simply bringing iPhone close to Mac, users can quickly and conveniently sign in to their Apple Account to get their files, photos, messages, passwords, and more on their new Mac Studio.
</div>
                 
             
                 <h2><strong>The Ultimate Studio Setup</strong>
</h2>
                 
             
                 <div>Mac Studio, together with Studio Display, empowers creative users to build the studio of their dreams. Studio Display perfectly pairs with Mac Studio with its expansive 27-inch 5K Retina display, 12MP Center Stage camera, studio-quality three-mic array, and six-speaker sound system with Spatial Audio. For users working on HDR workflows, Pro Display XDR offers a 32-inch Retina 6K display with up to 1600 nits of peak HDR brightness. Customers can also add matching Magic accessories ‚Äî including Magic Keyboard with Touch ID, Magic Trackpad, and Magic Mouse ‚Äî that beautifully complement the elegant design of Mac Studio and Studio Display.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, The 27-inch 5K Retina Studio Display with Mac Studio. ">
        <div>
             
              
              <div>
                Mac Studio was designed to pair beautifully with the expansive 27-inch 5K Retina Studio Display, making it the ultimate studio setup.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/article/Apple-Mac-Studio-Studio-Display-setup-250305.zip" download="" data-analytics-title="download image - Apple-Mac-Studio-Studio-Display-setup-250305_big" aria-label="Download media, The 27-inch 5K Retina Studio Display with Mac Studio. "></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Better for the Environment</strong>
</h2>
                 
             
                 <div>The new Mac Studio is designed with the environment in mind. As part of Apple 2030, the company‚Äôs ambitious goal to be carbon neutral across its entire carbon footprint by the end of this decade, Apple is transitioning to renewable electricity for its manufacturing, and investing in wind and solar projects around the world to address the electricity used to power all Apple products, including Mac Studio. Today, all Apple facilities run on 100 percent renewable electricity ‚Äî including the data centers that power Apple Intelligence.
</div>
                 
             
                 <div>To achieve Apple 2030, the company is designing products with more recycled and renewable materials, which further drives down the carbon footprint. Mac Studio features over 30 percent recycled content overall, including 100 percent recycled aluminum in the enclosure and 100 percent recycled rare earth elements in all magnets. Mac Studio uses far less energy and materials than desktops in its class, and is free of mercury, brominated flame retardants, and PVC. The packaging is entirely fiber-based, bringing Apple closer to its goal to remove plastic from all packaging by the end of 2025.<sup>6</sup>
</div>
                 
             
         </div>
 

    
    
    


     
     
    
    
        <div>
             
                 
                 
             
                 <div><ul>
<li>Customers can pre-order the new Mac Studio starting today on&nbsp;<a target="_blank" href="https://www.apple.com/store/">apple.com/store</a> and in the Apple Store app in 28 countries and regions, including the U.S. It will begin arriving to customers, and will be in Apple Store locations and Apple Authorized Resellers, starting Wednesday, March 12.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Mac Studio starts at <strong>$1,999</strong> (U.S.) and <strong>$1,799</strong> (U.S.) for education. Additional configure-to-order options are available at&nbsp;<a href="https://www.apple.com/store/" target="_blank">apple.com/store</a>.&nbsp;</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>More information on Studio Display, Pro Display XDR, and Magic accessories is available at <a href="https://www.apple.com/shop/buy-mac/" target="_blank">apple.com/shop/buy-mac</a>.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Apple Intelligence is available on all Mac models with M1 and later, in localized English for <em>Australia</em>, <em>Canada</em>, <em>Ireland</em>, <em>New Zealand</em>, <em>South Africa</em>, the <em>UK</em>, and the <em>U.S.</em> Additional languages ‚Äî including French, German, Italian, Portuguese (Brazil), Spanish, Japanese, Korean, Chinese (simplified), English (Singapore), and English (India) ‚Äî will be available in April, with more languages coming over the course of the year, including Vietnamese. Some features, applications, and services may not be available in all regions or all languages.&nbsp;</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>With Apple Trade In, customers can trade in their current computer and get credit toward a new Mac. Customers can visit <a href="https://www.apple.com/shop/trade-in/" target="_blank">apple.com/shop/trade-in</a> to see what their device is worth.&nbsp;</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>AppleCare+ for Mac provides unparalleled service and support. This includes unlimited incidents of accidental damage, battery service coverage, and 24/7 support from the people who know Mac best.&nbsp;</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Every customer who buys directly from Apple Retail gets access to Personal Setup. In these guided online sessions, a Specialist can walk them through setup, or focus on features that help them make the most of their new device. Customers can also learn more about getting started with their new device with a Today at Apple session at their nearest Apple Store.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Customers in the U.S. who shop at Apple using Apple Card can pay monthly at 0 percent APR when they choose to check out with Apple Card Monthly Installments, and they‚Äôll get 3 percent Daily Cash back ‚Äî all up front. More information ‚Äî including details on eligibility, exclusions, and Apple Card terms ‚Äî is available at <a href="https://www.apple.com/apple-card/monthly-installments/" target="_blank">apple.com/apple-card/monthly-installments</a>.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    




    
    
        
    


    
    
    



    
    
    




    




    
    
    





    
    
    <div>
            <ol>
<li>Testing was conducted by Apple in January and February 2025. See <a href="https://www.apple.com/mac-studio/" target="_blank">apple.com/mac-studio</a> for more information.</li>
<li>Results are compared to Mac Studio systems with Apple M4 Max, 16-core CPU, 40-core GPU, 128GB of RAM, and 8TB SSD.</li>
<li>Available on Mac computers with Apple&nbsp;silicon and Intel-based Mac computers with a T2 Security Chip. See requirements on <a href="https://www.apple.com/macos/macos-sequoia/" target="_blank">apple.com/macos/macos-sequoia</a>. Some iPhone features (for example, camera and microphone) are not compatible with iPhone Mirroring.</li>
<li>Testing was conducted by Apple in August 2024. See <a href="https://www.apple.com/safari/" target="_blank">apple.com/safari</a> for more information.</li>
<li>Available next month on macOS Sequoia 15.4 with iPhone and iPad running iOS 18.4, iPadOS 18.4, or a later version.</li>
<li>Based on retail packaging as shipped by Apple. Breakdown of U.S. retail packaging by weight. Adhesives, inks, and coatings are excluded from calculations of plastic content and packaging weight.</li>
</ol>

        </div>



    
    
    






    

















		
		
			
























		
		

</article>



</section>
</main>


	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple M3 Ultra (574 pts)]]></title>
            <link>https://www.apple.com/newsroom/2025/03/apple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme/</link>
            <guid>43266453</guid>
            <pubDate>Wed, 05 Mar 2025 13:59:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/newsroom/2025/03/apple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme/">https://www.apple.com/newsroom/2025/03/apple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme/</a>, See on <a href="https://news.ycombinator.com/item?id=43266453">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-US" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
    
        




    
    
    
	
	

</nav>





<main id="main" role="main"> 




<span id="opens-in-new-window">opens in new window</span>
<section>
<article data-analytics-activitymap-region-id="article">






    
    
    









    





    <div>
        
		
        <div>
                    
                    
                        <span>PRESS RELEASE</span>
                    
                    
                        <span>March 5, 2025</span>
                    
                    
                </div>

        <div>
                
                
                
                    <h2>
                        
    
        Apple reveals M3&nbsp;Ultra, <p>taking Apple silicon to a new extreme
    

                    </p></h2>
                
            </div>

        <div>
                
                
                    The new chip delivers up to 2.6x the performance of M1 Ultra, along with Thunderbolt 5 connectivity and support for more than half a terabyte of unified memory ‚Äî the most ever in a personal computer
                
            </div>

        
            
    
    
    
    
    

        

    </div>







    
    
    






  
    
    
    
    
      <figure aria-label="Media, Artwork representing the M3 Ultra chip.">
        <div>
             
              
              <div>
                M3 Ultra features a 32-core CPU, an 80-core GPU, double the Neural Engine cores, Thunderbolt 5, and support for the most unified memory ever in a personal computer.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2025/03/apple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme/article/Apple-M3-Ultra-hero-250305.zip" download="" data-analytics-title="download image - Apple-M3-Ultra-hero-250305_big" aria-label="Download media, Artwork representing the M3 Ultra chip."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div><span>CUPERTINO, CALIFORNIA</span> Apple today announced M3 Ultra, the highest-performing chip it has ever created, offering the most powerful CPU and GPU in a Mac, double the Neural Engine cores, and the most unified memory ever in a personal computer. M3 Ultra also features Thunderbolt 5 with more than 2x the bandwidth per port for faster connectivity and robust expansion. M3 Ultra is built using Apple‚Äôs innovative UltraFusion packaging architecture, which links two M3 Max dies over 10,000 high-speed connections that offer low latency and high bandwidth. This allows the system to treat the combined dies as a single, unified chip for massive performance while maintaining Apple‚Äôs industry-leading power efficiency. UltraFusion brings together a total of 184 billion transistors to take the industry-leading capabilities of the new <a href="https://www.apple.com/newsroom/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/" target="_blank">Mac Studio</a> to new heights.
</div>
                 
             
                 <div>‚ÄúM3 Ultra is the pinnacle of our scalable system-on-a-chip architecture, aimed specifically at users who run the most heavily threaded and bandwidth-intensive applications,‚Äù said Johny Srouji, Apple‚Äôs senior vice president of Hardware Technologies. ‚ÄúThanks to its 32-core CPU, massive GPU, support for the most unified memory ever in a personal computer, Thunderbolt 5 connectivity, and industry-leading power efficiency, there‚Äôs no other chip like M3 Ultra.‚Äù
</div>
                 
             
                 <h2><strong>Ultra Performance and Efficiency</strong>
</h2>
                 
             
                 <div>M3 Ultra provides the most performance of any Mac chip, while maintaining the industry-leading power-efficiency of Apple silicon. It features up to a 32-core CPU with 24 performance cores and eight efficiency cores, delivering up to 1.5x the performance of M2 Ultra, and up to 1.8x that of M1 Ultra. It also has the largest GPU in any Apple chip, with up to 80 graphics cores that bring up to 2x faster performance than M2 Ultra, and up to 2.6x faster than M1 Ultra.<sup>1</sup>
</div>
                 
             
                 <div>The advanced graphics architecture in M3 Ultra features dynamic caching, along with hardware-accelerated mesh shading and ray tracing, so it can fly through the most demanding content creation workloads and games. A powerful 32-core Neural Engine fuels AI and machine learning (ML), and powers Apple Intelligence, the personal intelligence system that puts powerful generative models right at the core of the new Mac Studio. In fact, M3 Ultra is built for AI, including ML accelerators in the CPU, Apple‚Äôs most powerful GPU, the Neural Engine, and over 800GB/s of memory bandwidth. AI professionals can use Mac Studio with M3 Ultra to run large language models (LLMs) with over 600 billion parameters directly on device, making it the ultimate desktop for AI development.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, In an office-like setting, a creative works on two Mac Studio Display devices with Mac Studio.">
        <div>
             
              
              <div>
                M3 Ultra delivers the ultimate performance and capabilities of Apple silicon for users running the most extreme pro workflows.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2025/03/apple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme/article/Apple-M3-Ultra-lifestyle-performance-250305.zip" download="" data-analytics-title="download image - Apple-M3-Ultra-lifestyle-performance-250305_big" aria-label="Download media, In an office-like setting, a creative works on two Mac Studio Display devices with Mac Studio."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Unparalleled Memory</strong>
</h2>
                 
             
                 <div>The unified memory architecture of M3 Ultra integrates the most high-bandwidth, low-latency memory ever available in a personal computer. Starting at 96GB, it can be configured up to 512GB, or over half a terabyte. This outpaces the memory available in today‚Äôs most advanced workstation graphics cards, removing limitations for pro workloads that demand large amounts of graphics memory like 3D rendering, visual effects, and AI.
</div>
                 
             
                 <h2><strong>Thunderbolt 5 for Next-Generation Connectivity</strong>
</h2>
                 
             
                 <div>M3 Ultra brings Thunderbolt 5 to Mac Studio for up to 120 Gb/s data transfer speeds ‚Äî more than double that of Thunderbolt 4. Each Thunderbolt 5 port is supported by its own custom-designed controller directly on the chip. This provides dedicated bandwidth for each port on Mac Studio, making it the industry‚Äôs most capable implementation of Thunderbolt 5.
</div>
                 
             
                 <div>Thunderbolt 5 ports on Mac Studio are a game changer for pro users who require faster data transfer speeds for external storage, docking, and hub solutions, and want to be ready for the next generation of expansion chassis. Thunderbolt 5 also enables connecting multiple Mac Studio systems together for workflows that push the limits of content creation and computer science exploration.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, A close-up of the back of Mac Studio, showing all of its ports.">
        <div>
             
              
              <div>
                Each Thunderbolt 5 port on Mac Studio is supported by its own custom-designed controller directly on M3 Ultra.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2025/03/apple-reveals-m3-ultra-taking-apple-silicon-to-a-new-extreme/article/Apple-Mac-Studio-back-250305.zip" download="" data-analytics-title="download image - Apple-Mac-Studio-back-250305_big" aria-label="Download media, A close-up of the back of Mac Studio, showing all of its ports."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Cutting-Edge Technologies Built In</strong>
</h2>
                 
             
                 <div>In the pursuit of maximizing performance and efficiency, M3 Ultra integrates Apple‚Äôs advanced technologies right on the chip:
</div>
                 
             
                 <div><ul>
<li>Apple‚Äôs custom-built <strong>UltraFusion</strong> packaging technology uses an embedded silicon interposer that connects two M3 Max dies across more than 10,000 signals, providing over 2.5TB/s of low-latency interprocessor bandwidth, and making M3 Ultra appear as a single chip to software.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>With 2x the resources of M3 Max, the <strong>media engine</strong> within M3 Ultra is capable of far more concurrent video processing. The chip offers dedicated, hardware-enabled H.264, HEVC, and four ProRes encode and decode engines, allowing M3 Ultra to play back up to 22 streams of 8K ProRes 422 video.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>The <strong>display engine</strong> supports up to eight Pro Display XDRs, driving more than 160 million pixels.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>The <strong>Secure Enclave</strong> works with hardware-verified secure boot and runtime anti-exploitation technologies to provide state-of-the-art security.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>M3 Ultra integrates Apple‚Äôs advanced technologies, including a media engine that offers dedicated, hardware-enabled H.264, HEVC, and four ProRes encode and decode engines.</div>
        
            <a aria-label="Download video: DaVinci Resolve on Mac Studio and Mac Studio Display" data-analytics-title="Download video - DaVinci Resolve on Mac Studio and Mac Studio Display" download="" href="https://www.apple.com/newsroom/videos/videos-2024/autoplay/2025/03/apple-mac-studio-m3-ultra-davinci-resolve/downloads/Apple-Mac-Studio-M3-Ultra-DaVinci-Resolve-250305.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Better for the Environment</strong>
</h2>
                 
             
                 <div>The power-efficient performance of M3 Ultra helps the new Mac Studio meet Apple‚Äôs high standards for energy efficiency and reduces the total amount of energy consumed over the product‚Äôs lifetime. Today, Apple is carbon neutral for global corporate operations and, as part of its ambitious Apple 2030 goal, plans to be carbon neutral across its entire carbon footprint by the end of this decade.
</div>
                 
             
         </div>
 

    
    
    




    
    
        
    


    
    
    



    
    
    




    




    
    
    





    
    
    <div>
            <ol>
<li>Results are compared to previous-generation Mac Studio systems with Apple M1 Ultra, 20-core CPU, 64-core GPU, and 128GB of RAM; and Mac Studio systems with Apple M2 Ultra, 24-core CPU, 76-core GPU, and 192GB of RAM.</li>
</ol>

        </div>



    
    
    






    

















		
		
			
























		
		

</article>



</section>
</main>


	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Who's Afraid of Peter Thiel? A New Biography Suggests We All Should Be (2021) (125 pts)]]></title>
            <link>https://time.com/6092844/peter-thiel-power-biography-the-contrarian/</link>
            <guid>43265955</guid>
            <pubDate>Wed, 05 Mar 2025 13:08:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://time.com/6092844/peter-thiel-power-biography-the-contrarian/">https://time.com/6092844/peter-thiel-power-biography-the-contrarian/</a>, See on <a href="https://news.ycombinator.com/item?id=43265955">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body-main"><p><span>P</span>aypal cofounder Peter Thiel is famous for <a href="https://time.com/4348840/peter-thiel-gawker-lawsuit-hulk-hogan/">destroying media outlets</a>, <a href="https://www.propublica.org/article/lord-of-the-roths-how-tech-mogul-peter-thiel-turned-a-retirement-account-for-the-middle-class-into-a-5-billion-dollar-tax-free-piggy-bank#:~:text=During%20the%20life%20of%20his,on%20the%20early%20Roth%20withdrawals." target="_blank" rel="noopener">not paying taxes</a>, and being a <a href="https://time.com/4416093/peter-thiel-donald-trump/" target="_blank" rel="noopener">conservative tech billionaire</a>. A new biography, <em>The Contrarian</em>, suggests that he is after more than riches. TIME chatted with its author, journalist Max Chafkin.</p><p><strong>Why should we care about Peter Thiel, apart from the fact that he is another rich tech billionaire and they‚Äôre all weirdly fascinating?</strong><br> I think that Peter Thiel is secretly the most important person in Silicon Valley. He‚Äôs this behind the scenes player, who is behind so many of the really important things that have happened over the last two decades. Obviously Facebook is one of the world‚Äôs largest companies; a lot of people think it‚Äôs uniquely bad for the world. And a lot of people are super skeptical of Mark Zuckerberg, Facebook‚Äôs CEO. And of course, Thiel is behind Facebook. He was the first outside money in the company. He is also the person who basically set up Mark Zuckerberg to be Mark Zuckerberg and turned him into this imperial CEO, who is now, arguably more powerful than a lot of world leaders.<br> A lot of people are really excited about cryptocurrency and you can connect it back to to PayPal, which is the company that Thiel co-founded in the late 1990s with an explicitly libertarian ethos. There‚Äôs this aspect of crypto-world now, where people are really excited about the idea of taking power away from institutions and governments and that‚Äôs something that Thiel and his libertarian brethren that were starting that company were really interested in. It‚Äôs not something that happens accidentally.</p><p><strong>Read More:</strong> <em><a href="https://time.com/6099105/us-china-digital-currency-central-bank/">The U.S. Is Losing the Global Race to Decide the Future of Money‚Äîand It Could Doom the Almighty Dollar</a></em></p><p><strong>Do you see Thiel as dangerous?</strong><br> It‚Äôs really important that we understand the ideology of Silicon Valley. Five of the top 10 companies in the world are tech companies. They exert an enormous cultural and economic influence over our lives. Those companies have been really successful at telling a story about the world and their place in the world: we‚Äôre just trying to make the world a better place. Thiel comes with a very different perspective. He comes out of activist conservative media. I think it‚Äôs really important that we explore the ideology of what somebody like Peter Thiel believes. When you start peeling back the layers, what you find is this very out-there political and economic philosophy that I think is a little bit scary.</p><p><strong>What do you find scary about his economic and political philosophy?</strong><br> It‚Äôs bordering on fascism. Thiel taught this class at Stanford and then turned it into a book called <em>Zero to One</em>. He talks about how companies are better run than governments because they have a single decision maker‚Äîa dictator, basically. He is hostile to the idea of democracy. That‚Äôs pretty scary when you consider the role the companies that he‚Äôs been involved in play. Facebook, I‚Äôd say is the most influential media entity in the history of humanity, but he also has a major stake in several defense contractors, including SpaceX.</p><p><strong>He would explain it as belief in efficiency and results, right? He would not say, ‚ÄòI don‚Äôt think everybody has the right to vote.‚Äô</strong><br> There would be some rationale and in fact, at various times he‚Äôs walked back things he said. His whole thing is being slippery, but I think when you look at the body of what he‚Äôs done and the things he‚Äôs been involved with, that‚Äôs the picture that emerges.</p><p><strong>A lot of people were very surprised that this nerdy, gay, Californian son of immigrants techpreneur decided to support Donald Trump‚Äôs presidency. Was it just the pure <em>I‚Äôm going to shut down government</em> aspect of Trump‚Äôs policies that he liked?</strong><br> If you look at <a href="https://time.com/4417679/republican-convention-peter-thiel-transcript/" target="_blank" rel="noopener">his convention speech</a>‚Äîwhich I think was a really good speech‚Äîhe talks about Trump as this guy who‚Äôs gonna shake things up, who‚Äôs going to remake government, so I think that‚Äôs one part of it. The other part of it is, Thiel is very committed to the idea of being able to say the unpopular thing. That‚Äôs a core part of what Trump was. I also think that Thiel is a really savvy investor and he correctly diagnosed that Trump had a pretty good chance of winning and that there wasn‚Äôt a lot of downside to betting on him.<br> He didn‚Äôt formally endorse Trump during the 2020 election, but if you look at the candidates he‚Äôs [now] supporting, they‚Äôre Trumpers: J.D. Vance, who‚Äôs running for Senate in Ohio was an employee of Peter Thiel‚Äôs and an investor in his fund. Before announcing that he was gonna run for Senate, <a href="https://time.com/6078483/j-d-vance-interview-trump/">Vance said that he was wrong in 2016 to oppose Trump</a>, and around the same time, he got this $10 million dollar donation from Thiel to his super PAC. Another candidate that Thiel is supporting in the coming cycle is Blake Masters, who is <em>literally</em> an employee. He runs Thiel‚Äôs foundation.</p><p><strong>Read More:</strong> <em>H<a href="https://time.com/4418475/republican-convention-peter-thiel-lgbt-gay-rights/">ow Donald Trump Courted Gay Voters at the Convention</a></em></p><p><strong>Did Thiel get blowback about the January 6th attack on Congress?</strong><br> I think certainly his reputation in certain corners of the establishment has suffered. But I don‚Äôt think he really cares about the blowback; he seems to really take pleasure in that.</p><p><strong>Do you have a guess at his net worth?</strong><br> There‚Äôs an estimate of $6 billion, but I have talked to people who think actually quite a bit higher than that. One reason is that he has been incredibly adept at finding clever ways to limit his tax exposure; those investments in Facebook, in Palantir, and some others were made through this investing vehicle known as a Roth IRA that was originally intended for middle class taxpayers. Through some very clever tax planning, Thiel has managed to stash up billions of dollars in this tax-free account.</p><p><strong>When that story [that he used Roth IRAs to massively lower his taxes] broke, there was an outrage that somebody who had made so much money in America was not contributing to the national purse. Do you think he cared?</strong><br> Some of the ideology that motivated PayPal, and that motivates a lot of this crypto stuff that has happened since, is all about going beyond nations. It‚Äôs about this idea that individuals should have more power than nations and should be able to basically do whatever they want. It‚Äôs about undermining the national interest and making sure they don‚Äôt contribute to it.</p><p><strong>Do you see Thiel as an outlier among his tech brethren, or as an exemplar?</strong><br> The conventional wisdom is Thiel is an outlier; he‚Äôs like the one conservative guy in this relatively liberal industry. I think that is basically wrong. Many of the things that he believes are reflected in the actions and behavior of many of his peers. Yes, they may have some disagreements. Many of his peers may vote for Democrats. But the idea that companies should basically be able to do whatever they want, that democracy isn‚Äôt the most important value, these things are reflected in the decisions and actions that many Silicon Valley companies are making, even Silicon Valley companies that are run by ostensibly liberal progressives.</p><p><strong>In the beginning of the book, you paint a portrait of Thiel as a bullied child. Other kids put For Sale signs in his yard and then asked when he was leaving and so on. Was that the cradle of his reactionariness?</strong><br> I think he was bullied as a child. And I think that it‚Äôs not surprising that somebody who maybe had a tough time navigating a place like Stanford would develop a strong revulsion to the idea of universities like Stanford and would undertake a project to replace or critique these universities. Thiel famously funds a fellowship where he encourages promising young people to start companies instead of going to college and he‚Äôs been a prominent critic of colleges. But he‚Äôs only a halfway critic. He says in a thousand different ways that Stanford is worthless, but he keeps teaching classes at Stanford. He keeps hiring Ivy League graduates.</p><p><strong>Most of your sources are anonymous. Why do you think people who spoke to you spoke to you?</strong><br> Thiel‚Äôs pretty unique in that he was involved in this elaborate and secretive litigation campaign that resulted in the destruction of a pretty substantial media outlet when he secretly funded Hulk Hogan‚Äôs lawsuit against Gawker Media, which resulted in this roughly $100 million dollar judgment. So I would talk to people and they would be like, ‚ÄòI‚Äôm a little afraid of him.‚Äô I wouldn‚Äôt really know what to say because I think there‚Äôs actual reason for people to be afraid of Peter Thiel.</p><p><strong>Read More:</strong> <em><a href="https://time.com/5100101/peter-thiel-buying-gawker/">Peter Thiel Made an Offer to Buy Gawker, the Website He Helped Close</a></em></p><p><strong>Are you personally worried?</strong><br> I‚Äôd be lying if I said that Thiel‚Äôs litigation against Gawker didn‚Äôt weigh on me and I think you‚Äôd be foolish to not think about that. That said, it‚Äôs not productive to be afraid.</p><p><strong>Thiel has been right a lot. I wonder if there‚Äôs a bit of you thinking, ‚ÄòIf he‚Äôs been right about these things, what should I be looking for now?‚Äô</strong><br> Recently, he gave a speech where he dissed bitcoin, which was a weird thing. If somebody who played a big role in the beginning of digital money is suddenly saying that maybe crypto is bad for the interests of the United States, we should pay attention. There‚Äôs an extent to which he‚Äôs a great prognosticator, a great futurist. But he‚Äôs also a marketer of himself and he‚Äôs been very good at accentuating the calls that have been right and and playing down the calls that have been wrong.</p><p><strong>You note that he has funded two senators‚ÄîTed Cruz and Josh Hawley‚Äîand is now funding two more candidates. Do you worry he‚Äôll wield outsized power over government?</strong><br> I have to say I worry less about the grandstanding of a handful of senators connected to Thiel than I do about the effect of Thiel-ism on the culture. When you combine the hostility to democracy and institutional norms with the bankroll of a billionaire you can potentially do some damage.</p><p><em>This conversation has been lightly condensed and edited for clarity.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Return of Digg, a Star of an Earlier Internet Era (126 pts)]]></title>
            <link>https://www.nytimes.com/2025/03/05/technology/digg-alexis-ohanian-kevin-rose.html</link>
            <guid>43265521</guid>
            <pubDate>Wed, 05 Mar 2025 12:06:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/03/05/technology/digg-alexis-ohanian-kevin-rose.html">https://www.nytimes.com/2025/03/05/technology/digg-alexis-ohanian-kevin-rose.html</a>, See on <a href="https://news.ycombinator.com/item?id=43265521">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/03/05/technology/digg-alexis-ohanian-kevin-rose.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[MS Paint IDE (107 pts)]]></title>
            <link>https://ms-paint-i.de/</link>
            <guid>43265431</guid>
            <pubDate>Wed, 05 Mar 2025 11:52:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ms-paint-i.de/">https://ms-paint-i.de/</a>, See on <a href="https://news.ycombinator.com/item?id=43265431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <div id="about">
    <h2>About</h2>
    <hr>
    <div>
            <p>MS Paint IDE is a program that can read a normal image file saved with MS Paint, and can then translate it to
            text with the ability to highlight the text in the image, parse the code, compile and execute it. It
            provides a clean and simple interface, with your actual programming in a program you and millions of others
            already know how to use, and already have. MS Paint IDE allows the use of external libraries, multiple
            classes, and much much more.</p>
        <div>
            <p>
                <iframe src="https://www.youtube.com/embed/fhSaLx6l9Xk" frameborder="0" allowfullscreen=""></iframe>
            </p>
        </div>
            </div>
    </div>
        <div id="features">
    <h2>Features</h2>
    <hr>
    <div>
                    <div>
            <h4>Assistant Support</h4>
            <p>MS Paint IDE supports basic IDE actions on the Google Assistant, to be used on a Google Home, or just on your phone.</p>
            
        </div>
                <div>
            <h4>Highlighting</h4>
            <p>The most advanced code highlighting is packaged with the IDE, allowing to make your code not just run beautifully, but look beautiful as well.</p>
            
        </div>
                <div>
            <h4>Parsing</h4>
            <p>Top of the line parsing is just standard with MS Paint IDE, providing with sometimes a 99% accuracy reading and parsing your programs.</p>
            
        </div>
                <div>
            <h4>Git</h4>
            <p>MS Paint IDE has all the essential Git features, including and limited to creating a git repository, adding a remote origin, adding files, and committing/pushing.</p>
            
        </div>
                <div>
            <h4>Theming</h4>
            <p>Don‚Äôt quite like how something looks in the IDE? Change it! All colors in the IDE are changeable, so you can make, share, and use themes.</p>
            
        </div>
                <div>
            <h4>Fastest Growing</h4>
            <p>The fastest growing IDE by our polls, with the most features of any modern day IDE. Since this features section isn't legally binding, we can say that.</p>
            
        </div>
                </div>
    </div>
        <div id="advantages">
    <h2>Advantages</h2>
    <hr>
    <div>
                    <div>
            <h4>Partially Native</h4>
            <p>The main IDE comes with your Windows computer, so you don't need to worry about potential viruses with your brand new computer.</p>
            
        </div>
                <div>
            <h4>Popular</h4>
            <p>Chances are, you probably know about MS Paint, or at least have heard of it. This makes adapting to it easier than most programs.</p>
            
        </div>
                <div>
            <h4>Oldest</h4>
            <p>MS Paint IDE's core program, MS Paint, is older than almost every modern IDE, also making it easily recognisable and bug-resistant.</p>
            
        </div>
                <div>
            <h4>Code Sharing</h4>
            <p>Code sharing is easy, as your code is just an image. Reduce overhead by ditchnig your screenshot program, when you can just copy and paste.</p>
            
        </div>
                <div>
            <h4>Open Source</h4>
            <p>If you wanted to help with the IDE, there are many different areas to work on, including the IDE, the OCR, several wikis, docs, and 2 websites.</p>
            
        </div>
                <div>
            <h4>Superiority</h4>
            <p>It's not Eclipse.</p>
            
        </div>
                </div>
    </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA Successfully Acquires GPS Signals on Moon (233 pts)]]></title>
            <link>https://www.nasa.gov/general/nasa-successfully-acquires-gps-signals-on-moon/</link>
            <guid>43265303</guid>
            <pubDate>Wed, 05 Mar 2025 11:29:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nasa.gov/general/nasa-successfully-acquires-gps-signals-on-moon/">https://www.nasa.gov/general/nasa-successfully-acquires-gps-signals-on-moon/</a>, See on <a href="https://news.ycombinator.com/item?id=43265303">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>NASA and the Italian Space Agency made history on March 3 when the Lunar GNSS Receiver Experiment (LuGRE) became the first technology demonstration to acquire and track Earth-based navigation signals on the Moon‚Äôs surface.&nbsp;&nbsp;</p>
<p>The LuGRE payload‚Äôs success in lunar orbit and on the surface indicates that signals from the GNSS (Global Navigation Satellite System) can be received and tracked at the Moon. These results mean NASA‚Äôs Artemis missions, or other exploration missions, could benefit from these signals to accurately and autonomously determine their position, velocity, and time. This represents a steppingstone to advanced navigation systems and services for the Moon and Mars.&nbsp;&nbsp;</p>

<p>‚ÄúOn Earth we can use GNSS signals to navigate in everything from smartphones to airplanes,‚Äù said Kevin Coggins, deputy associate administrator for NASA‚Äôs SCaN (<a href="https://www.nasa.gov/communicating-with-missions/" target="_blank" rel="noreferrer noopener">Space Communications and Navigation) Program</a>. ‚ÄúNow, LuGRE shows us that we can successfully acquire and track GNSS signals at the Moon. This is a very exciting discovery for lunar navigation, and we hope to leverage this capability for future missions.‚Äù&nbsp;&nbsp;</p>
<div id="">
<p>Kevin Coggins</p>
<p>Deputy Associate Administrator for NASA SCaN</p>
</div>
<p>The road to the historic milestone began on March 2 when the <a href="https://science.nasa.gov/lunar-science/clps-deliveries/to19d-firefly/" rel="noopener">Firefly Aerospace‚Äôs Blue Ghost</a> lunar lander touched down on the Moon and delivered LuGRE, one of 10 NASA payloads intended to advance lunar science. Soon after landing, LuGRE payload operators at <a href="https://www.nasa.gov/goddard/">NASA‚Äôs Goddard Space Flight Center</a> in Greenbelt, Maryland, began conducting their first science operation on the lunar surface.</p>

<p>With the receiver data flowing in, anticipation mounted. Could a Moon-based mission acquire and track signals from two GNSS constellations, GPS and Galileo, and use those signals for navigation on the lunar surface?&nbsp;&nbsp;&nbsp;</p>
<p>Then, at 2 a.m. EST on March 3, it was official: LuGRE acquired and tracked signals on the lunar surface for the first time ever and achieved a navigation fix ‚Äî approximately 225,000 miles away from Earth.&nbsp;&nbsp;</p>
<p>Now that Blue Ghost is on the Moon, the mission will operate for 14 days providing NASA and the Italian Space Agency the opportunity to collect data in a near-continuous mode, leading to additional GNSS milestones. In addition to this record-setting achievement, LuGRE is the first Italian Space Agency developed hardware on the Moon, a milestone for the organization.&nbsp;&nbsp;</p>
<p>The LuGRE payload also <a href="https://blogs.nasa.gov/artemis/2025/02/21/blue-ghost-prepares-for-landing-nasa-instrument-breaks-record/#:~:text=The%20Lunar%20GNSS,on%20the%20Moon." target="_blank" rel="noreferrer noopener">broke GNSS records on its journey to the Moon</a>. On Jan. 21, LuGRE surpassed the highest altitude GNSS signal acquisition ever recorded at 209,900 miles from Earth, a record formerly held by NASA‚Äôs <a href="https://science.nasa.gov/mission/mms/" target="_blank" rel="noreferrer noopener">Magnetospheric Multiscale Mission</a>. Its altitude record continued to climb as LuGRE reached lunar orbit on Feb. 20 ‚Äî 243,000 miles from Earth. This means that missions in cislunar space, the area of space between Earth and the Moon, could also rely on GNSS signals for navigation fixes.&nbsp;&nbsp;</p>

<p>Traditionally, NASA engineers track spacecraft by using a combination of measurements, including onboard sensors and signals from Earth-based tracking stations. The LuGRE payload demonstrates that using GNSS signals for navigation can reduce reliance on human operators because these signals can be picked up and used autonomously by the spacecraft, even as far away as the Moon.&nbsp;</p>
<p>The LuGRE payload is a collaborative effort between NASA‚Äôs Goddard Space Flight Center in Greenbelt, Maryland, the Italian Space Agency, their industry partner Qascom, and Politecnico di Torino. Funding and oversight for the LuGRE payload comes from NASA‚Äôs SCaN Program office. It was chosen by NASA as one of 10 funded research and technology demonstrations for delivery to the lunar surface by Firefly Aerospace Inc., a flight under the agency‚Äôs Commercial Lunar Payload Services initiative.</p>
<p><strong>Learn more about LuGRE:&nbsp;<a href="https://go.nasa.gov/41qwwQN" target="_blank" rel="noreferrer noopener">https://go.nasa.gov/41qwwQN</a></strong></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Shadow fleets' and sabotage: are Europe's undersea cables under attack? (159 pts)]]></title>
            <link>https://www.theguardian.com/world/ng-interactive/2025/mar/05/shadow-fleets-subaquatic-sabotage-europe-undersea-internet-cables-under-attack</link>
            <guid>43265224</guid>
            <pubDate>Wed, 05 Mar 2025 11:14:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/ng-interactive/2025/mar/05/shadow-fleets-subaquatic-sabotage-europe-undersea-internet-cables-under-attack">https://www.theguardian.com/world/ng-interactive/2025/mar/05/shadow-fleets-subaquatic-sabotage-europe-undersea-internet-cables-under-attack</a>, See on <a href="https://news.ycombinator.com/item?id=43265224">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><div data-gu-name="standfirst"><p>Europe is on high alert after a series of outages to cables and pipelines. This visual guide explains what happened and what‚Äôs being done</p></div><article data-gu-name="body"></article></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Richard Sutton and Andrew Barto Win 2024 Turing Award (368 pts)]]></title>
            <link>https://awards.acm.org/about/2024-turing</link>
            <guid>43264847</guid>
            <pubDate>Wed, 05 Mar 2025 10:03:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://awards.acm.org/about/2024-turing">https://awards.acm.org/about/2024-turing</a>, See on <a href="https://news.ycombinator.com/item?id=43264847">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<p>ACM has named <a href="https://awards.acm.org/award-recipients/barto_9471663" target="_blank">Andrew G. Barto</a> and <a href="https://awards.acm.org/award-recipients/sutton_0160594" target="_blank">Richard S. Sutton</a> as the recipients of the 2024 ACM A.M. Turing Award for&nbsp;developing the conceptual and algorithmic foundations of reinforcement learning. In a series of papers beginning in the 1980s, Barto and Sutton introduced the main ideas, constructed the mathematical foundations, and developed important algorithms for reinforcement learning‚Äîone of the most important approaches for creating intelligent systems.</p>

<p>Barto is Professor Emeritus of Information and Computer Sciences at the University of Massachusetts, Amherst. Sutton is a Professor of Computer Science at the University of Alberta, a Research Scientist at Keen Technologies, and a Fellow at Amii&nbsp;(Alberta Machine Intelligence Institute).</p>

<p>The ACM A.M. Turing Award, often referred to as the ‚ÄúNobel Prize in Computing,‚Äù carries a $1 million prize with financial support provided by Google, Inc. The award is named for Alan M. Turing, the British mathematician who articulated the mathematical foundations of computing.</p>

<p><u>What is Reinforcement Learning?</u></p>

<p>The field of artificial intelligence (AI) is generally concerned with constructing agents‚Äîthat is, entities that perceive and act. More intelligent agents are those that choose better courses of action. Therefore, the notion that some courses of action are better than others is central to AI. Reward‚Äîa term borrowed from psychology and neuroscience‚Äîdenotes a signal provided to an agent related to the quality of its behavior. Reinforcement learning (RL) is the process of learning to behave more successfully given this signal.</p>

<p>The idea of learning from reward has been familiar to animal trainers for thousands of years. Later, Alan Turing‚Äôs 1950 paper ‚ÄúComputing Machinery and Intelligence,‚Äù addressed the question ‚Äú<em>Can machines think?</em>‚Äù and proposed an approach to machine learning based on rewards and punishments.</p>

<div><p>While Turing reported having conducted some initial experiments with this approach and Arthur Samuel developed a checker-playing program in the late 1950s that learned from self-play, little further progress occurred in this vein of AI in the following decades. In the early 1980s, motivated by observations from psychology, Barto and his PhD student Sutton began to formulate reinforcement learning as a general problem framework.</p><p>

They drew on the mathematical foundation provided by Markov decision processes (MDPs), wherein an agent makes decisions in a stochastic (randomly determined) environment, receiving a reward signal after each transition and aiming to maximize its long-term cumulative reward. Whereas standard MDP theory assumes that everything about the MDP is known to the agent, the RL framework allows for the environment and the rewards to be unknown. The minimal information requirements of RL, combined with the generality of the MDP framework, allows RL algorithms to be applied to a vast range of problems, as explained further below.</p></div>

<p>Barto and Sutton, jointly and with others, developed many of the basic algorithmic approaches for RL. These include their foremost contribution, temporal difference learning, which made an important advance in solving reward prediction problems, as well as policy-gradient methods and the use of neural networks as a tool to represent learned functions. They also proposed agent designs that combined learning and planning, demonstrating the value of acquiring knowledge of the environment as a basis for planning.</p>

<p>Perhaps equally influential was their textbook, <em> Reinforcement Learning: An Introduction </em> (1998), which is still the standard reference in the field and has been cited over 75,000 times. It allowed thousands of researchers to understand and contribute to this emerging field and continues to inspire much significant research activity in computer science today.</p>

<p>Although Barto and Sutton‚Äôs algorithms were developed decades ago, major advances in the practical applications of RL came about in the past fifteen years by merging RL with deep learning algorithms (pioneered by 2018 Turing Awardees Bengio, Hinton, and LeCun). This led to the technique of deep reinforcement learning.</p>

<p>The most prominent example of RL was the victory by the AlphaGo computer program over the best human <em>Go</em> players in 2016 and 2017. Another major achievement recently has been the development of the chatbot ChatGPT. ChatGPT is a large language model (LLM) trained in two phases, the second of which employs a technique called reinforcement learning from human feedback (RLHF), to capture human expectations.</p>

<p>RL has achieved success in many other areas as well. A high-profile research example is robot motor skill learning in the in-hand robotic manipulation and solution of a physical (Rubik‚Äôs Cube), which showed it possible to do all the reinforcement learning in simulation yet ultimately be successful in the significantly different real world.</p>

<p>Other areas include network congestion control, chip design, internet advertising, optimization, global supply chain optimization, improving the behavior and reasoning capabilities of chatbots, and even improving algorithms for one of the oldest problems in computer science, matrix multiplication.</p>

<p>Finally, a technology that was partly inspired by neuroscience has returned the favor. Recent research, including work by Barto, has shown that specific RL algorithms developed in AI provide the best explanations for a wide range of findings concerning the dopamine system in the human brain.</p>

<p>‚ÄúBarto and Sutton‚Äôs work demonstrates the immense potential of applying a multidisciplinary approach to longstanding challenges in our field,‚Äù explains ACM President Yannis Ioannidis. ‚ÄúResearch areas ranging from cognitive science and psychology to neuroscience inspired the development of reinforcement learning, which has laid the foundations for some of the most important advances in AI and has given us greater insight into how the brain works. Barto and Sutton‚Äôs work is not a stepping stone that we have now moved on from. Reinforcement learning continues to grow and offers great potential for further advances in computing and many other disciplines. It is fitting that we are honoring them with the most prestigious award in our field.‚Äù</p>

<p>‚ÄúIn a 1947 lecture, Alan Turing stated ‚Äò<em>What we want is a machine that can learn from experience</em>,‚Äô‚Äù noted Jeff Dean, Senior Vice President, Google. ‚ÄúReinforcement learning, as pioneered by Barto and Sutton, directly answers Turing‚Äôs challenge. Their work has been a lynchpin of progress in AI over the last several decades. The tools they developed remain a central pillar of the AI boom and have rendered major advances, attracted legions of young researchers, and driven billions of dollars in investments. RL‚Äôs impact will continue well into the future. Google is proud to sponsor the ACM A.M. Turing Award and honor the individuals who have shaped the technologies that improve our lives.‚Äù</p>

<p><strong><a target="_blank" href="https://www.acm.org/media-center/2025/march/turing-award-2024">News Release</a> | <a target="_blank" href="https://awards.acm.org/binaries/content/assets/press-releases/2025/march/turing-award-2024.pdf">Printable PDF</a></strong></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[uBlock Origin Forcefully Removed by Chrome (195 pts)]]></title>
            <link>https://old.reddit.com/r/youtube/comments/1j2ec76/ublock_origin_is_gone/</link>
            <guid>43262531</guid>
            <pubDate>Wed, 05 Mar 2025 03:53:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/youtube/comments/1j2ec76/ublock_origin_is_gone/">https://old.reddit.com/r/youtube/comments/1j2ec76/ublock_origin_is_gone/</a>, See on <a href="https://news.ycombinator.com/item?id=43262531">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/youtube/comments/1j2ec76/ublock_origin_is_gone/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Delta Chat ‚Äì Email Based PGP Encrypted Chat (137 pts)]]></title>
            <link>https://delta.chat/</link>
            <guid>43262510</guid>
            <pubDate>Wed, 05 Mar 2025 03:50:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://delta.chat/">https://delta.chat/</a>, See on <a href="https://news.ycombinator.com/item?id=43262510">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[We're Charging Our Cars Wrong (154 pts)]]></title>
            <link>https://spectrum.ieee.org/ev-charging-2671242103</link>
            <guid>43262468</guid>
            <pubDate>Wed, 05 Mar 2025 03:45:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/ev-charging-2671242103">https://spectrum.ieee.org/ev-charging-2671242103</a>, See on <a href="https://news.ycombinator.com/item?id=43262468">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>A three-port public electric-vehicle charging station, such as this one operated by Electrify America, in Nebraska, can cost as much as half a million dollars.</p><div data-headline="We‚Äôre Charging Our Cars Wrong"><p><strong>If there‚Äôs one thing</strong> we could do now to hasten the transition to <a href="https://spectrum.ieee.org/search/?q=electric+vehicles" target="_self">electric vehicles</a>, it‚Äôs this: Build a robust public EV-charging infrastructure. While the media has focused on vehicle performance and range, consumers have always been clear that they want <a href="https://spectrum.ieee.org/tag/electric-cars">electric cars</a> to do essentially everything their old vehicles do‚Äîincluding long overnight trips.
</p><p>
	To those who don‚Äôt yet own an EV, a robust infrastructure may seem unimportant. Studies, after all, show that in developed markets, as much as 
	<a href="https://iea.blob.core.windows.net/assets/d560f6a6-8d40-4e63-aed2-4fc93139dd5c/PolicybriefonpubliccharginginfrastructurePromotingsuccessfulroll-outstrategiesandbusinessmodels.pdf" rel="noopener noreferrer" target="_blank">90 percent</a> of all charging takes place in the home. It turns out, however, that the remaining percentage of charging is critically important. Drivers of delivery trucks and <a href="https://spectrum.ieee.org/tag/taxis">taxis</a>, residents of apartment buildings, students on their way to college, families on vacation, and countless others have learned that driving an EV can be a struggle where public charging is scarce or unreliable. A 2022 <a href="https://www.forbes.com/wheels/features/ev-range-cost-confidence-survey/" rel="noopener noreferrer" target="_blank">survey</a> by <em><em><a href="https://spectrum.ieee.org/tag/forbes">Forbes</a></em></em>, for example, indicated that 62 percent of EV owners were so anxious about EV range that they had at times curtailed their travel plans.
</p><p>
	This is no secret to policymakers. A 
	<a href="https://iea.blob.core.windows.net/assets/d560f6a6-8d40-4e63-aed2-4fc93139dd5c/PolicybriefonpubliccharginginfrastructurePromotingsuccessfulroll-outstrategiesandbusinessmodels.pdf" rel="noopener noreferrer" target="_blank">recent brief</a> from the <a href="https://spectrum.ieee.org/tag/international-energy-agency">International Energy Agency</a> indicates that in <a href="https://spectrum.ieee.org/tag/China">China</a>, investing in charging infrastructure is considered four times as effective for EV success as providing subsidies to EV buyers.
</p><p>
	These are issues we‚Äôve been grappling with for decades. Back in 1992, we cofounded 
	<a href="https://www.acpropulsion.com/" rel="noopener noreferrer" target="_blank">AC Propulsion</a>, which offered the <a href="https://en.wikipedia.org/wiki/AC_Propulsion_tzero" rel="noopener noreferrer" target="_blank">tZero</a>, a high-performance electric sports car whose basic technologies and design were later incorporated into the original <a href="https://spectrum.ieee.org/tag/tesla">Tesla</a> Roadster. In the years since, we‚Äôve thought a lot about how to make vehicles that people actually want to own and drive.
</p><p data-rm-resized-container="25%"><img alt="An open-top yellow roadster is parked along the side of a bridge roadway." data-rm-shortcode-id="08a84e26ca7fef92ff01b32c98266755" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/an-open-top-yellow-roadster-is-parked-along-the-side-of-a-bridge-roadway.png?id=56626390&amp;width=980" height="1721" id="ca88d" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/an-open-top-yellow-roadster-is-parked-along-the-side-of-a-bridge-roadway.png?id=56626390&amp;width=980" width="3279"><small placeholder="Add Photo Caption...">The 1997 AC Propulsion TZero was a groundbreaking <a href="https://spectrum.ieee.org/tag/electric-vehicle">electric vehicle</a> featuring technical innovations that were later incorporated into the Tesla Roadster.</small><small placeholder="Add Photo Credit..."><a href="https://commons.wikimedia.org/wiki/File:Tzero_-_The_first_Tesla_vehicle.jpg" rel="noopener noreferrer" target="_blank">PeteGruber/Wikipedia</a></small></p><p>
	When we‚Äôve asked potential EV owners what‚Äôs limiting <a href="https://spectrum.ieee.org/collections/the-ev-transition-explained/">EV adoption</a>, they often point to limited access to charging stations‚Äîespecially to fast public charging. The operators who own these <a href="https://spectrum.ieee.org/tag/charging-stations">charging stations</a> have said it as well, and they also cite the high cost of equipment‚Äîa DC fast-charging station with four ports can cost between 
	<a href="https://spectrum.ieee.org/the-ev-transition-explained-2658463735" target="_self">US $470,000 and $725,000</a>. If equipment costs were lower, they say, they would install more recharging stations. It could be a virtuous circle: The recharge businesses would do better, EV owners would benefit, and more people would consider buying an EV.
</p><p>
	The question is, can <a href="https://spectrum.ieee.org/tag/ev-charging">EV charging</a> be done more economically and efficiently? More specifically, is there a way to reduce recharge station complexity and bring down the high cost of fast-charge stations‚Äîand, in so doing, significantly boost EV penetration without sacrificing safety?
</p><p>
	The answer is yes, and here‚Äôs why.
</p><h2> How EV charging works</h2><p>
	Before we explain our solution, let‚Äôs review some fundamentals, starting with the most basic. A charging station is a physical location that has one or more charging ports, each of which can charge a single EV. Each port may have multiple types of service connectors to support 
	<a href="https://spectrum.ieee.org/ev-charging-adapters" target="_self">different EV standards</a>.
</p><p>
	The function of the port is to convert AC power from the grid into DC, which is then applied to the battery. The recharge current must be controlled so that the following criteria are met at all times: The voltage of the battery cells must not exceed a critical limit; cell temperatures must not exceed a preset threshold; and current drawn from the <a href="https://spectrum.ieee.org/tag/electric-utility">electric utility</a> must remain below a certain value. If the first two are not met, cells may be damaged or catch fire. If the third is not met, the charger or utility may be overloaded, causing a breaker to trip or a fuse to blow.
</p><p><img alt="An illustration showing the steps of a process.  " data-rm-shortcode-id="de29d500af34098dd46136aa72868666" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/an-illustration-showing-the-steps-of-a-process.png?id=56630537&amp;width=980" height="3469" id="14845" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/an-illustration-showing-the-steps-of-a-process.png?id=56630537&amp;width=980" width="3039"><small placeholder="Add Photo Caption...">A key safety feature of existing EV chargers is an isolation link [in teal]. Within this circuit, a high-frequency transformer provides physical separation between grid power and the electric vehicle‚Äôs battery. The isolation link is inside the vehicle‚Äôs onboard charger for Level-2 charging (top). For Level-3, or <a href="https://spectrum.ieee.org/tag/fast-charging">fast, charging</a>, the link is located inside the charging station (bottom). </small><small placeholder="Add Photo Credit...">Chris Philpot</small></p><p>
	In addition to these requirements, the charger must protect users from <a href="https://spectrum.ieee.org/tag/electric-shock">electric shock</a>. That‚Äôs not always easy. Chargers operate in rugged environments, usually outdoors, with greatly varying levels of humidity and where contaminated water may be present. Equipment may also be damaged or even sabotaged.
</p><p>
	The time-tested way to prevent electric shock is to use electrical grounding. Grounding is exactly what it sounds like: a direct physical connection to the earth that provides a path for electric current. When such a path is present, stray electrical currents‚Äîin a chassis, for example‚Äîtravel directly to the ground, avoiding any people who might be standing close by. In an <a href="https://spectrum.ieee.org/tag/electric-car">electric car</a> that‚Äôs charging, the green ground wire in the charging cable becomes the path to ground. (Because an electric car has rubber tires, the car itself can‚Äôt serve as a path.)
</p><p>What happens if such a path is not present? If the ground connection in an electric car charger is broken or compromised, the charge port must have a backup solution. Today, that solution is something called galvanic isolation. In galvanic isolation, no direct conduction path is permitted between certain sections of an electrical system.</p><p><img alt="An series of illustration showing a shock hazard and how to prevent a shock hazard" data-rm-shortcode-id="3d841c9e4b5bc289855335b81e917e84" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/an-series-of-illustration-showing-a-shock-hazard-and-how-to-prevent-a-shock-hazard.png?id=56630539&amp;width=980" height="3491" id="f8928" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/an-series-of-illustration-showing-a-shock-hazard-and-how-to-prevent-a-shock-hazard.png?id=56630539&amp;width=980" width="3144"><small placeholder="Add Photo Caption..."> If an <a href="https://spectrum.ieee.org/tag/ev-charger">EV charger</a> does not have an isolation link, and the ground circuit is broken and if a current path exists between the battery and the vehicle body, a person touching the vehicle could receive a potentially deadly electric shock [top illustration]. However, with the simple and inexpensive ‚Äúdouble ground‚Äù circuit designed by Wally Rippel [bottom illustration, in teal], a detector circuit confirms that the ground is intact before closing contactors that enable current to flow. </small><small placeholder="Add Photo Credit...">Chris Philpot</small></p><p><span>The hardware for a charger‚Äôs galvanic isolation is called an isolation link, and it works by physically and electrically separating two circuits, so that a difference in potential won‚Äôt result in current flow from one circuit to the other. In the case of EV charging, the two circuits are the 
	</span><a href="https://spectrum.ieee.org/search/?q=electric+grid" target="_self">electric grid</a><span> on the one hand, and the vehicle battery and its associated circuitry on the other.</span></p><p>
	This isolation can be a literal lifesaver. Suppose an EV‚Äôs battery is leaking. The leaked fluid is conductive, and can therefore produce a current path between the battery circuit and the vehicle chassis. If the ground circuit happens to be broken, then, without isolation, the vehicle‚Äôs chassis would be at a <a href="https://spectrum.ieee.org/tag/high-voltage">high voltage</a>. So a person touching the car while standing on the ground could receive a potentially lethal electric shock (see illustration, ‚ÄúA shock hazard‚Äù). With isolation, there wouldn‚Äôt be a shock hazard, because no current path would exist from the electric utility to the car body.
</p><p>
	 Only one component exists that can provide separation between two circuits while transmitting kilowatt levels of power‚Äîa transformer. 
	<a href="https://spectrum.ieee.org/transformer-shortage" target="_self">The transformers</a> that connect directly to low-frequency utility power are heavy and bulky. But for EV charging, where weight and size are critical, the <a href="https://spectrum.ieee.org/tag/transformers">transformers</a> are much smaller‚Äîthey‚Äôre not even half the size of a standard building brick. That‚Äôs because the charging stations convert DC power to high-frequency AC, using an inverter. The high-frequency AC is then applied to the small transformer, which provides the galvanic isolation. Finally, the output of the transformer is changed back to DC by a high-frequency rectifier circuit, completing the process (as shown in the ‚Äúisolation link...‚Äù illustration).
</p><p>
	We‚Äôll get into the details of this 
	<a href="https://spectrum.ieee.org/silicon-carbide" target="_self">power conversion</a> in the next section, but this gives you an idea of how charging is done safely today, whether at a public charger or in a home garage by means of the car‚Äôs onboard charger.
</p><h2>Galvanic isolation costs a lot</h2><p>
	Virtually every EV has an onboard charger (OBC), which performs the AC-to-DC conversion function, like a public fast charger does, when the vehicle is charging at home. As its name suggests, the OBC resides in the vehicle. It‚Äôs capable of providing power levels from about 5 to 22 kilowatts to the battery, depending on the vehicle make and model. Such charge rates are low in comparison with fast charging, generally only available at 
	<a href="https://spectrum.ieee.org/porsche-claims-it-can-double-teslas-fastcharging-rate" target="_self">public chargers</a>, which starts at 50 kW and can go up to 350 kW.
</p><p>
	Today, all chargers‚Äîonboard and off-board‚Äîare galvanically isolated. The galvanic isolation is integrated into the power-conversion hardware, regardless of whether it‚Äôs in the car or in a public charger.
</p><p><span>A single 300-kW port in a public charging station includes about US $90,000 of <a href="https://spectrum.ieee.org/tag/power-electronics">power electronics</a>, of which about $54,000 is for the isolation link.</span></p><p>
	The 
	<a href="https://www.embitel.com/blog/embedded-blog/power-converter-topologies-for-electric-charging-stations" rel="noopener noreferrer" target="_blank">hardware of an EV charger</a> is basically a much larger and higher-power version of the <a href="https://spectrum.ieee.org/tag/switching-power-supplies">switching power supplies</a> that charge your smartphone or laptop. Earlier, we gave a basic idea about how power conversion in an EV works, but it‚Äôs actually a little more involved than that. For <a href="https://spectrum.ieee.org/tag/evs">EVs</a>, power conversion occurs in four stages (illustration, ‚ÄúA shock hazard‚Äù). In the first stage, AC power, either single-phase or three-phase, is converted to DC by an active rectifier. In the second, DC power from the first stage is converted to a high-frequency AC square wave (think of a classic sine wave but with a square shape rather than, well, a sinuous one) by a circuit known as an inverter. The reason for this high frequency is that in the third stage, a transformer converts the AC to a different voltage, and the high frequency allows this transformer to be much smaller and lighter than it would be for a lower frequency, like that of the <a href="https://spectrum.ieee.org/tag/power-grid">power grid</a>. Finally, at the fourth stage, a high-frequency rectifier converts the high-frequency AC back to DC, and then sends it to the vehicle‚Äôs battery. Collectively, stages two, three, and four make up the isolation link, which provides the galvanic isolation (see illustration, ‚ÄúThe isolation link separates utility power from the EV battery‚Äù).
</p><p>
	This isolation link is very expensive. It accounts for roughly 60 percent of the cost of the power electronics in a typical EV, and it‚Äôs also responsible for about 50 percent of the charger‚Äôs power loss. We estimate that the cost of the bill of materials and assembly of a galvanically isolated charging port is about $300 per kilowatt. So a single 300-kW port in a public charging station includes about $90,000 of power electronics, of which about $54,000 is for the isolation link.
</p><p>
	Do the math: A charging station with four ports includes approximately $360,000 in 
	<a href="https://spectrum.ieee.org/search/?q=power+electronics" target="_self">power electronics</a>, with more than $200,000 of that going for galvanic isolation. To get an idea of the total costs in a country, say the <a href="https://spectrum.ieee.org/tag/united-states">United States</a>, multiply that 60 percent cost reduction of the power electronics per charger by the multiple ports at the more than 61,000 public EV-charging stations in the United States.
</p><p>
	For an EV‚Äôs onboard charger, the isolation link adds not just cost but also bulk. The higher the charge capability, the greater the cost and size of the isolation system. That‚Äôs why you could never do fast charging with an OBC‚Äîthe cost and size would be too great to include it inside the vehicle.
</p><p>
	These are among the main reasons why we propose to eliminate galvanic isolation. Billions of dollars of capital and energy expenses could be saved. <a href="https://spectrum.ieee.org/tag/hardware-reliability">Hardware reliability</a> would improve because the chargers would use about half as many components. Eliminating galvanic isolation‚Äîthat is to say, eliminating stages two, three, and four of the charger hardware‚Äîwould also greatly reduce the size of onboard chargers and enable them to handle fast charging, also known as Level 3 power. This is the highest charging level, providing 100 kW or more of DC current.
</p><p><img alt="A black sports car is seen cruising by a retaining wall." data-rm-shortcode-id="708449a4fbc64363a63171d006ddfc87" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-black-sports-car-is-seen-cruising-by-a-retaining-wall.png?id=56626397&amp;width=980" height="936" id="98a7f" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-black-sports-car-is-seen-cruising-by-a-retaining-wall.png?id=56626397&amp;width=980" width="2066"><small placeholder="Add Photo Caption..."><a href="https://spectrum.ieee.org/tag/tesla-motors">Tesla Motors</a> unveiled its electric Roadster in Santa Monica in 2006.</small><small placeholder="Add Photo Credit...">Glenn Koenig/Los Angeles Times/Getty Images</small></p><p>
	With the isolation link eliminated, we could then take the next step: having the vehicle‚Äôs onboard inverter supply power to the motor for driving and also to the <a href="https://spectrum.ieee.org/tag/batteries">batteries</a> for charging. By having the car‚Äôs inverter do double duty, we would cut the remaining costs by half 
	<em><em>again</em></em>.
</p><p>
	None of this is a new idea. The original Tesla Roadster, which reached the market in 2008, and all of the products built by AC Propulsion successfully used non-galvanically isolated, integrated charging, in which the recharge function was carried out by the inverter. In those AC Propulsion vehicles, the nominal battery voltage was approximately 400 volts <a href="https://spectrum.ieee.org/tag/direct-current">direct current</a>, just as it is in most EVs today.
</p><h2>Can galvanic isolation be eliminated?</h2><p>
	The requirements for eliminating the isolation link are not terribly complex or costly. Two issues in particular need to be addressed: the risk of 
	<a href="https://spectrum.ieee.org/ev-safety" target="_self">electric shock</a> and the compatibility between the utility and battery voltages.
</p><p>
	First, let‚Äôs look at the shock hazard. <a href="https://spectrum.ieee.org/tag/electrocution">Electrocution</a> can occur if three conditions exist simultaneously: The vehicle isn‚Äôt grounded, power is applied to the ungrounded vehicle, and a current-leakage path has formed (see illustration, ‚ÄúA shock hazard‚Äù). A leakage path might be created if, for example, the battery‚Äôs electrolyte has begun leaking, forming a path between the battery and the car body. Because all EV charging systems include a ground connection, a leakage path is a problem only if the ground connection is broken or compromised.
</p><p>
	All charging systems, both onboard and off-board, include components called 
	<a href="https://www.wolfspeed.com/knowledge-center/article/whats-under-the-hood-contactors-implement-ev-safety/" rel="noopener noreferrer" target="_blank">safety contactors</a>, which apply power to the battery only after various electronic checks have been carried out. These checks include ground verification, which tests whether the ground connection is intact. If the ground connection is missing or faulty, charging power won‚Äôt be applied to the battery.
</p><p>
	For Level 2 charging‚Äîin a home garage, for example‚Äîthe safety contactors are located in a module called the 
	<a href="https://www.benderinc.com/solutions/electric-vehicles/charging-stations-evse/" rel="noopener noreferrer" target="_blank">electric vehicle supply equipment</a>. The <a href="https://www.evconnect.com/blog/what-is-evse" rel="noopener noreferrer" target="_blank">EVSE</a> is typically the size of a large shoebox and may be mounted on a wall or a post. In the case of public fast charging, the safety contactors are an integral part of the hardware.
</p><p>
	What this means is that removing galvanic isolation won‚Äôt pose a shock hazard. If the vehicle is grounded and leakage causes the vehicle chassis to be at a high voltage, the resulting surge of current to ground will instantly trip breakers in the charger.
</p><p>
	So the question then becomes: Can ground verification be trusted to be absolutely fail-safe? In other words, can we guarantee that power is never applied if the ground circuit is broken or compromised‚Äîeven if components within the ground verification circuit have failed? Such an absolute guarantee is necessary from both moral and legal standpoints. Removing an existing safety factor, such as galvanic isolation, is unacceptable unless it is replaced by something that provides a net gain in safety.
</p><p>
	We can do that. All it would take would be a relatively simple modification of the charger circuit.
</p><p>
	Such a level of safety can be provided by a double-ground combined with ground-continuity detection (see illustration, ‚ÄúA ‚Äòdouble-ground‚Äô circuit prevents shock‚Äù). This double-ground method is based on‚Äîyou guessed it‚Äîtwo ground wires. With this scheme, if one ground wire is severed, the other one ensures that the vehicle is still grounded. To further enhance safety, the broken ground would be detected and the power shut down, even if one ground wire was still intact.
</p><p><a href="https://www.saferack.com/product/vehicle-grounding/grounding-verification-monitors/" rel="noopener noreferrer" target="_blank">Detection of ground</a>-wire continuity is neither expensive nor complicated. One of us (Rippel) developed a prototype detection circuit about a year ago. The system uses two small transformers, one to inject a signal into one of the ground wires, and the other to detect the signal in the second ground wire. If the signal is not detected by the second transformer, the contactors‚Äîin the EVSE, for example‚Äîare opened so they can‚Äôt apply power. With this circuit, the overall system remains fail-safe in the event that one or more components fail.
</p><p>
	The arrangement makes charging doubly safe, literally. Moreover, because the two ground circuits are mutually independent, no single failure can cause both grounds to fail. This lowers the probability of a ground failure: If the probability of a single ground failure is 
	<em><em>P</em></em>, the probability of both failing is <em><em>P</em></em><span>2</span>. Safety is further improved with the addition of a circuit that senses that the two grounds form a complete circuit; power is turned off as soon as one of the two grounds is damaged or broken.
</p><p>
	Eliminating the risk of electric shock isn‚Äôt the only issue that we must deal with if we are to get rid of galvanic isolation. There‚Äôs also the issue of voltage‚Äîspecifically, the need to prevent mismatches between the utility‚Äôs AC line voltage and that of the EV battery.
</p><p>
	A voltage mismatch becomes a problem under one condition‚Äîwhen the input utility voltage exceeds the battery voltage. If this occurs, even for an instant, uncontrolled current can flow into the battery, possibly damaging it or causing a breaker to trip.
</p><p>
	The solution to this problem is a device called a 
	<a href="https://www.digikey.com/en/maker/tutorials/2024/how-do-buck-converters-work" target="_blank">buck regulator</a> (or buck converter). A buck regulator is similar, functionally, to a step-down transformer, except that it handles DC current rather than AC. In the event that the utility‚Äôs AC voltage exceeds the battery voltage, the buck regulator operates like a transformer and steps it down. In comparison with an isolation link of the same power rating, a buck regulator would cost less than 10 percent and the power loss would be less than 20 percent.
</p><h2>The future of public EV charging</h2><p>
	At this point, we hope you appreciate why the existing four-stage scheme for both onboard and public EV charging is unnecessarily complicated and expensive. Three of the four stages can be completely eliminated. This would leave a single active-rectifier stage, followed, if necessary, by a low-cost buck regulator. To enhance safety to levels as high as if not higher than existing EV charging gear, we would add a double ground with ground-continuity detection. We call this improved approach direct power conversion.
</p><p>
	Using the DPC approach could cut equipment costs by more than half while improving <a href="https://spectrum.ieee.org/tag/energy-efficiency">energy efficiency</a> by two to three percent. That‚Äôs precisely what we need at this stage of the EV revolution, because it would make EV charging stations more affordable for operators, and enable thousands more such sites to be built in just a few years, rather than a decade or more. It would also make EVs more attractive to people who‚Äôve resisted buying an EV because they‚Äôre put off by the 
	<a href="https://www.hbs.edu/bigs/the-state-of-ev-charging-in-america" target="_blank">feeble state of the charging infrastructure</a>.
</p><p>
	It‚Äôs time to simplify the EV recharging process and make it more cost effective. But that surely won‚Äôt happen without a discussion of galvanic isolation in the technical community. So let the discussion begin! We‚Äôre convinced that eliminating the isolation link should be the first step toward the robust charging infrastructure that 
	<a href="https://spectrum.ieee.org/collections/the-ev-transition-explained/" target="_self">the EV transition</a> so desperately needs. <span></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NetBSD on a JavaStation (146 pts)]]></title>
            <link>https://fatsquirrel.org/oldfartsalmanac/netbsd-on-a-javastation/</link>
            <guid>43262188</guid>
            <pubDate>Wed, 05 Mar 2025 03:11:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fatsquirrel.org/oldfartsalmanac/netbsd-on-a-javastation/">https://fatsquirrel.org/oldfartsalmanac/netbsd-on-a-javastation/</a>, See on <a href="https://news.ycombinator.com/item?id=43262188">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<div>
<figure><a href="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-2-scaled.jpg"><img fetchpriority="high" decoding="async" width="1024" height="629" src="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-2-1024x629.jpg" alt="" srcset="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-2-1024x629.jpg 1024w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-2-300x184.jpg 300w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-2-768x472.jpg 768w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-2-1536x943.jpg 1536w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-2-2048x1258.jpg 2048w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-2-624x383.jpg 624w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure></div>


<p>Hard as it may be to imagine, there was a time when Java was brand new and exciting. Long before it became the vast clunky back-end leviathan it is today, it was going to be the ubiquitous graphical platform that would be used on everything from cell phones to supercomputers: write once, run anywhere.</p>



<p>Initially I drank the kool-aid and was thrilled about this new ‚Äúmodern‚Äù language that was going to take over the world, and drooled at the notion of Java-based computers, containing Java chips that could run java byte-code as their native machine code.</p>



<p>I even had a promotional flyer stuck to my wall, alongside pictures of Lydia Lunch, drum machines, and Jimi Hendrix; it had a picture of the soon to be released JavaStation, looking like a designer purple coffee pot; it stimulated my imagination just by looking at it.&nbsp;</p>



<p>Of course, as it transpired, things didn‚Äôt quite go to plan: the JavaStation didn‚Äôt materialise for ages, and when it did, it didn‚Äôt look like a coffee pot, it looked like a mini SPARCstation with the java logo plastered on it. The Java-chip thing proved more difficult to realize than anticipated and so the machine was a based on SPARC, without a disk, intended to be used to run Java apps in a Java based OS called, cleverly, JavaOS.&nbsp;Despite not being a pure Java machine, it was, arguably, the first real network computer, and I wanted one.</p>



<p>Eventually the JavaStation 2 was released, and it did look more like a coffee pot, but was still ‚Äújust‚Äù a SPARC.&nbsp;They didn‚Äôt change the world.</p>



<p>Decades later, here we are, and I am finding myself wallowing in nostalgia for the unreachable technology of my youth.</p>



<p>After many months of searching I found a Mr Coffee JavaStation for sale in Canada; unfortunately the seller only accepted payments through a Canadian banking service which is pretty much inaccessible outside Canada. Many months later, a friend of mine moved to Canada for work and was kind enough to buy it and ship it to me.&nbsp;</p>



<p>Waiting for that package to arrive was painful; I was checking the tracking details obsessively and experienced a great deal of frustration and bafflement as it moved around the country in completely bizarre directions. It went from Canada, pretty much right past my house to a mail depot that was further away from me than the original location in Canada!&nbsp;</p>



<p>But eventually it arrived and, apart from a few ink stains and a scratch, was in nice condition. BTW, any advice on cleaning it up without damaging the casing would be very gratefully received.</p>



<figure><a href="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-1-scaled.jpg"><img decoding="async" width="1024" height="606" src="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-1-1024x606.jpg" alt="" srcset="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-1-1024x606.jpg 1024w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-1-300x178.jpg 300w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-1-768x455.jpg 768w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-1-1536x909.jpg 1536w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-1-2048x1213.jpg 2048w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/js1-1-624x369.jpg 624w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>I hooked it up to a monitor and keyboard and then powered it on. To my joy, the fan and the power LED came to life! But nothing on the screen. The keyboard LEDs didn‚Äôt light up either‚Ä¶</p>



<p>So, I hooked up the serial port to a Mac. This involved digging out an old DB-9 serial cable from my time machine, and an RS-232 to USB adapter. On the mac I fired up Minicom: nothing.</p>



<p>Even at the point of power on, nothing came out of the serial port. This was a great disappointment, but I knew it may need some love when I ordered it, and so I spent time looking around inside for anything obviously blown, and then had a crack at it with the service manual and a multi-meter. Thankfully, there are several great archives of Sun documentation on the Internet ‚Äì which is handy because it really does seem like Oracle went out of their way to scrub all traces of Sun‚Äôs existence from history.</p>



<p>Sadly, the service manual had no information about repairing the motherboard beyond declaring it a ‚Äúwhole unit replacement‚Äù.</p>



<figure><a href="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/Screenshot-from-2025-02-14-18-56-52.png"><img decoding="async" width="951" height="134" src="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/Screenshot-from-2025-02-14-18-56-52.png" alt="" srcset="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/Screenshot-from-2025-02-14-18-56-52.png 951w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/Screenshot-from-2025-02-14-18-56-52-300x42.png 300w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/Screenshot-from-2025-02-14-18-56-52-768x108.png 768w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/Screenshot-from-2025-02-14-18-56-52-624x88.png 624w" sizes="(max-width: 951px) 100vw, 951px"></a></figure>



<p>There are a lot of very old forums involving people discussing various problems with JavaStations, but they were all concerned with working units.</p>



<p>After some lackluster investigations, I began to fear that something was seriously wrong with it, that would involve a whole new level of investigation with a scope and logic analysers. I‚Äôm also not anywhere experienced enough at that type of thing to have confidence that I‚Äôd ever get it fixed, so it became another monument to failed attempts at reviving beautiful old tech.</p>



<p>Every time I thought I‚Äôd have another go at it, the memories of failed attempts at recapping other devices came to mind, and I couldn‚Äôt bear the idea of spending so many hours on trying to fix and replace components, only to have it still not work at the end.</p>



<p>Many months later I got inspired again, and thought I‚Äôd see if there were any videos of people working on JavaStations. I found a video of a guy doing a tear-down of a Mr Coffee and, despite being quite adept at getting into the thing, I gave it a watch.&nbsp;</p>



<p>The guy was clearly knowledgeable about the topic and pointed out that the battery backed up NVRAM was very likely to have a dead battery. This was something I was familiar with, having revived some old IPC/IPX boxes in the past. But then he said something magical: if the battery is dead, it won‚Äôt boot at all, and you‚Äôd need to configure it with a serial terminal. Well, I‚Äôd looked for life with a serial terminal and there was nothing. But when I watched him do the same thing, I noticed something different: it took a considerable amount of time before anything appeared on the serial port. Like a minute. Also, I hadn‚Äôt bothered to check the required baud rate (it‚Äôs been a while since I had to worry about that stuff). So, what if I had the wrong baud rate and also hadn‚Äôt waited long enough to see any transmissions. You probably know what I‚Äôm going to say next, and yes, the only thing wrong with my JavaStation turned out to be a dead NVRAM battery and my lack of patience. With a 9600 baud serial connection and some patience I was delighted to be greeted with some complaints about corrupt NVRAM followed by an ‚Äúok‚Äù prompt.</p>



<figure><a href="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/Screenshot-from-2025-02-11-21-18-26-1.png"><img loading="lazy" decoding="async" width="1024" height="716" src="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/Screenshot-from-2025-02-11-21-18-26-1-1024x716.png" alt="" srcset="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/Screenshot-from-2025-02-11-21-18-26-1-1024x716.png 1024w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/Screenshot-from-2025-02-11-21-18-26-1-300x210.png 300w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/Screenshot-from-2025-02-11-21-18-26-1-768x537.png 768w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/Screenshot-from-2025-02-11-21-18-26-1-624x436.png 624w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/Screenshot-from-2025-02-11-21-18-26-1.png 1154w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></a></figure>



<p>There is no way to convey to you how excited I was to see this. Even if I couldn‚Äôt get this thing to boot an OS, I had a lovely serial-based FORTH interpreter to play with. In case you‚Äôre confused by that last sentence, you need to know that Sun‚Äôs bootloader environment from that period was called OpenBoot, and consisted of a FORTH interpreter, from which you can interrogate the device tree and pretty much do whatever you want. At this point the JavaStation was a functional computer and that would have been sufficient on its own ‚Äì but getting it to run an actual OS would be a nice fun project.</p>



<p>After a bit of Googling, I plumped for NetBSD; their documentation was extremely comprehensive (as is frequently the case with the BSDs), and it appeared that even in the latest release (10.1), there was still support for not just SPARC, but specifically the JavaStation.</p>



<p>Before any of that, we had to deal with the NVRAM battery being dead. The dead battery meant that whenever the power is turned off, the data in the NVRAM is lost, resulting in junk memory contents. When the machine starts up, it performs a checksum on the a part of the NVRAM data called IDPROM, and if it‚Äôs incorrect, resets it to default values. So, until we get the battery replaced [this will be discussed in a later post], we have to give the IDPROM some sensible(ish) values every time we start up. Thankfully this is quite simple, and we can do it all at the ‚Äúok‚Äù prompt. Here is what that looks like:</p>



<pre><code>ok 01 00 mkp
ok real-machine-type 01 mkp
ok 8 02 mkp
ok 0 03 mkp
ok 20 04 mkp
ok b0 05 mkp
ok 0b 06 mkp
ok 13 07 mkp
ok 0 08 mkp
ok 0 09 mkp
ok 0 0a mkp
ok 0 0b mkp
ok b0 0c mkp
ok 0b 0d mkp
ok 13 0e mkp
ok 0 f 0 do i idprom@ xor loop f mkp</code></pre>



<p>This looks insanely complicated, but it‚Äôs actually pretty straightforward. Skip this section if you‚Äôre not interested.</p>



<hr>



<p>As mentioned above, we‚Äôre in a FORTH interpreter, but even if you‚Äôve never used (or even heard of) FORTH, it‚Äôs simple to understand, but a bit odd when you first have to deal with it. </p>



<p>The command ‚Äúmkp‚Äù takes the two numbers before it, and writes the first one into the IDPROM at the address of the second one. So, for example, the first line below ‚Äú01 00 mkp‚Äù writes the value 1 into address 0. Below is an explanation of what the numbers we‚Äôre writing mean:</p>



<figure><a href="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/explanation.png"><img loading="lazy" decoding="async" width="1024" height="719" src="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/explanation-1024x719.png" alt="" srcset="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/explanation-1024x719.png 1024w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/explanation-300x211.png 300w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/explanation-768x540.png 768w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/explanation-624x438.png 624w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/explanation.png 1153w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></a></figure>



<p>In case you‚Äôre wondering how we know what data to write where, there‚Äôs a handy guide at <a href="https://shrubbery.net/~heas/sun-feh-2_1/Devices/IDPROM/IDPROM_Overview.html">https://shrubbery.net/~heas/sun-feh-2_1/Devices/IDPROM/IDPROM_Overview.html</a>. The MAC address I‚Äôm using is made-up, but the first three bytes (08-00-20) are actually the correct Sun OUI for this device.</p>



<p>The ‚Äúchecksum‚Äù line is more complicated: it‚Äôs a tiny little FORTH program that generates the checksum and writes it to address 0xF. Figuring out how it works can be left as an exercise for the reader.</p>



<hr>



<p>So, after entering these commands, we have written a configuration to the IDPROM that is valid enough to allow it to boot. Now we have to reboot without removing the power, so we type ‚Äúreset‚Äù.</p>



<p>The JavaStation restarts, and this time it‚Äôs happy enough to start using its built in display (via the VGA port):</p>



<figure><a href="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/javastation-lost-carrier.jpg"><img loading="lazy" decoding="async" width="1024" height="523" src="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/javastation-lost-carrier-1024x523.jpg" alt="" srcset="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/javastation-lost-carrier-1024x523.jpg 1024w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/javastation-lost-carrier-300x153.jpg 300w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/javastation-lost-carrier-768x393.jpg 768w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/javastation-lost-carrier-624x319.jpg 624w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/javastation-lost-carrier.jpg 1215w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></a></figure>



<p>Yes! So it‚Äôs happy with the configuration, but now we have to get it to boot. It doesn‚Äôt have its own storage, so it needs to boot over the network. </p>



<p>The net-booting procedure works as follows:</p>



<ul>
<li>The JavaStation attempts to get an IP address by using the venerable RARP protocol.</li>



<li>Once it has an IP address, it presumptuously assumes that whatever machine gave it the IP address will also be able to supply a second stage bootloader using tftp, and attempts to fetch it.</li>



<li>If it manages to get a secondary bootloader over tftp, it runs it.</li>



<li>The secondary bootloader then attempts to fetch and run a kernel and filesystem using NFS.</li>
</ul>



<p>As well as the original Sun documentation, NetBSD has great documentation for booting diskless machines at <a href="https://www.netbsd.org/docs/network/netboot">https://www.netbsd.org/docs/network/netboot</a>, and within that document there are subsections for specific architectures and machines including the JavaStation.</p>



<p>As should be reasonably obvious, net-booting needs support from other computers on the network. The NetBSD documentation describes how to achieve this using a bunch of different operating systems, but as I have a few Linux boxes to hand, that‚Äôs what I used.</p>



<p>Firstly, we need to answer the JavaStation‚Äôs RARP request. This is a very old standard and well supported by Linux using software called ‚Äúrarpd‚Äù. On ubuntu, you can install this with a simple:</p>



<pre><code>sudo apt install rarpd</code></pre>



<p>Once installed, you only need to edit one file to configure it: /etc/ethers. Just add a line to the file consisting of the MAC address of the JavaStation, a space, and the IP address you want it to have on your network. In my case:</p>



<pre><code>08:00:20:B0:0B:13 192.168.128.45</code></pre>



<p>The MAC address is defined by whatever you configured into the NVRAM in the steps above. </p>



<p>Next, we need to be able to provide the secondary bootloader over tftp. This is also surprisingly easy. On ubuntu, install tftpd with:</p>



<pre><code>sudo apt install tftpd</code></pre>



<p>NetBSD provides a secondary bootloader specifically for SPARC/JavaStations; the latest NetBSD version (10.1) is at <a href="https://cdn.netbsd.org/pub/NetBSD/NetBSD-10.1/sparc/installation/netboot/">https://cdn.netbsd.org/pub/NetBSD/NetBSD-10.1/sparc/installation/netboot/</a>.<br><a href="https://cdn.netbsd.org/pub/NetBSD/NetBSD-10.1/sparc/installation/netboot/bootjs.net">bootjs.net</a> is specifically for the JavaStation, but it‚Äôs apparently intended for a newer version of the JavaStation than mine because when I tried it, it failed with an ‚Äúillegal instruction‚Äù error. Instead, I used the <a href="https://cdn.netbsd.org/pub/NetBSD/NetBSD-10.1/sparc/installation/netboot/boot.net">boot.net</a> version which worked fine. You need to rename the file with a specific format: the IP address of the JavaStation, but in 8 capitalized hex digits, followed a dot, and then the architecture (in this case ‚ÄúSUN4M‚Äù). So, in this example the IP address (as defined in rarpd above) is 192.168.128.45, which in hex is C0A8802D. I downloaded and renamed the bootloader with the command:</p>



<pre><code>curl -o /tftpboot/C0A8802D.SUN4M https://cdn.netbsd.org/pub/NetBSD/NetBSD-10.1/sparc/installation/netboot/boot.net</code></pre>



<p>This should be all you need for getting the secondary bootloader to run. At this point it‚Äôs a good idea to give the JavaStation a name so we don‚Äôt have to continually use the IP address to refer to it in further configurations. In my case I wanted to give it the name ‚Äúduke,‚Äù and so added the following to the /etc/hosts file:</p>



<pre><code>192.168.128.45 duke</code></pre>



<p>The next stage of the booting involves the JavaStation performing a DHCP request in order find the NFS server it is to boot from. Like most people, I already use DHCP on my LAN to configure everything, but in the DHCP server runs on my router, and the NFS server is running on a different Linux machine. The response from the DHCP server therefore needs to contain a field (‚Äúsiaddr‚Äù) referring to the address of the NFS server. While it should be possible to configure the router‚Äôs DHCP server to do this, I had a tough time making it happen. So instead, I decided to configure the router to ignore this MAC address, and instead ran a DHCP server on the Linux machine. You can install the ISC dhcp server on ubuntu with:</p>



<pre><code>sudo apt install isc-dhcp-server</code></pre>



<p>Then add an entry to /etc/dhcp/dhcpd.conf along the following lines:</p>



<pre><code>subnet 192.168.128.0 netmask 255.255.255.0 {
      
        class "javastation-class" {
               lease limit 1;
               default-lease-time 3600;
               max-lease-time 7200;
        }

        pool {
               allow members of "javastation-class";
               range dynamic-bootp 192.168.128.45 192.168.128.45;
        }

        host duke {
          hardware ethernet 08:00:20:B0:0B:13;
          fixed-address 192.168.128.45;
          option routers 192.168.128.1;
          option root-path "/export/client/root";
        }
}</code></pre>



<p>The ‚Äúhost duke‚Äù section contains the config we need the JavaStation to have. Apart from the fixed-address, we‚Äôre giving it the path to the NFS shares which will be located on the same machine.  As we‚Äôre specifically supplying a ‚Äúhardware ethernet‚Äù address, this configuration will only be supplied to our JavaStation, and all other clients will be ignored, leaving the Router DHCP server to deal with everything else as normal.</p>



<p>Finally we need to serve the NetBSD filesystem with NFS. Thankfully, despite its age and total lack of security, NFS is still well supported under Linux. In a nutshell, here‚Äôs how you set up the filesystem. These directions are taken from NetBSD‚Äôs excellent documentation: <a href="https://www.netbsd.org/docs/network/netboot/nfs.html#linux">https://www.netbsd.org/docs/network/netboot/nfs.html#linux</a></p>



<p>These instructions assume we‚Äôll host the filesystem at /export/client, but it‚Äôs really up to you where you put it, as long as you remember to add the path to the DHCP config above.</p>



<pre><code># mkdir -p /export/client/root/dev
# mkdir /export/client/usr
# mkdir /export/client/home
# cd /export/client/root
# curl -L https://cdn.netbsd.org/pub/NetBSD/NetBSD-10.1/sparc/binary/sets/kern-MRCOFFEE.tgz | tar xvpzf -
# mknod /export/client/root/dev/console c 0 0</code></pre>



<p>I suggest renaming the kernel from ‚Äúnetbsd‚Äù to ‚Äúkona‚Äù, which is the default name the Javastation uses:</p>



<pre><code># mv netbsd kona</code></pre>



<p>Add the following lines to /etc/exports:</p>



<pre><code>/export/client/root duke(rw,no_root_squash)
/export/client/usr duke(rw,root_squash)
/export/client/home duke(rw,root_squash)</code></pre>



<p>Obviously, substitute ‚Äúduke‚Äù for whatever you called your JavaStation.</p>



<pre><code># cd /export/client/root
# curl -L https://cdn.netbsd.org/pub/NetBSD/NetBSD-10.1/sparc/binary/sets/base.tgz | tar zxvpf -
# curl -L https://cdn.netbsd.org/pub/NetBSD/NetBSD-10.1/sparc/binary/sets/etc.tgz | tar zxvpf -
# mkdir /export/client/root/kern
# dd if=/dev/zero of=/export/client/swap bs=4k count=4k </code></pre>



<p>Create/edit /export/client/root/etc/ifconfig.&lt;ethernet device name&gt; i.e. ifconfig.le0 in this case, and add the following line (setting the netmask and broadcast for your network):</p>



<pre><code>inet duke netmask 255.255.255.0 broadcast 192.168.1.255</code></pre>



<p>Create/edit  /export/client/root/etc/fstab, with the following lines:</p>



<pre><code>/swap                           none  swap  sw 0 0
nfsserver:/export/client/root   /     nfs   rw 0 0
nfsserver:/export/client/usr    /usr  nfs   rw 0 0
nfsserver:/export/client/home   /home nfs   rw 0 0</code></pre>



<p>Edit  /export/client/root/etc/rc.conf. Make sure the following are set:</p>



<pre><code>hostname="client"
defaultroute="192.168.128.1"
nfs_client=YES
auto_ifconfig=NO
net_interfaces=""</code></pre>



<p>Add the following lines to /export/client/root/etc/hosts:</p>



<pre><code>192.168.1.10 client.test.net client
192.168.1.5  nfsserver.test.net nfsserver</code></pre>



<p>Then:</p>



<pre><code># mv /export/client/root/usr/* /export/client/usr/ </code></pre>



<p>Nowadays, the Linux kernel has a built-in NFS server so no extra daemons are needed. You just need to make sure you have the kernel-server installed:</p>



<pre><code># apt install nfs-kernel-server</code></pre>



<p>To make sure that your exported filesystems really are exported, you can force the NFS server to reread /etc/exports:</p>



<pre><code>exportfs -r</code></pre>



<p>Everything should now be in-place. so we‚Äôre ready to try booting the system. Plug an ethernet cable into the JavaStation, and at the ok prompt type ‚Äúreset‚Äù. If all goes well, the NetBSD second stage bootloader should be fetched and executed.</p>



<figure><a href="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsdboot.jpg"><img loading="lazy" decoding="async" width="1024" height="580" src="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsdboot-1024x580.jpg" alt="" srcset="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsdboot-1024x580.jpg 1024w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsdboot-300x170.jpg 300w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsdboot-768x435.jpg 768w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsdboot-1536x870.jpg 1536w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsdboot-624x353.jpg 624w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsdboot.jpg 1674w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></a></figure>



<p>If there are any problems, you can get a glimpse into what‚Äôs going on by sniffing the network traffic. Personally, I prefer to do captures with tcpdump and then analyse the dumps with Wireshark on my laptop: tcpdump is small and headless. For example running:</p>



<pre><code>tcpdump -n -s0 -w boot.pcap host duke</code></pre>



<p>This will create a capture file that will load straight into Wireshark or any other pcap compatible analyser. Here is what to expect.</p>



<p>Initially, you‚Äôll see the RARP response as the Linux machine tells the Javastation its IP address. Next, you‚Äôll see the JavaStation request the secondary bootloader with TFTP:</p>



<figure><a href="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/rarp-1.png"><img loading="lazy" decoding="async" width="1024" height="34" src="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/rarp-1-1024x34.png" alt="" srcset="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/rarp-1-1024x34.png 1024w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/rarp-1-300x10.png 300w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/rarp-1-768x26.png 768w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/rarp-1-1536x51.png 1536w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/rarp-1-624x21.png 624w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/rarp-1.png 1827w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></a></figure>



<p>If the bootloader transfer succeeds, you‚Äôll see a DHCP transaction followed by NFS traffic:</p>



<figure><a href="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/dhcp-nfs.png"><img loading="lazy" decoding="async" width="1024" height="102" src="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/dhcp-nfs-1024x102.png" alt="" srcset="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/dhcp-nfs-1024x102.png 1024w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/dhcp-nfs-300x30.png 300w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/dhcp-nfs-768x76.png 768w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/dhcp-nfs-1536x153.png 1536w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/dhcp-nfs-624x62.png 624w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/dhcp-nfs.png 1842w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></a></figure>



<p>If all goes well, the first time you boot NetBSD you will be prompted to hit return:</p>



<figure><a href="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/notconfigured.png"><img loading="lazy" decoding="async" width="1024" height="557" src="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/notconfigured-1024x557.png" alt="" srcset="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/notconfigured-1024x557.png 1024w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/notconfigured-300x163.png 300w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/notconfigured-768x418.png 768w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/notconfigured-1536x835.png 1536w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/notconfigured-624x339.png 624w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/notconfigured.png 1666w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></a></figure>



<p>At this point you are running an unconfigured version of NetBSD in single user mode. There are only two things left to do. Firstly, create all of the special device files in dev:</p>



<pre><code># mount /usr
# cd /dev
# /bin/sh MAKEDEV all</code></pre>



<p>This takes a lot longer than you‚Äôd imagine ‚Äì NFS over 10Mbps is slow‚Ä¶</p>



<p>Finally edit /etc/rc.conf and change the line that starts ‚Äúrc_configured‚Äù to ‚Äúrc_configured=yes‚Äù. N.B. Before you can run an editor like vi, you may need to set the terminal type to something generic:</p>



<pre><code># export TERM=vt100
# vi /etc/rc.conf</code></pre>



<p>And finally, reboot the system:</p>



<pre><code># reboot</code></pre>



<p>If the universe is behaving,  the system will reboot and you‚Äôll be rewarded with a login prompt. Login as root with no password, and you‚Äôre in business!</p>



<figure><a href="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsd-welcome.jpg"><img loading="lazy" decoding="async" width="1024" height="564" src="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsd-welcome-1024x564.jpg" alt="" srcset="https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsd-welcome-1024x564.jpg 1024w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsd-welcome-300x165.jpg 300w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsd-welcome-768x423.jpg 768w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsd-welcome-1536x846.jpg 1536w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsd-welcome-624x344.jpg 624w, https://fatsquirrel.org/oldfartsalmanac/wp-content/uploads/2025/02/netbsd-welcome.jpg 1671w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></a></figure>



<p>Obviously there is still a lot to do; as you can see postfix isn‚Äôt happy, and the swapfile security needs tightening up for a start. But we do now have a functional NetBSD system running on a vintage network computer!</p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Demoralization is just Beginning (401 pts)]]></title>
            <link>https://geohot.github.io//blog/jekyll/update/2025/03/03/demoralization-is-just-beginning.html</link>
            <guid>43261941</guid>
            <pubDate>Wed, 05 Mar 2025 02:35:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://geohot.github.io//blog/jekyll/update/2025/03/03/demoralization-is-just-beginning.html">https://geohot.github.io//blog/jekyll/update/2025/03/03/demoralization-is-just-beginning.html</a>, See on <a href="https://news.ycombinator.com/item?id=43261941">Hacker News</a></p>
Couldn't get https://geohot.github.io//blog/jekyll/update/2025/03/03/demoralization-is-just-beginning.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Brother accused of locking down third-party printer ink cartridges (485 pts)]]></title>
            <link>https://www.tomshardware.com/peripherals/printers/brother-accused-of-locking-down-third-party-printer-ink-cartridges-via-firmware-updates-removing-older-firmware-versions-from-support-portals</link>
            <guid>43261933</guid>
            <pubDate>Wed, 05 Mar 2025 02:34:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/peripherals/printers/brother-accused-of-locking-down-third-party-printer-ink-cartridges-via-firmware-updates-removing-older-firmware-versions-from-support-portals">https://www.tomshardware.com/peripherals/printers/brother-accused-of-locking-down-third-party-printer-ink-cartridges-via-firmware-updates-removing-older-firmware-versions-from-support-portals</a>, See on <a href="https://news.ycombinator.com/item?id=43261933">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body">
<p>Fabled RepairTuber and right to repair crusader Louis Rossmann has shared a new video encapsulating his surprise, and disappointment, that Brother has morphed into an ‚Äúanti-consumer printer company.‚Äù More information about Brother‚Äôs embrace of the dark side are shared on Rossmann‚Äôs <a data-analytics-id="inline-link" href="https://wiki.rossmanngroup.com/wiki/Brother_ink_lockout_%26_quality_sabotage" data-url="https://wiki.rossmanngroup.com/wiki/Brother_ink_lockout_%26_quality_sabotage" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">wiki</a>, with the major two issues being new firmware disabling third party toner, and preventing (on color devices) color registration functionality.</p><div data-nosnippet="">
<div>
<p><span>Brother turns heel &amp; becomes anti-consumer printer company üò¢ üò¢ üò¢ - YouTube</span>
<img src="https://img.youtube.com/vi/bpHX_9fHNqE/maxresdefault.jpg" alt="Brother turns heel &amp; becomes anti-consumer printer company üò¢ üò¢ üò¢ - YouTube" data-aspect-ratio="16/9" loading="lazy">
</p>
</div>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 234.67 165.33"><path fill="red" d="M229.763 25.817c-2.699-10.162-10.65-18.165-20.748-20.881C190.716 0 117.333 0 117.333 0S43.951 0 25.651 4.936C15.553 7.652 7.6 15.655 4.903 25.817 0 44.236 0 82.667 0 82.667s0 38.429 4.903 56.85C7.6 149.68 15.553 157.681 25.65 160.4c18.3 4.934 91.682 4.934 91.682 4.934s73.383 0 91.682-4.934c10.098-2.718 18.049-10.72 20.748-20.882 4.904-18.421 4.904-56.85 4.904-56.85s0-38.431-4.904-56.85"></path><path fill="#fff" d="m93.333 117.559 61.333-34.89-61.333-34.894z"></path></svg>
<a href="https://youtu.be/bpHX_9fHNqE" target="_blank" data-url="https://youtu.be/bpHX_9fHNqE" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Watch On <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 507.9 113.39"><g fill="#fff"><path d="M64.792 80.99V32.396l42.082 24.297zm93.803-63.285a20.285 20.285 0 0 0-14.32-14.32C131.642 0 80.99 0 80.99 0S30.337 0 17.705 3.385a20.286 20.286 0 0 0-14.32 14.32C0 30.338 0 56.693 0 56.693S0 83.049 3.385 95.68A20.285 20.285 0 0 0 17.705 110c12.632 3.386 63.285 3.386 63.285 3.386s50.652 0 63.285-3.386a20.284 20.284 0 0 0 14.32-14.32c3.385-12.632 3.385-38.988 3.385-38.988s0-26.355-3.385-38.988m94.473 74.326c.887-2.314 1.332-6.098 1.332-11.35V58.556c0-5.097-.445-8.822-1.332-11.178-.888-2.355-2.452-3.533-4.69-3.533-2.163 0-3.69 1.178-4.577 3.533-.888 2.356-1.332 6.081-1.332 11.178V80.68c0 5.25.424 9.035 1.275 11.35.848 2.318 2.392 3.475 4.633 3.475 2.239 0 3.803-1.157 4.691-3.475zm-17.953 11.122c-3.207-2.16-5.486-5.52-6.835-10.079-1.352-4.554-2.027-10.617-2.027-18.185v-10.31c0-7.644.771-13.784 2.316-18.417 1.544-4.633 3.956-8.011 7.24-10.135 3.282-2.123 7.587-3.186 12.916-3.186 5.251 0 9.459 1.082 12.626 3.243 3.165 2.162 5.482 5.542 6.95 10.136 1.466 4.595 2.2 10.715 2.2 18.36v10.31c0 7.567-.714 13.65-2.142 18.243-1.43 4.595-3.747 7.955-6.951 10.077-3.205 2.124-7.548 3.186-13.03 3.186-5.64 0-10.06-1.082-13.263-3.243m248.053-57.981c-.81 1.005-1.352 2.646-1.621 4.923-.272 2.278-.404 5.734-.404 10.367v5.097h11.697V60.46c0-4.555-.155-8.011-.463-10.367-.309-2.355-.868-4.014-1.678-4.98-.812-.966-2.067-1.449-3.766-1.449-1.7 0-2.954.503-3.765 1.506zm-2.025 29.886v3.591c0 4.557.132 7.974.404 10.251.269 2.279.828 3.94 1.68 4.982.849 1.041 2.16 1.564 3.938 1.564 2.392 0 4.035-.927 4.923-2.781.887-1.853 1.37-4.942 1.447-9.268l13.785.812c.077.62.116 1.469.116 2.548 0 6.565-1.795 11.47-5.387 14.712-3.589 3.242-8.669 4.865-15.232 4.865-7.876 0-13.398-2.47-16.564-7.414-3.168-4.94-4.75-12.586-4.75-22.935V63.589c0-10.657 1.641-18.436 4.924-23.342 3.281-4.903 8.9-7.355 16.854-7.355 5.482 0 9.691 1.004 12.626 3.012 2.933 2.01 5 5.137 6.197 9.383 1.197 4.247 1.796 10.117 1.796 17.607v12.163h-26.757m-284.953-1.33-18.187-65.68h15.869l6.37 29.77c1.623 7.339 2.82 13.594 3.591 18.766h.464c.54-3.706 1.738-9.922 3.591-18.65l6.603-29.886h15.869l-18.417 65.68v31.51h-15.754v-31.51M322.115 34.23v71.007h-12.511l-1.39-8.688h-.347c-3.399 6.564-8.496 9.845-15.291 9.845-4.71 0-8.185-1.543-10.425-4.633-2.24-3.087-3.359-7.915-3.359-14.48V34.23h15.985v52.126c0 3.168.348 5.426 1.043 6.776.695 1.353 1.853 2.027 3.475 2.027 1.39 0 2.722-.423 3.996-1.275 1.274-.849 2.22-1.928 2.838-3.241V34.229h15.986m81.995.001v71.007h-12.511l-1.391-8.688h-.345c-3.402 6.564-8.498 9.845-15.292 9.845-4.711 0-8.186-1.543-10.426-4.633-2.24-3.087-3.358-7.915-3.358-14.48V34.23h15.985v52.126c0 3.168.347 5.426 1.041 6.776.696 1.353 1.855 2.027 3.476 2.027 1.391 0 2.723-.423 3.996-1.275 1.275-.849 2.22-1.928 2.839-3.241V34.229h15.985"></path><path d="M365.552 20.908h-15.87v84.329h-15.637v-84.33h-15.869V8.05h47.376v12.858m76.811 53.636c0 5.174-.215 9.229-.639 12.162-.424 2.937-1.139 5.021-2.143 6.255-1.004 1.236-2.357 1.854-4.053 1.854a7.404 7.404 0 0 1-3.65-.927c-1.12-.618-2.026-1.544-2.722-2.78V50.796c.54-1.93 1.467-3.513 2.78-4.749 1.313-1.234 2.74-1.853 4.285-1.853 1.623 0 2.876.637 3.766 1.91.886 1.275 1.505 3.418 1.853 6.43.348 3.011.523 7.297.523 12.857zm14.652-28.964c-.967-4.478-2.531-7.721-4.692-9.73-2.163-2.007-5.136-3.011-8.919-3.011-2.935 0-5.676.83-8.224 2.49a16.926 16.926 0 0 0-5.908 6.545h-.117l.001-37.416h-15.405v100.777h13.204l1.622-6.717h.347c1.235 2.393 3.088 4.285 5.56 5.675 2.47 1.39 5.213 2.085 8.225 2.085 5.404 0 9.382-2.491 11.931-7.471 2.548-4.982 3.823-12.76 3.823-23.341V64.23c0-7.953-.484-14.17-1.448-18.65"></path></g></svg></a>
</div><p>Rossmann is clearly perturbed by Brother‚Äôs quiet volte-face with regard to aftermarket ink. Above he admits that he used to tell long-suffering HP or Canon printing device owners faces with cartridge DRM issues ‚ÄúBuy a brother laser printer for $100 and all of your woes will be solved.‚Äù</p><p>Sadly, ‚ÄúBrother is among the rest of them now,‚Äù mused the famous RepairTuber. With that, he admitted he would be stumped if asked to recommend a printer today. However, what he has recently seen of Brother makes him determined to keep his current occasionally used output peripheral off the internet and un-updated.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-320-80.jpg" alt="Brother printers - Lois Rossmann Wiki" srcset="https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL.jpg"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Louis Rossmann Wiki)</span></figcaption></figure><p>As mentioned in the intro, Rossmann has seen two big issues emerge for Brother printer users with recent firmware updates. Firstly, models that used to work with aftermarket ink, might refuse to work with the same cartridges in place post-update. Brother doesn‚Äôt always warn about such updates, so Rossmann says that it is important to keep your printer offline, if possible. Moreover, he reckons it is best to keep your printers offline, and ‚ÄúI highly suggest that you turn off your updates,‚Äù in light of these anti-consumer updates.</p><p>Another anti-consumer problem Rossmann highlights affects color devices. He cites reports from a Brother MFP user who noticed color calibration didn‚Äôt work with aftermarket inks post-update. They used to work, and if the update doesn‚Äôt allow the printer to calibrate with this aftermarket ink the cheaper carts become basically unusable.</p><p>Making matters worse, and an aspect of this tale which seems particularly dastardly, Rossmann says that older printer firmware is usually removed from websites. This means users can‚Äôt roll back when they discover the unwanted new ‚Äòfeatures‚Äô post-update.</p><p>While he admittedly can‚Äôt do much about these printer industry machinations, Rossmann says it feels important to document these changes which show that property rights for individuals are disappearing.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-2GJmEWwwXxqxV4o5KwumSW"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Trump's 'Crypto Reserve' Is Such Brazen Corruption (215 pts)]]></title>
            <link>https://zeteo.com/p/trumps-crypto-reserve-is-such-brazen</link>
            <guid>43261899</guid>
            <pubDate>Wed, 05 Mar 2025 02:30:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zeteo.com/p/trumps-crypto-reserve-is-such-brazen">https://zeteo.com/p/trumps-crypto-reserve-is-such-brazen</a>, See on <a href="https://news.ycombinator.com/item?id=43261899">Hacker News</a></p>
Couldn't get https://zeteo.com/p/trumps-crypto-reserve-is-such-brazen: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[GSA prepares to sell over 400 federal properties (102 pts)]]></title>
            <link>https://www.politico.com/news/2025/03/04/gsa-sell-400-federal-properties-00212071</link>
            <guid>43261761</guid>
            <pubDate>Wed, 05 Mar 2025 02:04:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.com/news/2025/03/04/gsa-sell-400-federal-properties-00212071">https://www.politico.com/news/2025/03/04/gsa-sell-400-federal-properties-00212071</a>, See on <a href="https://news.ycombinator.com/item?id=43261761">Hacker News</a></p>
Couldn't get https://www.politico.com/news/2025/03/04/gsa-sell-400-federal-properties-00212071: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Mox ‚Äì modern, secure, all-in-one email server (482 pts)]]></title>
            <link>https://www.xmox.nl/</link>
            <guid>43261729</guid>
            <pubDate>Wed, 05 Mar 2025 01:58:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.xmox.nl/">https://www.xmox.nl/</a>, See on <a href="https://news.ycombinator.com/item?id=43261729">Hacker News</a></p>
Couldn't get https://www.xmox.nl/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Writing an LLM from scratch, part 8 ‚Äì trainable self-attention (355 pts)]]></title>
            <link>https://www.gilesthomas.com/2025/03/llm-from-scratch-8-trainable-self-attention</link>
            <guid>43261650</guid>
            <pubDate>Wed, 05 Mar 2025 01:41:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gilesthomas.com/2025/03/llm-from-scratch-8-trainable-self-attention">https://www.gilesthomas.com/2025/03/llm-from-scratch-8-trainable-self-attention</a>, See on <a href="https://news.ycombinator.com/item?id=43261650">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            

            <div data-current-dropdown="" hx-on="click:
                    if (event.target.closest('.dropdown')) {
                        let targetId = event.target.closest('.dropdown').dataset.target;
                        this.dataset.currentDropdown = (this.dataset.currentDropdown === targetId) ? '' : targetId;
                        event.stopPropagation();
                    }">
                    
                        <p>
                            Archives <span></span>
                        </p>
                    
                    
                        <p>
                            Categories <span></span>
                        </p>
                    
                    <p>
                        Blogroll <span></span>
                    </p>
                </div>

            

    

    

    <p>This is the eighth post in my trek through <a href="https://sebastianraschka.com/">Sebastian Raschka</a>'s book
"<a href="https://www.manning.com/books/build-a-large-language-model-from-scratch">Build a Large Language Model (from Scratch)</a>".
I'm blogging about bits that grab my interest, and things I had to rack my
brains over, as a way
to get things straight in my own head -- and perhaps to help anyone else that
is working through it too.  It's been almost a month since my
<a href="https://www.gilesthomas.com/2025/02/llm-from-scratch-7-coding-self-attention-part-2">last update</a> -- and
if you were suspecting that I was
<a href="https://www.gilesthomas.com/2025/02/blogging-in-the-age-of-ai">blogging about blogging</a> and spending time
getting <a href="https://www.gilesthomas.com/2025/02/adding-maths-to-the-blog">LaTeX working on this site</a> as
procrastination because this next section was always going to be a hard one, then you
were 100% right!  The good news is that -- as so often happens with these things --
it turned out to not be all that tough when I really got down to it.  Momentum
regained.</p>

<blockquote>
  <p>If you found this blog through the blogging-about-blogging, welcome!  Those
  posts were not all that typical, though, and I hope
  you'll enjoy this return to my normal form.</p>
</blockquote>

<p>This time I'm covering section 3.4, "Implementing self-attention
with trainable weights".  How do we create a system that can learn how to interpret
how much attention to pay to words in a sentence, when looking at other words -- for
example, that learns that in "the fat cat sat on the mat", when you're looking at "cat",
the word "fat" is important, but when you're looking at "mat", "fat" doesn't matter
as much?</p>


    
        <p>Before diving into that, especially given the amount of time since the last post,
let's start with the 1,000-foot view of how the GPT-type
decoder-only transformer-based LLMs (hereafter "LLMs" to save me from RSI) work.
For each step I've linked to the posts where I went throught the details.</p>

<ul>
<li>You start off with a string, presumably of words. (<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-2">Part 2</a>)</li>
<li>You split it up into tokens (words like "the", or chunks like "semi"). (<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-2">Part 2</a>)</li>
<li>The job of the LLM is to predict the next token, given all of the tokens in the
string so far. (<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-1">Part 1</a>)</li>
<li>Step 1: map the tokens to a sequence of
vectors called <em>token embeddings</em>.  A particular token,
say, "the", will have a specific embedding -- these start out random but the LLM
works out useful embeddings as it's trained. (<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-3">Part 3</a>)</li>
<li>Step 2: generate another sequence of <em>position embeddings</em> -- vectors of the
same size as the token embeddings, also starting random but trained, that represent
"this is the first token", "this is
the second token", and so on.  (<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-3">Part 3</a>. <sup id="fnref-1"><a href="#fn-1">1</a></sup>)</li>
<li>Step 3: add the two sequences to generate a new sequence of <em>input embeddings</em>.
The first input embedding is the first token embedding plus the first position
embedding (added element-wise), the second is the second token embedding plus the second
position embedding, and so on. (<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-3">Part 3</a>)</li>
<li>Step 4: self-attention.  Take the input embeddings
and for each one, generate a list of <em>attention scores</em>.  These
are numbers that represent how much attention to pay to each other token when considering the token
in question.  So (assuming one token per word) in "the fat cat sat on the mat",
the token "cat" would need a list of 7 attention scores -- how much attention to
pay to the first "the", how much to pay to "fat", how much to pay to itself, "cat",
how much to pay to "sat", and so on.  Exactly how it does that is what this section
of the book covers -- up until now we've been using a "toy" example calculation.
(<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-4">Part 4</a>,
<a href="https://www.gilesthomas.com/2025/01/llm-from-scratch-5-self-attention">Part 5</a>, <a href="https://www.gilesthomas.com/2025/01/llm-from-scratch-6-coding-self-attention-part-1">Part 6</a>,
<a href="https://www.gilesthomas.com/2025/02/llm-from-scratch-7-coding-self-attention-part-2">Part 7</a>).</li>
<li>Step 5: normalise the attention scores to <em>attention weights</em>.  We
want each token's list of attention weights to add up to one -- we do this by running each list through
the <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> function.
(<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-4">Part 4</a>,
<a href="https://www.gilesthomas.com/2025/01/llm-from-scratch-5-self-attention">Part 5</a>, <a href="https://www.gilesthomas.com/2025/01/llm-from-scratch-6-coding-self-attention-part-1">Part 6</a>,
<a href="https://www.gilesthomas.com/2025/02/llm-from-scratch-7-coding-self-attention-part-2">Part 7</a>).</li>
<li>Step 6: generate a new sequence of <em>context vectors</em>.
In the system that we've built so far, this contains, for each token, the sum of multiplying all of the input embeddings
by their respective attention weights and adding the results together.
So in that example above, the context vector for "cat"
would be the input embedding for the first "the" times "cat"'s attention score for
that "the", plus the input embedding for "fat" times "cat"'s attention score for
"fat", and so on for every other token in the sequence.
(<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-4">Part 4</a>,
<a href="https://www.gilesthomas.com/2025/01/llm-from-scratch-5-self-attention">Part 5</a>, <a href="https://www.gilesthomas.com/2025/01/llm-from-scratch-6-coding-self-attention-part-1">Part 6</a>,
<a href="https://www.gilesthomas.com/2025/02/llm-from-scratch-7-coding-self-attention-part-2">Part 7</a>).</li>
</ul>

<p>After all of this is done, we have a sequence of context vectors,
each of which should in some way represent the meaning of its respective token in
the input, including those bits of meaning it gets from all of the other tokens.
So the context vector for "cat" will include some hint of its fatness, for example.</p>

<p>What happens with those context vectors that allows the LLM to use them to predict
what the next token might be?  That bit is still to be explained, so
we'll have to wait and see.  But the first thing to learn is how we create a trainable
attention mechanism that can take the input vectors and generate the attention
scores so that we can work out those context vectors in the first place.</p>

<p>The answer Raschka gives in this section is called <em>scaled dot product attention</em>.
He gives a crystal-clear runthrough of the code to do it, but I had to bang my head
against it for a weekend to get to a solid mental model.
So, instead of going through the
section bit-by-bit, I'll present my own explanation of how it works -- to save me
from future head-banging when trying to remember it, and perhaps to save other people's
foreheads from the same fate.</p>

<h3 id="the-summary-ahead-of-time">The summary, ahead of time</h3>

<p>I'm a <a href="https://www.gilesthomas.com/2011/10/teaching-programming">long-time fan</a> of the Pimsleur
style of language course, where they start each tutorial with minute or so of conversation
in the language you're trying to learn, then say "in 30 minutes, you'll hear that again
and you'll understand it".  You go through the lession, they play the conversation again, and you
do indeed understand it.</p>

<p>So here is a compressed summary of how self-attention works,
in my own words, based on Raschka's explanation.  It might look like a wall of jargon now, but
(hopefully) by the time
you've finished reading this blog post, you'll be able to re-read it and it will all make sense.</p>

<p>We have an input sequence of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math>, of tokens.  We have converted it to a
sequence of input embeddings,
each of which is a vector of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> -- each of these can be treated as a
point in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math>-dimensional
space.  Let's represent that sequence of embeddings with values like this: <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>x</mi><mn>3</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></math>.  Our goal is to produce a
sequence of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math> made up of context vectors, each of which represents the
the meaning of the respective input token in the context of the input as a whole.  These
context vectors will each be of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math> (which in practice is often equal to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math>,
but could in theory be of any length).</p>

<p>We define three matrices, the <em>query weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>, the <em>key weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math>,
and the <em>value weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow></math>. These are made up of trainable
weights; each one of them is sized <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi><mi>√ó</mi><mi>c</mi></mrow></math>.  Because of those dimensions, we
can treat them as operations that project a vector of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> -- a point in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math>-dimensonal
space -- to a vector of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math> -- a point in
a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>-dimensional space.  We will call these projected spaces <em>key space</em>,
<em>query space</em> and
<em>value space</em>.  To convert an input vector <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> into query space, for example, we just
multiply it by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>, like this <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>q</mi><mi>m</mi></msub><mo>=</mo><msub><mi>x</mi><mi>m</mi></msub><msub><mi>W</mi><mi>q</mi></msub></mrow></math>.</p>

<p>When we are considering input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>, we want to work out its <em>attention weights</em> for
every input in the sequence (including itself).  The first step is to work out the <em>attention score</em>,
which, when considering another input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math>, is calculated by taking the dot
product of the projection of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> into query space, and the projection of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> into
key space.  Doing this across all inputs provides us with an attention score
for every other token for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>.  We then divide these by the square root of the
dimensionality of the spaces we are projecting into, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>, and run the resulting
list through the softmax function to make them all add up to one.  This list is the
attention weights for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>.  This process is called <em>scaled dot product attention</em>.</p>

<p>The next step is to generate a context vector for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>.  This is simply the
sum of the projections of all of the inputs into the value space, each one multiplied
by its associated attention weight.</p>

<p>By performing these operations for each of the input vectors, we can generate a list
of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math> made up of context vectors of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>, each of which represents the meaning of a input token in the context of
the input as a whole.</p>

<p>Importantly, with clever use of matrix multiplication, all of this can be done for
all inputs in the sequence, producing a context vector for every one of them,
with just five matrix multiplications and a transpose.</p>

<h3 id="now-lets-explain-it">Now let's explain it</h3>

<p>First things first, if there's anyone there that understood all of that without
already knowing how attention mechanisms work, then I salute you!  It was pretty
dense, and I hope it didn't read like my friend Jonathan's
<a href="https://www.tartley.com/posts/a-guide-to-git-using-spatial-analogies/">parody of incomprehensible guides to using git</a>.
For me, it took eight re-reads of Raschka's (emininently clear and readable)
explanation to get to a level where I felt I understood it.  I think it's also worth noting
that it's very much a "mechanistic" explanation -- it says how we do these calculations
without saying why.  I think that the "why" is actually out of scope for this book,
but it's something that fascinates me, and I'll blog about it soon.  But,
in order to understand the "why", I think we need to have a solid grounding in the
"how", so let's dig into that for this post.</p>

<p>Up until this section of the book, we have been working out the attention scores by taking the dot product
of the input embeddings against each other -- that is, when you're looking
at <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>, the attention score for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> is just <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub><mi>¬∑</mi><msub><mi>x</mi><mi>p</mi></msub></mrow></math>.  I suspected
earlier that the reason that Raschka was using that specific operation for his
"toy" self-attention was that the real implementation is similar, and that has turned
out right, as we're doing scaled dot products here.  But what we do is adjust them first -- <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>, the one that we're considering,
is multiplied by the query weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math> first, and the other one <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> is
multiplied by the key weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math>.  Raschka refers to this as a projection,
which for me is a really nice way to look at it.  But his reference is just in passing,
and for me it needed a bit more digging in.</p>

<h3 id="matrices-as-projections-between-spaces">Matrices as projections between spaces</h3>

<blockquote>
  <p>If your matrix maths is a bit rusty -- like mine was -- and you haven't read the
  <a href="https://www.gilesthomas.com/2025/02/basic-neural-network-matrix-maths-part-1">primer I posted the other week</a>, then
  you might want to check it out now.</p>
</blockquote>

<p>From your schooldays, you might remember that matrices can be used to apply geometric
transformations.  For example, if you take a vector representing a point, you can multiply
it by a matrix to rotate that point about the origin.
You can use a matrix like this to rotate things anti-clockwise by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Œ∏</mi></mrow></math> degrees:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo>[</mo><mtable><mtr><mtd><mi>x</mi></mtd><mtd><mi>y</mi></mtd></mtr></mtable><mo>]</mo><mo>[</mo><mtable><mtr><mtd><mi>cos</mi><mi>Œ∏</mi></mtd><mtd><mo>‚àí</mo><mi>sin</mi><mi>Œ∏</mi></mtd></mtr><mtr><mtd><mi>sin</mi><mi>Œ∏</mi></mtd><mtd><mi>cos</mi><mi>Œ∏</mi></mtd></mtr></mtable><mo>]</mo><mo>=</mo><mo>[</mo><mtable><mtr><mtd><mi>x</mi><mo>.</mo><mi>cos</mi><mi>Œ∏</mi><mo>+</mo><mi>y</mi><mo>.</mo><mi>sin</mi><mi>Œ∏</mi></mtd><mtd><mi>x</mi><mo>.</mo><mo>‚àí</mo><mi>sin</mi><mi>Œ∏</mi><mo>+</mo><mi>y</mi><mo>.</mo><mi>cos</mi><mi>Œ∏</mi></mtd></mtr></mtable><mo>]</mo></mrow></math>

<p>This being matrix multiplication, you could add on more points -- that is, if the
first matrix had more rows, each of which was a point you wanted to rotate, the same
multiplication would rotate them all by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Œ∏</mi></mrow></math>.  So you can see that matrix as
being a function that maps sets of points to their rotated equivalents.  This
works in higher dimensions, too -- a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2</mn><mi>√ó</mi><mn>2</mn></mrow></math> matrix like this can represent
transformations in 2 dimensions, but, for example, in 3d graphics, people
use <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>3</mn><mi>√ó</mi><mn>3</mn></mrow></math> matrices to do similar transformations to the points that make up
3d objects. <sup id="fnref-2"><a href="#fn-2">2</a></sup></p>

<p>An alternative way of looking at this <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2</mn><mi>√ó</mi><mn>2</mn></mrow></math> matrix is that it's a function that
projects points from
one 2-dimensional space to another, the target space being the first space rotated
by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Œ∏</mi></mrow></math> degrees anti-clockwise.  For a simple 2d example like this, or even the
3d ones, that's not
necessarily a better way of seeing it.  It's a philosophical difference rather
than a practical one.</p>

<p>But imagine if the matrix wasn't square --
that is, it had a different number of rows to the number of columns.
If you had a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>3</mn><mi>√ó</mi><mn>2</mn></mrow></math> matrix, it could be used to multiply a matrix of vectors
in 3d space and produce a matrix in 2d space.  Remember the rule for matrix multiplication:
a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>√ó</mi><mn>3</mn></mrow></math> matrix times a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>3</mn><mi>√ó</mi><mn>2</mn></mrow></math> matrix will give you a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>√ó</mi><mn>2</mn></mrow></math> one.</p>

<p>That is actually super-useful;
if you've done any 3d graphics, you might remember the
<a href="https://en.wikipedia.org/wiki/Viewing_frustum">frustum</a> matrix which is used
to convert the 3d points you're working with to 2d points on a screen.  Without
going into too much detail, it allows you to project those 3d points into a 2d
space with a single matrix multiplication.</p>

<p>So: a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi><mi>√ó</mi><mi>c</mi></mrow></math> matrix can be seen as a way to project a vector that represents a
point in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math>-dimensional space into one that represents one in a different <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>-dimensional
space.</p>

<p>What we're doing in self-attention is taking our <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math>-dimensional vectors that make
up the input embedding sequence, then projecting them into three different <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>-dimensional
spaces, and working with the projected versions.  Why do we do this?  That's the
question I want to look into in my future post on the "why", but for now, I think one thing that is fairly
clear is that because these projections are learned as part of the training (remember,
the three matrices we're using for the projections are made up of trainable weights),
it's putting some kind of indirection into the mix that the simple dot product attention
that we were using before didn't have.</p>

<h3 id="how-to-do-the-dot-products-of-the-projected-input-embeddings">How to do the dot products of the projected input embeddings</h3>

<p>Sticking with this mechanistic view -- "how" rather than "why" -- for now,
let's look at the calculations and how matrix multiplication makes them efficient.
I'm going to loosely follow Raschka's
explanation, but using mathematical notation rather than code, as (unusually for me as a career
techie) I found it a bit easier to grasp what's going on that way.</p>

<p>We'll stick with the case where we're
considering token <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> and trying to work out its attention score for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math>.
The first thing we do is project <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> into query space, which we do by
multiplying it by the query weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>q</mi><mi>m</mi></msub><mo>=</mo><msub><mi>x</mi><mi>m</mi></msub><msub><mi>W</mi><mi>q</mi></msub></mrow></math>

<p>Now, let's project <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> into key space by multiplying it by the key weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math>:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>k</mi><mi>p</mi></msub><mo>=</mo><msub><mi>x</mi><mi>p</mi></msub><msub><mi>W</mi><mi>k</mi></msub></mrow></math>

<p>Our attention score is defined as being the dot product of these two vectors:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>œâ</mi><mrow><mi>m</mi><mo>,</mo><mi>p</mi></mrow></msub><mo>=</mo><msub><mi>q</mi><mi>m</mi></msub><mo>.</mo><msub><mi>k</mi><mi>p</mi></msub></mrow></math>

<p>So we could write a simple loop that iterated over all of the inputs <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>.</mo><mo>.</mo><mo>.</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></math> once,
generating the projections into query space for each one, and then inside that
loop iterated over <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>.</mo><mo>.</mo><mo>.</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></math> a second time, projecting them into key space, doing
the dot products, and storing those as attention scores.</p>

<p>But that would be wasteful!  We're doing matrix multiplications, so we can batch
things up.  Let's consider the projections of the inputs into the key space first;
those will always be the same, each time around our hypothetical loop.  So we can
do them in one shot.  Let's treat our input sequence as a matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>X</mi></mrow></math> like this:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo>[</mo><mtable><mtr><mtd><msubsup><mi>x</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>x</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr><mtr><mtd><mo>.</mo><mo>.</mo><mo>.</mo></mtd></mtr><mtr><mtd><msubsup><mi>x</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr></mtable><mo>]</mo></mrow></math>

<p>We have a row for every input embedding in our input
sequence <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow></math>, and so on, with the row being made up of the elements in that embedding.  So it
has <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math> rows, one per element in the input sequence, and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> columns, one for each
dimension in the input embeddings, so it's <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>√ó</mi><mi>d</mi></mrow></math>.  (I'm using <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi><mo>=</mo><mn>3</mn></mrow></math> as
an example here, like Raschka does in the book.)</p>

<p>That's just like our matrix of points in the rotation matrix example above, so
we can project it into key space in one go, just by multiplying it by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math>.  Let's
call the result of that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math>:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub></mrow></math>

<p>It will look like this (again, like Raschka, I'm using a 2-dimensional
key space -- that is, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi><mo>=</mo><mn>2</mn></mrow></math> -- so that it's easy to see whether a matrix is in the
original 3d input embedding space or a 2d projected one):</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo>[</mo><mtable><mtr><mtd><msubsup><mi>k</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>k</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>k</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>k</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr><mtr><mtd><mo>.</mo><mo>.</mo><mo>.</mo></mtd></mtr><mtr><mtd><msubsup><mi>k</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>k</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr></mtable><mo>]</mo></mrow></math>

<p>...where each of those rows is the projection of the input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>n</mi></msub></mrow></math> to key space.
It's just all of the projections stacked on top of each other.</p>

<p>Now, let's think about that dot product -- this bit from earlier:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>œâ</mi><mrow><mi>m</mi><mo>,</mo><mi>p</mi></mrow></msub><mo>=</mo><msub><mi>q</mi><mi>m</mi></msub><mo>.</mo><msub><mi>k</mi><mi>p</mi></msub></mrow></math>

<p>We now have a matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math> containing all of our <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>k</mi><mi>n</mi></msub></mrow></math> values.  When you're doing
a matrix multiplication, the value of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>M</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></math> -- that is, the element at
row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math>, column <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math> in the output matrix -- is the dot product of
row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math> in the first matrix, taken as a vector, with column <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math> in the second
matrix, also considered as a vector.</p>

<p>It sounds like we can make use of that to do all of our dot products in a batch.
Let's treat <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>q</mi><mi>m</mi></msub></mrow></math>, our projection of the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>m</mi></mrow></math>th input token into query space, as
a single-row matrix.  Can we multiply the key matrix by it, like this</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>q</mi><mi>m</mi></msub><mi>K</mi></mrow></math>

<p>...?</p>

<p>Unfortunately not.  <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>q</mi><mi>m</mi></msub></mrow></math> is a one-row matrix (size <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>1</mn><mi>√ó</mi><mi>c</mi></mrow></math>)
and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math> is our <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>√ó</mi><mi>c</mi></mrow></math> key matrix.  With matrix multiplication,
the number of columns in the first matrix -- <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math> in this case -- needs to match
the number of rows in the second, which is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math>.  But, if we transpose K, essentially
swapping rows for columns:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>q</mi><mi>m</mi></msub><msup><mi>K</mi><mi>T</mi></msup></mrow></math>

<p>...then we have a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>1</mn><mi>√ó</mi><mi>c</mi></mrow></math> matrix times a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi><mi>√ó</mi><mi>n</mi></mrow></math> one, which does make sense --
and, even better, it's every dot product for every pair of (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>q</mi><mi>m</mi></msub></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>k</mi><mi>p</mi></msub></mrow></math>) for all values
of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>p</mi></mrow></math> -- that is, with two matrix multiplications -- the one to work out <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math> and this one,
and a transpose, we've worked out all of the attention scores for element <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> in our input
sequence.</p>

<p>But it gets better!</p>

<p>First, let's do the same thing as we did to project the input sequence
into key space to project it all into query space as well.  We
calculated <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub></mrow></math> to work out the key matrix, so we can work out the query matrix the
same way, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>q</mi></msub></mrow></math>.  Just like <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math> was all of the input vectors projected into
key space, "stacked" on top of each other, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math> is all of the input vectors projected
into query space.</p>

<p>Now, what happens if we multiply that by the transposed key matrix?</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow></math>

<p>Well, our <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math> matrix is one row per input, one column per dimension in our projected
space, so it's <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>√ó</mi><mi>c</mi></mrow></math>.  And, as we know, the transposed <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math> matrix
is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi><mi>√ó</mi><mi>n</mi></mrow></math>.  So our result is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>√ó</mi><mi>n</mi></mrow></math> -- and because matrix multiplication
is defined in terms of dot products, what it contains is the dot product of every
row in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math> -- the inputs transformed into query space -- against every column
in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mi>K</mi><mi>T</mi></msup></mrow></math> -- the inputs transformed into key space.</p>

<p>The plan was to generate attention scores by working out exactly those dot products!</p>

<p>So with three matrix multiplications, we've done that:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>q</mi></msub></mrow></math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub></mrow></math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Œ©</mi><mo>=</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow></math>

<p>...where I'm using the capital <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Œ©</mi></mrow></math> to represent a matrix where each row
represents an input in the sequence, and each column within the row represents
an attention weight for that input.  The element <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>Œ©</mi><mrow><mi>m</mi><mo>,</mo><mi>p</mi></mrow></msub></mrow></math> represents how much
attention to pay to the input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> when you are trying to work out the context
vector for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>.  And it has done that by working out the dot product of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> projected
into query space and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> projected into key space.</p>

<p>That's the "dot product" part of "scaled dot product attention" done and dusted :-)</p>

<h3 id="normalising-it">Normalising it</h3>

<p>So we've worked out our attention scores.  The next thing we need to do is normalise
them; in the past we used the softmax function.  This function takes a list and adjusts
the values in it so that they all sum up to 1, but gives a boost to higher numbers and
a deboost to smaller ones.  I imagine it's named "soft" "max" because it's like finding
the maximum, but in a sense softer because it's leaving the other smaller numbers
in there deboosted.</p>

<p>Raschka explains that when we're working with large numbers of dimensions -- in
real-world LLMs, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math> can easily be in the thousands -- using pure softmax
can lead to small gradients -- he says that it can start acting "like a step function",
which I read as meaning that you wind up with all but the largest number in the list
being scaled to really tiny numbers and the largest one dominating.  So, as a workaround,
we divide the numbers by the square root of the number of dimensions in our projected
space <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>, and then only then do we run the result through softmax. <sup id="fnref-3"><a href="#fn-3">3</a></sup></p>

<p>Remember that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Œ©</mi></mrow></math> is a
matrix of attention scores, with one row for each input token, so we need to apply
the softmax function to each row separately.  Here's what we wind up with:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>A</mi><mo>=</mo><mtext>softmax</mtext><mrow><mo stretchy="true" fence="true" form="prefix">(</mo><mfrac><mrow><mi>Œ©</mi></mrow><mrow><msqrt><mrow><mi>c</mi></mrow></msqrt></mrow></mfrac><mo>,</mo><mtext>&nbsp;axis</mtext><mo>=</mo><mn>1</mn><mo stretchy="true" fence="true" form="postfix">)</mo></mrow></mrow></math>

<p>(The <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>a</mi><mi>x</mi><mi>i</mi><mi>s</mi><mo>=</mo><mn>1</mn></mrow></math> isn't really proper mathematical notation, it's just something
I've borrowed from PyTorch to say that we're applying softmax to a matrix on a
per-row basis.)</p>

<p>Once we've done that, we have our normalised attention scores -- that is, the
attention weights.  The next, and final, step, is to use those to work out the context
vectors.</p>

<h3 id="creating-the-context-vectors">Creating the context vectors</h3>

<p>Let's reiterate how we're working out the context vectors.  In the previous toy
example, for each token, we took the input embeddings, multiplied each one by
its attention weight, summed the results element-wise, and that was the result.
Now we're doing the same thing, but projecting the input embeddings into another
space first -- the value space.  So let's start off by doing that projection as
a simple matrix multiplication, just like we did for the other spaces:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>v</mi></msub></mrow></math>

<p>Now, from above we have our attention weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>A</mi></mrow></math>, which has in row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>m</mi></mrow></math> the attention
weights for every token in the input sequence for input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> -- that is,
at <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>A</mi><mrow><mi>m</mi><mo>,</mo><mi>p</mi></mrow></msub></mrow></math> we have the
attention weight for input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>p</mi></mrow></math> when we're working out the context vector for
input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>m</mi></mrow></math>.  That means that for our input sequence of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math>, it's
an <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>√ó</mi><mi>n</mi></mrow></math> matrix.</p>

<p>In our value matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>V</mi></mrow></math>, we also have one row per input.  The values in row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>m</mi></mrow></math>, treated
as a vector, are the projection of the input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> into value space.  So it's
an <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>√ó</mi><mi>c</mi></mrow></math> matrix.</p>

<p>What happens if we do the matrix multiplication</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>A</mi><mi>V</mi></mrow></math>

<p>...?  We'll get a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>√ó</mi><mi>c</mi></mrow></math> matrix of some kind, by the rules of matrix multiplication,
but what will it mean?</p>

<p>To reiterate, the rule for matrix multiplication is that the value of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>M</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></math> -- that is, the element at
row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math>, column <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math> in the output matrix -- is the dot product of
row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math> in the first matrix, taken as a vector, with column <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math> in the second
matrix, also considered as a vector.</p>

<p>So, at position <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math> -- first row, first column, we have the dot product of the first row in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>A</mi></mrow></math> -- the
attention weight for every token in the input sequence when we're considering the
first token -- and the first column in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>V</mi></mrow></math>, which is the first element of each
input embedding, projected into the value space.  So, that is the first element
of each input embedding times the attention weights for the first token.  Or,
in other words, it's the first element of the context vector for the first token!</p>

<p>At position <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math> -- first row,
second column -- we'll have the same calculation, but for the second element of each
input embedding.  That is the second element of the context vector for the first
token.</p>

<p>...and so on for the rest of the columns.  By the end of the first row,
we'll have something that (treated as a vector) is the sum of all of the input
embeddings, multiplied by the weights for the first input.  It's our context vector
for that input!</p>

<p>The same, of course, repeats for each row.  The result of that single matrix multiplication
is a matrix where the row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>m</mi></mrow></math> is the context vector for input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>.</p>

<p>We're done!</p>

<h3 id="bringing-it-all-together">Bringing it all together</h3>

<p>Let's put together those steps.  We start with our input matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>X</mi></mrow></math>, which is the
input embeddings we generated earlier for our sequence of tokens of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math>.  Each
row is an embedding, and there are <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> columns, where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> is the dimensionality of
our embeddings.</p>

<p>We also have our weight matrices to map input embeddings into different
spaces: the <em>query weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>, the <em>key weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math>,
and the <em>value weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow></math>.</p>

<p>So, we project our input matrix into those spaces with three matrix multiplications:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>q</mi></msub></mrow></math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub></mrow></math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>v</mi></msub></mrow></math>

<p>...to get our query matrix, our key matrix, and our value matrix.</p>

<p>We then calculate
our attention scores with one further matrix multiplication and a transpose to work out the dot
products:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Œ©</mi><mo>=</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow></math>

<p>We normalise those to attention weights by scaling them by the square root of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>
and then applying softmax:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>A</mi><mo>=</mo><mtext>softmax</mtext><mrow><mo stretchy="true" fence="true" form="prefix">(</mo><mfrac><mrow><mi>Œ©</mi></mrow><mrow><msqrt><mrow><mi>c</mi></mrow></msqrt></mrow></mfrac><mo>,</mo><mtext>&nbsp;axis</mtext><mo>=</mo><mn>1</mn><mo stretchy="true" fence="true" form="postfix">)</mo></mrow></mrow></math>

<p>...and then we use one final matrix multiplication to use that to work out the
context vectors:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>C</mi><mo>=</mo><mi>A</mi><mi>V</mi></mrow></math>

<p>And that's our self-attention mechanism :-)</p>

<p>Now, if you
<a href="#the-summary-ahead-of-time">go back to the explanation at the start</a>, then
hopefully it will make sense.</p>

<h3 id="back-to-the-book">Back to the book</h3>

<p>Section 3.4 in the book works through the above with PyTorch code, and comes out
with a nice simple <code>nn.Module</code> subclass that does exactly those matrix operations.
This is then improved -- the first version uses generic <code>nn.Parameter</code> objects for
the three weight matrices, and the second uses <code>nn.Linear</code> for more effective training.
That side of it was reasonably easy to understand.  And so, we've wrapped up what I
think is the hardest part of "Build a Large Language Model (from scratch)":
implementing self-attention
with trainable weights.</p>

<h3 id="next-steps">Next steps</h3>

<p>The remainder of chapter 3 is much easier now that we're over
this hump.  We'll be going through two things:</p>

<ul>
<li>Causal self-attention (which means that when we are looking at a given token, we don't pay any attention to later
ones, just like we humans do when reading -- our language is structured so that you
don't normally need to read forward to understand what a word means [except <a href="https://faculty.georgetown.edu/jod/texts/twain.german.html">in German</a> ;-]).</li>
<li>Multi-head attention (which isn't as complex an issue as I thought it was when I first read about it).</li>
</ul>

<p>So I think
I'll probably blog about those first, and then circle back to the "why" of this
form of self-attention.  It's pretty amazing that we can do all of this
-- projecting into differently-dimensioned spaces, taking dot products between
every token's input embeddings in those spaces, and weighting the projected input
tokens by the weights we generate -- with just five matrix multiplications.  But
why do we do that specifically?</p>

<p>The names of the matrices used -- query, key and value -- hint at
the roles they play in a metaphorical way; Raschka says in a sidebar that
it's a nod to information retrieval systems like databases.  However, it's different
enough to how DBs actually work that I can't quite make the connection.  I'm sure
it will come with time, though.</p>

<p>I also want to, probably in a separate post, consider what batches do to all of this.
With <a href="https://www.gilesthomas.com/2025/02/basic-neural-network-matrix-maths-part-1">normal neural networks</a>,
all of our activations when considering a given input are single-row or -column
matrices (depending on the ordering of our equations).  Extending to batches
just means moving to normal multi-row, multi-column matrices.</p>

<p>But ever since
we introduced the matrix of attention scores <a href="https://www.gilesthomas.com/2025/01/llm-from-scratch-6-coding-self-attention-part-1">for the first time</a>,
it's been clear that even with a single input sequence going through our LLM, we're
already using full matrices.  How do we handle batches where we're processing
multiple input sequences in parallel?  It seems that we're going to need to use
some kind of higher-order tensors -- if scalars are order zero tensors, vectors are
order one tensors, and matrices are order two tensors, we're going to need to start
considering order three tensors at least.  That will require a bit of thought!</p>

<p>But for now, that's all -- see you next time!  And please do comment below --
any thoughts, questions or suggestions would be very welcome, of course, but even
if you just found this post useful it would be great to know :-)</p>



    

    
        
    

    



            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Best Buy and Target CEOs say prices are about to go up because of tariffs (163 pts)]]></title>
            <link>https://www.theverge.com/news/624254/best-buy-target-raise-prices-tariffs</link>
            <guid>43261626</guid>
            <pubDate>Wed, 05 Mar 2025 01:36:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/news/624254/best-buy-target-raise-prices-tariffs">https://www.theverge.com/news/624254/best-buy-target-raise-prices-tariffs</a>, See on <a href="https://news.ycombinator.com/item?id=43261626">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><img alt="Emma Roth" data-chromatic="ignore" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195810/EMMA_ROTH.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=48 1x, https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195810/EMMA_ROTH.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96 2x" src="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195810/EMMA_ROTH.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96"></p><p><a href="https://www.theverge.com/authors/emma-roth">Emma Roth</a> <span>is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.</span></p></div><div id="zephr-anchor"><p>Target and Best Buy say Trump‚Äôs tariffs on Mexico, Canada, and China could raise prices in their stores as soon as this week. <a href="https://www.cnbc.com/2025/03/04/trump-mexico-tariffs-will-raise-produce-prices-target-ceo-cornell-says.html">During an interview with CNBC</a>, Target CEO Brian Cornell said consumers will ‚Äúlikely see prices increase over the next couple of days,‚Äù while Best Buy CEO Corie Barry <a href="https://www.cnbc.com/2025/03/04/best-buy-bby-q4-2025-earnings.html">similarly told investors</a> that more expensive prices are ‚Äúhighly likely.‚Äù</p><p>Cornell told CNBC that half of Target‚Äôs goods come from the United States, but the company depends on Mexico for ‚Äúa significant amount‚Äù of fruits and vegetables during winter, potentially leading to more expensive strawberries, bananas, and avocados. ‚ÄúThose are categories where we‚Äôll try to protect pricing, but the consumer will likely see price increases over the next couple of days,‚Äù Cornell added.</p><p>Meanwhile, Best Buy‚Äôs Barry <a href="https://go.skimresources.com/?id=1025X1701640&amp;xs=1&amp;url=https%3A%2F%2Fwww.fool.com%2Fearnings%2Fcall-transcripts%2F2025%2F03%2F04%2Fbest-buy-bby-q4-2025-earnings-call-transcript%2F" rel="sponsored">said during an earnings</a> call that China and Mexico remain the top two countries where the company gets its products. ‚ÄúWe expect our vendors across our entire assortment will pass along some level of tariff costs to retailers, making price increases for American consumers highly likely,‚Äù Barry said.</p><p>On Tuesday, <a href="https://www.theverge.com/news/623403/trump-imposes-tariffs-mexico-canada-china">Trump followed through on threats</a> to impose 25 percent tariffs on products imported from Canada and Mexico, while imports from China will face an additional 10 percent tax on top of the 10 percent tax previously enacted. However, Commerce Secretary Howard Lutnick told Fox Business that Trump <a href="https://www.bloomberg.com/news/articles/2025-03-04/lutnick-says-trump-considering-some-mexico-canada-tariff-relief?utm_source=website&amp;utm_medium=share&amp;utm_campaign=twitter">might ‚Äúwork something out‚Äù with Canada and Mexico</a>, adding that he could announce a potential compromise on Wednesday.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tmux ‚Äì The Essentials (2019) (129 pts)]]></title>
            <link>https://davidwinter.dev/2019/03/14/tmux-the-essentials</link>
            <guid>43261600</guid>
            <pubDate>Wed, 05 Mar 2025 01:31:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://davidwinter.dev/2019/03/14/tmux-the-essentials">https://davidwinter.dev/2019/03/14/tmux-the-essentials</a>, See on <a href="https://news.ycombinator.com/item?id=43261600">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header><img src="https://davidwinter.dev/me.4393b227d64b6ec018f390d273e19d38fd10ff51775a286566dfe659b4adfe33.jpg" alt="Photo of David Winter"><h3><a href="https://davidwinter.dev/">david winter</a></h3><ul><li><a href="https://davidwinter.dev/about/">About</a></li><li><a href="https://davidwinter.dev/index.xml">RSS</a></li><li><a href="https://mastodon.social/@davidwinter">Mastodon</a></li><li><a href="https://github.com/davidwinter">GitHub</a></li></ul></header><section><article><p>Tmux is a great tool for managing multiple terminal sessions and layouts. You can disconnect from a tmux session and then reconnect to it later and carry on where you left off. There is a vast amount you can configure with tmux, and many commands that can be used, and at first it can be intimidating to learn and some may fear a steep learning curve. But knowing only a handful of essential commands is enough to be productive day-to-day. I‚Äôve been using tmux for the past two years or so and haven‚Äôt really ventured beyond these, and still, have found plenty of value out of it.</p><h2 id="sessions">Sessions</h2><p>If you‚Äôve never run tmux before and have no open sessions, just type:</p><p>This will open a new window with a single pane filling the entire screen.</p><p>One of the features of tmux is the ability to detach from a running tmux session and then be able to reconnect to it at a later time with everything being as you‚Äôd left it.</p><p>To detach from the new session you‚Äôve just created type <code>ctrl</code> + <code>b</code> and then <code>d</code>. You will now have been returned to your original command prompt.</p><p>You can see the tmux session that you were just connected to by running:</p><p>To reattach to it, type:</p><h2 id="tmux-prefix-command">Tmux prefix command</h2><p>Let‚Äôs talk about the tmux prefix command; <code>ctrl</code> + <code>b</code>. All commands you run within a tmux session are initiated with this. You press this combination first, and then after, the command you want to run. From here on, we‚Äôll reference the prefix with <code>prefix</code> rather than the full <code>ctrl</code> + <code>b</code>.</p><h2 id="windows-and-panes">Windows and panes</h2><p>When within tmux there are two basic concepts that you need to know about if you‚Äôre going to use it productively; windows and panes. You can think of windows as tabs within a tmux session. They are listed along the bottom of the tmux window. Each window can have multiple panes, which are different terminal prompts split across the window in various layouts.</p><h3 id="windows">Windows</h3><p>Each window is prefixed with a number to represent it. Type <code>ctrl</code> + <code>b</code> and then <code>c</code> to create a new window. You‚Äôll see it appear in the window list and it has been selected. You now have two windows, one represented by <code>0</code> and the other by <code>1</code>.</p><p>You can switch between windows by typing <code>prefix</code> + <code>0</code> to jump to the first window. Just use the number index to choose which window you want to switch to.</p><p>When you start creating lots of windows, it becomes hard to remember what is within each. By default each window will be named with the default shell it is running. To rename, type <code>prefix</code> + <code>,</code> and then enter a new name, then hit enter. You‚Äôll see the name reflected in the window list displayed along the bottom of the screen.</p><h3 id="panes">Panes</h3><p>Within window 0, let‚Äôs create some panes. Type <code>prefix</code> + <code>%</code> to split the current window into two vertical panes. You‚Äôll notice the right-hand pane currently has the terminal focus.</p><p>Type <code>prefix</code> + <code>q</code> and the number for each pane will briefly appear on the screen. This is how you will navigate between panes. The numbers will disappear after a second or so. Just push the combination again <code>prefix</code> + <code>q</code> and then type <code>0</code> immediately to switch focus back to the leftmost pane.</p><p>Now go back to the rightmost pane with <code>prefix</code> + <code>q</code> and then <code>1</code>. We can split this pane in two horizontally by pressing <code>prefix</code> + <code>"</code>.</p><p>Press <code>prefix</code> + <code>q</code> again and you can now see we have three panes indexed with 0‚Äî2.</p><p>Sometimes it‚Äôs very helpful to <em>zoom in</em> on a single pane. A good example is when you want to copy and paste the contents of a pane that go over more than one line. To zoom press <code>prefix</code> + <code>z</code>. You‚Äôll notice the pane expands to fill the entire window. Also a <code>Z</code> will appear in the window list along the bottom. To unzoom, just press <code>prefix</code> + <code>z</code> again.</p><h2 id="summary-of-commands">Summary of commands</h2><ul><li><code>tmux</code> create a new tmux session</li><li><code>tmux ls</code> list any existing tmux sessions</li><li><code>tmux a</code> reattach to the last open tmux session</li><li><code>ctrl</code> + <code>b</code> the default tmux command prefix</li><li><code>prefix</code> + <code>d</code> detach from current tmux session</li><li><code>prefix</code> + <code>c</code> create a new window</li><li><code>prefix</code> + <code>0</code>-<code>9</code> to switch to the numbered window</li><li><code>prefix</code> + <code>,</code> rename the existing window</li><li><code>prefix</code> + <code>%</code> split the current pane into two vertical panes, left and right</li><li><code>prefix</code> + <code>"</code> split the current pane into two horizontal panes, top and bottom</li><li><code>prefix</code> + <code>q</code> view numbered panes for current window</li><li><code>prefix</code> + <code>q</code>, <code>0</code>-<code>9</code> switch to pane immediately after displaying pane numbers</li><li><code>prefix</code> + <code>z</code> to zoom and unzoom</li></ul></article></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Dead Planet Theory (117 pts)]]></title>
            <link>https://arealsociety.substack.com/p/the-dead-planet-theory</link>
            <guid>43261327</guid>
            <pubDate>Wed, 05 Mar 2025 00:45:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arealsociety.substack.com/p/the-dead-planet-theory">https://arealsociety.substack.com/p/the-dead-planet-theory</a>, See on <a href="https://news.ycombinator.com/item?id=43261327">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>Everyone loves to talk about the </span><a href="https://en.wikipedia.org/wiki/Dead_Internet_theory" rel="">Dead Internet Theory</a><span>, but less often discussed is how few people ‚Äúdo things‚Äù in any venue or on any platform. This phenomenon is known by several names, including </span><a href="https://dlab.berkeley.edu/news/explaining-80-20-rule-pareto-distribution#:~:text=He%20famously%20observed%20that%2080,alpha%E2%80%9D)%20and%20Xm." rel="">the Power Law, the Pareto Principle, and the 80-20 Rule</a><span>. One example of this phenomenon is that </span><a href="https://thesocialshepherd.com/blog/twitter-statistics" rel="">10% of Twitter users account for 92% of tweets</a><span>. This dynamic can be seen in interpersonal relationships, hobbies, and careers. You can use this to quickly rise to the top, or purely to get a little more enjoyment out of the things you do day to day. In the scope of all of creation it can be hard to see the impact of this principle in action, but by separating things, events, and people by category and interest it quickly becomes apparent.</span></p><div><figure><a target="_blank" href="https://rivalstracker.com/ranks" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png" width="1456" height="887" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:887,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:962323,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:&quot;https://rivalstracker.com/ranks&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://arealsociety.substack.com/i/157095326?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>A great way to visualize how few people </span><a href="https://arealsociety.substack.com/p/what-it-means-to-play-the-game?r=99bhj" rel="">participate in life</a><span> is by looking at rank distributions of competitive games. In Marvel Rivals, every player is initially Bronze 3 and ranks up from there. Almost everyone who actually plays the competitive mode of the game will rank up no matter how bad they are at the game, so we can see that over 30% of the playerbase has never even played the competitive mode. Simply by playing a competitive match, you are ranked in the top 70% of the playerbase. We can look at other activities and interests, such as film, to further reinforce this point.</span></p><div><figure><a target="_blank" href="https://x.com/michaelcurzi/status/1799748650589208624?t=kGJiVbk8fWMQfKxHKxT1eQ&amp;s=19" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png" width="1176" height="516" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:516,&quot;width&quot;:1176,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:105664,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:&quot;https://x.com/michaelcurzi/status/1799748650589208624?t=kGJiVbk8fWMQfKxHKxT1eQ&amp;s=19&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://arealsociety.substack.com/i/157095326?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png&quot;,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>In all fields, </span><a href="https://arealsociety.substack.com/p/how-i-learned-to-stop-worrying-and-love-the-sort?r=99bhj" rel="">the best quickly climb to the top</a><span>, so it doesn‚Äôt take that long to identify who they are, and to find out what they are doing. I'm mostly focusing on the start of the journey, but things get truly exceptional if you can show long term consistency. Just doing things in general is admirable, but intentional training is the true starting point.</span></p><div><figure><a target="_blank" href="https://x.com/alz_zyd_/status/1828909458480632028?t=wqslJNqSQ4uAxLet1sx0LA&amp;s=19" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png" width="1186" height="376" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:376,&quot;width&quot;:1186,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:80121,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:&quot;https://x.com/alz_zyd_/status/1828909458480632028?t=wqslJNqSQ4uAxLet1sx0LA&amp;s=19&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://arealsociety.substack.com/i/157095326?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><a href="https://x.com/alz_zyd_/status/1828909458480632028?t=wqslJNqSQ4uAxLet1sx0LA&amp;s=19" rel="">The thread above</a><span> has several great examples, and a common element is that plenty of these are activities that many people do, but few people train. Chess is a great demonstration. On chess.com, you can learn about move-sets by playing, as the app won‚Äôt let you make an invalid move. Many people play chess, but few people train chess. Just by spending an hour learning a single opening, you can be substantially better at chess than someone who has played chess for years without ever training. This leads into my next point: no one does the reading.</span></p><div><figure><a target="_blank" href="https://x.com/patio11/status/1800163389630824726?t=6UHrJNcaMzNGy5cPTwyAvg&amp;s=19" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png" width="1184" height="668" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:668,&quot;width&quot;:1184,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:166961,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:&quot;https://x.com/patio11/status/1800163389630824726?t=6UHrJNcaMzNGy5cPTwyAvg&amp;s=19&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://arealsociety.substack.com/i/157095326?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>When I was working in tech, I noticed a particular API parameter would reflect any input, and I wanted to know why it was doing that if it was supposed to be a simple identifier. I asked several coworkers before I found someone who knew, and we went into a meeting room where he walked me through all the different service calls that interacted with that identifier, and the reasoning behind it being dynamic. He told me that it had been that way for years, and was mostly unused these days. He also told me I was the first person that had asked about it in 2 years. I ultimately found a major vulnerability in our logging because of this innocuous identifier most people ignored.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg" width="640" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:640,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;1913: When Hitler, Trotsky, Tito, Freud and Stalin all lived together in  Vienna : r/MapPorn&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="1913: When Hitler, Trotsky, Tito, Freud and Stalin all lived together in  Vienna : r/MapPorn" title="1913: When Hitler, Trotsky, Tito, Freud and Stalin all lived together in  Vienna : r/MapPorn" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This brings me to my main point, actual actors are rare and tend to coexist. If I were to write a fictional story, and set this many characters of such magnitude in the same city at the same time, people would think the premise was absurd. The reality is you will see this play out in countless ways with countless groups. A common example is a simple friend group. You generally have core members, who are core not because they are more attractive or popular (though to be honest this is sometimes the case), but because they actually show up. When I was organizing frequent events, as long as I could convince 2 or 3 other people to go to an event, I would often get 10 to 15 people actually showing up. There would be an inner circle of 3 or 4 event planners, an additional 3 or so consistent attendees, and 20+ people that would rotate in and out. From the outside it may look like 5 people were more popular than than the others, but the reality is that without that core, the other 20+ rotators would just never hang out with each other.</p><p>From a career perspective there are plenty of ways to ‚Äúdo the reading‚Äù or to get ahead. A boomer classic is to speak to a manager and give a firm handshake, the seed of good advice is still there. By being willing to apply to jobs, or studying and practicing for interviews, or asking for a promotion, or by researching salary and negotiating well, you can quickly and substantially elevate yourself. An important thing to remember is that if you want other people to do things for or with you, think about their perspective or challenges and make it as easy as possible for the other person to help you. </p><p><span>A work example is if your job uses a ‚Äú</span><a href="https://staffeng.com/guides/promo-packets/" rel="">promotion packet</a><span>‚Äù, which is common in tech, you can literally write your own promotion packet and get your manager to sign it for you. Normally after a couple years at a big tech job your manager will ask you if you want to go for a promotion. You‚Äôll then spend the next year increasing your scope and taking on more complex projects. If that year goes well, in your next performance evaluation your manager will say he plans to promote you in the next six months to a year. He will then start reaching out to other managers or individual contributors you worked with over the year. He‚Äôll get feedback from each person, and all of your feedback providers will often have packed schedules and take weeks if not months to provide their feedback. Your manager will then compile said feedback, summaries of your projects and achievements, and your performance reviews into a document. This is your promotion packet. Wanting to push for promotion is already rare, and going so far as to write your own packet makes it way more probable that you get the promotion you want, and it can potentially push your promotion forward by years.</span></p><p><span>An added benefit to doing things, or being in the arena in general, is that by </span><a href="https://arealsociety.substack.com/p/what-it-means-to-play-the-game?r=99bhj" rel="">participating in the game</a><span> you enable luck. If you never leave your apartment you can‚Äôt have a serendipitous run-in with your future spouse. By entering the ranks of the doers, things can happen to you as well. If you don‚Äôt apply to a job you don‚Äôt 100% meet the requirements for, they aren‚Äôt going to email you a ‚Äúsorry we missed your application‚Äù, they‚Äôll go on to someone else who isn‚Äôt a perfect match, but was willing to apply. Too many things in life reward action for you to live in a state of stupor. </span></p><p>If you start anywhere, start with simply doing something. For socializing, this can just be showing up when people invite you to something. If you‚Äôve started doing things already, consider ‚Äútraining‚Äù. This can be doing the reading, hosting events yourself, or even literal training. This will allow you to quickly distinguish yourself, and it will make your life substantially more fulfilling. Go forth and remember that with even minimal effort, you can elevate yourself to the category of agentic people and be in that 20% of doers while others observe.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ARC-AGI without pretraining (335 pts)]]></title>
            <link>https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html</link>
            <guid>43259182</guid>
            <pubDate>Tue, 04 Mar 2025 19:52:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html">https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html</a>, See on <a href="https://news.ycombinator.com/item?id=43259182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
  <p><a name="topofpage"></a><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/teaser_figure_w_title.png" alt="image">
By <a href="https://iliao2345.github.io/">Isaac Liao</a> and <a href="https://goombalab.github.io/">Albert Gu</a></p>

  
  <p>In this blog post, we aim to answer a simple yet fundamental question:</p>

  <p><strong>Can lossless information compression by itself produce intelligent behavior?</strong></p>

  <p>The idea that efficient compression by itself lies at the heart of intelligence is not new (see, e.g., <a href="https://www.researchgate.net/publication/2472570_A_Formal_Definition_of_Intelligence_Based_on_an_Intensional_Variant_of_Algorithmic_Complexity">Hern√°ndez-Orallo &amp; Minaya-Collado, 1998</a>; <a href="https://gwern.net/doc/cs/algorithm/information/compression/1999-mahoney.pdf">Mahoney, 1999</a>; <a href="https://link.springer.com/book/10.1007/b138233">Hutter, 2005</a>; <a href="https://arxiv.org/abs/0712.3329">Legg &amp; Hutter, 2007</a>). Rather than revisiting those theoretical discussions, we make a practical demonstration instead.</p>

  <p>In this work, we give evidence that lossless compression during inference time is sufficient to produce intelligent behavior, by developing a method <strong>purely based on compression</strong> that performs well on the <a href="https://arcprize.org/">ARC-AGI challenge</a>, a dataset of IQ-test-like puzzles about inferring a procedure/rule from limited demonstrations. Crucially, our solution, which we name <em>CompressARC</em>, obeys the following three restrictions:</p>

  <ul>
    <li><strong>No pretraining</strong>; models are randomly initialized and trained during inference time.</li>
    <li><strong>No dataset</strong>; one model trains on just the target ARC-AGI puzzle and outputs one answer.</li>
    <li><strong>No search</strong>, in most senses of the word‚Äîjust gradient descent.</li>
  </ul>

  <p>Despite these constraints, CompressARC achieves 34.75% on the training set and 20% on the evaluation set‚Äîprocessing each puzzle in roughly 20 minutes on an RTX 4070. To our knowledge, this is the first neural method for solving ARC-AGI where the training data is limited to just the target puzzle. CompressARC‚Äôs intelligence emerges not from pretraining, vast datasets, exhaustive search, or massive compute‚Äîbut from compression. We challenge the conventional reliance on extensive pretraining and data, and propose a future where tailored compressive objectives and efficient inference-time computation work together to extract deep intelligence from minimal input.</p>

  

  <h2 id="what-is-arc-agi">What is ARC-AGI?</h2>

  <p><a href="https://arcprize.org/">ARC-AGI</a>, <a href="https://arxiv.org/abs/1911.01547">introduced in 2019</a>, is an artificial intelligence benchmark designed to test a system‚Äôs ability to infer and generalize abstract rules from minimal examples. The dataset consists of IQ-test-like puzzles, where each puzzle provides several example images that demonstrate an underlying rule, along with a test image that requires completing or applying that rule. While some have suggested that solving ARC-AGI might signal the advent of <a href="https://arxiv.org/abs/1911.01547">artificial general intelligence</a> (AGI), its true purpose is to spotlight the current challenges hindering progress toward AGI. Below are three of the 1000 puzzles:</p>

  <table>
    <thead>
      <tr>
        <th>Hidden rule: Shift every object to the right by one pixel, except the bottom/right edges of the object.</th>
        <th>Hidden rule: Shrink the big object and set its color to the scattered dots‚Äô color.</th>
        <th>Hidden rule: Extend the green line to meet the red line by turning when hitting a wall.</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/025d127b_problem.png" alt="image"></td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/5ad4f10b_problem.png" alt="image"></td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/2dd70a9a_problem.png" alt="image"></td>
      </tr>
    </tbody>
  </table>

  <p>For every puzzle, there is a hidden rule that maps each input grid to each output grid. You are given some number of examples of input-to-output mappings, and you get <strong>two attempts</strong> to guess the output grid for a given input grid, without being told the hidden rule. If either guess is correct, then you score 1 for that puzzle, else you score 0. You are allowed to change the size of the output grid and pick the color of every pixel. The puzzles are designed so that <strong>humans can reasonably find the answer, but machines should have more difficulty</strong>. <a href="https://arcprize.org/guide">The average human can solve 76.2% of the training set</a>, and <a href="https://arxiv.org/abs/2409.01374">a human expert can solve 98.5%.</a></p>

  <p>The 400 training puzzles are easier than the rest, and are meant to help you learn the following patterns:</p>
  <ul>
    <li><strong>Objectness:</strong> Objects persist and cannot appear or disappear without reason. Objects can interact or not depending on the circumstances.</li>
    <li><strong>Goal-directedness:</strong> Objects can be animate or inanimate. Some objects are ‚Äúagents‚Äù - they have intentions and they pursue goals.</li>
    <li><strong>Numbers &amp; counting:</strong> Objects can be counted or sorted by their shape, appearance, or movement using basic mathematics like addition, subtraction, and comparison.</li>
    <li><strong>Basic geometry &amp; topology:</strong> Objects can be shapes like rectangles, triangles, and circles which can be mirrored, rotated, translated, deformed, combined, repeated, etc. Differences in distances can be detected.</li>
  </ul>

  <p>The ARC Prize team has repeatedly launched competitions for solving ARC-AGI, with monetary rewards. <a href="https://www.kaggle.com/competitions/arc-prize-2024">The most recent competition</a> involved potential prizes and awards of upwards of <strong>$1,000,000</strong>, with the main prize reserved for methods which could achieve 85% on a private test set of 100 puzzles, using 12 hours of compute in a constrained environment.</p>

  

  <h2 id="our-solution-method">Our Solution Method</h2>

  <!--<img align="right" src="./resources/algorithm_environment.JPG" width="50%" style="margin: 20px 0 20px 10px;">-->

  <p><strong>We propose that lossless information compression can serve as an effective framework for solving ARC-AGI puzzles. A more efficient (i.e., lower-bit) compression of a puzzle correlates with a more accurate solution.</strong> To solve ARC-AGI puzzles, we design a system that transforms an incomplete puzzle into a completed one‚Äîfilling in the answers‚Äîby finding a compact representation that, when decompressed, reproduces the puzzle with any solution. The key challenge is to obtain this compact representation without needing the answers as inputs.</p>

  <p>CompressARC uses a neural network as the decoder. However, the encoding algorithm is not another network‚Äîinstead, encoding is realized by the gradient descent algorithm that performs inference-time training on the decoder while maintaining correct decoded output. In other words, running the encoder means optimizing the decoder‚Äôs parameters and input distribution to achieve the most compressed puzzle representation. The resulting optimized parameters (e.g., weights and input distribution settings) themselves serve as the compressed bit representation that encodes the puzzle along with its answer.</p>

  <p>In standard machine learning lingo: (without compression terminology, and with some simplifications)</p>

  <ol>
    <li>We start at inference time, and we are given an ARC-AGI puzzle to solve. (e.g., puzzle in the diagram below.)</li>
    <li>We construct a neural network $f$ (see <a href="#architecture">architecture</a>) designed for the puzzle‚Äôs specifics (e.g., number of examples, observed colors). The network takes random normal input $z \sim N(\mu, \Sigma)$, and per-pixel color logit predictions across all the grids, including an answer grid (3 input-output examples, for a total of 6 grids). Importantly, $f_\theta$ is equivariant to common augmentations‚Äîsuch as reordering input-output pairs (including the answer‚Äôs pair), color permutations, and spatial rotations/reflections.</li>
    <li>We initialize the network weights $\theta$ and set the parameters $\mu$ and $\Sigma$ for the $z$ distribution.</li>
    <li>We jointly optimize $\theta$, $\mu$, and $\Sigma$ to minimize the sum of cross-entropies over the known grids (5 of them,) ignoring the answer grid. A KL divergence penalty keeps $N(\mu, \Sigma)$ close to $N(0,1)$, as in a VAE.</li>
    <li>Since the generated answer grid is stochastic due to the randomness in $z$, we save the answer grids throughout training and choose the most frequently occuring one as our final prediction.</li>
  </ol>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Method_Overview.png"></p>

  <p>It isn‚Äôt obvious why such a method is performing compression. You‚Äôll see later <a href="#how-to-derive-our-solution-method">how we derived it</a> from trying to compress ARC-AGI. First, let‚Äôs see it try to solve the puzzle above.</p>

  <h3 id="watching-the-network-learn-color-the-boxes">Watching the Network Learn: Color the Boxes</h3>

  <h4 id="human-solution">Human Solution:</h4>
  <p>We first realize that the input is divided into boxes, and the boxes are still there in the output, but now they‚Äôre colored. We then try to figure out which colors go in which boxes. First, we notice that the corners are always black. Then, we notice that the middle is always magenta. And after that, we notice that the color of the side boxes depends on which direction they are in: red for up, blue for down, green for right, and yellow for left. At this point, we copy the input over to the answer grid, then we color the middle box magenta, and then color the rest of the boxes according to their direction.</p>

  <h4 id="compressarc-solution">CompressARC Solution:</h4>
  <table>
  <tbody><tr>
  <td>
  <strong> 50 steps of learning:</strong>
  <p>
  CompressARC's network outputs an answer grid (sample) with light blue rows/columns wherever the input has the same. It has noticed that all the other input-output pairs in the puzzle exhibit this correspondence. It doesn't know how the other output pixels are assigned colors; an exponential moving average of the network output (sample average) shows the network assigning mostly the same average color to non-light-blue pixels.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_50_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 150 steps of learning:</strong>
  <p>
  The network outputs a grid where nearby pixels have similar colors. It has likely noticed that this is common among all the outputs, and is guessing that it applies to the answer too.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_150_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 200 steps of learning:</strong>
  <p>
  The network output now shows larger blobs of colors that are cut off by the light blue borders. It has noticed the common usage of borders to demarcate blobs of colors in other outputs, and applies the same idea here. It has also noticed black corner blobs in other given outputs, which the network imitates.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_200_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 350 steps of learning:</strong>
  <p>
  The network output now shows the correct colors assigned to boxes of the correct direction from the center. It has realized that a single color-to-direction mapping is used to pick the blob colors in the other given outputs, so it imitates this mapping. It is still not the best at coloring within the lines, and it's also confused about the center blob, probably because the middle does not correspond to a direction. Nevertheless, the averate network output does show a tinge of the correct magenta color in the middle, meaning the network is catching on.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_350_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 1500 steps of learning:</strong>
  <p>
  The network is as refined as it will ever be. Sometimes it will still make a mistake in the sample it outputs, but this uncommon and filtered out.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_1500_steps.png"></td>
  </tr>
</tbody></table>

  <p>After training, <a href="#solution-analysis-color-the-boxes">we can deconstruct the learned z distribution</a> to find that it codes for a color-direction correspondence table and row/column divider positions!</p>

  

  <h2 id="how-to-derive-our-solution-method">How to Derive Our Solution Method</h2>

  <p>Again, it isn‚Äôt obvious how we get from trying to perform compression to the method we ended up using. The derivation of our algorithm takes us on a detour through <a href="https://en.wikipedia.org/wiki/Information_theory">information theory</a>, <a href="https://arxiv.org/abs/0809.2754">algorithmic information theory</a>, and <a href="https://en.wikipedia.org/wiki/Coding_theory">coding theory</a>, with machine learning only making an appearance near the end.</p>

  <h3 id="a-primer-on-lossless-information-compression">A Primer on Lossless Information Compression</h3>

  <p>In <a href="https://en.wikipedia.org/wiki/Information_theory">information theory</a>, lossless information compression is about trying to represent some information in as few bits as possible, while still being able to reconstruct that information from the bit representation. This type of problem is abstracted as follows:</p>
  <ul>
    <li>A source produces some symbol $x$ from some process that generates symbols from a probability distribution $p(x)$.</li>
    <li>A compressor/encoder $E$ must map the symbol $x$ to a string of bits $s$.</li>
    <li>A decompressor/decoder $D$ must exactly map $s$ back to the original symbol $x$.</li>
  </ul>

  <p>The goal is to use $p$ to construct functions $(E, D)$ which are bit-efficient, (ie. that minimize the expected length of $s$,) without getting any symbols wrong. In our case, the symbol $x$ is the ARC-AGI dataset (many puzzle + answer pairs), and we want to figure out what answers the best compression system might decompress the answers to be. Except, we don‚Äôt have the answers (only the puzzles) to give as input to $E$, and we don‚Äôt know $p$, since it‚Äôs hard to model the intelligent process of puzzle ideation in humans.</p>

  <h3 id="one-size-fits-all-compression">One-Size-Fits-All Compression</h3>

  <p>To build our compression scheme, you might think we need to know what $p$ is, but we argue that it doesn‚Äôt really matter since we can make a one-size-fits-all compressor. It all hinges on the following assumption:</p>
  <blockquote>
    <p>There exists some practically implementable, bit efficient compression system $(E, D)$ for ARC-AGI datasets $x$ sampled from $p$.</p>
  </blockquote>

  <p>If this were false, our whole idea of solving ARC-AGI with compression is doomed even if we knew $p$ anyways, so we might as well make this assumption.</p>

  <p>Our one-size-fits-all compressor $(E‚Äô, D‚Äô)$ is built without knowing $p$, and it is almost just as bit-efficient as the original $(E, D)$:</p>
  <ul>
    <li>$E‚Äô$ observes symbol $x$, picks a program $f$ and input $s$ to minimize $\text{len}(f)+\text{len}(s)$ under the constraint that running the program makes $f(s)=x$, and then sends the pair $(f, s)$.</li>
    <li>$D‚Äô$ is just a program executor that executes $f$ on $s$, correctly producing $x$.</li>
  </ul>

  <p>It is possible to prove with <a href="https://arxiv.org/abs/0809.2754">algorithmic information theory</a> that $(E‚Äô, D‚Äô)$ achieves a bit efficiency at most $\text{len}(f)$ bits worse than the bit efficiency of $(E, D)$, where $f$ is the <em>code for implementing D</em>. But since compression is practically implementable, the code for $D$ should be simple enough for a human engineer to write, so $\text{len}(f)$ must be short, meaning our one-size-fits-all compressor will be close to the best possible bit efficiency.</p>

  <p>Ironically, the only problem with using this to solve ARC-AGI is that implementing $E‚Äô$ is not practical, since $E‚Äô$ needs to minimize the length of a program-input pair $(f, s)$ under partial fixed output constraint $f(s)_{answers}=x_{answers}$.</p>

  <h3 id="neural-networks-to-the-rescue">Neural Networks to the Rescue</h3>

  <p>To avoid searching through program space, we just pick a program $f$ for a small sacrifice in bit efficiency. We hope the diversity of program space can be delegated to diversity in input $s$ space instead. Specifically, we write a program $f$ that runs the forward pass of a neural network, where $s=(\theta, z, \epsilon)$ are the weights, inputs, and corrections to the outputs of the neural network. Then, we can use gradient descent to ‚Äúsearch‚Äù over $s$.</p>

  <p>This restricted compression scheme uses <a href="https://arxiv.org/abs/2010.01185">Relative Entropy Coding</a> (REC)<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">1</a></sup> to encode noisy weights $\theta$ and neural network inputs $z$ into bits $s_\theta$ and $s_z$, and <a href="https://en.wikipedia.org/wiki/Arithmetic_coding">arithmetic coding</a> to encode output error corrections $\epsilon$ into bits $s_\epsilon$, to make a bit string $s$ consisting of three blocks $(s_\theta, s_z, s_\epsilon)$. The compression scheme runs as follows:</p>
  <ul>
    <li>The decoder runs $\theta = \text{REC-decode}(s_\theta)$, $z = \text{REC-decode}(s_z)$, $\text{logits} = \text{Neural-Net}(\theta, z)$, and $x=\text{Arithmetic-decode}(s_\epsilon, \text{logits})$.</li>
    <li>The encoder trains $\theta$ and $z$ to minimize the total code length $\mathbb{E}[\text{len}(s)]$. $s_\epsilon$ is fixed by arithmetic coding to guarantee correct decoding. To calculate the three components of the loss $\mathbb{E}[\text{len}(s)]$ in a differentiable way, we refer to the properties of REC and arithmetic coding:
      <ul>
        <li>It turns out that the $\epsilon$ code length $\mathbb{E}[\text{len}(s_\epsilon)]$ is equal to the total crossentropy error on all the given grids in the puzzle.</li>
        <li>REC requires us to fix some reference distribution $q_\theta$, and also add noise to $\theta$, turning it into a distribution $p_\theta$. Then, REC allows you to store noisy $\theta$ using a code length of $\mathbb{E}[\text{len}(s_\theta)] = KL(p_\theta|| q_\theta) = \mathbb{E}_{\theta \sim p_\theta} [\log (p_\theta(\theta) / q_\theta(\theta))]$ bits. We will choose to fix $q_\theta = N(0, I/2\lambda)$ for large $\lambda$, such that the loss component $\mathbb{E}[\text{len}(s_\theta)] \approx \lambda | \theta|^2 + \text{const}$ is equivalent to regularizing the decoder.</li>
        <li>We must also do for $z$ what we do for $\theta$, since it‚Äôs also represented using REC. We will choose to fix $q_z = N(0,I)$, so the code length of $z$ is $\mathbb{E}[\text{len}(s_z)] = KL(p_z|| q_z) = \mathbb{E}_{z \sim p_z} [\log (p_z(z) / q_z(z))]$.</li>
      </ul>

      <p>We can compute gradients of these code lengths via the <a href="https://arxiv.org/abs/1312.6114">reparameterization trick</a>.</p>
    </li>
  </ul>

  <p>At this point, we observe that the total code length for $s$ that we described is actually the VAE loss with decoder regularization (= KL for $z$ + reconstruction error + regularization). Likewise, if we port the rest of what we described above (plus modifications regarding equivariances and inter-puzzle independence, and ignoring regularization) into typical machine learning lingo, we get the <a href="#our-solution-method">above description of CompressARC</a>.</p>

  

  <h2 id="architecture">Architecture</h2>

  <p>We designed our own neural network architecture for decoding the latents $z$ into ARC-AGI puzzles. The most important feature of our architecture is it‚Äôs equivariances, which are symmetry rules dictating that whenever the input $z$ undergoes a transformation, the output ARC-AGI puzzle must also transform the same way. Some examples:</p>

  <ul>
    <li>reordering of input/output pairs</li>
    <li>shuffling colors</li>
    <li>flips, rotations, and reflections of grids</li>
  </ul>

  <p>There are too many equivariances for us to think about at once, so we decided to make a <strong>base architecture that‚Äôs fully symmetric</strong>, and break unwanted symmetries one by one by <strong>adding asymmetric layers</strong> to give it <a href="#what-puzzles-can-and-cant-we-solve">specific non-equivariant abilities</a>.</p>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Equivariant_Architecture.png"></p>

  <p>To illustrate what we mean, suppose that both $z$ and an ARC-AGI puzzle take the form of a tensor of shape $[n\_examples, n\_colors, height, width, 2 \text{ for input/output}]$ (This is not actually the format of the data (see <a href="#multitensors">multitensors</a>) but it gets the idea across best.) Then, our network starts out as equivariant to permutations of indices in the $example$, $color$, $height$, and $width$ dimensions. Some extra care must be taken with weight sharing, to force the network to also be equivariant to swapping the $width$ and $height$ dimensions. We may then add a layer involving a roll by one in the $width$ and $height$ dimensions, to let the network distinguish short range spatial interactions but not long-range ones.</p>

  <!-- When $z$ is fully symmetrical, then the outputted puzzle must also be fully symmetrical. But notice that [CompressARC](#our-solution-method) is allowed to learn asymmetrical $z$ in order to obtain asymmetrical outputs. Since the $z$ distribution is penalized for deviating from the fully symmetric $N(0,I)$, asymmetrical outputs are discouraged. So, the network must pay a penalty to use $z$ to distinguish the learned roles of two colors, two rows, two pixels, etc. in a puzzle. This makes $z$ naturally lean towards simpler representations of the puzzle. -->

  <p>The actual data ($z$, hidden activations, and puzzles) passing through our layers comes in a format that we call a ‚Äú<strong>multitensor</strong>‚Äù, which is just a bucket of tensors of various shapes. All the equivariances can be described in terms of how they change a multitensor. <strong>In order to understand any of the layers we list, you must first read the below section on multitensors.</strong></p>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Multitensor.png" width="50%"></p>

  <h3 id="multitensors">Multitensors</h3>

  <p>Most common classes of machine learning architectures operate on a single type of tensor with constant rank. LLMs operate on rank 3 tensors of shape $[n\_batch, n\_tokens, n\_channels]$, and CNNs operate on a rank 4 tensors of shape $[n\_batch, n\_channels, height, width]$. Our multitensors are a set of varying-rank tensors of unique type, whose dimensions are a subset of a rank 6 tensor of shape $[n\_examples$, $n\_colors$, $n\_directions$, $height$, $width$, $n\_channels]$. We always keep the $channel$ dimension, so there are at most 32 tensors in every multitensor. We also maintain <a href="#rules-for-legal-multitensors">several rules</a> that determine whether a tensor shape is ‚Äúlegal‚Äù or not, which reduces the number of tensors in a multitensor to 18.</p>

  <table>
    <thead>
      <tr>
        <th>Dimension</th>
        <th>Size</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Example</td>
        <td>Number of examples in the ARC-AGI puzzle, including the one with held-out answer</td>
      </tr>
      <tr>
        <td>Color</td>
        <td>Number of unique colors in the ARC-AGI puzzle, <a href="#number-of-colors">not including black</a></td>
      </tr>
      <tr>
        <td>Direction</td>
        <td>8</td>
      </tr>
      <tr>
        <td>Height</td>
        <td><a href="#output-shape-determination">Determined when preprocessing the puzzle</a></td>
      </tr>
      <tr>
        <td>Width</td>
        <td><a href="#output-shape-determination">Determined when preprocessing the puzzle</a></td>
      </tr>
      <tr>
        <td>Channel</td>
        <td>In the residual connections, the size is 8 if the $direction$ dimension is included, else 16. Within layers it is layer-dependent.</td>
      </tr>
    </tbody>
  </table>

  <p>To give an idea of how a multitensor stores data, an ARC-AGI puzzle can be represented by using the $[examples, colors, height, width, channel]$ tensor, by using the $channel$ dimension to select either the input or output grid, and the $width$/$height$ dimensions for pixel location, a one hot vector in the $color$ dimension, specifying what color that pixel is. The $[examples, width, channel]$ and $[examples, height, channel]$ tensors can similarly be used to store masks representing grid shapes for every example for every input/output grid. All those tensors are included in a single multitensor that is computed by the network just before the final <a href="#linear-heads">linear heads</a> layer.</p>

  <p>When we apply an operation on a multitensor, we by default assume that all non-$channel$ dimensions are treated identically as batch dimensions by default. The operation is copied across the indices of dimensions unless specified. This ensures that we keep all our symmetries intact until we use a specific layer meant to break a specific symmetry.</p>

  <p>A final note on the $channel$ dimension: usually when talking about a tensor‚Äôs shape, we will not even mention the $channel$ dimension as it is included by default.</p>

  <p><strong>The full architecture consists of the following layers, which are each described in the Appendix:</strong></p>
  <ul>
    <li>Begin with parameters of the $z$ distribution,</li>
    <li><a href="#decoding-layer">Decoding Layer</a></li>
    <li>4x
      <ul>
        <li><a href="#multitensor-communication-layer">Multitensor Communication Layer, Upwards</a></li>
        <li><a href="#softmax-layer">Softmax Layer</a></li>
        <li><a href="#directional-cummaxshift-layer">Directional Cummax Layer</a></li>
        <li><a href="#directional-cummaxshift-layer">Directional Shift Layer</a></li>
        <li><a href="#directional-communication-layer">Directional Communication Layer</a></li>
        <li><a href="#nonlinear-layer">Nonlinear Layer</a></li>
        <li><a href="#multitensor-communication-layer">Multitensor Communication Layer, Downwards</a></li>
        <li><a href="#normalization-layer">Normalization Layer</a></li>
      </ul>
    </li>
    <li><a href="#linear-heads">Linear Heads</a></li>
  </ul>

  

  <h2 id="results">Results</h2>

  <h3 id="training-set-3475">Training set: 34.75%</h3>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/accuracy_curve_at_n_training.png">
</p>

  <div>
  <table>
    <tbody><tr>
      <th>Training Iteration</th>
      <th>Time</th>
      <th>Pass@1</th>
      <th>Pass@2</th>
      <th>Pass@5</th>
      <th>Pass@10</th>
      <th>Pass@100</th>
      <th>Pass@1000</th>
    </tr>
    <tr>
      <td>100</td>
      <td>6 h</td>
      <td>1%</td>
      <td>2.25%</td>
      <td>3.5%</td>
      <td>4.75%</td>
      <td>6.75%</td>
      <td>6.75%</td>
    </tr>
    <tr>
      <td>200</td>
      <td>13 h</td>
      <td>11.5%</td>
      <td>14.25%</td>
      <td>16.5%</td>
      <td>18.25%</td>
      <td>23.25%</td>
      <td>23.5%</td>
    </tr>
    <tr>
      <td>300</td>
      <td>19 h</td>
      <td>18.5%</td>
      <td>21.25%</td>
      <td>23.5%</td>
      <td>26.75%</td>
      <td>31.5%</td>
      <td>32.5%</td>
    </tr>
    <tr>
      <td>400</td>
      <td>26 h</td>
      <td>21%</td>
      <td>25%</td>
      <td>28.75%</td>
      <td>31%</td>
      <td>36%</td>
      <td>37.5%</td>
    </tr>
    <tr>
      <td>500</td>
      <td>32 h</td>
      <td>23%</td>
      <td>27.5%</td>
      <td>31.5%</td>
      <td>33.5%</td>
      <td>39.25%</td>
      <td>40.75%</td>
    </tr>
    <tr>
      <td>750</td>
      <td>49 h</td>
      <td>28%</td>
      <td>30.5%</td>
      <td>34%</td>
      <td>36.25%</td>
      <td>42.75%</td>
      <td>44.5%</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>65 h</td>
      <td>28%</td>
      <td>31.75%</td>
      <td>35.5%</td>
      <td>37.75%</td>
      <td>43.75%</td>
      <td>46.5%</td>
    </tr>
    <tr>
      <td>1250</td>
      <td>81 h</td>
      <td>29%</td>
      <td>32.25%</td>
      <td>37%</td>
      <td>39.25%</td>
      <td>45.5%</td>
      <td>49.25%</td>
    </tr>
    <tr>
      <td>1500</td>
      <td>97 h</td>
      <td>29.5%</td>
      <td>33%</td>
      <td>38.25%</td>
      <td>40.75%</td>
      <td>46.75%</td>
      <td>51.75%</td>
    </tr>
    <tr>
      <td>2000</td>
      <td>130 h</td>
      <td>30.25%</td>
      <td>34.75%</td>
      <td>38.25%</td>
      <td>41.5%</td>
      <td>48.5%</td>
      <td>52.75%</td>
    </tr>
  </tbody></table>
</div>

  <h3 id="evaluation-set-20">Evaluation set: 20%</h3>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/accuracy_curve_at_n_evaluation.png">
</p>

  <div>
  <table>
    <tbody><tr>
      <th>Training Iteration</th>
      <th>Time</th>
      <th>Pass@1</th>
      <th>Pass@2</th>
      <th>Pass@5</th>
      <th>Pass@10</th>
      <th>Pass@100</th>
      <th>Pass@1000</th>
    </tr>
    <tr>
      <td>100</td>
      <td>7 h</td>
      <td>0.75%</td>
      <td>1.25%</td>
      <td>2.25%</td>
      <td>2.5%</td>
      <td>3%</td>
      <td>3%</td>
    </tr>
    <tr>
      <td>200</td>
      <td>14 h</td>
      <td>5%</td>
      <td>6%</td>
      <td>7%</td>
      <td>7.75%</td>
      <td>12%</td>
      <td>12.25%</td>
    </tr>
    <tr>
      <td>300</td>
      <td>21 h</td>
      <td>10%</td>
      <td>10.75%</td>
      <td>12.25%</td>
      <td>13.25%</td>
      <td>15.5%</td>
      <td>16.25%</td>
    </tr>
    <tr>
      <td>400</td>
      <td>28 h</td>
      <td>11.75%</td>
      <td>13.75%</td>
      <td>16%</td>
      <td>17%</td>
      <td>19.75%</td>
      <td>20%</td>
    </tr>
    <tr>
      <td>500</td>
      <td>34 h</td>
      <td>13.5%</td>
      <td>15%</td>
      <td>17.75%</td>
      <td>19.25%</td>
      <td>20.5%</td>
      <td>21.5%</td>
    </tr>
    <tr>
      <td>750</td>
      <td>52 h</td>
      <td>15.5%</td>
      <td>17.75%</td>
      <td>19.75%</td>
      <td>21.5%</td>
      <td>22.75%</td>
      <td>25.5%</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>69 h</td>
      <td>16.75%</td>
      <td>19.25%</td>
      <td>21.75%</td>
      <td>23%</td>
      <td>26%</td>
      <td>28.75%</td>
    </tr>
    <tr>
      <td>1250</td>
      <td>86 h</td>
      <td>17%</td>
      <td>20.75%</td>
      <td>23%</td>
      <td>24.5%</td>
      <td>28.25%</td>
      <td>30.75%</td>
    </tr>
    <tr>
      <td>1500</td>
      <td>103 h</td>
      <td>18.25%</td>
      <td>21.5%</td>
      <td>24.25%</td>
      <td>25.5%</td>
      <td>29.5%</td>
      <td>31.75%</td>
    </tr>
    <tr>
      <td>2000</td>
      <td>138 h</td>
      <td>18.5%</td>
      <td>20%</td>
      <td>24.25%</td>
      <td>26%</td>
      <td>31.25%</td>
      <td>33.75%</td>
    </tr>
  </tbody></table>
</div>

  <h3 id="what-puzzles-can-and-cant-we-solve">What Puzzles Can and Can‚Äôt We Solve?</h3>

  <p><strong>CompressARC tries to use its abilities to figure out as much as it can, until it gets bottlenecked by one of it‚Äôs inabilities.</strong></p>

  <p>For example, puzzle 28e73c20 in the training set requires extension of a pattern from the edge towards the middle:</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/28e73c20_problem.png">
</p>

  <p>Given the layers in it‚Äôs network, CompressARC is generally able to extend patterns for short ranges but not long ranges. So, it does the best that it can, and correctly extends the pattern a short distance before guessing at what happens near the center:</p>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/28e73c20_solutions.png" alt="image"></p>

  <p>A short list of abilities that <strong>can</strong> be performed by CompressARC includes:</p>
  <ul>
    <li>Assigning individual colors to individual procedures (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">0ca9ddb6</a>)</li>
    <li>Infilling (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">0dfd9992</a>)</li>
    <li>Cropping (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">1c786137</a>)</li>
    <li>Connecting dots with lines, including 45 degree diagonal lines (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">1f876c06</a>)</li>
    <li>Same color detection (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">1f876c06</a>)</li>
    <li>Identifying pixel adjacencies (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">42a50994</a>)</li>
    <li>Assigning individual colors to individual examples (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">3bd67248</a>)</li>
    <li>Identifying parts of a shape (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">025d127b</a>)</li>
    <li>Translation by short distances (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">025d127b</a>)</li>
  </ul>

  <p>A short list of abilities that <strong>cannot</strong> be performed by CompressARC includes:</p>
  <ul>
    <li>Assigning two colors to each other (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">0d3d703e</a>)</li>
    <li>Repeating an operation in series many times (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">0a938d79</a>)</li>
    <li>Counting/numbers (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">ce9e57f2</a>)</li>
    <li>Translation, rotation, reflections, rescaling, image duplication (see puzzles <a href="#list-of-mentioned-arc-agi-puzzles">0e206a2e</a>, <a href="#list-of-mentioned-arc-agi-puzzles">5ad4f10b</a>, and <a href="#list-of-mentioned-arc-agi-puzzles">2bcee788</a>)</li>
    <li>Detecting topological properties such as connectivity (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">7b6016b9</a>)</li>
    <li>Planning, simulating the behavior of an agent (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">2dd70a9a</a>)</li>
    <li>Long range extensions of patterns (see puzzle 28e73c20 above)</li>
  </ul>

  

  <h2 id="case-study-color-the-boxes">Case Study: Color the Boxes</h2>

  <p>(Additional case studies can be found in the <a href="#additional-case-studies">Appendix</a>.)</p>

  <p>We show the puzzle again for convenience.</p>
  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_problem.png">
</p>

  <p>During training, the reconstruction error fell extremely quickly. It remained low on average, but would spike up every once in a while, causing the KL from $z$ to bump upwards at these moments.</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_KL_vs_reconstruction.png">
</p>

  <h3 id="solution-analysis-color-the-boxes">Solution Analysis: Color the Boxes</h3>

  <p>So how does CompressARC learn to solve the puzzle? Let‚Äôs look at the representations stored in $z$ to find out.</p>

  <p>Since $z$ is a <a href="#multitensors">multitensor</a>, each of the tensors it contains produces an additive contribution to the total KL for $z$. By looking at the per-tensor contributions, we can determine which tensors in $z$ code for information that is used to represent the puzzle. Below is a plot showing the quantity of information stored in each tensor of $z$, ie. the KL contribution used by the <a href="#decoding-layer">decoding layer</a>.</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_KL_components.png">
</p>

  <p>All the tensors fall to zero information content during training, except for four tensors. In some replications of this experiment, we saw one of these four necessary tensors fall to zero information content, and CompressARC typically does not recover the correct answer after that. Here we are showing a lucky run where the $(color, direction, channel)$ tensor almost falls but gets picked up 200 steps in, which is right around when the samples from the model begin to show the correct colors in the correct boxes.</p>

  <p>We can look at the average output of the <a href="#decoding-layer">decoding layer</a> corresponding to individual tensors of $z$, to see what information is stored there. Each tensor contains a vector of dimension $n\_channels$ for various indices of the tensor. Taking the PCA of these vectors reveals some number of activated components, telling us how many pieces of information are coded by the tensor.</p>

  <table>
  <tbody><tr>
  <td>
  <strong> (Examples, height, channel) tensor:</strong>
  <p>
  For every example and row, there is a vector of dimension $n\_channels$. This forms a dataset of vectors. Taking the PCA of these vectors, the top principal component vector reformatted back into an $(examples, height)$ matrix (shown on right) can tell us which examples/row combinations are uniquely identified by the stored information. The top principal component (shown on right) is 1485 times stronger than the second principal component, which indicates to us that basically all of the information is in the above tensor. <strong>For every example, the two brightest pixels give the rows where the light blue rows in the grids are.</strong></p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_example_height_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Examples, width, channel) tensor:</strong>
  <p>
  A very similar story here: in the top principal component of this tensor, <strong>the two darkest pixels for every example give the columns where the light blue columns in the grids are.</strong> The top principal component is 1253 times stronger than the next principal component.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_example_width_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Direction, color, channel) tensor:</strong>
  <p>
  In this tensor, we see that the four brightest pixels identify blue with up, green with left, red with down, and yellow with right. <strong>This tensor seems to tell each direction which color to use for the opposite direction's corresponding box.</strong> The top principal component is 829 times stronger than the next principal component.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_color_direction_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Color, channel) tensor:</strong>
  <p>
  Here, we look at the top three principal components, since the first and second principal components are 134 and 87 times stronger than the third component, indicating that they play a role while the third component does not. The <strong>magenta and light blue colors</strong> are uniquely identified, indicating their special usage amongst the rest of the colors as <strong>the center color and the color of the row/column divisions</strong>, respectively.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_color_component_0.png"><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_color_component_1.png"></td>
  </tr>
</tbody></table>

  

  <h2 id="how-to-improve-our-work">How to Improve Our Work</h2>

  <p>At the time of release of CompressARC, there were several ideas which we thought of trying or attempted at some point, but didn‚Äôt manage to get working for one reason or another. Some ideas we still believe in, but didn‚Äôt use, are listed below.</p>

  <h4 id="joint-compression-via-weight-sharing-between-puzzles">Joint Compression via Weight Sharing Between Puzzles</h4>

  <p>CompressARC tries to solve each puzzle serially by compressing each puzzle on its own. We believe that joint compression of all the entire ARC-AGI dataset at once should yield better learned inductive biases per-puzzle, since computations learned for one puzzle can be transferred to other puzzles. We do not account for the complexity of $f$ in our <a href="#how-to-derive-our-solution-method">method</a>, allowing for $f$ to be used for memorization/overfitting. By jointly compressing the whole dataset, we only need to have one $f$, whereas when compressing each puzzle individually, we need to have an $f$ for every puzzle, allowing for more memorization/overfitting.</p>

  <p>To implement this, we would most likely explore strategies like:</p>
  <ul>
    <li>Using the same network weights for all puzzles, and training for puzzles in parallel. Each puzzle gets assigned some perturbation to the weights, that is constrained in some way, e.g., <a href="https://arxiv.org/abs/2106.09685">LORA</a>.</li>
    <li>Learning a ‚Äúpuzzle embedding‚Äù for every puzzle that is a high dimensional vector (more than 16 dim, less than 256 dim), and learning a linear mapping from puzzle embeddings to weights for our network. This mapping serves as a basic <a href="https://arxiv.org/abs/2306.06955">hypernetwork</a>, ie. a neural network that outputs weights for another neural network.
In a successful case, we might want to also try adding in some form of positional encodings, with the hope that $f$ is now small/simple enough to be incapable of memorization/overfitting using positional encodings.</li>
  </ul>

  <p>The reason we didn‚Äôt try this is because it would slow down the research iteration process.</p>

  <h4 id="convolution-like-layers-for-shape-copying-tasks">Convolution-like Layers for Shape Copying Tasks</h4>

  <p>This improvement is more ARC-AGI-specific and may have less to do with AGI in our view. Many ARC-AGI puzzles can be seen to involve copying shapes from one place to another, and our network has no inductive biases for such an operation. An operation which is capable of copying shapes onto multiple locations is the <a href="https://en.wikipedia.org/wiki/Convolution">convolution</a>. With one grid storing the shape and another with pixels activated at locations to copy to, convolving the two grids will produce another grid with the shape copied to the designated locations.</p>

  <p>There are several issues with introducing a convolutional operation for the network to use. Ideally, we would read two grids via projection from the residual stream, convolve them, and write it back in via another projection, with norms in the right places and such. Ignoring the fact that the grid size changes during convolution (can be solved with two parallel networks using different grid sizes), the bigger problem is that convolutions tend to amplify noise in the grids much more than the sparse signals, so their inductive bias is not good for shape copying. We can try to apply a softmax to one or both of the grids to reduce the noise (and to draw an interesting connection to attention), but we didn‚Äôt find any success.</p>

  <p>The last idea that we were tried before discarding the idea was to modify the functional form of the convolution:</p><p>

\[(f * g)(x) = \sum_y f(x-y)g(y)\]

  </p><p>to <a href="https://arxiv.org/abs/2103.02096">a tropical convolution</a>, which we found to work well on toy puzzles, but not well enough for ARC-AGI training puzzles (which is why we discarded this idea):</p><p>

\[(f*g)(x) = \max_y f(x-y) + g(y)\]

  </p><p>Convolutions, when repeated with some grids flipped by 180 degrees, tend to create high activations at the center pixel, so sometimes it is important to zero out the center pixel to preserve the signal.</p>

  <h4 id="kl-floor-for-posterior-collapse">KL Floor for Posterior Collapse</h4>

  <p>We noticed during testing that crucial posterior tensors whose <a href="https://arxiv.org/abs/1711.00937">KL fell to zero during learning</a> would never make a recovery and play their role in the encoding. We believe that the KL divergence may upper bound the information content of the gradient training signal for parts of the network that process the encoded information. Thus, when a tensor falls to zero KL, the network stops learning to use its information, so the KL is no longer given encouragement to recover. If we can hold the KL above zero for a while, the network may then learn to use the information, giving the KL a reason to stay above zero when released again.</p>

  <p>We implemented a mechanism to keep the KL above a minimum threshold so that the network always learns to use that information, but we do not believe it learns fast enough for this to be useful, as we have never seen a tensor recover before. Therefore, it might be useful to explore different ways to schedule this KL floor to start high and decay to zero, to allow learning when the KL is forced to be high, and to leave the KL unaffected later on in learning. This might cause training results to be more consistent across runs.</p>

  <h4 id="regularization">Regularization</h4>

  <p>We don‚Äôt use it. Maybe it matters, but we don‚Äôt know. Regularization measures the complexity of $f$ in our <a href="#how-to-derive-our-solution-method">problem formulation</a>, and is native to our derivation of CompressARC. It is somewhat reckless for us to exclude it in our implementation.</p>

  

  

  <h4 id="equivalence-of-compression-and-intelligence">Equivalence of Compression and Intelligence</h4>

  <p>The original inspiration of this work came from the <a href="http://prize.hutter1.net/">Hutter Prize</a>, which awards a prize for those who can compress a file of Wikipedia text the most, as a motivation for researchers to build intelligent systems. It is premised upon the idea that the ability to compress information is equivalent to intelligence.</p>

  <p>This equivalence between inteeligence and compression has a long history. For example, when talking about intelligent solutions to prediction problems, the ideal predictor implements <a href="https://www.sciencedirect.com/science/article/pii/S0019995864902232">Solomonoff Induction</a>, a theoretically best possible but uncomputable prediction algorithm that works universally for all prediction tasks. This prediction algorithm is then equivalent to a best possible compression algorithm whose compressed code length is the <a href="https://www.sciencedirect.com/science/article/pii/S0304397598000759?via%3Dihub">Kolmogorov Complexity</a> of the data. In our work, we try to approximate this best possible compression algorithm with a neural network. A related measure of complexity is known as the <a href="https://www.sciencedirect.com/science/article/abs/pii/0005109878900055?via%3Dihub">Minimum Description Length</a>.</p>

  <h4 id="information-theory-and-coding-theory">Information Theory and Coding Theory</h4>

  <p>Since we build an information compression system, we make use of many results in information theory and coding theory. The main result required to motivate our model architecture is the existence of <a href="https://arxiv.org/abs/2010.01185">Relative Entropy Coding</a> (REC). The fact that REC exists means that as long as a KL divergence can be bounded, the construction of a compression algorithm is always possible and the issue of realizing the algorithm can be abstracted away. Thus, problems about coding theory and translating information from Gaussians into binary and back can be ignored, since we can figure out the binary code length directly from the Gaussians instead. In other words, we only need to do enough information theory using the Gaussians to get the job done, with no coding theory at all. While the existence of <a href="https://en.wikipedia.org/wiki/Arithmetic_coding">arithmetic coding</a> would suffice to abstract the problem away when distributions are discrete, neural networks operate in a continuous space so we need REC instead.</p>

  <p>Our architecture sends $z$ information through an additive white Gaussian noise (AWGN) channel, so the <a href="https://en.wikipedia.org/wiki/Shannon%E2%80%93Hartley_theorem">AWGN channel capacity formula</a> (Gaussian input Gaussian noise) plays a heavy role in the design of our <a href="#decoding-layer">decoding layer</a>.</p>

  <h4 id="variational-autoencoders">Variational Autoencoders</h4>

  <p>The decoder side of the <a href="https://arxiv.org/abs/1312.6114">variational autoencoder</a> serves as our decompression algorithm. While we would use something that has more general capabilities like a <a href="https://arxiv.org/abs/1410.5401">neural Turing machine instead</a>, neural Turing machines are not very amenable to gradient descent-based optimization so we stuck with the VAE.</p>

  <p>VAEs have a long history of developments that are relevant to our work. At one point, we tried using multiple <a href="#decoding-layer">decoding layers</a> to make a <a href="https://arxiv.org/abs/1602.02282">hierarchical VAE</a> decoder instead. This does not affect Relative Entropy Coding with the AWGN channel because <a href="https://ieeexplore.ieee.org/document/1056798">channel capacity with feedback is equal to channel capacity without feedback</a>. But, we found empirically that the first decoding layer would absorb all of the KL contribution, making the later decoding layers useless. Thus, we only used one decoding layer at the beginning.</p>

  <p>The <a href="https://openreview.net/forum?id=Sy2fzU9gl">beta-VAE</a> introduces a reweighting of the reconstruction loss to be stronger than the KL loss, and we found that to work well in our case. The <a href="https://arxiv.org/abs/2007.03898">NVAE</a> applies a non-constant weighting to loss components. A rudimentary form of scheduled loss recombination is used in CompressARC.</p>

  <h4 id="arc-agi-methods">ARC-AGI Methods</h4>

  <p>Current methods for solving ARC-AGI focus primarily on using large language models (LLMs). ARC-AGI puzzles are converted into textual representations which are fed into LLMs as input. The LLM may directly output a textual representation of an answer, or some <a href="https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt">code which tries to convert input grids into output grids</a>. Top methods rely heavily on data augmentation and larger <a href="https://arxiv.org/abs/2411.02272">alternative datasets</a>, and sometimes perform autoregressive training on the target puzzle during inference time. Top solutions (<a href="https://ironbar.github.io/arc24/05_Solution_Summary/">example</a>) in the 2024 Kaggle prize competition frequently used <a href="https://arxiv.org/abs/1909.13231">test-time training</a>. <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">Reasoning models</a> have managed to get up to 87.5% on the semi-private evaluation set, albeit with astronomical amounts of compute.</p>

  <p>An older class of methods consists of hard-coded searches through program spaces in <a href="https://github.com/michaelhodel/arc-dsl">hand-written domain-specific languages designed specifically for ARC</a>. Another example <a href="https://github.com/victorvikram/ARC-icecuber">here</a>.</p>

  <p><a href="https://arxiv.org/html/2411.08706v1">Bonnet and Macfarlane introduced a VAE-based method</a> for searching through a latent space of programs.</p>

  <p>We believe CompressARC is the only method so far that uses deep learning without external pretraining nor any large-scale search.</p>

  <h4 id="deep-learning-architectures">Deep Learning Architectures</h4>

  <p>We designed our own neural network architecture from scratch, but not without borrowing crucial design principles from many others.</p>

  <p>Our architecture is fundamentally structured like a <a href="https://arxiv.org/abs/1706.03762">transformer</a>, consisting of a <a href="https://arxiv.org/abs/1512.03385">residual stream</a> where representations are stored and operated upon, followed by a linear head. <a href="https://arxiv.org/abs/2002.04745">Pre-and post-norms</a> with linear up- and down-projections allow layers to read and write to the residual stream. The <a href="https://arxiv.org/abs/1606.08415">SiLU</a>-based <a href="#nonlinear-layer">nonlinear layer</a> is especially similar to a transformer‚Äôs.</p>

  <p>Our equivariance structures are inspired by <a href="https://arxiv.org/abs/1703.06114">permutation-invariant neural networks</a>, which are a type of <a href="https://arxiv.org/abs/1602.07576">equivariant neural network</a>. Equivariance transformations are taken from common augmentations to ARC-AGI puzzles.</p>

  

  <hr>
  

  <h2 id="appendix">Appendix</h2>

  <h3 id="layers-in-the-architecture">Layers in the Architecture</h3>

  <h4 id="decoding-layer">Decoding Layer</h4>

  <p>This layer‚Äôs job is to sample a multitensor $z$ and bound its information content, before it is passed to the next layer. This layer and outputs the KL divergence between the learned $z$ distribution and $N(0,I)$. Penalizing the KL prevents CompressARC from learning a distribution for $z$ that memorizes the ARC-AGI puzzle in an uncompressed fashion, and forces it to represent the puzzle more succinctly. Specifically, it forces CompressARC to spend more bits on the KL whenever it uses $z$ to break a symmetry, and the larger the symmetry group broken, the more bits it spends.</p>

  <p>This layer takes as input:</p>
  <ul>
    <li>A learned target multiscalar, called the ‚Äútarget capacity‚Äù.<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" rel="footnote">2</a></sup> The decoding layer will output $z$ whose information content per tensor is close to the target capacity,<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" rel="footnote">3</a></sup></li>
    <li>learned per-element means for $z$,<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" rel="footnote">4</a></sup></li>
    <li>learned per-element capacity adjustments for $z$.</li>
  </ul>

  <p>We begin by normalizing the learned per-element means for $z$.<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" rel="footnote">5</a></sup> Then, we figure out how much Gaussian noise we must add into every tensor to make the <a href="https://en.wikipedia.org/wiki/Shannon%E2%80%93Hartley_theorem">AWGN channel capacity</a> equal to the target capacity for every tensor (including per-element capacity adjustments). We apply the noise to sample $z$, keeping unit variance of $z$ by rescaling.<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" rel="footnote">6</a></sup></p>

  <p>We compute the information content of $z$ as the KL divergence between the distribution of this sample and $N(0,1)$.</p>

  <p>Finally, we postprocess the noisy $z$ by scaling it by the sigmoid of the signal-to-noise ratio.<sup id="fnref:14" role="doc-noteref"><a href="#fn:14" rel="footnote">7</a></sup> This ensures that $z$ is kept as-is when its variance consists mostly of useful information and it is nearly zero when its variance consists mostly of noise. All this is done 4 times to make a $channel$ dimension of 4. Then we apply a projection (with different weights per tensor in the multitensor, ie. per-tensor projections) mapping the $channel$ dimension up to the dimension of the residual stream.</p>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Multitensor_Sharing.png" width="50%"></p>

  <h4 id="multitensor-communication-layer">Multitensor Communication Layer</h4>

  <p>This layer allows different tensors in a multitensor to interact with each other.</p>

  <p>First, the input from the residual stream passes through per-tensor projections to a fixed size (8 for downwards communication and 16 for upwards communication). Then a message is sent to every other tensor that has at least the same dimensions for upwards communication, or at most the same dimensions for downwards communication. This message is created by either taking means along dimensions to remove them, or unsqueezing+broadcasting dimensions to add them. All the messages received by every tensor are summed together and normalization is applied. This result gets up-projected back and then added to the residual stream.</p>

  <h4 id="softmax-layer">Softmax Layer</h4>

  <p>This layer allows the network to work with internal one-hot representations, by giving it the tools to denoise and sharpen noisy one-hot vectors. For every tensor in the input multitensor, this layer lists out all the possible subsets of dimensions of the tensor to take a softmax over<sup id="fnref:15" role="doc-noteref"><a href="#fn:15" rel="footnote">8</a></sup>, takes the softmax over these subsets of dimensions, and concatenates all the softmaxxed results together in the $channel$ dimension. The output dimension varies across different tensors in the multitensor, depending on their tensor rank. A pre-norm is applied, and per-tensor projections map to and from the residual stream. The layer has input $channel$ dimension of 2.</p>

  <h4 id="directional-cummaxshift-layer">Directional Cummax/Shift Layer</h4>

  <p>The directional cummax and shift layers allow the network to perform the non-equivariant cummax and shift operations in an equivariant way, namely by applying the operations once per direction, and only letting the output be influenced by the results once the directions are aggregated back together (by the <a href="#multitensor-communication-layer">multitensor communication layer</a>). These layers are the sole reason we included the $direction$ dimension when defining a multitensor: to store the results of directional layers and operate on each individually. Of course, this means when we apply a spatial equivariance transformation, we must also permute the indices of the $direction$ dimension accordingly, which can get complicated sometimes.</p>

  <p>The directional cummax layer takes the eight indices of the $direction$ dimension, treats each slice as corresponding to one direction (4 cardinal, 4 diagonal), performs a cumulative max in the respective direction for each slice, does it in the opposite direction for half the channels, and stacks the slices back together in the $direction$ dimension. The slices are rescaled to have min $-1$ and max $1$ before applying the cumulative max.</p>

  <p>The directional shift layer does the same thing, but for shifting the grid by one pixel instead of applying the cumulative max, and without the rescaling.</p>

  <p>Some details:</p>
  <ul>
    <li>Per-tensor projections map to and from the residual stream, with pre-norm.</li>
    <li>Input $channel$ dimension is 4</li>
    <li>These layers are only applied to the $[example, color, direction, height, width, channel]$ and $[example, direction, height, width, channel]$ tensors in the input multitensor.</li>
  </ul>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Directional_Shift_Cummax.png" alt="image"></p>

  <h4 id="directional-communication-layer">Directional Communication Layer</h4>

  <p>By default, the network is equivariant to permutations of the eight directions, but we only want symmetry up to rotations and flips. So, this layer provides a way to send information between two slices in the $direction$ dimension, depending on the angular difference in the two directions. This layer defines a separate linear map to be used for each of the 64 possible combinations of angles, but the weights of the linear maps are minimally tied such that the directional communication layer is equivariant to reflections and rotations. This gets complicated really fast, since the $direction$ dimension‚Äôs indices also permute when equivariance transformations are applied. Every direction slice in a tensor accumulates it‚Äôs 8 messages, and adds the results together.<sup id="fnref:16" role="doc-noteref"><a href="#fn:16" rel="footnote">9</a></sup></p>

  <p>For this layer, there are per-tensor projections to and from the residual stream with pre-norm. The input $channel$ dimension is 2.</p>

  <h4 id="nonlinear-layer">Nonlinear Layer</h4>

  <p>We use a SiLU nonlinearity with $channel$ dimension 16, surrounded by per-tensor projections with pre-norm.</p>

  <h4 id="normalization-layer">Normalization Layer</h4>

  <p>We normalize all the tensors in the multitensor, using means and variances computed across all dimensions except the $channel$ dimension. Normalization as used within other layers also generally operates this way.</p>

  <h4 id="linear-heads">Linear Heads</h4>

  <p>We must take the final multitensor, and convert it to the format of an ARC-AGI puzzle. More specifically, we must convert the multitensor into a distribution over ARC-AGI puzzles, so that we can compute the log-likelihood of the observed grids in the puzzle.</p>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Linear_heads.png" width="50%"></p>

  <p>The colors of every pixel for every example for both input and output, have logits defined by the $[examples, colors, height, width, channel]$ tensor, with the $channel$ dimension linearly mapped down to a size of 2, representing the input and output grids.<sup id="fnref:17" role="doc-noteref"><a href="#fn:17" rel="footnote">10</a></sup> The log-likelihood is given by the crossentropy, with sum reduction across all the grids.</p>

  <p>For grids of non-constant shape, the $[examples, width, channel]$ and $[examples, height, channel]$ tensors are used to create distributions over possible contiguous rectangular slices of each grid of colors. Again, the $channel$ dimension is mapped down to a size of 2 for input and output grids. For every grid, we have a vector of size $[width]$ and a vector of size $[height]$. The log likelihood of every slice of the vector is taken to be the sum of the values within the slice, minus the values outside the slice. The log likelihoods for all the possible slices are then normalized to have total probability one, and the colors for every slice are given by the color logits defined in the previous paragraph.</p>

  <p>With the puzzle distribution now defined, we can now evaluate the log-likelihood of the observed target puzzle, to use as the reconstruction error.<sup id="fnref:18" role="doc-noteref"><a href="#fn:18" rel="footnote">11</a></sup></p>

  <h3 id="other-architectural-details">Other Architectural Details</h3>

  <h4 id="rules-for-legal-multitensors">Rules for legal multitensors</h4>

  <ol>
    <li>At least one non-$example$ dimension must be included. Examples are not special for any reason not having to do with colors, directions, rows, and columns.</li>
    <li>If the $width$ or $height$ dimension is included, the $example$ dimension should also be included. Positions are intrinsic to grids, which are indexed by the $example$ dimension. Without a grid it doesn‚Äôt make as much sense to talk about positions.</li>
  </ol>

  <h4 id="weight-tying-for-reflectionrotation-symmetry">Weight Tying for Reflection/Rotation Symmetry</h4>

  <p>When applying a different linear layer to every tensor in a multitensor, we have a linear layer for tensors having a $width$ but not $height$ dimension, and another linear layer for tensors having a $height$ but not $width$ dimension. Whenever this is the case, we tie the weights together in order to preserve the whole network‚Äôs equivariance to diagonal reflections and 90 degree rotations, which swap the $width$ and $height$ dimensions.</p>

  <p>The softmax layer is not completely symmetrized because different indices of the output correspond to different combinations of dimension to softmax over. Tying the weights properly would be a bit complicated and time consuming for the performance improvement we expect, so we did not do this.</p>

  <h4 id="training">Training</h4>

  <p>We train for 2000 iterations using Adam, with learning rate 0.01, $\beta_1$ of 0.5, and $\beta_2$ of 0.9.</p>

  <h3 id="preprocessing">Preprocessing</h3>

  <h4 id="output-shape-determination">Output Shape Determination</h4>

  <p>The raw data consists of grids of various shapes, while the neural network operates on grids of constant shape. Most of the preprocessing that we do is aimed towards this shape inconsistency problem.</p>

  <p>Before doing any training, we determine whether the given ARC-AGI puzzle follows three possible shape consistency rules:</p>
  <ol>
    <li>The outputs in a given ARC-AGI puzzle are always the same shape as corresponding inputs.</li>
    <li>All the inputs in the given ARC-AGI puzzle are the same shape.</li>
    <li>All the outputs in the given ARC-AGI puzzle are the same shape.</li>
  </ol>

  <p>Based on rules 1 and 3, we try to predict the shape of held-out outputs, prioritizing rule 1 over rule 3. If either rule holds, we force the postprocessing step to only consider the predicted shape by overwriting the masks produced by the <a href="#linear-heads">linear heads</a>. If neither rule holds, we make a temporary prediction of the largest width and height out of the grids in the given ARC-AGI puzzle, and we allow the masks to predict shapes that are smaller than that.</p>

  <p>The largest width and height that is given or predicted, are used as the size of the <a href="#multitensors">multitensor</a>‚Äôs $width$ and $height$ dimensions.</p>

  <p>The predicted shapes are also used as masks when performing the <a href="#multitensor-communication-layer">multitensor communication</a>, <a href="#directional-communication-layer">directional communication</a> and <a href="#directional-cummaxshift-layer">directional cummax/shift</a> layers<sup id="fnref:19" role="doc-noteref"><a href="#fn:19" rel="footnote">12</a></sup>. We did not apply masks for the other layers because of time constraints and because we do not believe it will provide for much of a performance improvement.</p>

  <h4 id="number-of-colors">Number of Colors</h4>

  <p>We notice that in almost all ARC-AGI puzzles, colors that are not present in the puzzle are not present in the true answers. Hence, any colors that do not appear in the puzzle are not given an index in the $color$ dimension of the <a href="#multitensors">multitensor</a>.</p>

  <p>In addition, black is treated as a special color that is never included in the multitensor, since it normally represents the background in many puzzles. When performing color classification, a tensor of zeros is appended to the $color$ dimension after applying the <a href="#linear-heads">linear head</a>, to represent logits for the black color.</p>

  <h3 id="postprocessing">Postprocessing</h3>

  <p>Postprocessing primarily deals with denoising the answers sampled from the network. There are also <a href="#linear-heads">some operations</a> performed to convert the constant-shape grids outputted by the network to the variable shape grids present in some puzzles.</p>

  <p>Generally, when we sample answers from the network by taking the logits of the $[examples, colors, height, width, channels]$ tensor and argmaxxing over the $color$ dimension, we find that the grids are noisy and will often have the wrong colors for several random pixels. We developed several methods for removing this noise:</p>
  <ol>
    <li>Find the most commonly sampled answer.</li>
    <li>Construct an exponential moving average of the output color logits before taking the softmax to produce probabilities. Also construct an exponential moving average of the masks.</li>
    <li>Construct an exponential moving average of the output color probabilities after taking the softmax. Also construct an exponential moving average of the masks.</li>
  </ol>

  <p>When applying these techniques, we always take the slice of highest probability given the mask, and then we take the colors of highest probability afterwards.</p>

  <p>We explored several different rules for when to select which method, and arrived at a combination of 1 and 2 with a few modifications:</p>
  <ul>
    <li>At every iteration, count up the sampled answer, as well as the exponential moving average answer (decay $=0.97$).</li>
    <li>If before 150 iterations of training, then downweight the answer by a factor of $e^{-10}$. (Effectively, don‚Äôt count the answer.)</li>
    <li>If the answer is from the exponential moving average as opposed to the sample, then downweight the answer by a factor of $e^{-4}$.</li>
    <li>Downweight the answer by a factor of $e^{-10*uncertainty}$, where $uncertainty$ is the average (across pixels) negative log probability assigned to the top color of every pixel.</li>
  </ul>

  <h3 id="what-happens-to-the-representations-during-learning">What Happens to the Representations during Learning</h3>

  <p>During training, the gradient descent tries to find representations of the puzzle that require less and less information to encode. This information is measured by the KL term for $z$, plus the a heavily penalized reconstruction error.</p>

  <p>Due to the 10x penalization on reconstruction error, and the initial high capacity for $z$, the $z$ distribution (which we call the ‚Äúposterior‚Äù) quickly learns the information that is required to perfectly reconstruct the given input/output pairs in the puzzle, within the first 20 or so steps. The remainder of the training steps are about compressing $z$ information under the constraint of perfect reconstruction, by tuning the representations to be more concise.</p>

  <p>Our mental model of how gradient descent compresses the $z$ information consists of several steps which we list below:</p>
  <ol>
    <li>Suppose the posterior $p$ originally codes for some number $n$ pieces of information $z_1, \dots, z_n$ using thin Gaussians.</li>
    <li>The posterior widens and becomes more noisy to try to get closer to the wide Gaussian ‚Äúprior‚Äù $q=N(0,1)$, but since all $n$ pieces of information are needed to ensure good reconstruction, the noise is limited by the reconstruction loss incurred.</li>
    <li>The ever-widening posteriors push the neurons to become more and more resilient to noise, until some limit is reached.</li>
    <li>Learning remains stagnant for a while, as a stalemate between compression and reconstruction.</li>
    <li>If it turns out that $z_1$ is not reconstructible using $z_2, \dots, z_n$, then stop. Else, proceed to step 6.</li>
    <li>The neurons, pushed by the widening posterior of $z_1$, figure out a procedure to denoise $z_1$ using information from $z_2, \dots, z_n$, in the event that the noise sample for $z_1$ is too extreme.</li>
    <li>The posterior for the last piece keeps pushing wider, producing more extreme values for $z_1$, and the denoising procedure is improved, until the $z_1$ representation consists completely of noise, and its usage in the network is replaced by the output of the denoising procedure.</li>
    <li>The posterior for $z_1$ is now identical to the prior, so nothing is coded in $z_1$ and it no longer contributes to the KL loss.</li>
    <li>The posterior now codes for $n-1$ pieces of information $z_2, \dots, z_n$, and compression has occurred.</li>
  </ol>

  <p>These steps happen repeatedly for different unnecessarily coded pieces of information, until there are no more. More than one piece of information can be compressed away at once, and there is no need for the steps to proceed serially. The process stops when all information coded by the posterior is unique, and no piece is reconstructable using the others.</p>

  

  <h2 id="additional-case-studies">Additional Case Studies</h2>

  <p>Below, we show two additional puzzles and a dissection of CompressARC‚Äôs solution to them.</p>

  <h3 id="case-study-bounding-box">Case Study: Bounding Box</h3>

  <p>Puzzle 6d75e8bb is part of the training split.</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_problem.png" width="20%">
</p>

  <h4 id="watching-the-network-learn-bounding-box">Watching the Network Learn: Bounding Box</h4>

  <h5 id="human-solution-1">Human Solution:</h5>
  <p>We first realize that the input is red and black, and the output is also red and black, but some of the black pixels are replaced by light blue pixels. We see that the red shape remains unaffected. We notice that the light blue box surrounds the red shape, and finally that it is the smallest possible surrounding box that contains the red shape. At this point, we copy the input over to the answer grid, then we figure out the horizontal and vertical extent of the red shape, and color all of the non-red pixels within that extent as light blue.</p>

  <h5 id="compressarc-solution-1">CompressARC Solution:</h5>
  <table>
  <tbody><tr>
  <td>
  <strong> 50 steps of learning:</strong>
  <p>
  The average of sampled outputs shows that light blue pixels in the input are generally preserved in the output. However, black pixels in the input are haphazardly and randomly colored light blue and red. CompressARC does not seem to know that the colored input/output pixels lie within some kind of bounding box, or that the bounding box is the same for the input and output grids.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_at_50_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 100 steps of learning:</strong>
  <p>
  The average of sampled outputs shows red pixels confined to an imaginary rectangle surrounding the light blue pixels. CompressARC seems to have perceived that other examples use a common bounding box for the input and output pixels, but is not completely sure about where the boundary lies and what colors go inside the box in the output. Nevertheless, guess 2 (the second most frequently sampled output) shows that the correct answer is already being sampled quite often now.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_at_100_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 150 steps of learning:</strong>
  <p>
  The average of sampled outputs shows almost all of the pixels in the imaginary bounding box to be colored red. CompressARC has figured out the answer, and further training only refines the answer.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_at_150_steps.png"></td>
  </tr>
</tbody></table>

  <h4 id="solution-analysis-bounding-box">Solution Analysis: Bounding Box</h4>

  <p>Below is a plot of the amount of contained information in every tensor composing the latent $z$:</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_KL_components.png">
</p>

  <p>All the tensors in $z$ fall to zero information content during training, except for three tensors. From 600-1000 steps, we see the $(example, height, width, channel)$ tensor suffer a massive drop in information content, with no change in the outputted answer. We believe it was being used to identify the light blue pixels in the input, but this information then got memorized by the nonlinear portions of the network, using the $(example, height, channel)$ and $(example, width, channel)$ as positional encodings.</p>

  <p>We can look at the average output of the <a href="#decoding-layer">decoding layer</a> for these tensors to see what information is stored there.</p>

  <table>
  <tbody><tr>
  <td>
  <strong> (Examples, height, channel) tensor:</strong>
  <p>
  The first principal component is 771 times stronger than the second principal component. <strong>A brighter pixel indicates a row with more light blue pixels.</strong> It is unclear how CompressARC knows where the borders of the bounding box are.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_example_height_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Examples, width, channel) tensor:</strong>
  <p>
  The first principal component is 550 times stronger than the second principal component. <strong>A darker pixel indicates a column with more light blue pixels.</strong> It is unclear how CompressARC knows where the borders of the bounding box are.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_example_width_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Color, channel) tensor:</strong>
  <p>
  This tensor serves to distinguish the roles of the two colors apart.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_color_component_0.png" width="50%"></td>
  </tr>
</tbody></table>

  

  <h3 id="case-study-center-cross">Case Study: Center Cross</h3>

  <table>
    <thead>
      <tr>
        <th>Puzzle 41e4d17e from training split</th>
        <th>Our Network‚Äôs Answer</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_problem.png" alt="image"></td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_at_1500_steps.png" alt="image"></td>
      </tr>
    </tbody>
  </table>

  <h5 id="human-solution-2">Human Solution:</h5>
  <p>We first notice that the input consists of blue ‚Äúbubble‚Äù shapes (really they are just squares, but the fact that they‚Äôre blue reminds us of bubbles) on a light blue background and the output has the same. But in the output, there are now magenta rays emanating from the center of each bubble. We copy the input over to the answer grid, and then draw magenta rays starting from the center of each bubble out to the edge in every cardinal direction. At this point, we submit our answer and find that it is wrong, and we notice that in the given demonstrations, the blue bubble color is drawn on top of the magenta rays, and we have drawn the rays on top of the bubbles instead. So, we pick up the blue color and correct each point where a ray pierces a bubble, back to blue.</p>

  <h5 id="compressarc-solution-2">CompressARC Solution:</h5>
  <p>We don‚Äôt show CompressARC‚Äôs solution evolving over time because we think it is uninteresting; instead will describe. We don‚Äôt see much change in CompressARC‚Äôs answer over time during learning. It starts by copying over the input grid, and at some point, magenta rows and columns start to appear, and they slowly settle on the correct positions. At no point does CompressARC mistakenly draw the rays on top of the bubbles; it has always had the order correct.</p>

  <h4 id="solution-analysis-center-cross">Solution Analysis: Center Cross</h4>

  <p>Another plot of the amount of information in every tensor in $z$:</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_KL_components.png">
</p>

  <p>The only surviving tensors are the $(color, channel)$ and $(example, height, width, channel)$ tensors.</p>

  <table>
  <tbody><tr>
  <td>
  <strong> (Examples, height, width, channel) tensor:</strong>
  <p>
  The top principal component is 2496 times stronger than the second principal component. <strong>The (examples, height, width, channel) tensor codes for the centers of the bubbles.</strong> In the KL contribution plot, we can see that the information content of this tensor is decreasing over time. Likely, CompressARC is in the process of eliminating the plus shaped representation, and replacing it with a pixel instead, which takes fewer bits.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_example_height_width_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Color, channel) tensor:</strong>
  <p>
  The $(color, channel)$ tensor just serves to distinguish the individual roles of the colors in the puzzle.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_color_component_0.png"><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_color_component_1.png"></td>
  </tr>
</tbody></table>

  

  <h2 id="list-of-mentioned-arc-agi-puzzles">List of Mentioned ARC-AGI Puzzles</h2>

  <p>All the puzzles we mentioned are part of the training split.</p>

  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Puzzle</th>
        <th>Name</th>
        <th>Puzzle</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>025d127b</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/025d127b_problem.png" alt="image"></td>
        <td>0a938d79</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0a938d79_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>0ca9ddb6</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0ca9ddb6_problem.png" alt="image"></td>
        <td>0d3d703e</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0d3d703e_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>0dfd9992</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0dfd9992_problem.png" alt="image"></td>
        <td>0e206a2e</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0e206a2e_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>1c786137</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/1c786137_problem.png" alt="image"></td>
        <td>1f876c06</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/1f876c06_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>28e73c20</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/28e73c20_problem.png" alt="image"></td>
        <td>272f95fa</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>2bcee788</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/2bcee788_problem.png" alt="image"></td>
        <td>2dd70a9a</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/2dd70a9a_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>3bd67248</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/3bd67248_problem.png" alt="image"></td>
        <td>41e4d17e</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>42a50994</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/42a50994_problem.png" alt="image"></td>
        <td>5ad4f10b</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/5ad4f10b_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>6d75e8bb</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_problem.png" alt="image"></td>
        <td>7b6016b9</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/7b6016b9_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>ce9e57f2</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/ce9e57f2_problem.png" alt="image"></td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
      </tr>
    </tbody>
  </table>

  

  <h2 id="code">Code</h2>

  <p>Code for this project is available <a href="https://github.com/iliao2345/CompressARC">here</a>.</p>

  <p>If you‚Äôd like to cite this blog post, use the following entry:</p>
  <div><pre><code>@online{liao2025arcagiwithoutpretraining,
	author = {Isaac Liao and Albert Gu},
	title = {ARC-AGI Without Pretraining},
	year = {2025},
	url = {https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html},
}
</code></pre></div>

  

  

  

</div></div>]]></description>
        </item>
    </channel>
</rss>