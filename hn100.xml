<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 07 Dec 2025 08:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Z2 – Lithographically fabricated IC in a garage fab (134 pts)]]></title>
            <link>https://sam.zeloof.xyz/second-ic/</link>
            <guid>46178789</guid>
            <pubDate>Sun, 07 Dec 2025 03:03:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sam.zeloof.xyz/second-ic/">https://sam.zeloof.xyz/second-ic/</a>, See on <a href="https://news.ycombinator.com/item?id=46178789">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-2342">
	
	<!-- .entry-header -->

	<div>
		<p><em>Homemade&nbsp;1000+ transistor array chip&nbsp;</em></p>
<p>In 2018 I made the <a href="https://sam.zeloof.xyz/first-ic/">first lithographically fabricated integrated circuits</a>&nbsp;in my garage fab. I was a senior in high school&nbsp;when I made the Z1 amplifier, and now I’m a senior in college so there are some long overdue improvements to the amateur silicon process.<a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9414_an.jpg"><br>
</a><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9414ano.jpg"><img fetchpriority="high" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9414ano-1024x759.jpg" alt="DSC_9414ano" width="660" height="489" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9414ano-1024x759.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9414ano-300x222.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9414ano-768x569.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a><br>
The Z1 had&nbsp;6 transistors and was a great test chip to develop all the processes and equipment.&nbsp;The Z2 has 100 transistors on a 10µm <a href="http://www.intel4004.com/sgdm.htm">polysilicon gate</a> process – same technology as <a href="https://en.wikipedia.org/wiki/Intel_4004">Intel’s first processor</a>. My chip is a simple 10×10 array of transistors to test, characterize, and tweak the process but this is a huge step closer to more advanced DIY computer chips. The Intel 4004 has 2,200 transistors and I’ve now made 1,200&nbsp;on the same piece of silicon.</p>
<p><iframe title="YouTube video player" src="https://www.youtube.com/embed/IS5ycm7VfXg" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p><img decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Screen-Shot-2021-08-12-at-4.28.35-PM-1024x628.png" alt="Screen Shot 2021-08-12 at 4.28.35 PM" width="660" height="405" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Screen-Shot-2021-08-12-at-4.28.35-PM-1024x628.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/Screen-Shot-2021-08-12-at-4.28.35-PM-300x184.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/Screen-Shot-2021-08-12-at-4.28.35-PM-768x471.png 768w" sizes="(max-width: 660px) 100vw, 660px"></p>
<figure id="attachment_2440" aria-describedby="caption-attachment-2440"><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/mooreslaw-2.png"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/mooreslaw-2-1024x768.png" alt="Only half joking" width="660" height="495" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/mooreslaw-2-1024x768.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/mooreslaw-2-300x225.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/mooreslaw-2-768x576.png 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/mooreslaw-2.png 1280w" sizes="(max-width: 660px) 100vw, 660px"></a><figcaption id="caption-attachment-2440">Only half joking</figcaption></figure>
<p>Previously, I made chips&nbsp;with a <a href="https://en.wikipedia.org/wiki/Metal_gate">metal gate process</a>. The aluminum gate has a large work function difference with the silicon channel beneath it which results in a high threshold voltage (&gt;10V). I used these metal gate transistors in a few fun projects like a <a href="https://twitter.com/szeloof/status/1280249239495479297">guitar distortion pedal</a>&nbsp;and a <a href="https://twitter.com/szeloof/status/1263940735923093505">ring oscillator LED blinker</a>&nbsp;but both of these required one or two 9V batteries to run the circuit due to high Vth. By switching to a polysilicon gate process, I get a ton of performance benefits (self aligned gate means lower overlap capacitances) including a much lower Vth which makes these chips compatible with 2.5V and 3.3V logic levels. The new FETs have excellent characteristics:</p>
<pre><strong>NMOS Electrical Properties:</strong>
Vth             = 1.1 V
Vgs MAX         = 8 V
Cgs             = &lt;0.9 pF
Rise/fall time  = &lt;10 ns
On/off ratio    = 4.3e6
Leakage current = 932 pA (Vds=2.5V)
</pre>
<p>I was particularly surprised by the super low leakage current. This value goes up about 100x in ambient room lighting.</p>
<div id="gallery-1"><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9375.jpg"><img loading="lazy" decoding="async" width="660" height="505" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9375-1024x784.jpg" alt="" aria-describedby="gallery-1-2394" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9375-1024x784.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9375-300x230.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9375-768x588.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-1-2394">
				NMOS, 0.5V Vgs steps
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9439.jpg"><img loading="lazy" decoding="async" width="660" height="517" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9439-1024x802.jpg" alt="" aria-describedby="gallery-1-2395" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9439-1024x802.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9439-300x235.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9439-768x601.jpg 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9439.jpg 1979w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-1-2395">
				Diode curve
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9442.jpg"><img loading="lazy" decoding="async" width="660" height="524" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9442-1024x813.jpg" alt="" aria-describedby="gallery-1-2396" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9442-1024x813.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9442-300x238.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9442-768x609.jpg 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9442.jpg 1879w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-1-2396">
				C-V showing Vth = 1.1V
				</figcaption></figure>
		</div>

<p>Now we know that it’s possible to make really good transistors with impure chemicals, no cleanroom, and homemade equipment. Of course, yield and process repeatability are&nbsp;diminished. I’ll do more testing to collect data on the statistics and variability of FET properties but it’s looking good!</p>
<div id="gallery-2"><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9447-e1628800541548.jpg"><img loading="lazy" decoding="async" width="660" height="493" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9447-e1628800541548-1024x765.jpg" alt="" aria-describedby="gallery-2-2398" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9447-e1628800541548-1024x765.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9447-e1628800541548-300x224.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9447-e1628800541548-768x574.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-2-2398">
				1MHz into 50Ω load
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9446.jpg"><img loading="lazy" decoding="async" width="660" height="492" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9446-1024x763.jpg" alt="" aria-describedby="gallery-2-2397" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9446-1024x763.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9446-300x224.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9446-768x572.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-2-2397">
				20MHz into 50Ω load
				</figcaption></figure>
		</div>

<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9419-1.jpg"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9419-1-1024x678.jpg" alt="DSC_9419" width="660" height="437" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9419-1-1024x678.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9419-1-300x199.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9419-1-768x509.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>The chip is small, about one quarter the die area of my previous ICs (2.4mm^2) which makes it&nbsp;hard to probe.&nbsp;There’s a simple 10×10 array of N-channel FETs on each chip which will give me a lot of characterization data. Since it’s such a&nbsp;simple&nbsp;design, I was able to lay&nbsp;it out&nbsp;using Photoshop. Columns of 10 transistors share a common gate connection and each row is strung together in series with adjacent transistors sharing a source/drain terminal. It’s similar to NAND flash but I only did this to&nbsp;keep the metal pads large enough so I can reasonably probe them, if every FET had 3 pads for itself they would be too small.</p>
<p><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video id="video-2342-1" width="660" height="371" preload="metadata" controls="controls"><source type="video/mp4" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/probing.m4v?_=1"><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/probing.m4v">http://sam.zeloof.xyz/wp-content/uploads/2021/08/probing.m4v</a></video></p>
<p>It’s hard to convey the excitement of seeing a good FET curve displayed on the curve tracer after dipping a shard of rock into chemicals all day.</p>
<div id="gallery-3"><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/1r.png"><img loading="lazy" decoding="async" width="300" height="169" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/1r-300x169.png" alt="" aria-describedby="gallery-3-2362" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/1r-300x169.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/1r-768x432.png 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/1r-1024x576.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/1r.png 1920w" sizes="(max-width: 300px) 100vw, 300px"></a>
			</p>
				<figcaption id="gallery-3-2362">
				Source/drain
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/2r.png"><img loading="lazy" decoding="async" width="300" height="169" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/2r-300x169.png" alt="" aria-describedby="gallery-3-2363" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/2r-300x169.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/2r-768x432.png 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/2r-1024x576.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/2r.png 1920w" sizes="(max-width: 300px) 100vw, 300px"></a>
			</p>
				<figcaption id="gallery-3-2363">
				Poly gate
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/3r.png"><img loading="lazy" decoding="async" width="300" height="169" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/3r-300x169.png" alt="" aria-describedby="gallery-3-2364" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/3r-300x169.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/3r-768x432.png 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/3r-1024x576.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/3r.png 1920w" sizes="(max-width: 300px) 100vw, 300px"></a>
			</p>
				<figcaption id="gallery-3-2364">
				Contact
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/4r.png"><img loading="lazy" decoding="async" width="300" height="169" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/4r-300x169.png" alt="" aria-describedby="gallery-3-2365" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/4r-300x169.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/4r-768x432.png 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/4r-1024x576.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/4r.png 1920w" sizes="(max-width: 300px) 100vw, 300px"></a>
			</p>
				<figcaption id="gallery-3-2365">
				Metal
				</figcaption></figure>
		</div>

<p>A single 10µm NMOS transistor can be see below, with slight misalignment in the metal layer (part of the left contact is uncovered). Red outline is polycrystalline silicon, blue is the source/drain.</p>
<div id="gallery-4"><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002.jpg"><img loading="lazy" decoding="async" width="660" height="495" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-1024x768.jpg" alt="" aria-describedby="gallery-4-2403" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-1024x768.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-300x225.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-768x576.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-4-2403">
				Single NMOS transistor
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-copy.jpg"><img loading="lazy" decoding="async" width="660" height="495" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-copy-1024x768.jpg" alt="" aria-describedby="gallery-4-2402" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-copy-1024x768.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-copy-300x225.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-copy-768x576.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-4-2402">
				Single NMOS transistor
				</figcaption></figure>
		</div>

<p>So far I’ve made an opamp (Z1) and a memory-like array (Z2). More interesting&nbsp;circuits are definitely possible even with this low transistor density.&nbsp;The process needs&nbsp;some tweaking&nbsp;but now that I’m able to consistently make good quality transistors I should be able to design more complex digital and analog circuits. Testing each chip is very&nbsp;tedious&nbsp;so I am trying to automate the process and I’ll post more data then. I’ve made 15 chips (1,500 transistors) and know there’s at least one completely functional chip and at least two “mostly functional”, meaning ~80% of the transistors work instead of 100%. No proper yield data yet. The most common defect is a drain or source shorted to the bulk silicon channel, not a leaky or shorted gate like on my Z1 process.</p>
<figure id="attachment_2409" aria-describedby="caption-attachment-2409"><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/IMG_5821.jpg"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/IMG_5821.jpg" alt="Profilometer scan of gate" width="772" height="590" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/IMG_5821.jpg 772w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/IMG_5821-300x229.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/IMG_5821-768x587.jpg 768w" sizes="(max-width: 772px) 100vw, 772px"></a><figcaption id="caption-attachment-2409">Profilometer scan of gate layer (y axis in angstrom, x axis is micron)</figcaption></figure>
<p>I said before that the gate used to be made out of aluminum and now it’s silicon which makes the chips work a lot better. Silicon comes in three varieties that we care about: amorphous, polycrystalline, and monocrystalline.&nbsp;From left to right, these become more electrically conductive but also much harder to deposit.&nbsp;In fact,&nbsp;monocrystalline Si can’t be deposited, you can only grow it in contact with another mono-Si layer as a seed (epitaxy). Since the gate must be deposited on top of an insulating dielectric, poly is the best we can do. We can heavily dope the polysilicon anyway to make it more conductive.</p>
<div id="gallery-5"><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0003.jpg"><img loading="lazy" decoding="async" width="660" height="495" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0003-1024x768.jpg" alt="" aria-describedby="gallery-5-2404" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0003-1024x768.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0003-300x225.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0003-768x576.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-5-2404">
				2 FETs sharing gate
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001.jpg"><img loading="lazy" decoding="async" width="660" height="495" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001-1024x768.jpg" alt="" aria-describedby="gallery-5-2405" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001-1024x768.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001-300x225.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001-768x576.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-5-2405">
				Neighbors share source/drain
				</figcaption></figure>
		</div>

<p>A typical self-aligned polysilicon gate process&nbsp;requires silane, a&nbsp;toxic and explosive gas, to&nbsp;deposit polycrystalline silicon layers. It may also be possible by sputtering or <a href="https://sci-hub.st/https://sid.onlinelibrary.wiley.com/doi/abs/10.1002/sdtp.10835">evaporating amorphous silicon and annealing with a laser</a>.&nbsp;A major theme of this DIY silicon process is to circumvent expensive, difficult, or dangerous steps. So, I came up with a modified process flow. It’s a variation on the standard self-aligned&nbsp;methods&nbsp;to allow doping via high temperature diffusion rather than ion implantation. The effect is that I’m able to buy a silicon wafer with the&nbsp;polysilicon already deposited on it&nbsp;from the factory and pattern it to make transistors instead of putting my own polysilicon down halfway through the process. This is a nice short term workaround but it would be best to design a polysilicon deposition process using the laser anneal method mentioned above.</p>
<p>Wafers are available with all kinds of materials deposited on them already, so I just had to find one with a thin layer of SiO2 (gate oxide, ~10nm) followed by a thicker polysilicon (300nm). I found a lot of 25 200mm (EPI, prime, [1-0-0], p-type) wafers on eBay for $45 which is essentially a lifetime supply, so email me if you want one. The gate oxide is the most&nbsp;fragile layer and requires the most care during fabrication. Since I bought the wafer with a nice high quality oxide on it already that was capped off and kept clean by the thick polysilicon layer,&nbsp;I was able to eliminate all the&nbsp;aggressive&nbsp;cleaning chemicals (sulfuric acid, etc) from the process and still&nbsp;make great transistors. Minimal process chemicals and tools are listed below.</p>
<pre><strong>Chemicals used in home poly-gate process:
</strong>-Water
-Alcohol
-Acetone
-Phosphoric acid
-Photoresist
-Developer (2% KOH)
-N type dopant (filmtronics P509)
-HF (1%) or CF4/CHF3 RIE
-HNO3 for poly etch or SF6 RIE</pre>
<pre><strong>Equipment used in home poly-gate process:</strong>
-Hotplate
-Tube furnace
-<a href="https://sam.zeloof.xyz/maskless-photolithography/">Lithography apparatus
</a>-Microscope
-Vacuum chamber to deposit metal</pre>
<p>Z2 “gate first” process (similar to standard self-aligned process but without a field oxide):</p>
<div id="gallery-6"><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p1-1024x669.png" alt="" aria-describedby="gallery-6-2412" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p1-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p1-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p1-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2412">
				Buy wafer
				</figcaption></figure><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p2-1024x669.png" alt="" aria-describedby="gallery-6-2413" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p2-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p2-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p2-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2413">
				Etch active
				</figcaption></figure><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p3-1024x669.png" alt="" aria-describedby="gallery-6-2414" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p3-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p3-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p3-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2414">
				Dope source/drain
				</figcaption></figure><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p4-1024x669.png" alt="" aria-describedby="gallery-6-2415" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p4-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p4-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p4-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2415">
				Etch poly gate
				</figcaption></figure><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p5-1024x669.png" alt="" aria-describedby="gallery-6-2416" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p5-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p5-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p5-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2416">
				Deposit dielectric
				</figcaption></figure><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p6-1024x669.png" alt="" aria-describedby="gallery-6-2417" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p6-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p6-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p6-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2417">
				Etch contact
				</figcaption></figure><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p7-1024x669.png" alt="" aria-describedby="gallery-6-2418" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p7-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p7-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p7-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2418">
				Deposit metal
				</figcaption></figure><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p8-1024x669.png" alt="" aria-describedby="gallery-6-2419" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p8-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p8-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p8-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2419">
				Etch metal
				</figcaption></figure>
		</div>

<p>I snapped one of the test chips in half (functional Z2 but with bad layer alignment and thin metal, about 300nm) and put it in my <a href="https://sam.zeloof.xyz/category/electron-microscope/">SEM</a> for a cross section:</p>
<div id="gallery-7"><figure>
			<p><a href="https://sam.zeloof.xyz/second-ic/snap1/"><img loading="lazy" decoding="async" width="660" height="987" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/snap1-e1629399647701-685x1024.jpg" alt="" aria-describedby="gallery-7-2451" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/snap1-e1629399647701-685x1024.jpg 685w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/snap1-e1629399647701-201x300.jpg 201w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/snap1-e1629399647701-768x1149.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-7-2451">
				…snap
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/second-ic/s/"><img loading="lazy" decoding="async" width="660" height="977" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/s-692x1024.jpg" alt="" aria-describedby="gallery-7-2450" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/s-692x1024.jpg 692w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/s-203x300.jpg 203w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/s-768x1137.jpg 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/s.jpg 925w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-7-2450">
				Tilted SEM view
				</figcaption></figure>
		</div>

<p>Find&nbsp;the dust particle in the red circle below, use that to get oriented in the coming cross section views.</p>
<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/xsecloc.jpg"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/xsecloc-1024x515.jpg" alt="xsecloc" width="660" height="332" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/xsecloc-1024x515.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/xsecloc-300x151.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/xsecloc-768x387.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection-1.png"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection-1-1024x550.png" alt="Xsection (1)" width="660" height="354" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection-1-1024x550.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection-1-300x161.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection-1-768x412.png 768w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<figure id="attachment_2453" aria-describedby="caption-attachment-2453"><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection_ano.png"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection_ano-1024x550.png" alt="NMOS cross section" width="660" height="354" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection_ano-1024x550.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection_ano-300x161.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection_ano-768x412.png 768w" sizes="(max-width: 660px) 100vw, 660px"></a><figcaption id="caption-attachment-2453">NMOS cross section</figcaption></figure>
<p>Because I bought the wafer already with gate oxide and polysilicon on it, I can’t grow a field oxide. These thick oxide layers are typically used to mask dopants and require a long high temperature step which would oxidize all of my poly and there would be none remaining. So, my modified process uses an additional masking step (the “gate” mask is typically not found in a self-aligned process) that allows me to use the polysilicon itself as a dopant mask and hard-baked photoresist as the field dielectric. This alternative processing results in the stepped structure you can see in the orange region on the NMOS cross section above.&nbsp;This process subtlety&nbsp;is mentioned here, <a href="https://twitter.com/szeloof/status/1426534655197646857?s=20">read this twitter thread</a>.</p>
<figure id="attachment_2460" aria-describedby="caption-attachment-2460"><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/gatemeasure-1.jpg"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/gatemeasure-1-1024x692.jpg" alt="Gate length measurement" width="660" height="446" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/gatemeasure-1-1024x692.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/gatemeasure-1-300x203.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/gatemeasure-1-768x519.jpg 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/gatemeasure-1.jpg 1370w" sizes="(max-width: 660px) 100vw, 660px"></a><figcaption id="caption-attachment-2460">Gate length measurement</figcaption></figure>
<p>This process isn’t ideal and I want to make some changes so it’s CMOS compatible but it simplifies fabrication and makes it possible with a minimal set of tools. The 1µm dielectric layer (orange) would ideally be CVD SiO2 (it’s possible to build a&nbsp;TEOS oxide reactor at home) but I used a photoresist instead. Most photoresists can be baked around 250°C to form a hard permanent dielectric layer that is&nbsp;an easy alternative to CVD or PECVD oxide. A spin-on-glass/sol-gel could also be used here. SiO2 etching is done with a <a href="https://sam.zeloof.xyz/sio2-patterning/">buffered HF solution made from rust stain remover</a>&nbsp;or RIE.</p>
<p>Huge composite stitched die image:</p>
<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001_stitch.jpg"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001_stitch-1024x958.jpg" alt="0001_stitch" width="660" height="617" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001_stitch-1024x958.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001_stitch-300x281.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001_stitch-768x718.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>Thanks for following my work and feel free to contact me with your thoughts!</p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using LLMs at Oxide (328 pts)]]></title>
            <link>https://rfd.shared.oxide.computer/rfd/0576</link>
            <guid>46178347</guid>
            <pubDate>Sun, 07 Dec 2025 01:17:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rfd.shared.oxide.computer/rfd/0576">https://rfd.shared.oxide.computer/rfd/0576</a>, See on <a href="https://news.ycombinator.com/item?id=46178347">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>LLM use varies widely, and the ramifications of those uses vary
accordingly; it’s worth taking apart several of the (many) uses for LLMs.</p><div><h3 data-sectnum="2.1."><a href="#_llms_as_readers">LLMs as readers<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" role="img"><g fill="currentColor"><path d="m6.586 12.243 1.59-1.591a.75.75 0 0 1 1.061 0l.354.353a.75.75 0 0 1 0 1.06L8 13.658A4 4 0 0 1 2.343 8l1.591-1.591a.75.75 0 0 1 1.06 0l.354.354a.75.75 0 0 1 0 1.06l-1.59 1.591a2 2 0 1 0 2.828 2.829M12.066 9.591a.75.75 0 0 1-1.06 0l-.354-.354a.75.75 0 0 1 0-1.06l1.59-1.591a2 2 0 1 0-2.828-2.829l-1.59 1.591a.75.75 0 0 1-1.061 0l-.354-.353a.75.75 0 0 1 0-1.06L8 2.342A4 4 0 0 1 13.657 8z"></path><path d="M9.945 5.702a.75.75 0 0 0-1.061 0L5.702 8.884a.75.75 0 0 0 0 1.06l.353.354a.75.75 0 0 0 1.061 0l3.182-3.182a.75.75 0 0 0 0-1.06z"></path></g></svg></a></h3><div><p>LLMs are superlative at reading comprehension, able to process and meaningfully
comprehend documents effectively instantly.  This can be extraordinarily
powerful for summarizing documents — or of answering more specific questions
of a large document like a datasheet or specification.  (Ironically, LLMs are
especially good at evaluating documents to assess the degree that an LLM
assisted their creation!)</p><p>While use of LLMs to assist comprehension has little downside, it does come
with an important caveat:  when uploading a document to a hosted LLM (ChatGPT,
Claude, Gemini, etc.), there must be assurance of <strong>data privacy</strong> — and
specifically, assurance that the model will not use the document to train
future iterations of itself.  Note that this may be opt-out (that is, by
default, a model may reserve the right to train on uploaded documents), but can
generally be controlled via preferences — albeit occasionally via euphemism.
(OpenAI shamelessly calls this checked-by-default setting "Improve the model
for everyone", making anyone who doesn’t wish the model to train on their data
feel as if they suffer from a kind of reactionary avarice.)</p><p>A final cautionary note:  using LLMs to assist comprehension should not
substitute for actually reading a document where such reading is socially
expected.  More concretely:  while LLMs can be a useful tool to assist in the
evaluating of candidate materials per <a href="#rfd3">[rfd3]</a>, their use should be restricted
to be as a tool, not as a substitute for human eyes (and brain!).</p></div></div><div><h3 data-sectnum="2.2."><a href="#_llms_as_editors">LLMs as editors<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" role="img"><g fill="currentColor"><path d="m6.586 12.243 1.59-1.591a.75.75 0 0 1 1.061 0l.354.353a.75.75 0 0 1 0 1.06L8 13.658A4 4 0 0 1 2.343 8l1.591-1.591a.75.75 0 0 1 1.06 0l.354.354a.75.75 0 0 1 0 1.06l-1.59 1.591a2 2 0 1 0 2.828 2.829M12.066 9.591a.75.75 0 0 1-1.06 0l-.354-.354a.75.75 0 0 1 0-1.06l1.59-1.591a2 2 0 1 0-2.828-2.829l-1.59 1.591a.75.75 0 0 1-1.061 0l-.354-.353a.75.75 0 0 1 0-1.06L8 2.342A4 4 0 0 1 13.657 8z"></path><path d="M9.945 5.702a.75.75 0 0 0-1.061 0L5.702 8.884a.75.75 0 0 0 0 1.06l.353.354a.75.75 0 0 0 1.061 0l3.182-3.182a.75.75 0 0 0 0-1.06z"></path></g></svg></a></h3><div><p>LLMs can be excellent editors.  Engaging an LLM late in the creative process
(that is, with a document already written and broadly polished), allows for
LLMs to provide helpful feedback on structure, phrasing, etc. — all without
danger of losing one’s own voice.  A cautionary note here: LLMs are infamous
pleasers — and you may find that the breathless praise from an LLM is in fact
more sycophancy than analysis.  This becomes more perilous the earlier one uses
an LLM in the writing process:  the less polish a document already has, the
more likely it is that an LLM will steer to something wholly different — at
once praising your groundbreaking genius while offering to rewrite it for you.</p></div></div><div><h3 data-sectnum="2.3."><a href="#_llms_as_writers">LLMs as writers<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" role="img"><g fill="currentColor"><path d="m6.586 12.243 1.59-1.591a.75.75 0 0 1 1.061 0l.354.353a.75.75 0 0 1 0 1.06L8 13.658A4 4 0 0 1 2.343 8l1.591-1.591a.75.75 0 0 1 1.06 0l.354.354a.75.75 0 0 1 0 1.06l-1.59 1.591a2 2 0 1 0 2.828 2.829M12.066 9.591a.75.75 0 0 1-1.06 0l-.354-.354a.75.75 0 0 1 0-1.06l1.59-1.591a2 2 0 1 0-2.828-2.829l-1.59 1.591a.75.75 0 0 1-1.061 0l-.354-.353a.75.75 0 0 1 0-1.06L8 2.342A4 4 0 0 1 13.657 8z"></path><path d="M9.945 5.702a.75.75 0 0 0-1.061 0L5.702 8.884a.75.75 0 0 0 0 1.06l.353.354a.75.75 0 0 0 1.061 0l3.182-3.182a.75.75 0 0 0 0-1.06z"></path></g></svg></a></h3><div><p>While LLMs are adept at reading and can be terrific at editing, their writing
is much more mixed.  At best, writing from LLMs is hackneyed and cliché-ridden;
at worst, it brims with tells that reveal that the prose is in fact
automatically generated.</p><p>What’s so bad about this?  First, to those who can recognize an LLM’s reveals
(an expanding demographic!), it’s just embarrassing — it’s as if the writer is
walking around with their
<a href="https://bcantrill.dtrace.org/2025/12/05/your-intellectual-fly-is-open/">intellectual
fly open</a>.  But there are deeper problems:  LLM-generated writing undermines
the authenticity of not just one’s writing but of the thinking behind it as
well.  If the prose is automatically generated, might the ideas be too?  The
reader can’t be sure — and increasingly, the hallmarks of LLM generation cause
readers to turn off (or worse).</p><p>Finally, LLM-generated prose undermines a social contract of sorts:  absent
LLMs, it is presumed that of the reader and the writer, it is the writer that
has undertaken the greater intellectual exertion.  (That is, it is more work to
write than to read!)  For the reader, this is important:  should they struggle
with an idea, they can reasonably assume that the writer themselves understands
it — and it is the least a reader can do to labor to make sense of it.</p><p>If, however, prose is LLM-generated, this social contract becomes ripped up:
a reader cannot assume that the writer understands their ideas because they
might not so much have read the product of the LLM that they tasked to write it.
If one is lucky, these are LLM hallucinations: obviously wrong and quickly
discarded.  If one is unlucky, however, it will be a kind of LLM-induced
cognitive dissonance: a puzzle in which pieces don’t fit because there is in
fact no puzzle at all.  This can leave a reader frustrated:  why should they
spend more time reading prose than the writer spent writing it?</p><p>This can be navigated, of course, but it is truly perilous:  our writing
is an important vessel for building trust — and that trust can be quickly
eroded if we are not speaking with our own voice.  For us at Oxide, there
is a more mechanical reason to be jaundiced about using LLMs to write:
because our hiring process very much selects for writers, we know that
everyone at Oxide <strong>can</strong> write — and we have the luxury of demanding of
ourselves the kind of writing that we know that we are all capable of.</p><p>So our guideline is to generally not use LLMs to write, but this shouldn’t
be thought of as an absolute — and it doesn’t mean that an LLM can’t be
used as part of the writing process.  Just please: consider your
responsibility to yourself, to your own ideas — and to the reader.</p></div></div><div><h3 data-sectnum="2.4."><a href="#_llms_as_code_reviewers">LLMs as code reviewers<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" role="img"><g fill="currentColor"><path d="m6.586 12.243 1.59-1.591a.75.75 0 0 1 1.061 0l.354.353a.75.75 0 0 1 0 1.06L8 13.658A4 4 0 0 1 2.343 8l1.591-1.591a.75.75 0 0 1 1.06 0l.354.354a.75.75 0 0 1 0 1.06l-1.59 1.591a2 2 0 1 0 2.828 2.829M12.066 9.591a.75.75 0 0 1-1.06 0l-.354-.354a.75.75 0 0 1 0-1.06l1.59-1.591a2 2 0 1 0-2.828-2.829l-1.59 1.591a.75.75 0 0 1-1.061 0l-.354-.353a.75.75 0 0 1 0-1.06L8 2.342A4 4 0 0 1 13.657 8z"></path><path d="M9.945 5.702a.75.75 0 0 0-1.061 0L5.702 8.884a.75.75 0 0 0 0 1.06l.353.354a.75.75 0 0 0 1.061 0l3.182-3.182a.75.75 0 0 0 0-1.06z"></path></g></svg></a></h3><div><p>As with reading comprehension and editing, LLMs can make for good code
reviewers.  But they can also make nonsense suggestions or otherwise miss
larger issues.  LLMs should be used for review (and can be very helpful when
targeted to look for a particular kind of issue), but that review should not
be accepted as a human substitute.</p></div></div><div><h3 data-sectnum="2.5."><a href="#_llms_as_debuggers">LLMs as debuggers<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" role="img"><g fill="currentColor"><path d="m6.586 12.243 1.59-1.591a.75.75 0 0 1 1.061 0l.354.353a.75.75 0 0 1 0 1.06L8 13.658A4 4 0 0 1 2.343 8l1.591-1.591a.75.75 0 0 1 1.06 0l.354.354a.75.75 0 0 1 0 1.06l-1.59 1.591a2 2 0 1 0 2.828 2.829M12.066 9.591a.75.75 0 0 1-1.06 0l-.354-.354a.75.75 0 0 1 0-1.06l1.59-1.591a2 2 0 1 0-2.828-2.829l-1.59 1.591a.75.75 0 0 1-1.061 0l-.354-.353a.75.75 0 0 1 0-1.06L8 2.342A4 4 0 0 1 13.657 8z"></path><path d="M9.945 5.702a.75.75 0 0 0-1.061 0L5.702 8.884a.75.75 0 0 0 0 1.06l.353.354a.75.75 0 0 0 1.061 0l3.182-3.182a.75.75 0 0 0 0-1.06z"></path></g></svg></a></h3><div><p>LLMs can be surprisingly helpful debugging problems, but perhaps only because
our expectations for them would be so low.  While LLMs shouldn’t be relied upon
(clearly?) to debug a problem, they can serve as a kind of animatronic
<a href="https://en.wikipedia.org/wiki/Rubber_duck_debugging">rubber duck</a>, helping to
inspire the next questions to ask.  (And they can be surprising:  LLMs have been
known to debug I2C issues from the screenshot of a scope capture!)  When
debugging a vexing problem one has little to lose by using an LLM — but
perhaps also little to gain.</p></div></div><div><h3 data-sectnum="2.6."><a href="#_llms_as_programmers">LLMs as programmers<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" role="img"><g fill="currentColor"><path d="m6.586 12.243 1.59-1.591a.75.75 0 0 1 1.061 0l.354.353a.75.75 0 0 1 0 1.06L8 13.658A4 4 0 0 1 2.343 8l1.591-1.591a.75.75 0 0 1 1.06 0l.354.354a.75.75 0 0 1 0 1.06l-1.59 1.591a2 2 0 1 0 2.828 2.829M12.066 9.591a.75.75 0 0 1-1.06 0l-.354-.354a.75.75 0 0 1 0-1.06l1.59-1.591a2 2 0 1 0-2.828-2.829l-1.59 1.591a.75.75 0 0 1-1.061 0l-.354-.353a.75.75 0 0 1 0-1.06L8 2.342A4 4 0 0 1 13.657 8z"></path><path d="M9.945 5.702a.75.75 0 0 0-1.061 0L5.702 8.884a.75.75 0 0 0 0 1.06l.353.354a.75.75 0 0 0 1.061 0l3.182-3.182a.75.75 0 0 0 0-1.06z"></path></g></svg></a></h3><div><p>LLMs are amazingly good at writing code — so much so that there is borderline
mass hysteria about LLMs entirely eliminating software engineering as a craft.
As with using an LLM to write prose, there is obvious peril here!  Unlike
prose, however (which really should be handed in a polished form to an LLM to
maximize the LLM’s efficacy), LLMs can be quite effective writing code <em>de
novo</em>.  This is especially valuable for code that is experimental or auxiliary
or otherwise throwaway.  The closer code is to the system that we ship, the
greater care needs to be shown when using LLMs.  Even with something that seems
natural for LLM contribution (e.g., writing tests), one should still be
careful:  it’s easy for LLMs to spiral into nonsense on even simple tasks.
Still, they can be extraordinarily useful — and can help to provide an entire
spectrum of utility in writing software; they shouldn’t be dismissed out of
hand.</p><p>Wherever LLM-generated code is used, it becomes the responsibility of the
engineer.  As part of this process of taking responsibility, <strong>self-review</strong>
becomes essential:  LLM-generated code should not be reviewed by others if the
responsible engineer has not themselves reviewed it.  Moreover, once in the
loop of peer review, generation should more or less be removed:  if code review
comments are addressed by wholesale re-generation, iterative review becomes
impossible.</p><p>In short, where LLMs are used to generate code, responsibility, rigor, empathy
and teamwork must remain top of mind.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Trains cancelled over fake bridge collapse image (153 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cwygqqll9k2o</link>
            <guid>46178108</guid>
            <pubDate>Sun, 07 Dec 2025 00:37:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cwygqqll9k2o">https://www.bbc.com/news/articles/cwygqqll9k2o</a>, See on <a href="https://news.ycombinator.com/item?id=46178108">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><header></header><nav data-testid="level1-navigation-container" id="main-navigation-container"><section><nav><ul><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li></ul></nav></section></nav><main id="main-content"><article><div data-testid="byline-new" data-component="byline-block"><p><span data-testid="byline-new-contributors"><p><span>Zoe Toase<!-- -->,</span><span data-testid="byline-new-contributors-contributor-0-role-location">North West</span><span>and</span></p><p><span>Laura O'Neill<!-- -->,</span><span data-testid="byline-new-contributors-contributor-1-role-location">North West</span></p></span></p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251203-121739-f954f14c69-web-2.35.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp" loading="eager" alt="BBC/Network Rail A side-by-side photo showing a damaged bridge on the right. A section of the barriers that run along the top of the bridge appears to have collapsed and a pile of rubble can be seen underneath. A large hole can be seen in front of the bridge. The left is a photo of the bridge taken today showing it is undamaged."><span>BBC/Network Rail</span></p></div><p data-component="caption-block"><figcaption>A photo taken by a BBC North West Tonight reporter showed the bridge is undamaged </figcaption></p></figure><div data-component="text-block"><p>Trains were halted after a suspected AI-generated picture that seemed to show major damage to a bridge appeared on social media following an earthquake.</p><p>The tremor, <a target="_self" href="https://www.bbc.co.uk/news/articles/cgjn8wg53q7o">which struck on Wednesday night</a>, was felt across Lancashire and the southern Lake District.</p><p>Network Rail said it was made aware of the image which appeared to show major damage to Carlisle Bridge in Lancaster at 00:30 GMT and stopped rail services across the bridge while safety inspections were carried out.</p><p>A BBC journalist ran the image through an AI chatbot which identified key spots that may have been manipulated.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251203-121739-f954f14c69-web-2.35.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp" loading="lazy" alt="Network Rail A photo showing damage to a bridge. A section of the barriers that run along the top of the bridge appears to have collapsed and a pile of rubble can be seen underneath. A large hole can be seen in front of the bridge"><span>Network Rail</span></p></div><p data-component="caption-block"><figcaption>Network Rail said it was made aware that the image was on social media</figcaption></p></figure><div data-component="text-block"><p>Network Rail said the railway line was fully reopened at around 02:00 GMT and it has urged people to "think about the serious impact it could have" before creating or sharing hoax images.</p><p>"The disruption caused by the creation and sharing of hoax images and videos like this creates a completely unnecessary delay to passengers at a cost to the taxpayer," a spokesperson said.</p><p>"It adds to the high workload of our frontline teams, who work extremely hard to keep the railway running smoothly," the spokesperson said.</p><p>"The safety of rail passengers and staff is our number one priority and we will always take any safety concerns seriously."</p><p>The British Transport Police said it was "made aware" of the situation but there was no ongoing investigation into the incident.</p><p>Network Rail said 32 services including passenger and freight trains were delayed because of hoax. </p><p>A spokesperson for the rail provider said a mix of passenger and freight train would have been impacted.</p><p>They said some of them would have been directly stopped or slowed while it  checked the lines, but a lot of the trains were delayed as a result of earlier services still being in their path. </p><p>The spokesperson said many of them would have been local but because of the length of the West Coast Main Line some trains were delayed as far north as Scotland.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251203-121739-f954f14c69-web-2.35.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp" loading="lazy" alt="A photo showing the bridge is undamaged "></p></div><p data-component="caption-block"><figcaption>A BBC North West reporter visited the bridge today and confirmed it was undamaged</figcaption></p></figure><div data-component="text-block"><p>Railway expert Tony Miles said due to the timing of the incident, very few passengers will have been impacted by the hoax as the services passing through at that time were primarily freight and sleeper trains.</p><p>"They generally go slow so as not to disturb the passengers trying to sleep - this means they have a bit of leeway to go faster and make up time if they encounter a delay," he said.</p><p>"It's more the fact that Network Rail will have had to mobilise a team to go and check the bridge which could impact their work for days."</p><p>He urged people to consider hoaxes like this could have on real people.</p><p>"If they actually did delay a train it could have impacted someone who had to get to a medical appointment, or a flight or a funeral.</p><p>"It may seem like a game, but anyone who's thinking of doing this should consider how it will impact real people."</p></div></article></main><hr data-testid="main-footer-divider"></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kilauea erupts, destroying webcam [video] (299 pts)]]></title>
            <link>https://www.youtube.com/watch?v=TK2N99BDw7A</link>
            <guid>46177645</guid>
            <pubDate>Sat, 06 Dec 2025 23:39:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=TK2N99BDw7A">https://www.youtube.com/watch?v=TK2N99BDw7A</a>, See on <a href="https://news.ycombinator.com/item?id=46177645">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Screenshots from developers: 2002 vs. 2015 (2015) (262 pts)]]></title>
            <link>https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/</link>
            <guid>46176905</guid>
            <pubDate>Sat, 06 Dec 2025 21:55:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/">https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/</a>, See on <a href="https://news.ycombinator.com/item?id=46176905">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

	<article>

		

		<div>
			<p>In 2002 I asked a number of developers/Unix people for screenshots of their desktops. I recently <a href="https://anders.unix.se/2015/10/28/screenshots-from-developers--unix-people-2002/">republished them</a>, and, seeing <a href="https://news.ycombinator.com/item?id=10469824">the</a> <a href="https://www.reddit.com/r/linux/comments/3qlyf6/screenshots_from_developers_unix_people_2002/">interest</a> this generated, I thought it’d be fun to ask the same people* again 13 years later. To my delight I managed to reach many of them.</p>

<p><small>* Sans Dennis Ritchie and itojun, who are no longer with us.</small></p>

<p>So, without further ado:</p>

<div>

<div>
<figure>
    <a href="https://anders.unix.se/images/bwk_desktop.jpg">
        <img src="https://anders.unix.se/images/bwk_desktop.jpg">
    </a>
    
    <figcaption>
        July 2002
        
    </figcaption>
    
</figure>


<blockquote><p>my desktop is pretty boring, since it consists of xterm windows to whatever unix system i am using at the moment.  the machine itself is likely to be running some x-window server like exceed on some flavor of windows, though for many years i just used an x terminal.</p></blockquote>

</div>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_bwk_2015.png">
        <img src="https://anders.unix.se/images/desktop_bwk_2015.png.thumb.jpg">
    </a>
    
    <figcaption>
        October 2015
        
    </figcaption>
    
</figure>


<blockquote><p>If you thought it was boring last time, check this out!</p></blockquote>

</div>
</div>

<hr>

<div>

<div>
<p>2002:</p>
<blockquote><p>
I don’t know how to make a screenshot, because I normally use my computer in text-mode. I have X and GNOME installed, but I use them only occasionally.
</p></blockquote>

</div>
<div>
<p>2015:</p>
<blockquote><p>
Under X, I use the standard environment of Trisquel, but mostly I type at Emacs in a console.
</p></blockquote>

</div>
</div>

<hr>

<div>

<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_b_moolenaar.png">
        <img src="https://anders.unix.se/images/desktop_b_moolenaar.png.thumb.jpg">
    </a>
    
    <figcaption>
        September 2002
        
    </figcaption>
    
</figure>


<blockquote><p>
Well, my desktop is quite boring. I mostly work with four xterms and a few Netscape windows. The KDE bar hides automatically, you can only see a thin grey line at the bottom.
</p></blockquote>

</div>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_b_moolenaar_2015.png">
        <img src="https://anders.unix.se/images/desktop_b_moolenaar_2015.png.thumb.jpg">
    </a>
    
    <figcaption>
        November 2015
        
    </figcaption>
    
</figure>


<blockquote><p>
Here is the new one.  You'll see that, like before, I have lots of xterms where I work on Vim, Zimbu and email.  Now using the Chrome browser, showing off the Zimbu homepage.  But clearly everything has become bigger!
</p></blockquote>

</div>
</div>

<hr>

<div>

<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_rasmus_lerdorf.png">
        <img src="https://anders.unix.se/images/desktop_rasmus_lerdorf.png.thumb.jpg">
    </a>
    
    <figcaption>
        September 2002
        
    </figcaption>
    
</figure>


<blockquote><p>
Linux (2.4.20-pre5), Gnome2, vim, Pine.
</p></blockquote>

</div>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_rasmus_lerdorf_2015.png">
        <img src="https://anders.unix.se/images/desktop_rasmus_lerdorf_2015.png.thumb.jpg">
    </a>
    
    <figcaption>
        October 2015
        
    </figcaption>
    
</figure>


<blockquote><p>
Not that much has changed in 13 years. Still using Linux. Still just a browser window and a ton of terminals hiding behind them.  The main change is that switched from Pine to Thunderbird for email at some point. The OS on my laptop here is Ubuntu with Unity although there are a lot of Debian packages installed so it is a bit of a hybrid at this point. Oh, and yes, my son Carl is a lot older now.
</p></blockquote>

</div>
</div>

<hr>

<div>

<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_warren_toomey.gif">
        <img src="https://anders.unix.se/images/desktop_warren_toomey.gif">
    </a>
    
    <figcaption>
        August 2002
        
    </figcaption>
    
</figure>


<blockquote>
<p>Ah, my desktop is pretty boring, I used fvwm 1.24 as my window manager and I try to have no more than 1 or 2 windows open per virtual desktop.  I use FreeBSD 4-STABLE as my operating system. I first came across Unix when I got an account on a Pyramid 90x running OSx. This had a dual-universe setup: both AT&amp;T and BSD-style environments, chosen by an environment variable. Initially I was given the AT&amp;T environment, but my friends convinced me to ``come over” to BSD. Since then I’ve been a BSD afficionado.</p>

<p>After OSx, SunOS 3.5 and later SunOS releases, until 386BSD 0.1 came out and I started to run BSD at home. Then when 386BSD transmogrified to FreeBSD, I went with FreeBSD.</p>

<p>In terms of desktop, I’m a command-line guy, always will be. My favourite editor is vi, my favourite shell is tcsh (but kudos to rc for elegance).  So I don’t really feel the need for GUI things like Gnome or KDE :-)</p>
</blockquote>

</div>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_warren_toomey_2015.jpg">
        <img src="https://anders.unix.se/images/desktop_warren_toomey_2015.jpg">
    </a>
    
    <figcaption>
        October 2015
        
    </figcaption>
    
</figure>


<blockquote>
<p>How things have (and have not changed). I'm still a command-line junkie with at least two xterm windows open. I'm still using a 3x3 virtual desktop. However, instead of fvwm, it is now LXDE. I've also switched from FreeBSD to Linux and I'm running Lubuntu as my distribution.</p>

<p>There are a lot of indispensable GUI tools that I use. These include Firefox, lyx, Gimp, KeepassX, Shutter, viking, dia, Wireshark, calibre, audacity, Handbrake and VLC. But where possible I still prefer to script things. My main development languages are still shell, Perl and C.</p>

<p>My shell is now bash. The vi keystrokes are burned into my fingertips and, as long as vim can be ported to new systems, that will be my text editor until I pass on. My mail client is now mutt (definitely not a web client) and my mail is stored locally, not on someone else's server.</p>

<p>The only issue I have is that, since a job change, I now have to deal with Windoze things. Thus, I have VirtualBox, libreoffice and Wine to help me do that.</p>

<p>I started with Unix on a Pyramid 90x. I now have a smart phone that blows the 90x out of the water on performance, RAM and storage. But I'm so very happy that, somewhere down underneath, there is still a Bourne shell and an operating system that does open(), close(), read(), write(), fork() and exec()!</p>
</blockquote>

</div>
</div>

<hr>

<div>
<p><a href="https://en.wikipedia.org/wiki/Jordan_Hubbard">Jordan Hubbard</a> (FreeBSD co-founder, later Director of UNIX Technology at Apple; now CTO of iXsystems):
</p>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_jordan_hubbard.jpg">
        <img src="https://anders.unix.se/images/desktop_jordan_hubbard.jpg">
    </a>
    
    <figcaption>
        July 2002
        
    </figcaption>
    
</figure>

</div>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_jordan_hubbard_2015.png">
        <img src="https://anders.unix.se/images/desktop_jordan_hubbard_2015.png.thumb.jpg">
    </a>
    
    <figcaption>
        November 2015
        
    </figcaption>
    
</figure>


<blockquote>
<p>You’ll probably be sad (or perhaps not) to hear that my desktop hasn’t really changed much at all - still OS X, though because OS X has virtual desktops now I have multiple “desktops” (6 of them) where Mail.app runs on one, Safari on another, Calendar, Slack, etc - all on separate desktops.  This makes it a bit boring, but here’s the one I probably spend the most time in - the terminal window desktop. :)</p>
</blockquote>

</div>
</div>

<hr>



<hr>

<p>Discussion: <a href="https://news.ycombinator.com/item?id=10722536">Hacker News</a>; reddit: <a href="https://www.reddit.com/r/programming/comments/3wg48k/screenshots_from_developers_2002_vs_2015/">/r/programming</a>, <a href="https://www.reddit.com/r/linux/comments/3w83ta/screenshots_from_developers_2002_vs_2015/">/r/linux</a></p>

		</div>

		 


	</article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The past was not that cute (143 pts)]]></title>
            <link>https://juliawise.net/the-past-was-not-that-cute/</link>
            <guid>46176893</guid>
            <pubDate>Sat, 06 Dec 2025 21:53:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://juliawise.net/the-past-was-not-that-cute/">https://juliawise.net/the-past-was-not-that-cute/</a>, See on <a href="https://news.ycombinator.com/item?id=46176893">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main">
		
				
		
<article id="post-1585">

		
	
		
	<div>
		
<p>I was excited when <a href="https://en.wikipedia.org/wiki/Cottagecore">cottagecore</a> became a thing. Maybe my interest in retro clothes and handicrafts would be less embarrassing now!</p>


<div>
<figure><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="925" height="515" src="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?resize=925%2C515&amp;ssl=1" alt="" srcset="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?w=2111&amp;ssl=1 2111w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?resize=300%2C167&amp;ssl=1 300w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?resize=1024%2C570&amp;ssl=1 1024w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?resize=768%2C427&amp;ssl=1 768w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?resize=1536%2C855&amp;ssl=1 1536w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?resize=2048%2C1140&amp;ssl=1 2048w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?resize=1320%2C735&amp;ssl=1 1320w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?w=1850&amp;ssl=1 1850w" sizes="(max-width: 925px) 100vw, 925px"><figcaption>Cottagecore, Pinterest 2025</figcaption></figure>
</div>


<p>I still enjoy it. But in spaces focused on old-fashioned vibes, you encounter a lot of people who believe that the past was <em>actually</em> this charming.</p>



<figure>
<figure><img data-recalc-dims="1" decoding="async" width="680" height="1000" data-id="1602" src="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/MV5BODZjMGRkZmMtYzRhMC00NTMyLTg4MjAtNGIxZTIzYjY0MjNmXkEyXkFqcGc%40._V1_.jpg?resize=680%2C1000&amp;ssl=1" alt="" srcset="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/MV5BODZjMGRkZmMtYzRhMC00NTMyLTg4MjAtNGIxZTIzYjY0MjNmXkEyXkFqcGc%40._V1_.jpg?w=680&amp;ssl=1 680w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/MV5BODZjMGRkZmMtYzRhMC00NTMyLTg4MjAtNGIxZTIzYjY0MjNmXkEyXkFqcGc%40._V1_.jpg?resize=204%2C300&amp;ssl=1 204w" sizes="(max-width: 680px) 100vw, 680px"><figcaption>1879s farmers through the eyes of the 1970s.</figcaption></figure>



<figure><img decoding="async" width="678" height="948" data-id="1605" src="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-4.39.45-PM.png?fit=678%2C948&amp;ssl=1" alt="" srcset="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-4.39.45-PM.png?w=678&amp;ssl=1 678w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-4.39.45-PM.png?resize=215%2C300&amp;ssl=1 215w" sizes="(max-width: 678px) 100vw, 678px"><figcaption>Actual farmer, <a href="https://commons.wikimedia.org/wiki/File:Wife_of_Tenant_Farmer.jpg">Texas 1937</a>.</figcaption></figure>
</figure>



<p> <a href="https://en.wikipedia.org/wiki/Laura_Ingalls_Wilder">Laura Ingalls Wilder</a>‘s <em>Little House on the Prairie</em> books are problematic, and also I will always love them. She wrote about the beauty of family and hard work, but she wrote them because she spent her whole life supporting disabled family members. She and her daughter beautified her “pioneer girl” history to make good books. Her daughter describes the reality: &nbsp;“It took seven successive years of complete crop failure, with work, weather and sickness that wrecked [my father’s] health permanently, and interest rates of 36 percent on money borrowed to buy food, to dislodge us from that land.”</p>





<p>My own version of this mistake was thinking that people’s personalities were different in the past. I grew up listening to folk music and imagining a past where nice boys would admire a nice quiet girl like me, and I wouldn’t have to figure out dating because everything would just unfold, probably on a May morning. My mother pointed out that a lot of the <a href="https://en.wikipedia.org/wiki/Down_by_Blackwaterside">songs</a> along the lines of “my own true love proved false to me” were about unplanned pregnancies.</p>



<p>I also assumed the bonny lasses in these songs would be wholesome and nice. But were popular girls of the past nicer people than they are now?</p>



<figure>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="925" height="925" data-id="1591" src="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=925%2C925&amp;ssl=1" alt="" srcset="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=1024%2C1024&amp;ssl=1 1024w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=1536%2C1536&amp;ssl=1 1536w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=80%2C80&amp;ssl=1 80w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=1320%2C1320&amp;ssl=1 1320w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?w=1900&amp;ssl=1 1900w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?w=1850&amp;ssl=1 1850w" sizes="auto, (max-width: 925px) 100vw, 925px"></figure>



<figure><img loading="lazy" decoding="async" width="1164" height="1312" data-id="1594" src="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-2.42.31-PM.png?fit=908%2C1024&amp;ssl=1" alt="" srcset="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-2.42.31-PM.png?w=1164&amp;ssl=1 1164w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-2.42.31-PM.png?resize=266%2C300&amp;ssl=1 266w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-2.42.31-PM.png?resize=908%2C1024&amp;ssl=1 908w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-2.42.31-PM.png?resize=768%2C866&amp;ssl=1 768w" sizes="auto, (max-width: 925px) 100vw, 925px"></figure>
</figure>



<p>Some of my picture came from growing up in the Anglo-American folk dance and music community: it had a lot of aging hippies with graduate degrees. So I came away imagining a past with a lot of the kind of people who become engineers and English teachers. A more accurate picture would have been “Imagine a small town where the same 19 kids form your entire group of peers and potential partners.”</p>


<div>
<figure><img loading="lazy" decoding="async" width="1415" height="792" src="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Belle.webp?fit=925%2C518&amp;ssl=1" alt="" srcset="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Belle.webp?w=1415&amp;ssl=1 1415w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Belle.webp?resize=300%2C168&amp;ssl=1 300w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Belle.webp?resize=1024%2C573&amp;ssl=1 1024w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Belle.webp?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Belle.webp?resize=1320%2C739&amp;ssl=1 1320w" sizes="auto, (max-width: 925px) 100vw, 925px"></figure>
</div>


<p>Bookish girls like Belle didn’t really go to live in enchanted castles with huge libraries. They stayed in villages where everyone thought they were weird and their best option was Gaston.</p>





<p>Maybe my favorite podcast episode ever is <a href="https://www.econtalk.org/rachel-laudan-on-the-history-of-food-and-cuisine/">Rachel Laudan on food history</a>: “I did have the extraordinary good fortune to grow up eating what I think the romantic movement dreams of. We had milk fresh from the cow; I never had pasteurized milk until I went to school. We had fish from the river, pheasant from the farm. The food was extremely good. . . . everything was fresh from the garden. So, I&nbsp;<em>do</em>&nbsp;romanticize—some of that because the taste was often extraordinary. And then I tweak myself and I say, ‘Look, Rachel, your mother spent all day, every day gardening or cooking.’ Essentially. As well as doing other chores. And she said to you, ‘Rachel, it’s servitude. I want you to have a life I didn’t have.’&nbsp;“</p>



<p>I love living in a time and place where we get to choose aesthetics. I have bread rising in my kitchen right now, and I’m looking forward to baking it in an electric oven that doesn’t require me stacking wood or putting smoke into my house.</p>



<p>So I’ll continue to enjoy retro vibes, and draw on the past for lessons on how to be a human. (For example, making music together is one of life’s great experiences, and it’s a mistake to entirely substitute recorded music for that.) But I’ll enjoy doing so with indoor plumbing, dental care, and a desk job. </p>








	
	</div>
	
	
	
			


</article>
<!-- #comments -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>		
				
		</main>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Coffee linked to slower biological ageing among those with severe mental illness (142 pts)]]></title>
            <link>https://www.kcl.ac.uk/news/coffee-linked-to-slower-biological-ageing-among-those-with-severe-mental-illness-up-to-a-limit</link>
            <guid>46176766</guid>
            <pubDate>Sat, 06 Dec 2025 21:33:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.kcl.ac.uk/news/coffee-linked-to-slower-biological-ageing-among-those-with-severe-mental-illness-up-to-a-limit">https://www.kcl.ac.uk/news/coffee-linked-to-slower-biological-ageing-among-those-with-severe-mental-illness-up-to-a-limit</a>, See on <a href="https://news.ycombinator.com/item?id=46176766">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>New research from King’s College London finds that coffee consumption within the NHS recommended limit is linked to longer telomere lengths – a marker of biological ageing – among people with bipolar disorder and schizophrenia. The effect is comparable to roughly five years younger biological age.</p><div><div><p>Telomeres are structures that protect DNA. As people get older, their telomeres shorten as part of the natural human ageing process. This process has been shown to be accelerated among people with severe mental illness, such as bipolar disorder and schizophrenia, who have an average life expectancy 15 years shorter than the general population.</p>
<p>Previous research shows that coffee possesses health benefits. It may reduce oxidative stress in the general population, helping slow biological ageing processes like telomere shortening. The new study, published in <a href="https://mentalhealth.bmj.com/content/28/1/e301700" target="_blank" rel="noopener">BMJ Mental Health</a>, explores whether coffee consumption could slow this ageing process among those with severe mental illness.</p>
<p>Researchers at the Institute of Psychiatry, Psychology &amp; Neuroscience measured the effects of coffee consumption on telomere length among 436 participants aged 18 to 65 with schizophrenia, bipolar disorder or major depressive disorder with psychosis.</p>
<p>They found that coffee consumption of up to four cups per day was linked to longer telomeres, comparable to a biological age five years younger than non-coffee drinkers.</p>
<p>The longest telomeres were seen among those who consumed three to four cups per day. Too much coffee reduced this positive effect, with participants who consumed more than four cups having shorter telomeres than those who consumed between three and four cups.</p></div><p><img src="https://www.kcl.ac.uk/newimages/ioppn/news-spotlights/v-mlakar-coffee-and-telomere-length-figure.x4fa2266f.jpeg?f=webp" alt="V Mlakar et all 2025 figure: As coffee consumption (X axis) increases up to 3-4 cups, telomere length (Y axis) increases."></p><figcaption>Figure from Vid Mlakar et al. 2025: As coffee consumption increases up to 3-4 cups, telomere length increases. At 5+ cups, telomere length begins to shorten again.</figcaption><p>These effects remained after accounting for variations in age, sex, ethnicity, medication and tobacco use.</p><div><blockquote><p>We know that coffee can help slow biological ageing in the general population, but little is known about its effect on people with severe mental illness – a population whose lifespan is already shortened, in part due to age-related diseases. Our study shows that up to four cups of coffee per day is linked to longer telomeres among people with bipolar disorder and schizophrenia. This is comparable to a biological age of five years younger than non-coffee drinkers.</p><cite>Vid Mlakar, PhD student at King’s College London and first author of the study</cite></blockquote></div><div><blockquote><p>Coffee is a beverage that many people consume daily. On one hand, we know that excessive coffee consumption can have negative effects on health, such as reducing sleep quality. However, our new study suggests that coffee consumption up to a certain point may have benefits for biological ageing. Many of the factors that are known to affect biological ageing, such as genetics and negative stressful life experiences, are beyond our control. Lifestyle factors like coffee consumption are something we can actively modify, making research like this particularly valuable.</p><cite>Dr Monica Aas, MRC Research Fellow at King’s College London and senior author of the study</cite></blockquote></div><div><p>Dr Aas added: "Studies such as this also support the idea that we should move away from viewing coffee as simply “good or bad”, and instead consider a more balanced view. Still, these results need to be confirmed in other independent studies and longitudinal research before we can determine if this is a causal effect."</p>
<p>Data were from the Norwegian TOP study, collected between 2007 and 2018. The researchers included participants who had available data on mental health diagnosis (assessed using the Structured Clinical Interview for DSM-IV), telomere length (measured by extracting DNA from blood samples) and self-reported coffee consumption.</p>
<p>The researchers note that the study did not have information on the type of coffee consumed (instant versus filter) or the caffeine concentration of each cup. The NHS advises limiting caffeine intake to 400 mg/day (approximately four cups of coffee).</p>
<p>The study was funded by the Research Council of Norway, the KG Jebsen Stiftelsen and an Medical Research Council Fellowship. The team has recently received funding from the British Medical Association’s Margaret Temple grant to investigate telomere shortening in a longitudinal cohort of patients with psychosis. This project will allow them to explore further how several lifestyle factors, as well as stress, influence the rate of telomere shortening over time.</p>
<p><a href="https://mentalhealth.bmj.com/content/28/1/e301700" target="_blank" rel="noopener">"Coffee intake is associated with telomere length in severe mental disorders"</a> (Vid Mlakar et al.) was published in BMJ Mental Health. DOI: 10.1136/bmjment-2025-301700&nbsp;</p>
<p>For more information, please contact <a href="mailto:ioppn-pr@kcl.ac.uk">Milly Remmington</a> (School of Mental Health &amp; Psychological Sciences Communications Manager).</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The general who refused to crush Tiananmen's protesters (120 pts)]]></title>
            <link>https://www.economist.com/china/2025/12/04/the-general-who-refused-to-crush-tiananmens-protesters</link>
            <guid>46176072</guid>
            <pubDate>Sat, 06 Dec 2025 19:47:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/china/2025/12/04/the-general-who-refused-to-crush-tiananmens-protesters">https://www.economist.com/china/2025/12/04/the-general-who-refused-to-crush-tiananmens-protesters</a>, See on <a href="https://news.ycombinator.com/item?id=46176072">Hacker News</a></p>
Couldn't get https://www.economist.com/china/2025/12/04/the-general-who-refused-to-crush-tiananmens-protesters: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[OMSCS Open Courseware (173 pts)]]></title>
            <link>https://sites.gatech.edu/omscsopencourseware/</link>
            <guid>46175826</guid>
            <pubDate>Sat, 06 Dec 2025 19:14:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sites.gatech.edu/omscsopencourseware/">https://sites.gatech.edu/omscsopencourseware/</a>, See on <a href="https://news.ycombinator.com/item?id=46175826">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
	<p><a href="#content">Skip to content</a></p><nav id="navbar-secondary" aria-label="secondary">
	<div>
				
			<p><a href="http://www.gatech.edu/">
					<img alt="Georgia Institute of Technology" src="https://sites.gatech.edu/omscsopencourseware/wp-content/themes/gatech-flex/img/gt-logo-oneline-white.svg" width="245px" height="42px">
				</a>
			</p>

			</div>
</nav>
	
		<!-- #wrapper-navbar end -->

	
		<header id="hero-main" aria-label="page title and basic information">
		

		
						<p><img width="850" height="478" src="https://sites.gatech.edu/omscsopencourseware/files/2024/08/CoC-Article-2018-04-13-Three-of-GT-Computing-Awards-Luncheon-1.png" alt="" decoding="async" fetchpriority="high" srcset="https://sites.gatech.edu/omscsopencourseware/files/2024/08/CoC-Article-2018-04-13-Three-of-GT-Computing-Awards-Luncheon-1.png 850w, https://sites.gatech.edu/omscsopencourseware/files/2024/08/CoC-Article-2018-04-13-Three-of-GT-Computing-Awards-Luncheon-1-300x169.png 300w, https://sites.gatech.edu/omscsopencourseware/files/2024/08/CoC-Article-2018-04-13-Three-of-GT-Computing-Awards-Luncheon-1-768x432.png 768w" sizes="(max-width: 850px) 100vw, 850px">						</p>
						
						
	</header>
	
<div id="page-wrapper">

			<main id="main">
									
<article class="page" id="post-7">
			<!-- .page-header -->

		
	<div>
		
		
<p>Georgia Tech’s Online Master of Science in Computer Science (OMSCS) program is proud to make the course content* for many of its courses publicly available through Ed Lessons. Select a course below to view the public content for that course.</p>



<p>Note that students enrolled in OMSCS should access their course content through Canvas, as the for-credit versions of these courses may include graded components or recent content updates not available through OMSCS Open Courseware.</p>



<p>*<em>Course content typically includes things such as lecture videos and exercises; it will not include things like homeworks, projects quizzes, exams, or other graded assignments.</em></p>

























































		
			</div><!-- .entry-content -->

	</article><!-- #post-## -->

												</main><!-- #main -->

			<!-- Do the right sidebar check -->
			
</div><!-- #page-wrapper -->


<!-- wrapper end -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Perl's decline was cultural (241 pts)]]></title>
            <link>https://www.beatworm.co.uk/blog/computers/perls-decline-was-cultural-not-technical</link>
            <guid>46175112</guid>
            <pubDate>Sat, 06 Dec 2025 17:42:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.beatworm.co.uk/blog/computers/perls-decline-was-cultural-not-technical">https://www.beatworm.co.uk/blog/computers/perls-decline-was-cultural-not-technical</a>, See on <a href="https://news.ycombinator.com/item?id=46175112">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

		      
			  
		      


		      <div id="content">
			    <h3>According to the Discourse, somebody killed perl</h3><p>There's been a flurry of discussion <a href="https://news.ycombinator.com/item?id=45977900">on Hacker News</a> and <a href="https://lobste.rs/s/0m6yln/what_killed_perl">other tech forums</a> about what killed Perl. I wrote a lot of Perl in the mid 90s and subsequently worked on some of the most trafficked sites on the web in mod_perl in the early 2000s, so I have some thoughts. My take: it was mostly baked into the culture. Perl grew amongst a reactionary community with conservative values, which prevented it from evolving into a mature general purpose language ecosystem. Everything else filled the gap. </p><h3>I remember Perl</h3><p>Something to keep in mind, is that although this is my personal take, and therefore entirely an opinion piece, I <em>was</em> there at the time. I stopped doing Perl properly when I left Amazon, I think this would have been around 2005. It's based on the first hand impressions of somebody who was very deeply involved in Perl in its heyday, and moved on. I have a lot of experience, from both inside and outside the tent. </p><h3>Perl's roots are sysadmin</h3><p>What culture? Perl always had a significant amount of what you might call "BOFH" culture, which came from its old UNIX sysadmin roots. All of those passive aggressive idioms and in jokes like <em>"RTFM"</em>, <em>"lusers"</em>, <em>"wizards"</em>, <em>"asking for help the wrong way"</em> etc.  None of this is literally serious, but it does encode and inform social norms that are essentially tribal and introverted. There implicitly is a privileged population, with a cost of entry to join. Dues must be paid. Cultural conservatism as a first principle. </p><p>This stems from the old locked-down data centre command culture. When computer resource was expensive, centralised, fragile, and manually operated, it was rigidly maintained by gatekeepers, defending against inappropriate use. I started my career as an apprentice programmer at the very end of this era, (late 80s) pre-web, and before microcomputers had made much inroads, and this really was the prevailing view from inside the fort. (This is a drawback about fort-building. Once you live in a fort, it's slightly too easy to develop a siege mentality). Computers are special, users are inconvenient, disruption is the main enemy. </p><p>An unfortunate feedback loop in this kind of "perilous" environment is that it easily turns prideful. It's difficult to thrive here, if you survive and do well you are skilled; you've performed feats; you <em>should</em> mark your rites of passage. This can become a dangerous culture trap. If you're not careful about it, you may start to think of the hazards and difficulties, the "foot guns", as <em>necessary</em> features - they teach you those essential survival skills that mark you out. More unkindly, they keep the stupid folk out, and help preserve the high status of those who survived long enough to be assimilated. Uh-oh, now you've invented class politics. </p><p>The problem with this thinking is that it's self-reinforcing. Working hard to master system complexities was genuinely rewarding - you really were doing difficult things and doing them well. This is actually the same mechanism behind what eventually became known as 'meritocracy'<sup id="ref1"><a href="#fn1">1</a></sup>, but the core point is simpler - if difficulty itself becomes a badge of honour, you've created a trap: anything that makes the system more approachable starts to feel like it's cheapening what you achieved. You become invested in preserving the barriers you overcame. </p><p>(This is the same mentality that built leetcode interview pipelines BTW, but let's leave that sidebar alone for now) </p><p>So the UNIX operator culture tended to operate as a tribal meritocracy (as opposed to the UNIX <em>implementer</em> culture, which fell out of a different set of cultural norms, quite an interesting side bar itself<sup id="ref2"><a href="#fn2">2</a></sup>), a cultural priesthood, somewhat self-regarding, rewarding of cleverness and knowledge hoarding, prone to feats of bravado, full of lore, with a defensive mentality of keeping the flame aloft, keeping the plebs happy and fed, and warding off the barbarians. As we entered the 90s it was already gently in decline, because centralised computing was giving way to the rise of the microcomputer, but the sudden explosive growth of the WWW pulled internet / Unix culture suddenly back into the mainstream with an enormous and public opportunity vacuum. Everyone suddenly has an urgent need to write programs that push text off UNIX file-systems (and databases) and into web pages, and Perl is uniquely positioned to have a strong first-mover advantage in this suddenly vital, novel ecosystem. But it's culture and values are very much pulled across from this previous era. </p><p>(Springing out of this, Perl had an, at best grudging, tolerance for 'difficult genius' types, alongside this baseline culture. Unfortunately, this kind of toxic personality tends to thrive in the type of culture I've described, and they do set to help the tone. I'm not here to call out people specifically, because I'm trying to make a point rather than feed a culture war, or dig up gossip, but there were several significant examples, you can probably find lore if you like. I think the kindest way I can describe the compounding effect of this is that there was a strong cultural norm along the lines of "It's OK to be rude, as long as it's for a good cause".) </p><h3>A fort within a fort</h3><p>I remember this tension as always being tangibly there. Perl IRC and mailing lists were quite cliquey and full of venerated experts and in-jokes, rough on naivety, keen on robust, verbose debate, and a little suspicious of newcomers. And very cult-like. The "<a href="https://perl.fandom.com/wiki/TIMTOWTDI">TIMTOWTDI</a>" rule, although ostensibly liberal, literally means 'there is more than one way to do it <em>in Perl</em>' - and you can perhaps infer from that that there's little to no reason to do it using anything else. Elevating extreme flexibility like this is paradoxically also an engine of conservatism. If Perl can already do anything, flexibly, in multiple ways, then the language itself doesn't need to change - 'we already have one of those here, we don't need new things'. This attitude determined how Perl intended to handle evolution: the core language would remain stable (a fort inside a fort, only accessible to high level wizards), while innovation was pushed outward to CPAN. You could add features outside of core by writing and consuming third party libraries, you could bend language behaviour with pragmas without modifying Perl itself. The very best CPAN modules could theoretically be promoted into core, allowing the language to evolve conservatively from proven, widely-used features. </p><p>On paper, this sounds reasonable. In practice, I think it encoded a fundamental conflict of interest into the community early on, and set the stage for many of the later growth problems.  I'm not going to pretend that Perl <em>invented</em> dependency hell, but I think it turned out to be another one of those profound misfeatures that their cultural philosophy lead them to mistake for virtue, and embrace. </p><p>An interesting thing I think has been missed discussing the context of the original blog piece, about whether Perl 6 significantly impacted Perl growth, is the fact that Perl 6 itself manifested out of ongoing arguments. Perl 6 is a schism. Here's a oft-cited note from Larry Wall himself about the incident that sparked Perl 6, at <strike> YAPC</strike>  <a href="https://whitecamel.org/p/jon_orwant.html">OSCON 2000</a> </p><blockquote><p>We spent the first hour gabbing about all sorts of political and organizational issues of a fairly boring and mundane nature. Partway through, Jon Orwant comes in, and stands there for a few minutes listening, and then he very calmly walks over to the coffee service table in the corner, and there were about 20 of us in the room, and he picks up a coffee mug and throws it against the other wall and he keeps throwing coffee mugs against the other wall, and he says "we are f-ed unless we can come up with something that will excite the community, because everyone's getting bored and going off and doing other things". </p></blockquote><p>(Pause a second and ask yourself about the sort of social culture that both allows this kind of behaviour at public events, and then chooses to embrace it as a key piece of cultural lore) </p><h3>The impact of Perl 6</h3><p>Perl 6 was really a <em>schism</em>. Perl was already under a great amount of strain trying to accommodate the modernising influx of post dot-com mainstream web application building, alongside the entrenched conservatism of the core maintainers, and the maintenance burden of a few years exponential growth of third-party libraries, starting to build a fractal mess of slightly differentiating, incompatible approaches of those multiple ways to do things that were effectively now table-stakes language features, as the deployment landscape started to tiptoe towards a more modern, ubiquitous WWW<sup id="ref3"><a href="#fn3">3</a></sup>. </p><p>So, while I agree that it's wrong to generalise that 'Perl 6 killed Perl', I would say that Perl 6 was a symptom of the irreconcilable internal forces that killed Perl. Although, I also intend to go on to point out that Perl isn't dead, nothing has actually <em>killed</em> Perl. Killed Perl is a very stupid way to frame the discussion, but here we are. </p><p>So... Perl 6 is created as a valve to offset that pressure, and it kind of works. Up to a point. Unfortunately I think the side effect really is that the two branches of the culture, in the process of forking, double down on their encoded norms. Perl 5.x beds down as the practical, already solved way to do all the same things, with little need to change. Any requirements for more <em>modern</em> application patterns that are emerging in the broader web development environment, like idk, Unicode, REST clients, strict data structures, asynchronous I/O, whatever? That can <em>either</em> wait for Perl6 or you can pull things together using the CPAN if you want to move right now. Perl 6 leans the other way - they don't need to ship immediately, we have Perl 5 already here for doing things, Perl 6 is going to innovate on <em>everything</em>, and spend it's time getting there, designing up-front.<sup id="ref4"><a href="#fn4">4</a></sup> They spend at least two years writing high level requirement specs. They even spin out a side-project trying to build a universal virtual machine to run all dynamic programming languages that never delivers<sup id="ref5"><a href="#fn5">5</a></sup> </p><p>This is the landscape where Perl's central dominance of 'back end' web programming continues to slip. Unfortunately, alongside the now principled bias toward cultural conservatism, Perl 5 has an explicit excuse for it. The future is over there, and exciting, and meanwhile we're working usefully, and getting paid, and getting stuff done. Kind of OK from inside the fort. Some day we'll move to the newer fort, but right now <em>this is fine</em>. Not very attractive to newcomers though, really. And this is also sort of OK, because Perl doesn't really want those sort of newcomers, does it? The kind that turns up on IRC or forums and asks basic questions about Perl 6 and sadly often gets treated with open contempt. </p><h3>Meanwhile, over there</h3><p>Ruby has sprouted "Ruby on Rails", and it's taken the dynamic web building world by storm. Rails is a second generation web framework, that's proudly an 'opinionated web framework'. Given that the web application architecture is starting to stabilise into a kind of three-tier system , with a client as a web browser, a middle tier as a monolithic application server, and a persistence layer as a relational database , and a split server architecture serving static and dynamic content from different routes, here is just one way to do that, with hugely developer friendly tooling turning this into a cookie-cutter solution for the 80% core, and a plugin and client-side decoration approach that allows for the necessary per-site customisation. </p><p>Ruby is interesting as well. Ruby is kind of a Perl6 really. More accurately it's <a href="https://ruby-doc.org/docs/ruby-doc-bundle/FAQ/FAQ.html">a parallel universe Perl5</a> Ruby comes from Japan, and has developed as an attempt to build something similar to Perl, but it's developed much later, by programming language enthusiasts, and for the first ten years or so, it's mostly only used in Japan. To my line of thinking this is probably important. Ruby does not spring from decades of sysadmin or sysop culture. Ruby is a language for programmers, and is at this point an sensible candidate for building something like Rails with - a relatively blank canvas for dynamic programming, with many of the same qualities as Perl, with less legacy cruft, and more modern niceties, like an integrated object system, exceptions, straightforward data structures. Ruby also has adopted 'friendliness' as a core value, and the culture over there adopts a principled approach to aggressively welcoming newcomers, promoting easiness, and programmer happiness and convenience as strong first class principles. </p><p>Rails is a <em>huge</em> hit. At this point, which is around about the time I stopped significantly using Perl (2004-2005) (because I quit my job, not out of any core animosity toward it, in fact, in my day, I was really quite a Perl <em>fan</em>), Rails is the most appealing place to start as a new web programmer. Adoption rate is high, community is great, velocity of development is well-paced, and there's a lovely , well-lit, onboarding pipeline for how to start. You don't even really need to know ruby. It has a one-shot install tool, and generates working websites from templates, almost out of the box. It's an obvious starting point. </p><p>Perl being Perl, develops several analogue frameworks to Rails, all of them interdependently compatible and incompatible with each other and each other's dependencies, all of them designed to be as customisable and as user configurable as they possibly can be<sup id="ref6"><a href="#fn6">6</a></sup> </p><h3>PHP</h3><p>There are also the other obvious contenders. PHP has been there all along, and it's almost coming up from entirely the opposite cultural background of Perl. PHP is <em>a users language</em>. It's built to be deployed by copying script files to your home directory, with minimal server side impact or privileges. It's barely designed at all, but it encounters explosive growth all the way through the first (and through into the second) web era, almost entirely because it makes the barrier to onboarding so low as to be non-existent. PHP gets a couple of extra free shots in the arm </p><ol><li>Because it's architecture is so amenable to shared-server hosting, it is adopted as the primary implementation language of the blogging boom. An entire generation of web developers is born of installing and customising WordPress and text-pattern et. al by installing it directly into your home directory on a rented CPanel host account. It's the go-to answer for 'I'm not a programmer really but how do I get a personal web site'<sup id="ref7"><a href="#fn7">7</a></sup> This zero gate-keeping approach keeps the PHP stack firmly on the table of 'basic' web programmers all through the history of the web up to the current day. </li><li>Because of these initially lightweight deployment targets, PHP scales like little else, mostly because it's execution model leans strongly towards idempotent execution, with each web request tearing up and tearing down the whole environment. In a sense, this is slower than keeping hot state around, but it does lend itself extremely well to shared-nothing horizontal scaling, which as the web user base increases gigantically throughout the 2000s era, is the simplest route to scaling out. Facebook famously, is built in PHP at this point in time. </li></ol><h3>Python</h3><p>There is of course one other big horse in the race in this era, and it's a particularly interesting one in many ways, certainly when contrasted with Perl. This is of course, Python. Python is a close contemporary of Perl's but once again, it's roots are somewhere very different. Python doesn't come from UNIX culture either. Python comes from academia, and programming language culture. It's kind of a forgotten footnote, but Python was originally built for the <a href="https://en.wikipedia.org/wiki/Amoeba_(operating_system">Amoeba operating system</a>, and it's intention was to be a straightforward programming language for scripting this<sup id="ref8"><a href="#fn8">8</a></sup>. The idea was to build a language that could be the 'second programming language' for programmers. Given that this is the 1980s, early 1990s, the programmers would be expected to be mostly using C / C++ ,perhaps Pascal. Python was intended to allow faster development for lighter weight programs or scripting tasks. I suppose the idea was to take something that you might want to build in a shell script, but provide enough high level structured support that you could cleanly build the kind of things that quickly become a problem in shell scripts. So, it emphasises data structures, and scoped variables, and modules, and prioritises making it possible to extend the language with modules. Typical things that experienced programmers would want to use. The language was also designed to be portable between the different platforms programmers would use, running on the desktops of the day, but also on the server. As a consequence, it had a broad standard library of common <em>portable</em> abstractions around standard system features - file-systems, concurrency, time, FFI. For quite a long time, one of python's standard mottoes was 'batteries included'. </p><p>Python never set the world on fire at any particular moment, but it remained committed to a clear evolutionary incremental development, and clean engineering principles. Again, I think the key element here is cultural tone. Python is kind of boring, not trying to be anyone's best language, or even a universal language. Python was always a little fussy, maybe snobby, slightly abstracted away from the real world. It's almost as old as Perl and it just kept incrementally evolving, picking up users, picking up features, slowly broadening the standard library. The first time I saw Python pick up an undeniable mainstream advantage would also have been around the early 2000s, when Google publicly adopted it as one of their house standard languages. Never radical, just calmly evolving in it's environs. </p><h3>Nature abhors a vacuum</h3><p>When I sketch out this landscape, I remain firmly convinced that most of Perl's impedance to continued growth were cultural. Perl's huge moment of relevance in the 90s was because it cross-pollinated two diverging user cultures. Traditional UNIX / database / data-centre maintenance and admin users, and enthusiastic early web builders and scalers. It had a cultural shock phase from extremely rapid growth, the centre couldn't hold, and things slowly fell apart. </p><p>Circling back though, it's time to address the real elephant in the room. Perl manifestly did not die. It's here right now. It's installed I think by default, on almost every single computer I own and operate, without me doing a single thing to make that happen. It's still used every day by millions of people on millions of systems (even if that isn't deliberate). It's still used by many people <em>entirely deliberately</em> for building software, whether that's because they know it and like it and it works, or because they're interfacing with or working on legacy Perl systems (of which there are still many), or maybe they're using it still in it's original intentional role - A capable POSIX-native scripting language, with much better performance and a broader feature-set than any shell or awk. I still occasionally break it out myself, for small scripts I would like to use more than once, or as parts of CLI pipelines. </p><p>What I don't do any more is reach for Perl <em>first</em> to make anything new. In my case, it's just because I typically am spoilt for options that are a better fit for most tasks, depending on whatever it is I'm trying to achieve. By the time I came to Perl, (1998-ish), I was already on my third career phase, I had a strong UNIX background, and had already built real things in lisp, java, pascal, visual basic and C++. My attitude to languages was already informed by picking a tool to fit the task at hand. Boy did I love Perl for a few years. The product/market-fit for those early web days was just beautiful. The culture did have too much of the negative tropes I've been pointing at, but that wasn't really a problem personally for me, I'd grown up amongst the BOFHs inside the data centres already, it wasn't too hard for me to assimilate, nor pick up the core principles. I did occasionally bounce off a couple of abrasive characters in the community, but mostly this just kept me loosely coupled, I enjoyed how the language solved the problems I needed solving quickly, I enjoyed the flexibility, and I also enjoyed the way that it made me feel smart, and en-route to my wizard's robes and hat, when i used it to solve harder problems in creative ways, or designed ways around bugs and gremlins. For a good 3-4 years I would have immediately picked it as my favourite language. </p><p>So as I say, I didn't fall out of it with any sense of pique, I just naturally moved to different domains, and picked up tools that best fit. After Amazon, I spent t a lot of time concentrating on OS X and audio programming, and that involved a lot of objective C, C++. The scripting tools in that domain were often in ruby, sometimes python. For personal hacking, I picked up lisp again<sup id="ref9"><a href="#fn9">9</a></sup> (which I'd always enjoyed in school). I dipped in and out of Perl here and there for occasional contract work, but I tended to gravitate more towards larger database stuff, where I typically found C, java and python. The next time I was building web things, it was all Rails and ruby, and then moving towards the web services / REST / cloud era, the natural fits were go, and of course node and JavaScript or Typescript. I've always been a polyglot, and I've always been pretty comfortable moving between programming languages. The truth of the matter is, that the majority of programming work is broadly similar, and the specific implementation details of the language you use don't matter all that much, if it's a good fit for the circumstances. </p><p>I can't imagine Perl disappearing entirely in my lifetime. I can remember entire programming environments and languages that are much, much deader than I can ever see Perl becoming. </p><ul><li>Pascal used to be <em>huge</em> for teaching and also for desktop development in the 8/16 bit era</li><li>Objective C - only really useful inside the Apple ecosystem, and they're hell bent on phasing it out.</li><li>Before I got into the Internet, I used to build application software for 16 bit Windows (3.11) which was a vast market, in a mixture of database 4GLs (like PowerBuilder, Gupta/Centura SQLWindows) and Win16 C APIs. This entire universe basically no longer exists, and is fully obsolete. There must be many similar cases.</li><li>I mean who the hell realistically uses common lisp any more outside of legacy or enthusiast markets? Less people than Perl I'm sure.</li></ul><p>Perl also got to be if not first, then certainly early to dominate a new market paradigm. Plenty of things never manage that. It's hard to see Perl as anything other than an enormous success on these terms. Perl innovated and influenced languages that came after in some truly significant ways. </p><ul><li>Tightly embedding regular expressions and extending regular expressions (the most commonly used regular expression dialect in other tools is Perl)</li><li>CPAN, for package/library distribution via the internet, with dependency resolution - and including important concepts like supply chain verification with strong package signatures</li><li>A huge emphasis on testing, automated test harnesses, and CI. Perl test format (TAP) is also widely found in other CI/harness systems</li><li>Blending the gap between shell / scripting / and system programming in a single tool. I suppose this is debatable, but the way Perl basically integrated all the fundamental POSIX/libc as native built-ins with broadly the same semantics, but with managed memory and shell conventions was really revolutionary. Before this, most languages I had ever seen broadly tended to sit in one box, afterwards, most languages tended to span across several.</li><li>Amazing integrated documentation, online, in-tool and also man pages. POD is maybe the most successful ever implementation of literate programming ideas (although most of the real docs don't intertwingle the documentation very much iirc)</li></ul><p>Just these points, and I'm sure there are many others that could be made, are enough of a legacy to be proud of. </p><p>Counterfactuals are stupid (but also fun). If I squint, I can imagine that a Perl with a less reactionary culture, and a healthier acceptance of other ideas and environmental change might have been able to evolve alongside the other tools in the web paradigm shift, and still occupy a more central position in today's development landscape. That's not the Perl we have though, and that didn't happen. And I'm very confident that without the Perl we did have, the whole of modern software practice would be differently shaped. I do think Perl now lives in a legacy role, with a declining influence, but that's really nothing to feel shame or regret for. Nobody is going to forcibly take Perl away as long as POSIX exists, and so far as I can see, that means forever. In 2025 too, I can see the invisible hand creeping up on some of these other systems I've mentioned. Rust is slowly absorbing C and C++. Ruby (and of course Rails) is clearly in decline, in a way that probably consigns it to become a similar legacy state. From a certain angle, it looks a lot like Typescript is slowly supplanting Python. I won't be entirely surprised if that happens, although at my age I kind of doubt I'll live to see the day. </p><h3>Footnotes</h3><p><a id="fn1" href="#ref1">1</a> : Meritocracy is a fun word. It was originally coined as a pejorative term to describe a dystopian mechanism by which modern i.e. Western / British society entrenches and justifies an unfair and unequal distribution of privilege </p><p><a id="fn2" href="#ref2">2</a> : The UNIX <em>implementer</em> culture, is scientific/academic and fell out of Bell Labs. I guess you could extend this school of thought as a cultural sweep towards building abstracted cloud operations, toward plan 9/ Inferno / go </p><p><a id="fn3" href="#ref3">3</a> : Web 2.0 was first <a href="https://www.webdesignmuseum.org/web-design-history/web-2-0-1999">defined in 1999</a> by <a href="http://darcyd.com/fragmented_future.pdf">Darcy DiNucci in a print article</a> , the term didn't become mainstream until it was picked up and promoted by Tim O'Reilly (then owner/operator of perl.com, trivia fans), an astute inside observer of the forces driving web development </p><p><a id="fn4" href="#ref4">4</a>: Another unfortunate bit of luck here. Right at the point of time that <em>'agile'</em> starts getting some traction as a more natural way to embrace software development - i.e. iterating in small increments against a changing environment and requirements, Perl 6 decides to do perhaps the most waterfall open source development process ever attempted. . It is fifteen years before Perl 6 ships something resembling a usable programming language.<br>
</p><p><a id="fn5" href="#ref5">5</a> : <a href="http://www.parrot.org/">The Parrot VM</a>, a lovely quixotic idea, which sadly fizzled out, after even Perl 6 stopped trying to target it. Interestingly enough, both python and ruby both made relatively high profile ports to the JVM that were useful enough to be used for production deploys in certain niches. </p><p><a id="fn6" href="#ref6">6</a> : A side effect of this degree of abstraction, is that as well as being very hard to get started, it's easy to fall foul of performance overhead. </p><p><a id="fn7" href="#ref7">7</a> : This ubituitious ecosystem of small footprint wordpress custom installs gives birth to the web agency model of commercial website building / small ecommerce sites, which thrives and is suprisingly healthy today. Recent, and slighly optimistic surveys have pitched WordPress as powering over 40% of all websites today. Now this is certainly inflated, but even if the realistic number is half of that, that's still pretty damn healthy. </p><p><a id="fn8" href="#ref8">8</a> : It's often repeated that Python was designed as a teaching language, but as far as I know, that's not actually the case. The designer of Python, Guido Van Rossum <a href="https://www.artima.com/articles/the-making-of-python">was previously working on a project</a> that <em>was</em> a intended as training language, called ABC, and many of ABC's syntax and structural features influenced or made their way into Python. </p><p><a id="fn9" href="#ref9">9</a> : Common lisp is a better answer to an infinitely flexible 'everything' chainsaw language than perl, IMHO </p>
			    <section>
			      <span>posted by <a rel="me" href="https://www.beatworm.co.uk/">cms</a></span><span> on <time datetime="2025-11-20">2025-11-20</time></span>
			    </section>
			  <p>tagged as</p>
		      </div>

              </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HTML as an Accessible Format for Papers (232 pts)]]></title>
            <link>https://info.arxiv.org/about/accessible_HTML.html</link>
            <guid>46173825</guid>
            <pubDate>Sat, 06 Dec 2025 14:59:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://info.arxiv.org/about/accessible_HTML.html">https://info.arxiv.org/about/accessible_HTML.html</a>, See on <a href="https://news.ycombinator.com/item?id=46173825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div data-md-component="content">
              <article>
                
                  

  <a href="https://github.com/arXiv/arxiv-docs/blob/develop/source/about/accessible_HTML.md" title="Edit this page">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"></path></svg>
  </a>


<h2 id="html-as-an-accessible-format-for-papers">HTML as an accessible format for papers</h2>
<p>Accessibility barriers in research are not new, but they are urgent. The message we have heard from our community is that arXiv can have the most impact in the shortest time by offering HTML papers alongside the existing PDF.</p>
<p>arXiv has successfully launched papers in HTML format. We are gradually backfilling HTML for arXiv's corpus of over 2 million papers over time. Not every paper can be successfully converted, so a small percentage of papers will not have an HTML version. We will work to improve conversion over time.</p>
<p>The link to the HTML format will appear on abstract pages below the existing PDF download link. Authors will have the opportunity to preview their paper’s HTML as a part of the submission process.</p>
<p>The beta rollout is just the beginning. We have a long way to go to improve HTML papers and will continue to solicit feedback from authors, readers, and the entire arXiv community to improve conversions from LaTeX.</p>
<h2 id="why-experimental-html">Why "experimental" HTML?</h2>
<p>Did you know that 90% of submissions to arXiv are in TeX format, mostly LaTeX? That poses a unique accessibility challenge: to accurately convert from TeX—a very extensible language used in myriad unique ways by authors—to HTML, a language that is much more accessible to screen readers and text-to-speech software, screen magnifiers, and mobile devices. In addition to the technical challenges, the conversion must be both rapid and automated in order to maintain arXiv’s core service of free and fast dissemination.</p>
<p>Because of these challenges we know there will be some conversion and rendering issues. We have decided to launch in beta with “experimental” HTML because:</p>
<ol>
<li>Accessible papers are needed now. We have talked to the arXiv community, especially researchers with accessibility needs, and they overwhelmingly asked us not to wait.</li>
<li>We need your help. The obvious work is done. Reports from the community will help us identify issues we can track back to specific LaTeX packages that are not converting correctly.</li>
</ol>
<h2 id="error-messages-you-may-see-in-html-papers">Error messages you may see in HTML papers</h2>
<p>HTML papers on arXiv.org are a work in progress and will sometimes display errors. As we work to improve accessibility we share with you the causes of these errors and what authors can do to help minimize them. <a href="https://info.arxiv.org/about/accessibility_html_error_messages.html">Learn more about error messages you may see in HTML papers</a></p>
<h2 id="ways-to-help">Ways to help</h2>
<h3 id="1-read-html-papers-and-report-issues">1) Read HTML papers and report issues</h3>
<p>We encourage the community to try out HTML papers in your field:</p>
<h4 id="report-an-issue">Report an issue</h4>
<ul>
<li>Go to the abstract page for a paper you are interested in reading.</li>
<li>Look in the section where you find the link to the PDF download, and click the new link for HTML.</li>
<li>Report issues by either <strong>a)</strong> clicking on the Open Issue button <strong>b)</strong> selecting text and clicking on the Open Issue for Selection button or <strong>c)</strong> use <code>Ctrl+?</code> on your keyboard. If you are using a screen reader, use <code>Alt+y</code> to toggle accessible reporting buttons per paragraph.</li>
</ul>
<p><strong>Please do not create reports that the HTML paper doesn't look exactly like the PDF paper</strong></p>
<p>Our primary goal for this project is to make papers more accessible, so the focus during the beta phase will value function over form. HTML layouts that are incorrect or are illegible are important to report. But we do expect the HTML papers to present differently than the same paper rendered in PDF. Line breaks will occur in different places and there is likely to be more white space. In general, the HTML paper won't present as compactly. Intricate typographic layouts will not be rendered so intricately. This is by design.</p>
<p>HTML is a different medium and brings its own advantages versus PDF. In addition to being much more compatible with assistive technologies, HTML does a far better job adapting to the characteristics of the device you are reading on, including mobile devices.</p>
<h3 id="2-help-improve-the-conversion-from-latex">2) Help improve the conversion from LaTeX</h3>
<p>If you are an author you can help us improve conversions to HTML by following our guide to <a href="https://info.arxiv.org/help/submit_latex_best_practices.html">LaTeX Markup Best Practices for Successful HTML Papers</a>.</p>
<p>If you are a developer and have free development cycles, help us improve conversions! Our collaborators at LaTeXML maintain a <a href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML">list of issues</a> and welcome feedback and developer contributions.</p>
<p>If you are a publisher, member of a society, or conference organizer you can help us improve conversions to HTML by reviewing the .cls files your organization recommends to authors for unsupported packages. Providing .cls files that use supported packages is an easy way to support and sow accessibility in the scientific community. </p>
<h2 id="thank-you-to-our-collaborators">Thank you to our collaborators</h2>
<p>First, we want to share a special thank you to all the scientists with disabilities who have generously shared their insights, expertise, and guidance throughout this project.</p>
<p>We want to thank two organizations without which HTML papers on arXiv would not be possible: The <a href="https://www.latex-project.org/">LaTeX Project</a>, and the <a href="https://math.nist.gov/~BMiller/LaTeXML/">LaTeXML</a> team from NIST. We deeply thank each member of these teams for their knowledge, incredible work, and commitment to accessibility.</p>





                
              </article>
            </div>
        
      </main>
      






    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tiny Core Linux: a 23 MB Linux distro with graphical desktop (418 pts)]]></title>
            <link>http://www.tinycorelinux.net/</link>
            <guid>46173547</guid>
            <pubDate>Sat, 06 Dec 2025 14:18:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.tinycorelinux.net/">http://www.tinycorelinux.net/</a>, See on <a href="https://news.ycombinator.com/item?id=46173547">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
        <h3>Welcome to The Core Project - Tiny Core Linux</h3>
        
        <p>The Core Project is a highly modular based system with community build extensions.
</p><p>
 It starts with a recent Linux kernel, vmlinuz, and our root filesystem and start-up scripts packaged with a basic set of kernel modules in core.gz.
Core (11MB) is simply the kernel + core.gz - this is the foundation for user created desktops, servers, or appliances.
TinyCore is Core + Xvesa.tcz + Xprogs.tcz + aterm.tcz + fltk-1.3.tcz + flwm.tcz + wbar.tcz
</p><p>
TinyCore becomes simply an example of what the Core Project can produce, an 16MB FLTK/FLWM desktop.
</p><p>
CorePlus ofers a simple way to get started using the Core philosophy with its included community packaged
extensions enabling easy embedded frugal or pendrive installation of the user's choice of supported desktop, while
maintaining the Core principal of mounted extensions with full package management.
</p><p>

It is not a complete desktop nor is all hardware completely supported. It represents only the core needed to boot into a very minimal X desktop typically with wired internet access.</p><p>

The user has complete control over which applications and/or additional hardware to have supported, be it for a desktop, a netbook, an appliance, or server, selectable by the user by installing additional applications from online repositories, or easily compiling most anything you desire using tools provided.</p>

<p>The latest version: <b>16.2</b></p>

<h3>News</h3>



<h3>About Our Project</h3>

<p>Our goal is the creation of a nomadic ultra small graphical desktop operating system capable of booting from cdrom, pendrive, or frugally from a hard drive. The desktop boots extremely fast and is able to support additional applications and hardware of the users choice. While Tiny Core always resides in ram, additional applications extensions can either reside in ram, mounted from a persistent storage device, or installed into a persistent storage device.</p>

<p>We invite interested users and developers to explore Tiny Core. Within our forums we have an open developement model. We encourage shared knowledge. We promote community involvement and community built application extensions. Anyone can contribute to our project by packaging their favorite application or hardware support to run in Tiny Core. The Tiny Core Linux Team currently consists of eight members who peruse the forums to assist from answering questions to helping package new extensions.
</p><p>
Join us here and on IRC Freenode <a href="irc://irc.freenode.net/tinycorelinux">#tinycorelinux</a>.
</p><p>
Learn. Share. Grow your knowledge of Linux.
</p><p>
Robert Shingledecker, December 01, 2008 </p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GrapheneOS is the only Android OS providing full security patches (570 pts)]]></title>
            <link>https://grapheneos.social/@GrapheneOS/115647408229616018</link>
            <guid>46173407</guid>
            <pubDate>Sat, 06 Dec 2025 13:58:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://grapheneos.social/@GrapheneOS/115647408229616018">https://grapheneos.social/@GrapheneOS/115647408229616018</a>, See on <a href="https://news.ycombinator.com/item?id=46173407">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[How I discovered a hidden microphone on a Chinese NanoKVM (404 pts)]]></title>
            <link>https://telefoncek.si/2025/02/2025-02-10-hidden-microphone-on-nanokvm/</link>
            <guid>46173383</guid>
            <pubDate>Sat, 06 Dec 2025 13:54:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://telefoncek.si/2025/02/2025-02-10-hidden-microphone-on-nanokvm/">https://telefoncek.si/2025/02/2025-02-10-hidden-microphone-on-nanokvm/</a>, See on <a href="https://news.ycombinator.com/item?id=46173383">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/BlogPosting">
	<div itemprop="articleBody">
		<p>NanoKVM is a <strong>hardware KVM switch</strong> developed by the Chinese company Sipeed. Released last year, it enables remote control of a computer or server using a virtual keyboard, mouse, and monitor. Thanks to its compact size and low price, it quickly gained attention online, especially when the company promised to release its code as open-source. However, as we’ll see, the device has some serious security issues. But first, let’s start with the basics.</p>

<h2 id="how-does-the-device-work">How Does the Device Work?</h2>

<p>As mentioned, NanoKVM is a KVM switch designed for remotely controlling and managing computers or servers. It features an HDMI port, three USB-C ports, an Ethernet port for network connectivity, and a special serial interface. The package also includes a small accessory for managing the power of an external computer.</p>

<p>Using it is quite simple. First, you connect the device to the internet via an Ethernet cable. Once online, you can access it through a standard web browser (though <em>JavaScript JIT</em> must be enabled). The device supports Tailscale VPN, but with some effort (read: hacking), it can also be configured to work with your own VPN, such as WireGuard or OpenVPN server. Once set up, you can control it from anywhere in the world via your browser.</p>

<div>
<p><a href="https://telefoncek.si/static/2025/02/NanoKVM.jpg">
<img src="https://telefoncek.si/static/2025/02/NanoKVM.jpg" alt="NanoKVM"></a></p><p>
NanoKVM
</p>
</div>

<p>The device could be connected to the target computer using an HDMI cable, capturing the video output that would normally be displayed on a monitor. This allows you to view the computer’s screen directly in your browser, essentially acting as a virtual monitor.</p>

<p>Through the USB connection, NanoKVM can also emulate a keyboard, mouse, CD-ROM, USB drive, and even a USB network adapter. This means you can remotely control the computer as if you were physically sitting in front of it - but all through a web interface.</p>

<p>While it functions similarly to remote management tools like RDP or VNC, it has one key difference: there’s no need to install any software on the target computer. Simply plug in the device, and you’re ready to manage it remotely. NanoKVM even allows you to enter the BIOS, and with the additional accessory for power management, you can remotely turn the computer on, off, or reset it.</p>

<p>This makes it incredibly useful - you can power on a machine, access the BIOS, change settings, mount a virtual bootable CD, and install an operating system from scratch, just as if you were physically there. Even if the computer is on the other side of the world.</p>

<p>NanoKVM is also quite affordable. The fully-featured version, which includes all ports, a built-in mini screen, and a case, costs just over €60, while the stripped-down version is around €30. By comparison, a similar RaspberryPi-based device, PiKVM, costs around €400. However, PiKVM is significantly more powerful and reliable and, with a KVM splitter, can manage multiple devices simultaneously.</p>

<p>As mentioned earlier, the announcement of the device caused quite a stir online - not just because of its low price, but also due to its compact size and minimal power consumption. In fact, it can be powered directly from the target computer via a USB cable, which it also uses to simulate a keyboard, mouse, and other USB devices. So you have only one USB cable - in one direction it powers NanoKVM, on the other it helps it to simulate keyboard mouse and other devices on a computer you want to manage.</p>

<p>The device is built on the open-source RISC-V processor architecture, and the manufacturer eventually did release the device’s software under an open-source license at the end of last year. (To be fair, one part of the code remains closed, but the community has already found a suitable open-source replacement, and the manufacturer has promised to open this portion soon.)</p>

<p><strong>However, the real issue is security.</strong></p>

<p>Understandably, the company was eager to release the device as soon as possible. In fact, an early version had a minor hardware design flaw - due to an incorrect circuit cable, the device sometimes failed to detect incoming HDMI signals. As a result, the company recalled and replaced all affected units free of charge. Software development also progressed rapidly, but in such cases, the primary focus is typically on getting basic functionality working, with security taking a backseat.</p>

<p>So, it’s not surprising that the developers made some serious missteps - rushed development often leads to stupid mistakes. But some of the security flaws I discovered in my quick (and by no means exhaustive) review are genuinely concerning.</p>

<p>One of the <a href="https://www.hackster.io/news/security-researcher-warns-on-sipeed-s-nanokvm-finds-vulnerabilities-and-a-cat-in-the-firmware-e1157a9ff0f4">first security analysis revealed numerous vulnerabilities</a> - and some rather bizarre discoveries. For instance, a security researcher even found an image of a cat embedded in the firmware. While the Sipeed developers acknowledged these issues and relatively quickly fixed at least some of them, many remain unresolved.</p>

<div>
<p><a href="https://telefoncek.si/static/2025/02/device.jpg">
<img src="https://telefoncek.si/static/2025/02/device.jpg" alt="NanoKVM"></a></p><p>
NanoKVM
</p>
</div>

<p>After purchasing the device myself, I ran a quick security audit and found several alarming flaws. The device initially came with a default password, and <code>SSH</code> access was enabled using this preset password. I reported this to the manufacturer, and to their credit, they fixed it relatively quickly. However, many other issues persist.</p>

<p>The user interface is riddled with security flaws - there’s no CSRF protection, no way to invalidate sessions, and more. Worse yet, the encryption key used for password protection (when logging in via a browser) is <strong>hardcoded and identical</strong> across all devices. This is a major security oversight, as it allows an attacker to easily decrypt passwords. More problematic, this needed to be explained to the developers. Multiple times.</p>

<p>Another concern is the device’s reliance on Chinese DNS servers. And configuring your own (custom) DNS settings is quite complicated. Additionally, the device communicates with Sipeed’s servers in China - downloading not only updates but also the closed-source component mentioned earlier. For this closed source component it needs to verify an identification key, which is stored on the device in plain text. Alarmingly, the device does not verify the integrity of software updates, includes a strange version of the WireGuard VPN application (which does not work on some networks), and runs a heavily stripped-down version of Linux that lacks <code>systemd</code> and <code>apt</code>. And these are just a few of the issues.</p>

<p>Were these problems simply oversights? Possibly. But what additionally raised red flags was the presence of <code>tcpdump</code> and <code>aircrack</code> - tools commonly used for network packet analysis and wireless security testing. While these are useful for debugging and development, they are also <strong>hacking tools</strong> that can be dangerously exploited. I can understand why developers might use them during testing, but they have absolutely no place on a production version of the device.</p>

<p>A Hidden Microphone</p>

<p>And then I discovered something even more alarming - <strong>a tiny built-in microphone that isn’t clearly mentioned in the official documentation</strong>. It’s a miniature SMD component, measuring just 2 x 1 mm, yet capable of recording surprisingly high-quality audio.</p>

<p>What’s even more concerning is that all the necessary recording tools are already installed on the device! By simply connecting via <code>SSH</code> (remember, the device initially used default passwords!), I was able to start recording audio using the amixer and arecord tools. Once recorded, the audio file could be easily copied to another computer. With a little extra effort, it would even be possible to stream the audio over a network, allowing an attacker to eavesdrop in real time.</p>

<div>
<p><a href="https://telefoncek.si/static/2025/02/hidden_microphone.jpg">
<img src="https://telefoncek.si/static/2025/02/hidden_microphone.jpg" alt="Hidden Microphone in NanoKVM"></a></p><p>
Hidden Microphone in NanoKVM
</p>
</div>

<p>Physically removing the microphone is possible, but it’s not exactly straightforward. As seen in the image, disassembling the device is tricky, and due to the microphone’s tiny size, you’d need a microscope or magnifying glass to properly desolder it.</p>

<p><strong>To summarize</strong>: the device is riddled with security flaws, originally shipped with default passwords, communicates with servers in China, comes preinstalled with hacking tools, and even includes a built-in microphone - fully equipped for recording audio - without clear mention of it in the documentation. Could it get any worse?</p>

<p>I am pretty sure these issues stem from extreme negligence and rushed development rather than malicious intent. However, that doesn’t make them any less concerning.</p>

<p>That said, these findings don’t mean the device is entirely unusable.</p>

<p>Since the device is open-source, it’s entirely possible to install custom software on it. In fact, <a href="https://github.com/scpcom/sophgo-sg200x-debian">one user has already begun porting his own Linux distribution</a> - starting with Debian and later switching to Ubuntu. With a bit of luck, this work could soon lead to official Ubuntu Linux support for the device.</p>

<p>This custom Linux version already runs the manufacturer’s modified KVM code, and within a few months, we’ll likely have a fully independent and significantly more secure software alternative. The only minor inconvenience is that installing it requires physically opening the device, removing the built-in SD card, and flashing the new software onto it. However, in reality, this process isn’t too complicated.</p>

<p>And while you’re at it, you might also want to remove the microphone… or, if you prefer, connect a speaker. In my test, I used an 8-ohm, 0.5W speaker, which produced surprisingly good sound - essentially turning the NanoKVM into a tiny music player. Actually, the idea is not so bad, because <a href="https://docs.pikvm.org/audio/">PiKVM also included 2-way audio support for their devices end of last year</a>.</p>

<div>
<p><a href="https://telefoncek.si/static/2025/02/speaker.jpg">
<img src="https://telefoncek.si/static/2025/02/speaker.jpg" alt="Basic board with speaker"></a></p><p>
Basic board with speaker
</p>
</div>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>All this of course raises an interesting question: How many similar devices with hidden functionalities might be lurking in your home, just waiting to be discovered? And not just those of Chinese origin. Are you absolutely sure none of them have built-in miniature microphones or cameras?</p>

<p>You can start with your iPhone - <a href="https://arstechnica.com/tech-policy/2025/01/apple-agrees-to-pay-95m-delete-private-conversations-siri-recorded/">last year Apple has agreed to pay $95 million to settle a lawsuit alleging that its voice assistant Siri recorded private conversations</a>. They shared the data with third parties and used them for targeted ads. “Unintentionally”, of course! Yes, that Apple, that cares about your privacy so much.</p>

<p>And Google is doing the same. They are facing a similar lawsuit over their voice assistant, but the litigation likely won’t be settled until this fall. So no, small Chinese startup companies are not the only problem. And if you are worried about Chinese companies obligations towards Chinese government, let’s not forget that U.S. companies also have obligations to cooperate with U.S. government. While Apple is publicly claiming they do not cooperate with FBI and other U. S. agencies (because thy care about your privacy so much), some media revealed that Apple was holding a series secretive Global Police Summit at its Cupertino headquarters <a href="https://www.forbes.com/sites/thomasbrewster/2024/10/09/apple-sells-privacy-to-consumers-but-its-quietly-helping-police-use-iphones-for-surveillance/">where they taught police how to use their products for surveillance and policing work</a>. And as one of the police officers pointed out - he has “<em>never been part of an engagement that was so collaborative</em>.”. Yep.</p>

<h3 id="ps-how-to-record-audio-on-nanokvm">P.S. How to Record Audio on NanoKVM</h3>

<p>If you want to test the built-in microphone yourself, simply connect to the device via <code>SSH</code> and run the following two commands:</p>

<ul>
  <li><code>amixer -Dhw:0 cset name='ADC Capture Volume 20'</code> (<em>this sets microphone sensitivity to high</em>)</li>
  <li><code>arecord -Dhw:0,0 -d 3 -r 48000 -f S16_LE -t wav test.wav &amp; &gt; /dev/null &amp;</code> (<em>this will capture the sound to a file named <code>test.wav</code></em>)</li>
</ul>

<p>Now, speak or sing (perhaps the Chinese national anthem?) near the device, then press <code>Ctrl + C</code>, copy the <code>test.wav</code> file to your computer, and listen to the recording.</p>

	</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Speed Matters (123 pts)]]></title>
            <link>https://lemire.me/blog/2025/12/05/why-speed-matters/</link>
            <guid>46172902</guid>
            <pubDate>Sat, 06 Dec 2025 12:46:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lemire.me/blog/2025/12/05/why-speed-matters/">https://lemire.me/blog/2025/12/05/why-speed-matters/</a>, See on <a href="https://news.ycombinator.com/item?id=46172902">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main">

		
<article id="post-22361">
	
		<p><img width="825" height="510" src="https://lemire.me/blog/wp-content/uploads/2025/12/Capture-decran-le-2025-12-06-a-14.08.31-825x510.png" alt="" decoding="async" fetchpriority="high">	</p><!-- .post-thumbnail -->

	
	<!-- .entry-header -->

	<div>
		<p>The one constant that I have observed in my professional life is that people underestimate the need to move fast.</p>
<p>Of course, doing good work takes time. I once spent six months writing a URL parser. But the fact that it took so long is not a feature, it is not a positive, it is a negative.</p>
<p>If everything is slow-moving around you, it is likely not going to be good. To fully make use of your brain, you need to move as close as possible to the speed of your thought.</p>
<p>If I give you two PhD students, one who completed their thesis in two years and one who took eight years… you can be almost certain that the two-year thesis will be much better.</p>
<p>Moving fast does not mean that you complete your projects quickly. Projects have many parts, and getting everything right may take a long time.</p>
<p>Nevertheless, you should move as fast as you can.</p>
<p>For multiple reasons:</p>
<p>1. A common mistake is to spend a lot of time—too much time—on a component of your project that does not matter. I once spent a lot of time building a podcast-like version of a course… only to find out later that students had no interest in the podcast format.</p>
<p>2. You learn by making mistakes. The faster you make mistakes, the faster you learn.</p>
<p>3. Your work degrades, becomes less relevant with time. And if you work slowly, you will be more likely to stick with your slightly obsolete work. You know that professor who spent seven years preparing lecture notes twenty years ago? He is not going to throw them away and start again, as that would be a new seven-year project. So he will keep teaching using aging lecture notes until he retires and someone finally updates the course.</p>
<p>What if you are doing open-heart surgery? Don’t you want someone who spends days preparing and who works slowly? No. You almost surely want the surgeon who does many, many open-heart surgeries. They are very likely to be the best one.</p>
<p>Now stop being so slow. Move!</p>
	</div><!-- .entry-content -->

	
<div>
	
	<p><img alt="" src="https://secure.gravatar.com/avatar/a0c6c34cf4a45ff5083d5ea541e7f27c5e4457ac393889e077af10688f6b3831?s=56&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/a0c6c34cf4a45ff5083d5ea541e7f27c5e4457ac393889e077af10688f6b3831?s=112&amp;d=mm&amp;r=g 2x" height="56" width="56" decoding="async">	</p><!-- .author-avatar -->

	<!-- .author-description -->
</div><!-- .author-info -->

	<!-- .entry-footer -->

</article><!-- #post-22361 -->

<!-- .comments-area -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
		</main><!-- .site-main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Touching the Elephant – TPUs (170 pts)]]></title>
            <link>https://considerthebulldog.com/tte-tpu/</link>
            <guid>46172797</guid>
            <pubDate>Sat, 06 Dec 2025 12:29:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://considerthebulldog.com/tte-tpu/">https://considerthebulldog.com/tte-tpu/</a>, See on <a href="https://news.ycombinator.com/item?id=46172797">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><h4>Understanding the Tensor Processing Unit</h4><section><span></span>Reed<br><span>&nbsp;</span><time datetime="2025-12-01">December 1, 2025</time></section><p><img alt="latency hiding" src="https://considerthebulldog.com/assets/tpu/latency_hiding.jpeg"></p><h2 id="something-new">Something New</h2><p>There is mythological reverence for Google’s Tensor Processing Unit. While the world presently watches NVIDIA’s gravity drag more companies into its orbit, there sits Google, imperial and singular. Lots of companies participate in the “Cambrian-style explosion of new-interesting accelerators”<a href="https://considerthebulldog.com/tte-tpu/#ref-14">[14]</a> – Groq, Amazon, and Tenstorrent come to mind – but the TPU is the original existence proof. NVIDIA should take credit for the reemergence of deep learning, but the GPU wasn’t designed with deep learning in mind. What’s strange is that the TPU isn’t a secret. This research is indebted to Google’s public chest-thumping, but the devices themselves have long been exclusive to Google’s datacenters. That is over a decade of work on a hardware system sequestered behind their walls. That the TPU is so well documented yet without a true counterpart creates a strange asymmetry. Google is well positioned in the AI race because of their decision over a decade ago to build a hardware accelerator. It is because of the TPU.</p><p>On the back of DistBelief Google had gotten neural networks running at scale. In 2013 however they realized that they would need to double their datacenter capacity to meet the growing demand for these new services. “Even if this was economically reasonable, it would still take significant time, as it would involve pouring concrete, striking arrangements for windmill farm contracts, ordering and installing lots of computers, etc.” <a href="https://considerthebulldog.com/tte-tpu/#ref-14">[14]</a> The race against the clock began, and 15 months later the TPU was born. Fast forward to April of this year when Sundar Pichai announced the 7th generation TPU, Ironwood, at Google Cloud Next. The headline figures were eye-popping. 9,216 chips in a pod, 42.5 Exaflops, 10 MW <a href="https://considerthebulldog.com/tte-tpu/#ref-21">[21]</a>. In 12 years the TPU went from a research project to a goliath rack-scale system.</p><p>Perhaps reverence is warranted. The development of the TPU is set against the backdrop of a changing hardware scaling landscape. It used to be that to get better programs you just had to wait. With each new generation of chip Moore’s Law and Dennard Scaling brought enormous tailwinds in transistor density, power efficiency, and wall clock improvements. But in the aughts and 2010s there was no more sitting and no more waiting. The advancements in chip physics were not producing exponential returns as they once had, and workload demands continued growing.</p><p>Casting this as mythology however obscures the details and risks making the TPU seem like magic. The development of the TPU is the story of trade-offs and constraints and co-design. It touches hardware, software, algorithms, systems, network topology, and everything in between. It did not happen by accident, but through the deliberate process of design and iteration. When thinking about the TPU it’s natural to ask:</p><p>How did we get here?</p><h2 id="slowing-down">Slowing Down</h2><p>For decades the industry relied on Moore’s Law to pack more transistors into a smaller area and on Dennard Scaling to get more energy efficiency from those transistors. This netted out to smaller, faster, and more efficient devices. You didn’t need to change your software or architecture to realize significant gains, regardless of the domain. CPU performance doubled every 1.5 years from 1985-2003, and every 2 years from 2003-2010. The doubling speed since is closer to every 20 years <a href="https://considerthebulldog.com/tte-tpu/#ref-14">[14]</a>. The AlexNet moment in 2012 charted a course to the current renaissance in neural networks. Different hardware suddenly opened the door for new questions to be asked. The range of problems that neural networks were suited to solve, along with their appetite for bigger data and bigger models, meant that this algorithmic paradigm was taking off as our scaling paradigms began to languish.</p><p><img alt="Scaling Post Moore’s Law" src="https://considerthebulldog.com/assets/tpu/Post_Moores_Law.png"> <em>Degradation in the reliability of chip performance scaling under different regimes <a href="https://considerthebulldog.com/tte-tpu/#ref-14">[14]</a></em></p><p>The TPU falls into the broad classification of hardware accelerators, of which the marquee distinction is that it is specialized for certain computational domains, hence the name Domain Specific Accelerator. Whereas general purpose devices are designed to accommodate the maximum number of program shapes, specialized designs are defined as much by what they can do as what they can’t. They trade off generality for performance. If we can’t rely on Moore’s Law and Dennard Scaling, and there are new workloads demanding attention, the goal is to optimize for the characteristics of those workloads and to discard everything else. Specialization asks what the optimal way to spend a fixed transistor and energy budget is to squeeze out performance.</p><p>Linear algebra is ripe for specialization because a relatively small set of parallelizable operations dominate neural networks. For the TPU that meant a monastic focus on those primitives. Neural networks are simple compositions of Matrix-Vector, Matrix-Matrix, and Elementwise computations over large tensors. Consider that matrix multiplication has cubic complexity. While computationally expensive, this one class of operations is the spine for a large fraction of what is required for a neural network. This narrows the window of optimizations that need to be baked into silicon. Matrix multiplies have the property that as the size of inputs grow, the ratio of compute, O(n^3), to data access, O(n^2), improves <a href="https://considerthebulldog.com/tte-tpu/#ref-15">[15]</a>. If you can dedicate hardware to speeding up arithmetic and coordinating data movement you can exploit this, and the arithmetic properties are complemented by the runtime properties. Neural networks can be fully specified ahead of time. With clever planning a program can be entirely mapped out before an instruction is issued. There was rarely a need before to design, tape out, and deploy custom silicon. Free performance gains made the economics of simply waiting versus the cost of designing an ASIC a non-starter. The decline of hardware scaling made exploring these realities attractive.</p><p><img alt="Energy Per Operation of Common Operations" src="https://considerthebulldog.com/assets/tpu/Energy_Costs_Breakdown.png"> <em>Horowitz Energy per Operation <a href="https://considerthebulldog.com/tte-tpu/#ref-11">[11]</a></em></p><p>This opportunity is best exploited in the power budget. Compare the relative cost of arithmetic to control, memory access, and data movement. Horowitz <a href="https://considerthebulldog.com/tte-tpu/#ref-11">[11]</a> notes that over 50% of processor die energy is dissipated in caches and register files. These inefficiencies exist to mitigate the even greater inefficiency of large memory accesses. In <a href="https://considerthebulldog.com/tte-tpu/#ref-12">[12]</a> they cite that the energy to fetch and interpret instructions is 10-4000x more expensive than to perform simple operations. Moving and accessing data costs significantly more power, and what is required of deep learning is more arithmetic per unit control. Finding ways to circumvent relative power inefficiencies with specialization means rearchitecting chips to remove that waste.</p><h2 id="the-inference-chip">The Inference Chip</h2><p><img alt="TPU Block Diagram" src="https://considerthebulldog.com/assets/tpu/TPU_Block_Diagram.png"> <em>Block diagram of TPUv1 <a href="https://considerthebulldog.com/tte-tpu/#ref-1">[1]</a></em></p><p>Datacenter expansions plans are a hell of a drug. To stem the tide of models devouring datacenter capacity, the first ASIC needed to focus on inference. Inference only needs a forward pass through the neural network. A simple neural network layer might look like this:</p><p>$$ ReLU( (X \cdot W) + b ) $$</p><p>Where X and W are input data and model weights, ReLU is a non-linear activation function, and b is a bias term. A matrix multiply followed by some elementwise addition and an elementwise maximum function. Imagine that chaining a handful of these layers together forms the totality of an inference. This simplified view on early model architectures gives us the general template for designing TPUv1. Matrix multiply, some activation looking functions on that result, feed the results to storage, repeat. To meet the initial deadlines the TPU design exploited this loop-like behavior.</p><p>TPUv1 is a single-threaded co-processor connected over PCIe with a 24MiB software-controlled Unified Buffer, an 8-bit integer systolic array, and 8GiB DDR3 DRAM. The device runtime lays out tensors, plans memory transfers with a programmable DMA controller between the host and the Unified Buffer (on-chip SRAM), and tiles compute operands. The host sends 12-bit CISC instructions to the device’s instruction buffer which the in-order sequencer consumes to move data to DRAM and issue MXU ops. The datapath consumes ~2/3 of the die area of the chip <a href="https://considerthebulldog.com/tte-tpu/#ref-1">[1]</a>. Take care to notice what it is not. It is not a multi-level cache hierarchy. There is no multi-threading or branch prediction or prefetching or TLB. The systolic array executes arithmetic and the runtime eliminates control overhead. TPUv1 is a spartan device aimed at making inference fast.</p><p>The heart of the device is the Matrix Multiplication Unit (MXU). It is a 256x256, 2D weight-stationary systolic array of processing elements, in this case MACs. The MXU targets dense GEMMs to maximize arithmetic intensity. The TPU is designed to keep the MXU busy. You can find nice animated demonstrations of data moving through the systolic array <a href="https://fleetwood.dev/posts/domain-specific-architectures">here</a> or <a href="https://jax-ml.github.io/scaling-book/tpus/">here</a>.</p><p><img alt="MXU Cycle Timing" src="https://considerthebulldog.com/assets/tpu/MXU_Cycle_Timing.svg"> <em>MXU Cycle Timing</em></p><p>We’ll start with a simplified 4x4 systolic array. Although there are design variations of systolic execution <a href="https://considerthebulldog.com/tte-tpu/#ref-18">[18]</a><a href="https://considerthebulldog.com/tte-tpu/#ref-36">[36]</a>, we are concerned with the 2D weight-stationary variant. The weights are pre-loaded into the array from the right hand side (the top in this diagram), and the inputs stream in from the left hand side (conveniently on the left). Once the weights are loaded they sit resident in the MACs, one weight per MAC. As the inputs flow from left to right, the MACs compute the product of the resident weight and the streamed input each cycle. The result of that computation is passed downward to the next processing element. If a MAC has one of these partial sums, it adds it to the result of the weight/input product and passes that new sum downward. At the bottom edge of the array there are no more computations and the result is passed to a 4096 row x 256-element bank of 32-bit accumulators.</p><p><img alt="MXU Double Buffering" src="https://considerthebulldog.com/assets/tpu/MXU_Double_Buffering.svg"> <em>MXU Double Buffering</em></p><p>Notice that weight pre-loading doesn’t happen all at once. It would waste cycles to wait for each MAC to have a resident weight before streaming in inputs. Weight pre-loading instead happens diagonally, with the left-most part of the systolic array receiving weights first. When the left column of processing elements has weights, the inputs begin streaming diagonally top to bottom. This imposes significant timing coordination for such a simple component. Much of the rest of the chips’ design can be thought of as accommodating these timing needs, and a particular instantiation of that is the liberal use of double buffering.</p><p>MXUs can perform immense amounts of arithmetic, but data movement/control stops at the edges of the systolic array. Between processing elements there is only result-passing with chains of two-input adders. If either weight or input data is not where it needs to be, stalls burn cycles that hurt MXU utilization. Spelling it out:</p><ul><li>The MXU holds two 64KiB tiles of weights with one reserved for double buffering</li><li>Four 64KiB weight tiles act as a FIFO queue to decouple memory accesses and weight loads between DRAM and the MXU</li><li>The Unified Buffer stores intermediate results from the accumulators and prepares new data to feed to the systolic array</li><li>The bank of accumulators logically splits 4096 rows into two chunks of 2048 rows, one to feed outputs and one to drain them</li></ul><details><summary>Sizing the MXU</summary> The number of processing elements that touch data before it reaches the accumulators grows quadratically with the array size which affects the speed of the computation. For a 256x256 array that is 65,536 MACs vs. 262,144 MACs in the 512x512 configuration. During fill/drain you pay an O(num_edges) cost to populate the buffers. Fewer edges better amortize this overhead. As arrays shrink they are penalized by wiring constraints. They perform less compute per data access and require running many wires between components. Sizing this device is a delicate balance between compute intensity and layout constraints, which we will see again in later generations.</details><p>The runtime knows how long each operation it issues should take, so it can intelligently overlap them with one another. During matrix multiplications the UB prepares the next batch of inputs, the fixed activation units operate on the results in the accumulators, and the Weight FIFO banks more weights. Matrix multiplies are relatively long latency, which leaves lots of cycles between when work starts and when work ends. The runtime schedules memory accesses, data movement and computation deterministically to minimize stop-the-world pauses rather than make coordination dependent on the MXU. Hiding latency with overlapping improves parallelism, improves data reuse, and conserves energy otherwise wasted in control flow.</p><p>The headline figures from their paper are anachronistic by now, but they help contextualize the accomplishment of the first gen chip. 25x as many MACs and 3.5x the on-chip memory of the K80 GPU. 15-30x the inference speed and 30-80x the perf/W of the K80 and the Haswell CPU <a href="https://considerthebulldog.com/tte-tpu/#ref-1">[1]</a>. The fixed-latency, software-managed design created a hardware accelerator that eschewed prevailing designs that spent energy in cache hierarchies and control overhead. Maniacal focus on mitigating inference bottlenecks with large SRAM and coordinated data movement proved that TPUv1 worked.</p><h2 id="the-training-chip">The Training Chip</h2><p>Neural networks need to be trained before they can be used for inference, and TPUv1 was not designed for training. Requirements include backpropagation to modify weights during execution, gradients with higher precision than int8, and support for diverse activation functions. This costs orders of magnitude more FLOPs <a href="https://considerthebulldog.com/tte-tpu/#ref-2">[2]</a>, and those FLOPs must be distributed over multiple devices while maintaining deterministic execution. TPUv1’s fixed activation units were not flexible enough for experimenting with new algorithms. The memory subsystem was not flexible enough to coordinate work between multiple devices. The UB was not flexible enough to tuck more Matrix-Vector work in behind the MXU. The whole device was too tightly coupled. Adding that flexibility, without reverting to a general-purpose processor, needed a radically different datapath.</p><p><img alt="TPUv2 Block Diagram" src="https://considerthebulldog.com/assets/tpu/TPUv2_ICI.png"> <em>TPUv2 Block Diagram <a href="https://considerthebulldog.com/tte-tpu/#ref-2">[2]</a></em></p><p>TPUv2 was animated from the bones of TPUv1, but only the MXU feels familiar. TPUv2 is a dual-core chip. Each core pairs a scalar controller with programmable vector units, local SRAM, a 128x128 MXU, and HBM. It adds inter-core interconnects (ICI) to communicate between the memory systems of each core and across chips. Two 128x128 MXUs combine to total the same 256x256 array from TPUv1 but simplify the circuit design. Unequal logic, wire, and SRAM scaling on smaller process nodes made arithmetic improvements comparatively free, enabling the chip designers to focus on the laggard scaling axes <a href="https://considerthebulldog.com/tte-tpu/#ref-2">[2]</a>. For the second generation MXUs that meant two efficiencies over their predecessor: BrainFloat16 and wire routing.</p><p><img alt="BF16 floating point format" src="https://considerthebulldog.com/assets/tpu/bf16.png"> <em>BF16 floating point format <a href="https://considerthebulldog.com/tte-tpu/#ref-3">[3]</a></em></p><p>Dynamic range matters more than precision for neural network training. Gradients represented as integers don’t produce adequate convergence behavior; you need floating point numbers to make fine-grained weight updates. Accessing higher precision numerics however means sacrificing die area. Logic circuits need more adders to handle mantissa bits. Floating point adder arrays scale as (M+1) * (M+1), where M is the size of the mantissa, – 576 adders for fp32 and 121 adders for fp16 <a href="https://considerthebulldog.com/tte-tpu/#ref-14">[14]</a> – totalling more die area and more energy spent on arithmetic. Notice that although bf16 is the same number of bits as fp16, the proportion of exponent bits to mantissa bits is higher. bf16 only requires 64 adders in the MAC circuitry, and less circuitry means more MACs in the same package and power budget <a href="https://considerthebulldog.com/tte-tpu/#ref-2">[2]</a><a href="https://considerthebulldog.com/tte-tpu/#ref-14">[14]</a>.</p><p><img alt="MXU Sizing Considerations" src="https://considerthebulldog.com/assets/tpu/Why_128.png"> <em>MXU Sizing Considerations <a href="https://considerthebulldog.com/tte-tpu/#ref-32">[32]</a></em></p><p>Chip geometry considerations extend beyond individual processing elements. Big cores need long, global wires routed to/from functional units, FIFOs, and control units. Though wire diameters shrink on improved process nodes, their resistance and capacitance scale unevenly. Long wires are chunked into shorter segments connected with repeaters, but this induces signal delay making circuit timings more complex <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. MXU configurations with multiple smaller cores shorten average wire lengths but need wires routed all over the chip. The trade off is between compute bandwidth and array utilization. Compute utilization scales down quadratically with the array area, but smaller arrays use more energy-efficient wires. Splitting the die into two cores and running fewer, shorter wires to the vector and control units balances wiring scaling with utilization.</p><p><img alt="TPU Scalar Unit" src="https://considerthebulldog.com/assets/tpu/Scalar_Unit.png"> <em>TPU Scalar Unit <a href="https://considerthebulldog.com/tte-tpu/#ref-32">[32]</a></em></p><p>All those wires have to lead to somewhere. To drive the new datapath, TPUv2 introduces the scalar unit. When a user submits a program, the XLA compiler performs static analysis, lowering the program into 322-bit VLIW instruction bundles. XLA schedules DMAs, vector ops, and MXU work in a deterministic stream. The complexity of organizing program control flow is absorbed by software, keeping the scalar unit relatively simple. It is single-threaded and contains 4KB of scratchpad SRAM (SMEM), small instruction memory (IMEM), and a 32 element, 32-bit register file (SReg) connected to a dual-issue ALU. Sync registers flag when arithmetic and memory blocks are busy to explicitly synchronize execution. The host sends instructions over PCIe to HBM, where they are DMA’d into the Scalar Unit’s IMEM as overlays. Scalar instruction slots execute locally, and the vector/matrix slots are decoded and dispatched to the VPU/MXU <a href="https://considerthebulldog.com/tte-tpu/#ref-3">[3]</a>. There is no dynamic runtime scheduling, just instruction fetch, decode, and forward.</p><p>Two programmable vector processing units (VPU) consolidate the fixed function blocks from TPUv1. The VPU is a 2D SIMD processor designed to increase the ratio of vector operations to matrix operations. Each VPU has 128 vector lanes with 8 sublanes. Each sublane is connected to 32 dual-issue ALUs with lane-local register files (Vregs). The VPU is backed by 16MiB on-chip Vector Memory (VMEM) that mediates data movement to the MXU with pushes/pops onto a Result FIFO <a href="https://considerthebulldog.com/tte-tpu/#ref-3">[3]</a>. Each core’s VMEM has local access to half of the chip’s HBM, and DMAs to VMEM are strided to fetch contiguous tiles of data rather than issuing many small DMAs. The VPU accesses VMEM with explicit loads/stores to Vregs which remove the need for a cache hierarchy.</p><p>The simplicity of describing the rearchitected datapath belies the complexity that the subsystems represent. Whereas general purpose devices use branch predictors, TLBs, Out of Order execution, and a bevy of techniques to shuttle data and instructions, the TPU routes around a cache-centric design with software-managed execution. The aforementioned general purpose mechanisms alleviate runtime dependencies at the expense of more hardware and more energy. Control and caches consume massive amounts of the limited energy budget, so redesigning this subsystem is the difference between an economic chip and a renegotiated contract with power providers. When you know what operations you need, the order you need them in, and the operational characteristics of the hardware, you can move control flow to compile time. The VPU and Scalar Units are co-designed to leverage this operating paradigm, moving program orchestration to software.</p><p><img alt="VLIW Instruction Bundles" src="https://considerthebulldog.com/assets/tpu/VLIW_Bundle.svg"> <em>Sample VLIW Instructions</em></p><p>VLIW instructions expose this complexity. They contain slots for 2 scalar, 4 vector, 2 matrix, 1 miscellaneous, and 6 immediate instructions <a href="https://considerthebulldog.com/tte-tpu/#ref-3">[3]</a>. Slots map to scalar/vector/matrix arithmetic, loads and stores, DMAs, synchronization flags, and data literals. Though innocuously named, the miscellaneous slot controls heaven and earth. It is reserved for kernel launches, DMAs, and synchronization guards which we can think of as WAITs. Data dependencies must be carefully sequenced to ensure operation A finishes before operation B uses its results. XLA utilizes the misc slot to keep subsystems working while guarding against illegal instruction sequences. Operational latencies are known constants at compile time, and XLA can use those values to place WAIT instructions at exactly the right point in the VLIW stream to minimize stalls.</p><p><img alt="Simplified TPU Program" src="https://considerthebulldog.com/assets/tpu/TPU_Execution.svg"> <em>Simplified TPU Instruction Overlay</em></p><p>Subsystems operate with different latencies: scalar arithmetic might take single digit cycles, vector arithmetic 10s, and matrix multiplies 100s. DMAs, VMEM loads/stores, FIFO buffer fill/drain, etc. all must be coordinated with precise timing. The MXU might be busy executing a matrix multiply for 128 cycles, meanwhile the VPU is preparing the next tile of weights for the Result FIFO. While DMAs prepare new data for VMEM a DMA_OVERLAY instruction gets inserted to fetch new instructions for IMEM. When the MXU finishes a tile, the hardware sends a signal to clear the MXU_BUSY bit in the scalar unit’s sync registers. When the scalar unit evaluates a WAIT_MXU instruction it sees that the bit is unset and hops to the next instruction for decoding. The scalar unit JUMPs to the new VLIW bundle region and the program continues. Seamlessly overlapping the work of an arbitrary DAG requires extraordinary co-design between the device and the software.</p><p>Decoupling the hardware gave software the capacity to drive massive data and instruction level parallelism. VLIW slots can launch 8 operations per cycle. That is 2048 vector ALUs and two 128x128 systolic arrays with minimal control overhead. HBM, VMEM, Vregs, and the MXU all remain busy with the same pipelining and overlap philosophy from TPUv1, only now massively scaled up. XLA wrests power away from control and back into the arithmetic units with coordinated, deterministic execution. Determinism across devices requires explicit communication between chips.</p><p>ICI forms the backbone of the training pods. It creates a coherent communication fabric that lets chips operate locally while composing into a mesh of devices acting as one large core. Two on-chip ICI links route data between the HBM and VMEM of each core. Four 496Gbit/s bidirectional off-chip links connect a TPU to its neighbors in the rack with OSFP passive copper. RDMAs over this fabric let chips treat remote HBM as explicitly addressable endpoints. Racks arrange 256 chips as a 16x16 2D torus over ICI to form the full supercomputer pod. ICI removes frequent host communication, skipping the cost of network cards, switches, and communication delays. All this sacrifices 13% of the die area for gains in distributing computations <a href="https://considerthebulldog.com/tte-tpu/#ref-3">[3]</a>.</p><p><img alt="1D Torus" src="https://considerthebulldog.com/assets/tpu/1D_Torus.svg"> <em>One dimensional torus wraparound</em></p><p>Let’s imagine that we’re playing a game of telephone. You and 8 friends are arranged in a 3x3 grid, and you can only communicate with your adjacent neighbors. Your goal is to send a message from the person at (0,0) to the person at (2,2) in the fewest messages. Many paths achieve this, but the shortest one is always four. Now imagine that the people on the left edge of the grid can wrap messages around to people on the right edge of the grid. This is logically like mirroring you and all your friends over that wraparound axis. These 3 new connections make our shortest path 3 instead of 4.</p><p><img alt="2D Torus" src="https://considerthebulldog.com/assets/tpu/2D_Torus.svg"> <em>Logical mirroring in two dimensional torus wraparound, adapted from <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a></em></p><p>ICI plays this game of telephone in two dimensions. During backpropagation and optimizer state updates intermediate values accumulate across different partitions of the model located on different chips. Results must be broadcast to all the chips participating in the computation for synchronization. Whereas on-chip work is explicitly synchronized with hardware flags, work across chips is implicitly synchronized with MPI-style collectives (All-to-All, AllReduce, etc.). Torus topologies improve communication bandwidth and increase access to different communication patterns during synchronization.</p><p>32 wraparound links at 496Gbit/s enable 15.9Tbit/s of bisection bandwidth <a href="https://considerthebulldog.com/tte-tpu/#ref-3">[3]</a>, which tells us how much data can move through the network. In a 4x4 array, a cut down the middle would sever 4 connections. That same cut down the middle of a 2D torus severs 8 connections. Even if each connection carries the same amount of data, there are more paths for data to move through which helps reduce congestion. XLA absorbs the complexity of cross-device scheduling. Software can trust that RDMAs will reach their intended stacks of HBM traveling along the ICI interface.</p><p>The same DNA ostensibly runs through TPUv1, yet the chips look and feel utterly different. The microarchitecture, software, and networking each became independently sophisticated parts of a larger system. Subsystems decoupled from one another yet still composed neatly. Where TPUv1 tightly choreographed everything, TPUv2 divided components into independent, asynchronously operating units communicating through explicit queues and synchronization points. TPUv3 was a minor revision in comparison. It has two MXUs per core, an increased clock, double the HBM capacity with 30% higher bus speeds, higher ICI bandwidth, and scales up to a 1024 node liquid-cooled rack. The dies only increased 6% relative to TPUv2 because engineers learned how to better lay the chip out <a href="https://considerthebulldog.com/tte-tpu/#ref-3">[3]</a>. Scaling the system to meet the continued growth of neural networks pushed future designs into new territory.</p><h2 id="scaling-up">Scaling Up</h2><p>As the footprint of the system grew, so too did the complexity of operating it. Our focus up to now has emphasized chip-local comparisons, e.g. How expensive are these operations relative to one another? How does the memory hierarchy work? How do subsystems A and B communicate on-device? While the TPUs remain the atom of the supercomputer, as we zoom out we observe the crystalline structure of the racks and pods. The fourth generation TPU is better examined thinking about memory as one unified domain. Specialization forces care in the microarchitecture, but the questions change. Where are collectives slow? How are larger tensors handled? Can we scale the racks further? Viewing the world from low altitude we find that TPUv4’s design emphasizes system scaling and energy management.</p><p>Peeking behind the accounting curtain for a moment, they note that “most OpEx cost is for provisioning power and not for electricity use, so saving power already provisioned doesn’t improve TCO as much as one might hope” <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. Total Cost of Ownership (TCO) tries to consider the all in cost of the pods. On the back of a napkin we break this out into CapEx (equipment, installation, etc.) and OpEx (personnel, maintenance, power, etc.). Initially CapEx might dominate ASIC design, but as the platform matures, thinking through operational requirements produces different sets of optimizations. The need for fast, power efficient devices remains but extends out into the unknowable future. As model demands increase, better economics need compositional scalability in an efficient power envelope.</p><p>A brief note: TPUv4 is the training design and TPUv4i is the inference design. The impetus was to keep training and inference chips nearly identical so that there weren’t two separate designs awkwardly diverging into separate projects <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. The relevant change is that the inference chip has one core while the training chip is dual-core.</p><p><img alt="4th Gen MXU" src="https://considerthebulldog.com/assets/tpu/4th_Gen_MXU.svg"> <em>Simplified Model of TPUv4 Systolic Execution</em></p><p>Fourth generation chips keep TPUv3’s MXU footprint, totaling 4 MXUs per core. In previous MXU designs partial sums moved downwards each cycle through a series of N two-input adders, where N is the size of the array, before reaching the output accumulators. TPUv4 batches groups of four products before passing them to custom 4-input adders. Batching products reduces the length of the adder chain from N to N/4, quartering the operational latency. Above we see 12 PEs bank four multiplies to reduce hops from 12 to 3. The specific implementation of these circuits isn’t clear from the paper, but this should provide enough motivation to understand the change. This circuit design decreases die area 40% and reduces peak power 12% <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>.</p><p><img alt="CMEM Speed Ups" src="https://considerthebulldog.com/assets/tpu/CMEM_SpeedUp.png"> <em>CMEM Speed Ups <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a></em></p><p>Accessing DRAM is still expensive, and inference workloads underutilize chips. TPUv4 adds 128MiB shared CMEM that is like an L3 cache but with the niceties of software-managed programmability. CMEM helps to keep all 4 MXUs busy with computations at the cost of 28% of the TPUv4 die area. On the 7nm process node, SRAM memory accesses are 20x more energy-efficient than DRAM accesses <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. CMEM’s memory bandwidth sits in between HBM and VMEM, but unlike HBM it can both read and write data. Expanding the memory hierarchy and keeping data closer to the arithmetic units allows XLA to cut out expensive trips to DRAM. During inference, prefetching model weights into SRAM for multi-tenancy drives higher utilization of chip resources that may otherwise be sitting idle. The ability to swap weights out from SRAM rather than DRAM makes paying the context switching cost feasible. All that die area and upfront CapEx gets amortized over the life of the chip in TCO so long as XLA can effectively leverage it.</p><details><summary>SparseCores</summary> <p><img alt="SparseCore Block Diagram" src="https://considerthebulldog.com/assets/tpu/SparseCore.png"> <em>SparseCore Block Diagram <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a></em></p> <p>Contrary to the prevailing LLMs-everywhere paradigm, ad serving and recommendation models (DLRMs) run the world. SparseCores (SC) are built to accelerate these models at the cost of 5% die area and power <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>. The key features of these models are their usage of embeddings. Embeddings map data into enormous sparse matrices. Efficiently handling these sparse matrices requires clever strategies to shard tensors across devices and to make looking up the correct slice of data fast. Unstructured sparsity suffers massive memory traffic and imbalances between compute, communication, and data-dependent execution. The MXU is ill-suited to make progress on sparse workloads because they waste cycles on empty computations and don’t directly manage communication.</p> <p>SparseCores address this class of models with a “Sea of Cores” architecture designed to accelerate collectives and memory accesses <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>. SCs are segmented into 16 individual compute elements (tiles) near DRAM that support multiple outstanding memory accesses <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>. Tiles communicate over a data crossbar with one another and over the on-chip fabric to the rest of the device. A stream of CISC instructions enabling data-dependent communication gets issued by the processor’s core sequencer. The Fetch unit (8-wide SIMD vector processor, scVPU) reads data from HBM into 2.5MiB of sparse memory (Spmem), and the Flush Unit writes data out to HBM. Five on-board cross channel units (XPU) perform embedding specific operations. When embeddings are distributed across remote devices SCs leverage the existing ICI bandwidth to access remote memory. The dataflow looks as follows:</p> <ul><li>HBM DMA issued and read by Fetch Unit to Spmem</li><li>scVPU and XPUs operate on data</li><li>Flush unit writes data out to HBM (or remote HBM via RDMAs)</li></ul> <p>SCs alleviate the need for the MXU to handle computation and memory traffic on sparse data. They remove the CPU/DRAM bottleneck and shift sparse phases off the MXU path. The cores issue many RDMAs across the global address space of the TPUv4 pods, speeding up embeddings based models 30.1x versus CPUs <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>. Dedicating a small amount of die area to the gather/scatter intensive DLRMs allows the device to be flexible and efficient under multiple algorithmic regimes.</p></details><p>Cores are getting crowded: MXUs, SparseCores, VPUs, HBM, and ICI routers. We see this component management pressure in the VLIW bundles. Driving the additional MXUs and CMEM required the VLIW bundle size to expand ~25% <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. Adding new subsystems to the microarchitecture adds efficiencies that bubble up to system level performance, but lurking behind each of these changes is the specter of wiring. Fitting more efficient work onto the package with point-to-point connections became too great a tax. Training racks need to be close to one another in the datacenter to amortize the cost of cooling infrastructure, and this physical constraint forces the usage of optical fiber. ICI cabling in TPUv2/v3 coupled rack deployments so that a supercomputer couldn’t go into operation until the full pod was deployed <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. To realize the TCO and energy wins of the microarchitecture system scaling needed to decouple and compose.</p><p><img alt="TPUv4i Floorplan" src="https://considerthebulldog.com/assets/tpu/TPUv4i_Floor_Plan.png"> <em>TPUv4i Floorplan <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a></em></p><p>The ICI needed to breathe. Previous revisions of ICI handled both on-chip communication and off-chip communication. More wires needed to be routed to/from the ICI interface as the number of components grew. This circuit layout pressure was complemented by the equally frustrating reality that handling on-chip and off-chip communication increased contention for ICI bandwidth. TPUv4 separates these concerns by adding a dedicated on-chip interconnect (OCI) fabric. The OCI interface handles data movement on-chip so that ICI can solely route traffic across chips. Notice in the fourth generation floorplan how much die area is reserved for OCI <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. Shorter wires run between components and OCI rather than point-to-point. The OCI interface acts as the mailman. The Scalar Unit drops a message off at the OCI to submit a DMA to DRAM, and the OCI routes it to the memory controller. It tucks subsystem communication behind a unified data exchange interface that shortens wire routes and opens a path to flexible scaling in future designs.</p><p>Arbitrating memory accesses between HBM, VMEM, IMEM, SMEM and now CMEM meant maintaining too many sets of independent lanes. OCI uses 512B-wide native data paths segmented into four, 128B-wide groups across the memory hierarchy. Each group serves a quarter of the total HBM bandwidth (153GB/s) so that independent transfers don’t serialize behind one another <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. Transferring small IMEM overlays shouldn’t have to wait on the completion of a long-latency tensor DMA. This partitioning strategy gives software more flexibility when scheduling work across a device. The full HBM bandwidth is available to each group, but software can schedule multiple concurrent transfers instead of funneling everything through one contested path. XLA plans large transfers to CMEM, CMEM feeds the arithmetic units, OCI handles message passing, and ICI routes and manages RDMAs. OCI and CMEM jointly help to improve spatial locality and reduce trips to HBM.</p><details><summary>4D Tensor (R)DMAs</summary> <p>TPUv2/v3 used two-dimensional, relaxed order DMAs to stride along two axes when moving data. This forced XLA to decompose complex tensor reshapes into multiple DMA operations. TPUv4(i) uses four-dimensional DMAs that stride along three axes moving 512-byte chunks <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. Operations that previously required multiple round-trips to memory now happen in a single DMA. The architecture distributes DMA engines throughout the chip rather than centralizing them. Each engine acts as a co-processor that can decode and execute tensor operations independently. The unified design works identically for on-chip transfers, cross-chip transfers, and host transfers. XLA inserts explicit synchronization, but in exchange it gets predictable performance and the freedom to schedule data movement aggressively. The compiler knows the latency and pipelines around it.</p></details><p>TPUv3 had already resorted to optical fiber across racks to enable the full 2D torus, but the 1024 node supercomputer could not expand its physical footprint. Rigid ICI wiring constraints meant individual racks couldn’t be used until each pod was deployed, and the system topology was fixed as configured unless a technician recabled the pod. Rack maintenance brought the whole pod offline with it. Optical Circuit Switching (OCS) infrastructure was the cure. Even though optical solutions are expensive, OCS optical components represent less than five percent of both system and power costs <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a><a href="https://considerthebulldog.com/tte-tpu/#ref-10">[10]</a>. Centralizing cross-rack communications inserted massive programmability into the system. Substituting the cross-rack links with a programmable OCS provided massive gains in “scale, availability, utilization, modularity, deployment, security, power, and performance” <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>, unlocking a new scaling paradigm.</p><p><img alt="OCS Logical Diagram" src="https://considerthebulldog.com/assets/tpu/TPUv4_OCS.png"> <em>OCS Logical Diagram</em></p><p>Each rack in TPUv4 is a 4x4x4 cube, where this cube configuration is chosen to optimize all-to-all communications. Previous pod sizes (16x16 in v2, up to 128x32 in v3) were topology-limited. Devices could communicate between racks over ICI, but the system topology was statically programmed by the cabling. OCS removed these hard limits by centralizing cross-rack communication over an optical switching fiber. OCS offloads link establishment to an array of MEMS mirrors that dynamically configure links between devices in milliseconds <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>. New system topologies can be programmed on the fly by software, placing workloads on idle, non-contiguous machines. Dynamically reconfiguring the OCS improves system availability, tolerating outages in 0.1% - 1.0% of the CPU hosts <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a>. TPUv4 pods scale up to 8x8 racks totaling a 4096 node cluster connected over OCS.</p><p>The OCSes isolate scaling complexity. Each rack contains 64 chips laid out logically as a cube. With 6 cube faces (+/- X/Y/Z), and 16 (4x4) chips per face, 96 optical links go to the OCS per rack. In the full 64 (8x8) rack pod, that is 6,144 uplinks to the OCS. This requires 48 OCSes that have 128 active ports to connect all the uplinks <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>. Moving cross-rack interconnects to a dedicated optical panel at this scale enabled programmable topologies, eased deployment by decoupling racks, and allowed software to effectively use OCS as a “plugboard” to route around node and link failures.</p><details><summary>Mirror, Mirror on the Wall</summary> <p><img alt="MEMS Mirrors" src="https://considerthebulldog.com/assets/tpu/MEMS_Mirrors.png"> <em>MEMS Mirrors <a href="https://considerthebulldog.com/tte-tpu/#ref-10">[10]</a></em></p> <p>OCSes use micro-electro-mechanical systems (MEMS) mirrors that tilt in three dimensions to steer optical beams. Each OCS contains two arrays of 136 mirrors. Each mirror has four voltage-controlled actuators that rotate it along two axes, steering light from any input port to any output port with sub-degree accuracy. Rather than monitoring each of the 136 mirrors with a dedicated photodetector, OCS uses a single camera per array with an 850nm monitoring laser. Image processing algorithms optimize the high-voltage driver signals to minimize insertion loss across the entire array. Once positioned, each mirror draws 10s of milliwatts to maintain alignment <a href="https://considerthebulldog.com/tte-tpu/#ref-10">[10]</a>.</p> <p><img alt="Circulators" src="https://considerthebulldog.com/assets/tpu/OCS_Circulators.png"> <em>Circulators <a href="https://considerthebulldog.com/tte-tpu/#ref-10">[10]</a></em></p> <p>Circulators double the OCS’s effective capacity by enabling bidirectional communication. A circulator is a three-port optical device. Light entering port 1 exits port 2, light entering port 2 exits port 3. This cyclic property means a single fiber and a single OCS port can carry traffic in both directions simultaneously halving the required fiber count and OCS ports <a href="https://considerthebulldog.com/tte-tpu/#ref-10">[10]</a>.</p></details><p>Full connectivity of the OCS across the pods meant that the torus topologies of the previous generations could now add a third wraparound dimension. The distance between racks was no longer a constraint, and since the OCS can program chip-to-chip connections on the fly a path to new topologies emerged. Not only could the connections between racks wrap around the z-dimension, they could twist.</p><p><img alt="Example Twisted Tori" src="https://considerthebulldog.com/assets/tpu/Twisted_Torus.svg"> <em>Sample 1D Twisted Tori</em></p><p>We’ll make one modification to our previous wraparound topology diagram. Instead of wraparounds connecting only to the other side of their respective row/column, OCS programmability means that these connections can be offset. Adding twists to the wraparounds is an option not a requirement. Having the option to twist the network topology allows for new questions, e.g. given the communication pattern of this model, how should data be sent between participating chips? Twists make algorithmic experimentation and optimization two independently tractable targets and broadens the horizon of available efficiencies. Even without twisted topologies a third wraparound dimension adds bisection bandwidth to the network. The bisection bandwidth of 2D tori scales with the side length of the interconnects, N^(1/2). Adding the additional wraparound dimension scales bisection bandwidth with the area of the interconnects, N^(2/3). More paths in the topology shorten hops between participating nodes and alleviate system congestion along busy routes during synchronization. OCS better utilizes available devices and diversifies achievable topologies.</p><p>TPUv4(i) requires our thinking to broaden. We shouldn’t forget the impacts that microarchitecture improvements drive, but we need to consider the economics of the system holistically. Building warehouse scale solutions requires thinking about power provisioning, rack availability, interconnects, network topology, and accounting. Energy efficiency is still the overarching principle, but at datacenter scale. The message is simple: Target TCO over CapEx <a href="https://considerthebulldog.com/tte-tpu/#ref-5">[5]</a>. Adding CMEM is more expensive now but less expensive over time. Optical interconnects are expensive now but cost &lt;3% of the fully operational system <a href="https://considerthebulldog.com/tte-tpu/#ref-4">[4]</a>. The duration of the design trade-offs became smeared into the future. All the same apparitions motivating TPUv1 go bump in the night, but they cast shorter shadows. TCO implies a system that requires operation, and the software that keeps the system available is an equal part of TPU’s development.</p><h2 id="island-hopping">Island Hopping</h2><p>Up to now we have enjoyed the quiet refuge of spreadsheet analysis, but the world is imperfect. Hardware dies, electricity spikes, and networks suffer congestion. The triumph of composing the system into decoupled, single responsibility units is not trivial, but infrastructure needs to serve real users. A cast of supporting software must keep chips available. Rock solid hardware relies on software to rationalize TCO obsession. The software is as much a part of the TPU story as the hardware.</p><p>We want to train a model. We decide which devices we need, pay rent, and start gawking at loss curves. When we submit our job for execution we don’t worry about the thousands of eager folks just like us. This mass of users vying for a fixed number of TPUs in sporadic intervals presents a problem. As the infrastructure provider what matters is that users don’t experience downtime. Components regularly fail and workloads are hard to predict. Once power has been provisioned every second of idle chip time or suboptimal workload allocation works against your best TCO approximations. Whether by underutilization or oversubscription, wasted resources are the enemy. Outer loop software that manages TPUs coordinates with XLA to find available nodes, check resource health, and configure ICI/OCS <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a>. XLA needs to know which TPUs the computation will run on as well as the requested network topology because device placement is part of the program. Optimizing the system for high availability means dealing with the constraints imposed by ahead of time scheduling.</p><p><img alt="TPU Resource Fragmentation" src="https://considerthebulldog.com/assets/tpu/TPU_Fragmentation.png"> <em>TPU Fragmentation <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a></em></p><p>Slices, Single Program Multiple Data (SPMD), and gang scheduling undergird TPU execution. Most workloads don’t consume an entire pod. Slices are declarations in code that allow developers to request an &lt;X,Y,Z&gt; device mesh which XLA uses to partition and shard models. This abstraction squirrels away both topology size and communication patterns. Pipeline parallelism may want a 2x2x1024 slice while data parallelism wants a 16x16x16 slice. The topology choice optimizes which communications are fast and which are slow. Mapping communications to a slice topology gives developers the freedom to experiment with parallelism strategies.</p><p>ICI coupling in TPUv3 meant the scheduler needed to find contiguous, healthy chips for workload placement. OCS lifted that restriction in TPUv4, but in both generations once a set of devices is selected the topology remains static for the duration of the program. A program owns the devices that it runs on until the program exits <a href="https://considerthebulldog.com/tte-tpu/#ref-8">[8]</a>. Concurrent users submitting unknowable slice sizes makes assigning devices like Tetris. The scheduler must place new jobs onto devices as old jobs pop in and out of existence. It needs mechanisms to rebalance suboptimal device allocations.</p><p>A single executable distributed to each participating device runs an identical program. SPMD encapsulates this many devices, single program framework. Developers write models as if they are running on one giant device, and the complexity of managing device-level data placement disappears from view. XLA’s partitioner rewrites every operation in the model to work on local tensor shards, inserting an AllReduce where gradients need to sync, scattering data where it needs to spread, and gathering results where they need to combine <a href="https://considerthebulldog.com/tte-tpu/#ref-7">[7]</a>. The single logical program becomes thousands of coordinated physical programs each operating on its local slice of data. Control is synchronized explicitly on-device with VLIW barriers and implicitly between devices by collectives. Gang scheduled execution means that each device launches the program all at once, trading off runtime resilience for performance. When a fault crops up during execution the job must be checkpointed and relocated <a href="https://considerthebulldog.com/tte-tpu/#ref-8">[8]</a>. The hardware stays simple, the software stays deterministic, but the orchestration layer must handle outages, link failures, and maintenance.</p><p><img alt="TPU Job Lifecycle" src="https://considerthebulldog.com/assets/tpu/TPU_Job_Lifecycle.png"> <em>TPU Job Lifecycle <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a></em></p><p>Software must anticipate failures to juggle pre-allocated workloads. In <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a> they note “To train a model, all TPU processes must be simultaneously up to synchronously update their weights via ICI collectives. A single failed, or interrupted process will interrupt the whole training process.” When a user submits a job, the cluster management client Borg queues it. If resources are fragmented or a job fails, Borg can preempt running workloads to shuffle them to different devices. When a job is ready to be scheduled, Borg selects a subset of devices and publishes an xconnect to the Pod Manager. The PM discovers pending xconnects and sends commands to the appropriate OCSes to connect the requested ICI channels. Once ICI connections stabilize, libtpunet configures the device’s ICI and programs its forwarding tables. XLA consumes the topology built by libtpunet to shard the model. Once execution begins, each device has its compiled program in local memory, knows its neighbors via ICI routing tables, and has its slice of the model weights in HBM. Thousands of devices execute in lockstep, synchronizing through collectives, without a single global runtime controller. The user does not see any of this background orchestration.</p><details><summary>Fault Tolerant Routing</summary> <p><img alt="ICI Interface" src="https://considerthebulldog.com/assets/tpu/ICI_Interface.png"> <em>ICI Interface <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a></em></p> <p>Packets hop through a path of ICI switches and optical fibers to arbitrary pairs of TPUs determined by libtpunet once during setup. xconnects initiate mirror configuration in the OCS, triggering on-chip device managers to initialize physical connections between ICIs. When libtpunet issues an ICI session start it clears and rightsizes the ICI buffers in the data layer for new RDMAs. Routing is handled by forwarding tables that provide a simple abstraction to locate destination TPUs. XLA emits sets of RDMA operations called transactions for collective communications. On-chip DMA engines read data from HBM and send the data to the ICI’s transaction layer to send over the network <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a>. All the required hardware for training drags down MTBF <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a>, so the system needs to be resilient to outages without bringing everything down.</p> <p><img alt="TPU Fault Tolerance" src="https://considerthebulldog.com/assets/tpu/TPU_Fault_Tolerance.png"> <em>TPU Fault Tolerance <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a></em></p> <p>The system manages faulty links with fault tolerant routing. An offline integer linear program simulates link outages and frames the route selection as a max flow problem, using an all-to-all collective as the canonical use case. The results from the ILP are cached and accessible by libtpunet. Fault tolerant routing uses Wild First Routing as its heuristic. Packets can take a wild hop around faulty links before reverting to fault free routing. Though using fault tolerant routing may induce network congestion, TPU availability benefits <a href="https://considerthebulldog.com/tte-tpu/#ref-6">[6]</a>.</p></details><p>Getting the whole system to cooperate at scale needs clear boundaries and hand-offs. Borg, PM, and libtpunet bless the configuration of the workload before triggering execution. When TCO skews towards operation, getting these pieces right is as important as systolic arrays and memory hierarchies. But this presentation of how the software works is also subject to the constant evolution of the TPU. Cores communicate over OCI. Chips communicate over ICI. Racks connect remote ICI links over OCS. That leaves us with one final communication frontier: the datacenter network.</p><p><img alt="Mixture of Experts Routing" src="https://considerthebulldog.com/assets/tpu/MoE_Layer.png"> <em>Mixture of Experts Routing <a href="https://considerthebulldog.com/tte-tpu/#ref-38">[38]</a></em></p><p>SPMD assumes every device can communicate over ICI with predictable latency, which constrains developers to slice sizes that fit on a single pod. Islands of accelerators <a href="https://considerthebulldog.com/tte-tpu/#ref-8">[8]</a> leave idle capacity stranded across pods, and under contention, jobs struggle to get the right-shaped device allocation. Individual pods also constrain algorithmic flexibility. Unlike traditional transformers, Mixture-of-Experts models include runtime data dependencies. The gating mechanism in MoEs introduces non-deterministic routing during execution. The SPMD model has to be stretched to express the fine-grained, data-dependent control flow these models need. If you want to shard experts across pods there is no natural way to do so. Without the DCN there is no dynamic routing, resource sharing, or use of idle chips across pods.</p><p>The datacenter network (DCN) connects islands using Google’s Jupiter fabric <a href="https://considerthebulldog.com/tte-tpu/#ref-9">[9]</a>. From the TPU’s point of view it is the communication that doesn’t occur over ICI. Extending the many cores, one logical system scaling approach gets complicated by varying latency and bandwidth characteristics. Two solutions emerged from these limitations. Multislice extends SPMD across pod boundaries. It is a conservative but compatible approach with existing code. Pathways abandoned synchronous execution for asynchronous dataflow. It is more complex but necessary for true heterogeneity.</p><p><img alt="Multislice over DCN logical Diagram" src="https://considerthebulldog.com/assets/tpu/MultiSlice.png"> <em>Logical diagram of Multislice over DCN <a href="https://considerthebulldog.com/tte-tpu/#ref-26">[26]</a></em></p><p>Multislice extends existing SPMD code across pod boundaries with minimal changes. Pod boundaries are treated as just another level in the communication hierarchy. SPMD still uses gang-scheduled execution, but XLA understands that some collectives happen over ICI and others happen over slower DCN. The familiar declarative slice syntax adds a parameter to select devices across islands. The compiler optimizes collective placement to minimize cross-pod traffic. Multislice expands the number of devices available for training by providing access to resources across pods <a href="https://considerthebulldog.com/tte-tpu/#ref-26">[26]</a>.</p><p><img alt="Pathways System Overview" src="https://considerthebulldog.com/assets/tpu/PW_System_Overview.png"> <em>Pathways System Overview <a href="https://considerthebulldog.com/tte-tpu/#ref-8">[8]</a></em></p><p>Pathways is a plug-in replacement for JAX’s backend that virtualizes the datacenter <a href="https://considerthebulldog.com/tte-tpu/#ref-8">[8]</a>. Instead of one giant SPMD program running in lockstep, it models execution as a DAG of compiled functions distributed across islands. Gang scheduling still happens within each island, but between islands coordination is asynchronous. There’s no single global runtime controller for the whole job. Mixture-of-Experts models can route activations dynamically to experts on different pods, and pipeline parallel stages can span multiple islands connected over DCN. Multiple programs can time-multiplex accelerators without context-switching overhead. Users request devices and the client compiles programs into a device-agnostic Pathways IR. XLA analyzes the program, the resource manager assigns physical TPUs, and the system inserts data movement operations between shards. Orchestration is complete by the time execution begins. Each device knows its program, its neighbors, and its slice of model weights.</p><p>Pathways uses a sharded dataflow model built on Plaque <a href="https://considerthebulldog.com/tte-tpu/#ref-8">[8]</a>. Each node represents a compiled function executing across thousands of TPU shards. The system uses parallel asynchronous dispatch. Pathways pipelines host side work in parallel instead of waiting for computation A to finish before preparing computation B. A control-plane scheduler per island enforces gang scheduling across programs. Between islands, Pathways uses centralized dispatch to coordinate placement and data movement. Data moves directly between accelerators over ICI within islands and DCN between islands. Pathways matches multi-controller performance by front-loading coordination work, even though cross-island dispatch is mediated by the control plane rather than issued independently by each host. This execution model performs as well as JAX and lifts restrictions on algorithmic expressibility <a href="https://considerthebulldog.com/tte-tpu/#ref-8">[8]</a>.</p><p>A dedicated upstart could reproduce the hardware design philosophy, but the software co-design makes the TPU a mammoth. Borg allocates resources and preempts jobs. The Pod Manager configures optical switches. libtpunet knows every ICI routing edge case and manages fault tolerance. XLA compiles with full knowledge of topology and latencies. SPMD partitions models while maintaining the illusion of one giant device. Multislice extends that illusion across pods. Pathways rethinks distributed execution and virtualizes the datacenter as one programmable pool. Schedulers, compilers, and coordination systems all play one long song. Building a TPU competitor needs generations of hard earned experience points. Each new design reconsiders which approaches were dead ends. Admitting you were wrong and doubling back is the game. Thinking about the TPU is thinking about Everything Else.</p><h2 id="ceci-n-est-pas-une-tpu">Ceci n’est pas une TPU</h2><p>After TPUv4 the well of detailed microarchitecture papers runs dry. You can still find information scattered across the internet, but not in the same succinct, curated way. Maybe more papers will be released publicly and we’ll be able to study these designs in greater detail, but until then we have to cobble together an understanding of our own. TPUv4 and v4i are followed by TPUv5p (performance) and v5e (efficiency), Trillium (v6e), and Ironwood (v7). We know that the inference (e) optimized designs retain a single-core architecture and use 2D tori instead of 3D tori. We know the interconnect and HBM performance numbers for the fifth, sixth, and seventh generation chips. We know that Trillium and Ironwood revert to 256x256 systolic arrays. We know that Ironwood scales up to 9,216 chips for training and 256 for inference with 1.77PB HBM that delivers 42.5 FP8 ExaFlops (6x Perf/W improvement over TPUv4) with a chiplet design for next generation reasoning and MoE workloads <a href="https://considerthebulldog.com/tte-tpu/#ref-16">[16]</a><a href="https://considerthebulldog.com/tte-tpu/#ref-20">[20]</a><a href="https://considerthebulldog.com/tte-tpu/#ref-21">[21]</a><a href="https://considerthebulldog.com/tte-tpu/#ref-23">[23]</a><a href="https://considerthebulldog.com/tte-tpu/#ref-24">[24]</a>.</p><p>And I know that all of this fails to capture the totality of the enhancements since TPUv4. But a spec sheet like the one <a href="https://www.nextplatform.com/2025/04/09/with-ironwood-tpu-google-pushes-the-ai-accelerator-to-the-floor/">here</a> or a primer like the one <a href="https://jax-ml.github.io/scaling-book/tpus/">here</a> could have told us that. The subsequent papers have focused on the system, but discussions of the system hide the simple origins of the device behind a hodgepodge of specs and new thundering heights. The essence of the thing becomes a folklorish amalgam of TPU lore. Myths are about meaning. Moore’s Law was never free in the literal sense. It required diligent engineering and enduring frustration, but decade after decade the compounding continued. The idea of Moore’s Law cast a spell that actualized its reality.</p><p>By nature the TPU is what it is not. The thrust and posturing of papers, talks, slides, and internet chatter focus on the technical minutiae, but the seams that hold this constellation of facts and figures together are the ordinary and the human. They are long emails and oscilloscopes in equal measure. How many of these choices go unseen? Hand-wringing about the system internals helps us to glimpse the creative act, but we mistake the painting for the paint chemistry. In this new world where nothing is free, every decision comes at an intentional, excruciating cost. The weight of the space of possibilities grows heavier knowing that each decision may foreclose another. Each choice is an act of reinvention in the face of a future that folds onto itself.</p><p>The TPU is an artifact born out of the quiet solace of steady hands doing careful engineering. AI DSAs are unlikely to be self-fulfilling in the same infinite feeling way as Moore’s Law. They will be five hundred ordinary decisions that compose into something greater. Can we make it smaller? Can we make it bigger? Can we make it easier to use? When we skim specs like the ones strewn above we notice the changes and feel the weight of what they represent. As new pressures get applied new entities emerge. For a moment we sense each decision branching into some unknown. Our new AI-obsessed world brings with it the demands of new ways of thinking. It is a reminder that the future is always at hand, and that if we participate in the myth-making we find that there are dragons after all.</p><hr><h2 id="references">References:</h2><p><a id="ref-1" target="_blank"></a>[1]: <a href="https://arxiv.org/ftp/arxiv/papers/1704/1704.04760.pdf">In-Datacenter Performance Analysis of a Tensor Processing Unit​</a></p><p><a id="ref-2"></a>[2]: <a href="https://gwern.net/doc/ai/scaling/hardware/2021-norrie.pdf">The Design Process for Google’s Training Chips: TPUv2 and TPUv3</a></p><p><a id="ref-3"></a>[3]: <a href="https://dl.acm.org/doi/pdf/10.1145/3360307">A Domain-Specific Supercomputer for Training Deep Neural Networks</a></p><p><a id="ref-4"></a>[4]: <a href="https://arxiv.org/pdf/2304.01433">TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings</a></p><p><a id="ref-5"></a>[5]: <a href="https://gwern.net/doc/ai/scaling/hardware/2021-jouppi.pdf">Ten Lessons From Three Generations Shaped Google’s TPUv4i</a></p><p><a id="ref-6"></a>[6]: <a href="https://www.usenix.org/system/files/nsdi24spring_prepub_zu.pdf">Resiliency at Scale: Managing Google’s TPUv4 Machine Learning Supercomputer</a></p><p><a id="ref-7"></a>[7]: <a href="https://arxiv.org/pdf/2105.04663">GSPMD: General and Scalable Parallelization for ML Computation Graphs</a></p><p><a id="ref-8"></a>[8]: <a href="https://arxiv.org/pdf/2203.12533">PATHWAYS: ASYNCHRONOUS DISTRIBUTED DATAFLOW FOR ML</a></p><p><a id="ref-9"></a>[9]: <a href="https://dl.acm.org/doi/pdf/10.1145/3544216.3544265">Jupiter Evolving: Transforming Google’s Datacenter Network via Optical Circuit Switches and Software-Defined Networking</a></p><p><a id="ref-10"></a>[10]: <a href="https://arxiv.org/pdf/2208.10041">Mission Apollo: Landing Optical Circuit Switching at Datacenter Scale</a></p><p><a id="ref-11"></a>[11]: <a href="https://gwern.net/doc/cs/hardware/2014-horowitz-2.pdf">Computing’s Energy Problem</a></p><p><a id="ref-12"></a>[12]: <a href="https://dl.acm.org/doi/pdf/10.1145/3361682">Domain-Specific Hardware Accelerators</a></p><p><a id="ref-13"></a>[13]: <a href="https://parallel.princeton.edu/papers/wall-hpca19.pdf">The Accelerator Wall: Limits of Chip Specialization</a></p><p><a id="ref-14"></a>[14]: <a href="https://arxiv.org/pdf/1911.05289">The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design</a></p><p><a id="ref-15"></a>[15]: <a href="https://fleetwood.dev/posts/domain-specific-architectures">Domain specific architectures for AI inference</a></p><p><a id="ref-16"></a>[16]: <a href="https://jax-ml.github.io/scaling-book/tpus/">How to Think About TPUs – Chapter 2</a></p><p><a id="ref-17"></a>[17]: <a href="https://henryhmko.github.io/posts/tpu/tpu.html">TPU Deep Dive</a></p><p><a id="ref-18"></a>[18]: <a href="https://www.telesens.co/2018/07/30/systolic-architectures/">Understanding Matrix Multiplication on a Weight-Stationary Systolic Architecture</a></p><p><a id="ref-19"></a>[19]: <a href="https://www.nextplatform.com/2017/04/05/first-depth-look-googles-tpu-architecture/">First in-depth look at Google’s TPU Architecture</a></p><p><a id="ref-20"></a>[20]: <a href="https://www.nextplatform.com/2025/04/09/with-ironwood-tpu-google-pushes-the-ai-accelerator-to-the-floor/">WITH “IRONWOOD” TPU, GOOGLE PUSHES THE AI ACCELERATOR TO THE FLOOR</a></p><p><a id="ref-21"></a>[21]: <a href="https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/">Ironwood: The first Google TPU for the age of inference</a></p><p><a id="ref-22"></a>[22]: <a href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm">TPU Architecture – Google Documentation</a></p><p><a id="ref-23"></a>[23]: <a href="https://www.servethehome.com/google-ironwood-tpu-swings-for-reasoning-model-leadership-at-hot-chips-2025/">Google Ironwood TPU Swings for Reasoning Model Leadership at Hot Chips 2025</a></p><p><a id="ref-24"></a>[24]: <a href="https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus">Announcing Trillium, the sixth generation of Google Cloud TPU</a></p><p><a id="ref-25"></a>[25]: <a href="https://openxla.org/xla/sparsecore">A deep dive into SparseCore for Large Embedding Models</a></p><p><a id="ref-26"></a>[26]: <a href="https://cloud.google.com/blog/products/compute/using-cloud-tpu-multislice-to-scale-ai-workloads">How to scale AI training to up to tens of thousands of Cloud TPU chips with Multislice</a></p><p><a id="ref-27"></a>[27]: <a href="https://hc2023.hotchips.org/assets/program/conference/day2/ML%20training/HC2023.Session5.ML_Training.Google.Norm_Jouppi.Andy_Swing.Final_2023-08-25.pdf">A Machine Learning Supercomputer With An Optically Reconfigurable Interconnect and Embeddings Support – HotChips Slides</a></p><p><a id="ref-28"></a>[28]: <a href="https://hc33.hotchips.org/assets/program/tutorials/HC2021.Google.Sameer%20Kumar.pdf">Challenges in large scale training of Giant Transformers on Google TPU machines – HotChips Slides</a></p><p><a id="ref-29"></a>[29]: <a href="https://hc32.hotchips.org/assets/program/tutorials/HC2020.Google.SameerKumarDehaoChen.v02.pdf">Exploring Limits of ML Training on Google TPUs – HotChips Slides</a></p><p><a id="ref-30"></a>[30]: <a href="https://old.hotchips.org/hc31/HC31_T3_Cloud_TPU_Codesign.pdf">Cloud TPU: Codesigning Architecture and Infrastructure – HotChips Slides</a></p><p><a id="ref-31"></a>[31]: <a href="https://pages.cs.wisc.edu/~markhill/seminar2020/jouppi2020_10_tpu-v2-v3.pdf">A DOMAIN-SPECIFIC TPU SUPERCOMPUTER FOR TRAINING DEEP NEURAL NETWORKS – Slides</a></p><p><a id="ref-32"></a>[32]: <a href="https://www.hc32.hotchips.org/assets/program/conference/day2/HotChips2020_ML_Training_Google_Norrie_Patil.v01.pdf">Google’s Training Chips Revealed: TPUv2 and TPUv3 – Slides</a></p><p><a id="ref-33"></a>[33]: <a href="https://chips-compilers-mlsys-22.github.io/assets/slides/10%20Lessons%204%20TPU%20gens%20%2B%20CO2e%2045%20minutes.pdf">A Decade of Machine Learning Accelerators: Lessons Learned and Carbon Footprint – MLSys Slides</a></p><p><a id="ref-34"></a>[34]: <a href="https://tnm.engin.umich.edu/wp-content/uploads/sites/353/2020/08/2020.6.sparse-tpu_ics2020.pdf">Sparse-TPU: Adapting Systolic Arrays for Sparse Matrices</a></p><p><a id="ref-35"></a>[35]: <a href="https://www.eecs.harvard.edu/htk/static/files/1978-cmu-cs-report-kung-leiserson.pdf">Systolic Array For VLSi</a></p><p><a id="ref-36"></a>[36]: <a href="https://www.eecs.harvard.edu/~htk/publication/1982-kung-why-systolic-architecture.pdf">Why Systolic Architectures?</a></p><p><a id="ref-37"></a>[37]: <a href="https://dl.acm.org/doi/pdf/10.5555/800052.801897">Doubly Twisted Torus Networks for VLSI Processor Arrays</a></p><p><a id="ref-38"></a>[38]: <a href="https://huggingface.co/blog/moe">Mixture of Experts Explained</a></p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Autism's confusing cousins (265 pts)]]></title>
            <link>https://www.psychiatrymargins.com/p/autisms-confusing-cousins</link>
            <guid>46172443</guid>
            <pubDate>Sat, 06 Dec 2025 11:18:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.psychiatrymargins.com/p/autisms-confusing-cousins">https://www.psychiatrymargins.com/p/autisms-confusing-cousins</a>, See on <a href="https://news.ycombinator.com/item?id=46172443">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!gvh6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!gvh6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 424w, https://substackcdn.com/image/fetch/$s_!gvh6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 848w, https://substackcdn.com/image/fetch/$s_!gvh6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!gvh6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!gvh6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg" width="1152" height="384" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:384,&quot;width&quot;:1152,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:37942,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.psychiatrymargins.com/i/180764157?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!gvh6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 424w, https://substackcdn.com/image/fetch/$s_!gvh6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 848w, https://substackcdn.com/image/fetch/$s_!gvh6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!gvh6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d6dc22d-7d94-465c-9379-b191248a32e0_1152x384.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><blockquote><p>“I think that these days what we mean by “autism” is basically “weird person disease.””</p></blockquote><p><strong>Sorbie Richner</strong><span>, </span><a href="https://www.psychiatrymargins.com/p/rich-girl-rehab" rel="">Rich Girl Rehab</a></p><blockquote><p>“Accurate diagnosis requires consideration of multiple diagnoses. Sometimes, different diagnoses can overlap with one another and can only be differentiated in subtle and nuanced ways, but particular diagnoses vary considerably in levels of public awareness. As such, an individual may meet the diagnostic criteria for one diagnosis but self-diagnoses with a different diagnosis because it is better known.”</p></blockquote><p><strong>Sam Fellowes</strong><span>, </span><a href="https://www.cambridge.org/core/journals/royal-institute-of-philosophy-supplements/article/abs/selfdiagnosis-in-psychiatry-and-the-distribution-of-social-resources/91A981A3908EE250DE19CF597277F197" rel="">Self-Diagnosis in Psychiatry and the Distribution of Social Resources</a></p><p>Unsurprisingly, these days I meet many people in the psychiatric clinic who are convinced that they have autism, or suspect (with various degrees of confidence) that they have autism, or report being diagnosed with autism at some point in their lives by some clinician. And for a fair number of such individuals, I cannot say with reasonable certitude that they have autism. The reasons they give for considering autism vary widely, but tend to be along the lines of…</p><ul><li><p>“Eye contact makes me very uncomfortable.”</p></li><li><p>“I suck at small talk.”</p></li><li><p>“I have rigid routines.”</p></li><li><p>“I hyper-focus on my hobbies.”</p></li><li><p>“I am always fidgeting.”</p></li><li><p>“Social interaction exhausts me.”</p></li><li><p>“I really bad at making friends.”</p></li><li><p>“I don’t fit in; people find me weird.”</p></li></ul><p>What’s interesting about many of the items above is that the number one diagnostic possibility in my mind is an anxiety disorder of some sort. I remember seeing a woman who was a classic example of someone with high neuroticism, poor self-esteem, and severe social anxiety, and she had believed for much of her life that she was autistic because some random doctor somewhere at some point (she couldn’t even remember who or what sort of assessment this involved) had told her that she had autism, and she believed it because it fit in with her experience of being awkward-shy-weird.</p><p>It is common for me to meet individuals who think they have autism and find myself thinking, “schizoid,” “obsessive compulsive,” “cluster B,” “social anxiety,” “generalized anxiety,” “trauma,” “socially awkward,”… None of these, however, have the mimetic virality of autism.</p><p>I don’t want to come across as being skeptical of the reality of autism as a diagnosis or as asserting that most people are misdiagnosed. Autism exists, to the extent that any psychiatric disorder exists. Not everyone is misdiagnosed, perhaps even most people.  I am not trying to say, “autism is bullshit.” It’s not. I offer the diagnosis of autism as a clinician perhaps as often as I find myself doubting it.</p><p><span>What intrigues me is that people are drawn to autism as a diagnosis because it seems to offer recognition of something they’ve lived with: they may be deeply awkward, terribly shy, or bad with people, they may struggle with social interactions, they may find other people annoying, other people may find them weird, they may have a hard time connecting to others, they may have been bullied, and they may have directed their loneliness or introversion towards peculiar interests or hobbies. Autism seems to them to capture all that. It seems like an apt and appealing narrative. But autism may also be the only relevant diagnosis they’ve heard of or are familiar with. They haven’t seen any cool TikToks about being schizoid. No one’s offering them quizzes about being schizotypal. A random pediatrician or primary care doc is not going to tell them they have an obsessive-compulsive style of personality. So when some professional doubts that they have “autism,” they see it as a dismissal or rejection of their “lived experience.” </span><em>Of course, I am weird-anxious-awkward. How can you say otherwise?</em><span> What they don’t know is that the choice is not between autism or nothing, but rather between autism and about a dozen other diagnostic possibilities.</span></p><p>So for the sake of our collective sanity, let’s consider a few of them…</p><p><span>To be diagnosed with autism spectrum disorder according to DSM-5, a person must have ongoing </span><strong>difficulties in social communication and interaction</strong><span> in all three areas: trouble with back-and-forth social connection, problems with nonverbal communication like eye contact and body language, and difficulty making or keeping friendships. They also must show at least two types of </span><strong>repetitive or restricted behaviors</strong><span>, such as repetitive movements or phrases, needing things to stay the same, having very intense focused interests, or being unusually sensitive (or under-sensitive) to things like sounds, textures, or lights. These patterns must have been </span><strong>present since early childhood</strong><span> (even if they weren’t noticed until later when life got more complicated), lead to substantial </span><strong>impairment in functioning</strong><span>, and can’t simply be explained by intellectual disability (or other psychiatric disorders).</span></p><p>To “have” autism is simply to demonstrate this cluster of characteristics at the requisite level of severity and pervasiveness. It doesn’t mean that the person has a specific type of brain attribute or a specific set of genes that differentiates them from non-autistics. No such internal essence exists for the notion as currently conceptualized.</p><p><span>Autism spectrum is wide enough to have very different prototypes within it. On one end we have profound autism, representing someone with severe autistic traits who is completely dependent on others for care and has substantial intellectual disability or very limited language ability. At the other end, we have successful nerdy individuals with autistic traits and superior intelligence, often seen in science or academia, à la Sheldon Cooper. (Holden Thorp, editor-in-chief of the </span><em>Science</em><span> journals and former UNC chancellor, for example, has publicly disclosed his own autism diagnosis.) This wide range is confusing enough on its own, even without considering other conditions that can present with autism-like features.</span></p><p><span>Autism cannot be identified via medical “tests.” It is identified via clinical information in the form of history, observation, and interaction, and the less information available or the more unreliable the information provided is, the more uncertain we’ll be. To </span><em>have</em><span> autism is basically a judgment call that one is a good match to a descriptive prototype. We can get this judgment wrong, and we sometimes do get it wrong. (There is nothing wrong with this fallibility as such, as long as we recognize it. Lives have been built on foundations less sturdy.)</span></p><p><span>Autism as a category or identity has taken on a life of its own. I am aware that not everyone in the neurodiversity crowd accepts the legitimacy of clinician judgments or clinical criteria as outlined in the diagnostic manuals, such as the DSM and ICD. There are other ways to ground the legitimacy of self-diagnoses, </span><a href="https://www.tandfonline.com/doi/full/10.1080/09515089.2024.2327823" rel="">in theoretically virtuous accounts or pragmatic uses</a><span>, which require distinct considerations of their own; I don’t reject that. But here, I am concerned with autism as a clinical diagnosis and the accuracy of autism understood in terms of alignment with clinical diagnosis. Would competent and knowledgeable clinicians with access to all relevant clinical information concur that the person’s presentation meets diagnostic criteria for autism? If you don’t really care about that, this post is not for you.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!NXb7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!NXb7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 424w, https://substackcdn.com/image/fetch/$s_!NXb7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 848w, https://substackcdn.com/image/fetch/$s_!NXb7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!NXb7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!NXb7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg" width="720" height="472" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:472,&quot;width&quot;:720,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:97251,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.psychiatrymargins.com/i/180764157?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!NXb7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 424w, https://substackcdn.com/image/fetch/$s_!NXb7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 848w, https://substackcdn.com/image/fetch/$s_!NXb7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!NXb7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34fe964c-f33d-4e4e-9ad2-8ccb624fe08b_720x472.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Lascaux Cave</figcaption></figure></div><p>Schizoid personality describes people who have little desire for close relationships and prefer solitary activities. Unlike people who are simply shy or socially anxious, individuals with schizoid personality style genuinely don’t find relationships rewarding or necessary. They typically appear emotionally detached or cold, show restricted emotional expression, seem indifferent to praise or criticism, and have few if any close friends or confidants. They often live quietly on the margins of society, pursuing solitary interests or jobs. They keep their inner worlds (which can be quite rich) private and don’t seek emotional intimacy with others.</p><p>In autism, social difficulties stem from genuine challenges with processing social information: difficulty reading facial expressions, understanding implied meanings, picking up on social cues, knowing unwritten social rules, etc. In schizoid personality, the person typically understands social conventions but simply isn’t motivated to engage with them. They withdraw from genuine disinterest. Schizoid personality also lacks the additional features of autism (repetitive or restricted behaviors, various sensory sensitivities).</p><p>Schizotypal personality describes people who have odd or eccentric beliefs, unusual perceptual experiences, and difficulties with close relationships. Unlike schizoid personality (which involves simple disinterest in relationships), schizotypal includes strange ways of thinking and perceiving the world. People with schizotypal personality might believe in telepathy, feel they have special powers, think random events have special meaning for them personally, or have unusual perceptual experiences (like feeling a presence in the room or hearing whispers). They typically have few close friends, experience social anxiety that doesn’t improve with familiarity, and may appear paranoid or suspicious of others’ motives. Both schizotypal personality and autism can involve social difficulties and odd or eccentric behavior, but in schizotypal personality, the peculiarity comes from magical thinking, paranoid ideas, and perceptual distortions.</p><p>Obsessive-compulsive personality describes people who are preoccupied with orderliness, perfectionism, and control. These individuals are rigid rule-followers who want things to be done “the right way,” have difficulty delegating tasks, and get caught up in details and lists to the point where they lose sight of the main goal. They tend to be workaholics who neglect leisure and friendships, are inflexible about matters of morality or ethics, and are often stubborn and controlling. Both obsessive-compulsive personality and autism can involve rigid adherence to routines, rules, and specific ways of doing things. In obsessive-compulsive personality, the inflexibility comes from anxiety about loss of control. The person is trying to, consciously or unconsciously, manage anxiety through control and perfectionism. In autism, the need for sameness and routine serves different functions. It provides predictability in a world that feels confusing or it helps with sensory regulation rather than anxiety-driven perfectionism.</p><p>Severe social anxiety is an intense, persistent fear of social situations where a person might be judged, embarrassed, or humiliated. Social anxiety disorder involves overwhelming fear that interferes with daily life. People with this condition worry excessively about saying something stupid, looking foolish, or being rejected. They often avoid social situations entirely, which can lead to isolation, difficulty maintaining employment, and problems forming relationships. Both social anxiety and autism involve social difficulties and withdrawal. Social anxiety usually improves significantly in comfortable, safe environments (like with close family or friends), while autistic social differences tend to be more consistent across all contexts.</p><p>Borderline personality disorder involves intense emotional instability, unstable relationships, fear of abandonment, and a shifting sense of self, with people experiencing rapid mood swings and chaotic relationships that alternate between idealization and devaluation of others. While it can resemble autism through social difficulties, emotional dysregulation, rigid thinking, and feeling different from others, the key distinctions are that borderline centers on intense relationship preoccupations and emotional chaos, whereas autism involves genuine difficulty understanding social cues and communication; borderline features rapidly shifting identity and relationship-triggered mood swings, while autism includes stable self-concept, sensory sensitivities, restricted interests, and literal communication that aren’t present in borderline; and borderline symptoms fluctuate dramatically with relationship stability while autistic traits remain consistent across contexts.</p><p>Social communication disorder is a condition in DSM-5 where someone has significant, ongoing difficulty using verbal and nonverbal communication appropriately in social contexts. People with social communication disorder struggle with the “pragmatic” aspects of language, that is, knowing how to use language effectively in social situations. They may have trouble understanding when to take turns in conversation, knowing how much detail to give, adjusting their speaking style for different situations, understanding implied meanings or hints, picking up on nonverbal cues like body language and facial expressions, and knowing how to start, maintain, or end conversations naturally. This makes forming friendships and relationships difficult and affects life functioning. The social communication problems in social communication disorder look nearly identical to the “Criterion A” features of autism. However, unlike autism, people with social communication disorder don’t show repetitive behaviors, restricted interests, sensory sensitivities, or the need for sameness and routine.</p><p>Social communication disorder is rarely diagnosed in favor of autism primarily because autism provides access to critical services, insurance coverage, educational support, and legal protections that social communication disorder does not reliably offer, creating strong practical incentives for families and clinicians to prefer the autism diagnosis. Additionally, autism has an established evidence base, validated assessment tools, clear intervention protocols, and a large supportive community with a neurodiversity-affirming culture, while social communication disorder has none of these. It has no community, minimal research, no specific treatments, and little professional awareness since it was only introduced in the DSM in 2013. Service delivery, insurance, and educational systems are built entirely around autism rather than social communication disorder, and since both conditions require similar interventions for social-communication difficulties, there’s little practical incentive to make the diagnostic distinction, especially when the boundary between them (whether restricted/repetitive behaviors are truly absent or just subtle) is often unclear and clinicians are often unsure the distinction really matters.</p><p>Trauma-related disorders, particularly from early developmental trauma, severe neglect, or disrupted attachment, can mimic autism through social withdrawal and avoidance of eye contact (defensive protection rather than social processing difficulties), communication delays and difficulties (from lack of language exposure or trauma’s impact on brain development), emotional dysregulation and meltdowns (from emotional dysregulation rather than sensory overload), repetitive self-soothing behaviors (anxiety management rather than stimming), sensory sensitivities (hypervigilance rather than sensory processing differences), and rigid need for routine (anxiety-driven safety-seeking rather than cognitive processing style). </p><p>Severe early deprivation can create “quasi-autistic” patterns that can be genuinely difficult to distinguish. The critical distinctions are that trauma-related difficulties typically improve significantly in safe, nurturing environments and with adequate psychological treatment, show more variability across contexts (worse with triggers), are tied to identifiable adverse experiences rather than present from earliest infancy, and lack the restricted interests and genuine social communication processing deficits of autism.</p><p>Social awkwardness refers to social ineptness without meaningful impairment that falls within what is considered normal or typical human variation. This can be mistaken for autism because both may involve limited friendships, preference for solitude, conversation difficulties, reduced eye contact, and intense interests, particularly fueled by online self-diagnosis culture and broad autism awareness. The key distinctions are that socially awkward individuals understand what they should do socially but find it difficult or uninteresting (versus genuinely not understanding unwritten rules), show significant improvement with practice and maturity, are more comfortable in specific contexts, lack the sensory sensitivities and restricted/repetitive behaviors required for autism diagnosis, and generally achieve life goals despite awkwardness rather than experiencing clinically significant impairment.</p><p>Selective Mutism, Intellectual Disability (without autism), Stereotypic Movement Disorder, Attention-Deficit/Hyperactivity Disorder (ADHD), Schizophrenia Spectrum Disorders, Avoidant Personality Disorder, Attachment Disorders, Generalized Anxiety Disorder, Obsessive-Compulsive Disorder, and Rett Syndrome (a characteristic pattern of developmental regression after initial normal development, typically 6-18 months).</p><p>Comorbidity is possible and expected. Someone can be autistic and have maladaptive personality patterns, trauma histories, or anxiety disorders that complicate the presentation. Developmental context, response to relationships, and subjective experiences are all very important in looking beyond the surface presentation to understanding the meaning and functions of behaviors.</p><p><em>See also:</em></p><div data-component-name="DigestPostEmbed"><a href="https://www.psychiatrymargins.com/p/it-is-not-ludicrous-for-mildly-and" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!m4UH!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29b0ee8e-8905-4305-bd23-9a32a74a0fc0_736x414.jpeg"><img src="https://substackcdn.com/image/fetch/$s_!m4UH!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29b0ee8e-8905-4305-bd23-9a32a74a0fc0_736x414.jpeg" sizes="100vw" alt="It is not ludicrous for mildly and severely impaired to have the same diagnosis" width="140" height="140"></picture></div></a></div><div data-component-name="DigestPostEmbed"><a href="https://www.psychiatrymargins.com/p/the-overdiagnosis-confusion" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!pyp8!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb66d64d-46e4-4555-b163-1871fe295d11_742x550.jpeg"><img src="https://substackcdn.com/image/fetch/$s_!pyp8!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb66d64d-46e4-4555-b163-1871fe295d11_742x550.jpeg" sizes="100vw" alt="The “Overdiagnosis” Confusion" width="140" height="140"></picture></div></a></div><p data-attrs="{&quot;url&quot;:&quot;https://www.psychiatrymargins.com/p/autisms-confusing-cousins?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.psychiatrymargins.com/p/autisms-confusing-cousins?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linux Instal Fest Belgrade (163 pts)]]></title>
            <link>https://dmz.rs/lif2025_en</link>
            <guid>46172167</guid>
            <pubDate>Sat, 06 Dec 2025 10:20:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dmz.rs/lif2025_en">https://dmz.rs/lif2025_en</a>, See on <a href="https://news.ycombinator.com/item?id=46172167">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        

        <h2>Where and when</h2>

        <p>Linux Install Fest will be held on December 9, 2025 in the JAG3 classroom of the Faculty of Mathematics, at
            <a href="https://www.openstreetmap.org/node/3807078606">Jagićeva 5, Belgrade</a>. Entry to the classroom is possible from 6 pm to 9 pm.</p>

        <p>Jagićeva street is located between the  <a href="https://www.openstreetmap.org/node/6670711291"><em>Pijaca
                    Đeram</em></a> station where trams 5, 6, 7L and 14 stop, and the <a href="https://www.openstreetmap.org/node/1693535022"><em>Crveni krst</em></a> station where buses 21 and 83 stop, as well as trolleybuses 19, 22 and 29.</p>

        <h2>Program schedule</h2>

        <p>The goal of the gathering is to help interested install the Linux operating system on laptops. Several people with working Linux experience will be present at the event. In addition, depending on the interest of those present, short trainings related to the command line, git, web services, C programming, etc. can be held.</p>

        <p>After 9 p.m., we can continue socializing in one of the nearby bars.</p>

        <h2>Linux distributions</h2>

        <p>Linux is the core of the operating system, on which other programs are installed. All of these together make up a particular <em>Linux distribution</em>. There are many distributions, but we recommend the ones with a long tradition like the following:
        </p>

        <ul>
            <li><strong>The Debian</strong> distribution is probably the most suitable for Linux beginners. Known derivatives of Debian are Ubuntu, Mint and Zorin.</li>
            <li><strong>Fedora</strong> is also suitable for Linux beginners. It differs from the Debian distribution by the faster release of new versions, which in practice means that users have newer versions of the program.</li>
            <li><strong>Arch</strong> is a Linux distribution that allows the user to easily configure all parts of the system. This distribution is intended for people with significant Linux experience.</li>
        </ul>

        <p>If you are a beginner and haven't decided which distribution you want to install, we recommend Fedora or Debian. Regardless of which distribution you have, you will be able to run all programs intended for Linux.</p>

        <h2>End of 10</h2>

        <p>This year's Linux Install Fest is organized as part of the global <a href="https://endof10.org/">End of 10</a>
            campaign, which promotes the Linux operating system as a replacement for Windows 10.</p>

        <p>For a long time now, the Windows operating system has become increasingly unfriendly to users. On the contrary, many Linux distributions have improved the user experience to the maximum, and today we can claim that Linux enables significantly more pleasant work, regardless of the user's technical knowledge.</p>

        <p>Windows imposes on users functionalities that users do not want to use, such as: cloud integrations, AI, advertisements, mandatory accounts, and the like. These functionalities serve above all to increase Microsoft's profits, and have no benefit for most end users. Also, basic programs such as calendars, calculators or text editors have become slow and full of bugs. With useless functionalities, Windows becomes more demanding every year and requires the purchase of better hardware, leading to an increase in electronic waste. Unlike Windows, the latest Linux distributions work very well on computers that are more than a decade old.</p>

        <p>The choice of an operating system is no longer just a technical decision, but also an environmental attitude.</p>

        <h2>Installation methods</h2>

        <p>We can install Linux in three ways:</p>

        <ol>
            <li><strong>Inside a virtual machine on Windows.</strong> In this way, the user retains his existing operating system and the data on it. Linux in a virtual machine will be significantly slower than an installation without virtualization.
            </li>
            <li><strong>In addition to the existing operating system.</strong> If it is possible to shrink one of your partitions and free up at least 10GB of space, you can install a Linux operating system in addition to Windows. When booting the computer, the user will be able to choose whether to boot Windows or Linux. With such an installation, there is a certain risk that one of the subsequent Windows updates will reset the bootloader settings, after which a small intervention is required to make the Linux system accessible again.</li>
            <li><strong>By completely removing the Windows system.</strong> In place of the Windows partition, a new partition with the Linux distribution will be placed. Additional partitions that exist may or may not be removed.</li>
        </ol>

        <h2>Before arrival</h2>

        <p>In order for the installation to be effective, before coming to the Linux Instal Fest, it is necessary to make a backup of the data from the system partition if you decide on the second or third installation option. If you have two partitions (for example, C and D), move the data from the system partition (C:) that you want to keep to the non-system partition (D:). If you don't have an additional partition, you can use a USB flash drive. Pay attention to the files inside the user directory (Desktop, Downloads, Documents,... ), and export bookmarks and passwords from the browser.</p>

        <p>Also, before your arrival, you can familiarize yourself with the appearance and way of functioning of various Linux distributions. You can try some Linux distributions through the browser, without any installation, on the
        <a href="https://distrosea.com/">DistroSea</a> website (sometimes it is necessary to wait a short time to free up resources on the site). Please note that the operating system on this site is many times slower than the system installed on your computer.
        </p>

        <h2>Organizer</h2>

        <p>The organizer of the event is <a href="https://dmz.rs/en/">Decentrala</a> - a group of enthusiasts gathered around the ideas of decentralization and free dissemination of knowledge. So far, we have organized more than <a href="https://dmz.rs/en/events_archive">300 events</a>, and we regularly announce the next events on the <a href="https://dmz.rs/en/events">Events</a> page.
        </p>

        <p>In the following period, two more events for Linux beginners will be held at the same location (classroom JAG3):</p>
        <ul>
            <li><strong>Tuesday December 16</strong> -  Introduction to the Linux command line</li>
            <li><strong>Tuesday, December 23</strong> - Introduction to Git</li>
        </ul>
        <p>Events start at 6pm.</p>

        <h2>Ponovo</h2>
        <p>You can bring defective devices to the Linux install fest: laptops, phones, desktop computers, monitors... We will deliver them to the organization <a href="https://ponovo.rs/">Ponovo</a> in Kikinda during January. This organization will repair these devices and thereby prevent the increase of electronic waste.</p>
    </div></div>]]></description>
        </item>
    </channel>
</rss>