<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 09 Jul 2023 14:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Fairphone 4 is coming to the US (117 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2023/07/fairphone-is-coming-to-america/</link>
            <guid>36653224</guid>
            <pubDate>Sun, 09 Jul 2023 10:18:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2023/07/fairphone-is-coming-to-america/">https://arstechnica.com/gadgets/2023/07/fairphone-is-coming-to-america/</a>, See on <a href="https://news.ycombinator.com/item?id=36653224">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      Declare independence from unrepairable phones    —
</h4>
            
            <h2 itemprop="description">Fairphone teams up with the developer of the /e/ Android fork to enable US sales.</h2>
                    </header>
        <div itemprop="articleBody">
                                    
  




<!-- cache hit 69:single/related:504795d6a56d5ad471413097da82c7c8 --><!-- empty -->
  <div>
    <ul>
              <li data-thumb="https://cdn.arstechnica.net/wp-content/uploads/2023/07/FP4-front-back-angled-1-150x150.png" data-src="https://cdn.arstechnica.net/wp-content/uploads/2023/07/FP4-front-back-angled-1.png" data-responsive="https://cdn.arstechnica.net/wp-content/uploads/2023/07/FP4-front-back-angled-1.png 1080, https://cdn.arstechnica.net/wp-content/uploads/2023/07/FP4-front-back-angled-1.png 2560" data-sub-html="#caption-1951608">
          <figure>
            
                          <figcaption id="caption-1951608">
                <span></span>
                                  <p>
                    The Fairphone 4 with /e/ OS.                   </p>
                                                  <p><span></span>
                                          /e/                                      </p>
                              </figcaption>
                      </figure>
        </li>
              <li data-thumb="https://cdn.arstechnica.net/wp-content/uploads/2023/07/FP4-Grey-Exploded-150x150.png" data-src="https://cdn.arstechnica.net/wp-content/uploads/2023/07/FP4-Grey-Exploded.png" data-responsive="https://cdn.arstechnica.net/wp-content/uploads/2023/07/FP4-Grey-Exploded-980x653.png 1080, https://cdn.arstechnica.net/wp-content/uploads/2023/07/FP4-Grey-Exploded-1440x960.png 2560" data-sub-html="#caption-1951609">
          <figure>
            
                          <figcaption id="caption-1951609">
                <span></span>
                                  <p>
                    Just think of all the repairing you could do.                   </p>
                                                  <p><span></span>
                                          /e/                                      </p>
                              </figcaption>
                      </figure>
        </li>
              <li data-thumb="https://cdn.arstechnica.net/wp-content/uploads/2023/07/e_os_v1_launcher-150x150.jpg" data-src="https://cdn.arstechnica.net/wp-content/uploads/2023/07/e_os_v1_launcher.jpg" data-responsive="https://cdn.arstechnica.net/wp-content/uploads/2023/07/e_os_v1_launcher.jpg 1080, https://cdn.arstechnica.net/wp-content/uploads/2023/07/e_os_v1_launcher.jpg 2560" data-sub-html="#caption-1951610">
          <figure>
            
                          <figcaption id="caption-1951610">
                <span></span>
                                  <p>
                    Here's what the /e/ homescreen looks like, if you're wondering.                   </p>
                                                  <p><span></span>
                                          /e/                                      </p>
                              </figcaption>
                      </figure>
        </li>
          </ul>
  </div>

<p>Fairphone is a unique Android manufacturer; it offers a smartphone designed for repair, long software support times, and a selection of parts available online. The Dutch electronics manufacturer has mostly only focused on the European market and says, "When is Fairphone coming to the US?" is one of the most common questions it gets asked. Well, ask no more! The company is finally bringing the Fairphone 4 to the US—with some caveats.</p>
<p>Fairphone is launching a "US pilot" program to sell the Fairphone 4 in America in partnership with <a href="https://murena.com/">Murena</a>, the primary developer behind the /e/ Android fork. Murena has been selling Fairphones in Europe for a while, and they come pre-loaded with the /e/ OS, instead of Fairphone's build of Android, and now the US is getting the deal. The phone costs $599.</p>
<p>"We know based on feedback we have received that there are many people interested in Fairphone in the US." Fairphone CEO Eva Gouwens said in the press release. "However, currently our main focus is on the European market. This collaboration with e/OS/ is a great opportunity for us to pilot selling devices in the US market with a long-standing partner and learn more about the American market.”
</p><figure><img alt="The Fairphone parts. Note that the motherboard is not a &quot;commercial spare part&quot; which will make water damage repairs tough." src="https://cdn.arstechnica.net/wp-content/uploads/2023/07/msedge_H78BE5UzU2-980x569-1.png" width="980" height="569"><figcaption><p>The Fairphone parts. Note that the motherboard is not a "commercial spare part" which will make water damage repairs tough.</p><p>Fairphone</p></figcaption></figure>
<p>/e/ is a "fully <a href="https://doc.e.foundation/what-s-e#degoogling--ungoogling">deGoogled</a>," "privacy by design" fork of Android that dumps Google's apps and services for alternatives provided by /e/. Instead of the Play Store, you get the "<a href="https://doc.e.foundation/app-lounge#is-x-app-available-on-app-lounge/">App Lounge</a>" full of Android APKs and progressive web apps. Instead of Google Play Services, you get the open source <a href="https://microg.org/">microG</a> reimplementation. Instead of Google Maps, you'll get the OpenStreetMap-based <a href="https://www.magicearth.com/">Magic Earth</a>, Cloud data can be stored on the NextCloud-based <a href="https://murena.io/">Murena cloud</a>&nbsp;based on NextCloud, or&nbsp;<a href="https://gitlab.e.foundation/e/infra/ecloud-selfhosting">self-hosted</a> by the user. If you just wanted a repairable phone, this will be a big change over regular Android, but presumably it's also possible to flash the normal, <a href="https://support.fairphone.com/hc/en-us/articles/4405858261777-Fairphone-4-OS-Manual-Installation">Google-approved builds</a> of Android.</p>                                            
                                                        
<p>As a quick recap, the Fairphone 4 came out <a href="https://arstechnica.com/gadgets/2021/09/fairphone-4-has-an-incredible-5-year-warranty-aims-for-6-years-of-updates/">in 2021</a> with a Qualcomm Snapdragon 750G SoC, 6GB of RAM, and 128GB of storage. Just like before the days of sealed phones, the back comes off, and the 3905 mAh battery is user-removable. The whole phone is designed to be modular with repairable parts, as the camera array, loudspeaker, USB-C port, display, and phone body are all individual components. Normally Fairphone sells the parts on its website for easy repairs, but for the US, we're told parts will be available through Murena. If you go online and activate your warranty at <a href="http://fairphone.com/warranty">Fairphone.com/warranty</a>, the phone has a five year warranty.</p>
<p>By the time you read this, the phone should be up for sale at <a href="https://murena.com/shop/">murena.com/shop</a></p>

        <p><em>Listing image by /e/</em></p>
                                                      </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Development of the C Language (1993) (107 pts)]]></title>
            <link>https://www.bell-labs.com/usr/dmr/www/chist.html</link>
            <guid>36652934</guid>
            <pubDate>Sun, 09 Jul 2023 09:15:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bell-labs.com/usr/dmr/www/chist.html">https://www.bell-labs.com/usr/dmr/www/chist.html</a>, See on <a href="https://news.ycombinator.com/item?id=36652934">Hacker News</a></p>
<div id="readability-page-1" class="page">

<dl><dd><i>Dennis M. Ritchie<br>
Bell Labs/Lucent Technologies<br>
Murray Hill, NJ 07974  USA<tt>dmr@bell-labs.com</tt>
</i></dd></dl>
<dl><dd><h4>ABSTRACT</h4>
The C programming language was devised in the early 1970s as a system
implementation language for the nascent Unix operating system.
Derived from the typeless language BCPL, it evolved
a type structure;
created on a tiny machine as a tool to improve
a meager programming environment, it has become
one of the dominant languages of today.
This paper studies its evolution.
</dd></dl>
<h4>Introduction
</h4>

<dl>
<dt></dt><dt> </dt><dd>
NOTE:<i> *Copyright 1993 Association for Computing Machinery, Inc.
This electronic reprint made available by the author as a courtesy.
For further publication rights contact ACM or the author.
This article was presented at Second History of Programming
Languages conference, Cambridge, Mass., April, 1993.
<br>
It was then collected in the conference proceedings:
<i>History of Programming Languages-II</i>
ed. Thomas J. Bergin, Jr. and Richard G. Gibson, Jr.
ACM Press (New York) and Addison-Wesley (Reading, Mass), 1996;
ISBN 0-201-89502-1.
</i></dd><dt> </dt><dd></dd></dl>
<br>
This paper is about the development of the C programming language,
the influences on it,
and the conditions under which it was created.
For the sake of brevity, I omit full descriptions of C itself,
its parent B [Johnson 73] and its grandparent BCPL [Richards 79],
and instead concentrate on characteristic elements
of each language and how they evolved.
<p>
C came into being in the years 1969-1973,
in parallel with the early development
of the Unix operating system;
the most creative period occurred during 1972.
Another spate of changes peaked between 1977 and 1979,
when portability of the Unix system was being demonstrated.
In the middle of this second period, the first widely available description
of the language appeared:
<i>The C Programming Language,</i>
often called the `white book' or `K&amp;R' [Kernighan 78].
Finally, in the middle 1980s, the language was officially standardized
by the ANSI X3J11 committee, which made further changes.
Until the early 1980s, although compilers existed for a variety
of machine architectures and operating systems, the language was almost exclusively
associated with Unix;
more recently, its use has spread much more widely, and today it
is among the languages most commonly used throughout the computer industry.
</p>
<h4>History: the setting
</h4>
<p>
The late 1960s were a turbulent era
for computer systems research at Bell Telephone Laboratories
[Ritchie 78] [Ritchie 84].
The company was pulling
out of the Multics project [Organick 75], which had started as a joint venture
of MIT, General Electric, and Bell Labs; by 1969, Bell Labs management,
and even the researchers, came to believe
that the promises of Multics could be fulfilled
only too late and too expensively.  Even before the GE-645 Multics
machine was removed from the premises, an informal group, led
primarily by Ken Thompson, had begun investigating alternatives.
</p>
<p>
Thompson wanted to create a comfortable computing
environment constructed according to his own design, using whatever
means were available.  His plans, it is evident in retrospect,
incorporated many of the innovative aspects of Multics, including an
explicit notion of a process as a locus of control,
a tree-structured file system, a command interpreter
as user-level program, simple representation of text files, and generalized
access to devices.
They excluded others, such as unified access to
memory and to files.  At the start, moreover, he and the rest
of us deferred another pioneering (though not original)
element of Multics, namely writing almost exclusively in a
higher-level language.
PL/I, the implementation language of Multics, was not
much to our tastes, but we were also using other languages, including BCPL, and
we regretted losing the advantages of writing programs in a
language above the level of assembler, such as
ease of writing and clarity
of understanding.
At the time we did not put much weight
on portability; interest in this arose later.
</p>
<p>
Thompson was faced with
a hardware environment cramped and spartan even for the time:
the DEC PDP-7 on which he started in 1968 was a machine with 8K 18-bit words
of memory and no software useful to him.
While wanting to use a higher-level language,
he wrote the original Unix system in PDP-7 assembler.
At the start, he did not even program
on the PDP-7 itself, but instead used a set of macros
for the GEMAP assembler on a
GE-635 machine.
A postprocessor
generated a paper tape readable by the PDP-7.
</p>
<p>
These tapes were
carried from the GE machine to the PDP-7 for testing until a primitive Unix kernel,
an editor, an assembler, a simple shell (command interpreter), and a few utilities
(like
the Unix
<i>rm, cat, cp</i>
commands)
were completed.  After this point, the operating system was self-supporting:
programs could be written and tested without resort to paper tape,
and development continued on the PDP-7 itself.
</p>
<p>
Thompson's PDP-7 assembler outdid even DEC's in simplicity;
it evaluated expressions and emitted the corresponding bits.
There were no libraries,
no loader or link editor: the entire source of a program was presented to
the assembler, and the output file—with a fixed name—that emerged was directly
executable.
(This name,
<i>a.out</i>,
explains a bit of Unix etymology;
it is the output of the assembler.
Even after the system gained a linker
and a means of specifying another name explicitly,
it was retained as the default executable
result of a compilation.)
</p>
<p>
Not long after Unix first ran on the PDP-7, in 1969, Doug McIlroy created
the new system's first higher-level language: an implementation of
McClure's TMG [McClure 65].  TMG is a language for writing compilers
(more generally, TransMoGrifiers) in a top-down, recursive-descent
style that combines context-free syntax notation with
procedural elements.  McIlroy and Bob Morris had used TMG to write the early
PL/I compiler for Multics.
</p>
<p>
Challenged by McIlroy's feat in reproducing TMG,
Thompson decided that Unix—possibly it had not even been
named yet—needed a system programming language.
After a rapidly scuttled attempt at Fortran,
he created instead a language of his own,
which he called B.
B can be thought of
as C without types; more accurately, it is BCPL squeezed into 8K bytes of memory
and filtered through Thompson's brain.
Its name most probably represents
a contraction of BCPL, though
an alternate theory holds that
it derives from Bon [Thompson 69],
an unrelated language created by
Thompson during the Multics days.
Bon in turn was named either after his wife Bonnie,
or (according to an encyclopedia quotation in its manual),
after a religion whose rituals involve the murmuring of magic formulas.
</p>
<h4>Origins: the languages
</h4>
<p>
BCPL was
designed by Martin Richards in the mid-1960s while he was visiting MIT,
and was used during the early 1970s
for several interesting projects, among them the OS6 operating system
at Oxford [Stoy 72], and parts of the seminal Alto work at Xerox PARC [Thacker 79].
We became familiar with it
because the
MIT CTSS system [Corbato 62] on which Richards worked was used for Multics development.
The original BCPL compiler was transported both to Multics and to the GE-635
GECOS system
by Rudd Canaday
and others at Bell Labs [Canaday 69];
during the final throes of Multics's life at Bell Labs
and immediately after, it was the language of choice
among the group of people who would later become involved with Unix.
</p>
<p>
BCPL, B, and C all fit firmly in the traditional
procedural family typified by Fortran and Algol 60.
They are particularly oriented towards
system programming, are small and compactly described,
and are amenable to translation by simple compilers.  They are `close
to the machine' in that the abstractions they introduce are readily
grounded in the concrete data types and operations supplied by
conventional computers, and they rely on library routines
for input-output and other interactions with an operating system.
With less success, they also use library procedures to specify interesting
control constructs such as coroutines and procedure
closures.  At the same time, their abstractions lie at a sufficiently
high level that, with care, portability between machines can
be achieved.
</p>
<p>
BCPL, B and C differ syntactically in many details, but broadly
they are similar.
Programs consist of a sequence of
global declarations and function (procedure) declarations.
Procedures can be nested in BCPL, but may not refer to non-static
objects defined in containing procedures.
B and C avoid this restriction by imposing a more severe one:
no nested procedures at all.
Each of the languages (except for earliest versions of B)
recognizes
separate compilation, and provides a means for including
text from named files.
</p>
<p>
Several syntactic and lexical mechanisms of BCPL are
more elegant and regular than those of B and C.
For example, BCPL's procedure and data declarations
have a more uniform structure, and it supplies a more complete
set of looping constructs.
Although BCPL programs are notionally supplied from an undelimited
stream of characters, clever rules allow most semicolons to be elided
after statements that end on a line boundary.
B and C omit this convenience, and end
most statements with semicolons.
In spite of the differences, most of the statements and operators of BCPL map
directly into corresponding B and C.
</p>
<p>
Some of the structural differences between BCPL and B
stemmed from limitations on intermediate memory.
For example, BCPL declarations may take the form
</p><dl><dt></dt><dd><tt><pre>let P1 be <i>command</i>
and P2 be <i>command</i>
and P3 be <i>command</i>
 ...
</pre></tt></dd></dl>
where the program text represented by the commands
contains whole procedures.
The subdeclarations
connected by
<tt>and</tt>
occur simultaneously, so the name
<tt>P3</tt>
is known inside procedure
<tt>P1</tt>.
Similarly, BCPL can package a group of declarations and statements into
an expression that yields a value, for example
<dl><dt></dt><dd><tt><pre>E1 := valof <i>( </i><i>declarations</i><i> ; </i><i>commands</i><i> ; resultis E2 </i><i>) + 1
</i></pre></tt></dd></dl><i>
</i>The BCPL compiler readily handled such constructs
by storing and analyzing a parsed representation of the entire
program in memory before producing output.
Storage limitations on the B compiler demanded a one-pass technique in which
output was generated as soon as possible, and the syntactic
redesign that made this possible was carried forward into C.

<p>
Certain less pleasant aspects of BCPL owed to its own technological
problems and were consciously
avoided in the design of B.
For example, BCPL uses a `global vector' mechanism for communicating
between separately compiled programs.
In this scheme,
the programmer explicitly associates the name of each externally visible
procedure and data object with a numeric offset in the global
vector; the linkage is accomplished in the compiled code
by using these numeric offsets.
B evaded this inconvenience initially by insisting that the entire
program be presented all at once to the compiler.
Later implementations of B, and all those of C, use a conventional
linker to resolve external names occurring in files compiled separately,
instead of placing the burden of assigning offsets on the programmer.
</p>
<p>
Other fiddles in the transition from BCPL to B were introduced as
a matter of taste, and some remain controversial, for example the decision
to use the single character
<tt>=</tt>
for assignment instead of
<tt>:=</tt>.
Similarly, B uses
<tt>/**/</tt>
to enclose comments, where BCPL uses
<tt>//</tt>,
to ignore text up to the end of the line.
The legacy of PL/I is evident here.
(C++ has resurrected the BCPL comment convention.)
Fortran influenced the syntax
of declarations:
B declarations begin with a specifier
like
<tt>auto</tt>
or
<tt>static</tt>,
followed by a list of names, and C
not only followed this style but ornamented it by
placing its type keywords at the start of declarations.
</p>
<p>
Not every difference between the BCPL language documented in
Richards's book
[Richards 79]
and B was deliberate;
we started from an earlier version of BCPL [Richards 67].
For example, the
<tt>endcase</tt>
that escapes from a BCPL
<tt>switchon</tt>
statement was not present in the language when we learned it
in the 1960s,
and so the overloading of the
<tt>break</tt>
keyword
to escape from the B and C
<tt>switch</tt>
statement
owes to divergent
evolution rather than conscious change.
</p>
<p>
In contrast to the pervasive syntax variation that occurred
during the creation of B, the core semantic content of BCPL—its
type structure and expression evaluation rules—remained intact.
Both languages are typeless, or rather have a single data type,
the `word,' or `cell,' a fixed-length bit pattern.  Memory in these languages
consists of a linear array of such cells, and the meaning of
the contents of a cell depends on the operation applied.
The
<tt>+</tt>
operator, for example, simply adds its operands using the machine's
integer add instruction, and the other arithmetic
operations are equally unconscious of the actual meaning
of their operands.  Because memory is a linear array, it is possible
to interpret the value in a cell as an index in this array,
and BCPL supplies an operator for this purpose.  In the
original language it was spelled
<tt>rv</tt>,
and later
<tt>!</tt>,
while B uses the unary
<tt>*</tt>.
Thus, if
<tt>p</tt>
is a cell containing
the index of (or address of, or pointer to) another cell,
<tt>*p</tt>
refers to the contents of the pointed-to cell, either
as a value in an expression or as the target of
an assignment.
</p>
<p>
Because pointers in BCPL and B are merely integer indices
in the memory array, arithmetic on them is meaningful:
if
<tt>p</tt>
is the address of a cell, then
<tt>p+1</tt>
is the address of
the next cell.
This convention is the basis for the semantics
of arrays in both languages.  When in BCPL one writes
</p><dl><dt></dt><dd><tt><pre>let V = vec 10
</pre></tt></dd></dl>
or in B,
<dl><dt></dt><dd><tt><pre>auto V[10];
</pre></tt></dd></dl>
the effect is the same: a cell named
<tt>V</tt>
is allocated, then
another group of 10 contiguous cells is set aside, and the memory index
of the first of these is placed into
<tt>V</tt>.
By a general rule, in B the expression
<dl><dt></dt><dd><tt><pre>*(V+i)
</pre></tt></dd></dl>
adds
<tt>V</tt>
and
<tt>i</tt>,
and refers to the
<tt>i</tt>-th
location after
<tt>V</tt>.
Both
BCPL and B each add special notation to sweeten such array accesses;
in B an equivalent expression is
<dl><dt></dt><dd><tt><pre>V[i]
</pre></tt></dd></dl>
and in BCPL
<dl><dt></dt><dd><tt><pre>V!i
</pre></tt></dd></dl>
This approach to arrays was unusual even at the time;
C would later assimilate it in an
even less conventional way.

<p>
None of BCPL, B, or C supports character data strongly
in the language; each treats strings
much like vectors of integers and supplements general rules by
a few conventions.
In both BCPL and B a string literal denotes the address of a
static area initialized with the characters of the string,
packed into cells.
In BCPL, the first packed byte contains the number of characters
in the string;
in B, there is no count and strings are terminated by
a special character, which B spelled
`<tt>*e</tt>'.
This change was made partially to avoid the limitation
on the length of a string caused by holding the count
in an 8- or 9-bit slot, and partly because maintaining
the count seemed, in our experience, less convenient than using a
terminator.
</p>
<p>
Individual characters in a BCPL string were usually manipulated
by spreading the string out into another array, one character per cell,
and then repacking it later;
B provided corresponding routines, but people more often used
other library functions that accessed or replaced individual
characters in a string.
</p>
<h4>More History
</h4>
<p>
After the TMG version of B was working, Thompson rewrote B in itself
(a bootstrapping step).
During development, he continually struggled against memory limitations:
each language addition
inflated the compiler so it could barely fit, but each
rewrite taking advantage of the feature reduced its size.
For example, B introduced generalized assignment operators, using
<tt>x=+y</tt>
to add
<tt>y</tt>
to
<tt>x</tt>.
The notation came from
Algol 68 [Wijngaarden 75] via McIlroy, who had incorporated
it into his version of TMG.
(In B and early C, the operator was spelled
<tt>=+</tt>
instead of
<tt>+=</tt>
; this mistake, repaired in 1976, was induced by a seductively easy
way of handling the first form in B's lexical analyzer.)
</p>
<p>
Thompson went a step further by inventing the
<tt>++</tt>
and
<tt>--</tt>
operators, which increment or decrement;
their prefix
or postfix position determines whether the alteration
occurs before or after noting the value of the operand.
They were not in the earliest versions of B, but appeared
along the way.
People often guess that
they were created to use the auto-increment and
auto-decrement address modes provided by the DEC PDP-11 on which C and Unix
first became popular.
This is historically impossible, since there was no PDP-11
when B was developed.
The PDP-7, however,
did have a few `auto-increment' memory cells, with the property
that an indirect memory reference through them incremented the cell.
This feature probably suggested such operators to Thompson;
the generalization to make them both prefix and postfix
was his own.
Indeed, the auto-increment cells were not used directly in implementation of the
operators, and a stronger motivation for the innovation was probably
his observation that
the translation of
<tt>++x</tt>
was smaller than that of
<tt>x=x+1</tt>.
</p>
<p>
The B compiler on the PDP-7 did not generate machine instructions,
but instead `threaded code' [Bell 72], an interpretive scheme in which
the compiler's output consists
of a sequence of addresses of code fragments that perform the
elementary operations.
The operations typically—in particular for B—act on a simple stack machine.
</p>
<p>
On the PDP-7 Unix system, only a few things were written in B except B itself,
because the machine was too small and too slow to do more than
experiment; rewriting the operating system and the utilities
wholly into B was too expensive a step to
seem feasible.
At some point Thompson relieved the address-space crunch by offering a
`virtual B' compiler that allowed the interpreted program to occupy more than 8K bytes
by paging the code and data within the interpreter,
but it was too slow to be practical for the common utilities.
Still, some utilities written in B appeared, including an early version of
the variable-precision calculator
<i>dc</i>
familiar to Unix users [McIlroy 79].
The most ambitious enterprise I undertook was a genuine
cross-compiler that translated B to GE-635 machine instructions, not threaded code.
It was a small
<i>tour de force</i>:
a full B compiler, written in its
own language and generating code for a 36-bit mainframe,
that ran on an 18-bit machine with 4K words of user address space.
This project was possible only because of the simplicity
of the B language and its run-time system.
</p>
<p>
Although we entertained occasional thoughts
about implementing one of the major languages of the time like Fortran,
PL/I, or Algol 68, such a project seemed hopelessly large for our resources:
much simpler and smaller tools were called for.
All these languages influenced our work,
but it was more fun to do things on our own.
</p>
<p>
By 1970, the Unix project had shown enough promise that we were
able to acquire the new DEC PDP-11.
The processor was among the first of its line delivered by DEC, and three months
passed before its disk arrived.
Making B programs
run on it using the threaded technique
required only writing the code fragments for the operators,
and a simple assembler which I coded in B;
soon,
<i>dc</i>
became the first
interesting program to be tested, before any operating system, on our PDP-11.
Almost as rapidly, still waiting for the disk, Thompson recoded
the Unix kernel and some basic commands in PDP-11 assembly language.
Of the 24K bytes of memory on the machine, the earliest PDP-11 Unix system
used 12K bytes for the operating system,
a tiny space for user programs, and the remainder as a RAM disk.
This version was only for testing, not for real work;
the machine marked time by enumerating closed knight's
tours on chess boards of various sizes.
Once its disk appeared, we quickly migrated to it after
transliterating assembly-language commands to the PDP-11 dialect, and
porting those already in B.
</p>
<p>
By 1971, our miniature computer center was beginning to have users.
We all wanted to create interesting software more easily.
Using assembler was dreary enough that B, despite its performance
problems, had been supplemented by a small library of useful service routines
and was being used for more and more new programs.
Among the more notable results of this period was Steve Johnson's
first version of the
<i>yacc</i>
parser-generator [Johnson 79a].
</p>
<h4>The Problems of B
</h4>
<p>
The machines on which we first used BCPL and then B were word-addressed,
and these languages' single data type, the `cell,' comfortably
equated with the hardware machine word.
The advent of the PDP-11 exposed several inadequacies of B's semantic model.
First, its character-handling mechanisms, inherited with few changes from BCPL,
were clumsy:
using library procedures to spread packed strings into individual
cells and then repack, or to access and replace
individual characters,
began to feel awkward, even silly, on a byte-oriented machine.
</p>
<p>
Second, although the original PDP-11 did not provide for floating-point
arithmetic,
the manufacturer promised that it would soon be available.
Floating-point operations had been added to BCPL
in our Multics and GCOS compilers by defining
special operators, but the mechanism was possible
only because on the relevant machines, a single word
was large enough to contain a floating-point number;
this was not true on the 16-bit PDP-11.
</p>
<p>
Finally, the B and BCPL model implied overhead in dealing
with pointers: the language rules, by defining a pointer
as an index in an array of words, forced pointers to be represented
as word indices.
Each pointer reference
generated a run-time scale conversion from the pointer to the
byte address expected by the hardware.
</p>
<p>
For all these reasons, it seemed that a typing scheme
was necessary to cope
with characters and byte addressing, and to prepare for the
coming floating-point hardware.
Other issues, particularly type safety and interface checking, did not
seem as important then as they became later.
</p>
<p>
Aside from the problems with the language itself, the B compiler's
threaded-code technique yielded programs
so much slower than their assembly-language counterparts
that we discounted the possibility of recoding the
operating system or its central utilities in B.
</p>
<p>
In 1971 I began to extend the B language by adding a character type
and also rewrote its compiler to generate PDP-11 machine instructions
instead of threaded code.
Thus the transition from B to C
was contemporaneous with the creation of a compiler
capable of producing programs fast and small enough
to compete with assembly language.
I called the slightly-extended language NB, for `new B.'
</p>
<h4>Embryonic C
</h4>
<p>
NB existed so briefly that no full description of
it was written.
It supplied the types
<tt>int</tt>
and
<tt>char</tt>,
arrays of them, and pointers to them, declared in a style typified by
</p><dl><dt></dt><dd><tt><pre>int i, j;
char c, d;
int iarray[10];
int ipointer[];
char carray[10];
char cpointer[];
</pre></tt></dd></dl>
The semantics of arrays remained exactly as in B and BCPL:
the declarations of
<tt>iarray</tt>
and
<tt>carray</tt>
create cells dynamically initialized with a value pointing to the
first of a sequence of 10 integers and characters respectively.
The declarations for
<tt>ipointer</tt>
and
<tt>cpointer</tt>
omit the size, to assert that no storage should be allocated automatically.
Within procedures, the language's interpretation of
the pointers was identical to that of the array variables:
a pointer declaration created a cell differing from
an array declaration only in that the programmer was expected to assign
a referent, instead of letting the compiler allocate the space
and initialize the cell.

<p>
Values stored in the cells bound to
array and pointer names
were the machine addresses,
measured in bytes, of the corresponding storage area.
Therefore, indirection through a pointer implied no
run-time overhead to scale the pointer from word to byte offset.
On the other hand, the machine code for array subscripting and pointer arithmetic
now depended on the type of the array or the pointer:
to compute
<tt>iarray[i]</tt>
or
<tt>ipointer+i</tt>
implied scaling the addend
<tt>i</tt>
by the size of the object referred to.
</p>
<p>
These semantics represented an easy transition from B,
and I experimented with them for some months.
Problems became evident when I tried to extend the type notation, especially
to add structured (record) types.
Structures, it seemed, should map in an intuitive way
onto memory in the machine,
but in a
structure containing an array, there was no good place to stash the
pointer containing the base of the array, nor any
convenient way to arrange that it be initialized.
For example, the directory entries of early Unix systems
might be described in C as
</p><dl><dt></dt><dd><tt><pre>struct {
	int	inumber;
	char	name[14];
};
</pre></tt></dd></dl>
I wanted the structure not merely to characterize an abstract object
but also to describe a collection of bits that might be read from
a directory.
Where could the compiler hide the pointer to
<tt>name</tt>
that the semantics demanded?
Even if structures were thought of more abstractly,
and the space for pointers could be hidden somehow,
how could I handle the technical problem of properly initializing
these pointers when allocating a complicated object, perhaps one that specified
structures containing arrays containing structures to arbitrary depth?

<p>
The solution constituted the crucial jump
in the evolutionary chain between typeless BCPL and typed C.
It eliminated the
materialization of the pointer in storage, and instead caused the
creation of the pointer when the array name is mentioned in an expression.
The rule, which survives in today's C, is that values of array
type are converted, when they appear in expressions, into
pointers to the first of the objects making up the array.
</p>
<p>
This invention enabled most existing B code to continue
to work, despite the underlying shift in the language's semantics.
The few programs that assigned new values to
an array name to adjust its origin—possible in B and BCPL,
meaningless in C—were easily repaired.
More important, the new language retained a coherent and workable (if unusual)
explanation of the semantics of arrays, while opening the way to a
more comprehensive type structure.
</p>
<p>
The second innovation that most clearly
distinguishes C from its predecessors is
this fuller type structure and especially its expression in the syntax of declarations.
NB offered the basic types
<tt>int</tt>
and
<tt>char</tt>,
together with arrays of them, and pointers to them,
but no further ways of composition.
Generalization was required:
given an object of any type, it should
be possible to describe a new object that gathers several into an array,
yields it from a function, or is a pointer to it.
</p>
<p>
For each object of such a composed type, there
was already a way to mention the underlying object:
index the array,
call the function, use the indirection operator on the pointer.
Analogical reasoning led to a declaration syntax for names
mirroring that of the expression syntax in which the names typically appear.
Thus,
</p><dl><dt></dt><dd><tt><pre>int i, *pi, **ppi;
</pre></tt></dd></dl>
declare an integer, a pointer to an integer, a pointer to
a pointer to an integer.
The syntax of these declarations reflects the
observation that
<tt>i</tt>,
<tt>*pi</tt>,
and
<tt>**ppi</tt>
all yield an
<tt>int</tt>
type when used in an expression.  Similarly,
<dl><dt></dt><dd><tt><pre>int f(), *f(), (*f)();
</pre></tt></dd></dl>
declare a function returning an integer, a function returning
a pointer to an integer, a pointer to a function returning
an integer;
<dl><dt></dt><dd><tt><pre>int *api[10], (*pai)[10];
</pre></tt></dd></dl>
declare an array of pointers to integers, and a pointer to
an array of integers.
In all these cases the declaration of a variable resembles
its usage in an expression whose type is the one named at the head of
the declaration.

<p>
The scheme of type composition adopted by C owes considerable debt
to Algol 68, although it did not, perhaps, emerge in a form
that Algol's adherents would approve of.
The central notion I captured from Algol was a type structure
based on atomic
types (including structures), composed into arrays, pointers (references),
and functions (procedures).
Algol 68's concept of unions
and casts also had an influence that appeared later.
</p>
<p>
After creating the type system, the associated
syntax, and the compiler for the new language,
I felt that it deserved a new name;
NB seemed insufficiently distinctive.
I decided to follow the single-letter style and called it C,
leaving open the question whether the name represented
a progression through the alphabet or through the letters in BCPL.
</p>
<h4>Neonatal C
</h4>
<p>
Rapid changes continued after the language had been named,
for example
the introduction of the
<tt>&amp;&amp;</tt>
and
<tt>||</tt>
operators.
In BCPL and B, the evaluation of expressions depends
on context: within
<tt>if</tt>
and other conditional statements that compare
an expression's value with zero,
these languages place a special interpretation on the
<tt>and</tt>
(<tt>&amp;</tt>)
and
<tt>or</tt>
(<tt>|</tt>)
operators.
In ordinary contexts, they operate bitwise, but
in the B statement
</p><dl><dt></dt><dd><tt><pre>if (e1 &amp; e2) ...
</pre></tt></dd></dl>
the compiler must evaluate
<tt>e1</tt>
and if it is non-zero, evaluate
<tt>e2</tt>,
and if it too is non-zero, elaborate the statement dependent on
the
<tt>if</tt>.
The requirement descends recursively on
<tt>&amp;</tt>
and
<tt>|</tt>
operators within
<tt>e1</tt>
and
<tt>e2</tt>.
The short-circuit semantics of the Boolean operators in such
`truth-value' context seemed desirable,
but the overloading of the operators was difficult to explain and use.
At the suggestion of Alan Snyder,
I introduced the
<tt>&amp;&amp;</tt>
and
<tt>||</tt>
operators
to make the mechanism more explicit.

<p>
Their tardy introduction explains an
infelicity of C's precedence rules.  In B one writes
</p><dl><dt></dt><dd><tt><pre>if (a==b &amp; c) ...
</pre></tt></dd></dl>
to check whether
<tt>a</tt>
equals
<tt>b</tt>
and
<tt>c</tt>
is non-zero;
in such a conditional expression it is better that
<tt>&amp;</tt>
have lower precedence than
<tt>==</tt>.
In converting from B to C, one wants to replace
<tt>&amp;</tt>
by
<tt>&amp;&amp;</tt>
in such a statement;
to make the conversion less painful,
we decided to keep the precedence of the
<tt>&amp;</tt>
operator the same relative to
<tt>==</tt>,
and merely split the precedence of
<tt>&amp;&amp;</tt>
slightly from
<tt>&amp;</tt>.
Today, it seems that it would have been preferable to move
the relative precedences of
<tt>&amp;</tt>
and
<tt>==</tt>,
and thereby simplify a common C idiom:
to test a masked value
against another value, one must write
<dl><dt></dt><dd><tt><pre>if ((a&amp;mask) == b) ...
</pre></tt></dd></dl>
where the inner parentheses are required but easily forgotten.

<p>
Many other changes occurred around 1972-3, but the most important
was the introduction of the preprocessor,
partly at the urging of Alan Snyder [Snyder 74],
but also in recognition of the utility of the
the file-inclusion mechanisms available in BCPL and PL/I.
Its original version was exceedingly simple,
and provided only included files and
simple string replacements:
<tt>#include</tt>
and
<tt>#define</tt>
of parameterless macros.
Soon thereafter, it was extended, mostly by Mike Lesk
and then by John Reiser,
to incorporate macros with arguments and conditional
compilation.
The preprocessor was originally considered an optional adjunct
to the language itself.  Indeed, for some years,
it was not even invoked unless the source program contained
a special signal at its beginning.
This attitude persisted, and explains
both the incomplete integration of the syntax of the
preprocessor with the rest of the language
and the imprecision of its description in early reference
manuals.
</p>
<h4>Portability
</h4>
<p>
By early 1973, the essentials of
modern C were complete.
The language and compiler were strong enough to permit us to
rewrite the Unix kernel for the PDP-11 in C during the summer of that year.
(Thompson had made a brief attempt to produce a system coded in an early version of
C—before structures—in 1972, but gave up the effort.)
Also during this period, the compiler was retargeted to other nearby machines,
particularly the Honeywell 635 and IBM 360/370;
because the language could not live in isolation,
the prototypes for the modern libraries
were developed.
In particular, Lesk wrote a `portable I/O package' [Lesk 72]
that was later reworked to become the C `standard I/O' routines.
In 1978 Brian Kernighan and I published
<i>The C Programming Language</i>
[Kernighan 78].
Although it did not describe some additions
that soon became common, this book served as the language
reference until a formal standard was adopted more than
ten years later.
Although we worked closely together on this book, there was a clear division of labor:
Kernighan wrote almost all the expository material, while
I was responsible for the appendix containing the reference manual and
the chapter on interfacing with the Unix system.
</p>
<p>
During 1973-1980,
the language grew a bit:
the type structure gained unsigned, long, union, and enumeration types,
and structures became nearly first-class objects
(lacking only a notation for literals).
Equally important developments appeared in its environment and the accompanying
technology.
Writing the Unix kernel in C had given us enough confidence in the language's
usefulness and efficiency that we began to recode the
system's utilities and tools as well,
and then to move the most interesting among them to the other
platforms.
As described in [Johnson 78a], we discovered that the hardest problems
in propagating Unix tools lay not in the
interaction of the C language with new hardware,
but in adapting to the existing software of other
operating systems.
Thus Steve Johnson began to work on
<i>pcc</i>,
a C compiler intended to be easy to retarget to new machines [Johnson 78b],
while he, Thompson, and I began to move the Unix system itself to
the Interdata 8/32 computer.
</p>
<p>
The language changes during this period, especially around 1977,
were largely focused on considerations of portability and type safety,
in an effort to cope with the problems we foresaw and observed
in moving a considerable body of code to the new Interdata
platform.
C at that time still manifested strong signs of its typeless
origins.
Pointers, for example, were barely distinguished from
integral memory indices in early language manuals or extant code;
the similarity of the arithmetic properties of
character pointers and unsigned integers made it hard
to resist the temptation to identify them.
The
<tt>unsigned</tt>
types were added to make unsigned arithmetic available
without confusing it with pointer manipulation.
Similarly, the early language condoned assignments between
integers and pointers, but this practice began to be discouraged;
a notation for type conversions (called `casts' from the example of Algol 68)
was invented to specify type conversions more explicitly.
Beguiled by the example of PL/I, early C
did not tie structure pointers firmly to the structures
they pointed to, and permitted programmers to write
<tt>pointer-&gt;member</tt>
almost without regard to the type of
<tt>pointer</tt>;
such an expression was taken uncritically as a reference
to a region of memory designated by the pointer, while the member
name specified only an offset and a type.
</p>
<p>
Although the first edition of K&amp;R described most of the
rules that brought C's type structure to its present form,
many programs written in the older, more relaxed style
persisted, and so did compilers that tolerated it.
To encourage people to pay more attention to the
official language rules, to detect legal but suspicious constructions,
and to help find interface mismatches
undetectable with simple mechanisms for separate compilation,
Steve Johnson adapted his
<i>pcc</i>
compiler to produce
<i>lint</i>
[Johnson 79b],
which scanned a set of files and remarked on dubious constructions.
</p>
<h4>Growth in Usage
</h4>
<p>
The success of our portability experiment on the
Interdata 8/32 soon led to another by Tom London and John Reiser
on the DEC VAX 11/780.
This machine became much more popular than the Interdata, and
Unix and the C language began to spread rapidly, both within AT&amp;T and
outside.
Although by the middle 1970s
Unix was in use by
a variety of projects within the Bell System
as well as a small group of research-oriented
industrial, academic, and government organizations outside our company,
its real growth began only after portability had been achieved.
Of particular note were the System III and System V
versions of the system from the emerging Computer Systems division of AT&amp;T, based
on work by the company's development and research groups,
and the BSD series of releases by the University
of California at Berkeley that derived from research
organizations in Bell Laboratories.
</p>
<p>
During the 1980s the use of the C language spread widely,
and compilers became available on nearly every machine architecture
and operating system; in particular it became popular as a
programming tool for personal computers, both for manufacturers
of commercial software for these machines, and for end-users
interested in programming.
At the start of the decade, nearly every compiler was based on Johnson's
<i>pcc</i>;
by 1985 there were many independently-produced compiler products.
</p>
<h4>Standardization
</h4>
<p>
By 1982 it was clear that C needed formal standardization.
The best approximation to a standard,
the first edition of K&amp;R, no longer described the language in actual use;
in particular, it mentioned neither the
<tt>void</tt>
or
<tt>enum</tt>
types.
While it foreshadowed the newer approach to structures, only after
it was published did the language support assigning them, passing them
to and from functions, and associating the names of members firmly
with the structure or union containing them.
Although compilers distributed by AT&amp;T incorporated these changes,
and most of the purveyors of compilers not based on
<i>pcc</i>
quickly picked up them up, there remained no complete, authoritative
description of the language.
</p>
<p>
The first edition of K&amp;R was also insufficiently precise on many details
of the language, and it became increasingly impractical to regard
<i>pcc</i>
as a `reference compiler;'
it did not perfectly
embody even the language described by K&amp;R, let alone subsequent extensions.
Finally, the incipient use of C in projects subject to commercial
and government contract meant that the imprimatur of an official
standard was important.
Thus (at the urging of M. D. McIlroy), ANSI established the X3J11
committee under the direction of CBEMA
in the summer of 1983, with the goal of producing
a C standard.
X3J11 produced its report [ANSI 89] at the end of 1989,
and subsequently this standard was accepted by ISO as
ISO/IEC 9899-1990.
</p>
<p>
From the beginning, the X3J11 committee took a cautious,
conservative view of language extensions.
Much to my
satisfaction, they took seriously their goal:
`to develop a clear, consistent, and unambiguous Standard
for the C programming language which codifies the common,
existing definition of C and which promotes the portability
of user programs across C language environments.' [ANSI 89]
The committee realized that mere promulgation of a standard
does not make the world change.
</p>
<p>
X3J11 introduced only one genuinely important change to the language itself:
it incorporated the types of formal arguments in the type
signature of a function, using syntax borrowed from C++ [Stroustrup 86].
In the old style, external functions were declared like this:
</p><dl><dt></dt><dd><tt><pre>double sin();
</pre></tt></dd></dl>
which says only that
<tt>sin</tt>
is a function returning a
<tt>double</tt>
(that is, double-precision floating-point) value.
In the new style, this better rendered
<dl><dt></dt><dd><tt><pre>double sin(double);
</pre></tt></dd></dl>
to make the argument type explicit
and thus encourage better type checking and appropriate conversion.
Even this addition, though it produced a noticeably better language,
caused difficulties.
The committee justifiably felt that simply outlawing
`old-style' function definitions and declarations was not
feasible, yet also agreed that the new forms were better.
The inevitable compromise was as good as it
could have been, though the language definition is complicated by
permitting both forms, and writers of portable software must contend
with compilers not yet brought up to standard.

<p>
X3J11 also introduced
a host of smaller additions and adjustments, for example,
the type qualifiers
<tt>const</tt>
and
<tt>volatile</tt>,
and slightly different type promotion rules.
Nevertheless, the standardization process did not change the character
of the language.
In particular, the C standard did not attempt to specify formally
the language semantics, and so there can be dispute over fine points;
nevertheless, it successfully accounted for changes in
usage since the original description, and is sufficiently precise to
base implementations on it.
</p>
<p>
Thus the core C language escaped nearly unscathed from the
standardization process, and the Standard emerged more
as a better, careful codification than a new invention.
More important changes took place in the language's surroundings:
the preprocessor and the library.
The preprocessor performs macro substitution, using conventions
distinct from the rest of the language.
Its
interaction with the compiler had never
been well-described, and X3J11 attempted to remedy the
situation.
The result is noticeably better than the explanation in the first edition of K&amp;R;
besides being more comprehensive, it provides
operations, like token concatenation, previously available
only by accidents of implementation.
</p>
<p>
X3J11 correctly believed that a full and careful
description of a standard C library was as important as its
work on the language itself.
The C language itself does not provide for input-output
or any other interaction with the outside world, and thus
depends on a set of standard procedures.
At the time of publication of K&amp;R, C was thought of mainly
as the system programming language of Unix; although we
provided examples of library routines intended to be readily transportable
to other operating systems, underlying support from Unix was implicitly
understood.
Thus, the X3J11 committee spent much of its time designing
and documenting a set
of library routines required to be available in all
conforming implementations.
</p>
<p>
By the rules of the standards process, the current activity of the X3J11
committee is confined to issuing interpretations on the existing
standard.
However, an informal group originally convened by Rex Jaeschke
as NCEG (Numerical C Extensions Group) has been officially accepted
as subgroup X3J11.1,
and they continue to consider extensions to C.
As the name implies, many of these possible extensions are intended to make the language
more suitable for numerical use: for example, multi-dimensional arrays
whose bounds are dynamically determined, incorporation of facilities
for dealing with IEEE arithmetic, and making the language more effective on machines
with vector or other advanced architectural features.
Not all the possible extensions are specifically numerical; they
include a notation for structure literals.
</p>
<h4>Successors
</h4>
<p>
C and even B have several direct descendants, though they
do not rival Pascal in generating progeny.
One side branch developed early.
When Steve Johnson visited the University of Waterloo on sabbatical
in 1972,
he brought B with him.  It became popular
on the Honeywell machines there, and later spawned Eh and Zed
(the Canadian answers to `what follows B?').
When Johnson returned to Bell Labs in 1973, he was disconcerted to
find that the language whose seeds he brought to Canada
had evolved back home;
even his own
<i>yacc</i>
program had been rewritten in C, by Alan Snyder.
</p>
<p>
More recent descendants of C proper include Concurrent C [Gehani 89],
Objective C [Cox 86], C* [Thinking 90],
and especially C++ [Stroustrup 86].
The language is also widely used as an intermediate
representation (essentially, as a portable assembly language)
for a wide variety of compilers, both for direct descendents
like C++, and independent languages like
Modula 3 [Nelson 91] and
Eiffel
[Meyer 88].
</p>
<h4>Critique
</h4>
<p>
Two ideas are most characteristic of C among languages of its class:
the relationship between arrays and pointers,
and the way in which declaration syntax mimics expression syntax.
They are also among its most frequently criticized features,
and often serve as stumbling blocks to the beginner.
In both cases, historical accidents or mistakes have exacerbated
their difficulty.
The most important of these has been the tolerance of C compilers
to errors in type.
As should be clear from the history above, C evolved from typeless
languages.
It did not suddenly appear to its earliest
users and developers as an entirely new language with its own rules;
instead we continually had to adapt existing programs as the
language developed, and make allowance for an existing body
of code.  (Later, the ANSI X3J11 committee standardizing C would
face the same problem.)
</p>
<p>
Compilers in 1977, and even well after,
did not complain about usages such as assigning between integers
and pointers or using objects of the wrong type to refer
to structure members.
Although the language definition presented in the first edition of K&amp;R
was reasonably (though not completely) coherent in its treatment of type rules,
that book admitted that existing compilers didn't enforce them.
Moreover, some rules designed to ease early transitions
contributed to later confusion.
For example, the empty square brackets in the function declaration
</p><dl><dt></dt><dd><tt><pre>int f(a) int a[]; { ... }
</pre></tt></dd></dl>
are a living fossil, a remnant of NB's way of declaring a pointer;
<tt>a</tt>
is, in this special case only, interpreted in C as a pointer.
The notation survived in part for the sake of
compatibility, in part under the rationalization
that it would allow programmers
to communicate to their readers
an intent to pass
<tt>f</tt>
a pointer generated from an array, rather than a reference to a single
integer.
Unfortunately, it serves as much to confuse the learner
as to alert the reader.

<p>
In K&amp;R C, supplying arguments of the proper type to a function call
was the responsibility of the programmer, and the extant compilers
did not check for type agreement.
The failure of the original language to include argument types
in the type signature of a function
was a significant weakness,
indeed the one that required the X3J11 committee's boldest and most painful
innovation to repair.
The early design is explained (if not justified) by my avoidance of technological
problems, especially cross-checking between separately-compiled source files,
and my incomplete assimilation of the implications of moving between
an untyped to a typed language.
The
<i>lint</i>
program,
mentioned above,
tried to alleviate the problem:
among its other functions,
<i>lint</i>
checks the consistency and coherency of a whole program by scanning a set
of source files,
comparing the types of function arguments used in calls with those
in their definitions.
</p>
<p>
An accident of syntax contributed to the perceived complexity of the language.
The indirection operator, spelled
<tt>*</tt>
in C, is syntactically a unary prefix operator, just as in BCPL and B.
This works well in simple expressions, but in more complex cases,
parentheses are required to direct the parsing.
For example, to distinguish indirection through the value
returned by a function from calling a function designated by
a pointer, one writes
<tt>*fp()</tt>
and
<tt>(*pf)()</tt>
respectively.
The style used in expressions carries through to declarations, so the names might be
declared
</p><dl><dt></dt><dd><tt><pre>int *fp();
int (*pf)();
</pre></tt></dd></dl>
In more ornate but still realistic cases, things become worse:
<dl><dt></dt><dd><tt><pre>int *(*pfp)();
</pre></tt></dd></dl>
is a pointer to a function returning a pointer to an integer.
There are two effects occurring.
Most important, C has a relatively rich set of ways of
describing types (compared, say, with Pascal).
Declarations in languages
as expressive as C—Algol 68, for example—describe objects equally hard
to understand, simply because the objects themselves are complex.
A second effect owes to details of the syntax.
Declarations in C must be read
in an `inside-out' style that many find
difficult to grasp [Anderson 80].
Sethi [Sethi 81] observed that many of the nested declarations
and expressions would become simpler
if the indirection operator had been taken as a postfix operator
instead of prefix, but by then it was too late to change.

<p>
In spite of its difficulties,
I believe that the C's approach to declarations remains plausible,
and am comfortable with it; it is a useful unifying principle.
</p>
<p>
The other characteristic feature of C, its treatment of arrays,
is more suspect on practical grounds, though it also has
real virtues.
Although the relationship between pointers and arrays
is unusual, it can be learned.
Moreover, the language shows considerable power to describe important
concepts, for example, vectors whose length varies at run time,
with only a few basic rules and conventions.
In particular, character strings are handled by the same mechanisms
as any other array, plus the convention that a null character
terminates a string.
It is interesting to compare C's approach with that of two
nearly contemporaneous languages, Algol 68 and Pascal [Jensen 74].
Arrays in Algol 68 either have fixed bounds, or are `flexible:'
considerable mechanism is required both in the language
definition, and in compilers, to accommodate flexible arrays
(and not all compilers fully implement them.)
Original Pascal had only fixed-sized arrays and strings,
and this proved confining [Kernighan 81].
Later, this was partially fixed, though the resulting
language is not yet universally available.
</p>
<p>
C treats strings as arrays of characters
conventionally terminated by a marker.
Aside from one special rule about initialization by string literals,
the semantics of strings are fully subsumed by more general
rules governing all arrays, and
as a result the language is simpler to describe and
to translate than one incorporating the string as a unique
data type.
Some costs accrue from its approach:
certain string operations are more expensive than in other designs
because application code or
a library routine must occasionally search for the end of a string,
because few built-in operations are available, and because
the burden of storage management for strings falls more
heavily on the user.
Nevertheless, C's approach to strings works well.
</p>
<p>
On the other hand, C's treatment of arrays in general (not just strings)
has unfortunate implications both for optimization
and for future extensions.
The prevalence of pointers in C programs, whether those declared
explicitly or arising from arrays, means that
optimizers must be cautious, and must use careful dataflow techniques
to achieve good results.
Sophisticated compilers can understand what most pointers
can possibly change, but some important usages remain difficult
to analyze.
For example, functions with pointer arguments derived from
arrays are hard to compile into efficient code on
vector machines, because it is seldom possible to determine
that one argument pointer does not overlap data also
referred to by another argument, or accessible externally.
More fundamentally, the definition of C so specifically describes
the semantics of arrays that
changes or extensions treating arrays as more
primitive objects, and permitting operations on them as wholes,
become hard to fit into the existing language.
Even extensions to permit the declaration and use of multidimensional arrays whose
size is determined dynamically are not entirely straightforward [MacDonald 89]
[Ritchie 90],
although they would make it much easier to write numerical
libraries in C.
Thus, C covers the most important uses of strings and arrays
arising in practice by a uniform and simple mechanism,
but leaves problems for highly efficient implementations and for extensions.
</p>
<p>
Many smaller infelicities exist in the language
and its description
besides those discussed above, of course.
There are also
general criticisms to be lodged that transcend detailed points.
Chief among these is that the language and its generally-expected
environment provide little help for writing very large systems.
The naming structure provides only two main levels,
`external' (visible everywhere) and `internal' (within
a single procedure).
An intermediate level
of visibility (within a single file of data and procedures)
is weakly tied to the language definition.
Thus, there is little direct support for modularization,
and project designers are forced to create their own conventions.
</p>
<p>
Similarly, C itself provides two durations of storage:
`automatic' objects that exist while control resides in or below
a procedure, and `static,' existing throughout execution of a program.
Off-stack, dynamically-allocated storage is provided only
by a library routine and
the burden of managing it is placed on
the programmer: C is hostile to automatic garbage collection.
</p>
<h4>Whence Success?
</h4>
<p>
C has become successful to an extent far surpassing any early
expectations.  What qualities contributed to its widespread use?
</p>
<p>
Doubtless the success of Unix itself was the most important factor;
it made the language available to hundreds of thousands of people.
Conversely, of course, Unix's use of C and its consequent
portability to a wide variety of machines
was important in the system's success.
But the language's invasion of other environments suggests more
fundamental merits.
</p>
<p>
Despite some aspects mysterious to the beginner and
occasionally even to the adept,
C remains a simple and small language, translatable with simple and small compilers.
Its types and operations are
well-grounded in those provided by
real machines, and for
people used to how computers work,
learning the idioms for generating time- and space-efficient programs
is not difficult.
At the same time the language is sufficiently abstracted from machine
details that program portability can be achieved.
</p>
<p>
Equally important, C and its central library support always
remained in touch with a real environment.
It was not designed in isolation to prove a point, or to serve
as an example, but as a tool to write programs that did
useful things; it was always meant to interact with a larger
operating system, and was regarded as a
tool to build larger tools.
A parsimonious, pragmatic approach influenced the things that went into C:
it covers
the essential needs of many programmers,
but does not try to supply too much.
</p>
<p>
Finally, despite the changes that it has undergone since its first
published description, which was admittedly informal
and incomplete, the actual C language as seen by millions of users
using many different compilers has remained remarkably stable
and unified compared to those of similarly widespread currency,
for example Pascal and Fortran.
There are differing dialects of C—most noticeably, those described by
the older K&amp;R and the newer Standard C—but on the whole, C has remained
freer of proprietary extensions than other languages.
Perhaps the most significant extensions are the `far' and `near'
pointer qualifications intended to deal with peculiarities
of some Intel processors.
Although C was not originally designed with portability
as a prime goal, it succeeded in expressing
programs, even including operating systems,
on machines ranging from the smallest personal
computers through the mightiest supercomputers.
</p>
<p>
C is quirky, flawed, and an enormous success.
While accidents of history surely helped,
it evidently satisfied a need for a system implementation language
efficient enough
to displace assembly language, yet sufficiently abstract and fluent to
describe algorithms and interactions in a wide variety of environments.
</p>
<h4>Acknowledgments
</h4>
<p>
It is worth summarizing compactly the roles of the direct contributors to today's
C language.
Ken Thompson created the B language in 1969-70; it was derived directly
from Martin Richards's BCPL.
Dennis Ritchie turned B into C during 1971-73, keeping most of B's syntax
while adding types and many other changes, and writing the
first compiler.
Ritchie, Alan Snyder, Steven C. Johnson, Michael Lesk, and Thompson contributed language
ideas during 1972-1977,
and Johnson's portable compiler remains widely used.
During this period, the collection of library routines grew
considerably, thanks to these people and many others at Bell Laboratories.
In 1978, Brian Kernighan and Ritchie wrote the book that
became the language definition for several years.
Beginning in 1983, the ANSI X3J11 committee standardized
the language.  Especially notable in keeping its
efforts on track were its officers
Jim Brodie, Tom Plum, and  P. J. Plauger, and the successive draft redactors,
Larry Rosler and Dave Prosser.
</p>
<p>
I thank Brian Kernighan, Doug McIlroy, Dave Prosser, Peter
Nelson, Rob Pike, Ken Thompson, and HOPL's referees
for advice in the preparation of this paper.
</p>
<h4>References
</h4>
<dl compact="">
<dt>[ANSI 89]</dt><dd>
American National Standards Institute,
<i>American National Standard for Information Systems—Programming Language C,</i>
X3.159-1989.
</dd><dt>[Anderson 80]</dt><dd>
B. Anderson,
`Type syntax in the language C: an object lesson in syntactic innovation,'
SIGPLAN Notices
<b>15</b>
(3), March, 1980, pp. 21-27.
</dd><dt>[Bell 72]</dt><dd>
J. R. Bell, `Threaded Code,' C. ACM
<b>16</b>
(6), pp. 370-372.
</dd><dt>[Canaday 69]</dt><dd>
R. H. Canaday and D. M. Ritchie,
`Bell Laboratories BCPL,'
AT&amp;T Bell Laboratories internal memorandum, May, 1969.
</dd><dt>[Corbato 62]</dt><dd>
F. J. Corbato, M. Merwin-Dagget, R. C. Daley,
`An Experimental Time-sharing System,' AFIPS Conf. Proc. SJCC,
1962, pp. 335-344.
</dd><dt>[Cox 86]</dt><dd>
B. J. Cox and A. J. Novobilski,
<i>Object-Oriented Programming: An Evolutionary Approach,</i>
Addison-Wesley: Reading, Mass., 1986. Second edition, 1991.
</dd><dt>[Gehani 89]</dt><dd>
N. H. Gehani and W. D. Roome,
<i>Concurrent C,</i>
Silicon Press: Summit, NJ, 1989.
</dd><dt>[Jensen 74]</dt><dd>
K. Jensen and N. Wirth,
<i>Pascal User Manual and Report,</i>
Springer-Verlag: New York, Heidelberg, Berlin.  Second Edition, 1974.
</dd><dt>[Johnson 73]</dt><dd>
S. C. Johnson and B. W. Kernighan, `The Programming Language B,'
Comp. Sci. Tech. Report #8, AT&amp;T Bell Laboratories (January 1973).
</dd><dt>[Johnson 78a]</dt><dd>
S. C. Johnson and D. M. Ritchie,
`Portability of C Programs and the UNIX System,'
Bell Sys. Tech. J.
<b>57</b>
(6) (part 2), July-Aug, 1978.
</dd><dt>[Johnson 78b]</dt><dd>
S. C. Johnson,
`A Portable Compiler: Theory and Practice,'
Proc. 5th ACM POPL Symposium (January 1978).
</dd><dt>[Johnson 79a]</dt><dd>
S. C. Johnson, `Yet another compiler-compiler,' in
<i>Unix Programmer's Manual,</i>
Seventh Edition, Vol. 2A, M. D. McIlroy and B. W. Kernighan, eds.
AT&amp;T Bell Laboratories: Murray Hill, NJ, 1979.
</dd><dt>[Johnson 79b]</dt><dd>
S. C. Johnson, `Lint, a Program Checker,' in
<i>Unix Programmer's Manual,</i>
Seventh Edition, Vol. 2B, M. D. McIlroy and B. W. Kernighan, eds.
AT&amp;T Bell Laboratories: Murray Hill, NJ, 1979.
</dd><dt>[Kernighan 78]</dt><dd>
B. W. Kernighan and D. M. Ritchie,
<i>The C Programming Language,</i>
Prentice-Hall: Englewood Cliffs, NJ, 1978.
Second edition, 1988.
</dd><dt>[Kernighan 81]</dt><dd>
B. W. Kernighan,
`Why Pascal is not my favorite programming language,'
Comp. Sci. Tech. Rep. #100, AT&amp;T Bell Laboratories, 1981.
</dd><dt>[Lesk 73]</dt><dd>
M. E. Lesk, `A Portable I/O Package,'
AT&amp;T Bell Laboratories internal memorandum ca. 1973.
</dd><dt>[MacDonald 89]</dt><dd>
T. MacDonald,
`Arrays of variable length,'
J. C Lang. Trans
<b>1</b>
(3), Dec. 1989, pp. 215-233.
</dd><dt>[McClure 65]</dt><dd>
R. M. McClure, `TMG—A Syntax Directed Compiler,'
Proc. 20th ACM National Conf. (1965), pp. 262-274.
</dd><dt>[McIlroy 60]</dt><dd>
M. D. McIlroy, `Macro Instruction Extensions of Compiler Languages,'
C. ACM
<b>3</b>
(4), pp. 214-220.
</dd><dt>[McIlroy 79]</dt><dd>
M. D. McIlroy and B. W. Kernighan, eds,
<i>Unix Programmer's Manual,</i>
Seventh Edition, Vol. I,
AT&amp;T Bell Laboratories: Murray Hill, NJ, 1979.
</dd><dt>[Meyer 88]</dt><dd>
B. Meyer,
<i>Object-oriented Software Construction,</i>
Prentice-Hall: Englewood Cliffs, NJ, 1988.
</dd><dt>[Nelson 91]</dt><dd>
G. Nelson,
<i> Systems Programming with Modula-3,</i>
Prentice-Hall: Englewood Cliffs, NJ, 1991.
</dd><dt>[Organick 75]</dt><dd>
E. I. Organick,
<i>The Multics System: An Examination of its Structure,</i>
MIT Press: Cambridge, Mass., 1975.
</dd><dt>[Richards 67]</dt><dd>
M. Richards, `The BCPL Reference Manual,'
MIT Project MAC Memorandum M-352, July 1967.
</dd><dt>[Richards 79]</dt><dd>
M. Richards and C. Whitbey-Strevens,
<i>BCPL: The Language and its Compiler,</i>
Cambridge Univ. Press: Cambridge, 1979.
</dd><dt>[Ritchie 78]</dt><dd>
D. M. Ritchie, `UNIX: A Retrospective,' Bell Sys. Tech. J.
<b>57</b>
(6) (part 2), July-Aug, 1978.
</dd><dt>[Ritchie 84]</dt><dd>
D. M. Ritchie, `The Evolution of the UNIX Time-sharing System,'
AT&amp;T Bell Labs. Tech. J.
<b>63</b>
(8) (part 2), Oct. 1984.
</dd><dt>[Ritchie 90]</dt><dd>
D. M. Ritchie,
`Variable-size arrays in C,'
J. C Lang. Trans.
<b>2</b>
(2), Sept. 1990, pp. 81-86.
</dd><dt>[Sethi 81]</dt><dd>
R. Sethi,
`Uniform syntax for type expressions and declarators,'
Softw. Prac. and Exp.
<b>11</b>
(6), June 1981, pp. 623-628.
</dd><dt>[Snyder 74]</dt><dd>
A. Snyder,
<i>A Portable Compiler for the Language C,</i>
MIT: Cambridge, Mass., 1974.
</dd><dt>[Stoy 72]</dt><dd>
J. E. Stoy and C. Strachey, `OS6—An experimental operating
system for a small computer. Part I: General principles and structure,'
Comp J.
<b>15</b>,
(Aug. 1972), pp. 117-124.
</dd><dt>[Stroustrup 86]</dt><dd>
B. Stroustrup,
<i>The C++ Programming Language,</i>
Addison-Wesley: Reading, Mass., 1986.
Second edition, 1991.
</dd><dt>[Thacker 79]</dt><dd>
C. P. Thacker, E. M. McCreight, B. W. Lampson, R. F. Sproull,
D. R. Boggs, `Alto: A Personal Computer,' in
<i>Computer Structures: Principles and Examples,</i>
D. Sieworek, C. G. Bell, A. Newell,
McGraw-Hill: New York, 1982.
</dd><dt>[Thinking 90]</dt><dd>
<i>C* Programming Guide,</i>
Thinking Machines Corp.: Cambridge Mass., 1990.
</dd><dt>[Thompson 69]</dt><dd>
K. Thompson, `Bon—an Interactive Language,' undated AT&amp;T Bell Laboratories
internal memorandum (ca. 1969).
</dd><dt>[Wijngaarden 75]</dt><dd>
A. van Wijngaarden, B. J. Mailloux, J. E. Peck, C. H. Koster, M. Sintzoff,
C. Lindsey, L. G. Meertens, R. G. Fisker, `Revised report on the algorithmic
language Algol 68,'  Acta Informatica
<b>5</b>,
pp. 1-236.
</dd></dl>
<p> 
<a href="http://www.lucent.com/copyright.html">
Copyright</a> © 2003 Lucent Technologies Inc.  All rights reserved.

</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How small is the smallest .NET Hello World binary? (157 pts)]]></title>
            <link>https://blog.washi.dev/posts/tinysharp/</link>
            <guid>36652824</guid>
            <pubDate>Sun, 09 Jul 2023 08:50:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.washi.dev/posts/tinysharp/">https://blog.washi.dev/posts/tinysharp/</a>, See on <a href="https://news.ycombinator.com/item?id=36652824">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p>Here is a dumb question that you probably never asked yourself: What is the minimal amount of bytes we need to store in a .NET executable to have the CLR print the string<code>"Hello, World!"</code> to the standard output?</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/thumbnail.png"><img data-src="/assets/img/posts/tinysharp/thumbnail.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/thumbnail.png"></a>
<em>How small can we get?</em></p>

<p>In this post, we will explore the limits of the .NET module file format, get it as small as possible, while still having it function like a normal executable on a typical Windows machine with the .NET Framework installed.</p>

<p>The final source code for this post can be found on my GitHub:</p>

<p><a href="https://gist.github.com/Washi1337/367eede6e00b31e29355626d5e2f3078" target="_blank">
 Full Source Code
</a></p>

<h2 id="rules"><span>Rules</span><a href="#rules"><i></i></a></h2>

<p>Here are the arbitrary rules I set up for myself:</p>

<ul>
  <li>
    <p><strong>The application must run a managed entry point implemented in C# or CIL.</strong>
This entry point must be responsible for printing <code>"Hello, World!"</code> to the standard output. 
This means we cannot do any of the native entry point shenanigans like we did in a <a href="https://blog.washi.dev/posts/entry-points">previous post</a>.
How it actually does the printing, however, is fully up to this method body.</p>
  </li>
  <li>
    <p><strong>The application runs on .NET Framework 4.x.x.</strong>
We do this to give ourselves a little bit more freedom, and it allows us to have a single executable only and leverage some of the features of the Windows PE loader. 
It is also nice to have an executable that we can just double click.</p>
  </li>
  <li>
    <p><strong>No third-party dependencies.</strong> 
We are only allowed to reference the BCL (i.e., mscorlib) and/or other libraries that are installed on a typical Windows machine. 
Otherwise, we could replace all code within our small application with a call to a custom-made dependency, which would be cheating!</p>
  </li>
  <li>
    <p><strong>Ignore zero bytes at the end of the file.</strong>
The PE file format, as well as the CLR itself, puts a hard limit on offset alignments for each section stored in the PE. 
Effectively it means that the theoretically smallest .NET PE that is able to run on Windows 10 or higher cannot be smaller than 1KB. 
As we will see this is rather easy to achieve.
To challenge ourselves a bit more, we strive to get to the “bare minimum description” of a .NET hello world PE file, where we consider all trailing zero bytes as non-existent.</p>
  </li>
</ul>

<p>Let’s get hacking!</p>

<h2 id="establishing-a-baseline"><span>Establishing a baseline</span><a href="#establishing-a-baseline"><i></i></a></h2>

<p>To establish a baseline that we want to beat, let’s first start by compiling the following Hello World application using the latest version of the C# compiler by the time of writing this post.</p>

<div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
</pre></td><td><pre><span>using</span> <span>System</span><span>;</span>

<span>namespace</span> <span>ConsoleApp1</span><span>;</span>

<span>internal</span> <span>static</span> <span>class</span> <span>Program</span>
<span>{</span>
    <span>public</span> <span>static</span> <span>void</span> <span>Main</span><span>(</span><span>string</span><span>[]</span> <span>args</span><span>)</span>
    <span>{</span>
        <span>Console</span><span>.</span><span>WriteLine</span><span>(</span><span>"Hello, World!"</span><span>);</span>
    <span>}</span>
<span>}</span>
</pre></td></tr></tbody></table></code></p></div>

<p>We accompany it with the following <code>.csproj</code> file:</p>

<div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
</pre></td><td><pre><span>&lt;Project</span> <span>Sdk=</span><span>"Microsoft.NET.Sdk"</span><span>&gt;</span>

    <span>&lt;PropertyGroup&gt;</span>
        <span>&lt;OutputType&gt;</span>Exe<span>&lt;/OutputType&gt;</span>
        <span>&lt;TargetFramework&gt;</span>net472<span>&lt;/TargetFramework&gt;</span>
        <span>&lt;LangVersion&gt;</span>10<span>&lt;/LangVersion&gt;</span>
        <span>&lt;Nullable&gt;</span>enable<span>&lt;/Nullable&gt;</span>
    <span>&lt;/PropertyGroup&gt;</span>

<span>&lt;/Project&gt;</span>
</pre></td></tr></tbody></table></code></p></div>

<p>This gives us a binary of a whopping <code>4.6KB</code> file:</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/size01.png"><img data-src="/assets/img/posts/tinysharp/size01.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/size01.png"></a>
<em>The size of a standard hello world application.</em></p>

<p>That seems excessive… Clearly we can do better than this.</p>

<h2 id="removing-nullable-reference-annotations"><span>Removing nullable reference annotations</span><a href="#removing-nullable-reference-annotations"><i></i></a></h2>

<p>Inspecting the application in a .NET decompiler gives us a bit more insight on what is going on.
Since C# 8.0 we have known the concept of <a href="https://learn.microsoft.com/en-us/dotnet/csharp/language-reference/builtin-types/nullable-reference-types">nullable reference types</a>.
These are special annotations that allows the C# compiler to reason about potentially unwanted null references to be passed on to functions, variables and parameters.
The downside is that these annotations are implemented in the form of custom attributes, which are linked into the executable statically and notoriously large:</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/dnSpy01.png"><img data-src="/assets/img/posts/tinysharp/dnSpy01.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/dnSpy01.png"></a>
<em>Nullable Reference Types add many Custom Attributes to a .NET image</em></p>

<p>Let’s disable that with one option in our <code>.csproj</code> file:</p>

<div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td><pre><span>&lt;Project</span> <span>Sdk=</span><span>"Microsoft.NET.Sdk"</span><span>&gt;</span>

    <span>&lt;PropertyGroup&gt;</span>
        <span>&lt;OutputType&gt;</span>Exe<span>&lt;/OutputType&gt;</span>
        <span>&lt;TargetFramework&gt;</span>net472<span>&lt;/TargetFramework&gt;</span>
        <span>&lt;LangVersion&gt;</span>10<span>&lt;/LangVersion&gt;</span>
        
         <span>&lt;!-- Disable nullable reference type checks. --&gt;</span>
        <span>&lt;Nullable&gt;</span>disable<span>&lt;/Nullable&gt;</span>
    <span>&lt;/PropertyGroup&gt;</span>

<span>&lt;/Project&gt;</span>
</pre></td></tr></tbody></table></code></p></div>

<p>While this does get rid of all the attributes, we are unfortunately still left with a binary that is <code>4.6KB</code> in size, due to the PE file alignments.</p>

<h2 id="manually-crafting-a-net-module"><span>Manually crafting a .NET module</span><a href="#manually-crafting-a-net-module"><i></i></a></h2>

<p>Further inspecting the output in a decompiler shows that, even with nullable references disabled, the C# compiler still emits many type references to custom attributes in our application. 
In particular, they include many attributes assigned to the assembly itself, such as file version metadata and copyright information.
Additionally, besides our class <code>Program</code> we also have a hidden <code>&lt;Module&gt;</code> type that looks rather empty:</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/dnSpy02.png"><img data-src="/assets/img/posts/tinysharp/dnSpy02.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/dnSpy02.png"></a>
<em>The C# compiler still emits a lot of unnecessary metadata</em></p>

<p>We could try and figure out how to instruct the compiler to disable generating all this metadata, but I figured, if we are going to the extreme, we may as well just build a .NET executable file from scratch by ourselves. 
This way we have more control over the final output, allowing us to just emit the bare minimum that is required to print <code>"Hello World"</code>, and not emit those unnecessary file metadata attributes.
Furthermore, we can just place our <code>main</code> function into the <code>&lt;Module&gt;</code> type and get rid of our <code>Program</code> class as well.
Below is an example implementation of building a small Hello World application using <a href="https://blog.washi.dev/posts/tinysharp/github.com/washi1337/asmresolver">AsmResolver</a>:</p>

<div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
</pre></td><td><pre><span>// Define new assembly and module.</span>
<span>var</span> <span>assembly</span> <span>=</span> <span>new</span> <span>AssemblyDefinition</span><span>(</span><span>"assembly"</span><span>,</span> <span>new</span> <span>Version</span><span>(</span><span>1</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>));</span>
<span>var</span> <span>module</span> <span>=</span> <span>new</span> <span>ModuleDefinition</span><span>(</span><span>"module.exe"</span><span>);</span>
<span>assembly</span><span>.</span><span>Modules</span><span>.</span><span>Add</span><span>(</span><span>module</span><span>);</span>

<span>// Obtain &lt;Module&gt; type.</span>
<span>var</span> <span>moduleType</span> <span>=</span> <span>module</span><span>.</span><span>GetOrCreateModuleType</span><span>();</span>

<span>// Craft a new Main method.</span>
<span>var</span> <span>factory</span> <span>=</span> <span>module</span><span>.</span><span>CorLibTypeFactory</span><span>;</span>
<span>var</span> <span>main</span> <span>=</span> <span>new</span> <span>MethodDefinition</span><span>(</span><span>"main"</span><span>,</span> <span>MethodAttributes</span><span>.</span><span>Static</span><span>,</span> <span>MethodSignature</span><span>.</span><span>CreateStatic</span><span>(</span><span>factory</span><span>.</span><span>Void</span><span>));</span>
<span>main</span><span>.</span><span>CilMethodBody</span> <span>=</span> <span>new</span> <span>CilMethodBody</span><span>(</span><span>main</span><span>)</span>
<span>{</span>
    <span>Instructions</span> <span>=</span>
    <span>{</span>
        <span>{</span><span>Ldstr</span><span>,</span> <span>"Hello, World!"</span><span>},</span>
        <span>{</span><span>Call</span><span>,</span> <span>factory</span><span>.</span><span>CorLibScope</span>
            <span>.</span><span>CreateTypeReference</span><span>(</span><span>"System"</span><span>,</span><span>"Console"</span><span>)</span>
            <span>.</span><span>CreateMemberReference</span><span>(</span><span>"WriteLine"</span><span>,</span> <span>MethodSignature</span><span>.</span><span>CreateStatic</span><span>(</span><span>factory</span><span>.</span><span>Void</span><span>,</span> <span>factory</span><span>.</span><span>String</span><span>))</span>
            <span>.</span><span>ImportWith</span><span>(</span><span>module</span><span>.</span><span>DefaultImporter</span><span>)</span>
        <span>},</span>
        <span>Ret</span>
    <span>}</span>
<span>};</span>

<span>// Add main to &lt;Module&gt;</span>
<span>moduleType</span><span>.</span><span>Methods</span><span>.</span><span>Add</span><span>(</span><span>main</span><span>);</span>

<span>// Register main as the entry point of the module:</span>
<span>module</span><span>.</span><span>ManagedEntryPointMethod</span> <span>=</span> <span>main</span><span>;</span>

<span>// Write to disk.</span>
<span>module</span><span>.</span><span>Write</span><span>(</span><span>"output.exe"</span><span>);</span>
</pre></td></tr></tbody></table></code></p></div>

<p>This did a great deal already, we cut our file size in half:</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/size02.png"><img data-src="/assets/img/posts/tinysharp/size02.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/size02.png"></a>
<em>The size of a hello world application emitted by AsmResolver.</em></p>

<p>But we can do better…</p>

<h2 id="getting-rid-of-imports-and-base-relocations"><span>Getting rid of Imports and Base Relocations</span><a href="#getting-rid-of-imports-and-base-relocations"><i></i></a></h2>

<p>If we look into the resulting executable file using a tool like <a href="https://ntcore.com/?page_id=388">CFF Explorer</a>, we can see that the file contains two sections <code>.text</code> and <code>.reloc</code>.
Furthermore, it also contains two very large data directories called the Imports and Base Relocations directory respectively.</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/cff01.png"><img data-src="/assets/img/posts/tinysharp/cff01.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/cff01.png"></a>
<em>By default, 32-bit .NET images contain imports and base relocations that take a lot of space.</em></p>

<p>This is pretty typical for any AnyCPU or 32-bit .NET executable file.
The imports directory is required because 32-bit .NET executable files require an unmanaged entry point calling <code>mscoree!_CorExeMain</code>, as we have seen in a <a href="https://blog.washi.dev/posts/entry-points">previous post</a>.
Furthermore, by default .NET executables are relocatable, that is, the Windows PE Loader is free to map the executable at any memory address it likes.
This means every 32-bit .NET executable also needs a base relocation for the call to this imported function to be registered in the relocation directory.
This is problematic, because it is by default put in a fully separate section.
As every section needs to be aligned to the smallest possible section alignment of <code>0x200</code> bytes (1KB), we inflate our file by at least that amount of bytes just because of that.</p>

<p>Fortunately for us, 64-bit .NET executables do not need such an unmanaged entry point anymore.
With just two extra lines added to our previous script, we can get rid of both directories, an entire PE section, and thus shave off an entire kilobyte worth of data from our binary file:</p>

<div><p><code><table><tbody><tr><td><pre>1
2
3
</pre></td><td><pre><span>// Output a 64-bit module.</span>
<span>module</span><span>.</span><span>PEKind</span> <span>=</span> <span>OptionalHeaderMagic</span><span>.</span><span>PE64</span><span>;</span>
<span>module</span><span>.</span><span>MachineType</span> <span>=</span> <span>MachineType</span><span>.</span><span>Amd64</span><span>;</span>
</pre></td></tr></tbody></table></code></p></div>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/cff02.png"><img data-src="/assets/img/posts/tinysharp/cff02.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/cff02.png"></a>
<em>64-bit .NET images do not need imports or base relocations.</em></p>

<p>And indeed, we get to the theoretically possible minimum size of 1KB of a valid .NET PE file:</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/size03.png"><img data-src="/assets/img/posts/tinysharp/size03.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/size03.png"></a>
<em>The minimum size of a PE file is reached.</em></p>



<p>We could have called it quits here, but I decided to look a little bit deeper into what really we can strip out of a binary to get to the absolute bare minimum of a .NET hello world executable.
From now on, we won’t be looking at the file size as reported by Windows Explorer.
Instead, we will be looking at a hex editor and see where the last non-zero byte is stored and consider that to be our final file size.
If we do this for our current file, we can actually see we are already down to a size of 991 bytes (<code>0x3DF</code>):</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/size04.png"><img data-src="/assets/img/posts/tinysharp/size04.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/size04.png"></a>
<em>The size we will be considering is the index of of the byte after the last non-zero byte in the file.</em></p>

<p>What is still contributing to this amount of bytes?
If we look again in a disassembler, we can see that the <code>#Strings</code> heap in a .NET binary is the second largest metadata stream stored in the file.
It contains all the names that the tables stream (<code>#~</code>) uses, which stores all the types and methods that our application defines and uses.
As it so turns out, many of these names are actually not really important to the runtime:</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/dnSpy03.png"><img data-src="/assets/img/posts/tinysharp/dnSpy03.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/dnSpy03.png"></a>
<em>Names take up a lot of space.</em></p>

<p>Thus, setting these to <code>null</code> instead will give us an application that looks a bit like the following:</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/dnSpy04.png"><img data-src="/assets/img/posts/tinysharp/dnSpy04.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/dnSpy04.png"></a>
<em>Truncating names.</em></p>

<p>Believe it or not, the application still runs fine and happily outputs “Hello World”, regardless of whether this looks fine or not.
Best of all, it shaved a whopping 32 bytes from our file:</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/size05.png"><img data-src="/assets/img/posts/tinysharp/size05.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/size05.png"></a>
<em>The size of the file after stripping names.</em></p>



<p>What other unnecessary metadata is there that the CLR does not really care about?</p>

<p>Our next target is getting rid of the <code>#GUID</code> stream.
This stream is present in virtually any .NET executable, and contains, as its names implies, a list of GUIDs.
However, the only type of metadata that really references it, is the Module table.
This table has a column called <code>Mvid</code>, which is supposed to reference a GUID that makes the module uniquely identifiable across different versions of compiled binaries.</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/cff04.png"><img data-src="/assets/img/posts/tinysharp/cff04.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/cff04.png"></a>
<em>A module contains an optional MVID, which is a GUID of 16 bytes.</em></p>

<p>We do not care about versioning, we just want the smallest binary possible. 
We can just get rid of it and save 16 bytes that were originally making up the Mvid.
However, by doing so, the <code>#GUID</code> stream is now empty and thus is no longer needed. 
By removing the stream in its entirety, we save another 16 bytes that make up its header, making a total of 32 bytes that we save with this.</p>

<p>Additionally, the <code>Console::WriteLine</code> method that we call in our <code>Main</code> function is defined in <code>mscorlib</code>. 
Typically, references to BCL assemblies are annotated with a public key token of 8 bytes.</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/cff03.png"><img data-src="/assets/img/posts/tinysharp/cff03.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/cff03.png"></a>
<em>The reference to <code>mscorlib</code> contains a long public key token.</em></p>

<p>It so turns out that if there is no public key token present in this reference, the CLR then just does not verify this assembly token for authenticity.
Since we do not care about security anyways in our experiment, we can get rid of this too.</p>

<p>This brings us down to a file of 918 bytes in total:</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/size06.png"><img data-src="/assets/img/posts/tinysharp/size06.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/size06.png"></a>
<em>The size after stripping GUIDs and public key tokens.</em></p>

<h2 id="getting-rid-of-consolewriteline"><span>Getting rid of Console.WriteLine</span><a href="#getting-rid-of-consolewriteline"><i></i></a></h2>

<p>If we look at other metadata streams defined in our assembly, we find that our <code>"Hello, World!"</code> string is actually stored in a rather inefficient manner.
In .NET, all user strings are put in the <code>#US</code> metadata stream as a length-prefixed array of 16-bit wide characters followed by an additional zero byte.
This is done to support a wide range of the UNICODE character set.
However, all the characters in the string that we want to print have a code-point value smaller than 255 (<code>0xFF</code>), the max value of a single byte.
Why should we then use 2 bytes per character?
Furthermore, this is the only user string that we need in our binary.
Having an entire 12-bytes stream header for just one string seems rather excessive:</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/cff05.png"><img data-src="/assets/img/posts/tinysharp/cff05.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/cff05.png"></a>
<em>User strings in .NET always use wide character encoding.</em></p>

<p>Unfortunately, there is no way turn this wide-character string in the <code>#US</code> stream to a single-byte ASCII string, and to tell the CLR to interpret it as such.</p>

<p>Time to get creative!</p>

<p>If we want to print an ASCII string as opposed to a wide-character string, we need a function that accepts those types of strings.
<code>Console::WriteLine</code> is not a function that fits this criterium, so we need to get rid of it.
However, the unmanaged function <code>ucrtbase!puts</code> does.
.NET allows for invoking unmanaged functions by using a feature called <a href="https://learn.microsoft.com/en-us/dotnet/standard/native-interop/pinvoke">Platform Invoke (P/Invoke)</a>.
We can define <code>puts</code> using P/Invoke in the following manner in C#:</p>

<div><p><code><table><tbody><tr><td><pre>1
2
</pre></td><td><pre><span>[</span><span>DllImport</span><span>(</span><span>"ucrtbase"</span><span>)]</span>
<span>static</span> <span>extern</span> <span>int</span> <span>puts</span><span>(</span><span>nint</span> <span>str</span><span>);</span>
</pre></td></tr></tbody></table></code></p></div>

<p>However, there is a problem.
The <code>puts</code> function accepts a pointer to a string. 
This pointer must be a valid <strong>runtime address</strong> that points to the start of the zero-terminated ASCII string that we want to print.
How do we know where our string is stored at compile-time so that we can push it in our <code>main</code> method?</p>

<p>It so turns out we can solve this by unchecking the <code>DynamicBase</code> flag in the <code>DllCharacteristics</code> field of the PE’s optional header.
This allows us to fix the base address the module will be mapped on at runtime.
We can then decide an arbitrary base address, put the ASCII string anywhere in our <code>.text</code> section, and calculate the runtime address by the formula <code>module_base_address + rva_ascii_string</code>.</p>

<div><p><code><table><tbody><tr><td><pre>1
2
3
4
</pre></td><td><pre><span>var</span> <span>image</span> <span>=</span> <span>module</span><span>.</span><span>ToPEImage</span><span>();</span>

<span>image</span><span>.</span><span>ImageBase</span> <span>=</span> <span>0x00000000004e0000</span><span>;</span>
<span>image</span><span>.</span><span>DllCharacteristics</span> <span>&amp;=</span> <span>~</span><span>DllCharacteristics</span><span>.</span><span>DynamicBase</span><span>;</span>
</pre></td></tr></tbody></table></code></p></div>

<p>In order to have the CLR actually respect this flag, we also need to unset the <code>ILOnly</code> flag in the .NET data directory:</p>

<div><p><code><table><tbody><tr><td><pre>1
</pre></td><td><pre><span>image</span><span>.</span><span>DotNetDirectory</span><span>!.</span><span>Flags</span> <span>&amp;=</span> <span>~</span><span>DotNetDirectoryFlags</span><span>.</span><span>ILOnly</span><span>;</span>
</pre></td></tr></tbody></table></code></p></div>

<p>We can then simply pass the calculated address directly in our <code>puts</code> function call as a normal integer:</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/dnSpy05.png"><img data-src="/assets/img/posts/tinysharp/dnSpy05.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/dnSpy05.png"></a>
<em>Replace <code>Console::WriteLine</code> with <code>ucrtbase!puts</code>, allowing us to use an ASCII string instead.</em></p>

<p>And there we go, we not only got rid of our wide-character string, but also the entire <code>#US</code> stream, as well as the reference to <code>System.Console::WriteLine</code> which also contributes quite a few bytes to the size of our file.
In turn, we got a few bytes back due to the new required <code>puts</code> method definition and its associated P/Invoke metadata, but it is for sure a big shrink.</p>

<p>We are now down to 889 bytes (<code>0x379</code>):</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/size07.png"><img data-src="/assets/img/posts/tinysharp/size07.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/size07.png"></a>
<em>The size of the file after removing <code>Console::WriteLine</code> and using ASCII strings.</em></p>

<h2 id="other-micro-optimizations"><span>Other micro optimizations</span><a href="#other-micro-optimizations"><i></i></a></h2>

<p>There are a few things we still can do.</p>

<p>Our <code>puts</code> definition follows the canonical definition as provided by the C runtime library. 
This means the function is defined to return an <code>int32</code> representing the number of characters that were written to the standard output.
However, we do not care about this value.
Indeed, in our main method, we pop this value right after the call to keep the CLR happy:</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/dnSpy06.png"><img data-src="/assets/img/posts/tinysharp/dnSpy06.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/dnSpy06.png"></a>
<em>Returning an <code>int32</code> means the value needs to be popped from the evaluation stack again.</em></p>

<p>Since this is a 64-bit PE file anyways, the <code>puts</code> function will use the <a href="https://learn.microsoft.com/en-us/cpp/build/x64-calling-convention?view=msvc-170">x64 calling conventions</a> as described by Microsoft.
In simple terms, this means at runtime the return value is not really pushed on the stack as with normal .NET method calls, but rather put in the <code>RAX</code> register.
Since we do not use this value anyways, we can just turn the definition into <code>void</code>, effectively disregarding whatever is put into this register.
As the function is now no longer returning anything, nothing is also pushed onto the evaluation stack in our main method.
This allows us to get rid of the <code>pop</code> instruction in our main method:</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/dnSpy07.png"><img data-src="/assets/img/posts/tinysharp/dnSpy07.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/dnSpy07.png"></a>
<em>Changing to a <code>void</code> means the <code>pop</code> instruction is no longer required.</em></p>

<p>We can also move the ASCII string that we pass on to the <code>puts</code> function to a slightly better place.
The PE file format contains a lot of segments that are aligned to a certain byte-boundary.
In particular, as was mentioned before, sections are aligned to the nearest multiple of <code>0x200</code> (1KB).
This also includes the first section.
However, since the PE file headers of our file take up less space than <code>0x200</code> bytes, we end up with a chunk of padding data between our headers and first section data:</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/hxd01.png"><img data-src="/assets/img/posts/tinysharp/hxd01.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/hxd01.png"></a>
<em>PE images contain some padding between the headers and the first section.</em></p>

<p>It so turns out the Windows PE Loader always maps the PE headers as a chunk of readable memory. 
The good news is, it also includes this padding data.</p>

<p>Let’s move our string there!</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/hxd02.png"><img data-src="/assets/img/posts/tinysharp/hxd02.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/hxd02.png"></a>
<em>Place the string to print into the unused padding segment.</em></p>

<p>By moving our string there, we effectively truncated our file by 13 bytes.</p>

<p>Since we also do not reference <code>Console::WriteLine</code> anymore, we also do not longer need the reference to <code>mscorlib</code> to be stored in our binary.
This also saves quite a bit of space, since it means one less table to store in the tables stream (<code>#~</code>), as well as the name <code>mscorlib</code> to be removed from the <code>#Strings</code> stream.</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/cff07.png"><img data-src="/assets/img/posts/tinysharp/cff07.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/cff07.png"></a>
<em>We no longer depend on <code>"mscorlib"</code>, thus we do no longer need a reference to it.</em></p>

<p>Finally, we can end with a bit of a weird one.
The .NET metadata directory contains a field called <code>VersionString</code>, containing the minimum required version of the .NET Framework that is required to run this .NET executable.
By default, for .NET 4.0+ binaries, this contains the string <code>"v4.0.30319"</code> padded with zero bytes to the nearest multiple of 4 (totaling 12 bytes).
However, we can truncate this string to just <code>v4.0.</code>, stripping a total of 4 bytes after padding, to trick .NET to still boot up the CLR version 4.0 and run the program successfully.</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/cff06.png"><img data-src="/assets/img/posts/tinysharp/cff06.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/cff06.png"></a>
<em>The .NET metadata directory contains a version string specifying the required runtime which can be truncated.</em></p>

<p>Note that, for some reason, the trailing <code>.</code> seems to be important. I have no idea why, but getting rid of anything more than this string will make the program not boot up correctly.</p>

<p>Our final size is 834 bytes (<code>0x342</code>):</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/size08.png"><img data-src="/assets/img/posts/tinysharp/size08.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/size08.png"></a>
<em>The final size of our binary.</em></p>

<p>We can ZIP it to get it to a mere 476 bytes (compared to 582 bytes if we did not do any optimizations after reaching the 1KB limit).
This is where I decided to call it quits.</p>

<p>Finally, to prove the program still works fine, here is a screenshot:</p>

<p><a href="https://blog.washi.dev/assets/img/posts/tinysharp/final.png"><img data-src="/assets/img/posts/tinysharp/final.png" alt="" data-proofer-ignore="" src="https://blog.washi.dev/assets/img/posts/tinysharp/final.png"></a>
<em>It still works!</em></p>

<h2 id="final-words"><span>Final Words</span><a href="#final-words"><i></i></a></h2>

<p>This was a dumb way to spend my Saturday.</p>

<p>Even though this is probably quite a useless project, I still like diving into these dumb rabbit holes every now and then.
Exploring the limits of well-established systems is always fun, even if the end result is kind of pointless.</p>

<p>To summarize what we have done, we went from a Hello World file of <code>4.6 KB</code> compiled by the C# compiler to a handcrafted PE file of <code>834 B</code> excluding trailing zero bytes.
I don’t think we can get any smaller than this, but I am happy to be proven wrong!</p>

<p>As said before, the final source code to produce the binary can be found on my GitHub:</p>

<p><a href="https://gist.github.com/Washi1337/367eede6e00b31e29355626d5e2f3078" target="_blank">
 Full Source Code
</a></p>

<p>Happy hacking!</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google's Privacy Policy Now Admits to Collecting All Your Data for AI Training (137 pts)]]></title>
            <link>https://www.pcgamer.com/in-case-there-was-any-doubt-googles-privacy-policy-now-explicitly-states-that-its-going-to-suck-up-all-your-data-to-train-its-ai/</link>
            <guid>36651717</guid>
            <pubDate>Sun, 09 Jul 2023 05:09:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pcgamer.com/in-case-there-was-any-doubt-googles-privacy-policy-now-explicitly-states-that-its-going-to-suck-up-all-your-data-to-train-its-ai/">https://www.pcgamer.com/in-case-there-was-any-doubt-googles-privacy-policy-now-explicitly-states-that-its-going-to-suck-up-all-your-data-to-train-its-ai/</a>, See on <a href="https://news.ycombinator.com/item?id=36651717">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-label="article" data-id="mTSjXezVBz6VRxDMMJKq8d">
<header>
<nav aria-label="Breadcrumbs">
<ol>
<li>
<a href="https://www.pcgamer.com/" aria-label="Return to Home">Home</a>
</li>
<li>
<a href="https://www.pcgamer.com/uk/news/" aria-label="Return to News">News</a>
</li>
</ol>
</nav>


</header>
<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture><source type="image/webp" alt="Google campus sign" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1920-80.jpg.webp 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P.jpg"><source type="image/jpeg" alt="Google campus sign" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1920-80.jpg 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P.jpg"><img src="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-320-80.jpg" alt="Google campus sign" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P-1920-80.jpg 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P.jpg"></picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/uyEje6YtnET6euVcPGbG3P.jpg">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span itemprop="copyrightHolder">(Image credit: Bloomberg (Getty Images))</span>
</figcaption>
</div>

<div id="article-body">
<p>To me, artificial intelligence is a lot like magnets: I have no idea how they work. But I do understand, in a very general sense, that AI is not actually intelligent. It's just data, collected on a massive scale, algorithmically digested, and spit out in conversational tones designed to make us think that the machine is "smart."</p><p>The popular versions of these systems, like ChatGPT, live and die based on the amount of data they can harvest, which essentially means they're reliant on you. And in case there's any doubt about what "you" means in this particular context, <a href="https://policies.google.com/privacy?hl=en#footnote-sources" target="_blank" data-url="https://policies.google.com/privacy?hl=en#footnote-sources">Google</a> (via <a href="https://www.techspot.com/news/99281-google-policy-update-confirms-itll-scrape-everything-you.html" target="_blank" data-url="https://www.techspot.com/news/99281-google-policy-update-confirms-itll-scrape-everything-you.html">Techspot</a>) has updated its privacy policy to explicitly state that pretty much anything you say or do online can be scooped up and used to train its AI models.</p><p>Naturally, Google collects data from your online activity, like the stuff you search for, the videos you watch, the things you buy, and the people you talk to, and the location data accessed through your Android mobile device. But "in some circumstances," it also collects information from "publicly accessible sources": If your name appears in a local newspaper article, for instance, Google may index the article and then share it with people searching for your name.</p><p>That in itself isn't new: What's changed, as can be seen on Google's <a href="https://policies.google.com/privacy/archive?hl=en" target="_blank" data-url="https://policies.google.com/privacy/archive?hl=en">policy updates page</a>, is how Google says it can use the information it picks up from those public sources. Previously, the policy stated that publicly available data could be used "to help train Google’s language models and build features like Google Translate." The latest update broadens the policy considerably: "We may collect information that’s publicly available online or from other public sources to help train Google’s AI models and build products and features like Google Translate, Bard, and Cloud AI capabilities."</p><p>Bard is essentially Google's answer to ChatGPT, announced <a href="https://www.pcgamer.com/google-unveils-its-own-chatgpt-like-ai-chatbot/" target="_blank">earlier this year</a>, and much like other AI models it hasn't been entirely smooth sailing. In April, for instance, a report claimed that several Google employees had urged the company not to roll out Bard because the information it provided in response to queries was "worse than useless" and effectively made the chatbot a "<a href="https://www.pcgamer.com/google-employees-reportedly-begged-it-not-to-release-pathological-liar-ai-chatbot-bard/" target="_blank">pathological liar</a>."</p><p>More data should, in theory at least, lead to better results for Google's bots. But updated privacy policy or not, the legal status of this behaviour has not been clearly established. OpenAI is facing multiple lawsuits over the way it harvests and uses data to train ChatGPT: Policies like the one recently implemented by Google might seem to make some of it fair game but, but as <a href="https://www.washingtonpost.com/technology/2023/06/28/openai-chatgpt-lawsuit-class-action/" target="_blank" data-url="https://www.washingtonpost.com/technology/2023/06/28/openai-chatgpt-lawsuit-class-action/">The Washington Post</a> reported, AI models will hoover up pretty much anything from Wikipedia pages to news posts and individual tweets, a habit that a growing number of people take issue with.&nbsp;</p><p>And not all of the material in question is in fact fair game: Authors Mona Awad and Paul Tremblay recently <a href="https://www.theguardian.com/books/2023/jul/05/authors-file-a-lawsuit-against-openai-for-unlawfully-ingesting-their-books" target="_blank" data-url="https://www.theguardian.com/books/2023/jul/05/authors-file-a-lawsuit-against-openai-for-unlawfully-ingesting-their-books">filed their own lawsuit</a> against OpenAI, alleging that ChatGPT violated copyright laws by using their works to train its AI model without permission.</p><p>I've reached out to Google for more information on its reasons for changing its privacy policies, and will update if I receive a reply.</p>
</div>
<div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent"><section><p>Sign up to get the best content of the week, and great gaming deals, as picked by the editors.</p></section></div>
<div id="slice-container-authorBio"><p>Andy has been gaming on PCs from the very beginning, starting as a youngster with text adventures and primitive action games on a cassette-based TRS80. From there he graduated to the glory days of Sierra Online adventures and Microprose sims, ran a local BBS, learned how to build PCs, and developed a longstanding love of RPGs, immersive sims, and shooters. He began writing videogame news in 2007 for The Escapist and somehow managed to avoid getting fired until 2014, when he joined the storied ranks of PC Gamer. He covers all aspects of the industry, from new game announcements and patch notes to legal disputes, Twitch beefs, esports, and Henry Cavill. <em>Lots</em> of Henry Cavill.</p></div>



</section>


<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[KDE for Travelers (120 pts)]]></title>
            <link>https://kde.org/for/travelers/</link>
            <guid>36651387</guid>
            <pubDate>Sun, 09 Jul 2023 03:59:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kde.org/for/travelers/">https://kde.org/for/travelers/</a>, See on <a href="https://news.ycombinator.com/item?id=36651387">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><main><div><p>Travel the World Using KDE Applications</p></div><div><figure><img src="https://apps.kde.org/app-icons/org.kde.itinerary.svg" alt="" title=""></figure><h2 id="kde-itinerary">KDE Itinerary</h2><p>KDE Itinerary is a digital travel assistant that protects your privacy. It makes collecting all the information about your travel inside a single application easy and straightforward. KDE Itinerary is available for <a href="https://plasma-mobile.org/">Plasma Mobile</a> and Android.</p><p><a href="https://community.kde.org/Android/FDroid#KDE_F-Droid_Release_Repository"><img src="https://kde.org/store_badges/fdroid/en.svg" alt="Get it on F-Droid"></a></p></div><div><div><h2 id="store-your-reservations">Store your reservations</h2><p>Store all the information about your reservations in Itinerary. This includes QR-codes, check-in times, arrivial times, real-time delays, seat reservations, coach layout, and more.</p><p>Itinerary supports train, bus and flight bookings, as well as hotel, restaurant, event and rental car reservations. Traveling in a group? Not a problem, Itinerary supports multi-traveler bookings.</p></div><p><img src="https://kde.org/for/travelers/itinerary-trip.png"></p></div><div><div><h2 id="local-first">Local first</h2><p>Itinerary automatically extracts booking data from various input formats. It's all performed locally on <strong>your</strong> device and your data is not sent to any remote servers.</p><p>This works best when using <a href="https://kontact.kde.org/components/kmail/">KMail</a> to extract tickets from your email and then <a href="https://kdeconnect.kde.org/">KDE Connect</a> to transfer tickets to your phone. This also works great with <a href="https://apps.nextcloud.com/apps/mail">Nextcloud Mail</a> and <a href="https://f-droid.org/en/packages/at.bitfire.davdroid/">DavDroid</a> to sync your tickets from <a href="https://nextcloud.com/">Nextcloud</a>.</p><figure><img src="https://kde.org/for/travelers/kmail.png" alt="KMail ticket extraction showing a train trip from Berlin to Tübingen" title="KMail ticket extraction showing a train trip from Berlin to Tübingen"></figure></div><p><img src="https://kde.org/for/travelers/itinerary-ticket.png"></p></div><div><div><h2 id="add-your-connections">Add your connections</h2><p>Aside from finding reservations automatically in your email, Itinerary lets you add train trips manually to your journey, find alternative connections if your train is cancelled, or, for some providers, import your train trip directly from your reservation number.</p></div><p><img src="https://kde.org/for/travelers/itinerary-connections.png"></p></div><div><div><h2 id="find-your-way">Find your way</h2><p>Powered by <a href="https://www.openstreetmap.org/">Open Street Map</a>, the indoor map at train stations or airports can be a life saver. Use Itinerary to locate your platform is, and, if you have seat reservation and the train layout is available, it can even show you exactly which platform section is best for you.</p><figure><img src="https://www.volkerkrause.eu/assets/posts/139/kde-itinerary-platform-section-highlighting.jpg" alt="Train station map in KDE Itinerary, highlighting relevant platform sections." title="Train station map in KDE Itinerary, highlighting relevant platform sections."></figure><p>The indoor map also shows you which shops and resturantes are currently open, which elevator is broken (yet again!), where the toilets are and where the correct exit is.</p></div><p><img src="https://kde.org/for/travelers/itinerary-opening-hours.png"></p></div><div><div><h2 id="real-time">Real time</h2><p>It is rare that a train or bus depart or arrive on time. Itinerary keeps you updated when delays are announced.</p><p>On supported train and long distance buses, Itinerary will also use the onboard APIs to fetch the current live status of the vehicle and keep you updated on your current position and any announcements.</p></div><p><img src="https://kde.org/for/travelers/itinerary-live-status.png"></p></div><div><h2 id="arianna">Arianna</h2><p>Arianna is an excellent ebook reader that lets you read your favorite books while traveling. Arianna will track your reading progress
and classify your books by genre and author automatically.</p><figure><img src="https://cdn.kde.org/screenshots/arianna/reader.png" alt="Screenshot of Arianna" title="Screenshot of Arianna"></figure></div><div><h2 id="kasts">Kasts</h2><p>Enjoy listening to podcasts on the move with Kasts! Subscribe to your favorite podcasts and get updated as soon as a new episode is out.</p><figure><img src="https://cdn.kde.org/screenshots/kasts/kasts-desktop.png" alt="Screenshot of Kasts" title="Screenshot of Kasts"></figure><p>Kasts is also available on Android and <a href="https://plasma-mobile.org/">Plasma Mobile</a>.</p></div><div><p><a href="https://community.kde.org/Android/FDroid#KDE_F-Droid_Release_Repository"><img src="https://kde.org/store_badges/fdroid/en.svg" alt="Get it on F-Droid"></a></p></div><div><h2 id="kgeotag">KGeoTag</h2><p>KGeoTag is a geotagging program. It lets you tag your images with geocoordinates by matching them to a corresponding GPX track or by manually setting them by drag and dropping the images, or entering the coordinates by hand.</p><p>It is very helpful in combination with <a href="https://www.kphotoalbum.org/">KPhotoAlbum</a> or <a href="https://www.digikam.org/">Digikam</a> when you want to organize your photo collection after your trip and then visualize all the locations you visited.</p><figure><img src="https://cdn.kde.org/screenshots/kgeotag/kgeotag_shadow.png" alt="Screenshot of KGeoTag showing a track on a map with some image preview allong the track" title="Screenshot of KGeoTag showing a track on a map with some image preview allong the track"></figure></div><div><h2 id="marble">Marble</h2><p>Explore the world with Marble. Marble contains a huge collection of maps that let you travel all over the globe from your desktop. Visit remote places via detailed satellite images, travel the world before the discovery of America, and check out the average temperature and precipitation in winter and summer in other countries.</p><p>Marble also allows you to bookmark your favorite locations and lets you find your way thanks to its routing algorithm and OpenStreetMap powered maps.</p><figure><img src="https://cdn.kde.org/screenshots/marble/marble-world.png" alt="Screenshot of Kasts" title="Screenshot of Kasts"></figure></div><div><h2 id="other-open-source-apps-for-you">Other open source apps for you</h2><p>Here are some other applications from other open source communities that will make your travels fun.</p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Perl first commit: a “replacement” for Awk and sed (203 pts)]]></title>
            <link>https://github.com/Perl/perl5/commit/8d063cd8450e59ea1c611a2f4f5a21059a2804f1</link>
            <guid>36650120</guid>
            <pubDate>Sun, 09 Jul 2023 00:24:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Perl/perl5/commit/8d063cd8450e59ea1c611a2f4f5a21059a2804f1">https://github.com/Perl/perl5/commit/8d063cd8450e59ea1c611a2f4f5a21059a2804f1</a>, See on <a href="https://news.ycombinator.com/item?id=36650120">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  

    
    

    






  
  

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container">
  <p>
  <h2>Commit</h2>
</p>

<p><a href="https://github.com/Perl/perl5/commit/8d063cd8450e59ea1c611a2f4f5a21059a2804f1" data-hotkey="y">Permalink</a></p>


<div>
  <p><a id="browse-at-time-link" href="https://github.com/Perl/perl5/tree/8d063cd8450e59ea1c611a2f4f5a21059a2804f1" rel="nofollow">Browse files</a></p><tool-tip id="tooltip-48b3911f-a378-408f-be27-09a795a734bd" for="browse-at-time-link" data-direction="ne" data-type="description" data-view-component="true">Browse the repository at this point in the history</tool-tip>
    <p>
      a "replacement" for awk and sed
    </p>

    <div><pre>[  Perl is kind of designed to make awk and sed semi-obsolete.  This posting
   will include the first 10 patches after the main source.  The following
   description is lifted from Larry's manpage. --r$  ]

   Perl is a interpreted language optimized for scanning arbitrary text
   files, extracting information from those text files, and printing
   reports based on that information.  It's also a good language for many
   system management tasks.  The language is intended to be practical
   (easy to use, efficient, complete) rather than beautiful (tiny,
   elegant, minimal).  It combines (in the author's opinion, anyway) some
   of the best features of C, sed, awk, and sh, so people familiar with
   those languages should have little difficulty with it.  (Language
   historians will also note some vestiges of csh, Pascal, and even
   BASIC-PLUS.) Expression syntax corresponds quite closely to C
   expression syntax.  If you have a problem that would ordinarily use sed
   or awk or sh, but it exceeds their capabilities or must run a little
   faster, and you don't want to write the silly thing in C, then perl may
   be for you.  There are also translators to turn your sed and awk
   scripts into perl scripts.</pre></div>

  <div>
  <include-fragment src="/Perl/perl5/branch_commits/8d063cd8450e59ea1c611a2f4f5a21059a2804f1" id="async-branches-list">
    
    <ul>
      <li>Loading branch information<span></span></li>
    </ul>
</include-fragment></div>


  
</div>


  


  <diff-layout>
    
        </diff-layout>


</div>

</turbo-frame>


    </main>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[California needs real math education, not gimmicks (231 pts)]]></title>
            <link>https://www.noahpinion.blog/p/california-needs-real-math-education</link>
            <guid>36650010</guid>
            <pubDate>Sun, 09 Jul 2023 00:05:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.noahpinion.blog/p/california-needs-real-math-education">https://www.noahpinion.blog/p/california-needs-real-math-education</a>, See on <a href="https://news.ycombinator.com/item?id=36650010">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg" width="716" height="290.1373626373626" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:590,&quot;width&quot;:1456,&quot;resizeWidth&quot;:716,&quot;bytes&quot;:183684,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><em><span>In an </span><a href="https://vimeo.com/65921206" rel="">old Saturday Night Live skit</a><span>, Chevy Chase, portraying Gerald Ford, says “It was my understanding that there would be no math.” Half a century later, it seems like this has become America’s national motto. Even as high-tech manufacturing has migrated relentlessly to China, plenty of Americans seem to think that they — or anyone — should be able to flourish in a modern economy without a functional understanding of mathematics. U.S. high school math scores </span><a href="https://www.bloomberg.com/opinion/articles/2019-12-12/u-s-schools-do-fine-in-international-rankings-except-in-math?sref=R8NfLgwS" rel="">lag significantly</a><span> behind other countries, even though scores in reading and science are above average, and our country is </span><a href="https://www.bloomberg.com/view/articles/2020-07-29/u-s-will-need-talented-refugees-if-skilled-immigrants-won-t-come?sref=R8NfLgwS" rel="">utterly dependent </a><span>on a continuous inflow of foreign talent for a number of critical STEM fields. Math, out of all subjects, seems to hold a special terror for Americans, who often seem to view the subject as a test of innate intelligence rather than </span><a href="https://www.theatlantic.com/education/archive/2013/10/the-myth-of-im-bad-at-math/280914/" rel="">a skill that can be acquired and honed</a><span> through hard work. </span></em></p><p><em><span>In response to lagging math scores, educators in California have been trying to water down math education — banning students from taking algebra in 8th grade, replacing advanced algebra classes with “data science” courses that don’t even teach the algebra required to understand basic statistics, and so on. I see this as an extremely wrongheaded move, and I’ve been meaning to write about it for a while. But data analyst </span><a href="https://twitter.com/ArmandDoma" rel="">Armand Domalewski</a><span>, a friend of mine, has been following the issue far more closely than I have, and has been outspoken about it on social media, so I thought I would ask him to take a crack at saying what needs to be said.</span></em></p><p>One of the strangest things about California is that it is simultaneously one of the technology capitals of the world and has some of the worst math scores for children in the entire United States. In practice, California has relied on a combination of pockets of home grown math excellence and imported math whizzes from around the globe to bridge the gap between the math skills it needs and the math skills it has. It works, somewhat—but we can and should do better by all of the kids in our state.</p><p>We have a chance to do exactly that with the release of a new California Math Framework (C.M.F.), a document used by the state to establish math curricula for all public schools in California. Unfortunately, that process has been hijacked by a “math reform” movement led by Stanford Professor Dr. Jo Boaler, who claims to advocate for a more inclusive way of teaching that would replace memorizing times tables with real-world problem-solving. Her worldview has gained credence in influential educational circles because, to many people, including myself, the basic premise is extremely appealing: replacing rote memorization with creative problem solving, making math more inclusive to all kinds of students, embracing a growth mindset, etc. all sound lovely. And honestly, I don’t think at a high level that these concepts are wrong—what is broken, however, is the specific implementation of these ideas as advocated by Dr. Boaler and implemented by California education policymakers.</p><div><p><span>Math </span><em>should </em><span>be more inclusive. Math </span><em>should </em><span>be more engaging. I think one of the biggest mistakes both Dr. Boaler’s supporters and detractors have made in this debate is to try to slot what should be a practical, fact based argument about optimal math education into an ideological struggle invoking silly phrases like Woke Math. Dr. Boaler and her fans did not make math education worse by being too left wing in their math—whatever that even means—but by sloppy with their science and lazy with their facts. You don’t make math education better by advocating for changes based on lies, and unfortunately, that is exactly what happened here. </span></p><p><span>Two of the major policy changes proposed in the draft CMF are already showing indications of disaster. The first is moving Algebra education out of 8th Grade.</span></p></div><div><p><span>One of the ideas underpinning the California Math Framework is the notion that math needs to be “detracked”—instead of allowing some students to take Algebra I in 8th grade, it would require all students to enroll in the same math curriculum until the 9th grade. Advocates argue&nbsp; this promotes equity, while detractors argue that it diminishes excellence. Years ago, I supported San Francisco’s efforts to rework the curriculum based on that argument—I believed those who said it would improve educational outcomes for the kids struggling the most without hurting those who were already succeeding. </span></p><p><span>I was wrong. Not only did pushing out 8th grade Algebra hurt kids who were at the top of their class by forcing them to pay for private classes or other workarounds to get the credits they needed to apply for UCs, the claim that it would help outcomes for kids who were struggling turned out to be a bald faced lie.</span></p></div><p><span>In 2017, </span><a href="http://www.sfusdmath.org/uploads/2/4/0/9/24098802/historic_shifts_in_math_show_promise.pdf" rel="">SFUSD</a><span> claimed a "dramatic increase in student comprehension" and a drop in Algebra 1 repeaters from 40% to 7%, and credited detracking. An analysis by</span><a href="https://www.familiesforsanfrancisco.com/updates/inequity-in-numbers" rel=""> Families for San Francisco</a><span> found that this claim was utter nonsense. Algebra 1 grades did not improve at all, and the only reason the repeat rate went down was because SFUSD straight up </span><em>eliminated the requirement that you had take an exit exam in order to progress!</em><span>&nbsp;</span></p><p>Not only that, but the group was unable to replicate the 40% to 7% drop using the data provided by SFUSD through a records request, and no other independent entity has been able to validate that number as well.</p><p><span>Page 6 of this </span><a href="https://static1.squarespace.com/static/60412a3a51d4863950d1bdf2/t/616e2f823696906267609f3f/1634611077888/Report-+Inequity+in+Numbers.pdf" rel="">report</a><span> is particularly damning–SFUSD provides numbers that, when reverse engineered with some basic Algebra, would imply a class size of 2475 students when the actual class size was 4011. What happened to the other 1,536 students? The shoddiness of this evidence did not stop Dr. Boaler from touting this as a major success for her ideas, and until very recently, did not stop the CMF from citing it as a major argument in favor of detracking.</span></p><div><p><span>Not only did detracking not achieve its stated goals of advancing math equity in San Francisco, it actually harmed Black and brown students. By the end of 10th grade, Algebra 2 enrollments of Black and brown students declined, since their families were less likely to afford the expensive work arounds that white and Asian families pursued. Instead, most of the district’s Black and Latino students ended up in a diluted “compression” course that lacked about 75% of the state’s</span><a href="https://www.cde.ca.gov/ci/ma/cf/documents/mathfwprecalculus.pdf" rel=""> precalculus</a><span> “+” standards, where the “+” standards are defined as “additional mathematics to prepare students for advanced courses,” making it difficult for students to pursue more advanced math in college. (Which is why, counter the claims of some detracking advocates, the UCs do not officially credit this compression course as “advanced math.”) </span></p><p><span>The result? They’re grim. If you compare </span><a href="https://www.educationnext.org/san-franciscos-detracking-experiment/" rel="">statewide results against SFUSD results on California’s Smarter Balanced tests,</a><span> which assess student performance across the state, you see that between 2015 and 2019, at the state level, the eleventh-grade Black-White student&nbsp; gap grew by 11 points—from 94 to 105—while in SFUSD, the gap expanded by 15 points (from 143 to 158). The outcomes are even worse for Hispanic students. The Hispanic-White gap at the state level gap grew by only 5 points, but in SF, it grew by 31 points.</span></p></div><p><span>As with all education data, there are always a million variables and you can never conclusively say that a single policy change caused a specific outcome, but at the very least, it is hard to argue that these 8th Grade Algebra changes advocated by Boaler helped SFUSD, and even harder to argue they serve </span><em>as a model for our state.&nbsp;</em></p><p>Unfortunately, that is not the only controversial policy change being pitched in the CMF: another poorly conceived notion is the replacement of the second year of Algebra with “data science.”</p><p><span>For decades, American math curriculum has followed a standard sequence: arithmetic, algebra, geometry, algebra II, precalculus and trigonometry, and calculus. The University of California required you to take three years of high-school math, culminating in Algebra II. In October 2020, the UC Board of Admissions and Relations with Schools (BOARS) </span><a href="https://senate.universityofcalifornia.edu/_files/committees/boars/documents/statement-on-mathematics-preparation-for-uc.pdf" rel="">recommended allowing alternatives</a><span> to the second year of algebra—including data science. Courses like “</span><a href="https://www.introdatascience.org/" rel="">Introduction to Data Science</a><span>,” developed by UCLA and “</span><a href="https://hsdatascience.youcubed.org/" rel="">Explorations in Data Science</a><span>,” developed by Dr. Boaler, started popping up. The argument was that these classes would teach data skills relevant to the 21st century, such as collecting and analyzing data on “real-world topics,” in contrast to Algebra II, which Boaler said was as relevant as “</span><a href="https://www.latimes.com/opinion/story/2019-10-23/math-high-school-algebra-data-statistics" rel="">sock darning and shorthand</a><strong><span>.” </span></strong><span>And look, in theory, this sounds nice. I mean, my job is literally data analyst—I analyze, evaluate, and interpret data for a living! When I first heard about this, I was thrilled. But when I thought about it a bit more, it gave me pause—the skills I use daily as a data analyst are based on a foundation of Algebra and Calculus. It didn’t quite make sense to me how you could </span><em>replace </em><span>Algebra II with data science—the formulas that make up linear regression, for example, don’t make any sense unless you have at least a basic grasp of algebra.&nbsp; Logarithms and trigonometric functions are pretty core to doing data science work! So I started digging into what was actually being taught in these “data science” courses and was…frankly, I was horrified.</span></p><p><span>While UC Admissions requirements state that Algebra II alternatives still have to “build on” certain core concepts in Algebra II, in practice this does not seem to be enforced. The “Introduction to Data Science” produced by UCLA contains </span><a href="https://www.ucladatascienceed.org/wp-content/uploads/California-Common-Core-Mathematics-Standards-addressed-by-IDS.pdf" rel="">very little Algebra II</a><strong> </strong><span>and “Explorations in Data Science” only claims to</span><a href="https://docs.google.com/presentation/d/e/2PACX-1vQ4tVbSk5qZsgARWwctjKa6joNKKYi7_jzi2-hkDyr7yGzQSQgKCndzhfaICVooye55ZZqCBEVXpxSv/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.p" rel=""> teach the portions of Algebra II that overlap with statistics</a><span>, leaving huge swathes of math necessary for an eventual career in STEM completely untouched.&nbsp;</span></p><p><span>Frankly, reading the CMF does not give me the impression that its authors have a strong understanding of what data science </span><em>is</em><span>, exactly. It includes phrases like “the numbers are staggering: around 1.7 megabytes of digital data were created and stored every second for every person on Earth in 2020, and the vast majority of data goes unanalyzed.’ As </span><a href="https://sites.google.com/view/publiccommentsonthecmf/#h.w46loj4uaiev" rel="">Dr. Brian Conrad</a><span> points out, this is a nonsense statement. Is 1.7MB a large amount? (No, literally one JPEG can be that size.) Most of that is likely video, how exactly should that “data” be “analyzed”? And how do the authors even know it’s not being analyzed? After all, every video uploaded to YouTube gets tossed into an algorithm that produces viewing metrics, there are data scientists analyzing those uploads for trends, etc? (The authors probably found it by googling “impressive big data stats'', which spat out that statistic as the first result at the time the CMF was drafted.).&nbsp;</span></p><p><span>I wish this ignorance of the subject matter was limited to cute illustrative examples, but unfortunately, it permeates the basic thinking and structure of the document. The core issue of the CMF’s “data science” section is that it claims to be discussing data science while it is actually discussing </span><em>data literacy. </em><span>Don’t get me wrong—data literacy is good! Society would be better off if more people understood how to clean data or read a poll accurately. </span><em>But this is not data science and it is not math. </em><span>&nbsp;The CMF is replete with </span><a href="https://drive.google.com/file/d/1QI9XDw77ZlvwtcnLn_rKbhaWHo2UX-E2/view" rel="">statements</a><span> like “high-school data-science class students can learn to clean data sets – removing any data that is incorrect, corrupted, incorrectly formatted, duplicated, or incorrect in some other way [...] High school students can also learn to download and upload data, and develop the more sophisticated “data moves” that are important to learn if students are tackling real data sets.'' This teaches you how to use Excel, sure—but it does not teach you how regressions work, how statistical tests work, the multivariable calculus and linear algebra </span><em>you need to do the job of an actual data scientist!</em></p><p><span>In response to this, science and math professors across the state </span><a href="https://sites.google.com/view/mathindatamatters/home" rel="">have</a><span> been </span><a href="https://edsource.org/2022/proposed-mathematics-pathways-for-california-high-school-students-raise-equity-concerns/674400" rel="">raising</a><span> </span><a href="https://www.chronicle.com/article/the-university-of-california-changed-its-math-standards-some-faculty-arent-happy" rel="">alarms.</a><span> In response, Dr. Boaler turned to a tactic she often relies on—trying to wrap her ideas in the context of a broader culture war, painting critics as stodgy conservatives fighting her efforts to make math more equitable and diverse. She described her critics as those resisting change. The notion that teaching this version of data science rather than Algebra II is somehow more equitable permeates the CMF in often bizarre ways.&nbsp;</span></p><p><span>The CMF says data science is more </span><a href="https://sites.google.com/view/publiccommentsonthecmf/" rel="">equitable</a><span> than other STEM fields because “data scientists work together to address uncertainty in data while avoiding bias.”</span></p><p>Err, what? I’ve met many data scientists who do not work together or address uncertainty in data while avoiding bias, and many non-data science STEM professionals who do. There is absolutely nothing inherent to data science that makes it more collaborative or unbiased than other STEM fields…</p><div><p><span>It goes on to say…“Traditional mathematics lessons that have taught the subject as a set of procedures to follow have resulted in widespread disengagement as students see no relevance for their lives. This is particularly harmful for students of color and for girls…The data science field provides opportunities for equitable practice, with multiple opportunities for students to pursue answers to wonderings and to accept the reality that all students can excel in data science fields.'' </span></p><p><span>I agree that traditional math as currently taught does disengage a lot of students, and in particular women. But there is absolutely no evidence offered in the CMF to suggest that data science education would somehow be different, and there is something profoundly weird about the suggestion that students of color and girls can excel in data science fields but not excel in other fields of mathematics. The primary reason girls, for example, diverge from boys in math performance is </span><em><a href="https://techcrunch.com/2016/01/05/why-stems-future-rests-in-the-hands-of-12-year-old-girls/" rel="">because society teaches them that math is not for girls.</a></em><span> That is not something swapping out actual math for a watered down “data science” course can solve, and it’s pretty gross for people to claiming to be trying to make math more equitable for women and people of color to be pushing a program that will actually make them </span><em>worse at math. </em><span>Dr. Boaler and the CMF are basically saying “women and people of color aren’t doing as well in math, so we should just </span><em>give up on teaching them actual math</em><span>. It’s bananas!</span></p></div><p><span>But don’t take my white, male word for it—a group of Black UC faculty members in data science-related fields wrote a </span><a href="https://www.chronicle.com/article/the-university-of-california-changed-its-math-standards-some-faculty-arent-happy" rel="">letter</a><span> stating, “‘Introduction to Data Science’...make[s] claims that they specifically support learning for women and minorities, which are not only baseless, but fail to appreciate that they actually do the opposite and harm students from such groups by steering them away from being prepared for STEM majors.”</span></p><p><span>There’s a reason why these folks have been joined by other Black mathematicians around the country, such as </span><a href="https://www.chronicle.com/article/the-divider" rel="">Dr. Jelani Nelson,</a><span> in pushing back fiercely against the ideas around 8th Grade Algebra and data science proposed in the CMF. (And a reason, perhaps, that Dr. Boaler </span><a href="https://nypost.com/2022/04/08/stanford-prof-calls-cops-on-berkeley-prof-who-exposed-her-5k-hour-consulting-fee/" rel="">threatened to call the police on him for it!</a><span>) There’s a reason why Stanford Mathematics professor Dr. Brian Conrad&nbsp; wrote, in a comprehensive </span><a href="https://sites.google.com/view/publiccommentsonthecmf/?ref=stanfordreview.org#h.ns5n6hdqa4x8" rel="">takedown</a><span> of the CMF you really should read, that “whatever author is responsible for such a myopic view of mathematics should never again be involved in the setting of public policy guidance on math education.” There’s a reason why the </span><em><span>authors of papers Dr. Boaler cites to </span><a href="https://www.chronicle.com/article/the-divider?cid=gen_sign_in" rel="">back up her work consistently say she has misread and misrepresented their work</a></em><a href="https://www.chronicle.com/article/the-divider?cid=gen_sign_in" rel="">,</a><span> and that it does not support the claims she is making. And the reason, simply, is that her ideas have not worked. Forcing all children to defer Algebra until 9th grade,trying to squeeze two years of schooling into one year of a watered down “compression course” rejected by the University of California for not meeting its </span><a href="https://icas-ca.org/wp-content/uploads/2020/05/ICAS-Statement-Math-Competencies-2013.pdf" rel="">standards</a><span>, and replacing Algebra II with a glorified data literacy course masquerading as a “data science” course does not help high achieving kids or struggling kids or any kids in between—it hurts them all.</span></p><p><span>California students need different answers on math–what we’ve been doing for the past few decades hasn’t worked. But that doesn’t mean we should embrace the ideas embodied in the current CMF draft, which were built on decades of </span><a href="https://www.nonpartisaneducation.org/Review/Articles/v8n1.pdf" rel="">shoddy and dishonest academic research,</a><span> and throw up our hands at the notion of teaching underperforming kids advanced math entirely. The good news is that there are answers out there—we can learn from </span><a href="https://www.theatlantic.com/education/archive/2013/10/the-myth-of-im-bad-at-math/280914/" rel="">other countries teach math differently than we do,</a><span> we can integrate findings from the </span><a href="https://www.cis.org.au/publication/myths-that-undermine-maths-teaching/" rel="">“science of math”</a><span>, and more. Our kids deserve a better California Math Framework than the one we’re being offered now—let’s get it done.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://www.noahpinion.blog/p/california-needs-real-math-education?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.noahpinion.blog/p/california-needs-real-math-education?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pocket gets worse the more you use it (2019) (128 pts)]]></title>
            <link>https://web.archive.org/web/20190512092903/https://old.reddit.com/r/dredmorbius/comments/5x2sfx/pocket_it_gets_worse_the_more_you_use_it/</link>
            <guid>36649740</guid>
            <pubDate>Sat, 08 Jul 2023 23:19:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://web.archive.org/web/20190512092903/https://old.reddit.com/r/dredmorbius/comments/5x2sfx/pocket_it_gets_worse_the_more_you_use_it/">https://web.archive.org/web/20190512092903/https://old.reddit.com/r/dredmorbius/comments/5x2sfx/pocket_it_gets_worse_the_more_you_use_it/</a>, See on <a href="https://news.ycombinator.com/item?id=36649740">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Having used Pocket's article-archival-and-management tool -- sort of a bookmarks-on-steroids product -- for the past year or two, and with news that the company has just been acquired by Mozilla (it's been getting increasingly integrated into Firefox over the past year or so), I've realised one of my most fundamental complaints about it.</p>

<p><strong>The more I use Pocket, the worse it gets.</strong></p>

<p>As I've mentioned (a few too many times), I do a lot of research, much of it with articles and documents, and tools for managing the pile / heap / mountain / swamp are a constant and growing consideration.  Pocket addresses one part of the problem:  online HTML-based content.</p>

<p>I've compiled a large set of articles.  I'm a top-1% reader, according to an emailed report sent a month or so back.  Some 2,000 or so in Readability, prior to its demise, manually transferred in the week before that service was to go dark, and another 1,000 - 3,000 likely in Pocket itself.  I make copious use of tagging features.  My intent is to have <em>a vetted, known, classified, and useful set of references to comb through for specific research needs as I organise and write</em>.</p>

<p>The fact that I can't even say, not even <em>approximately</em>, how much material I've archived, is a profound signifier of the design failures and lack of consideration of use-cases.  The folks at Pocket seem to have given absolutely no thought to how people might use their product, or benefit by self-directed use.  (And it's not as if they've not heard:  I've shared this complaint with them multiple times over the past two years.)  Which is to say:  even at the most basic level, <em>the product isn't getting better or more useful</em>.</p>

<p>The problem is, the bigger the pile gets, the less manageable it becomes.</p>

<h2>Tags</h2>

<p>Great, I have an unlimited set of tags.</p>

<p>It takes me 45 seconds just to scroll from the top of the list to the end on the Android app.</p>

<p>The tags are not searchable.  A "type and autocomplete" feature -- you know, the sort of thing software has offered since the 1980s, would be peachy.  No such luck.</p>

<p>The tags don't auto-complete <em>and activate such that, say, I can hit &lt;enter&gt; to select one</em> when filing new material.</p>

<p>If I make a tyop, I cannot, say, <em>select a tag and edit it right there</em>.  No, I've got to:</p>

<ol>
<li>Switch to the "Tags" view.</li>
<li>Select the "Edit" option.  <em>Before</em> I search for the tag I want to edit.</li>
<li>Scroll through the list to where the tag in question is.  Given a 45s full-list scan, this takes an average of about 22 seconds.</li>
<li>Edit and save the tag.</li>
</ol>

<p>What <em>could</em> be a two-second, in-place operation, becomes an epic-journey-to-a-distant-land-and-quest-for-a-holy-grail, fraught with pitfalls and traps -- if you make the wrong turn, you waste time, have to backtrack, and start over again.  During all of which your flow-of-thought is being completely interrupted.</p>

<p>Desktop clients (Web) don't appear to be any better.</p>

<p>If I happen to typo a tab and want to delete it, in the Android app, with my Bluetooth keyboard attached ... I cannot.  I've got to switch to the <em>software</em> keyboard, re-select the tag in question (because, of course, it doesn't stay selected), then delete it.</p>

<p>It's not possible to filter by <em>multiple</em> tags.  Something which is hugely annoying as that is a fast and efficient way to cut through a large mass of material -- items which are cross-referenced and multi-tagged could be, say, filtered with the remaining tags within the set listed.  Check the ones you're interested in and you should end up with a small set of items of interest.</p>

<p>And finally:  the tags display is highly minimal and hidden under windows.  These should be presented <em>on the document itself</em> (head or foot, preferably), <em>with all tags visible at all times</em>.  Selecting a given tag should call up all content under it (and provide for further filtering by other tags).</p>

<h2>Search</h2>

<p>For a time, it seemed that Pocket had a full-text article search, able to use multiple words.  That ... seems to have vanished.</p>

<p><strong>Keep in mind:  Pocket archives its articles locally.  <em>The search can run over the local archive, and doesn't impose any server load.</em>  But it doesn't.</strong></p>

<p>No bueno.</p>

<p>Which means that <em>despite having an archive of data sitting on my own device, in text form, eminently searchable</em>, my best option is to try an online search (of a much larger corpus), and hoping I might land the items of interest.</p>

<p>This is, to say the least, slightly frustrating.</p>

<p>The fact that Pocket's search, such as it is, seems to be limited to <em>a single keyword</em>, to not support <code>"quoted strings"</code>, or <code>-excluded terms</code>, or field-specific criteria (author, date, publisher, website), ranges, etc., is ... similarly a staggering oversight.</p>

<p>Frankly, I'd get more utility (and am strongly considering how I might accomplish this) downloading or fetching content locally, and running various search/index tools over it.</p>

<h2>Search</h2>

<p>No, that's not a typo:  I'm referring to <em>in-document</em> search.  On the Android app, there is no text search <em>within</em> documents.</p>

<p>If I want to find a particular passage, I've either got to vgrep for it (scan manually), switch to one of the Web interfaces, or pull up the original article online.  Another staggering oversight.  (Though in fairness:  fairly common amongst Android apps, which only means the firing squad's work is all the larger.)</p>

<h2>Workflow</h2>

<p>There's no concept of workflow.</p>

<p>Generally, I'm stashing stuff for later review, on which I'll be associating it with various projects, dumping into a general file, or indicating it's been seen and found wanting.  A set of workflow-oriented features would help in this.  No such thing exists.</p>

<h2>Going from Browser to Pocket</h2>

<p>One of my most frequent operations is to open a Web page, discover that it is utterly fucked over in its page design, and want to open it in Pocket.  What I'd <em>like</em> to do is:</p>

<ol>
<li>Open the current tab immediately in Pocket, whilst closing the present window.</li>
</ol>

<p>What I've got to do instead is:</p>

<ol>
<li>Save to Pocket.</li>
<li>Try to close the current tab -- tricky at best on a mobile device given imprecise location control and click-vs-drag ambiguity, plus focus-stealing by Pocket meaning a keyboard &lt;ctrl&gt;-W generally doesn't work.</li>
<li>Punch the Pocket icon that appears to switch to Pocket.</li>
<li>Land on the Pocket <em>article list</em> rather than <em>the article I've just added to it</em>, requiring a 2nd step to get to that.</li>
</ol>

<p>A one-step process has become a 4-5 step process.  Every. Single. Time.</p>

<p>In the way of Pains Suffered Through Life, it's not among the largest.  But it's a telling failure of attention to detail, or consideration of What the User Might Want.</p>

<h2>Reputation</h2>

<p>Since I'm referencing my corpus, over time I'll build up a set of "hot" articles that are referenced more frequently than others.  I might want to have these turn up quickly in searches ... or perhaps exclude them to find other potentially relevant material.  Again, lack of any user-oriented statistics means this isn't supported.</p>

<p>Similarly, I'd like to be able to indicate reputation of sources, for accuracy or insight.  I don't make a habit of choosing a <em>large</em> amount of bogus material, but since one of my research areas is the concept of bogosity itself, there are a few.  And there are also sources and authorities who are particularly compelling.  Being able to track reputation by author, publication, and URL, would again be highly useful.</p>

<h2>Export lists</h2>

<p>A chief value of being able to categorise content is <em>to be able to call it up, and share it</em>, when desired.</p>

<p>I can ... very barely sort-of ... search through and find some subset of my articles.</p>

<p>What I <em>cannot</em> do, and what I've wanted to do many times, is to apply one of the non-existent advanced search features above, to select out a set of, say, 2-12 articles or references I think will be particularly useful to someone, and dump that as a set of URLs (to either Pocket or the original sources).  A hugely useful capability, and one which could well help to popularise Pocket itself.  But not present.</p>

<h2>Highlights and notes</h2>

<p>Again:  for research, I'm reading material for synthesis, not just pleasure.  Which means I want to make comments, mark relevant passages, etc.  There's no capability to do so.</p>

<h2>Paying for it wouldn't help</h2>

<p>I had, for a while, the trial-mode advanced usage features of Pocket.  That <em>may</em> have included full-text search (it's not clear that this was or wasn't included, and ... it's painfully difficult to find out just what the full-product features are).  There was a "suggested tags" feature, which was nice, though not essential, and I find I somewhat prefer <em>thinking</em> about tags <em>without</em> having them suggested to me (though the ability to check against suggestions would be useful).</p>

<p>But none of the other features I've listed are currently in the paid app.  There's been no visible work I'm aware toward any of them over the past year or so.</p>

<p>I'm familiar with arguments for paying for software.  I've never found them particularly convincing, as an individual user, particularly given my experience with Free Software over the years.  Whilst I've seen a fair number of FS projects with crappy user relations, in general I've found that:</p>

<ol>
<li>The software already anticipates my needs.</li>
<li>Developers are responsive to intelligent requests.</li>
<li>I can contribute myself, to my abilities -- a small number of bugfixes, rather more bug reports, occasional documentation.</li>
</ol>

<p>In the proprietary world, <em>if you are a significant customer</em>, it's possible to see requests built out.  Ordinary users, particularly in the Web world where userbases are measured in the 100s of millions or billions, rather less so.  In fact, generally, I've seen long-standing requests <em>from myself and numerous others</em> utterly ignored, for years.  Most especially if they are for "advanced user" features -- anything remotely generative.</p>

<p>A few hours ago I posted an item at Ello about the DMOZ hierarchy categories -- a list of 800,000+ classifications of online content.  I'd done some quick classification of it, on my Android tablet, using Termux, an add-on Linux environment with actually-capable shell tools, including auto-generating a Markdown table.  It's a small example of the power of such tools -- the ability to scan through nearly a million items and reduce them to a meaningful report in 18 rows, ready for publication.  (Mind:  Ello's table support appears broken, though I've also posted a copy to a Reddit sandbox.)</p>

<p><strong>That is the level of power and flexibility I expect from, no, demand from, my tools.  And am finding increasingly lacking.</strong></p>

<p>It took me most of a year to even discover Termux, and another several months to learn of the API features enabling clipboard interaction with the Android environment.  Which I'm using, incidentally, to compose this Reddit post, given the pains and pitfalls of using the Web interfaces on Android.</p>

<p>I suspect too that the individual-subscriber market isn't worth all that much to Pocket either.  Dealing with individual payments and the hassles thereof (from <em>both</em> sides:  credit card and identity fraud affect customers as well), make the margins afforded by support minimal.  Bundling and large-account sales are, with very few exceptions, where the money in software has been made.  Researching a set of Murphy's and related laws earlier, I came across Mark Miller's exception to Crane's Law:</p>

<blockquote>
<p>There are no "free lunches", but sometimes it costs more to collect money than to give away food.  </p>
</blockquote>

<p>That's among the motivating influences behind Free Software as well, though it helps to realise that costs can impose themselves in numerous ways.  An avoided cost of free software <em>can</em> be (though not always) the contributions and assistance of others in improving your product.  Going closed and proprietary loses that, though with possibly other beneficial trade-offs.</p>

<h2>What to do?</h2>

<p>Back to Pocket:  my view for now is much as I was treating Readability for the 2-3 years in which that project was obviously a dead man walking.  It was unsuitable to my needs, but marginally better than nothing at all, or other options.  The transition costs (to another proprietary tool, to a nonproprietary tool, my own solution) are all high.  I'm not sure how much metadata I can extract from Pocket, and loss of my tags would be a major hassle.  The evaluation cost of alternatives (Pinboard.in is highest on the list) is itself high.  I've been looking at an Emacs-based option, or ... something.  Using mutt or an email-storage format as an article reference tool has crossed my mind more than once.  With an IMAPS server, that gives me remote access, search, filter, annotation, and, with some add-on tagging or other classification systems, more organising capabilities.</p>

<p>Or some sort of DIY web-based interface, with some virtual filesystem approaches to addressing a <a href="https://web.archive.org/web/20190512092903/https://news.ycombinator.com/item?id=13751560">larger set of concepts</a> including metadata-as-search based on, say, title, author, other persons, dates, organisations, hashes, or content.</p>

<p>I'd also very much like to have other document formats -- PDFs, ePubs, DJVU, Mobi -- included.  For which, the capability to search meaningfully <em>within those docs, as text</em> would be handy (I'm getting well past the point of badly wanting a <code>pdfgrep</code> or <code>ocrgrep</code> type tool -- which reminds me that converting a substantial set of scanned books to eBook format(s) is another pending project).</p>

<p>Mostly though, I wonder why such things are, 25 years into the WWW, approaching 50 years from the birth of Unix, seventy years after Vannevar Bush's Memex proposal, so fucking hard to get right.  </p>

<p>Is it the problem itself, the people working on it, the dynamics of trying to commercialise such systems, or what?</p>

<hr>

<h2>Updates</h2>

<h3>17 March, 2017</h3>

<p>I'd submitted a request to Pocket support when I first posted this, about two weeks ago now.  Having heard no response, other than an automated acknowledgement, I've just submitted it again yesterday.  Again, an automated response but nothing more.</p>

<p>This is disappointing.</p>

<p>Pocket <em>have</em> tended to be responsive, and friendly, to requests, which was a ray of hope.  Mind:  actually <em>addressing</em> the substance of those requests through fixes and enhancements, not so much.  I'm aware that there can be long lists of such requests, that there's a lot of behind-the-scenes work, and more.  But over the course of 1-2 years, with some pretty significant issues, I'd hope to see <em>some</em> progress.  There's been ... none.</p>

<p>Elements of the product as it stands are good.  But at least for my use-case, it's the oversights which are increasingly glaring and inexcuseable.  I don't like rolling my own for all the obvious reasons, but it really seems as if what I'm looking for doesn't exist.</p>

<h3>7 May, 2017</h3>

<p>I did finally hear back from Pocket a few days ago, they've seen the comments here and acknowledged them.  For that alone, I'm grateful.</p>

<p>Foulups happen and messages get dropped, I've seen that.  I've <em>also</em> seen projects go completely dark (Readability had done that for a few years before shutting down), so ... it's a concerning sign.</p>

<p>I <em>would</em> like to see movement.  If not, I'm considering other options.  Semantic filesystems or something along those lines is starting to sound more interesting.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Machine Unlearning Challenge (158 pts)]]></title>
            <link>https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html</link>
            <guid>36649710</guid>
            <pubDate>Sat, 08 Jul 2023 23:14:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html">https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html</a>, See on <a href="https://news.ycombinator.com/item?id=36649710">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-667019077470952746">
<p><span>Posted by Fabian Pedregosa and Eleni Triantafillou, Research Scientists, Google</span>

</p><p>
Deep learning has recently driven tremendous progress in a wide array of applications, ranging from <a href="https://imagen.research.google/">realistic image generation</a> and <a href="https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html">impressive retrieval systems</a> to <a href="https://blog.google/technology/ai/bard-google-ai-search-updates/">language models that can hold human-like conversations</a>. While this progress is very exciting, the widespread use of deep neural network models requires caution: as guided by Google’s AI <a href="https://ai.google/responsibility/principles/">Principles</a>, we seek to develop AI technologies responsibly by understanding and mitigating potential risks, such as the propagation and amplification of unfair biases and protecting user privacy.
</p> <p>
Fully erasing the influence of the data requested to be deleted is challenging since, aside from simply deleting it from databases where it’s stored, it also requires erasing the influence of that data on other artifacts such as trained machine learning models. Moreover, recent research [<a href="https://arxiv.org/abs/1610.05820">1</a>, <a href="https://arxiv.org/abs/2112.03570">2</a>] has shown that in some cases it may be possible to infer with high accuracy whether an example was used to train a machine learning model using <a href="https://en.wikipedia.org/wiki/Adversarial_machine_learning#Model_extraction">membership inference attacks</a> (MIAs). This can raise privacy concerns, as it implies that even if an individual's data is deleted from a database, it may still be possible to infer whether that individual's data was used to train a model. 
</p>
<p>
Given the above, <em>machine unlearning</em> is an emergent subfield of machine learning that aims to remove the influence of a specific subset of training examples — the "forget set" — from a trained model. Furthermore, an ideal unlearning algorithm would remove the influence of certain examples <em>while maintaining</em> other beneficial properties, such as the accuracy on the rest of the train set and generalization to held-out examples. A straightforward way to produce this unlearned model is to retrain the model on an adjusted training set that excludes the samples from the forget set. However, this is not always a viable option, as retraining deep models can be computationally expensive. An ideal unlearning algorithm would instead use the already-trained model as a starting point and efficiently make adjustments to remove the influence of the requested data.
</p>
<p>
Today we're thrilled to announce that we've teamed up with a broad group of academic and industrial researchers to organize the <a href="https://unlearning-challenge.github.io/">first Machine Unlearning Challenge</a>. The competition considers a realistic scenario in which after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. The competition will be hosted on <a href="https://www.kaggle.com/">Kaggle</a>, and submissions will be automatically scored in terms of both forgetting quality and model utility. We hope that this competition will help advance the state of the art in machine unlearning and encourage the development of efficient, effective and ethical unlearning algorithms.
</p>




<h2>Machine unlearning applications</h2>


<p>
Machine unlearning has applications beyond protecting user privacy. For instance, one can use unlearning to erase inaccurate or outdated information from trained models (e.g., due to errors in labeling or changes in the environment) or remove harmful, manipulated, or outlier data. 
</p>
<p>
The field of machine unlearning is related to other areas of machine learning such as <a href="https://en.wikipedia.org/wiki/Differential_privacy">differential privacy</a>, <a href="https://arxiv.org/abs/1802.07569">life-long learning</a>, and <a href="https://en.wikipedia.org/wiki/Fairness_(machine_learning)">fairness</a>. Differential privacy aims to guarantee that no particular training example has too large an influence on the trained model; a stronger goal compared to that of unlearning, which only requires erasing the influence of the designated forget set. Life-long learning research aims to design models that can learn continuously while maintaining previously-acquired skills. As work on unlearning progresses, it may also open additional ways to boost fairness in models, by correcting unfair biases or disparate treatment of members belonging to different groups (e.g., demographics, age groups, etc.).
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s720/image1.png" imageanchor="1"><img data-original-height="405" data-original-width="720" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s16000/image1.png"></a></td></tr><tr><td><b>Anatomy of unlearning.</b> An unlearning algorithm takes as input a pre-trained model and one or more samples from the train set to unlearn (the "forget set"). From the model, forget set, and retain set, the unlearning algorithm produces an updated model. An ideal unlearning algorithm produces a model that is indistinguishable from the model trained without the forget set.</td></tr></tbody></table>



<h2>Challenges of machine unlearning</h2>


<p>
The problem of unlearning is complex and multifaceted as it involves several conflicting objectives: forgetting the requested data, maintaining the model’s utility (e.g., accuracy on retained and held-out data), and efficiency. Because of this, existing unlearning algorithms make different trade-offs. For example, full retraining achieves successful forgetting without damaging model utility, but with poor efficiency, while <a href="https://arxiv.org/abs/2007.02923">adding noise</a> to the weights achieves forgetting at the expense of utility. 
</p>
<p>
Furthermore, the evaluation of forgetting algorithms in the literature has so far been highly inconsistent. While some <a href="https://arxiv.org/abs/1911.04933">works</a> report the classification accuracy on the samples to unlearn, <a href="https://proceedings.mlr.press/v119/wu20b.html">others</a> report distance to the fully retrained model, and yet others use the error rate of membership inference attacks as a metric for forgetting quality [<a href="https://arxiv.org/abs/2302.09880">4</a>, <a href="https://arxiv.org/abs/2010.10981">5</a>, <a href="https://arxiv.org/abs/2005.02205">6</a>].
</p>
<p>
We believe that the inconsistency of evaluation metrics and the lack of a standardized protocol is a serious impediment to progress in the field — we are unable to make direct comparisons between different unlearning methods in the literature. This leaves us with a myopic view of the relative merits and drawbacks of different approaches, as well as open challenges and opportunities for developing improved algorithms. To address the issue of inconsistent evaluation and to advance the state of the art in the field of machine unlearning, we've teamed up with a broad group of academic and industrial researchers to organize the first unlearning challenge.
</p>




<h2>Announcing the first Machine Unlearning Challenge</h2>


<p>
We are pleased to announce the <a href="https://unlearning-challenge.github.io/">first Machine Unlearning Challenge</a>, which will be held as part of the <a href="https://neurips.cc/Conferences/2023/CompetitionTrack">NeurIPS 2023 Competition Track.</a> The goal of the competition is twofold. First, by unifying and standardizing the evaluation metrics for unlearning, we hope to identify the strengths and weaknesses of different algorithms through apples-to-apples comparisons. Second, by opening this competition to everyone, we hope to foster novel solutions and shed light on open challenges and opportunities.
</p>
<p>
The competition will be hosted on <a href="https://www.kaggle.com/">Kaggle</a> and run between mid-July 2023 and mid-September 2023. As part of the competition, today we're announcing the availability of the <a href="https://github.com/unlearning-challenge/starting-kit">starting kit</a>. This starting kit provides a foundation for participants to build and test their unlearning models on a toy dataset.
</p>
<p>
The competition considers a realistic scenario in which an age predictor has been trained on face images, and, after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. For this, we will make available as part of the starting kit a dataset of synthetic faces (samples shown below) and we'll also use several real-face datasets for evaluation of submissions. The participants are asked to submit code that takes as input the trained predictor, the forget and retain sets, and outputs the weights of a predictor that has unlearned the designated forget set. We will evaluate submissions based on both the strength of the forgetting algorithm and model utility. We will also enforce a hard cut-off that rejects unlearning algorithms that run slower than a fraction of the time it takes to retrain. A valuable outcome of this competition will be to characterize the trade-offs of different unlearning algorithms.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s2000/image2.png" imageanchor="1"><img data-original-height="400" data-original-width="2000" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s16000/image2.png"></a></td></tr><tr><td>Excerpt images from the <a href="https://github.com/microsoft/FaceSynthetics">Face Synthetics</a> dataset together with age annotations. The competition considers the scenario in which an age predictor has been trained on face images like the above, and, after training, a certain subset of the training images must be forgotten.</td></tr></tbody></table>



<p>
For evaluating forgetting, we will use tools inspired by MIAs, such as <a href="https://arxiv.org/abs/2112.03570">LiRA</a>. MIAs were first developed in the privacy and security literature and their goal is to infer which examples were part of the training set. Intuitively, if unlearning is successful, the unlearned model contains no traces of the forgotten examples, causing MIAs to fail: the attacker would be <em>unable</em> to infer that the forget set was, in fact, part of the original training set. In addition, we will also use statistical tests to quantify how different the distribution of unlearned models (produced by a particular submitted unlearning algorithm) is compared to the distribution of models retrained from scratch. For an ideal unlearning algorithm, these two will be indistinguishable. 
</p>



<h2>Conclusion</h2>


<p>
Machine unlearning is a powerful tool that has the potential to address several open problems in machine learning. As research in this area continues, we hope to see new methods that are more efficient, effective, and responsible. We are thrilled to have the opportunity via this competition to spark interest in this field, and we are looking forward to sharing our insights and findings with the community.
</p>



<h2>Acknowledgements</h2>


<p>
<em>The authors of this post are now part of Google DeepMind. We are writing this blog post on behalf of the organization team of the Unlearning Competition: Eleni Triantafillou*, Fabian Pedregosa* (*equal contribution), Meghdad Kurmanji, Kairan Zhao, Gintare Karolina Dziugaite, Peter Triantafillou, Ioannis Mitliagkas, Vincent Dumoulin, Lisheng Sun Hosoya, Peter Kairouz, Julio C. S. Jacques Junior, Jun Wan, Sergio Escalera and Isabelle Guyon.</em>
</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You Suck at Excel (2015) [video] (185 pts)]]></title>
            <link>https://www.youtube.com/watch?v=0nbkaYsR94c</link>
            <guid>36649047</guid>
            <pubDate>Sat, 08 Jul 2023 21:41:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=0nbkaYsR94c">https://www.youtube.com/watch?v=0nbkaYsR94c</a>, See on <a href="https://news.ycombinator.com/item?id=36649047">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Open Letter to Tim O’Reilly to Free the Perl Camel (157 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36648949</link>
            <guid>36648949</guid>
            <pubDate>Sat, 08 Jul 2023 21:27:44 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36648949">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>This is (was?) O'Reilly's stance on the matter:<p><a href="https://web.archive.org/web/20180425080044/http://archive.oreilly.com/pub/a/oreilly/perl/usage" rel="nofollow noreferrer">https://web.archive.org/web/20180425080044/http://archive.or...</a></p><p>The Perl Camel Usage and Trademark Information</p><p>As most of you probably know, O'Reilly started putting animal images on the covers of our books about thirteen years ago. To millions of readers, the animals mean O'Reilly. They've become our signature "trade dress." We've also trademarked the association between particular animals and the subject of their books. After all, the only reason that people think of camels in association with Perl is because we used a camel on the cover of Programming Perl.</p><p>We recognize that things do get more complicated, though, when an image like the camel is so widely known that it comes to symbolize not just our products but also the entire Perl language. This is a good thing, and we want it to continue. But trademark law is sticky on this point. If a trademark isn't "protected" (by letters asking people not to use it, or by licenses that allow them to use it only in specific ways), it gets into the public domain and loses its protected status. If this happened, anyone could use the camel without restriction, including in ways that were detrimental to the language. For example, you might imagine a company creating a Perl-compatible language, branding it with a camel, and pushing it as the "official Perl" in an attempt to drive Larry Wall's Perl out of existence.</p><p>Another important issue is that a brand is strong in proportion to two things: its ubiquity and its distinctiveness. It's important that, just as we want one version of Perl (so we don't have the fragmentation that was the downfall of UNIX), we have one symbol for Perl. To protect the integrity and impact of that symbol, we need to maintain some artistic control over what kinds of camel images are used. We believe that "one camel" will strengthen the overall Perl brand.</p><p>In short, we're walking a fine line, trying to make the camel as available as possible as a symbol for Perl while protecting it as a trademark. So, here's our policy on using the camel image:</p><p>Non-commercial use</p><p>We will license the camel image widely for open source products and non-commercial sites related to Perl, requiring only an acknowledgement of its trademark status and a link to www.perl.com. To request the camel artwork, please send email to permissions@oreilly.com, indicating where, how, and for what purpose you plan to use the image. Please note that we generally do not allow alterations of the Perl camel artwork.</p><p>Some non-commercial sites currently using the Perl camel:</p><p>(snipped)</p><p>We also offer the Programming Republic of Perl logo for some non-commercial sites. Feel free to download these logos for use on your pages. Please make the logo a link to www.perl.com.</p><p>Some sites using the Programming Republic of Perl logo:</p><p>(snipped)</p><p>We may also license the Perl camel image for some commercial products and sites related to Perl. To inquire about the use of a camel image on any commercial product or site, please send email to permissions@oreilly.com with a description of the product or web site, indicating where and how you'd like to use the camel.</p><p>We've also created "Powered by Perl" buttons that any site using Perl may use on web pages. Feel free to download and use these buttons. Please make the buttons link to www.perl.com.</p><p>And the Camel FAQ:</p><p><a href="https://web.archive.org/web/20180123132933/http://archive.oreilly.com/pub/a/oreilly/perl/usage/faq.html" rel="nofollow noreferrer">https://web.archive.org/web/20180123132933/http://archive.or...</a></p><p>Q: So are you saying that O'Reilly has trademarked an entire animal?</p><p>A: No. When a company receives a trademark, it receives protection for a symbol in a particular category of products or services. For example, Owens Corning has trademarked the color pink. The whole color? No, only for insulation. O'Reilly has protected the camel image for books and online publications related to the Perl language, and related product and services. The only reason an association exists between camels and the Perl programming language is because we've used a camel image on our Perl-related products.</p><p>Q: Do you just own the particular Camel on the cover of Programming Perl, or all camels?</p><p>A: We own the particular camel image shown above, which has lead to an association between camels and the Perl language. If someone were to use a different camel on their Perl book, there could be confusion over which one "The Camel Book" referred to, and we might need to step in and stop use of that camel image. That's how trademarks work, helping to protect confusion in the marketplace.</p><p>Q: I want to design a T-shirt with the Perl camel on it. Do I need to get your permission?</p><p>A: Yes. But we're willing to make allowances for those of you who have creative ideas and want to do something fun for your friends. So, if the lifetime print run of the T-shirt design is less than 100, you may consider permission automatically granted. For larger print runs, please ask first. We promise to answer quickly!</p><p>Q: Why isn't your trademark just restricted to books?</p><p>A: We also do conferences, software, research, and online publishing in Perl, and we use the camel image for those things as well. We may want to camel-brand other Perl-related products in the future.</p><p>Q: I want to use $camel as a variable name in a Perl program. Do I need to acknowledge the trademark?</p><p>A: No.</p><p>Q: I want to use a cartoon camel as the logo for my software product. Is that okay?</p><p>A: It depends on what your product is, how it was developed, and how you intend to distribute it. Please send email to permissions@oreilly.com, with information about what you'd like to do, and we'll get back to you.</p><p>Q: I want to place a picture of a camel on my Perl web page. Am I allowed to do that? Do I have to use your camel?</p><p>A:Yes, as long as your page is non-commercial, and the context in which the camel is placed portrays Perl in a positive light. You will need to include the following language in small text somewhere on the page where the camel appears:</p><p>"The Perl camel image is a trademark of O'Reilly Media, Inc. Used with permission."</p><p>Please make the "O'Reilly Media, Inc." part of the statement a link to our home page (<a href="http://www.oreilly.com/" rel="nofollow noreferrer">http://www.oreilly.com</a>).</p><p>We'd encourage you to use the Perl camel we use, as it has wide recognition as "the Perl camel." But if you have another camel you'd like to use on a non-commercial site we generally would not object, so as long as the image is in no way derogatory.</p><p>Please note: If you use the "Powered by Perl" or the "Programming Republic of Perl" buttons, please make those active links to <a href="http://www.perl.com/" rel="nofollow noreferrer">http://www.perl.com</a>, not the O'Reilly home page.</p><p>Q: What is the Programming Republic of Perl logo?</p><p>A: The Programming Republic of Perl logo was developed some years ago for non-commercial use on web sites, and serves as a pointer to www.perl.com. Feel free to use it on any non-commercial pages. You can find it on the main Perl Camel Usage and Trademark Information page.</p><p>Q: Where can I find out more about camels?</p><p><a href="http://www.sandiegozoo.org/animalbytes/t-camel.html" rel="nofollow noreferrer">http://www.sandiegozoo.org/animalbytes/t-camel.html</a></p><p>If you have questions or comments about the Perl camel or any other O'Reilly trademarks, or if you want to use one of our trademarks in some way that we haven't explicitly described on this page, please send a detailed request to permissions@oreilly.com. For more information, see the Perl Camel FAQ.
              </p></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open sourcing the Nginx playground (181 pts)]]></title>
            <link>https://jvns.ca/blog/2023/07/08/open-sourcing-the-nginx-playground/</link>
            <guid>36648821</guid>
            <pubDate>Sat, 08 Jul 2023 21:11:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jvns.ca/blog/2023/07/08/open-sourcing-the-nginx-playground/">https://jvns.ca/blog/2023/07/08/open-sourcing-the-nginx-playground/</a>, See on <a href="https://news.ycombinator.com/item?id=36648821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
     

<p>Hello! In 2021 I released a small playground for testing nginx configurations
called <a href="https://nginx-playground.wizardzines.com/">nginx playground</a>. There’s a
<a href="https://jvns.ca/blog/2021/09/24/new-tool--an-nginx-playground/">blog post about it here</a>.</p>

<p>This is an extremely short post to say that at the time I didn’t make it open source,
but I am making it open source now. It’s not a lot of code but maybe it’ll be
interesting to someone.</p>

<p>Here’s <a href="https://github.com/jvns/nginx-playground/">the github repo</a>. The
frontend is in <code>static/</code> and the backend is in <code>api/</code>. The README is mostly an
extended apology for the developer experience and note that the project is
unmaintained. But I did test that the build instructions work!</p>

<h3 id="why-didn-t-i-open-source-this-before">why didn’t I open source this before?</h3>

<p>I’m not very good at open source. Some of the problems I have with open sourcing things are:</p>

<ul>
<li>I dislike (and am very bad at) maintaining open source projects – I usually
ignore basically all feature requests and most bug reports and then feel bad about it.
I handed off maintainership to both of the open source projects that I
started (<a href="https://github.com/rbspy/rbspy">rbspy</a> and <a href="https://github.com/rust-bpf/rust-bcc">rust-bcc</a>) to other people who are doing a MUCH better job than I ever did.</li>
<li>Sometimes the developer experience for the project is pretty bad</li>
<li>Sometimes there’s configuration in the project (like the <code>fly.toml</code> or the
analytics I have set up) which don’t really make sense for other people to
copy</li>
</ul>

<h3 id="new-approach-don-t-pretend-i-m-going-to-improve-it">new approach: don’t pretend I’m going to improve it</h3>

<p>In the past I’ve had some kind of belief that I’m going to improve the problems
with my code later. But I haven’t touched this project in more than a year and
I think it’s unlikely I’m going to go back to it unless it breaks in some dramatic way.</p>

<p>So instead of pretending I’m going to improve things, I decided to just:</p>

<ul>
<li>tell people in the README that the project is unmaintained</li>
<li>write down all the security caveats I know about</li>
<li>test the build instructions I wrote to make sure that they work (on a fresh machine, even!)</li>
<li>explain (but do not fix!!) some of the messy parts of the project</li>
</ul>

<h3 id="that-s-all">that’s all!</h3>

<p>Maybe I will open source more of my tiny projects in the future, we’ll see!
Thanks to <a href="https://www.changeset.nyc/">Sumana Harihareswara</a> for helping me
think through this.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is Htmx Gaining in Popularity? (139 pts)]]></title>
            <link>https://trends.builtwith.com/javascript/Htmx</link>
            <guid>36648817</guid>
            <pubDate>Sat, 08 Jul 2023 21:10:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://trends.builtwith.com/javascript/Htmx">https://trends.builtwith.com/javascript/Htmx</a>, See on <a href="https://news.ycombinator.com/item?id=36648817">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<div>
<div>
<p>
<label for="tk">Top 10k</label></p><svg width="25" height="20" style="margin-left: 0px;">
<g>
<line x1="5" y1="10" x2="35" y2="10"></line>
<circle style="opacity: 1" cx="15" cy="10" r="6"></circle>
</g></svg>
</div>
<div>
<p>
<label for="hk">Top 100k</label></p><svg width="25" height="20" style="margin-left: 0px;">
<g>
<line x1="5" y1="10" x2="35" y2="10"></line>
<circle style="opacity: 1" cx="15" cy="10" r="6"></circle>
</g></svg>
</div>
<div>
<p>
<label for="m">Top 1m</label></p><svg width="25" height="20" style="margin-left: 0px;">
<g>
<line x1="5" y1="10" x2="35" y2="10"></line>
<circle style="opacity: 1" cx="15" cy="10" r="6"></circle>
</g></svg>
</div>
<div>
<p>
<label for="ei">All Internet</label></p><svg width="25" height="20" style="margin-left: 0px;">
<g>
<line x1="5" y1="10" x2="35" y2="10"></line>
<circle style="opacity: 1" cx="15" cy="10" r="6"></circle>
</g></svg>
</div>
</div>

<div>
<div>
<p><img data-src="https://deo39crpw7zzn.cloudfront.net/thumb/0c-50-90-ed-cz-c1/n" alt="Htmx" width="48" height="48">
</p>
<div>
<p>Gives you access to AJAX, CSS Transitions, WebSockets and Server Sent Events directly in HTML, using attributes.</p>
<p><a href="https://htmx.org/" target="_blank" rel="nofollow noopener">https://htmx.org</a> </p>
<p><a href="https://trends.builtwith.com/javascript">JavaScript Libraries and Functions</a></p>
</div>
</div>
<div>
<h5>Htmx Customers</h5>
<p>
Get access to data on <a href="https://trends.builtwith.com/websitelist/Htmx/Historical">9,204 websites</a> that are Htmx Customers. We know of <a href="https://trends.builtwith.com/websitelist/Htmx">7,188 live websites</a> using Htmx and an additional 2,016 sites that used Htmx historically and <a href="https://trends.builtwith.com/websitelist/Htmx/Switzerland">178 websites in Switzerland</a>.</p>
<p>
<a href="https://trends.builtwith.com/websitelist/Htmx">
<svg>
<use xlink:href="#icon-arrow-alt-circle-down"></use></svg>
Download Lead List</a>
</p>
</div>
</div>
<div>
<div>
<p>
<h5><svg><use xlink:href="#icon-award"></use></svg>Htmx Awards</h5>
</p>
</div>
<div>
<div>
<ul>
</ul></div> <p>No awards yet.</p>
</div></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Build Personal ChatGPT Using Your Data (290 pts)]]></title>
            <link>https://github.com/raghavan/PdfGptIndexer</link>
            <guid>36648794</guid>
            <pubDate>Sat, 08 Jul 2023 21:07:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/raghavan/PdfGptIndexer">https://github.com/raghavan/PdfGptIndexer</a>, See on <a href="https://news.ycombinator.com/item?id=36648794">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">PdfGptIndexer</h2>
<h2 tabindex="-1" dir="auto">Description</h2>
<p dir="auto">PdfGptIndexer is an efficient tool for indexing and searching PDF text data using OpenAI's GPT-2 model and FAISS (Facebook AI Similarity Search). This software is designed for rapid information retrieval and superior search accuracy.</p>
<h2 tabindex="-1" dir="auto">Libraries Used</h2>
<ol dir="auto">
<li><a href="https://github.com/deanmalmgren/textract">Textract</a> - A Python library for extracting text from any document.</li>
<li><a href="https://github.com/huggingface/transformers">Transformers</a> - A library by Hugging Face providing state-of-the-art general-purpose architectures for Natural Language Understanding (NLU) and Natural Language Generation (NLG).</li>
<li><a href="https://python.langchain.com/" rel="nofollow">Langchain</a> - A text processing and embeddings library.</li>
<li><a href="https://github.com/facebookresearch/faiss">FAISS (Facebook AI Similarity Search)</a> - A library for efficient similarity search and clustering of dense vectors.</li>
</ol>
<h2 tabindex="-1" dir="auto">Installing Dependencies</h2>
<p dir="auto">You can install all dependencies by running the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install langchain openai textract transformers langchain faiss-cpu"><pre>pip install langchain openai textract transformers langchain faiss-cpu</pre></div>
<h2 tabindex="-1" dir="auto">How It Works</h2>
<p dir="auto">The PdfGptIndexer operates in several stages:</p>
<ol dir="auto">
<li>It first processes a specified folder of PDF documents, extracting the text and splitting it into manageable chunks using a GPT-2 tokenizer from the Transformers library.</li>
<li>Each text chunk is then embedded using the OpenAI GPT-2 model through the LangChain library.</li>
<li>These embeddings are stored in a FAISS index, providing a compact and efficient storage method.</li>
<li>Finally, a query interface allows you to retrieve relevant information from the indexed data by asking questions. The application fetches and displays the most relevant text chunk.</li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/131585/252057499-2e71dd82-bf4f-44db-b1ae-908cbb465deb.png"><img src="https://user-images.githubusercontent.com/131585/252057499-2e71dd82-bf4f-44db-b1ae-908cbb465deb.png" alt="Untitled-2023-06-16-1537"></a></p>
<h2 tabindex="-1" dir="auto">Advantages of Storing Embeddings Locally</h2>
<p dir="auto">Storing embeddings locally provides several advantages:</p>
<ol dir="auto">
<li>Speed: Once the embeddings are stored, retrieval of data is significantly faster as there's no need to compute embeddings in real-time.</li>
<li>Offline access: After the initial embedding creation, the data can be accessed offline.</li>
<li>Compute Savings: You only need to compute the embeddings once and reuse them, saving computational resources.</li>
<li>Scalability: This makes it feasible to work with large datasets that would be otherwise difficult to process in real-time.</li>
</ol>
<h2 tabindex="-1" dir="auto">Running the Program</h2>
<p dir="auto">To run the program, you should:</p>
<ol dir="auto">
<li>Make sure you have installed all dependencies.</li>
<li>Clone the repository to your local machine.</li>
<li>Navigate to the directory containing the Python script.</li>
<li>Replace "&lt;OPENAI_API_KEY&gt;" with your actual OpenAI API key in the script.</li>
<li>Finally, run the script with Python.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python3 pdf_gpt_indexer.py"><pre><span>python3</span> <span>pdf_gpt_indexer</span>.<span>py</span></pre></div>
<p dir="auto">Please ensure that the folders specified in the script for PDF documents and the output text files exist and are accessible. The query interface will start after the embeddings are computed and stored. You can exit the query interface by typing 'exit'.</p>
<h2 tabindex="-1" dir="auto">Exploring Custom Data with ChatGPT</h2>
<p dir="auto">Check out the post <a href="https://devden.raghavan.studio/p/chatgpt-using-your-own-data" rel="nofollow">here</a> for a comprehensive guide on how to utilize ChatGPT with your own custom data.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learn Electronics by Practice (323 pts)]]></title>
            <link>https://beletronics.wordpress.com/</link>
            <guid>36647364</guid>
            <pubDate>Sat, 08 Jul 2023 18:35:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://beletronics.wordpress.com/">https://beletronics.wordpress.com/</a>, See on <a href="https://news.ycombinator.com/item?id=36647364">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header>
<div>
<div>

<p>(residence of electronics enthusiasts)</p></div>


<nav aria-label="Primary" data-wp-interactive="" data-wp-context="{ &quot;core&quot;: { &quot;navigation&quot;: { &quot;isMenuOpen&quot;: { &quot;click&quot;: false, &quot;hover&quot;: false }, &quot;overlay&quot;: true, &quot;roleAttribute&quot;: &quot;&quot; } } }">
			<div id="modal-3" aria-label="Menu" data-wp-bind--aria-modal="selectors.core.navigation.isMenuOpen" data-wp-bind--role="selectors.core.navigation.roleAttribute" data-wp-effect="effects.core.navigation.initMenu" tabindex="-1" data-micromodal-close="" data-wp-class--has-modal-open="selectors.core.navigation.isMenuOpen" data-wp-class--is-menu-open="selectors.core.navigation.isMenuOpen" data-wp-on--keydown="actions.core.navigation.handleMenuKeydown" data-wp-on--focusout="actions.core.navigation.handleMenuFocusout">
							<ul><li><a href="https://beletronics.wordpress.com/mission/"><span>Mission</span></a></li><li><a href="https://beletronics.wordpress.com/about/"><span>About</span></a></li></ul>
						</div></nav></div>




</header>


<p><h2>Learn electronics by&nbsp;practice</h2></p>



<main>

<div>
<p>This website is dedicated to electronics enthusiasts and aspirants who believe that true knowledge comes through the persistence of constant practice.</p>



<p>The content is divided into parts presented hereafter. </p>



<figure><a href="https://beletronics.wordpress.com/learn-basic-electronics/"><img decoding="async" data-attachment-id="52" data-permalink="https://beletronics.wordpress.com/home/basic/" data-orig-file="https://beletronics.files.wordpress.com/2022/05/basic.png" data-orig-size="1497,729" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="basic" data-image-description="" data-image-caption="" data-medium-file="https://beletronics.files.wordpress.com/2022/05/basic.png?w=300" data-large-file="https://beletronics.files.wordpress.com/2022/05/basic.png?w=1024" src="https://beletronics.files.wordpress.com/2022/05/basic.png?w=1024" alt="Learn basic electronics" width="498" height="243" srcset="https://beletronics.files.wordpress.com/2022/05/basic.png?w=498 498w, https://beletronics.files.wordpress.com/2022/05/basic.png?w=996 996w, https://beletronics.files.wordpress.com/2022/05/basic.png?w=150 150w, https://beletronics.files.wordpress.com/2022/05/basic.png?w=300 300w, https://beletronics.files.wordpress.com/2022/05/basic.png?w=768 768w" sizes="(max-width: 498px) 100vw, 498px"></a><figcaption><a href="https://beletronics.wordpress.com/learn-basic-electronics/">Learn basic electronics – click to read</a></figcaption></figure>



<figure><a href="https://beletronics.wordpress.com/pong-game/"><img decoding="async" width="3660" height="1275" data-attachment-id="1345" data-permalink="https://beletronics.wordpress.com/home/pong-2/" data-orig-file="https://beletronics.files.wordpress.com/2022/05/pong-2.png" data-orig-size="3660,1275" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pong-2" data-image-description="" data-image-caption="" data-medium-file="https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=300" data-large-file="https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=1024" src="https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=1024" alt="" srcset="https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=1024 1024w, https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=2048 2048w, https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=150 150w, https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=300 300w, https://beletronics.files.wordpress.com/2022/05/pong-2.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption><a href="https://beletronics.wordpress.com/pong-game/">Create a legendary Pong game – click to read</a></figcaption></figure>



<figure><a href="https://beletronics.wordpress.com/cp4u-processor/"><img decoding="async" data-attachment-id="2594" data-permalink="https://beletronics.wordpress.com/home/cp4u/" data-orig-file="https://beletronics.files.wordpress.com/2022/06/cp4u.png" data-orig-size="2494,1636" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cp4u" data-image-description="" data-image-caption="" data-medium-file="https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=300" data-large-file="https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=1024" src="https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=1024" alt="" width="475" height="311" srcset="https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=475 475w, https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=948 948w, https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=150 150w, https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=300 300w, https://beletronics.files.wordpress.com/2022/06/cp4u.png?w=768 768w" sizes="(max-width: 475px) 100vw, 475px"></a><figcaption><a href="https://beletronics.wordpress.com/cp4u-processor/">Create a 4-bit processor – click to read</a></figcaption></figure>



<figure><a href="https://beletronics.wordpress.com/z80-computer/"><img decoding="async" data-attachment-id="1841" data-permalink="https://beletronics.wordpress.com/home/z80/" data-orig-file="https://beletronics.files.wordpress.com/2022/05/z80.jpg" data-orig-size="4007,2093" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone XS Max&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1653660645&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.25&quot;,&quot;iso&quot;:&quot;320&quot;,&quot;shutter_speed&quot;:&quot;0.016666666666667&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;,&quot;latitude&quot;:&quot;45.812958333333&quot;,&quot;longitude&quot;:&quot;15.942352777778&quot;}" data-image-title="z80" data-image-description="" data-image-caption="" data-medium-file="https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=300" data-large-file="https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=1024" src="https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=1024" alt="" width="445" height="232" srcset="https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=445 445w, https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=888 888w, https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=150 150w, https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=300 300w, https://beletronics.files.wordpress.com/2022/05/z80.jpg?w=768 768w" sizes="(max-width: 445px) 100vw, 445px"></a><figcaption><a href="https://beletronics.wordpress.com/z80-computer/">Create a complete Z80 computer – click to read</a></figcaption></figure>
			
			
			</div></main>



<div>
<!-- You can start editing here. -->

	<h3 id="comments">
		6 responses to “Learn electronics by&nbsp;practice”	</h3>

	

	<ol>
			<li id="comment-2">
			<article id="div-comment-2">
				<!-- .comment-meta -->

				<div>
					<p>Very nice 🙂</p>
<p id="comment-like-2" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=2&amp;_wpnonce=9ed7fde271" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-2">Liked by <a href="#" data-like-count="1">1 person</a></span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=2#respond" data-commentid="2" data-postid="36" data-belowelement="div-comment-2" data-respondelement="respond" data-replyto="Reply to Heidobito" aria-label="Reply to Heidobito">Reply</a></p></div>			</article><!-- .comment-body -->
		<ul>
		<li id="comment-3">
			<article id="div-comment-3">
				<!-- .comment-meta -->

				<div>
					<p>Thank you!</p>
<p id="comment-like-3" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=3&amp;_wpnonce=08fa246496" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-3">Like</span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=3#respond" data-commentid="3" data-postid="36" data-belowelement="div-comment-3" data-respondelement="respond" data-replyto="Reply to daniel bele" aria-label="Reply to daniel bele">Reply</a></p></div>			</article><!-- .comment-body -->
		</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li id="comment-11">
			<article id="div-comment-11">
				<!-- .comment-meta -->

				<div>
					<p>Respect!<br>
Thank you for your effort and enthusiasm!</p>
<p id="comment-like-11" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=11&amp;_wpnonce=f04ba9290e" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-11">Liked by <a href="#" data-like-count="1">1 person</a></span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=11#respond" data-commentid="11" data-postid="36" data-belowelement="div-comment-11" data-respondelement="respond" data-replyto="Reply to Tomislav Valecic" aria-label="Reply to Tomislav Valecic">Reply</a></p></div>			</article><!-- .comment-body -->
		<ul>
		<li id="comment-12">
			<article id="div-comment-12">
				<!-- .comment-meta -->

				<div>
					<p>Thank you Tomislav!</p>
<p id="comment-like-12" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=12&amp;_wpnonce=ddb5fad259" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-12">Like</span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=12#respond" data-commentid="12" data-postid="36" data-belowelement="div-comment-12" data-respondelement="respond" data-replyto="Reply to daniel bele" aria-label="Reply to daniel bele">Reply</a></p></div>			</article><!-- .comment-body -->
		</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li id="comment-13">
			<article id="div-comment-13">
				<!-- .comment-meta -->

				<div>
					<p>Thank you, Bele! You’re the best.</p>
<p id="comment-like-13" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=13&amp;_wpnonce=0f7e0dc91d" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-13">Like</span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=13#respond" data-commentid="13" data-postid="36" data-belowelement="div-comment-13" data-respondelement="respond" data-replyto="Reply to Danko Kozar" aria-label="Reply to Danko Kozar">Reply</a></p></div>			</article><!-- .comment-body -->
		<ul>
		<li id="comment-14">
			<article id="div-comment-14">
				<!-- .comment-meta -->

				<div>
					<p>Thank you Danko. Means a lot, especially from you!</p>
<p id="comment-like-14" data-liked="comment-not-liked"><a href="https://beletronics.wordpress.com/?like_comment=14&amp;_wpnonce=f8e8a70822" rel="nofollow" data-blog="206281479"><span>Like</span></a><span id="comment-like-count-14">Like</span></p>
				</div><!-- .comment-content -->

				<div><p><a rel="nofollow" href="https://beletronics.wordpress.com/?replytocom=14#respond" data-commentid="14" data-postid="36" data-belowelement="div-comment-14" data-respondelement="respond" data-replyto="Reply to daniel bele" aria-label="Reply to daniel bele">Reply</a></p></div>			</article><!-- .comment-body -->
		</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
	</ol>

	

	<div id="respond">
		<h3 id="reply-title">Leave a Reply <small></small></h3><form action="https://beletronics.wordpress.com/wp-comments-post.php" method="post" id="commentform" novalidate="">


<div>
	<p><label for="comment">Enter your comment here…</label></p>
</div>

		<div id="comment-form-identity">
	<div id="comment-form-nascar">
		<p>Fill in your details below or click an icon to log in:</p>
		<ul>
			
			<li>
				<a href="#comment-form-load-service:WordPress.com" id="postas-wordpress" title="Login via WordPress.com">
					<svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#0087be" d="M12.158 12.786l-2.698 7.84c.806.236 1.657.365 2.54.365 1.047 0 2.05-.18 2.986-.51-.024-.037-.046-.078-.065-.123l-2.762-7.57zM3.008 12c0 3.56 2.07 6.634 5.068 8.092L3.788 8.342c-.5 1.117-.78 2.354-.78 3.658zm15.06-.454c0-1.112-.398-1.88-.74-2.48-.456-.74-.883-1.368-.883-2.11 0-.825.627-1.595 1.51-1.595.04 0 .078.006.116.008-1.598-1.464-3.73-2.36-6.07-2.36-3.14 0-5.904 1.613-7.512 4.053.21.008.41.012.58.012.94 0 2.395-.114 2.395-.114.484-.028.54.684.057.74 0 0-.487.058-1.03.086l3.275 9.74 1.968-5.902-1.4-3.838c-.485-.028-.944-.085-.944-.085-.486-.03-.43-.77.056-.742 0 0 1.484.114 2.368.114.94 0 2.397-.114 2.397-.114.486-.028.543.684.058.74 0 0-.488.058-1.03.086l3.25 9.665.897-2.997c.456-1.17.684-2.137.684-2.907zm1.82-3.86c.04.286.06.593.06.924 0 .912-.17 1.938-.683 3.22l-2.746 7.94c2.672-1.558 4.47-4.454 4.47-7.77 0-1.564-.4-3.033-1.1-4.314zM12 22C6.486 22 2 17.514 2 12S6.486 2 12 2s10 4.486 10 10-4.486 10-10 10z"></path></g></svg>				</a>
			</li>
			<li>
				<a href="#comment-form-load-service:Facebook" id="postas-facebook" title="Login via Facebook">
					<svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#3B5998" d="M20.007 3H3.993C3.445 3 3 3.445 3 3.993v16.013c0 .55.445.994.993.994h8.62v-6.97H10.27V11.31h2.346V9.31c0-2.325 1.42-3.59 3.494-3.59.993 0 1.847.073 2.096.106v2.43h-1.438c-1.128 0-1.346.537-1.346 1.324v1.734h2.69l-.35 2.717h-2.34V21h4.587c.548 0 .993-.445.993-.993V3.993c0-.548-.445-.993-.993-.993z"></path></g></svg>				</a>
			</li>
		</ul>
	</div>

	<div id="comment-form-guest">
			<p><a href="https://gravatar.com/site/signup/" target="_blank">				<img src="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G" srcset="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G 1x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=38&amp;d=identicon&amp;forcedefault=y&amp;r=G 1.5x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=50&amp;d=identicon&amp;forcedefault=y&amp;r=G 2x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=75&amp;d=identicon&amp;forcedefault=y&amp;r=G 3x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=100&amp;d=identicon&amp;forcedefault=y&amp;r=G 4x" alt="Gravatar" width="25">
</a>			</p>

				<div>
				<div>
					<p><label for="email">Email <span>(required)</span> <span>(Address never made public)</span></label></p>
				</div>
				<div>
					<p><label for="author">Name <span>(required)</span></label></p>
				</div>
				<div>
					<p><label for="url">Website</label></p>
				</div>
			</div>
			
		</div>

	<div id="comment-form-wordpress">
			<p><img src="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G" srcset="https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=25&amp;d=identicon&amp;forcedefault=y&amp;r=G 1x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=38&amp;d=identicon&amp;forcedefault=y&amp;r=G 1.5x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=50&amp;d=identicon&amp;forcedefault=y&amp;r=G 2x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=75&amp;d=identicon&amp;forcedefault=y&amp;r=G 3x, https://1.gravatar.com/avatar/ad516503a11cd5ca435acc9bb6523536?s=100&amp;d=identicon&amp;forcedefault=y&amp;r=G 4x" alt="WordPress.com Logo" width="25">
			</p>

				<div>
				<p>
			<strong></strong>
			You are commenting using your WordPress.com account.			<span>
				(&nbsp;Log&nbsp;Out&nbsp;/&nbsp;
				<a href="#" onclick="javascript:HighlanderComments.switchAccount();return false;">Change</a>&nbsp;)
			</span>
			<span><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#0087be" d="M12.158 12.786l-2.698 7.84c.806.236 1.657.365 2.54.365 1.047 0 2.05-.18 2.986-.51-.024-.037-.046-.078-.065-.123l-2.762-7.57zM3.008 12c0 3.56 2.07 6.634 5.068 8.092L3.788 8.342c-.5 1.117-.78 2.354-.78 3.658zm15.06-.454c0-1.112-.398-1.88-.74-2.48-.456-.74-.883-1.368-.883-2.11 0-.825.627-1.595 1.51-1.595.04 0 .078.006.116.008-1.598-1.464-3.73-2.36-6.07-2.36-3.14 0-5.904 1.613-7.512 4.053.21.008.41.012.58.012.94 0 2.395-.114 2.395-.114.484-.028.54.684.057.74 0 0-.487.058-1.03.086l3.275 9.74 1.968-5.902-1.4-3.838c-.485-.028-.944-.085-.944-.085-.486-.03-.43-.77.056-.742 0 0 1.484.114 2.368.114.94 0 2.397-.114 2.397-.114.486-.028.543.684.058.74 0 0-.488.058-1.03.086l3.25 9.665.897-2.997c.456-1.17.684-2.137.684-2.907zm1.82-3.86c.04.286.06.593.06.924 0 .912-.17 1.938-.683 3.22l-2.746 7.94c2.672-1.558 4.47-4.454 4.47-7.77 0-1.564-.4-3.033-1.1-4.314zM12 22C6.486 22 2 17.514 2 12S6.486 2 12 2s10 4.486 10 10-4.486 10-10 10z"></path></g></svg></span>
		</p>
					</div>
	
		</div>

	<div id="comment-form-facebook">
			<p><img src="" alt="Facebook photo" width="25">
			</p>

				<div>
				<p>
			<strong></strong>
			You are commenting using your Facebook account.			<span>
				(&nbsp;Log&nbsp;Out&nbsp;/&nbsp;
				<a href="#" onclick="javascript:HighlanderComments.switchAccount();return false;">Change</a>&nbsp;)
			</span>
			<span><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#3B5998" d="M20.007 3H3.993C3.445 3 3 3.445 3 3.993v16.013c0 .55.445.994.993.994h8.62v-6.97H10.27V11.31h2.346V9.31c0-2.325 1.42-3.59 3.494-3.59.993 0 1.847.073 2.096.106v2.43h-1.438c-1.128 0-1.346.537-1.346 1.324v1.734h2.69l-.35 2.717h-2.34V21h4.587c.548 0 .993-.445.993-.993V3.993c0-.548-.445-.993-.993-.993z"></path></g></svg></span>
		</p>
					</div>
	
		</div>


	<div id="comment-form-load-service">
		<div><p>Cancel</p></div>
		<p>Connecting to %s</p>
	</div>

</div>



<div id="comment-form-subscribe">
	<p> <label id="subscribe-label" for="subscribe">Notify me of new comments via email.</label></p><p> <label id="subscribe-blog-label" for="subscribe_blog">Notify me of new posts via email.</label></p></div>

	






</form>	</div><!-- #respond -->
	</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Container Training (155 pts)]]></title>
            <link>https://container.training/</link>
            <guid>36647211</guid>
            <pubDate>Sat, 08 Jul 2023 18:20:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://container.training/">https://container.training/</a>, See on <a href="https://news.ycombinator.com/item?id=36647211">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <table>
      <tbody><tr><td colspan="3">Container Training</td></tr>
      <tr><td colspan="3">Note: while some workshops are delivered in other languages, slides are always in English.</td></tr>

      <tr><td colspan="3">Free Kubernetes intro course</td></tr>

      <tr>
      	<td>Getting Started With Kubernetes and Container Orchestration</td>
      	<td><a href="https://qconuk2019.container.training/"></a></td>
      	<td><a href="https://www.youtube.com/playlist?list=PLBAFXs0YjviJwCoxSUkUPhsSxDJzpZbJd"></a></td>
      </tr>
      <tr>
        <td>This is a live recording of a 1-day workshop that took place at QCON London in March 2019.</td>
      </tr>
      <tr>
        <td>If you're interested, we can deliver that workshop (or longer courses) to your team or organization.</td>
      </tr>
      <tr>
        <td>Contact <a href="mailto:jerome.petazzoni@gmail.com">Jérôme Petazzoni</a> to make that happen!</td>
      </tr>

      

      
        <tr><td colspan="3">Past workshops</td></tr>

        
          <tr>
            <td>Opérer Kubernetes (en français)</td>
            <td>
              
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Delivered November 18th-19th, 2021 at ENIX SAS in streaming.</td>
          </tr>

        
          <tr>
            <td>Kubernetes avancé (en français)</td>
            <td>
              
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Delivered November 8th-16th, 2021 at ENIX SAS in streaming.</td>
          </tr>

        
          <tr>
            <td>Packaging et CI/CD pour Kubernetes (en français)</td>
            <td>
              
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Delivered October 11th-12th, 2021 at ENIX SAS in streaming.</td>
          </tr>

        
          <tr>
            <td>Fondamentaux Kubernetes (en français)</td>
            <td>
              
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Delivered October 4th-7th, 2021 at ENIX SAS in streaming.</td>
          </tr>

        
          <tr>
            <td>Docker intensif (en français)</td>
            <td>
              
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Delivered September 27th-29th, 2021 at ENIX SAS in streaming.</td>
          </tr>

        

        
          <tr>
            <td>... and at least <a href="https://container.training/past.html">111 more</a>.</td>
          </tr>
        
      

      
        <tr><td colspan="3">Recorded workshops</td></tr>

        
          <tr>
            <td>Getting started with Kubernetes and container orchestration</td>
            <td><a href="https://pycon2019.container.training/"></a></td>
            <td><a href="https://www.youtube.com/watch?v=J08MrW2NC1Y"></a></td>
          </tr>
          <tr>
            <td>Delivered May 1st, 2019 at PyCon in Cleveland, OH.</td>
          </tr>
        
          <tr>
            <td>Getting Started With Kubernetes and Container Orchestration</td>
            <td><a href="https://qconuk2019.container.training/"></a></td>
            <td><a href="https://www.youtube.com/playlist?list=PLBAFXs0YjviJwCoxSUkUPhsSxDJzpZbJd"></a></td>
          </tr>
          <tr>
            <td>Delivered March 8th, 2019 at QCON in London.</td>
          </tr>
        
          <tr>
            <td>Introduction to Docker and Containers</td>
            <td><a href="http://qconsf2017intro.container.training/"></a></td>
            <td><a href="https://www.youtube.com/playlist?list=PLBAFXs0YjviLgqTum8MkspG_8VzGl6C07"></a></td>
          </tr>
          <tr>
            <td>Delivered November 16th, 2017 at QCON SF in San Francisco, CA.</td>
          </tr>
        
          <tr>
            <td>Deploying and scaling microservices with Docker and Kubernetes</td>
            <td><a href="http://osseu17.container.training/"></a></td>
            <td><a href="https://www.youtube.com/playlist?list=PLBAFXs0YjviLrsyydCzxWrIP_1-wkcSHS"></a></td>
          </tr>
          <tr>
            <td>Delivered October 26th, 2017 at Open Source Summit Europe in Prague.</td>
          </tr>
        
          <tr>
            <td>Deploying &amp; Scaling microservices with Docker Swarm</td>
            <td><a href=""></a></td>
            <td><a href="https://www.youtube.com/watch?v=DABbqyJeG_E"></a></td>
          </tr>
          <tr>
            <td>Delivered July 25th, 2017 at devopsdays in Minneapolis, MN.</td>
          </tr>
        
          <tr>
            <td>Deploy and scale containers with Docker native, open source orchestration</td>
            <td><a href=""></a></td>
            <td><a href="https://www.youtube.com/watch?v=EuzoEaE6Cqs"></a></td>
          </tr>
          <tr>
            <td>Delivered May 18th, 2017 at PyCon in Portland, OR.</td>
          </tr>
        
          <tr>
            <td>Deploying and Scaling Applications with Docker Swarm</td>
            <td><a href="http://lisa16t1.container.training/"></a></td>
            <td><a href="https://www.youtube.com/playlist?list=PLBAFXs0YjviIDDhr8vIwCN1wkyNGXjbbc"></a></td>
          </tr>
          <tr>
            <td>Delivered December 6th, 2016 at LISA in Boston, MA.</td>
          </tr>
        
          <tr>
            <td>Introduction to Docker and containers</td>
            <td><a href="https://us.pycon.org/2016/site_media/media/tutorial_handouts/DockerSlides.pdf"></a></td>
            <td><a href="https://www.youtube.com/watch?v=ZVaRK10HBjo"></a></td>
          </tr>
          <tr>
            <td>Delivered May 29th, 2016 at PyCon in Portland, OR.</td>
          </tr>
        
      

      
        <tr><td colspan="3">Self-paced tutorials</td></tr>
        
          <tr>
            <td>Introduction to Docker and Containers</td>
            <td><a href="https://container.training/intro-selfpaced.yml.html"></a></td>
          </tr>
        
          <tr>
            <td>Container Orchestration with Docker and Swarm</td>
            <td><a href="https://container.training/swarm-selfpaced.yml.html"></a></td>
          </tr>
        
          <tr>
            <td>Deploying and Scaling Microservices with Docker and Kubernetes</td>
            <td><a href="https://container.training/kube-selfpaced.yml.html"></a></td>
          </tr>
        
      

      

      <tr><td></td></tr>

      <tr>
        <td>
          Maintained by Jérôme Petazzoni (<a href="https://twitter.com/jpetazzo">@jpetazzo</a>) and <a href="https://github.com/jpetazzo/container.training/graphs/contributors">contributors</a>.
        </td>
      </tr>
    </tbody></table>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Use Pascal? (263 pts)]]></title>
            <link>https://castle-engine.io/why_pascal</link>
            <guid>36646890</guid>
            <pubDate>Sat, 08 Jul 2023 17:54:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://castle-engine.io/why_pascal">https://castle-engine.io/why_pascal</a>, See on <a href="https://news.ycombinator.com/item?id=36646890">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<h3 id="_modern_clean_language_to_develop_maintainable_applications">2.1. Modern clean language to develop maintainable applications</h3>
<div>
<ul>
<li>
<p>Object Pascal is a modern programming language. It supports classes, units, properties, generics, interfaces, reflection, closures…​ Everything you expect from a modern OOP language.</p>
</li>
<li>
<p>The syntax puts emphasis on readable code.</p>
</li>
<li>
<p>The language is type-safe. E.g. special types for booleans, strings, chars, sets, enums, ranges. Type conversions are either really safe, or have to be done explicitly.</p>
</li>
<li>
<p>There are additional run-time checks, e.g. array range checking, integer overflow checking, assertions, memory leak checking. Notes:</p>
<div>
<ul>
<li>
<p>You can turn off these checks in <em>release</em> version, but use them in <em>debug</em>. When compiling using CGE build tool / editor, we have debug / release modes that automatically do this for you.</p>
</li>
<li>
<p><a href="https://github.com/michaliskambi/modern-pascal-introduction/wiki/What-are-range-and-overflow-checks-(and-errors)-in-Pascal">What are range and overflow checks (and errors) in Pascal</a></p>
</li>
<li>
<p><a href="https://castle-engine.io/detecting_memory_leaks_using_heaptrc">Detecting Memory Leaks</a></p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div>
<h3 id="_fast">2.2. Fast</h3>
<div>
<ul>
<li>
<p>It is compiled to a native code and so is fast <em>"out of the box"</em>. There’s seldom any need to do low-level optimizations.</p>
</li>
<li>
<p>But if you need to, language can be as low-level as you want. E.g. you can use pointers, do pointer math, write OS and CPU-specific code, even add pieces in assembly. You can work on the same level as C or C++ does.</p>
<div>
<table>
<tbody><tr>
<td>
<p>Note</p>
</td>
<td>
But you will probably not need to get too "low level" in usual applications. E.g. <a href="https://castle-engine.io/">Castle Game Engine</a> has <strong>zero assembler code</strong> to maximize portability and code readability and we’re still fast.
</td>
</tr>
</tbody></table>
</div>
</li>
<li>
<p>Compilation is also fast.</p>
<div>
<p><img src="https://castle-engine.io/images/not_resized/pascal-fast-compilation.webp" alt="pascal fast compilation">
</p>
</div>
<p>2.5 seconds to get desktop build, 10.1 seconds to get Android build <strong>of a new project, opened for the 1st time</strong>. Try to match that with your engine :)</p>
</li>
</ul>
</div>
</div>
<div>
<h3 id="_cross_platform">2.3. Cross-platform</h3>
<div>
<ul>
<li>
<p>Desktop (Windows, Linux, macOS, Raspberry Pi, FreeBSD, probably every Unix…​),</p>
</li>
<li>
<p>mobile (Android, iOS),</p>
</li>
<li>
<p>consoles (Nintendo Switch, special in CGE),</p>
</li>
<li>
<p>web (both WebAssembly and JS (using pas2js)).</p>
</li>
</ul>
</div>

</div>
<div>
<h3 id="_welcoming">2.4. Welcoming</h3>
<div>
<ul>
<li>
<p>In <a href="https://castle-engine.io/">Castle Game Engine</a> case, engine code and game code are in the same language. Every user is contributor!</p>
</li>
<li>
<p>And the engine is open-source.</p>
</li>
</ul>
</div>
<p>Don’t hesitate to fork CGE to adjust it to your needs.</p>
</div>
<div>
<h3 id="_general_purpose">2.5. General purpose</h3>
<p>There are existing libraries (units) in Pascal for everything:</p>
<div>
<ul>
<li>
<p>database</p>
</li>
<li>
<p>XML, JSON</p>
</li>
<li>
<p>A.I.</p>
</li>
<li>
<p>blockchain</p>
</li>
<li>
<p>networking</p>
</li>
</ul>
</div>
<p>Moreover you can easily integrate with (link to) any existing library with C API. Any renderer, sound library, physics - we can use everything.</p>

</div>
<div>
<h3 id="_ecosystem_of_tools">2.6. Ecosystem of tools</h3>
<div>
<ul>
<li>
<p><a href="https://www.freepascal.org/">FPC</a> - Free Pascal Compiler, open-source.</p>
</li>
<li>
<p><a href="https://www.lazarus-ide.org/">Lazarus</a> - IDE for Pascal, on top of FPC, also open-source.</p>
</li>
<li>
<p><a href="https://www.embarcadero.com/products/Delphi">Delphi</a> - commercial compiler and IDE for Pascal.</p>
</li>
<li>
<p><a href="https://castle-engine.io/vscode">VS Code</a> support - CGE, as well as many others in the Pascal ecosystem, explicitly support integration with VS Code.</p>
</li>
</ul>
</div>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How and Why I Stopped Buying New Laptops (2020) (316 pts)]]></title>
            <link>https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/</link>
            <guid>36646791</guid>
            <pubDate>Sat, 08 Jul 2023 17:47:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/">https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/</a>, See on <a href="https://news.ycombinator.com/item?id=36646791">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>
<div id="content"><div>
<figure data-imgstate="dither">
<img alt="Image: Low-tech Magazine is now written and published on a 2006 ThinkPad X60s." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/X60-on-its-side-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/X60-on-its-side-white_hu753fd43d283e60bf1aefb8ea5c1440fe_3642029_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/X60-on-its-side-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: Low-tech Magazine is now written and published on a 2006 ThinkPad X60s. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<p>Being an independent journalist – or an office worker if you wish – I always reasoned that I needed a decent computer and that I need to pay for quality. Between 2000 and 2017, I consumed three laptops that I bought new and which cost me around 5,000 euros in total – roughly 300 euros per year over the entire period. The average useful life of my three laptops was 5.7 years.</p>
<p>In 2017, somewhere between getting <a href="https://solar.lowtechmagazine.com/2018/09/how-to-build-a-low-tech-website/">my office</a>, I decided not to buy any more new laptops. Instead, I switched to a 2006 second-hand machine that I purchased online for 50 euros and which does everything that I want and need. Including a new battery and a simple hardware upgrade, I invested less than 150 euros.</p>
<p>If my 2006 laptop lasts as long as my other machines – if it runs for another 1.7 years – it will have cost me only 26 euros per year. That’s more than 10 times less than the cost of my previous laptops. In this article, I explain my motivations for not buying new laptops, and how you could do the same.</p>
<h2 id="energy-and-material-use-of-a-laptop">Energy and material use of a laptop</h2>
<p>Not buying new laptops saves a lot of money, but also a lot of resources and environmental destruction. According to the most recent life cycle analysis, it takes 3,010 to 4,340 megajoules of primary energy to make a laptop – this includes mining the materials, manufacturing the machine, and bringing it to market. <sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p>
<p>Each year, we purchase between 160 and 200 million laptops. Using the data above, this means that the production of laptops requires a yearly energy consumption of 480 to 868 petajoules, which corresponds to between one quarter and almost half of all solar PV energy produced worldwide in 2018 (2,023 petajoules). <sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> The making of a laptop also involves a high material consumption, which includes a wide variety of minerals that may be considered scarce due to different types of constraints: economic, social, geochemical, and geopolitical. <sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup><sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></p>
<p>The <a href="https://solar.lowtechmagazine.com/2009/06/the-monster-footprint-of-digital-technology/">production of microchips is a very energy- and material-intensive process</a>, but that is not the only problem. The high resource use of laptops is also because they have a very short lifespan. Most of the 160-200 million laptops sold each year are replacement purchases. The average laptop is replaced every 3 years (in business) to five years (elsewhere). <sup id="fnref1:3"><a href="#fn:3" role="doc-noteref">3</a></sup> My 5.7 years per laptop experience is not exceptional.</p>
<h2 id="laptops-dont-change">Laptops don’t change</h2>
<p>The study cited dates from 2011, and it refers to a machine made in 2001: a Dell Inspiron 2500.  You are forgiven for thinking that this “most recent life cycle analysis of a laptop” is outdated, but it’s not. A 2015 research paper discovered that the embodied energy of laptops is static over time. <sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup></p>
<p>The scientists disassembled 11 laptops of similar size, made between 1999 and 2008, and weighed the different components. Also, they measured the silicon die area for all motherboards and 30 DRAM cards produced over roughly the same period (until 2011). They found that the mass and material composition of all key components – battery, motherboard, hard drive, memory – did not change significantly, even though manufacturing processes became more efficient in energy and material use.</p>
<p>The reason is simple: improvements in functionality balance the efficiency gains obtained in the manufacturing process. Battery mass, memory, and hard disk drive mass decreased per unit of functionality but showed roughly constant totals per year. The same dynamic explains why newer laptops don’t show lower operational electricity consumption compared to older laptops. New laptops may be more energy-efficient per computational power, but these gains are offset by more computational power. <a href="https://solar.lowtechmagazine.com/2018/01/bedazzled-by-energy-efficiency/">Jevon’s paradox</a> is nowhere as evident as it is in computing.</p>
<h2 id="the-challenge">The challenge</h2>
<p>All this means that there’s no environmental or financial benefit whatsoever to replacing an old laptop with a new one. On the contrary, the only thing a consumer can do to improve their laptop’s ecological and economic sustainability is to use it for as long as possible. This is facilitated by the fact that laptops are now a mature technology and have more than sufficient computational power. One problem, though. Consumers who try to keep working on their old laptops are likely to end up frustrated. I shortly explain my frustrations below, and I’m pretty confident that they are not exceptional.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: The three new laptops I used from 2000 to 2017." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/3-laptops-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/3-laptops-white_hud91339aa34e3ccb38ac1a26db4506f2c_3604453_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/3-laptops-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: The three new laptops I used from 2000 to 2017. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="my-first-laptop-apple-ibook-2000-2005">My first laptop: Apple iBook (2000-2005)</h2>
<p>In 2000, when I was working as a freelance science and tech journalist in Belgium, I bought my first laptop, an Apple iBook. Little more than two or three years later, the charger started malfunctioning. When informed of the price for a new charger, I was so disgusted with Apple’s sales practices – chargers are very cheap to produce, but Apple sold them for a lot of money – that I refused to buy it. Instead, I managed to keep the charger working for a few more years, first by putting it under the weight of books and furniture, and when that didn’t work anymore, by putting it in a firmly tightened clamp.</p>
<h2 id="my-second-laptop-ibm-thinkpad-r52-2005-2013">My second laptop: IBM ThinkPad R52 (2005-2013)</h2>
<p>When the charger eventually died entirely in 2005, I decided to look for a new laptop. I had only one demand: it should have a charger that lasts or is at least cheap to replace. I found more than I was looking for. I bought an <a href="http://www.thinkwiki.org/wiki/Category:R52" target="_blank">IBM Thinkpad R52</a>, and it was love at first use. My IBM laptop was the Apple iBook counterpart, not just in terms of design (a rectangular box available in all colours as long as it’s black). More importantly, the entire machine was built to last, built to be reliable, and built to be repairable.</p>
<p><a href="https://solar.lowtechmagazine.com/2019/06/how-to-make-wind-power-sustainable-again/">Circular and modular products are all the hype these days</a>, its lifetime could be extended endlessly by gradually repairing and replacing every part that it consists of. The question is not how we can evolve towards a circular economy, but instead why we continue to evolve away from it.</p>
<blockquote>
<p>The question is not how we can evolve towards a circular economy, but instead why we continue to evolve away from it.</p>
</blockquote>
<p>My Thinkpad was more expensive to buy than my iBook, but at least I didn’t spend all that money on a cute design but a decent computer. The charger gave no problems, and when I lost it during a trip and had to buy a new one, I could do so for a fair price. Little did I know that my happy purchase was going to be a once-in-a-lifetime experience.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: The IBM ThinkPad R52 from 2005." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/Thinkpad-r52-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/Thinkpad-r52-white_hu02c8b35b61eabe045fd7fdd9f38bf53d_4952865_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/Thinkpad-r52-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: The IBM ThinkPad R52 from 2005. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="my-third-laptop-lenovo-thinkpad-t430-2013-2017">My third laptop: Lenovo Thinkpad T430 (2013-2017)</h2>
<p>Fast forward to 2013. I am now living in Spain and I’m running Low-tech Magazine. I’m still working on my IBM Thinkpad R52, but there are some problems on the horizon. First of all, Microsoft will soon force me to upgrade my operating system, because support for Windows XP is to end in 2014. I don’t feel like spending a couple of hundred euros on a new operating system that would be too demanding for my old laptop anyway. Furthermore, the laptop had gotten a bit slow, even after it had been restored to its factory settings. In short, I fell into the trap that the hardware and software industries have set up for us and made the mistake of thinking that I needed a new laptop.</p>
<p>Having been so fond of my Thinkpad, it was only logical to get a new one. Here’s the problem: in 2005, shortly after I had bought my first Thinkpad, Lenovo, a Chinese manufacturer that is now the largest computer maker in the world, bought IBM’s PC business. Chinese companies don’t have a reputation for building quality products, especially not at the time. However, since Lenovo was still selling Thinkpads that looked almost identical to those built by IBM, I decided to try my luck and bought a <a href="http://www.thinkwiki.org/wiki/Category:T430" target="_blank">Lenovo Thinkpad T430</a> in April 2013. At a steep price, but I assumed that quality had to be paid for.</p>
<p>My mistake was clear from the beginning. I had to send the new laptop back twice because its case was deformed. When I finally got one that didn’t wobble on my desk, I quickly ran into another problem: the keys started breaking off. I can still remember my disbelief when it happened for the first time. The IBM Thinkpad is known for its robust keyboard. If you want to break it, you need a hammer. Lenovo obviously didn’t find that so important and had quietly replaced the keyboard with an inferior one. Mind you, I can be an aggressive typist, but I have never broken any other keyboard.</p>
<p>I grumpily ordered a replacement key for 15 euros. In the months after that, replacement keys became a recurring cost. After spending more than 100 euros on plastic keys, which would soon break again, I calculated that my keyboard had 90 keys and that replacing them all just once would cost me 1,350 euros. I stopped using the keyboard altogether, temporarily finding a solution in an external keyboard. However, this was impractical, especially for working away from home – and why else would I want a laptop?</p>
<p>There was no getting around it anymore: I needed a new laptop. Again. But which one? For sure it would not be one made by Lenovo or Apple.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: Replacing all keys on my Lenovo T430 would have cost me 1,350 euros." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/broken-keyboard-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/broken-keyboard-white_huaf323a230a1b423848610266c7325189_7786692_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/broken-keyboard-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: Replacing all keys on my Lenovo T430 would have cost me 1,350 euros.  <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="my-fourth-laptop-ibm-thinkpad-x60s-2017-now">My fourth laptop: IBM Thinkpad X60s (2017-now)</h2>
<p>Not finding what I was looking for, I decided to go back in time. By now, it had dawned on me that new laptops are of inferior quality compared to older laptops, even if they carry a much higher price tag.  I found out that Lenovo switched keyboards around 2011 and started searching auction sites for Thinkpads built before that year. I could have changed back to my ThinkPad R52 from 2005, but by now, I had become accustomed to a Spanish keyboard, and the R52 had a Belgian one.</p>
<p>In April 2017, I settled on a used <a href="http://www.thinkwiki.org/wiki/Category:X60s" target="_blank">Thinkpad X60s</a> from 2006. <sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> As of December 2020, the machine is in operation for almost 4 years and is 14 years old – three to five times older than the average laptop. If I loved my Thinkpad R52 from 2005, I adore my Thinkpad X60s from 2006. It’s just as sturdily built – it already survived a drop from a table on a concrete floor – but it’s much smaller and also lighter: 1.43 kg vs. 3.2 kg.</p>
<p>My 2006 Thinkpad X60s does everything I want it to do. I use it to write articles, do research, and maintain the websites. I have also used it on-stage to give lectures, projecting images on a large screen. There’s only one thing missing on my laptop, especially nowadays, and that’s a webcam. I solve this by firing up the cursed 2013 laptop with the broken keys whenever I need to, happy to give it some use that doesn’t involve its keyboard. It could also be solved by a switch to the <a href="http://www.thinkwiki.org/wiki/Category:X200" target="_blank">Thinkpad X200</a> from 2008, which is a newer version of the same model and has a webcam.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: My ThinkPad X60s." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-x60s-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/thinkpad-x60s-white_hu30a06554e31ecbc7d8a684e77fe1da4d_3965389_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-x60s-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: My ThinkPad X60s. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="how-to-make-an-old-laptop-run-like-its-new">How to make an old laptop run like it’s new</h2>
<p>Not buying any more new laptops is not as simple as buying a used laptop. It’s advisable to upgrade the hardware, and it’s essential to downgrade the software. There are two things you need to do:</p>
<h2 id="1-use-low-energy-software">1. Use low energy software</h2>
<p>My laptop runs on <a href="https://www.linuxliteos.com/" target="_blank">Linux Lite</a>, one of several open-source operating systems <a href="https://lotoftech.com/10-best-lightweight-operating-system-for-old-computers/" target="_blank">specially designed to work on old computers</a>. The use of a Linux operating system is not a mere suggestion. There’s no way you’re going to revive an old laptop if you stick to Microsoft Windows or Apple OS because the machine would freeze instantly. Linux Lite does not have the flashy visuals of the newest Apple and Windows interfaces, but it has a familiar graphical interface and looks anything but obsolete. It takes very little space on the hard disk and demands even less computing power. The result is that an old laptop, despite its limited specifications, runs smoothly. I also use light browsers: <a href="https://vivaldi.com/" target="_blank">Vivaldi</a> and <a href="https://astian.org/en/midori-browser/" target="_blank">Midori</a>.</p>
<p>Having used Microsoft Windows for a long time, I find Linux operating systems to be remarkably better, even more so because they are free to download and install. Furthermore, Linux operating systems do not steal your personal data and do not try to lock you in, like the newest operating systems from both Microsoft and Apple do. That said, even with Linux, obsolescence cannot be ruled out. For example, Linux Lite will stop its support for 32-bit computers in 2021, which means that I will soon have to look for an alternative operating system, or buy a slightly younger 64-bit laptop.</p>
<h2 id="2-replace-the-hard-disk-drive-with-a-solid-state-drive">2. Replace the hard disk drive with a solid-state drive</h2>
<p>In recent years, solid-state drives (SSD) have become available and affordable, and they are much faster than hard disk drives (HDD). Although you can revive an old laptop by merely switching to a light-weight operating system, if you also replace the hard disk drive with a solid-state drive, you’ll have a machine that is just as fast as a brand new laptop. Depending on the storage capacity you want, an SSD will cost you between 20 euro (120 GB) and 100 euro (960 GB).</p>
<p>Installment is pretty straightforward and well documented online. Solid-state drives run silently and are more resistant to physical shock, but they have a shorter life expectancy than hard disk drives. Mine is now working for almost 4 years. It seems that both from an environmental and financial viewpoint, an old laptop with SSD is a much better choice than buying a new laptop, even if the solid-state drive needs replacement now and then.</p>
<h2 id="spare-laptops">Spare laptops</h2>
<p>Meanwhile, my strategy has evolved. I have bought two identical models for a similar price, in 2018 and early 2020, to use as spare laptops. Now I plan to keep working on these machines for as long as possible, having more than sufficient spare parts available. Since I bought the laptop, it had two technical issues. After roughly a year of use, the fan died. I had it repaired overnight in a tiny and messy IT shop run by a Chinese man in Antwerp, Belgium. He said that my patched fan would run for another six months, but it’s still working more than two years later.</p>
<p>Then, last year, my X60s suddenly refused to charge its battery, an issue that had also appeared with my cursed 2013 laptop. It seems to be a common problem with Thinkpads, but I could not solve it yet. Neither did I really have to because I had a spare laptop ready and started using that one whenever I needed or wanted to work outside.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: Three identical 2006 laptops, all in working order, for less than 200 euros." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/spare-laptops-white_hu3766708df1a6ba8bdbeb0add86f0194b_6142617_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: Three identical 2006 laptops, all in working order, for less than 200 euros. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<div>
<figure data-imgstate="dither">
<img alt="Image: Inside the Thinkpad X60s. Source: Hardware Maintenance Manual." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-inside_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/thinkpad-inside_huad20844be5ac8ba7ef947df62b823a02_178972_800x800_fit_q90_h2_box_3.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-inside_dithered.png"> </figure>

</div>
<h2 id="the-magical-sd-card">The magical SD-card</h2>
<p>Now to introduce you to my magical SD-card, which is another hardware upgrade that facilitates the use of old (but also new) laptops. Many people have their personal documents stored on their laptop’s hard drive and then make backups to external storage media if all goes well. I do it the other way around.</p>
<p>I have all my data on a 128 GB SD-card, which I can plug into any of the Thinkpads that I own. I then make monthly backups of the SD-card, which I store on an external storage medium, as well as regular backups of the documents that I am working on, which I temporarily store on the drive of the laptop that I am working on. This has proven to be very reliable, at least for me: I have stopped losing work due to computer problems and insufficient backups.</p>
<p>The other advantage is that I can work on any laptop that I want and that I’m not dependent on a particular machine to access my work. You can get similar advantages when you keep all your data in the cloud, but the SD-card is <a href="https://solar.lowtechmagazine.com/2015/10/why-we-need-a-speed-limit-for-the-internet/">the more sustainable option</a>, and it works without internet access.</p>
<p>Hypothetically, I could have up to two hard drive failures in one day and keep working as if nothing happened. Since I am now using both laptops alternately – one with battery, the other one without – I can also leave them at different locations and cycle between these places while carrying only the SD-card in my wallet. Try that with your brand new, expensive laptop. I can also use my laptops together if I need an extra screen.</p>
<p>In combination with a hard disk drive, the SD-card also increases the performance of an old laptop and can be an alternative to installing a solid-state drive. My spare laptop does not have one and it can be slow when browsing heavy-weight websites. However, thanks to the SD-card, opening a map or document happens almost instantly, as does scrolling through a document or saving it. The SD-card also keeps the hard disk running smoothly because it’s mostly empty. I don’t know how practical using an SD-card is for other laptops, but all my Thinkpads have a slot for them.</p>
<h2 id="the-costs">The costs</h2>
<p>Let’s make a complete cost calculation, including the investment in spare laptops and SD-card, and using today’s prices for both solid-state drives and SD-cards, which have become much cheaper since I have bought them:</p>
<ul>
<li>ThinkPad X60s: 50 euro</li>
<li>ThinkPad X60s spare laptop: 60 euro</li>
<li>ThinkPad X60 spare laptop: 75 euro</li>
<li>Two replacement batteries: 50 euro</li>
<li>240 GB solid-state drive: 30 euro</li>
<li>128 GB SD-card: 20 euro</li>
<li>Total: 285 euros</li>
</ul>
<p>Even if you buy all of this, you only spent 285 euros. For that price, you may be able to buy the crappiest new laptop on the market, but it surely won’t get you two spare laptops. If you manage to keep working with this lot for ten years, your laptop costs would be 28.5 euros per year. You may have to replace a few solid-state drives and SD-cards, but it won’t make much difference. Furthermore, you save the ecological damage that is caused by the production of a new laptop every 5.7 years.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: My laptop needs are met for the foreseeable future." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-2-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/spare-laptops-2-white_hu2dc6db9faa5724cc3c1faebbecf747bd_5522562_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-2-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: My laptop needs are met for the foreseeable future. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="dont-take-it-too-far">Don’t take it too far</h2>
<p>Although I have used my Thinkpad X60s as an example, the same strategy works with other Thinkpad models – <a href="http://www.thinkwiki.org/wiki/ThinkPad_History" target="_blank">here’s an overview of all historical models</a> – and laptops from other brands (which I know nothing about). If you prefer not to buy on auction sites, you can walk to the nearest pawnshop and get a used laptop with a guarantee. The chances are that you don’t even need to buy anything, as many people have old laptops lying around.</p>
<p>There’s no need to go back to a 2006 machine. I hope it’s clear that I am trying to make a statement here, and I probably went as far back as one can while keeping things practical. My first try was a used ThinkPad X30 from 2002, but that was one step too far. It uses a different charger type, it has no SD-card slot, and I could not get the wireless internet connection working. For many people, it may serve to choose a somewhat younger laptop. That will give you a webcam and a 64-bit architecture, which makes things easier. Of course, you can also try to beat me and go back to the 1990s, but then you’ll have to do without USB and wireless internet connection.</p>
<p>Your choice of laptop also depends on what you want to do with it. If you use it mainly for writing, surfing the web, communication, and entertainment, you can do it as cheaply as I did. If you do graphical or audiovisual work, it’s more complicated, because in that case, you’re probably an Apple user. The same strategy could be applied, on a somewhat younger and more expensive laptop, but it would suggest switching from a Mac to a Linux operating system. When it comes to office applications, Linux is clearly better than its commercial alternatives. For a lack of experience, I cannot tell you if that holds for other software as well.</p>
<h2 id="this-is-a-hack-not-a-new-economical-model">This is a hack, not a new economical model</h2>
<p>Although capitalism could provide us with used laptops for decades to come, the strategy outlined above should be considered a hack, not an economical model. It’s a way to deal with or escape from an economic system that tries to force you and me to consume as much as possible. It’s an attempt to break that system, but it’s not a solution in itself. We need another economical model, in which we build all laptops like pre-2011 Thinkpads. As a consequence, laptop sales would go down, but that’s precisely what we need. Furthermore, with today’s computing efficiency, we could significantly reduce the operational and embodied energy use of a laptop if we reversed the trend towards ever higher functionality.</p>
<p>Significantly, hardware and software changes drive the fast obsolescence of computers, but the latter has now become the most crucial factor. A computer of 15 years old has all the hardware you need, but it’s not compatible with the newest (commercial) software. This is true for operating systems and every type of software, from games to office applications to websites. Consequently, to make laptop use more sustainable, the software industry would need to start making every new version of its products lighter instead of heavier. The lighter the software, the longer our laptops will last, and we will need less energy to use and produce them.</p>
<p>Kris De Decker</p>
<p>Images: Jordi Manrique Corominas, Adriana Parra, Roel Roscam Abbing</p>
<p>Proofreading: Eric Wagner</p>
</div>



</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An ARM Assembler Written in Lisp (143 pts)]]></title>
            <link>http://forum.ulisp.com/t/an-arm-assembler-written-in-lisp/1237</link>
            <guid>36646277</guid>
            <pubDate>Sat, 08 Jul 2023 16:58:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://forum.ulisp.com/t/an-arm-assembler-written-in-lisp/1237">http://forum.ulisp.com/t/an-arm-assembler-written-in-lisp/1237</a>, See on <a href="https://news.ycombinator.com/item?id=36646277">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
        <p>You can write machine-code functions in uLisp with the help of the ARM assembler written in Lisp, and I’ve recently updated it to make it more compact. It will now fit on a board with about 2000 objects of workspace, with room to spare to write assembler programs and run them.</p>
<p>This post describes how the latest version of the ARM assembler works. The aim is to help anyone who wants to extend the assembler to cater for ARM instructions that it doesn’t currently support. It will also be helpful if you want to write an assembler for another processor, or even design your own processor and write an assembler for it; Lisp is an excellent language to do this. For example, a printout of the whole ARM assembler fits on two A4 pages.</p>
<h3>Instruction encodings</h3>
<p>The starting point for writing an assembler is to get hold of a summary of the processor’s table of instruction encodings. For the ARM Thumb instruction set these are as follows:</p>

<p><em>ARM Thumb instruction encodings for instructions starting <span>#x0</span> to <span>#x8</span>.</em></p>

<p><em>ARM Thumb instruction encodings for instructions starting <span>#x9</span> to <span>#xF</span>.</em></p>
<p>You can see from these diagrams that the 16-bit instructions are arranged into consistent field patterns. This is true of most processor instruction sets, but some are more orderly than others (RISC-V is a nightmare!).</p>
<h4>An example - LSL</h4>
<p>As an example, consider the first instruction in the first table, LSL immediate:</p>

<p>This consists of:</p>
<ul>
<li>The four-bit value <span>#b0000</span>.</li>
<li>A one-bit <em>op</em> code, which is 0 for LSL and 1 for LSR.</li>
<li>An <em>immed5</em> value, which is a 5-bit integer from 0 to 31 giving the size of the left shift.</li>
<li>
<em>Lm</em>, which is a value from 0 to 7 representing the source register R0 to R7.</li>
<li>
<em>Ld</em>, which is a value from 0 to 7 representing the destination register R0 to R7.</li>
</ul>
<h3>Emitting bit fields</h3>
<p>The first function we need is <strong>emit</strong>, which takes a specification defining the widths of the bit fields, and a list of arguments, and packs the values of the arguments into the bit fields:</p>
<pre><code>(defun emit (bits &amp;rest args)
  (let ((word 0) (shift -28))
    (mapc #'(lambda (value)
              (let ((width (logand (ash bits shift) #xf)))
                (incf shift 4)
                (unless (zerop (ash value (- width))) (error "Won't fit"))
                (setq word (logior (ash word width) value))))
          args)
    word))
</code></pre>
<p>The first argument, <strong>bits</strong>, is a 32-bit hexadecimal number in which each hex digit specifies the width of the next bit field. The function <strong>emit</strong> reads the hex digits in <strong>bits</strong> from left to right, packs the appropriate number of bits from each argument into <strong>word</strong>, and then returns the result.</p>
<p>For example, the bit fields for the LSL instruction could be specified by:</p>
<pre><code>#x41533000
</code></pre>
<p>To make it easier to process the bit fields the widths are left-aligned, so you should add zeros to make the <strong>bits</strong> parameter eight hex digits.</p>
<p>The remaining arguments are the values to be packed into the bit fields. If any argument won’t fit into the corresponding bit field the error <strong>Won’t fit</strong> will be displayed.</p>
<p>So for example, to emit the op code for the instruction:</p>
<pre><code>LSL r7, r4, #31
</code></pre>
<p>evaluate:</p>
<pre><code>&gt; (emit #x41533000 0 0 31 4 7)
2023
</code></pre>
<p>If you print this as a 16-bit binary number with:</p>
<pre><code>&gt; (format t "~16,'0b" 2023)
0000011111100111
</code></pre>
<p>you can see that the values have been put into the correct fields as required.</p>
<h3>Specifying registers</h3>
<p>The next step is to be able to specify registers as r0 to r15, or their synonyms <strong>sp</strong> (for r13), <strong>lr</strong> (for r14), and <strong>pc</strong> (for r15). This is handled by the function <strong>regno</strong>:</p>
<pre><code>(defun regno (sym)
  (case sym (sp 13) (lr 14) (pc 15)
    (t (read-from-string (subseq (string sym) 1)))))
</code></pre>
<p>For example:</p>
<pre><code>&gt; (regno 'r12)
12
</code></pre>
<p>Finally, we can now define the LSL instruction as the convenient Lisp function <strong>$lsl</strong> as follows:</p>
<pre><code>(defun $lsl (argd argm immed5)
  (emit #x41533000 0 0 immed5 (regno argm) (regno argd))
</code></pre>
<p>This allows us to specify the instruction using syntax that’s close to ARM assembler syntax:</p>
<pre><code>&gt; ($lsl 'r7 'r4 31)
2023
</code></pre>
<p>I’ve used the convention that functions representing ARM instructions are prefixed by a $ sign; otherwise there would be a problem with instructions that are also existing Lisp functions, such as <strong>push</strong> and <strong>pop</strong>.</p>
<h3>Handling addressing modes</h3>
<p>The final complication is that some instruction mnemonics can generate different op codes, depending on the types of their arguments.</p>
<p>For example, there’s also a variant of LSL that shifts a register Rd by the shift value specified in the register Rs:</p>

<p>Using this syntax, the following assembler instruction shifts the value in R7 by the value in R1:</p>
<pre><code>LSL r7, r1
</code></pre>
<p>The block of register-to-register instructions that include LSL is handled by the routine <strong>reg-reg</strong>:</p>
<pre><code>(defun reg-reg (op argd argm)
  (emit #xa3300000 op (regno argm) (regno argd)))
</code></pre>
<p>Finally, we need to modify <strong>$lsl</strong> to include the register-to-register variant:</p>
<pre><code>(defun $lsl (argd argm &amp;optional arg2)
  (cond
   ((numberp arg2)
    (lsl-lsr-0 0 arg2 argm argd))
   ((numberp argm)
    (lsl-lsr-0 0 argm argd argd))
   (t
    (reg-reg #b0100000010 argd argm))))
</code></pre>
<p>where <strong>lsl-lsr-0</strong> is defined as:</p>
<pre><code>(defun lsl-lsr-0 (op immed5 argm argd)
  (emit #x41533000 0 op immed5 (regno argm) (regno argd)))
</code></pre>
<p>This expanded version of <strong>$lsl</strong> also handles the two-argument case where the source and destination registers are the same in an immediate shift; for example:</p>
<pre><code>($lsl 'r1 31)
</code></pre>
<h3>Running the assembler</h3>
<p>To run the assembler in uLisp you use the built-in command <strong>defcode</strong>, which generates an assembler listing, and puts the machine code into RAM so you can execute it as if it’s a normal Lisp function.</p>
<h4>Greatest Common Divisor example</h4>
<p>For example, to assemble a machine-code routine <strong>gcd</strong> to calculate Greatest Common Divisor you’d evaluate:</p>
<pre><code>; Greatest Common Divisor
(defcode gcd (x y)
  swap
  ($mov 'r2 'r1)
  ($mov 'r1 'r0)
  again
  ($mov 'r0 'r2)
  ($sub 'r2 'r2 'r1)
  ($blt swap)
  ($bne again)
  ($bx 'lr))
</code></pre>
<p>and you could then call:</p>
<pre><code>&gt; (gcd 3287 3460)
173
</code></pre>
<h3>Running the assembler in Common Lisp</h3>
<p>You can also run the ARM assembler in a standard Common Lisp implementation. The Common Lisp version of the ARM Assembler includes the following <strong>defcode</strong> macro that lets you assemble an ARM function and print the machine code, like the <strong>defcode</strong> special form built into uLisp:</p>
<pre><code>(defparameter *pc* 0)

(defmacro defcode (&amp;body body)
  (let ((*print-pretty* t) (assembler (cddr body)))
    (dotimes (pass 2)
      (setq *pc* 0)
      (mapc
       #'(lambda (ins)
           (cond
            ((atom ins)
             (unless (zerop pass) (format t "~4,'0x      ~(~a~)~%" *pc* ins))
             (set ins *pc*))
            ((listp (eval ins))
             (unless (zerop pass)
               (format t "~4,'0x ~4,'0x ~(~a~)~%" *pc* (first (eval ins)) ins)
               (format t "~4,'0x ~4,'0x~%" (+ *pc* 2) (second (eval ins))))
             (incf *pc* 4))
            (t
             (unless (zerop pass)
               (format t "~4,'0x ~4,'0x ~(~a~)~%" *pc* (eval ins) ins))
             (incf *pc* 2))))
       assembler)
      nil)))
</code></pre>
<p>Evaluating the <strong>Greatest Common Divisor example</strong> above generates the following output:</p>
<pre><code>0000      swap
0000 000A ($mov 'r2 'r1)
0002 0001 ($mov 'r1 'r0)
0004      again
0004 0010 ($mov 'r0 'r2)
0006 1A52 ($sub 'r2 'r2 'r1)
0008 DBFA ($blt swap)
000A D1FB ($bne again)
000C 4770 ($bx 'lr)
</code></pre>
<p>In this case you obviously won’t be able to run the machine code.</p>
<h3>Resources</h3>
<p>For both versions of the assembler see: <a href="https://github.com/technoblogy/lisp-arm-assembler">https://github.com/technoblogy/lisp-arm-assembler</a>.</p>
<p>For more information see <a href="http://www.ulisp.com/show?2YRU">ARM assembler overview</a>.</p>
<p>For a list of the ARM Thumb instructions supported by the assembler see: <a href="http://www.ulisp.com/show?30B8">ARM assembler instructions</a>.</p>
<p>For ARM assembler examples see: <a href="http://www.ulisp.com/show?30BD">ARM assembler examples</a>.</p>
<h3>Update</h3>
<p>6th July 2023: Updated the <strong>defcode</strong> macro to handle forward references.</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Can a Rubik's Cube be brute-forced? (136 pts)]]></title>
            <link>https://www.stylewarning.com/posts/brute-force-rubiks-cube/</link>
            <guid>36645846</guid>
            <pubDate>Sat, 08 Jul 2023 16:19:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.stylewarning.com/posts/brute-force-rubiks-cube/">https://www.stylewarning.com/posts/brute-force-rubiks-cube/</a>, See on <a href="https://news.ycombinator.com/item?id=36645846">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<article>
<p><em>By Robert Smith</em></p>
<div>
<hr>
<h2>Contents</h2>
<nav id="TableOfContents">
<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#computer-puzzling-without-brute-force">Computer puzzling without brute-force</a></li>
<li><a href="#taking-a-step-back-puzzles-as-permutations">Taking a step back: puzzles as permutations</a></li>
<li><a href="#brute-force-still-ignorant-but-kinda-smart">Brute-force, still ignorant, but kinda smart?</a>
<ol>
<li><a href="#observation-1-decomposition-as-intersection">Observation #1: decomposition as intersection</a></li>
<li><a href="#observation-2-sorting-really-helps">Observation #2: sorting really helps!</a></li>
<li><a href="#what-is-a-move">What is a move?</a></li>
<li><a href="#what-is-a-word">What is a word?</a></li>
<li><a href="#observation-3-sorting-as-solving">Observation #3: sorting as solving</a></li>
<li><a href="#more-splitting">More splitting?</a></li>
<li><a href="#iterating-through-products-with-schroeppel--shamir">Iterating through products with Schroeppel–Shamir</a></li>
<li><a href="#permutation-tries">Permutation tries</a></li>
<li><a href="#the-4-list-algorithm-and-solving-the-rubiks-cube">The 4-List Algorithm and solving the Rubik’s Cube</a></li>
</ol>
</li>
<li><a href="#example-and-source-code">Example and source code</a></li>
<li><a href="#tips-for-optimizing-the-4-list-algorithm">Tips for optimizing the 4-List Algorithm</a></li>
<li><a href="#sample-benchmarks">Sample benchmarks</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#references">References</a></li>
</ol>
</nav>
<hr>
</div>
<h2 id="introduction">Introduction</h2>
<p>When I was about 13, while still a middle-schooler, I became
fascinated with the Rubik’s Cube<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. I never got terribly good
at solving it, maybe eventually getting into the 30 to 40 seconds
range. While I didn’t have a penchant for memorizing move sequences, I
was drawn into how we <em>find</em> these move sequences.</p>
<p>The story about my interest and exploration in the Rubik’s Cube is for
another post. Long story short, I got interested in “computer
puzzling”—using computers to manipulate combinatorial puzzles, like
the Rubik’s Cube, either to solve them quickly, to discover patterns,
or to find novel move sequences for use in speedcubing—and ever
since, I’ve been working on different programs for solving Rubik-like
puzzles.</p>
<p>Purely in principle, it shouldn’t be hard to solve a Rubik’s Cube with
a computer, right? Our program would have three parts:</p>
<ol>
<li>A model of the Rubik’s Cube, that is, some data structure that
represents a cube state.</li>
<li>Some functions which can simulate turns of each side.</li>
<li>A solving procedure which takes a scrambled cube, tries every
possible turn sequence, and stops when solved.</li>
</ol>
<p>Truth be known, and details aside, this is a provably correct method
for solving a Rubik’s Cube. If you leave your computer on long enough,
it will return a solution.</p>
<p>The problem is that it takes a long time. Probably longer than your
lifetime.</p>
<h2 id="computer-puzzling-without-brute-force">Computer puzzling without brute-force</h2>
<p>“Brute-force” generally means to try every possibility of something
without much of any strategy. Our method above is a brute-force
algorithm. Brute-force algorithms generally aren’t practical, because
if you have $N$ of something to explore, a brute-force algorithm will
take $O(N)$ time. For a Rubik’s Cube, $N$ is 43 quintillion—a very
large number.</p>
<p>It has been known, practically since the Rubik’s Cube’s inception,
that something else is needed to solve a Rubik’s Cube. Rubik’s Cube
solutions, obviously, take into account the specific structure and
properties of the cube so as to implicitly or explicitly avoid
mindless search. These methods have turned out to be:</p>
<ol>
<li>Solving methods for humans: memorize some sequences which let you
move only a few pieces around in isolation, and apply these
sequences mechanically until all pieces are in place. The more
sequences you memorize, the faster you’ll be.</li>
<li>Heuristic tree search: do a tree search (with e.g.,
iterative-deepening depth-first search<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>), but aggressively
prune off branches by way of clever heuristics<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.</li>
<li>Phase-based solvers: a deeply mathematical way which involves
characterizing the Rubik’s Cube as a sequence of nested
(mathematical) subgroups so that each successive coset space small
enough that it can be solved by computer.</li>
</ol>
<p>Computer puzzling mostly deals with the latter two approaches, usually
in some combination. Both approaches lead to extraordinarily
high-performing solvers. For example:</p>
<ul>
<li>Korf’s algorithm (approach #2) finds optimal solutions—solutions
of shortest length—but can take hours to find one.</li>
<li>Thistlethwaite’s algorithm (approach #3) solves a cube in four
phases almost instantaneously. The solutions are guaranteed to be no
longer than triple the optimal length.</li>
</ul>
<p>The story may as well end here. We have slow but optimal ways of
solving the Rubik’s Cube, and fast but sub-optimal ways. Pick your
poison (sub-optimal or slow), depending on what you’re trying to
achieve.</p>
<h2 id="taking-a-step-back-puzzles-as-permutations">Taking a step back: puzzles as permutations</h2>
<p>It seems that any Rubik’s Cube solver <em>has</em> to know <em>something</em> about
the structure of the cube. It might be worth asking how little
structure we can get away with, so as to make whatever solving
algorithm we write generic over a broad class of puzzles.</p>
<p>For a brute-force algorithm with tree search, we would need something
like the following:</p>
<pre tabindex="0"><code>interface GenericPuzzle:
  type State
  type Move

  function isSolved(State) -&gt; Boolean
  function allMoves() -&gt; List(Move)
  function performMove(State, Move) -&gt; State
</code></pre><p>With this, we could write the following solver based off of
iterative-deepening depth-first search, which is totally generic on
the above interface.</p>
<pre tabindex="0"><code>function solve(State) -&gt; List(Move)
function solve(p):
  if isSolved(p):
    return []

  for maxDepth from 1 to infinity:
    solved?, solution = dfs(0, maxDepth, p)
    
    if solved?:
      return solution

function dfs(Integer, Integer, State, List(Move)) -&gt; (Boolean, List(Move))
function dfs(depth, maxDepth, p, s):
  if isSolved(p):
    return (True, s)

  if depth == maxDepth:
    return (False, [])

  for m in allMoves():
    p' = performMove(p, m)
    (solved?, solution) = dfs(depth+1, maxDepth, p', append(s, [m])

    if solved?:
      return (solved?, solution)
</code></pre><p>As discussed before, while this strategy is effective for problems
with small search spaces, it’s no help when the space is
large. Unfortunately, the <code>GenericPuzzle</code> interface doesn’t give us
much room for improvement. Can we still remain generic, while giving
us at least a little more room for exploring other algorithms?</p>
<p>The answer is yes, if we restrict ourselves to <em>permutation
puzzles</em>. Roughly speaking, a permutation puzzle is one where pieces
shift around according to a fixed and always available set of shifting
moves. The Rubik’s Cube is a phenomenal and non-trivial example: We
can label each mobile<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> sticker with a number 1 to 48, and
these stickers can always be shifted around with a twist of any of the
six sides. Since we can twist any of the six sides at any time, the
puzzle is a permutation puzzle. (Not all similar puzzles are
permutation puzzles. There are some puzzles which are “bandaged”, that
is, pieces of the puzzle are fused together, restricting some
available moves depending on the configuration.)</p>
<p>In this view, we looked at a solved configuration as a list of
numbers. For example, the solved Rubik’s Cube as a permutation would
be</p>
<p>$$
(1, 2, \ldots, 47, 48).
$$</p>
<p>When we turn a side, these numbers get permuted. For instance,
assuming a particular labeling of stickers with numbers, turning the
top face of a Rubik’s Cube might permute the first sticker in the list
to the third, the second sticker to the fifth, the third sticker to
the eighth, etc. We can use the same notation</p>
<p>$$
(3, 5, 8, 2, 7, 1, \ldots)
$$</p>
<p>This notation has two interpretations:</p>
<ol>
<li>The literal position of numbered stickers on a physical cube (with
an agreed upon labeling).</li>
<li>An instruction for how to relabel the stickers of a given cube.</li>
</ol>
<p>If we look at the notation under the second interpretation, a
permutation actually represents a <em>function</em> that’s applied to
<em>individual stickers</em>. For instance, if</p>
<p>$$
F := (3, 5, 8, 2, 7, 1, \ldots)
$$</p>
<p>then $F(1) = 3$, $F(2) = 5$, etc. All of the clockwise face
turns—Front, Right, Up, Back, Left, Down—of a Rubik’s Cube can be
described like so:</p>
<p>$$
\begin{align*}
F &amp;:= (1, 2, 3, 4, 5, 25, \ldots)\\
R &amp;:= (1, 2, 38, 4, 36, 6, \ldots)\\
U &amp;:= (3, 5, 8, 2, 7, 1, \ldots)\\
B &amp;:= (14, 12, 9, 4, 5, 6, \ldots)\\
L &amp;:= (17, 2, 3, 20, 5, 22, \ldots)\\
D &amp;:= (1, 2, 3, 4, 5, 6, \ldots, 48, 42, 47, 41, 44, 46)
\end{align*}
$$</p>
<p>We wrote some of the last elements of $D$ because a “down” move doesn’t
change the first six stickers in this labeling scheme.</p>
<p>This gives is a whole new interpretation of what it means to “solve” a
cube. Given a scrambled cube, we first write down the permutation that
describes how the stickers moved from a solved state to the scrambled
state. Let’s call it $s$. This is easy, because we can just read the
labeled stickers off of a cube one-by-one, in order. For example, $s$
might be:</p>
<p>$$
s := (27, 42, 30, 15, 39, 6, \ldots).
$$</p>
<p><em>This is a description of a function!</em> The value of $s(1)$ describes
how the first sticker of a cube will be shifted to its scrambled
position, in this case $27$. Next, solving a cube is finding a
sequence of $k$ moves $m_1, m_2, \ldots, m_k$ such that, for all $1\leq
i\leq 48$,</p>
<p>$$
i = m_k(m_{k-1}(\cdots(m_2(m_1(s(i)))))).
$$</p>
<p>Stated another way in function composition notation, the function</p>
<p>$$
m_k \circ m_{k-1} \circ \cdots \circ m_2 \circ m_1\circ s
$$</p>
<p>must be the identity function—a permutation that doesn’t move
anything.</p>
<p>In the permutation puzzle way of thinking, we can still implement our
<code>GenericPuzzle</code> interface:</p>
<ul>
<li><code>State</code> would be a permutation;</li>
<li><code>Move</code> would also be a permutation;</li>
<li><code>isSolved</code> would check if a permutation is $(1, 2, 3, \ldots)$;</li>
<li><code>allMoves</code> would be a hard-coded list of the possible moves, like
$F$, $R$, $U$, $B$, $L$, and $D$ for the Rubik’s cube; and</li>
<li><code>performMove</code> would take the input move permutation, and apply it as
a function to each element of the state permutation.</li>
</ul>
<p>This might even be <em>more</em> efficient than another choice of
representation, since permutations can be represented very efficiently
on a computer as packed arrays of bytes!</p>
<p>But we didn’t do all this mathematical groundwork just to goof around;
there’s something amazing lurking in these permutations.</p>
<h2 id="brute-force-still-ignorant-but-kinda-smart">Brute-force, still ignorant, but kinda smart?</h2>
<p>In the late 1980s, Adi Shamir<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> and his students made a
brilliant series of observations that came together to make for a
beautiful result. Unfortunately, to my knowledge, only two writings
exist on the topic.</p>
<ol>
<li>
<p>Shamir and his colleagues wrote a paper about it [1], sort of in
the style of a brief conference proceeding, but it’s very light on
details and skips implementation considerations. It’s the kind of
paper where you follow it, but you have to fill in a great number
of blanks to make anything from it work.</p>
</li>
<li>
<p>Shamir gave a talk sometime in the 80’s about his result, and
somebody (none other than Alan Bawden) wrote a brief email [2] to a
mailing list about his recollection of it.</p>
</li>
</ol>
<p>An amazing result, buried in history, without any good exposition that
I could find.</p>
<p>What’s the result? The essence of the result is this. Reminiscent of a
“meet in the middle” algorithm, if we want to brute-force a problem
that ordinarily requires visiting $N$ states to find an answer, we can
instead cleverly split the work into two searches that requires visits
to around $\sqrt{N}$ states. For a Rubik’s Cube, that cuts work
associated with 43 quintillion states, down to work associated with 6
billion states. The best part is, this is <em>still brute-force</em>;
virtually no knowledge of the structure of the problem is required to
make it work.</p>
<p>Let’s walk through the requisite steps and build up to the
result. I’ll attempt to write in a general framework (since it’s a
general algorithm), but make frequent appeals to the Rubik’s Cube
specifically.</p>
<h3 id="observation-1-decomposition-as-intersection">Observation #1: decomposition as intersection</h3>
<p>Suppose the following:</p>
<ul>
<li>We have a mysterious permutation $s$, say, a scrambled puzzle;</li>
<li>We have two sets of permutations $X$ and $Y$; and</li>
<li>We assume there’s an $\hat x\in X$ and $\hat y\in Y$ such that $s =
\hat y\circ \hat x$.</li>
</ul>
<p>The goal is to find precisely $\hat x$ and $\hat y$ are. The simplest
way to do this is to check every combination of elements in $X$ and
$Y$.</p>
<pre tabindex="0"><code>for x in X:
  for y in Y:
    when s = compose(y, x):
      return (x, y)
</code></pre><p>This will take time proportional to the product of the set sizes:
$O(\vert X\vert\cdot\vert Y\vert)$. Shamir noticed the following: If
$s=\hat y\circ\hat x$, then $\hat y^{-1}\circ s = \hat x$. With this, we
preprocess our $Y$ set to be instead</p>
<p>$$
Y' := \{y^{-1}\circ s : y\in Y\}.
$$</p>
<p>By doing this, there must be an element in common between $X$ and
$Y'$, since $\hat x\in X$ and $\hat y^{-1}\circ s\in Y'$ and those are
equal. So we’ve reduced the problem to determining what the
intersection between $X$ and $Y'$ is.</p>
<p>Once we find our $z$ which is in common with $X$ and $Y'$, then our
recovered permutation will be $\hat x = z$ and $\hat y = (z\circ
s^{-1})^{-1}$.</p>
<p>We’ve just established that the problem of decomposing an element like
$s$ is identical to the problem of calculating a set
intersection. Still, if we want to do the intersection, our intuition
tells us we still need a quadratic algorithm, which brings us to the
second observation.</p>
<h3 id="observation-2-sorting-really-helps">Observation #2: sorting really helps!</h3>
<p>Permutations have a natural ordering, called <em>lexicographic
ordering</em>. If you have two permutations, and you read their elements
left-to-right, you can compare them like ordinary numbers. Just
as $123 &lt; 213$, we can say that</p>
<p>$$
(1,2,3) &lt; (2,1,3).
$$</p>
<p>A nice property of this is that the identity permutation $(1, 2, 3,
\ldots)$ is the smallest permutation of a given size.</p>
<p>How does this help us? Well, suppose we sort our sets $X$ and $Y'$
into lists $L_X$ and $L_{Y'}$, so the permutations are in order. If
$L_X$ and $L_{Y'}$ have an element in common, we can find it in linear
time: $O(\min\{\vert X\vert, \vert Y'\vert\})$. How? Something like
the following:</p>
<pre tabindex="0"><code>function findCommon(Lx, Ly):
  x = pop(Lx)
  y = pop(Ly)
  loop:
    if x == y:
      return x
    
    if empty(Lx) or empty(Ly):
      error("No common elements found.")

    if x &lt; y:
      x = pop(Lx)
    else if x &gt; y:
      y = pop(Ly)
</code></pre><p>This works because we are essentially looking at all of the elements
of $L_X$ and $L_{Y'}$ together in sorted order. It’s like a merge
sort, without the merge part.</p>
<p>Before continuing, we should take a little scenic tour on a more
formal meaning of “moves” and “move sequences”, since ultimately any
permutation puzzle solving algorithm must produce them as output.</p>
<h3 id="what-is-a-move">What is a move?</h3>
<p>A quick bit about notation. If we have a permutation $f$, then its
inverse is written $f^{-1}$, and it’s $k$-fold repetition $f\circ
f\circ\cdots\circ f$ is written $f^k$. If we have a collection of
permutations $S := \{f_1, f_2, \ldots\}$, then we write the
following shorthands:</p>
<p>$$
\begin{align*}
S^{-1} &amp;:= \{f^{-1} : f \in S\}\\
S^{\times k} &amp;:= \{f^k : f \in S\}.
\end{align*}
$$</p>
<p>If $g$ is some permutation, we also write these shorthands:</p>
<p>$$
\begin{align*}
g\circ S &amp;:= \{g\circ f : f \in S\}\\
S\circ g &amp;:= \{f\circ g : f \in S\}.
\end{align*}
$$</p>
<p>Similarly, if $T := \{g_1, g_2, \ldots\}$, then we can write</p>
<p>$$
\begin{align*}
S\circ T &amp;:= \{f\circ g : f\in S, g\in T\}\\
&amp;= \{f_1\circ g_1, f_2\circ g_1, \ldots, f_1\circ g_2, \ldots\}.
\end{align*}
$$</p>
<p>With that out of the way, let’s talk about the concept of a single
“move”. What counts as a “move” in a permutation puzzle?</p>
<p>Really, we can choose any set of moves we please, so long as every
state of the puzzle is reachable through some combination of the
moves. For example, let</p>
<p>$$
C := \{F, R, U, B, L, D\},
$$</p>
<p>the basic and well understood ninety-degree clockwise moves of the
Rubik’s Cube. Indeed, $C$ itself is a fine definition of available
moves. All of the following are also valid definitions of moves:</p>
<p>$$
C\cup C^{-1},\quad C\cup C^{\times 2},\quad C^{-1},\quad C\cup C^{\times 2}\cup C^{-1},
$$</p>
<p>and so on. Perhaps surprisingly, we can take any element of $C$ and
remove it, and it would still be a valid set of moves for the Rubik’s
Cube<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup>!</p>
<p>Which set of moves we select usually has little relevance
mathematically (they are all expressible as one another), but has
great relevance when we are synthesizing efficient move sequences, or when
we want to talk about “optimality”. For instance, consider a
counterclockwise move: $F^{-1}$. It’s natural to consider this a
single move, but if we consider our set to be $C$, then we’d have to
count it as three moves, since $F^{-1} = F\circ F\circ F = F^3$. What
about $F^2$? Is that one move or two? Speedcubers generally consider
$F^2$ to be one motion, so counting that as one move is natural, but
many computer puzzlers like the simplicity of $C\cup C^{-1}$, i.e.,
only ninety-degree turns<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup>.</p>
<p>For the rest of this note, we’ll be in the former camp, where half-turns count as one, and we’ll denote this set of moves as:</p>
<p>$$
\bar C := C \cup C^{-1} \cup C^{\times 2}.
$$</p>
<h3 id="what-is-a-word">What is a word?</h3>
<p>After we agree on what we consider a move, we can be more specific as
to what we mean about move sequences. A <em>move sequence</em> is a possibly
empty list of moves. A move sequence can be <em>composed</em> to form the
permutation it represents. This composition operator is called
$\kappa$, and is easily defined. Let $M$ be a move set, and let $s =
[s_1, s_2, \ldots, s_n]$ be a sequence of $n$ moves with each
$s_{\bullet}$ a move from $M$. The <em>length</em> is $s$ is naturally $n$,
and its composition is defined as:</p>
<p>$$
\begin{align*}
\kappa([\,]) &amp;:= (1, 2, 3, \ldots)\\
\kappa([s_1, s_2, \ldots, s_{n-1}, s_n]) &amp;:= \kappa([s_1, s_2, \ldots, s_{n-1}])\circ s_n.
\end{align*}
$$</p>
<p>If $M$ is a move set, then the set of all move sequences (including
the empty sequence) is denoted $M^{*}$, a notation kindly borrowed
from formal language theory.</p>
<p>If we identify the elements of $M$ with symbols, then a move sequence
is called a <em>word</em>. We’ll always type symbols in $\texttt{typewriter}$
font. The moves $\{F, R, U, B, L, D\}$ have the symbols
$\{\texttt{F}, \texttt{R}, \texttt{U}, \texttt{B}, \texttt{L},
\texttt{D}\}$, an inverse $F^{-1}$ has the symbol $\texttt{F'}$, and
a square $F^2$ has the symbol $\texttt{F2}$. And we type words as
symbols joined together in <em>reverse</em> order<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup>, so $[R^{-1},
U^2, L]$ can be represented by the word $\texttt{L U2 R'}$.</p>
<p>The distinction is subtle but important. In a computer program, a move
sequence is a list of permutations, while a word is a list of
symbols. A Rubik’s Cube solving program should take as input a
permutation, and output a word which when composed as a move sequence,
brings that permutation to identity.</p>
<p>When doing math, we often mix up all of these concepts since they have
little bearing on the correctness of an argument. Whether it’s the
permutation $F\circ R^{-1}$ or the move sequence $[F, R^{-1}]$ or the
word $\texttt{R' F}$ or otherwise, they all represent roughly
the same thing, but computers need to be explicit about which
representation is being manipulated.</p>
<p>So, in summary:</p>
<ul>
<li>A <strong>move set</strong> is a set of permutations that “count” as one move.</li>
<li>A <strong>move sequence</strong> is a list of moves from a move set.</li>
<li>The <strong>composition</strong> of a move sequence is the permutation that move sequence represents.</li>
<li>A <strong>symbol</strong> is a designator for a move in a move set.</li>
<li>A <strong>word</strong> is a sequence of symbols.</li>
</ul>
<p>Back to this brute-force thing…</p>
<h3 id="observation-3-sorting-as-solving">Observation #3: sorting as solving</h3>
<p>As silly as the example is, let’s suppose we know, for a fact, that a
Rubik’s Cube was mixed up using only six moves from $\bar C$. Since
$\bar C$ has 18 elements, without any optimization, we might have to
try $18^6$ move sequences to find a solution.</p>
<p>Instead of brute-forcing in that way, we can do another trick. Let <code>s</code>
be our scrambled permutation.</p>
<ol>
<li>
<p>Write out every combination of 3 moves into a table. The key would
be the permutation, and the value would be the word associated with
that permutation. Call this table <code>A</code>.</p>
</li>
<li>
<p>Sort <code>A</code> in ascending lexicographic order on the permutation.</p>
</li>
<li>
<p>Make a copy of <code>A</code>, call it <code>B</code>. For all <code>(perm, word)</code> in <code>B</code>,
reassign <code>perm := compose(invert(perm), s)</code>. We do this because of
Observation #1.</p>
</li>
<li>
<p>Sort <code>B</code>.</p>
</li>
<li>
<p>Call <code>x := findCommon(A, B)</code>. We do this via Observation #2.</p>
</li>
<li>
<p>Reconstruct a word equal to <code>s</code> by <code>A[x].word ++ reverse(B[x].word)</code>. We do this to recover a final result via Observation
#1.</p>
</li>
</ol>
<p>Since we have a word that brings us <em>from solved to <code>s</code></em>, we can
invert the word to bring us <em>from <code>s</code> to solved</em>.</p>
<p>By this method, we avoided visiting all $16^6$ move sequences by
instead pre-calculating two groups of $16^3$ sequences and exploring
them for an intersection. We have cut the amount of work down to its
square root.</p>
<p>If we generalize to length $n+m$ (for some splitting of $n$ and $m$),
then we can replace the work of visiting $16^{n+m}$ states with
$16^m + 16^n$ states, which is much better.</p>
<p>So we’re done? We now know that the Rubik’s Cube requires no more than
20 moves, so if we make two tables enumerating 10 moves, we should be
good?</p>
<p>Well, err, $16^{10} = 1,099,511,627,776$. Unless we have trillions of
resources to space, be it time or space, it’s still not going to work.</p>
<h3 id="more-splitting">More splitting?</h3>
<p>An enterprising computer science student, at this point, might smell
recursion. If we split once, can we split again? If we know a Rubik’s
Cube can be solved in 20 moves, can we split it into two 10 move
problems, and each of those into two 5 move problems?</p>
<p>The problem with this is that at the top layer of recursion, it’s
clear what we are solving. At lower layers, it’s no longer clear. What
<em>actually</em> is the recursive structure at play? And if we could do this
trick, couldn’t we decimate any brute-force problem of exponential
complexity (e.g., in number of moves) into one of linear?</p>
<p>That isn’t going to work, but we can be inspired by it. Let $L := \bar
C^5$ be the set of 5-move combinations from $\bar C$. The size of $L$
is going to be $621,649$ if we don’t store redundant
permutations. This is definitely possible to compute. Then our goal is
to find a decomposition of $s$ in terms of an element in $L\circ
L\circ L\circ L$. Using the same trick from Observation #1, suppose
there is a decomposition $$s = l_4\circ l_3\circ l_2\circ l_1.$$ Then
$$l_3^{-1}\circ l_4^{-1} \circ s = l_2\circ l_1.$$ So we create four
tables:</p>
<ul>
<li>$L_1 = L$,</li>
<li>$L_2 = L_1$,</li>
<li>$L_4 = L_1^{-1}$, and</li>
<li>$L_3 = L_4\circ s$.</li>
</ul>
<p>No, the $4$ before $3$ is not a typo! We put this in order to save on
computation and avoid redundant work. Now our goal is to find an
element in common between the two sets</p>
<p>$$
\begin{align*}
X &amp;= L_2 \circ L_1\\
Y &amp;= L_4 \circ L_3.
\end{align*}
$$</p>
<p>Somehow, we must do this without actually calculating all elements of
$L_i\circ L_j$. And, to add insult to injury, for <code>findCommon</code> to
work, we need to be able to go through the set in sorted order.</p>
<h3 id="iterating-through-products-with-schroeppel--shamir">Iterating through products with Schroeppel–Shamir</h3>
<p>Suppose we have two lists of positive numbers $A$ and $B$. How can we
print the elements of $\{a+b : a\in A, b\in B\}$ in numerical order
without explicitly constructing and sorting this set? Shamir and his
collaborator Schroeppel did so with the following algorithm.</p>
<ol>
<li>
<p>Sort $A$ in ascending order. Pop off the first (and therefore
smallest) element $a_1$.</p>
</li>
<li>
<p>Create a priority queue $Q$ and initialize it with $(a,b)$ with
priority $a_1 + b$ for all $b\in B$.</p>
</li>
<li>
<p>Repeat the following until $Q$ is empty:</p>
<ol>
<li>Pop $(a,b)$ off $Q$. This will form the next smallest sum, so print $a+b$.</li>
<li>Find $a'$ which immediately succeeds $a$ in our sorted list $A$.</li>
<li>Push $(a',b)$ with priority $a+b$ onto $Q$.</li>
</ol>
</li>
</ol>
<p>This algorithm will terminate, having printed each sum successively
with at most $O(\vert A\vert + \vert B\vert)$ space and almost linear
time. (The sorting and priority queue maintenance require some
logarithmic factors.)</p>
<p>With a little work, one can see why this works. In a sense it’s a
two-dimensional sorting problem, that depends on one crucial fact: If
$x \le y$ then $x+z \le y+z$. (This is to say that addition is
<em>monotonic</em>.) Given how the priority queue is constructed, it will
<em>always</em> contain the smallest sum.</p>
<p>Could we do this with permutations? If we have two lists of
permutations $A$ and $B$, and $a_1$ is the “smallest” (i.e.,
lexicographically least) permutation of $A$, and $b_1$ is the
“smallest” permutation of $B$, then it is <strong>patently not true</strong> that
$a_1\circ b_1$ is the smallest element of $A\circ B$. In symbols,</p>
<p>$$
(\min A) \circ (\min B) \neq \min (A\circ B).
$$</p>
<p>Similarly, if two permutations satisfy $a &lt; b$, then it is <strong>patently
not true</strong> that</p>
<p>$$
a\circ z &lt; b\circ z
$$</p>
<p>for a permutation $z$.</p>
<p>The monotonicity of addition is what allows us to do steps 3.2 and 3.3
so easily. If we did the same with permutations, we would no longer
have the guarantee that the minimum composition exists within the
queue.</p>
<p>This was the next hurdle Shamir cleared. Constant in the size of $A$
or $B$, Shamir found a way to solve the following problem: Given a
permutation $a\in A$ and $b\in B$, find the element $b'\in B$ such
that $a\circ b'$ immediately succeeds $a\circ b$. In other words, we
can generate, one-by-one, a sequence of $b$’s needed for step 3.2 and
3.3. With this algorithm (which we’ll describe in the next section),
our Shamir–Schroeppel algorithm for permutations becomes the
following:</p>
<p><strong>Algorithm (Walk Products)</strong>:</p>
<ol>
<li>Initialize an empty priority queue $Q$ whose elements are pairs of
permutations with priority determined by another permutation in
lexicographic ordering.</li>
<li>For each permutation $b\in B$:
<ol>
<li>With Shamir’s trick, find the $a\in B$ such that $a\circ b = \min (A\circ b)$.</li>
<li>Push $(a, b)$ onto $Q$ with priority $a\circ b$.</li>
</ol>
</li>
</ol>
<ul>
<li>(Invariant: At this point, we will certainly have $\min (A\circ B)$ in the queue.)</li>
</ul>
<ol start="3">
<li>Repeat the following until $Q$ is empty:
<ol>
<li>Pop $(a,b)$ off $Q$. This will form the next smallest $a\circ b$, so print it<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup>.</li>
<li>With Shamir’s trick, find $a'$ such that $a'\circ b$ immediately succeeds $a\circ b$.</li>
<li>Push $(a',b)$ with priority $a'\circ b$ onto $Q$.</li>
</ol>
</li>
</ol>
<p>This algorithm will produce the elements of $A\circ B$, one-by-one in
lexicographic order.</p>
<p>What is Shamir’s trick? We need a data structure and a clever observation.</p>
<h3 id="permutation-tries">Permutation tries</h3>
<p>In order to handle sets of ordered permutations better, Shamir created
a data structure. I call it a permutation trie. A <em>permutation trie</em>
of size-$k$ permutations is a $k$-deep, $k$-ary tree, such that a
path from root-to-leaf follows the elements of a permutation. The leaf
contains data which we want to associate with the permutation.</p>
<p>For example, consider permutations of size $5$. Suppose we wanted to
associate the symbol $\texttt{p6}$ with the permutation
$(2,4,1,3,5)$. Then we would have a $5$-layer tree with a root node
$R$, such that $R[2][4][1][3][5] = \texttt{p6}$.</p>
<p>More generally, let’s associate the following symbols with the
following permutations in a permutation trie $R$:</p>
<p>$$
\begin{align*}
\texttt{p1} &amp;\leftarrow (1,2,3,4,5) &amp; \texttt{p2} &amp;\leftarrow (1,2,3,5,4) &amp; \texttt{p3} &amp;\leftarrow (1,2,4,3,5)\\
\texttt{p4} &amp;\leftarrow (1,2,5,3,4) &amp; \texttt{p5} &amp;\leftarrow (1,3,4,5,2) &amp; \texttt{p6} &amp;\leftarrow (2,4,1,3,5)\\
\texttt{p7} &amp;\leftarrow (4,1,3,2,5) &amp; \texttt{p8} &amp;\leftarrow (4,1,3,5,2) &amp; \texttt{p9} &amp;\leftarrow (5,1,2,3,4)\\
\end{align*}
$$</p>
<p>The trie would be a data structure that looks like this:</p>
<p><img src="https://www.stylewarning.com/posts/brute-force-rubiks-cube/images/perm-trie.svg" alt="An example permutatioen trie." decoding="async">
</p>
<p>Even though we don’t show them, conceptually, each node in the trie
has a full length-$5$ array, with some elements empty (i.e., there are
no children).</p>
<p>What’s good about this data structure? First and foremost, pre-order
traversal will visit the permutations in lexicographic order. We can
use this data structure to store two things at the leaves (i.e.,
$\texttt{p}n$):</p>
<ol>
<li>The actual permutation data structure representing that path, and</li>
<li>The word we used to construct that permutation.</li>
</ol>
<p>This is the data structure, and now we get to Shamir’s
insight. Suppose we have a permutation $s$ and a permutation trie $R$
(which represents a set of permutations), and we want to traverse
$s\circ R$ in lexicographic order. The naive way is to construct a new
trie, but we wish to avoid that. To explain the idea, we’ll choose a
concrete example.</p>
<p>Let’s use $R$ from above. Let $s := (3,1,4,2,5)$. (Note that $s\not\in
R$, but that’s not important.) We wish to find an $r'\in R$ such that
$s\circ r' = \min (s\circ R)$. Well, the smallest permutation would be
one such that $r'(1) = 2$, because then $s(r'(1)) = s(2) = 1$. Looking
at our trie $R$, we can see the only candidate is that associated with
$\texttt{p6}$: $(2,4,1,3,5)$, which is the minimum.</p>
<p>What about the next smallest $s\circ r''$? For ease, let’s call this
product $m$. We would want a permutatation such that $r''(1) = 4$,
because $m(1) = s(r''(1)) = s(1) = 2$. This time, there are two
candidates:</p>
<p>$$
(4,1,3,2,5)\qquad (4,1,3,5,2)
$$</p>
<p>So at least we know $m = (2, \ldots)$. To disambiguate, we need to
look at $r''(2)$. These are the same, likewise $r''(3)$, so we have no
degree of freedom at $2$ or $3$ to minimize the product. Thus $m = (2,
3, 4, \ldots)$. We have a choice at $r''(4)$, however. The best choice
is $r''(4) = 2$, because $m(4) = s(r''(4)) = s(2) = 1$, the smallest
possible choice. This disambiguates our choice of $r''$ to be
$(4,1,3,2,5)$ so that $m = (2,3,4,1,5)$.</p>
<p>We could repeat the procedure to find the next smallest product
$s\circ r'''$. What exactly is the procedure here? Well, we walked
down the tree $R$, but instead of walking down it straight, we instead
did so in a permuted order based on $s$—specifically
$s^{-1}$. Consider our normal algorithm for walking the tree<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup> in
lexicographic order:</p>
<pre tabindex="0"><code>function walkLex(R):
  if notTree(R):
    print R
  else:
    for i from 1 to length(R):
      if R[i] exists:
        walkLex(R[i])
</code></pre><p>We can instead walk in <em>permuted</em> order, so that we produce a sequence
$[r, r'', r''', \ldots]$ such that</p>
<p>$$
s\circ r &lt; s \circ r' &lt; s \circ r''' &lt; \cdots,
$$</p>
<p>we modify our walking algorithm as so:</p>
<pre tabindex="0"><code>function walkProductLex(R, s):
  walk'(R, inverse(s))

function walk'(R, s):
  if notTree(R):
    print R
  else:
    for i from 1 to length(R):
      j = s(i)
      if R[j] exists:
        walk'(R[j], s)
</code></pre><p>Note that $s$ was inverted before the recursion to make quick permuting of each node.</p>
<p>With this, we have the remarkable ability to iterate through products
in lexicographic order, without having to enumerate them all and sort
them. This was the last and critical ingredient.</p>
<h3 id="the-4-list-algorithm-and-solving-the-rubiks-cube">The 4-List Algorithm and solving the Rubik’s Cube</h3>
<p>Now we want to put this all together to create the <em>4-List
Algorithm</em>. Let’s restate the problem in clear terms.</p>
<p><strong>Problem (4-List)</strong>: Let $s$ be a permutation. Let $L_1$, $L_2$,
$L_3$, and $L_4$ be sets of permutations such that we know $s\in
L_4\circ L_3\circ L_2\circ L_1$. Find $l_1\in L_1$, $l_2\in L_2$,
$l_3\in L_3$, and $l_4\in L_4$ such that $s = l_4\circ l_3\circ
l_2\circ l_1$.</p>
<p>Piecing together the elements above, we arrive at the 4-List Algorithm.</p>
<p><strong>Algorithm (4-List)</strong>:</p>
<ol>
<li>Construct $L'_3 := L_3^{-1}\circ s$ and $L'_4 := L_4^{-1}$.</li>
<li>Create two generators<sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup>: $X_1$ that walks $L_2\circ L_1$ in
lexicographic order, and $X_2$ that walks $L'_3\circ L'_4$ in
lexicographic order. Do this by using the <strong>Walk Products</strong>
algorithm, which itself is implemented by constructing permutation
tries and using <code>walkProductLex</code>.</li>
<li>Call <code>findCommon</code> on $X_2$ and $X_1$. This is guaranteed to find a
solution $(l_3^{-1},l_4^{-1}\circ s,l_2,l_1)$. Process the solution
to return $(l_4, l_3, l_2, l_1)$.</li>
</ol>
<p>The main difficulty of this algorithm, aside from implementing each
subroutine correctly, is plumbing the right data around.</p>
<p>Now, we can use this to solve a scrambled Rubik’s Cube $s$.</p>
<p><strong>Algorithm (Solve Cube)</strong>:</p>
<ol>
<li>Let $L = \bar C^5$, keeping a record of the words used to construct
each element of $L$. (We recommend immediately making a permutation
trie, where the leaves store the words.)</li>
<li>Apply the <strong>4-List Algorithm</strong> to the problem $s \in L\circ L\circ
L\circ L$ to produce $(l_4, l_3, l_2, l_1)$.</li>
<li>Return words $(w_4, w_3, w_2, w_1)$ associated with the
permutations $(l_4, l_3, l_2, l_1)$.</li>
</ol>
<p>Amazingly, this algorithm really works, and answers our blog post
question in the affirmative: <em>yes, the Rubik’s Cube can be
brute-forced</em>.</p>
<h2 id="example-and-source-code">Example and source code</h2>
<p>This algorithm is implemented in Common Lisp, in my computational
group theory package
<a href="https://github.com/stylewarning/cl-permutation">CL-PERMUTATION</a>. CL-PERMUTATION
already has built in support for Rubik’s Cubes as permutation
groups. Starting a new Common Lisp session, we have the following:</p>
<pre tabindex="0"><code>&gt; (ql:quickload '(:cl-permutation :cl-permutation-examples))
&gt; (in-package :cl-permutation)
&gt; (group-order (perm-examples:make-rubik-3x3))
43252003274489856000
&gt; (format t "~R" *)
forty-three quintillion two hundred fifty-two quadrillion three trillion
two hundred seventy-four billion four hundred eighty-nine million
eight hundred fifty-six thousand
NIL
</code></pre><p>The built-in Rubik’s Cube model only uses $\{F, R, U, B, L, D\}$, so
we make new generators corresponding to $\bar C$.</p>
<pre tabindex="0"><code>&gt; (defvar *c (loop :with cube := (perm-examples:make-rubik-3x3)
                   :for g :in (perm-group.generators cube)
                   :collect (perm-expt g 1)
                   :collect (perm-expt g 2)
                   :collect (perm-expt g 3)))
*C
&gt; (length *c)
18
</code></pre><p>Now we construct $\bar C^5$.</p>
<pre tabindex="0"><code>&gt; (defvar *c5 (generate-words-of-bounded-length *c 5))
*C5
&gt; (perm-tree-num-elements *c5)
621649
</code></pre><p>Note that this constructs a <code>perm-tree</code> object, which automatically
stores the words associated with each permutation generated.</p>
<p>Now let’s generate a random element of the cube group.</p>
<pre tabindex="0"><code>&gt; (defvar *s (random-group-element (perm-examples:make-rubik-3x3)))
*S
&gt; *s
#&lt;PERM 43 44 41 20 47 11 28 9 24 13 17 42 36 40 37 25 6 21 1 29 7 19 10 3 35 39 22 18 34 33 31 48 16 15 30 2 23 32 26 46 8 4 27 12 45 14 5 38&gt;
</code></pre><p>Lastly, we run the 4-list algorithm and wait.</p>
<pre tabindex="0"><code>&gt; (decompose-by-4-list *s *c5 *c5 *c5 *c5 :verbose t)
10,000,000: 52 sec @ 192,553 perms/sec; .0013% complete, eta 1114 hours 58 minutes
20,000,000: 48 sec @ 206,858 perms/sec; .0026% complete, eta 1037 hours 51 minutes
Evaluation took:
  145.094 seconds of real time
  145.097120 seconds of total run time (144.961382 user, 0.135738 system)
  [ Run times consist of 2.405 seconds GC time, and 142.693 seconds non-GC time. ]
  100.00% CPU
  421,375,385,955 processor cycles
  11,681,934,352 bytes consed

((8 11 14 2 4)
 (1 16 9 15 1)
 (7 6 18 8 15)
 (9 13 16 15 8))
</code></pre><p>We are pretty lucky this one ended in a mere 2 minutes 25 seconds! It
usually isn’t so prompt with an answer.</p>
<p>The results are printed as four words: our $l_4$, $l_3$, $l_2$, and
$l_1$. Each integer $n$ represents the 1-indexed $n$th permutation of
$\bar C$ (ordered by how it was constructed). We can create a more
traditional notation:</p>
<pre tabindex="0"><code>&gt; (defvar *solution (reduce #'append *))
*SOLUTION
&gt; (defun notation (ws)
    (dolist (w (reverse ws))
      (multiple-value-bind (move order)
          (floor (1- w) 3)
        (format t "~C~[~;2~;'~] "
                (aref "FRUBLD" move)
                order))))
NOTATION
&gt; (notation *solution)
U2 L' D L U' L' U2 D' R' U F L' U' D F R F2 L2 B2 U2
</code></pre><p>How do we know if this is correct? We need to check that the
composition of this word equals our random element, which we do by
composing the word (using something CL-PERMUTATION calls a “free-group
homomorphism”), inverting the permutation, and composing it with our
scramble to see that it brings us to an identity permutation.</p>
<pre tabindex="0"><code>&gt; (defvar *hom (free-group-&gt;perm-group-homomorphism
                (make-free-group 18)
                (generate-perm-group *c)))
*HOM
&gt; (perm-compose (perm-inverse (funcall *hom *solution)) *s)
#&lt;PERM 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48&gt;
</code></pre><p>Indeed, we found a reconstruction of our cube.</p>
<h2 id="tips-for-optimizing-the-4-list-algorithm">Tips for optimizing the 4-List Algorithm</h2>
<p>One of the most troubling aspects of implementing this algorithm is
making it fast enough. My initial implementation worked at a whopping
200 permutations per second. That’s incredibly slow, and meant that it
would take well over a century (in the worst case) for my program to
finish. Now, it works at about 190,000 permutations per second, with
an estimated worst-case search time of 2 months. (I haven’t
encountered a scrambled cube position which has taken more than 10
hours.)</p>
<p>Here are some ways I sped things up.</p>
<ol>
<li>Be economical with memory. When doing exploratory programming, it’s
desirable to tag and store everything, but each of those storages
and accesses take time.</li>
<li><em>Don’t</em> use actual arrays in the permutation trie. When I did that,
I ran out of memory. I instead opted for a sparse representation
using an “a-list” (that is, a linked list of <code>(index, value)</code>
pairs).</li>
<li>Make the permutation handling fast, like composition, equality
testing, and lexicographic ordering. I was originally using generic
arithmetic and 64-bits to represent each permutation element, and
it degraded speed.</li>
<li>Use a good priority queue implementation. You’ll be pushing and
popping hundreds of millions of elements.</li>
<li>Do some analysis and compress the permutation trie
representation. Most nodes of the trie will only contain one
value. If that’s the case, just store instead the permutation (and
whatever value associated with it) at the shallowest depth. This
will save a lot of time by avoiding a lot of needless (permuted)
recursion.</li>
</ol>
<p>If you have other tips for speeding up the algorithm, please email me!</p>
<h2 id="sample-benchmarks">Sample benchmarks</h2>
<p>In the following, we only consider the problem of solving the Rubik’s
Cube using the 4-list algorithm, assuming a solution length of 20
moves.</p>
<p>My computer is a ThinkPad 25th Anniversary Edition. It has an Intel
Core i7-7500U processor at 2.70 GHz, but boosting to 3.50 GHz. It has
32 GiB of RAM, but comfortably runs the solver with around 3–4 GiB.</p>
<p>The algorithm as implemented is able to check around 190,000 elements
per second.</p>
<p>Generating the move lists and pre-processing is a relatively fixed
cost. The lists can be generated once, but the preprocessing (i.e.,
composing the scramble with one of the lists) needs to happen each
solve. In my implementation, the initialization cost is consistently 9
seconds.</p>
<p>After initialization, the search is conducted. The run time varies
wildly, anywhere from seconds to hours.</p>
<ul>
<li>64 s, 188 billion CPU cycles, 4 GiB of allocation</li>
<li>165 s, 480 billion CPU cycles, 12 GiB of allocation</li>
<li>2210 s, 6 trillion CPU cycles, 162 GiB of allocation</li>
<li>4613 s, 13 trillion CPU cycles, 356 GiB of allocation</li>
<li>24010 s, 70 trillion CPU cycles, 2 TiB of allocation</li>
</ul>
<p>These are randomly sampled Rubik’s Cube scrambles, sorted by time.</p>
<p>In principle, with the current level of optimization, the algorithm
can take as much as 2 months to finish. I’m confident that my
implementation can be brought down a factor of 2, less confident it
can be easily brought down a factor of 50—but it wouldn’t surprise
me either way.</p>
<p>One interesting thing about this algorithm is that it seems to return
very, very quickly if the solution is 10 or fewer moves. Why? I
haven’t done a careful analysis, but I believe it is essentially
because the solution will be in $L_2\circ L_1$. The permutations $l_3$
and $l_4$ will be identity, which reduces to the problem of just
finding $s\in L_2\circ L_1$.</p>
<h2 id="conclusion">Conclusion</h2>
<p>“Meet in the middle” algorithms are old and well understood. When we
can’t brute-force an entire space, we can try splitting it in two and
try to combine them. That’s of course the spirit of the 4-List
Algorithm, but the devil is always in the details, and I hope this
blog post showed a lot of disparate facts needed to come together to
realize the algorithm.</p>
<p>I think the algorithm communicated by Shamir and his colleagues has
been remarkable but forgotten. While better algorithms exist for the
specific task of solving the Rubik’s Cube, the generality of the
4-List Algorithm ought not be understated.</p>
<h2 id="references">References</h2>
<ol>
<li>A. Fiat, S. Moses, A. Shamir, I. Shimshoni and G. Tardos, “Planning and learning in permutation groups,” 30th Annual Symposium on Foundations of Computer Science, Research Triangle Park, NC, USA, 1989, pp. 274–279, doi: 10.1109/SFCS.1989.63490. (<a href="https://ieeexplore.ieee.org/document/63490">Link</a>)</li>
<li>A. Bawden. “Shamir’s talk really was about how to solve the cube!”. Alan Bawden. From the <em>Cube Lovers</em> mailing list. 27 May 1987. (<a href="http://www.math.rwth-aachen.de/~Martin.Schoenert/Cube-Lovers/Alan_Bawden__Shamir%27s_talk_really_was_about_how_to_solve_the_cube!.html">Link</a>)</li>
</ol>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><em>The</em> Rubik’s Cube? Why not just “Rubik’s Cube”?!&nbsp;<a href="#fnref:1" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><em>Iterative-deepening depth-first search</em> (IDDFS) is an interesting hybrid between breadth-first and depth-first search. Breadth-first search (BFS) can find an optimal path to a target, but requires lots of memory to keep track of nodes that have been seen. Depth-first search (DFS) uses almost no memory, but can’t guarantee finding the shortest path. IDDFS is an algorithm which tries DFS up to a maximum depth of 1, then of 2, then of 3, etc. until a path to the target is found. While we re-visit nodes in each successive increase in the maximum depth, the savings in memory and the guarantee of finding the shortest path usually make it worth it.&nbsp;<a href="#fnref:2" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>A heuristic might be something like this. First, suppose we’ve built a table which maps every <em>corner</em> configuration (ignoring edges) to the number of moves needed to solve it. This problem can be brute-forced, as there are “only” $8!\cdot 3^7=88,179,340$ corner configurations. Suppose we are doing IDDFS to solve a whole Rubik’s Cube, and the algorithm is currently at a depth limit of 10. During our DFS (with a limited depth), we arrive at a position at depth 7, and want to decide if we shall continue with it. We can consult our corner configuration table: If we would require more than 3 moves to solve just the corners, then there’s no hope in continuing, since we’ll exceed our depth limit of 10. So we drop the line of search on this configuration entirely by returning from the depth-7 recursive call empty-handed.&nbsp;<a href="#fnref:3" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>The centers are typically seen as immobile, and hence aren’t numbered.&nbsp;<a href="#fnref:4" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>Shamir, isn’t that name familiar? Yes, he’s the ‘S’ from “RSA”, the encryption algorithm for which he and colleagues ‘R’ Rivest and ‘A’ Adleman won a Turing award.&nbsp;<a href="#fnref:5" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>Formally, any subset of five elements of $C$ generates the Rubik’s Cube group.&nbsp;<a href="#fnref:6" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p>Rubik’s Cube enthusiasts have names for these concepts. If we measure the length of a move sequence by the number of quarter turns, we say we are measuring in the <em>quarter-turn metric</em> or <em>QTM</em>. If instead we are measuring the length of a move sequence by the number of face turns of any degree, we say we are measuring in the <em>half-turn metric</em> or <em>HTM</em>.&nbsp;<a href="#fnref:7" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p>Speedsolvers like to write words in last-to-first order, so they can read off the moves as they’re applied.&nbsp;<a href="#fnref:8" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p>A note on the phrase “print it”. We use the term “print it” to signify that the permutation has been constructed and it may be consumed. We might not literally <em>print it</em>, and instead <em>emit it</em> for use. What this means precisely depends on the programming language you’re using. In our final algorithm, we’ll actually need to explicitly construct generators, so keep that in mind.&nbsp;<a href="#fnref:9" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:10" role="doc-endnote">
<p>Again, as in the other footnote, we can see “walking” or “printing” or … as again a manifestation of a process of generating something one-by-one.&nbsp;<a href="#fnref:10" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:11" role="doc-endnote">
<p>Construct generators?! This is the third footnote dedicated to walking/printing/generating, because it’s important and sometimes difficult. Making a generator may be utterly trivial in your language (Scheme with <code>call/cc</code> or Python with <code>yield</code>), cumbersome (Common Lisp with <code>cl-cont</code>), or downright annoying. One trick we used when implementing the algorithm in Common Lisp is to keep track of where we are in the permutation trie by a permutation itself. We can always go to the next one if we can find the current one.&nbsp;<a href="#fnref:11" role="doc-backlink">↩︎</a></p>
</li>
</ol>
</section>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reverse-engineering the 8086 processor's address and data pin circuits (116 pts)]]></title>
            <link>https://www.righto.com/2023/07/8086-pins.html</link>
            <guid>36645821</guid>
            <pubDate>Sat, 08 Jul 2023 16:17:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.righto.com/2023/07/8086-pins.html">https://www.righto.com/2023/07/8086-pins.html</a>, See on <a href="https://news.ycombinator.com/item?id=36645821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-2795813864611579036" itemprop="description articleBody">


<p>The Intel 8086 microprocessor (1978) started the x86 architecture that continues to this day.
In this blog post, I'm focusing on a small part of the chip: the address and data pins that connect the chip to
external memory and I/O devices.
In many processors, this circuitry is straightforward, but it is complicated in the 8086 for two reasons.
First, Intel decided to package the 8086 as a 40-pin DIP, which didn't provide enough pins for all the functionality.
Instead, the 8086 multiplexes address, data, and status.
In other words, a pin can have multiple roles, providing an address bit at one time and a data bit at another time.</p>
<p>The second complication is that the 8086 has a 20-bit address space (due to its infamous segment registers), while the
data bus is 16 bits wide.
As will be seen, the "extra" four address bits have more impact than you might expect.
To summarize, 16 pins, called AD0-AD15, provide 16 bits of address and data.
The four remaining address pins (A16-A19) are multiplexed for use as status pins,
providing information about what the processor is doing for use by other parts of the system.
You might expect that the 8086 would thus have two types of pin circuits, but it turns out that there are four
distinct circuits, which I will discuss below.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/die-labeled.jpg"><img alt="The 8086 die under the microscope, with the main functional blocks and address pins labeled. Click this image (or any other) for a larger version." height="623" src="https://static.righto.com/images/8086-ad-pins/die-labeled-w700.jpg" title="The 8086 die under the microscope, with the main functional blocks and address pins labeled. Click this image (or any other) for a larger version." width="700"></a></p><p>The 8086 die under the microscope, with the main functional blocks and address pins labeled. Click this image (or any other) for a larger version.</p>
<p>The microscope image above shows the silicon die of the 8086.
In this image, the metal layer on top of the chip is visible, while the silicon and polysilicon underneath are obscured.
The square pads around the edge of the die are connected by tiny bond wires to the chip's 40 external pins.
The 20 address pins are labeled: Pins AD0 through AD15 function as
address and data pins. Pins A16 through A19 function as address pins and status pins.<span id="fnref:ad"><a href="#fn:ad">1</a></span>
The circuitry that controls the pins is highlighted in red.
Two internal busses are important for this discussion: the 20-bit AD bus (green) connects the AD pins to the rest of the CPU,
while the 16-bit C bus (blue) communicates with the registers.
These buses are connected through a circuit that can swap the byte order or shift the value.
(The lines on the diagram are simplified; the real wiring twists and turns to fit the layout.
Moreover, the C bus (blue) has its bits spread across the width of the register file.)</p>
<h2>Segment addressing in the 8086</h2>
<p>One goal of the 8086 design was to maintain backward compatibility with the earlier 8080 processor.<span id="fnref:compatibility"><a href="#fn:compatibility">2</a></span>
This had a major impact on the 8086's memory design, resulting in the much-hated segment registers.
The 8080 (like most of the 8-bit processors of the early 1970s) had a 16-bit address space, able to access 64K (65,536 bytes) of memory,
which was plenty at the time.
But due to the exponential growth in memory capacity described by Moore's Law, it was clear that the 8086 needed to
support much more. Intel decided on a 1-megabyte address space, requiring 20 address bits.
But Intel wanted to keep the 16-bit memory addresses used by the 8080.</p>
<p>The solution was to break memory into segments. Each segment was 64K long, so a 16-bit offset was sufficient to access memory
in a segment.
The segments were allocated in a 1-megabyte address space, with the result that you could access a megabyte of memory, but
only in 64K chunks.<span id="fnref:pointers"><a href="#fn:pointers">3</a></span>
Segment addresses were also 16 bits, but were shifted left by 4 bits (multiplied by 16) to support the 20-bit address space.</p>
<p>Thus, every memory access in the 8086 required a computation of the physical address.
The diagram below illustrates this process: the logical address consists of the segment base address and the offset within the segment.
The 16-bit segment register was shifted 4 bits and added to the 16-bit offset to yield the 20-bit physical memory address.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/physical-address-generation.jpg"><img alt="The segment register and the offset are added to create a 20-bit physical address.  From iAPX 86,88 User's Manual, page 2-13." height="260" src="https://static.righto.com/images/8086-ad-pins/physical-address-generation-w500.jpg" title="The segment register and the offset are added to create a 20-bit physical address.  From iAPX 86,88 User's Manual, page 2-13." width="500"></a></p><p>The segment register and the offset are added to create a 20-bit physical address.  From <a href="http://www.bitsavers.org/components/intel/_dataBooks/1981_iAPX_86_88_Users_Manual.pdf">iAPX 86,88 User's Manual</a>, page 2-13.</p>
<p>This address computation was not performed by the regular ALU (Arithmetic/Logic Unit), but by a separate adder that
was devoted to address computation.
The address adder is visible in the upper-left corner of the die photo.
I will discuss the address adder in more detail below.</p>
<h2>The AD bus and the C Bus</h2>
<p>The 8086 has multiple internal buses to move bits internally, but the relevant ones are the AD bus and the C bus.
The AD bus is a 20-bit bus that connects the 20 address/data pins to the internal circuitry.<span id="fnref:patent"><a href="#fn:patent">4</a></span>
A 16-bit bus called the C bus provides the connection between
the AD bus, the address adder and some of the registers.<span id="fnref:registers"><a href="#fn:registers">5</a></span>
The diagram below shows the connections.
The AD bus can be connected to the 20 address pins through latches. The low 16 pins can also be used for data input, while the upper 4 pins
can also be used for status output.
The address adder performs the 16-bit addition necessary for segment arithmetic. Its output is shifted left by four bits
(i.e. it has four 0 bits appended), producing the 20-bit result.
The inputs to the adder are provided by registers, a constant ROM that holds small constants such as +1 or -2, or the C bus.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/buses.jpg"><img alt="My reverse-engineered diagram showing how the AD bus and the C bus interact with the address pins." height="311" src="https://static.righto.com/images/8086-ad-pins/buses-w350.jpg" title="My reverse-engineered diagram showing how the AD bus and the C bus interact with the address pins." width="350"></a></p><p>My reverse-engineered diagram showing how the AD bus and the C bus interact with the address pins.</p>
<p>The shift/crossover circuit provides the interface between these two buses, handling the 20-bit to 16-bit conversion. The busses can be connected in three ways: direct, crossover, or shifted.<span id="fnref:swapping"><a href="#fn:swapping">6</a></span>
The direct mode connects the 16 bits of the C bus to the lower 16 bits of the address/data pins.
This is the standard mode for transferring data between the 8086's internal circuitry and the data pins.
The crossover mode performs the same connection but swaps the bytes. This is typically used for unaligned memory accesses, where the low memory byte corresponds to
the high register byte, or vice versa.
The shifted mode shifts the 20-bit AD bus value four positions to the right.
In this mode, the 16-bit output from the address adder goes to the 16-bit C bus.
(The shift is necessary to counteract the 4-bit shift applied to the address adder's output.)
Control circuitry selects the right operation for the shift/crossover circuit at the right time.<span id="fnref:shift"><a href="#fn:shift">7</a></span></p>
<p>Two of the registers are invisible to the programmer but play an important role in memory accesses.
The <code>IND</code> (Indirect) register specifies the memory address; it holds the 16-bit memory offset in a segment.
The <code>OPR</code> (Operand) register holds the data value.<span id="fnref:prefetch"><a href="#fn:prefetch">9</a></span>
The <code>IND</code> and <code>OPR</code> registers are not accessed directly by the programmer; the microcode for a machine instruction moves the appropriate
values to these registers prior to the write.</p>
<h2>Overview of a write cycle</h2>
<p>I hesitate to present a timing diagram, since I may scare off of my readers,
but the 8086's communication is designed around a four-step bus cycle.
The diagram below shows simplified timing for a write cycle, when the 8086 writes to memory or an I/O device.<span id="fnref:timing"><a href="#fn:timing">8</a></span>
The external bus activity is organized as four states, each one clock cycle long: T1, T2, T3, T4.
These T states are very important since they control what happens on the bus.
During T1, the 8086 outputs the address on the pins. During the T2, T3, and T4 states, the 8086 outputs the data word on the pins.
The important part for this discussion is that the pins are multiplexed depending on the T-state: the pins provide the address during T1 and data during
T2 through T4.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/write-cycle.jpg"><img alt="A typical write bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16." height="130" src="https://static.righto.com/images/8086-ad-pins/write-cycle-w700.jpg" title="A typical write bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16." width="700"></a></p><p>A typical write bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16.</p>
<p>There two undocumented T states that are important to the bus cycle.
The physical address is computed in the two clock cycles before T1 so the address will be available in T1.
I give these "invisible" T states the names TS (start) and T0.</p>
<h2>The address adder</h2>
<!--
This computation is a bit tricky because the input buses and the adder are 16 bits, but the physical address is 20 bits.
-->
<p>The operation of the address adder is a bit tricky since the 16-bit adder must generate a 20-bit physical address.
The adder has two 16-bit inputs: the B input is connected to the upper registers via the B bus, while the C input is connected to the C bus.
The segment register value is transferred over the B bus to the adder during the second half
of the TS state (that is, two clock cycles before the bus cycle becomes externally visible during T1).
Meanwhile, the address offset is transferred over the C bus to the adder, but the adder's C input shifts the value four bits to the right,
discarding the four low bits. (As will be explained later, the pin driver circuits latch these bits.)
The adder's output is shifted left four bits and transferred to the AD bus during the second half of T0. 
This produces the upper 16 bits of the 20-bit physical memory address.
This value is latched into the address output flip-flops at the start of T1, putting the computed address on the pins.
To summarize, the 20-bit address is generated by storing the 4 low-order bits during T0 and then the 16 high-order sum bits
during T1.</p>
<p>The address adder is not needed for segment arithmetic during T1 and T2.
To improve performance, the 8086 uses the adder during this idle time to increment or decrement memory addresses.
For instance, after popping a word from the stack, the stack pointer needs to be incremented by 2.
The address adder can do this increment "for free" during T1 and T2, leaving the ALU available for other operations.<span id="fnref:pipelining"><a href="#fn:pipelining">10</a></span>
Specifically, the adder updates the memory address in <code>IND</code>, incrementing it or decrementing it as appropriate.
First, the <code>IND</code> value is transferred over the B bus to the adder during the second half of T1.
Meanwhile, a constant (-3 to +2) is loaded from the Constant ROM and transferred to the adder's C input.
The output from the adder is transferred to the AD bus during the second half of T2.
As before, the output is shifted four bits to the left. However, the shift/crossover circuit between the AD bus and the C bus
is configured to shift four bits to the right, canceling the adder's shift.
The result is that the C bus gets the 16-bit sum from the adder, and this value is stored in the <code>IND</code> register.<span id="fnref:predecrement"><a href="#fn:predecrement">11</a></span>
For more information on the implemenation of the address adder, see my <a href="https://www.righto.com/2020/08/reverse-engineering-adder-inside-intel.html">previous blog post</a>.</p>
<!-- 
The use of the address pins is closely tied to the 8086's external timing.
The diagram below shows how a typical bus cycle is divided into four "T" states, each one corresponding to one clock cycle.
During T1, the CPU puts the memory address on the bus using the address pins.
During T3 and T4, the CPU writes to memory by putting the data value on the data pins.
Alternatively, the CPU reads from memory by reading the data value during T3 and T4.
State T2 acts a buffer period to ensure that memory and the CPU don't try to write to the bus at the same time.

![A typical bus cycle consists of four T states. Diagram from The 8086 Family Users Manual, figure 4-5.](bus-cycle.jpg "w500")
-->

<h2>The pin driver circuit</h2>
<p>Now I'll dive down to the hardware implementation of an output pin.
When the 8086 chip communicates with the outside world, it needs to provide relatively high currents.
The tiny logic transistors can't provide enough current, so the chip needs to use large output transistors.
To fit the large output transistors on the die, they are constructed of multiple wide transistors in parallel.<span id="fnref:ratio"><a href="#fn:ratio">12</a></span>
Moreover, the drivers use a somewhat unusual "superbuffer" circuit with two transistors: one to pull the output high, and one to pull the output low.<span id="fnref:superbuffer"><a href="#fn:superbuffer">13</a></span></p>
<p>The diagram below shows the transistor structure for one of the output pins (AD10), consisting of three
parallel transistors between the output and +5V, and five parallel transistors between the output and ground.
The die photo on the
left shows the metal layer on top of the die. This shows the power and ground wiring and the connections to
the transistors.
The photo on the right shows the die with the metal layer removed, showing the underlying silicon and the
polysilicon wiring on top.
A transistor gate is formed where a polysilicon wire crosses the doped silicon region. 
Combined, the +5V transistors are equivalent to about 60 typical transistors, while the ground transistors are
equivalent to about 100 typical transistors.
Thus, these transistors provide substantially more current to the output pin.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/output-transistor.jpg"><img alt="Two views of the output transistors for a pin. The first shows the metal layer, while the second shows the polysilicon and silicon." height="402" src="https://static.righto.com/images/8086-ad-pins/output-transistor-w800.jpg" title="Two views of the output transistors for a pin. The first shows the metal layer, while the second shows the polysilicon and silicon." width="800"></a></p><p>Two views of the output transistors for a pin. The first shows the metal layer, while the second shows the polysilicon and silicon.</p>
<h3>Tri-state output driver</h3>
<p>The output circuit for an address pin uses a tri-state buffer, which allows the output to be disabled
by putting it into a high-impedance "tri-state" configuration.
In this state, the output is not pulled high or low but is left floating.
This capability allows the pin to be used for data input.
It also allows external devices to device can take control of the bus, for instance, to perform
DMA (direct memory access).</p>
<p>The pin is driven by two large MOSFETs, one to pull the output high and one to pull it low.
(As described earlier, each large MOSFET is physically multiple transistors in parallel, but I'll ignore that for now.)
If both MOSFETs are off, the output floats, neither on nor off.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/output-circuit.jpg"><img alt="Schematic diagram of a &quot;typical&quot; address output pin." height="230" src="https://static.righto.com/images/8086-ad-pins/output-circuit-w400.jpg" title="Schematic diagram of a &quot;typical&quot; address output pin." width="400"></a></p><p>Schematic diagram of a "typical" address output pin.</p>
<p>The tri-state output is implemented by driving the MOSFETs with two "superbuffer"<span id="fnref:and"><a href="#fn:and">15</a></span> AND gates.
If the <code>enable</code> input is low, both AND gates produce a low output and both output transistors are off.
On the other hand, if <code>enable</code> is high, one AND gate will be on and one will be off.
The desired output value is loaded into a flip-flop to hold it,<span id="fnref:clock"><a href="#fn:clock">14</a></span>
and the flip-flop turns one of the output transistors on, driving the output pin high or low as appropriate.
(Conveniently, the flip-flop provides the data output Q and the inverted data output <span>Q</span>.)
Generally, the address pin outputs are enabled for T1-T4 of a write but only during T1 for a read.<span id="fnref:enable"><a href="#fn:enable">16</a></span></p>
<p>In the remainder of the discussion, I'll use the tri-state buffer symbol below, rather than showing the implementation of the buffer.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/output-simplified.jpg"><img alt="The output circuit, expressed with a tri-state buffer symbol." height="137" src="https://static.righto.com/images/8086-ad-pins/output-simplified-w350.jpg" title="The output circuit, expressed with a tri-state buffer symbol." width="350"></a></p><p>The output circuit, expressed with a tri-state buffer symbol.</p>
<h3>AD4-AD15</h3>
<p>Pins AD4-AD15 are "typical" pins, avoiding the special behavior of the top and bottom pins, so I'll discuss them first.
The behavior of these pins is that the value on the AD bus is latched by the circuit and then put on the output pin
under the control of the <code>enaable</code> signal.
The circuit has three parts: a multiplexer to select the output value, a flip-flop to hold the output value, and a tri-state driver to
provide the high-current output to the pin.
In more detail, the multiplexer selects either the value on the AD bus or the current output from the flip-flop.
That is, the multiplexer can either load a new value into the flip-flop or hold the existing value.<span id="fnref:implementation"><a href="#fn:implementation">17</a></span>
The flip-flop latches the input value on the falling edge of the clock, passing it to the output driver.
If the enable line is high, the output driver puts this value on the corresponding address pin.</p>
<!-- datasheet: output low tested at 2.0mA, output high tested at -400 microamps -->

<p><a href="https://static.righto.com/images/8086-ad-pins/ad415.jpg"><img alt="The output circuit for AD4-AD15 has a latch to hold the desired output value, an address or data bit." height="129" src="https://static.righto.com/images/8086-ad-pins/ad415-w400.jpg" title="The output circuit for AD4-AD15 has a latch to hold the desired output value, an address or data bit." width="400"></a></p><p>The output circuit for AD4-AD15 has a latch to hold the desired output value, an address or data bit.</p>
<p>For a write, the circuit latches the address value on the bus during the second half of T0 and puts it on the pins during T1.
During the second half of the T1 state, the data word is transferred from the <code>OPR</code> register over the C bus to the AD bus and loaded
into the AD pin latches.
The word is transferred from the latches to the pins during T2 and held for the remainder of the bus cycle.</p>
<h3>AD0-AD3</h3>
<p>The four low address bits have a more complex circuit because these address bits are latched from the bus before the address adder computes its sum, as described earlier.
The memory offset (before the segment addition) will be on the C bus during the second half of TS and is loaded into the lower
flip-flop. This flip-flop delays these bits for one clock cycle and then they are loaded into the upper flip-flop.
Thus, these four pins pick up the offset prior to the addition, while the other pins get the result of the segment addition.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/ad03.jpg"><img alt="The output circuit for AD0-AD3 has a second latch to hold the low address bits before the address adder computes the sum." height="174" src="https://static.righto.com/images/8086-ad-pins/ad03-w500.jpg" title="The output circuit for AD0-AD3 has a second latch to hold the low address bits before the address adder computes the sum." width="500"></a></p><p>The output circuit for AD0-AD3 has a second latch to hold the low address bits before the address adder computes the sum.</p>
<p>For data, the AD0-AD3 pins transfer data directly from the AD bus to the pin latch, bypassing the delay that was used to get the address bits.
That is, the AD0-AD3 pins have two paths: the delayed path used for addresses during T0 and the direct path otherwise used for data.
Thus, the multiplexer has three inputs: two for these two paths and a third loop-back input to hold the flip-flop value.</p>
<!--
ad-latch-load loads the  AD0-15 latches.
-->

<!--
If the memory access was an instruction fetch,
the address adder is immediately reused to update the instruction pointer (program counter).
In the second half of T1, one input of the address adder is loaded with the instruction pointer increment from the constant ROM (2 if a word was fetched).
This value is added to the instruction pointer value and
the updated instruction pointer value is written back in the second half of T2.
A similar process is used for other memory accesses that update a pointer, such as stack operations or string operations.

If another bus cycle follows, the T3 and T4 states act like the T0 and T1 states described above, preparing the next memory address.
Thus, address calculation is pipelined in the 8086: the address adder performs the segment computation during the last half of the previous bus cycle, so the physical memory address will be ready at the start of the bus cycle.

For a memory write, the address latches are reloaded during T1, loading them with the `OPR` register???
-->

<h3>A16-A19: status outputs</h3>
<p>The top four pins (A16-A19) are treated specially, since they are not used for data.
Instead, they provide processor status during T2-T4.<span id="fnref:status"><a href="#fn:status">18</a></span> The pin latches for these
pins are loaded with the address during T0 like the other pins, but loaded with status instead of data during T1.
The multiplexer at the input to the latch selects the address bit during T0 and the status bit during T1, and
holds the value otherwise.
The schematic below shows how this is implemented for A16, A17, and A19.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/ad1619.jpg"><img alt="The output circuit for AD16, AD17, and AD19 selects either an address output or a status output." height="115" src="https://static.righto.com/images/8086-ad-pins/ad1619-w400.jpg" title="The output circuit for AD16, AD17, and AD19 selects either an address output or a status output." width="400"></a></p><p>The output circuit for AD16, AD17, and AD19 selects either an address output or a status output.</p>
<p>Address pin A18 is different because it indicates the current status of the interrupt enable flag bit.
This status is updated every clock cycle, unlike the other pins.
To implement this, the pin has a different circuit that isn't latched,
so the status can be updated continuously.
The clocked transistors act as "pass transistors", passing the signal through when active.
When a pass transistor is turned off, the following logic gate holds the previous value due to the capacitance of the
wiring.
Thus, the pass transistors provide a way of holding the value through the clock cycle.
The flip-flops are implemented with pass transistors internally, so in a sense the circuit below is a flip-flop
that has been "exploded" to provide a second path for the interrupt status.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/ad18.jpg"><img alt="The output circuit for AD18 is different from the rest so the I flag status can be updated every clock cycle." height="162" src="https://static.righto.com/images/8086-ad-pins/ad18-w540.jpg" title="The output circuit for AD18 is different from the rest so the I flag status can be updated every clock cycle." width="540"></a></p><p>The output circuit for AD18 is different from the rest so the I flag status can be updated every clock cycle.</p>
<h2>Reads</h2>
<p>A memory or I/O read also uses a 4-state bus cycle, slightly different from the write cycle.
During T1, the address is provided on the pins, the same as for a write.
After that, however, the output circuits are tri-stated so they float, allowing the external memory to put data on the bus.
The read data on the pin is put on the AD bus at the start of the T4 state.
From there, the data passes through the crossover circuit to the C bus. Normally the 16 data bits pass straight through to
the C bus, but the bytes will be swapped if the memory access is unaligned.
From the C bus, the data is written to the <code>OPR</code> register, a byte or a word as appropriate.
(For an instruction prefetch, the word is written to a prefetch queue register instead.)</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/read-cycle.jpg"><img alt="A typical read bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16." height="139" src="https://static.righto.com/images/8086-ad-pins/read-cycle-w600.jpg" title="A typical read bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16." width="600"></a></p><p>A typical read bus cycle consists of four T states. Based on The 8086 Family Users Manual, B-16.</p>
<p>To support data input on the AD0-AD15 pins, they have a circuit to buffer the input data and transfer it to the AD bus.
The incoming data bit is buffered by the two inverters and sampled when the clock is high.
If the enable' signal is low, the data bit is transferred to the AD bus when the clock is low.<span id="fnref:read-enable"><a href="#fn:read-enable">19</a></span>
The two MOSFETs act as a "superbuffer", providing enough current for the fairly long AD bus.
I'm not sure what the capacitor accomplishes, maybe avoiding a race condition if the data pin changes just as the clock goes low.<span id="fnref:race"><a href="#fn:race">20</a></span></p>
<p><a href="https://static.righto.com/images/8086-ad-pins/read-circuit.jpg"><img alt="Schematic of the input circuit for the data pins." height="134" src="https://static.righto.com/images/8086-ad-pins/read-circuit-w500.jpg" title="Schematic of the input circuit for the data pins." width="500"></a></p><p>Schematic of the input circuit for the data pins.</p>
<p>This circuit has a second role, precharging the AD bus high when the clock is low, if there's no data.
Precharging a bus is fairly common in the 8086 (and other NMOS processors) because NMOS transistors are better at pulling a
line low than pulling it high. Thus, it's often faster to precharge a line high before it's needed and then pull it low for a 0.<span id="fnref:adder"><a href="#fn:adder">21</a></span></p>
<p>Since pins A16-A19 are not used for data, they operate the same for reads as for writes: providing address bits and then status.</p>
<h2>The pin circuit on the die</h2>
<p>The diagram below shows how the pin circuitry appears on the die. The metal wiring has been removed to show the silicon and polysilicon.
The top half of the image is the input circuitry, reading a data bit from the pin and feeding it to the AD bus.
The lower half of the image is the output circuitry, reading an address or data bit from the AD bus and amplifying it for output
via the pad.
The light gray regions are doped, conductive silicon. The thin tan lines are polysilicon, which forms transistor gates where it crosses doped silicon.</p>
<p><a href="https://static.righto.com/images/8086-ad-pins/pin-labeled.jpg"><img alt="The input/output circuitry for an address/data pin. The metal layer has been removed to show the underlying silicon and polysilicon. Some crystals have formed where the bond pad was." height="482" src="https://static.righto.com/images/8086-ad-pins/pin-labeled-w600.jpg" title="The input/output circuitry for an address/data pin. The metal layer has been removed to show the underlying silicon and polysilicon. Some crystals have formed where the bond pad was." width="600"></a></p><p>The input/output circuitry for an address/data pin. The metal layer has been removed to show the underlying silicon and polysilicon. Some crystals have formed where the bond pad was.</p>
<h2>A historical look at pins and timing</h2>
<p>The number of pins on Intel chips has grown exponentially, more than a factor of 100 in 50 years.
In the early days, Intel management was convinced that a 16-pin package was large enough for any integrated circuit.
As a result, the Intel 4004 processor (1971) was crammed into a 16-pin package.
Intel chip designer Federico Faggin<span id="fnref:faggin"><a href="#fn:faggin">22</a></span> describes 16-pin packages as a completely silly requirement that was throwing away
performance,
but the "God-given 16 pins" was like a religion at Intel.
When Intel was forced to use 18 pins by the 1103 memory chip, it "was like the sky had dropped from heaven"
and he had "never seen so many long faces at Intel."
Although the 8008 processor (1972) was able to use 18 pins, this low pin count still harmed performance by forcing pins to be used for multiple
purposes.</p>
<p>The Intel 8080 (1974) had a larger, 40-pin package that allowed it to have 16 address pins and 8 data pins.
Intel stuck with this size for the 8086, even though competitors used larger packages with more pins.<span id="fnref:ti"><a href="#fn:ti">23</a></span>
As processors became more complex, the 40-pin package became infeasible and the pin count rapidly expanded;
The 80286 processor (1982) had a 68-pin package, while the
i386 (1985) had 132 pins; the i386 needed many more pins because it had a 32-bit data bus and a 24- or 32-bit address bus.
The i486 (1989) went to 196 pins while the original Pentium had 273 pins.
Nowadays, a modern <a href="https://www.intel.com/content/www/us/en/products/sku/232167/intel-core-i913900ks-processor-36m-cache-up-to-6-00-ghz/specifications.html">Core I9 processor</a> uses the <a href="https://en.wikipedia.org/wiki/LGA_1700">FCLGA1700</a> socket with a whopping 1700 contacts.</p>
<p>Looking at the history of Intel's bus timing, the 8086's complicated memory timing goes back to the Intel 8008 processor (1972). Instruction execution in the 8008 went through
a specific sequence of timing states; each clock cycle was assigned a particular state number.
Memory accesses took three cycles:
the address was sent to memory during states T1 and T2, half of the address at a time since there were only 8 address pins.
During state T3, a data byte was either transmitted to memory or read from memory.
Instruction execution took place during T4 and T5.
State signals from the 8008 chip indicated which state it was in.</p>
<!-- http://www.bitsavers.org/components/intel/MCS8/Intel_8008_8-Bit_Parallel_Central_Processing_Unit_Rev1_Apr72.pdf -->

<p>The 8080 used an even more complicated timing system.
An instruction consisted of one to five "machine cycles", numbered M1 through M5, where each machine cycle corresponded to
a memory or I/O access. Each machine cycle consisted of three to five states, T1 through T5, similar to the 8008 states.
The 8080 had 10 different types of machine cycle such as instruction fetch, memory read, memory write, stack read or write,
or I/O read or write. The status bits indicated the type of machine cycle.
The 8086 kept the T1 through T4 memory cycle. Because the 8086 decoupled instruction prefetching from execution, it no
longer had explicit M machine cycles. Instead, it used status bits to indicate 8 types of bus activity such as instruction
fetch, read data, or write I/O.</p>
<h2>Conclusions</h2>
<p>Well, address pins is another subject that I thought would be straightforward to explain but turned out to be surprisingly
complicated.
Many of the 8086's design decisions combine in the address pins: segmented addressing, backward compatibility, and the small 40-pin package.
Moreover, because memory accesses are critical to performance, Intel put a lot of effort into this circuitry.
Thus, the pin circuitry is tuned for particular purposes, especially pin A18 which is different from all the rest.</p>
<p>There is a lot more to say about memory accesses and how the 8086's Bus Interface Unit performs them.
The process is very complicated, with interacting state machines for memory operation and instruction prefetches, as well
as handling unaligned memory accesses.
I plan to write more, so 
follow me on Twitter <a href="https://twitter.com/kenshirriff">@kenshirriff</a> or <a href="https://www.righto.com/feeds/posts/default">RSS</a> for updates.
I've also started experimenting with Mastodon recently as <a href="https://oldbytes.space/@kenshirriff">@<span data-cfemail="eb808e859883829999828d8dab84878f89929f8e98c5989b8a888e">[email&nbsp;protected]</span></a>
and Bluesky as <a href="https://staging.bsky.app/profile/righto.com">@righto.com</a> so you can follow me there too.</p>
<h2>Notes and references</h2>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Langchain Is Pointless (319 pts)]]></title>
            <link>https://old.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/</link>
            <guid>36645575</guid>
            <pubDate>Sat, 08 Jul 2023 15:56:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/">https://old.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/</a>, See on <a href="https://news.ycombinator.com/item?id=36645575">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>It's filled with crap like this:</p>

<pre><code>    for i in range(n_results, 0, -1):
        try:
            return self._collection.query(
                query_texts=query_texts,
                query_embeddings=query_embeddings,
                n_results=i,
                where=where,
                **kwargs,
            )
</code></pre>

<p>and this:</p>

<pre><code>def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:
    texts = list(map(lambda x: x.replace("\n", " "), texts))
    embeddings = self.client.encode(texts, **self.encode_kwargs)
    return embeddings.tolist()
</code></pre>

<p>and this:</p>

<pre><code>class CharacterTextSplitter(TextSplitter):
    """Implementation of splitting text that looks at characters."""

def __init__(self, separator: str = "\n\n", **kwargs: Any):
    """Create a new TextSplitter."""
    super().__init__(**kwargs)
    self._separator = separator

def split_text(self, text: str) -&gt; List[str]:
    """Split incoming text and return chunks."""
    # First we naively split the large input into a bunch of smaller ones.
    if self._separator:
        splits = text.split(self._separator)
    else:
        splits = list(text)
</code></pre>

<p>In short: <a href="https://i.imgur.com/OffEJTR.gifv">https://i.imgur.com/OffEJTR.gifv</a></p>

<p>Embeddings is just a do-nothing wrapper for SentenceTransformers. Chroma is just a do-nothing wrapper for ChromaDB. It's filled with "helper" functions that just call normal Python functions. A dedicated TextSplitter that calls split() from builtins.py? What? Why? Templates are no more useful than calling .replace() on a string. "texts" are just strings and "documents" are just a pointless dict that contain "texts." Just load the strings from your datasource yourself. The README is both grandiose and vague. The documentation is out-of-date and inconsistent. The import footprint is weirdly massive--highly modularized but nothing seems to do anything that'd take more than a few CPU cycles. There's not really a standard interoperable datatype, so you're actually led further afield than if you had just clearly defined the simple lists and strings required for hitting an LLM. </p>

<p>The very concept of chaining operations when interacting with LLMs doesn't really make sense to me: it's basically one <code>requests</code> call to a generation backend, but it's not like it even handles websockets and streaming for you. Why chain together wrapper classes when you can just do the operations yourself?</p>

<p>This seems like a beginner's project that blew up because it's riding a tidal wave of interest in the broader topic.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Flickr Foundation is building a new bridge between Flickr and Wikimedia Commons (133 pts)]]></title>
            <link>https://diff.wikimedia.org/2023/07/07/flickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons/</link>
            <guid>36645448</guid>
            <pubDate>Sat, 08 Jul 2023 15:42:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diff.wikimedia.org/2023/07/07/flickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons/">https://diff.wikimedia.org/2023/07/07/flickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons/</a>, See on <a href="https://news.ycombinator.com/item?id=36645448">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-97236">
	<div>
			


<p>We are pleased to announce a new partnership with the <a href="https://www.flickr.org/">Flickr Foundation</a> to extend the great work already done via the <a href="https://commons.wikimedia.org/wiki/Commons:Flickr2Commons">Flickr2Commons</a> tool to make it even easier to upload CC-licensed images from Flickr into Wikimedia Commons.&nbsp;</p>


<div>
<figure><a href="https://diff.wikimedia.org/?attachment_id=97298"><img decoding="async" loading="lazy" src="https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231" alt="" width="256" height="231" srcset="https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231?w=1024 1024w, https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231?w=768 768w" sizes="(max-width: 256px) 100vw, 256px" data-recalc-dims="1"></a></figure></div>


<p>Wikipedia is a foundational source of information on the internet. It provides content to Google and other search engines, social media platforms, voice assistants, and, increasingly, AI applications. To illustrate that information, we have Wikimedia Commons, the central visual platform for Wikipedia and one of the primary sources for open licensed visual content online. You may not know that one of the largest sources for Wikimedia Commons is Flickr.&nbsp;&nbsp;</p>



<p>Since 2004, Flickr has been one of the most popular platforms for photographers and amateurs to upload photographs, videos, illustrations, and more online. It is also one of the largest online repositories of Creative Commons-licensed content. Flickr members can assign a license to their uploads, including those Creative Commons licenses accepted on Wikimedia Commons: Attribution (CC-BY), Attribution-ShareAlike (CC-BY-SA), Public Domain Dedication (CC0), and the Public Domain Mark.</p>





<p>In 2008, Flickr launched the <a href="https://www.flickr.com/commons">Flickr Commons</a> program, to increase public access to photography collections held at libraries, museums, and archives around the world. Images in Flickr Commons are shared with something a bit different from a license. It’s an <em>assertion</em> called&nbsp; “no known copyright restrictions.”&nbsp; The program supports <a href="https://www.flickr.com/commons/institutions">over 100 member institutions</a>, including <a href="https://www.flickr.com/photos/usnationalarchives/">The U.S. National Archives</a>, <a href="https://www.flickr.com/photos/nasacommons/">NASA on Commons</a>, the <a href="https://www.flickr.com/photos/nlscotland/">National Library of Scotland</a>, and <a href="https://www.flickr.com/photos/reykjavikmuseumofphotography/">Ljósmyndasafn Reykjavíkur</a>.</p>



<p>In 2022, the <a href="https://www.flickr.org/">Flickr Foundation</a> was established. It’s a US 501(c)(3) non-profit organization with the objective of safeguarding Flickr and its tens of billions of photos for the future. It seeks to develop and sustain “…an accessible social and technical infrastructure to protect [this] invaluable collection.”&nbsp;<br>This bridge between Flickr and Wikimedia Commons—which we’ve started calling “<strong>Flickypedia</strong>”—is one of the flagship projects of the Flickr Foundation. Building in partnership with the Wikimedia Foundation, and supported by the Culture and Heritage team, we will be building on the utility of the <a href="https://commons.wikimedia.org/wiki/Commons:Flickr2Commons">Flickr2Commons</a> tool, extending it, and then tending it for the long term.</p>



<p>This project has been mentioned in the 2023-2024 <a href="https://meta.wikimedia.org/wiki/Wikimedia_Foundation_Annual_Plan/2023-2024/Goals/Equity">Wikimedia Foundation Annual Plan</a> under the Equity / Culture &amp; Heritage section, in <a href="https://web.archive.org/web/20230602045536/https://mailchi.mp/c6a1d40b1748/new-news-for-you-about-flickrorg">this mailing list update</a> from Flickr Foundation, and <a href="https://commons.wikimedia.org/wiki/Commons:Village_pump/Archive/2023/06#Flickr_Foundation_adopts_Flickr2Commons">in the ensuing discussion</a> on Wikimedia Commons’s Village Pump.</p>



<h2>About Flickr2Commons</h2>



<p>Flickr2Commons is a popular tool used by Wikimedia Commons contributors to upload single or multiple files from Flickr into Wikimedia Commons. It was created by <a href="https://en.wikipedia.org/wiki/Magnus_Manske">Magnus Manske</a>, and first launched in 2013, ten years ago! The tool allows for user authentication, checks for the required licenses, includes a metadata editing step, and then file transfer.&nbsp;</p>



<h2>Metrics important for Flickypedia</h2>



<p>In order to gauge the possible reach of Flickypedia, we wanted to understand Flickr2Commons metrics. Magnus helped pull together the stats to show that roughly 5.4M files have been uploaded by about 2K users since launch. Using the <a href="https://hashtags.wmcloud.org/">Wikimedia Hashtags</a> tool, we can also see how much Flickr2Commons is used today. In June 2023 only, for example, 71,689 files were uploaded by 147 users.</p>


<div>
<figure><a href="https://diff.wikimedia.org/?attachment_id=97279"><img decoding="async" loading="lazy" width="1024" height="427" src="https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?resize=1024%2C427" alt="" srcset="https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=1920 1920w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=768 768w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=1024 1024w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=1536 1536w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></a><figcaption>Number of edits (ie. uploads), with the Flickr2Commons tool in June 2023 (<a href="https://hashtags.wmcloud.org/graph/?query=flickr2commons&amp;project=&amp;startdate=2023-06-01&amp;enddate=2023-06-30&amp;search_type=or&amp;user=">Hashtags tool</a>)</figcaption></figure></div>


<p>We were also able to discover the most active users of Flickr2Commons in the last six months, from January to June 2023.</p>


<div>
<figure><a href="https://diff.wikimedia.org/?attachment_id=97281"><img decoding="async" loading="lazy" width="1024" height="1024" src="https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?resize=1024%2C1024" alt="" srcset="https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=1254 1254w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=150 150w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=768 768w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=1024 1024w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></a><figcaption>Most active users of Flickr2Commons January to June 2023 (<a href="https://hashtags.wmcloud.org/graph/?query=flickr2commons&amp;project=&amp;startdate=2023-01-01&amp;enddate=2023-06-30&amp;search_type=or&amp;user=">Hashtags</a>)</figcaption></figure></div>


<p>It’s been great to collate all these usage statistics for Flickr2Commons—both the more recent numbers, but also in total over the last 10 years. Seeing it all together gives us a clear target for the new version to try to match.&nbsp;</p>



<p>It is also worth noting that another tool connects Flickr to Wikimedia Commons, called the UploadWizard. We’re bearing in mind that this means there will have been even more images from Flickr through that tool. Preparing these metrics has given us ideas on how we might make it even simpler to count into the future using Flickypedia.</p>



<h2>Our timeline</h2>



<p>The Flickypedia partnership project officially started in June 2023. We plan to spend the next six months or so building our Alpha (hopefully to show in October) and then Version 1.0 (hopefully December). Please <a href="https://commons.wikimedia.org/wiki/Commons_talk:Flickypedia">stay in touch</a> if you’d like to be involved in testing or have feedback about Flickr2Commons we should know about.</p>



<ul>
<li>For the July-December plan, please <a href="https://commons.wikimedia.org/wiki/Commons:Flickypedia">visit the project page</a> on Wikimedia Commons.</li>



<li>For feedback, please contact us via the <a href="https://commons.wikimedia.org/wiki/Commons_talk:Flickypedia">talk page</a> on Wikimedia Commons.</li>
</ul>
				<div id="translate-post">
					<p><img src="https://diff.wikimedia.org/wp-content/themes/interconnection/assets/images/translate-post.jpg" alt="">
					</p>

					<div>
						<h2>Can you help us translate this article?</h2>

						<p>In order for this article to reach as many people as possible we would like your help. Can you translate this article to get the message out?</p>

													<p><a href="https://diff.wikimedia.org/wp-login.php?redirect_to=%2F2023%2F07%2F07%2Fflickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons%2F%23translate-post">Start translation</a>
												</p></div>
				</div>
				
		</div>

	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Superrational (132 pts)]]></title>
            <link>https://skunkledger.substack.com/p/superrational</link>
            <guid>36645240</guid>
            <pubDate>Sat, 08 Jul 2023 15:19:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://skunkledger.substack.com/p/superrational">https://skunkledger.substack.com/p/superrational</a>, See on <a href="https://news.ycombinator.com/item?id=36645240">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>“Will Vicktoreya Keyneslee Beauxpensees please come see the principal right now?” said the voice over the intercom.</p><p>The voice didn’t actually tell me where to go, but I met the principal at his office anyway, because I knew that was the Schelling point for where we could find each other. I pushed my luxuriant raven hair behind my ear demurely and used my delicate, doll-like hand to knock on the door.</p><p>During the three seconds I spent waiting for it to open, I thought sadly about how I’d never known my parents. They’d died when I was a baby, gored in a stag hunt. Instead, I’d been raised by a series of mysterious guardians. But everybody always sensed there was something different about me. While the other children played their foolish games, I would twirl my luxuriant raven hair and draw Laffer curves in sidewalk chalk. Even though people were constantly falling in love with me, I kept to myself because I was so absorbed in my daydreams of a result definitively demonstrating whether the natural growth rate of capital production was exogenous or endogenous to demand.</p><p>I was snapped out of my wistful reverie when the principal opened the door. In front of his desk sat the most popular girl in school, Ariela Nudge.</p><p>“Ariela tells me you tried to take the locker she’d been using all year and dismissed her preferences for it as ‘reference-dependent,’” the principal said to me.</p><p>“She was taking advantage of the absence of a market system with enforceable property rights to sabotage allocative efficiency!”</p><p>“Fine,” said the principal. “My stomach is rumbling because it's almost lunchtime, so I'm going to be extra hard on the two of you.</p><p>“I’ll speak to you each separately. If neither of you confesses to wrongdoing, I’ll give you both detention today. If you both confess, I’ll give you detention today and tomorrow. If one of you confesses but the other doesn’t, the one who confessed will go free and the other will get detention for a week.”</p><p>I computed all the outcomes very fast in my elegant, doelike head. If Ariela and I cooperated to both deny wrongdoing, then we would both get detention for just one day. But if she didn’t confess, it would be even better for me to confess and go free. And if she did confess, defecting on me, then it would also be best for me to confess to avoid a week’s detention. School was out later that month, so we wouldn’t have much of a chance to get back at each other. The only equilibrium was for us to both confess, so naturally that’s what I did.</p><p>But Ariela denied everything. “Ugh, Vicktoreya! Why would you act so unfairly!” she squealed when she learned she’d be staying after school all week.</p><p>I sighed. I had almost resigned myself to detention; that way at least I wouldn’t get made fun of for not going to watch the stupid zero-sum lacrosse game. But I knew it was in both of our best interests for her to learn her lesson. “Every reasonable person knows you don’t play tit-for-tat in a one-off.”</p><p>“That’s enough from you two,” the principal said. He turned to his secretary. “Will you please fill out the paperwork for Ariela’s punishment and give Vicktoreya a little talking-to?”</p><p>But as soon as he had left the room, the principal’s secretary said, “Never mind the detention, Ariela. You can go. Just keep your nose clean next time.”</p><p>Ariela skipped out the door back to class.</p><p>The secretary turned to me. “It’s hard for a principal to ensure that an agent acting on his behalf will carry out his will in a manner aligned with his incentives, isn’t it? But of course, you knew that already. Anyway, have a seat. I have something very important to tell you.”</p><p>She flipped off her hood. She was actually my mysterious guardian, Emhily, in disguise.</p><p>“Vicktoreya Keyneslee Beauxpensees, there’s something special that makes you different from all your peers. You parents were no mere Homo sapiens, and neither are you.”</p><p>My mysterious eyes shone with feeling seen.</p><p><span>“You are one of the last living members of </span><em>Homo economicus</em><span>, the mythical race of perfectly rational utility-maximizing economic agents.”</span></p><p>I gasped, covering my dainty mouth with the sharp sparkly nails on my slender fingers. “You mean I’m… I’m a tradeoff-talking rational economic person, or TOTREP?”</p><p>“Please don’t tell your classmates,” Emhily cautioned wisely. “Because of their status quo bias, they’ll assign undue psychological weight to your increased status and generate positional externalities.”</p><p>It was all I could do to contain my secret for the rest of the school day. As soon as the bell rang, I knew where I was headed: off to the mall to exchange monetary payment for the bundle of goods I deemed preferable to all others.</p><p>But when I turned around from picking out the blood-red lipstick that would most perfectly complement my porcelain skin and ethereally melancholy eyes, in strutted Ariela and her clique of western, educated, industrialized, rich, democratic preppies. I ducked behind a shelf.</p><p>“I love those shoes on you,” Ariela was saying to her best friend, Blinklynn Neucoque.</p><p>“I’ll probably go with them, then,” Blinklynn said.&nbsp;</p><p>I almost spilled my consumer surplus all over the floor. Imagine making a purchasing decision under the influence of herd mentality! Munching on my two marshmallows from the food court, I listened to Blinklynn go on.</p><p>“Where’s the price tag? I’d get these if they’re under like 80.”</p><p>“It says they’re thirty percent off online,” Ariela said. “Oh – but they take a week to ship.”</p><p>“And prom is Friday.”</p><p>The horror of having a temporal discounting function that was not only nonexponential but also discontinuous was too much for me to take. With a shudder, I went back to meticulously filling out the table in my notebook of the price and characteristics of every item in the store.</p><p>Friday rolled around: the big night. I didn’t have a date because nobody had shown up to my Gale-Shapley matchmaking session, but it didn’t matter because I looked ravishing in my dramatic lipstick, heavy eyeliner, exquisite dress, hair that I didn’t even have to style because it always fell perfectly, and shoes I cleverly picked out because they wouldn’t even be visible under the dress and I have a very low time preference and prom is only 0.0004% of my life so I’d just grabbed the cheapest ones on the rack.</p><p>Unfortunately, pretty soon my feet started to hurt. But I remembered that Blinklynn had clearly stated she valued her shoes at $80, so I went up and offered her $80. She refused. I was horrified.</p><p>“Your willingness to pay should equal your willingness to accept!” I protested.</p><p>“But then I wouldn’t have any shoes!”</p><p>“$95, to factor in transportation costs and compensate you for your assumption of risk?”</p><p>“You have lipstick on your teeth.”</p><p>Before I could sputter out a response, a mysterious stranger swept me off my feet.</p><p>Cradled in his sculpted arms, I turned to see his face. It was the most beautiful boy I'd ever seen in my life. "Hi, I'm Nashwell Maxington," he said. "I'm new here. I hope you don't mind me rescuing you, but I couldn't believe such a lovely girl with such luxuriant raven hair could have been victimized by others’ vulnerability to the endowment effect."</p><p>I gazed into his breathtakingly dark eyes. There was something deep and familiar about them, glimmering in his eye sockets like two spherical cows. Was it... could it be?</p><p>"I'm Vicktoreya Keyneslee Beauxpensees," I said. "An average blue whale weighs two hundred thousand pounds. How much do you think I weigh?"</p><p>He laughed. "Not more than a hundred pounds sopping wet."</p><p>A delighted shiver passed through my itty bitty little waist and ran down my spine.</p><p><span>"Anchoring has no effect on me," Nashwell said, "and judging by the fact that you tried, I think I know what you are." His voice dropped to a whisper. "I'm a </span><em>Homo economicus</em><span> too."</span></p><p>I gasped. "I thought I'd never meet another like me."</p><p>Nashwell smiled rationally. "Let me relieve you of these sunk costs." He slipped the shoes off my feet and beckoned me to dance.</p><p>We reached our saturation point of music consumption after one song. He took my hand and led me out the door for a moonlit stroll, where we walked and talked for hours. The transaction costs incurred by our interpersonal negotiations were infinitesimal; it felt magical.</p><p>But then Nashwell got a depressed look on his face.</p><p>"What's wrong?" I said.</p><p>"Vicktoreya, I'm smitten by your effortless good looks, outlier IQ, and captivating allure. I observe evidence that you may reciprocate my affections, but I can't generalize from this small sample, lest I fall prey to the fallacious law of small numbers. Most people hallucinate patterns in stochastic noise, but I could never allow myself to be fooled by randomness like that."</p><p>He paused to brood axiomatically and self-consistently.</p><p>"By the trivial rules of conjunction," he lamented, "it’s more likely that you’re beautiful, charming, brilliant, and swooning every time I speak than that you’re beautiful, charming, brilliant, swooning every time I speak, and in love with me.”</p><p>"Oh, Nashwell!" I exclaimed, deeply moved. "Forty-nine percent of the fibers of my being want to fall in love with you. But I can't let myself fall for you, because it would be irrational for my present self to allow my future self to vastly alter its value distribution away from my current one."</p><p>A ray of moonlight suddenly broke through the trees and shone dazzlingly onto his chiseled face. It seemed to strike him with an insight.</p><p>"Ah, my darling!” he said. "Consider this.</p><p>“We, two perfectly rational agents, can still fail to coordinate because we each generate strategies independently of the other. But if we instead performed costly signaling by undertaking elaborate social rituals whose participants are selected for compatibility, and if we subsequently wove the threads of our two lives into one so that future changes to both our decision functions would be influenced by a roughly identical set of inputs – then, my angel, to a first approximation we would be able to avoid multipolar traps by relying on the assumption that our strategies will be the same, and hence outcompete individual actors who fail to implement this meta-coordination.</p><p>“This is what I understand the romantics call ‘true love.’”</p><p>I gazed at him longingly.</p><p>"Plus," he added, "even if I’m wrong – marriage is heavily tax-advantaged."</p><p>I updated directly into his arms. We shared a breathtakingly beautiful kiss. As we held each other, I felt Nashwell's utility function slowly start to nest into mine. I immediately came of age.</p><p>Nine months later, our beautiful baby girl was born.</p><p>"Galaxii is a joy to have in class," the preschool teacher said. "Cooperative, behaves appropriately – she fits in so well with her peers."</p><p>I squeezed Nashwell’s hand.</p><p>"The other day," the teacher said, "I praised her for taking just one juice box when some of the other children were sneaking two. She replied, 'If we model individuals in a shared sociocultural milieu as adapting to similar selection pressures, many stable strategies incorporate altruistic cooperation.'"</p><p>Nashwell and I beamed with pride. That was our tiny TOTREP, all right.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spying on a smartphone remotely by the authorities: feasibility and operation (109 pts)]]></title>
            <link>https://security.stackexchange.com/questions/271146/spying-on-a-smartphone-remotely-by-the-authorities-feasibility-and-operation</link>
            <guid>36644952</guid>
            <pubDate>Sat, 08 Jul 2023 14:49:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://security.stackexchange.com/questions/271146/spying-on-a-smartphone-remotely-by-the-authorities-feasibility-and-operation">https://security.stackexchange.com/questions/271146/spying-on-a-smartphone-remotely-by-the-authorities-feasibility-and-operation</a>, See on <a href="https://news.ycombinator.com/item?id=36644952">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
                
<p>French lawmakers agreed to a justice reform bill that includes a provision granting police the power to remotely activate suspects' geolocation, microphone and camera (<a href="https://www.lemonde.fr/en/france/article/2023/07/06/france-set-to-allow-police-to-spy-through-phones_6044269_7.html" rel="noreferrer">source</a>).</p>
<p>Following the senators, the deputies also gave their green light to allow certain features of smartphones and other devices to be activated remotely, thus turning them into surveillance trackers.</p>
<p>The deputies determined a list of professions "protected" from any capture: journalists, doctors, notaries, and bailiffs, in addition to lawyers, magistrates and members of parliament.</p>
<p><strong>Technically, will it be possible for them to set up this type of surveillance on phones? If so, how will they go about it?</strong></p>
<p>I wonder how authorities can effectively activate microphones and cameras from popular smartphones using iOS or Android.</p>
<p>I am also wondering how can the privacy of individuals not involved in any criminal activity be safeguarded when such wide-reaching surveillance measures are implemented.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When an app asks for permissions, it should have a “feed fake data” option (957 pts)]]></title>
            <link>https://mastodon.gamedev.place/@Nifflas/110668040598715116</link>
            <guid>36644895</guid>
            <pubDate>Sat, 08 Jul 2023 14:43:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.gamedev.place/@Nifflas/110668040598715116">https://mastodon.gamedev.place/@Nifflas/110668040598715116</a>, See on <a href="https://news.ycombinator.com/item?id=36644895">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[iVentoy (224 pts)]]></title>
            <link>https://www.iventoy.com/en/index.html</link>
            <guid>36644806</guid>
            <pubDate>Sat, 08 Jul 2023 14:35:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iventoy.com/en/index.html">https://www.iventoy.com/en/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=36644806">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="contentpage"> 


<h2>News</h2>

<p>
2023/07/05 ---  New release <a href="https://www.iventoy.com/en/download.html">iventoy-1.0.08 </a>
<span> <a href="https://www.iventoy.com/en/doc_news.html">More ...</a></span>
</p>

<h2>What is iVentoy</h2>
<p>   
    iVentoy is an enhanced version of the PXE server. <br>
    With iVentoy you can boot and install OS on multiple machines at the same time through the network.<br> 
    iVentoy is extremely easy to use, without complicated configuration, just put the ISO file in the specified location and select PXE boot in the client machine.<br>
    iVentoy supports x86 Legacy BIOS, IA32 UEFI, x86_64 UEFI and ARM64 UEFI mode at the same time.<br>
    iVentoy support 110+ common types of OS (Windows/WinPE/Linux/VMware) (<a href="https://www.iventoy.com/en/isolist.html">list</a>)。
</p>

<br>

<h2>Features</h2>
<div>
<div>
<ul>
  <li>Simple to use <a href="https://www.iventoy.com/en/doc_start.html">(Get Started)</a>  </li>  
  <li>Cross-platform, can run in both Windows and Linux.</li>      
  <li>Specially optimized for PXE scenarios, with flexible functions.</li>      
  <li>Directly boot ISO files, no extraction needed.</li>
  <li>Native boot menu style for Legacy &amp; UEFI</li> 
  <li>Directory layout corresponded boot menu.</li> 
  <li>Supports Legacy BIOS and IA32/X86_64/ARM64 UEFI mode.</li>    
  <li>Supports 110+ common types of OS (Windows/WinPE/Linux/VMware) </li>      
  <li>System or ISO level boot password protection.</li>  
  <li>Multiple devices to install different OSs at the same time.</li>
</ul>
</div>

<div>
<ul>  

  <li>Device filtering by MAC address.</li>
  <li>Support querying MAC address filtering status.</li>
  <li>Support MAC address attribution query.</li>
  <li>Client device information. (Manufacture, product name etc.)</li>
  <li>Directly get ISO internal files with HTTP.<a href="https://www.iventoy.com/en/doc_http_url.html">Notes</a></li>  
  <li>File Injection feature. <a href="https://www.iventoy.com/en/doc_injection.html">Notes</a> </li>  
  <li>Windows auto installation supported. <a href="https://www.iventoy.com/en/doc_autoinstall.html">Notes</a></li>
  <li>Linux auto installation supported. <a href="https://www.iventoy.com/en/doc_autoinstall.html">Notes</a></li>
  <li>Variables Expansion supported for install script <a href="https://www.iventoy.com/en/doc_autoinstall.html">Notes</a></li>
  <li>Automatically solve the driver missing during Linux installation.</li>
  
</ul>
</div>
</div>


<p><img src="https://www.iventoy.com/static/img/screen/boot_ground.png?v=4">
</p>





    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A $182B Chip Maker: AMD's Labs – Full Documentary [video] (175 pts)]]></title>
            <link>https://www.youtube.com/watch?v=7H4eg2jOvVw</link>
            <guid>36644506</guid>
            <pubDate>Sat, 08 Jul 2023 14:02:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=7H4eg2jOvVw">https://www.youtube.com/watch?v=7H4eg2jOvVw</a>, See on <a href="https://news.ycombinator.com/item?id=36644506">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
    </channel>
</rss>