<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 06 May 2025 13:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Getting things “done” in large tech companies (109 pts)]]></title>
            <link>https://www.seangoedecke.com/getting-things-done/</link>
            <guid>43903741</guid>
            <pubDate>Tue, 06 May 2025 11:04:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seangoedecke.com/getting-things-done/">https://www.seangoedecke.com/getting-things-done/</a>, See on <a href="https://news.ycombinator.com/item?id=43903741">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header></header><section><p>What does it mean to get things done? In the abstract, you can complete a mathematical proof or a problem set, but the real world is much fuzzier. Suppose I plant a tree in my backyard. Once the sapling is in the ground, is that done? Not really. There’s always more work to do: clearing the ground around it, watering, keeping pests away, pruning, and so on. Programming large web applications is more like planting a tree than completing a mathematical proof. Once you write a service, you can keep working on it forever if you want to.</p>
<p>In large tech companies, this fact is a trap for competent but unagentic engineers. They see an infinite queue of tasks that they’re capable of doing, and they start delivering a stream of marginal improvements to a particular subsystem<sup id="fnref-1"><a href="#fn-1">1</a></sup>. From their perspective, it feels like they’re crushing it. After all, they’re putting out work at their top speed: no downtime, no waiting on other teams. But they’re not doing their actual job, which is to deliver the most value they can to their company. From the perspective of their manager and skip-level, they’re not getting anything done.</p>
<p>What does it mean to get things done in large companies? Most importantly, it means <strong>finishing</strong> things. How can you finish things in a world where you can keep improving systems indefinitely? It means <strong>getting them to a point where the decision-makers at the company are happy</strong>. At that point, you have to declare victory and walk away! Go and do something else! I’ve seen many engineers stick around delivering one more tweak or one more refactor - long past the point where their work stopped being perceived as a successful project and started being perceived as wasted time. Better to deliver two more things in that time.</p>
<p>Second, it means delivering the kind of things that are <strong>legible</strong> to the decision-makers at the company: i.e. visible to your manager, plus 1-3 skip levels, depending on your title. The easiest way to do this is to deliver things that they already know about, such as projects that they’ve asked you to do, or incidents that are serious enough that they’re involved in them. It’s possible to make other work legible to them as well. If your work produces or saves money, that will make it immediately legible, for instance (or you could just be really convincing). By default, work you do isn’t legible: to the decision-makers, it’s generic technical nonsense. They don’t know whether it’s crucial high-impact work or pointless code reshuffling, and will tend to assume the latter.</p>
<p>In short, getting things done means getting them in a state where:</p>
<p>(a) executives at the company understand what’s happened, and
(b) are happy with it</p>
<p>To many, this will be an unsatisfying definition of what it means to get things done. Lots of engineers will want a more solid definition than “it’s a social construct”. However, as someone with a philosophy background, I have a healthy respect for social constructs. The concept “chair” is a social construct, and chairs are plenty real[2]. In some ways, “getting things done” is even more real. It can pay your rent! If you don’t respect it, it can even get you fired.</p>
<h3>Summary</h3>
<ul>
<li>Just because you’re doing work doesn’t mean you’re getting things done</li>
<li>Nothing is ever “done” in the non-abstract world. Everything can be worked on indefinitely</li>
<li>If you want to get things done you can’t be a gardener. You have to aim for a bullet-point list of achievements</li>
<li>So what’s “done” mean in that context? It means the company is happy with the state of it</li>
<li>Declare victory and walk away: go and do something else!</li>
</ul>
</section><p>If you liked this post, consider <a href="https://buttondown.com/seangoedecke" target="_blank">subscribing</a> to email updates about my new posts.</p><p>May 6, 2025<!-- -->&nbsp;│ Tags: <a href="https://www.seangoedecke.com/tags/shipping/">shipping</a>, <a href="https://www.seangoedecke.com/tags/tech%20companies/">tech companies</a></p><hr></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sneakers (1992) – 4K Restoration (206 pts)]]></title>
            <link>https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/</link>
            <guid>43902263</guid>
            <pubDate>Tue, 06 May 2025 06:15:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/">https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/</a>, See on <a href="https://news.ycombinator.com/item?id=43902263">Hacker News</a></p>
<div id="readability-page-1" class="page"><div width="1262">
	<tbody><tr>
			<td>
				<br>
			</td>
			<td>
				
							<br>

							
<br>
<center>
</center>
			</td>
			<td>
				<br>
			</td>
<td>

<span>4K Restoration</span>
<span>
<a href="https://www.blu-ray.com/movies/movies.php?studioid=280" rel="nofollow">Kino Lorber</a> | <a href="https://www.blu-ray.com/movies/movies.php?year=1992" rel="nofollow">1992</a> | <span id="runtime" title="2 hr 6 min" onclick="var runtime = document.getElementById('runtime'); var tmp = runtime.title; runtime.title = runtime.innerHTML; runtime.innerHTML = tmp">126 min</span> | Rated PG-13 | <a alt="Sneakers Blu-ray Release Date April 22, 2025" title="Sneakers Blu-ray Release Date April 22, 2025" href="https://www.blu-ray.com/movies/releasedates.php?year=2025&amp;month=4#April22">Apr 22, 2025</a></span>
		<table>
		<tbody><tr>
<td id="menu_overview"><a id="menu_overview_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#Overview" rel="history">Overview</a></td>
<td id="menu_review" nowrap=""><a id="menu_review_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#Review" rel="history">Blu-ray review</a></td>
<td id="menu_screenshots"><a id="menu_screenshots_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#Screenshots" rel="history">Screenshots</a></td>
<td id="menu_screenshots_num"><small>(20)</small></td>
<td id="menu_packaging"><a id="menu_packaging_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#Packaging" rel="history">Packaging</a></td>
<td id="menu_userreviews"><a id="menu_userreviews_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#UserReviews" rel="history">User&nbsp;reviews</a></td>
<td id="menu_regioncoding"><a id="menu_regioncoding_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#RegionCoding" rel="history">Region&nbsp;coding</a></td>
<td id="menu_news"><a id="menu_news_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#News" rel="history">News</a></td>
<td id="menu_forum"><a id="menu_forum_link" href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/381411/#Forum" rel="history">Forum</a></td>
<td><br></td>
		</tr>
		</tbody></table>
		<span><br></span>
		


<div id="movie_review_video">
<a name="#video"><h3 id="movie_review_video_heading">Sneakers Blu-ray, Video Quality</h3></a>
&nbsp; <p><img src="https://images.static-bluray.com/rating/b10.jpg" width="69" height="13" alt="5.0 of 5"></p><p><a href="https://www.blu-ray.com/movies/screenshot.php?movieid=381411&amp;position=2"><img id="load60766577487553" src="https://images.static-bluray.com/transparent.gif" loading="lazy" width="728" height="409"></a></p><p>
Presented in its original aspect ratio of 1.85:1, encoded with MPEG-4 AVC and granted a 1080p transfer, <i>Sneakers</i> arrives on Blu-ray courtesy of Kino Lorber. </p><p>

The release introduces a new 4K makeover of <i>Sneakers</i> struck from the original camera negative. The 4K makeover is also made available on 4K Blu-ray in <a href="https://www.blu-ray.com/movies/Sneakers-4K-Blu-ray/343185/#Review"><i>this</i></a> combo pack.</p><p>

I have only one other release of <i>Sneakers</i> in my library, which is <a href="https://www.blu-ray.com/movies/Sneakers-Blu-ray/65706/#Review"><i>this</i></a> Region-B release, produced by Universal Pictures-UK in 2013. I think that the previous presentation of the film is mostly good, but it has a dated appearance that was undeniable even more than a decade ago. (At that time, virtually all older masters that emerged from the major's vault produced very harsh visuals with unpleasant digital appearance. The old master that was used to produce the previous release of the film was one of the few that had fine organic qualities). After viewing the 4K makeover in its entirety on 4K Blu-ray and then spending time with it on the Blu-ray, I did not think that comparisons with the previous release were needed. The 4K makeover brings substantial improvements in all areas we scrutinize in our reviews, and they are very, very easy to appreciate on 4K Blu-ray and Blu-ray. For example, the darker/dark material where colored light and shadows constantly interact convey outstanding delineation, clarity, and depth. In some areas, there is simply a lot more to see, and the perception of depth is completely different now because of how entire ranges of nuances look. Also, the density levels are terrific on 4K Blu-ray and Blu-ray. The entire 4K makeover is graded with outstanding precision, too. All primaries and supporting nuances are impeccably set and balanced, allowing the visuals to reveal an enormously attractive period appearance. Because of the expanded color gamut and superior dynamic range of 4K, I think that the strength and accuracy of the color grade are more impressive in native 4K. However, I must make it clear that the Blu-ray still offers a mighty fine upgrade in quality. Image stability is excellent. The entire film looks spotless as well. (<i>Note</i>: This is a Region-A "locked" Blu-ray release. Therefore, you must have a native Region-A or Region-Free player in order to access its content). </p></div>
<div id="movie_review_audio">
<a name="#audio"><h3 id="movie_review_audio_heading">Sneakers Blu-ray, Audio Quality</h3></a>
&nbsp; <p><img src="https://images.static-bluray.com/rating/b10.jpg" width="69" height="13" alt="5.0 of 5"></p><p><a href="https://www.blu-ray.com/movies/screenshot.php?movieid=381411&amp;position=3"><img id="load48553884002189" src="https://images.static-bluray.com/transparent.gif" loading="lazy" width="728" height="409"></a></p><p>
There are two standard audio tracks on this release: English DTS-HD Master Audio 5.1 and English DTS-HD Master Audio 2.0. Optional English SDH subtitles are provided for the main feature. </p><p>

I viewed the new 4K makeover of <i>Sneakers</i> in its entirety and later spent time with it on the Blu-ray. The comments below are from our review of the 4K Blu-ray/Blu-ray combo pack release.</p><p>

I viewed a good portion of <i>Sneakers</i> with the 2.0 track and liked it a lot. It has a great dynamic range and is very, very healthy. The 5.1 track is good, too. However, I have always felt that it could have had more surround movement to impress as a 5.1 track. Some of the action sequences throughout the film create great opportunities for impressive surround movement, but there is hardly anything meaningful happening there. The dialog is clear, sharp, stable, and easy to follow. 

</p></div>



<div id="movie_news">
<h2>Sneakers: Other Editions</h2><table><tbody><tr>
<td><center>
<a data-globalproductid="1720666" data-globalparentid="35604" data-categoryid="7" data-productid="343185" href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Sneakers-4K-Blu-ray/343185/" title="Sneakers 4K Blu-ray (1992)"><img loading="lazy" id="load62452077454263" src="https://images.static-bluray.com/transparent.gif" width="88" height="110" title="Sneakers 4K Blu-ray (1992)"></a><br>
<b>4K</b><br>
2-disc set<br><b>$31.49</b></center></td>
<td><center>
<a data-globalproductid="421530" data-globalparentid="35604" data-categoryid="7" data-productid="125225" href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Sneakers-Blu-ray/125225/" title="Sneakers Blu-ray (1992)"><img loading="lazy" id="load23897872488079" src="https://images.static-bluray.com/transparent.gif" width="86" height="110" title="Sneakers Blu-ray (1992)"></a><br>
<b>Blu-ray</b><br>
1-disc</center></td>
<td><center>
<a data-globalproductid="127086" data-globalparentid="35604" data-categoryid="7" data-productid="48172" href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Sneakers-Blu-ray/48172/" title="Sneakers Blu-ray (1992)"><img loading="lazy" id="load42182786428910" src="https://images.static-bluray.com/transparent.gif" width="85" height="110" title="Sneakers Blu-ray (1992)"></a><br>
<b>Blu-ray</b><br>
1-disc<br><b>$8.82</b></center></td>
<td colspan="5"></td>
</tr></tbody></table><br>

<h3>Sneakers Blu-ray, News and Updates</h3><p>
• <a href="https://www.blu-ray.com/news/?id=35937">Sneakers 4K Blu-ray</a></p><p><span> - February 5, 2025</span></p><p>Kino Lorber have detailed their upcoming 4K Blu-ray and Blu-ray releases of Phil Alden Robinson's Sneakers (1992), starring Robert Redford, Dan Aykroyd, Ben Kingsley, Mary McDonnell, and River Phoenix. The two releases are scheduled to arrive on the market on April ...</p>
<p>
• <a href="https://www.blu-ray.com/news/?id=33042">Sneakers 4K Blu-ray</a> <span> - August 6, 2023</span></p><p>Kino Lorber are preparing a 4K Blu-ray release of Phil Alden Robinson's Sneakers (1992), starring Robert Redford, Dan Aykroyd, Ben Kingsley, Mary McDonnell, and River Phoenix. The release is expected to arrive on the market later this year.</p>
<p>
• <a href="https://www.blu-ray.com/news/?id=15738">Upcoming Universal Blu-ray Releases</a> <span> - December 29, 2014</span></p><p>Universal Studios Home Entertainment will add three new titles to its Blu-ray catalog in February:  Steven Spielberg's Munich (2005), Robert Wise's The Andromeda Strain (1971), and  Phil Alden Robinson's Sneakers (1992).</p>
<p><b>»</b> Show more <a href="https://www.blu-ray.com/movies/Sneakers-Blu-ray-Forum/381411/#News" onclick="window.scrollTo(0, 0)" rel="history">related news posts</a> for Sneakers Blu-ray</p></div>












<br>






		

		</td>
		<td>
		</td>
		<td>
<a href="https://play.google.com/store/apps/details?id=com.bluray.android.mymovies"><img id="load72156764134705" src="https://images.static-bluray.com/transparent.gif" loading="lazy" width="148" height="52"></a>&nbsp;<a href="https://itunes.apple.com/us/app/my-movies-by-blu-ray.com/id514571960"><img id="load57773101475148" src="https://images.static-bluray.com/transparent.gif" loading="lazy" width="148" height="52"></a><br><span><br></span>
			
			<br>
		
		<br>
		
		<table>
		<tbody><tr>
			<td>
			</td>
			<td>
<p><a href="https://www.blu-ray.com/link/click.php?p=1569816&amp;tid=070" onclick="document.cookie = 'tid=070; expires='+new Date(new Date().getTime()+20*60*1000).toUTCString()+'; path=/; domain=.blu-ray.com'"><img src="https://images.static-bluray.com/movies/covers/316463_medium.jpg"></a><br>
<b>$11.89</b><br><span><b>-$2.1</b></span><br><span><small>1 hour ago</small></span></p><p><a href="https://www.blu-ray.com/link/click.php?p=1870368&amp;tid=070" onclick="document.cookie = 'tid=070; expires='+new Date(new Date().getTime()+20*60*1000).toUTCString()+'; path=/; domain=.blu-ray.com'"><img src="https://images.static-bluray.com/movies/covers/367676_medium.jpg"></a><br>
<b>$48.99</b><br><span><b>-$3</b></span><br><span><small>2 hours ago</small></span></p><p><a href="https://www.blu-ray.com/link/click.php?p=1464140&amp;tid=070" onclick="document.cookie = 'tid=070; expires='+new Date(new Date().getTime()+20*60*1000).toUTCString()+'; path=/; domain=.blu-ray.com'"><img src="https://images.static-bluray.com/movies/covers/297693_medium.jpg"></a><br>
<b>$57.34</b><br><span><b>-$22.61</b></span><br><span><small>2 hours ago</small></span></p><p><a href="https://www.blu-ray.com/link/click.php?p=1723705&amp;tid=070" onclick="document.cookie = 'tid=070; expires='+new Date(new Date().getTime()+20*60*1000).toUTCString()+'; path=/; domain=.blu-ray.com'"><img src="https://images.static-bluray.com/movies/covers/343800_medium.jpg"></a><br>
<b>$40</b><br><span><b>-$0.08</b></span><br><span><small>2 hours ago</small></span></p><p><a href="https://www.blu-ray.com/link/click.php?p=1904825&amp;tid=070" onclick="document.cookie = 'tid=070; expires='+new Date(new Date().getTime()+20*60*1000).toUTCString()+'; path=/; domain=.blu-ray.com'"><img src="https://images.static-bluray.com/movies/covers/373447_medium.jpg"></a><br>
<b>$194.99</b><br><span><b>-$3</b></span><br><span><small>3 hours ago</small></span></p><p><a href="https://www.blu-ray.com/link/click.php?p=1896880&amp;tid=070" onclick="document.cookie = 'tid=070; expires='+new Date(new Date().getTime()+20*60*1000).toUTCString()+'; path=/; domain=.blu-ray.com'"><img src="https://images.static-bluray.com/movies/covers/371342_medium.jpg"></a><br>
<b>$19.99</b><br><span><b>-$2.5</b></span><br><span><small>3 hours ago</small></span></p>				<br>
				<a href="https://www.blu-ray.com/deals/?sortby=time" rel="nofollow">Show new deals »</a>
				</td>
			<td>
			</td>
		</tr>
		</tbody></table>
		
<p>Trending Blu-ray Movies <img id="load76656009377113" src="https://images.static-bluray.com/transparent.gif" loading="lazy" width="18" height="12"></p>
<table>
<tbody><tr><td><span>1.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Anora-4K-Blu-ray/379728/">Anora 4K</a></td></tr>
<tr><td><span>2.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Dirty-Harry-4K-Blu-ray/379700/">Dirty Harry 4K</a></td></tr>
<tr><td><span>3.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Dirty-Work-4K-Blu-ray/387582/">Dirty Work 4K</a></td></tr>
<tr><td><span>4.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Lethal-Weapon-4K-Blu-ray/387483/">Lethal Weapon 4K</a></td></tr>
<tr><td><span>5.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/The-Outlaw-Josey-Wales-4K-Blu-ray/379701/">The Outlaw Josey Wales 4K</a></td></tr>
<tr><td><span>6.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Pale-Rider-4K-Blu-ray/378291/">Pale Rider 4K</a></td></tr>
<tr><td><span>7.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/The-Golden-Child-4K-Blu-ray/387580/">The Golden Child 4K</a></td></tr>
<tr><td><span>8.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Dirty-Harry-4K-Blu-ray/382391/">Dirty Harry 4K</a></td></tr>
<tr><td><span>9.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Stripes-4K-Blu-ray/380958/">Stripes 4K</a></td></tr>
<tr><td><span>10.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Jade-4K-Blu-ray/387579/">Jade 4K</a></td></tr>
<tr><td><span>11.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Timecop-4K-Blu-ray/380144/">Timecop 4K</a></td></tr>
<tr><td><span>12.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Tombstone-4K-Blu-ray/384615/">Tombstone 4K</a></td></tr>
<tr><td><span>13.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/The-Outlaw-Josey-Wales-4K-Blu-ray/382392/">The Outlaw Josey Wales 4K</a></td></tr>
<tr><td><span>14.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/Ace-Ventura-Pet-Detective-4K-Blu-ray/387416/">Ace Ventura: Pet Detective 4K</a></td></tr>
<tr><td><span>15.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/movies/The-Rocky-Horror-Picture-Show-4K-Blu-ray/387515/">The Rocky Horror Picture Show 4K</a></td></tr>
</tbody></table><p>Trending in Theaters</p>
<table>
<tbody><tr><td><span>1.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Sinners/1897027/">Sinners</a></td></tr>
<tr><td><span>2.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Accountant-2/1872803/">The Accountant 2</a></td></tr>
<tr><td><span>3.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Until-Dawn/1902101/">Until Dawn</a></td></tr>
<tr><td><span>4.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Captain-America-Brave-New-World/1467785/">Captain America: Brave New World</a></td></tr>
<tr><td><span>5.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Heart-Eyes/1910716/">Heart Eyes</a></td></tr>
<tr><td><span>6.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Last-Breath/1856515/">Last Breath</a></td></tr>
<tr><td><span>7.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Legend-of-Ochi/1613853/">The Legend of Ochi</a></td></tr>
<tr><td><span>8.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Drop/1906284/">Drop</a></td></tr>
<tr><td><span>9.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Warfare/1879700/">Warfare</a></td></tr>
<tr><td><span>10.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Death-of-a-Unicorn/1755689/">Death of a Unicorn</a></td></tr>
<tr><td><span>11.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/A-Minecraft-Movie/598331/">A Minecraft Movie</a></td></tr>
<tr><td><span>12.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Novocaine/1812774/">Novocaine</a></td></tr>
<tr><td><span>13.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Paddington-in-Peru/1452862/">Paddington in Peru</a></td></tr>
<tr><td><span>14.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Mickey-17/1594399/">Mickey 17</a></td></tr>
<tr><td><span>15.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Black-Bag/1856516/">Black Bag</a></td></tr>
<tr><td><span>16.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/A-Working-Man/1834507/">A Working Man</a></td></tr>
<tr><td><span>17.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Hell-of-a-Summer/1941861/">Hell of a Summer</a></td></tr>
<tr><td><span>18.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Freaky-Tales/1688953/">Freaky Tales</a></td></tr>
<tr><td><span>19.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Love-Hurts/1908196/">Love Hurts</a></td></tr>
<tr><td><span>20.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Amateur/1710126/">The Amateur</a></td></tr>
<tr><td><span>21.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Day-the-Earth-Blew-Up-A-Looney-Tunes-Movie/1775994/">The Day the Earth Blew Up: A Looney Tunes Movie</a></td></tr>
<tr><td><span>22.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Neighborhood-Watch/1968058/">Neighborhood Watch</a></td></tr>
<tr><td><span>23.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/On-Swift-Horses/1676554/">On Swift Horses</a></td></tr>
<tr><td><span>24.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Ash/1940611/">Ash</a></td></tr>
<tr><td><span>25.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Woman-in-the-Yard/1833692/">The Woman in the Yard</a></td></tr>
<tr><td><span>26.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Monkey/1856645/">The Monkey</a></td></tr>
<tr><td><span>27.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/October-8/1948250/">October 8</a></td></tr>
<tr><td><span>28.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Locked/1947168/">Locked</a></td></tr>
<tr><td><span>29.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Ballad-of-Wallis-Island/1856514/">The Ballad of Wallis Island</a></td></tr>
<tr><td><span>30.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Friend/1899553/">The Friend</a></td></tr>
<tr><td><span>31.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Shrouds/1697619/">The Shrouds</a></td></tr>
<tr><td><span>32.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Snow-White/1450301/">Snow White</a></td></tr>
<tr><td><span>33.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Ugly-Stepsister/1968057/">The Ugly Stepsister</a></td></tr>
<tr><td><span>34.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Mob-Cops/1972363/">Mob Cops</a></td></tr>
<tr><td><span>35.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/In-the-Lost-Lands/1710131/">In the Lost Lands</a></td></tr>
<tr><td><span>36.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Rule-of-Jenny-Pen/1950435/">The Rule of Jenny Pen</a></td></tr>
<tr><td><span>37.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Opus/1804841/">Opus</a></td></tr>
<tr><td><span>38.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Penguin-Lessons/1901077/">The Penguin Lessons</a></td></tr>
<tr><td><span>39.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/The-Wedding-Banquet/1918822/">The Wedding Banquet</a></td></tr>
<tr><td><span>40.</span>&nbsp;</td><td><a href="https://www.blu-ray.com/Bring-Them-Down/1889967/">Bring Them Down</a></td></tr>
</tbody></table><br>
<table>
<tbody><tr><td colspan="2">
</td></tr><tr><td><span>1.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Pink-Floyd-Live-at-Pompeii-Blu-ray/383017/" rel="nofollow">Pink Floyd: Live at Pompeii</a><br>
</td></tr><tr><td><span>2.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Captain-America-Brave-New-World-4K-Blu-ray/385906/" rel="nofollow">Captain America: Brave New World 4K</a><br>
</td></tr><tr><td><span>3.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Alexander-Revisited-The-Final-Cut-4K-Blu-ray/387007/" rel="nofollow">Alexander Revisited: The Final Cut 4K</a><br>
</td></tr><tr><td><span>4.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Jaws-4K-Blu-ray/387417/" rel="nofollow">Jaws 4K</a><br>
</td></tr><tr><td><span>5.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Wicked-Blu-ray/376117/" rel="nofollow">Wicked</a><br>
</td></tr><tr><td><span>6.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Final-Destination-5-Film-Collection-Blu-ray/315427/" rel="nofollow">Final Destination 5 Film Collection</a><br>
</td></tr><tr><td><span>7.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Abigail-4K-Blu-ray/387010/" rel="nofollow">Abigail 4K</a><br>
</td></tr><tr><td><span>8.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Lilo-and-Stitch-4K-Blu-ray/385554/" rel="nofollow">Lilo &amp; Stitch 4K</a><br>
</td></tr><tr><td><span>9.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Captain-America-Brave-New-World-4K-Blu-ray/385905/" rel="nofollow">Captain America: Brave New World 4K</a><br>
</td></tr><tr><td><span>10.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Captain-America-Brave-New-World-Blu-ray/386295/" rel="nofollow">Captain America: Brave New World</a><br>
</td></tr><tr><td colspan="2">
&nbsp;&nbsp;<b>»</b> <a href="https://www.blu-ray.com/movies/top.php">See more top sellers</a></td></tr></tbody></table><br>
<table>
<tbody><tr><td colspan="2">
</td></tr><tr><td><span>1.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Captain-America-Brave-New-World-4K-Blu-ray/385906/" rel="nofollow">Captain America: Brave New World 4K</a><br>
</td></tr><tr><td><span>2.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Alexander-Revisited-The-Final-Cut-4K-Blu-ray/387007/" rel="nofollow">Alexander Revisited: The Final Cut 4K</a><br>
</td></tr><tr><td><span>3.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Jaws-4K-Blu-ray/387417/" rel="nofollow">Jaws 4K</a><br>
</td></tr><tr><td><span>4.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Abigail-4K-Blu-ray/387010/" rel="nofollow">Abigail 4K</a><br>
</td></tr><tr><td><span>5.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Captain-America-Brave-New-World-4K-Blu-ray/385905/" rel="nofollow">Captain America: Brave New World 4K</a><br>
</td></tr><tr><td><span>6.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Captain-America-Brave-New-World-Blu-ray/386295/" rel="nofollow">Captain America: Brave New World</a><br>
</td></tr><tr><td><span>7.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Looney-Tunes-Collectors-Vault-Volume-1-Blu-ray/384090/" rel="nofollow">Looney Tunes Collector's Vault: Vol...</a><br>
</td></tr><tr><td><span>8.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Sinners-4K-Blu-ray/384583/" rel="nofollow">Sinners 4K</a><br>
</td></tr><tr><td><span>9.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Grave-of-the-Fireflies-Blu-ray/387023/" rel="nofollow">Grave of the Fireflies</a><br>
</td></tr><tr><td><span>10.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Small-Soldiers-4K-Blu-ray/386359/" rel="nofollow">Small Soldiers 4K</a><br>
</td></tr><tr><td colspan="2">
&nbsp;&nbsp;<b>»</b> <a href="https://www.blu-ray.com/movies/top.php?show=preorders">See more pre-orders</a></td></tr></tbody></table><br>
<table>
<tbody><tr><td colspan="2">
</td></tr><tr><td><span>1.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Final-Destination-5-Film-Collection-Blu-ray/315427/" rel="nofollow">Final Destination 5 Film Collection</a><br>
<span>$12.99, Save 48%</span><br>
</td></tr><tr><td><span>2.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Solo-A-Star-Wars-Story-4K-Blu-ray/205706/" rel="nofollow">Solo: A Star Wars Story 4K</a><br>
<span>$16.79, Save 57%</span><br>
</td></tr><tr><td><span>3.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Gladiator-II-4K-Blu-ray/365746/" rel="nofollow">Gladiator II 4K</a><br>
<span>$19.96, Save 47%</span><br>
</td></tr><tr><td><span>4.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/The-Day-the-Earth-Blew-Up-A-Looney-Tunes-Movie-Blu-ray/384325/" rel="nofollow">The Day the Earth Blew Up: A Looney...</a><br>
<span>$14.84, Save 45%</span><br>
</td></tr><tr><td><span>5.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Andor-The-Complete-First-Season-4K-Blu-ray/356918/" rel="nofollow">Andor: The Complete First Season 4K</a><br>
<span>$44.96, Save 48%</span><br>
</td></tr><tr><td><span>6.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Presence-4K-Blu-ray/382881/" rel="nofollow">Presence 4K</a><br>
<span>$21.99, Save 45%</span><br>
</td></tr><tr><td><span>7.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/How-to-Train-Your-Dragon-3-Movie-Collection-Blu-ray/234903/" rel="nofollow">How to Train Your Dragon: 3-Movie C...</a><br>
<span>$12.59, Save 58%</span><br>
</td></tr><tr><td><span>8.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Blade-Runner-2049-4K-Blu-ray/189774/" rel="nofollow">Blade Runner 2049 4K</a><br>
<span>$14.99, Save 57%</span><br>
</td></tr><tr><td><span>9.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/Andor-The-Complete-First-Season-Blu-ray/347799/" rel="nofollow">Andor: The Complete First Season</a><br>
<span>$39.96, Save 47%</span><br>
</td></tr><tr><td><span>10.</span>&nbsp;
</td><td><a href="https://www.blu-ray.com/link/link.php?url=https://www.blu-ray.com/movies/The-Mandalorian-The-Complete-Third-Season-4K-Blu-ray/374175/" rel="nofollow">The Mandalorian: The Complete Third...</a><br>
<span>$39.36, Save 48%</span><br>
</td></tr><tr><td colspan="2">
&nbsp;&nbsp;<b>»</b> <a href="https://www.blu-ray.com/movies/deals.php?action=bestdeals">See more deals</a></td></tr></tbody></table><br>	</td>
	</tr>
	</tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The curse of knowing how, or; fixing everything (452 pts)]]></title>
            <link>https://notashelf.dev/posts/curse-of-knowing</link>
            <guid>43902212</guid>
            <pubDate>Tue, 06 May 2025 06:01:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://notashelf.dev/posts/curse-of-knowing">https://notashelf.dev/posts/curse-of-knowing</a>, See on <a href="https://news.ycombinator.com/item?id=43902212">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-gjtny2mx="">  <p>It starts innocently.</p>
<p>You rename a batch of files with a ten-line Python script, or you alias a common
<code>git</code> command to shave off two keystrokes. Maybe you build a small shell
function to format JSON from the clipboard.</p>
<p>You’re not even trying to be clever. You’re just solving tiny problems. Making
the machine do what it should have done in the first place. And then something
happens. You cross a <em>threshold</em>. You look at your tools, your environment, your
operating system—even your editor—and suddenly <strong>everything</strong> is fair game.</p>
<p>You <em>could</em> rebuild that (if you wanted to).<br>
You could <em>improve</em> that (if you wanted to).</p>
<p>Then someone challenges you. As banter maybe, perhaps jokingly but also with a
dash of hope. Then the air in the room changes.</p>
<p>It suddenly becomes something else. It becomes:</p>
<p>You <em>should</em>.</p>
<p>And from that moment forward, the world is broken in new and specific ways that
only <em>you</em> can see.</p>
<h2 id="technical-capability-as-a-moral-weight">Technical Capability as a Moral Weight</h2>
<p>Before I could program, broken software was frustrating but ignorable. For years
I’ve simply “used” a computer, as a consumer. I was what companies were
concerned with tricking into buying their products, or subscribing to their
services. Not the technical geek that they prefer to avoid with their software
releases, or banning from their games based on an OS.</p>
<p>Now it has become <em>provocative</em>. I can see the patterns that I wish I couldn’t,
find oversights that I can attribute to a certain understanding (or the lack
thereof) of a certain concept and I can <em>hear</em> what has been echoing in the head
of the computer illiterate person who conjured the program I have to debug.</p>
<p>I notice flaws like a good surgeon notices a limp.<br>
Why the <em>hell</em> does this site send ten megabytes of JavaScript for a static
page?<br>
Why is the CLI output not parseable by <code>awk</code>?<br>
Why is this config hardcoded when it could be declarative?</p>
<p>Those things are <em>not</em> just questions, they are <em>accusations</em>. And,
unfortunately, they do not stop.</p>
<p>Now that I’ve learned to notice, my perception of software has changed in its
entirety.</p>
<p>Every piece of software becomes a TODO list.<br>
Every system becomes a scaffolding for a better one.<br>
Every inconvenience becomes an indictment of inaction.</p>
<h2 id="one-must-imagine-sisyphus-happy">One Must Imagine Sisyphus Happy</h2>
<p>Like Camus’ Sisyphus, we are condemned to push the boulder of our own systems
uphill—one fix, one refactor, one script at a time. But unlike the story of
Sisyphus, the curse is not placed onto you by some god. We built the boulder
ourselves. And we keep polishing it on the way up.</p>
<p>I’ve lost count of how many projects I have started that began with some
variation of “Yeah, I could build this <em>but better</em>.”</p>
<ul>
<li>A static site generator because the existing ones had too many opinions.</li>
<li>A note-taking tool because I didn’t like the way others structured metadata.</li>
<li>A CLI task runner because Make is cryptic and Taskfile is YAML hell.</li>
<li>A personal wiki engine in Rust, then in Go, then in Nim, then back to
Markdown.</li>
<li>A homelab dashboard because I don’t like webslop.</li>
</ul>
<p>The list continues, and trust me it <em>does</em> continue. My dev directory, as it
stands, is nearing 30 gigabytes.</p>
<p>If you ask me, I was solving real, innocent problems. But in hindsight, I was
also feeding something else: a compulsion to assert control. Every new tool I
built was a sandbox I <em>owned</em>: No weird bugs. No legacy constraints. No
decisions I didn’t entirely agree with. Until, of course, I became the legacy.</p>
<p>Kafka once wrote that “<strong>a cage went in search of a bird</strong>.” <sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> That is what
these projects can become. Empty systems we keep building, waiting for purpose,
for clarity, for… salvation? I’m not sure how else would you call this
pursuit.</p>
<h2 id="entropy-is-undefeated">Entropy Is Undefeated</h2>
<p>Now let’s go back. Back to when we didn’t know better.</p>
<p>Software doesn’t stay solved. Every solution you write starts to rot the moment
it exists. Not now, not later, but eventually. Libraries deprecate. APIs
change. Performance regressions creep in. Your once-perfect tool breaks silently
because <code>libfoo.so</code> is now <code>libfoo.so.2</code>. <sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup></p>
<p>I <em>have</em> had scripts silently fail because a website changed its HTML layout.<br>
I <em>have</em> had configuration formats break because of upstream version bumps.<br>
I <em>have</em> had Docker containers die because Alpine Linux rotated a mirror URL.</p>
<p>In each case, the immediate emotional response was not just inconvenience but
something that moreso resembles <em>guilt</em>. I built this, and I do know better. How
could I not have foreseen this? Time to fix it.</p>
<p>If you replace every part of the system over time, is it still the same tool?
Does it still serve the same purpose? Do <em>you</em>?</p>
<h2 id="the-illusion-of-finality">The Illusion of Finality</h2>
<p>I think we lie to ourselves.</p>
<blockquote>
<p>“If I just get this setup right, I’ll never have to touch it again."<br>
"If I just write this one tool, my workflow will be seamless."<br>
"If I automate this, I’ll save time forever.” <sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup><br>
“Write once, run everywhere.” My ass.</p>
</blockquote>
<p>It is, I admit, a seductive lie. It frames programming as a conquest of sorts. A
series of battles you win, or challenges you complete. But the imaginary war
never ends. You don’t build a castle. You dig trenches. And they flood every
time it rains. The trials are <em>never</em> complete.</p>
<h2 id="technical-work-as-emotional-regulation">Technical Work as Emotional Regulation</h2>
<p>On the theme of filling this post with literary references, let me quote the
Stoic Marcus Aurelius.</p>
<blockquote>
<p>You have power over your mind—not outside events. Realize this, and you will
find strength.</p>
</blockquote>
<p>But programming lures us into believing we <em>can</em> control the outside events.
That is where the suffering begins. There is something deeper happening here.
This is <em>not</em> just about software.</p>
<p>I believe sometimes building things is how we self-soothe. We write a new tool
or a script because we are in a desperate need for a small victory. We write a
new tool because we are overwhelmed. Refactor it, not because the code is messy,
but your life is. We chase the perfect system because it gives us something to
hold onto when everything else is spinning. This is the lesson I’ve taken from
using NixOS.</p>
<p>I have written entire applications just to avoid thinking about why I was
unhappy. Programming gives you instant feedback. You run the thing, and it
works. Or it <em>doesn’t</em>, and you fix it. Either way, you’re <em>doing something</em>.</p>
<p>That kind of agency is addictive. Especially when the rest of life doesn’t offer
it. We program because we <em>can</em>, even when we shouldn’t. Because at least it
gives us something to rebel against.</p>
<h2 id="the-burnout-you-dont-see-coming">The Burnout You Don’t See Coming</h2>
<p>Burnout doesn not just come from overwork. It comes from <em>overresponsibility</em>.</p>
<p>And programming, once internalized deeply enough, makes everything feel like
your responsibility. The bloated website. The inefficient script. The clunky
onboarding process at your job. You <em>could</em> fix it. So why aren’t you?</p>
<p>The truth you are very well aware of is that you can’t fix it all. You <em>know</em>
this, you always knew it regardless of your level of skill. But try telling that
to the part of your brain that sees every inefficiency as a moral failing.</p>
<p>Nietzsche warned of gazing too long into the abyss. But he didn <em>not</em> warn what
happens when the abyss is a <code>Makefile</code> or a 30k line of code Typescript project.</p>
<h2 id="learning-to-let-go">Learning to Let Go</h2>
<p>So where is the exit? Is this akin to Sartre’s depiction of hell, where hell
<em>is</em> other people and how they interact with your software? Or is it some weird
backwards hell where people create software that you have to interact with?</p>
<p>The first step is recognizing that <em>not everything broken is yours to fix</em>.<br>
Not every tool needs replacing.<br>
Not every bad experience is a call to action.</p>
<p>Sometimes, it’s OK to just <em>use</em> the thing. Sometimes it’s enough to know <em>why</em>
it’s broken—even if you don’t fix it. Sometimes the most disciplined thing you
can do is <em>walk away</em> from the problem you know how to solve. There’s a kind of
strength in that.</p>
<p>Not apathy, no. Nor laziness. Just… some restraint.</p>
<h2 id="a-new-kind-of-skill">A New Kind of Skill</h2>
<p>What if the real skill isn’t technical mastery? Or better yet what if it’s
emotional clarity?</p>
<ul>
<li>Knowing which problems are worth your energy.</li>
<li>Knowing which projects are worth maintaining.</li>
<li>Knowing when you’re building to help—and when you’re building to cope.</li>
<li>Knowing when to stop.</li>
</ul>
<p>This is what I’m trying to learn now. After the excitement. After the obsession.
After the burnout. I’m trying to let things stay a little broken. Because I’ve
realized I don’t want to <em>fix everything</em>. I just want to feel OK in a world
that often isn’t. I can fix something, but not everything.</p>
<hr>
<p>You learn how to program. You learn how to fix things. But the hardest thing
you’ll ever learn is when to <em>leave them broken</em>.</p>
<p>And maybe that’s the most human skill of all.</p>
<section data-footnotes="">
<ol>
<li id="user-content-fn-1">
<p>From, I believe, Kafka’s <em>The Zurau Aphorisms</em>. <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>Nix solves this. Or does it? Nix was a can of worms of its own. <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>Remember when you spent 2 hours automating a 30 minute task? Yeah, it’s
that again. <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3">↩</a></p>
</li>
</ol>
</section>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Appeal to Apple from Anukari: one tiny macOS detail to make Anukari fast (160 pts)]]></title>
            <link>https://anukari.com/blog/devlog/an-appeal-to-apple</link>
            <guid>43901619</guid>
            <pubDate>Tue, 06 May 2025 03:40:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anukari.com/blog/devlog/an-appeal-to-apple">https://anukari.com/blog/devlog/an-appeal-to-apple</a>, See on <a href="https://news.ycombinator.com/item?id=43901619">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>TL;DR: To make Anukari’s performance reliable across all Apple silicon macOS devices, I need to talk to someone on the Apple Metal team. It would be great if someone can connect me with the right person inside Apple, or direct them to my feedback request FB17475838 as well as this devlog entry.</strong></p>
<p><em>This is going to be a VERY LONG HIGHLY TECHNICAL post, so either buckle your seatbelt or leave while you still can.</em></p>
<h2>Background</h2>
<p>The <a href="https://anukari.com/">Anukari 3D Physics Synthesizer</a> simulates a large spring-mass model in real-time for audio generation. To support a nontrivial number of physics objects, it requires a GPU for the simulation. The physics code is ALU-bound, not memory-bound. All mutable state in the simulation is stored in the GPU’s threadgroup memory, which is roughly equivalent to a manually-allocated L1 cache, so it is extremely fast.</p>
<p>The typical use-case for Anukari is running it as an AudioUnit (AU) or VST3 plugin inside a host application like Pro Tools or Ableton, also called a Digital Audio Workstation (DAW). The DAW invokes Anukari for each audio buffer block, which is a request to generate/process N samples of audio. For each block, Anukari invokes the physics simulation GPU kernel, waits for the result, and returns.</p>
<p>The audio buffer block system is important because GPU kernel scheduling has a certain amount of latency overhead, and for real-time audio we have fixed time constraints. By amortizing the GPU scheduling latency over, say, 512 audio samples, it becomes negligible. But the runtime of the kernel itself is still very important.</p>
<h2>Basic Problem</h2>
<p>Apple’s macOS is obviously extremely clever about power management, and Apple silicon hardware is built to support the OS in achieving high power efficiency.</p>
<p>As with all modern hardware, the clock rate for Apple silicon chips can be slowed down to reduce power consumption. When the OS detects that the processing demand for a given chip is low (or non-existent), it can decrease the clock rate for that chip. This is awesome.</p>
<p>The problem is that due to the way Anukari runs inside a DAW and interacts with the GPU, the heuristics that macOS uses to determine whether there is sufficient demand upon the GPU to increase its clock rate do not work.</p>
<p>Consider the chart below. The CPU does some preparatory work, there’s a small gap which represents kernel invocation latency, and then the GPU does a large block of work. Finally there’s another small gap representing the real-time headroom.</p>
<p><img src="https://static.anukari.com/blog-media/cpu-gpu-time-sequence.jpg"></p><p><em>(An aside: chalkboards are way better than whiteboards, unless you enjoy getting high on noxious fumes. in which case whiteboards are the way to go.)</em></p>
<p>I don’t have any real knowledge of macOS’s heuristics for deciding when to increase the GPU clock speed, but I might reasonably guess that it relies on something like the <a href="https://en.wikipedia.org/wiki/Load_(computing)">load average</a>. In the diagram above, the GPU load average might be only 60%, because between audio buffer blocks it is idle. Perhaps this does not meet the threshold for increasing the GPU clock rate.</p>
<p>But this is terrible for Anukari, because to meet real-time constraints, it needs the absolute lowest latency possible, which requires the highest GPU clock rate. I’m not sure how low the Apple GPU clock rate can go, but it definitely goes low enough to make Anukari unusable.</p>
<p>To be clear, it’s pretty understandable that macOS handles this situation poorly, because the GPU is mostly used for throughput workflows like graphics or ML. Audio on the GPU is really new, and there are only a couple of companies doing it right now.</p>
<h2>Are you sure the clock rate is the problem?</h2>
<p><em>Oh yes.</em> Thankfully, Apple’s first-part Instruments tools that come with Xcode have a handy Metal profiler. Among other things, this is how I first learned that Anukari is ALU-bound.</p>
<p>The Metal profiler has an incredibly useful feature: it allows you to choose the Metal “Performance State” while profiling the application. This is not configurable outside of the profiler. This is how I first figured out that the GPU clock rate was the issue: Anukari works perfectly under the Maximum performance state, and abysmally under the Minimum performance state.</p>
<h2>Wait, Anukari mostly works great on macOS. How is that possible?</h2>
<p>Given the explanation above, this is a great question. Most people with macOS find that Anukari works great. For example, I intentionally bought a base-model Macbook M1 for development, so that if Anukari worked well for me, I’d know it worked well for people with beefier hardware.</p>
<p>So how is it that Anukari works great on macOS for most people? Well, as Steve Jobs said, “It’s better to be a pirate than to join the Navy.”</p>
<p>Without being able to rely on macOS to do the right thing, I thought like a pirate and came up with a workaround: in parallel with the audio computation on the GPU, Anukari runs a second workload on the GPU that is designed to create a high load average and trick macOS into clocking up the GPU. This workload is tuned to use as little of the GPU as possible, while still creating a big enough artificial load to trigger the clock heuristics.</p>
<p>In other words, it runs a spin loop to heat up the GPU. On my Macbook M1, this completely solves the issue. Anukari runs completely reliably. I called this strategy “waste makes haste” and it is documented in detail on my devlog <a href="https://anukari.com/blog/devlog/waste-makes-haste">here</a>.</p>
<p>To be clear, the spin loop is an unholy abomination and I hate that it’s necessary. But it is absolutely required for Anukari to work well for macOS users. And for most macOS users, it works great.</p>
<h2>So with the “waste makes haste” strategy, what’s the problem?</h2>
<p>Like I said, on my M1 things work perfectly. But then I released the Anukari Beta and some macOS users are having problems. What’s different?</p>
<p>First: I’m not certain. But I have a couple hypotheses.</p>
<p>Weirdly, it appears that most users with performance issues are using Pro or Max Apple hardware. These have additional GPU chiplets. I am completely speculating here, but Apple’s hardware is amazing so it stands to reason that each GPU chiplets's clock rate can be changed independently. Do you see where this is going?</p>
<p>If macOS is really smart, it should see that Anukari is running two independent GPU workloads: the physics kernel, and the spin kernel. Why not run those on separate GPU chiplets? That’s smart. And then, if I’m right that the GPU chiplets have independent clock rates, well, Anukari is hosed because the stupid spin workload will get a fast-clock GPU and the audio workload will get a slow-clock GPU.</p>
<p>I could be wrong. Another possibility is that the GPU has a single clock rate, and my spin workload is too conservative to convince the much more powerful GPU to clock up. Maybe my spin kernel can heat up an M1 GPU but not an M4 Pro GPU, because the M4 Pro is faster.</p>
<h2>What do you think macOS should do to fix this?</h2>
<p>Apple engineers will obviously know better than I do here, but I’ll present a couple of obvious possibilities.</p>
<p><strong>Solution 1:</strong> On macOS, audio processing is done on a thread (or group of threads) called an Audio Workgroup. These are explained in Apple’s documentation <a href="https://developer.apple.com/documentation/audiotoolbox/understanding-audio-workgroups?language=objc">here</a>. Within an Audio Workgroup, the OS understands that the threads have real-time constraints, and prioritizes those threads appropriately. This is actually a fantastic innovation, because before Audio Workgroups, it wasn’t really possible to do real-time audio processing safely across multiple threads without problems like priority inversion, etc.</p>
<p>The Audio Workgroup concept could be extended to cover processing on the GPU. Any MTLCommandQueue managed by an Audio Workgroup thread could be treated as real-time and the GPU clock could be adjusted accordingly.</p>
<p><strong>Solution 2:</strong> The Metal API could simply provide an option on MTLCommandQueue to indicate that it is real-time sensitive, and the clock for the GPU chiplet handling that queue could be adjusted accordingly.</p>
<p><strong>Solution 3:</strong> Someone could point out that I'm an idiot and there's already some way to get what I want, and this whole post was a waste of my time. This would be wonderful.</p>
<h2>Wouldn’t Apple’s new Game Mode help?</h2>
<p>Game Mode certainly seems similar to what Anukari needs. However, Game Mode is at the process-level, and Anukari is mostly used as a plugin inside other processes, which don’t support Game Mode, and anyway Anukari has no control. Also Anukari is usually not fullscreen, which Game Mode requires.</p>
<h2>What about Windows performance?</h2>
<p>Not a problem at all. I don’t know if it’s because Windows gives users more control over their system’s performance state, or if e.g. NVIDIA drivers are less careful about power consumption, or what. But the spin loop is not necessary on Windows.</p>
<p>It's not a great look for Apple that a Windows PC with a pretty wimpy GPU can run Anukari just fine, and the most expensive Mac M4 Max stutters, because obviously Apple's hardware is <em>incredible</em> and just needs to be let off the leash a bit.</p>
<h2>Why don’t you just pipeline the GPU code so that it saturates the GPU?</h2>
<p>For a throughput workload, this is exactly what I’d do. But Anukari is not throughput-sensitive, it is latency-sensitive.</p>
<p>The idea with pipelining is that maybe Anukari could schedule multiple physics simulation kernels in advance, so that the GPU could be processing the current audio sample block at the same time that the CPU is preparing the next block for the GPU. (This would also alleviate the kernel invocation latency overhead, but that’s not as important.)</p>
<p>But anyone who understands pipelining knows that it increases throughput at the cost of latency. Anukari processes audio in real-time, so each kernel invocation needs access to the real-time audio input data (e.g. from the microphone). Thus Anukari can’t do something like speculative execution where it processes the next audio block early, because it wouldn’t have the input data required to do so.</p>
<h2>Why don’t you run the spin kernel in the same MTLCommandQueue as the physics kernel?</h2>
<p>This solution might fix the problem where the spin and physics kernels end up running on separate GPU chiplets (if that is indeed the issue).</p>
<p>I <em>have</em> tried this, actually. The reason it doesn’t work is again because Anukari is latency-sensitive. What happens is that sometimes the spin kernel runs a little too long and cuts into the time for running the physics kernel. I experimented with small spin kernels, using volatile unified memory to allow the CPU to write an “exit kernel early” flag. Even with these hijinx, sometimes the spin kernel cut into physics kernel time.</p>
<h2>Why not just make the GPU code more efficient?</h2>
<p>Because the simulation is ALU-bound, there’s not much performance to be gained from the typical optimization low-hanging fruit: improving memory access patterns. To speed up Anukari’s physics kernel, the only thing that really helps is optimizing arithmetic throughput. For example, Anukari uses FP16 math where possible to better-saturate Apple’s ALUs. Instructions have been reordered using micro-benchmarks. All physics state is in L1 memory. Loads are reordered for vectorization. The list goes on.</p>
<p>Furthermore, Anukari heavily exploits the fact that threads within Apple’s SIMD-groups (roughly) share an instruction pointer. Different physics objects have highly divergent code branch paths, so simulating two types of objects within a SIMD-group is slow due to the requirement for instruction masking while the top-level branch paths are executed serially. To avoid this, Anukari dynamically optimizes the memory layout of physics objects to minimize the number of object types executed within each SIMD-group. This optimization is <a href="https://anukari.com/blog/devlog/the-new-warp-alignment-optimizer">described here in great detail</a>, and is a massive performance win.</p>
<p>What I'm trying to say is that I have <em>bent over backwards</em> to squeeze every last drop of performance out of Apple's hardware. It's been fun to do!</p>
<p>There are further arithmetic optimizations to be done, but they will be fairly marginal. We’re talking single-digit percentage point speedups. Anukari’s GPU code is already VERY FAST. If you’re curious, there’s more info on Anukari’s optimizations <a href="https://anukari.com/blog/devlog/tags/optimization">here</a>.</p>
<h2>Why use the GPU at all?</h2>
<p>On powerful machines, Anukari can simulate 768 - 1024 physics objects. Each object can be arbitrarily connected to other objects, meaning that they influence one another. Each object has to be stepped forward in an implicit Euler integration at the audio sample rate, typically 48,000 samples per second. Each object has somewhere between 3 and 10 parameters that affect its behavior. Some of the behaviors involve expensive math, like vector rotation, exp(), log(), etc.</p>
<p>This is simply not even close to feasible on the CPU! Trust me, I tried. It’s not even a little bit close. The GPU just has a ridiculous number of ALUs, it gives me explicit control over the L1 cache layout, and the concurrency constructs like threadgroup_barrier allow the physics integration steps to be done massively in parallel without consistency issues, without expensive CPU mutexes.</p>
<p>I’ll say it again: Anukari does not exist without GPU processing.</p>
<h2>Why should Apple care about what Anukari needs?</h2>
<p>Maybe they shouldn’t, I don’t know. Anukari is a tiny startup. It’s a niche product. It’s doing something weird.</p>
<p>On the other hand, the people who like Anukari really like it. Mick Gordon, the composer for IMO the greatest DOOM games of all time, randomly showed up and blew everyone away with an <a href="https://x.com/Mick_Gordon/status/1918146487948919222">incredible demo using Anukari</a>. Anukari is receiving praise from people who really love synthesizers like <a href="https://cdm.link/anukari-physics-based-instrument/">CDM</a>. Comments have appeared on random internet threads that I didn't start, like, <em>"it's the most creative plugin I've tried in the last 10 years.</em></p>
<p>But mostly, Anukari is using Apple’s hardware in what I consider an incredibly cool way, allowing users to do something that they’ve never been able to do before. And Apple’s hardware is completely up to the task. But it just needs a little push in the right direction.</p>
<h2>Why don’t you use GPU Audio’s APIs?</h2>
<p>I don’t really want to include this last question, but it is here to address the fact that the CEO of GPU Audio, Alexander Talashov, likes to drive by threads about Anukari and suggest that if Anukari used his APIs it would solve our problems. I would be so happy if this were true.</p>
<p>Alexander is a great guy, and his product (<a href="https://www.gpu.audio/">GPU Audio</a>) is a great product. I’ve met him in person, and he’s incredibly passionate about making the GPU accessible for DSP. Audio folks should check it out, it’s really cool. I wish GPU Audio huge success, and support their product.</p>
<p>But… GPU Audio has nothing useful for Anukari. Fundamentally the problem is that Anukari is not anything like a traditional DSP application. It’s a numerical differential equation integrator, far more similar to a video game physics engine than a DSP application. Sure, there are bits and bobs of DSP inside the physics engine, for example Anukari’s mics in the physics world do have compression, and that’s done in-line with the physics calculations on the GPU. But the vast majority of computation is Eulerian integration.</p>
<p>It needs to be understood that I’m programming the GPU at the bare Metal layer (yes pun intended), and am taking advantage of a number of hardware features and ridiculous domain-specific optimizations that are quite necessary to make this work. <em>And all I need is for Apple to reliably turn up the clock rate on the GPU.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Critical CSS (134 pts)]]></title>
            <link>https://critical-css-extractor.kigo.studio/</link>
            <guid>43901495</guid>
            <pubDate>Tue, 06 May 2025 03:13:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://critical-css-extractor.kigo.studio/">https://critical-css-extractor.kigo.studio/</a>, See on <a href="https://news.ycombinator.com/item?id=43901495">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Google has most of my email because it has all of yours (2014) (217 pts)]]></title>
            <link>https://mako.cc/copyrighteous/google-has-most-of-my-email-because-it-has-all-of-yours</link>
            <guid>43901204</guid>
            <pubDate>Tue, 06 May 2025 02:09:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mako.cc/copyrighteous/google-has-most-of-my-email-because-it-has-all-of-yours">https://mako.cc/copyrighteous/google-has-most-of-my-email-because-it-has-all-of-yours</a>, See on <a href="https://news.ycombinator.com/item?id=43901204">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-2527">
		<!-- .entry-header -->

	
	<div>
		<p><em>Republished by <a href="http://www.slate.com/blogs/future_tense/2014/05/13/don_t_use_gmail_here_s_how_to_determine_how_many_of_your_emails_google_may.html">Slate</a>. Translations available in <a href="http://www.slate.fr/life/87045/gmail-google-emails-prive">French (Français)</a>, <a href="http://www.genbeta.com/web/no-usas-gmail-da-igual-google-tiene-muchos-de-tus-emails">Spanish (Español)</a>, <a href="http://www.labazhou.net/2014/05/google-has-most-of-my-email-because-it-has-all-of-yours/">Chinese (中文)</a></em></p>
<p>For almost 15 years, I have run my own email server which I use for all of my non-work correspondence. I do so to keep <a href="http://autonomo.us/">autonomy</a>, control, and privacy over my email and so that no big company has copies of all of my personal email.</p>
<p>A few years ago, I was surprised to find out that my friend <a href="https://www.eff.org/about/staff/peter-eckersley">Peter Eckersley</a> — a very privacy conscious person who is Technology Projects Director at the <a href="https://eff.org/">EFF</a> — used Gmail. I asked him why he would willingly give Google copies of all his email. Peter pointed out that if all of your friends use Gmail, Google has your email anyway. Any time I email somebody who uses Gmail — and anytime they email me — Google has that email.</p>
<p>Since our conversation, I have often wondered just how much of my email Google really has. This weekend, I wrote a small program to go through all the email I have kept in my personal inbox since April 2004 (when Gmail was started) to find out.</p>
<p>One challenge with answering the question is that many people, like Peter, use Gmail to read, compose, and send email but they configure Gmail to send email from a non-<tt>gmail.com</tt> “From” address. To catch these, my program looks through each message’s headers that record which computers handled the message on its way to my server and to pick out messages that have traveled through <tt>google.com</tt>, <tt>gmail.com</tt>, or <tt>googlemail.com</tt>. Although I usually filter them, my personal mailbox contains emails sent through a number of mailing lists. Since these mailing lists often “hide” the true provenance of a message, I exclude all messages that are marked as coming from lists using the (usually invisible) “Precedence” header.</p>
<p>The following graph shows the numbers of emails in my personal inbox each week in red and the subset from Google in blue. Because the number of emails I receive week-to-week tends to vary quite a bit, I’ve included a <a href="https://en.wikipedia.org/wiki/LOESS">LOESS</a> “smoother” which shows a moving average over several weeks.</p>
<p><a href="https://mako.cc/copyrighteous/wp-content/uploads/2014/05/emails_gmail_over_time.png"><img fetchpriority="high" decoding="async" src="https://mako.cc/copyrighteous/wp-content/uploads/2014/05/emails_gmail_over_time.png" alt="Emails, total and from GMail, over time" width="720" height="432"></a>From eyeballing the graph, the answer to seems to be that, although it varies, about a third of the email in my inbox comes from Google!</p>
<p>Keep in mind that this is all of my personal email and includes automatic and computer generated mail from banks and retailers, etc. Although it is true that Google doesn’t have these messages, it suggests that the proportion of my truly “personal” email that comes via Google is probably much higher.</p>
<p>I would also like to know how much of the email I send goes <em>to</em> Google. I can do this by looking at emails in my inbox that I have replied to. This works if I am willing to assume that if I reply to an email sent from Google, it ends up back at Google. In some ways, doing this addresses the problem with the emails from retailers and banks since I am very unlikely to reply to those emails. In this sense, it also reflects a measure of more truly personal email.</p>
<p>I’ve broken down the proportions of emails I received that come from Google in the graph below for all email (top) and for emails I have replied to (bottom). In the graphs, the size of the dots represents the total number of emails counted to make that proportion. Once again, I’ve included the LOESS moving average.</p>
<p><a href="https://mako.cc/copyrighteous/wp-content/uploads/2014/05/emails_gmail_prop_over_time.png"><img decoding="async" src="https://mako.cc/copyrighteous/wp-content/uploads/2014/05/emails_gmail_prop_over_time.png" alt="Proportion of emails from GMail over time" width="720" height="576"></a>The answer is surprisingly large. Despite the fact that I spend hundreds of dollars a year and hours of work to host my own email server, Google has about half of my personal email! Last year, Google delivered 57% of the emails in my inbox that I replied to. They have delivered more than a third of all the email I’ve replied to every year since 2006 and more than half since 2010. On the upside, there is some indication that the proportion is going down. So far this year, only 51% of the emails I’ve replied to arrived from Google.</p>
<p>The numbers are higher than I imagined and reflect somewhat depressing news. They show how it’s complicated to think about privacy and autonomy for communication between parties. I’m not sure what to do except encourage others to consider, in the wake of the Snowden revelations and everything else, whether you really want Google to have all your email. And half of mine.</p>
<p>If you want to run the analysis on your own, you’re welcome to <a href="http://projects.mako.cc/source/?p=gmail-maildir-counter">the Python and R code I used to produce the numbers and graphs</a>.</p>
	</div><!-- .entry-content -->

	 <!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Analyzing Modern Nvidia GPU Cores (150 pts)]]></title>
            <link>https://arxiv.org/abs/2503.20481</link>
            <guid>43900463</guid>
            <pubDate>Mon, 05 May 2025 23:38:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2503.20481">https://arxiv.org/abs/2503.20481</a>, See on <a href="https://news.ycombinator.com/item?id=43900463">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2503.20481">View PDF</a></p><blockquote>
            <span>Abstract:</span>GPUs are the most popular platform for accelerating HPC workloads, such as artificial intelligence and science simulations. However, most microarchitectural research in academia relies on GPU core pipeline designs based on architectures that are more than 15 years old.
<br>This paper reverse engineers modern NVIDIA GPU cores, unveiling many key aspects of its design and explaining how GPUs leverage hardware-compiler techniques where the compiler guides hardware during execution. In particular, it reveals how the issue logic works including the policy of the issue scheduler, the structure of the register file and its associated cache, and multiple features of the memory pipeline. Moreover, it analyses how a simple instruction prefetcher based on a stream buffer fits well with modern NVIDIA GPUs and is likely to be used. Furthermore, we investigate the impact of the register file cache and the number of register file read ports on both simulation accuracy and performance.
<br>By modeling all these new discovered microarchitectural details, we achieve 18.24% lower mean absolute percentage error (MAPE) in execution cycles than previous state-of-the-art simulators, resulting in an average of 13.98% MAPE with respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that this new model stands for other NVIDIA architectures, such as Turing. Finally, we show that the software-based dependence management mechanism included in modern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in terms of performance and area.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Rodrigo Huerta [<a href="https://arxiv.org/show-email/03496778/2503.20481" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 26 Mar 2025 12:10:53 UTC (246 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pixels in Islamic art: square Kufic calligraphy (121 pts)]]></title>
            <link>https://uwithumlaut.wordpress.com/2020/07/24/pixels-in-islamic-art-square-kufic-calligraphy/</link>
            <guid>43899251</guid>
            <pubDate>Mon, 05 May 2025 20:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://uwithumlaut.wordpress.com/2020/07/24/pixels-in-islamic-art-square-kufic-calligraphy/">https://uwithumlaut.wordpress.com/2020/07/24/pixels-in-islamic-art-square-kufic-calligraphy/</a>, See on <a href="https://news.ycombinator.com/item?id=43899251">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>When I was a little kid whenever we drove by mosques, I would be intrigued by the complex motifs they’re decorated by. I always tried to figure out the pattern; to me it was just a pattern, I never thought it can be writing because they didn’t look like letters from any alphabet I knew at the time. I remember thinking that I had finally found a full pattern that was only to be broken in the next square. As I grew older I learned that these pieces that resemble a labyrinth, at first sight, have so much more to offer than just lush visuals. Verses from the Quran were taken and turned into striking artwork carrying valuable messages. The more I learned about Islamic calligraphy and especially square Kufic, the more interesting it became and I will be delighted to share this fascination with my readers.</p>



<p>The importance of calligraphy in Islamic culture is indisputable. Unlike in Christian culture, the visual depiction of verses from the Holy Book is not used in decoration. Calligraphy shows itself reciting the Quran with beautiful writing decorating everything from mosques to plates to clothes to carpets. Islamic Calligraphy derives from two main styles; Naskh and Kufic. The peculiarity of Kufic Calligraphy is the straight and structured lettering. The Kufic style in itself comes in a variety of styles such as floriated, square, knotted, new style…  The patterns that appear complex and random comes with strict rules and systematics. This form of Islamic Art is gaining popularity again due to its moderln and graphic look and resurfacing the traditional alluring patterns.</p>



<figure><img data-attachment-id="388" data-permalink="https://uwithumlaut.wordpress.com/kufic_mjriam_nach_fabris/" data-orig-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg" data-orig-size="1312,1322" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="kufic_mjriam_nach_fabris" data-image-description="" data-image-caption="" data-medium-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg?w=298" data-large-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg?w=750" width="1016" height="1023" src="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg?w=1016" alt="" srcset="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg?w=1016 1016w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg?w=150 150w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg?w=298 298w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg?w=768 768w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/kufic_mjriam_nach_fabris.jpg 1312w" sizes="(max-width: 1016px) 100vw, 1016px"><figcaption>Work done by Sergio Fabris found on <a href="https://commons.wikimedia.org/wiki/File:Kufic_mjriam_nach_Fabris.jpg">https://commons.wikimedia.org/wiki/File:Kufic_mjriam_nach_Fabris.jpg</a> (no changes were made)              This image is licensed under the&nbsp;<a href="https://en.wikipedia.org/wiki/en:Creative_Commons">Creative Commons</a>&nbsp;<a href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">Attribution-Share Alike 4.0 International</a>&nbsp;license.</figcaption></figure>



<p>Square Kufic dates all the way back to the 12th-13th century. Sometimes it is also referred to as banna-i which is a Farsi word and it is mostly used to refer to the architectural square Kufic. There are two main theories about how Square Kufic came into being. The first one suggests a fusion between the Arabic script and the Chinese seal script; the Arabic script was fitted into the forms of Chinese scripts and created square Kufic. The second theory argues that it came from architectural adaptations of Arabic script. The second theory seems to be more accreditable and I saw some articles mentioning this assumption as well. This is further supported by the first examples of square Kufic. These were seen in architecture and was done using bricks packed next to one another. The earliest example is from Ghazni (modern-day Afghanistan) in Sultan Meshud Tower also known as the Victory Towers or Masud III Tower.</p>



<div><figure><img data-attachment-id="386" data-permalink="https://uwithumlaut.wordpress.com/513px-shahada-svg_/" data-orig-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/513px-shahada.svg_.png" data-orig-size="513,288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="513px-shahada.svg_" data-image-description="" data-image-caption="" data-medium-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/513px-shahada.svg_.png?w=300" data-large-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/513px-shahada.svg_.png?w=513" width="513" height="288" src="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/513px-shahada.svg_.png?w=513" alt="" srcset="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/513px-shahada.svg_.png 513w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/513px-shahada.svg_.png?w=150 150w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/513px-shahada.svg_.png?w=300 300w" sizes="(max-width: 513px) 100vw, 513px"><figcaption>Shada written in square Kufic style, shaped like buildings</figcaption></figure></div>



<p>As I have mentioned above, Kufic calligraphy has strict rules but in square Kufic, these rules are often bent, and even broken. The letters and the overall script is skewed to fit it into shape. This is why sometimes it can be very hard to decipher what was written in the text. I think the fact that it is not always done to pass on the message openly excites me too. Having a somewhat hidden message in the image makes things a lot more amusing. The only rule that is always followed is keeping the filled out and empty spaces even especially when it comes to thickess. Although this is the rule, the artist might knowingly leave out spaces to create a pattern within the pattern. Typically, the diacritical dots are not used when writing down the script in this form but there are examples of square Kufic using diacritical dots as well. Sometimes dots are used not for diacritical purposes but as filler dots.&nbsp;</p>



<p>The technicalities can get a little bit confusing but I’ll do my best to simplify them and keep it short. The script is written commonly clockwise but it can be anticlockwise or a zigzag pattern. Furthermore, the direction of the writing can change midway to make it fit into the shape. Not only the direction of writing but even the letters and their forms are played with to fill up space evenly. Although the name is Square Kufic, there are many shapes the script can be fitted into. Most commonly it is rectangular shapes but it can be even other words that the Kufic is written into. Once a verse or a single word is written in Square Kufic form, it can be repeated to create a pattern. Sometimes the initial piece is rotated in various ways to create different patterns. This is why I think it is sort of like a pixel, it is the one of many which an image, pattern is created. I envisage that the flexibility it has in comparison to other forms of Kufic makes it easier to adapt to the aesthetic likes we have today. There are two modern-day square Kufic artists that I very much enjoy the work of. These are Kamal Boullata and Ahmed Moustafa. I wasn’t sure if I can include designs of these artists due to copyrights but I most definitely recommend a quick Google search. Ahmed Moustafa can combine different styles of Calligraphy creating multi-layered masterpieces. Kamal Boulata, a Palestinian Christian combines the traditional styles with modern movements combined with impeccable color harmonizations. He uses not only verses from the Quran but also the Gospel of Saint John.</p>



<div><figure><img data-attachment-id="391" data-permalink="https://uwithumlaut.wordpress.com/topkapi_scroll_p308-1/" data-orig-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapi_scroll_p308-1.jpg" data-orig-size="553,677" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="topkapi_scroll_p308-1" data-image-description="" data-image-caption="" data-medium-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapi_scroll_p308-1.jpg?w=245" data-large-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapi_scroll_p308-1.jpg?w=553" loading="lazy" width="553" height="677" src="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapi_scroll_p308-1.jpg?w=553" alt="" srcset="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapi_scroll_p308-1.jpg 553w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapi_scroll_p308-1.jpg?w=123 123w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapi_scroll_p308-1.jpg?w=245 245w" sizes="(max-width: 553px) 100vw, 553px"><figcaption>A pattern done by using square Kufic, from the Topkapı Scroll</figcaption></figure></div>



<p>Taking a step back to the past, one specific example of square Kufic I want to look at in more detail is located in Topkapı Palace, about 30 minutes away from where I am writing this article. The examples on this scroll are some of the oldest of its kind that survived to this day. It is not known exactly when, where, or by whom it was made, although we do know that it was done by a single person. It is assumed that it was done either 15th or 16th century in Iran but not known where exactly in Iran (there is different evidence pointing to different cities). The scroll is impressive in size and design. It is almost 30 meters long which is almost 100 feet. It is actually two different parchments that were combined into one scroll. The scroll shows the different geometrical shapes done in the Timurid architectural style. It is made as a guide for architectural designs and these include square Kufic as well. I can’t help but think how much one had to master different styles of design and calligraphy to be able to prepare a guide for it. Although I have been to the Topkapı Palace multiple times, I don’t remember seeing this scroll (well, the Palace is enormous) but I am very excited to go and examine it in detail.</p>



<figure><img data-attachment-id="383" data-permalink="https://uwithumlaut.wordpress.com/topkapc4b1-scroll/" data-orig-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png" data-orig-size="1920,1080" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="topkapc4b1-scroll" data-image-description="" data-image-caption="" data-medium-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=300" data-large-file="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=750" loading="lazy" src="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=1024" alt="" width="780" height="438" srcset="https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=1024 1024w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=780 780w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=1557 1557w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=150 150w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=300 300w, https://uwithumlaut.wordpress.com/wp-content/uploads/2020/07/topkapc4b1-scroll.png?w=768 768w" sizes="(max-width: 780px) 100vw, 780px"></figure>



<p>Valérie González, an expert on Islamic visual culture, mentions “Sometimes calligraphy is so manipulated visually that the legibility is very difficult” in<a href="https://www.youtube.com/watch?v=f5aDyxoTr6g"> a lecture she gave in Casa Árabe</a>. Probably the eyes of the little girl I used to be were interested in this elaborate style because it was not meant to be understood right away. This form of art has so many layers to be discovered and every layer is even more valuable than the previous one. At first, the eyes are blessed with the intricate design. Then the mind tries to decipher the pattern and the words. Lastly, the meaning and the message makes it more memorable and leaves you with something to think over. Unfortunately, I am not able to decipher these on my own and I need a little bit of help to go deeper than the first layer. Nevertheless, my &nbsp;fascination exceeds what my mind comprehends.</p>







<p>For anyone who wants to learn more about calligraphy in general here is a documentary I very much enjoyed: <a href="https://www.youtube.com/watch?v=v9uuNagb4po">https://www.youtube.com/watch?v=v9uuNagb4po</a></p>



<p>Some of the useful technical information I found was on two sources;&nbsp;</p>



<p>First one is kufic.info, it is a website explaining many aspects of Square Kufic</p>



<p>The second one is the article here; <a href="https://design.tutsplus.com/tutorials/creative-arabic-calligraphy-square-kufic--cms-23012">https://design.tutsplus.com/tutorials/creative-arabic-calligraphy-square-kufic–cms-23012</a></p>



<p>If you speak Turkish, I found the article “Kufi Yazıda Geometrik Yorumlar Üzerine Bir Deneme” by Omiir Bakirer very helpful. It can be found on dergipark.</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Replacing Kubernetes with systemd (2024) (328 pts)]]></title>
            <link>https://blog.yaakov.online/replacing-kubernetes-with-systemd/</link>
            <guid>43899236</guid>
            <pubDate>Mon, 05 May 2025 20:40:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.yaakov.online/replacing-kubernetes-with-systemd/">https://blog.yaakov.online/replacing-kubernetes-with-systemd/</a>, See on <a href="https://news.ycombinator.com/item?id=43899236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                            <p>Yes, I'm fully aware those are two separate things, but hear me out here for a moment.</p><p>Back in 2018 I was hearing a lot of stuff from all angles and all sorts of friends and influences about Kubernetes, and from what I heard it seemed like a pretty promising piece of kit to use. At the time, I actually went out and bought a NUC to act as a little hypervisor so that I could play around with a small cluster at home.</p><p>Funnily enough, my blog post on this was <a href="https://blog.yaakov.online/learning-kubernetes-at-home/" rel="noreferrer">six years ago to the very day</a>.</p><p>The main lesson that I learned is that although Kubernetes is made up of all sorts of pieces and web services and sidecars and webhooks, basically acts as a giant <code>while</code> loop as follows:</p><pre><code>while (true)
{
    var current_state = GetCurrentState();
    var desired_state = GetDesiredState();
    var diff = CalculateDiff(current_state, desired_state);
    ApplyDiff(diff);
}</code></pre><p>If I said there should be a Pod here, and there wasn't, Kubernetes would create it. If I said there should be 3 replicas and there were 4, Kubernetes would get rid of one.</p><p>This actually extended out in really cool ways, such as with <a href="https://cert-manager.io/?ref=blog.yaakov.online" rel="noreferrer">cert-manager</a>. If I said there should be a valid TLS certificate for some domain, and told Kubernetes how it could request one, then if the certificate was missing or expiring, Kubernetes would go out and get a new certificate and install it in the web server automagically.</p><p>But as the memes go, what I was using Kubernetes for was fun to experiment with, but total overkill.</p><figure><img src="https://blog.yaakov.online/content/images/2024/02/C-NknkeUwAAxSQs.jpeg" alt="A few small pieces of wood tied down to a large flatbed truck." loading="lazy" width="2000" height="1984" srcset="https://blog.yaakov.online/content/images/size/w600/2024/02/C-NknkeUwAAxSQs.jpeg 600w, https://blog.yaakov.online/content/images/size/w1000/2024/02/C-NknkeUwAAxSQs.jpeg 1000w, https://blog.yaakov.online/content/images/size/w1600/2024/02/C-NknkeUwAAxSQs.jpeg 1600w, https://blog.yaakov.online/content/images/2024/02/C-NknkeUwAAxSQs.jpeg 2048w" sizes="(min-width: 720px) 720px"><figcaption><span>"Deployed my blog on Kubernetes." - @dexhorthy, </span><a href="https://twitter.com/dexhorthy/status/856639005462417409?lang=en&amp;ref=blog.yaakov.online"><span>https://twitter.com/dexhorthy/status/856639005462417409?lang=en</span></a></figcaption></figure><p>Whilst most problems I encountered did provide a legitimate learning experience, it turns out that Kubernetes, particularly on a NUC, is not bedroom-friendly. Kubernetes chews through a lot of resources, and <code>while (true)</code> loops tend to chew through a lot of CPU. This made my computers run constantly, run hot, keep the fan running, and made it hotter and noisier and harder to sleep.</p><p>Even in the cloud, this effect gets felt in different ways. My personal experience on Azure Kubernetes Service was that I immediately lose a massive chunk of RAM to their Kubernetes implementation, and it uses about 7-10% idle CPU on worker nodes. Even with single-instance <a href="https://microk8s.io/?ref=blog.yaakov.online" rel="noreferrer">Microk8s</a> on a small VPS I had an idle CPU load hovering around 12% on a 2x vCPU x86_64 box, and <a href="http://k3s.io/?ref=blog.yaakov.online" rel="noreferrer">K3S</a> which is supposed to be leaner is at about 6% constant CPU consumption on a 2x vCPU Ampere A1 machine.</p><p>(No guesses as to which cloud provider that latter one is running on.)</p><p>I even tried running Kubernetes on a Raspberry Pi but I couldn't actually find an implementation that would happily run without kicking up heat/fans and that would actually leave enough CPU behind for my workloads.</p><p>Yet the thing that kept bringing me back was the automation. Particularly with GitOps and <a href="https://www.weave.works/oss/flux/?ref=blog.yaakov.online" rel="noreferrer">Flux</a>, making changes was a breeze. With the container image automation and recent addition of <a href="https://fluxcd.io/flux/guides/webhook-receivers/?ref=blog.yaakov.online" rel="noreferrer">webhooks in Flux v2</a>, all I had to do was push a new container image and within seconds my servers had pulled the new container image and were running the new version of the application in production.</p><p>Every so often I would stick my head back out of Kubernetes and look at the rest of the world and see if there is something else that can do the same container automation and just like Noah's dove, I would come back empty-handed.</p><p>The only solutions I could find were "just recreate the whole container with all of the original command line arguments" as though I have the patience to manage that or remember each flag, or "here is some magic goop that works if you give it full control of <code>docker.sock</code>" which I never really liked the idea of.</p><p>I have even been sorely tempted to build my own thing but surely, <em>surely</em>, there is something out there already that does this, right?</p><p>Well, recently, I came across <a href="https://docs.podman.io/en/latest/markdown/podman-auto-update.1.html?ref=blog.yaakov.online" rel="noreferrer">Podman auto-updating</a>. The simplest way to explain <a href="https://podman.io/?ref=blog.yaakov.online" rel="noreferrer">Podman</a> is an alternative Docker CLI (yes I know that is oversimplified), but it has one particular feature that caught my eye.</p><p>Once you create a container, Podman can automatically <a href="https://docs.podman.io/en/latest/markdown/podman-generate-systemd.1.html?ref=blog.yaakov.online" rel="noreferrer">generate a systemd service</a> file to start and stop that container. Starting the service creates (or replaces) the container, and stopping the service removes the container. So that already takes care of my "manage each original flag" problem.</p><p>But the cherry on top if that if you tag your containers with <code>io.containers.autoupdate</code>, then once a day on a timer or on-demand when you <code>podman auto-update</code>, it will check for a new image and if there is one it will recreate the container for you with the new image.</p><p><a href="https://fedoramagazine.org/auto-updating-podman-containers-with-systemd/?ref=blog.yaakov.online" rel="noreferrer">This article from Fedora Magazine</a> basically gave me 99% of the magic sauce. There were only two more components I needed to make this work:</p><ol><li>Run <code>systemctl --user enable mycontainer.service</code> to make the container start up automatically, whenever I log in.</li><li>Run <code>loginctl enable-linger</code> so that I "log in" when the server starts up.</li></ol><p>These three components - Podman, systemd, and user lingering, now give me 99% of the benefit I was getting from Kubernetes with vastly reduced complexity and none of the CPU/memory hits associated with it.</p><p>I've migrated a full set of services from one VPS to a new one with half the vCPUs and RAM. It's only been running for a handful of hours so far, but I can see that it is running significantly lighter, snappier, and with a lower compute cost to boot, which gives me higher service density and more bang for my buck.</p><p>Of course, as my luck would have it, Podman integration with systemd appears to be deprecated already and they're now talking about defining containers in "Quadlet" files, whatever those are. I guess that will be something to learn some other time.</p>
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Real-time AI Voice Chat at ~500ms Latency (429 pts)]]></title>
            <link>https://github.com/KoljaB/RealtimeVoiceChat</link>
            <guid>43899028</guid>
            <pubDate>Mon, 05 May 2025 20:17:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/KoljaB/RealtimeVoiceChat">https://github.com/KoljaB/RealtimeVoiceChat</a>, See on <a href="https://news.ycombinator.com/item?id=43899028">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Real-Time AI Voice Chat 🎤💬🧠🔊</h2><a id="user-content-real-time-ai-voice-chat-" aria-label="Permalink: Real-Time AI Voice Chat 🎤💬🧠🔊" href="#real-time-ai-voice-chat-"></a></p>
<p dir="auto"><strong>Have a natural, spoken conversation with an AI!</strong></p>
<p dir="auto">This project lets you chat with a Large Language Model (LLM) using just your voice, receiving spoken responses in near real-time. Think of it as your own digital conversation partner.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description FastVoiceTalk_compressed_step3_h264.mp4">FastVoiceTalk_compressed_step3_h264.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/7604638/440153612-16cc29a7-bec2-4dd0-a056-d213db798d8f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDY0ODQ1MDIsIm5iZiI6MTc0NjQ4NDIwMiwicGF0aCI6Ii83NjA0NjM4LzQ0MDE1MzYxMi0xNmNjMjlhNy1iZWMyLTRkZDAtYTA1Ni1kMjEzZGI3OThkOGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MDVUMjIzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTY0YWVlN2NiNDc3ZjZkMDMzZGJlY2QyNGNlZTQzNDM0M2I1ZjJmM2EwOTgxYzVmMzZlYjU2MjdhNzIzNTRlNSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.dbPAgkPJn2NKy48qo_JdokryT9DGts4KHQ767eBEWGs" data-canonical-src="https://private-user-images.githubusercontent.com/7604638/440153612-16cc29a7-bec2-4dd0-a056-d213db798d8f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDY0ODQ1MDIsIm5iZiI6MTc0NjQ4NDIwMiwicGF0aCI6Ii83NjA0NjM4LzQ0MDE1MzYxMi0xNmNjMjlhNy1iZWMyLTRkZDAtYTA1Ni1kMjEzZGI3OThkOGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MDVUMjIzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTY0YWVlN2NiNDc3ZjZkMDMzZGJlY2QyNGNlZTQzNDM0M2I1ZjJmM2EwOTgxYzVmMzZlYjU2MjdhNzIzNTRlNSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.dbPAgkPJn2NKy48qo_JdokryT9DGts4KHQ767eBEWGs" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><em>(early preview - first reasonably stable version)</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What's Under the Hood?</h2><a id="user-content-whats-under-the-hood" aria-label="Permalink: What's Under the Hood?" href="#whats-under-the-hood"></a></p>
<p dir="auto">A sophisticated client-server system built for low-latency interaction:</p>
<ol dir="auto">
<li>🎙️ <strong>Capture:</strong> Your voice is captured by your browser.</li>
<li>➡️ <strong>Stream:</strong> Audio chunks are whisked away via WebSockets to a Python backend.</li>
<li>✍️ <strong>Transcribe:</strong> <code>RealtimeSTT</code> rapidly converts your speech to text.</li>
<li>🤔 <strong>Think:</strong> The text is sent to an LLM (like Ollama or OpenAI) for processing.</li>
<li>🗣️ <strong>Synthesize:</strong> The AI's text response is turned back into speech using <code>RealtimeTTS</code>.</li>
<li>⬅️ <strong>Return:</strong> The generated audio is streamed back to your browser for playback.</li>
<li>🔄 <strong>Interrupt:</strong> Jump in anytime! The system handles interruptions gracefully.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key Features ✨</h2><a id="user-content-key-features-" aria-label="Permalink: Key Features ✨" href="#key-features-"></a></p>
<ul dir="auto">
<li><strong>Fluid Conversation:</strong> Speak and listen, just like a real chat.</li>
<li><strong>Real-Time Feedback:</strong> See partial transcriptions and AI responses as they happen.</li>
<li><strong>Low Latency Focus:</strong> Optimized architecture using audio chunk streaming.</li>
<li><strong>Smart Turn-Taking:</strong> Dynamic silence detection (<code>turndetect.py</code>) adapts to the conversation pace.</li>
<li><strong>Flexible AI Brains:</strong> Pluggable LLM backends (Ollama default, OpenAI support via <code>llm_module.py</code>).</li>
<li><strong>Customizable Voices:</strong> Choose from different Text-to-Speech engines (Kokoro, Coqui, Orpheus via <code>audio_module.py</code>).</li>
<li><strong>Web Interface:</strong> Clean and simple UI using Vanilla JS and the Web Audio API.</li>
<li><strong>Dockerized Deployment:</strong> Recommended setup using Docker Compose for easier dependency management.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technology Stack 🛠️</h2><a id="user-content-technology-stack-️" aria-label="Permalink: Technology Stack 🛠️" href="#technology-stack-️"></a></p>
<ul dir="auto">
<li><strong>Backend:</strong> Python 3.x, FastAPI</li>
<li><strong>Frontend:</strong> HTML, CSS, JavaScript (Vanilla JS, Web Audio API, AudioWorklets)</li>
<li><strong>Communication:</strong> WebSockets</li>
<li><strong>Containerization:</strong> Docker, Docker Compose</li>
<li><strong>Core AI/ML Libraries:</strong>
<ul dir="auto">
<li><code>RealtimeSTT</code> (Speech-to-Text)</li>
<li><code>RealtimeTTS</code> (Text-to-Speech)</li>
<li><code>transformers</code> (Turn detection, Tokenization)</li>
<li><code>torch</code> / <code>torchaudio</code> (ML Framework)</li>
<li><code>ollama</code> / <code>openai</code> (LLM Clients)</li>
</ul>
</li>
<li><strong>Audio Processing:</strong> <code>numpy</code>, <code>scipy</code></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Before You Dive In: Prerequisites 🏊‍♀️</h2><a id="user-content-before-you-dive-in-prerequisites-️" aria-label="Permalink: Before You Dive In: Prerequisites 🏊‍♀️" href="#before-you-dive-in-prerequisites-️"></a></p>
<p dir="auto">This project leverages powerful AI models, which have some requirements:</p>
<ul dir="auto">
<li><strong>Operating System:</strong>
<ul dir="auto">
<li><strong>Docker:</strong> Linux is recommended for the best GPU integration with Docker.</li>
<li><strong>Manual:</strong> The provided script (<code>install.bat</code>) is for Windows. Manual steps are possible on Linux/macOS but may require more troubleshooting (especially for DeepSpeed).</li>
</ul>
</li>
<li><strong>🐍 Python:</strong> 3.9 or higher (if setting up manually).</li>
<li><strong>🚀 GPU:</strong> <strong>A powerful CUDA-enabled NVIDIA GPU is <em>highly recommended</em></strong>, especially for faster STT (Whisper) and TTS (Coqui). Performance on CPU-only or weaker GPUs will be significantly slower.
<ul dir="auto">
<li>The setup assumes <strong>CUDA 12.1</strong>. Adjust PyTorch installation if you have a different CUDA version.</li>
<li><strong>Docker (Linux):</strong> Requires <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" rel="nofollow">NVIDIA Container Toolkit</a>.</li>
</ul>
</li>
<li><strong>🐳 Docker (Optional but Recommended):</strong> Docker Engine and Docker Compose v2+ for the containerized setup.</li>
<li><strong>🧠 Ollama (Optional):</strong> If using the Ollama backend <em>without</em> Docker, install it separately and pull your desired models. The Docker setup includes an Ollama service.</li>
<li><strong>🔑 OpenAI API Key (Optional):</strong> If using the OpenAI backend, set the <code>OPENAI_API_KEY</code> environment variable (e.g., in a <code>.env</code> file or passed to Docker).</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started: Installation &amp; Setup ⚙️</h2><a id="user-content-getting-started-installation--setup-️" aria-label="Permalink: Getting Started: Installation &amp; Setup ⚙️" href="#getting-started-installation--setup-️"></a></p>
<p dir="auto"><strong>Clone the repository first:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/KoljaB/RealtimeVoiceChat.git
cd RealtimeVoiceChat"><pre>git clone https://github.com/KoljaB/RealtimeVoiceChat.git
<span>cd</span> RealtimeVoiceChat</pre></div>
<p dir="auto">Now, choose your adventure:</p>
<details>
<summary><strong>🚀 Option A: Docker Installation (Recommended for Linux/GPU)</strong></summary>
<p dir="auto">This is the most straightforward method, bundling the application, dependencies, and even Ollama into manageable containers.</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Build the Docker images:</strong>
<em>(This takes time! It downloads base images, installs Python/ML dependencies, and pre-downloads the default STT model.)</em></p>

<p dir="auto"><em>(If you want to customize models/settings in <code>code/*.py</code>, do it <strong>before</strong> this step!)</em></p>
</li>
<li>
<p dir="auto"><strong>Start the services (App &amp; Ollama):</strong>
<em>(Runs containers in the background. GPU access is configured in <code>docker-compose.yml</code>.)</em></p>

<p dir="auto">Give them a minute to initialize.</p>
</li>
<li>
<p dir="auto"><strong>(Crucial!) Pull your desired Ollama Model:</strong>
<em>(This is done <em>after</em> startup to keep the main app image smaller and allow model changes without rebuilding. Execute this command to pull the default model into the running Ollama container.)</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Pull the default model (adjust if you configured a different one in server.py)
docker compose exec ollama ollama pull hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M

# (Optional) Verify the model is available
docker compose exec ollama ollama list"><pre><span><span>#</span> Pull the default model (adjust if you configured a different one in server.py)</span>
docker compose <span>exec</span> ollama ollama pull hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M

<span><span>#</span> (Optional) Verify the model is available</span>
docker compose <span>exec</span> ollama ollama list</pre></div>
</li>
<li>
<p dir="auto"><strong>Stopping the Services:</strong></p>

</li>
<li>
<p dir="auto"><strong>Restarting:</strong></p>

</li>
<li>
<p dir="auto"><strong>Viewing Logs / Debugging:</strong></p>
<ul dir="auto">
<li>Follow app logs: <code>docker compose logs -f app</code></li>
<li>Follow Ollama logs: <code>docker compose logs -f ollama</code></li>
<li>Save logs to file: <code>docker compose logs app &gt; app_logs.txt</code></li>
</ul>
</li>
</ol>
</details>
<details>
<summary><strong>🛠️ Option B: Manual Installation (Windows Script / venv)</strong></summary>
<p dir="auto">This method requires managing the Python environment yourself. It offers more direct control but can be trickier, especially regarding ML dependencies.</p>
<p dir="auto"><strong>B1) Using the Windows Install Script:</strong></p>
<ol dir="auto">
<li>Ensure you meet the prerequisites (Python, potentially CUDA drivers).</li>
<li>Run the script. It attempts to create a venv, install PyTorch for CUDA 12.1, a compatible DeepSpeed wheel, and other requirements.

<em>(This opens a new command prompt within the activated virtual environment.)</em>
Proceed to the <strong>"Running the Application"</strong> section.</li>
</ol>
<p dir="auto"><strong>B2) Manual Steps (Linux/macOS/Windows):</strong></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Create &amp; Activate Virtual Environment:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m venv venv
# Linux/macOS:
source venv/bin/activate
# Windows:
.\venv\Scripts\activate"><pre>python -m venv venv
<span><span>#</span> Linux/macOS:</span>
<span>source</span> venv/bin/activate
<span><span>#</span> Windows:</span>
.<span>\v</span>env<span>\S</span>cripts<span>\a</span>ctivate</pre></div>
</li>
<li>
<p dir="auto"><strong>Upgrade Pip:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m pip install --upgrade pip"><pre>python -m pip install --upgrade pip</pre></div>
</li>
<li>
<p dir="auto"><strong>Navigate to Code Directory:</strong></p>

</li>
<li>
<p dir="auto"><strong>Install PyTorch (Crucial Step - Match Your Hardware!):</strong></p>
<ul dir="auto">
<li><strong>With NVIDIA GPU (CUDA 12.1 Example):</strong>
<div dir="auto" data-snippet-clipboard-copy-content="# Verify your CUDA version! Adjust 'cu121' and the URL if needed.
pip install torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121"><pre><span><span>#</span> Verify your CUDA version! Adjust 'cu121' and the URL if needed.</span>
pip install torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121</pre></div>
</li>
<li><strong>CPU Only (Expect Slow Performance):</strong>
<div dir="auto" data-snippet-clipboard-copy-content="# pip install torch torchaudio torchvision"><pre><span><span>#</span> pip install torch torchaudio torchvision</span></pre></div>
</li>
<li><em>Find other PyTorch versions:</em> <a href="https://pytorch.org/get-started/previous-versions/" rel="nofollow">https://pytorch.org/get-started/previous-versions/</a></li>
</ul>
</li>
<li>
<p dir="auto"><strong>Install Other Requirements:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<ul dir="auto">
<li><strong>Note on DeepSpeed:</strong> The <code>requirements.txt</code> may include DeepSpeed. Installation can be complex, especially on Windows. The <code>install.bat</code> tries a precompiled wheel. If manual installation fails, you might need to build it from source or consult resources like <a href="https://github.com/erew123/deepspeedpatcher">deepspeedpatcher</a> (use at your own risk). Coqui TTS performance benefits most from DeepSpeed.</li>
</ul>
</li>
</ol>
</details>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running the Application <g-emoji alias="arrow_forward">▶️</g-emoji></h2><a id="user-content-running-the-application-️" aria-label="Permalink: Running the Application ▶️" href="#running-the-application-️"></a></p>
<p dir="auto"><strong>If using Docker:</strong>
Your application is already running via <code>docker compose up -d</code>! Check logs using <code>docker compose logs -f app</code>.</p>
<p dir="auto"><strong>If using Manual/Script Installation:</strong></p>
<ol dir="auto">
<li><strong>Activate your virtual environment</strong> (if not already active):
<div dir="auto" data-snippet-clipboard-copy-content="# Linux/macOS: source ../venv/bin/activate
# Windows: ..\venv\Scripts\activate"><pre><span><span>#</span> Linux/macOS: source ../venv/bin/activate</span>
<span><span>#</span> Windows: ..\venv\Scripts\activate</span></pre></div>
</li>
<li><strong>Navigate to the <code>code</code> directory</strong> (if not already there):

</li>
<li><strong>Start the FastAPI server:</strong>

</li>
</ol>
<p dir="auto"><strong>Accessing the Client (Both Methods):</strong></p>
<ol dir="auto">
<li>Open your web browser to <code>http://localhost:8000</code> (or your server's IP if running remotely/in Docker on another machine).</li>
<li><strong>Grant microphone permissions</strong> when prompted.</li>
<li>Click <strong>"Start"</strong> to begin chatting! Use "Stop" to end and "Reset" to clear the conversation.</li>
</ol>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration Deep Dive 🔧</h2><a id="user-content-configuration-deep-dive-" aria-label="Permalink: Configuration Deep Dive 🔧" href="#configuration-deep-dive-"></a></p>
<p dir="auto">Want to tweak the AI's voice, brain, or how it listens? Modify the Python files in the <code>code/</code> directory.</p>
<p dir="auto"><strong><g-emoji alias="warning">⚠️</g-emoji> Important Docker Note:</strong> If using Docker, make any configuration changes <em>before</em> running <code>docker compose build</code> to ensure they are included in the image.</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>TTS Engine &amp; Voice (<code>server.py</code>, <code>audio_module.py</code>):</strong></p>
<ul dir="auto">
<li>Change <code>START_ENGINE</code> in <code>server.py</code> to <code>"coqui"</code>, <code>"kokoro"</code>, or <code>"orpheus"</code>.</li>
<li>Adjust engine-specific settings (e.g., voice model path for Coqui, speaker ID for Orpheus, speed) within <code>AudioProcessor.__init__</code> in <code>audio_module.py</code>.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>LLM Backend &amp; Model (<code>server.py</code>, <code>llm_module.py</code>):</strong></p>
<ul dir="auto">
<li>Set <code>LLM_START_PROVIDER</code> (<code>"ollama"</code> or <code>"openai"</code>) and <code>LLM_START_MODEL</code> (e.g., <code>"hf.co/..."</code> for Ollama, model name for OpenAI) in <code>server.py</code>. Remember to pull the Ollama model if using Docker (see Installation Step A3).</li>
<li>Customize the AI's personality by editing <code>system_prompt.txt</code>.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>STT Settings (<code>transcribe.py</code>):</strong></p>
<ul dir="auto">
<li>Modify <code>DEFAULT_RECORDER_CONFIG</code> to change the Whisper model (<code>model</code>), language (<code>language</code>), silence thresholds (<code>silence_limit_seconds</code>), etc. The default <code>base.en</code> model is pre-downloaded during the Docker build.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Turn Detection Sensitivity (<code>turndetect.py</code>):</strong></p>
<ul dir="auto">
<li>Adjust pause duration constants within the <code>TurnDetector.update_settings</code> method.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>SSL/HTTPS (<code>server.py</code>):</strong></p>
<ul dir="auto">
<li>Set <code>USE_SSL = True</code> and provide paths to your certificate (<code>SSL_CERT_PATH</code>) and key (<code>SSL_KEY_PATH</code>) files.</li>
<li><strong>Docker Users:</strong> You'll need to adjust <code>docker-compose.yml</code> to map the SSL port (e.g., 443) and potentially mount your certificate files as volumes.</li>
</ul>
<details>
<summary><strong>Generating Local SSL Certificates (Windows Example w/ mkcert)</strong></summary>
<ol dir="auto">
<li>Install Chocolatey package manager if you haven't already.</li>
<li>Install mkcert: <code>choco install mkcert</code></li>
<li>Run Command Prompt <em>as Administrator</em>.</li>
<li>Install a local Certificate Authority: <code>mkcert -install</code></li>
<li>Generate certs (replace <code>your.local.ip</code>): <code>mkcert localhost 127.0.0.1 ::1 your.local.ip</code>
<ul dir="auto">
<li>This creates <code>.pem</code> files (e.g., <code>localhost+3.pem</code> and <code>localhost+3-key.pem</code>) in the current directory. Update <code>SSL_CERT_PATH</code> and <code>SSL_KEY_PATH</code> in <code>server.py</code> accordingly. Remember to potentially mount these into your Docker container.</li>
</ul>
</li>
</ol>
</details>
</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing 🤝</h2><a id="user-content-contributing-" aria-label="Permalink: Contributing 🤝" href="#contributing-"></a></p>
<p dir="auto">Got ideas or found a bug? Contributions are welcome! Feel free to open issues or submit pull requests.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License 📜</h2><a id="user-content-license-" aria-label="Permalink: License 📜" href="#license-"></a></p>
<p dir="auto">The core codebase of this project is released under the <strong>MIT License</strong> (see the <a href="https://github.com/KoljaB/RealtimeVoiceChat/blob/main/LICENSE">LICENSE</a> file for details).</p>
<p dir="auto">This project relies on external specific TTS engines (like <code>Coqui XTTSv2</code>) and LLM providers which have their <strong>own licensing terms</strong>. Please ensure you comply with the licenses of all components you use.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Databricks in talks to acquire startup Neon for about $1B (182 pts)]]></title>
            <link>https://www.upstartsmedia.com/p/scoop-databricks-talks-to-acquire-neon</link>
            <guid>43899016</guid>
            <pubDate>Mon, 05 May 2025 20:16:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.upstartsmedia.com/p/scoop-databricks-talks-to-acquire-neon">https://www.upstartsmedia.com/p/scoop-databricks-talks-to-acquire-neon</a>, See on <a href="https://news.ycombinator.com/item?id=43899016">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg" width="1456" height="972" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:972,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:5707007,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.upstartsmedia.com/i/162915455?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9928bd89-81f5-42f4-bb84-ae87628c8122_5193x3466.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Databricks is busy buying up a bunch of startups to join its San Francisco office. CREDIT: Smith Collection/Gado/Getty Images</figcaption></figure></div><p>Data and AI unicorn Databricks is in talks to make a splash with another startup acquisition, Upstarts has learned.</p><p>Databricks is in advanced talks to acquire startup Neon, a maker of an open-source database engine, four sources tell Upstarts exclusively. Databricks is expected to pay in the ballpark of $1 billion for Neon, two of the sources say.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.upstartsmedia.com/subscribe?coupon=e8236a37&amp;utm_content=162915455&quot;,&quot;text&quot;:&quot;Get 10% off for 1 year&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.upstartsmedia.com/subscribe?coupon=e8236a37&amp;utm_content=162915455" rel=""><span>Get 10% off for 1 year</span></a></p><p>But despite some industry insiders describing the deal as if it’s done, the talks remain ongoing and could still fall through, several sources add. The total amount could also still exceed $1 billion when employee retention packages are factored in.</p><p>Neon and CEO Nikita Shamgunov did not respond to a comment request. Databricks declined to comment through a spokesperson.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Possibly a Serious Possibility (234 pts)]]></title>
            <link>https://kucharski.substack.com/p/possibly-a-serious-possibility</link>
            <guid>43898380</guid>
            <pubDate>Mon, 05 May 2025 19:11:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kucharski.substack.com/p/possibly-a-serious-possibility">https://kucharski.substack.com/p/possibly-a-serious-possibility</a>, See on <a href="https://news.ycombinator.com/item?id=43898380">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Sherman Kent was rattled. It was March 1951 and Kent, a CIA analyst, had found himself in a troubling conversation about some recent intelligence. A few days earlier, Kent's team had </span><a href="https://www.cia.gov/resources/csi/studies-in-intelligence/archives/vol-8-no-4/words-of-estimative-probability/" rel="">released a report</a><span> titled ‘Probability of an Invasion of Yugoslavia in 1951’, which concluded that Soviet aggression against Yugoslavia ‘should be considered a serious possibility’.</span></p><p>Kent thought the phrase was clear. But when he ran into the chairman of the Policy Planning Staff, he realised his message hadn’t landed as intended.</p><p>‘By the way, what did you mean by “serious possibility”?’ the chairman asked.</p><p>‘I told him that my personal estimate was on the dark side, namely that the odds were around 65 to 35 in favor of an attack’, Kent would later recall.</p><p>The chairman was taken aback. He—and others—had read the phrase as meaning something much less likely.</p><p>Things got worse when Kent talked to his colleagues on the Board of National Estimates. Some interpreted the phrase as meaning a probability as high as 80%, others as low as 20%. This was the 29th such report that they’d produced. ‘Had Board members been seeming to agree on five months' worth of estimative judgments with no real agreement at all?’ Kent wondered.</p><p>The latest report, intended to sound an alarm, had instead created confusion.</p><p>This wasn’t just a one-off problem. It was a structural flaw in how intelligence was being communicated at the time. Reflecting on the experience, Kent categorised intelligence assessments into three categories:</p><ol><li><p><strong>Near-indisputable facts</strong><span>, like the length of a runway visible in a satellite image.</span></p></li><li><p><strong>Judgement or estimate of something knowable</strong><span>, like whether the runway belongs to a military airfield.</span></p></li><li><p><strong>Judgement or estimate of something unknowable</strong><span>, like whether the airfield will be expanded into a strategic base. Even the adversary may not yet know what they’re going to do with it.</span></p></li></ol><p>Kent noted that most intelligence work tends to sit in the second and third categories, where uncertainty dominates. But Kent realised that even among professionals, the language of such uncertainty was wildly inconsistent. Photo interpreters would use ‘possible’ where he would use ‘probable’. And they used ‘probable’ where Kent would say ‘almost certain’. </p><p><span>Decades later, a </span><a href="https://digitalcommons.usf.edu/jss/vol10/iss1/11/" rel="">NATO study</a><span> would find something similar: ask 23 officers what ‘likely’ means in terms of probability, and you’ll get a dozen different numbers. Modern online surveys show people haven’t improved much, as shown in the below data:</span></p><p>Kent suggested that the toughest intelligence challenge is the ‘difficult but not impossible estimate’. Unfortunately, this was also the kind of judgment people tried hardest to avoid. As Kent put it: ‘What we consciously or subconsciously seek is an expression which conveys a definite meaning but at the same time either absolves us completely of the responsibility or makes the estimate at enough removes from ourselves as not to implicate us’.</p><p><span>The problem reaches beyond the intelligence world. Legal scholars </span><a href="https://scholarship.richmond.edu/law-faculty-publications/193/" rel="">have found</a><span> that courts also rely on ambiguous phrases for ambiguous evidence, with everything-to-everyone phrases like ‘non-whimsical suspicion’ or ‘clear indication’. To Kent, these kinds of ‘lurking weasel’ phrases were simply a way to avoid decisions: language that sounds authoritative but dodges responsibility. ‘Let the judgment be unmistakable and let it be unmistakably ours,’ as Kent put it.</span></p><p><span>Since then, some governments have tried to clean up the language of probability. After the Iraq War—which was influenced by misinterpreted intelligence—the </span><a href="https://osf.io/preprints/psyarxiv/kuyhb_v1" rel="">UK introduced</a><span> a ‘probability yardstick’ for intelligence assessments, standardizing terms like ‘unlikely’, ‘highly likely’, and ‘probable’ with numerical definitions:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png" width="1456" height="325" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8d118724-fddf-4308-9998-d603b5989a96_1480x330.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:325,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d118724-fddf-4308-9998-d603b5989a96_1480x330.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Such standarisation matters, because in a world where uncertainty is unavoidable, the risk isn’t just that we’re wrong—it’s that we’re misunderstood.</p><p><em><strong><span>If you’re interested in reading more about analysing and communicating uncertainty, you’ll probably like my new book </span><a href="https://proof.kucharski.io/" rel="">Proof: The Uncertain Science of Certainty</a><span>, which is available now. It was </span><a href="https://www.nytimes.com/2025/04/30/books/review/proof-adam-kucharski.html" rel="">reviewed in The New York Times</a><span> this week if you’d like to know more.</span></strong></em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Evolving OpenAI's Structure (550 pts)]]></title>
            <link>https://openai.com/index/evolving-our-structure/</link>
            <guid>43897772</guid>
            <pubDate>Mon, 05 May 2025 18:08:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/evolving-our-structure/">https://openai.com/index/evolving-our-structure/</a>, See on <a href="https://news.ycombinator.com/item?id=43897772">Hacker News</a></p>
Couldn't get https://openai.com/index/evolving-our-structure/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[As an experienced LLM user, I don't use generative LLMs often (346 pts)]]></title>
            <link>https://minimaxir.com/2025/05/llm-use/</link>
            <guid>43897320</guid>
            <pubDate>Mon, 05 May 2025 17:22:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://minimaxir.com/2025/05/llm-use/">https://minimaxir.com/2025/05/llm-use/</a>, See on <a href="https://news.ycombinator.com/item?id=43897320">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Lately, I’ve been working on codifying a personal ethics statement about my stances on generative AI as I have been very critical about <a href="https://minimaxir.com/2023/10/ai-sturgeons-law/">several</a> <a href="https://minimaxir.com/2024/08/ai-seinfeld/">aspects</a> of modern GenAI, and yet <a href="https://thenib.com/mister-gotcha/">I participate in it</a>. While working on that statement, I’ve been introspecting on how I myself have been utilizing large language models for both my professional work as a Senior Data Scientist at <a href="https://www.buzzfeed.com/">BuzzFeed</a> and for my personal work blogging and <a href="https://github.com/minimaxir">writing open-source software</a>. For about a decade, I’ve been researching and developing tooling around <a href="https://minimaxir.com/2017/04/char-embeddings/">text generation from char-rnns</a>, to the <a href="https://minimaxir.com/2019/09/howto-gpt2/">ability to fine-tune GPT-2</a>, to <a href="https://minimaxir.com/2020/07/gpt3-expectations/">experiments with GPT-3</a>, and <a href="https://minimaxir.com/2023/03/new-chatgpt-overlord/">even more experiments with ChatGPT</a> and other LLM APIs. Although I don’t claim to the best user of modern LLMs out there, I’ve had plenty of experience working against the cons of next-token predictor models and have become very good at finding the pros.</p><p>It turns out, to my surprise, that I don’t use them nearly as often as people think engineers do, but that doesn’t mean LLMs are useless for me. It’s a discussion that requires case-by-case nuance.</p><h2 id="how-i-interface-with-llms">How I Interface With LLMs</h2><p>Over the years I’ve utilized all the tricks to get the best results out of LLMs. The most famous trick is <a href="https://en.wikipedia.org/wiki/Prompt_engineering">prompt engineering</a>, or the art of phrasing the prompt in a specific manner to coach the model to generate a specific constrained output. Additions to prompts such as <a href="https://minimaxir.com/2024/02/chatgpt-tips-analysis/">offering financial incentives to the LLM</a> or simply <a href="https://minimaxir.com/2025/01/write-better-code/">telling the LLM to make their output better</a> do indeed have a quantifiable positive impact on both improving adherence to the original prompt and the output text quality. Whenever my coworkers ask me why their LLM output is not what they expected, I suggest that they apply more prompt engineering and it almost always fixes their issues.</p><p><strong>No one in the AI field is happy about prompt engineering</strong>, especially myself. Attempts to remove the need for prompt engineering with more robust <a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">RLHF</a> paradigms have only made it <em>even more rewarding</em> by allowing LLM developers to make use of better prompt adherence. True, “Prompt Engineer” as a job title <a href="https://www.wsj.com/articles/the-hottest-ai-job-of-2023-is-already-obsolete-1961b054?st=DMVDgm&amp;reflink=desktopwebshare_permalink">turned out to be a meme</a> but that’s mostly because prompt engineering is now an expected skill for anyone seriously using LLMs. Prompt engineering works, and part of being a professional is using what works even if it’s silly.</p><p>To that end, <strong>I never use ChatGPT.com</strong> or other normal-person frontends for accessing LLMs because they are harder to control. Instead, I typically access the backend UIs provided by each LLM service, which serve as a light wrapper over the API functionality which also makes it easy to port to code if necessary. Accessing LLM APIs like the ChatGPT API directly allow you to set <a href="https://promptengineering.org/system-prompts-in-large-language-models/">system prompts</a> which control the “rules” for the generation that can be very nuanced. Specifying specific constraints for the generated text such as “keep it to no more than 30 words” or “never use the word ‘delve’” tends to be more effective in the system prompt than putting them in the user prompt as you would with ChatGPT.com. Any modern LLM interface that does not let you explicitly set a system prompt is most likely <a href="https://docs.anthropic.com/en/release-notes/system-prompts">using their own system prompt</a> which you can’t control: for example, when ChatGPT.com had an issue where it was <a href="https://openai.com/index/sycophancy-in-gpt-4o/">too sycophantic</a> to its users, OpenAI <a href="https://simonwillison.net/2025/Apr/29/chatgpt-sycophancy-prompt/">changed the system prompt</a> to command ChatGPT to “avoid ungrounded or sycophantic flattery.” I tend to use <a href="https://www.anthropic.com/">Anthropic</a> Claude’s API — Claude Sonnet in particular — more than any ChatGPT variant because Claude anecdotally is less “robotic” and also handles coding questions much more accurately.</p><p>Additionally with the APIs, you can control the “<a href="https://www.hopsworks.ai/dictionary/llm-temperature">temperature</a>” of the generation, which at a high level controls the creativity of the generation. LLMs by default do not select the next token with the highest probability in order to allow it to give different outputs for each generation, so I prefer to set the temperature to <code>0.0</code> so that the output is mostly deterministic, or <code>0.2 - 0.3</code> if some light variance is required. Modern LLMs now use a default temperature of <code>1.0</code>, and I theorize that higher value is accentuating <a href="https://en.wikipedia.org/wiki/Hallucination_%28artificial_intelligence%29">LLM hallucination</a> issues where the text outputs are internally consistent but factually wrong.</p><h2 id="llms-for-professional-problem-solving">LLMs for Professional Problem Solving!</h2><p>With that pretext, I can now talk about how I have used generative LLMs over the past couple years at BuzzFeed. Here are outlines of some (out of many) projects I’ve worked on using LLMs to successfully solve problems quickly:</p><ul><li>BuzzFeed site curators developed a new <a href="https://www.siteguru.co/seo-academy/website-taxonomy">hierarchal taxonomy</a> to organize thousands of articles into a specified category and subcategory. Since we had no existing labeled articles to train a traditional <a href="https://scikit-learn.org/stable/modules/multiclass.html">multiclass classification</a> model to predict these new labels, I wrote a script to hit the Claude Sonnet API with a system prompt saying <code>The following is a taxonomy: return the category and subcategory that best matches the article the user provides.</code> plus the JSON-formatted hierarchical taxonomy, then I provided the article metadata as the user prompt, all with a temperature of <code>0.0</code> for the most precise results. Running this in a loop for all the articles resulted in appropriate labels.</li><li>After identifying hundreds of distinct semantic clusters of BuzzFeed articles using data science shenanigans, it became clear that there wasn’t an easy way to give each one unique labels. I wrote another script to hit the Claude Sonnet API with a system prompt saying <code>Return a JSON-formatted title and description that applies to all the articles the user provides.</code> with the user prompt containing five articles from that cluster: again, running the script in a loop for all clusters provided excellent results.</li><li>One BuzzFeed writer asked if there was a way to use a LLM to sanity-check grammar questions such as “should I use an <a href="https://www.merriam-webster.com/grammar/em-dash-en-dash-how-to-use">em dash</a> here?” against the <a href="https://www.buzzfeed.com/buzzfeednews/buzzfeed-style-guide">BuzzFeed style guide</a>. Once again I hit the Claude Sonnet API, this time copy/pasting the <em>full</em> style guide in the system prompt plus a command to <code>Reference the provided style guide to answer the user's question, and cite the exact rules used to answer the question.</code> In testing, the citations were accurate and present in the source input, and the reasonings were consistent.</li></ul><p>Each of these projects were off-hand ideas pitched in a morning standup or a Slack DM, and yet each project only took an hour or two to complete a proof of concept (including testing) and hand off to the relevant stakeholders for evaluation. For projects such as the hierarchal labeling, without LLMs I would have needed to do more sophisticated R&amp;D and likely would have taken days including building training datasets through manual labeling, which is not intellectually gratifying. Here, LLMs did indeed follow the <a href="https://en.wikipedia.org/wiki/Pareto_principle">Pareto principle</a> and got me 80% of the way to a working solution, but the remaining 20% of the work iterating, testing and gathering feedback took longer. Even after the model outputs became more reliable, LLM hallucination was still a concern which is why I also advocate to my coworkers to use caution and double-check with a human if the LLM output is peculiar.</p><p>There’s also one use case of LLMs that doesn’t involve text generation that’s as useful in my professional work: <a href="https://platform.openai.com/docs/guides/embeddings">text embeddings</a>. Modern text embedding models technically are LLMs, except instead of having a head which outputs the logits for the next token, it outputs a vector of numbers that uniquely identify the input text in a higher-dimensional space. All improvements to LLMs that the ChatGPT revolution inspired, such as longer context windows and better quality training regimens, also apply to these text embedding models and caused them to improve drastically over time with models such as <a href="https://www.nomic.ai/blog/posts/nomic-embed-text-v1">nomic-embed-text</a> and <a href="https://huggingface.co/Alibaba-NLP/gte-modernbert-base">gte-modernbert-base</a>. Text embeddings have done a lot at BuzzFeed from identifying similar articles to building recommendation models, but this blog post is about generative LLMs so I’ll save those use cases for another time.</p><h2 id="llms-for-writing">LLMs for Writing?</h2><p>No, I don’t use LLMs for writing the text on this very blog, which I suspect has now become a default assumption for people reading an article written by an experienced LLM user. My blog is far too weird for an LLM to properly emulate. My writing style is blunt, irreverent, and occasionally cringe: even with prompt engineering plus <a href="https://www.promptingguide.ai/techniques/fewshot">few-shot prompting</a> by giving it examples of my existing blog posts and telling the model to follow the same literary style precisely, LLMs output something closer to Marvel movie dialogue. But even if LLMs <em>could</em> write articles in my voice I still wouldn’t use them due of the ethics of misrepresenting authorship by having the majority of the work not be my own words. Additionally, I tend to write about very recent events in the tech/coding world that would not be strongly represented in the training data of a LLM if at all, which increases the likelihood of hallucination.</p><p>There is one silly technique I discovered to allow a LLM to improve my writing without having it do <em>my writing</em>: feed it the text of my mostly-complete blog post, and ask the LLM to pretend to be a cynical <a href="https://news.ycombinator.com/news">Hacker News</a> commenter and write five distinct comments based on the blog post. This not only identifies weaker arguments for potential criticism, but it also doesn’t tell me what I <em>should</em> write in the post to preemptively address that negative feedback so I have to solve it organically. When running a rough draft of this very blog post and the Hacker News system prompt through the Claude API (<a href="https://github.com/minimaxir/llm-use/blob/main/criticism_hn.md">chat log</a>), it noted that my examples of LLM use at BuzzFeed are too simple and not anything more innovative than traditional <a href="https://aws.amazon.com/what-is/nlp/">natural language processing</a> techniques, so I made edits elaborating how NLP would not be as efficient or effective.</p><h2 id="llms-for-companionship">LLMs for Companionship?</h2><p>No, I don’t use LLMs as friendly chatbots either. The runaway success of LLM personal companion startups such as <a href="https://character.ai/">character.ai</a> and <a href="https://replika.com/">Replika</a> are alone enough evidence that LLMs have a use, even if the use is just entertainment/therapy and not more utilitarian.</p><p>I admit that I am an outlier since treating LLMs as a friend is the most common use case. Myself being an introvert aside, it’s hard to be friends with an entity who is trained to be as friendly as possible but also habitually lies due to hallucination. I <em>could</em> prompt engineer an LLM to call me out on my bullshit instead of just giving me positive affirmations, but there’s no fix for the lying.</p><h2 id="llms-for-coding">LLMs for Coding???</h2><p>Yes, I use LLMs for coding, but only when I am reasonably confident that they’ll increase my productivity. Ever since the dawn of the original ChatGPT, I’ve asked LLMs to help me write <a href="https://en.wikipedia.org/wiki/Regular_expression">regular expressions</a> since that alone saves me hours, embarrassing to admit. However, the role of LLMs in coding has expanded far beyond that nowadays, and coding is even more nuanced and more controversial on how you can best utilize LLM assistance.</p><p>Like most coders, I Googled coding questions and clicked on the first <a href="https://stackoverflow.com/">Stack Overflow</a> result that seemed relevant, until I decided to start asking Claude Sonnet the same coding questions and getting much more detailed and bespoke results. This was more pronounced for questions which required specific functional constraints and software frameworks, the combinations of which would likely not be present in a Stack Overflow answer. One paraphrased example I recently asked Claude Sonnet while writing <a href="https://minimaxir.com/2025/02/embeddings-parquet/">another blog post</a> is <code>Write Python code using the Pillow library to composite five images into a single image: the left half consists of one image, the right half consists of the remaining four images.</code> (<a href="https://github.com/minimaxir/llm-use/blob/main/pil_composition.md">chat log</a>). Compositing multiple images with <a href="https://pypi.org/project/pillow/">Pillow</a> isn’t too difficult and there’s enough <a href="https://stackoverflow.com/questions/3374878/with-the-python-imaging-library-pil-how-does-one-compose-an-image-with-an-alp">questions/solutions about it on Stack Overflow</a>, but the specific way it’s composited is unique and requires some positioning shenanigans that I would likely mess up on the first try. But Claude Sonnet’s code <a href="https://github.com/minimaxir/mtg-embeddings/blob/main/mtg_related_card_img.ipynb">got it mostly correct</a> and it was easy to test, which saved me time doing unfun debugging.</p><p>However, for more complex code questions particularly around less popular libraries which have fewer code examples scraped from Stack Overflow and <a href="https://github.com/">GitHub</a>, I am more cautious of the LLM’s outputs. One real-world issue I’ve had is that I need a way to log detailed metrics to a database while training models — for which I use the <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer">Trainer class</a> in <a href="https://huggingface.co/docs/transformers/en/index">Hugging Face transformers</a> — so that I can visualize and analyze it later. I asked Claude Sonnet to <code>Write a Callback class in Python for the Trainer class in the Hugging Face transformers Python library such that it logs model training metadata for each step to a local SQLite database, such as current epoch, time for step, step loss, etc.</code> (<a href="https://github.com/minimaxir/llm-use/blob/main/hf_trainer_logger_sqlite.md">chat log</a>). This one I was less optimistic about since there isn’t much code about creating custom callbacks, however the Claude-generated code implemented some helpful ideas that weren’t on the top-of-my-mind when I asked, such a buffer to limit blocking I/O, SQLite config speedups, batch inserts, and connection handling. Asking Claude to “make the code better” twice (why not?) results in a few more unexpected ideas such as SQLite connection caching and using a single column with the JSON column type to store an arbitrary number of metrics, in addition to making the code much more Pythonic. It is still a lot of code such that it’s unlikely to work out-of-the-box without testing in the full context of an actual training loop. However, even if the code has flaws, the ideas themselves are extremely useful and in this case it would be much faster and likely higher quality code overall to hack on this generated code instead of writing my own SQLite logger from scratch.</p><p>For actual data science in my day-to-day work that I spend most of my time, I’ve found that code generation from LLMs is less useful. LLMs cannot output the text result of mathematical operations reliably, with some APIs working around that by <a href="https://platform.openai.com/docs/assistants/tools/code-interpreter">allowing for a code interpreter</a> to perform data ETL and analysis, but given the scale of data I typically work with it’s not cost-feasible to do that type of workflow. Although <a href="https://pandas.pydata.org/">pandas</a> is the standard for manipulating tabular data in Python and has been around since 2008, I’ve been using the relatively new <a href="https://pola.rs/">polars</a> library exclusively, and I’ve noticed that LLMs tend to hallucinate polars functions as if they were pandas functions which requires documentation deep dives to confirm which became annoying. For data visualization, which I don’t use Python at all and instead use <a href="https://www.r-project.org/">R</a> and <a href="https://ggplot2.tidyverse.org/">ggplot2</a>, I really haven’t had a temptation to consult a LLM, in addition to my skepticism that LLMs would know both those frameworks as well. The techniques I use for data visualization have been <a href="https://minimaxir.com/2017/08/ggplot2-web/">unchanged since 2017</a>, and the most time-consuming issue I have when making a chart is determining whether the data points are too big or too small for humans to read easily, which is not something a LLM can help with.</p><p>Asking LLMs coding questions is only one aspect of coding assistance. One of the other major ones is using a coding assistant with in-line code suggestions such as <a href="https://github.com/features/copilot">GitHub Copilot</a>. Despite my success in using LLMs for one-off coding questions, I actually dislike using coding assistants for an unexpected reason: it’s distracting. Whenever I see a code suggestion from Copilot pop up, I have to mentally context switch from writing code to reviewing code and then back again, which destroys my focus. Overall, it was a net neutral productivity gain but a net negative cost as Copilots are much more expensive than just asking a LLM ad hoc questions through a web UI.</p><p>Now we can talk about the elephants in the room — agents, <a href="https://www.anthropic.com/news/model-context-protocol">MCP</a>, and vibe coding — and my takes are spicy. Agents and MCP, at a high-level, are a rebranding of the Tools paradigm popularized by the <a href="https://arxiv.org/abs/2210.03629">ReAct paper</a> in 2022 where LLMs can decide whether a tool is necessary to answer the user input, extract relevant metadata to pass to the tool to run, then return the results. The rapid LLM advancements in context window size and prompt adherence since then have made Agent workflows more reliable, and the standardization of MCP is an objective improvement over normal Tools that I encourage. However, <strong>they don’t open any new use cases</strong> that weren’t already available when <a href="https://www.langchain.com/">LangChain</a> first hit the scene a couple years ago, and now <a href="https://www.polarsparc.com/xhtml/MCP.html">simple implementations of MCP</a> workflows are even more complicated and confusing <a href="https://minimaxir.com/2023/07/langchain-problem/">than it was back then</a>. I personally have not been able to find any novel use case for Agents, not then and not now.</p><p>Vibe coding with coding agents like <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">Claude Code</a> or <a href="https://www.cursor.com/en">Cursor</a> is something I have little desire to even experiment with. On paper, coding agents should be able to address my complaints with LLM-generated code reliability since it inherently double-checks itself and it’s able to incorporate the context of an entire code project. However, I have also heard the horror stories of people spending hundreds of dollars by accident and not get anything that solves their coding problems. There’s a fine line between experimenting with code generation and <em>gambling</em> with code generation. Vibe coding can get me 80% of the way there, and I agree there’s value in that for building quick personal apps that either aren’t ever released publicly, or are released with disclaimers about its “this is released as-is” nature. But it’s unprofessional to use vibe coding as a defense to ship knowingly substandard code for serious projects, and the only code I can stand by is the code I am fully confident in its implementation.</p><p>Of course, the coding landscape is always changing, and everything I’ve said above is how I use LLMs for now. It’s entirely possible I see a post on Hacker News that completely changes my views on vibe coding or other AI coding workflows, but I’m happy with my coding productivity as it is currently and I am able to complete all my coding tasks quickly and correctly.</p><h2 id="whats-next-for-llm-users">What’s Next for LLM Users?</h2><p>Discourse about LLMs and their role in society has become bifuricated enough such that making the extremely neutral statement that <a href="https://bsky.app/profile/hankgreen.bsky.social/post/3lnjohdrwf22j">LLMs have some uses</a> is enough to justify a barrage of harrassment. I strongly disagree with AI critic Ed Zitron <a href="https://www.wheresyoured.at/reality-check/">about his assertions</a> that the reason the LLM industry is doomed because OpenAI and other LLM providers can’t earn enough revenue to offset their massive costs as LLMs have no real-world use. Two things can be true simultaneously: (a) LLM provider cost economics are too negative to return positive ROI to investors, and (b) LLMs are useful for solving problems that are meaningful and high impact, albeit not to the AGI hype that would justify point (a). This particular combination creates a frustrating gray area that requires a nuance that an ideologically split social media can no longer support gracefully. Hypothetically, If OpenAI and every other LLM provider suddenly collapsed and no better LLM models would ever be trained and released, open-source and permissively licensed models such as <a href="https://huggingface.co/Qwen/Qwen3-235B-A22B">Qwen3</a> and <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1">DeepSeek R1</a> that perform comparable to ChatGPT are valid <a href="https://en.wikipedia.org/wiki/Substitute_good">substitute goods</a> and they can be hosted on dedicated LLM hosting providers like <a href="https://www.cerebras.ai/">Cerebras</a> and <a href="https://groq.com/">Groq</a> who can actually make money on each user inference query. OpenAI collapsing would not cause the end of LLMs, because LLMs are useful <em>today</em> and there will always be a nonzero market demand for them: it’s a bell that can’t be unrung.</p><p>As a software engineer — and especially as a data scientist — one thing I’ve learnt over the years is that it’s always best to use the right tool when appropriate, and LLMs are just another tool in that toolbox. LLMs can be both productive and counterproductive depending on where and when you use them, but they are most definitely not useless. LLMs are more akin to forcing a square peg into a round hole (at the risk of damaging either the peg or hole in the process) while doing things without LLM assistance is the equivalent of carefully defining a round peg to pass through the round hole without incident. But for some round holes, sometimes shoving the square peg through and asking questions later makes sense when you need to iterate quickly, while sometimes you have to be more precise with both the peg and the hole to ensure neither becomes damaged, because then you have to spend extra time and money fixing the peg and/or hole.</p><p>…maybe it’s okay if I ask an LLM to help me write my metaphors going forward.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: TextQuery – Query CSV, JSON, XLSX Files with SQL (151 pts)]]></title>
            <link>https://textquery.app/</link>
            <guid>43897129</guid>
            <pubDate>Mon, 05 May 2025 16:59:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://textquery.app/">https://textquery.app/</a>, See on <a href="https://news.ycombinator.com/item?id=43897129">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <main data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6="">  <p data-astro-cid-j7pv25f6="">
TextQuery is an <span data-astro-cid-j7pv25f6="">
all-in-one desktop app
</span> to import, query, modify, and visualize your raw data with SQL.
</p> <a href="https://textquery.app/downloads" role="button" data-astro-cid-j7pv25f6="">
Download TextQuery For Free <svg data-src="https://s2.svgbox.net/hero-outline.svg?ic=download" width="24" height="24" data-astro-cid-j7pv25f6=""></svg> </a> <p>
Available for macOS and Windows
</p> </div> <p><img src="https://i.magecdn.com/d2b508/29b284_hero?sz=100p" data-astro-cid-j7pv25f6=""> </p> </div> <div data-astro-cid-j7pv25f6=""> <p>
Trusted by 1500+ individuals from organizations like these
</p>  </div> <div data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">
Import Data in a Few Clicks
</h2> <p data-astro-cid-j7pv25f6="">
TextQuery lets you import multiple data files without needing to
            define schema, writing code, or commands.
</p> <p>
Supported Formats
</p> <div data-astro-cid-j7pv25f6=""> <p><span data-astro-cid-j7pv25f6="">
Data Files
</span><span data-astro-cid-j7pv25f6="">.xlsx, .xls, .csv, .tsv</span> </p> <p><span data-astro-cid-j7pv25f6="">
Structured Data
</span><span data-astro-cid-j7pv25f6="">.json, .jsonld</span> </p> <p><span data-astro-cid-j7pv25f6="">
Compressed Archives
</span><span data-astro-cid-j7pv25f6="">.zip, .gz, .tgz, .tar</span> </p> </div> </div> <p><img src="https://i.magecdn.com/d2b508/2e9a66_import?sz=100p" alt="Product screenshot" data-astro-cid-j7pv25f6=""> </p> </div> <div data-astro-cid-j7pv25f6=""> <p><img src="https://i.magecdn.com/d2b508/6c7d49_code_editor_4?sz=100p" alt="Product screenshot" data-astro-cid-j7pv25f6=""> </p> <div data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">
Powerful SQL Editor
</h2> <p data-astro-cid-j7pv25f6="">
Our best-in-class SQL editor offers multiple features to help you
            write queries faster.
</p> <div data-astro-cid-j7pv25f6=""> <svg data-src="https://s2.svgbox.net/materialui.svg?ic=check_circle" width="20" height="20" data-astro-cid-j7pv25f6=""></svg><p><span data-astro-cid-j7pv25f6="">Query History and Favorites</span> </p></div> </div> </div> <div data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">
Create Beautiful Charts
</h2> <p data-astro-cid-j7pv25f6="">
Create clean, beautiful charts right within the app. Simple to use
            and easy to customize.
</p> <div data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <svg data-src="https://s2.svgbox.net/materialui.svg?ic=check_circle" width="20" height="20" data-astro-cid-j7pv25f6=""></svg><p><span data-astro-cid-j7pv25f6="">
Supports Line, Bar, Area, Scatter, Pie Charts
</span> </p></div> <div data-astro-cid-j7pv25f6=""> <svg data-src="https://s2.svgbox.net/materialui.svg?ic=check_circle" width="20" height="20" data-astro-cid-j7pv25f6=""></svg><p><span data-astro-cid-j7pv25f6="">
Customize Title, Description, and Colors
</span> </p></div> <div data-astro-cid-j7pv25f6=""> <!-- Please refer: https://github.com/shubhamjain/svg-loader --> <svg data-src="https://s2.svgbox.net/materialui.svg?ic=check_circle" width="20" height="20" data-astro-cid-j7pv25f6=""></svg><p><span data-astro-cid-j7pv25f6="">
Export to a file or share directly from clipboard
</span> </p></div> </div> </div> <p><img src="https://i.magecdn.com/d2b508/ac4a84_chart_creator?sz=100p" alt="Product screenshot" data-astro-cid-j7pv25f6=""> </p> </div> <div data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">
Thoughtful Features That Simplify Work
</h2> <ul role="list" data-astro-cid-j7pv25f6=""> <li data-astro-cid-j7pv25f6=""> <img src="https://i.magecdn.com/d2b508/566360_inline_editor_2?sz=100p" alt="" data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-j7pv25f6="">
Inline Editor
</h3> <p data-astro-cid-j7pv25f6="">
Quickly edit values, collect your changes, and commit them in one
              go.
</p> </div> </li> <li data-astro-cid-j7pv25f6=""> <img src="https://i.magecdn.com/d2b508/242263_filters_2?sz=100p" alt="" data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-j7pv25f6="">Filters</h3> <p data-astro-cid-j7pv25f6="">
Quickly narrow down table's rows using filters. No need to write
              queries manually.
</p> </div> </li> <li data-astro-cid-j7pv25f6=""> <img src="https://i.magecdn.com/d2b508/dcea52_tabs?sz=100p" alt="" data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-j7pv25f6="">Tabs</h3> <p data-astro-cid-j7pv25f6="">
Work on multiple queries and tables simultaneously using tabs.
</p> </div> </li> <li data-astro-cid-j7pv25f6=""> <img src="https://i.magecdn.com/d2b508/32b263_data_export?sz=100p" alt="" data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-j7pv25f6="">
Data Export
</h3> <p data-astro-cid-j7pv25f6="">
Export data in multiple formats (CSV, JSON, Excel, SQL) or create
              another table with the results.
</p> </div> </li> </ul> </div> <section data-astro-cid-j7pv25f6=""> <figure data-astro-cid-j7pv25f6=""> <p data-astro-cid-j7pv25f6="">5 out of 5 stars</p>  <blockquote data-astro-cid-j7pv25f6=""> <p data-astro-cid-j7pv25f6="">
This is a wonderful app! There's essentially no learning curve if
            you already know SQL, and the chart creation feature is fantastic.
</p> </blockquote> <figcaption data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <p>Greg Williams</p> </div> <p>Gxwilso</p> </div> </figcaption> </figure> </section> <div data-astro-cid-j7pv25f6=""> <p data-astro-cid-j7pv25f6="">
First-class Desktop Experience
</p> <div data-astro-cid-j7pv25f6=""> <div data-astro-cid-j7pv25f6=""> <svg data-src="https://s2.svgbox.net/hero-outline.svg?ic=currency-dollar" width="32" height="32" data-astro-cid-j7pv25f6=""></svg> <h3 data-astro-cid-j7pv25f6="">
Pay Once, Use Forever
</h3> <p data-astro-cid-j7pv25f6="">
We hate needless subscriptions as much as you do. TextQuery comes
              with a <mark data-astro-cid-j7pv25f6="">perpetual license with free updates</mark>.
</p> </div> <div data-astro-cid-j7pv25f6=""> <!-- Please refer: https://github.com/shubhamjain/svg-loader --> <svg data-src="https://s2.svgbox.net/materialui.svg?ic=security" width="28" height="32" data-astro-cid-j7pv25f6=""></svg> <h3 data-astro-cid-j7pv25f6="">
Built with Privacy &amp; Security in Mind
</h3> <p data-astro-cid-j7pv25f6="">
TextQuery <mark data-astro-cid-j7pv25f6="">
neither records nor sends anything related to your usage
</mark>. You can query sensitive data with complete peace of mind.
</p> </div> <div data-astro-cid-j7pv25f6=""> <!-- Please refer: https://github.com/shubhamjain/svg-loader --> <svg data-src="https://s2.svgbox.net/materialui.svg?ic=keyboard" width="32" height="32" data-astro-cid-j7pv25f6=""></svg> <h3 data-astro-cid-j7pv25f6="">
Keyboard Shortcuts
</h3> <p data-astro-cid-j7pv25f6="">
Every frequently used function is <mark data-astro-cid-j7pv25f6="">
accessible via a keyboard shortcut,
</mark> making you fast and efficient.
</p> </div> <div data-astro-cid-j7pv25f6=""> <!-- Please refer: https://github.com/shubhamjain/svg-loader --> <svg data-src="https://s2.svgbox.net/materialui.svg?ic=update" width="32" height="32" data-astro-cid-j7pv25f6=""></svg> <h3 data-astro-cid-j7pv25f6="">
Always improving
</h3> <p data-astro-cid-j7pv25f6="">
We are constantly improving the app from your feedback, and from
              own usage. In past few months, <mark data-astro-cid-j7pv25f6="">
we have shipped hundreds of improvements
</mark>.
</p> </div> </div> </div> <div data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">
Try TextQuery For Free
</h2> <p data-astro-cid-j7pv25f6="">
Evaluate our free version with <a href="https://textquery.app/pricing" data-astro-cid-j7pv25f6="">
certain limitations
</a> and upgrade to Pro when you feel like it.
</p>  </div> </main>    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[No Instagram, no privacy (124 pts)]]></title>
            <link>https://blog.wouterjanleys.com/blog/no-instagram-no-privacy/</link>
            <guid>43896228</guid>
            <pubDate>Mon, 05 May 2025 15:37:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.wouterjanleys.com/blog/no-instagram-no-privacy/">https://blog.wouterjanleys.com/blog/no-instagram-no-privacy/</a>, See on <a href="https://news.ycombinator.com/item?id=43896228">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/BlogPosting">
        

        

        <div itemprop="articleBody">
            <p>I somehow escaped having an Instagram account.</p>
<p>This means that I am oblivious to all of my friends’ fun broadcasted on Instagram. I don’t feel pressured either to update the abstract audience of everyone I ever connected with online, on where I am, what I am doing or who I am hanging out with.</p>
<p>I feel very blessed.</p>
<p>Still, Instagram hasn’t completely escaped me. And it bugs me.</p>
<p>My wife and I live a pretty social life. My wife’s job required us to temporarily move to a place far away from home. Luckily the culture of the place we moved to is very friendly to foreigners and there is a big bunch of people like us who are also new to this place and eager to make new friends for the time that they are there.</p>
<p>This makes the lines of our friend circles blurred and our social life very dynamic. To illustrate: old friends leave and new friends arrive with such regularity, that the guest list of a birthday party this year will look vastly different from the one we threw last year.</p>
<p>Over the past few months, it has struck me multiple times how people know more about my life than I tell them or likely hear from others. Like: where we travelled last weekend and with whom. How can they know? Instagram. A post from someone else on that trip about that trip. Of course. You don’t have to be on Instagram, to <em>be</em> on instagram.</p>
<p>It’s not the worst problem to have, I know. It’s great that I participate in happenings that are fun enough for other people to post about. And without social media, people also talk about common friends or friends of friends.</p>
<p>Yet sometimes I am not sure about how I feel about other people hearing about my weekend adventures through social media of other people. Not that there is anything to hide, I just hope the message came across well.</p>
<p>Real life interactions usually offer the possibility to be mindful of sensitivities around a certain subject, as can be the case if the topic of discussion is about who is hanging out with whom. The goal of social interactions (offline), I believe, is often less about what all the talking is about than it is to have the interaction. So surely you will try to avoid hurting the feelings of whom you are talking with.</p>
<p>In the loud bursts of social media self-promotion and personal branding, any such nuance of tone is impossible. The audience is way too diverse to cater to all possible sensitivities. Just think of the unease to post something about a night out with the boys that strikes the right tone to all of: partner, boss and said boys. It becomes even more complicated if you have to count in the social fabric between people featuring in your post and your followers. Yet those relationships are equally real, and have just as much context.</p>
<p>Imagine a friend you were on a weekend trip with. This friend talks with another common friend. This common friend could have equally well been on that weekend trip because you like him or her but, due to circumstances, as is life, you did not invite him. You probably would feel uncomfortable with that first friend talking about that trip as if it was the most awesome trip ever, that everyone had non-stop fun and now everyone who was on that trip are best friends for life.</p>
<p>Yet this is the kind of impression an Instagram post or story typically evokes. It’s probably the content most of the first friends’ followers love to see. Except for maybe the few people who wonder why you didn’t ask them to join the trip.</p>
<p>I am happy not to be on Instagram. I would probably freak out by the pressure to post while worrying about how the same post is interpreted by different people. Yet other people still post about me. And now I worry how common friends, who may have seen those posts, interpret such posts, potentially not entirely positively.</p>
<p>Without an Instagram account, I luckily stay blissfully oblivious to whatever content is going around from which my whereabouts or activities can be deduced. It’s probably a lot less about me than I think anyway. At the same time, not really knowing what other people know, can also be a nagging sense of worry. </p>
<p>Every time I talk with someone who gives me a hint that he or she <em>knows</em> what I was doing last Friday night I get a real eerie feeling. Did this person get to see that picture we took right before dinner? Or did she see a story at the end of the night, one too many drinks later, when I was doing that silly dance that surely someone had filmed?</p>
<p>From when I was in law school, I remember it being challenging to find a simple and encompassing definition of the right to privacy. A definition that has worked for me is “being in control of what other people know about you”.</p>
<p>Given this definition, there are many flagrant privacy violations I am subject to and aware of (yes, internet companies that do large scale gathering of personal data, I am thinking of you). I know that I am not in control of what the online store knows about me when it’s nudging me to buy this or that pair of headphones. But somehow this is easy to shrug off. Not being in control of what your friends know about you, due to social media stories spread by other people, feels more unsettling.</p>
<p>How does one fix or regulate this? Maybe we need some sort of social etiquette where it’s frowned upon to post about social gatherings to any audience beyond who already was at that gathering. Although this arguably defies the purpose of social media altogether.</p>
<p>Meanwhile, I’ll continue feeling blessed, not knowing that my new cool friend threw a birthday party last week to which I was not invited. There’s no social disappointment that will discourage me to further invest in that relationship, improving the chances that I will be celebrating his or her birthday next year.</p>
<p>And what about that new friend who wasn’t on the trip, but may or may not have just scrolled through some flashy posts about it after a tough day at work? Hopefully he forgives me and still wants to join the next trip.</p>
        </div>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dimension 126 Contains Twisted Shapes, Mathematicians Prove (135 pts)]]></title>
            <link>https://www.quantamagazine.org/dimension-126-contains-strangely-twisted-shapes-mathematicians-prove-20250505/</link>
            <guid>43896199</guid>
            <pubDate>Mon, 05 May 2025 15:34:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/dimension-126-contains-strangely-twisted-shapes-mathematicians-prove-20250505/">https://www.quantamagazine.org/dimension-126-contains-strangely-twisted-shapes-mathematicians-prove-20250505/</a>, See on <a href="https://news.ycombinator.com/item?id=43896199">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-role="selectable">
    <p>It can be tempting to assume that your intuitions about three-dimensional space carry over to higher-dimensional realms. After all, adding another dimension simply creates a new direction to move around in. It doesn’t change the defining features of space: its endlessness and its uniformity.</p>
<p>But different dimensions have decidedly different personalities. In dimensions 8 and 24, it’s possible to <a href="https://www.quantamagazine.org/sphere-packing-solved-in-higher-dimensions-20160330/">pack balls together</a> especially tightly. In other dimensions, there are “exotic” spheres that look irremediably crumpled. And dimension 3 is the only one that can contain knots — in any higher dimension, you can untangle a knot even while holding its ends fast.</p>
<p>Now, mathematicians have put the finishing touches on a story of dimensional weirdness that has been 65 years in the making. For many decades, researchers have wanted to know which dimensions can host particularly strange shapes — ones so twisted that they cannot be converted into a sphere through a simple procedure called surgery. The existence of these shapes, mathematicians have shown, is intimately intertwined with fundamental questions in topology about the relationships between spheres of different dimensions.</p>
<p>Over the years, mathematicians found that the twisted shapes exist in dimensions 2, 6, 14, 30 and 62. They also showed that such shapes could not possibly exist in any other dimension — save one. No one could determine the status of dimension 126.</p>
<p>Three mathematicians have now settled this final problem. In a <a href="https://arxiv.org/abs/2412.10879">paper posted online</a> last December, <a href="https://waynelin92.github.io/">Weinan Lin</a> and <a href="https://pouiyter.github.io/">Guozhen Wang</a> of Fudan University in Shanghai, along with <a href="https://sites.google.com/view/xuzhouli">Zhouli Xu</a> of the University of California, Los Angeles, proved that 126 is indeed one of the rare dimensions that can host these strangely twisted shapes.</p>
<p>It’s “a very long program, finally finished,” said <a href="https://people.maths.ox.ac.uk/tillmann/">Ulrike Tillmann</a> of the University of Oxford.</p>
<p>The proof, which uses a combination of computer calculations and theoretical insights, is “like a monumental engineering project,” said <a href="https://people.math.harvard.edu/~mjh/">Michael Hopkins</a> of Harvard University. “It’s just jaw-dropping how they did it.”</p>
<h2><strong>The Doomsday Hypothesis</strong></h2>
<p>In the 1950s, the mathematician <a href="https://www.math.stonybrook.edu/~jack/">John Milnor</a> astonished the mathematical world by showing that dimension 7 is <a href="https://mathscinet.ams.org/mathscinet/relay-station?mr=0082103">home to “exotic” spheres</a>. An exotic sphere looks exactly like an ordinary sphere from the perspective of topology, which only considers the features of a shape that don’t change when it is stretched or deformed. But the two spheres have incompatible definitions of smoothness — a curve that’s smooth on an ordinary sphere might not be considered smooth on an exotic sphere. Milnor was eager to explore and classify these exotic spheres, which in some dimensions turned out to be rare and in others numbered in the thousands.</p>
<p>To do this, he <a href="https://mathscinet.ams.org/mathscinet/relay-station?mr=0130696">introduced a technique</a> called surgery, a controlled way to simplify a mathematical shape, or manifold, and potentially convert it into an exotic sphere. The method would become essential to the study of manifolds more generally.</p>
<p>As its name suggests, surgery involves slicing out a piece of a manifold and then sewing in one or more new pieces along the boundary of the cut. You must sew in these new pieces smoothly, without creating sharp corners or edges. (When it comes to questions about twisted shapes, mathematicians also require the surgery to respect the manifold’s “framing,” a technical attribute of how the manifold sits in space.)</p>
<p>To see this process in action, let’s surgically transform a torus (the two-dimensional surface of a doughnut) into a sphere (the two-dimensional surface of a ball):</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/05/Mathematical-surgery-Desktop.V2.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2025/05/Mathematical-surgery-Mobile.V2.svg" alt="" decoding="async">    </p>
            <figcaption>
            <p>Samuel Velasco/<em>Quanta Magazine</em></p>
        </figcaption>
    </figure>

<p>The result is an ordinary sphere — in fact, there are no 2D exotic spheres. But in certain dimensions, surgery converts some manifolds into ordinary spheres and others into exotic spheres. And sometimes, there’s yet another possibility: manifolds that can’t be converted into a sphere at all.</p>
<p>To visualize this last scenario, we can again look at a torus, only this time we’ll give it some special twists to obstruct surgeries:</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/05/Twists-and-Turns-Desktop.V3.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2025/05/Twists-and-Turns-Mobile.V3.svg" alt="" decoding="async">    </p>
            <figcaption>
            <p>Samuel Velasco/<em>Quanta Magazine</em></p>
        </figcaption>
    </figure>

<p>Mathematicians have proved that there is no surgery that can transform this twisted torus into a sphere, whether regular or exotic. It’s an entirely different class of manifold.</p>
<p>In 1960, the French mathematician Michel Kervaire came up with an <a href="https://link.springer.com/article/10.1007/BF02565940">invariant</a> — a number you can calculate for a given smooth manifold — that equals zero when the manifold can be surgically converted into a sphere, and 1 when it cannot. So the ordinary torus has a Kervaire invariant of zero, while the twisted torus has a Kervaire invariant of 1.</p>
<p>Kervaire used his invariant to explore the menagerie of possible manifolds in different dimensions. He even used it to come up with a 10-dimensional manifold that has no Kervaire invariant, either zero or 1 — meaning that this manifold must be so crooked that it can have no sensible notion of smoothness at all.</p>
<p>No one had imagined that such a manifold could exist. Faced with the power of the new invariant, mathematicians rushed to determine the Kervaire invariants of manifolds in different dimensions.</p>
<p>Within a few years, they’d proved that twisted manifolds of Kervaire invariant 1 exist in dimensions 2, 6, 14 and 30. These dimensions fit a pattern: Each number is 2 less than a power of 2 (for example, 30 is 2<sup>5</sup> − 2). In 1969, the mathematician William Browder <a href="https://www.jstor.org/stable/1970686?origin=crossref">proved that dimensions of this form</a> are the only ones that might host shapes with a Kervaire invariant of 1.</p>
<p>It was natural to assume that twisted manifolds would exist in all dimensions of this form: 62, 126, 254 and so forth. Based on this assumption, one mathematician even built an entire edifice of conjectures about exotic spheres and other shapes. But the possibility that the original assumption might be false still loomed. It came to be known as the doomsday hypothesis, since it would falsify all these other conjectures.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How are cyber criminals rolling in 2025? (243 pts)]]></title>
            <link>https://vin01.github.io/piptagole/cybcecrime/security/cybersecurity/2025/05/05/state-cyber-security.html</link>
            <guid>43896188</guid>
            <pubDate>Mon, 05 May 2025 15:33:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vin01.github.io/piptagole/cybcecrime/security/cybersecurity/2025/05/05/state-cyber-security.html">https://vin01.github.io/piptagole/cybcecrime/security/cybersecurity/2025/05/05/state-cyber-security.html</a>, See on <a href="https://news.ycombinator.com/item?id=43896188">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Speaking of earning a living, would you expect them to pay for web hosting/ cloud providers?</p>

<h2 id="free-content-hosting">Free content hosting</h2>

<h3 id="governments">Governments</h3>

<p>Government departments are not known for their amazing cyber security posture. Couldn’t they be utilized as free hosting providers?</p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/gov.png" alt="free-gov-hosting"></p>

<p>and it is truly international</p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/international.png" alt="internaitonal"></p>

<h3 id="universities--schools">Universities &amp; schools</h3>

<p>How about universities? they have a lot of web facing services and while they do teach cyber security sometimes, they can’t be that secure?</p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/uni.png" alt="yale and oregon state university"></p>

<p>For the zeitgeist-ignorant:</p>

<p><strong>What is Robux?</strong>: A virtual in-game currency used by gaming platform <a href="https://en.wikipedia.org/wiki/Roblox">Roblox</a></p>

<p>Content mostly revolves around: Onlyfans accounts/account generators, Robux, Amazon gift cards and Free movies. I suppose these are the most popular things on the internet these days.</p>



<p>Who doesn’t use an antivirus or a VPN to keep themselves, their family and their employees safe? These obviously unsafe links would get blocked and flagged by these advanced tools. You wish!</p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/safe-web.png" alt="safe-web"></p>

<p>and more ..</p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/safe-web2.png" alt="safe-web-2"></p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/safe-web3.png" alt="safe-web-3"></p>

<p>Turns out, they know which domains get a good reputation from these link m-analyzers.</p>

<p>Norton, Kaspersky, Zscaler, F-secure, NordVPN, Virustotal, Palo Alto: all of them marked these links as safe. The same for SOAR tools like URLscan.</p>

<h2 id="but-if-we-only-allow-access-to-a-pre-approved-list-of-domains-that-should-help-right">But if we only allow access to a pre-approved list of domains, that should help, right?</h2>

<p>No! Turns out, we are again one step behind. They know that there are some domains which everyone trusts. e.g., everything hosted by google, including the shiny “Looker studio” used to visualize data as pretty graphs.</p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/looker.png" alt="Looker studio"></p>

<h2 id="does-this-actually-work">Does this actually work?</h2>

<p>Apparently so! Here are some benign search results performed from a true Robux seeker and movie freeloader’s perspective.</p>

<p><img src="https://vin01.github.io/piptagole/assets/free-cms/seo.png" alt="good seo"></p>

<p>They have to be good at SEO and know their targets. Business as usual. ¯\_(ツ)_/¯</p>

<h2 id="how-are-they-exploiting-these-sites">How are they exploiting these sites?</h2>

<p>I have been advised not to disclose specific vulnerabilities since the parties involved are not most friendly and transparent in handling security reports. While most of these got reported and some even got fixed, I can only disclose high-level details of the compromise path. Some just ghosted me after conveniently fixing the flaws, and one even gave me a phone call, which was somewhat scary and perhaps not worth the adrenaline.</p>

<ul>
  <li>Outdated Wordpress plugins and CMS systems</li>
  <li>Cache poisoning via features like “search my site”</li>
  <li>Credential stuffing</li>
  <li>Dangling DNS records/ Subdomain takeovers</li>
</ul>

<h2 id="why-are-they-doing-it">Why are they doing it?</h2>

<p>Most of these files are not <em>malware</em> per se, since that might be a bit easy to detect out in the wild. But these PDFs and web pages link to a chain of websites. Each link would take the brave person who clicks them to another link and then to another, going through an affiliate network and making small amounts of money on the way.</p>

<p>Some of these links are also just plain phishing, mostly targeting kids looking for free/ cheap gaming gold “Robux”.</p>

<h2 id="this-is-not-new">This is not new</h2>

<p>I came across this post from July 2020 which discusses something similar: <a href="https://medium.com/@thezedwards/july-2020-compromised-paf-subdomains-mostly-via-microsoft-azure-5834ae22733a">https://medium.com/@thezedwards/july-2020-compromised-paf-subdomains-mostly-via-microsoft-azure-5834ae22733a</a></p>


  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: VectorVFS, your filesystem as a vector database (251 pts)]]></title>
            <link>https://vectorvfs.readthedocs.io/en/latest/</link>
            <guid>43896011</guid>
            <pubDate>Mon, 05 May 2025 15:17:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vectorvfs.readthedocs.io/en/latest/">https://vectorvfs.readthedocs.io/en/latest/</a>, See on <a href="https://news.ycombinator.com/item?id=43896011">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" v-pre="">
            
  <p><a href="https://vectorvfs.readthedocs.io/en/latest/_images/logo_vectorvfs.png"><img alt="Banner" src="https://vectorvfs.readthedocs.io/en/latest/_images/logo_vectorvfs.png">
</a></p><section id="vectorvfs-your-filesystem-as-a-vector-database">

<p>VectorVFS is a lightweight Python package that transforms your Linux filesystem into a vector database by
leveraging the native VFS (Virtual File System) extended attributes. Rather than maintaining a separate
index or external database, VectorVFS stores vector embeddings directly alongside each file—turning your
existing directory structure into an efficient and semantically searchable embedding store.</p>
<p>VectorVFS supports Meta’s Perception Encoders (PE) <a href="https://arxiv.org/abs/2504.13181">[arxiv]</a> which
includes image/video encoders for vision language understanding, it outperforms InternVL3, Qwen2.5VL
and SigLIP2 for zero-shot image tasks. We support both CPU and GPU but if you have a large
collection of images it might take a while in the first time to embed all items if you are
not using a GPU.</p>
<div>
<p>Note</p>
<p>This is the first release of VectorVFS and we are expanding models and data types.
Currently we support only Perception Encoders (PE) and images.</p>
</div>
<section id="key-features">
<h2>Key Features<a href="#key-features" title="Link to this heading">¶</a></h2>
<ul>
<li><p><strong>Zero-overhead indexing</strong>
Embeddings are stored as extended attributes (xattrs) on each file, eliminating the need for external
index files or services.</p></li>
<li><p><strong>Seamless retrieval</strong>
Perform searches across your filesystem, retrieving files by embedding similarity.</p></li>
<li><p><strong>Flexible embedding support</strong>
Plug in any embedding model—from pre-trained transformers to custom feature extractors—and let
VectorVFS handle storage and lookup.</p></li>
<li><p><strong>Lightweight and portable</strong>
Built on native Linux VFS functionality, VectorVFS requires no additional daemons, background
processes or databases.</p></li>
</ul>
<div>
<p role="heading"><span>Contents</span></p>
<ul>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/installation.html">Installation</a><ul>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/installation.html#installing-with-pip">Installing with pip</a></li>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/installation.html#using-the-vfs-script">Using the vfs Script</a></li>
</ul>
</li>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/architecture.html">Design and architecture</a><ul>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/architecture.html#what-is-an-inode">What is an inode?</a></li>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/architecture.html#how-ext4-stores-extended-attributes">How Ext4 stores extended attributes</a></li>
</ul>
</li>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/usage.html">Using the <cite>vfs</cite> command</a><ul>
<li><a href="https://vectorvfs.readthedocs.io/en/latest/usage.html#vfs-search-command"><cite>vfs search</cite> command</a></li>
</ul>
</li>
</ul>
</div>
</section>
</section>
<section id="indices-and-tables">
<h2>Indices and tables<a href="#indices-and-tables" title="Link to this heading">¶</a></h2>
<ul>
<li><p><a href="https://vectorvfs.readthedocs.io/en/latest/genindex.html"><span>Index</span></a></p></li>
<li><p><a href="https://vectorvfs.readthedocs.io/en/latest/py-modindex.html"><span>Module Index</span></a></p></li>
<li><p><a href="https://vectorvfs.readthedocs.io/en/latest/search.html"><span>Search Page</span></a></p></li>
</ul>
</section>


          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Geometrically understanding calculus of inverse functions (2023) (117 pts)]]></title>
            <link>https://tobylam.xyz/2023/11/27/inverse-functions-legendre-part-1</link>
            <guid>43895852</guid>
            <pubDate>Mon, 05 May 2025 15:01:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tobylam.xyz/2023/11/27/inverse-functions-legendre-part-1">https://tobylam.xyz/2023/11/27/inverse-functions-legendre-part-1</a>, See on <a href="https://news.ycombinator.com/item?id=43895852">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">

    <!-- Makes it possible to have an excerpt that has HTML code in it (instead of only plaintext in a string) -->
    
      <p>Given a function such as \(\tan x\), could you write \(\frac{d}{dx} \arctan x\) and \(\int \arctan x \; dx\), just from \(\tan x\), \(\frac{d}{dx} \tan x\) and \(\int \tan x \; dx\)? With some caveats, the inverse function theorem answers the former while the Legendre transformation answers the later. We’ll approach this with as much geometric intuition as possible, avoiding the dry application of formulas.</p>



<p>Instead of approaching the inverse function theorem through formulas, we’ll explore it geometrically—it’s much more intuitive and enjoyable!</p>

<p>But first, to refresh our memory, let’s revisit the formal statement of the inverse function theorem, which relates the derivative of \(f(x)\) and its inverse \(f^{-1}(x)\).</p>

<blockquote>
  <p>Given a continuously differentiable function \(f: \mathbb{R} \to \mathbb{R}\) with \(f'(a) \neq 0\) at some point, the inverse function theorem states that there is some interval \(I\) with \(a \in I\) such that there exists a continuously differentiable inverse \(f^{-1}\) defined on \(f(I)\) such that for all \(x \in I\)</p>
</blockquote><p>

\[\frac{df^{-1}}{dx}(x) = \frac{1}{f'(f^{-1}(x))}.\]

</p><p>A simple example of the theorem in action is finding the derivative of \(\ln x\), which evaluates to \(1/e^{\ln x} = 1/x\). The standard high school approach to deduce the above in high school is to differentiate both sides of \(f^{-1}(f(x)) = x\).</p>

<p>This formal approach is quite dry and things get a lot more interesting when we think geometrically.</p>

<p>Geometrically, given some function \(f(x) = y\) the inverse is just taking the plot of the function and reflecting it about the diagonal line \(y=x\). As such all tangent lines are also reflected along the diagonal line and hence the slope is inversed. Below we have the graph of \(e^x\) (the blue line) and the graph of \(\ln x\) (the red line). And you can see how the tangent line of \(e^x\) at \((2,e^2)\) is reflected along \(y=x\) to give the tangent line of \(\ln x\) at \((e^2, 2)\).</p>

<p><img src="https://tobylam.xyz/generated/assets/images/2023-11-28-e%5Ex-ivt-600-8400210a9.webp" srcset="https://tobylam.xyz/generated/assets/images/2023-11-28-e%5Ex-ivt-480-8400210a9.webp 480w, https://tobylam.xyz/generated/assets/images/2023-11-28-e%5Ex-ivt-600-8400210a9.webp 600w, https://tobylam.xyz/generated/assets/images/2023-11-28-e%5Ex-ivt-720-8400210a9.webp 720w, https://tobylam.xyz/generated/assets/images/2023-11-28-e%5Ex-ivt-900-8400210a9.webp 900w, https://tobylam.xyz/generated/assets/images/2023-11-28-e%5Ex-ivt-1080-8400210a9.webp 1080w, https://tobylam.xyz/generated/assets/images/2023-11-28-e%5Ex-ivt-1250-8400210a9.webp 1250w" sizes="(max-width: 600px) 100vw, (max-width: 1000px) 600px, (max-width: 1400px) 600px, (min-width: 1401px) 600px" width="1250" height="628"></p>

<p>Another way of looking at it would be to subjugate \(S=\{(x, f(x)) \, \vert \, x \in I\}\), the plot of \(f(x)\), with a graph transformation \(\phi:(x,y) \to (\hat{x}, \hat{y}) = (y,x)\) and express \(\phi(S)\) as a plot \((\hat{x}, \hat{y}(\hat{x}))\). This way \(\hat{y}\) would be in effect the inverse of \(y\). We then wish to understand how \(\phi\) acts on \(\frac{dy}{dx}\) to give \(\frac{d\hat{y}}{d\hat{x}}\).</p>

<p>You can express this with the following graph</p>

<p><img alt="https://q.uiver.app/#q=WzAsNixbMywwLCIoeSh4KSx4KSJdLFswLDIsIih4LCB5Jyh4KSkiXSxbMywyLCIoXFxoYXR7eH0sXFxmcmFje2RcXGhhdHt5fX17ZFxcaGF0e3h9fSkgIl0sWzAsMCwiKHgseSh4KSkiXSxbNCwwLCIoXFxoYXR7eH0sXFxoYXR7eX0oXFxoYXR7eH0pKSAiXSxbNCwxLCJcXGhhdHt4fSh4KT0geSh4KSBcXFxcIFxcaGF0e3l9KFxcaGF0e3h9KHgpKT15XnstMX0oXFxoYXR7eH0pIl0sWzAsMiwiXFxmcmFje2R9e2RcXGhhdHt4fX0iXSxbMSwyXSxbMywwLCJcXHBoaSgoeCx5KSk9KHkseCkiXSxbMywxLCJcXGZyYWN7ZH17ZHh9Il0sWzAsNCwiPSIsMSx7InN0eWxlIjp7ImJvZHkiOnsibmFtZSI6Im5vbmUifSwiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFs0LDUsIiIsMSx7InN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XV0=" src="https://tobylam.xyz/generated/assets/images/legend1-600-fb2c4463e.png" srcset="https://tobylam.xyz/generated/assets/images/legend1-480-fb2c4463e.webp 480w, https://tobylam.xyz/generated/assets/images/legend1-600-fb2c4463e.webp 600w, https://tobylam.xyz/generated/assets/images/legend1-720-fb2c4463e.webp 720w, https://tobylam.xyz/generated/assets/images/legend1-900-fb2c4463e.webp 900w, https://tobylam.xyz/generated/assets/images/legend1-1080-fb2c4463e.webp 1080w, https://tobylam.xyz/generated/assets/images/legend1-1260-fb2c4463e.webp 1260w, https://tobylam.xyz/generated/assets/images/legend1-1304-fb2c4463e.webp 1304w" sizes="(max-width: 600px) 100vw, (max-width: 1000px) 600px, (max-width: 1400px) 600px, (min-width: 1401px) 600px" width="1304" height="782"></p>

<p>and the equalities</p><p>

\[\begin{align*}
    \hat x(x) &amp;= y(x), \\
    \hat y(\hat x(x)) &amp;= x \\
    &amp;= y^{-1}(\hat x(x)).
\end{align*}\]

</p><p>We could then directly calculate</p><p>

\[\begin{align*}
    \frac{d\hat{y}}{d\hat{x}}(\hat x(x)) &amp;= \frac{d\hat{y}}{dx}(x) / \frac{d\hat{x}}{dx}(x) &amp; \text{(Chain rule)} \\
    &amp;= 1 / \frac{dy}{dx}(x).
\end{align*}\]

</p><p>This recovers the inverse function theorem.</p>

<p>This method generalises to other maps \(\phi: \mathbb{R}^2 \to \mathbb{R}^2\) (e.g. \(\phi:(r, \theta) \to (r \cos \theta, r \sin \theta)\)) . These ideas are explored much more deeply in <a href="https://www.cambridge.org/core/books/symmetry-methods-for-differential-equations/805DEEB7D5D0D2040A3A8444B1C8A33E">Hydon’s book on Symmetry Methods for Differential Equations</a>.</p>

<p>By calculating \(\frac{d}{dx} \tan x = \sec^2 x\), we now know that \(\frac{d}{dx} \arctan x = \frac{1}{\text{sec}^2(\arctan x)} = \frac{1}{1+x^2}\). So what about \(\int \arctan x \, dx\)?</p>

<h2 id="integrals-of-inverse-functions-and-the-legendre-transformation">Integrals of inverse functions and the Legendre transformation</h2>

<p>Geometrically, we could show that with \(f\) strictly monotone, we have</p><p>

\[\int_{f(a)}^{f(b)} f^{-1}(y) \, dy + \int^{b}_a f(x) \, dx  = bf(b)-af(a)\]

</p><p>This is <a href="https://en.wikipedia.org/wiki/Integral_of_inverse_functions">a result by Laisant in 1905</a>. Here’s a proof-without-words.</p>

<p><img src="https://tobylam.xyz/generated/assets/images/legend3-800-1a031bab5.png" srcset="https://tobylam.xyz/generated/assets/images/legend3-400-1a031bab5.png 400w, https://tobylam.xyz/generated/assets/images/legend3-600-1a031bab5.png 600w, https://tobylam.xyz/generated/assets/images/legend3-800-1a031bab5.png 800w, https://tobylam.xyz/generated/assets/images/legend3-1000-1a031bab5.png 1000w"></p>

<!-- <small> By <a href="//commons.wikimedia.org/w/index.php?title=User:Jonathan.Steinbuch&amp;amp;action=edit&amp;amp;redlink=1" title="Jonathan Steinbuch - Own work"> Jonathan Steinbuch - Own work, </a>
 <a href="https://creativecommons.org/licenses/by-sa/3.0" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=16592836">Link</a> </small> -->

<p>You can use this to recover the explicit formula of the integral of \(f^{-1}\), which is left as an exercise! Instead we will use the graph reflection approach used earlier for the inverse function theroem.</p>

<p>Suppose we’re interested in finding \(\int \arctan x \, dx\). Notationally, it is the easiest to let \(y = \int \tan x \, dx\) up to some constant such that \(\frac{dy}{dx} = \tan x\) is the function we want to invert. Let \(S\) be the plot of \(\frac{dy}{dx}\), i.e. \(\{(x, \frac{dy}{dx}) \vert \, x \in I\}\) for some appropiate interval \(I\). Applying the map \(\phi: (x,y) \to (\hat{x}, \hat{y}) = (y,x)\), we assume the existence of some \(\hat{y}\) such that its derivative is the plot of \(\phi(S)\). Now \(\hat{y} = \int \arctan \hat{x} \, d\hat{x}\) which is what we seek.</p>

<p>Consider</p>

<p><img alt="https://q.uiver.app/#q=WzAsNSxbMywwLCIoXFxoYXR7eH0sXFxoYXR7eX0oXFxoYXQgeCkiXSxbMCwyLCIoeCwgXFxmcmFje2R5fXtkeH0oeCkpIl0sWzMsMiwiKFxcaGF0IHgsIFxcZnJhY3tkXFxoYXQgeX17ZFxcaGF0IHh9KFxcaGF0IHgpKSAiXSxbMCwwLCIoeCx5KHgpKSJdLFs1LDNdLFswLDIsIlxcZnJhY3tkfXtkXFxoYXR7eH19Il0sWzEsMiwiXFxwaGkoKHgseSkpPSh5LHgpIl0sWzMsMSwiXFxmcmFje2R9e2R4fSJdXQ==" src="https://tobylam.xyz/generated/assets/images/legend2-600-9a72c68ab.png" srcset="https://tobylam.xyz/generated/assets/images/legend2-480-9a72c68ab.webp 480w, https://tobylam.xyz/generated/assets/images/legend2-600-9a72c68ab.webp 600w, https://tobylam.xyz/generated/assets/images/legend2-720-9a72c68ab.webp 720w, https://tobylam.xyz/generated/assets/images/legend2-900-9a72c68ab.webp 900w, https://tobylam.xyz/generated/assets/images/legend2-1080-9a72c68ab.webp 1080w, https://tobylam.xyz/generated/assets/images/legend2-1232-9a72c68ab.webp 1232w" sizes="(max-width: 600px) 100vw, (max-width: 1000px) 600px, (max-width: 1400px) 600px, (min-width: 1401px) 600px" width="1232" height="712"></p>

<p>with the equalities</p><p>

\[\begin{align*}
    \hat x(x) &amp;= \frac{dy}{dx}(x), \\
    \frac{d\hat y}{d\hat x}(\hat x(x)) &amp;= x. \\
\end{align*}\]

</p><p>Now consider</p><p>

\[\begin{align*}
    &amp;\;\hat{y}(\hat{x}) \\
    &amp;= \int\frac{d\hat y}{d\hat x}(\hat{x}) d\hat{x}  \\
    &amp;= \hat{x} \frac{d\hat y}{d\hat x}(\hat{x}) - \int \hat{x}\frac{d^2\hat y}{d\hat x^2}(\hat{x}) d\hat{x}  &amp; (\text{by parts}) \\
    &amp;= \hat{x} \frac{d\hat y}{d\hat x}(\hat{x}) - \int \hat{x}(x)\frac{d^2\hat y}{d\hat x^2}(\hat{x}(x))  \frac{d\hat x}{dx}(x) dx &amp;\\
    &amp;= \hat{x} \frac{d\hat y}{d\hat x}(\hat{x}) - \int \hat{x}(x)\bigg[\frac{d}{dx}\bigg(\frac{d\hat y}{d\hat x}(\hat{x}(x))\bigg)(x)\bigg] dx &amp; (\text{chain rule})\\ 
    &amp;= \hat{x} \frac{d\hat y}{d\hat x}(\hat{x}) - \int \hat{x}(x) dx &amp; \bigg(\frac{d\hat y}{d\hat x}(\hat{x}(x)) = x\bigg) \\ 
    &amp;= \hat{x} \frac{d\hat y}{d\hat x}(\hat{x}) - \int \frac{dy}{dx}(x) dx &amp; (\text{by def of } \hat{x})\\ 
    &amp;= \hat{x} \frac{d\hat y}{d\hat x}(\hat{x}) - y(x) + C \\
    &amp;= \hat{x} \frac{d\hat y}{d\hat x}(\hat{x}) - y\bigg(\frac{d\hat{y}}{d\hat x}(\hat{x})\bigg) + C.  &amp; (\text{by def of } \hat{x})
\end{align*}\]

</p><p>Let’s try to compute a simple example, the integral of \(\ln x\). Consider</p>

<p><img src="https://tobylam.xyz/generated/assets/images/legend4-600-36dc952bd.png" srcset="https://tobylam.xyz/generated/assets/images/legend4-480-36dc952bd.webp 480w, https://tobylam.xyz/generated/assets/images/legend4-600-36dc952bd.webp 600w, https://tobylam.xyz/generated/assets/images/legend4-720-36dc952bd.webp 720w, https://tobylam.xyz/generated/assets/images/legend4-900-36dc952bd.webp 900w, https://tobylam.xyz/generated/assets/images/legend4-1080-36dc952bd.webp 1080w, https://tobylam.xyz/generated/assets/images/legend4-1260-36dc952bd.webp 1260w, https://tobylam.xyz/generated/assets/images/legend4-1440-36dc952bd.webp 1440w, https://tobylam.xyz/generated/assets/images/legend4-1736-36dc952bd.webp 1736w" sizes="(max-width: 600px) 100vw, (max-width: 1000px) 600px, (max-width: 1400px) 600px, (min-width: 1401px) 600px" width="1736" height="718"></p>

<p>Repeating what we’ve done above, we have</p><p>

\[\begin{align*}
    \hat{y} &amp;= \int \ln \hat{x} \, d\hat{x} \\
    &amp;= \hat{x} \times \ln(\hat{x})  - \int \hat{x} \times \frac{d}{d\hat x}(\ln \hat{x}) d\hat{x}\\
    &amp;= \hat{x} \times \ln(\hat{x}) - \int e^x \times \frac{d}{d\hat x}(\ln \hat x) \times \frac{d}{dx} (\hat x) dx \\
    &amp;= \hat{x} \times \ln(\hat{x}) - \int e^x \times \frac{d}{dx} \big(\ln(e^x)\big) dx \\
    &amp;= \hat{x} \times \ln(\hat{x}) - \int e^x dx \\
    &amp;= \hat{x} \times \ln(\hat{x}) - e^x + C \\
    &amp;= \hat{x} \times \ln(\hat{x}) - e^{\ln (\hat{x})} + C \\
    &amp;= \hat{x} \times \ln(\hat{x}) - \hat{x} + C. 
\end{align*}\]

</p><p>Finally, just by using the formula \(\hat{y} = \hat{x} \times\frac{d\hat y}{d\hat x}(\hat{x}) - y(\hat{y}'(\hat{x})) + C\), if we let \(y = - \ln \vert \cos(x) \vert\), then we have \(y' = \tan(x)\) and \(\hat{y}' = \arctan(x)\) so</p><p>

\[\begin{align*}
    \int \arctan \hat{x} &amp;= \hat{x} \arctan \hat{x} - \ln \vert \cos(\arctan(\hat{x}))\vert + C\\
    &amp;=  \hat{x} \arctan \hat{x} - \frac{1}{2} \ln \big( \frac{1}{1+\hat{x}^2}\big) + C
\end{align*}\]

</p><p>as expected.</p>

<p>The map \(y(x) \to \hat{y}(\hat{x}) := \hat{x}\frac{d\hat y}{d\hat x}(\hat{x}) - y(\frac{d\hat y}{d\hat x}(\hat{x}))\) is called the Legendre transformation, which has wide applications in areas of physics. But mathematically, one could think of it as an analogue to the inverse function theorem: It tells you how the inverse map acts on integrals.</p>

<h2 id="summary">Summary</h2>

<p><img alt="https://q.uiver.app/#q=WzAsNixbMCwxLCJmKHgpIl0sWzAsMiwiZicoeCkiXSxbMCwwLCJGKHgpIl0sWzMsMSwiZyh5KTo9Zl57LTF9KHkpIl0sWzMsMCwiRyh5KSJdLFszLDIsImcnKHkpIl0sWzAsMSwiXFxmcmFje2R9e2R4fSJdLFsyLDAsIlxcZnJhY3tkfXtkeH0iXSxbMCwzXSxbNCwzLCJcXGZyYWN7ZH17ZHl9Il0sWzMsNSwiXFxmcmFje2R9e2R5fSJdLFsxLDUsIlxcdGV4dHtJbnZlcnNlIEZ1bmMuIFRobS59Il0sWzIsNCwiXFx0ZXh0e0xlZ2VuZHJlIFRyYW5zZm9ybX0iXV0=" src="https://tobylam.xyz/generated/assets/images/2023-11-29-summary-600-47e06eab1.png" srcset="https://tobylam.xyz/generated/assets/images/2023-11-29-summary-480-47e06eab1.webp 480w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-600-47e06eab1.webp 600w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-720-47e06eab1.webp 720w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-900-47e06eab1.webp 900w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-1080-47e06eab1.webp 1080w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-1260-47e06eab1.webp 1260w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-1440-47e06eab1.webp 1440w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-1800-47e06eab1.webp 1800w, https://tobylam.xyz/generated/assets/images/2023-11-29-summary-1992-47e06eab1.webp 1992w" sizes="(max-width: 600px) 100vw, (max-width: 1000px) 600px, (max-width: 1400px) 600px, (min-width: 1401px) 600px" width="1992" height="824"></p>

<p>with the following relations</p><p>

\[\begin{align*}
    \text{IFT:} \quad &amp; g'(y) = \frac{1}{f'(g(y))} \\
    \text{Legendre:} \quad &amp; G(y) = y\times g(y) - F(g(y)) + C
\end{align*}\]

</p><p>For more ways to understand the ideas covered in this post you can look at the <a href="https://tobylam.xyz/assets/documents/2023-11-27-inverse-functions-legendre.pdf">article</a> I’ve written for the Oxford Invariants journal, as well as the <a href="https://en.wikipedia.org/wiki/Integral_of_inverse_functions">wiki page</a> on the integral of inverse functions.</p>

<!-- 
$$\begin{align*}
    &\int g(y) dy \\
    &= y \times g(y) - \int y dg(y) \\
    &= y \times g(y) - \int y g'(y) dy \\
    &= y \times g(y) - \int f(x) g'(f(x)) f'(x) dx & (\text{Let }y = f(x))\\
    &= y \times g(y) - \int f(x) (g \circ f)'(x) dx \\
    &= y \times g(y) - \int f(x) dx & (g \circ f = \text{id}) \\
    &= y \times g(y) - F(x) + C \\
    &= y \times g(y) - F(g(y)) + C
\end{align*}$$
-->

    
    

    <!-- Only change in this layout is here. Next posts and previous posts. Refer to _plugins/WithinCategoryPostNavigation.rb-->
    <!-- I have no idea why the next and previous posts are swapped. Oh well. The link to all this is here https://ajclarkson.co.uk/blog/jekyll-category-post-navigation/ -->

    <!-- <a href="/2023/12/11/visualization-fundamental-theorem-calculus" class="previous-link">Next post</a> 
    
     <a href="/2023/11/04/graph-sketching-techniques" class="next-link">Previous post</a> 

    <p>All posts in <a href="/archive/categories#Maths"> "maths" </a></p> -->

  </div>
</article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Tektronix TDS 684B Oscilloscope Uses CCD Analog Memory (102 pts)]]></title>
            <link>https://tomverbeure.github.io/2025/05/04/TDS684B-CCD-Memory.html</link>
            <guid>43895622</guid>
            <pubDate>Mon, 05 May 2025 14:37:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tomverbeure.github.io/2025/05/04/TDS684B-CCD-Memory.html">https://tomverbeure.github.io/2025/05/04/TDS684B-CCD-Memory.html</a>, See on <a href="https://news.ycombinator.com/item?id=43895622">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#the-tds600-series" id="markdown-toc-the-tds600-series">The TDS600 Series</a></li>
  <li><a href="#the-acquisition-board" id="markdown-toc-the-acquisition-board">The Acquisition Board</a></li>
  <li><a href="#measuring-along-the-signal-path" id="markdown-toc-measuring-along-the-signal-path">Measuring Along the Signal Path</a></li>
  <li><a href="#a-closer-look-at-the-noise-issue" id="markdown-toc-a-closer-look-at-the-noise-issue">A Closer Look at the Noise Issue</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p><img src="https://tomverbeure.github.io/assets/tds684b/tds684b.jpg" alt="TDS 684B showing 4 1 GHz sine waves"></p>

<p>I have a <a href="https://tomverbeure.github.io/2024/04/17/Tektronix-TDS-Button-Swap.html">Tektronix TDS 684B oscilloscope</a> that I bought 
cheaply at an auction. It has 4 channels, 1 GHz of BW and a sample rate of 5 Gsps. Those are respectable 
numbers even by today’s standards. It’s also the main reason why I have it: compared to modern oscilloscopes, 
the other features aren’t nearly as impressive. It can only 
record 15k samples per channel at a time, for example. But at least the sample rate doesn’t go down when
you increase the number of recording channels: it’s 5 Gsps even all 4 channels are enabled.</p>

<p>I’ve always wondered how Tektronix managed to reach such high specifications back in the nineties, 
so in this blog post I take a quick look at the internals, figure out how it works, and
do some measurements along the signal path.</p>

<h2 id="the-tds600-series">The TDS600 Series</h2>

<p>The first oscilloscopes of the TDS600 series were introduced around 1993. The last one, the 
TDS694C was released in 2002. The TDS684 version was from sometime 1995. The ICs on my 
TDS684C have date codes from as early as the first half of 1997.</p>

<p>The main characteristic of these scopes was their extreme sample rate for that era, going from
2 Gsps for the TDS620, TDS640 and TDS644, 5 Gsps for the TDS654, TDS680 and TDS684, and 10 Gsps for
the TDS694C which was developed under the <a href="https://w140.com/tekwiki/wiki/TDS694C"><em>Screamer</em></a>
code name.</p>

<p>The oscilloscopes have 2 main boards:</p>

<ul>
  <li>
    <p>the acquisition board contains all the parts from the analog input down to the sample memory as well as some
triggering logic.</p>

    <p><a href="https://tomverbeure.github.io/assets/tds684b/acquisition_board.jpg"><img src="https://tomverbeure.github.io/assets/tds684b/acquisition_board.jpg" alt="Acquisition board"></a>
<em>(Click to enlarge)</em></p>
  </li>
  <li>
    <p>a very busy CPU board does the rest.</p>

    <p><a href="https://tomverbeure.github.io/assets/tds684b/cpu_board.jpg"><img src="https://tomverbeure.github.io/assets/tds684b/cpu_board.jpg" alt="CPU board"></a>
<em>(Click to enlarge)</em></p>
  </li>
</ul>

<p>2 flat cables and a PCB connect the 2 boards.</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/interconnections.jpg" alt="Interconnections"></p>

<p>The interconnect PCB traces go to the memory on the acquisition board. It’s safe to assume that
this interface is used for high-speed waveform data transfer while the flat cables are for lower speed
configuration and status traffic.</p>

<p><em>If you ever remove the interconnection PCB, make sure to put it back with the same orientation.
It will fit just fine when rotated 180 degrees but the scope won’t work anymore!</em></p>

<h2 id="the-acquisition-board">The Acquisition Board</h2>

<p>The TDS 684B has 4 identical channels that can easily be identified.</p>

<p><a href="https://tomverbeure.github.io/assets/tds684b/acquisition_board_annotated.jpg"><img src="https://tomverbeure.github.io/assets/tds684b/acquisition_board_annotated.jpg" alt="Annotated acquisition board"></a>
<em>(Click to enlarge)</em></p>

<p>There are 6 major components in the path from input to memory:</p>

<ul>
  <li>
    <p>Analog front-end</p>

    <p>Hidden under a shielding cover, but you’d expect to find a bunch of relays there to switch between 
different configurations: AC/DC, 1Meg/50 Ohm termination, …</p>

    <p>I didn’t open it because it requires disassembling pretty much the whole scope.</p>
  </li>
  <li>
    <p>Signal Conditioner IC(?)</p>

    <p>This is the device with the glued-on heatsink. I left it in place because there’s no metal 
attachment latch. Reattaching it would be a pain. Since the acquisition board has a bunch 
of custom ICs already, chances are this one is custom as well, so knowing the exact part 
number wouldn’t add a lot of extra info.</p>

    <p>We can see one differential pair going from the analog front-end into this IC and a 
second one going from this IC to the next one, an ADG286D.</p>
  </li>
  <li>
    <p>National Semi ADG286D Mystery Chip</p>

    <p>Another custom chip with unknown functionality.</p>
  </li>
  <li>
    <p><a href="https://tomverbeure.github.io/assets/tds684b/MC10319.PDF">Motorola MC10319DW</a> 8-bit 25 MHz A/D Converter</p>

    <p>Finally, an off-the-shelf device! But why is it only rated for 25MHz?</p>
  </li>
  <li>
    <p>National Semi ADG303 - A Custom Memory Controller Chip</p>

    <p>It receives the four 8-bit lanes from the four ADCs on one side and connects
to four SRAMs on the other.</p>
  </li>
  <li>
    <p>4 <a href="https://tomverbeure.github.io/assets/tds684b/as7c256.pdf">Alliance AS7C256-15JC</a> SRAMs</p>

    <p>Each memory has a capacity of 32KB and a 15ns access time, which allows for
a maximum clock of 66 MHz.</p>

    <p>The TDS 684B supports waveform traces of 15k points, so they either only use
half of the available capacity or they use some kind of double-buffering scheme.</p>

    <p>There are four unpopulated memory footprints. In one of my TDS 420A blog posts,
I extend the waveform memory 
<a href="https://tomverbeure.github.io/2020/07/11/Option-Hacking-the-Tektronix-TDS-420A.html#in-search-of-the-missing-memory">by soldering in extra SRAM chips</a>.
I’m not aware of a TDS 684B option for additional memory, so I’m not optimistic
about the ability to expand its memory. There’s also no such grayed-out option
in the acquisition menu.</p>
  </li>
</ul>

<p>When googling for “ADG286D”, I got my answer when I stumbled on 
<a href="https://bsky.app/profile/retr0.id/post/3ljqf5bvkys2d">this comment on BlueSky</a> which speculates that 
it’s an analog memory, probably some kind of CCD FIFO. Analog values are captured
at a rate of up to 5 GHz and then shifted out at a much lower speed and fed into the ADC. I later
found a 
<a href="https://groups.io/g/TekScopes/message/162247">few</a> 
<a href="https://www.eevblog.com/forum/testgear/conversion-of-500mhz-tds744a-to-1ghz-tds784a/msg1032649/#msg1032649">other</a> 
comments that confirm this theory.</p>

<h2 id="measuring-along-the-signal-path">Measuring Along the Signal Path</h2>

<p>Let’s verify this by measuring a few signals on the board with a different scope.
The ADC input pins are large enough to attach a 
<a href="https://tomverbeure.github.io/2025/04/12/DSLogic-U3Pro16-Teardown.html#probe-cables-and-clips">Tektronix logic analyzer probe</a>:</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/probing_the_adc.jpg" alt="Probing the ADC input"></p>

<p><strong>ADC sampling the signal</strong></p>

<p>With a 1 MHz signal and using a 100Msps sample rate, the input to the ADC looks like this:</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/1_1MHz_100Msps_no_clock.png" alt="1 MHz, 100 Msps waveform at ADC"></p>

<p>The input to the ADC is clearly chopped into discrete samples, with a new sample
every 120 ns. We can discern a sine wave in the samples, but there’s a lot of noise on the
signal too. Meanwhile the TDS684B CRT shows a nice and clean 1 MHz signal. I haven’t
been able to figure out how that’s possible.</p>

<p>For some reason, simply touching the clock pin of the ADC with a 1 MOhm oscilloscope probe 
adds a massive amount of noise to the input signal, but it shows the clock nicely:</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/2_1MHz_100Msps_with_clock.png" alt="1 MHz, 100 Msps waveform at ADC with clock included"></p>

<p>The ADC clock matches the input signal. It’s indeed 8.33 MHz.</p>

<p><strong>Acquistion refresh rate</strong></p>

<p>The scope only records in bursts. When recording 500, 1000 or 2500 sample points at 100Msps, 
it records a new burst every 14ms or 70Hz.</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/3_100MSps_2500pts_70Hz.png" alt="Acquistion refresh for 2500 points, 100Msps"></p>

<p>When recording 5000 points, the refresh rate drops to 53Hz. For 15000 points, it drops
even lower, to 30Hz:</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/4_100Msps_15000pts_29Hz.png" alt="Acquistion refresh for 15000 points, 100Msps"></p>

<p><strong>Sampling burst duration</strong></p>

<p>The duration of a sampling burst is always 2 ms, irrespective of the sample rate
of the oscilloscope or the number of points acquired! The combination of a
2 ms burst and 8 MHz sample clock results in 16k samples. So the scope always
acquires what’s probably the full contents of the CCD FIFO and throws a large
part away when a lower sample length is selected.</p>

<p>Here’s the 1 MHz signal sampled at 100 Msps:</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/5_1MHz_100Msps_2ms_acq.png" alt="Sampling burst of 2ms at 100 Msps"></p>

<p>And here’s the same signal sampled at 5 Gsps:</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/6_1MHz_5Gsps_2ms_acq.png" alt="Sampling burst of 2ms at 5 Gsps"></p>

<p>It looks like the signal doesn’t scan out of the CCD memory in the order
it was received, hence the signal discontinuity in the middle.</p>

<p><strong>Sampling a 1 GHz signal</strong></p>

<p>I increased the input signal from 1 MHz to 1 GHz. Here’s the
ADC input at 5 Gsps:</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/9_1GHz_5Gsps_8.3Mhz_sample_clock.png" alt="Sampling a 1 GHz signal at 5 Gsps"></p>

<p>With a little bit of effort, you can once again imagine a sine wave
in those samples. There’s periodicity of 5 samples, as one would expect
for a 1 GHz to 5 Gsps ratio. The sample rate is still 8.3 MHz.</p>

<p><strong>Sampling a 200 MHz signal</strong></p>

<p>I also applied a 200 MHz input signal.</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/12_200MHz_5Gsps_2.73us_period.png" alt="Sampling a 200 MHz signal at 5 Gsps"></p>

<p>The period is now ~22 samples, as expected.</p>

<p>200 MHz is low enough to measure with my 350 MHz bandwidth Siglent oscilloscope. To confirm that
the ADG286D chip contains the CCD memory, I measured the signal on one of the
differential pins going into that chip:</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/probing_input_adg286b.jpg" alt="Probing the input of ADG286D"></p>

<p>And here it is, a nice 200 MHz signal:</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/13_200MHz_going_into_CCD.png" alt="200 MHz seen on the input of the ADG286D chip"></p>

<h2 id="a-closer-look-at-the-noise-issue">A Closer Look at the Noise Issue</h2>

<p>After initially publishing this blog post, I had a discussion on Discord about the noise issue
which made me do a couple more measurements.</p>

<p><strong>Input connected to ground</strong></p>

<p>Here’s what the ADC input looks like when the input of the scope is connected to ground:</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/20_input_to_gnd_full.png" alt="Input connected to ground - full burst"></p>

<p>2 major observations:</p>

<ul>
  <li>there’s a certain amount of repetitiveness to it.</li>
  <li>there are these major voltage spikes in between each repetition. 
They are very faint on the scope shot.</li>
</ul>

<p>Let’s zoom in on that:</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/21_input_to_gnd_zoom.png" alt="Input connected to ground - zoomed"></p>

<p>The spikes are still hard to see so I added the arrows, but look how the sample pattern repeats 
after each spike!</p>

<p>The time delay between each spike is ~23.6 us. With a sample rate of 120ns, that converts into
a repetitive pattern of ~195 samples.</p>

<p>I don’t know why a pattern of 195 samples exists, but it’s clear that each of those 195 locations
have a fixed voltage offset. If the scope measures those offsets during calibration, it can
subtract them after measurement and get a clean signal out.</p>

<p><strong>50 kHz square wave</strong></p>

<p>Next I applied a 50kHz square wave to the input. This frequency was chosen so that, for the selected
sample rate, a single period would cover the 15000 sampling points.</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/23_square_wave_full.png" alt="Square wave - full burst"></p>

<p>2 more observations:</p>

<ul>
  <li>the micro-repetitiveness is still there, irrespective of the voltage offset due to the input signal. 
That means that subtracting the noise should be fine for different voltage inputs.</li>
  <li>We don’t see a clean square wave outline. It looks like there’s some kind of address interleaving
going on.</li>
</ul>

<p><strong>50kHz sawtooth wave</strong></p>

<p>We can see the interleaving even better when applying a sawtooth wavefrom that covers one burst:</p>

<p><img src="https://tomverbeure.github.io/assets/tds684b/26_sawtooth_full.png" alt="Sawtooth wave - full burst"></p>

<p>Instead of a clean break from high-to-low somewhere in the middle, there is a transition period
where you get both high and low values. This confirms that some kind of interleaving is happening.</p>

<h2 id="conclusion">Conclusion</h2>

<ul>
  <li>The TDS684B captures input signals at high speed in an analog memory and digitizes them at 8 MHz.</li>
  <li>The single-ended input to the ADC is noisy yet the signal looks clean when displayed on the CRT of the
scope, likely because the noise pattern is repetitive and predictable.</li>
  <li>In addition to noise, there’s also an interleaving pattern during the reading out of the analog FIFO 
contents.</li>
  <li>The number of samples digitized is always the same, irrespective of the settings in the
horizontal acquisition menu.</li>
</ul>

<p><em>(Not written by ChatGPT, I just like to use bullet points…)</em></p>


  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Bracket – selfhosted tournament system (141 pts)]]></title>
            <link>https://github.com/evroon/bracket</link>
            <guid>43895456</guid>
            <pubDate>Mon, 05 May 2025 14:20:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/evroon/bracket">https://github.com/evroon/bracket</a>, See on <a href="https://news.ycombinator.com/item?id=43895456">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/evroon/bracket/blob/master/frontend/public/favicon-wide.svg"><img width="500" src="https://github.com/evroon/bracket/raw/master/frontend/public/favicon-wide.svg" alt="Bracket - Tournament System"></a>
</p>
<p dir="auto">
  <a href="https://github.com/evroon/bracket/actions"><img src="https://camo.githubusercontent.com/ccfa14f52676b0092543f3b72359e8a3595d4e5a9f111b58b308d96ef9c1fdad/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f6576726f6f6e2f627261636b65742f6261636b656e642e796d6c" alt="build status" data-canonical-src="https://img.shields.io/github/actions/workflow/status/evroon/bracket/backend.yml"></a>
  <a href="https://crowdin.com/project/bracket" rel="nofollow"><img src="https://camo.githubusercontent.com/1f64c284e1fa22e1e72374f0e4a8f589c7313400d62c337a565936e2672e6c60/68747470733a2f2f6261646765732e63726f7764696e2e6e65742f627261636b65742f6c6f63616c697a65642e737667" alt="translations" data-canonical-src="https://badges.crowdin.net/bracket/localized.svg"></a>
  <a href="https://github.com/evroon/bracket/commits/"><img src="https://camo.githubusercontent.com/38a5ac206dbef151607627d0ac681e963d4d1b595f604c03bb680d3285519072/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6173742d636f6d6d69742f6576726f6f6e2f627261636b6574" alt="last commit" data-canonical-src="https://img.shields.io/github/last-commit/evroon/bracket"></a>
  <a href="https://github.com/evroon/bracket/releases"><img src="https://camo.githubusercontent.com/467f10ca6354ece9832b9f02af28b3a525b3ee24d56b168c68264ebff36e32df/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f6576726f6f6e2f627261636b6574" alt="release" data-canonical-src="https://img.shields.io/github/v/release/evroon/bracket"></a>
  <a href="https://codecov.io/gh/evroon/bracket" rel="nofollow"><img src="https://camo.githubusercontent.com/a0c86e929956b818f5646c51cc8a509eca10d82e7b98c12e42e57363de24efff/68747470733a2f2f636f6465636f762e696f2f67682f6576726f6f6e2f627261636b65742f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d594a4c30445650464647" alt="codecov" data-canonical-src="https://codecov.io/gh/evroon/bracket/branch/master/graph/badge.svg?token=YJL0DVPFFG"></a>
</p>
<p dir="auto">
  <a href="https://www.bracketapp.nl/demo" rel="nofollow">Demo</a>
  ·
  <a href="https://docs.bracketapp.nl/" rel="nofollow">Documentation</a>
  ·
  <a href="https://docs.bracketapp.nl/docs/running-bracket/quickstart" rel="nofollow">Quickstart</a>
  ·
  <a href="https://github.com/evroon/bracket">GitHub</a>
  ·
  <a href="https://github.com/evroon/bracket/releases">Releases</a>
</p>

<p dir="auto">Tournament system meant to be easy to use. Bracket is written in async Python (with
<a href="https://fastapi.tiangolo.com/" rel="nofollow">FastAPI</a>) and <a href="https://nextjs.org/" rel="nofollow">Next.js</a> as frontend using the
<a href="https://mantine.dev/" rel="nofollow">Mantine</a> library.</p>
<p dir="auto">It has the following features:</p>
<ul dir="auto">
<li>Supports <strong>single elimination, round-robin and swiss</strong> formats.</li>
<li><strong>Build your tournament structure</strong> with multiple stages that can have multiple groups/brackets in
them.</li>
<li><strong>Drag-and-drop matches</strong> to different courts or reschedule them to another start time.</li>
<li>Various <strong>dashboard pages</strong> are available that can be presented to the public, customized with a
logo.</li>
<li>Create/update <strong>teams</strong>, and add players to <strong>teams</strong>.</li>
<li>Create <strong>multiple clubs</strong>, with <strong>multiple tournaments</strong> per club.</li>
<li><strong>Swiss tournaments</strong> can be handled dynamically, with automatic scheduling of matches.</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/evroon/bracket/blob/master/docs/content/img/bracket-screenshot-design.png"><img alt="" src="https://github.com/evroon/bracket/raw/master/docs/content/img/bracket-screenshot-design.png" width="100%"></a></p>
<p dir="auto">
<a href="https://docs.bracketapp.nl/" rel="nofollow"><strong>Explore the Bracket docs&nbsp;&nbsp;▶</strong></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Live Demo</h2><a id="user-content-live-demo" aria-label="Permalink: Live Demo" href="#live-demo"></a></p>
<p dir="auto">A demo is available for free at <a href="https://www.bracketapp.nl/demo" rel="nofollow">https://www.bracketapp.nl/demo</a>. The demo lasts for 30 minutes, after which
your data will de deleted.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quickstart</h2><a id="user-content-quickstart" aria-label="Permalink: Quickstart" href="#quickstart"></a></p>
<p dir="auto">To quickly run bracket to see how it works, clone it and run <code>docker compose up</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:evroon/bracket.git
cd bracket
sudo docker compose up -d"><pre>git clone git@github.com:evroon/bracket.git
<span>cd</span> bracket
sudo docker compose up -d</pre></div>
<p dir="auto">This will start the backend and frontend of Bracket, as well as a postgres instance. You should now
be able to view bracket at <a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a>. You can log in with the following credentials:</p>
<ul dir="auto">
<li>Username: <code>test@example.org</code></li>
<li>Password: <code>aeGhoe1ahng2Aezai0Dei6Aih6dieHoo</code>.</li>
</ul>
<p dir="auto">To insert dummy rows into the database, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo docker exec bracket-backend pipenv run ./cli.py create-dev-db"><pre>sudo docker <span>exec</span> bracket-backend pipenv run ./cli.py create-dev-db</pre></div>
<p dir="auto">See also the <a href="https://docs.bracketapp.nl/docs/running-bracket/quickstart" rel="nofollow">quickstart docs</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Read the <a href="https://docs.bracketapp.nl/docs/usage/guide" rel="nofollow">usage guide</a> for how to organize a tournament in Bracket from start to finish.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">Read the <a href="https://docs.bracketapp.nl/docs/running-bracket/configuration" rel="nofollow">configuration docs</a> for how to configure Bracket.</p>
<p dir="auto">Bracket's backend is configured using <code>.env</code> files (<code>prod.env</code> for production, <code>dev.env</code> for development etc.).
But you can also configure Bracket using environment variables directly, for example by specifying them in the <code>docker-compose.yml</code>.</p>
<p dir="auto">The frontend doesn't can be configured by environment variables as well, as well as <code>.env</code> files using Next.js' way of loading environment variables.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running Bracket in production</h2><a id="user-content-running-bracket-in-production" aria-label="Permalink: Running Bracket in production" href="#running-bracket-in-production"></a></p>
<p dir="auto">Read the <a href="https://docs.bracketapp.nl/docs/deployment" rel="nofollow">deployment docs</a> for how to deploy Bracket and run it in production.</p>
<p dir="auto">Bracket can be run in Docker or by itself (using <code>pipenv</code> and <code>yarn</code>).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development setup</h2><a id="user-content-development-setup" aria-label="Permalink: Development setup" href="#development-setup"></a></p>
<p dir="auto">Read the <a href="https://docs.bracketapp.nl/docs/community/development" rel="nofollow">development docs</a> for how to run Bracket for development.</p>
<p dir="auto">Prerequisites are <code>yarn</code>, <code>postgresql</code> and <code>pipenv</code> to run the frontend, database and backend.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Translations</h2><a id="user-content-translations" aria-label="Permalink: Translations" href="#translations"></a></p>
<p dir="auto">Based on your browser settings, your language should be automatically detected and loaded. For now,
there's no manual way of choosing a different language.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Languages</h2><a id="user-content-supported-languages" aria-label="Permalink: Supported Languages" href="#supported-languages"></a></p>
<p dir="auto">To add/refine translations, <a href="https://crowdin.com/project/bracket" rel="nofollow">Crowdin</a> is used.
See the <a href="https://docs.bracketapp.nl/docs/community/contributing/#translating" rel="nofollow">docs</a> for more information.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">More screenshots</h2><a id="user-content-more-screenshots" aria-label="Permalink: More screenshots" href="#more-screenshots"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/evroon/bracket/blob/master/docs/content/img/schedule_preview.png"><img alt="" src="https://github.com/evroon/bracket/raw/master/docs/content/img/schedule_preview.png" width="50%"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/evroon/bracket/blob/master/docs/content/img/planning_preview.png"><img alt="" src="https://github.com/evroon/bracket/raw/master/docs/content/img/planning_preview.png" width="50%"></a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/evroon/bracket/blob/master/docs/content/img/builder_preview.png"><img alt="" src="https://github.com/evroon/bracket/raw/master/docs/content/img/builder_preview.png" width="50%"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/evroon/bracket/blob/master/docs/content/img/standings_preview.png"><img alt="" src="https://github.com/evroon/bracket/raw/master/docs/content/img/standings_preview.png" width="50%"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Help</h2><a id="user-content-help" aria-label="Permalink: Help" href="#help"></a></p>
<p dir="auto">If you're having trouble getting Bracket up and running, or have a question about usage or configuration, feel free to ask.
The best place to do this is by creating a <a href="https://github.com/evroon/bracket/discussions">Discussion</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supporting Bracket</h2><a id="user-content-supporting-bracket" aria-label="Permalink: Supporting Bracket" href="#supporting-bracket"></a></p>
<p dir="auto">If you're using Bracket and would like to help support its development, that would be greatly appreciated!</p>
<p dir="auto">Several areas that we need a bit of help with at the moment are:</p>
<ul dir="auto">
<li>⭐ <strong>Star Bracket</strong> on GitHub</li>
<li>🌐 <strong>Translating</strong>: Help make Bracket available to non-native English speakers by adding your language (via <a href="https://crowdin.com/project/bracket" rel="nofollow">crowdin</a>)</li>
<li>📣 <strong>Spread the word</strong> by sharing Bracket to help new users discover it</li>
<li>🖥️ <strong>Submit a PR</strong> to add a new feature, fix a bug, extend/update the docs or something else</li>
</ul>
<p dir="auto">See the <a href="https://docs.bracketapp.nl/docs/community/contributing" rel="nofollow">contribution docs</a> for more information on how to contribute</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributors</h2><a id="user-content-contributors" aria-label="Permalink: Contributors" href="#contributors"></a></p>

<markdown-accessiblity-table><table>
<tbody><tr>
    <td>
        <a href="https://github.com/evroon">
            <img src="https://avatars.githubusercontent.com/u/11857441?v=4" width="100;" alt="evroon">
            <br>
            <sub><b>Erik Vroon</b></sub>
        </a>
    </td>
    <td>
        <a href="https://github.com/robigan">
            <img src="https://avatars.githubusercontent.com/u/35210888?v=4" width="100;" alt="robigan">
            <br>
            <sub><b>Null</b></sub>
        </a>
    </td>
    <td>
        <a href="https://github.com/BachErik">
            <img src="https://avatars.githubusercontent.com/u/75324423?v=4" width="100;" alt="BachErik">
            <br>
            <sub><b>BachErik</b></sub>
        </a>
    </td>
    <td>
        <a href="https://github.com/djpiper28">
            <img src="https://avatars.githubusercontent.com/u/13609136?v=4" width="100;" alt="djpiper28">
            <br>
            <sub><b>Danny Piper</b></sub>
        </a>
    </td>
    <td>
        <a href="https://github.com/Sevichecc">
            <img src="https://avatars.githubusercontent.com/u/91365763?v=4" width="100;" alt="Sevichecc">
            <br>
            <sub><b>SevicheCC</b></sub>
        </a>
    </td>
    <td>
        <a href="https://github.com/nvanheuverzwijn">
            <img src="https://avatars.githubusercontent.com/u/943226?v=4" width="100;" alt="nvanheuverzwijn">
            <br>
            <sub><b>Nicolas Vanheuverzwijn</b></sub>
        </a>
    </td></tr>
<tr>
    <td>
        <a href="https://github.com/IzStriker">
            <img src="https://avatars.githubusercontent.com/u/44909896?v=4" width="100;" alt="IzStriker">
            <br>
            <sub><b>IzStriker</b></sub>
        </a>
    </td>
    <td>
        <a href="https://github.com/babeuh">
            <img src="https://avatars.githubusercontent.com/u/60193302?v=4" width="100;" alt="babeuh">
            <br>
            <sub><b>Raphael Le Goaller</b></sub>
        </a>
    </td></tr>
</tbody></table></markdown-accessiblity-table>

<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Bracket is licensed under <a href="https://choosealicense.com/licenses/agpl-3.0/" rel="nofollow">AGPL-v3.0</a>.</p>
<p dir="auto">Please note that any contributions also fall under this license.</p>
<p dir="auto">See <a href="https://github.com/evroon/bracket/blob/master/LICENSE">LICENSE</a></p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>